{"title": "COS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via Language Models", "authors": ["Tobias Materzok"], "abstract": "We present COS(M+O)S, a System 2-inspired framework for open-ended plot development that systematically explores the vast space of possible story expansions, enabling a 3B-parameter language model to approach the plot quality of a 70B model on select short-story tasks. The method accomplishes this by combining Monte Carlo Tree Search (MCTS), guided by a step-level value model that rewards moderate surprisal (curiosity) while penalizing incoherence, and Odds Ratio Preference Optimization (ORPO) to fine-tune the policy on high-value plot expansions. This iterative reinforcement learning loop systematically explores multiple candidate plot branches, back-propagates quality signals, and adapts the policy for faster convergence, notably shifting the policy from puzzle-based Chain-of-Thought to more character-driven storytelling. In small-scale tests with short-story prompts, 67%-77% of participants favored COS(M+O)S's highest-rated expansions over lower-rated ones, suggesting that our learned value function aligns. GPT-40 ratings further show that COS(M+O)S surpasses naive single-pass decoding from Llama 3.2 3B by 0.59 SD, coming within 0.06 SD of Llama 3.1 70B (no significant difference, p = 0.93). Pairwise comparisons with o1 place COS(M+O)S 1.5 SD above the 3B baseline and find no statistically significant gap from 70B. Nevertheless, absolute story quality remains modest, constrained by the small model's capacity and limited training data.", "sections": [{"title": "1 Introduction", "content": "Recent advances in large language models (LLMs) have revealed their remarkable capacity for composing coherent prose, storytelling, and even tackling mathematical or coding problems. Yet LLM generators default to a single-pass System 1 style of reasoning (Kahneman and Frederick, 2002), generating story text (or solutions) in one shot based on autoregressive next-token predictions. This approach yields predictable or formulaic expansions that, in the context of plot quality, lack novelty and engagement. Indeed, under a standard cross-entropy training regime, the most frequent continuations from the training corpus tend to dominate, limiting creativity. Even with higher sampling temperatures, autoregressive decoding still draws from these archetypes."}, {"title": "2 Related Work", "content": "MCTS has demonstrated its effectiveness across multiple decision-making domains, from the seminal work in AlphaGo (Silver et al., 2016) and AlphaZero (Silver et al., 2017) to mathematical problem-solving (Trinh et al., 2024), and robotics tasks under partial observability (Garrett et al., 2020). In text generation, Yao et al. (Yao et al., 2019) proposed a plan-and-write framework that creates a plot outline before expanding it into a full narrative, but it operates primarily in a single pass rather than through iterative search. In parallel, reward-shaping approaches have been used to control neural story plot generation toward specific goals (Tambwekar et al., 2019). Meanwhile, the notion of surprisal from psycholinguistics (Hale, 2001; Levy, 2008) has motivated curiosity-driven"}, {"title": "3 Methods", "content": "Overview of the Proposed Framework\nCOS(M+O)S integrates a policy model, a simulation model, and a step-level value model within the MCTS framework. The policy model \\(\\pi(a|s)\\) proposes candidate plot actions at a given story state s, the simulation model uses these actions to advance the story (Section 3.2). The step-level value model \\(V(s)\\) (Section 3.3) evaluates the quality of the resulting plots. By iteratively applying MCTS, we explore a large space of potential stories, balancing exploration of new plot branches with exploitation of promising ones. We then apply ORPO to fine-tune the policy model, using MCTS-derived preferences as training signals."}, {"title": "3.1 Monte Carlo Tree Search (MCTS)", "content": "We treat plot development as a sequential decision process represented by a tree T. Each node s corresponds to a partial (or fully generated) story state, and edges represent possible plot-expanding actions \\(a \\in A(s)\\). Fully generated stories appear at final-depth states \\(L_f\\), from which we can define\n\\[V_{\\text{max}}^{(\\text{final})} = \\max_{s \\in L_f} V(s),\\]\nthe highest estimated plot quality found during search.\nWe employ a standard MCTS procedure (Kocsis and Szepesv\u00e1ri, 2006; Browne et al., 2012) with five phases per iteration:"}, {"title": "3.2 Policy and Simulation Models", "content": "Policy Model and ORPO Optimization We employ a language model (Llama 3.2 3B) to serve as our policy \\(\\pi(a|s)\\) that generates possible plot actions a given the current story state s (akin to a AlphaZero approach). To elicit plot actions (CoTs) in a structured format, we prompt our base policy model \\(\\pi_0(a|s)\\) with a fixed template (sampling temperature of 0.7). Initially, MCTS exclusively uses \\(\\pi_0(a|s)\\) to propose actions at each step.\nAfter each round of MCTS searching (Section 4.1), we apply ORPO to fine-tune the policy. Specifically, we leverage the action-value estimates Q(s, a) (obtained via the value model and backpropagation; Equation 4) to label actions as \"chosen\" (i.e., higher-quality) or \"rejected\" (lower-quality). During ORPO, we increase the probability that the policy selects chosen actions while decreasing the probability of rejected ones, progressively approximating an optimal policy \\(\\pi^*\\). This process aligns the policy \\(\\pi_{\\text{trained}}(a|s)\\) toward higher-value trajectories uncovered by MCTS."}, {"title": "3.3 Step-Level Value Model", "content": "We define V(s) as our quality measure for a story state s. A calibrated Support Vector Classifier (SVC) distinguishes plots as \"good\" or \"bad\" based on PCA-transformed features, outputting a probability that the story is \"good\". We adopt this probability as \\(V(s) \\in [0,1]\\). Higher values indicate a greater likelihood of high plot quality under our chosen metrics, while we recognize that true story quality remains inherently subjective.\nCuriosity Index To approximate intellectual engagement (novelty), we introduce a curiosity index based on token-level surprisal computed using the Phi-3.5 3B model (Abdin et al., 2024).\nSurprisal for each token i is \\(S(i) = - \\log_2 P(i | \\text{context}_{<i})\\), where \\(P(i | \\text{context}_{<i})\\) is the token probability given its preceding context. We map S(i) to an interest function I(i) inspired by the (inverted-U) Wundt curve (Bianchi, 2014):\n\\[I(i) = e^{-\\frac{(S(i) - S_0)^2}{2\\sigma^2}}. \\]\nIn Equation 5, \\(S_0\\) represents the optimal surprisal level, and \\(\\sigma\\) controls the spread of the engagement peak. We set \\(S_0\\) = 4 bits and \\(\\sigma\\) = 0.6 bits based on hyperparameter tuning (Figure 2). Averaging I(i) over all tokens yields the curiosity index. Figure 9 in the Appendix C shows the distribution of the curiosity index for both classes."}, {"title": "4 Experiments and Results", "content": "4.1 Experimental Setup\nWe evaluated our proposed framework on short-story generation tasks, using 18 randomly selected stories in total.\nIn Round 0, MCTS used the base (untrained) policy to propose actions during MCTS. After collecting each action's Q-value, we applied ORPO fine-tuning to form the Round 1 policy. We then repeated MCTS on fresh prompts, gathered new data, performed another round of ORPO, and obtained the Round 2 policy. Throughout, we measured final-depth story quality via \\(V_{\\text{max}}^{(\\text{final})}\\) (Section 3.1; Equation 1). Notably, each new round used a fresh set of initial prompts (randomly chosen), ensuring that the fine-tuned policy was evaluated on out-of-distribution story contexts rather than reusing the same data from previous rounds."}, {"title": "4.2 Results", "content": "Value Model Performance We first evaluate the value model's ability to distinguish \"good\" and \"bad\" stories. Table 1 shows detailed classification metrics on the held-out test set. The model achieves a macro-average F1 score of 0.82.\nAppendix D, we visualize the SVC's decision boundaries using the first two and three principal components, present the confusion matrix, and show the value model's accuracy as a function of how much of the story has been generated (Figure 13). These results confirm that accuracy improves with plot progression.\nEffectiveness of MCTS and Policy Model Refinement We hypothesized that our MCTS framework would improve plot quality through iterative exploration alone, and that further preference alignment via ORPO would accelerate these gains. To test this, we conducted three rounds of experiments, each comprising six separate MCTS runs initialized with different story prompts, resulting in just"}, {"title": "5 Discussion", "content": "Our results show that combining MCTS with preference-based fine-tuning (ORPO) can improve plot quality for a smaller 3B-parameter model, narrowing the gap to a 23\u00d7 larger 70B model on our short-story tasks. Rather than relying on a single-pass strategy, the MCTS approach systematically explores multiple plot branches, and ORPO progressively internalizes the best expansions for faster convergence. Qualitative and quantitative evaluations (including a small A/B test with 8 participants on 4 prompts, in which 67%\u201377% favored MCTS's top trajectories, plus GPT-40- and o1-based ratings) confirm these improvements, suggesting that System 2-inspired iterative techniques can act as another improvement dimension. At the same time, we observe a log-linear relationship between the number of MCTS iterations and quality gains in our tested regime, meaning significant computational overhead as stories grow in length. On these limited test prompts, the 3B approach performed comparably to the 70B baseline. The demonstrated synergy between MCTS-driven exploration and fine-tuned policy refinement offers a promising path for gen-"}, {"title": "6 Conclusion", "content": "We presented COS(M+O)S as a search-based framework integrating MCTS-guided CoT generation, step-level value modeling, and ORPO-based reinforcement learning to enhance LLM-driven storytelling. By using step-level evaluations after each expansion, we uncovered higher-value plot expansions. We showed that MCTS alone improved plot quality but scales only log-linearly with compute; by fine-tuning the policy on MCTS-discovered preferences, we accelerated convergence and reduce brute-force search costs. Nevertheless, these are only relative improvements, and the absolute story quality remains modest, limited by the small model size and training data. Evaluations with human readers, GPT-40, and o1 confirmed that COS(M+O)S outperforms naive decoding on our test set. Future work can investigate larger backbones, domain-specific rewards, or explicit reference-tracking to further enhance long-form story coherence."}, {"title": "6.1 Limitations and Future Directions", "content": "Model size: We rely on 3B-parameter models for both policy and simulation due to limited compute resources. Using far larger models (e.g., 70B+ parameters) would boost story quality and may reveal emergent capabilities, but could also complicate the log-linear improvement pattern. We might need careful hyperparameter tuning to maintain stable gains at larger scales.\nValue model coverage: Our surprisal- and coherence-based metrics are aimed to approximate cognitive processes but do not capture deeper literary elements such as thematic development or nu-"}, {"title": "Appendices", "content": "These Appendices detail our implementation strategies, data processing pipelines, and evaluation procedures. Section A describes the specific algorithmic strategies for Monte Carlo Tree Search"}, {"title": "A Detailed MCTS Implementation and Algorithmic Strategies", "content": "Tree Structure and Representation We represent the plot development process as a tree T. Each node \\(s \\in S\\) corresponds to a partial (or fully generated) story state, containing all plot content generated so far. In other words, each node embodies the entire text from the beginning up to that node's depth in the story; deeper nodes represent progressively longer story prefixes. A leaf node might be a fully written story (maximum depth) or a partially written one that is not (yet) further expanded.\nEach edge \\(a \\in A(s)\\) represents a possible plot development, transitioning from one story state s to another s'.\nMCTS Algorithm Steps Our Monte Carlo Tree Search procedure (Kocsis and Szepesv\u00e1ri, 2006; Browne et al., 2012) consists of five main steps each iteration:\nSelection: Starting from the root, we select child nodes that maximize the Upper Confidence Bound (UCB) criterion (Equation 7):\n\\[a^* = \\arg \\max_a \\Big[Q(s, a) + c \\sqrt{\\frac{\\ln N(s)}{N(s, a)}}\\Big],\\]\nwhere Q(s, a) is the current action-value estimate, N(s) is the visit count of node s, N(s, a) is the visit count of action a at state s, and c is the exploration parameter.\nExpansion: When we select a node s that allows further exploration, we generate new actions a using the policy model \\(\\pi(a|s)\\). Each selected action leads to a new child node s', growing T.\nSimulation: For each newly expanded node s', the simulation model produces the next segment of the story by applying the chosen action a to the current story state. This step advances the plot."}, {"title": "B ORPO Fine-Tuning: Implementation Details and Dataset Construction", "content": "Pairwise Preference Dataset Generation and Preference-Based Fine-Tuning with ORPO To construct the preference dataset for ORPO, we begin by examining each parent node s in the MCTS-generated search tree T. For each child action \\(a \\in A(s)\\) with successor node s', we retrieve its action-value estimate Q(s, a) (see Equation 4). We then form ordered pairs ((s, ai), (s, aj)) in which ai is deemed preferable to aj if the difference in their action values exceeds a minimum threshold, \\(\\Delta Q_{\\text{min}}\\,i.e.,\\,\\Delta Q = Q(s, a_i) - Q(s, a_j) \\geq \\Delta Q_{\\text{min}}\\,\\) and if Q(s, ai) is above 0.5, indicating that ai leads to an overall \"good\" trajectory. Next, we compute a score for each pair to balance the absolute action-value of ai with its advantage over aj. Concretely,\nScore = \\(\\beta_{\\text{tradeoff}} Q(s, a_i) + (1 - \\beta_{\\text{tradeoff}}) \\Delta Q\\),"}, {"title": "C Additional Value Model Details", "content": "Semantic Coherence Score To measure thematic consistency across the plot, we encode each sentence into a high-dimensional embedding vector using a pretrained SentenceTransformer model, then compute all pairwise cosine similarities. The semantic coherence score is the average pairwise similarity across all sentences:\n\\[\\text{Coherence Score} = \\frac{2}{n(n-1)} \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\frac{\\vec{E_i} \\cdot \\vec{E_j}}{|\\vec{E_i}|| |\\vec{E_j}||},\\]\nHere, n is the number of sentences, and \\(E_i\\) is the embedding of the ith sentence. By considering all sentence pairs, we aim to capture both local and global coherence, such that stories featuring delayed thematic connections are not penalized"}, {"title": "F Evaluation Protocols and A/B Preference Tests", "content": "A/B Preference Test with Human Participants\nWe recruited eight university-educated participants\n(mean age around 30 years) from the authors' social circles. All were fluent in English, were informed about the study's nature, and gave consent. Prior to viewing any stories, each participant received a short \"Participant Manual\" that explained the study's purpose and how to interpret the bullet-point story outlines. The manual emphasized that the stories came from a relatively simple language model, that they should evaluate the plots rather"}]}