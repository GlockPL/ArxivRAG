{"title": "Scaling CS1 Support with Compiler-Integrated Conversational AI", "authors": ["Jake Renzella", "Alexandra Vassar", "Lorenzo Lee Solano", "Andrew Taylor"], "abstract": "This paper introduces DCC Sidekick, a web-based conversational AI tool that enhances an existing LLM-powered C/C++ compiler by generating educational programming error explanations. The tool seamlessly combines code display, compile- and run-time error messages, and stack frame read-outs alongside an AI interface, lever-aging compiler error context for improved explanations. We analyse usage data from a large Australian CS1 course, where 959 students engaged in 11,222 DCC Sidekick sessions, resulting in 17,982 error explanations over seven weeks. Notably, over 50% of interactions occurred outside business hours, underscoring the tool's value as an always-available resource. Our findings reveal strong adoption of AI-assisted debugging tools, demonstrating their scalability in supporting extensive CS1 courses. We provide implementation in-sights and recommendations for educators seeking to incorporate Al tools with appropriate pedagogical safeguards.", "sections": [{"title": "1 INTRODUCTION", "content": "The ability to write, understand and debug code are essential compo-nents to becoming a computer programmer [3, 9, 20]. High-quality feedback and guidance are often necessary to help with these types of skills. Explanations of code can also help students make deeper connections and build schemas of programming which improve their overall reasoning skills [19, 20].\nTraditionally, higher education models utilise, in part, teaching staff to deliver explanations to students. Drop-in help sessions, consultation hours, or online forums [6] provide important oppor-tunities for students to debug errors, clarify misconceptions, and otherwise keep students on track. With increasing enrolments and larger class sizes, the demands on help resources grows.\nOver the past two years, educators have explored utilising Large Language Models (LLMs) to generate error explanations [17, 21, 26], supporting time-limited educators and potentially re-imagining the way we teach and learn [10].\nIn this paper, we attempt to answer two main research questions:\nRQ1 How do CS1 students engage with a compiler-integrated, conversational AI tool for programming error explanations, and to what extent do they prefer this over a non-conversational, one-shot alternative?\nRQ2 How are generative AI explanation tools used across compile-time vs run-time programming errors?"}, {"title": "2 RELATED WORK", "content": "There has been increased interest in generative AI over the last few years, sparked by the release of OpenAI's ChatGPT. LLMs, such as ChatGPT, are based upon transformer architecture, and at their core exploit the fact that language follows a specific orderly structure. These models are trained over large quantities of text scraped from the internet. Most LLMs are such as Codex, are also trained on millions of lines of code scraped from open-source repositories, and demonstrate capabilities in code authorship using code-writing benchmarking [2].\n2.1 Large Language Models and Tools in CS1\nThere are many ideas around how large language models can be applied in introductory computing to support student learning, and provide efficiencies. In introductory programming, LLMs have been used to solve simple CS1 programming exercises, with varying degrees of success [4, 5, 7, 8, 29]. Models have also been used to generate explanations for CS1 code. MacNeil et al. [18] generated code explanations and integrated these into an interactive e-book. They found that students viewed these explanations and perceived them to be helpful [18]. Another example of using large language models in generating code explanations has been in Harvard's CS50 program, via a rubber duck persona [17]. This tool has been used over 50,000 times since June 2023 with an average of 15 prompts per user per day. Integrated into the programming IDE, it is able to provide explanations of code and has been positively received by students, however, no analysis of student learning was provided. Others have integrated large language models into the compiler to assist with interpreting and solving programming error messages as they occur [26].\nStudies have evaluated the efficacy of LLM explanations, demon-strating that they produce sufficient explanations of code in in-troductory programming [11, 13]. Tools such as the web-based CodeHelp tool, provide immediate support to students working on programming exercises and wanting assistance [16], which re-duced the overall anxiety of students who were worried about asking educators for help. One limitation of CodeHelp and other error one-shot explanation tools is that they do not take advantage of the conversational capabilities of large language models, and as such do not allow for follow-up clarifications.\nThere is growing research into understanding the impact these tools have on learning outcomes. One study found that students who used Codex to learn Python could write better code than the group who did not use Codex, and demonstrated a similar under-standing to the control group [12]. Prather et al. [22] found that novices struggled to understand and use these generative AI tools, and were wary about the use of such tools. Issues of over-reliance were also observed [22]. Instructors report serious concerns re-garding academic integrity, cheating, and a potential lack of equity, access as well as ethical objections [15].\n2.2 The Socratic Method\nThe Socratic method has a long history of use in various educa-tional domains to develop student critical thinking skills [14]. This method of instruction proposes guiding student comprehension and learning through the use of guided questions, paying attention to key aspects of the target topic or concept, to help arrive at a so-lution. These guided questions are based on constructivist learning theories, which propose that learners build knowledge by doing rather than being told [1]. This is also supported by cognitive psy-chology literature, which states acquiring knowledge is a function of time and conscious effort [23]. The Socratic method has been used extensively in other domains, most commonly law, but there have been few applications in computing education [24]. One such study by Tamang et al. [24] used the Socratic method in teaching introductory computing skills and found that this method of guided self-explanation is more effective than free self-explanations for novice code comprehension. We seek to explore how DCC Sidekick can serve as a vehicle for the Socratic method within the intro-ductory computing course, guiding students to arrive at a solution independently, thereby improving learning outcomes."}, {"title": "3 MOTIVATION", "content": "In our large Australian University, 3,500 students enrol in the CS1 course each year. Recent and continued growth of student enrol-ments is managed with a comprehensive programme of student support. In addition to scheduled class time, we run frequent sched-uled help sessions for the student cohort. Over a seven week period in a previous term's help sessions, there was a total of 1,484 requests for tutor help from students, with an average time to resolution of 20 minutes. However, students were waiting on average 38 minutes before they could be assisted and only 73.9% of queries were re-solved. Waiting time increased during assessment deadline periods. Typically, unresolved student queries are directed to the online class forum, or told to wait until their scheduled class time. Such help sessions, whilst an important component of an overall experi-ence, are still not enough to adequately meet the demand of student queries and required assistance.\nExtending previous work by Taylor et al. which incorporated one-off LLM-generated error message explanations into educational compilers [25, 27], this work introduces a conversational error explanation module that allows students to engage in dialogue with an LLM about their compile- or run-time programming error.\nDCC Sidekick continues to utilise the full context and mem-ory stack details provided by the compiler, while also generating a unique URL to a web interface. This interface allows students the capability to continue conversations with the LLM, providing the opportunity to guide the LLM to Socratically question the stu-dent's understanding of their program and the associated error. The tool can provide reworded, simplified, or expanded explanations depending on the student's needs."}, {"title": "4 DCC SIDEKICK", "content": "DCC Sidekick is an extension to the existing DCC Help tool, forked from the open-source repository as presented by Taylor et al. [27]. The project utilises LLMs to produce enhanced error explanations of C/C++ programming error messages. Functioning as an add-on, our new DCC Sidekick tool leverages DCC's extensive error detection and explanations system [25] to present source code, error information and a conversational interface to an LLM.\nWhile DCC Help provides in situ (in-terminal), one-off error ex-planations, DCC Sidekick offers conversational guidance. Students are launched into an accessible, web-based dashboard, which presents a comprehensive birds-eye-view of their source code, error message and program state, presented alongside a chat interface. Users can ask follow-up questions and receive Socratic guidance tailored to their current level of understanding.\nLike DCC Help, DCC Sidekick's integration with the DCC com-piler constructs detailed LLM prompts with no input from the stu-dent. This reduces the complexity of prompting a response, distin-guishing it from contemporary tools such as CodeHelp [16], which require users to manually provide relevant information about their programming error.\n4.1 Compiler Integration\nTo ensure a consistent learning environment, our students complete all coursework within a personal virtual environment located on the institution's privately hosted LINUX servers, which they access remotely via SSH or a VNC client. When students encounter a run-time or compile-time error using our forked version of DCC, they are presented with a programming error message that contains instructions to request additional \"AI-powered\" clarifications by running either dcc-help or dcc-sidekick in the command line. The latter command displays a URL in the command line, that can be used to launch a new, private DCC Sidekick session and access the web-based dashboard.\nThe DCC Sidekick command integrates with DCC Help, allowing students to seamlessly transition back and forth between the two environments.\nThe DCC Sidekick compiler integration is implemented using a small set of shell scripts that are installed globally, alongside a forked DCC in the institution's environment. These scripts utilise existing plugin hooks within the DCC compiler, caching error data and allowing DCC Sidekick to be launched in response to errors or undefined behaviours.\n4.2 Graphical Interface\nThe DCC Sidekick web-based dashboard is implemented in the React web framework, and prioritises accessibility for novices with a visually clear graphical interface exploring the complexities of a C error. Its separation from the locally installed integration scripts ensures student access irrespective of connection method to the institution's programming environment. Once a URL has been generated, the dashboard itself can either be accessed remotely via a VNC client, or locally on a personal device if they are instead utilising a text-only SSH connection.\nWhen first visiting a session, DCC Sidekick automatically gen-erates and displays an initial error explanation within the chat interface. This aims to provide debugging guid-ance on the error, after which users can continue to request follow-up questions and clarifications. The dashboard also displays error message information provided directly by DCC, below the user's program code. For run-time errors, this section of the dashboard dynamically displays the different categories of information across separated tabs, such as variable values, allowing users to explore the underlying complexity of the error at their own pace.\nUsers can also create read-only session links from within the DCC Sidekick dashboard, allowing course staff to view the existing debugging information and generated explanations, streamlining the process of requesting tutor intervention.\nThis dashboard is backed by a privately hosted server and data-base, which is responsible for session creation, explanation prompt-ing, and maintaining session data.\n4.3 Prompting Strategy\nDCC Sidekick currently utilises OpenAI's ChatGPT3.5-turbo model via the chat completions API to generate both an initial error expla-nation and responses to follow-up queries. For each session, DCC Sidekick maintains a conversation history to provide to the API, facilitating a coherent, relevant and accurate conversation.\nThe initial error explanation is generated using a context-rich prompt, derived from the program information provided by DCC. The exact structure of the prompt is dependent on the type of error and the availability of data from DCC, but run-time errors in particular can include context such as the original error message, the user's program code, the function call stack and local variable assignments.\n4.4 Additional Guardrails\nEarlier work found that large language models often ignore instruc-tions to not provide solutions [16], with one study observing this phenomenon in 48% of cases [26]. To protect against this behaviour, additional guardrails based on prior work by Liffiton et al. [16], were incorporated into the DCC Sidekick prompting strategy. All generated responses are filtered through an additional layer of LLM guardrails, which aim to rewrite overly prescriptive responses that contain code blocks. The system prompt for this rewriting step can be seen in Figure 4.\nAdditionally, DCC Sidekick's standard system prompt was modi-fied to include provisions to discourage off-topic requests and code solutions. This prompting strategy aims to encourage the gener-ation of responses that provide valuable debugging guidance to novices, without undermining the development of their debugging skills and understanding.\nFinally, the tool warns students against over-use if used fre-quently in a short period of time, gently reminding them that AI assistance will not be available in the final exam.\n4.5 Risks\nThe tool is powered by large language models, which are innately vulnerable to hallucinations, biases, and attempts designed to break guardrails, a well-known and documented phenomenon [28]. This could result in unsatisfactory or inappropriate responses, although we have not observed this behaviour directly."}, {"title": "5 METHODOLOGY", "content": "To evaluate the user adoption, impact and early markers of efficacy, we deployed DCC Sidekick at our large Australian university in 2024, to a cohort of approximately 1,200 introductory programming students. An insignificant number of students in subsequent C courses may also have used the tools.\nStudents were instructed on how to use DCC Sidekick, alongside alternative tools like DCC Help, and are prompted to create a session after any compile-time or run-time error within the university's programming environment, as described in subsection 4.1.\n5.1 Data Collection\nWe collected usage data for both DCC Sidekick and DCC Help over the first seven weeks of an introductory programming course in C, which spanned several fundamental CS1 topics such as basic IO, control flow, arrays, and linked lists.\n5.1.1 Usage Logs. DCC Sidekick usage was measured by track-ing session launches, defined as instances in which a student first creates and visits a DCC Sidekick session in-browser, either in re-sponse to a programming error or to ask follow-up questions after first running DCC Help. For both compile- and run-time errors, we log the source code, compiler error messages, and all generated inferences during each session, allowing us to monitor the student engagement across both DCC Sidekick and DCC Help.\n5.1.2 Heatmaps and Session Recordings. Anonymised session record-ings, user events and click-based heatmaps were collected during the 7-week period using the Microsoft Clarity platform\u00b9, providing further insight into common user interaction patterns. We reflect on the usability and design of DCC Sidekick's graphical interface in subsection 7.1.\n5.2 Data Filtering\nTo comply with the relevant ethics requirements for this study, all collected student data was redacted before analysis. Identifiable features were automatically parsed and removed from usage logs and recordings. This involved removing all comments from logged student source code, in an attempt to remove occurrences of user information, such as names, student IDs and emails.\nTo ensure that the data accurately represents novice program-mers, all uses of DCC Sidekick and DCC Help by university staff members, who may have initiated sessions for testing or demon-stration purposes, were removed from analyses."}, {"title": "6 RESULTS", "content": "DCC Sidekick was made available to students on the first day of the term and we summarise the results of usage over the first seven weeks of the term. Overall, DCC Sidekick has been used by 959 unique users within 11,222 sessions, and generating 17,982 responses. There were an average of 11.7 sessions per student.\nOn average, each student has spent about 4.3 minutes actively engaging with DCC Sidekick. The maximum active time spent on any single interaction was 11.3 minutes. Of all these sessions, 72.8% of sessions are from returning students, whereas 27.2% of sessions are with new users only. This shows a high return rate and deep engagement with the tool. A total of 25.6% of all conversations resulted in multiple inferences within one session, indicating take up of follow-up dialogue resulting from the initial error. Of all sessions launched, 22.7% had multiple inferences within one session for compile-time errors, and 35.4% for run-time errors. There was an increasing trend in run-time errors as the term progressed. Across all the sessions, there was an average number of 0.6 follow-up questions to each starting message. In any given session where a student asked at least one follow-up question, there was an average of 2.6 follow-up questions asked. In 12.5% of cases, whilst a unique address was created, it was never visited.\nA total of 32,090 clicks were recorded across the sessions in the last seven weeks. The main engagement was with the conversa-tional aspect of the tool, with 61.2% of all clicks focused on the conversational side of the tool.\nThe tool was used extensively both in and out of business hours, with 44% of use occurring within business hours (9am-5pm), and 56% of usage occurring out of business hours (5pm-9am). Ten per-cent of usage occurs between the hours of midnight and 6am, when no one else is available to provide assistance to the student.\n6.1 Usage and Adoption\nFigure 5 shows DCC Sidekick launches over each teaching week broken down into compile- and run-time, and total. Adoption grows steadily throughout the teaching period, with usage peaking each Monday corresponding with the weekly assessment due dates. The results show that compile-time errors contribute to the majority of the session launches, especially at the start of term. Run-time errors start to increase steadily from week 5.\nWhile there are more compile-time error sessions of DCC Side-kick, run-time sessions receive higher engagement. At compile-time, around 23% of DCC Sidekick sessions contain follow-on conversa-tions, and approximately 35% at run-time.\nApproximately 50% of generative AI error explanations origi-nated from the in-terminal DCC Help tool consistently throughout the term, indicating that students found value in both tools."}, {"title": "7 DISCUSSION", "content": "Addressing RG1, adoption trends indicate strong user acceptance. With 11,222 sessions initiated by 959 unique stu-dents, the tool demonstrates its capacity to support a large-scale introductory course. This level of engagement suggests that stu-dents find value not just in Al-enabled error explanations, but conversational debugging assistance provided by DCC Sidekick.\nOf the total sessions with follow-up dialogue, 1,969 were related to compile-time errors, while 899 addressed run-time issues. The predominance of follow-on dialogue in compile-time error sessions, particularly in the early weeks, aligns with the typical progression of novice programmers who often grapple with syntax and basic structural errors in the initial stages of learning.\nAddressing RG2, we observed a marked increase in run-time error assistance requests starting from week 4. This shift coincides with the introduction of more complex programming tasks in the CS1 course, particularly those involving memory management and pointers. As students tackle these more challenging concepts, they appear to be turning to DCC Sidekick rather than external resources like ChatGPT, and the non-conversational, but in-terminal DCC Help tool. This uptake is noteworthy, as it suggests that the ben-efits of the DCC Sidekick conversations outweigh any perceived inconvenience of switching environments.\nUsage patterns reveal a concentration of activity around assess-ment deadlines, peaking on Mondays when weekly assessments are typically due, and especially in week 6, which correlates with the major assignment deadline. This trend underscores the role of DCC Sidekick in supporting students during high-stress periods when the teaching team may become overwhelmed by requests for assis-tance. Similarly, more than half the usage of the tool occurs outside of business hours, when students would not otherwise be able to get help as needed. This underscores the capability of AI-enabled tools to support teaching teams when they need it most.\nThe high return rate and number of follow-up questions in DCC Sidekick sessions demonstrates students are meaningfully engaging to deepen their understanding and debugging techniques.\nWhile DCC Sidekick adoption is strong, it's important to note that the existing DCC Help tool remains the primary source of Al-generated assistance. This inclination for in situ help suggests that immediacy and seamless integration is preferred, until a more thorough debugging session is warranted. This is a relevant finding to the wider community, as many general-purpose AI tools such as ChatGPT, CodeHelp [16], or Harvard's CS50 forum bot [17] are external to the development environment.\n7.1 Lessons Learned\nOur experience developing and deploying DCC Sidekick presents valuable insights for tool makers, researchers, and educators:\nIntegration is key: Despite allowing external LLMs like Chat-GPT, DCC Help and DCC Sidekick usage indicates that our approach of a) integrating with the compiler to produce more accurate re-sponses, and b) seamlessly integrating within existing workflows, is crucial for adoption. This is important, as we can retain our ped-agogical guardrails such as rate limiting and encouraging language models to provide guidance rather than solutions.\nBalancing immediacy and depth: While DCC Sidekick offers a conversational interface, the continued high usage of the simpler, terminal-based DCC Help tool indicates that students value imme-diate, in situ assistance for certain types of errors such as simpler syntax errors in earlier weeks. Future tools should consider how to combine the benefits of both approaches.\nInvestment in conversations: When students do choose to engage with conversational debugging in DCC Sidekick, they invest considerable time and effort. This is higher for run-time error cases, evidencing the role that conversational AI has in supporting more complex debugging tasks."}, {"title": "8 FUTURE WORK", "content": "We identify three avenues for future research:\nStudent surveys and pedagogical impact: We plan to inter-view and survey CS1 students to gain deeper insights into DCC Sidekick's efficacy and impact on long-term learning.\nTime-series analyses: By analysing student progress immedi-ately following a DCC Help or DCC Sidekick invocation, we can quantitatively explore the impact these tools have on metrics like successful compilation and successfully passing autotests.\nModel evaluation and refinement: We plan to assess alter-native commercial and open-source LLMs, exploring on-premises hosting options and fine-tuning techniques to improve CS1 error debugging performance, and to address privacy concerns."}, {"title": "9 LIMITATIONS", "content": "We identify three limitations that impact the validity and generalis-ability of our findings.\nDespite integrating an open-ended feedback form into the DCC Sidekick tool, low response rates mean that this work lacks under-standing of the qualitative aspects of DCC Sidekick's impact on the learning experience. For example, it is not clear why students choose to transition from the in-terminal DCC Help responses to the conversational DCC Sidekick tool. Future work such as surveys will help ascertain students' perceptions of the tool's efficacy or its influence on their problem-solving strategies. Surveys would also allow us to evidence our claims that DCC Sidekick prevented or dissuaded students away from general-purpose LLMs like ChatGPT.\nSecondly, our study lacks analysis of learning outcomes. While engagement metrics indicate user acceptance, they do not neces-sarily correlate with improved programming skills or conceptual understanding. A comparative study of academic performance be-tween DCC Sidekick users and non-users would provide more conclusive evidence of its educational value.\nFinally, this study explores a single institution's CS1 course, limiting the generalisability and long-term adoption trends."}, {"title": "10 CONCLUSION", "content": "This paper presents DCC Sidekick, a novel, compiler-integrated conversational AI tool designed to support debugging activities in a large-scale CS1 course. Our approach combines in-terminal responses via the forked DCC Help, with DCC Sidekick: a web-based conversational interface, providing students with flexible, context-aware debugging sessions. The high adoption rate - 959 students initiating over 11,222 sessions in seven weeks, and signifi-cant engagement - over 4.3 minutes on average, demonstrates the tool's impact. We explore behaviours across compile- and run-time errors, indicating that the nature of a programming error influences the type of Al help students engage with.\nDCC Sidekick's compiler integration offers students a compelling alternative to general-purpose AI tools like ChatGPT, despite DCC Sidekick's pedagogical guardrails. By keeping students within our guided learning environment, we aim to foster genuine understand-ing and skill development - dissuading the use of tools that are not pedagogically aligned.\nDCC Sidekick demonstrates significant potential for scaling sup-port in large programming courses, particularly in its ability to allow ongoing exploration and Socratic guidance of programming errors in a novel, compiler-integrated environment. The approach is shown to handle high volumes of student queries, which is espe-cially valuable outside business hours and near assignment dead-lines. As we continue to refine our approach to AI-generated sup-port in introductory computer science courses, we believe that AI-assisted tools will play a crucial role in computing education. The advancement of these tools, whilst promising, needs to be ex-ecuted responsibly, ensuring safe pedagogical environments that encourage learning."}]}