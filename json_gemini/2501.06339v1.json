{"title": "On The Statistical Complexity of Offline Decision-Making", "authors": ["Thanh Nguyen-Tang", "Raman Arora"], "abstract": "We study the statistical complexity of offline decision-making with function approximation, establishing (near) minimax-optimal rates for stochastic contextual bandits and Markov decision processes. The performance limits are captured by the pseudo-dimension of the (value) function class and a new characterization of the behavior policy that strictly subsumes all the previous notions of data coverage in the offline decision-making literature. In addition, we seek to understand the benefits of using offline data in online decision-making and show nearly minimax-optimal rates in a wide range of regimes.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL) has achieved remarkable empirical success in a wide range of challenging tasks, from playing video games at the same level as humans (Mnih et al., 2015), surpassing champions at the game of Go (Silver et al., 2018), to defeating top-ranked professional players in StarCraft (Vinyals et al., 2019). However, many of these systems require extensive online interaction in gameplay with other players who are experts at the task or some form of self-play (Li et al., 2016; Ouyang et al., 2022). Such online interaction may not be affordable in many real-world scenarios due to concerns about cost, safety, and ethics (e.g., healthcare and autonomous driving). Even in domains where online interaction is possible (e.g., dialogue systems), we would still prefer to utilize available historical, pre-collected datasets to learn useful decision-making policies efficiently. Such an approach would allow leveraging plentiful data, possibly replicating the success that supervised learning has had recently (LeCun et al., 2015). Offline RL has emerged as an alternative to allow learning from existing datasets and is particularly attractive when online interaction is prohibitive (Ernst et al., 2005; Lange et al., 2012; Levine et al., 2020).\nNevertheless, learning good policies from offline data presents a unique challenge not present in online decision-making: distributional shift. In essence, the policy that interacts with the environment and collects data differs from the target policy we aim to learn. This challenge becomes more pronounced in real-world problems with large state spaces, where it necessitates function approximation to generalize from observed states to unseen ones.\nRepresentation learning is a basic challenge in machine learning. It is not surprising, then, that function approximation plays a pivotal role in reinforcement learning (RL) problems with large state spaces, mirroring its significance in statistical learning theory (Vapnik, 2013). Empirically, deep RL, which employs neural networks for function approximation, has achieved remarkable success across diverse tasks (Mnih et al., 2015; Schulman et al., 2017). The choice of function approximation class determines the inductive bias we inject into learning, e.g., our belief that the learner's environment is relatively simple even though the state space may be large.\nIt is natural, then, to understand different function approximation classes in terms of a tight characterization of their complexity and learnability. In statistical supervised learning, specific combinatorial properties of the function class are known to completely characterize sample-efficient supervised learning in both realizable and agnostic settings (Vapnik & Chervonenkis, 1971; Alon et al., 1997; Attias et al., 2023). For offline RL, a similar characterization is not known. With that as our motivation, we pose the following fundamental question that has largely remained unanswered:\nWhat is a sufficient and necessary condition for learnability in offline RL with function approximation?\nWe note that given the additional challenge of distribution shift in offline RL, such a characterization would depend not only on the properties of the function class but, more importantly, on the quality of the offline dataset. The existing literature on offline RL provides theoretical understanding only for limited scenarios of distributional shifts. These works capture the quality of offline data via a notion of data coverage. The strongest of these notions is that of uniform coverage, which requires that the behavior policy"}, {"title": "2. Background and Problem Formulation", "content": "We represent a stochastic contextual bandit environment with a tuple (\\( \\mathcal{X}, \\mathcal{A}, D \\)), where \\( \\mathcal{X} \\) denotes the set of contexts, \\( \\mathcal{A} \\) denotes the space of actions and \\( D \\in \\Delta(\\mathcal{X} \\times \\mathcal{Y}) \\) denotes an unknown joint distribution over the contexts and rewards. Without loss of generality, we take \\( \\mathcal{Y} := [0, 1]^\\mathcal{A} \\). A learner interacts with the environment as follows. At each time step, the environment samples \\( (X, Y) \\sim D \\), the learner is presented with the context \\( X \\), she commits to an action \\( a \\in \\mathcal{A} \\), and observes reward \\( Y(a) \\).\nWe model the learner as stochastic. The learner maintains a stochastic policy \\( \\pi : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A}) \\), i.e., a map from the context space to a distribution over the action space. The value, \\( V \\), of a policy \\( \\pi \\) is defined to be its expected reward,\n\\[ V_D(\\pi) := \\mathbb{E}_{(X, Y) \\sim D, A \\sim \\pi(\\cdot|X)} [Y(A)]. \\]\nThe sub-optimality of \\( \\pi \\) w.r.t. any policy \\( \\pi^* \\) is defined as:\n\\[\\text{SubOpt}_D(\\pi) = V_D(\\pi^*) - V_D(\\pi).\\tag{1}\\]\nWe often suppress the subscript in \\( V \\) and \\( \\text{SubOpt}(\\pi) \\)."}, {"title": "2.2. Offline data", "content": "Let \\( S = \\{(x_i, a_i, r_i)\\}_{i\\in[n]} \\) be a dataset collected by a (fixed, but unknown) \u201cbehavior\u201d policy \\( \\mu \\), i.e., \\( (x_i, Y_i) \\stackrel{i.i.d.}{\\sim} D \\), \\( a_i \\sim \\mu(\\cdot|x_i) \\), and \\( r_i = y_i(a_i) \\) for all \\( i \\in [n] \\). The goal of offline learning is to learn a policy from the offline data such that it has small sub-optimality \\( \\text{SubOpt}(\\pi) \\) for as wide as possible a range of comparator policies \\( \\pi \\) (possibly including an optimal policy \\( \\pi^* \\in \\arg \\max_{\\pi} V(\\pi) \\))."}, {"title": "2.3. Function approximation", "content": "A central aspect of any value-based method for a sequential decision-making problem is to employ a certain function class \\( F \\subset [0, 1]^{\\mathcal{X}\\times\\mathcal{A}} \\) for modeling rewards in terms of contexts and actions; it is typical to solve a regression problem using squared loss. The choice of the function class reflects learner's inductive bias or prior knowledge about the task"}, {"title": "3. Offline Decision-Making as Transfer Learning", "content": "We view offline decision-making as transfer learning where the goal is to utilize pre-collected experiences for learning new tasks. A key observation we leverage is that there are parallels in how the two areas capture distribution shift \u2014 a common challenge in both settings. Transfer learning uses various notions of distributional discrepancies (Ben-David et al., 2010; David et al., 2010; Germain et al., 2013; Sugiyama et al., 2012; Mansour et al., 2012; Tripuraneni et al., 2020; Watkins et al., 2023) to capture distribution shift between the source tasks and the target task, much like how offline decision-making uses various notions of data coverage to measure the distributional mismatch due to offline data. We consider a new notion of data coverage inspired by transfer learning, which will be shown shortly to tightly capture the statistical complexity of offline decision-making from a behavior policy.\nDefinition 3.1 (Policy transfer coefficients). Given any policy \\( \\pi \\), \\( \\rho_\\pi \\geq 0 \\) is said to be a policy transfer exponent from \\( \\mu \\) to \\( \\pi \\) w.r.t. \\( (D, F) \\) if there exists a finite constant \\( C \\), called policy transfer factor, such that:\n\\[ \\forall f \\in F : (\\mathbb{E}_{D \\otimes \\pi}[f^* - f])^{2\\rho_{\\pi}} < C \\mathbb{E}_{D \\otimes \\mu}[(f^* - f)^2]. \\tag{2}\\]\nAny such pair \\( (\\rho, C) \\) is said to be a policy transfer coefficient from \\( \\mu \\) to \\( \\pi \\) w.r.t. \\( (D, F) \\). We denote the minimal policy transfer exponent by \\( \\rho_\\pi \\).\u00b9 The minimal policy transfer factor corresponding to \\( \\rho \\) is denoted as \\( C_\\pi \\).\nRemark 3.2. Our definition of policy transfer resembles and is directly inspired by the notion of transfer exponent by (Hanneke & Kpotufe, 2019), which we refer to as Hanneke-Kpotufe (HK) transfer exponent for distinction. A direct adaptation of the HK transfer exponent would result in:\n\\[ \\forall f \\in F : \\mathbb{E}_{D \\otimes \\pi}[(f^* - f)^2]^{\\rho} < C \\mathbb{E}_{D \\otimes \\mu}[(f^* - f)^2]. \\tag{3}\\]\nNote the difference in the LHS of Equation (2) and that of Equation (3). When defining policy transfer coefficients, we use l2-distance\u00b2 \\( \\mathbb{E}_{D \\otimes \\mu}[(f^* - f)^2] \\) w.r.t. the behavior policy to control the expected value gap \\( \\mathbb{E}_{D \\otimes \\pi}[f^* - f] \\), whereas the HK transfer exponent directly requires a bound on squared distance \\( \\mathbb{E}_{D \\otimes \\pi}[(f^* - f)^2] \\) w.r.t. to the target policy. While this appears to be a small change, our notion bears a deeper connection with offline decision-making"}, {"title": "3.1. Relations with other notions of data coverage", "content": "In this section, we highlight the properties of transfer exponents and compare them with other notions of data coverage considered in offline decision-making literature. Perhaps the most common notions of data coverage are that of single-policy concentrability coefficients (Liu et al., 2019; Rashidinejad et al., 2021), relative condition numbers (for linear function classes) (Agarwal et al., 2021; Uehara & Sun, 2021), and data diversity (Nguyen-Tang & Arora, 2023).\u00b3 We demonstrate that policy transfer coefficients strictly generalize all these prior notions, in the sense that bounds on the prior notions of data coverage always imply bounds on transfer exponents but not vice versa. Specifically, there are problem instances for which the existing measures of data coverage tend to infinity, yet these problems are learnable given the characterization in terms of transfer coefficients.\nCompared with concentrability coefficients. The concentrability coefficient between \\( \\pi \\) and \\( \\mu \\) is defined as \\( \\kappa_\\pi := \\sup_{x, a} \\frac{\\pi(a|x)}{\\mu(a|x)} \\). The finiteness of \\( \\kappa_\\pi \\) is widely used as one of the sufficient conditions for sample-efficient offline decision-making. By definition, the policy transfer factor corresponding to the policy transfer exponent of 1, is always upper-bounded by \\( \\kappa_\\pi \\). The finiteness of \\( \\kappa_\\pi \\) requires the support of \\( \\mu \\) to contain that of \\( \\pi \\). However, offline decision-making does not even need overlapping support between a target policy and the behavior policy."}, {"title": "4. Lower Bounds", "content": "Let \\( \\mathcal{B}(\\rho, C, d) \\) denote the class of offline learning problem instances with any distribution \\( D \\) over contexts and rewards, any function class \\( F \\) that satisfies Assumptions 2.1 and 2.3, a behavior policy \\( \\mu \\), and all policies \\( \\pi \\in \\Pi \\) such that policy transfer coefficients w.r.t. \\( \\mu \\) are \\( (\\rho, C) \\). For this class, we give a lower bound on the sub-optimality of any offline learning algorithm.\nTheorem 4.1. For any \\( C > 0, \\rho \\geq 1, n \\geq d \\cdot \\max\\{2^{2\\rho-4C}, \\frac{C}{32}\\} \\), we have\n\\[\\inf_{\\widehat{\\pi}(\\cdot)} \\sup_{(D,\\mu,\\pi,F)\\in\\mathcal{B}(\\rho,C,d)} \\mathbb{E}_{D} [\\text{SubOpt}(\\widehat{\\pi})] \\geq O\\left(\\frac{Cd}{n}\\right)^{1/(2\\rho)} \\]\nwhere the infimum is taken over all offline algorithm \\( \\widehat{\\pi}(\\cdot) \\) (a randomized mapping from the offline data to a policy).\nThe lower bound in Theorem 4.1 is information-theoretic, i.e., it applies to any algorithm for problem class \\( \\mathcal{B}(\\rho, C, d) \\). The lower bound is obtained by constructing a set of hard contextual bandit (CB) instances \\( \\{D_i\\} \\) that are supported on \\( d \\) data points. Then, for each \\( D_i \\), we pick the hardest comparator policy \\( \\pi = \\pi^* \\) and design a behavior policy that satisfies the policy transfer condition. We pick a simple enough function class that satisfies realizability and ensures that the policy transfer exponents and the pseudo-dimension are bounded. We then proceed to show that given a behavior policy \\( \\mu \\), for any two CB instances \\( D_i \\) and \\( D_j \\) that are close to each other (i.e., \\( KL[(D_i \\otimes \\mu)^n||(D_j \\otimes \\mu)^n] \\) is small) the corresponding optimal policies disagree. A complete proof is given in Appendix B."}, {"title": "5. Upper Bounds", "content": "Next, we show that there exists an offline learning algorithm that is agnostic to the minimal policy transfer coefficient of any policy, yet it can compete uniformly with all comparator policies, as long as their minimal policy transfer exponent is finite. For this algorithm, we give an upper bound for VC-type classes that matches the lower bound in the previous section up to log factors, ignoring the dependence on \\( K = |\\mathcal{A}| \\). For more general function classes, we only provide upper bounds.\nThe general recipe for our algorithm (Algorithm 1) is rather standard. We follow the actor-critic framework for offline RL studied in several prior works (Zanette et al., 2021; Xie et al., 2021a; Nguyen-Tang & Arora, 2023). The algorithm alternates between computing a pessimistic estimate of the actor and improving the actor with the celebrated Hedge algorithm (Freund & Schapire, 1997)."}, {"title": "6. Offline Data-assisted Online Decision-Making", "content": "In this section, we consider a hybrid setting, where in addition to the offline data \\( S = \\{(x_i, a_i, r_i)\\}_{i\\in[n]} \\), the learner is allowed to interact with the environment for \\( m \\) rounds. The goal is to output a policy \\( \\widehat{\\pi}_{\\text{hyb}} \\) with small \\( \\text{SubOpt}(\\widehat{\\pi}_{\\text{hyb}}) \\) w.r.t. \\( \\pi^* \\) with a high value \\( V_{\\pi^*} \\). We assume realizability (Assumption 2.1) and for simplicity, we focus only on VC-type function classes with pseudo-dimension at most \\( d \\). Let \\( \\rho^* = \\rho_{\\pi^*} \\) and \\( C^* = C_{\\pi^*} \\) denote the policy transfer coefficients for an optimal policy \\( \\pi^* \\).\nTo avoid deviating from the main point, in this section, we focus on the \"large sample, difficult transfer\" regime, where \\( d > Kd \\log(dn) \\) and \\( \\rho^* > 1 \\). The key question we ask is whether a learner can perform better in a hybrid setting than in a purely online or offline setting."}, {"title": "6.1. Lower bounds", "content": "We start with a lower bound for any hybrid learner for the class of problems \\( \\mathcal{B}(\\rho, C, d) \\) as in Section 4.\nTheorem 6.1. For any \\( C > 0, \\rho \\geq 1 \\), and sample size \\( n \\geq d \\max\\{2^{2\\rho-4C}, \\frac{C}{32}\\} \\), we have\n\\[\\inf_{\\widehat{\\pi}_{\\text{hyb}} (\\x)} \\sup_{(D,\\mu,\\pi,F) \\in \\mathcal{B}(\\rho,C,d)} \\mathbb{E}_{D} [\\text{SubOpt}(\\widehat{\\pi}_{\\text{hyb}})] \\geq \\Omega\\left(\\min \\left\\{\\left(\\frac{Cd}{n}\\right)^{1/(2\\rho)}, \\sqrt{\\frac{d}{m}}\\right\\}\\right) \\]\nwhere the infimum is taken over all possible hybrid algorithm \\( \\widehat{\\pi}_{\\text{hyb}}() \\).\nIgnoring log factors and dependence on \\( K \\), the first term in the lower bound matches the upper bound for OfDM-Hedge. Therefore, if that is the dominating term, the learner does not benefit from online interaction. The second term, \\( \\sqrt{d/m} \\), matches the upper bound of the state-of-the-art online learner (Simchi-Levi & Xu, 2022) with an online interaction budget of \\( m \\). In the regime where the latter term is dominating there is no advantage to having offline data.\nTo summarize, the lower bound suggests that if the policy transfer coefficient is known a priori to the hybrid learner, there is no benefit of mixing the offline data with the online data at least in the worst case. That is, obtaining the nearly minimax-optimal rates for hybrid learning is akin to either discarding the online data and running the best offline learner or ignoring the offline data and running the best online learner. Which algorithm to run depends on the transfer coefficient. That there is no benefit to mixing online and offline data is a phenomenon that has also been discovered in a related setting of policy finetuning (Xie et al., 2021b)."}, {"title": "6.2. Upper bounds", "content": "The discussion following the lower bound suggests different algorithmic approaches (purely offline vs. purely online) for different regimes defined in terms of the policy transfer coefficient. However, it is unrealistic to assume that the learner has prior knowledge of the transfer coefficient. We present a hybrid learning algorithm (Algorithm 2) that offers the best of both worlds. Without requiring the knowledge of the policy transfer coefficient, it produces a policy with nearly optimal minimax rates.\nThe key algorithmic idea is rather simple and natural. We invoke both an offline policy optimization algorithm and an online learner, resulting in policies \\( \\pi_{\\text{off}} \\) and \\( \\widehat{\\pi}_{\\text{on}} \\), respectively. Half of the interaction budget, i.e., \\( m/2 \\) rounds, is utilized for learning \\( \\widehat{\\pi}_{\\text{on}} \\). For the remaining \\( m/2 \\) rounds, we run the EXP4 algorithm (Auer et al., 2002) with \\( \\pi_{\\text{off}} \\) and \\( \\widehat{\\pi}_{\\text{on}} \\) as expert policies. The output of EXP4 is a uniform distribution over all of the iterates. We denote this randomized policy as \\( \\widehat{\\pi}_{\\text{hyb}} \\). We can equivalently represent \\( \\widehat{\\pi}_{\\text{hyb}} \\) as a distribution over \\( \\{\\pi_{\\text{off}}, \\widehat{\\pi}_{\\text{on}}\\} \\).\nProposition 6.2. Algorithm 2 return a randomized policy \\( \\widehat{\\pi}_{\\text{hyb}} \\) such that for any \\( \\delta \\in (0,1) \\), with probability at least 1 - \\( \\delta \\),\n\\[\\mathbb{E}_D [\\text{SubOpt}(\\widehat{\\pi}_{\\text{hyb}})|S, S_{\\text{on}}] \\leq \\max_{\\pi\\in\\{\\pi_{\\text{off}}, \\widehat{\\pi}_{\\text{on}}\\}} \\mathbb{E}_{D} [\\text{SubOpt}(\\pi)] + \\frac{4\\sqrt{2 \\log 2}}{\\sqrt{m}} + \\frac{32 \\log(\\log(m/2)/\\delta))}{3m} + \\frac{4}{m}, \\]\nwhere \\( S \\) is the offline data and \\( S_{\\text{on}} \\) is the online data collected by \\( \\widehat{\\pi}_{\\text{on}} \\) in Algorithm 2.\nThe first term in the bound above comes from the guarantees on EXP4 with two experts (Lattimore & Szepesv\u00e1ri, 2020, Theorem 18.3) and the remaining terms result from an (improved) online-to-batch conversion (Nguyen-Tang et al., 2023, Lemma A.5) and the assumption that contexts are sampled i.i.d. Note that the result above does not require any assumption on \\( F \\) and \\( D \\)."}, {"title": "7. Offline Decision-Making in MDPs", "content": "In this section, we extend our results to offline decision-making in Markov decision processes (MDPs). We show that the key insights developed for the contextual bandit model extend naturally to offline learning of MDPs as we establish nearly matching upper and lower bounds."}, {"title": "8. Discussion", "content": "We study the statistical complexity of offline decision-making with value function approximation. We identify a large class of offline learning problems, characterized by the pseudo-dimension of the value function class and a new characterization of the offline data, for which we provide tight minimax lower and upper bounds. We also provide insights into the role of offline data for online decision-making from a minimax perspective.\nWe remark that our results do not imply that pseudo-dimension and policy transfer coefficients are necessary conditions for learnability in offline decision-making with function approximation. Consequently, there are several"}, {"title": "A. Uniform Bernstein's Inequality", "content": "A central tool for our analysis is the uniform concentrability of a subset of functions near the ERM (empirical risk minimizer) predictor. In order to be able to match the lower bounds of offline decision-making (at least) for parametric classes, we require that every ball centered around the ERM predictor within a sufficiently small radius (in fact, of the order of \\( O(\\epsilon) \\)) has an \\( O(1) \\) order in the excess risk. While fast rates are in general not possible, they are so under certain conditions (Van Erven et al., 2015), including the bounded, squared loss in parametric classes that we consider. A uniform Bernstein's inequality is also presented in (Cucker & Smale, 2002, Theorem B), but using a strong notion of domain-wide \\( L_{\\infty} \\) covering numbers. (Zhang, 2023, Theorem 3.21) presents a version of uniform Bernstein's inequality using the population \\( L_1 \\) covering numbers. The population \\( L_1 \\) however requires the knowledge of the data distribution. Here, we present a more practical version of uniform Bernstein's inequality using the empirical \\( L_1 \\) covering numbers, which is, at least in principle, computable given the empirical data. More importantly, and also as a key technical result in this section, we prove in Proposition A.3 a uniform Bernstein's inequality for Bellman-like loss functions using the empirical \\( L_1 \\) covering numbers. Proposition A.3 applies to handle the data structure generated by RL and, as a special case, naturally applies to contextual bandits. These results are central to our analysis tool and might be of independent interest. Notably, our proof for Proposition A.3 relies on an elegant argument of localizing a function class into a set of balls centered around the covering functions, and then performing uniform convergence in each of such local balls before combining them via a union bound. This localization argument is inspired by a similar argument by (Mehta & Williamson, 2014).\nBefore stating the uniform Bernstein's inequality for Bellman-like loss functions in Proposition A.3, we start with the uniform Bernstein's inequality for a simpler case, which we also use in our analysis and also serves a good point for demonstrating the localization argument."}, {"title": "A.1. Uniform Bernstein's inequality for generic case", "content": "Proposition A.1 (Uniform Bernstein's inequality for generic case). Let \\( G \\) be a set of functions \\( g : Z \\rightarrow [-b, b] \\). Fix \\( n \\in \\mathbb{N} \\). Denote \\( \\widehat{\\mathbb{E}}g := \\frac{1}{n} \\sum_{i=1}^n g(z_i) \\) where \\( \\{z_i\\}_{i=1}^n \\stackrel{i.i.d.}{\\sim} P \\) and \\( \\mathbb{E}g = \\mathbb{E}_{z\\sim P}[g(z)] \\), and \\( V[g] \\) is the variance of \\( g(z) \\). Then, for any \\( \\delta > 0 \\), with probability at least \\( 1 - \\delta \\), we have\n\\[\\forall g \\in G : \\mathbb{E}g - \\widehat{\\mathbb{E}}g \\leq \\inf_{\\epsilon > 0} \\left\\{\\sqrt{\\frac{2V[g] \\log(2N_1(G,\\epsilon,n)/\\delta)}{n}} + \\frac{62b \\log(6N_1(G, \\epsilon,n)/\\delta)}{n} + 61\\epsilon\\right\\}. \\]\nFix \\( \\epsilon > 0 \\) and let \\( N = N_1(G, \\epsilon, n) \\). Let \\( \\{g_i\\}_{i\\in[N]} \\) be an \\( \\epsilon \\)-cover of \\( G \\) w.r.t. \\( L_1(P_n) \\). For any \\( i \\in [N] \\), denote \\( G_i = \\{g \\in G : \\mathbb{E}|g - g_i| \\leq \\epsilon\\} \\). We have \\( G \\subset \\bigcup_{i\\in[N]}G_i \\). The proof of Proposition A.1 relies on the following lemma."}, {"title": "A.2. Uniform Bernstein's inequality for Bellman-like loss classes", "content": "We now establish the uniform Bernstein's inequality for the Bellman-like loss functions. The nature of the result and the proof is similar to those for Proposition A.1, but only more involved as we deal with a more structural random tuple.\nConsider a tuple of random variables \\( (x, a, r, x') \\in \\mathcal{X} \\times \\mathcal{A} \\times [0, 1] \\times \\mathcal{X} \\) distributed according to distribution \\( P \\). For any \\( u : \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathbb{R} \\) and \\( g : \\mathcal{X} \\rightarrow \\mathbb{R} \\), we define the random variable\n\\[ \\mathcal{M}(u, g) = (u(x, a) - r - g(x'))^2 - (g_*(x, a) - r - g(x'))^2, \\]\nwhere \\( g_*(x, a) := \\mathbb{E}_{(r, x')\\sim P(\\cdot|x, a)} [r + g(x')] \\). Let \\( \\{(x_t, a_t, r_t, x'_t)\\}_{t\\in[n]} \\) be an i.i.d. sample from \\( P \\). We write \\( \\mathcal{M}_t \\) in replace of \\( \\mathcal{M} \\) when we replace \\( (x, a, r, x') \\) in \\( \\mathcal{M} \\) by \\( (x_t, a_t, r_t, x'_t) \\). We consider function classes \\( U \\subset \\{u : \\mathcal{X} \\times \\mathcal{A} \\rightarrow [0, b]\\} \\) and \\( G \\subset \\{g : \\mathcal{X} \\rightarrow [0, b]\\} \\). We assume, for simplicity, that \\( r + g(x') \\in [0, b] \\) almost surely.\nProposition A.3 (Uniform Bernstein's inequality for Bellman-like loss functions). Fix any \\( \\epsilon > 0 \\). With probability at least \\( 1 - \\delta \\), for any \\( u \\in U \\), \\( g \\in G \\),\n\\[ \\mathbb{E}[(u(x, a) - g_*(x, a))^2] < \\frac{2}{n} \\sum_{t=1}^n \\mathcal{M}_t(u, g) + \\inf_{\\epsilon > 0} \\left\\{ \\frac{36 b \\epsilon + b^2 \\frac{36 \\log N_1(U, \\epsilon, n) + 83 \\log N_1(G, \\epsilon, n) + 108 \\log(12/\\delta)}{n}}{2} \\right\\}. \\]\n\nIn addition, with probability at least \\( 1 - \\delta \\), for any \\( u \\in U \\), \\( g \\in G \\),\n\\[ \\frac{1}{n} \\sum_{t=1}^n \\mathcal{M}_t(u, g) < \\inf_{\\epsilon > 0} \\left\\{ 32 H + b^2 \\frac{4 \\log N_1(U, \\epsilon, n) + 28 \\log N_1(G, \\epsilon, n) + 28 \\log(6/\\delta)}{n} \\right\\}. \\]"}, {"title": "B. Proofs of Section 4", "content": "Proof of Theorem 4.1. Fix any \\( (C, \\rho, d) \\) as stated in the theorem. Let \\( \\epsilon = (\\frac{Cd}{32n})^{1/(2\\rho)} \\). We have \\( 0 < \\epsilon < 1/2 \\) and \\( \\frac{\\epsilon^{2\\rho-2}}{C} < 1 \\).\nConstruction of hard instances. Let \\( \\mathcal{A} = \\{a_1, a_2\\} \\). Pick any \\( d \\) mutually distinct points \\( x_1, ..., x_d \\). We construct a family of the context-reward distributions \\( D_{\\sigma} \\) indexed by \\( \\sigma \\in \\{-1, 1\\}^d \\) where \\( D_{\\sigma}(x, y) = P_X(x) \\times P_{Y|X}(y) \\),\n\\[ P_X(x_i) = \\frac{1}{d}, \\ P_{Y|X}(y(a_1)|x_i) = \\text{Ber}(\\frac{1}{2}), \\text{ and } P_{Y|X}(y(a_2)|x_i) = \\text{Ber}(\\frac{1}{2} + \\sigma_i \\epsilon). \\]"}]}