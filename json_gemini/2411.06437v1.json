{"title": "CTC-ASSISTED LLM-BASED CONTEXTUAL ASR", "authors": ["Guanrou Yang", "Ziyang Ma", "Zhifu Gao", "Shiliang Zhang", "Xie Chen"], "abstract": "Contextual ASR or hotword customization holds substantial practical value. Despite the impressive performance of current end-to-end (E2E) automatic speech recognition (ASR) systems, they often face challenges in accurately recognizing rare words. Typical E2E contextual ASR models commonly feature complex architectures and decoding mechanisms, limited in performance and susceptible to interference from distractor words. With large language model (LLM)-based ASR models emerging as the new mainstream, we propose a CTC-Assisted LLM-Based Contextual ASR model with an efficient filtering algorithm. By using coarse CTC decoding results to filter potential relevant hotwords and incorporating them into LLM prompt input, our model attains WER/B-WER of 1.27%/3.67% and 2.72%/8.02% on the Librispeech test-clean and test-other sets targeting on recognizing rare long-tail words, demonstrating significant improvements compared to the baseline LLM-based ASR model, and substantially surpassing other related work. More remarkably, with the help of the large language model and proposed filtering algorithm, our contextual ASR model still performs well with 2000 biasing words.", "sections": [{"title": "1. INTRODUCTION", "content": "End-to-end (E2E) automatic speech recognition (ASR) systems demonstrate impressive performance [1] but often struggle with accurately recognizing rare phrases, such as technical terms and name entities that are infrequently encountered in training data. Integrating contextual knowledge into E2E ASR models for biased decoding is crucial for improving recognition accuracy in practical applications, such as commercial systems where users need personalized recognition for specific names, places, and entities, highlighting the academic and commercial value of supporting hotword customization. [2]\nTo address this challenge, researchers have developed various methods to inject contextual information into E2E models. The traditional approaches involve shallow fusion [3-6], which utilizes an independently built language model (LM). Similarly, on-the-fly (OTF) rescoring method [7, 8] compiles known word-level biasing phrases into a weighted finite state transducer (WFST), combined with a \u201cspeller\" FST to convert grapheme or word-piece sequences into words. A contextual LM score derived from this combination is integrated into the decoding criterion of standard log-likelihood, controlled by a tunable hyperparameter.\nIn contrast, all-neural contextual biasing methods [2, 7, 9-11] integrate an additional contextual module with the ASR module. For instance, CLAS [7] dynamically incorporates contextual information during speech recognition by embedding context phrases and integrating them into the ASR components via the attention mechanism. These methods can handle out-of-vocabulary terms effectively and do not demand careful tuning of hyperparameters like rescoring weights, leading to significant improvements over OTF rescoring.\nIn practical applications, a biasing list often includes hundreds to thousands of biasing words. Retrieving a meaningful bias context vector is challenging, for it's difficult for the cross-attention mechanism to accurately link the ASR decoder output with the large-scale sparse hotword embeddings. As the length of the biasing list increases and the number of interfering items grows, the model's performance tends to decline. To tackle this problem, different works have introduced various filtering methods for processing the biasing list tailored to their specific models. For instance, CLAS proposes a \"bias-conditioning\u201d technique that activates a bias phrase only if its prefix matches part of the partially decoded hypothesis. A two-stage contextual word filter module [12] is introduced for attention-based context bias, which includes computing \"Posterior Sum Confidence\" and \"Sequence Order Confidence\", especially designed for cascaded encoder ASR framework.\nThe above approaches often involve intricate model structures and decoding processes. In this paper, inspired by the recent emergence of LLM-based ASR models [13-16] with simple structures and powerful performance, we propose an effective LLM-based contextual ASR model, along with a robust filtering algorithm. Prior work MaLa-ASR [15] has"}, {"title": "2. METHOD", "content": "In this section, we introduce our model architecture and propose a filtering algorithm, concretely interpreted in Figure 1.\n2.1. Model Architecture\nAttaching a CTC head to a speech SSL pre-trained model allows for obtaining an initial decoding result with minimal additional computational overhead. Meanwhile, the fine-tuning process with CTC loss yields a better encoder for LLM-based ASR training, outperforming the pre-trained model. Thus, we propose an LLM-based speech model utilizing a CTC fine-tuned WavLM [19] encoder for contextual ASR based on the framework of MaLa-ASR. The powerful SSL model WavLM Large is used for feature extraction, pre-trained on 94,000 hours of data, and fine-tuned on 960h hours of Librispeech data. The Vicuna 7B [20] is used as the LLM decoder. A lightweight simple-structured linear projector is utilized for aligning the extracted speech feature with the LLM input space. It first downsamples the 50Hz features to 10Hz by a 1-D convolution layer and then projects it by two linear layers with an intermediate hidden layer dimension of 2048.\nFor contextual ASR, we incorporate a biasing list in the prompt to assist the LLM in accurately recognizing rare words that are often misidentified. Given the differences in tasks and scenarios compared to previous LLM-based ASR models, we design a new task instruction and introduce novel methods for generating biasing lists. The training biasing list is generated randomly from the transcriptions of speech in the current training batch, adhering to prior research [7, 10, 21]. Each transcription has a probability of $P_{keep}$ of being used to extract biasing words. For each kept transcription, k word-level n-grams are randomly chosen, where k is sampled uniformly from [1, $N_{phrases}$] and n is sampled uniformly from"}, {"title": "2.2. Filtering Algorithm", "content": "Given a biasing list that includes named entities or rare words, such as a contact name list or a song playlist, we aim to employ a filtering mechanism to identify the most likely relevant hotwords with the sentence to be decoded. These selected hotwords are then incorporated into the prompt of the LLM to enhance speech recognition. We mainly utilize the coarse decoding results obtained from the WavLM Large model finetuned using CTC loss and decoded with the Viterbi algorithm, which we denote as \"inference sentence\".\nFirstly, we build a 2-gram index dictionary for the predefined biasing list. For example, assuming the biasing list is [Bob, Joe], the 2-gram index dict will be ['bo' : 'bob',' ob' : 'bob',' jo': 'joe','oe' : 'joe']. Next, we remove \u201ccommon words\u201d from the inference sentence, which are defined as the top 5000 most frequently occurring words in the Librispeech training corpus. Then, we utilize the inference sentence to obtain preliminary screening candidates by indexing the dictionary. For instance, assuming the inference sentence is \u201cI like reading books\u201d, the candidate will be ['bob']. Finally, for each rare word in the processed inference sentence, we calculate the edit distance to identify the most similar word from the candidate list and include it in the prompt."}, {"title": "3. EXPERIMENTS", "content": "3.1. Experimental Setup\n3.1.1. Dataset\nWe utilize the Librispeech corpus as our dataset, following previous literature [10, 21-25]. We finetune the official WavLM Large pre-trained model and train the LLM-based ASR model on the complete 960 hours of training data. The standard dev (dev-clean and dev-other) and test (test-clean and test-other) sets are adopted for performance evaluation.\nThe artificial biasing list constructed in [22] is utilized for contextual ASR testing. They categorize the 5,000 most frequent words in the Librispeech training corpus as common words, with the remainder classified as rare words. The biasing list generated for the test set consists of two segments: rare words in the transcriptions, and distractors sampled from the 209.2K rare words vocabulary. Biasing lists of varying lengths are generated by incorporating $N$ = {100, 500, 1000, 2000} distractors into the lists.\n3.1.2. Configuration\nFine-tuning WavLM Large: We fine-tune the pre-trained model for 80k steps on the 960 hours of training data. The learning rate increases linearly from 0 to 3e-5 in the initial 8k steps, stays constant for 32k steps, and then decays exponentially to 5% of the peak rate in the remaining 40k steps. Parameters are frozen except for the final projection matrix for the initial 10k steps. Model selection is based on the WER result on the dev-other subset. A batch contains at most 200 seconds of speech audio.\nLLM-based ASR Training and Inference: Following [14, 15], the format of a training data input for the LLM is \u201c<speech> USER: <prompt> ASSISTANT: <transcription>\u201d. <speech> denotes the speech embedding, which has a dimension of 4096 aligned with the LLM. To steer the LLM towards performing ASR task, the <prompt> is designed as \u201cTranscribe speech to text.\u201d <transcription> refers to the ground-truth transcription of the speech.\nDuring the training process, we freeze the speech encoder (315.5M) and the LLM (6.7B) and only train the lightweight projector (15.7M). We compute the LM loss only on <transcription>. The model is trained for 100k steps, with the learning rate increasing linearly from 0 to a peak of le-4 within the first 1k steps and then linearly decaying to 0 during the remaining training period. We utilize the AdamW optimizer with $\\beta$ = (0.9,0.999) and zero weight decay. The experiments are conducted on 2 80GB A800 GPUs and the batch size is set to 4.\nDuring the inference process, the format of a test data input for the LLM is \u201c<speech> USER: <prompt> ASSISTANT:", "Inference": "nThe experimental setup is essentially consistent with LLM-based ASR, except that we need to modify the prompt to include a biasing list. Specifically, the <prompt> template is modified to \"Transcribe speech to text. Some hotwords might help. The hotwords are {}"}, {"title": "3.3. Ablation Study", "content": "The key to our LLM-based contextual ASR model is the filtering algorithm. The comparison of different filtering methods is illustrated in Table 4, and the evaluation results are presented in Table 3.\nAll three filtering methods first involve building a 2-gram index dictionary for the predefined biasing list. For F1 filtering method, we use the inference sentence to retrieve candidate hotwords by indexing the dictionary. Next, we calculate the similarity score between each candidate and the inference sentence. Specifically, we compute the edit distance between the candidate and each word in the sentence and take the smallest value as the result; in other words, we calculate the \"shortest distance\" between the candidate and the inference sentence. Finally, we rank the candidates based on their similarity scores in descending order. We then select either all words with a similarity score greater than 0.95 or the top five words (the average number of words in the biasing list during training). Employing this filtering method, the WER/B-WER are relatively reduced by 34.12%/53.99% and 24.05%/52.25% on clean and test sets respectively compared to the baseline LLM-based ASR model when using a biasing list of 100. As the size of the biasing list grows, both WER and B-WER increase slightly but still show a noticeable reduction compared to the baseline.\nThe experimental results reveal an issue with this filtering approach. Due to the unknown specific locations of rare words within the inference sentence, we need to compute the similarity between the candidate and each word of the sentence, leading to unnecessary matching of bias words highly similar to common words in the sentence. Thus, for F2 filtering method, we utilize the dataset's provided 5000 common words list to remove common words from the inference sentence, before retrieving initial candidate hotwords from the index dictionary. This improvement results in a relative reduction of 7.82%/6.41% in WER/B-WER on the test-other set, and a modest reduction of 4.44% and 1.40% on the testclean set, except for a minor performance drop for biasing list of 100.\nThe limitation of F2 filtering method is that, while the inference sentence only contains a few rare words, solely using a similarity threshold for filtering may result in the highestscoring hotwords being similar to some of these rare words, leaving other rare words unmatched with any relevant hotwords. To address this, the F3 filtering method matches every word in the inference sentence with the most similar hotword from the biasing list. This refinement shows a significant enhancement over the F2 filtering method. On the test-clean set, WER/B-WER is relatively reduced by 9.21%/22.26%, while on the test-other set, WER/B-WER is relatively reduced by 6.08%/16.13%. Ultimately, we adopt the F3 filtering method as our proposed approach."}, {"title": "4. CONCLUSION", "content": "In this work, we extend the LLM-based ASR model to contextual ASR tasks. We introduce an effective filtering algorithm designed to retrieve truly relevant hotwords. Our experiment results demonstrate that the CTC-Assisted LLM-Based Contextual ASR model significantly outperforms the baseline, improving the performance significantly compared to the previous methods. This underscores the accuracy and effectiveness of our filtering method, further validating that large language models can efficiently incorporate hotwords from prompts to enhance speech recognition accuracy, serving as a highly effective framework for performing contextual ASR tasks. In the future, we will further explore LLM-based contextual ASR and attempt to adapt and optimize our method to other datasets, and different languages other than English."}]}