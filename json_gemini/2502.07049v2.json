{"title": "LLMs in Software Security: A Survey of Vulnerability Detection Techniques and Insights", "authors": ["ZE SHENG", "ZHICHENG CHEN", "SHUNING GU", "HEQING HUANG", "GUOFEI GU", "JEFF HUANG"], "abstract": "Large Language Models (LLMs) are emerging as transformative tools for software vulnerability detection. Traditional methods, including static and dynamic analysis, face limitations in efficiency, false-positive rates, and scalability with modern software complexity. Through code structure analysis, pattern identification, and repair suggestion generation, LLMs demonstrate a novel approach to vulnerability mitigation.\nThis survey examines LLMs in vulnerability detection, analyzing problem formulation, model selection, application methodologies, datasets, and evaluation metrics. We investigate current research challenges, emphasizing cross-language detection, multimodal integration, and repository-level analysis. Based on our findings, we propose solutions addressing dataset scalability, model interpretability, and low-resource scenarios.\nOur contributions include: (1) a systematic analysis of LLM applications in vulnerability detection; (2) a unified framework examining patterns and variations across studies; and (3) identification of key challenges and research directions. This work advances the understanding of LLM-based vulnerability detection. The latest findings are maintained at https://github.com/OwenSanzas/LLM-For-Vulnerability-Detection", "sections": [{"title": "1 INTRODUCTION", "content": "Vulnerability detection plays an important part in the design and maintenance of modern software. Statistical evidence indicates that approximately 70% of security vulnerabilities originate from defects in the software development process [4]. According to the metrics provided by Common Vulnerabilities and Exposures Numbering Authorities (CNAs), a growth is witnessed that in the past 5 years, about 120,000 CVEs have been discovered and reported [20]. According to FBI's cybercrime report shown in Figure 1, the period from 2018 to 2023 suffers from a large amount of cybersecurity crimes and complaints. A recent example is the CrowdStrike incident in July 2024 [110], where a faulty software update caused widespread system crashes across critical infrastructure sectors including healthcare, transportation, and finance. Therefore, enhanced focus and investment in vulnerability detection technology is in demand.\nState-of-the-art vulnerability detection approaches/tools can be broadly classified into static analysis and dynamic analysis [18, 76, 100, 133]. Static analysis examines source code or bytecode to"}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 Paper Selection And Scope", "content": "To ensure a comprehensive and systematic review, we began our search with top-tier security conferences, such as IEEE Symposium on Security and Privacy (S&P), USENIX Security, and ACM Conference on Computer and Communications Security (CCS), as well as journals like IEEE Transactions on Software Engineering. Then we searched by extracting key terms such as \"vulnerability detection,\" \"LLM,\" \"large language model,\" and \"AI\" from papers published in conferences and journals. Using these keywords, we conducted iterative searches every three weeks, refining the selection over time. Over a two-month period, we screened approximately 500-600 papers and selected 58 highly relevant studies.\nThis survey focuses exclusively on the application of LLMs in vulnerability detection, analyzing techniques, datasets, benchmarks, and challenges. We reviewed works targeting programming languages like C/C++, Java, and Solidity, which are the primary focus areas for LLM-based vulnerability detection. Studies centered on traditional machine learning methods, such as CNNs and RNNs, and those unrelated to vulnerability detection, such as malware analysis or network intrusion detection, were excluded.\nIn terms of datasets, we primarily evaluated function-level and file-level granularity, noting that C/C++ datasets dominate the field. However, repository-level datasets that better reflect real-world development scenarios are significantly lacking. This limitation poses challenges for LLMs in generalizing to complex, multi-file vulnerabilities."}, {"title": "2.2 Related Reviews", "content": "In the past five years, many studies have been proposed on leveraging LLMs for vulnerability de-tection. Several comprehensive surveys have been presented on vulnerability detection techniques, covering traditional approaches (static/dynamic analysis) and machine learning methods (CNN, RNN) [4, 15, 27, 134, 146]. They do not specifically address the integration of LLMs in vulnerability detection. Yao et al. [129] reviewed LLMs in the security and privacy domain, proposing their positive impacts, potential threats, and inherent threat. However, their analysis focuses on a broad overview of these issues rather than providing a methodological summary of LLM-based detection approaches. Xu et al. [124] presented an overview of LLMs in the entire cybersecutiry domain, including malware analysis, network intrusion detection, etc. Our survey specifically focuses on LLM-based vulnerability detection with a more detailed summary of techniques and methodologies.\nAt the same time, Zhou et al. [140] investigated how LLMs are adapted for vulnerability detection and repair. While their work provides valuable insights, our survey differs in several aspects: (1) by the time of writing, both OpenAI and Anthropic have released more powerful LLMs (GPT-4o, o1 and Claude 3.5 Sonnet) that have stronger inference abilities and larger context windows; (2) we conduct a comprehensive analysis of benchmarks and evaluation metrics for LLM-based vulnerability detection systems; (3) we focus more on the details of vulnerability detection and understanding."}, {"title": "2.3 Large Language Models (LLMs)", "content": "Large Language Models (LLMs) have emerged as a significant progress in the evolution of language models [138]. The Transformer architecture has enabled unprecedented scaling capabilities. LLMs are characterized by their massive scale, typically incorporating hundreds of billions of parameters trained on vast corpus. Therefore, it leads to remarkable capabilities in general human tasks [21]."}, {"title": "2.4 Vulnerability Detection Problem", "content": ""}, {"title": "2.4.1 Domain Knowledge", "content": "Some popular vulnerability databases, such as Common Weakness Enumeration (CWE)\u00b9, Common Vulnerabilities and Exposures (CVE)\u00b2, Common Vulnerability Scoring System (CVSS)\u00b3 and National Vulnerability Database (NVD)\u2074, have been built to record the definition and evaluation of common vulnerabilities. CWE focuses on all vulnerabilities in the software development lifecycle (from development to maintenance.) Rather than focusing on specific real-world security vulnerabilities (e.g., Heartbleed and Log4Shell), CWE focuses on the root causes of these real-world vulnerabilities like Use-After-Free (CWE-416) and Out-of-bounds Write (CWE-787). CVE is a public community that identifies and catalogs security vulnerabilities in software and hardware. The community will allocate a unique identifier to each real-world vulnerability. For example, the identifier of Log4Shell is CVE-2021-44228. CVSS is a standardized framework for assessing the risk level of a vulnerability through a number of metrics: exploitability, impact, exploit code maturity, and remediation level, etc. These metrics result in an overall score on a scale of 0 to 10, and severity from low to critical. Log4Shell was given a CVSS score of 10 (severity: critical). NVD is a database that contains basic information about real-world vulnerabilities, such as"}, {"title": "2.4.2 Vulnerability Detection", "content": "Vulnerability detection serves as the core focus of all selected papers in this survey, representing the primary application of LLMs in software security. The fundamental task can be formally defined as a binary classification problem:\nLet $C_i$ denote the input source code and $VD_i$ represent an LLM-driven vulnerability detector. The output $Y_i \\in \\{0,1\\}$ indicates the vulnerability status where $Y_i = 1$ indicates that the code is vulnerable, and $Y_i = 0$ indicates that the code is non-vulnerable."}, {"title": "2.4.3 Vulnerability Classification", "content": "Beyond binary detection, some studies explore LLMs' capability in multi-class vulnerability classification to enhance model reliability. This task requires LLMs to not only identify the presence of vulnerabilities but also determine their specific types according to established standards like CWE.\nFormally, vulnerability classification can be defined as: Let $C_i$ denote the input source code and $VC_i$ represent an LLM-driven vulnerability classifier. The output $Y_i$ indicates the specific vulnerability type: $Y_i = VC_i(C_i) \\in \\{type_1, type_2, ..., type_n\\}$, where $type_i$ could be vulnerability names (e.g., Buffer Overflow, SQL Injection) or standardized identifiers (e.g., CWE-119, CWE-89). For example, when examining a code snippet, an LLM might not only detect its vulnerability but also classify it as \"CWE-79: Cross-site Scripting (XSS)\", providing more detailed guidance for security mitigation."}, {"title": "2.4.4 Vulnerability Severity Prediction", "content": "Some studies extend vulnerability analysis to include sever-ity prediction alongside detection. This can be formulated as either a multi-class classification problem or a regression task, depending on the granularity of severity measurement. Formally, let $C_i$ denote the input source code and $VS_i$ represent an LLM-driven severity predictor. The output $Y_i$ can be defined in two forms: it may represent a severity score such as \"low,\" \"medium,\" or \"high,\" based on the vulnerability severity level of the input source code $C_i$. Different studies have adopted varying approaches to severity prediction:"}, {"title": "3 RESEARCH RESULTS", "content": ""}, {"title": "3.1 Overview", "content": "Our analysis shows that research on LLM-based vulnerability detection mainly focuses on C/C++, Java, and Solidity. Each language has unique challenges and research priorities. Studies on C/C++ focus on memory-related vulnerabilities, which are critical in this domain. Java research addresses framework-specific vulnerabilities and complex interactions across components in web applications. Solidity research targets vulnerabilities in smart contracts, which are central to blockchain security.\nRecently, large decoder-only models, such as GPT and CodeLlama, have become the main choice due to their size and strong generalization abilities. These models are used in 65% of fine-tuning experiments. For example, Alam et al. [2] fine-tuned GPT-4 and achieved 99% accuracy in Solidity"}, {"title": "3.2 RQ1. What LLMs have been applied to vulnerability detection?", "content": "In this paper, we use the LLM categorization and taxonomy outlined by Pan et al. [94] and classify the primary LLMs into three architectural groups: 1) encoder-only, 2) encoder-decoder, and 3) decoder-only models. Due to space constraints, we will briefly introduce some representative LLMs in each category. Table 1 gives a clear overview of the strengths and weaknesses of each category of methods based on their inherent capabilities and limitations.\nEncoder-only LLMs. Encoder-only LLMs utilize only the encoder component of the Transformer model [94]. These models are specifically designed to analyze and represent code or language context without generating output sequences, making them ideal for tasks that demand a detailed understanding of syntax and semantics. By employing attention mechanisms, encoder-only models encode input sequences into structured representations that capture essential syntactic and semantic information [47]. In the software engineering (SE) domain, encoder-only models such as CodeBERT [30], GraphCodeBERT [43], CuBERT [55], VulBERTa [45], CCBERT [142], SOBERT [50], and BERTOverflow [107] have been widely used.\nEncoder-Decoder LLMs.Encoder-decoder models combine both the encoder and decoder com-ponents of the Transformer model, allowing them to handle tasks that require both understanding and generation of sequences. The encoder processes an input sequence, transforming it into a struc-tured representation, which is then decoded to produce an output sequence. This structure makes encoder-decoder models versatile for tasks that involve translating, summarizing, or transforming text or code. Prominent examples include PLBART [1], T5 [98], CodeT5 [117], UniXcoder [42], and NatGen [14].\nDecoder-only LLMs. Decoder-only LLMs focus exclusively on the decoder component of the Transformer architecture to generate text or code based on input prompts. This approach leverages the model's capacity to interpret and extend context, enabling it to produce complex and coherent sequences by predicting subsequent tokens. Widely adopted for tasks that emphasize generation, such as vulnerability detection and code suggestion, decoder-only models excel in identifying relevant patterns and potential issues within code. Notable examples in this category include the GPT series (GPT-2 [97], GPT-3 [9], GPT-3.5 [91], GPT-4 [92]), as well as models tailored specifically for code in software engineering, such as CodeGPT [78], Codex [16], Polycoder [123], Incoder [34], CodeGen series[90], Copilot [40], Code Llama [99], and StarCoder [64]. [143]\nIn analyzing 58 studies on vulnerability detection, we identified 33 distinct LLMs used across various tasks. GPT-4 emerged as the most frequently used model, appearing in 29 instances, followed by GPT-3.5 with 25 mentions. Among the categories, encoder-only models represented 24.2% of total usage, with CodeBERT, GraphCodeBERT, UniXcoder, and BERT as prominent examples. Encoder-decoder models, including CodeT5, made up 8.7% of usage, serving dual roles in code"}, {"title": "3.3 RQ2. What benchmarks, dataset and metrics have been designed to evaluate vulnerability detection?", "content": "In this section, we will start by examining the distribution of vulnerabilities across various programming languages and key software systems. We will then move on to discuss the benchmarks, datasets, and metrics commonly used in the field. Due to differences in design and feature, such as memory management in C/C++, unsafe deserialization in Python, and object injection and reflection in Java, different programming languages have different types of high-occurrence vulnerabilities. This is particularly significant, as many studies on vulnerability detection by LLMs focus on language-specific challenges [31, 66, 131]. Understanding these nuances is essential to evaluate and improve the effectiveness of vulnerability detection across different contexts.\nWe analyzed the CVE statistics across major software systems over the past five years (2019-2024), as shown in Table 3."}, {"title": "3.4 RQ3. What techniques are used in LLMs for vulnerability detection?", "content": "Currently, LLM-based vulnerability detection faces several key challenges: (1) data leakage leading to inflated performance metrics, (2) difficulty in understanding complex code context, (3) positional bias in large context windows causing information loss, and (4) high false positive rates and poor performance on zero-day vulnerabilities. Researchers have conducted extensive studies to address these challenges. This section summarizes and discusses current techniques applied to LLM-based vulnerability detection."}, {"title": "3.4.1 Code Data Preprocessing", "content": "Code processing techniques serve two primary objectives: (1) optimizing the utilization of LLMs' limited context window to improve efficiency, and (2) enhancing LLMs' comprehension of semantic information within the code to improve vulnerability detection capability.\nAbstract Syntax Tree Analysis. Abstract Syntax Tree (AST) provides a hierarchical repre-sentation of program structure, where code elements are organized into a tree format based on their syntactic relationships [118]. This structural representation eliminates non-essential syntax details while preserving the semantic relationships between code components.  AST applications in vulnerability detection can be categorized into several primary approaches: code segmentation and structural representation, where ASTs parse code into function-level segments for efficient processing within LLMs' context limits, as demonstrated by Zhou et al. [141] and Mao et al. [83]; semantic enhancement, where ASTs are integrated with natural language annotations to form structured comment trees (SCT), as implemented in SCALE"}, {"title": "Data/Control Flow Analysis", "content": "AST lacks a representation of the data and control flow of a program. Thus, in some papers [58, 66, 72, 75, 77, 106], data flow graph (DFG) and control flow graph (CFG) have been applied to help LLMs understand the interprocedural data flow and control flow in a program.  DFG summarizes the possible execution paths in a program, using nodes (or basic blocks, i.e., statements that are executed sequentially without any branching operation) to represent program constructs, and edges to represent the flow of data. CFG has the same basic blocks as nodes, but edges are used to represent the branching operations between basic code blocks. There are two main usages of DFG/CFG like providing extra contextual information in prompt and knowledge base. Combine source code with its DFG and CFG into prompt will result in a significant improvement in LLm's performance in identifying vulnerabilities [58, 72, 75, 77]; Sun et al. has proposed that DFG and CFG can be used in knowledge base, with graph-based similarity-search algorithm, to provide LLMs with the information of code segments with similar data and control flow structure [106]. Except for DFG and CFG, call graph has been used in vulnerability detection [79] to give LLM more information about dependencies between functions."}, {"title": "Retrieval-augmented Generation", "content": "Retrieval Augmented Generation (RAG) enhances the capabilities of LLMs by integrating an information retrieval system that provides extra related information to LLMs [62]. LLMs receives user input and applies a searcher to find relevant documents or pieces of information from a knowledge base. The retrieved information is combined with the original prompt to generate a response. In this way, RAG can solve the problem of insufficient knowledge of LLMs in certain domains, illusions and that large language models cannot update data in real time. Many papers have discussed how to choose the right knowledge as a high priority when building RAG for LLMs. [12, 57, 59, 86, 106]. Cao et al. directly use CWE database as external knowledge [12]. Many papers focused on combining code snippets, static analysis results with documentation of corresponding vulnerabilities as knowledge [57, 59, 86]. In addition to the knowledge mentioned above, Sun et al. used GPT-4 to summarize"}, {"title": "Program Slicing", "content": "Program slicing technique has been used to reduce vulnerability-irrelevant lines of code and keep critical lines related to trigger vulnerability [13, 82, 96, 137]. Purba et al. apply program slicing technique to extract code snippets for buffer-overflow detection [96]. These code snippets usually contains key functions, like strcmp and memset, and statements related to call these functions. Cao et al. use program slicing in similar way [13]. They first find all potential vulnerability triggers, and apply program slicing technique to collect all statements related to these triggers. While Purba et al. [96] and Cao et al. [13] only use the program slicing technique for code preprocess, Zhang et al. proposes to fine-tune LLMs with sliced code to improve the performance of LLMs vulnerability detection [137]. Instead of setting explicit slicing criteria, LLMs learns to segregate vulnerable lines of code from a given function during training. This approach helps the model to focus on relevant parts of the code and identify vulnerabilities more accurately. The program slicing technique has been shown to improve LLMs' ability to detect vulnerability [96, 137]."}, {"title": "LLVM Intermediate Representation", "content": "By converting source code to LLVM Intermediate Rep-resentation (IR) [61], analysis and detection methods do not need to be specifically adapted to different programming languages. This improves the versatility of vulnerability detection methods, and LLVM IR also preserves program structure and semantics, making it easier for LLM to analyze potential dependencies in the code. But the downside is obvious: LLVM IR doesn't work with Java and Javascript. To make the approach generalizable to programming languages, the authors converted the source code to LLVM IR and trained LLMs on these IR [82].\nWe can see that in the field of LLMs in vulnerability detection, the techniques mainly comes from traditional software analysis and LLM research. And basically the outputs of these techniques are used as part of the prompts to evaluate the LLMs' vulnerability detection capabilities. These approaches not only optimize the efficiency of LLMs' context window utilization, but also improve its understanding of potential vulnerabilities by preserving or extracting semantic information from the code. However, it also inevitably increases token consumption, and there is also the possibility that too much prompt content may reduce the ability of LLMs to detect vulnerabilities."}, {"title": "3.4.2 Prompt Engineering Techniques", "content": "This is one of the most widely used strategies for optimizing LLM-based vulnerability detection systems, as it enables precise control over model responses by tailoring input prompts.\nChain-of-Thought Prompting. Chain-of-Thought (CoT) Prompting is a technique where LLMs are guided to follow step-by-step instructions to enhance reasoning accuracy before generating a"}, {"title": "Few-shot Learning", "content": "In LLM-based vulnerability detection, few-shot learning (FSL) enables models to leverage a small set of labeled examples within prompts to improve task-specific per-formance. In this approach, vulnerability detection can be enhanced by embedding classification standards, such as CWE, directly into the prompt [37]. By incorporating CWE vulnerability cate-gories-complete with numbers and descriptive names\u2014the model gains contextual knowledge that aids in identifying and classifying vulnerabilities accurately.  illustrates the principle of few-shot learning, where the model is provided with labeled examples (e.g., Solidity Code 1 and Solidity Code 2) to understand the task before analyzing a new testing set. These examples, combined with the task-specific prompt, guide the fine-tuned LLM to generate accurate outputs."}, {"title": "Hierarchical Context Representation", "content": "Hierarchical context representation is a technique used to manage the context length limitations of LLMs when analyzing extensive codebases. In vulnerability detection, code can be organized hierarchically into modules, classes, functions, and statements. By representing the code in this hierarchical manner, the LLM can process and analyze the code at different levels of abstraction. This approach allows the model to focus on higher-level structures before delving into detailed code segments, effectively managing the context and improving the detection of vulnerabilities within the constraints of LLMs's maximum input length."}, {"title": "Multi-level Prompting", "content": "The multi-level prompting strategy involves breaking down the vulnerability detection task into multiple prompts, each targeting a specific level of analysis. Instead of presenting the entire code and task in a single prompt, the strategy divides the process into stages. For example, the first prompt may ask the LLM to provide a high-level summary of the code's functionality. The second prompt might focus on identifying potential security issues, and subsequent prompts could request detailed analyses of specific sections. This layered approach helps the LLM to systematically process complex code, enhancing its ability to detect vulnerabilities by focusing on one aspect at a time.  illustrates an instance of multi-level prompting."}, {"title": "Multiple Prompt Agents and Templates", "content": "This technique employs several specialized prompt agents, each designed with a specific template to perform distinct roles in the vulnerability detection"}, {"title": "3.4.3 Fine-tuning", "content": "Fine-tuning helps Large Language Models (LLMs) learn specific tasks better. It works by training pre-trained models again with new data for these tasks. Fine-tuning is important for three main reasons [129, 140]: (1) security problems in code follow special patterns that LLMs must learn to find, (2) computer code is different from normal text, so LLMs need to learn how to read and understand code better, and (3) finding security problems needs to be very accurate"}, {"title": "3.5 RQ4. What are the challenges that LLMs are facing in detecting vulnerabilities and potential directions to solve them?", "content": "The field currently faces four major challenges, along with corresponding potential directions, as illustrated in Figure 12. First, researchers struggle to obtain high-quality datasets. Second, large language models (LLMs) show reduced effectiveness when dealing with complex vulnerabilities. Third, these models have limited success in real-world repository applications. Fourth, the models lack robust generation capabilities. Multiple studies confirm these challenges as the main barriers to progress. The following sections examine each challenge in detail."}, {"title": "Challenge 1: Limited scope of research problems", "content": "Current research focuses primarily on determining whether a given code snippet is vulnerable or not. In this survey, approximately 40 studies (83%) concentrate on the analysis of isolated code snippets, where LLM performance often exceeds the results observed in real-world code detection scenarios [37]. While this provides a controlled environment for evaluation, it overlooks the complexities present in practical applications, such as analyzing entire codebases or addressing vulnerabilities that emerge during collaborative development. This indicates that research focused solely on isolated functions or snippets has limited usefulness for real-world scenarios.\nPotential Directions: Beyond analyzing isolated code segments, future research should be structured around the following key problems in real-world development:\nResearch problems categorized by code evolution in development:\n\u2022 Full-scale/Incremental Detection: Full-scale detection requires analyzing entire codebases across multiple files, while incremental detection focuses on new code commits. Current LLMs excel at analyzing isolated code segments but struggle with broader contexts [53]. As a result, LLMs typically assist static analysis tools or support fuzzing tasks [80, 126, 127] rather than performing standalone analysis. For commit-level detection, existing methods combine commits with static analysis results [63, 128], but may fail when encountering APIs outside their training corpus [53]."}, {"title": "Research problems categorized by vulnerability report workflow", "content": "\u2022 Reproducing Vulnerabilities: Vulnerability reproduction will be essential in future research and a key to reducing false positives. For each detected vulnerability, LLMs should attempt reproduction using input drivers such as fuzzers [53]. By generating inputs that trigger potential vulnerabilities, LLMs can provide evidence of vulnerability existence [122, 126]. This approach validates the detection and ensures findings are actionable, thus improving vulnerability report reliability."}, {"title": "\u2022 Vulnerability Repair", "content": "While many studies discussed vulnerability repair, practical imple-mentation in real-world projects remains challenging [143]. Successful vulnerability repair in production environments must meet several criteria:\nRepaired code must pass all existing tests\nRepaired code must prevent vulnerability reproduction\nRepaired code should not introduce new vulnerabilities\nCurrent limitations in dataset quality and LLM capabilities hinder effective vulnerability repair validation. While LLMs can identify vulnerable code, they often misidentify vulnerability locations, leading to incorrect explanations and repairs. The ability to generate proof-of-concept exploits and simulate program operations would improve validation, but this requires significant advances in LLM capabilities [60]."}, {"title": "Research problems categorized by vulnerability characteristics", "content": "\u2022 Specialized Vulnerability Detection: The development of targeted detection approaches for specific vulnerability categories represents a significant challenge. Current research [36] demonstrates that LLMs exhibit varying capabilities across different vulnerability types - achieving high accuracy in detecting Out-of-bounds Write vulnerabilities (CWE-787) while performing poorly with Missing Authorization issues (CWE-862). This performance variation necessitates specialized detection mechanisms for distinct CWE categories, particularly for high-frequency vulnerabilities such as memory-related issues. The lack of category-specific research and datasets has left this critical area largely unexplored.\n\u2022 Vulnerability Classification and Severity Assessment: Alam et al. [2] and Gao et al. [36] highlight two fundamental challenges: First, accurately categorizing detected vulnerabilities according to established frameworks (e.g., CWE) remains difficult. This classification problem is essential to practical development workflows, as different vulnerability types require distinct remediation approaches. Second, predicting vulnerability severity levels directly influences remediation priorities and timelines, with high-risk vulnerabilities requiring expedited mitigation."}, {"title": "Challenge 2: Complexity of Representing Vulnerability Semantics", "content": "Vulnerability patterns are often very complex [65]. More than 95% of studies report that code complexity-such as external dependencies, multiple function calls, global variables, and complicated software states\u2014makes it hard to detect vulnerabilities. We can measure this complexity by lines of code or cyclical complexity [108], and visualize it using program dependency graphs (DFGs) or call graphs. However, most methods focus on single code blocks at the function or file level [25, 137], which is not very helpful for large projects. When dealing with complex projects, LLMs often have limited input and face much \u201cunseen code", "Directions": "Two core approaches can address this challenge. The first approach focuses on enabling LLMs to read more code. This increases their understanding of the entire repos-itory. The second approach emphasizes using abstract representations to simplify code semantics. This enhances LLMs' comprehension of code structure and behavior.\n\u2022 Dynamic Code Knowledge Expansion: Through feedback loops and adaptive mechanisms, LLMs should be enabled to freely access and understand repository code [127, 136]. This would address high false positive rates by providing broader context for vulnerability analysis."}, {"title": "\u2022 Optimized Code Representation", "content": "Studies [58, 72, 75, 77, 106] utilize AST, CFG, and DFG representations to reduce token counts for limited context windows. While current imple-mentations haven't significantly improved detection accuracy, future research opportunities include sophisticated semantic processing, multi-method integration, better context preser-vation, and hybrid graph representations."}, {"title": "\u2022 Specialized LLM Agents", "content": "Research explores optimization through specialized LLM agents. Studies [115] demonstrate that task division among multiple agents increases output robust-ness. Each agent specializes in specific aspects of vulnerability detection. Prompt engineering research [75] evaluates zero-shot, few-shot, and chain-of-thought approaches for detection accuracy. However, code complexity introduces challenges. Multiple agents show accuracy degradation with complex code. Few-shot and chain-of-thought methods cannot provide sufficient additional context."}, {"title": "\u2022 Integration with External Tools", "content": "External tools provide important support mechanisms. LangChain improves efficiency through simplified and asynchronous LLM calls. Retrieval-Augmented Generation (RAG) gains popularity due to its cost-effectiveness and efficiency. Studies [12, 26] implement RAG to vectorize code contexts and enhance detection through LLM-based retrieval. However, code contexts differ fundamentally from natural language. This difference necessitates specialized approaches for code semantic extraction, storage, and comparison.\nThese optimization techniques could potentially bridge the gap between LLMs' current capabilities and the complex requirements of real-world vulnerability detection in large codebases."}, {"title": "Challenge 3: Intrinsic Limitations of LLMs", "content": "Detection solutions must maintain robustness against data changes and adversarial attacks [25]. However, research by Yin et al. [132] reveals that LLMs lack this robustness. They show vulnerability to data perturbations. These findings indicate the need for more reliable approaches.\nAdditionally, LLMs need better explainability and consistency for real-world applications. Hau-rogne et al. and Du et al. [25, 46] demonstrate that LLMs produce inconsistent vulnerability explanations. They cannot guarantee correct explanations in every instance. The outputs show randomness across different runs. Even when LLMs correctly identify vulnerable code, they often fail to provide accurate vulnerability explanations. This limitation creates significant problems for subsequent repair and review processes. Current research in this area remains insufficient.\nFuture research should focus on these key areas: improving accuracy, enhancing robustness, and increasing output reliability and explainability. These improvements will make LLM-based solutions more practical for real-world use."}, {"title": "Potential Directions", "content": "The core to this challenge is to improve the inherent vulnerability detection ability of LLMs. Researchers may focus on training new models or fine-tuning LLMs.\n\u2022 Fine-tune Frontier LLMs: Recent findings on scaling laws [56] indicate that larger decoder-only language models, such as GPT-4 and Claude3.5-Sonnet, can achieve systematically improved performance as model size, training data, and compute are scaled up. Improvements in model capabilities enhance vulnerability detection, classification, and explanation. Research by Alam et al. [2] shows that GPT-4 and GPT-3.5 achieve higher detection accuracy than earlier models like Llama2 and CodeT5 under identical prompts. The release of GPT-O1, with its visible reasoning process, suggests improvements in both detection capability and output explainability.\nFine-tuning approaches show promise. Several studies [2, 13, 25, 44, 81, 84] fine-tune open-source models like CodeLlama and CodeBERT. These achieve results comparable to general LLMs. However, dataset limitations present challenges. Research [113] indicates that practical applications require datasets of at least 100,000 examples. This creates significant training cost barriers. Researchers can enhance vulnerability detection performance through the following approaches:\nVulnerability-specific Fine-tuning: Fine-tune models on specific vulnerability types. Research [2, 13, 25] shows models trained on specific vulnerability categories (e.g., memory-related issues, injection flaws) achieve higher detection accuracy. This targeted approach allows models to learn deeper patterns within each vulnerability type.\nRepository-adaptive Fine-tuning: Adapt models to specific codebases through fine-tuning on repository-specific data. Studies [45, 82] demonstrate this approach improves detection accuracy by helping models understand project-specific coding patterns and architecture. This method benefits large, complex projects with unique coding conventions."}, {"title": "\u2022 Ensemble Learning and Domain-Adaptive Pretraining", "content": "Combining predictions from multiple models effectively reduces false positives and improves detection accuracy. DAPT can refine LLMs' understanding of specific contexts by leveraging curated datasets including both public records (e.g., NVD) and domain-specific data. This enables better identification of niche vulnerabilities and improved generalization."}, {"title": "\u2022 Adaptive Learning Mechanisms", "content": "To address the dynamic nature of security threats and enhance model robustness, adaptive learning [85] mechanisms allow continuous knowledge updates through feedback loops and periodic retraining. Advanced optimization techniques can further improve prompt configurations and learning rates, ensuring reliability in real-world applications."}, {"title": "Challenge 4: Lack of High-Quality Datasets", "content": "High-quality vulnerability benchmark datasets remain scarce. Current datasets face several problems. These include data leakage", "121": ".", "Incorrectness": "A critical issue in these challenges is the incorrect labeling of vulnera-bilities, which harms the reliability and effectiveness of datasets. Automated collection methods [89"}]}