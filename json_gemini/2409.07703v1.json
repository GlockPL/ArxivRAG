{"title": "DSBENCH: HOW FAR ARE DATA SCIENCE AGENTS TO\nBECOMING DATA SCIENCE EXPERTS?", "authors": ["Liqiang Jing", "Zhehui Huang", "Xiaoyang Wang", "Wenlin Yao", "Wenhao Yu", "Kaixin Ma", "Hongming Zhang", "Xinya Du", "Dong Yu"], "abstract": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)\nhave demonstrated impressive language/vision reasoning abilities, igniting the re-\ncent trend of building agents for targeted applications such as shopping assistants\nor Al software engineers. Recently, many data science benchmarks have been\nproposed to investigate their performance in the data science domain. However,\nexisting data science benchmarks still fall short when compared to real-world data\nscience applications due to their simplified settings. To bridge this gap, we in-\ntroduce DSBench, a comprehensive benchmark designed to evaluate data science\nagents with realistic tasks. This benchmark includes 466 data analysis tasks and 74\ndata modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench\noffers a realistic setting by encompassing long contexts, multimodal task back-\ngrounds, reasoning with large data files and multi-table structures, and performing\nend-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs,\nand agents shows that they struggle with most tasks, with the best agent solving\nonly 34.12% of data analysis tasks and achieving a 34.74% Relative Performance\nGap (RPG). These findings underscore the need for further advancements in de-\nveloping more practical, intelligent, and autonomous data science agents. 1.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) (OpenAI, 2023a; Touvron et al., 2023b) and Large Vision-\nLanguage Models (LVLMs) (OpenAI, 2023b; Liu et al., 2023b) have achieved compelling success\non various vision and language tasks, such as natural language understanding (Wang et al., 2019),\nvisual question answering (Antol et al., 2015), and image captioning(Lin et al., 2014), demonstrat-\ning their adaptability and effectiveness. However, despite their achievements, LLMs and LVLMs\nface limitations when applied to certain real-world tasks due to the lack of integration with practical\napplications, such as computer manipulation. To address this, advanced LLMs and LVLMs are in-\ncreasingly being incorporated into interactive intelligent systems, enabling them to tackle complex\ntasks with additional tools and interfaces. A prominent example of this is the data science agent,\nan emerging research area that assists individuals and organizations in making informed decisions,\npredicting trends, and improving processes by analyzing large volumes of data (Hong et al., 2024;\nGuo et al., 2024; chapyer team, 2023; Zhang et al., 2024b).\nThe data science agent aims to address data-centric scientific problems, including machine learning,\ndata analysis, and mathematical problem-solving, which present unique challenges, such as complex\nand lengthy task-handling steps. For example, Jupyter AI (jupyter-ai team, 2023) connects gener-\native language models with Jupyter notebooks\u00b2 and provides a user-friendly and powerful way to\nimprove developer productivity in the Jupyter Notebook. MLCopilot (Zhang et al., 2024a) leverages\nLLMs to make solutions for novel real-world machine learning tasks, based on the existing experi-\nences from historical tasks. To evaluate the performance of the data science agent, the existing work\nfocuses on developing either code generation benchmarks (Zhang et al., 2024b; Zan et al., 2022;"}, {"title": "DATA SCIENCE AGENT BENCHMARK", "content": "Our proposed data science benchmark DSBench mainly consists of two parts: the data analysis task\nand the data modeling task. These tasks were chosen because they mirror the real-world challenges\nthat data science experts face in their professional work. Data science often requires handling com-\nplex data, extracting insights, and building models to solve problems. To ensure DSBench reflects\nthese practical demands, we focus on these two task types, making the benchmark more realistic and\naligned with the actual needs of data scientists. In our search for appropriate datasets and challenges,\nwe found that both ModelOff and Kaggle provide excellent examples of tasks that fit our require-\nments, as they cover diverse and realistic problems, making them well-suited for our benchmark."}, {"title": "DATA ANALYSIS", "content": "Data analysis tasks focus on answering a data analysis question that needs the agent to fully under-\nstand the data and the question's intent. The answer is unique and certain."}, {"title": "DATA COLLECTION", "content": "Modeloff is a global data analysis competition that challenges contestants to use Excel to solve data-\ncentric questions and case studies. It mostly focuses on independent questions that involve mini\nexercises in Excel. We found that the questions in Modeloff challenges consist of data analysis tasks\nthat can be solved by different tools, such as Python, Excel, and Matlab. Therefore, we resort to the\nModeloff challenge for the evaluation of data analysis ability in data science agents. We collected all\nModeloff challenges and then filtered all the challenges that didn't contain any questions. Finally,\nthe original 43 challenges are filtered down to 38 challenges with 466 questions. The question\ntypes can be categorized into multi-choice questions and fill-in-the-blank questions. All the statics\ninformation of our data analysis tasks is detailed in Table 2."}, {"title": "TASK FORMULATION", "content": "Input and output. Suppose we have the task introduction I, the data files D = {d\u2081,\u2026\u2026,dna},\nand the question Q, we then feed them into a data science agent G to answer the question Q, i.e.,\n\u00c2 = G(I, D,Q). Na is the total number of data files and it can be 1. A is the generated answer\nby the G with the tool set T. The toolset T can contain any tools that can operate the data and be\nutilized by LLMs/LVLMs.\nEvaluation metrics. To evaluate the performance of the whole agent system, we compare the\nsemantics of ground-truth answer A and the predicted answer \u00c2 by S(A, \u00c2). If the semantics\nof ground-truth answer A and the predicted answer A are the same, we consider the generated\nanswer to have successfully answered the question. S(\u00b7) is the semantics comparison function that is\nimplemented by a LLM and prompt in Appendix A. The metric for our benchmark is the percentage\nof data science questions that are answered correctly, i.e., task-level accuracy. In addition, we also\nintroduce competition-level accuracy for comprehensive evaluation. Competition-level accuracy is\ncalculated by averaging the accuracy scores obtained from each competition."}, {"title": "FEATURES", "content": "Different from the previous data analysis benchmark, our data analysis tasks, have several distinct\nproperties, which are discussed as follows:\nVaries Modalities. Different from the previous works which mainly focus on textual modality\n(e.g., (Lai et al., 2023; Cobbe et al., 2021)), our task consists of various modalities, such as images,\nExcel files, and tables. To show the distribution of the different modalities in different challenges,\nwe visualized the number of challenges in different modalities, as shown in Figure 2(a).\nComplex Table. Various tables are contained in this dataset. There may be several tables for\none question. Therefore, the data science agent should identify which table is important for the\ncurrent question. Furthermore, some questions are solved depending on multiple tables. Different\nfrom the previous benchmark, the length of the table is longer. It is unlikely to solve this kind of\nquestion without additional tools, even for humans. In addition, the formats of different tables in\none challenge are changeable. For example, some tables have properties in their first row, and some\ntables have properties in their first column.\nDiverse Long Context. The context in our dataset consists of long textual descriptions (815.44\nwords on average) as well as multiple modality content. Resolving this kind of data analysis question\nrequires a full understanding of the long description text and the corresponding tables, data files, and\nimages, and identifying the important context semantic content with the current question that needs\nto be answered.\nWide Scope for Possible Solutions. Our evaluation task provides a critical platform for assessing\nthe capabilities of data science agents and the corresponding systems. Our data analysis tasks can\nbe designed to compare a variety of approaches, from pure LLMs/LVLMs to cutting-edge data\nscience agents. The task setting greatly expands the freedom of tool use and encourages developers\nto employ creative strategies that may diverge from established norms (such as using Excel). For\nexample, we can use either Excel or Python to calculate the amount of tax a company pays."}, {"title": "DATA MODELING", "content": "Data modeling tasks focus on using machine learning technology to learn from data and generalize\nto unseen data, making predictions with the task instruction. Different from data analysis tasks that\naim to make a single and verifiable result, data modeling results in predictions that are evaluated\nbased on metrics for a testing set, such as accuracy."}, {"title": "DATA COLLECTION", "content": "To evaluate the performance of data science agents on data modeling tasks, we resort to machine\nlearning competitions. Kaggle is a data science competition platform and online community for\ndata scientists and machine learning practitioners. From the platform, we found there are total 648\ncompetitions. Since the testing set in the Kaggle competition is inaccessible, we split the original\ntraining set into the training set and testing set as an 8:2 ratio for evaluation. In this way, we could\ndirectly get the performance of the solution devised by a data science agent, avoiding submitting the\nsolution to the Kaggle website. To split the dataset easily, we only retain the competitions with a\ntraining file, a testing file, and a sample of submission file. Then we can use an automatic code to\nsplit the original data files in the Kaggle competition. Finally, we got 74 data modeling competitions\nin our data science benchmark. All the statics information of our data modeling tasks is detailed in\nTable 3."}, {"title": "TASK FORMULATION", "content": "Input and output. Suppose we have the competition description E, the training set A, the testing\nset S, and the sample of the submission file M, we then feed them into a data science agent G,\nwhich could devise an algorithm and implement the corresponding code to generate the submission\nfile F which is the predicted result for the input testing set, i.e., F = G(E,N,S,M). \u00ceF is the\ngenerated submission file by the G with the toolset T and it has a similar format with the sample of\nsubmission file M. The toolset T can contain any tools that can operate the data and be utilized by\nLLMS/LVLMs.\nEvaluation metrics. To evaluate the overall performance in all competitions, we first adopted\nTask Sucess Rate as the evaluation metric, i.e., whether the agent can build an ML model and\ngenerate the submission file in a bug-free manner within a fixed number of steps. To further learn\nthe performance of the model devised by the agent, we also evaluate the predicted submission file F\nwith the original metric corresponding to the competition as p = f(F, F). F is the ground-truth file\nfor the testing set, f(\u00b7) is the evaluation function of the specific metric (such as F1 and accuracy),\nand p is the performance. However, the metrics are different across different competitions, so we\ncan not directly take the average across tasks. Thus we calculate the gap between the performance\nof the file submitted by the agent and the performance of the human expert as an evaluation indicator\nof the data modeling task. Specifically, we devise the Relative Performance Gap (RPG) metric to\nshow the performance of the data science model, formulated as\n$$\\frac{1}{N}\\sum_{i=1}^{N} max((p_i - b_i)/(g_i \u2013 b_i), 0).$$\nN is the total number of competitions. pi is the performance of the predicted submission file for\nthe i-th competition and gi is the highest performance value = for the i-th competition. bi is the\nperformance of a baseline. More details about how to get the performance of baseline can be found\nin Appendix B."}, {"title": "FEATURES", "content": "Long context. For each competition in Kaggle, we crawled the corresponding task, evaluation, and\ndataset description, as shown in Figure 3. The description depicts the task background and the aim\nof the competition. The evaluation introduces what metric is used to evaluate the performance of the\ncompetition and how the metric is computed. The data description contains the overall description\nof all data sets and the explanation of each attribution in the data file. The average length of context\ntext and data of our data modeling tasks is 79187.\nEnd-to-end setting. Different from the existing work that only asks the model to generate the\ncode, our task setting is more difficult and requires a wider range of capabilities of agents, e.g.,\nmodel design, code implementation, self-debugging, etc. In other words, our data modeling task\nis end-to-end and every step in task resolving focus on the different ability of agents. The end-to-\nend setting allows the model to complete the task with minimal constraints, which is similar to the\nreal-life task the data science experts face. Hence, our data modeling task is not only to test the\ncapability of the underlying LLMs/LVLMs but also to test whether the interactions and environment\nin the system are properly designed.\nExecution-based Evaluation. We use execution Python script to verify the usability of the sub-\nmission file and evaluate the performance of the submission file from data science agents. Hence,\nthe generated submission file from data science agents should strictly comply with the file format\nrequirements in the input. For different competitions, we may use different metrics. In total, the\nnumber of metrics in our dataset is 18 and the distribution of the number of competitions in each\nmetric is shown in Figure 4."}, {"title": "EXPERIMENT", "content": "As we mentioned before, we mainly use the accuracy for the data analysis task, and RPG score for\nthe data modeling task. In addition, we also report costs (if the agent utilizes a charging API) and the"}, {"title": "QUANTATIVE ANALYSIS", "content": "We show the performance comparison among different baselines on data analysis tasks in terms\nof accuracy rate, cost, inference time, and challenge accuracy in Table 4. From the Table, we ob-\nserved that: (1) Models that perform better on basic language tasks are more capable of performing\ndata analysis tasks. For example, GPT-4o achieves the best performance among all vanilla model-\nonly baselines and it also shows advanced performance on several general language tasks, such as\nMMLU (Hendrycks et al., 2021a), GPQA (Rein et al., 2023) and MATH (Hendrycks et al., 2021c).\n(2) The AutoGen framework tends to consume more time to finish data analysis tasks and has higher\ncosts compared to the original vanilla model-only method. The reason is that AutoGen usually de-\nvises a multi-turn conversation between multi-agents to resolve a data analysis task. (3) AutoGen\nwith GPT-3.5/GPT-4/GPT-4o/GPT-4o mini perform better than the original vanilla GPT-3.5/GPT-\n4/GPT-4o/GPT-4o mini. This indicates the devised interaction way and tools within AutoGen are\nbeneficial for resolving data analysis tasks. (4) Although the advanced GPT-4o achieved the best\nperformance, it consumes more time and money compared with GPT-3.5.\nBesides, to learn the performance of humans on these data analysis tasks and reduce the cost of\nhuman annotation, we randomly sampled 10 data analysis challenges from our benchmark for human\nlabeling. We show the performance of baselines and manual annotation in Table 5. We found that\nbaselines show similar performance across whole and sampled testing data. For example, LLaVA\nachieved the worst performance on both original and sampled testing data. Even the most advanced"}, {"title": "QUALITATIVE ANALYSIS", "content": "To learn the intuitive performance of the baseline, due to the limited space, we only present the\nperformance of GPT-40 on one testing analysis task in Figure 8. More examples and in-depth\nanalyses can be found in Appendix E.\nIn Figure 8, we show the introduction of the challenge, the question, a screenshot of a portion of the\ndata file, and the generation text from GPT-40. The introduction part shows the background of the\nwhole challenge: The fictional country of Excelstan held an election for Congress, dividing its 1,000\nvoters across 9 districts (named after Greek letters) where voters ranked their preferences among 8\npolitical parties. The data file shows how each voter voted. The question asks the model to count\nhow many voters there are in every district and answer which district has the smallest number of\nvoters. Based on the generation from GPT-40, we found that the model misunderstood the meaning\nof the introduction and misinterpreted the district code as the voter identity code, leading to the\nwrong answer."}, {"title": "ERROR ANALYSIS", "content": "In our analysis, we identified several common types of errors that future work can address: (1)\nMisinterpretation of Data: This occurs when the agent misinterprets data, such as confusing a year\nwith a person's ID. Such errors indicate a failure in accurately perceiving and understanding the\ndataset. (2) Inadequate Data Identification: When this error occurs, the agent fails to identify and\nretrieve the necessary data for the task. As a result, the agent simply indicates that it lacks the\ndata needed to complete the task and cannot compute an answer without further input. (3) Lack\nof Problem-Solving Strategy: Incorrect approaches or formulas lead to erroneous answers. This\nhighlights the agent's deficiency in developing a correct problem-solving strategy, which is crucial\nfor deriving accurate results."}, {"title": "RELATED WORK", "content": "LLMS/LVLMs as Agent. Recently, with the rapid advancements in natural language process-\ning (NLP) and computer vision, LLMs and LVLMs have become pivotal components in intelligent\nagent systems. The advanced language models, such as GPT-3.5 (OpenAI, 2022), GPT-4 (Ope-\nnAI, 2023a), LLaMA (Touvron et al., 2023a;b), and LLaVA (Liu et al., 2023b), have demonstrated\nexceptional performance across various tasks, including language comprehension, image recogni-\ntion, dialogue generation, and complex task execution. Therefore, many researchers have shifted\ntheir research attention from fundamental models to agents. Initially, most work research focuses on\nmaking decisions in a simulated textual environment (Gao et al., 2023; Yao et al., 2023a; Shinn et al.,\n2023; Liu et al., 2023a; Gu et al., 2023). For example, ReAct (Yao et al., 2023b) firstly proposed to\ncombine Chain-of-Thought (CoT) (Wei et al., 2022) for agent tasks. Despite these works achieving\ncompelling success, they can not be applied in many real-world scenarios since they lack the capa-\nbility of using additional tools and dynamic interaction environments. Therefore, LLMs/LVLMs are\nequipped with a code interpreter (Yang et al., 2024; Hu et al., 2024), web browser (He et al., 2024),\nor Microsoft Office (Wu et al., 2024). Meanwhile, AppAgent (Zhang et al., 2023a), OS-Copilot (Wu\net al., 2024) and SWE-agent (Yang et al., 2024) observe the actual work environment rather than the\nsimplified simulated scenarios and then take actions. These agents can be applied to tackle vari-\nous tasks, such as web manipulation tasks (Li et al., 2023), playing Minecraft (Wang et al., 2023),\nautomating spreadsheet control (Xie et al., 2024), and data science (Team, 2023; Guo et al., 2024;\nHong et al., 2024). Although the existing work has achieved greater advances in various tasks, how\nto evaluate the performance of the agent for specific sceneries remains a challenge.\nEvaluation in LLMs/LVLMs. Evaluating the performance of LLMs/LVLMs can bring new in-\nsights and tell researchers how to improve the model. Therefore, how to evaluate the advanced\nLLMs/LVLMs always is a fundamental research direction. Early research focuses on evaluating\nlanguage models with specific NLP tasks, such as sentiment classification (Sun et al., 2023), named\nentity recognition (Sang & Meulder, 2003), information extraction (Sundheim, 1992), and text sum-\nmarization (Nallapati et al., 2016). Meanwhile, evaluation metrics also emerged to evaluate the per-\nformance of models in these specific NLP task benchmarks, such as BLEU (Papineni et al., 2002),"}, {"title": "CONCLUSION", "content": "The complexity of real-world data science projects extends far beyond mere code generation and\nsimplified numerical calculation. In this paper, we propose a data science benchmark, named DS-\nBench, which consists of 466 data analysis tasks and 74 data modeling tasks. By incorporating\nchallenges from ModelOff and Kaggle competitions, our benchmark offers a genuine representation\nof practical data science environments. This authentic context stimulates the creation of innovative\nsolutions that can be readily applied to actual data science problems. We believe that this benchmark,\ntogether with our other contributions, will prove to be valuable assets in advancing the development\nof more practical, intelligent, and autonomous data science models."}, {"title": "LIMITATION", "content": "Our data modeling tasks are exclusively sourced from Kaggle, which provides a robust starting\npoint. However, this approach has its limitations, as there are numerous other challenges and datasets\navailable beyond Kaggle that encompass a wider variety of modalities. Future work should consider\nincorporating these additional sources to provide a more comprehensive evaluation of data science\nagents and to address a broader spectrum of challenges. Besides, our experiments aim to establish a\nbaseline by employing the simplest and most straightforward approaches for evaluating data science\nagents. It is important to note that we do not intend to constrain future methodologies to these basic\napproaches. We acknowledge the inherent limitations in current data science models and encourage\nfuture research to explore diverse and innovative methods, such as agent-based approaches and tool-\naugmented language models, to overcome these limitations and advance the field."}]}