{"title": "Covering Numbers for Deep ReLU Networks with Applications to Function Approximation and Nonparametric Regression", "authors": ["Weigutian Ou", "Helmut B\u00f6lcskei"], "abstract": "Covering numbers of families of (deep) ReLU networks have been used to characterize their approximation-theoretic performance, upper-bound the prediction error they incur in nonparametric regression, and quantify their classification capacity. These results are based on covering number upper bounds obtained through the explicit construction of coverings. Lower bounds on covering numbers do not seem to be available in the literature. The present paper fills this gap by deriving tight (up to a multiplicative constant) lower and upper bounds on the covering numbers of fully-connected networks with bounded weights, sparse networks with bounded weights, and fully-connected networks with quantized weights. Thanks to the tightness of the bounds, a fundamental understanding of the impact of sparsity, quantization, bounded vs. unbounded weights, and network output truncation can be developed. Furthermore, the bounds allow to characterize the fundamental limits of neural network transformation, including network compression, and lead to sharp upper bounds on the prediction error in nonparametric regression through deep networks. Specifically, we can remove a log(n)-factor in the best-known sample complexity rate in the estimation of Lipschitz functions through deep networks thereby establishing optimality. Finally, we identify a systematic relation between optimal nonparametric regression and optimal approximation through deep networks, unifying numerous results in the literature and uncovering general underlying principles.", "sections": [{"title": "1 Introduction", "content": "It is well known that neural networks exhibit universal approximation properties [1, 2, 3, 4], but these results typically require infinitely large, specifically infinitely wide, networks. Neural networks employed in practice are, however, subject to constraints on width, depth, weight magnitude and precision, and connectivity (i.e., the number of nonzero weights). \u03a4\u03bf"}, {"title": "1.1 Important Definitions", "content": "We start with the definition of ReLU networks.\nDefinition 1.1. Let $L, N_0, N_1, \\ldots, N_L \\in \\mathbb{N}$. A network configuration $\\Phi$ is a sequence of matrix-vector tuples\n$$\\Phi = ((A_i, b_i))_{i=1}^L,$$\nwith $A_i \\in \\mathbb{R}^{N_i \\times N_{i-1}}, b_i \\in \\mathbb{R}^{N_i}, i = 1,...,L$. We refer to $N_i$ as the width of the $i$-th layer,\n$i = 0,...,L$, and call the tuple $(N_0,..., N_L)$ the architecture of the network configuration.\n$\\mathcal{N}(d)$ denotes the set of all network configurations with input dimension $N_0 = d$ and output\ndimension $N_L = 1$. The depth of the configuration $\\Phi$ is $L(\\Phi) := L$, its width $W(\\Phi) :=\\max_{i=0,...,L} N_i$, its weight set $\\operatorname{coef}(\\Phi) := \\bigcup_{i=1,...,L} (\\operatorname{coef}(A_i) \\cup \\operatorname{coef}(b_i))$, with $\\operatorname{coef}(A)$ and $\\operatorname{coef}(b)$\ndenoting the value set of the entries of $A$ and $b$, respectively, its weight magnitude $B(\\Phi) :=\\max_{i=1,...,L} \\{\\|A_i\\|_{\\infty}, \\|b_i\\|_{\\infty}\\}$, and its connectivity $M(\\Phi) := \\sum_{l=1}^L (\\|A_l\\|_0 + \\|b_l\\|_0)$.\nWe define, recursively, the network realization $R(\\Phi) : \\mathbb{R}^{N_0} \\rightarrow \\mathbb{R}^{N_L}$, associated with the\nnetwork configuration $\\Phi$, according to\n$R(\\Phi) = \\begin{cases}\nS(A_L, b_L), & \\text{if } L = 1, \\\\\nS(A_L, b_L) \\circ \\rho \\circ R(((A_i, b_i))_{i=1}^{L-1}), & \\text{if } L \\geq 2,\n\\end{cases}$\nwhere $S(A,b)$ is the affine mapping $S(A,b)(x) = Ax + b, x \\in \\mathbb{R}^{n_2}$, with $A \\in \\mathbb{R}^{n_1 \\times n_2}, b \\in \\mathbb{R}^{n_1}$,\nand $\\rho(x) := \\max\\{x,0\\}$, for $x \\in \\mathbb{R}$, is the ReLU activation function, which, when applied to\nvectors, acts elementwise.\nThe family of network configurations with depth at most $L$, width at most $W$, weight magni-\ntude at most $B$, where $B\\in \\mathbb{R}_+ \\cup \\{\\infty\\}$, connectivity at most $s$, weights taking values in $A \\in \\mathbb{R}$,\n$d$-dimensional input, and 1-dimensional output, for $d \\in \\mathbb{N}, W, L, s \\in \\mathbb{N}\\cup \\{\\infty\\}$, with\u00b9 $W \\geq d$,\nis denoted as\n$\\mathcal{N}_A(d, W, L, B, s) = \\{\\Phi \\in \\mathcal{N}(d) : W(\\Phi) \\leq W, L(\\Phi) \\leq L, B(\\Phi) \\leq B, M(\\Phi) \\leq s, \\operatorname{coef}(\\Phi) \\subseteq A\\}$,\nwith the family of associated network realizations\n$\\mathcal{R}_A(d, W, L, B, s) := \\{R(\\Phi) : \\Phi\\in \\mathcal{N}_A(d, W, L, B,s)\\}$.\nTo simplify notation, for $A = \\mathbb{R}$, we allow omission of the argument $A$ in $\\mathcal{N}_A(d, W, L, B, s)$\nand $\\mathcal{R}_A(d, W, L, B, s)$. When $s = \\infty$, we allow omission of the argument $s$ in $\\mathcal{N}_A(d, W, L, B, s)$\nand $\\mathcal{R}_A(d, W, L, B, s)$. Furthermore, we allow omission of both arguments $B, s$ in $\\mathcal{N}_A(d, W, L, B, s)$\nand $\\mathcal{R}_A(d, W, L, B, s)$ when $B = s = \\infty$. One specific, frequently used, incarnation of these poli-\ncies is $\\mathcal{N}(d, W, L) = \\mathcal{N}_{\\mathbb{R}}(d, W, L, \\infty, \\infty)$ and $\\mathcal{R}(d, W, L) = \\mathcal{R}_{\\mathbb{R}}(d, W, L, \\infty, \\infty)$.\nWe emphasize the importance of differentiating between network configurations and network\nrealizations. Different network configurations may result in the same realization. Nevertheless,\nwhenever there is no potential for confusion, we shall use the term network to collectively refer\nto both configurations and realizations.\nThroughout the paper, we shall frequently use the covering number and the packing number,\ndefined as follows.\nDefinition 1.2 (Covering number and packing number). [6, Definitions 5.1 and 5.4] Let $(\\mathcal{V}, \\delta)$\nbe a metric space. An $\\epsilon$-covering of $X \\subseteq \\mathcal{Y}$ is a subset $\\{x_1,...,x_n\\}$ of $X$ such that for all\n$x \\in X$, there exists an $i \\in \\{1,...,n\\}$ so that $\\delta(x,x_i) \\leq \\epsilon$. The $\\epsilon$-covering number $N(\\epsilon, X, \\delta)$ is\nthe cardinality of a smallest $\\epsilon$-covering of $X$. An $\\epsilon$-packing of $X$ is a subset $\\{x_1,...,X_n\\}$ of $X$\nsuch that $\\delta(x_i, x_j) > \\epsilon$, for all $i, j \\in \\{1, ..., n\\}$ with $i \\neq j$. The $\\epsilon$-packing number $M(\\epsilon, X, \\delta)$ is\nthe cardinality of a largest $\\epsilon$-packing of $X$.\n1The condition $W > d$ is formally stated here so as to prevent the trivial case of $\\mathcal{N}_A(d, W, L, B, s)$ being an\nempty set. It will be a standing assumption throughout the paper."}, {"title": "2 Fully-connected ReLU Networks with Uniformly Bounded Weights", "content": "Our covering number bounds for fully-connected ReLU networks with uniformly bounded\nweights are as follows.\nTheorem 2.1. Let $p \\in [1,\\infty], d, W, L \\in \\mathbb{N}, B,\\epsilon\\in \\mathbb{R}_+$ with $B > 1$ and $\\epsilon \\in (0,1/2)$. We have\n$\\log\\left(N(\\epsilon, \\mathcal{R}(d,W, L, B), L^p ([0, 1]^d))\\right) \\leq CW^2L\\log\\left( \\frac{(W+1)^L B^L}{\\epsilon}\\right);$\nwhere $C \\in \\mathbb{R}_+$ is an absolute constant. Further, if, in addition, $W, L \\geq 60$, then\n$\\log\\left(N(\\epsilon, \\mathcal{R}(d,W, L, B), L^p ([0, 1]^d))\\right) \\geq cW^2L\\log\\left( \\frac{(W + 1)^L BL}{\\epsilon}\\right)$\nwhere $c \\in \\mathbb{R}_+$ is an absolute constant.\nProof. The proofs of the upper bound and the lower bound are provided in Sections 2.1 and\n2.2, respectively.\nWe remark that, for $W, L > 60$ and $\\epsilon\\in (0,1/2)$, the upper bound (3) and the lower bound\n(4) differ only by the multiplicative absolute constants $C, c$ to be specified in the proof. These\nconstants as well as the condition $W, L > 60$ are chosen for expositional convenience of the\nproof; improvements are possible, but will not be pursued here.\nThe covering number upper bounds available in the literature apply to specific settings. For\nexample, [13, Lemma 5] addresses the case $B = 1$ and [14, Lemma 5.3] pertains to $p = \\infty$,\nwhile we consider the general case $B\\in [1,\\infty), p \\in [1, \\infty]$. We note that [13, Lemma 5] and [14,\nLemma 5.3] also apply to sparse ReLU networks with uniformly bounded weights as considered\nin Section 5, again for $B\\in [1, \\infty), p \\in [1, \\infty]$."}, {"title": "2.1 Proof of the Upper Bound in Theorem 2.1", "content": "The proof is effected by constructing an explicit $\\epsilon$-covering of $\\mathcal{R}(d, W, L, B)$ with elements in\n$\\mathcal{R}_{[-B,B]\\cap 2^{-b}\\mathbb{Z}}(d, W, L)$, where $b \\in \\mathbb{N}$ is a parameter suitably depending on $\\epsilon$. We start with three\ntechnical lemmata, and then provide the proof of the upper bound at the end of the section.\nThe first lemma quantifies the distance between the realizations of two networks sharing the\nsame architecture.\nLemma 2.2. [18, Lemma E.1] Let $d, W, L, l \\in \\mathbb{N}$ with $l < L$, $B \\in \\mathbb{R}_+$ with $B > 1$, and let\n$\\Phi^i = ((A_j^i, b_j^i))_{j=1}^l \\in \\mathcal{N}(d, W, L, B), i = 1, 2,$\nhave the same architecture. Then,\n$$\\|R(\\Phi^1) - R(\\Phi^2)\\|_\\infty([0,1]^d) \\leq L(W+1)^L B^{L-1}\\|\\Phi^1 - \\Phi^2\\|,$$\nwhere\n$$\\|\\Phi^1 - \\Phi^2\\| := \\max_{j=1,...,l} \\max\\{\\|A_j^1 - A_j^2\\|_\\infty, \\|b_j^1 - b_j^2\\|_\\infty\\}.$$\nBased on Lemma 2.2, we now construct the announced $\\epsilon$-covering of $\\mathcal{R}(d, W, L, B)$.\nLemma 2.3. Let $p\\in [1,\\infty], d,W, L,b \\in \\mathbb{N}$, and $B \\in \\mathbb{R}_+$ with $B > 1$. Then, the set\n$\\mathcal{R}_{[-B,B]\\cap 2^{-b}\\mathbb{Z}}(d, W, L)$ is an $(L(W + 1)^L B^{L-1}2^{-b})$-covering of $\\mathcal{R}(d, W, L, B)$ with respect to the\n$L^p([0, 1]^d)$-norm.\nProof. Define the quantization mapping $q_b : [-B, B] \\rightarrow [-B, B] \\cap 2^{-b}\\mathbb{Z}$ as\n$q_b(x) = \\begin{cases}\n2^{-b} \\lfloor 2^b x \\rfloor, & \\text{for } x \\in [0, B], \\\\\n2^{-b} \\lceil 2^b x \\rceil, & \\text{for } x \\in [-B,0),\n\\end{cases}$\nand note that $|x - q_b(x)| \\leq 2^{-b}$, for all $x \\in [-B, B]$. When applied to matrices or vectors,\n$q_b(.)$ acts elementwise. Now, arbitrarily fix $R(\\Phi) \\in \\mathcal{R}(d, W, L, B)$ with $\\Phi = ((A_l, b_l))_{l=1}^{\\tilde{L}} \\in$\n$\\mathcal{N}(d, W, L, B)$ and $\\tilde{L} < L$, and quantize the weights of $\\Phi$ according to\n$Q_b(\\Phi) = ((q_b(A_l), q_b(b_l)))_{l=1}^{\\tilde{L}} \\in \\mathcal{N}_{[-B,B]\\cap 2^{-b}\\mathbb{Z}}(d, W, L)$.\nWe then have\n$$\\|\\Phi - Q_b(\\Phi)\\| = \\max_{l=1,..., \\tilde{L}} \\max\\{\\|A_l - q_b(A_l) \\|_\\infty, \\|b_l - q_b (b_l) \\|_\\infty\\} \\leq 2^{-b},$$\nwhich, together with Lemma 2.2, yields\n$$\\|R(\\Phi) - R(Q_b(\\Phi))\\|_\\infty([0,1]^d) \\leq L(W + 1)^L B^{L-1}2^{-b}.$$\nAs\n$$\\|R(\\Phi) - R(Q_b(\\Phi)) \\|_p ([0,1]^d) \\leq \\sup_{x\\in[0,1]^d} |R(\\Phi)(x) - R(Q_b(\\Phi))(x)| = \\|R(\\Phi) - R(Q_b(\\Phi)) \\|_\\infty ([0,1]^d),$$\n(7) implies that\n$$\\|R(\\Phi) - R(Q_b(\\Phi))\\|_p([0,1]^d) \\leq L(W + 1)^L B^{L-1}2^{-b}.$$\nWe can therefore conclude that $\\mathcal{R}_{[-B,B]\\cap 2^{-b}\\mathbb{Z}}(d, W, L)$ is an $(L(W + 1)^L B^{L-1}2^{-b})$-covering of\n$\\mathcal{R}(d, W, L, B)$ in the $L^p([0, 1]^d)$-norm.\nIt remains to upper-bound the cardinality of the covering $\\mathcal{R}_{[-B,B]\\cap 2^{-b}\\mathbb{Z}}(d, W, L)$ identified in\nLemma 2.3. To this end, we first state an auxiliary result from [18].\nLemma 2.4. [18, Proposition 2.4] For $d, W, L \\in \\mathbb{N}$ and a finite set $A \\subseteq \\mathbb{R}$ with $|A| \\geq 2$, it\nholds that\n$$\\log(|\\mathcal{R}_A (d, W, L)|) \\leq \\log(|\\mathcal{N}_A(d, W, L)|) \\leq 5W^2L \\log(|A|).$$\nWe next make the choice of $b$ explicit. Specifically, we set\n$$b := \\left\\lceil\\log \\left(\\frac{L(W+1)^L B^{L-1}}{\\epsilon}\\right)\\right\\rceil$$\nNoting that $L(W+1)^L B^{L-1}2^{-b} < \\epsilon$, it follows from Lemma 2.3 that $\\mathcal{R}_{[-B,B]\\cap 2^{-b}\\mathbb{Z}}(d, W, L)$ is an\n$\\epsilon$-covering of $\\mathcal{R}(d, W, L, B)$ with respect to the $L^p([0, 1]^d)$-norm. By minimality of the covering\nnumber, we have\n$$N(\\epsilon, \\mathcal{R}(d, W, L, B), L^p([0, 1]^d)) \\leq |\\mathcal{R}_{[-B,B]\\cap 2^{-b}\\mathbb{Z}}(d, W, L)|.$$\nApplication of Lemma 2.4 yields an upper bound on the cardinality of the covering according\nto\n$\\log(|\\mathcal{R}_{[-B,B]\\cap 2^{-b}\\mathbb{Z}}(d, W, L)|) \\leq 5W^2L \\log(|[-B, B] \\cap 2^{-b}\\mathbb{Z}|).$\nThe term $\\log(|[-B, B] \\cap 2^{-b}\\mathbb{Z}|)$ can now be bounded as follows\n$$\\log(|[-B, B] \\cap 2^{-b}\\mathbb{Z}|) = \\log(|[-2^bB, 2^bB] \\cap \\mathbb{Z}|)$$\n$$\\leq \\log(\\lfloor 2^{b+1}B + 1 \\rfloor)$$\n$$\\leq \\log(4 \\cdot 2^{b}B)$$\n$$=2+ b + \\log(B)$$\n$$=2+ \\left\\lceil\\log \\left(\\frac{L(W+1)^L B^{L-1}}{\\epsilon}\\right)\\right\\rceil + \\log(B)$$\n$$\\leq 3 + \\log\\left(\\frac{L(W+1)^L B^{L-1}}{\\epsilon}\\right) + \\log(B)$$\n$$\\leq 3 \\log\\left(\\frac{L(W + 1)^L B^L}{\\epsilon}\\right),$$\nwhere (15) is by $1 < 2 \\cdot 2^{b}B$ and in (18) we used $3 < 2\\log(\\frac{L(W + 1)^L B^L}{\\epsilon})$ owing to $\\epsilon \\in (0,1/2)$.\nPutting (11)-(18) together, yields\n$$\\log\\left(N(\\epsilon, \\mathcal{R}(d, W, L, B), L^p([0, 1]^d))\\right) \\leq 15W^2L \\log \\left(\\frac{L(W+1)^L B^L}{\\epsilon}\\right)$$\n$$\\leq 30W^2L\\log\\left(\\frac{(W+1)^L B^L}{\\epsilon}\\right),$$\nwhere (20) follows from $\\frac{L(W+1)^L B^L}{\\epsilon} < (\\frac{(W+1)^4 B^4}{\\epsilon})^L \\leq (\\frac{(W+1)^4 B^4}{\\epsilon})^2$. The proof is concluded\nby taking $C:= 30$."}, {"title": "2.2 Proof of the Lower Bound in Theorem 2.1", "content": "We again start with a series of technical results, which will then be synthesized to the\nproof of the lower bound. The first of these results reduces the problem of lower-bounding the\ncovering number of $\\mathcal{R}(d, W, L, B)$ with respect to the $L^p([0, 1]^d)$-norm to that of lower-bounding\nthe packing number of $\\mathcal{R}(1, W, L, B)$ with respect to the $L^1([0, 1])$-norm.\nLemma 2.5. Let $p \\in [1,\\infty], d, W, L \\in \\mathbb{N}, B,\\epsilon \\in \\mathbb{R}_+$ with $B > 1$ and $\\epsilon \\in (0,1/2)$. We have\n$$N(\\epsilon, \\mathcal{R}(d, W, L, B), L^p([0, 1]^d)) \\geq M(2\\epsilon, \\mathcal{R}(d, W, L, B), L^p([0, 1]^d)))$$\n$$\\geq M(2\\epsilon, \\mathcal{R}(1, W, L, B), L^1([0, 1])).$$\nProof. The inequality (21) follows from Lemma F.1. To establish (22), we show that a maximal\n$(2\\epsilon)$-packing of $\\mathcal{R}(1, W, L, B)$ with respect to the $L^1([0, 1])$-norm induces a $(2\\epsilon)$-packing of\n$\\mathcal{R}(d, W, L, B)$ with respect to the $L^p([0, 1]^d)$-norm and of the same cardinality. The proof of\nthis statement is provided in Appendix B.2.\nWe shall next make use of the fact that ReLU networks can efficiently realize one-dimensional\nbounded continuous piecewise linear functions, defined as follows.\nDefinition 2.6 (One-dimensional bounded continuous piecewise linear functions). [18, Defini-\ntion B.2] Let $M \\in \\mathbb{N}$, with $M > 3$, $E \\in \\mathbb{R}_+ \\cup \\{\\infty\\}$, and let $X = (x_i)_{i=0}^{M-1}$ be a strictly increasing\nsequence taking values in $\\mathbb{R}$. Define the set of functions\n$$\\Sigma(X, E) = \\{f \\in C(\\mathbb{R}) : \\|f\\|_\\infty(\\mathbb{R}) \\leq E, f \\text{ is constant on } (-\\infty, x_0] \\text{ and } [x_{M-1}, \\infty),\n\\quad f \\text{ is affine on } [x_i, x_{i+1}], i = 0, . . ., M - 2\\}.$$\nFor a function $f \\in \\Sigma(X, E)$, we call $X$ the set of its breakpoints, as the slope of $f$ can change\nonly at these points. We refer to the intervals $(-\\infty, x_0], [x_i, x_{i+1}], i = 0, . . ., M - 2, [x_{M-1}, \\infty)$\nas the piecewise linear regions of $f$.\nWe only need to consider breakpoint sets of the form\n$$X_N := (i/N)_{i=0}^N, N\\in\\mathbb{N},$$\nalong with the associated function families $\\Sigma(X_N, E), E \\in \\mathbb{R}_+$, whose $L^1([0, 1])$-covering num-\nber can be lower-bounded as follows.\nLemma 2.7. For $N \\in \\mathbb{N}, \\epsilon, E \\in \\mathbb{R}_+$, we have\n$$\\log\\left(M(\\epsilon, \\Sigma(X_N, E), L^1([0, 1])))\\right) \\geq N \\log\\left(\\left\\lfloor \\frac{E}{4\\epsilon N}\\right\\rfloor\\right).$$\nwith $X_N = (i/N)_{i=0}^N$.\nProof. See Appendix B.3.\nTo realize functions in $\\Sigma(X_N, E)$ efficiently by ReLU networks, we need two technical results\nfrom [18], which we restate for convenience.\nLemma 2.8. [18, Proposition C.1] Let $M \\in \\mathbb{N}$ with $M > 3$, $E \\in \\mathbb{R}_+$, and let $X = (x_i)_{i=0}^{M-1}$ be\na strictly increasing sequence taking values in $[0,1]$. Then, for all $u, v \\in \\mathbb{N}$ such that $u^2v > M$,\nwe have\n$$\\Sigma(X, E) \\subseteq \\mathcal{R}(1,20u, 30v, \\max\\{1, C_k M^9 (R_m(X))^4 E\\}),$$\nfor an absolute constant $C_k \\in \\mathbb{R}$ satisfying $2 < C_k \\leq 10^5$, and where $R_m(X) := \\max_{i=1,...,M}(x_i -\nx_{i-1})^{-1}$.\nThe second result is as follows.\nLemma 2.9. [18, Proposition H.4] Let $W_1, L_1 \\in \\mathbb{N}$, with $W_1 \\geq 2$, $L_2 \\in \\mathbb{N} \\cup \\{0\\}$, and $B_1, B_2 \\in\n$\\mathbb{R}_+$, with $B_1, B_2 \\geq 1$. It holds that\n$$\\frac{(B_2)^{L_1+L_2}}{\\left(B_1^{\\frac{1}{L_1}}\\right)^{\\lfloor W_1/2\\rfloor^{L_2}}} \\cdot \\mathcal{R}(1, W_1, L_1, B_1) \\subseteq \\mathcal{R}(1, W_1, L_1 + L_2, B_2).$$\nWe are now ready to prove the lower bound in Theorem 2.1 and start by noting that\nthanks to Lemma 2.5, it suffices to lower-bound $M(2\\epsilon, \\mathcal{R}(1, W, L, B), L^1([0, 1]))$. We proceed\nto identify the family of bounded continuous piecewise linear functions corresponding to the\nset $\\mathcal{R}(1, W, L, B)$. To this end, we first introduce notation, namely we set\n$$u := \\left\\lfloor \\frac{W}{20} \\right\\rfloor \\qquad v := \\left\\lfloor \\frac{L}{60} \\right\\rfloor.$$\nAs $W, L \\geq 60$, we have $u \\geq 3$ and $v \\geq 1$. Application of Lemma 2.8 with $M = u^2v, X =$\n$X_{u^2v-1} = (\\frac{i}{u^2v})_{i=0}^{u^2v-1}$, $R_m(X_{u^2v-1}) = u^2v - 1$, and $E = \\frac{1}{C_k (u^2v)^{10}}$, with the absolute constant\n$C_k$ per Lemma 2.8, yields\n$$\\Sigma\\left(X_{u^2v-1}, \\frac{1}{C_k (u^2v)^{10}}\\right) \\subseteq \\mathcal{R}(1,20u, 30v, 1).$$\nNext, application of Lemma 2.9 with $W_1 = 20u \\geq 2$, $L_1 = 30v, B_1 = 1, L_2 = 30v,$\n$B_2 = B > 1$, yields $(B^{60v} (10u)^{30v}) \\cdot \\mathcal{R}(1, 20u, 30v, 1) \\subseteq \\mathcal{R}(1, 20u, 60v, B)$, which together with\n$\\mathcal{R}(1, 20u, 60v, B) = \\mathcal{R}(1,20\\lfloor\\frac{W}{20}\\rfloor, 60\\lfloor\\frac{L}{60}\\rfloor, B) \\subseteq \\mathcal{R}(1, W, L, B)$ establishes that\n$$(B^{60v} (10u)^{30v}) \\cdot \\mathcal{R}(1, 20u, 30v, 1) \\subseteq \\mathcal{R}(1, W, L, B).$$\nMoreover, as $\\alpha \\cdot \\Sigma(X_{u^2v-1},b) = \\Sigma(X_{u^2v-1}, a b)$, for all $a, b \\in \\mathbb{R}_+$, we have\n$$(B^{60v} (10u)^{30v}) \\cdot \\Sigma\\left(X_{u^2v-1}, \\frac{1}{C_k (u^2v)^{10}}\\right) = \\Sigma\\left(X_{u^2v-1}, \\frac{B^{60v} (10u)^{30v}}{C_k (u^2v)^{10}}\\right).$$\nCombining (26), (27), and (25), then yields\n$$\\Sigma\\left(X_{u^2v-1}, \\frac{B^{60v} (10u)^{30v}}{C_k (u^2v)^{10}}\\right) \\subseteq \\mathcal{R}(1, W, L, B).$$\nWe have\n$$\\log(M(2\\epsilon, \\mathcal{R}(1, W, L, B), L^1([0, 1])))$$\n$$\\geq \\log\\left(M\\left(2\\epsilon, \\Sigma\\left(X_{u^2v-1}, \\frac{B^{60v} (10u)^{30v}}{C_k (u^2v)^{10}}\\right), L^1([0, 1])\\right)\\right)$$\n$$\\geq (u^2v - 1) \\log\\left(\\left\\lfloor \\frac{B^{60v} (10u)^{30v}}{4 \\epsilon C_k (u^2v)^{10} (u^2v - 1)}\\right\\rfloor\\right)$$\n$$\\geq (u^2v - 1) \\log\\left(\\left\\lfloor \\frac{B^{60v} (10u)^{30v}}{8C_k \\epsilon}\\right\\rfloor\\right)$$\n$$\\geq (u^2v - 1) \\log\\left(\\frac{100 B u v}{\\epsilon}\\right)$$\nwhere in (30) we used the inclusion relation (28) together with Lemma F.1, (31) is by Lemma 2.7,\n(32) follows from $(10u)^{30v} \\geq 10^6 \\cdot u^{22} \\cdot (20)^11 \\cdot (10u)^{2v} > 10^6 \\cdot u^{22} \\cdot v^{11} \\cdot (10u)^{2v} \\geq 10^6(u^2v)^{10}(u^2v -\n1)(100u^2)$, and in (33) we employed $u \\geq 3, v \\geq 1, B > 1$, and $8C_k < 8\\cdot 10^5 < 10^6$. We next\nlower-bound $(u^2v - 1) \\log\\left(\\frac{100 B u v}{\\epsilon}\\right)$ in terms of $W, L, B$ according to\n$$(u^2v - 1) \\log\\left(\\frac{100 B u v}{\\epsilon}\\right) \\geq \\frac{1}{2} u^2 v \\log\\left(\\frac{100 B u v}{\\epsilon}\\right)$$\n$$\\geq \\frac{1}{2} \\frac{W^2}{20^2} \\frac{L}{60} \\log\\left(\\frac{100 B \\frac{W}{20} \\frac{L}{60}}{\\epsilon}\\right)$$\n$$= \\frac{1}{2 \\cdot 40^2 \\cdot 120} W^2L \\log\\left(\\frac{(BWL)}{\\epsilon}\\right)$$\n$$\\geq \\frac{1}{4 \\cdot 40^2 \\cdot 120} W^2L \\log \\left(\\frac{(W + 1)^L B^L}{\\epsilon}\\right),$$\nwhere (34) follows from $u^2v - 1 > \\frac{1}{2} u^2v$ as $u \\geq 3$ and $v \\geq 1$, in (35) we used\n$u = \\lfloor\\frac{W}{20}\\rfloor \\geq \\frac{W}{40}$ and $v = \\lfloor\\frac{L}{60}\\rfloor \\geq \\frac{L}{120}$, and (38) is by $2 \\log(\\frac{W + 1}{\\epsilon}) = \\log(\\frac{(W + 1)^L B^L}{\\epsilon})^2 > \\log(\\frac{(W + 1)^L B^L}{\\epsilon})$\nas $W \\geq 60$ by assumption. The proof is concluded by setting $c = \\frac{1}{4 \\cdot 40^2 \\cdot 120}$."}, {"title": "3 Neural Network Transformation and Function Approximation", "content": "We now show how the precise characterization of ReLU network covering numbers obtained\nin the previous section can be put to work to characterize the fundamental limits of neural\nnetwork transformation and function approximation. Before describing the specifics of these\ntwo problems, we need a general result which relates the covering numbers of sets $G$ and $F$\nthat are close in terms of"}]}