{"title": "Learning Co-Speech Gesture Representations in Dialogue through Contrastive Learning: An Intrinsic Evaluation", "authors": ["Esam Ghaleb", "Bulat Khaertdinov", "Wim Pouw", "Marlou Rasenberg", "Judith Holler & Asl\u0131 \u00d6zy\u00fcrek", "Raquel Fern\u00e1ndez"], "abstract": "In face-to-face dialogues, the form-meaning relationship of co-speech gestures varies depending on contextual factors such as what the gestures refer to and the individual characteristics of speakers. These factors make co-speech gesture representation learning challenging. How can we learn meaningful gestures representations considering gestures' variability and relationship with speech?\nThis paper tackles this challenge by employing self-supervised contrastive learning techniques to learn gesture representations from skeletal and speech information. We propose an approach that includes both unimodal and multimodal pre-training to ground gesture representations in co-occurring speech. For training, we utilize a face-to-face dialogue dataset rich with representational iconic gestures. We conduct thorough intrinsic evaluations of the learned representations through comparison with human-annotated pairwise gesture similarity. Moreover, we perform a diagnostic probing analysis to assess the possibility of recovering interpretable gesture features from the learned representations. Our results show a significant positive correlation with human-annotated gesture similarity and reveal that the similarity between the learned representations is consistent with well-motivated patterns related to the dynamics of dialogue interaction. Moreover, our findings demonstrate that several features concerning the form of gestures can be recovered from the latent representations. Overall, this study shows that multimodal contrastive learning is a promising approach for learning gesture representations, which opens the door to using such representations in larger-scale gesture analysis studies.", "sections": [{"title": "1 INTRODUCTION", "content": "Co-speech hand gestures are intentionally used along with speech to convey meaning [43]. For instance, representational iconic gestures depict objects, events or actions through various representational techniques such as enacting, tracing, and hand-shaping [31]. Gesture analysis is an active research area in fields such as Human-Computer Interaction (HCI) [39], Sign Language Recognition (SLR) [29, 32], and human behavior analysis [17, 34], where sensory data collected through wearable sensors [22] or, more commonly, through passive sensors like RGB or depth cameras are widely used for studying gestures [50, 59, 60].\nIn face-to-face interaction, the form-meaning relationship of co-speech gestures is influenced by various situational and contextual factors, including what a gesture refers to and the characteristics of individual speakers. Although multiple current studies aim to model and represent gestures, there are prominent areas with room for improvement, particularly concerning gesture representation learning in conversations [18, 19, 41, 61, 62, 64]. First, most studies train deep learning architectures from scratch on specific downstream tasks, including gesture segmentation [18, 19, 61] or generation [41, 62, 64]. Thus, the employed objectives are focused on the task-specific discriminative and generative power of the models rather than on their ability to effectively encode general meaningful properties of gestures and relationships between them. The research literature has already pointed out the lack of models to represent gestures [41, 62, 64]. For example, some authors use autoencoders to extract and compare latent representations to evaluate synthetic gestures [41, 62, 64]. Second, much of the research has also focused on emblems (which are conventionalized and can stand alone without speech) [6, 33, 65] or pure beat gestures (rhythmic gestures that lack semantic content), particularly within monologue-based datasets as surveyed in [45]. Yet, in real-world interactions, gestures are semantically, pragmatically, and temporally related to speech, especially in face-to-face conversations. For instance, along with"}, {"title": "2 RELATED WORK", "content": "2.1 Gesture Representation and Modelling\n2.1.1 Gesture Representation and Similarity. Prior to the rise of deep learning, researchers utilized handcrafted features, such as velocity, rhythm, acceleration, and anatomical models of hands, to encode kinematic, physical, and shape characteristics of gestures [53]. Nowadays, the focus has shifted to training neural networks end-to-end for certain downstream tasks, such as gesture recognition [42], detection [18], and generation [64]. The studies exploring gesture generation frequently involve quantitative evaluations for the quality and fidelity of synthetic gestures using custom metrics, such as Fr\u00e9chet Gesture Distance (FGD) and Beat Alignment Score (BeatAlign) [62, 64]. For example, FGD evaluates generated gestures based on learned visual features through autoencoders [40, 64].\nThe similarity of gesture or sign pairs has been studied in a range of contexts, for example, in silent gestures, co-speech gestures, co-singing gestures, as well as sign language generation and production [30]. For instance, Kanakanti et al. [30] used Dynamic Time Warping (DTW) to evaluate speech-to-sign language generation models. These metrics assess the alignment between the predicted sign language sequences and the ground truth of Indian Sign Language sequences. One drawback of spatio-temporal alignment metrics such as DTW is that they operate at the level of key points and might not be optimal for measuring the more context-dependent similarity of two co-speech gestures. Indeed, a very weak but reliable correlation is generally found between the dissimilarity of gesture kinematics relative to the dissimilarity of what the gestures are about [49]. In our work, we assess the properties of the learned representations by performing pairwise gestural analysis. Hence, FGD and BeatAlign are not suitable metrics for this study. Rather, we calculate the cosine similarity of the learned representations, as often done in Natural Language Processing and speech research, where it is common to use embeddings for representational and similarity analysis [12, 44, 46, 48].\n2.1.2 Joint Speech and Gesture Models. Joint modeling of speech and gestures through speech and vision models has been studied through different gesture analysis tasks. For instance, Ghaleb et al. [18] used speech to enhance gesture detection methods. A study by Kucherenko et al. [35] found that the meaning and timing of gestures can be predicted using prosodically relevant acoustic features. In gesture generation, according to a survey by Nyatsanga et al. [45], adding information about speech prosody and meaning can improve the quality of generated beat and representational gestures, respectively, making them appear more natural based on evaluations by raters. A typical pipeline representing gesture generation models is the work by Bhattacharya et al. [10], who developed a dual-speech model. The first model uses prosodic features, while the second one leverages word embeddings to capture speech's prosodic and semantic characteristics separately. Another example closely related to ours is the study by Lee et al. [36], who proposed an approach that exploits spoken language and gestures for gesture generation through cross-modal contrastive learning. The authors cluster speech and gesture embeddings cross-modally. In contrast, we include cross-modal objectives to complement unimodal gesture representations, as explained in Section 4."}, {"title": "3 DATASET AND PRE-PROCESSING", "content": "3.1 Dataset\nWe use a dataset collected by Rasenberg et al. [52] that consists of 19 naturalistic face-to-face dialogues. In these dialogues, Dutch-speaking participants play a referential game where they need to identify different objects or referents that do not have a conventional label. The speakers are free to communicate as they please while playing the game; they were not given any instructions on the use of gestures. However, due to the nature of the referential task, the dataset is rich with representational iconic gestures, i.e., gestures used to depict objects. In this dataset, gesture strokes were manually identified and segmented. This annotation resulted in 4949 gesture segments with an average duration of 610 milliseconds.\n3.2 Manual Similarity Coding of Gesture Pairs\nA subset of the dataset was annotated by Rasenberg et al. [52] to study the extent to which speakers mimic each other through gestures. The annotation codes, in a binary manner, whether two gestures are similar or not with respect to five form features: shape, movement, rotation, position, and handedness. The distribution of the number of form features for which the gesture pairs are similar can be found in Figure 2. The majority of referentially related gesture pairs by two speakers are similar with respect to at least one feature, and around 38% are similar with respect to at least 3 features out of 5. Handedness similarity occurs the most frequently, followed by orientation, shape, movement, and position.\n3.3 Data Samples\nUsing the manually segmented 4949 gesture strokes as an anchor, we sample one-second windows around these strokes. We apply"}, {"title": "4 MODELS AND CONTRASTIVE OBJECTIVES", "content": "In this study, we use contrastive self-supervised learning to refine the embeddings extracted by two pre-trained backbone (\"tower\") models: a speech model (Section 4.1.1) and a skeleton model (Section 4.1.2). Given that contrastive objectives aim to align representations, our goal here is to bring representations of speech and co-speech gestures closer in a latent space. More formally, given a dataset {$\\bm{x}, \\bm{X}_{G}^{i}$}$_{i=1}^{N}$ of size N containing matching speech and gestural signals, their encoded representations can be written as {$\\bm{f}_{s}(\\bm{x}), \\bm{f}_{G}(\\bm{X}_{G}^{i})$}$_{i=1}^{N}$, where $\\bm{f}_{s}(.)$ and $\\bm{f}_{G}(.)$ are feature encoders for the speech and gesture modality, respectively. We also use the projection heads $\\bm{g}_{s}(.)$ and $\\bm{g}_{G}(.)$ (Section 4.1.3), that map features from two modalities $\\{\\bm{g}_{s}(\\bm{f}_{s}(\\bm{x})), \\bm{g}_{G}(\\bm{f}_{G}(\\bm{X}_{G}^{i}))\\}$, to the same size in a joint latent space [14, 51]. The following subsections describe the backbone models, projection heads, and contrastive learning objectives. Section B of the Supplementary Materials details the implementation and data augmentation methods. Additionally, all corresponding data and code needed to reproduce the results are publicly available on GitHub.\n4.1 Backbone Models and Projection Heads\n4.1.1 Speech Model. For the speech stream, we use the pre-trained WAV2VEC-2 model, which is trained with a masked language-modeling objective on large amounts of speech data [2]. Sequences of these representations are then used as separate tokens in the 24 self-attention blocks. WAV2VEC-2 has shown state-of-the-art performance across different speech-based tasks, such as automatic speech recognition [27] and speech emotion recognition [13]. We expect that the information related to speech prosody and content captured by this model will be beneficial for co-speech gesture representations.\nIn our implementations, we build a downstream network on top of the output sequences corresponding to two-second speech windows, obtained from all 24 self-attention layers. To map information from these features into a lower-dimensional vector, we apply a weighted average with learnable weights for embeddings from each network layer. Furthermore, two pointwise CNN layers are implemented to fuse signals along the temporal dimension [47]. As a result, the output of the employed architecture is a one-dimensional vector containing 128 dimensions.\n4.1.2 Skeletal Model. To encode the spatio-temporal graphs of body joints for each one-second gesture window, we use Spatio-Temporal Graph Convolutional Networks (ST-GCNs) [63], which generalize traditional convolution operations to graph neural networks. We use a pre-trained model for Sign Language Recognition (SLR) by Jiang et al. [29]. The model was trained for Turkish Sign Language recognition using only 27 hand and upper body joints' coordinates. This model takes a spatio-temporal graph of a skeleton window as input and produces an embedding vector with 256 dimensions.\n4.1.3 Projection Heads. The projection head for each modality consists of a three-layer MLP."}, {"title": "4.2 Unimodal Contrastive Objective", "content": "We use the SimCLR [14] for training the encoder from the SLR baseline. This framework crafts the positive pairs for the contrastive objective by applying skeletal augmentations that generate multiple views of the same instance, training the model to build representations of the underlying gesture, regardless of the augmentations.\nWe use a combination of mirroring, scaling, random moving, jittering, and shearing, each with a 50% probability. Thus, a gesture instance $\\bm{X}_{G}$ is transformed into two views with two random skeleton augmentations $t_{i}(\\bm{X})$ and $t_{j}(\\bm{X})$. For this positive pair, the contrastive loss function treating $z^{j}$ as an anchor can be written as follows:\n$l_{uni}(i, j) = -log\\frac{exp(\\frac{s(z_{i},z_{j})}{\\tau})}{\\sum_{k=1}^{2N_{b}}\\mathbb{1}_{[k \\neq i]} exp(\\frac{s(z_{i},z_{k})}{\\tau})}$               (1)\nwhere $\\tau$ is a temperature hyperparameter and s(.) is a cosine similarity, and $N_{b}$ is a batch size. The unimodal contrastive objective for the whole batch, then, is formulated as follows:\n$L_{uni} = \\frac{1}{2N_{b}}\\sum_{(i,j)\\in N_{b}}(l_{uni}(i, j) + l_{uni}(j, i)).$   (2)"}, {"title": "4.3 Multimodal Contrastive Objective", "content": "For multimodal training, we use Contrastive Multiview Coding [58], also frequently referred to as the CLIP-like objective [51]. This framework allows us to align and ground representations of co-speech gestures in corresponding speech without using data annotations.\nConcretely, contrastive learning aims to maximize the similarity between representations of matching gesture-speech pairs {$\\bm{z}_{G}^{i} = \\bm{g}_{s}(\\bm{f}_{s}(\\bm{x}^{i})), \\bm{z}_{G}^{i} = \\bm{g}_{G}(\\bm{f}_{G}(\\bm{X}_{G}^{i}))\\}$ by contrasting them against other instances in mini-batches. In this case, the loss function for the l-th pair of instances contains two terms, $l_{mm}(G, S)$ and $l_{mm}(S, G)$. The former considers gestural features as anchors and mines positive and negative speech representations to form pairs, whereas the latter takes speech representations as anchors.\nMore formally, the multimodal contrastive objective $l_{mm}(G, S)$ can be written as follows:\n$l_{mm}(G, S) = -log\\frac{exp(\\frac{s(z_{G}^{i},z_{S}^{i})}{\\tau})}{\\sum_{k=1}^{N_{b}} exp(\\frac{s(z_{G}^{i},z_{S}^{k})}{\\tau})}$                  (3)\nSimilarly to Equation 2, the loss for the whole batch with multi-modal representations is formulated as follows:\n$L_{mm} = \\frac{1}{2N_{b}}\\sum_{l=1}^{N_{b}}(l_{mm}(G, S) + l_{mm}(S, G)).$                         (4)"}, {"title": "4.4 Combined Objective", "content": "Finally, we propose an architecture that employs unimodal and multimodal objectives jointly to map related augmented skeleton representations closer while also grounding them in the co-occuring spoken language. We achieve this combined objective as follows:\n$L_{co} = \\frac{1}{2}(L_{uni} + L_{mm}).$"}, {"title": "5 GESTURE SIMILARITY ANALYSIS", "content": "In this section, we first evaluate the representations obtained with our four models: the SLR baseline model and the three contrastive models with unimodal, multimodal, and combined objectives. For the three proposed contrastive models, the gestural representations are extracted from the last layer of the skeletal projection head. For the SLR baseline model, the representations are obtained from the last FC layer. We then use the best model's gestural representations to study gesture similarity within and across speakers in face-to-face referential dialogue.\n5.1 Evaluation of Model Representations\nWe assess the effectiveness of the models' representations by examining the extent to which pairwise gesture similarity correlates with the number of form features shared by gesture pairs. For this evaluation, we use the 419 gesture pairs manually coded for similarity in the dataset by Rasenberg et al. [52]. For each model, we compute the Spearman correlation coefficient between the pairwise cosine similarity of model representations and the number of form features these pairs share."}, {"title": "5.2 Gesture Similarity in Referential Dialogues", "content": "In this section, we investigate to what extent the learned gesture representations comply with well-motivated expectations regarding gesture similarity within and across speakers. We report the results obtained with the best model's representations according to the evaluation carried out above. For these analyses, we include the model representations for all gestures in the dataset by Rasenberg et al. [52].\n5.2.1 Referent vs. Speaker Driven Similarity. In the first analysis, we focus on gestures that take place within each two-participant dialogue and investigate the interplay between two variables regarding gesture pairs: the speaker and the referent\nH1 a. Representations of gestures by the same speaker will be more similar if the gestures have the same referent than if they refer to different objects.\nb. Representations of gestures made by different speakers will be more similar if the gestures have the same referent than if they refer to different objects.\nH2 Representations of gestures with the same referent will be more similar if the gestures are produced by the same speaker than if they are made by different speakers.\nFor example, to refer to the object part highlighted in Figure 1, the participants produce a gesture resembling a book while uttering \"you mean with that open book, so to speak\", \"yeah a bit like you've opened a book\". Hence, we hypothesize that our model representations should be well-equipped to capture the visual and speech similarities that tend to characterize same-referent gestures, both when they are produced by the same speaker as well as when they are produced by different speakers\nHypothesis H2 is motivated by the assumption that there are individual idiosyncrasies in how speakers gesture and speak.\nTo test whether the learned representations comply with these hypotheses, we extract four sets of gesture representations: same-referent-same-speaker (7K pairs), same-referent-different-speaker (5.5K pairs), different-referent-same-speaker (455K pairs), and different-referent-different-speaker (358K pairs).\nResults. Figure 5 shows the distribution of cosine similarity scores for each of these sets of gesture pairs."}, {"title": "5.2.2 Referent vs. Interaction Driven Similarity", "content": "In the previous analysis, we observed that referentially related gestures by different speakers are significantly more similar than cross-speaker gestures that do not refer to the same object (\u00b5 = 0.10 vs. \u03bc = 0.07 in Figure 5). We therefore formulate the following hypothesis:\nH3 Representations of gestures by different speakers will be more similar when the two speakers are interlocutors within a dialogue than when the speakers are from different dialogues.\nTo test whether the learned model representations confirm this hypothesis, we consider the two sets of gesture pairs by different speakers from the same dialogue we had extracted for the previous analysis (same-referent-different-speaker and different-referent-different-speaker) and extract two additional sets of pairs from different dialogues: same-referent-different-speaker-diff-dialogue (137K pairs) and different-referent-different-speaker-diff-dialogue (10M pairs).\nResults. Figure 6 shows the mean similarities for the sets of pairs mentioned above. Taken together, these results lend evidence to H3: the model representations reflect that gesture similarity is partially driven by alignment processes due to dialogic interaction."}, {"title": "6 PROBING ANALYSIS", "content": "The analysis presented in Section 5.1 showed that the similarity between model gesture representations significantly correlates with manually coded gesture similarity. In this section, we shed light on this issue via diagnostic probing [4], a technique widely applied to decode linguistic properties present in textual representations [5, 15, 54] that has also been used to interpret features of other modalities, including visual signals [3] and spoken utterances [16].\nDiagnostic probing is frequently formulated as a classification task. Here, we use the 419 gesture pairs manually annotated with 5 binary features and build 5 classification models, one per feature. For each pair of representations, the probing classifiers predict whether the representations are similar in handedness, shape, rotation, movement, or position.\n6.1 Probing Models\nWe probe the representations learned by the model with the combined contrastive objective, in particular, the representation extracted from the last layer of the ST-GCN encoder. For comparison,"}, {"title": "6.2 Results", "content": "The results of the probing analysis are summarized in Figure 7. If the performance of the probe with the actual gesture representations is significantly higher than its performance with random features, we can conclude that the model representations encode, to some extent, the property being probed for.\nIn sum, our analysis indicates that the models, in particular our combined objective model, learn gesture representations that encode different properties identified by experts through manual annotation. The results also suggest that information about movement appears to be the most challenging for the models to learn."}, {"title": "7 CONCLUSION", "content": "We proposed an approach for learning co-speech gesture representations using contrastive self-supervised learning. This approach employs loss functions to bring two views of gestural movements closer in a latent space while also grounding them in spoken language. To train the proposed models, we used segmented gestures obtained from a dataset of face-to-face referential dialogues without any additional labeling. While most work on gesture representation learning has focused on task-specific evaluation, in this paper, we performed a thorough intrinsic evaluation of the learned representations by analyzing the extent to which they exhibit desirable properties that align with expert intuitions. Our results showed that a model that combines both unimodal and multimodal contrastive objectives has advantages over a Sign Language Recognition baseline model and other contrastive variants. We believe that this makes them a valuable tool for gesture analysis studies."}]}