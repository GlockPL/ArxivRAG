{"title": "Retrieval-augmented generation in multilingual settings", "authors": ["Nadezhda Chirkova", "David Rau", "Herv\u00e9 D\u00e9jean", "Thibault Formal", "St\u00e9phane Clinchant", "Vassilina Nikoulina"], "abstract": "Retrieval-augmented generation (RAG) has recently emerged as a promising solution for incorporating\nup-to-date or domain-specific knowledge into large language models (LLMs) and improving LLM\nfactuality, but is predominantly studied in English-only settings. In this work, we consider RAG in the\nmultilingual setting (mRAG), i.e. with user queries and the datastore in 13 languages, and investigate\nwhich components and with which adjustments are needed to build a well-performing mRAG pipeline,\nthat can be used as a strong baseline in future works. Our findings highlight that despite the availability\nof high-quality off-the-shelf multilingual retrievers and generators, task-specific prompt engineering is\nneeded to enable generation in user languages. Moreover, current evaluation metrics need adjustments\nfor multilingual setting, to account for variations in spelling named entities. The main limitations to be\naddressed in future works include frequent code-switching in non-Latin alphabet languages, occasional\nfluency errors, wrong reading of the provided documents, or irrelevant retrieval. We release the code\nfor the resulting mRAG baseline pipeline at https://github.com/naver/bergen\u00b9.", "sections": [{"title": "1. Introduction", "content": "Retrieval-augmented generation (RAG) (Lewis et al.,\n2020; Ram et al., 2023, inter alia) has recently emerged\nas a promising solution for incorporating up-to-date or\ndomain-specific knowledge into large language mod-\nels (LLMs) and improving LLM factuality, especially in\nknowledge-intensive tasks such as open-domain ques-\ntion answering or fact-checking. RAG augments user\nqueries with relevant context retrieved from the Inter-\nnet or a given collection and then passes the result to\nan LLM to generate a knowledge-grounded response.\nRecent works focus on improving various components\nof the complex RAG pipeline, e.g. generator (Yoran\net al., 2024) or search query processor (Ma et al., 2023),\nas well as addressing fragility of the RAG approach,\ne.g. filtering irrelevant retrieved context (Wang et al.,\n2023; Xu et al., 2023; Kim et al., 2024) or dynamically\ndeciding for which user queries retrieval is actually\nneeded (Jiang et al., 2023; Asai et al., 2024).\nUnfortunately, all listed efforts are focusing on English\nas the data language in their experiments, i.e. the lan-"}, {"title": "2. Related Work", "content": "Despite mRAG being not well studied in the literature,\nsome of the individual components of the RAG pipeline\nwere rather well developed for multilingual settings,\ne.g. multilingual retrievers and generator LLMs; we\ndiscuss them in Section 3.\nThe closest line of work to ours is multilingual open\nquestion answering (Asai et al., 2021b; Muller et al.,\n2022; Sorokin et al., 2022; Asai et al., 2022, inter alia)\ndefined as a the task of answering non-English ques-\ntions from a large collection of multilingual documents,\nas introduced in (Asai et al., 2021b). Those aforemen-\ntioned works train task-specific models combining cross-\nlingual retrievers and multilingual generation models,\ne.g. with iterative extension of annotated data used\nin the CORA approach (Asai et al., 2021b). The key\ndifference of our work is that we compose the mRAG\nsystem in a zero-shot manner, using off-the-shelf com-\nponents without dedicated training. This approach,\ndominating nowadays in the literature, is enabled by"}, {"title": "3. Multilingual RAG pipeline", "content": "The high-level illustration of the mRAG pipeline is pre-\nsented in Figure 1. The input is represented by a user\nquery q in language Lq. This could be an arbitrary user\nrequest to an LLM. Following the common practice of\ntesting RAG systems on open-domain question answer-\ning, we assume q is an information-seeking question.\nThe model is expected to output response r which cor-\nrectly answers the given question. An important (and\nreasonable) expectation is that the model replies in the\nuser language, i.e. r is written in Lq.\nStep 1: retrieval. The first step in mRAG is retrieving\ncontext c relevant to the query q from the Internet or\na particular collection C, using the retriever system R:\nc = R(\u1fb7, C), q = Q(q). Here Q denotes an optional query\ngeneration model which infers a search query \u1fb7 from\na user query c, e.g. it can be an LLM prompted to\nreformulate the query, or simply copying the user query\nq. Following a standard practice in testing RAG systems,\nwe use Wikipedia as our collection C. In most of the\nexperiments we assume monolingual C in language Lc\n(English or user language), but we also experiment with\nMultilingual retrievers. The described problem set-\nting requires strong monolingual and cross-lingual\nrankers and rerankers, for cases when Lq = Lc and\nLq \u2260 Lc, correspondingly. We pick a strong recently\nreleased and publicly available BGE-m32 (Chen et al.,\n2024) which provides all listed functionalities and in-\ncludes all languages we consider in its training data.\nWe also consider a baseline including query translation,\nwhere query generator Q translates q from Lq to Lc.\nWe employ the NLLB-600M translation model\u00b3 (Team\net al., 2022).\nMultilingual generation. Most of current state-of-the-\nart LLMs are either English-centric or support a limited\nset of languages, possibly due to under-investigated\neffects of the \"curse of multilinguality\" for large mod-\nels (Conneau et al., 2020), i.e. it is yet unclear how\nmany languages LLMs can fit without hurting per-\nformance, or due to limited availability of multilin-\ngual instruction tuning and alignment datasets. At\nthe same time, it was shown that even English-centric\nLLMs, which were pretrained and finetuned mostly\non English data, may exhibit good multilingual ca-\npabilities due to the occasional presence of multilin-\ngual data in pretraining (Ye et al., 2023; Chirkova and\nNikoulina, 2024). As such, we experiment with both\nstrong English-centric and recent multilingual models.\nAmong English-centric models we pick commonly-used\nLLAMA-2-7B-chat (Touvron et al., 2023) and state-\nof-the-art SOLAR-10.7B (Kim et al., 2023), and among\nmultilingual models we pick Mixtral-8x7B (Jiang\net al., 2024) and Command-R-35B4. All models were\ninstruction-tuned. Command-R-35B was developed\nwith keeping RAG application in mind and officially\nsupports 11 languages, including most of our consid-\nered languages, and also includes 13 more languages\n(incl. Russian) in pretraining but not instruction tun-\ning. Mixtral-8x7B was pretrained on the multilin-\ngual data with 5 languages6, we use it's instruction-\ntuned version.\nSystem prompt. In our preliminary experiments we\nnoticed that models sometimes reply in English even"}, {"title": "4. Experimental details", "content": "Retrieval. We follow Asai et al. (2021b) and\n(Karpukhin et al., 2020) and construct passages by"}, {"title": "5. Results and discussion", "content": "Table 1 summarizes the results across different lan-\nguages on MKQA and XOR TyDi QA datasets. We ob-\nserve a high performance improvement brought by RAG\nfor all languages, but in many cases there is an impor-\ntant gap in performance in English and non-English.\nIn what follows we present multiple ablation studies\nto demonstrate steps needed to achieve shown results,\nto better understand the reasons behind the gap with\nEnglish, and identify future research directions. We\nstudy the effect of the system prompt, generator model,\nretrieval system and language. We run ablations on\nthree languages: French, Korean, and Russian.\nGenerator model: importance of using a strong mul-\ntilingual base model. Table 6 compares four consid-\nered generator LLMs with and without retrieval. We\nfind that Command-R-35B is the only model which con-\nsistently achieves high CLR and highest ranges of recall\nfor all considered languages (with advanced prompts\ndiscussed above). Another considered multilingual-\nby-design model, Mixtral-8x7B, reaches consistently\nhigh CLR and recall only for French which was present\nin its pretraining. English-centric LLAMA-2-chat-7B\nmost often replies in English. Interestingly, English-\ncentric SOLAR-10.7B reaches high CLR and recall for\nFrench and Russian (with advanced prompts). This\ncould be attributed to its strong capabilities in prompt\nunderstanding and accidental multilingual data present\nin pretraining.\nDespite Command-R-35B being a leader model for non-\nEnglish, its recall in English is much lower than of\nEnglish-centric SOLAR-10.7B which is possibly due\nto the \"curse of multilinguality\" effect. This highlights\nthe need for future models which would be fluent\nand accurate in both English and non-English.\nRetrieval: high performance of off-the-shelf multi-\nlingual retrievers in the in-domain setting. In our\nwork we rely on a strong multilingual retriever and\nreranker, BGE-m3, which was shown by its authors to\noutperform other approaches on multilingual retrieval\nbenchmarks. In Table 7 we evaluate its performance\nin the cross-lingual setting (documents in English and\nuser queries in non-English), by comparing to the base-\nlines involving query translation from user languages\nto English. We find that BGE-m3 outperforms a strong\nEnglish model, SPLADE, used with translated queries.\nWe note that BGE-m3 was trained on the datasets which\nalso use Wikipedia as the document datastore, therefore\nin our experiments it is used in the in-domain setting.\nThe retrieval performance in the multilingual setting\nwith domain-shift is yet to be explored.\nWhich language to retrieve from: highest perfor-\nmance with retrieving from multilingual Wikipedia.\nTable 1 compares retrieval from English Wikipedia,\nWikipedia in the user language, their union, and also\nWikipedia in all considered languages. In the latter\ntwo cases with run retrieval over the embeddings of\npassages in multiple languages, so that the selected\npassages may be also in multiple languages.\nComparing retrieval from English and user language,\nwe observe different behavior on the two considered\ndatasets. On the MKQA dataset, retrieval from English\nis more beneficial, which is expected since questions in\nPrompting strategy: importance of translating the\nsystem prompt into target languages and specify-\ning the desired language of the response. Table 5\nsummarizes an impact of prompt formulation (defined\nin Table 2) on RAG performance with English-centric\nSOLAR-10.7B and multilingual Command-R-35B mod-\nels.\nThe left part reporting Correct Language Rate (CLR)\nallows us to assess how often the model replies in the\nuser language. Due to multilingual pretraining and\ninstruction tuning, Command-R-35B, equipped with\nthe default system prompt (\"Reply short (EN)\"), replies\nin the user language in most, but not all, cases. Im-\nportantly, it gets \"distracted\" by the English context\nwhen retrieving from English Wikipedia and replies in\nEnglish for around 50% of non-English user queries.\nEnglish-centric SOLAR-10.7B, provided with the de-\nfault system prompt, also often replies in English. These\nresults demonstrate the need for using more advanced\nlanguage-related prompting strategies for both models.\nExplicitly specifying an instruction to reply in the given\nlanguage, while keeping the system prompt it-\nself in English (\"+ reply in UL (EN)\"), substantially\nalleviates the problem of generation in English and\ncorrespondingly increases recall, but still does not en-\nable correct language rate (CLR) close to 100%. More\ngeneric prompt with \"meta-instruction\" to reply in the\nsame language as the inout language (+ reply in same\nlang (EN)) leads to considerably lower CRL than explicit\nlanguage specification.\nThe further improvement in CRL (and thus recall) for\nboth models is enabled by translating the system prompt\ninto user languages. With the system prompt which\nincludes explicit specification to generate in the given\nuser language and is also written in the user language,\nboth models achieve CRL > 95% in most cases (ex-\ncept SOLAR-10.7B for Korean). Such an approach is\nhowever less convenient in practice, as it requires lan-\nguage expertise to control the quality of translating\nprompts (see footnote 7) and dynamic selection of the\nsystem prompt based on the user query. We believe that\nenabling multilingual LLMs to follow instructions\nwithin mixed-language prompts is an interesting\nresearch direction that would help to eliminate the\nneed for the described ad-hoc prompting.\nThe high CLR is necessary but not sufficient for high\noverall performance, as LLMs may use code-switching\nand tend to insert English named entities in their re-\nsponses in user languages. We attempt to alleviate\nthis issue by augmenting the system prompt with an\nexplicit instruction to write all named entities in ULs.\nWhile it does slightly improve character 3-gram recall\nfor Command-R in many cases, it does not solve the\nissue fully. We believe that addressing the described\ncode-switching problem is an important direction\nfor future research.\nManual inspection of errors. To better analyze fail-\nure cases, we perform a manual analysis of predictions\nin French, Chinese, and Russian and report results in\nTable 8. We find that system improvements can be\nmade at all steps, including retrieval, reading from\nthe retrieved documents, addressing issues with code-"}, {"title": "6. Conclusion", "content": "In this work we study RAG in multilingual settings and\nbuild a strong pipeline to be used as a baseline in future\nworks. Better understanding of mRAG would enable\nreliable information access across different languages\nand cultures. We analyze an impact of each mRAG\ncomponent impact on overall performance and provide\nguidelines and future research direction to further im-\nprove it.\nPossible research directions include:\n\u2022 The need for stronger multilingual LLMs and decod-\ning strategies. Our study highlights multilingual\ngeneration as a weakest part of the mRAG pipeline,\nespecially with mixed-language context. We show\nthat even strongest available multilingual LLMs can\nget distracted by the language of the prompt, and\nrequire ad-hoc prompting to enable consistent gen-\neration in the user language. Even then, they are\nstill prone to code-switching especially when writ-\ning named entities. We believe listed limitations\ncould be addressed by including mixed-language\nexamples in instruction tuning or by developing\nspecific decoding strategies.\n\u2022 LLM-based evaluation in multilingual settings. In\nour work we rely on the lexical matching-based\nmetrics due to their transparency and interpretabil-\nity. At the same time, recent works use LLM-based\nevaluation which captures better semantic similar-\nities but is currently underexplored in multilingual\nsettings.\n\u2022 Multi-domain multilingual retrieval. Current multi-\nlingual retrievers and rerankers are predominantly\ntrained on Wikipedia-based data which could limit\ntheir applicability to other domains.\nFollowing common practice in RAG and as a first step in\nmRAG, we run evaluation on the open question answer-"}, {"title": "Ethics Statement", "content": "We do not anticipate negative societal impact from our\nwork and on the reverse hope that it will help to broaden\nthe accessibility of modern NLP to other languages."}, {"title": "A. Additional related works", "content": "Lorem ipsum dolor sit amet, consectetuer adipiscing\nelit. Ut purus elit, vestibulum ut, placerat ac, adipiscing\nvitae, felis. Curabitur dictum gravida mauris. Nam arcu\nlibero, nonummy eget, consectetuer id, vulputate a,\nmagna. Donec vehicula augue eu neque. Pellentesque\nhabitant morbi tristique senectus et netus et malesuada\nfames ac turpis egestas. Mauris ut leo. Cras viverra\nmetus rhoncus sem. Nulla et lectus vestibulum urna\nfringilla ultrices. Phasellus eu tellus sit amet tortor gra-\nvida placerat. Integer sapien est, iaculis in, pretium\nquis, viverra ac, nunc. Praesent eget sem vel leo ul-\ntrices bibendum. Aenean faucibus. Morbi dolor nulla,\nmalesuada eu, pulvinar at, mollis ac, nulla. Curabitur\nauctor semper nulla. Donec varius orci eget risus. Duis\nnibh mi, congue eu, accumsan eleifend, sagittis quis,\ndiam. Duis eget orci sit amet orci dignissim rutrum.\nNam dui ligula, fringilla a, euismod sodales, sollicitudin\nvel, wisi. Morbi auctor lorem non justo. Nam lacus\nlibero, pretium at, lobortis vitae, ultricies et, tellus.\nDonec aliquet, tortor sed accumsan bibendum, erat\nligula aliquet magna, vitae ornare odio metus a mi.\nMorbi ac orci et nisl hendrerit mollis. Suspendisse ut\nmassa. Cras nec ante. Pellentesque a nulla. Cum sociis\nnatoque penatibus et magnis dis parturient montes,\nnascetur ridiculus mus. Aliquam tincidunt urna. Nulla\nullamcorper vestibulum turpis. Pellentesque cursus\nluctus mauris.\nNulla malesuada porttitor diam. Donec felis erat, con-\ngue non, volutpat at, tincidunt tristique, libero. Vivamus\nviverra fermentum felis. Donec nonummy pellentesque\nante. Phasellus adipiscing semper elit. Proin fermen-\ntum massa ac quam. Sed diam turpis, molestie vitae,\nplacerat a, molestie nec, leo. Maecenas lacinia. Nam\nipsum ligula, eleifend at, accumsan nec, suscipit a, ip-\nsum. Morbi blandit ligula feugiat magna. Nunc eleifend\nconsequat lorem. Sed lacinia nulla vitae enim. Pellen-\ntesque tincidunt purus vel magna. Integer non enim.\nPraesent euismod nunc eu purus. Donec bibendum\nquam in tellus. Nullam cursus pulvinar lectus. Do-\nnec et mi. Nam vulputate metus eu enim. Vestibulum\npellentesque felis eu massa.\nQuisque ullamcorper placerat ipsum. Cras nibh. Morbi\nvel justo vitae lacus tincidunt ultrices. Lorem ipsum\ndolor sit amet, consectetuer adipiscing elit. In hac habi-\ntasse platea dictumst. Integer tempus convallis augue.\nEtiam facilisis. Nunc elementum fermentum wisi. Ae-\nnean placerat. Ut imperdiet, enim sed gravida sollicitu-\ndin, felis odio placerat quam, ac pulvinar elit purus eget\nenim. Nunc vitae tortor. Proin tempus nibh sit amet\nnisl. Vivamus quis tortor vitae risus porta vehicula.\nFusce mauris. Vestibulum luctus nibh at lectus. Sed\nbibendum, nulla a faucibus semper, leo velit ultricies\ntellus, ac venenatis arcu wisi vel nisl. Vestibulum diam.\nAl"}]}