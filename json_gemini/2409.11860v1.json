{"title": "Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for Large-Scale Product Retrieval Evaluation", "authors": ["Kasra Hosseini", "Thomas Kober", "Josip Krapac", "Roland Vollgraf", "Weiwei Cheng", "Ana Peleteiro Ramallo"], "abstract": "Evaluating production-level retrieval systems at scale is a crucial yet challenging task due to the limited availability of a large pool of well-trained human annotators. Large Language Models (LLMs) have the potential to address this scaling issue and offer a viable alternative to humans for the bulk of annotation tasks. In this paper, we propose a framework for assessing the product search engines in a large-scale e-commerce setting, leveraging Multimodal LLMs for (i) generating tailored annotation guidelines for individual queries, and (ii) conducting the subsequent annotation task. Our method, validated through deployment on a large e-commerce platform, demonstrates comparable quality to human annotations, significantly reduces time and cost, facilitates rapid problem discovery, and provides an effective solution for production-level quality control at scale.", "sections": [{"title": "1 Introduction", "content": "Search functionality is a fundamental component of e-commerce platforms, with the objective of finding the most relevant products in a dynamic product database. Customers using search often exhibit a higher intent to find specific products (Moe, 2003), leading to greater engagement and conversion rates. However, they may struggle to articulate their needs in a search query. Even if they do express their intent clearly, information retrieval (IR) systems might fail to interpret it correctly, resulting in irrelevant search results (Wang and Na, 2024).\nEvaluating product retrieval systems on a large scale in a multilingual setting and for a diverse set of customer queries is an intricate but essential task for maintaining a high-quality user experience and driving business success. A prerequisite for this evaluation is the availability of a large enough pool of query-product relevance labels (Voorhees, 2001; Halvey et al., 2015), which indicate whether a retrieved product is semantically relevant to the query. Semantic relevance depends solely on the query and the product, excluding other contextual factors such as personal customer preferences.\nCreating annotation guidelines that codify what is semantically relevant is a complex task (Spark-Jones, 1975). It requires describing the guidelines in a digestible, concise, yet precise manner, as well as curating a set of illustrative examples of varying difficulty. Even with well-defined guidelines and well-trained human annotators, manual annotation is slow and costly.\nThe advent of crowd-sourcing platforms has increased scalability (Blanco et al., 2011; Alonso and Mizzaro, 2012; Lease and Yilmaz, 2013; Marcus et al., 2015; Chen et al., 2016), allowing for a trade-off between speed and cost. However, increasing the number of annotators can lead to inconsistencies, as even the same annotator may provide contradictory annotations for the same query-product pair, let alone multiple annotators. Consistency can be improved by using more annotators per pair (see, e.g., Ferrante et al. (2017)), but this results in increased cost. In large e-commerce systems, the volume of data that needs to be annotated leads to prohibitively high costs when using crowd-sourcing platforms that rely on human annotators.\nWhile the rate of manual relevance judgement varies depending on the task (Voorhees, 2001; Sanderson et al., 2010; Chen et al., 2022; Soviero et al., 2024), in our use case, we estimate a throughput of 2-3 query-product pairs per minute. As an example, 50,000 queries\u00b9 and 20 products per query results in one million query-product pairs, which takes 5,500-8,500 hours of human labour, assuming one annotation per pair. Moreover, evaluation is not a one-off practice; ideally, companies continuously assess their search engines to ensure effectiveness over time.\nThe sheer volume of required annotations in multiple languages, along with the need for continuous evaluation, makes human-generated relevance judgements the primary bottleneck in creating product retrieval evaluation datasets. To overcome these challenges, there has been growing interest in leveraging LLMs (Faggioli et al., 2023; Thomas et al., 2023; Soviero et al., 2024; Rahmani et al., 2024; Upadhyay et al., 2024; Bergum, 2024).\nIn this study, we propose a framework that leverages the capabilities of Multimodal Large Language Models (MLLMs) for assessing the relevance of query-product pairs (Fig. 1). Our method combines the strengths of LLMs and MLLMs in understanding natural language queries across various languages and processing both textual and visual features of products. Unlike traditional per-task annotation guidelines, such as those discussed by Soviero et al. (2024), we employ LLMs to generate annotation guidelines specific to each query. Additionally, our pipeline's modular design allows for caching and parallel processing, which is crucial for scaling up to larger systems. This framework has enabled daily evaluations of our product retrieval systems. It has also facilitated the comparison of different search models, increasing our confidence in offline evaluations and complementing our online evaluation techniques, such as A/B testing and other controlled online experiments (Kohavi et al., 2009). Moreover, we have used the relevance assessments' outputs to train, evaluate and analyse other components of our search and ranking systems."}, {"title": "2 Multimodal LLM-based relevance assessment", "content": "The setup of our method is depicted in Fig. 2. It is designed to leverage the capabilities of (M)LLMs for efficient evaluation of large-scale product retrieval systems, and it consists of six main steps: (1) For a given query and its context (e.g., selected gender and market), an LLM generates a query requirement list and a query-specific annotation guideline. The query requirement list captures the relevant pieces of information in the user's query and their level of importance. For example, for the query Nike red shoes, the query requirement list includes the brand (Nike), colour (red) and product category (shoe). The query-specific annotation guideline is generated by the LLM based on the query and its requirement list. It outlines criteria for each predefined label (see Appendix A for a detailed example). In our experiments, we defined three relevance labels for a query-product pair: \"irrelevant\", \"acceptable substitute\" and \"highly relevant\". 2\n(2) The query and its context are sent to the search engine, which retrieves a set of products. For simplicity, we illustrate this process using a single query-product pair. However, in practice, we work with multiple query-product pairs and may utilise two or more retrieval systems, particularly when comparing their performance.\n(3a,b) For each retrieved product, we have access to its textual description and its associated image.\n(4) Using MLLMs and the product image, a visual description in textual form is generated.\n(5a,b) The combined textual and visual product descriptions are sent to an LLM together with the outputs of Step 1 (i.e., query requirement list and query-specific annotation guideline).\n(6) The LLM assigns a relevance score to the query-product pair using a set of predefined labels. In its simplest form, the output is a database with one row for each (query, product, relevance score).\nIn Steps 1 and 6, we utilise chain-of-thought (CoT) prompting (Wei et al., 2022; Nye et al., 2021) to enhance the quality of (M)LLM outputs and for debugging. An example of the reasoning steps is shown in Appendix A.\nAs illustrated with dashed lines in Fig. 2, all outputs and intermediate steps are stored in a database. This caching serves two key functions in our pipeline. Firstly, it facilitates efficient retrieval and reuse. When evaluating a new search engine configuration (or a variation of existing ones), the database is queried to retrieve relevant pieces of information, including the query requirement list, query-specific annotation guidance, textual and visual product descriptions, and relevance scores. We only compute the missing pieces of information. Secondly, it ensures consistent evaluation across different search engines, as intermediate steps (such as query-specific annotation guidelines) are computed only once and then used to evaluate various search engines."}, {"title": "3 Experiments and Results", "content": "Data collection. As a starting point for our data collection, we used one year's worth of production search query traffic\u00b3. We then performed stratified sampling along the following dimensions: a) search engine, b) activated gender filter on the website, c) query frequency, and d) query length in tokens."}, {"title": "3.2 LLM versus Human Annotators", "content": "Using our proposed framework, we assessed the relevance quality of the 20,000 unique query-product pairs. Table 2 summaries the results of our experiments using few-shot prompting, where we incorporated examples into the system prompts of the (M)LLMs. Here, we provided the LLMs with example customer queries, their corresponding requirement lists, and quality labels, but not complete product descriptions or images.\nInitially, we randomly sampled 100 examples from the English dataset and examined the relevance labels assigned by both LLMs and human annotators. We used the results of this step to adjust the few-shot examples in the system prompt.\nWe compare the performance of different versions of our pipeline with human annotations. In Table 2, these versions are labelled as \"LLM-text\", \"MLLM-text\", and \"MLLM-multi\".\n\"LLM-text\" is the simplest version where only product descriptions in textual form are used, without incorporating product images. In \u201cMLLM-text', we employ a vision model to generate textual descriptions of product images (Step 4 in the pipeline, see Fig. 2). The generated textual description of the product image is then concatenated with the product description itself (Step 5a). \u201cMLLM-multi\" utilises the same textual input as \u201cLLM-text\u201d, while also incorporating the product image as an additional input.7 Comparing \u201cMLLM-multi\" and \"LLM-text\" highlights the impact of multimodal inputs on our task.\nIn all cases, the (M)LLM uses product information (in different modalities, depending on the version), query requirements, and query-specific annotation guidance to assign relevance labels.\nOverall, Table 2 shows that the agreement between human annotators and LLMs is on par with that between human annotators, supporting the scalability of LLM annotation for production-level traffic.\nTable 2 also shows the results of an ablation study that removed the query-specific annotation guideline (Step 1 in Fig. 2). The inclusion of this guideline improved agreements by approximately 4-10%. More importantly, this component in our framework is essential for enhancing the interpretability and debugging of LLM-based decisions. However, as expected, incorporating query-specific annotation guidelines and chain-of-thought reasoning increased the evaluation costs.\nWe also tested the impact of different (M)LLM architectures in our pipeline. The results shown in Table 2 are based on \u201cGPT-40", "MLLM-multi (gpt-4-turbo)\". In the case of GPT-4 Turbo, the agreement with human annotators consistently fell below that of GPT-40, while its costs and evaluation times exceeded those of all other architectures.\nIn Table 4, we repeated the experiments using GPT-3.5 Turbo. As expected, the results were significantly worse compared to GPT-40 or GPT-4 Turbo. However, the cost and time required for GPT-3.5 Turbo were lower than for the other architectures.\"\n    },\n    {\n      \"title\"": "4 Discussion"}, {"content": "Agreement between LLM and human annotators. The human-LLM agreements between \"MLLM-multi\" and the human majority vote are 65.6% for EN and 64.7% for DE in Table 2 are in line with the human inter-annotator agreement, which is 60.2% for EN and 60.5% for DE.\nTo better identify discrepancies between LLM and human annotations, we focused our analysis on hard disagreements between the two. We consider a hard disagreement to be when, for example, the LLM considers a product to be \"highly relevant\" for the given query, whereas the human majority judgement would be \u201cirrelevant\u201d, and vice versa. In total, we found that approximately 15% of annotations in our dataset were hard disagreements. For manual analysis, we sampled 20% of the hard disagreements and found that in 50% of the cases, the human annotation was wrong, in 31% the LLM was wrong, and in 19% of cases, both the LLM and the humans provided a wrong annotation.\nWe further categorised the hard disagreements into 9 error classes10, and found LLMs and humans tend to make very different kinds of errors. For example, as shown in Fig. 3, the main errors made by the LLM are either being too strict in their judgement (e.g., considering a product as \"irrelevant\", where \"acceptable substitute\u201d would have been more appropriate), or misunderstanding a part of the query (e.g., interpreting On Vacation in its literal sense rather than the fashion brand). On the other hand, humans would oftentimes be too lenient when LLMs were too strict (e.g., considering a product as \"highly relevant\u201d when \u201cacceptable substitute\u201d would have been more appropriate).\nFurthermore, human annotations frequently exhibited brand errors (e.g., considering a pair of Lee jeans as \"highly relevant\" for a query requesting Levi's jeans), product errors (e.g., considering an Adidas Samba sneaker as \u201chighly relevant\" for a query requesting an Adidas Stan Smith sneaker), or category errors (e.g., considering a pair of Nike shirts as \u201chighly relevant\u201d for a query requesting Nike shoes), which we barely ever observed for LLMs. We hypothesise that the latter three kinds of human errors are primarily due to annotation fatigue as specifically these cases have been prominently and unambiguously featured in the annotation guidelines.\nThese findings suggest that LLMs might be a more reliable source for the bulk of annotations, freeing human labour to focus on trickier cases. In the human-machine collaboration spectrum introduced by Faggioli et al. (2023), our approach can be classified as a \u201cHuman Verification\" (or human-in-the-loop) approach.\nSubjective nature of relevance judgements. We found that human disagreement was dominated by two main factors, (i) human errors due to annotation fatigue as described above12, and (ii) the inherent subjective nature of the task. For the latter, we attribute the source of disagreement to either the ambiguity in the annotation guidelines (even comprehensive guidelines cannot cover all possible cases), or to the subjective judgement of the annotator. 13 Ideally, the annotation guidelines should make the task as objective as possible; however, in practice, there is always a level of subjectivity.\nAnnotation time and cost. (M)LLMs are approximately 100 to 1,000 times cheaper than human annotators, and the time required to complete all 20,000 annotations using (M)LLMs is significantly smaller (around 20 minutes for (M)LLMs compared to about 3 weeks for human annotators). Note that several human annotators worked in each group (i.e., \u201cA1\u201d and \u201cA2\u201d in Table 2), and the total time reported in Table 2 is for annotating all query-product pairs. This excludes the time spent on scoping and onboarding human annotators. For (M)LLMs, the reported time excludes the pipeline development time and only includes the actual annotation time.\nWe anticipate that both cost and time will decrease even further as LLMs and their APIs become more efficient. Moreover, new approaches, such as batch processing, can further reduce costs (e.g., OpenAI's new batch processing is half the price of non-batch queries\u00b94). Indeed, in production, we use batch processing to assess query-product pairs across markets on a nightly basis.\nRelevance assessment in production. High relevance is a necessary, but not a sufficient condition, for high customer engagement, as it is also determined by other factors, e.g. personal preferences, product availability, and price expectations. In this paper, we focus on semantic relevance, but in production we rank the retrieved documents based on various features to take into account both relevance to the query and customers' personal preferences.\nCurrently, we use the LLM-powered evaluation framework presented in this paper in production to continuously perform relevance assessments at scale. We typically focus on monitoring the performance of high-volume queries with our framework. Additionally, we evaluate the retrieval performance for low-performing queries. We identify such queries based on signals indicating low relevance in top ranked results, such as low engagement with the result set and high friction in customer experience (e.g., a high reformulation rate15) or high exit rate. This approach enables us to significantly reduce costs and to enhance customer experience faster by prioritising the queries that need the most attention and optimising our resources accordingly.\""}, {"title": "5 Conclusion", "content": "Our novel evaluation method leveraging Multimodal LLMs demonstrates a highly efficient approach to assessing large-scale IR systems in product retrieval. We introduce query-level annotation guidelines for calibration and utilise the multimodal capabilities of foundation models to assess the relevance of retrieved products for a query. Our LLM-powered framework, combined with caching and parallel processing, leads to significant reductions in both time and cost. The method's scalability, ability to handle multilingual queries and products, and support for continuous offline evaluations are crucial for large IR systems operating in diverse markets. Experimental results, validated against 20,000 human annotations, confirm the effectiveness and efficiency of our approach. A detailed analysis of human and (M)LLM annotations indicates that (M)LLMs are a more reliable source for relevance assessment in large-scale IR systems. We are currently leveraging this framework in production to continuously perform relevance assessments at scale and maintain a high-quality user experience. Additionally, we utilise its outputs to train, evaluate, and analyse other components of our search and ranking systems."}, {"title": "6 Ethics Statement", "content": "Our data collection process strictly adheres to the General Data Protection Regulation (GDPR) and other relevant data privacy and safety laws within the European Union. We ensure that all data utilised, including human evaluation data, is anonymised to safeguard against the disclosure of any personally identifiable information.\nWe do not suggest replacing human annotators with large language models (LLMs). Instead, we focus on leveraging the strengths of both. Our analysis indicates that human annotators may make errors due to annotation fatigue or lack of domain knowledge-errors not observed with LLMs. Therefore, we recommend using LLMs for bulk annotation work while reserving human expertise for more complex cases.\nWe are committed to advancing responsible and unbiased AI technologies and welcome any inquiries regarding the ethical aspects of our work."}, {"title": "A Multimodal LLM-powered relevance assessment: evaluation steps for an example query", "content": "Fig. 4 illustrates the various steps of our evaluation framework using the example query women's long sleeve t-shirt with green stripes.\nGiven this query, the LLM infers its requirements and their importance (Step 1 of our framework, refer to Fig. 2). The outputs of this step for the example query are detailed in the paragraph \"Query requirements and their importance\" in Fig. 4b. As shown in Fig. 4b, the LLM has inferred four query requirements: \u201cassortment category", "sleeve length": "product type\" and \"pattern\". An importance level is also assigned to each requirement (in this case, the first three requirements are \"must_have\", and the last one is \"approximate_is_okay\u201d). Additionally, the LLM provides a reason for each requirement and its importance (not shown here). The LLM also translates the query into English and assigns a \u201cspecificity\u201d level, as shown in Fig. 4b.\nIn panel (c), the translated query, its specificity, its requirements and their importance are used to create query-specific annotation guidelines. The three quality labels (i.e., \u201cirrelevant\u201d, \u201cacceptable_substitute\u201d and \u201chighly_relevant\u201d) are predefined. However, the guidance for each label is generated by the LLM. The LLM provides clear and detailed descriptions for each relevance label, tailored to the given query. In the ablation study of Table 2, we assessed the impact of query-specific annotation guidelines on our method's performance. To do this, we replaced the query-specific guidelines with a generic one, as shown in Fig. 5.\nIn Fig. 4d, an example product, its attributes, and its image are shown. These attributes are read from an existing database and are not generated by the LLM, except for the \"visual description of packshot\", highlighted by a red rectangle which is generated by a vision model (e.g., GPT-40). The (M)LLM uses the query-specific annotation guidance in panel (c), along with the extracted and generated product attributes in panel (d), to assign a relevance label. In this example, as shown in panel (e), the label is \u201chighly_relevant\u201d, and the reasoning (aka the chain-of-thought step) of the (M)LLM is shown for inspection and debugging purposes.\""}, {"title": "B Human Annotation Guidelines", "content": "For human annotators, we focused on three classes:\n\u2022 highly relevant: The retrieved product satisfies all the specifications in the query.\n\u2022 acceptable substitute: The item fulfils some, but not all aspects of the query and the retrieved item can be used as a functional substitute.\n\u2022 irrelevant: A central aspect of the query is not fulfilled (e.g. wrong brand, wrong category, wrong product).\nWe decided against a more granular annotation scale to reduce mental load on annotators and to (hopefully) harness higher agreement scores among annotators.\nOur annotation guidelines also reflect requirements that are more business-specific rather than content-specific. For example, annotators have been explicitly briefed that if a query requests a specific brand (e.g. Polo Ralph Lauren jumpers), any retrieved item that is not from the requested brand is to be regarded as \u201cirrelevant", "irrelevant\".\nDespite the explicit mentions of these rules, numerous provided examples across product categories, and an additional briefing session after the annotation pilot phase, brand and product errors were among the most commonly made human annotation errors.\"\n    },\n    {\n      \"title\": \"C Experiments with LLM types: GPT-3.5, GPT-4, and GPT-40\",\n      \"content\": \"In this section, we compare human annotator groups with (M)LLMs using different architectures. The results in Table 3 are primarily based on \u201cGPT-40\" (OpenAI, 2024), except for the row labelled \u201cMLLM-multi (gpt-4-turbo)\". For GPT-4 Turbo, the agreement with human annotators was consistently lower than that of GPT-40, while its costs and evaluation times exceeded those of all other architectures.\nIn Table 4, we repeated the experiments using GPT-3.5 Turbo. As expected, its results were significantly worse compared to GPT-40 and GPT-4 Turbo, but its cost and time requirements were lower than those of the other architectures.\"\n    },\n    {\n      \"title\": \"DLLM versus Human error types\",\n      \"content\": \"After manually inspecting a sample of hard disagreements16, we defined the following 9 error classes, some of which are applicable to LLMs and humans, and some to LLMs only:\n1. Brand error. When a user specifies a brand name in the search query, e.g. Lee jeans, Nike sneakers, or Mascara dresses, we consider any retrieved item as \\\"irrelevant\\\" if it is not from the requested brand. This is independent of whether the retrieved item would be visually similar to the requested one. This requirement has been covered in the LLM prompt as well as the human annotation guidelines. Predominantly, this error has been made by human annotators (see Fig. 3).\n2. Product error. When a user specifies a specific product in the search query, e.g. Levis 501 or Adidas Stan Smith, we consider any retrieved item that is not exactly the requested item as \u201cirrelevant\". This requirement has been covered in the LLM prompt as well as the human annotation guidelines. Predominantly, this error has been made by human annotators (see Fig. 3).\n3. Too strict. This error happened when a product was judged as \u201cirrelevant": "or a given query despite fulfilling almost all the requirements of the query. This error has been predominantly made by LLMs (see Fig. 3), for example when a query requested black Levis jeans with holes, but the retrieved product was a grey pair of Levis jeans with holes, the LLM would typically annotate the retrieved products as \u201cirrelevant", "highly relevant": "or a given query, despite not fulfilling all requirements that the query specified. This error has been exclusively made by human annotators (see Fig. 3), for example where for a query like Nike Air Force One high-top, humans annotated a Nike Air Force One low-top sneaker as \u201chighly relevant\u201d.\n5. Category error. When a user specifies the category of a fashion item in the search query, e.g. dress, sneakers, belts, we consider any retrieved item that does not match the category as \"irrelevant\". This requirement has been covered in the LLM prompt as well as the human annotation guidelines. Predominantly, this error has been made by human annotators (see Fig. 3).\n6. LLM hallucination error. We rarely observed hallucinations as a source of error. Interestingly, when hallucinations did occur, they were exclusively related to size queries, such as t-shirt xxl. In such cases, the LLM would hallucinate various available sizes for a given retrieved product and make a relevancy judgement on the basis of its hallucinations.\n7. LLM translation error. Since our dataset contained German and English queries, the LLM was prompted to translate a German query into English before starting its reasoning process. This sometimes resulted in translation errors that subsequently led to incorrect relevancy judgements. For example, it happened for queries containing the term Unterziehhose, meaning some sports leggings one can wear underneath sports shorts, which the LLM incorrectly translated as underpants.\n8. LLM understanding error. This error category is somewhat broader. We would categorise an LLM error as understanding error, whenever the LLM misinterpreted a part of the query or the product. For example, this error occurred when the LLM would misinterpret a query for Nike Tech Fleece to be focused on the material whereas Tech Fleece typically refers to a particular series of Nike sports clothing. Another example is the misinterpretation of brand names, such as for On Vacation (interpreted in its literal meaning), or for Evry Jewels (where Evry would be interpreted to mean Every). To our amusement during error analysis, we also observed a brand misinterpretation for the query miniature winter jackets for kids, where Mini A Ture is a kids' clothing brand. The LLM interpreted miniature in its literal sense and reasoning that [...] the sizes available are for kids, which fits the 'miniature' requirement.\n9. LLM vision error. Some of our models included the visual interpretation of a product image in its relevancy assessment.17 We only rarely observed LLM vision errors. If they did occur, it was typically when the product image was taken at a slight angle for instance, with a pair of sneakers where the LLM erroneously identified them as high-top due to the photo angle. Errors were also more likely when the image included a human model, which acted as a distractor."}, {"title": "E Subjective Nature of Relevance Judgements", "content": "The difficulty in judging query-product relevancy can vary widely. For example, for queries such as Nike Air Max 95 or Paul Smith long sleeve polo shirt, there is barely any room for subjectivity the retrieved products either are matches, or they are not. And indeed, this is reflected in the human-human inter-annotator agreement (95% and 82%, respectively) and the LLM-human agreement (98% and 89%, respectively), for these two examples.\nHowever, there are numerous queries that are much more open to subjective judgement. One such example is the query smart casual shoes, where the human-human agreement was only 12% and the LLM-human agreement was 24%. The range of suitable products for this query spans various types of shoes, and whether or not a particular shoe can be categorised as smart casual is typically not included in the product data. In these cases, humans and LLMs would draw on their prior knowledge for making a relevance judgement. LLMs would generally be a stricter judge and consider anything that resembles a sneaker too closely, or is not in a shade of black or brown, as \"irrelevant\". Human strictness for relevancy judgements for this query varied between the very formal and the (loosely speaking) anything goes extremes."}]}