{"title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use? CoALM: A Unified Conversational Agentic Language Model", "authors": ["Emre Can Acikgoz", "Jeremiah Greer", "Akul Datta", "Ze Yang", "William Zeng", "Oussama Elachqar", "Emmanouil Koukoumidis", "Dilek Hakkani-T\u00fcr", "Gokhan Tur"], "abstract": "Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolution-izing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA)\u2014and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CoALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CoALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using COALM-IT, we train three models CoALM 8B, CoALM 70B, and CoALM 405B, which outperform top domain-specific models, including GPT-40, across all three benchmarks. This demonstrates the feasibility of a single model approach for both TOD and LA, setting a new standard for conversational agents\u00b9.", "sections": [{"title": "Introduction", "content": "The concept of intelligent agents has been the cornerstone of artificial intelligence research for a long time (Minsky, 1986), developing in parallel with the field of human-to-machine conversation (Young, 2002). The advent of LLMs (OpenAI et al., 2024; Dubey et al., 2024) has revolutionized both fields and enabled powerful Language Agents (LA)"}, {"title": "Related Work", "content": "Dialogues and the Domain Shift. Earlier studies work on applying LLMs to dialog applications through supervised fine-tuning (Su et al., 2022; Gupta et al., 2022) or different prompting methods (Hu et al., 2022; Chung et al., 2023; Zhang et al., 2023). Following these, Hude\u010dek and Dusek (2023) have examined the dialogue management abilities of instruction-tuned LLMs in handling goal-oriented multi-turn conversations. More recently, existing work in dialogue agents primarily focuses on leveraging dialogue acts to derive API calls for backend services (Li et al., 2024; Xu et al., 2024; King and Flanigan, 2024). FNCTOD (Li et al., 2024) fine-tunes on a small dataset restricted to a limited set of domain-specific APIs for state tracking, whereas AutoTOD (Xu et al., 2024) uses GPT-4 with hand-crafted prompts that rely on a narrow set of predefined APIs with long instructions for each dialogue domain. However, these approaches are brittle and difficult to scale in real life scenarios, as they require costly re-trainings or extensive prompt engineering to handle new services, unseen domains, and unexpected user requests. Our work aligns with these studies in building such agents, but CoALM can manage thousands of complex APIs at the same time and can generalize to unseen domains without expensive training cycles and time-intensive prompt engineering.\nLanguage Agents. Tool learning with LLMs has evolved from simple simple reasoning (Wei et al., 2022) to more sophisticated approaches (Yao et al., 2023; Ling et al., 2023). Early work relied on prompting to enable tool usage (Yao et al., 2023; Paranjape et al., 2023), but more recent research has focused on specialized fine-tuning approaches for effective function calling accuracy (Schick et al., 2024; Patil et al., 2023; Wang et al., 2024; Zhang et al., 2024). For example, Toolformer (Schick et al., 2024) have explored how LLMs autonomously learn when and how to call APIs, leading to improved performance in task-specific settings. In this direction, recent works (Abdelaziz et al., 2024; Liu et al., 2024; Lin et al., 2024) focus on fine-tuning synthetically generated data to integrate more complex tool calling capabilities, such as nested function calls and irrelevance detection. These approaches shown promising results on LA benchmarks, however they mostly operate on single-turn interactions with the user and fall short of enabling user-driven, multi-domain, and multi-turn task completion which is essential for real-world conversational systems."}, {"title": "Preliminaries", "content": "A Conversational Agent, at its core, must understand user intents, maintain context across multi-turn interactions, and respond contextually. Beyond traditional TOD tasks, modern conversational agents are also expected to exhibit agentic abilities, like tool calling, planning, and decision making, to fulfill complex user requests. An effective conversational agent integrates these capabilities as skills, ensuring natural and relevant interactions while efficiently completing the user's objectives. The detailed task formulations for TOD systems and LA are provided in Appendix A."}, {"title": "Why we need both TOD and LA Capabilities?", "content": "Multi-turn interactions are critical for refining ambiguous user requests. For example, when a user says \"Find me a hotel\", the system can ask clarifying questions to clarify the user's intention (e.g., location, price range) instead of returning generic results. This ensures meaningful and task-specific conversations. That said, traditional TOD systems excel at handling these multi-turn interactions but over a small set of APIs (e.g., query_restaurant, book_hotel) (Ye et al., 2022). By training on structured dialogue flows, they achieve high task success rates in controlled scenarios (e.g., standard booking or reservation tasks) without requiring complex function-calling capabilities. However, these systems struggle to adapt to new services (e.g., airline, retail) without expensive re-training.\nIn real-world settings, users may need to access a wide variety of APIs (e.g., search_direct_flight, get_product_details). This is where LA shines: they leverage LLMs and can rapidly learn how to use unseen new tools since they are already proficient with determining when to invoke an API and decide which API to use from a diverse set of available functions. Without these skills, agents fail to fulfill complex user goals, limiting their utility.\nTogether, these skills form the backbone of a unified conversational agents, enabling them to transition from being passive responders to proactive collaborators capable of managing intricate tasks and sustaining user engagement."}, {"title": "Can TOD Systems Solve Function Calling Tasks?", "content": "The benchmark results demonstrate the limitations of TOD systems in function calling scenarios. Despite achieving top performance on MultiWOZ metrics as in Table 2, these systems show significantly lower accuracy on both API-Bank (Table 3) and BFCL (Table 4) benchmarks. This performance gap reveals that TOD systems' traditional strengths in dialogue management do not translate well to handling diverse, unseen, and complex API calls."}, {"title": "Can LAs Handle Task-oriented Multi-turn Conversations?", "content": "Conversely, agentic models like ToolAce (Liu et al., 2024), Hammer (Lin et al., 2024), and Granite (Abdelaziz et al., 2024) while achieving accurate results on API-Bank and BFCL V3, perform poorly on MultiWOZ's task completion metrics. These results highlight a critical weakness: while they deliver strong performance on function execution tasks, they fall short in maintaining coherent multi-turn conversations and properly fulfilling user intents. Their specialized optimization for tool calling impairs their dialogue management abilities, indicating that current LAs need more balanced capabilities to handle task-oriented conversations more effectively."}, {"title": "Methodology", "content": "Our approach, illustrated in Figure 2, develops a unified agent skilled in goal-oriented multi-turn conversations and function calling. First, we build the CoALM-IT, a broad instruction-tuning (IT) dataset that spans multiple domains, tasks, and unique reasoning structures. Next, we do fine-tuning on the proposed CoALM-IT dataset to produce CoALM; a balanced conversational agent model series capable of complex reasoning, fluent dialogue, user intent fulfillment, and function calling."}, {"title": "Conversational Agent Dataset Generation", "content": "To develop a conversational agent with diverse capabilities, we created a comprehensive dataset that combines samples across multiple skills essential for both multi-turn task-oriented conversations and tool utilization. Figure 2 summarizes how the dataset is created and Table 1 provides detailed statistics of CoALM-IT.\nTOD Datasets. An accurate dialogue system needs to master three fundamental capabilities: providing accurate information to users, fulfilling user goals, and tracking dialogue states to understand user intents and goals throughout conversations (Walker et al., 1997). To equip our model with these skills, we utilized the SNIPS dataset (Coucke et al., 2018), originally designed for language understanding but repurposed for single-turn dialogue state tracking (DST). We extracted its training split and converted it into the state tracking IT format by crafting a detailed instruction prompt, as illustrated in Figure 4. This transformation resulted in a training set of 24,542 samples for effective DST.\nFunction Calling Datasets. Tool calling capability is the ability to select appropriate APIs and access external knowledge, which is crucial in modern LAs. An effective agent must not only choose the correct API but also provide properly typed parameters (e.g., integers or strings) and manage complex scenarios involving sequential or parallel function calls. To develop these skills, we incorporated datasets from two state-of-the-art agent models: Hammer (Lin et al., 2024) and ToolACE (Liu et al., 2024). Hammer's training dataset incorporates random noise by replacing function and parameter names to prevent overfitting (see Figure 2), forcing the model to reason about API functionality through provided descriptions rather than memorizing specific identifiers. ToolACE provides multi-turn conversational scenarios in open-domain settings, where function calls may occur across multiple turns, but no database is provided. We post-process these datasets by incorporating the prompt instructions and adding conversation history if available. As reported in Table 1, the combined API calling corpus contains 216,319 samples.\nConversational ReAct-based API Calling (CRA) Dataset. While state tracking enables the understanding of user intent and function calling provides external knowledge access, integrating these capabilities within multi-turn task-oriented conversations requires additional reasoning about when to make API calls and how to interpret their results. Our primary contribution is a completely new User and Agent conversation structure as User-Thought1-Action-Observation-Thought2-Response. Starting from multi-turn SGD dataset (Rastogi et al., 2020), we systematically transform each turn to include two distinct reasoning steps (Thought1 and Thought2) and potential API calls (Action and Observation), extending traditional ReAct format (Yao et al., 2023) by incorporating GPT-40 for content generation (Figure 2 top row). Our structure includes two main parts: (i) User-Thought1-Action, which focuses on understanding the user's intent with reasoning and invoking the right API, if necessary (Figure 6 bottom). (ii) Observation-Thought2-Response, where the agent analyzes the returned observations and formulates an appropriate response to the user (Figure 7 bottom). This transformation is achieved with a carefully designed prompt in Table 6, which enforces strict \"Role Definition\u201d, \u201cTask Information\", and \u201cOutput Format\u201d. Since CRA is generated via GPT-40 (OpenAI et al., 2024), it is also validated by human evaluators (Appendix D). Best of our knowledge, this is the first ReAct-based Conversational API dataset that incorporates multiple intermediate reasoning steps in multi-turn settings for TOD. This process yielded 82,236 samples, specifically tailored for task-oriented domains such as hotel bookings and restaurant reservations.\nWe merge all three datasets into a single training set called CoALM-IT, please refer to Table 1 for details. We fine-tune our CoALM models on this merged dataset in one pass. By interleaving samples from TOD, LA, and CRA, the model continuously practices different conversational skills without overfitting to any single domain or task type.\""}, {"title": "Fine-tuning Towards Conversational Agents", "content": "We followed a multitask fine-tuning approach to develop CoALM models' diverse capabilities across TOD, function calling, and multi-turn reasoning by training on CoALM-IT. Our training process is structured to target specific skills through different optimization objectives completely in zero-shot settings, as our CoALM-IT dataset does not contain any of the evaluation benchmark training sets.\nMultitask Fine-tuning. As described in Section 4.1 and illustrated in Figure 2, our CoALM-IT dataset combines samples from three distinct domains, each designed to cultivate a specific skill: (i) TOD (Task-Oriented Dialogue) for strengthening dialogue state tracking, (ii) LA (Language Agent) for teaching the model when and how to invoke function calls, and (iii) ReAct for multi-turn conversation, multi-step reasoning and function calling.\nFor TOD, we augment SNIPS data with prompt instructions (Figure 4), training the model to generate structured dialogue states in response to user queries. For function calling (LA), we optimize COALM to select the correct APIs and produce accurate function calls with proper parameter types (Figure 5), emphasizing reasoning over memorized patterns. We then address complex multi-turn conversations with API integration using our CRA dataset, formatted in the ReAct style. This stage uses two objectives: (1) action prediction (Figure 6), where the model learns to issue the appropriate function call given the conversation history, and (2) response generation (Figure 7), where it synthesizes coherent replies based on both API results and intermediate reasoning steps. Rather than merely producing answers, the model learns to reason, decide, and act in multiple stages before arriving at a final response. Notably, we trained our models on CoALM-IT by interleaving TOD, LA, and CRA samples, enabling the model to continuously practice diverse conversational skills while avoiding overfitting to any single domain or task type.\nTraining Details. We developed the CoALM model series by fine-tuning Llama 3.1 8B, Llama 3.3 70B, and Llama 3.1 405B (Dubey et al., 2024) using a consistent Alpaca (Instruction-Input-Output) format. To balance efficiency and model quality, we applied LoRA (Hu et al., 2021) rank (r) = 16 and scaling factor (a) = 32 to all linear layers, and trained in mixed-precision bfloat16 (bf16) on 8 NVIDIA H100 GPUs. Under these settings, CoALM 8B required approximately 8 hours of training, while CoALM 70B took about 60 hours. We used a global batch size of 8, trained for 3 epochs with a learning rate of le-4, and employed a linear warm-up schedule with a 0.1 ratio. For CoALM 405B, we fine-tuned Llama 3.1 405B and using QLoRA (Dettmers et al., 2023) with the same rank and scaling factor using bitsandbytes (BitsAndBytes, 2025) with a quantization type of normalized float 4 (nf4). The precise training configurations for CoALM 8B, CoALM 70B and CoALM 405 are included in the HuggingFace pages. Our training pipeline leveraged the Oumi framework to ensure reproducibility and streamlined management (Oumi, 2025)."}, {"title": "Experiments", "content": "This section presents results highlighting CoALM's effectiveness in unifying conversational management and advanced API calling, outperforming specialized models across both TOD and LA benchmarks."}, {"title": "Experimental Settings", "content": "Evaluation Benchmarks. We evaluate our approach on three complementary benchmarks that assess different aspects of model performance: MultiWOZ 2.4 (TOD), API-Bank (LA), and BFCL V3 (LA). Specifically, MultiWOZ 2.4 (Ye et al., 2022) is a multi-domain TOD dataset covering scenarios such as hotel booking and transportation, where we measure Success Rate and Joint Goal Accuracy (JGA); in our zero-shot setting, we rely on the test set of 999 samples, using a slightly modified AutoTOD prompt (Xu et al., 2024). API-Bank (Li et al., 2023) focuses on evaluating tool-augmented LAs through 314 tool-use dialogues and 753 API calls, tested at two levels: L-1 (invoking a known API) and L-2 (retrieving and calling from multiple candidates). Lastly, BFCL V3 (Patil et al., 2023) provides over 1,800 test cases spanning tasks like simple, multiple, and parallel function calls, evaluated by Abstract Syntax Tree (AST) accuracy and Executable Function Accuracy. See Appendix B for further details.\nBaselines. In the LA tasks, we included strong baselines like Hammer (Lin et al., 2024), ToolAce (Liu et al., 2024), Granite (Abdelaziz et al., 2024) which represent state-of-the-art models in agentic tasks, including OpenAI models. For MultiWOZ evaluations, we recognize that many existing TOD models are trained with classification-based supervised fine-tuning, focusing primarily on DST. Such models do not support free-form dialogue generation, nor do they exhibit broader \u201cchat\u201d capabilities. In contrast, our approach aims to unify both conversational (LA) and agentic (TOD) tasks into a single, generative framework. On the other hand, there are some models evaluated in zero-shot settings but as per domain JGA, rather than overall JGA. That said, we used top popular zero-shot models FNCTOD (Li et al., 2024) and NC-Latent-TOD (King and Flanigan, 2024) as our TOD baselines in TOD. Please see Appendix C for more details of these baseline models."}, {"title": "Results on MultiWOZ", "content": "LA models struggle with TOD. Table 2 summarizes results on MultiWOZ 2.4. Baseline models optimized for function calling (ToolAce, Hammer, Granite, CodeAct) achieve low Success Rate and JGA. Although these agents can call APIs effectively, they fail to track user intents across multiple sessions or deliver correct final answers to the user, except ToolAce JGA reaches 34.4% accuracy close with domain-specific TOD models like FNCTOD.\nInstruction-tuned base LLMs like Llama 3.1 8B perform moderately better on MultiWOZ, reaching a 19.9% Success rate and 26.3% JGA.\nCOALM surpasses and generalizes in TOD. In contrast, our smallest CoALM 8B achieves 51.6% Success Rate, more than doubling the Success performance compared to Llama 3.1 8B and surpassing other LAs. Moreover, our CoALM 70B model achieves top results on DST with achieving 43.8% JGA, even outperforming GPT-40 and GPT-40-mini. This shows CoALM's ability with coherent multi-turn state-tracking, outperforming existing baselines and domain-specific models like FNCTOD. Notably, CoALM's strong performance is achieved without any MultiWOZ samples in its CoALM-IT training dataset, demonstrating its robustness in out-of-distribution (OOD) generalization."}, {"title": "Results on API-Bank and BFCL", "content": "COALM adeptly orchestrates function calls. Table 3 shows API-Bank scores to test model's API calling capabilities where Rouge-L is the primary evaluation metric. TOD models in the bottom row yield suboptimal results in this task. On the other hand, CoALM 8B achieves a Rouge-L score of 92.8 at Level-1 and 81.9 at Level-2, surpassing both TOD-oriented models and tool-centric LAs by a significant margin. It also achieves top performance on nearly all metrics. Moreover, we scale CoALM 8B accuracy with CoALM 70B and CoALM 405B models achieving top best and second best scores. This suggests that CoALM's balanced approach enables it not only to retrieve and call the correct API but also to generate precise responses grounded in the returned results, fulfilling complex user requests effectively.\nCOALM outperforms specialized LAs and GPT-40. We next assess function calling accuracy on BFCL V3 (Table 4). Models trained only for TOD or basic instruction-following underperform. While LAs like Hammer and ToolAce fare better, our smallest model CoALM 8B surpasses them (see Figure 3 for error analysis examples). Our larger scale models outperform GPT-40, GPT-40-mini and Llama-3.1-405B in overall accuracy. Remarkably, CoALM 405B achieves 100% accuracy on the relevance detection task, highlighting its agentic reasoning capabilities through hallucination. CoALM 405B stands as the top-performing fully open-source model on BFCL V3 leaderboard."}, {"title": "Domain Impact on Performance", "content": "Table 5 highlights the performance impact of CoALM-IT's fine-tuning components. Removing LA datasets significantly reduces function calling performance, with API-Bank Rouge-L1 dropping 47.3% and BFCL success falling 18.3%. Excluding the DST dataset leads to a notable decline in CoALM's JGA, dropping by 11.0% relative to CoALM and even underperforming base Llama by 6.9%. This underscores the essential role of fine-tuning on state tracking to capture user intents effectively. Finally, removing the GPT-4-generated CRA dataset has negative impact on MultiWOZ 2.4's Success metric, which plummets by 11.7%. Also, multi-turn function calling accuracy dropped in API-Bank, both in L1 and L2 metrics. This indicates that the CRA dataset is instrumental in developing coherent and contextually aware responses in multi-turn settings. However, JGA and BFCL's overall success see slight improvements, suggesting that certain specialized skills may benefit marginally in the absence of broader conversational reasoning. These results confirm that each dataset is crucial for balanced task performance, enabling CoALM to generalize effectively across different tasks without overfitting to one domain."}, {"title": "Conclusion and Future Work", "content": "In this work, we highlighted a critical gap between LA and TOD systems, where each excels in complementary capabilities - function calling and multi-turn conversation management, respectively. To solve this, we introduced CoALM, unified conversational agents that seamlessly integrates sophisticated API usage with natural multi-turn dialogue. Through fine-tuning on CoALM-IT with a hybrid fine-tuning strategy, CoALM achieves leading performance on both TOD and LA benchmarks, demonstrating that a single model can indeed master multi-turn conversations and tool use effectively.\nFuture work can investigate using reinforcement learning (RL) to generate large-scale interaction trajectories supported with API calls could further enhance the self-evolution of conversational agents through purely RL-based optimization. Another direction is, improving multi-turn function calling and user interaction abilities of these models, which remains a difficult problem with generally low accuracy. We believe that our findings, methodologies,"}, {"title": "Limitations", "content": "While CoALM demonstrates improved performance across both conversational TOD and agentic tasks, we conducted all experiments solely using the Llama model family, limiting our insights into other architectures like Mistral and Qwen. Furthermore, many TOD systems rely on classification-based supervised fine-tuning (DST-only), lacking free-form chat capabilities, so we are not able to integrate them in our chat-based evaluation setup for head-to-head comparisons. We also did not systematically assess CoALM's general reasoning abilities after post-training, leaving open the question of potential catastrophic forgetting if any. Even though we introduced the open source model CoALM 405B, the computational cost of doing inference with CoALM 405B requires 16 H100 GPUs, which may limit accessibility for some researchers. Lastly, our current approach still relies on curated fine-tuning data; future work might investigate self-evolving methods that learns complex function calling skills continuously leveraging RL."}, {"title": "Appendix", "content": "A Problem Formulation\nA.1 End-to-End TOD Systems with LLMS\nLLM-based end-to-end TOD systems generate contextually relevant responses based on dialogue history and task instructions. Let $F$ be a language model parameterized by $\\theta$, which maps an input context given as prompt $T$ to an output system response $Y_t$. At each dialogue turn $t$, the system receives three key components: task instructions $G$, dialogue history $H_t$ comprising of prior user-system interactions $\\{(u_1, Y_1), ..., (u_{t-1}, Y_{t-1})\\}$, and the current user input $u_t$. These elements are combined to form the complete prompt $T_t = (G, H_t, u_t)$. The model generates a response $y_t$ by modeling the conditional probability:\n$P(y_t | T_t; \\theta) = P(y_t | G, H_t, u_t; \\theta),$ (1)\nwhere $P(s_t | T_t; \\theta)$ denotes the probability of generating the response $y_t$ given the prompt $T_t$ and the model parameters $\\theta$. The dialogue progresses by updating the history after each turn $H_{t+1} = H_t + [(u_t, s_t)]$, maintaining the sequential nature of the interaction while preserving task orientation through $G$.\nA.2 Function Calling with Language Agents\nA language model $F_\\theta$ maps an input $x = (G, u, \\Omega)$, where $G$ is the task prompt, $u$ is the user query, and $\\Omega = \\{f_1,..., f_n\\}$ is the set of available functions with their arguments and descriptions to a structured function call $y$. The model generates target function call in a structured format, such as JSON or text schema. The generation probability is defined as:\n$P(y | x; \\theta) = P(y | G, u, \\Omega; \\theta)$ (2)\nThis formulation enables the model to translate natural language inputs into precise and well-structured function calls, facilitating seamless integration with external systems.\nReAct Prompting. ReAct (Yao et al., 2023) integrates reasoning and action-taking to enable more effective decision-making. It facilitates intermediate reasoning by breaking down complex tasks into smaller, interpretable reasoning steps. Additionally, it enables interaction with external tools or APIs by producing structured actions that integrate effectively with external systems. As a result of an API execution, ReAct incorporates observations dynamically, adapting subsequent reasoning and actions based on the results of previous steps, thus improving the system's responsiveness and overall task performance."}, {"title": "Details of the Evaluation Benchmarks", "content": "MultiWOZ 2.4. MultiWOZ 2.4 (Ye et al., 2022) is a multi-domain TOD dataset designed to evaluate dialogue systems' ability to handle complex conversations across multiple domains such as hotel booking, restaurant reservations, and transportation. We employ two different metrics during our TOD evaluations MultiWOZ: Success Rate, which assesses whether all user-requested information related to the entity is successfully provided and Joint Goal Accuracy (JGA) which measures the accuracy of predicted dialogue states, reflecting the system's ability to track user intents. During our zero-shot evaluations, we used its test set that contains 999 samples and incorporated AutoTOD prompt (Xu et al., 2024) with slight modifications, thereby generating system responses analogous to those produced in a chat-based inference setting.\nAPI-Bank. API-Bank (Li et al., 2023) is designed to evaluate tool-augmented LAs, focusing on their ability to plan, retrieve, and invoke APIs effectively. It includes 314 tool-use dialogues and 753 API calls, with two evaluation levels: Level 1 (L-1), which tests the accuracy of invoking a known API based on a given query, and Level 2 (L-2), which assesses the retrieval and invocation of APIs from a candidate list, simulating real-world scenarios with multiple API options. By addressing these challenges, API-Bank advances the understanding and enhancement of tool-augmented reasoning in LLMs. During evaluations, we used the official evaluation code from the repository of previous works (Lin et al., 2024).\nBerkeley Function Calling Leaderboard. In addition to API-Bank, we also used BFCL V3"}, {"title": "Baseline Model Overviews Used in Experiments", "content": "In this section, we provide an overview of the models used in our experiments, including their brief descriptions, checkpoints, and the training re-production code references.\nC.1 Base Models\nLlama 3.1. The Llama (Large Language Model Meta AI) (Dubey et al., 2024) family is a set of open-source language models from Meta AI, ranging from 7 to 405 billion parameters. It trained on a large corpus of web content, academic texts, and books, they excel at reasoning, question-answering, and code generation. Their architecture supports efficient fine-tuning and deployment. In our experiments, we use Llama-3.1-8B-Instruct, released in July 2024, which offers improved multilingual capabilities, longer context windows, and state-of-the-art performance in general knowledge, math, and tool usage\nMistral v03. Mistral 7B (Jiang et al., 2023) is one of the state-of-the-art, open-source LLMs produced by Mistral AI. It employs innovative mechanisms such as grouped-query and sliding window attention, which enable efficient processing of longer sequences and faster inference times. In our experiments, we use Mistral-7B-Instruct-v0.3, released on May 22, 2024, and available on Hugging Face.\nC.2 TOD Models\nLDST. LDST (LLM-driven Dialogue State Tracking) (Feng et al., 2023) is an approach that overcomes the limitations of proprietary models in state tracking by leveraging a fine-tuned LLaMa 7B model. The approach combines a novel assembled domain-slot instruction tuning technique with parameter-efficient strategies, enabling resource-efficient performance that tries matches larger models. During our experiments and to fine-tune LDST we used the provided checkpoints and implementation details for LDST are available in their public repository.\nFnc-TOD. FNC-TOD (Function-Calling for Task-Oriented Dialogue) focuses on DST in LLMs through function calling mechanisms. The method conceptualizes domain schemas as functions and embeds function specifications within the system prompt. This approach achieved improved conversation state tracking in task-oriented dialogues using a fine-tuned Llama-2-13b-chat-hf model, trained on a focused dataset of 7,200 task-oriented dialogues spanning 36 domains. For our experiments, we utilized the authors' publicly released Zekunli/FncTOD-Llama-13b model available on Huggingface.\nNC-Latent-TOD. This work introduces an unsupervised approach to TOD systems that operates solely with API schemas and unlabeled dialogues, eliminating the need for costly turn-level annotations. The system generates pseudo-labels for API calls and system actions while using a Hard-Expectation maximization approach with LLM predictions for iterative fine-tuning, enhanced by a noisy-channel reranking method (King and Flanigan, 2024). During our experiments, we used two different models nc-latent-tod-step-2-final and tod-zero-bqag3oyb-32000 shared by the authors.\nC.3 Language Agents\nCodeAct-Agent. CodeAct (Wang et al., 2024) is a framework that enables LLM agents to generate and execute Python code as actions to interact with environment, rather than being limited to JSON or structured text formats. By integrating a Python interpreter, it allows agents to dynamically adjust their actions based on execution results, leverage existing Python packages, and utilize programming constructs like loops and conditionals for complex operations. authors developed CodeActAgent by fine-tuning both Mistral 7B and Llama2 7B models on the CodeAct-Instruct dataset. For our experiments, we utilized the authors' officially released CodeActAgent-Mistral-7b-v0.1 model, available on Huggingface.\nGranite-20B. This work introduces Granite-20B, an open-source LLM, specifically designed for function calling capabilities. The model is trained using a multi-task approach on seven core function calling tasks: Nested Function Calling, Function Chaining, Parallel Functions, Function Name Detection, Parameter-Value Pair Detection, Next-Best Function, and Response Generation. We used the offical model weights granite-20b-code-instruct-8k provided in Huggingface.\nHammer2.0-7B. Hammer (Lin et al., 2024) is a small scale model family up to 7B parameter models designed for on-device function calling and addresses generalization challenges in function calling through two key innovations: an irrelevance-augmented dataset that enhances models' ability to identify inappropriate functions, and a function masking technique that reduces naming-based misinterpretations by focusing on function descriptions. Built by fine-tuning the xLAM-function-calling dataset with 7,500 additional instances for irrelevance detection, Hammer achieves state-of-the-art performance on BFCL Benchmark. For our experiments, we utilized the official Hammer 2.0 model weights available on Huggingface, along with training it from scratch for reproducibility using provided public repository and training scripts.\nToolAce 8B. This work introduces ToolACE (Liu et al., 2024), an automated pipeline for generating high-quality function-calling training data. The system features a self-evolution synthesis process that curates a pool of 26,507 diverse APIs, coupled with a multi-agent dialogue generation system and a dual-layer verification process for ensuring data accuracy. Using data generated and fine-tuning on Llama-3.1-8B-Instruct, ToolACE achieve top results on the BFCL Leaderboard. We used the official Huggingface checkpoint and dataset."}, {"title": "Human Validation for Generated CRA Dataset", "content": "To analyze the quality of generated conversations", "dimensions": "nUndefined Function Call: Validating API names and parameters against the predefined function list to identify undefined functions or invalid arguments.\nIncorrect Argument Type: Checking argument structures to ensure compliance with the function schemas.\nArgument Hallucination: Detecting unnecessary or irrelevant arguments misaligned with the conversation context.\nLow-Quality Reasoning and Planning: Identifying logical gaps in though steps or unnecessary API calls in ReAct structure.\nWe asked for a binary score (1 for no errors, 0 for detected issues) for each generated dialogue and provided mandatory feedback for any errors. Our evaluation of 100 dialogues"}]}