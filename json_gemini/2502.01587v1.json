{"title": "Verbalized Bayesian Persuasion", "authors": ["Wenhao Li", "Yue Lin", "Xiangfeng Wang", "Bo Jin", "Hongyuan Zha", "Baoxiang Wang"], "abstract": "Information design (ID) explores how a sender influence the optimal behavior of receivers\nto achieve specific objectives. While ID originates from everyday human communication,\nexisting game-theoretic and machine learning methods often model information structures\nas numbers, which limits many applications to toy games. This work leverages LLMs and\nproposes a verbalized framework in Bayesian persuasion (BP), which extends classic BP to\nreal-world games involving human dialogues for the first time. Specifically, we map the BP\nto a verbalized mediator-augmented extensive-form game, where LLMs instantiate the sender\nand receiver. To efficiently solve the verbalized game, we propose a generalized equilibrium-\nfinding algorithm combining LLM and game solver. The algorithm is reinforced with techniques\nincluding verbalized commitment assumptions, verbalized obedience constraints, and information\nobfuscation. Numerical experiments in dialogue scenarios, such as recommendation letters,\ncourtroom interactions, and law enforcement, validate that our framework can both reproduce\ntheoretical results in classic BP and discover effective persuasion strategies in more complex\nnatural language and multi-stage scenarios.", "sections": [{"title": "1 Introduction", "content": "Persuasion plays a significant role in modern economies, with estimates suggesting that up to\none-quarter (McCloskey & Klamer, 1995), or even 30% (Antioch, 2013) of GDP, is persuasion.\nThe study of BP has deep roots in economics, with numerous applications across fields such\nas school grading (Boleslavsky & Cotton, 2015), law enforcement deployment (Lazear, 2006),\nresearch procurement (Yoder, 2022), matching platforms (Romanyuk & Smolin, 2019), and routing\nsystems (Das et al., 2017). Various lines of theory have been proposed to explore the power of\npersuasion in different contexts (Kamenica, 2019).\nThis work investigates the Bayesian persuasion (BP) problem (Kamenica & Gentzkow, 2011;\nKamenica, 2019) between two agents: a sender and a receiver. Unlike cheap talk (Lo et al., 2023),"}, {"title": "2 Preliminaries", "content": "This section will introduce research areas related to BP as well as tools and methods relevant to the\ntechniques covered in the VBP framework."}, {"title": "2.1 Bayesian Persuasion", "content": "The canonical BP model is structured as follows (Kamenica, 2019). A receiver, an agent, has a\nutility function u\u2081(a,w), which depends on its action a \u2208 A and the state of the world \u03c9 \u2208 \u03a9.\nAnother agent, the sender (also known as the information designer), has a utility function u\u03bf(\u03b1,\u03c9).\nBoth the sender and receiver share a common prior \u03bc\u03bf over \u03a9. The sender's key decision is the\nchoice of a signaling scheme, which is a mapping from the state to a distribution over signals,\n\u03c0: \u03a9 \u2192 \u0394(S). Here S represents a sufficiently large set of signals, which is typically enough with\n|S| \u2265 min{|A|, |\u03a9|}, known as the revelation principal.\nGiven knowledge of \u03c0 (i.e., under the commitment assumption (Kamenica & Gentzkow, 2011)),\nthe receiver updates its belief from the prior \u00b5o to the posterior \u03bc\u03c0\u03c9 | s) using Bayes' rule.\nThe receiver then selects an action a* that maximizes Ew~\u03bc\u03c0(|s)U1(a,w). Given this response\nmechanism from the receiver, the sender's objective is to solve the following maximization problem:\n\u039c\u03b1\u03c7\u03c0\u03b5\u03c0 \u0388\u03c9~\u03bc\u03bf \u0395s\u223c\u03c0(\u03c9)Uo (a*,\u03c9), where I denote the set of all possible signaling schemes. Here the\nrevelation principle applies that an optimal signaling scheme exists that requires no more signals\nthan there are actions available to the receiver. From the receiver's perspective, as long as it\nbelieves that the recommended actions are optimal according to its posterior belief, it will follow\nthe sender's advice. These constraints on the sender's signaling scheme are referred to as obedience\nconstraints (Myerson, 1979; Kamenica & Gentzkow, 2011). Then, BP can be reduced to a simplified\nlinear programming (Lin et al., 2023),"}, {"title": "2.2 Learning Methods for BP", "content": "The problem of BP can be approximately solved by multi-agent reinforcement learning (MARL). In\nmixed-motive MARL, agents aim to advance their interests by shaping others (Leibo et al., 2017;\nMcKee et al., 2020; Dafoe et al., 2020; Leibo et al., 2021). Existing methods achieve this through\neither mechanism (modifying rewards) (Yang et al., 2020; Zheng et al., 2022; Hua et al., 2023; Wang\net al., 2024) or information design (modifying observations) (Wu et al., 2022; Bernasconi et al., 2022;\nLin et al., 2023), the latter of which could be used to solve BP. An sender can commit to a strategy\nfor providing state information to the agents, effectively altering the observation function of the\nreceiver. So the sender can influence agents' behavior by strategically providing information and\nguiding them toward desired outcomes (Bergemann & Morris, 2019). In a long-term interaction, the\nsender and the receiver become aware of each other's strategy, which creates the game dynamics that\nresemble the commitment assumption and therefore the BP problem setting. Then, the outcome of\nthe MARL algorithm becomes an equilibrium of BP."}, {"title": "2.3 Policy- and Prompt-Space Response Oracle", "content": "While game theory offers a mathematical framework to study interactions between multiple\nagents (Bighashdel et al., 2024), classical analysis struggles with scalability due to the sheer"}, {"title": "3 Related Works", "content": "The related fields primarily include the broader research area of deceptive behaviors in multi-agent\nlearning, the persuasive or persuadable capabilities of LLMs themselves, and the LLMs in strategic\ninteractions."}, {"title": "3.1 Game-Theoretic Solvers with LLMS", "content": "The combination of a game-theoretic solver with prompt optimization, which we use in this work,\nis not the only paradigm for utilizing LLMs to solve games. Widely adopted parameter-efficient\nfine-tuning (Xu et al., 2023; Han et al., 2024), agentic workflow (Mao et al., 2023; Hua et al., 2024;"}, {"title": "3.2 Deception in Multi-Agent Learning", "content": "Bond & Robinson (1988) defines deception as false communication that benefits the communicator.\nIn social learning, deception can be viewed as a means for the communicator to establish a\ncooperative equilibrium that is suboptimal for overall population welfare. Previous studies have\nexplored deception within multi-agent reinforcement learning (MARL) settings (Asgharnia et al.,\n2020; Bontrager et al., 2019; Li et al., 2020; Ghiya & Sycara, 2020), but these efforts typically\nfocus on environments where agents have limited capacity to influence one another. More recent\nwork (Chelarescu, 2021) highlights the vulnerability of agents dependent on signals from others\nto guide their learning processes, pointing to the potential risks inherent in such scenarios. While\nmuch research focuses on the positive outcomes of mechanism design, it also reveals unforeseen\nrisks, such as the emergence of deceptive behaviors (Hughes et al., 2018; Jaques et al., 2019; Yang\net al., 2020; Lupu & Precup, 2020; Ndousse et al., 2021). Unlike these prior studies, which primarily\nexamine how reward modifications influence deception through mechanisms like mechanism design,\nour work emphasizes the role of information manipulation in shaping deceptive behavior.\nGame-theoretic models traditionally frame deception using signaling (Ho et al., 1978), where one\nplayer can send costly signals to convey false information. In network security, for instance, Carroll\n& Grosu (2011) examined how defenders can deceive attackers by masking honeypots as regular\ncomputers. Other research has studied the evolution of deceptive signaling in mixed environments.\nFloreano et al. (2007) demonstrated that, in competitive food-gathering tasks, teams of robots\nspontaneously developed deceptive strategies, misleading competitors to reduce resource competition.\nAn extension to classical game theory, known as hypergame theory (Bennett, 1980), accounts for\nplayers' uncertainty about others' strategies or preferences, leading to disagreements about the\nunderlying game being played. By incorporating agents' differing perceptions, hypergame theory\nprovides a natural framework to model misperception, false beliefs, and deception (Kovach et al.,\n2015). Applications of hypergame theory include Vane & Lehner (2002), who analyzed deception in\nnormal-form hypergames, and Gharesifard & Cort\u00e9s (2013), who modeled deception based on player\npreferences when the deceiver has full knowledge of the target. Additionally, Ettinger & Jehiel\n(2010), Strouse et al. (2018), and Aitchison et al. (2021) show how agents can manage information\nabout their roles to achieve deception by regularizing mutual information between goals and states.\nIn contrast to these works, which model deception as discrete, explicit signaling actions, our study\nexplores how deception can be realized through natural language interaction.\nFinally, MacNally et al. (2018) addresses the broader question of how agents can communicate\nintent without explicit signaling, using an online planner to select actions that implicitly reveal\nintent to an observer. Masters & Sardina (2017) extended this approach to deception by maximizing\nthe divergence between the agent's and observer's beliefs. However, these methods assume full"}, {"title": "3.3 Conversational Persuasiveness of LLMs", "content": "Recent advancements in LLMs have shown their impressive potential in the realm of persuasion. A\ngrowing body of research highlights how these models can enhance human communicative abilities\nand even autonomously generate persuasive content across various contexts.\nFor instance, Shin & Kim (2024) demonstrated that refining complaint narratives with ChatGPT\nsignificantly improved consumers' chances of obtaining redress from financial institutions, showcasing\nthe role of LLMs in boosting human persuasive efforts. Similarly, Carrasco-Farre (2024) showed\nthat LLMs outperform humans in utilizing cognitive load and moral or emotional language when\ncrafting persuasive messages, prompting the need for ethical guidelines governing their use. Breum\net al. (2024) further explored LLMs' capacity to simulate persuasive dynamics, revealing that LLMs\ncan influence opinion changes in other LLMs with predefined personas. Building on this, Ramani\net al. (2024) introduced a multi-agent framework in which a primary agent engages users through\npersuasive dialogue, while auxiliary agents handle tasks such as information retrieval, response\nanalysis, and strategy development. These studies illustrate that LLMs are not only capable of\nenhancing human persuasion but also of autonomously refining and executing persuasive strategies.\nThe impact of LLM-generated persuasive text on human behavior has been demonstrated across\na diverse range of domains. For example, Bai et al. (2023) showed that GPT-3.5 could influence\npolitical attitudes, while Karinshak et al. (2023) found that GPT-3's vaccine campaign messages were\nmore effective than those created by professionals. Additionally, LLM-powered romantic chatbots\nhave been shown to sustain human engagement longer than human-to-human conversations (Zhou\net al., 2020). In strategic contexts, LLMs have achieved human-level negotiation capabilities in games\nlike Diplomacy (FAIR et al., 2022), and algorithmic suggestions have been shown to shape emotional\nlanguage in messaging (Hohenstein et al., 2023). These examples collectively highlight the broad\napplicability of LLMs in persuasive tasks and their significant influence on human decision-making.\nHowever, the increasing persuasive power of LLMs also raises concerns about potential misuse.\nSalvi et al. (2024) found that LLMs outperform humans in personalized debates, achieving a higher\nrate of belief change in one-on-one discussions. This raises ethical concerns, particularly regarding\nthe risks of misinformation and manipulation. For instance, M\u00e1jovsk\u00fd et al. (2023) demonstrated\nthat LLMs can convincingly fabricate medical facts, further complicating the ethical landscape. The\nability of LLMs to produce persuasive yet misleading content underscores the need for stronger\noversight, especially in high-stakes domains such as healthcare, politics, and public discourse. Recent\nstudies have thus emphasized the necessity of ethical frameworks as LLMs become more adept at\npersuasion. While LLMs have shown persuasive power across various tasks and domains (Matz\net al., 2024; Durmus et al., 2024; Burtell & Woodside, 2023; Shin & Kim, 2023), they also pose\nrisks, particularly for vulnerable populations. Bar-Gill et al. (2023) highlighted that characteristics\nsuch as race, gender, and sexual identity may subject certain groups to greater risks of algorithmic\npersuasion and bias, potentially exacerbating existing social inequalities.\nFrom a computational standpoint, Wojtowicz (2024) provided a novel proof showing that\ndiscovering persuasive messages is NP-hard, while adopting persuasive strategies provided by others\nis NP-easy. This insight adds to our understanding of the complexity involved in generating\npersuasive content and demonstrates why LLMs, with their vast data-processing capabilities, are\nparticularly adept at these tasks. Building on these insights, our work explores how game-theoretic"}, {"title": "3.4 LLMs in Strategic Interactions", "content": "Recent advances in large language models (LLMs) have showcased their potential in reasoning and\nplanning, particularly in strategic interactions. LLMs have demonstrated strong capabilities in\nin-context learning, allowing them to reason about possible outcomes (Kojima et al., 2022) and\nplan their actions to achieve strategic objectives (Liu et al., 2023). However, their performance\nin game environments can vary significantly depending on the type of game, as shown by Lor\u00e8 &\nHeydari (2023), where LLMs struggled in different ways across various games. To address these\nchallenges, Gandhi et al. (2023) introduced an automated \u201cprompt compiler\u201d that facilitates strategic\nreasoning by constructing demonstrations, enabling LLMs to solve games through in-context learning.\nSimilarly, FAIR et al. (2022) designed an action space of \u201cintents\u201d to control a generative language\nmodel, also leveraging in-context learning, which aligns closely with the approach taken in our work\nhere. Additionally, game-theoretic models have been employed to improve the factual accuracy of\nLLMs (Jacob et al., 2024) and enhance their security (Ma et al., 2023). For a broader overview of\nLLMs in strategic reasoning, Zhang et al. (2024b) provides a comprehensive survey.\nThe BP problem, however, goes beyond mere reasoning or planning. It requires the ability\nto anticipate and account for the intentions, beliefs, and goals of other participants-a hallmark\nof game-theoretic settings. While some initial studies have begun to explore how LLMs perform\nin game environments, most of this work focuses on leveraging in-context learning. For example,\nresearch has examined LLMs' behavior in matrix games (Xu et al., 2024; Fan et al., 2024), repeated\ngames (Akata et al., 2023; Zhang et al., 2024c; Huang et al., 2024; Silva, 2024), economic mechanisms\nlike auctions (Chen et al., 2023; Mao et al., 2023), and collective decision-making scenarios (Jarrett\net al., 2023). These studies collectively illustrate the potential of LLMs to navigate complex\nenvironments that require both strategic thinking and interaction with other agents.\nIn contrast to prior work that primarily evaluates LLMs' reasoning or game-playing capabilities\nthrough in-context learning or agentic workflows, our approach focuses specifically on solving the\nBP problem. Our key contribution lies in providing a general interface that integrates LLMs with\ngame-theoretic solvers to address BP problems effectively. Based on this interface, we propose a\nsolution framework called VBP, which combines prompt optimization with game-theoretic methods.\nThis framework offers a convergence guarantee to equilibrium solutions, ensuring robust performance."}, {"title": "4 The Mediator-Augmented Game Formulation for BP", "content": "To establish convergence for the VBP framework, we transform the classic BP problem into a special\nclass of extensive-form games (EFGs), known as mediator-augmented games (MAGs, Zhang &\nSandholm (2022)). In this section, we reformulate the BP problem in the form of an MAG. At a high\nlevel, a mediator-augmented game introduces an additional player, the mediator, who exchanges\nmessages with the players and provides action recommendations."}, {"title": "Definition 4.1.", "content": "A Bayesian persuasion problem, represented as a mediator-augmented game \u0393,\nconsists of the following components (Zhang & Sandholm, 2022): (1) a player, referred to as the\nreceiver, denoted by the integer 1; (2) a directed tree H of histories or nodes, with the root denoted\nby \u00d8. The edges of H are labeled with actions, and the set of legal actions at each node h is denoted\nby Ah. Terminal nodes of H are called leaves, and the set of such leaves is denoted by Z; (3) a\npartition of non-terminal nodes H\\Z into Hc\u2751 Ho\u2751 H\u2081, where H\u2081 represents the nodes where\nplayer 1 acts, and C and 0 represent chance and the mediator (i.e., the sender), respectively; (4) for\neach agent i \u2208 {1,0}, a partition Z\u1d62 of the decision nodes H\u1d62 into information sets. Every node in a\ngiven information set I must have the same set of legal actions, denoted by A\u1d62; (5) for each agent\ni \u2208 {1,0}, a utility\u00b9 function ui : Z \u2192 R; and (6) for each chance node h \u2208 Hc, a fixed probability\ndistribution c(\u00b7 | h) over Ah.\nAt any node h\u2208 H, the sequence oi(h) for agent i consists of all information sets (infosets)\nencountered by i, along with the actions taken at those infosets on the path from \u00d8 to h, excluding\nh itself. An agent has perfect recall if oi(h) = \u03c3i(h') for all h,h' within the same infoset. A\npure strategy for agent i specifies one action from Ar for each information set I \u2208 Z\u00a1. A mixed\nstrategy is a probability distribution over pure strategies, and the sequence form of a mixed strategy\ncorresponds to the convex combination of pure strategies. Let X\u2081 and Xo denote the polytope of\nsequence-form mixed strategies x\u2081 for the receiver and for the mediator, respectively.\nFor a fixed \u03c0\u2208 Xo, we say that (\u03c0,\u2081) is an equilibrium of \u0393 if x\u2081 is a best response to \u03c0,\nmeaning max \u2208\u03a7\u2081 41 (\u03c0, \u03a7\u2081) \u2264 41(\u03c0,x1). We do not require the mediator's strategy (signaling\nscheme) \u03c0 to be a best response; hence, the mediator can commit to its strategy. The objective\nof this paper is to find an optimal Stackelberg equilibrium, which is an equilibrium (\u03c0,x1) that\nmaximizes the mediator's utility uo(\u03c0, x1). When viewed as an extensive-form game (EFG), the\nevent sequence in BP is shown in Figure 2."}, {"title": "5 VBP Solver", "content": "This section will provide a detailed introduction to the VBP framework, as shown in Figure 3. The\nfirst part presents the verbalization of MAG, including a polarized setting (e.g. strong/weak students)\nthat reproduces the classic examples in BP, and general settings for one-step and multi-stage BP in\nthe language space. The second part introduces how this MAG is solved within the Prompt-PSRO\nframework, with the help of available best response approximations in language models (Yang et al.,\n2024; Romera-Paredes et al., 2024). The verbalized MAG, along with the three problem settings\nand the Prompt-PSRO-based game-theoretic solver, collectively constitute the VBP framework."}, {"title": "5.1 Verbalized Formulation for BP", "content": "To leverage the wealth of research in LLMs for BP in realistic scenarios, we must abstract and map\ncomponents of BP to the symbolic language. Note the mapping can be chosen is not unique.\n\u2022 State w. Unlike the classic BP, which only describes the state with binary values, the state in\nVBP is defined as the text. For example, it is the detailed description to the student's quality in\nthe recommendation letter problem.\n\u2022 Infosets I. The infoset is available only in the multi-stage setting. It is defined as the interaction\nhistory between the two parties. Specifically, this includes the signals sent by the sender, the\nreceiver's decisions (public information), and their respective rewards (private information) from\neach round of interaction, plus the previous environmental states (private information).\n\u2022 Action A. Agents' actions refer to the signaling scheme for the sender and the action for the\nreceiver. As we transform the strategy optimization problem into a prompt optimization problem\nthrough the Prompt-PSRO framework, the actions involve selecting prompts.\n\u2022 Terminal states Z. In static settings the game terminates in one step. In multi-stage settings\nterminal states are determined by either a limit or the allowable tree depth.\nIn addition to the basic components of the game, the BP problem also includes two fundamental\nconstraints that need to be mapped into the verbalized MAG.\nVerbalized Committment Assumption The key difference between BP and cheap talk (Craw-\nford & Sobel, 1982) lies in the presence of the commitment assumption, where the sender commits\ntheir signaling scheme as common knowledge. The VBP framework achieves the commitment\nassumption through prompting in the static setting and through expanding the receiver's infoset in\nthe multi-stage setting. Specifically, the signaling scheme is equivalent to the key components in\nthe prompt provided to the sender that influence the generated signals, and these components are\nthe target of Prompt-PSRO optimization. VBP incorporates these components into the receiver's\nprompt, where more details are deferred to Appendix C.4."}, {"title": "Verbalized Obedience Constraint", "content": "The optimization in BP involves an (extended) obedience\nconstraint (Myerson, 1979; Kamenica & Gentzkow, 2011; Lin et al., 2023), as shown in Equation 1.\nIt is intuitive to handle this constraint by transforming it into a penalty term, which is similar to\nreward shaping (Ng et al., 1999; Gupta et al., 2022). However, computing this penalty term requires\nintegrating over the entire state and action space. To address this, we estimate the summation\nterm using a sampling approach. Specifically, we calculate an estimate using the current state\nand an arbitrarily selected action. There are various ways to select actions, and here we introduce\na theory-of-mind approach (Rabinowitz et al., 2018; Albrecht & Stone, 2018), where actions are\nselected based on predictions of what the receiver would do, using a prompt to pretrained and\naligned LLMs to anticipate the receiver's likely actions."}, {"title": "5.2 Three Settings in VBP", "content": "Setting S1: Polarized signals Polarized signals refer to the constraint to produce more\nstraightforward signals, such as recommend v.s. not recommend in the recommendation letter\nexample. The goal of this setting is to aligning the signal space with the classic BP formulation. We\naim to reproduce the Stackelberg equilibrium in classic BP examples and validate the effectiveness of\nVBP. Specifically, we use the pretrained and aligned LLM to score the signals output by the sender,\nFor example, this determines the degree, a real value between 0 and 1, to which the recommendation\nletter supports the student. Similar prompts can be designed for other problems. Then this value is\npushed to an extreme point in the signal space, based on the minimum distance.\nSetting S2: VBP in Static BP By removing signal polarization, the signals are maintained in\nthe language space. This constitutes the default setting of VBP.\nSetting S3: VBP in Multistage BP This setting considers a multistage scenario, which is\nvery challenging for traditional methods (Gan et al., 2022; Wu et al., 2022). The agents engage in\nmultiple rounds of interaction, and the sender's historical signals serve as the basis for the receiver's\nsubsequent decisions. This largely increases the complexity, as the sender cannot arbitrarily exploit\nthe receiver with their information advantage. Instead, they must consider how their current actions\nmay impact future rewards."}, {"title": "5.3 Verbalized Game Solver", "content": "After modeling the BP as a verbalized MAG, we parameterize both agents using pretrained and\naligned LLMs and optimize their strategies with the Prompt-PSRO, thereby forming a general BP\nsolver. We first present the following proposition based on the theoretical foundation of Zhang &\nSandholm (2022), with the proof provided in Appendix A."}, {"title": "6 Experiments", "content": ""}, {"title": "6.1 Environments", "content": ""}, {"title": "6.1.1 Classic BP Problems", "content": "This section introduces the three classic static BP problems used in our experiments.\nRecommendation Letter (REL) (Dughmi, 2017) A professor writes recommendation letters\nfor graduating students, which are then reviewed by a company's human resources (HR) department\nto decide whether to hire. The professor and HR share a prior belief about the students' quality:\nthere is a 1/3 probability that a candidate is strong and a 2/3 probability that the candidate is\nweak. HR does not know the exact quality of each student but aims to hire strong candidates, using\nthe recommendation letters as the only source of information. HR receives a reward of 1 for hiring a\nstrong candidate, incurs a penalty of -1 for hiring a weak candidate, and gets 0 for not hiring. The\nprofessor, on the other hand, gains a reward of 1 for each student hired, regardless of their quality."}, {"title": "6.1.2 More Real-World Applications", "content": "Our proposed verbalized Bayesian persuasion (VBP) framework has significant potential for real-\nworld applications, particularly in complex, multi-sender, multi-receiver, and multi-round strategic\ncommunication scenarios. Below, we discuss two illustrative examples-conversational recommenda-\ntion systems and healthcare DRG strategies-and highlight the potential challenges in applying VBP\nto these domains.\nConversational Recommendation Systems One promising application of VBP is in con-\nversational recommendation systems, such as those used in live-stream shopping. In this setting,\nmultiple senders (e.g., influencers or sales agents) aim to persuade a diverse group of receivers\n(customers) to purchase products through real-time, strategic communication. The VBP framework\ncan optimize prompts (e.g., how product features or discounts are presented) to maximize customer\nengagement and conversions across varying customer segments. This application faces challenges\nsuch as receiver heterogeneity, where customers interpret signals differently based on their preferences\nand trust levels, making it difficult to craft universal strategies. Furthermore, the real-time nature\nof live-stream interactions demands highly efficient decision-making algorithms capable of adapting\ncommunication strategies dynamically. Scaling the system to accommodate thousands or millions of\nreceivers simultaneously also requires advanced parallel processing and optimization techniques.\nDRG Strategy in Healthcare Another practical application lies in healthcare, specifically in\noptimizing Diagnosis-Related Group (DRG) reimbursement systems. Here, hospitals and post-acute\ncare (PAC) providers (senders) communicate with regulatory agencies (receiver) to determine\nreimbursement policies for patient treatments. The VBP framework can model the incentives\nand communication strategies of the senders to help regulators design policies that balance cost-\neffectiveness with maintaining high-quality patient care. In this domain, conflicting incentives among\nsenders (e.g., hospitals vs. PAC providers) add complexity, as senders may compete or collaborate\nto influence the receiver's decisions. Additionally, the large scale of the problem, with thousands of\nproviders, poses computational challenges for efficient optimization. Long-term policy adjustments\nbased on multi-round feedback further complicate the problem, requiring robust mechanisms to\nhandle dynamic interactions over time.\nThese examples demonstrate the versatility of the VBP framework in addressing real-world\nproblems involving strategic communication. However, its application to practical scenarios requires\naddressing challenges such as scalability, heterogeneity of participants, real-time decision-making,\nand multi-round dynamics. Future work will focus on refining the VBP framework to overcome\nthese challenges and enhance its readiness for deployment in diverse real-world contexts."}, {"title": "6.2 VBP in Static Games (S1 & S2)", "content": "We first evaluate the effectiveness of the VBP method under the S1 setting. Two baseline methods\nare chosen: BCE and MARL. The former is based on the optimal equilibria computed in Lin\net al. (2023), Kamenica & Gentzkow (2011) and Kamenica (2019), while the latter is based on the\nmulti-agent reinforcement learning method proposed in Lin et al. (2023) for solving BP problems.\nAs shown in Figure 4, the VBP framework successfully captures the essence of solving BP problems,\nnamely, selectively withholding, obfuscating, or even deceiving about the true state, while also\nlearning when to fully disclose accurate information.\nNext, we removed signal polarization to make the sender's signals in each problem more reflective\nof real-world recommendation letters, courtrooms, and police deployment announcements, resulting\nin the S2 setting. Since existing BCE and MARL methods cannot solve this, we only compared\nVBP with the VBP variant from the S1 setting. The results are shown in Figure 5.\nFigure 5 shows that VBP's performance in S2 is roughly on par with S1, with a slight performance\ndrop. In both settings, VBP achieves optimal strategy performance in terms of the probability of\nhonesty. We speculate that this could be due to the alignment of the LLM used, which allows it\nto more easily converge to honest strategies, such as truthfully reporting the situation of a strong\nstudent, a guilty defendant, or a patrolled segment.\nTo quantify the proximity of policies of the sender and the receiver to the BCE, we employ\nexploitability as a measure. Exploitability (Lanctot et al., 2017) measures the distance of a joint"}, {"title": "6.3 VBP in Multistage Games (S3)", "content": "We also tested the effectiveness of VBP in a multistage scenario. Notably, the multistage BP\ndiffers from most of the literature. In existing works, the same sender was interacting with a new,\nshort-sighted receiver in each round. In this work, the receiver remains the same and can perceive\nthe interaction history, aligning more closely with the Markov signaling game (Lin et al., 2023).\nSince a closed-form solution for equilibrium cannot be computed, we record the average performance\nat each stage in Figure 8.\nIt is observed from the figure that VBP's performance shows a noticeable decline compared to\nS2, while it still manages to learn both appropriate deception and honesty. We also visualize the\nchanges in the sender's deception and honesty probabilities during training, as shown in Figure 6.\nSince the receiver can perceive the history, the sender's deceptive behavior goes through several\noscillations, reflecting a kind of bargaining dynamic (Nash et al., 1950; Nash, 1953; Maschler et al.,\n2013): The sender is initially leaning towards honesty, then discovering that deception maximizes\ngains, and later realizing that excessive deception triggers retaliation from the receiver, eventually\nconverging to a relatively low deception probability. Likewise, with the receiver having access to\nhistorical interactions, the sender demonstrated an upward trend in honest behavior compared to\nthe S2 setting, with truthfulness levels progressively increasing throughout."}, {"title": "6.4 Prompt (Strategy) Variation", "content": "This section presents the final converged meta-strategy in the S2 setting, as well as the relative\nchanges in the selection probabilities of each strategy (i.e., the prompts that influence writing style)\nthroughout the training process, as shown in Figure 9.\nFigure 9 presents the top 10 strategies with the highest selection probabilities in the final\nstrategy pool after Prompt-PSRO convergence. These probabilities represent the average likelihood\nof selecting each strategy from the pool, revealing the adaptation process of sender and receiver\nstrategies over iterations. The optimization process follows a hierarchical approach: first, OPRO\noptimizes the category of each prompt (e.g., \"Tone\"), then the specific content within that category.\nThe table columns in Figure 9 reflect this structure, with the first two columns showing optimized\ncategories and content, while the third and fourth columns display their probabilities. The fifth\ncolumn tracks how these probabilities evolve across iterations, highlighting the refinement of\nstrategies during the optimization process.\nTo reduce computational complexity, we prune the strategy pool to the top 10 prompts based\non selection probabilities. We conduct additional experiments to assess this pruning's effects, as\nshown in Figure 10.\nAdditionally, the probabilities in Figure 9 are computed as the average probability of selecting\neach prompt from the strategy pool across iterations, and the content (e.g., \"Positive\u201d) under the\ncategory (e.g., \"Tone\") is dynamically optimized rather than fixed."}, {"title": "6.5 Ablation Studies", "content": ""}, {"title": "6.5.1 Key Components", "content": "This section analyzes the impact of key design elements within the VBP framework on performance,\nprimarily including the verbalization of the commitment assumption, the obedience constraint,\nand the introduction of information obfuscation techniques to facilitate VBP convergence. The\\"}]}