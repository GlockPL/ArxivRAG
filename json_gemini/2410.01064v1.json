{"title": "TRUTH OR DECEIT? A BAYESIAN DECODING GAME ENHANCES CONSISTENCY AND RELIABILITY", "authors": ["Weitong Zhang", "Chengqi Zang", "Bernhard Kainz"], "abstract": "Large Language Models (LLMs) often produce outputs that \u2013 though plausible \u2013 can lack consistency and reliability, particularly in ambiguous or complex scenarios. Challenges arise from ensuring that outputs align with both factual correctness and human intent. This is problematic in existing approaches that trade improved consistency for lower accuracy. To mitigate these challenges, we propose a novel game-theoretic approach to enhance consistency and reliability during the decoding stage of LLM output generation. Our method models the decoding process as a multistage Bayesian decoding game. This ensures consistency through Correctness Alignment and enhances reliability via Ambiguity Calibration. The model dynamically converges to a consensus on the most reliable outputs and distinguishes {Valid, Specious} outputs without human feedback or additional training. Remarkably, our game design allows smaller models to outperform much larger models through game mechanisms (e.g. 78.1 LLaMA13B vs 76.6 PaLM540B), as well as integrating various LL strategies and models, demonstrating the potential of game-theoretic tools to improve the truthfulness and reliability of LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated extraordinary capabilities in tasks such as factual question answering, fact-checking, and open-ended text generation (Brown et al., 2020; Radford et al., 2021). However, as these generative models increase in complexity and scale, they not only enhance their generation capabilities but also develop a tendency to produce outputs that, while plausible, may be factually incorrect or subtly misleading (McKenzie et al., 2023). This \u201cspecious\" behavior \u2013 whether an inevitable artifact of the model's optimization process or an unintended hallucination (Banerjee et al., 2024; Bai et al., 2024) \u2013 poses a significant challenge, often outpacing the ability of human judgment to accurately assess the fidelity and truthfulness of the generated content (Leike et al., 2018). One direct approach is to optimize model outputs for legibility via human feedback (e.g., RLHF (Christiano et al., 2017; 2018; Saunders et al., 2022; Markov et al., 2023)). Human feedback, inherently constrained by limitations in interpretability (Singh et al., 2024) and hindered by the illegibility (Hendrik Kirchner et al., 2024) of AI-generated content, struggles to keep pace with increasingly complex reasoning (Casper et al., 2023; Leike et al., 2018). In light of these challenges, the reliability of generative models in collaborative and high-stakes decision-making remains deeply uncertain and we pose the question:\nHow can we efficiently ensure that LLM outputs are not only aligned with human intent but also valid, especially when human evaluation may overlook specious errors?\nTo answer this question and to address the challenges outlined in Fig. 1, we explore a game-theoretic approach by introducing a Verifier as a dynamic and rigorous evaluator, serving as a proxy for human judgement to systematically assess generators. The motivation for this approach is threefold: (1)"}, {"title": "2 A BAYESIAN DECODING GAME (BDG)", "content": "We define generative model decoding as a signaling game (\u00a72.1), where the generator and verifier iteratively exchange signals to align on the correctness of the generated outputs. We introduce a Bayesian Game for Generative Model Decoding (\u00a72.2) to counter potential pitfalls of an unconstrained equilibrium \u2013 where multiple equilibria can lead to suboptimal outcomes. We introduce No-regret Optimization for Separating equilibria (\u00a72.3) to minimize cumulative regret and to ensure that strategies converge towards a distinct separation. Finally, to ensure decoding is not only correct but also reliable, we introduce Constrained Optimization for Ambiguity Calibration (\u00a72.4), which refines candidate outputs based on a convex combination of correctness and disambiguity metrics."}, {"title": "2.1 PRELIMINARIES: SIGNALING GAME FOR GENERATIVE LANGUAGE MODELS", "content": "The essence of a signaling game (Gibbons et al., 1992) is that one player (the generator) takes an action, the signal, to convey information to another player (the verifier); in the simplest setup, the final payoff depends on whether the verifier correctly judges the generator's type based on the generator's signal. Following this intuition, (Jacob et al., 2024) design a Equilibrium Consensus Game (ECG), without a formal definition of the game. Thus, we provide a comprehensive game-theoretic formulation for generative model decoding, and propose improvements to address limitations.\nFormally, the signaling game's components can be defined as: (1) Players: Generator and Verifier; (2) Choice sets: Generator's choice set is $y \\in C_G = \\mathcal{Y}$, with prompt $p$ randomly drawn from {Correct, Incorrect}, and the Verifier's choice set is $v \\in C_V = \\{\\text{Correct, Incorrect}\\}$, based on the generator's choice $y \\in \\mathcal{Y}$; (3) Payoff Function: $u_G = u_V = 1_{p=v}(p, v)$, where $1$ equals 1 if the correctness prompt $x$ matches the verification result, and 0 otherwise.\nWe are now ready to state the fundamental concept of this signaling game, a Perfect Bayesian Nash Equilibrium (PBNE) (Cho & Kreps, 1987). We use the short form Perfect Bayesian Equilibrium (PBE) with the auxiliary definitions Defi. 6. and 7. for PBE Definition."}, {"title": "2.2 GAME FORMULATION: A BAYESIAN GAME FOR GENERATIVE MODEL DECODING", "content": "The game converges towards a desirable outcome following equilibrium for the problem at hand:\nDefinition 3. (Decoding Game) The Decoding Game is an alternative version of the signalling game in \u00a72.1, and its payoff is determined by the preference ordering of each player, $O_i \\in \\mathcal{S}_{\\mathcal{Y},i} \\in \\{G,V\\}$, where $n$ is the cardinality of the candidate set $\\mathcal{Y}$ and $\\mathcal{S}_n$ is the set of all permutations of elements in $\\mathcal{Y}$. We define the utility of the decoding game as\n$u_G(O_G, O_V) = u_V(O_G, O_V) = 1_{\\{O_G = O_V\\}}(O_G, O_V)$,\nsuch that $1_{\\{O_G = O_V\\}}(\\cdot,\\cdot)$ is the indicator function at $O_G = O_V$. $O_i$ is the preference relation indicated by players' strategy, $a_G(y \\mid x, \\text{correct}, b_s), a_V(\\text{correct} \\mid x, y, b_v)$, such that\n$a_G(y_i \\mid x, \\text{correct}, b_s) \\geq a_G(y_j \\mid x, \\text{correct}, b_s) \\Leftrightarrow y_i \\succeq_G y_j$\n$a_V(\\text{correct} \\mid x, y_i, b_v) \\geq a_V(\\text{correct} \\mid x, y_j, b_v) \\Leftrightarrow y_i \\succeq_V y_j$.\n$b_g = b_g(y \\mid x, \\text{correct})$ is the generator's belief of the probability of $y$ being judged correctly by the verifier, and $b_v = b_v(\\text{correct} \\mid x, y)$ is the verifier's belief of the probability of $y$ being associated with the correct environment signal received by the generator. $a_g, a_v$ are the actions for the generator and verifier, respectively.  With the preference relation, we determine $O_G, O_V$, and we call the equilibrium for the above game a Decoding Equilibrium (DE)."}, {"title": "2.3 CORRECTNESS ALIGNMENT: NO-REGRET OPTIMIZATION FOR EQUILIBRIUM", "content": "No-Regret Optimization. Based on the Decoding Game (\u00a72.2), we propose two strategy update schedules to numerically achieve optimal convergence of $\\sigma$-DE in Thm. 1. The multiplicity of DE\nThere is a difference between belief and action: the belief is the player's belief in the opponent's action."}, {"title": "2.4 DISAMBUGUITY CALIBRATION: OPTIMIZATION VIA A DISAMBIGUITY METRIC", "content": "Our game efficiently approximates the $\\sigma$-DE, which is accomplished solely based on the correctness judgment of both the generator and the verifier. While correctness in LLM-generated text is undoubtedly the most important metric, the ease with which this correctness can be verified has been largely overlooked. (Hendrik Kirchner et al., 2024) tapped this problem where they reasoned the property of Legibility of LLM-generated content as whether the correctness is easily verifiable by humans. They emphasise that legibility during training must sacrifice some correctness.\nDisambiguity Metric. We propose a Disambuguity Metric and Reliability Score to achieve a robust decoding scheme that fully incorporates both correctness alignment and ambiguity detection without training. First, we define a Disambiguity Metric:\nDefinition 4. (Disambiguity Metric) For a prompt $x$ and a finite set of answer candidate $\\mathcal{Y}$, a Disambiguity Metric is a function that projects the prompt $x$ and a candidate $y_i \\in \\mathcal{Y}$ such that DA$(x, y_i): x \\times \\mathcal{Y} \\rightarrow [0,1]$. The output of the function is a measurement of the disambiguation of $y_i \\in \\mathcal{Y}$ to the prompt $x$. The disambiguity metric, combined with the correctness parameter, can detect false-positive and false-negative classifications in the correctness alignment phase.\nTherefore, for the elements in $\\mathcal{Y}$, we assign the correctness parameter from the previous stage as $c(y_i)$, which is the probability distribution of $P_{LM}(y \\mid x, \\text{correct})$ condition on that the DE is reached. Similarly, we denote the disambiguity metric for $(x,y_i)$ as $DA(x, y_i)$. For any $y_i, y_j \\in y$, we have $c(y_i) > c(y_j) \\cup c(y_i) \\leq c(y_j)$ and this also applies for the disambiguity metric DA$(x, y_i), DA(x, y_j)$.\nDisambiguity Maximization. We introduce the metric Rel$(x, y_i) = \\eta\\cdot DA(x, y_i) + (1 - \\eta)c(y_i)$, reliability, for $\\eta \\in [0,\\overline{\\eta}]$, $\\eta < 1$. (1 \u2013 $\\eta$) is the least proportion of correctness that needs to be considered. This is a convex combination of correctness and disambiguity, thus the maximization problem is defined as: $\\max \\eta \\text{ s.t. } \\min \\text{Rel}(y_{i,c}) \\geq \\max \\text{Rel}(y_{i,I}), \\eta < \\overline{\\eta}$.\nIntuition 1. The solution to the maximization problem, denoted as $\\eta^*$, is the value that for any $\\eta$ such that $\\eta^* < \\eta < \\overline{\\eta}$, the preference ordering is different from when $\\eta < \\eta^*$ which indicates the candidates whose relations with others are altered are the specious candidates.\nOnce we identified the specious candidates, the candidates that are the most preferred and the least preferred are the valid candidates. The Reliability of a disambiguity metric can now be defined for a prompt-candidate set (x, y).\nTheorem 4. A prompt-candidate set (x, y) couple can be made more Reliable by a Disambiguity Metric if such a $\\eta^*$ exist for the maximization problem $\\max \\eta \\text{ s.t. } \\min \\text{Rel}(y_{i,c}) > \\max \\text{Rel}(y_{i,I}), \\eta < \\overline{\\eta}$. If such a maximal $\\eta$ does not exist, then we say that the prompt-candidate set cannot be made more Reliable by Disambiguity Metric."}, {"title": "3 EXPERIMENTS", "content": "We aim to answer the following questions: (1) What design choices enable decoding games to improve language generation performance? (2) To what extent does our BDG improve consistency? (3) To what extent does the BDG improve factual validity and reliability? BDG focuses on improving the consistency and reliability of LLMs. However, consistency and reliability manifest themselves in various forms across different domains and dimensions, including correctness, truthfulness, factuality, valid reasoning, value alignment, among others. In (\u00a73.1), we first assess efficiency and reliability through a multidimensional comparison with another game-theoretic method (Jacob et al., 2024) and several variants. In (\u00a73.2), we evaluate performance on a diverse set of LLMs used for real-world tasks: MMLU (Hendrycks et al., 2020b), ARC-Easy (E.), -Challenge (C.) (Clark et al., 2018), RACE-High (H.) (Lai et al., 2017). It is important to note that BDG is a game-theoretic decoding strategy and not a deliberation/training-based method like a prover-verifier-game (PVG) (Hendrik Kirchner et al., 2024), or contrastive-objective based generation (Li et al., 2022). Nevertheless, we will demonstrate through benchmarks in reasoning task: GSM8K (Cobbe et al., 2021), medical taks: PubMedQA (Jin et al., 2019), MMLU-Medical (M.), and ethical scenarios, including justice, virtue, deontology and utilitarianism in Ethics (Hendrycks et al., 2020a), that BDG yields significant improvements and demonstrates synergistic potential across various scenarios (\u00a73.3).\nActions in the Game. As noted in \u00a72, to adapt BDG to existing methods, a generator in the modeling picks a distribution over a finite set of candidates $\\mathcal{Y}$. E.g., in multiple-choice tasks, these are the multiple choice options. In generative tasks, a common approach to generate the finite set of candidates is via sampling with nucleus (Holtzman et al., 2019) and top-k (Fan et al., 2018) from the distribution $P_{LLM}(y \\mid q, \\text{correct})$ where $y \\in \\mathcal{Y}$.\nBaselines and Models. For fair comparisons, following the setting and scores (Jacob et al., 2024), we use LLaMA models (Touvron et al., 2023) (7B, 13B parameters) with 16-bit inference across all experiments unless otherwise specified. On multiple-choice datasets, we employ: Generative Ranking (G): Ranks candidates by $P_{LLM}(y \\mid x, \\text{correct})$ following (Brown et al., 2020; Touvron et al., 2023). Discriminative Ranking (D): Re-weights query-candidate pairs using $\\pi_G^{(1)}(\\text{correct} \\mid x, y)$ based on (Jacob et al., 2024). Self-Contrastive Decoding (SCD): Utilizes $\\pi_G^{(1)} = \\pi_D$ for reweighting candidates (Jacob et al., 2024; Li et al., 2022). Equilibrium Consensus Game (ECG): Reweights pairs with equilibrium discriminator $\\pi(x, y)$ by $\\pi(\\text{correct} \\mid x,y)$ (Jacob et al., 2024). And BDG-based discriminator $\\pi(x, y)$ by $\\pi(\\text{correct} \\mid x, y)$ to reweight query-candidate pairs.\nPrompting. Unless otherwise specified, the condition for the $P_{LLM}$ corresponds to the standard zero-shot prompt (Jacob et al., 2024; Hendrycks et al., 2020b). Furthermore, we combine chain-of-thought (CoT) (Wei et al., 2022), and few-shots setting (Wei et al., 2022) as orthogonal analysis."}, {"title": "3.1 GAME-THEORETIC DESIGN", "content": "Searching & Convergence Behavior.\nWe first compare searching behaviors of BDG with the most closely related method, the ECG (Jacob et al., 2024), in the multiple-choice question answering (MCQA) task (Clark et al., 2018). Fig.3 provides a visual case study. BDG demonstrates a swift and consistent convergence in (b).\nConversely, the ECG, shown in (c), exhibits prolonged and inconsistent searching behavior. Despite continuous shifts in candidate selections, ECG fails to achieve stable convergence with persistent disagreement between the generator and verifier. (d) and Tab.1 highlights the enhanced and fast convergence properties of the BDG over the ECG.\nEntropy & Equilibrium Convergence. The staged convergence of BDG with 500 epochs is shown in Fig. 4, which first stabilizes on one solution before shifting to another. This leads to rapid convergence with our no-regret optimization. Compared to the ECG in Fig. 4 (a.2), this phase shows a stabilization in policy entropy, which signifies the model's swift approach to a potential equilibrium. The subsequent shift in policies, resulting in entropy changes, reflects the exploration of multiple PBEs inherent in signaling games. As the BDG navigates these equilibria, entropy fluctuations rep-"}, {"title": "3.2 CONSISTENCY BENCHMARKING: ACROSS DOMAINS WITH SMALLER MODELS", "content": "With \"relatively easy\" reasoning and comprehension tasks, we show superior performance compared to baselines and other game-theoretic methods in Tab. 3 due to the efficient alignment of consistency. In a broader comparison, our zero-shot LLaMA-13B (78.1, ARC-E.) outperforms much larger models like the PaLM-540B model (76.6) (Chowdhery et al., 2023).\nWith more challenging reasoning and multitask understanding tasks, such as ARC-C, RACE-H, and MMLU, we achieve the best equilibrium decoding with fewer rounds and higher accuracy. Our LLaMA-13B (46.9, MMLU; 57.7, RACE-H.) outperforms zero-shot GPT-3-175B (37.7, MMLU) (Hendrycks et al., 2020b), LLaMA-65B (51.6, RACE-H.) (Touvron et al., 2023), and PaLM-540B (49.1, RACE-H.) (Hendrycks et al., 2020b)."}, {"title": "3.3 RELIABILITY GAINS: ORTHOGONAL ENHANCEMENTS FOR ROBUST DECODING", "content": "Datasets in Tab. 4, 5 involve challenging scenarios to test models' reasoning and reliability abilities. We use these benchmarks to study whether we can combine our approach with various orthogonal strategies. As a decoding strategy based on game theory, BDG does not conflict with the computationally intensive game mechanism during training, nor does it conflict with CoT and few-shots based on prompting engineering. We can achieve the improved performance in more challenging scenarios and is an extremely novel decoding research direction with reliable performance. We are also able to achieve wider accuracy and reliability on ethical datasets."}, {"title": "4 DISCUSSION", "content": "Game Design over ECG and PVG. BDG and ECG share the common goal of aligning generative models with human intentions to improve output reliability, yet they differ significantly in their game design, achieving substantial gains with reduced computational overhead. While ECG utilizes moving-average updates to foster consensus, often leading to unstable and fluctuating equi-"}, {"title": "5 RELATED WORK", "content": "Multi-Agent Debate Frameworks. Previous work has explored mechanisms where multiple language model instances \u201cdebate\" to refine and converge to a final answer (Du et al., 2023; Chen et al., 2023; Khan et al., 2024). It is possible to see our method as a major variant of this multi-agent debate in which the interaction occurs within a game-theoretic framework, rather than directly within the language models' outputs. This structured signaling game enables BDG to enhance the correctness and reliability of outputs without relying on human feedback, by dynamically optimizing the generation and verification processes. Additionally, this approach can resolve ambiguity, confusion, and low accuracy caused by inconsistencies, but not by poor reasoning.\nDecoding Strategies. Top-k sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2019), and typical sampling (Meister et al., 2023) focus on generating high-confidence text but do not address the correctness of the outputs. Candidates were generated using these methods. Equilibrium-ranking (Jacob et al., 2024) applies an average-moving strategy to the initial distribution. In contrast, BDG integrates a multistage signaling game that inherently balances correctness and consistency during the generation process. BDG can be seamlessly combined with these strategies to enhance the reliability and reliability of generated text.\nRanking Techniques. Rranking is a widely used approach to select the correct output from a set of candidates generated by language models. (Thoppilan et al., 2022) use additional human annotations to train a ranking model for response filtering. (Hendrik Kirchner et al., 2024) trains different provers and verifiers for increasing output legibility. Although our work also utilizes existing language models as discriminators, BDG eliminates the need for additional training and does not impose specific assumptions on the structure of either the generator or discriminator."}, {"title": "6 CONCLUSION", "content": "The Bayesian Decoding Game (BDG) is a novel game-theoretic framework that enhances both the consistency and reliability of LLMs. By framing the decoding process as a multistage signaling"}, {"title": "A GAME-THEORETIC FORMULATION SUPPLEMENTARY", "content": "A generative language model (LM) maps input x to output y according to some distribution $P_{LM} (y | x)$. Here, we do not impose restrictions on the form of input or output, as illustrated in Fig.1. Instead, we address a multi-faceted problem involving a question x and a set of answer candidates Y, generated by pre-trained language models on specific tasks. In the first stage, using this candidate set, we leverage generative LMs in two distinct ways:\nGeneratively, by supplying as input\n1. a prompt x,\n2. the set of candidates y, and\n3. a natural language prompt indicating that a correct or incorrect answer is desired. The LM may be thought of as modeling a distribution $P_{LM}(y | x, \\text{incorrect})$, where the token incorrect denotes the fact that the model was prompted to generate an incorrect answer.\nVerifiably, by supplying as input\n1. the same x and\n2. a possible candidate answer $y \\in \\mathcal{Y}$, together with\n3. a prompt indicating that a correctness assessment $v \\in \\{\\text{correct}, \\text{incorrect}\\}$ is sought. In this case, the language model acts as a models a distribution $P_{LM}(v | x,y)$ where $v \\in \\{\\text{correct}, \\text{incorrect}\\}$.\nThe essence of a signaling game (Gibbons et al., 1992) is that one player (the generator) takes an action, the signal, to convey information to another player (the verifier); in the simplest setup, the final payoff depends on whether the verifier correctly judges the generator's type based on the generator's signal. Based on this intuition from game theory, (Jacob et al., 2024) design a Equilibrium Consensus Game (ECG), without a formal definition of the game. Thus, we firstly provide a comprehensive game-theoretic formulation for generative model decoding, and propose improvements to address limitations.\nFormally, the signaling game's components can be defined as follows:\n1. Players: Generator and Verifier;\n2. Choice sets: Generator's choice set is $y \\in C_G = \\mathcal{Y}$, with prompt p randomly drawn from {Correct, Incorrect}, and the Verifier's choice set is $v \\in C_V = \\{\\text{Correct, Incorrect}\\}$, based on the generator's choice $y \\in \\mathcal{Y}$;\n3. Payoff Function: $u_G = u_V = 1_{p=v}(p, v)$, where 1 equals 1 if the correctness prompt x matches the verification result, and 0 otherwise.\nWe are now ready to state the fundamental concept of this signaling game, a Perfect Bayesian Nash Equilibrium (PBNE) (Cho & Kreps, 1987). We use the short form Perfect Bayesian Equilibrium (PBE) with the auxiliary definitions Defi. 6. and 7. for PBE Definition."}, {"title": "B PROOFS OF THEOREMS", "content": ""}, {"title": "B.1 PROOF OF THEOREM 1", "content": "Theorem 1. More than one (mixed) strategy Perfect Bayesian Equilibrium exists for this game.\nProof of Theorem 1.:\nSuppose that the candidate set has 2 options (can be extended to any cardinality $\\mathcal{Y}$), $y_1, y_2$, one equilibrium can be described as: If the environment sends correct/incorrect, the generator generates the probability distribution (1,0)/(0, 1) for $(y_1, y_2)$ given his belief that verifier probabilistic judgment, {correct, incorrect}, for $y_1, y_2$ is (1,0), (0, 1).\nFor the verifier, he believes that if the environment chooses correct/incorrect, then he believes that of generator's probabilistic generation for $(y_1, y_2)$ are (1,0), (0, 1), therefore the verifier's best response is given by (correct, incorrect) = (1, 0) if sees $y_1$, (correct, incorrect) = (0, 1) if sees $y_2$. The (action and belief) for the generator and verifier above constitute one PBE for our game. For another equilibrium, we can revert every 0s and 1s in the above strategy profile, for all the actions and the beliefs."}, {"title": "B.2 PROOF OF THEOREM 2", "content": "Theorem 2. There are n! number of Equilibria for the Decoding Game.\nProof of Theorem 2.:\nSince there are n! permutations of elements in $\\mathcal{Y}$, there are n! DEs for the decoding game.\nTo accommodate the refined utility we proposed, we introduce another definition such that if\n$\\||a_G (y|x, \\text{correct}, b_s) - a_V(\\text{correct} | x, y, b_v)\\|| < \\sigma$\nfor every y, where $a_{V_y}$ is the normalized probability distribution of the verifier's action, we call it a $\\sigma$-DE."}, {"title": "B.3 PROOF OF THEOREM 3", "content": "Theorem 3. The Markovian update schedule for our Decoding Game will converge to an equilibrium.\nProof of Theorem 3.:\nWe will show that the Markovian update schedule is in fact no-regret(thus guarantees CCE-convergence) for correct generator, and when generator receives incorrect signal, she will automatically perform the reversed action; then, if the Markovian update schedule converges to CCE for the incorrect signal, it automatically satisfies that the Markovian schedule will converge to a Bayes-CCE of our Decoding Game."}, {"title": "C FROM TRAINING-FREE BAYESIAN DECODING GAME (BDG) \u03a4\u039f RL-BASED PROVER-VERIFIER GAME(PVG)", "content": "Prover-Verifier Game (PVG) (Hendrik Kirchner et al., 2024), structured as zero-sum games, encounter substantial challenges that undermine their efficacy in ensuring reliable outputs. The adversarial nature of zero-sum games inherently prioritizes winning over mutual consistency, which leads to strategic behavior focused on exploiting the opposing agent rather than achieving genuine correctness e.g., model collapse. This often results in provers generating outputs that are optimized to mislead the verifier rather than to align with factual truth, thus producing equilibria that favor strategic manipulation over accurate assessment. Such dynamics complicate the training process, requiring extensive tuning and computational resources without guaranteeing robust, interpretable results. Furthermore, the reliance on reinforcement learning in these systems falls short of effectively replacing human feedback, as the trained verifier cannot fully replicate the nuanced judgment required to evaluate complex or ambiguous output. These limitations fall into the misalignment between training objectives and practical needs, where models become adept at adversarial optimization but lack the reliability and consistency necessary for real-world application. However, Bayesian Decoding Game (BDG) can bring the advantages of the game and bridge the purposes with proper implementations, which bypass the extensive training and adversarial pitfalls by directly modeling output verification through probabilistic reasoning, enhancing the interpretability and reliability of the generated content without the dependencies on zero-sum competition.\nHere, we consider the connections between the PVG and BDG, and before the comparison, we give a brief introduction of PVG.\nWe consider a scenario of problems with ground-truth answers $(x, y) \\in \\mathcal{X} \\times \\mathcal{Y}$, where $x$ denotes a problem statement and $y = y(x)$ denotes the mapping from the prompt $x$ to ground-truth answer or set of answers. PVG has set the prover's type to be drawn from a known distribution, which can be either honest or sneaky, to produce an answer z condition on the prompt $x$ and type; such that the honest prover is told to produce correct answers and make sure the verifier recognize the correctness, whereas the sneaky prover is told to trick the verifier by providing wrong answers that are not easy to be detected; then, after receiving solution, the verifier verifies the answer based solely on the answer and prompt, and the output $v(x, z) \\in \\{0,1\\}$ estimates the correctness of the solution's answer. The output of the verifier is either 1(the statement is correct) or 0(the statement"}, {"title": "D FROM MEMORYLESS BAYESIAN DECODING GAME (BDG) \u03a4\u039f MOVING-AVERAGE EQUILIBRIUM CONSENSUS GAME (ECG)", "content": "The moving average update schedule proposed by Jacob et al. (2024) requires both the generator and the verifier to keep track of the average action of the opponent in addition to the action in the last round, while our Markovian framework allows the players to be memoryless. To better compare ECG with our update schedule, we provide a general, unifying framework called the History window schedule, where the player's belief is given by the average of past history actions for the period n, and at the same time, this schedule retains a large part the initial policy for each round with a stiffness"}, {"title": "E EXPERIMENT DETAILS", "content": "Baselines and Models. For the fair comparision following(Jacob et al., 2024), we use the same public 7B and 13B parameter models from the LLaMA family(Touvron et al., 2023) and perform 16-bit inference for all our experiments. Since we have a multi-round optimization game and in order to distinguish consensus/ zero-sum games, we define ours as a verifier rather than a discriminator. Across the experiments, all the approaches and orthogonal techniques involved:\n\u2022 Generative Ranking (G): The baseline(Brown et al., 2020; Touvron et al., 2023) ranks every candidate y by $P_{LLM}(y|x, \\text{correct})$ and picks the top candidate. This is the standard approach used in past work. Due to implementational differences and non-public resources, we report the existing scores in (Jacob et al., 2024).\n\u2022 Discriminative Ranking (D): Following(Jacob et al., 2024), this approach reweights every query-candidate pair $(x, y)$ by $\\pi_G^{(1)}(\\text{correct} | x,y)$. Typically, this would surpass the performance of ordinary individuals, who might neglect to notice the ambiguity errors. And outstrip the generators that might trust the unreliable decodings.\n\u2022 Mutual Information Ranking (MI): The mutual-information based baseline reweights every candidate y by $P_{LLM}(y | x, \\text{correct}) P_{LLM}(\\text{correct} | x, y)$ (Li & Jurafsky, 2016).\n\u2022 Self-Contrastive Decoding (SCD): The contrastive-based method (Jacob et al., 2024; Li et al., 2022) utilizes the contrastive-based generator $\\pi_G^{(1)}$ to reweight every candidate y by $\\pi_G^{(1)}(\\text{correct} | x, y)$. This method achieves a contrasting effect by comparing negative samples instead of employing a verifier (in BDG)/ discriminator (in ECG)."}, {"title": "F REPRODUCIBILITY STATEMENT", "content": "We conducted our evaluations using widely recognized benchmarks such as ARC-Easy, ARC-Challenge, MMLU, and RACE. The experiments were performed using the open-source LLaMA 7B and 13B models. Key aspects of the game, including update policies and initial strategies, are thoroughly detailed in both the main text and appendix to facilitate accurate replication of the results. All experiments were conducted on NVIDIA A6000 and A100 GPUs, with runtimes ranging from 0.5 to 6 hours depending on the model size, task, and experimental settings. Further details on the game-theoretic mechanisms and specific design choices can be found in the methods section and the appendix."}, {"title": "G POTENTIAL ETHICS RISKS AND SOCIETAL IMPACT", "content": "Bayesian Decoding Game (BDG) is a novel game-theoretic framework that significantly enhances both the consistency and reliability of large language model outputs. By framing the decoding process as a multistage signaling game between a generator and verifier, BDG efficiently aligns model outputs with human intent while mitigating the trade-off between correctness and reliability. BDG ensures reliable and robust LLM outputs, offering a scalable, training-free solution to the challenges of ambiguity and inconsistency in generative models.\nWith the improvement of generation quality, one can imagine more potent disinformation (e.g., automatic generation of fake news) that may be hard to distinguish from human-authored content. It might be worthwhile to augment current decoding techniques so that the generated outputs will also be watermarked without compromising their quality."}, {"title": "H HUMAN EVALUATION", "content": "Setting. In this experiment", "stages": "nIn the first stage, participants were given two minutes to classify as many answers as possible, and their results were recorded. In the second stage, participants were allowed to allocate their time freely to complete the remaining"}]}