{"title": "Prospective Learning: Learning for a Dynamic Future", "authors": ["Ashwin De Silva", "Rahul Ramesh", "Rubing Yang", "Siyu Yu", "Joshua T. Vogelstein", "Pratik Chaudhari"], "abstract": "In real-world applications, the distribution of the data, and our goals, evolve\nover time. The prevailing theoretical framework for studying machine learning,\nnamely probably approximately correct (PAC) learning, largely ignores time. As a\nconsequence, existing strategies to address the dynamic nature of data and goals\nexhibit poor real-world performance. This paper develops a theoretical framework\ncalled \"Prospective Learning\" that is tailored for situations when the optimal\nhypothesis changes over time. In PAC learning, empirical risk minimization (ERM)\nis known to be consistent. We develop a learner called Prospective ERM, which\nreturns a sequence of predictors that make predictions on future data. We prove that\nthe risk of prospective ERM converges to the Bayes risk under certain assumptions\non the stochastic process generating the data. Prospective ERM, roughly speaking,\nincorporates time as an input in addition to the data. We show that standard ERM\nas done in PAC learning, without incorporating time, can result in failure to learn\nwhen distributions are dynamic. Numerical experiments illustrate that prospective\nERM can learn synthetic and visual recognition problems constructed from MNIST\nand CIFAR-10. Code at https://github.com/neurodata/prolearn.", "sections": [{"title": "1 Introduction", "content": "All learning is for the future. Learning involves updating decision rules or policies, based on past\nexperiences, to improve future performance. Probably approximately correct (PAC) learning has been\nextremely useful to develop algorithms that minimize the risk-typically defined as the expected\nloss-on unseen samples under certain assumptions. The assumption, that samples are independent\nand identically distributed (IID) within the training dataset and at test time, has served us well. But it\nis neither testable nor believed to be true in practice. The future is always different from the past:\nboth distributions of data and goals of the learner may change over time. Moreover, those changes\nmay cause the optimal hypothesis to change over time as well. There are numerous mathematical\nand empirical approaches that have been developed to address this issue, e.g., techniques for being\ninvariant to [1], or adapting to, distribution shift [2], modeling the future as a different task, etc. But\nwe lack a first-principles framework to address problems where data distributions and goals may\nchange over time in such a way that the optimal hypothesis is time-dependent. And as a consequence,\nmachine learning-based AI today is brittle to changes in distribution and goals.\nThis paper develops a theoretical framework called \"Prospective Learning\" (PL). Instead of data\narising from an unknown probability distribution like in PAC learning, prospective learning assumes\nthat data comes from an unknown stochastic process, that the loss considers the future, and that\nthe optimal hypothesis may change over time. A prospective learner uses samples received up to\nsome time t \u2208 N to output an infinite sequence of predictors, which it uses for making predictions on"}, {"title": "Why should one care about prospective learning?", "content": "Imagine a deployed machine learning system.\nThe designer of this system desires to optimize-not the risk upon the past training data, or the risk\non the immediate future data\u2014but the risk on all data that the model will make predictions upon in\nthe future. As data evolves, e.g., due to changing trends and preferences of the users, the optimal\nhypothesis to make predictions also changes. Time is the critical piece of information if the system\ndesigner is to achieve their goals. Both in the sense of how far back in time a particular datum was\nrecorded, and in the sense of how far ahead in the future this system will be used to make predictions.\nThe designer must take time into account to avoid retraining the model periodically, ad infinitum.\nBiology is also rich with examples where systems seem to behave prospectively. The principle of\nallostasis, for example, states that regulatory processes of living things anticipate the needs of the\norganism and prepare to satisfy these needs before, rather than after, they arise [3]. For example,\nmitochondria increase their energy production to anticipate the demands of muscles [4], neural\ncircuits anticipate changes in sensory stimuli and the task (i.e., predictive coding [5]), and individual\norganisms optimize their actions with respect to anticipated changes in their environments [6, 7].\nThese regulatory principles were learned early in evolutionary time so they must be important. In\nshort, the world\u2014including our internal drives-changes all the time, and learning systems must\nanticipate (that is, prospect) these changes to thrive."}, {"title": "Contributions", "content": "Section 2 defines Prospective Learning (PL) as an approach to address problems where the\noptimal hypothesis may evolve over time (due to shifts in distributions and/or goals of the\nlearner). It also provides illustrative examples of PL.\nSection 3 and Appendix A put prospective learning in context relative to existing ideas in\nthe literature to address changes in the data distribution.\nSection 4 takes steps towards a theoretical foundation for prospective learning. We de-\nfine strongly learnability (i.e., there exists a prospective learner whose risk is arbitrarily\nclose to the Bayes optimal learner) and weakly learnability (i.e., there exists a prospective\nlearner whose risk is better than chance) [8]. Empirical risk minimization (ERM) without\nincorporating time, can result in failure to strongly, or even weakly, learn prospectively [9].\nSection 5 introduces prospective empirical risk minimization, and proves that it can learn\nprospectively under certain assumptions on the stochastic process and loss.\nSection 6 demonstrates that our prospective learners can prospectively learn several canonical\nproblems constructed using synthetic, MNIST [10] and CIFAR-10 [11] data. In contrast, a\nnumber of existing algorithms, including ERM, online and continual learning algorithms, fail.\nAppendix H demonstrates that current large language models, which use Transformer-based\narchitectures trained using auto-regressive losses, fail to learn prospectively."}, {"title": "2 A definition of prospective learning", "content": "A prospective learner minimizes the expected cumulative risk of the future using past data. Such a\nlearner is defined by the following key ingredients (see Fig. 1 (left) for schematic illustration).\nData. Let the input and output at time t be denoted by xt \u2208 X and yt \u2208 Y respectively. Let\nzt = (xt, yt). We will model the data as a stochastic process Z = (Zt)t\u2208N defined on an appropriate\nprobability space (\u03a9, F, P). At time t \u2208 N, denote past data by z<t = (\u22481,..., zt) and future data by\nz>t = (zt+1,...). We will find it useful to distinguish between the realization of the data, denoted by\nz<t, and the corresponding random variable, Z<t.\nHypothesis class. At each time t, a prospective learner selects an infinite sequence h =\n(h1,..., ht, ht+1,...) which it uses to make predictions on data at any time in the future. Each\nelement of this sequence ht : X \u2192 Y and therefore ht \u2208 yx."}, {"title": "2.1 Different prospective learning scenarios with illustrative examples", "content": "We next discuss four prospective learning scenarios that are relevant to increasingly more general\nclasses of stochastic processes. Our goal is to illustrate, using examples, how the definitions developed\nin the previous section capture these scenarios. We will assume that for all times t we have Xt = 1,\nYt \u2208 {0, 1}. We will also focus on the time-invariant zero-one loss l(t, \u0177, y) = d(\u0177 \u2260 y) for all t, here\n8 is the Dirac delta function. Fig. 1 shows example realizations of the data for each scenario.\nScenario 1 (Data is independent and identically distributed). Formally, this consists of stochastic\nprocesses where Pzt|Z<t = Pzt for all t,t' \u2208 N. As an example, consider Yt ~ Bernoulli(p) for\nsome unknown parameter p \u2208 [0, 1]. Prospective Bayes risk is equal to min(p, 1 \u2013 p) in this case. A"}, {"title": "Scenario 2 (Data is independent but not identically distributed).", "content": "This consists of stochastic\nprocesses where PZt|Z<t = Pzt for all t \u2208 N. Consider Yt ~ Bernoulli(p) if t is odd, and Yt ~\nBernoulli(1-p) if t is even, i.e., data is drawn from two different distributions at alternate times.\nProspective Bayes risk is again equal to min(1 \u2013 p,p) in this case. A time-agnostic hypothesis can\nonly perform at chance level. But a prospective learner, for example one that selects a hypothesis that\nalternates between two predictors at even and odd times, can converge to prospective Bayes optimal\nrisk. We can also construct variants, e.g., when the relationship between the Bernoulli probabilities\nare not known (Variant 1 in Fig. 1), or when the learner does not know that the data distribution\nchanges at every time step (Variant 2 in Fig. 1 where we implemented a generalized likelihood ratio\ntest to determine whether the distribution changes). The risk of these variants also converges to\nprospective Bayes risk, but they need more samples because they use more generic models of the\nstochastic process. This scenario is closely related to (online) multitask/meta-learning [12]."}, {"title": "Scenario 3 (Data is neither independent nor identically distributed).", "content": "Formally, this scenario\nconsists of general stochastic processes. As an example, consider a Markov process P(Yt+1 = k |\nYt = k) = 0 with two states k \u2208 {0, 1} and Y\u2081 ~ Bernoulli(0). The invariant distribution of this\nMarkov process is P(0) = P(1) = 1/2. Prospective Bayes risk is also equal to 1/2. For stochastic\nprocesses that have a invariant distribution, it is impossible to predict the next state infinitely far into\nthe future and therefore it is impossible to prospect. The prospective Bayes risk is trivially chance\nlevels. In such situations, the learner could consider losses that are discounted over time. For example,\none could use a slightly different loss than the one in Eq. (1) to write"}, {"title": "Scenario 4 (Future depends upon the current prediction).", "content": "Problems where predictions of the\nlearner affect future data are an interesting special case of Scenario 3. Prospective learning can\nalso be used to address such scenarios. For 00, 01 \u2208 [0, 1], consider a Markov decision process\n(MDP) P(Yt+1 = j | Yt = j', ht+1(1) = k) = \u03b8k if j = j' and 1 \u03b8\u03ba otherwise. I.e., the prediction\nht+1(Xt+1) = k (recall that Xt = 1 for all times) is the decision and the MDP remains in the same\nstate with probability \u03b8k. Prospective Bayes risk for this example is the same as that of the example\nin Scenario 3. We can construct a prospective learner using a variant of Q-learning to first estimate\nthe hypothesis and then estimate the probability P(Yt' | yt) like Scenario 3 above to predict on future\ndata at time t'. See Appendix B.3. Prospective risk of this learner converges to Bayes risk in Fig. 1.\nThis scenario is closely related to reinforcement learning [15]."}, {"title": "3 How is prospective learning related to other learning paradigms?", "content": "Distribution shift. Prospective learning [16] is equivalent to PAC learning [17] in Scenario 1 when\ndata is IID. Situations when this assumption may not be valid are often modeled as a distribution\nshift between train and test data [2]. Techniques such as propensity scoring [18, 19] or domain\nadaptation [20, 21] reweigh or map the train/test data to get back to the IID setting; techniques like\ndomain invariance [22, 1] build a statistic that is invariant to the shift. Typically, the loss is unchanged\nacross train and test data. If the set of marginals {P(Zt)} of the stochastic process only has two\nelements, then PL is equivalent to the classical distribution shift setting. But otherwise, in PL, data is\ncorrelated across time, distributions (marginals) can shift multiple times, and risk changes with time.\nMulti-task, meta-, continual, and lifelong learning. A changing data distribution could be modeled\nas a sequence of tasks. Depending upon the stochastic process, different concepts are relevant, e.g.,\nmulti-task learning [23] is useful for Scenario 2 and Appendix D when there are a finite number\nof tasks. Much of continual or lifelong learning [14, 13] focuses on \u201ctask-incremental\u201d and \u201cclass-\nincremental\" settings [24], in which the learner knows when the task switches. PL does not make\nthis assumption, and therefore, the problem is substantially more difficult. \u201cData-incremental", "goal": "continual or lifelong\nlearning seeks to minimize past error. As a consequence, continual learning methods are poor\nprospective learners; see Section 6. Online meta-learning [26-29] is close to task-agnostic continual\nlearning, except that the former models tasks as being sampled IID from some distribution of tasks.\nDue to this, one cannot predict which task is next, and therefore cannot prospect.\nSequential decision making and online learning. PL builds upon works on learning from streaming\ndata. But our goals are different. For example, Gama et al. [30] minimize the error on samples from\na stationary process; Hayes et al. [31] minimize the error on a fixed held-out dataset or on all past\ndata-neither of these emphasizes prospection. There is a rich body of work on sequential decision\nmaking, e.g., predicting a finite-state, stationary ergodic process from past data [32]. Even in this\nsimple case, there does not exist a consistent estimator using the finite past Z1:t [33-35]. This is\nalso true for regression [36, 37], when the true hypothesis f* s.t. Yt = f*(Xt) is fixed. In other\nwords, Bayes risk R in Theorem 1 may be non-zero in PL even for finite-state, stationary ergodic\nprocesses. Hanneke [38] lifts the restriction on stationarity and ergodicity. They obtain conditions on\nthe input process X for consistent inductive (predict at time t' > t using data up to t), self-adaptive\n(predict at time t' using Z<t and Xt+1:t') and online learning [39, 40] (predict at t' using Z<t'). They\nprove the existence of a learning rule that is consistent for every X that admits self-adaptive learning.\nIf X is"}, {"title": "4 Theoretical foundations of prospective learning", "content": "Definition 1 (Strong Prospective Learnability). A family of stochastic processes is strongly\nprospectively learnable, if there exists a learner with the following property: there exists a time t' (\u20ac, \u03b4)\nsuch that for any \u20ac, \u03b4 > 0 and for any stochastic process Z from this family, the learner outputs a\nhypothesis h such that P [Rt (h) \u2013 R < \u0454] \u2265 1 \u2013 8, for any t > t'.\nThis definition is similar to the definition of strong learnability in PAC learning with one key difference.\nProspective Bayes risk R depends upon the realization of the stochastic process z<t up to time t.\nIn PAC learning, it would only depend upon the true distribution of the data. Not all families of\nstochastic processes are strongly prospectively learnable. We therefore also define weak learnability\nwith respect to a \u201cchance\u201d learner that predicts E[Y] and achieves a prospective risk R.\nDefinition 2 (Weak Prospective Learnability). A family of stochastic processes is weakly prospec-\ntively learnable, if there exists a learner with the following property: there exists an \u20ac > 0 such that\nfor any \u03b4 > 0, there exists a time t' (\u20ac, 8) such that for any stochastic process Z from this family,\nP [R - Rt(h) > \u20ac] \u2265 1 \u2013 8, for any t > t'.\nIn PAC learning for binary classification, strong and weak learnability are equivalent [47] in the\ndistribution agnostic setting, i.e., when strong and weak learnability is defined as the ability of a\nlearner to learn any data distribution. But even in PAC learning, if there are restrictions on the data\ndistribution, strong and weak learnability are not equivalent [48]. This motivates Proposition 1 below.\nBefore that, we define a time-agnostic empirical risk minimization (ERM)-based learner. In PAC\nlearning, ERM selects a hypothesis that minimizes the empirical loss on the training data. It outputs a\ntime-agnostic hypothesis, i.e., using data, say, z<t standard ERM returns the same predictor for future\ndata from any time t' > t. There is a natural application of ERM to prospective learning problems,\ndefined below.\nDefinition 3 (Time-agnostic ERM). Let H be a hypothesis class that consists of time-agnostic\npredictors, i.e., ht = ht for any t,t' \u2208 N for all predictors h \u2208 H. Given data z<t, a learner that\nreturns"}, {"title": "5 Prospective Empirical Risk Minimization (ERM)", "content": "In PAC learning, the hypothesis returned by ERM using the training data can predict arbitrarily well\n(approximate the Bayes risk arbitrarily well with arbitrarily high probability), with a sufficiently large\nsample size. This statement holds if (a) there exists a hypothesis in the hypothesis class whose risk\nmatches the Bayes risk asymptotically, and (b) if risk on training data converges to that on the test\ndata sufficiently quickly and uniformly over the hypothesis class [49, 50]. Theorem 1 is an analogous\nresult for prospective learning.\nTheorem 1 (Prospective ERM is a strong prospective learner). Consider a finite family of\nstochastic processes Z. If we have (a) consistency, i.e., there exists an increasing sequence of\nhypothesis classes H\u2081 \u2286 H2 \u2286 . . . with each Ht \u2286 (VX)N such that \u2200Z \u2208 Z,\nwhere h \u2208 Ht is a random variable in \u03c3(Z<t), and (b) uniform concentration of the limsup, i.e.,\nVZ \u2208 Z,\nfor some Yt \u2192 0 and ut \u2192 \u221e with ut \u2264 t (all uniform over the family of stochastic processes), then\nthere exists a sequence it that depends only on yt such that a learner that returns\nis a strong prospective learner for this family. We define prospective ERM as the learner that\nimplements Eq. (8) given train data z<t.\nAppendix E.2 provides a proof, it builds upon the work of Hanneke [38]. The first condition, Eq. (6),\nis analogous to the consistency condition in PAC learning. In simpler words, it states that the Bayes\nrisk can be approximated well using the chosen sequence of hypothesis classes {Ht}t_1. The second\ncondition, Eq. (7), is analogous to concentration of measure in PAC learning, it requires that the\nlimsup in Eq. (1) is close to an empirical estimate of the limsup (the second term inside the absolute\nvalue in Eq. (7)). At each time t, prospective ERM in Eq. (8) selects the best hypothesis \u0125 \u2208 Ht8 for\nfuture times t' > t, that minimizes an empirical estimate of the limsup using the training data z<t.\nProspective ERM can exploit the difference between the latest datum in the training set with time t\nand the time for which it makes predictions t' by selecting specific sequences inside the hypothesis\nclass Ht. For example, in Scenario 2 it can select sequences where alternating elements can be used\nto predict on data from even and odd times.\nRemark 1 (How to implement prospective ERM?). An implementation of prospective ERM is\ntherefore not much different than an implementation of standard ERM, except that there are two\ninputs: time s and the datum xs. Suppose we use a hypothesis class where each predictor is a neural\nnetwork, this could be a multi-layer perceptron or a convolutional neural network. The training set\nz<t consists of inputs xs along with corresponding time instants s and outputs ys. To implement\nprospective ERM, we modify the network to take (s, xs) as input (using any encoding of time, we\ndiscuss one in Section 6) and train the network to predict the label ys. In Eq. (8) we can set ui\u2081 = t,\ndoing so only changes the sample complexity. At inference time, this network is given the input\n(t', xt) to obtain the prediction yt. Note that if prospective ERM is implemented in this fashion, the\nlearner need not explicitly calculate the infinite sequence of predictors.\nCorollary 1. There exist stochastic processes for which time-agnostic ERM is not a strong prospective\nlearner, but prospective ERM is a strong learner.\nRemark 2 (Why we need an increasing sequence of hypothesis classes H\u2081 \u2286 H2 ...). We could\nhave chosen Ht = Ht, for all t, t' \u2208 N to set up Theorem 1. But since the learner does not have a lot of\ndata at early times, it should use a small hypothesis class. Just like PAC learning, the sequence (Yt)t\u2208N\nin Eq. (7) determines the convergence rate of a prospective learner. Therefore, using a monotonically\nincreasing sequence of hypothesis classes is useful to ensure a good sample complexity."}, {"title": "6 Experimental Validation", "content": "This section demonstrates that we can implement prospective ERM on prospective learning prob-\nlems constructed on synthetic data, MNIST and CIFAR-10. In practice, prospective ERM may\napproximately achieve the guarantees of Theorem 1. We will focus on the distribution changing,\nindependently or not (Scenarios 2 and 3). Recall that Scenario 1 is the same as the IID setting used in\nstandard supervised learning problems. Scenario 4 is more involved (see an example in Appendix B.3)\nand, therefore, we leave more elaborate experiments for future work. We discuss experiments that\ncheck whether large language models can do prospective learning in Appendix H.\nLearners and hypothesis classes. Task-agnostic online continual learning methods are the closest\nalgorithms in the literature that can address situations when data evolves over time. We use the\nfollowing three methods.\n(i) Follow-the-Leader minimizes the empirical risk calculated on all past data and is a no-regret\nalgorithm [52]. We note that while this is a popular online learning algorithm, we do not\nimplement the algorithm in an online fashion.\n(ii) Online SGD fine-tunes the network using new data in an online fashion. At every time-step,\nweights of the network are updated once using the last eight samples.\n(iii) Bayesian gradient descent [53] is an online continual learning algorithm designed to\naddress situations where the identity of the task is not known during both training and\ntesting, i.e., it implements continual learning without knowledge of task boundaries.\nThese three methods are not explicitly designed for prospective learning but they are designed to\naddress the changing data distribution t.10 We calculate the prospective risk of the predictor returned\nby these methods; note that they do not output a time-varying predictor and consequently, these\nmethods output a time-agnostic hypothesis. As a result, when we evaluate the prospective risk of\nthese methods, we use the same hypothesis for all future time. For all three methods, we use a\nmulti-layer perceptron (MLP) for synthetic data and MNIST, and a convolutional neural network\n(CNN) for CIFAR-10.\nFor prospective ERM the sequence of predictors is built by incorporating time as an additional input\nto an MLP or CNN as follows. For frequencies w\u2081 = \u03c0/\u03af for i = 1, . . ., d/2, we obtain a d-dimensional\nembedding of time t as y(t) = (sin(w\u2081t), ..., sin(wa/2t), cos(w\u2081t),..., cos(wa/2t)). This is similar to\nthe position encoding in Vaswani et al. [55]. A predictor ht (.) uses a neural network that takes as\ninput, an embedding of time (s), and the input xs to predict the output ys for any time s \u2208 N. Using\nsuch an embedding of time is useful in prospective learning because, then, one need not explicitly\nmaintain the infinite sequence of predictors h = (h1,...,)."}, {"title": "6.1 Prospective learners for independent but not identically distributed data (Scenario 2)", "content": "We create tasks using synthetic data, MNIST and CIFAR-10 datasets to design prospective learning\nproblems when data are independent but not identically distributed across time (Scenario 2).\nDataset and Tasks. For the synthetic data, we consider two binary classification problems (\u201ctasks\")\nwhere the input is one-dimensional. Inputs for both tasks are drawn from a uniform distribution\non the set [-2, \u22121] U [1, 2]. Ground-truth labels correspond to the sign of the input for Task 1, and\nthe negative of the sign of the input for Task 2. For MNIST and CIFAR-10 we consider 4 tasks\ncorresponding to data from classes 1-5, 4-7, 6-9 and 8-10 in the original dataset, i.e., the first task\nconsiders classes 1-5 labelled 1-5 respectively, the second task considers classes 4-7 labelled 1-4, the\nthird task considers classes 6-9 labeled 1-4 and the last task considers labels 8-10 labelled 1-3. In\nother words, images from class 1 in task 1, class 4 from task 2 and class 6 from task 3 are all assigned\nthe label 1. For the prospective learning problem based on synthetic data, the task switches every\n20 time steps. For MNIST and CIFAR-10, the data distribution cycles through the 4 tasks, and the\ndistribution of data changes every 10 time-steps. For more details, see Appendix F.\""}, {"title": "6.2 Prospective learners when data are neither independent nor identically distributed\n(Scenario 3)", "content": "For synthetic data, we construct 4 binary classification problems with two-\ndimensional input data (see Fig. 3 and caption for details). For CIFAR-10 and MNIST, we consider\nfour tasks corresponding to the classes 1-5, 4-7, 6-9 and 8-10. Using these tasks, we construct\nproblems where the data distribution is governed by a stochastic process which is a hierarchical\nhidden Markov model (Scenario 3). After every 10 time-steps, a different Markov chain governs"}, {"title": "7 Discussion", "content": "Prospective learning, as we see it, is a paradigm of learning that characterizes many real-world\nscenarios which are currently modeled using much stronger (and less accurate) assumptions. These\nsimplifying assumptions have certainly enabled progress in machine learning. But systems deployed\nbuilt upon these approaches have proven to be extremely fragile in certain real-world settings. Today's\nmachine learning-based systems fail to track changes in the data. They certainly do not model how\nbiological organisms learn robustly and effectively over time. We believe characterizing which kinds\nof stochastic processes are prospectively learnable under which kinds of time-sensitive loss functions\nwill be an important next theoretical step. Developing algorithms, from the perspective of prospective\nlearning, which have theoretical guarantees in practice, will be another next step. Finally, building\nalgorithms that scale and can therefore be deployed in real-world systems, will be important to\ndemonstrate the utility of this approach. The precise real-world applications in which prospective\nlearning based methods will outperform PAC learning, remains to be seen."}, {"title": "Calculations for scenarios in Section 2.1", "content": "Consider Scenario 1 where the learner returns the hypothesis h\u2081 = ht = threshold(pt > 0.5) for\nall t' > t, where \u017fpt = 1=1 ys is the maximum likelihood estimator (MLE). Alternatively, if\nwe assume a prior distribution Beta(\u03b1, \u03b2) over p, then the resulting maximum a posteriori (MAP)\nestimate is pt =. We define a second prospective learner based on MAP that returns\nthe sequence h>t = (ht, ht,\u2026), where ht = threshold(pt > 0.5) for all future times beyond t. If\nthe prior distribution has a small divergence with respect to the true posterior distribution, then the\nsecond learner converges faster to the Bayes optimal risk; for a poor choice of prior, the convergence\nis slower. However, in such situations, we show that we can modify the MAP-based learner to use\nprospection and incorporate \u201ctime\u201d to result in faster convergence to the Bayes risk.\nLet y1,..., yt be the IID sample sequence observed up to time t. The idea here is to compute the rate\nof change Ap(t) of the MAP estimate at time t which is given by,\nTaking expectation on both sides of Equation (10), and plugging in \u00ee\u00eet = MAP(y1,..., yt) for the true\nparameter p, we construct the following estimate for Ap(t).\nWe refer to this as the prospective MAP estimate. Based on it, we set the hypothesis to be ht' =\nthreshold(pt' > 0.5) for all future times beyond t. In Figure A.1, we plot the prospective risk the MLE,\nMAP, and prospective MAP-based learners. Due to an unfavorable prior, the MAP-based learner\nconverges slowly. However, prospective MAP-based learner manages to leverage its forecasting to\nachieve a faster convergence rate despite having the same prior as the MAP. This shows that we can\nindeed benefit from prospection even in the IID case."}, {"title": "B.2 Bayes risk for a Markov chain", "content": "We would like to compute the prospective Bayes risk, when the evolution of the samples is governed\nby a Markov transition matrix where P(Yt+1 = 0 | Yt = 0) = 00 and P(Yt+1 = 1 | Yt = 1) = 0\u2081, \u0456.\u0435.,\nthe transition matrix is\nThe probability distribution at time t' is given by Pt' -t(zt, 1 \u2013 zt)T. The eigenvalues of the transition"}, {"title": "B.3 Prospective learning in Scenario 4 when the future depends upon the current prediction", "content": "There are two types of prospective learners-one that passively observes the environment and makes\ninferences and another that acts on the environment and influences it. Scenario 1, Scenario 2,\nScenario 3 fall into the first category which is the primary emphasis of our paper. Scenario 4 presents\na prospective learning problem where the learner can influence the future realizations of the stochastic\nprocess through its decisions.\nOur prospective learner is inspired from reinforcement learning, where the current state is Yt\u22121, the ac-\ntion is ht and the next state is Y\u0142. The reward at the tth time-step is r(t, ht+1, Yt+1) = 1 {ht+1 = Yt+1}\nas a result of selecting action ht+1 given that the previous output was yt, and next output\nYt+1. The learner estimates the transition matrix corresponding to the MDP for each decision\nht+1(Xt+1) \u2208 {0,1} using a similar procedure as that of Scenario 3,\nwhere \u03b8 = \u03b8t \u2208 [0, 1]2 after t time steps. Using this estimate of \u00cet, the learner solves for the value\nfunction (corresponding to the discounted prospective risk) that satisfies\nThe value function can be solved using value iteration, iteratively until convergence. For a given f,\nBanach's fixed point theorem guarantees this procedure will converge to the optimal value function\nin the tabular setting [83]. Once we have the Q-values Qt, we can use it to take actions. The optimal\naction at time t' is arg maxp Q(yt', h). However, unlike reinforcement learning, we do not know Yt'\nfor t' > t and we must instead make a sequence of decisions using state yt. The sequence of decision\nmade by the learner is h>t = (ht+1,...) where\nin Fig. 1, we have used 00 = 0\u2081 = 0.1 and a discount factor \u03b3 = 0.9. We find that this learner\napproaches the Bayes risk (0.357 which we calculate in Appendix B.2)."}, {"title": "C Prospective ERM for discounted losses", "content": "Like we discussed in Scenario 3, in order to prospect meaningfully for some stochastic processes, we\nmight need to consider a discounted future loss, e.g., the one in Eq. (4). Theorem 1 was proved only\nfor the averaged future loss in Eq. (1). Here, we sketch out the proof of an analogous theorem for the\ndiscounted loss. Let\nwhere l : Y \u00d7 \u0423 \u2192 [0, 1] is a bounded loss function and 0 < y < 1 is a constant. In general, we can\nuse a probability measure \u03bc(\u03c4) supported on integers {1, ..., 7} to write the loss as"}, {"title": "D An illustrative example of prospective ERM", "content": "Suppose we have a stochastic process such that Zt ~ P(t mod T) for some known period T", "n{h": "ht+T = ht and ht \u2208 G \u2200t} satisfies Eq. (9), and it is also countable. This implies consistency and\nuniform concentration of the limsup for sequences in"}]}