{"title": "An Attention-based Framework with Multistation Information for Earthquake Early Warnings", "authors": ["Yu-Ming Huang", "Kuan-Yu Chen", "Wen-Wei Lin", "Da-Yi Chen"], "abstract": "Earthquake early warning systems play crucial roles in reducing the risk of seismic disasters. Previously, the dominant modeling system was the single-station models. Such models digest signal data received at a given station and predict earthquake parameters, such as the p-phase arrival time, intensity, and magnitude at that location. Various methods have demonstrated adequate performance. However, most of these methods present the challenges of the difficulty of speeding up the alarm time, providing early warning for distant areas, and considering global information to enhance performance. Recently, deep learning has significantly impacted many fields, including seismology. Thus, this paper proposes a deep learning-based framework, called SENSE, for the intensity prediction task of earthquake early warning systems. To explicitly consider global information from a regional or national perspective, the input to SENSE comprises statistics from a set of stations in a given region or country. The SENSE model is designed to learn the relationships among the set of input stations and the locality-specific characteristics of each station. Thus, SENSE is not only expected to provide more reliable forecasts by considering multistation data but also has the ability to provide early warnings to distant areas that have not yet received signals. This study conducted extensive experiments on datasets from Taiwan and Japan. The results revealed that SENSE can deliver competitive or even better performances compared with other state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Earthquake early warning systems are designed to issue early warnings within seconds after the onset of an earthquake to reduce the risk of earthquake disasters [1], [2], [3], [4], [5], [6]. These systems identify the seismic information of an earthquake based on signals received from stations and the differences in the propagation speeds of these statistics. When a seismic event occurs, stations sequentially detect seismic signals based on their distance from the epicenter. The information is transmitted to the earthquake early warning system to estimate the parameters of the earthquake, including information on the epicenter, intensity, and magnitude. Once the warning is confirmed, the system rapidly issues warnings to the areas expected to be affected through channels, such as text messages and television broadcasts.\nDeep learning has recently demonstrated outstanding potential in various fields. Thus, deep learning-based methods,\nwhose model structures are neural networks, have been pro- posed in the seismology context, including earthquake early warning [7], intensity prediction [8], and p-phase picking [9]. Most of these methods are trained using large amounts of seismic data and attributes, enabling them to understand the relationship between the received data and events [10], [11], [12], [13]. Most of the approaches belong to single- station modeling, which digests signal data received at a given station and predicts if an earthquake will occur at that location. Although the performances of these methods are acceptable and they rapidly deliver responses, they present certain inherent limitations. First, single-station models only consider data from a single seismic station as input while ignoring the rich information from other stations. Second, single-station models typically only alert local regions, lacking the ability to signal distant areas [14], [15].\nOwing to these challenges, multistation modeling, which considers information from multiple stations, has emerged as a suitable alternative [16], [17]. By leveraging information from numerous seismic stations, multistation modeling can deliver more comprehensive and accurate predictions of seis- mic events to enhance earthquake early warning performance. Previous studies have attempted to employ convolutional neu- ral networks [8], [18], graph convolutional neural networks [19], or Transformer models [16] to process information from multiple stations. These methods highlight the importance and effectiveness of multistation information for earthquake alarming.\nThus, we attempted to create a deep learning-based mod- eling system with multistation information for earthquake early warning. The motivation is twofold. First, we intended to investigate the feasibility, effectiveness, and efficiency of the currently used self-attention mechanism to model the relationships among a set of stations [20]. Second, recent methods only take the waveform, position, or a combination of both as station input. However, certain physical or geological information regarding each station can be used to further enhance the performance. Consequently, this study developed a framework called SENSE, which leverages the self-attention mechanism to consider multistation data and employs locality- specific embeddings to enable the model to learn and encode the specific characteristics of each station. Thus, SENSE is expected to produce more accurate predictions. We evaluated SENSE using the Japan and Taiwan earthquake datasets [21]. The present experiments showed that SENSE can achieve com- petitive or even better results than state-of-the-art baselines."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Transformer and Conformer Models", "content": "The Transformer model has recently become a cutting-edge deep learning architecture in various fields [20], such as natural language [22], speech [23], and vision processing [24]. Al- though the Transformer model was originally intended to han- dle time-series data and sequence generation problems, its abil- ity to tackle common tasks was subsequently investigated and verified. The Transformer model exhibits an encoder-decoder architecture. The encoder dissects the input sequence while the decoder generates output sequences, rendering it an integral part of sequence-to-sequence tasks. Noteworthily, the encoder and decoder can be used individually or combined. Neverthe- less, the key components of the encoder and decoder are the attention mechanism, positional encoding, feedforward neural network, layer normalization, and residual connection. Among the components, the fundamental innovation is the self-attention mechanism, which was leveraged in sequential modeling to process sequential data without relying on recurrence-based neural networks (i.e., vanilla recurrent neural networks [25] and long short-term memory models [26]) or convolutional layers [27]. Dissim- ilar to the recurrence-based and convolutional models that sequentially process sequences, the self-attention mechanism enables the model to dynamically weigh the importance of each unit in a sequence relative to other units, capturing dependencies and relationships in a parallelized manner. The design allows the model to grasp long-range dependencies effectively. In addition to theoretical improvements, such a network architecture can easily handle large-scale datasets and speed up training because of its parallelizable design [20].\nBased on the fundamentals of the Transformer model, the Conformer model features pivotal modifications to enhance the efficiency and effectiveness of the Transformer encoder architecture [28]. The major components of a Conformer include the self-attention mechanism, convolution module, feedforward neural network, layer normalization, and resid- ual connection. These components synergistically enable the model to discern intricate patterns, abstract representations, and hierarchical structures in a time-series data sequence. The main innovation is in the convolution module, which incorporates depthwise separable convolutions alongside the self-attention mechanism. The specialized design simultaneously performs convolutions across the time (sequence length) and depth (embedding dimensions) axes. Consequently, Conformers can capture long- term dependency, similar to the Transformers, and discern fine- grained local patterns.\nBeyond their roots in natural language processing, the Transformer and Conformer models are versatile across do- mains, such as speech recognition, audio processing, and sequential data analysis [29], [30], [31]. Their ability to capture dependencies across varying scales while maintaining computational efficiency renders them exceptional in diverse machine learning applications. Various studies have shown that they are promising frameworks for a wide range of real-world applications that rely on comprehensive sequence understanding and analysis."}, {"title": "B. Deep Learning for Seismic Research", "content": "Deep learning has recently become a cornerstone of seismo- logical research. Through deep learning, many tasks have been improved, including earthquake intensity prediction [15], [32], earthquake epicenter estimation [33], [34], phase picking [35], and earthquake early warning [16], [36]. Among popular neu- ral networks, convolutional neural networks (CNNs) have been widely used in seismic data preprocessing, feature extraction and as building blocks for models [37], [13]. Thibaut et al. [38] stacked multiple layers of CNNs for earthquake detection and source area estimation; they accurately predicted earthquake amplitudes. Hong et al. [36] proposed two CNN-based models for estimating the surface response of earthquakes. Chiang et al. [13] developed a CNN-based model to predict whether the peak ground acceleration (PGA) of a specific seismic monitoring station exceeded a predefined threshold or not. In addition to CNNs, recurrent neural networks (RNNs) are suitable building blocks for mitigating seismic tasks since the signal data from seismometers are time series. Chin et al. [39] employed multilayers of long short-term memory models (LSTMs) to identify the occurrence of an earthquake event and the durations of P- and S-waves. Berhich et al. [40] also used LSTM to create an earthquake prediction model. Cofr\u00e9 et al. [14] developed an LSTM-based method to address the problem of earthquake magnitude estimation for earthquake early warning. Berhich et al. [41] examined the abilities of the LSTM and gated recurrent unit (GRU) to predict earthquake magnitude. To further combine the advantages of CNN and LSTM, Kail et al. [42] proposed a recurrent CNN-based method to predict the location of earthquakes.\nRecently, neural networks based on attention mechanisms, particularly self-attention techniques, have become a main- stream architecture. The most famous representative is the Transformer [20], which holds significant advantages over CNNs and RNNs in dealing with time-series data. Compared with RNNs, Transformers are parallelizable and can model long-term dependencies between data. Thus, they are more computationally efficient than RNNs. CNNs, although power- ful for spatial data such as images, are not inherently designed for sequential data. Thus, they cannot effectively capture long- range context dependencies embedded in sequences. Owing to these advantages, studies have applied Transformers to seismic research. Mousavi et al. [35] proposed a very deep network architecture based on CNNs and Transformers for simultaneous earthquake detection and phase picking. Chin et al. [43] presented a Transformer-based hypocenter estimator for earthquake localization. M\u00fcnchmeyer et al. [16], [44], [45], [46] used a series of Transformers to create an earthquake early warning system. Saad et al. [47] employed the model architecture used in vision computing, which is also based on Transformers, to perform real-time earthquake detection and magnitude estimation. Although the Conformer is an extension that injects CNN components into the Transformer to obtain the best of both worlds, there is still a dearth of studies using the architecture in the context of seismological research."}, {"title": "III. THE PROPOSED METHODOLOGY", "content": "With the rapid development of deep learning and the use of neural networks to enhance the performance of various seismic tasks, many interesting methods have been proposed. However, numerous existing methods only concentrate on a given seismic station or location. Therefore, these single- station methods are employed to process the data received from a station and deliver corresponding predictions regarding the location. To enhance the performance of an earthquake early warning system by leveraging information from multiple stations and advanced deep learning models, we present an attention-based framework called SENSE."}, {"title": "A. Model Architecture", "content": "SENSE is designed with an encoder-decoder architecture, as depicted in Fig. 1(c). The inputs comprise statistics from a set of stations in a given region or country. The output comprises intensity predictions for locations corresponding to the input stations. The encoder consists of a convolution module, a positional encoding module, and an early locality- specific encoding module. The decoder is based on a feature blending module, followed by a late local-specific encoding module and prediction module. Depending on the modeling strategy, even if only one station receives earthquake statistics because of the initial onset of an earthquake, SENSE is expected to signal early warnings to all locations related to the input stations.\nThe waveform $w \\in \\mathbb{R}^{3 \\times T}$ (i.e., three-axis acceleration of time $T$) and geographical information $g \\in \\mathbb{R}^3$ (i.e., longitude, latitude, and instrument height) are basic features of each station. For a given set of $N$ stations, ${w_1,..., w_n}$ and ${9_1,..., g_n}$ are input statistics to the model. The convolu- tion module is initially employed to scan the waveform and encapsulate the information into feature vectors:\n$[W_1, ..., W_N] = \\text{ConvolutionModule}([w_1,...,w_n]),$ (1)\nwhere $\\text{ConvolutionModule}(\\cdot)$ is implemented with layers of CNNs; each resulting vector $W_n$ is in the $d_{\\text{model}}$ dimension (i.e., $W_n \\in \\mathbb{R}^{d_{\\text{model}}}$), and $d_{\\text{model}}$ is a predefined model configuration. The geographical information is processed by the positional encoding module to calculate the corresponding vector for each station:\n$[G_1, ..., G_N] = \\text{PositionalEncoding}([9_1,\u2026\u2026,g_N]),$ (2)\nwhere the calculation function $\\text{PositionalEncoding}(\\cdot)$ is implemented using pairs of sinusoidal functions (i.e., sine and cosine) as usual [20], and $G_n$ is also in the $d_{\\text{model}}$ dimension. Afterward, the two sets of features are summed together with learnable weighting factors for each station individually:\n$[H^{\\circ}_1,...,H^{\\circ}_N] = \\text{ADD}([\\alpha_1, \u2026\u2026\u2026, \\alpha_N] \\cdot [W_1, ..., W_N], \\\\ [(1 - \\alpha_1), ..., (1 - \\alpha_N)] \\cdot [G_1, ..., G_N]),$ (3)\nwhere the range of each weighting factor $\\alpha_n$ is 0-1. The weighting factors are used to automatically adjust the bal- ance between the waveform and geographical information."}, {"title": "B. Learning Objectives", "content": "As an earthquake early warning system, SENSE is designed to predict intensity levels for target locations. This paper pro- poses a simple discrete classification method and a continuous counterpart with a mixture density network [48].\nFor the former, the prediction module in SENSE is created by a simple feedforward neural network (FFNN) followed by a sigmoid activation function:\n$[D^{\\text{dis}}_1,..., D^{\\text{dis}}_N] = \\text{PredictionModule}([H^{\\prime\\prime}_1,...,H^{\\prime\\prime}_N]) \\\\ = \\text{Sigmoid}(\\text{FFNN}([H^{\\prime\\prime}_1,\u2026\u2026\u2026,H^{\\prime\\prime}_N])),$ (8)\nwhere $D^{\\text{dis}}_n$ is a $C$ dimensional vector (i.e., $D^{\\text{dis}}_n \\in \\mathbb{R}^C$), and $C$ denotes the number of predefined intensity levels. Thus, the result is the probability of each intensity level, and we selected the highest as the final output. The objective of the training is to minimize the classification errors calculated by cross-entropy loss:\n$L_{\\text{dis}} = -\\sum_{n=1}^N\\sum_{y \\leq y_n} \\log P(y|D^{\\text{dis}}_n),$ (9)\nwhere $y_n$ denotes the ground truth label for the $n^{th}$ station. In other words, $y_n$ is the category of the largest PGA actually observed on the $n^{th}$ station. A reasonable scenario is that if the intensity level of the $n^{th}$ station is $y_n$, the low-intensity classes (i.e., $y \\leq y_n$) should also be triggered. Thus, the learning objective is to onset all classes that are smaller than the ground truth $y_n$ while offsetting other classes.\nFor the continuous counterpart, we employed a mixture density network to approximate the complex functionality of the PGA behavior. A Gaussian mixture model returned by the mixture density network was used to calculate the probability of each PGA value for a target location. According to the probability, we can decide whether or not to issue an alarm. In sum, the prediction module is a mixture density network implemented by an FFNN in this study, and the output is Gaussian mixture statistics:\n$[D^{\\text{cont}}_1, ..., D^{\\text{cont}}_N] = \\text{PredictionModule}([H^{\\prime\\prime}_1, \u2026\u2026\u2026, H^{\\prime\\prime}_N]) \\\\ = \\text{FFNN}([H^{\\prime\\prime}_1,...,H^{\\prime\\prime}_N]).$ (10)\nIf the number of Gaussians is set to $K$, $D^{\\text{cont}}_n$ is a $3K$- dimensional vector, which comprises $K$ mixture weights $[a_1,...,a_K]$, $K$ mean values $[\\mu_1,\u2026\u2026\u2026,\\mu_K]$, and $K$ standard deviations $[\\sigma_1,...,\\sigma_K]$. To maintain the property of the mix- ture weights (i.e., $\\sum_{k=1}^K a_k = 1$), a softmax activation function is applied to $[a_1,...,a_K]$. A rectified linear unit activation function (ReLU) is applied to $[\\sigma_1,...,\\sigma_K]$. The training loss is calculated by minimizing the negative log-likelihood:\n$L_{\\text{cont}} = -\\sum_{n=1}^N\\log p(y_n|D^{\\text{cont}}_n) \\\\ = -\\sum_{n=1}^N\\log\\left(\\sum_{k=1}^K a_kN(y_n|\\mu_k, \\sigma_k)\\right),$ (11)\n$N(y_n; \\mu_k, \\sigma_k) = \\frac{1}{\\sigma_k\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y_n - \\mu_k)^2}{2\\sigma_k^2}\\right),$ (12)\nwhere $y_n$ denotes the maximum PGA observed at the $n^{th}$ station. During inference, for a given location, we can calculate the occurring probability for each PGA $U_{\\text{PGA}}$ by integrating from infinite back to $U_{\\text{PGA}}$:\nP(U_{\\text{PGA}}|D^{\\text{cont}}) = \\sum_{k=1}^K a_k\\left(\\int_{-\\infty}^{U_{\\text{PGA}}}N(y|\\mu_k, \\sigma_k)dy\\right).$ (13)\nAn earthquake alarm would be issued if the probability of a predefined PGA exceeds a threshold."}, {"title": "C. Summary", "content": "The proposed SENSE model exhibits an encoder-decoder structure. The encoder is mainly used to process the infor- mation collected from seismic stations. For that reason, a convolution module is employed to process the waveform, and a positional encoding module is used to transform physical location information into digital representations. An early locality-specific encoding module is designed to model the detailed characteristics of each station. Based on the metic- ulous design, the input information can be encapsulated as a set of high-level abstractive feature vectors. After the en- coder, a decoder, comprising a feature blending module, late locality-specific encoding module, and prediction module, is introduced. The feature blending module is a vital component of SENSE. It can model the relationships among stations, remove impurities, and keep important information in feature vectors. After the process, a late locality-specific encoding module is further applied to compensate for the unique biases of each station again and generate the resulting representations. Thereafter, the prediction module translates the representations into intensity predictions.\nSeismological information can be categorized into two broad types: waveform and geographical information. Wave- form information includes seismic wave data, including am- plitude, frequency, wave velocity, and waveform shape. Op- positely, geographical information refers to various surface features of the earth, such as topography, geomorphology, and geological structures [1]. SENSE employs the convolution module at the beginning of the encoder to digest the waveform information, and the positional encoding module is employed to encode the raw geographic data. In addition, SENSE introduces early and late locality-specific encodings to enable the model to automatically learn the hidden characteristics of each station to compensate for the input. By combining the waveform, geographical, and hidden information, SENSE establishes a connection between the physical characteristics of the earth and seismic wave data. Consequently, SENSE is expected to lead to a more comprehensive understanding of seismic phenomena and improve earthquake prediction accuracy [49], [50], [51]."}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. DataSets", "content": "This study used two national-level datasets from highly seismically active regions with dense seismic networks: the Japan and Taiwan datasets. Both datasets were divided into training, validation, and test sets to perform model training, hyperparameter tuning, and fair evaluation. We employed an event-based splitting approach by assigning the records of a specific event to the same subset. The Japan dataset was sourced from Okada and Kasahara [21], and the Taiwan Central Weather Administration provided the Taiwan dataset. To evaluate the model performance, five PGA values were se- lected for shaking from light to strong. To align with previous studies, [1%g, 2%g, 5%g, 10%g, 20%g] and [0.81%g, 2.5%g, 8.1%g, 14%g, 25%g] were selected and used for the Japan and Taiwan datasets, respectively [13], [45].\n1) Japan Dataset: The Japan dataset comprises 13,512 events compiled from the NIED Kiban Kyoshin network (KiK- net) catalog between 1997 and 2018 [52], as shown in Fig. 3. The data includes records of triggered strong motion, with each trace containing 15 seconds of pre-trigger data for a total length of 120 seconds. Each station had two three- component strong-motion sensors (one at the surface and one underground). The dataset was split into training, validation, and test sets at a 60:10:30 ratio. The training set ended in March 2012, the test set began in August 2013, and the intervening events were used as the validation set. The total number of stations was 707.\n2) Taiwan Dataset: The Taiwan dataset comprises 9,311 earthquake events recorded by the dense seismic network in Taiwan from 2012 to 2021, as shown in Fig. 4. Each event includes seismic waveforms recorded by three-component strong-motion sensors. We partitioned the dataset into training, validation, and test sets in chronological order at a ratio of 60:25:15. The training, validation, and test sets included 2012-2017, 2018\u20132019, and 2020-2021 events, respectively. There were 250 stations in the dataset."}, {"title": "B. Implementation Details", "content": "1) Training Configurations: The training process was stabi- lized to obtain better model parameters through three training procedures. In the first round, the parameters were updated, except for the early locality-specific encoding, and the learn- able weighting factors {$\\alpha_1,...,\\alpha_N$} were set to 0.5. In the second round, the early locality-specific encoding was trained, except for the learnable weighting factors. Finally, the model parameters were simultaneously updated. For the Japan dataset, the training epochs were 100, 40, and 40. For the Taiwan dataset, the training epochs were 50, 20, and 20 for the three steps. Table I lists the detailed configuration of the convolution module. The feature blending module comprises six layers of either the Transformer encoder or Conformer. For the Transformer encoder and Conformer, the hidden dimension of the FFNN was 1000, and the head number was set to 10. Considering that five PGA values were selected to report the experimental results, the number of classes $C$ for the discrete classification objective was set to 5. The FFNN(\u00b7) in the prediction module for the discrete case comprised five hidden layers in the sizes of 500, 150, 100, 50, and 30 (cf. (8)). In the continuous case, FFNN(\u00b7) was implemented by six hidden layers in the sizes of 500, 150, 100, 50, 30, and 10 (cf. (10)).\nIt is important to highlight that the training procedure is carefully crafted based on empirical insights to optimize both model stability and performance. During the first phase, the model focuses on learning general seismic patterns and capturing broad relationships among input features. Concur- rently, late locality-specific embeddings, which encapsulate station-specific biases and corrections, are updated to align core feature representations with the unique characteristics of each station. This phase establishes a foundational mapping between general seismic features and localized adjustments, creating a stable baseline and preparing the model for more fine-grained, station-specific tuning. The second phase intro- duces early locality-specific embeddings, allowing the model to incorporate nuanced, location-specific information. By iso- lating the training of these embeddings, the model can encode unique features of each seismic station without disrupting the broader relationships learned in the initial phase. This modular approach enables the seamless integration of specific and general information, mitigating the risks of overfitting or instability. In the final phase, all parameters, including core features and both early and late locality-specific embeddings, are updated simultaneously. This comprehensive fine-tuning process harmonizes general seismic insights with station- specific characteristics, ensuring a balanced and accurate pre- dictive capability. By progressively building complexity and refining accuracy across these phases, the model is designed to achieve robustness and high performance as an earthquake early warning system. While alternative training configurations might further enhance results, a comprehensive search for optimal settings was constrained by computational resources.\n2) Evaluation Metrics: To evaluate the performance of the proposed SENSE model, we defined the true positives (TP), false positives (FP), false negatives (FN), and true negatives (TN) as described in Table II. Based on these statistics, we computed the precision, recall, and F1 scores:\n$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}},$ (14)\n$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}},$ (15)\n$\\text{F1 score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}.$ (16)"}, {"title": "C. Main Results", "content": "In the first set of experiments, we evaluated the proposed SENSE model using the Japan and Taiwan datasets. Tables III and IV present the results for different datasets and model configurations. Several observations were drawn from the results. Pairing the discrete objective with the Transformer model for the Japan dataset resulted in better F1 scores in all five PGA levels. In most cases, the precision scores exceeded 0.8, and the recall rates exceeded 0.6. However, the experimental results were poor when the discrete training objective was combined with the Conformer model. For the continuous objective, the behavior of the Transformer and Conformer models differed considerably. The Transformer and Conformer models obtained better recall and precision scores, respectively. The recall scores of the Conformer model were unsatisfactory in the discrete and continuous cases. Summar- ily, the Transformer model achieved better results with the Japan dataset regardless of whether the discrete or continuous training targets.\nNext, we analyzed the results of the Taiwan dataset. For the discrete training objective, the F1 scores of the Transformer and Conformer models were close for each PGA level while the precision and recall scores were mixed. In the continuous case, the Transformer was generally better than the Conformer in terms of precision, recall, and F1 scores. In a nutshell, the first set of experiments showed that pairing the continuous training objective and Transformer model may be a better and more stable choice for SENSE because the setting can lead to acceptable results for both datasets."}, {"title": "D. Compared to Other Advanced Deep Learning-based Meth- ods", "content": "In the second set of experiments, we compared SENSE with two advanced deep learning-based baselines: the intel- ligent strong-motion prediction (ISMP) model [13] and the Transformer earthquake alerting model (TEAM) [16]. The ISMP model is designed to predict early ground motion after an earthquake. The model uses a CNN to extract key features from the initial P-waves and predicts if the PGA of subsequent waves will exceed 80 Gal. The TEAM considers information from multiple seismic stations, as opposed to tra- ditional single-station models. The TEAM can issue warnings for an arbitrary number of locations by inputting waveform information and coordinates from multiple stations, thereby flexibly adapting to various earthquake monitoring networks and warning objectives. TEAM is a Transformer-based model, and the training objective is similar to the continuous case for SENSE. Thus, we compared the TEAM with the SENSE with similar settings, i.e., the continuous training objective and Transformer model. The Taiwan dataset was used as an example, and Table V presents the results.\nThe experiments yielded valuable findings. ISMP is a single-station method; therefore, it cannot make predictions for stations that do not receive waveform signals. Thus, the results were only computed based on stations with earthquake records. Contrarily, both TEAM and SENSE considered all the stations. In a way, the comparison is unfair and more critical for TEAM and SENSE. However, Table V shows that SENSE achieved the best results in all the cases. Second, the performance gap between TEAM and SENSE was wide; thus, the second set of experiments confirmed that SENSE was a better modeling strategy than TEAM in the use of multistation data. Third, SENSE significantly outperformed the TEAM and exceeded ISMP as expected because multistation information is more informative than single-station data. Although SENSE and TEAM are multistation models, different model architectures may yield different results. A possible flaw of TEAM is its decoder design. Although TEAM constitutes a flexible method that can predict the intensity of a target location only by providing the coordinates of the target location, the input in the decoder (i.e., coordinates) considerably differed from the encoded seismic information in the Transformer layer in the encoder. Thus, cross-attention between heterogeneous information may not yield satisfactory and stable results.\nIn addition to the performance evaluation, we examined the model properties and leading time. Table VI lists the experimental results. Regarding the model parameters, ISMP was a relatively smaller model than TEAM and SENSE, and TEAM and SENSE had similar sizes. Regarding execution time, all the models were executed on a GeForce GTX TITAN graphics card for testing. ISMP is the fastest because it is a single-station method. TEAM and SENSE were approximately three times slower than ISMP, and the difference between TEAM and SENSE was tolerable. It was complicated to determine the best and worst models in terms of the leading"}, {"title": "E. Ablation Study", "content": "Next, we examined the efficiency of the components in the SENSE model. Table VII presents the experimental results obtained using the Japan and Taiwan datasets. (A) denotes the SENSE model with the Transformer model, and (B), (C), (D), and (E) are models without certain components. SENSE achieved the best results in most of the cases, indicating that none of the components was dispensable. Second, when comparing (D) and (E), we concluded that the weighting factors, which are used to balance the information between the waveform and geographical statistics (cf. (3)), are important because the performance gaps are to be reckoned with. Third, when comparing (C) and (E), the results showed that early locality-specific encoding provided consistent improvements for the Japan dataset in all the cases and achieved better results for the Taiwan dataset in certain cases. Comparisons between (B) and (E) showed that late locality-specific encoding was more useful in the Japan dataset than in the Taiwan dataset. Considering (B), (C), and (E), we can conclude that early locality-specific encoding appeared to favor large PGA cases, whereas late locality-specific encoding influenced small PGA cases."}, {"title": "V. CONCLUSION", "content": "This paper proposes an earthquake early warning system called SENSE. SENSE comprises an encoder and a de- coder. The encoder distills and refines statistics from stations. The meticulous design is the convolution module and early locality-specific encoding components. The former employs a stack of CNN layers to manipulate local information embed- ded in the raw waveform. The latter is a set of automatically learned parameters for each station to store station-dependent characteristics. The special designs of the decoder include the feature blending module, late locality-specific encoding, and prediction module. The feature blending module is built using either the Transformer or Conformer models, both based on the self-attention mechanism. The late locality- specific encoding is a set of parameters for each station to represent station-dependent biases. For the prediction module, we present discrete and continuous learning objectives. Based on a series of experiments, we confirmed the effectiveness of SENSE for improving earthquake early warning systems. The study findings contribute to the reduction in losses caused by earthquakes, enhancement of public safety awareness, and the effective implementation of earthquake disaster management. Future studies will focus on further optimizing the model architecture, enhancing the prediction performance, and re- ducing the training cost."}]}