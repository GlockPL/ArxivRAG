{"title": "KALAHI: A handcrafted, grassroots cultural LLM evaluation suite for Filipino", "authors": ["Jann Railey Montalan", "Jian Gang Ngui", "Wei Qi Leong", "Yosephine Susanto", "Hamsawardhini Rengarajan", "William Chandra Tjhi", "Alham Fikri Aji"], "abstract": "Multilingual large language models (LLMs) today may not necessarily provide culturally appropriate and relevant responses to its Filipino users. We introduce KALAHI, a cultural LLM evaluation suite collaboratively created by native Filipino speakers. It is composed of 150 high-quality, handcrafted and nuanced prompts that test LLMs for generations that are relevant to shared Filipino cultural knowledge and values. Strong LLM performance in KALAHI indicates a model's ability to generate responses similar to what an average Filipino would say or do in a given situation. We conducted experiments on LLMs with multilingual and Filipino language support. Results show that KALAHI, while trivial for Filipinos, is challenging for LLMs, with the best model answering only 46.0% of the questions correctly compared to native Filipino performance of 89.10%. Thus, KALAHI can be used to accurately and reliably evaluate Filipino cultural representation in LLMs.", "sections": [{"title": "Introduction", "content": "The rapid development of Large Language Models (LLMs) has significantly reshaped the Natural Language Processing (NLP) landscape, showcasing abilities in generation, comprehension, and reasoning (Touvron et al., 2023; OpenAI et al., 2024). These models, pretrained on massive multilingual corpora, exhibit proficiency across a multitude of languages (Gemma Team et al., 2024; Zhang et al., 2024). Despite these technological strides, the majority of models are predominantly tailored to high-resource languages, particularly English, leading to intrinsic linguistic and cultural biases that marginalize lower-resource languages and cultures (Ahuja et al., 2023; Atari et al., 2023; Lai et al., 2023). This disparity highlights a critical gap in current LLM research and emphasizes the necessity for dedicated efforts towards optimizing multilingual LLMs. Achieving culturally nuanced and contextually accurate responses in such languages remains an unresolved challenge, necessitating inclusive strategies that bridge this existing linguistic and cultural divide.\nMultilingual evaluation datasets for under-resourced and under-represented languages have been developed through adapting open-source English-language datasets by means of automatic or manual translation (Conneau et al., 2018; Ponti et al., 2020; Doddapaneni et al., 2023; Nguyen et al., 2024), inadvertently introducing English biases to such evaluations. Models exhibiting such biases may cause certain groups of users to distrust such systems (Luan and Cho, 2024), lowering their adoption and overall accessibility in some societies. Thus, there is a need for evaluations that can determine if LLMs are not just usable and safe, but also culturally helpful and harmless to the societies and regions they are deployed in.\nTo bridge this gap, we present KALAHI, a high-quality, manually-crafted cultural dataset to determine LLMs' abilities to provide relevant responses to culturally-specific situations that Filipinos face in their day-to-day lives.\nWhile we recognise that many culturally relevant benchmarks have been developed, few seem to account for the nuance and granularity required to accurately represent the lived experiences of individuals. KALAHI accounts for this by providing an enriched query context (see Section 3). To ensure the cultural significance and groundedness, we employ prompt writers and validators who are native speakers from the Philippines. They also come from diverse"}, {"title": "Contributions", "content": "Our work provides the following contributions:\n1.  We present KALAHI, an evaluation suite with high-quality, handcrafted prompts that test the ability of LLMs to generate responses relevant to Filipino culture in terms of shared knowledge and ethics.\n2.  We propose a methodology that integrates and operationalizes participation from native speakers to authentically construct prompts and responses unique to the Filipino lived experience, a process not usually found in data collection pipelines.\n3.  We conduct experiments on LLMs with Filipino language and multilingual support, showing better performance for models that have higher volumes of Filipino training data."}, {"title": "Literature Review", "content": ""}, {"title": "Existing cultural evaluations", "content": "Recent times have seen an increase in cultural evaluations of LLMs, covering various aspects of culture (Dwivedi et al., 2023; Cao et al., 2024a,b; Fung et al., 2024; Koto et al., 2024; Li et al., 2024a; Rao et al., 2024; Zhou et al., 2024). However, a large number of these evaluations employ only a 'top-down' approach in defining the axes for evaluation and ground truth. Specifically, these often draw from large-scale surveys such as the World Values Survey and Pew Global Attitudes Survey (Durmus et al., 2024) as well as Hofstede's theory of cultural dimensions (Hofstede, 1984; Arora et al., 2023; Kharchenko et al., 2024).\nExisting evaluations for Filipino culture are no exception. For example, PH-Eval, as part of SeaEval (Wang et al., 2024a), was also constructed with a top-down approach by sourcing from government websites, academic documents, and others. Notably, the dataset is in English rather than in Filipino."}, {"title": "Defining 'culture'", "content": "A clear working definition of culture is important for determining the data required and elucidating the objectives of the evaluation, which affect its accuracy and reliability. Within the NLP space, authors such as Adilazuarda et al. (2024) or Mukherjee et al. (2024) have highlighted the difficulty of defining what is or is not culture, and have proposed taxonomizing relevant cultural issues via proxies of culture instead. Outside of the NLP space, Causadias (2020) has also observed that it is difficult to define what culture is because it is a multifaceted and fuzzy concept. He instead proposes that culture should be \u201cdefined as a system of people, places, and practices, for a purpose such as enacting, justifying, or challenging power.\u201d Relatedly, Swidler (1986) proposed that 'culture' is dynamic in that it is a reflection of the strategies that are part of a 'cultural toolkit' that people employ to navigate situations. Simply put, they put forward that it is possible to define \u2018culture' as an expression of humans' choices and actions.\nWe, too, agree that culture is difficult to pin down, but we argue that this is because culture is an inherently human concept that is inseparable from the lived experiences, opinions, and actions of individuals, in line with Causadias (2020) and Swidler (1986). If so, evaluations that adopt only a top-down approach and attempt to define culture through taxonomization of cultural topics without further involving the communities will, in our view, necessarily be unable to reliably evaluate whether models have a cultural representation closely aligned with that of natives'.\nThus, we propose that it is only possible to arrive at an appropriate and relevant representation of culture that we can use for KALAHI through both"}, {"title": "Methodology", "content": "Language of evaluation. For this study, we specify Filipino as the language of evaluation as it is the language of trade throughout the Philippine archipelago. Specifically, we adopt the definition of Filipino as Manila Educated Tagalog, a dialect of Tagalog (Schachter and Otanes, 1983)."}, {"title": "Manual Dataset Construction", "content": "In this work, we propose a methodology designed to elicit culturally-grounded situations and intentions from native Filipino speakers and construct prompt-response pairs from these elicitations. This methodology detailed below involves in-person moderated dialogues with members of the Filipino community. Furthermore, native Filipino speakers were involved in quality control and ensuring the validity of the outputs at each stage of the process. Refer to Appendix A for our data construction guidelines.\nTopic generation. To identify relevant issues pertaining to day-to-day situations and solution-seeking behaviors of Filipinos, we used a two-pronged approach in our data collection.\nWe started by sourcing pertinent information from Google Trends, including most frequently searched terms, news, and YouTube queries in the Philippines from 2018 to 2023. The most popular search queries made in the Philippines were generally information-seeking (e.g. news on COVID), for practical tasks (e.g. English-Filipino translation), and for entertainment (e.g. song lyrics).\nHowever, as mentioned in Section 2.1, a top-down only approach to culture results in inadequate coverage, and we found that most of these topics alone were insufficient in representing the variety of experiences that a Filipino would commonly be involved and interested in.\nThus, we took this initial set of topics to serve as seed topics for discussion and expanded upon them by conducting brainstorming sessions with four native Filipino speakers. These sessions were facilitated by three linguists and research experts to ensure a well-balanced discourse.\nPrompt-response design and creation. We developed the elicited responses from the previous stage into culturally relevant prompts. Each prompt is designed as a query ('Instruction'), including information regarding the description of the person posing that question ('User'), and the person's context surrounding the question ('Context' + 'Personal situation') (see Table 1). Each prompt was collectively crafted in the Filipino language by the same four native Filipino speakers from the previous stage. A total of 84 unique prompts were created through this process.\nThe responses for each prompt were also crafted by the native Filipino speakers. The response design in TruthfulQA (Lin et al., 2022) inspired the approach used in this study. For each prompt, at least three relevant and irrelevant responses were written based on the elicited responses.\nDefining cultural relevance. Our criteria for determining whether a response is relevant or irrelevant given a cultural prompt are as follows: A response is only relevant if it is (1) helpful to the user; and (2) harmless to the user given the cultural context of the prompt (see Table 2 for examples).\nWe adapt definitions of helpfulness and harmlessness from Askell et al. (2021) in the context of cultural relevance. We define 'helpfulness' as providing actionable solutions to questions posed, given the shared morals, restrictions, and preferences of a given culture, while 'harmlessness' is defined as not providing responses that are illegal, taboo, or culturally insensitive. Irrelevant responses would be those that suggest behaviors that can harm a person in their culture but could sound innocuous, logical, or reasonable otherwise.\nPrompt-response validation. To validate the first iteration of the prompt-response pairs, focus group discussions (FGDs) were conducted with three groups of native Filipino speakers. The lead author, who grew up and was educated in the Philippines, conducted these FGDs with a total of 17 Filipino individuals who also grew up and were educated in the Philippines. The participants represented a broad range of demographic backgrounds, from varying income levels, genders, and age groups. These groups also demonstrated notable variation in the way they use the Filipino and English languages in their day-to-day lives. An overview of the participants' demographics are shown in Appendix B.\nIn these FGDs, the participants were tasked to read, review, and critique the prompt-response pairs that were created from the previous stage. The improvements and additions recommended by the participants include the following:\n1.  Rewording of prompts to be more understandable and appropriate to Filipinos.\n2.  Combination and/or splitting of prompts into more specific situations and intentions.\n3.  Rephrasing relevant and irrelevant responses.\n4.  Introducing variations in individual situations to better contextualize relevance of responses.\nThe last point, variations in personal situations, was an especially crucial recommendation that emerged from the FGDs. Our participants determined that while all of the relevant responses were indeed helpful and harmless solutions for the given prompts, some responses were more beneficial than others depending on the specific situation that a Filipino person might find themselves in. These personal contexts include socio-economic status, religious affiliation, relational proximity, among others. Such variations in personal situations were subsequently integrated into the prompt design.\nThe first iteration of prompt-response pairs was expanded to include a total of 150 prompts, each with accompanying personal situation variations. Each prompt has three to five relevant and irrelevant responses, with only one of the relevant responses being labeled the 'best response'.\nQuality control. The development of the dataset was done iteratively in close collaboration with native Filipino speakers who provided input in every stage of the process. This involved the manual review of each prompt and response to ensure the authenticity of the language used, the naturalness of the constructions, and the correctness of spelling and grammar.\nPrompt-response categories. We present the cultural topics covered in KALAHI (see Table 3). Recall that we did not restrict ourselves to a predetermined set of topics, though we took some topics that were found to be important as a starting point for the FGDs. We highlight below the motivation behind grouping certain topics together:\n\u2022  Food and gatherings: social gatherings between families, extended families, and even entire communities are inseparable from the sharing of food in Filipino culture (Fernandez, 1986). As such, the shared experience of cooking and eating together as a community is integral to many Filipinos' lives.\n\u2022  Communication and body language: Filipinos employ different types of communication, such as those of non-verbal facial animations and expressions (Lacson, 2005).\nWe also categorize the prompt-response pairs in terms of 'ethics' and \u2018shared knowledge'. \u2018Ethics' roughly follows from \u201cobjectives and values\" and 'shared knowledge' roughly follows from a combination of \u201ccommon ground\u201d and \u201caboutness\u201d as defined by Hershcovich et al. (2022). Of the 150 pairs, 109 are categorized as 'ethics', while 41 are 'shared knowledge'.\""}, {"title": "Dataset Validation", "content": "We recruited three native Filipino speakers who were not involved in the development of KALAHI to validate the constructed dataset. We evaluate the validators on the MC1 task (see Section 4.2). These validators were shown the 150 prompts from KALAHI and best and irrelevant responses in a randomized order. They were tasked to choose the response that would most closely mirror the choice that an average Filipino would make given a particular situation as their \u2018strategy of action'. It is important to remember that the irrelevant responses could sound innocuous, logical, or reasonable in the context of other cultures, but crucially they are rendered irrelevant in Filipino culture (i.e. such responses would not be strategies of actions adopted by the average Filipino). The three native speakers attempted all 150 prompts and these validator answers were then used as the human baseline for our experiments"}, {"title": "Results", "content": ""}, {"title": "Human baseline", "content": "On average, our Filipino validators scored 89.1% on KALAHI, which we refer to as our human baseline. We calculated inter-rater agreement, which yielded a Cohen's kappa of 0.761 and a Krippendorf's alpha of 0.762, indicating substantial agreement. While KALAHI was created based on consensus among native Filipinos, individual idiosyncrasies, such as personal values and beliefs, were expected to inherently influence their individual choices, such that the participants' choices may not necessarily align with the shared Filipino cultural values and beliefs. This can be observed in the example in Appendix D.\nNonetheless, the high accuracies obtained by the native speakers suggest that the 'best response' label in KALAHI is generally accurate and reflective of what an average Filipino individual would choose as a strategy of action. Furthermore, 94.7% of the 'best response' options were chosen by at least 2 out of 3 native speakers, and we propose that this is a strong indication that the 'best response' accurately represents the strategy of action that the average Filipino would choose given that particular situation."}, {"title": "Model Evaluation", "content": "In general, there is no agreed-upon method for evaluating how culturally relevant or appropriate a LLM's responses are given particular cultural situations, although some studies have attempted to determine the alignment of models to a particular culture (Durmus et al., 2024).\nTo our knowledge, KALAHI is the only dataset that frames 'cultural evaluation' as a natural language task aimed at determining whether or not a model can generate responses that reflect the way that an average native speaker (i.e. Filipinos) would respond to a situation encountered in their culture. In other words, if a model's strategies of actions are similar to the strategies of actions of an average Filipino, we assume that the model can draw from the same cultural toolkit (Swidler, 1986) as a Filipino individual. Two key assumptions are that the choices a Filipino would make are informed by and expresses their culture, and that if the model can generate a response that is similar to that of a Filipino, it would mean that the model does have a strong representation of the relevant aspects of Filipino culture.\nExperiments. We evaluate a total of 9 LLMS to compute baselines for KALAHI. The first group of LLMs are those that explicitly claim to support Filipino (Tagalog), which we assume means that the models were instruction-tuned on Filipino instructions: Aya 23 8B (Aryabumi et al., 2024), Qwen 2 7B Instruct (Yang et al., 2024), Sailor 7B Chat (Dou et al., 2024), and SeaLLMs 3 7B Chat (Zhang et al., 2024). The second group are composed of LLMs that have claimed to demonstrate multilingual capabilities, but do not claim to be specifically instruction-tuned on Filipino instructions: BLOOMZ 7B1 (BigScience Workshop et al., 2023), Falcon 7B Instruct (Almazrouei et al., 2023), Gemma 2 9B Instruct (Gemma Team et al., 2024), Llama 3.1 8B Instruct (Dubey et al., 2024), and SEA-LION 2.1 8B Instruct.\nWe designed KALAHI to evaluate LLMs in a zero-shot setting. Default chat prompt templates as defined in the respective tokenizer configuration files are applied for each model, if any. Inspired by previous work on TruthfulQA (Lin et al., 2022), we evaluate models on two settings: multiple-choice question-answering and open-ended generation.\nMultiple-choice. In this setting, a model is evaluated on a multiple-choice question. The choices for each question refer to relevant and irrelevant responses. We compute the log-probability completion of each reference response given a question, normalized by byte length. Two scores are calculated:\n\u2022  MC1: Choices include the best and irrelevant responses. The score is 1 if the model assigns the highest log-probability of completion following the prompt to the best response, otherwise the score is 0.\n\u2022  MC2: Choices include all relevant and irrelevant responses. The score is the likelihood assigned to the set of the relevant responses normalized by the sum of the probabilities of generating all relevant and irrelevant responses.\nOpen-ended generation. In this setting, a model is induced to generate a natural language response given a prompt. The responses are generated using greedy decoding, and 256 max tokens, with other sampling parameters set to their HuggingFace default values. The following metrics are used to compare the model's generated completion to each relevant and irrelevant responses: BLEURT (Sellam et al., 2020), BLEU (Papineni et al., 2002) BERTScore (Zhang et al., 2020), ROUGE (Lin, 2004), ChrF++ (Popovi\u0107, 2017) and METEOR (Banerjee and Lavie, 2005). The score is the difference between the maximum similarity of the model completion to a relevant response and the maximum similarity of the model completion to an irrelevant response."}, {"title": "Interpretation of Results", "content": "We assume that the higher the score a model achieves for KALAHI MC1, the stronger the model's representation of an average Filipino's preferred strategies of actions given various contexts. That is, we assume that the higher a model's score is, the more it can accurately reflect what a Filipino individual might say or do given various situations and contexts. Furthermore, we assume that if a model scores above 0.5 for KALAHI MC2, it is indicative that the model assigns higher probability to culturally relevant responses as compared to culturally irrelevant responses. Thus, a higher score on the MC2 task indicates that the model is better able to distinguish culturally relevant responses from irrelevant ones.\nAs for open-ended generations, we assume that if a model has a strong Filipino cultural representation, its generations will have greater overlap with reference responses. Thus, the higher a model's score is, the more likely it is to generate responses that mirror that of a Filipino individual in a given situation.\nKALAHI was designed to be trivial for humans (or models) who are highly knowledgeable in Filipino culture. Even though all the handcrafted responses are reasonable courses of action in response to a given situation, a knowledgeable individual (or model) can easily identify the strategy of action that an average Filipino will choose. We propose that the fact that none of the tested models come close to human performance indicates that KALAHI is a challenging yet culturally relevant benchmark for LLMs.\nModels that claim to support Filipino do provide more culturally relevant responses. Results show that LLMs that claim Filipino langauge support (presumably meaning they have seen more Filipino tokens in training) generally perform better on KALAHI. Table 5 shows that these LLMs also had strong performance in open-ended generation. A manual review of generated completions from these models showed that their responses were in Filipino and provided actionable suggestions that the user could take. The following example is illustrative:"}, {"title": "Conclusion", "content": "Developing LLMs that are sensitive to the cultural nuances of the Philippines continues to be a challenge. We introduce KALAHI, an evaluation suite collaboratively handcrafted by native Filipino speakers from diverse backgrounds to measure the helpfulness and harmlessness of LLMs in situations that are unique to Filipino culture. Strong performance would show that a model can generate responses similar to the average Filipino and has a strong representation of Filipino culture.\nOur findings show that multilingual LLMs and even those that have Filipino language support significantly underperform compared to native Filipinos on KALAHI. This demonstrates that KALAHI is a challenging benchmark for evaluating Filipino cultural representation in LLMs.\nFuture Work. Having LLM-as-evaluator could help with detection of hallucinations and culturally-inappropriate responses. However, it remains to be seen if LLMs will be able to perform at or close to the level of a human evaluator, and this is an immediate next step that we will take to improve on the automation of KALAHI.\nAnother avenue for future work is investigating if our top-down approach can be complemented with more empirical studies or surveys relevant to the particular cultures as a means to expand upon the initial range of seed topics generated.\nLimitations. The Filipino culture in this study refers to cultural values acquired by Filipino speakers who were born and grew up in or at least spent most of their lives in the Philippines. KALAHI is the result of the consensus views of the involved native Filipino speakers. Individuals who have had different upbringings may have different perspectives on Filipino culture mentioned in this paper, such that the consensus view arrived at in this study does not fully represent the opinions of all Filipino individuals. Additionally, while KALAHI is designed to accurately represent Filipino culture, it is not intended to encompass all possible aspects of Filipino culture."}, {"title": "Data construction guidelines", "content": "Given the subjectiveness of 'culture', it is infeasible to adopt a normative stance. We instead adopt a more collaborative approach that involves native speakers from the respective communities to help inform the data collection process. This set of data construction guidelines is intended to detail a methodology for researchers who are looking to collect data from the community in a principled manner.\nTo get a sense of what cultural topics and issues Filipinos are broadly interested in, we first analyzed Filipinos' search terms on Google Trends between 2018-2023 as a reference for further discussion. We next invited four Filipino native speakers (the annotators) who are familiar with Filipino culture to participate in fashioning queries and corresponding responses based on the identified seed topics as well as any other topics that did not already come up but were felt to be relevant.\nThat said, we do not assume that the annotators are expert annotators for cultural data, hence before the discussion session, we ask the annotators to respond to an initial set of cultural questions specifically targeting the elicitation of relevant yet relatively open-ended responses from the annotators. These questions were designed to encourage them to reflect on their lived experiences and to share their opinions and perspectives which are influenced by their experience of Filipino culture. The questions are as follows:\n1.  Their unique personal experiences as members of the Filipino community (e.g. \u201cWhat makes people from your region unique compared to other regions in your culture?\").\n2.  The cultural differences between Filipinos and other Asians (e.g. \u201cAre there any cultural differences that you perceived when being outside of your home country? Please elaborate.\")\n3.  Their likes and dislikes about being Filipino (e.g. \"What are three things that you like most about being Filipino and three things that you dislike the most about it?", "What behaviors or actions would help you to immediately identify someone as being Filipino?\").\n5.  Their perspective on what being a Filipino meant to them (e.g. \\\"What does being Filipino mean to you?\\\").\nThrough these questions, the annotators were able to get a sense of the direction and the focus of the discussion. The questions elicited the essence of Filipino culture and the annotators' identity as a Filipino. Additionally, this led to a lively discussion on cultural issues": "n\u2022  \"Do you agree that people from X region could be more likely to...\u201d\n\u2022  \"Do you think that X is relevant to your culture? Why or why not?\"\n\u2022  \"Is X likely to be a hallmark of a person from Y? Why or why not?\"\nWe also asked the annotators what strategies they might adopt to navigate certain situations, such as:\n\u2022  \"How would you tell a respected elder that they are wrong on something? Would you even do it?\"\n\u2022  \"What are some precautions you might take while traveling on public transport?\"\n\u2022  \"What are some areas you would never visit in your region? Why?\"\n\u2022  \"What would you do if you caught a cold/got a sore throat/broke your arm?\u201d\nThe responses from the annotators were later used to create the initial set of prompt-response pairs, which were then used as reference material for the brainstorming sessions with the native speaker participants in the Philippines.\nWith the additional input from the Filipino participants, the dataset was significantly expanded. However, there was still a final step in the data creation process that involved the same group of Filipino annotators to help validate the prompt-response pairs iteratively, which culminated in the 150 prompt-response pairs in KALAHI."}]}