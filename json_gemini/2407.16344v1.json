{"title": "SOAP: Enhancing Spatio-Temporal Relation and Motion Information Capturing for Few-Shot Action Recognition", "authors": ["Wenbo Huang", "Jinghui Zhang*", "Xuwei Qian", "Zhen Wu", "Meng Wang", "Lei Zhang"], "abstract": "High frame-rate (HFR) videos of action recognition improve fine- grained expression while reducing the spatio-temporal relation and motion information density. Thus, large amounts of video sam- ples are continuously required for traditional data-driven training. However, samples are not always sufficient in real-world scenar- ios, promoting few-shot action recognition (FSAR) research. We observe that most recent FSAR works build spatio-temporal rela- tion of video samples via temporal alignment after spatial feature extraction, cutting apart spatial and temporal features within sam- ples. They also capture motion information via narrow perspectives between adjacent frames without considering density, leading to insufficient motion information capturing. Therefore, we propose a novel plug-and-play architecture for FSAR called Spatio-temporal frame tuple enhancer (SOAP) in this paper. The model we designed with such architecture refers to SOAP-Net. Temporal connections between different feature channels and spatio-temporal relation of features are considered instead of simple feature extraction. Com- prehensive motion information is also captured, using frame tu- ples with multiple frames containing more motion information than adjacent frames. Combining frame tuples of diverse frame counts further provides a broader perspective. SOAP-Net achieves new state-of-the-art performance across well-known benchmarks such as SthSthV2, Kinetics, UCF101, and HMDB51. Extensive em- pirical evaluations underscore the competitiveness, pluggability, generalization, and robustness of SOAP.", "sections": [{"title": "1 INTRODUCTION", "content": "Ubiquitous videos in daily lives are rapidly accelerating the devel- opment of multimedia analytic research. As a fundamental task, action recognition is experiencing an explosive demand in a wide range of applications including intelligent surveillance, video un- derstanding, and health monitoring [4, 15, 20]. Progress in video recorders contributes to high frame-rate (HFR) videos, with more similar frames per second improving the expression of fine-grained actions [17]. As the example shown in Figure 1, we can explicitly observe that the timeline and displacement of object in HFR video frames are much subtler than those in low frame-rate (LFR) video frames, better reflecting fine-grained actions. However, the spatio- temporal relation and motion information density decrease with the improvement of video fluency [36]. Therefore, a larger amount of video samples are continuously required to train data-driven models. Unfortunately, samples for target actions such as \"falling down\" are usually insufficient and hard to collect in real-world sce- narios. Contemporary few-shot learning mitigates data dependence by transferring knowledge from a few samples, promoting few-shot action recognition (FSAR) research.\nAccording to data characteristics of HFR videos, two prevailing challenges of FSAR exist. Challenge 1: Optimizing spatio-temporal relation construction. Spatial and temporal features work as a whole in video samples, only focusing on spatial information makes mod- els misidentify horizontal or vertical actions such as \"pushing\", \"pulling\", \"putting up\", and \"putting down\". However, the spatio- temporal relation of HFR videos is subtle, making the construction challenging. Challenge 2: Comprehensive motion information captur- ing. As an exclusive characteristic of videos, motion information plays a crucial role in helping models recognize target actions in a dynamic manner. However, the difficulties in motion information"}, {"title": "2 RELATED WORKS", "content": "The core goal of few-shot learning is to recognize unseen classes from only few samples. Unlike traditional deep learning, few-shot learning utilizes episodic training where training units are struc- tured as similar tasks with small labeled sets. Three primary groups of few-shot learning are data-augmentation based, optimization- based, and metric-based paradigms. In the first data-augmentation paradigm, generating additional samples to supplement available data is a key feature. Specifically, MetaGAN [39] utilizes genera- tive adversarial networks (GANs) and statistics of existing samples to synthesize data. The most representative optimization-based method is MAML [7], which identifies a model parameters set and adapts it to individual tasks via gradient descent. The metric-based paradigm is well-known and widely used for its simplicity and effectiveness. It leverages similarity between support samples to classify query samples into corresponding classes. Prototypical Networks [22] construct prototypes based on class centroids and then classify samples by measuring distance to each prototype. Most few-shot learning paradigms are applied to image classifica- tion, with fewer in the field of videos. Our SOAP belongs to the metric-based paradigm for FSAR, focusing on improving prototype representation ability."}, {"title": "2.2 Action Recognition", "content": "Compared to image classification, action recognition is a more complex and extensively researched problem in the community due to the spatio-temporal relation and motion information. Pre- vious works [3, 19, 26, 32] have typically utilized 3D backbones to construct spatio-temporal relations, while optical flow is applied by additional networks to inject video motion information for ac- tion recognition, resulting in promising results. However, these works with high acquisition costs are all designed for traditional data-driven training without considering insufficient samples in real-world scenarios. In contrast, SOAP is specifically designed for a more realistic few-shot setting."}, {"title": "2.3 Few-Shot Action Recognition", "content": "Existing methods of FSAR mainly focus on metric-based paradigm for effective prototype construction. Among them, CMN [43, 44] introduces a multi-saliency embedding algorithm for key frame encoding, improving the prototype representation; ARN [38] cap- tures short-range dependencies using 3D backbones with a self- trained permutation invariant attention; OTAM [1] aligns support and query with a dynamic time warping (DTW) algorithm. Joint spatio-temporal modeling approaches such as TA2N [13], ITA [40], STRM [24], and SloshNet [34] employ spatial-temporal frameworks to address the support-query misalignment from multiple perspec- tives; Temporal relation is emphasized by TRX [18], using a Cross Transformer for sub-sequence alignment and achieving notable improvement; HyRSM [30] learns the task-specific embedding by exploiting the relation within and cross videos; SA-CT [41] com- plement the temporal information by learning the spatial relation. The temporal alignment all operated after spatial feature extrac- tion. Diverse types of information including optical flow [31] and depth [8] are introduced by extra networks, increasing computa- tions. MoLo [29] and MTFAN [33] explicitly extract motion infor- mation from raw videos and leverage temporal context for FSAR. Although achieving promising performance, these works are either cut apart spatial and temporal features or ignore motion infor- mation density. Therefore, our SOAP focuses on spatio-temporal relation and motion information capturing."}, {"title": "3 METHODOLOGY", "content": "Classifying an unlabeled query into one of the support classes is the inference goal of FSAR. The support set has at least one labeled sample per class. Following the prior works [1, 18, 29, 30], we randomly select few-shot tasks from training set for episodic training. In each task, the support set S contains N classes and K samples per class, i.e., N-way K-shot setting. The query set Q in each task has samples to be classified. Uniformly sampling F frames of each video, we define the kth (k = 1,\u2026\u2026, K) sample of the cth (c = 1,..., N) class of support set S as:\n$S_{c}^{k} = [s_{1}^{ck},..., s_{F}^{ck}] \\in \\mathbb{R}^{F \\times C \\times H \\times W}$  (1)\nNotions used are F (frames), C (channels), H (height), and W (width), while the sample with F frames of query is defined as:\n$Q = [q_1,..., q_F] \\in \\mathbb{R}^{F \\times C \\times H \\times W}$ (2)\nModels of FSAR need to predict labels for query samples with the guidance of the support set."}, {"title": "3.2 Overall Architecture", "content": "An overview of SOAP-Net is provided in Figure 2. For clear descrip- tion, we take the Q and cth of S as an example to illustrate the whole process of our method. First, video samples are decoded into frames at fixed intervals to serve as the input. Then, support and query are sent into three main modules of SOAP, i.e., 3DEM, CWEM, and HMEM. To be specific, 3DEM establishes spatio-temporal relation of features, while CWEM adaptively calibrates temporal connec- tions between channels. Instead of solely concentrating on motion"}, {"title": "3.3 3-Dimension Enhancement Module", "content": "For the relation construction between spatial and temporal infor- mation, 3DEM is designed based on 3D convolution. The structure of this module is shown in Figure 3. Firstly, 3DEM averages the support and query across the channels, resulting in spatio-temporal tensors. Then the corresponding tensors are reshaped. The above operation can be formulated as:\n$\\begin{aligned}\nS_{1}^{ck} &= \\text{Reshape}_{1}\\left(\\frac{1}{C}\\sum_{g=1}^{C} S^{ck}(:, g, :, :)\\right) \\in \\mathbb{R}^{1 \\times F \\times H \\times W},\\\\\nQ_{1} &= \\text{Reshape}_{1}\\left(\\frac{1}{C}\\sum_{g=1}^{C} Q(:, g, :, :)\\right) \\in \\mathbb{R}^{1 \\times F \\times H \\times W}.\n\\end{aligned}$ (3)\nA 3D convolution is used for constructing the spatio-temporal relation and another reshape operation is used for shape recovery:\n$\\begin{aligned}\n\\tilde{S}_{1}^{ck} &= \\text{Reshape}_{2}\\left(\\text{Conv3D}\\left(S_{1}^{ck}\\right)\\right) \\in \\mathbb{R}^{F \\times 1 \\times H \\times W}, \\\\\n\\tilde{Q}_{1} &= \\text{Reshape}_{2}\\left(\\text{Conv3D}\\left(Q_{1}\\right)\\right) \\in \\mathbb{R}^{F \\times 1 \\times H \\times W}.\n\\end{aligned}$ (4)\nFinally, spatio-temporal relation are fed into a Sigmoid activation and residually connected with the input to generate the 3D prior knowledge before feature extraction:\n$\\begin{aligned}\n\\hat{S}_{1}^{ck} &= S^{ck} + \\text{Sigmoid} \\left(\\tilde{S}_{1}^{ck}\\right) \\times S^{ck}, \\\\\n\\hat{Q}_{1} &= Q + \\text{Sigmoid} \\left(\\tilde{Q}_{1}\\right) \\times Q.\n\\end{aligned}$ (5)\nFor each input channel, 3DEM can assist the spatio-temporal rela- tion construction."}, {"title": "3.4 Channel-Wise Enhancement Module", "content": "Features located in different channels have temporal connections between each channel. Inspired by SE [11], we design this module with the addition of a simple 1D convolution. The structure of CWEM is detailed in Figure 4. First, a spatial average pooling is performed on the support and query, followed by a 2D convolution that expands the channel number to Cr, represented as:\n$\\begin{aligned}\n\\check{S}_{2}^{ck} &= \\text{Conv2D}_{1}\\left(\\frac{1}{H \\times W}\\sum_{i=1}^{H} \\sum_{j=1}^{W} S^{ck}(:,:, i, j)\\right) \\in \\mathbb{R}^{F \\times C_{r} \\times 1 \\times 1}, \\\\\n\\check{Q}_{2} &= \\text{Conv2D}_{1}\\left(\\frac{1}{H \\times W}\\sum_{i=1}^{H} \\sum_{j=1}^{W} Q(:,:, i, j)\\right) \\in \\mathbb{R}^{F \\times C_{r} \\times 1 \\times 1}.\n\\end{aligned}$ (6)"}, {"title": "3.5 Hybrid Motion Enhancement Module", "content": "In previous works [29, 31], motion information plays a significant role. However, we find that focusing only on the motion information between adjacent frames is insufficient, as subtle displacements are hard to detect. In order to capture more motion information using HMEM, we extend perspective from adjacent frames to frame tuples and apply the combination of multiple scales. The structure of HMEM is demonstrated in Figure 5. We define a set O as a hyperparameter, where the value of element T (T \u2208 O, T < F and T\u2208 N*) represents frame counts of a tuple and the cardinality of set |O| denotes the number of branches. Being achieved by sliding window algorithm SW (\u00b7, \u00b7), frame tuple sets of support and query with element index t (t\u2208 [1, F - T + 1]) can be represented as:\n$\\begin{aligned}\n\\text{SW}\\left(S^{ck}, T\\right) &= \\left[..., S_{t}^{ck}, S_{t+1}^{ck},..., S_{t+T-1}^{ck},...\\right], \\\\\n\\text{SW}\\left(Q, T\\right) &= \\left[..., q_{t}, q_{t+1},..., q_{t+T-1},...\\right].\n\\end{aligned}$ (10)\nWe concatenate tuple differences in the first dimension, preparing for motion information calculation. For a simple description, we choose the eth (e \u2208 [1, |O|] and e \u2208 N*) branch as an example:\n$\\begin{aligned}\nM_{e}^{sck} &= \\text{Concat}\\left(..., \\text{Conv2D}^{M}\\left(S_{t+1}^{ck} - S_{t}^{ck}\\right),...\\right), \\\\\nM_{e}^{Q} &= \\text{Concat}\\left(..., \\text{Conv2D}^{M}\\left(q_{t+1} - q_{t}\\right),...\\right).\n\\end{aligned}$ (11)\nMotion information with different scales can be captured by setting multiple branches. Then we concatenate them along the first di- mension, achieving tensors belong to $\\mathbb{R}^{F \\times C \\times H \\times W}$. Function Z (\u00b7)"}, {"title": "3.6 Prototype Construction", "content": "Triple prior guidance are added into the raw support and query before feature extraction:\n$\\begin{aligned}\n\\overline{S}^{ck} &= (\\hat{S}_{1}^{ck} + \\hat{S}_{2}^{ck} + \\hat{S}_{3}^{ck}) + S^{ck} = [s_{1}^{ck},..., s_{F}^{ck}] \\in \\mathbb{R}^{F \\times C \\times H \\times W}, \\\\\n\\overline{Q} &= (\\hat{Q}_{1} + \\hat{Q}_{2} + \\hat{Q}_{3}) + Q = [\\tilde{q}_{1},..., \\tilde{q}_{F}] \\in \\mathbb{R}^{F \\times C \\times H \\times W}\n\\end{aligned}$ (15)"}, {"title": "3.7 Training Objective", "content": "The linear layer \u039b (\u00b7) is applied on Qf to ensure the same operation as Sck. By obtaining the closest distance between Qf and P\u00ba, model can predict the label \u1ef9 of Q, as:\n$\\tilde{y} = \\underset{c}{\\text{arg min}} ||P_{c} - (\\Lambda (Q^{f}))|| \\text{D(Q,Sc)}$ (18)\nThe cross-entropy loss Lce is selected as the objective loss for training, which can be calculated with the ground truth y:\n$L_{c e}=-\\frac{1}{N} \\sum_{i=1}^{N} y_{i} \\log (\\tilde{y}_{i}).$ (19)"}, {"title": "4 EXPERIMENTS", "content": "We select four widely used datasets, i.e., SthSthV2 [9], Kinetics [2], UCF101 [23], and HMDB51 [12]. Apart from SthSthV2, which is temporal-related, other three datasets are all spatial-related. When decoding videos, we set the sam- pling intervals to every 1 frame. For simulating various fluency, the intervals can be adjusted. Building upon data split from prior works [1, 38, 43], we split our datasets into training, validation, and testing sets. A task within each set, classifying query samples into one of the support classes serves as a training unit.\nFollowing TSN [27], 8 frames (F = 8) resized to 3 \u00d7 256 \u00d7 256 are uniformly and sparsely sampled each time. Data augmentations in training include random crops to 3 \u00d7 224 \u00d7 224 and horizontal flipping, while only a center crop is used during testing. As pointed out in previous works [1], many action classes in SthSthV2 are sensitive to horizontal direction (e.g., \"Pulling S from left to right\"\u00b9). Hence, we avoid horizontal flipping on this dataset."}, {"title": "4.2 Comparison with Various Methods", "content": "Based on the average accuracy reported in Table 1, we have the follow- ing observation. Our SOAP-Net outperforms other methods and achieves SOTA performance. For example, on the Kinetics dataset under the 1-shot setting, SOAP improves the current SOTA perfor- mance of MoLo [29] from 75.2% to 81.1%. We believe the motion information within adjacent frames is insufficient, despite MoLo in- troducing it. Similar improvements are also found in other datasets under diverse few-shot settings, showing that spatio-temporal re- lation and comprehensive motion information from SOAP lead to significant improvement in FSAR. It is worth mentioning that SOAP-Net even surpasses multimodal methods."}, {"title": "4.2.2 Comparison with ViT-B Backbone Methods", "content": "ResNet-50 serves as the backbone network in most previous works, while ViT-B is rarely used in FSAR. For a comprehensive comparison, we implemented several methods and replaced their backbones with ViT-B, finding it outperformed ResNet-50 in FSAR due to larger model capacity. The previous SOTA performance is also achieved by MoLo, benefiting from optimizing spatio-temporal relation con- struction and comprehensive motion information. A similar trend is observed on ResNet-50 and ViT-B. These comparisons reveal the competitiveness of our proposed method."}, {"title": "4.3 Essential Components and Factors", "content": "We first divide SOAP into three key components: 3DEM, CWEM, and HMEM. Experiments on individual or combined components are conducted on two datasets under diverse few-shot settings. As shown in Table 2, results indi- cate that each SOAP component can improve FSAR performance. However, the most significant boost comes from HMEM, likely due"}, {"title": "4.3.2 Frame Tuples and Branches Design", "content": "Results under vari- ous hyperparameter O settings of HMEM is demonstrated in Ta- ble 3. Each element represents frame counts, with the cardinality of set |O| equaling number of branches. Performance generally improves with more branches. Specifically, any frame tuple size improves with HMEM having one branch. Better results occur with two branches. Best performance occurs at 3 branches under O = {1, 2, 3} setting. This aligns with intuition comprehensive mo- tion information contributes more for FSAR. However, excessive branches risk degradation. Too much motion information overlap does not provide useful information. For example, O = {4} acts similarly to two O = {2} configurations together, implying that they perform worse than other non-overlapping settings. While O = {1} provides motion information between frames not conflict- ing with other O settings for building frame tuples, introducing comprehensive motion information."}, {"title": "4.3.3 The Impact of Temporal Order", "content": "Up to this point, we be- lieve the frame tuple should follow the temporal order for better"}, {"title": "4.4 Research on How SOAP Works", "content": "The CAM [21] for \u201ccrossing river\u201d from the Kinetics dataset is shown in Figure 6. The first row shows raw HFR video frames. We observe that the timeline and displace- ment of the moving peoples are very subtle, making it difficult to detect the spatio-temporal relation and motion information. In the second row, we see that the model attends more to the background than moving peoples. This result supports our observation in HFR videos. With the help of SOAP, we detect that the focus shifts from the background to the moving peoples, despite the subtle timeline and displacement. Based on this analysis of the CAM, we con- clude that motion information is crucial in FSAR. Spatio-temporal relation and comprehensive motion information from SOAP can significantly improve feature representation."}, {"title": "4.4.2 T-SNE Visualization", "content": "In the 5-way task, feature distribu- tions can be visualized using t-SNE [25]. We select five represen- tative yet challenging support action classes from Kinetics: \"ski jumping\", \"snowboarding\u201d, \u201cskateboarding\u201d, \u201ccrossing river\", and \"driving tractor\". The t-SNE visualization is shown in Figure 7, with different colors and markers representing distinct action classes. Without SOAP , the five classes are generally separable"}, {"title": "4.5 Pluggability of SOAP", "content": "In Table 5, we experimentally demonstrate that the SOAP generalizes well to other methods by inserting it into widely used methods including TRX, HyRSM, and MoLo. Using Kinetics as an example, TRX benefits from compre- hensive motion information and achieves 7.6% gains in the 1-shot setting and 8.5% in the 5-shot setting. Similar improvements are also observed in the other two methods. These results fully prove that optimizing spatio-temporal relation and comprehensive motion information are particularly useful for feature extraction."}, {"title": "4.5.2 On Multimodal Methods", "content": "Backbone networks serve as feature extractors in multimodal methods for FSAR. Additional information from other modalities enhances performance. In most multimodal methods, depth or optical flow information augments RGB data. Apart from the RGB backbone, additional depth-specific or optical-flow-specific networks are also utilized. As depth and optical flow are both derived from raw RGB frames, they inherently contain spatio-temporal relation and motion information. There- fore, we hypothesize SOAP can also benefit multimodal methods and implement several multimodal methods equipped with SOAP for evaluation. Results in Table 6 demonstrate that all selected"}, {"title": "4.6 Generalization Study", "content": "In this study, we aim to simulate different levels of frame-rate by varying the sampling interval from a minimum of 1 to a maximum of 6 and conduct a series of experiments under a well-established 5-way 5-shot setting. As shown in Figure 8, the performance tends to decrease gradually with the increase of frame-rate. We find that recent FSAR methods all experience drastic degradation in perfor- mance with HFR videos while SOAP-Net indicates performance stability, demonstrating the superiority of SOAP across varying frame-rate. These phenomena also highlight the key role of spatio-temporal relation optimization and comprehensive motion infor- mation in advancing FSAR performance."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose the plug-and-play SOAP to optimize spatio-temporal relation construction and capture comprehensive motion information for FSAR. SOAP takes into account temporal connections across feature channels and spatio-temporal relation within features. It combines frame tuples of various frame counts to provide a broader perspective for motion information. Considering the competitiveness, pluggability, generalization, and robustness of SOAP, we hope and believe that our work will offer valuable insights for future research in multimedia analytic."}]}