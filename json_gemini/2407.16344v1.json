{"title": "SOAP: Enhancing Spatio-Temporal Relation and Motion Information Capturing for Few-Shot Action Recognition", "authors": ["Wenbo Huang", "Jinghui Zhang", "Xuwei Qian", "Zhen Wu", "Meng Wang", "Lei Zhang"], "abstract": "High frame-rate (HFR) videos of action recognition improve fine- grained expression while reducing the spatio-temporal relation and motion information density. Thus, large amounts of video sam- ples are continuously required for traditional data-driven training. However, samples are not always sufficient in real-world scenar- ios, promoting few-shot action recognition (FSAR) research. We observe that most recent FSAR works build spatio-temporal rela- tion of video samples via temporal alignment after spatial feature extraction, cutting apart spatial and temporal features within sam- ples. They also capture motion information via narrow perspectives between adjacent frames without considering density, leading to insufficient motion information capturing. Therefore, we propose a novel plug-and-play architecture for FSAR called Spatio-temporal frame tuple enhancer (SOAP) in this paper. The model we designed with such architecture refers to SOAP-Net. Temporal connections between different feature channels and spatio-temporal relation of features are considered instead of simple feature extraction. Comprehensive motion information is also captured, using frame tu- ples with multiple frames containing more motion information than adjacent frames. Combining frame tuples of diverse frame counts further provides a broader perspective. SOAP-Net achieves new state-of-the-art performance across well-known benchmarks such as SthSthV2, Kinetics, UCF101, and HMDB51. Extensive em- pirical evaluations underscore the competitiveness, pluggability, generalization, and robustness of SOAP.", "sections": [{"title": "1 INTRODUCTION", "content": "Ubiquitous videos in daily lives are rapidly accelerating the devel- opment of multimedia analytic research. As a fundamental task, action recognition is experiencing an explosive demand in a wide range of applications including intelligent surveillance, video un- derstanding, and health monitoring [4, 15, 20]. Progress in video recorders contributes to high frame-rate (HFR) videos, with more similar frames per second improving the expression of fine-grained actions [17]. As the example shown in Figure 1, we can explicitly observe that the timeline and displacement of object in HFR video frames are much subtler than those in low frame-rate (LFR) video frames, better reflecting fine-grained actions. However, the spatio- temporal relation and motion information density decrease with the improvement of video fluency [36]. Therefore, a larger amount of video samples are continuously required to train data-driven models. Unfortunately, samples for target actions such as \"falling down\" are usually insufficient and hard to collect in real-world sce- narios. Contemporary few-shot learning mitigates data dependence by transferring knowledge from a few samples, promoting few-shot action recognition (FSAR) research.\nAccording to data characteristics of HFR videos, two prevailing challenges of FSAR exist. Challenge 1: Optimizing spatio-temporal relation construction. Spatial and temporal features work as a whole in video samples, only focusing on spatial information makes mod- els misidentify horizontal or vertical actions such as \"pushing\", \"pulling\", \"putting up\", and \"putting down\". However, the spatio-temporal relation of HFR videos is subtle, making the construction challenging. Challenge 2: Comprehensive motion information captur- ing. As an exclusive characteristic of videos, motion information plays a crucial role in helping models recognize target actions in a dynamic manner. However, the difficulties in motion information capturing is exaggerated by low motion information density from HFR videos and limited frames processed at one time with main-stream methods. In summary, the challenges mentioned above not only stem from HFR videos but are also aggravated by the insuffi- cient samples in few-shot settings.\nIn FSAR, the metric-based paradigm predominates due to its simplicity and efficacy. It embeds samples into action prototypes to calculate support-query distances for classification in an episodic task [1, 18, 30, 41, 43]. We observe that these works prioritize tem- poral alignment after spatial feature extraction, cutting apart spatial and temporal features within samples and overlooking the impor- tance of motion information capture. Some works [29, 31, 33] in- corporate motion information into action recognition and achieve remarkable performance. However, the motion information is cap- tured between adjacent frames. This narrow perspective inevitably overlooks density of motion information and results in inadequate capturing. So far, no solution exists for both challenges.\nMotivated by aforementioned challenges, we propose a novel plug-and-play architecture for FSAR called Spatio-temporal frame tuple enhancer (SOAP). The model we designed with such architec- ture is defined as SOAP-Net. The cores are optimizing construction of spatio-temporal relation and capturing comprehensive motion information. For the first goal, simply extracting features from video frames is insufficient. These features are located in different channels and have temporal connections between each channel. Spatio-temporal relation within features also play a key role in FSAR. For the second goal, we consider motion information density and find that frame tuples with multiple frames contain richer mo- tion information than adjacent frames. Combining frame tuples of various frame counts further provides a broader perspective. To be specific, SOAP has three components: 3-Dimension Enhancement Module (3DEM) uses a 3D convolution for spatio-temporal relation construction and Channel-Wise Enhancement Module (CWEM) calibrates channel-wise feature responses adaptively while Hybrid Motion Enhancement Module (HMEM) applies a broader perspec- tive to help models capture comprehensive motion information."}, {"title": "2 RELATED WORKS", "content": "2.1 Few-Shot Learning\nThe core goal of few-shot learning is to recognize unseen classes from only few samples. Unlike traditional deep learning, few-shot learning utilizes episodic training where training units are struc-tured as similar tasks with small labeled sets. Three primary groups of few-shot learning are data-augmentation based, optimization-based, and metric-based paradigms. In the first data-augmentation paradigm, generating additional samples to supplement available data is a key feature. Specifically, MetaGAN [39] utilizes generative adversarial networks (GANs) and statistics of existing samples to synthesize data. The most representative optimization-based method is MAML [7], which identifies a model parameters set and adapts it to individual tasks via gradient descent. The metric-based paradigm is well-known and widely used for its simplicity and effectiveness. It leverages similarity between support samples to classify query samples into corresponding classes. Prototypical Networks [22] construct prototypes based on class centroids and then classify samples by measuring distance to each prototype. Most few-shot learning paradigms are applied to image classifica-tion, with fewer in the field of videos. Our SOAP belongs to the metric-based paradigm for FSAR, focusing on improving prototype representation ability.\n2.2 Action Recognition\nCompared to image classification, action recognition is a more complex and extensively researched problem in the community due to the spatio-temporal relation and motion information. Pre-vious works [3, 19, 26, 32] have typically utilized 3D backbones to construct spatio-temporal relations, while optical flow is applied by additional networks to inject video motion information for ac-tion recognition, resulting in promising results. However, these works with high acquisition costs are all designed for traditional data-driven training without considering insufficient samples in real-world scenarios. In contrast, SOAP is specifically designed for a more realistic few-shot setting."}, {"title": "2.3 Few-Shot Action Recognition", "content": "Existing methods of FSAR mainly focus on metric-based paradigm for effective prototype construction. Among them, CMN [43, 44] introduces a multi-saliency embedding algorithm for key frame encoding, improving the prototype representation; ARN [38] cap-tures short-range dependencies using 3D backbones with a self- trained permutation invariant attention; OTAM [1] aligns support and query with a dynamic time warping (DTW) algorithm. Joint spatio-temporal modeling approaches such as TA2N [13], ITA [40], STRM [24], and SloshNet [34] employ spatial-temporal frameworks to address the support-query misalignment from multiple perspec-tives; Temporal relation is emphasized by TRX [18], using a Cross Transformer for sub-sequence alignment and achieving notable improvement; HyRSM [30] learns the task-specific embedding by exploiting the relation within and cross videos; SA-CT [41] com-plement the temporal information by learning the spatial relation. The temporal alignment all operated after spatial feature extrac-tion. Diverse types of information including optical flow [31] and depth [8] are introduced by extra networks, increasing computa-tions. MoLo [29] and MTFAN [33] explicitly extract motion infor-mation from raw videos and leverage temporal context for FSAR. Although achieving promising performance, these works are either cut apart spatial and temporal features or ignore motion infor-mation density. Therefore, our SOAP focuses on spatio-temporal relation and motion information capturing."}, {"title": "3 METHODOLOGY", "content": "3.1 Problem Formulation\nClassifying an unlabeled query into one of the support classes is the inference goal of FSAR. The support set has at least one labeled sample per class. Following the prior works [1, 18, 29, 30], we randomly select few-shot tasks from training set for episodic training. In each task, the support set S contains N classes and K samples per class, i.e., N-way K-shot setting. The query set Q in each task has samples to be classified. Uniformly sampling F frames of each video, we define the kth (k = 1,\u2026\u2026, K) sample of the cth (c = 1,..., N) class of support set S as:\n$S_{ck} = [s_{1}^{ck}...., s_{F}^{ck}] \u2208 R^{F\u00d7C\u00d7H\u00d7W}$\n(1)\nNotions used are F (frames), C (channels), H (height), and W (width), while the sample with F frames of query is defined as:\n$Q = [q_{1},..., q_{F}] \u2208 R^{F\u00d7C\u00d7H\u00d7W}$\n(2)\nModels of FSAR need to predict labels for query samples with the guidance of the support set.\n3.2 Overall Architecture\nAn overview of SOAP-Net is provided in Figure 2. For clear descrip-tion, we take the Q and cth of S as an example to illustrate the whole process of our method. First, video samples are decoded into frames at fixed intervals to serve as the input. Then, support and query are sent into three main modules of SOAP, i.e., 3DEM, CWEM, and HMEM. To be specific, 3DEM establishes spatio-temporal relation of features, while CWEM adaptively calibrates temporal connec-tions between channels. Instead of solely concentrating on motion"}, {"title": "3.3 3-Dimension Enhancement Module", "content": "For the relation construction between spatial and temporal infor-mation, 3DEM is designed based on 3D convolution. The structure of this module is shown in Figure 3. Firstly, 3DEM averages the support and query across the channels, resulting in spatio-temporal tensors. Then the corresponding tensors are reshaped. The above operation can be formulated as:\n$S_{1}^{ck} = Reshape_{1} (\\frac{1}{C} \\sum_{g=1}^{C} S_{ck} (:, g, :, :)) \u2208 R^{1XF\u00d7H\u00d7W}$ (3)\n$Q_{1} = Reshape_{1} (\\frac{1}{C} \\sum_{g=1}^{C} Q (:, g, :, :)) \u2208 R^{1\u00d7F\u00d7H\u00d7W}$\nA 3D convolution is used for constructing the spatio-temporal relation and another reshape operation is used for shape recovery:\n$S_{1}^{ck} = Reshape_{2} (Conv3D (S_{1}^{ck})) \u2208 R^{F\u00d71\u00d7H\u00d7W}$ (4)\n$Q_{1} = Reshape_{2} (Conv3D (Q_{1})) \u2208 R^{F\u00d71\u00d7H\u00d7W}.$\nFinally, spatio-temporal relation are fed into a Sigmoid activation and residually connected with the input to generate the 3D prior knowledge before feature extraction:\n$S_{1}^{ck} = S_{ck} + Sigmoid (S_{1}^{ck}) \u00d7 S_{ck}$, (5)\n$Q_{1} = Q + Sigmoid (Q_{1}) \u00d7 Q.$\nFor each input channel, 3DEM can assist the spatio-temporal rela-tion construction."}, {"title": "3.4 Channel-Wise Enhancement Module", "content": "Features located in different channels have temporal connections between each channel. Inspired by SE [11], we design this module with the addition of a simple 1D convolution. The structure of CWEM is detailed in Figure 4. First, a spatial average pooling is performed on the support and query, followed by a 2D convolution that expands the channel number to Cr, represented as:\n$\\check{S}_{ck} = Conv2D_{1} (\\frac{1}{H\u00d7W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} S_{ck} (:, :, i, j)) \u2208 R^{F\u00d7C_{r}\u00d71\u00d71},$ (6)\n$\\check{Q}_{2} = Conv2D_{1} (\\frac{1}{H\u00d7W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} Q (:, :, i, j)) \u2208 R^{F\u00d7C_{r}\u00d71\u00d71}$"}, {"title": "3.5 Hybrid Motion Enhancement Module", "content": "In previous works [29, 31], motion information plays a significant role. However, we find that focusing only on the motion information between adjacent frames is insufficient, as subtle displacements are hard to detect. In order to capture more motion information using HMEM, we extend perspective from adjacent frames to frame tuples and apply the combination of multiple scales. The structure of HMEM is demonstrated in Figure 5. We define a set O as a hyperparameter, where the value of element T (T \u2208 O, T < F and T\u2208 N*) represents frame counts of a tuple and the cardinality of set |O| denotes the number of branches. Being achieved by sliding window algorithm SW (\u00b7, \u00b7), frame tuple sets of support and query with element index t (t\u2208 [1, F - T + 1]) can be represented as:\n$SW (S_{ck}, T) = [..., s_{t}^{ck}, s_{t+1}^{ck}, ...],$\n$SW (Q, T) = [..., q_{t}, q_{t+1}, ...].$ (10)\nWe concatenate tuple differences in the first dimension, preparing for motion information calculation. For a simple description, we choose the eth (e \u2208 [1, |O|] and e \u2208 N*) branch as an example:\n$M_{e}^{ck} = Concat (..., Conv2D_{M} (s_{t+1}^{ck} - s_{t}^{ck}),...),$ (11)\n$M_{e} = Concat (..., Conv2D_{M} (q_{t+1} - q_{t}),...).$\nMotion information with different scales can be captured by setting multiple branches. Then we concatenate them along the first di-mension, achieving tensors belong to R^{F\u00d7C\u00d7H\u00d7W}. Function Z (\u00b7)"}, {"title": "3.6 Prototype Construction", "content": "that performs flatten (R^{F\u00d7C\u00d7H\u00d7W} \u2192 R^{F\u00d7(C\u00d7H\u00d7W)}), linear trans-formation (R^{F\u00d7(C\u00d7H\u00d7W)} \u2192 R^{F\u00d7(C\u00d7H\u00d7W)}), and reshape opera-tion (R^{F\u00d7(C\u00d7H\u00d7W)} \u2192 R^{F\u00d7C\u00d7H\u00d7W}) in sequence, acquiring com-prehensive motion information. This process is concluded as:\n$S_{3}^{ck} = Z (Concat (..., M_{e}^{ck},...)) \u2208 R^{F\u00d7C\u00d7H\u00d7W},$ (12)\n$Q_{3} = Z (Concat (..., M_{e},...)) \u2208 R^{F\u00d7C\u00d7H\u00d7W}.$\nA spatial average pooling is operated for reducing computation, as:\n$\\hat{S}_{3}^{ck} = \\frac{1}{H\u00d7W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} S_{3}^{ck} (:,:, i, j) \u2208 R^{F\u00d7C\u00d71\u00d71},$ (13)\n$\\hat{Q}_{3} = \\frac{1}{H\u00d7W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} Q_{3} (:,:, i, j) \u2208 R^{F\u00d7C\u00d71\u00d71}.$\nSimilar with the prior knowledge calculated by two previous mod-ules, the hybrid motion prior can be represented as:\n$\\tilde{S}_{3}^{ck} = \\hat{S}_{3}^{ck} + Sigmoid (\\hat{S}_{3}^{ck}) \u00d7 \\hat{S}_{3}^{ck},$ (14)\n$\\tilde{Q}_{3} = \\hat{Q}_{3} + Sigmoid (\\hat{Q}_{3}) \u00d7 \\hat{Q}_{3}.$\nTriple prior guidance are added into the raw support and query before feature extraction:\n$\\bar{S}_{ck} = (S_{1}^{ck} + \\tilde{S}_{2}^{ck} + \\tilde{S}_{3}^{ck}) + S_{ck} = [s_{1}^{ck},..., s_{F}^{ck}] \u2208 R^{F\u00d7C\u00d7H\u00d7W},$ (15)\n$\\bar{Q} = (Q_{1} + \\tilde{Q}_{2} + \\tilde{Q}_{3}) + Q = [\\tilde{q}_{1}, \u2026\u2026\u2026, \\tilde{q}_{F}] \u2208 R^{F\u00d7C\u00d7H\u00d7W}$"}, {"title": "3.7 Training Objective", "content": "$\\bar{S}_{ck}$ and $\\bar{Q}$ are sent to the backbone network. Features of support and query are defined as:\n$f_{S}^{ck} = [f_{\u03b8} (\\bar{s}_{1}^{ck}) + f_{pe} (1),..., f_{\u03b8} (\\bar{s}_{F}^{ck}) + f_{pe} (F)] \u2208 R^{F\u00d7D},$ (16)\n$f_{Q} = [f_{\u03b8} (\\bar{q}_{1}) + f_{pe} (1), ..., f_{\u03b8} (\\bar{q}_{F}) + f_{pe} (F)] \u2208 R^{F\u00d7D}.$\nf_{\u03b8}(): R^{C\u00d7H\u00d7W} \u2192 R^{D} is the backbone for embedding eachframe into a D-dimensional vector. While f_{pe} () represents theposition embedding function, which can be a cosine/sine functionor a learnable function, it is important to note that the functiontype can impact the performance of the model.\nThree linear layers including \u03a8 (\u00b7), \u0393 (\u00b7) : R^{F\u00d7D} \u2192 R^{F\u00d7d_{k}} and\u039b (\u00b7) : R^{F\u00d7D} \u2192 R^{F\u00d7d_{u}} are applied for constructing the prototypeof support P_{c}, refers to:\n$A_{ck} = LN (\u03a8 (Q_{f})). LN (\u0393 (S_{f}^{ck})),$ (17)\n$P_{c} = \\frac{1}{K} \\sum_{k=1}^{K} (Softmax (A_{ck}). \u039b (S_{f}^{ck})).$\nThe LN () means the layer normalization and Softmax () repre-sents the Softmax function.\nThe linear layer \u039b (\u00b7) is applied on Q_{f} to ensure the same operation as $\\bar{S}_{ck}$. By obtaining the closest distance between Q_{f} and P_{c}, model can predict the label \u1ef9 of Q, as:\n$\\hat{y} = arg \\underset{c}{min} ||P_{c} - (\u039b (Q_{f}))||$ (18)\nD(Q,Sc)\nThe cross-entropy loss L_{ce} is selected as the objective loss for training, which can be calculated with the ground truth y:\n$L_{ce} = - \\sum_{i=1}^{N} y_{i} log (\\hat{y}_{i}).$ (19)"}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Configuration\n4.1.1 Datasets Processing. We select four widely used datasets, i.e., SthSthV2 [9], Kinetics [2], UCF101 [23], and HMDB51 [12]. Apart from SthSthV2, which is temporal-related, other three datasets are all spatial-related. When decoding videos, we set the sam-pling intervals to every 1 frame. For simulating various fluency, the intervals can be adjusted. Building upon data split from prior works [1, 38, 43], we split our datasets into training, validation, and testing sets. A task within each set, classifying query samples into one of the support classes serves as a training unit.\nFollowing TSN [27], 8 frames (F = 8) resized to 3 \u00d7 256 \u00d7 256 are uniformly and sparsely sampled each time. Data augmentations in training include random crops to 3 \u00d7 224 \u00d7 224 and horizontal flipping, while only a center crop is used during testing. As pointed out in previous works [1], many action classes in SthSthV2 are sensitive to horizontal direction (e.g., \"Pulling S from left to right\"). Hence, we avoid horizontal flipping on this dataset."}, {"title": "4.2 Comparison with Various Methods", "content": "4.2.1 Comparison with ResNet-50 Backbone Methods. Based on the average accuracy reported in Table 1, we have the follow-ing observation. Our SOAP-Net outperforms other methods and achieves SOTA performance. For example, on the Kinetics dataset under the 1-shot setting, SOAP improves the current SOTA perfor-mance of MoLo [29] from 75.2% to 81.1%. We believe the motion information within adjacent frames is insufficient, despite MoLo in-troducing it. Similar improvements are also found in other datasets under diverse few-shot settings, showing that spatio-temporal re-lation and comprehensive motion information from SOAP lead to significant improvement in FSAR. It is worth mentioning that SOAP-Net even surpasses multimodal methods.\n4.2.2 Comparison with ViT-B Backbone Methods. ResNet-50 serves as the backbone network in most previous works, while ViT-B is rarely used in FSAR. For a comprehensive comparison, we implemented several methods and replaced their backbones with ViT-B, finding it outperformed ResNet-50 in FSAR due to larger model capacity. The previous SOTA performance is also achieved by MoLo, benefiting from optimizing spatio-temporal relation con-struction and comprehensive motion information. A similar trend is observed on ResNet-50 and ViT-B. These comparisons reveal the competitiveness of our proposed method."}, {"title": "4.3 Essential Components and Factors", "content": "4.3.1 Analysis of Key Components. We first divide SOAP into three key components: 3DEM, CWEM, and HMEM. Experiments on individual or combined components are conducted on two datasets under diverse few-shot settings. As shown in Table 2, results indi-cate that each SOAP component can improve FSAR performance. However, the most significant boost comes from HMEM, likely due to fuller motion information usage. Combining two components with HMEM performs better than without HMEM. We conclude that comprehensive motion information plays a more significant role in FSAR. Using all SOAP components yields the best results, proving all are essential.\n4.3.2 Frame Tuples and Branches Design. Results under vari-ous hyperparameter O settings of HMEM is demonstrated in Ta-ble 3. Each element represents frame counts, with the cardinality of set |O| equaling number of branches. Performance generally improves with more branches. Specifically, any frame tuple size improves with HMEM having one branch. Better results occur with two branches. Best performance occurs at 3 branches under O = {1, 2, 3} setting. This aligns with intuition comprehensive mo-tion information contributes more for FSAR. However, excessive branches risk degradation. Too much motion information overlap does not provide useful information. For example, O = {4} acts similarly to two O = {2} configurations together, implying that they perform worse than other non-overlapping settings. While O = {1} provides motion information between frames not conflict-ing with other O settings for building frame tuples, introducing comprehensive motion information.\n4.3.3 The Impact of Temporal Order. Up to this point, we be-lieve the frame tuple should follow the temporal order for better representation. After determining O = {1, 2, 3} setting, we conduct experiments with temporal and reversed order. In an extreme sce-nario, the frames in support take the temporal order while frames in query are reversely ordered during inference. As the results in Table 4, SthSthV2 and Kinetics datasets both have a drop with re-versed order settings. However, the drop in SthSthV2 is much larger than in Kinetics. The dataset type supports our previous observa-tion that the temporal-related SthSthV2 is more sensitive to order than the spatial-related Kinetics. Based on above results, we can infer that SOAP is closely tied to temporal order."}, {"title": "4.4 Research on How SOAP Works", "content": "4.4.1 CAM Visualization. The CAM [21] for \u201ccrossing river\u201d from the Kinetics dataset is shown in Figure 6. The first row shows raw HFR video frames. We observe that the timeline and displace-ment of the moving peoples are very subtle, making it difficult to detect the spatio-temporal relation and motion information. In the second row, we see that the model attends more to the background than moving peoples. This result supports our observation in HFR videos. With the help of SOAP, we detect that the focus shifts from the background to the moving peoples, despite the subtle timeline and displacement. Based on this analysis of the CAM, we con-clude that motion information is crucial in FSAR. Spatio-temporal relation and comprehensive motion information from SOAP can significantly improve feature representation.\n4.4.2 T-SNE Visualization. In the 5-way task, feature distribu-tions can be visualized using t-SNE [25]. We select five represen-tative yet challenging support action classes from Kinetics: \"ski jumping\", \"snowboarding\u201d, \u201cskateboarding\u201d, \u201ccrossing river\", and \"driving tractor\". The t-SNE visualization is shown in Figure 7, with different colors and markers representing distinct action classes. Without SOAP (Figure 7(a)), the five classes are generally separable"}, {"title": "4.5 Pluggability of SOAP", "content": "4.5.1 On RGB-Based Methods. In Table 5, we experimentally demonstrate that the SOAP generalizes well to other methods by inserting it into widely used methods including TRX, HyRSM, and MoLo. Using Kinetics as an example, TRX benefits from compre-hensive motion information and achieves 7.6% gains in the 1-shot setting and 8.5% in the 5-shot setting. Similar improvements are also observed in the other two methods. These results fully prove that optimizing spatio-temporal relation and comprehensive motion information are particularly useful for feature extraction.\n4.5.2 On Multimodal Methods. Backbone networks serve as feature extractors in multimodal methods for FSAR. Additional information from other modalities enhances performance. In most multimodal methods, depth or optical flow information augments RGB data. Apart from the RGB backbone, additional depth-specific or optical-flow-specific networks are also utilized. As depth and optical flow are both derived from raw RGB frames, they inherently contain spatio-temporal relation and motion information. There-fore, we hypothesize SOAP can also benefit multimodal methods and implement several multimodal methods equipped with SOAP for evaluation. Results in Table 6 demonstrate that all selected multimodal methods achieve further improvement via optimized spatio-temporal relation construction and comprehensive motion information capturing from SOAP. These results provide legiti-mate validation for our hypothesis and emphasize the advanced pluggability of SOAP on multimodal methods."}, {"title": "4.6 Generalization Study", "content": "In this study, we aim to simulate different levels of frame-rate by varying the sampling interval from a minimum of 1 to a maximum of 6 and conduct a series of experiments under a well-established 5-way 5-shot setting. As shown in Figure 8, the performance tends to decrease gradually with the increase of frame-rate. We find that recent FSAR methods all experience drastic degradation in perfor-mance with HFR videos while SOAP-Net indicates performance stability, demonstrating the superiority of SOAP across varying frame-rate. These phenomena also highlight the key role of spatio-temporal relation optimization and comprehensive motion infor-mation in advancing FSAR performance."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose the plug-and-play SOAP to optimize spatio-temporal relation construction and capture comprehensive motion information for FSAR. SOAP takes into account temporal connections across feature channels and spatio-temporal relation within features. It combines frame tuples of various frame counts to provide a broader perspective for motion information. Considering the competitiveness, pluggability, generalization, and robustness of SOAP, we hope and believe that our work will offer valuable insights for future research in multimedia analytic."}, {"title": "A GENERALIZATION STUDY", "content": "A.1 Generalization on More Complex Tasks\nIn a range of real-world scenarios, tasks are often more complex than a mere 5-way classification. Recognizing this complexity, we seek to enhance the challenge of these tasks by augmenting the num-ber of ways (classes). Experiments are conducted on SthSthV2 andKinetics. From Figure I, performance of various methods [18, 29, 30]decreases with increasing complexity. Although task complexityhinders performance boosts, SOAP-Net consistently outperformother methods. Surprisingly, SOAP-Net decays less than other meth-ods on complex tasks. The above results show better generalizationof SOAP on more complex tasks. It also indicates spatio-temporalrelation and comprehensive motion information support bettergeneralization.\nA.2 Generalization on Any-Shot Setting\nIn real-word applications, it is often challenging to ensure thatevery task has an equal number of samples. To create a more au-thentic testing environment for assessing the generalization abilityof SOAP, we utilize a shot number in the range of 1 to 5, effec-tively establishing an any-shot setting for our experiments. Resultswith 95% confidence intervals are shown in Table I. SOAP-Net out-performs three recent methods under the any-shot setting, andshows similar performance in normal 1-shot or 5-shot settings. Theconfidence intervals indicate that our SOAP-Net exhibits more sta-ble performance under the more realistic evaluation scenario. Theabove experiments underscore the potential of SOAP in practicalapplications."}, {"title": "B ROBUSTNESS STUDY", "content": "B.1 Robustness on Sample-Level Noise\nIn a more authentic testing environment, noise inevitably occursduring sample collection, and removing it incurs additional costs.Specifically, a particular class might be mixed with samples fromother classes. As shown in Figure II, directly replacing one or moresamples with noise within a few-shot task during inference is calledsample-level noise. Evaluating FSAR methods with sample-levelnoise simulates more realistic sample scenarios. For a clear demon-stration, we conduct 10-shot experiments and reveal the results inTable II. In general, performance decreases with increasing noise.For every 10% increase in the sample-level noise ratio, performancedrops by about 4%. Although the overall trend remains regardlessof the method, we observe a distinction between SOAP-Net andother methods. The performance decrease with the sample-levelnoise ratio for SOAP-Net is about 2%, much less than others. Thestable performance against sample-level noise highlights superiorrobustness of our SOAP.\nB.2 Robustness on Frame-Level Noise\nDue to the uncertainty of video recorder, it is not guaranteed thatthe view of all frames is aligned with the moving subject. In some un-desirable cases, multiple irrelevant frames are mixed in. Under suchconditions, higher requirements are placed on the FSAR method.As shown in Figure III, we replace some pure frames with irrele-vant frames from other action classes during inference. We definethis as frame-level noise. The 5-way 10-shot setting is the same asthe evaluation, and the results are detailed in Table III. Comparedto sample-level noise, the negative impact of frame-level noise issmall, with a similar overall trend. SOAP-Net outperforms othermethods under any frame-level noise number. We find the impactof frame noise on TRX and HyRSM is greater than on MoLo andour SOAP-Net. The commonality of MoLo and SOAP-Net is bothapply motion information, which plays a vital role in FSAR. Ourproposed method is less affected, proving comprehensive motioninformation provides better robustness to frame-level noise."}, {"title": "C ADDITIONAL CAM VISUALIZATIONS", "content": "As a complement to the CAM visualization in the main paper, weprovide additional examples in Figure IV. For the example \u201csnowboarding\u201d, each frame is sampled without much background varia-tion. However, the high fluency makes motion information weak,and the model attention mistakenly focuses on the background.Fortunately, the model with SOAP concentrates more on the skierinstead of the outlying background. Due to disordered scenery likedriftwood and tourists, the example \u201cdiving cliff\u201d is more complexthan the first. In the second row, we can clearly see the model isdisrupted and does not know where to focus. When motion in-formation is highlighted, people diving the cliff is easier to find."}, {"title": "D HYPERPARAMETER STUDY", "content": "Table IV illustrates the SOAP-Net structure in our implementation", "hyperparameters": "the sizeof Conv2DM and the design of O. The former determines the vi-sual receptive field size, while the latter determines the number ofbranches and frame tuple sizes. The results obtained using onlyHMEM through permutation and combination are presented in Ta-ble VII. Performance with"}]}