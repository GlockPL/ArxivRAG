{"title": "BioPose: Biomechanically-accurate 3D Pose Estimation from Monocular Videos", "authors": ["Farnoosh Koleini", "Muhammad Usama Saleem", "Pu Wang", "Hongfei Xue", "Ahmed Helmy", "Abbey Fenwick"], "abstract": "Recent advancements in 3D human pose estimation from single-camera images and videos have relied on parametric models, like SMPL. However, these models over-simplify anatomical structures, limiting their accuracy in capturing true joint locations and movements, which reduces their applicability in biomechanics, healthcare, and robotics. Biomechanically accurate pose estimation, on the other hand, typically requires costly marker-based motion capture systems and optimization techniques in specialized labs. To bridge this gap, we propose BioPose, a novel learning-based framework for predicting biomechanically accurate 3D human pose directly from monocular videos. BioPose includes three key components: a Multi-Query Human Mesh Recovery model (MQ-HMR), a Neural Inverse Kinematics (NeurIK) model, and a 2D-informed pose refinement technique. MQ-HMR leverages a multi-query deformable transformer to extract multi-scale fine-grained image features, enabling precise human mesh recovery. NeurIK treats the mesh vertices as virtual markers, applying a spatial-temporal network to regress biomechanically accurate 3D poses under anatomical constraints. To further improve 3D pose estimations, a 2D-informed refinement step optimizes the query tokens during inference by aligning the 3D structure with 2D pose observations. Experiments on benchmark datasets demonstrate that BioPose significantly outperforms state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "Biomechanically-accurate 3D human pose estimation (BA-HPE) refers to the process of predicting a human's 3D body posture, including joint positions and movements, in a way that aligns with the actual anatomical and physical constraints of the human skeletal system. BA-HPE plays a crucial role in fields like physical therapy, sports science, and ergonomics [17, 32, 36], where precise modeling of human motion is necessary for injury prevention, rehabilitation, or performance analysis. It is also useful in robotics, animation, and human-computer interaction [1,28,39,41,42].\nTraditionally, marker-based motion capture systems have been the gold standard for obtaining biomechanically accurate 3D pose data. These systems utilize multiple cameras to track reflective markers placed on the human body in controlled laboratory environments [34, 35]. The captured marker data is then processed using sophisticated biomechanical optimization systems, such as OpenSim [37], which often requires skilled professionals to configure and refine. Although these marker-based systems provide high accuracy, they are expensive, labor-intensive, and impractical for use outside of specialized labs, particularly in dynamic, real-world environments.\nOn the other hand, significant advancements have been made in 3D pose estimation from monocular videos (single-camera footage) [11, 14], where deep neural networks are used to infer 3D poses, specifically, the rotation angles of body joints, from 2D image sequences, leveraging parametric human body models like SMPL [22, 29]. While monocular pose estimation offers greater accessibility compared to traditional marker-based systems, it faces notable limitations due to the anatomical simplifications inherent in body models like SMPL. These body models are designed to produce visually plausible poses, but fail to achieve biomechanical accuracy, particularly in joint positioning and skeletal movements.\nTo address these challenges, we propose BioPose, a novel learning-based framework for biomechanically accurate 3D human pose estimation from monocular videos. BioPose consists of three core components: a multi-query human mesh recovery model (MQ-HMR), a neural inverse kinematics (NeurIK) model, and a 2D-informed pose refinement technique. The MQ-HMR model utilizes a multi-query deformable transformer to extract fine-grained, multi-scale image features from monocular video frames, enabling precise recovery of a 3D human mesh. This model simultaneously estimates 3D body pose, shape parameters, and camera parameters, resulting in a accurate and detailed body mesh. In the next stage, the NeurIK model treats the recovered mesh vertices as virtual markers and uses a spatial-temporal network to regress biomechanically accurate 3D poses. This process is guided by the biomechanical skeleton model with anatomical realism, ensuring the predicted poses adhere to human biomechanics, such as anatomical locations and degrees of freedom of the joints. To further enhance accuracy, BioPose introduces a 2D-informed refinement step during inference, optimizing pose queries in latent space to align the predicted 3D poses with 2D pose cues from the video. This refinement corrects 3D-to-2D projection discrepancies, ensuring both visual coherence and biomechanical precision. Our key contributions are summarized as follows:\n\u2022 We introduce BioPose, a novel learning-based framework for biomechanically accurate 3D pose estimation from monocular videos, offering performance comparable to traditional marker-based optimization methods while retaining the convenience and accessibility of monocular learning-based approaches.\n\u2022 We propose a MQ-HMR model for accurate 3D mesh reconstruction and virtual marker tracking along with the NeurIK module that incorporates a biomechanical skeleton model to ensure anatomically valid 3D poses from monocular videos.\n\u2022 We develop a novel 2D-informed refinement technique that further enhances 3D pose estimation via inference-time optimization that aligns the predicted 3D structure with 2D pose cues.\n\u2022 Extensive experiments demonstrate that MQ-HMR model outperforms state-of-the-art methods in monocular human mesh recovery from single images and BioPose system achieves the very competitive performance, compared with golden-standard multi-camera marker-based techniques."}, {"title": "2. Related Work", "content": "2.1. Biomechanically-accurate 3D Pose Estimation\nThe gold standard for biomechanically accurate 3D pose estimation combines multi-camera, marker-based tracking systems with biomechanical optimization tools. This process involves three main steps: first, retro-reflective markers are attached to the subject's body and tracked using synchronized infrared cameras. Next, a calibration pose (T-pose) is captured, allowing tools like OpenSim [37] to scale a biomechanical skeleton model to the subject's anatomy. Finally, the subject performs the target motion, and the marker data, along with the scaled skeleton model, is used to compute 3D joint rotation angles through inverse kinematics optimization.\nTo reduce the cost of marker-based systems, marker-less multi-camera approaches like OpenCap [15, 36] and Pose2Sim [32] have emerged. These methods use 2D pose estimation algorithms like OpenPose [6] to detect keypoints from video frames, triangulating them from multiple camera views to reconstruct 3D positions. However, the resulting keypoints are sparse and anatomically imprecise, requiring post-processing techniques like keypoint augmentation [33] to improve anatomical accuracy. These systems also require tedious camera calibration and synchronization, along with optimization tools for final 3D pose estimation. To address these limitations, D3KE [4] was recently proposed, using deep neural networks to directly regress biomechanically accurate 3D poses from monocular videos. However, D3KE struggles with generalization due to limited paired training data. In contrast, BioPose leverages a novel MQ-HMR model to generate robust 3D human meshes from in-the-wild videos, serving as virtual markers. These markers, combined with our biomechanics-guided NeurIK model and 2D-informed inference-time optimization, yield highly generalizable 3D pose estimations with strong anatomical accuracy.\n2.2. HMR from Monocular Images\nHuman Mesh Recovery (HMR) from monocular images has evolved significantly, focusing on estimating 3D meshes from single 2D images. Early approaches mainly leverage convolutional neural networks to directly regress the parameters of parametric human model SMPL [29], from"}, {"title": "3. Proposed Method: BioPose", "content": "The goal of BioPose is to predict biomechanically accurate 3D human poses directly from monocular videos. As shown in Figure 3, BioPose has three core components. The MQ-HMR model uses a multi-query deformable transformer decoder to extract multi-scale image features from vision transformer encoder, enabling precise recovery of 3D human meshes (Section 3.2). These meshes are then used by the NeurIK model, which treats the mesh vertices as virtual markers, applying a spatial-temporal network to infer biomechanically accurate 3D poses while maintaining anatomical constraints (Section 3.3). To further improve accuracy, a 2D-informed pose refinement step aligns the 3D predictions with 2D observations, enhancing both visual coherence and biomechanical validity (Section 3.4).\n3.1. Preliminaries\n3.1.1 SMPL Human Mesh Model\nWe make use of the SMPL model, a differentiable parametric framework for representing human surface geometry [29]. This model encodes the human body using pose parameters \\( \\theta \\in \\mathbb{R}^{24\\times3} \\) and shape parameters \\( \\beta \\in \\mathbb{R}^{10} \\). The pose parameters \\( \\theta = [\\theta_1,...,\\theta_{24}] \\) include both the global orientation \\( \\theta_1 \\in \\mathbb{R}^3 \\) of the entire body and the local joint rotations \\( [\\theta_2,..., \\theta_{24}] \\in \\mathbb{R}^{23\\times3} \\), with each \\( \\theta_k \\) describing the axis-angle rotation of joint k relative to its parent joint in the kinematic tree. By combining these pose and shape parameters, the SMPL model produces a detailed 3D mesh\n\\( M(\\theta,\\beta) \\in \\mathbb{R}^{3\\times N} \\), where N = 6890 vertices represent the surface of the body. The positions of the body joints \\( J \\in \\mathbb{R}^{3\\times k} \\) are then derived as a weighted sum of these vertices, formulated as \\( J = MW \\), where \\( W \\in \\mathbb{R}^{N\\times k} \\) contains the predefined weights that map vertices to the corresponding joints.\n3.1.2 Biomechanical Skeleton (BSK) Model\nThe BSK model, e.g., widely-adopted OpenSim models, is represented by a series of bone segments that are interconnected through movable anatomical joints, which possess anatomical movement constraints, such as Degrees of Freedom, to limit the range of motion of the respective body parts. In particular, the BSK model generally consists of 24 rigid bone segments, which are represented by three sets of parameters \\( (q^o, q^r, s) \\). The anatomical joint orientation \\( q^o \\in [q^o_1,..., q^o_{24}] \\) with \\( q^o_i \\in \\mathbb{R}^3 \\) defines the relative orientation of each joint with respective to its parent joint along the kinematic skeleton tree. Therefore, \\( q^o \\) are determined by the anatomical structure of human skeleton. \\( q^r \\in [q^r_1,..., q^r_{24}] \\) represents the motion-induced joint rotation with \\( q^r_i \\in \\mathbb{R}^{D_i} \\) and \\( D_i \\leq 3 \\) represents the Euler's angle rotation of joint i relative to its parent in the kinematic tree under the constraints imposed by the degree of freedom \\( D_i \\) of each joint i. The bone scale \\( s \\in [s_1,..., s_{24}] \\) with \\( s_i \\in \\mathbb{R}^3 \\) aims to tailor the generic anatomical skeleton model (in the rest pose) by scaling each bone length and shape along with the (x, y, z) axis. The scaled skeleton yields the body joints \\( p_J \\in \\mathbb{R}^{3\\times24} \\) at anatomical positions. The differences between SMPL and BSK models are shown in Fig. 2\n3.1.3 Optimization-based Biomechanical Kinematics\nWith assistance of the BSK model, the kinematic analysis aims to find the optimal pose, i.e., joint rotation angles \\( q^r \\), which can best fit the BSK model to the motion capture sequences. Towards this goal, a set of model markers is first attached to the bone segments in such a way that each bone"}, {"title": "3.2. MQ-HMR", "content": "segment is associated with at least \\( D_i \\) markers to ensure the unique solutions of the derived rotation angles at joint i with \\( D_i \\) degrees of freedom. Then, a set of corresponding experiential markers is placed on the human subject. Then, the pose \\( q^r \\) and bone scale s can be obtained by solving an optimization problem that minimizes the distance between each experimental marker and its corresponding model marker. i.e.,\n\\( q^{r*}, s^* = \\arg \\min_{q^r,s} \\sum_{i=1}^M ||f_{FK}(q^r, s, q^o, p_i) - x^{exp}_i|| \\)\nwhere M is the number of markers. \\( p_i \\in \\mathbb{R}^3 \\) denotes the position of i-th model marker in the local coordination system of the body segment to which it is attached. \\( f_{FK}(q^r, s, q^o, p_i) \\) is the forward kinematics transformation that converts the model marker i from its local coordination frame to the world coordination system under the scaled skeleton with the pose of \\( q^r \\). \\( x^{exp}_i \\in \\mathbb{R}^3 \\) is the position of the experiential marker i in the world coordination system. We leverage OpenSim, the classic biomechanical optimizer, and BML-MoVi dataset [13] to obtain \\( q^{r*}, s^* \\), which serve as the ground-truth data to train the NeurIK model.\nThe goal of MQ-HMR is predicting accurate virtual experiential markers, XM, from the 3D mesh, which serves as the inputs for NeurIK model. Towards this goal, MQ-HMR model consists of two key components: the image encoder and the multi-query deformable transformer decoder.\nImage Encoder. Our image encoder is based on the Vision Transformer (ViT), specifically the ViT-H/16 variant [2, 11]. The encoder begins by dividing the input image into 16x16 pixel patches, which are processed through multiple transformer layers to produce a set of feature tokens that encode the visual information. To enhance this process, we implement a multi-scale feature extraction approach [2]. After generating the initial feature map, the encoder upsamples it to produce feature maps at various resolutions. These higher-resolution maps capture fine-grained visual details, such as joint positions and orientations, while lower-resolution maps preserve broader, high-level semantic information, such as the overall human skeleton structure. This multi-scale strategy allows the encoder to capture both intricate local details and global contextual information simultaneously, which is crucial for accurate pose estimation.\nMulti-Query Deformable Transformer Decoder. Building upon multi-scale feature maps, the multi-query deformable transformer decoder introduces a novel mechanism designed to recover precise 3D human poses by extracting fine-grained semantic information from diverse resolutions. The multi-query approach initializes multiple pose queries as zero pose tokens, which interact with the encoder's multi-scale feature maps. These queries generate pose anchors crucial for accurately estimating complex human poses, especially in challenging scenarios like occlusions or ambiguous body positions. To process high-resolution feature maps efficiently, MQ-HMR incorporates a multi-scale deformable cross-attention mechanism [26, 40], focusing each query on a small set of sampling points near the pose anchors and dynamically adjusting attention to the most relevant regions. This optimizes both computational efficiency and accuracy, allowing the model to concentrate on critical spatial features with minimal overhead. The deformable attention mechanism (DAM) for multi-scale features is formulated as:\n\\( DAM(Q, \\hat{k}, \\{F_s\\}_{s=1}^S) = \\sum_{s=1}^S \\sum_{m=1}^{M_s} a_{ksm} W F_s (r_k + \\Delta r_{ksm}) \\)\nWhere Q represents the pose token queries, \\( \\hat{k} \\) denotes"}, {"title": "3.3. NeurIK", "content": "the learnable reference points, \\( \\Delta r_{ksm} \\) refers to the learnable sampling offsets around the reference points, \\( \\{F_s\\}_{s=1}^S \\) are the multi-scale image features, \\( a_{ksm} \\) are the attention weights, and W is a learnable weight matrix. This deformable cross-attention strategy allows the model to capture fine-grained details while balancing computational efficiency. From this accurately reconstructed 3D mesh, virtual markers \\( X_{VM} \\) are extracted, serving as input to the NeurIK module for further refinement and achieving precise biomechanical accuracy.\nMQ-HMR Losses. In line with established practices in HMR research [11, 14], we train our MQ-HMR model using a combination of losses based on SMPL parameters, 3D keypoints, and 2D keypoints. The final overall loss is:\n\\( L_{total} = L_{smpl} + L_{3D} + L_{2D} \\)\nwhere \\( L_{smpl} \\) minimizes the error between the predicted and ground-truth SMPL pose (\\( \\theta \\)) and shape (\\( \\beta \\)) parameters, \\( L_{3D} \\) supervises the accuracy of the 3D keypoint predictions, and \\( L_{2D} \\) enforces consistency between the projected 3D keypoints and their corresponding 2D annotations.\nAfter extracting virtual markers \\( X_{VM} \\) from MQ-HMR, the NeurIK module processes these markers to predict biomechanically accurate 3D poses \\( q^r \\) and bone scale s. To achieve this, it employs three key components: i) a Spatial Convolution Encoder to model spatial relationships among body parts, ii) a Temporal Transformer Encoder to capture dynamic motion patterns over time, and iii) multiple loss functions that incorporate biomechanical constraints from a musculoskeletal model.\nSpatial Convolution Encoder. The Spatial Convolution Encoder is designed to extract high-dimensional spatial features from a single frame. Given the 3D human mesh generated by the pre-trained MQ-HMR model, we extract M virtual markers from the mesh, where each marker m has 3D coordinates \\( (x_m, y_m, z_m) \\). These marker positions \\( X_{VM} \\in \\mathbb{R}^{M\\times3} \\) are first projected into a higher-dimensional space using a trainable linear projection, resulting in the spatial embedding \\( Z_{n_i} \\in \\mathbb{R}^{M\\times c} \\), where c is the spatial embedding dimension. To capture spatial relationships across the body, this spatial embedding is processed through a series of 1-D convolutional layers, which capture both local relationships between neighboring markers and global dependencies across the entire body structure. The output of this spatial convolution process for frame \\( n_i \\), \\( Z_{n_i} \\in \\mathbb{R}^{M\\times c} \\), represents a refined spatial feature embedding, which is passed to the Temporal Transformer Encoder for temporal modeling.\nTemporal Transformer Encoder. After encoding high-dimensional spatial features for each individual frame, the"}, {"title": "3.4. 2D Pose-Informed Refinement at Inference", "content": "Temporal Transformer Encoder models the dependencies across the sequence of frames. For frame \\( n_i \\), the spatially encoded feature matrix \\( Z_{n_i} \\in \\mathbb{R}^{M\\times c} \\) is flattened into a vector \\( z_{n_i} \\in \\mathbb{R}^{1\\times(M\\cdot c)} \\). We concatenate these vectors across all n frames to form the sequence matrix \\( Z_{seq} \\in \\mathbb{R}^{n\\times(M\\cdot c)} \\), which represents the spatial features for the entire motion sequence. To capture temporal relationships, we add a learnable temporal positional embedding \\( PE_n \\in \\mathbb{R}^{n\\times(M\\cdot c)} \\) to the sequence matrix. The temporal transformer then applies multi-head self-attention blocks and feed-forward layers to model both short-term and long-term dependencies across frames. This allows the model to understand the progression of motion and how body parts evolve over time. The final output of the temporal transformer, \\( Y_n \\in \\mathbb{R}^{n\\times(M\\cdot c)} \\), is used to predict key biomechanical parameters such as body scales s and joint angles \\( q^r \\). These predictions are further refined through a Forward Kinematics (FK) module to ensure biomechanically accurate marker and joint positions.\nNeurIK Losses. Spatial and temporal models are trained using multiple supervisions, including joint positions, marker positions, body scales, and joint angles. The joint positions correspond to anatomical landmarks in the musculoskeletal model, ensuring biomechanical accuracy. The overall loss function L is a weighted sum of four terms: \\( L_j \\) for joint positions, \\( L_m \\) for marker positions, \\( L_s \\) for body scales, and \\( L_q \\) for joint angles. These terms are weighted by coefficients \\( \\lambda_j, \\lambda_m, \\lambda_s \\), and \\( \\lambda_q \\), respectively, to control their contributions to the total loss. The overall loss function is defined as:\n\\( L_{neurIK} = \\lambda_j L_j + \\lambda_m L_m + \\lambda_s L_s + \\lambda_q L_q \\)\nIn particular, both \\( L_j \\) and \\( L_m \\) incorporate biomechanical constraints during training through the forward kinematics (FK) layer. As shown in Fig. 3, the FK layer transforms the rest-pose BSK model markers and anatomical joints to the new positions according to the estimated rotation angles \\( q^r \\) in such a way that the model markers best match the virtual experimental markers. Since we employed a full-body skeletal model from OpenSim [34], the FK transformation is inherently contrained by the realsitic degrees of freedom and range of motions of body joints. The details of loss functions are shown in the supplementary material.\nDuring inference time, to further reduce uncertainties in the 3D reconstruction process, we fine-tune the pose query tokensQ within the MQ-HMR model while keeping the rest of the network frozen as shown in Figure 4. This refinement ensures alignment between the predicted 3D poses and 2D pose data, leveraging robust 2D pose detectors such as OpenPose [6]. The process begins with the pose query"}, {"title": "4. Experiments", "content": "tokens generated by MQ-HMR, and the initial pose parameters \\( \\theta' \\) are derived from the output of MQ-HMR for a given input image. These tokens and parameters are iteratively adjusted to minimize discrepancies between the inferred 3D and observed 2D poses by optimizing a guidance function \\( F(Q_t, J_{2D}, \\theta') \\). This function penalizes misalignments between the two pose domains while enforcing regularization through the following expression:\n\\( Q^* = \\arg \\min_Q (L_{2DRefine}(J_{3D}) + \\lambda_{or} L_{or}(\\theta')) \\)\nwhere the term \\( L_{2DRefine}(J_{3D}) \\) aims to align the reprojected 3D joints \\( J_{3D} \\) with the detected 2D keypoints \\( J_{2D} \\), using the following relation:\n\\( L_{2DRefine}(J_{3D}) = |\\Pi(K(J_{3D})) - J_{2D}|^2 \\)\nwhere \\( \\Pi(K(\\cdot)) \\) represents the perspective projection function governed by the camera intrinsics K. Simultaneously, the regularization term \\( L_{or}(\\theta') \\) constrains the pose parameters \\( \\theta' \\), ensuring that they do not diverge excessively from their initial values, thereby avoiding anatomically implausible body configurations. The pose tokens \\( Q_t \\) are iteratively updated at each step t via gradient-based optimization as follows:\n\\( Q_{t+1} = Q_t - \\eta \\nabla_{Q_t} F(Q_t, J_{2D}, \\theta') \\)\nwhere \u03b7 determines the step size for the update, and \\( \\nabla_{Q_t} F(Q_t, J_{2D}, \\theta') \\) denotes the gradient of the objective function with respect to the pose tokens at the current iteration t. This iterative refinement process, carried out over T total iterations, gradually fine-tunes the pose representation, reducing the gap between the 3D estimates and their 2D counterparts while keeping realistic pose configurations.\n4.1. Datasets\nTo train MQ-HMR, we employs a diverse collection of datasets. In line with prior research [14] and to ensure con-"}, {"title": "4.2. Evaluation Metrics", "content": "sistency in comparisons with other baselines, a diverse set of standard datasets (SD) is used for training, including Human3.6M (H36M) [16], COCO [27], MPI-INF-3DHP [31], and MPII [3]. For NeurIK, we utilizes BML-MoVi [13] for training, which provides rich biomechanical motion capture and video data from multiple actors performing everyday activities.\nThe performance of MQ-HMR and NeurIK is evaluated using a set of key metrics designed for evaluating 3D human pose accuracy and biomechanical estimation. For MQ-HMR, the metrics used include Mean Per Joint Position Error (MPJPE) and Mean Vertex Error (MVE) to evaluate the accuracy of 3D pose estimation. Additionally, we report Procrustes-Aligned MPJPE (PA-MPJPE), which measures the alignment between predicted and ground-truth poses after a rigid transformation, providing a more precise comparison of pose structure. MQ-HMR is evaluated on the Human3.6M testing split, consistent with prior studies [24], and for generalization testing, we assess the model on challenging real-world datasets like 3DPW [38] and EMDB [21]. Importantly, no training is performed on these datasets, ensuring a fair evaluation on unseen data.\nFor NeurIK, we employ metrics that focus on biomechanical accuracy. These include Mean Per Bony Landmarks Position Error (MPBLPE), which measures the accuracy of predicted bony landmarks against ground truth positions, and Mean Absolute Error for body scale (MAEbody), which evaluates the correctness of predicted body segment dimensions by comparing their longest axes in millimeters. Additionally, we report Mean Absolute Error for joint angles (MAEangle), which assesses the precision of joint angle predictions in degrees, critical for biomechanical simulations such as joint force and muscle force analysis. To assess computational efficiency, we use the metric Average Inference Time per Image (AITI) in seconds, similar to the average inference or optimization time per image described by OpenPose [6]. Unlike that, AITI tracks the processing time per iteration, allowing for a more detailed evaluation of our 2D Pose-Informed Refinement technique. AITI is obtained on a single NVIDIA Quadro RTX A4000. Lower values in these metrics indicate better model performance. For NeurIK, we test the model on the BMLmovi [13], OpenCap [36], and BEDLAM [5] datasets. NeurIK is not trained on OpenCap and BEDLAM datasets to show its cross-dataset generalization performance."}, {"title": "4.3. Comparison to State-of-the-art Approaches", "content": "4.3.1 Quantitative Evaluation of MQ-HMR\nAs shown in Table 1, MQ-HMR demonstrates significant improvements over existing methods across multiple benchmark datasets, particularly on Human3.6M, 3DPW, and"}, {"title": "5. Ablation Study", "content": "5.1. Effectiveness of Pose Query Token\nThe Effectiveness of Pose Query Tokens in MQ-HMR highlights the significant influence of token quantity on model performance across datasets as shown in Table 4. Starting with 4 tokens, the model underperforms, showing an MPJPE of 98.8 mm on EMDB. As the pose token count increases, there is a clear improvement, with 96 tokens yielding the best results: an MPJPE of 69.0 mm and MVE of 79.8 mm on 3DPW, and 98.9 mm on EMDB. However, beyond 96 tokens, the improvements diminish. At 192 tokens, MPJPE increases to 74.7 mm, and performance plateaus or declines further with 384 tokens. This indicates that 96 tokens strike the optimal balance between maximizing accuracy and maintaining computational efficiency.\n5.2. Impact of Feature Resolutions\nThe ablation study of MQ-HMR reveals clear trends in how multi-scale feature resolutions impact 3D human pose estimation performance as shown in Table 5. As feature resolutions increase from 1x to 16x, the MPJPE consistently improves, reducing errors by 3.5 mm on 3DPW and 2.9 mm on EMDB. Including intermediate scales such as 4\u00d7 and 8\u00d7 further reduces errors, showing that the model benefits from a balance of both global and local information. Conversely, when lower resolutions (1x or 4x) are excluded, errors increase sharply, particularly on 3DPW, where MPJPE rises by 6.1 mm, indicating the importance of low-resolution features for maintaining overall pose structure. These trends emphasize the effectiveness of multi-scale feature fusion in MQ-HMR, balancing fine detail and broader spatial context for optimal performance.\n5.3. Effectiveness of 2D Pose-Informed Refinement\nTable 3 shows the effect of varying iteration numbers on the 2D pose-informed model across the BML-MoVi, BEDLAM, and OpenCap datasets. Performance generally improves with more iterations, peaking at 10 iterations for BML-MoVi and BEDLAM, where the model achieves the lowest (MAEbody and MAEangle, 3.97 and 2.84 for BML-MoVi, and 4.28 and 3.14 for BEDLAM). However, beyond 10 iterations, performance slightly declines, showing diminishing returns. For the OpenCap dataset, performance remains optimal at 20 iterations, with MAEbody of 4.87 and MAEangle of 3.19. This underscores the method's ability to achieve significant improvements in just a few iterations while balancing computational time. For example, at 5 iterations, the model performs well (MAEbody of 4.12 on BML-MoVi) with an AITI of just 0.405 seconds (NVIDIA Quadro RTX A4000). This shows that effective refinement is achieved early, making the approach highly efficient for real-time applications."}, {"title": "6. Conclusion", "content": "We introduce BioPose, a framework for biomechanically accurate 3D human pose estimation from monocular video. BioPose integrates human mesh recovery with biomechanical analysis, combining Multi-Query Human Mesh Recovery (MQ-HMR) for mesh reconstruction, Neural Inverse Kinematics (NeurIK) for biomechanical refinement, and 2D-informed pose alignment. Experiments show its significant gains over state-of-the-art methods, making it a powerful tool for clinical assessments, sports analysis, and rehabilitation technol-"}, {"title": "1. Appendix", "content": "1.1. Overview\nThe appendix is organized into the following sections:\n\u2022 Section 1.2: Implementation Details\n\u2022 Section 1.3: Datasets\n\u2022 Section 1.4: Evaluation Metrics\n\u2022 Section 1.5: Data Augmentation\n\u2022 Section 1.6: Camera Model\n\u2022 Section 1.7: Impact of Backbones\n\u2022 Section 1.8: Impact of Deformable Cross-Attention Layers\n\u2022 Section 1.9: Effect of losses on NeurIK model\n\u2022 Section 1.10: Multi-frame out vs Single frame out\n\u2022 Section 1.11: Impact of Number of Frames in NeuralIK\n\u2022 Section 1.12: Qualitative Results\n1.2. Implementation Details\nMQ-HMR. Our MQ-HMR model is implemented in PyTorch. To achieve this, we utilized multi-resolution feature maps at 4x, 8x, and 16x scales, ensuring the model captures both local detail and global structure. The total loss function in MQ-HMR is defined as \\( L_{total} = L_{SMPL} + L_{3D} + L_{2D} \\). This combines 3D loss (L3D), 2D loss (L2D), and SMPL parameter loss (LSMPL) to optimize the shape (\\( \\beta \\)) and pose (\\( \\theta \\)) parameters in the SMPL space. The loss weights for this stage are carefully tuned to balance each objective, where the pose component is weighted at \\( \\lambda_{\\theta} = 1 \\times 10^{-3} \\) and the shape component at \\( \\lambda_{\\beta} = 5 \\times 10^{-4} \\). For the 3D loss, \\( \\lambda_{3D} = 5 \\times 10^{-2} \\), and for the 2D loss, \\( \\lambda_{2D} = 1 \\times 10^{-2} \\). The architecture incorporates 96 Pose Token Queries (Q) and 4 deformable cross-attention layers as default, which enables the model to attend to relevant spatial information across different scales. The model was trained for 100K iterations using the Adam optimizer with a batch size of 48 and a learning rate of \\( 1 \\times 10^{-5} \\).\nNeurIK. Our NeurIK module is implemented in PyTorch and processes virtual markers \\( X_{VM} \\in \\mathbb{R}^{142\\times3} \\) extracted from the 3D mesh produced by MQ-HMR. The architecture includes a Spatial Convolution Encoder that utilizes 1D convolutional layers for spatial feature extraction and a Temporal Transformer Encoder that employs multi-head self-attention to model temporal dependencies across multiple frames. The total loss function is defined as \\( L_{neurIK} = \\lambda_j L_j + \\lambda_m L_m + \\lambda_s L_s + \\lambda_q L_q \\), with the weights set as \\( \\lambda_j = 1.0, \\lambda_m = 2.0, \\lambda_s = 0.1 \\), and \\( \\lambda_q = 0.06 \\). The model was trained for 25 epochs using the Adam optimizer with an initial learning rate of 0.001, decaying to \\( 5 \\times 10^{-6} \\), and a batch size of 128. Data augmentation techniques such as scaling, rotation, translation, and noise injection were applied to increase the model's robustness to occlusions and real-world variations."}, {"title": "1.3. Datasets", "content": "We utilized videos from BMLmovi [13", "36": "and BEDLAM [5", "datasets.\nBML-MoVi": "BMLMovi consists of 90 subjects performing 21 different actions", "13": ".", "nOpenCap": "OpenCap includes data from ten subjects performing various actions such as walking, squatting, standing up from a chair, drop jumps, and their asymmetric variations. The recordings were made using five RGB cameras alongside a marker-based motion capture system. Additionally, OpenCap offers processed marker data and kinematic annotations for a comprehensive full-body OpenSim skeletal model"}]}