{"title": "Is an Ultra Large Natural Image-Based Foundation Model Superior to a Retina-Specific Model for Detecting Ocular and Systemic Diseases?", "authors": ["Qingshan Hou", "Yukun Zhou", "Jocelyn Hui Lin Goh", "Ke Zou", "Samantha Min Er Yew", "Sahana Srinivasan", "Meng Wang", "Thaddaeus Lo", "Xiaofeng Lei", "Siegfried K. Wagner", "Mark A. Chia", "Dawei Yang", "Hongyang Jiang", "AnRan Ran", "Rui Santos", "Gabor Mark Somfai", "Juan Helen Zhou", "Haoyu Chen", "Qingyu Chen", "Carol Yim-Lui Cheung", "Pearse A. Keane", "Yih Chung Tham"], "abstract": "The advent of foundation models (FMs) is transforming the medical domain. In ophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4 million natural images and 1.6 million retinal images, has demonstrated high adaptability across clinical applications. Conversely, DINOv2, a general-purpose vision FM pre-trained on 142 million natural images, has shown promise in non-medical domains. However, its applicability to clinical tasks remains underexplored. To address this, we conducted head-to-head evaluations by fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular disease detection and systemic disease prediction tasks, across eight standardized open-source ocular datasets, as well as the Moorfields AlzEye and the UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting diabetic retinopathy (AUROC=0.850\u20130.952 vs. 0.823-0.944, across three datasets, all P\u22640.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In glaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940, P<0.001). Conversely, RETFound achieved superior performance over all DINOv2 models in predicting heart failure, myocardial infarction, and ischaemic stroke (AUROC =0.732-0.796 vs. 0.663\u20130.771, all P<0.001). These trends persisted even with 10% of the fine-tuning data. These findings showcase the distinct scenarios where general-purpose and domain-specific FMs excel, highlighting the importance of aligning FM selection with task-specific requirements to optimise clinical performance.", "sections": [{"title": "Introduction", "content": "The advent of foundation models (FMs) have ushered in a paradigm shift in medical deep learning (DL) approaches[1][2][3][4]. Driven by the availability of large-scale high-quality unstructured datasets and advanced self-supervised learning methods, FMs excel in performing multiple tasks with high adaptability and generalizability to new, unseen tasks, often requiring minimal fine-tuning [5][6]. In ophthalmology, the introduction of RETFound, the first retina-specific FM developed by Zhou et al. [7] in 2023, led to a significant transformation in DL approaches for medical image analysis. Using a Vision Transformer (ViT) [8][9] backbone and a Masked Autoencoder (MAE)-based [10] self-supervised learning approach, RETFound was pre-trained on a diverse dataset of 1.4 million natural images (ImageNet-1K), ~900,000 color fundus photographs (CFPs), and ~730,000 OCT scans. This pretraining approach enabled RETFound to effectively extract and learn retinal features and representations, enhancing diagnostic performance, data efficiency, and generalizability across ophthalmic and systemic clinical tasks [11][12][13].\nIn parallel, DINOv2 is a general-purpose vision FM developed by Meta AI, pre-trained on 142 million natural images from the LVD-142M dataset [14]. With its sheer scale and versatile feature representation capabilities, DINOv2 has demonstrated strong adaptability across some medical domains, such as pathology [15][16] and radiology[17]. However, the applicability of DINOV2 to ophthalmic tasks remains largely unexplored, and a direct comparison with RETFound in fine-tuning for retinal image analysis has not yet been performed.\nTo address these gaps, we conducted a head-to-head performance comparison between three DINOv2 models (large, base, small) and RETFound across a range of ocular disease detection and systemic disease prediction tasks. For a fair and standardized comparison, we replicated the RETFound study's original fine-tuning methodology on all three DINOv2 models (Fig. 1). The models' comparative performances were evaluated across eight open-source datasets for ocular disease detection tasks including diabetic retinopathy (APTOS-2019, IDRID, MESSIDOR2), glaucoma (PAPILA, Glaucoma Fundus), and multi-class eye diseases (JSIEC, Retina, OCTID). For systemic disease prediction tasks (3-year incidence of heart failure, myocardial infarction, ischemic stroke), we fine-tuned all models on the Moorfields AlzEye dataset, and externally tested on the UK Biobank dataset. Model performance was primarily measured using the area under receiver operator characteristics curve (AUROC), and the significant differences in AUROC values between models were determined through two-sided t-tests with Bonferroni correction. This comprehensive evaluation provides critical insights into the respective strengths and optimal applications of general-purpose and retina-specific FMs, contributing new understanding to the utilization of FM in ophthalmology and beyond."}, {"title": "Results", "content": "The demographic characteristics and details of study datasets utilized are detailed in Supplementary Table 1 and 2.\nComparative Performance of Models in Ocular Disease Detection Tasks:\nDINOv2 fine-tuned models generally outperformed RETFound across ocular disease detection tasks in internal and external testing (Fig.2, Extended Data Fig. 1-4, and Supplementary Table 3-4).\nFor diabetic retinopathy (DR) detection task, DINOv2-Large achieve higher AUROC values across the datasets. For example, after fine-tuning and internally validating using the APTOS2019 dataset, DINOv2-Large achieved an AUROC of 0.952 (95% CI: 0.950\u20130.954), surpassing RETFound (AUROC: 0.944, 95% CI: 0.941\u20130.946, all P<0.001, Fig. 2 and Extended Data Fig. 1). Consistently, in the MESSIDOR-2 dataset, DINOv2-Large achieved an AUROC of 0.906 (95% CI: 0.902\u20130.910), higher than RETFound (AUROC: 0.883, 95% CI: 0.878\u20130.889, P < 0.001). Similarly, in the IDRID dataset, DINOv2-Large obtained an AUROC of 0.850 (95% CI: 0.836-0.865), outperforming RETFound (AUROC: 0.823, 95% CI: 0.809\u20130.836, P = 0.007). Similarly, in external validations, where cross-evaluation was performed by fine-tuning models on one dataset and testing them on the other two (repeating this permutation across all three datasets), DINOv2 models generally performed better than RETFound (Extended Data Fig. 3 and Supplementary Table 4). For instance, when fine-tuned on APTOS2019 and externally tested MESSIDOR2, DINOv2-Large achieved the highest AUROCs of 0.817, outperforming RETFound (AUROCs: 0.725 in MESSIDOR2, Extended Data Fig. 3a, all p<0.001). When fine-tuned on IDRID and tested on APTOS2019, DINOv2-Large (AUROC:0.783) and DINOv2-Small (AUROC:0.808) outperformed RETFound (AUROC:0.752, all P<0.001). Evaluations based on accuracy, Kappa values, and F1-score, consistently showed that DINOv2 models outperformed RETFound in most scenarios (Extended Data Fig. 3b-3d).\nFor the glaucoma detection task, when fine-tuned and internally validated on the Glaucoma_fundus dataset, DINOv2 models achieved higher AUROC scores than RETFound (Fig.2). Specifically, DINOv2-Base and -Small achieved AUCs of 0.958 (95% CI: 0.955\u20130.961), outperforming RETFound (AUROC: 0.940, 95% CI: 0.936\u20130.945, all P<0.001, Fig. 2 and Extended Data Fig. 1). Similarly, in external validation, when fine-tuned on the Glaucoma_fundus dataset and tested on the PAPILA dataset, DINOv2-Large achieved the highest performance with an AUROC of 0.625, surpassing RETFound's 0.558 (P<0.001, ExtendedDataFig.4a, Supplementary Table 4). Evaluations based on accuracy, Kappa values,\nand F1-score, consistently largely showed that DINOv2 models outperformed RETFound in most scenarios (Extended Data Fig. 4b-4d).\nFor the multi-class eye disease classification tasks, the models were finetuned and validated internally on datasets containing CFPs (the JSIEC and Retina datasets) and OCT scans (Fig.2). In the JSIEC CFP dataset, all models were comparable and achieved AUROC scores close to 1. In the Retina CFP dataset, the DINOv2 models achieved AUROC scores ranging from 0.874 (DINOv2-Base: 95% CI: 0.864-0.883) to 0.892 (DINOv2-Large: 95% CI: 0.883-0.902), surpassing RETFound's performance (AUROC: 0.846, 95% CI: 0.836-0.856, all P\u22640.001). For the OCTID dataset with OCT scans, all models showed comparable performance.\nComparative Performance of Models in Predicting 3-year incidence of Systemic Diseases:\nWe further evaluated the comparative performance of DINOv2 and RETFound models in predicting three cardiovascular-related systemic diseases - heart failure, myocardial infarction, and ischemic stroke. Across all three systemic disease outcomes, the models were fine-tuned and tested internally on the Moorfields AlzEye study dataset, and externally validated using the UK Biobank dataset (Fig. 3 & Extended Data Fig. 5-6 & Supplementary Table 5).\nFor heart failure prediction, in internal testing, RETFound significantly outperformed all DINOv2 variants, achieving an AUROC of 0.796 (95% CI: 0.767-0.827), compared to DINOv2-Base's 0.771 (95% CI: 0.738-0.807), the best-performing DINOv2 model (P < 0.001, Fig 3). For myocardial infarction prediction, RETFound achieved the highest AUROC of 0.732 (95% CI: 0.663-0.805), significantly outperforming the best DINOv2 model, DINOv2-Large (AUROC: 0.711, 95% CI: 0.638-0.784, P < 0.001). For ischemic stroke prediction, RETFound demonstrated consistent superiority with an AUROC of 0.754 (95% CI: 0.684-0.838), surpassing the best DINOv2 model, DINOv2-base (AUROC: 0.714, 95% CI: 0.633-0.789, P < 0.001).\nIn the external validation using the UK Biobank dataset (Extended Data Fig.6), RETFound consistently outperformed all DINOv2 models across the three systemic disease prediction tasks. For heart failure prediction, RETFound achieved the highest AUROC (0.674), surpassing all DINOv2 models (AUROC: 0.615-0.623, all P<0.001). Similarly, for myocardial infarction prediction, RETFound maintained its advantage with an AUROC of 0.594, outperforming all DINOv2 models (AUROC: 0.523-0.559, all<0.001). In ischemic stroke prediction, RETFound achieved an AUROC of 0.586, exceeding DINOv2-Base (0.556) and DINOv2-Small (0.519) (all P \u22640.008). Secondary metric, including accuracy, further reinforced RETFound's superiority, with higher values compared to all DINOv2 models across all three systemic outcomes (Extended Data Fig. 6)."}, {"title": "Comparative Performance in Label Efficiency:", "content": "To investigate the impact of training data size on FMs performance, we conducted an in-depth analysis using varying proportions of training/ fine-tuning data (10%, 20%, 50%, 90%, and 100%) across different ocular and systemic disease prediction tasks (Fig. 4 and Supplementary Table 6-7).\nFor DR detection task, in the APTOS2019 dataset, the DINOv2 models and RETFound achieved comparable AUROC performance, with minimal variation across different proportions of training data (Fig. 4 and Supplementary Table 6). However, in the MESSIDOR2 and IDRID datasets, the DINOv2 models consistently outperformed RETFound, achieving higher AUROCs as the training data proportion decreased. Furthermore, in the APTOS2019 and MESSIDOR2 datasets, the performances of DINOv2 models remained relatively stable (i.e. no significant drop) despite decrease in training data size. In the multi-class eye disease detection tasks (e.g., JSIEC, Retina, OCTID), DINOv2 models consistently exhibited higher AUROCs than RETFound across nearly all training data proportions. Notably, DINOv2 models required fewer training samples to achieve superior performance (particularly in the OCTID dataset), demonstrating better label efficiency in these multi-class tasks. Nevertheless, in glaucoma detection task, all models largely showed similar performances across the scenarios.\nOn the other hand, in systemic disease prediction tasks, RETFound exhibited a distinct advantage where it consistently achieved higher AUROC values across all training data proportions (Fig. 4 and Supplementary Table 7). Notably, as training data decreased, RETFound maintained relatively stable or similar performances across the three systemic prediction tasks, reflecting its robustness. At lower training proportions (e.g., 10%-50%), DINOv2 models, particularly DINOv2-Small, exhibited more variability and consistently underperformed compared to RETFound.)."}, {"title": "Comparison of Calibration Performance between DINOv2 and RETFound Models", "content": "We compared the calibration performance of DINOv2 models (Large, Base, and Small) with RETFound across ocular and systemic disease tasks using corresponding datasets. Overall, we observed that DINOv2 models generally demonstrated better calibration for ocular diseases, while RETFound exhibited superior calibration for systemic disease predictions.\nFor DR detection task, in the APTOS2019 dataset, it was revealed that DINOv2-Base and -Small were better calibrated than RETFound, as indicated by their calibration curves being closer to the perfectly calibrated line and their lower expected calibration error (ECE) values (Fig. 5 &\nExtended Data Fig. 7\u20138). In the IDRID dataset, DINOv2-Small demonstrated the best calibration performance with its curve closely following the diagonal line. Similarly in glaucoma detection, DINOv2-Base (ECE: 0.015) performed best, especially in high-probability regions in the Glaucoma_fundus dataset, while DINOv2-Large (ECE:0.040) achieved superior overall calibration on the PAPILA dataset, outperforming RETFound (ECE=0.065). On the other hand, for datasets covering multiple retinal diseases, such as Retina, DINOv2-small (ECE:0.013) had more superior calibration than RETFound (ECE:0.027). In the JSIEC and OCTID datasets, all models exhibited fluctuations and had similar calibration.\nAcross the tasks of predicting heart failure and myocardial infarction (Fig. 5), RETFound demonstrates stable calibration performance. Its calibration curves were closer to the perfectly calibrated line, and it achieved ECE values mostly lower than DINOv2 models' values. Nevertheless, when predicting ischemic stroke, RETFound exhibited poorer calibration compared to its performance in other systemic disease tasks, with an ECE of 0.212. By contrast, DINOv2-Base achieved the best calibration among all models with an ECE of 0.024, followed by DINOv2-Small (ECE = 0.125). DINOv2-Large, however, displayed the highest calibration error among all models, with an ECE of 0.312, indicating substantial miscalibration in its probability estimates for ischemic stroke prediction."}, {"title": "Discussion", "content": "In this study, we evaluated the comparative performance and label efficiency between fine-tuning on DINOv2, a general-purpose vision FM, with RETFound, a retina-specific FM, across a range of ocular disease detection and systemic disease prediction tasks. We ensured a standardized and robust comparison with RETFound by replicating RETFound study's original fine-tuning methodology on all three DINOv2 models (large, base, small). This is a rare feat and has not been conducted prior. Overall, in ocular disease detection tasks, we observed that DINOv2-large outperformed RETFound and with better label efficiency. Additionally, DINOv2-base and -small models generally required three times less computing resources compared to RETFound. Conversely, in systemic disease prediction tasks, RETFound generally achieved superior performance and better label efficiency than all the DINOv2 models. This comprehensive head-to-head evaluation highlights distinct scenarios where general-purpose and domain-specific FMs excel, offering practical information to users on the optimal application of these FMs for task-specific fine-tuning.\nIn ocular disease detection tasks, DINOv2 generally outperformed RETFound, when fine-tuned on the complete dataset, highlighting the transferability of DINOv2 to these tasks. Additionally, DINOv2 demonstrated exceptional label efficiency, maintaining robust AUROC performance even with limited training data (Fig. 4 and Supplementary Table 6). DINOv2's superior performance may be attributed to its pretraining strategy. First, its exposure to a large corpus of 142 million natural images enabled DINOv2 to learn diverse visual patterns and contexts, enhancing its feature extraction capabilities and thereby establishing a stronger foundation for various downstream visual tasks. This could have contributed to DINOv2's exceptional transfer learning capabilities, a finding consistent with prior research on AMD and DR detection[20][21]. Secondly, DINOv2 employs a self-distillation process where knowledge transfer occurs between the teacher and student network, enabling the model efficiently learn refined visual representations. This broad knowledge of visual representations acquired during its larger pretraining could have enhanced DINOv2's generalizability to ocular disease detection tasks. Altogether, this suggests that DINOv2 could serve as a competitive alternative to RETFound particularly in low-data resource settings for specific ocular diseases detection tasks.\nInterestingly, while DINOv2 demonstrated superior performance over RETFound in the ocular disease diagnosis task, this trend was not fully replicated in all external validations (Extended Data Figures 3 and 4). Our observation highlighted a common challenge in model generalizability, where variations in data distribution between datasets can impact model reliability and performance especially when the models are deployed in a new real-world environment. Therefore, addressing these issues of model generalization would help researchers to determine the clinical applicability of these FMs in real-world practice."}, {"title": "Additionally, in terms of computational resource consumption (Supplementary Table 8), RETFound required more GPU RAM during the fine-tuning phase but had a lower inference speed (i.e. took more time to process the same number of images) as compared to the DINOv2- small and -base. DINOv2-small (22M parameters) and DINOv2-base (86M parameters) are significantly smaller models compared to RETFound and DINOv2-large (both models have 303M parameters). As such, when tested on 100 retinal images (with 224\u00d7224 resolution and processed on a single A100 GPU), DINOv2-small (0.48 seconds) and DINOv2-Base (0.64 seconds), both had lower inference time compared to RETFound (1.48 seconds) and DINOv2-Large (1.70 seconds). This underscores the potential of smaller and more computational resource efficient general-purpose vision FMs like DINOv2-small and base as a competitive alternative to RETFound for ocular disease detection tasks under resource-scarce settings. More importantly, it highlights the need to consider the trade-off between model performance and computational resource requirements depending on unique deployment sites.", "content": "On the other hand, in systemic disease prediction tasks, RETFound consistently outperformed the DINOv2 models across all varying proportions of fine-tuning data used, indicating RETFound's superior label efficiency and its ability to detect subtle retinal changes associated with systemic diseases. (Fig. 3 - 4 and Supplementary Table 5 and 7). This superior performance could be attributed to its pretraining strategy, which harnessed domain-specific knowledge from 900,000 unlabeled retinal images using the MAE-supervised learning approach [7]. The MAE approach involves having the model reconstruct highly masked input images, forcing the model to infer missing information from the masked patches of the images during training. Through this iterative training process, the model learns to extract subtle, fine-grained vasculature patterns and features from the masked retinal images, thereby potentially contributing to RETFound's retina-specific domain knowledge. Overall, these findings suggest the advantages of incorporating domain- specific data in the pretraining process of FMs. For oculomics tasks, RETFound superior label efficiency demonstrated its potential as an effective pre-trained model, especially well-annotated data still remain scare and expensive to curate.\nOur study's key strength lies in replicating the intricate methodology of the original RETFound experiments, ensuring consistent and robust head-to-head evaluation between DINOv2 and RETFound across diverse ocular disease detection and systemic disease prediction tasks. We fine- tuned the DINOv2 models using multiple open-source datasets, applying the same fine-tuning and test data split as applied in RETFound's task-specific adaptions. To our knowledge, such a benchmarking evaluation of a general-purpose vision FM and RETFound has never been conducted to date. This open approach provides credibility to the repeatability of our study findings, and would help to facilitate comparable benchmarking evaluations for future FMs."}, {"title": "Nevertheless, this study has a few limitations. In this study, the range of detection tasks included was limited by the availability of annotated open-source CFP and OCT datasets with specific disease labels. Therefore, the comparative performance of DINOv2 and RETFound models was not demonstrated on other modalities such as ultra-wide field retinal imaging. Furthermore, even though we evaluated the models' performance in detecting multiple ocular diseases and predicting multiple systemic diseases, other rare ocular (e.g. Retinitis Pigmentosa) and systemic diseases were not included in this study. Future work should include benchmarking of the performance of FMs on other conditions such as dementia and coronary artery diseases. Additionally, due to the rarity of open-source datasets with systemic disease annotation, the performance of DINOv2 and RETFound models in predicting the 3-year incidence of heart failure, myocardial infarction and ischemic stroke was evaluated using the private MEH-AlzEye and the UK Biobank datasets. This implies that a benchmarking dataset that encompasses a broader spectrum of use cases, including ocular diseases, systemic disease prediction, general medical conditions, and rare diseases, would be essential for standardizing performance evaluation of these FMs. The reliance on private datasets implies that future benchmarking evaluations on these systemic disease outcomes would necessitate obtaining relevant data access approvals from the respective institutions.", "content": "Building on our findings, to deepen insights on the clinical applicability of these FMs, it will be critical to evaluate their comparative performance using datasets that are representative of real- world setting. Specifically, there is a need to further investigate novel methods to improve the performance, generalizability and efficiency of these FMs in scarce-data and resource conditions (i.e. the real-world clinical challenge). One method could be the use of multimodality to synergize feature-rich image data and contextual information-rich text data from electronic health records in the pre-training phase, hence allowing the models to learn underlying dynamic relationship between the different data type [20]. Future work would involve iterative refinement and continuous validations of these FMs for diverse clinical task scenarios across the medical domain to broaden their application to wider healthcare contexts. Globally, this will require collaborative efforts from researchers worldwide to curate diverse datasets, adopt standardized reporting practices to ensure reproducibility and comparability, and engage stakeholders in collectively democratizing access to datasets and advanced FMs [21]."}, {"title": "Conclusion", "content": "This study provides a comprehensive evaluation of general-purpose vision FMs (DINOv2 models) and a domain-specific FM (RETFound), highlighting DINOv2's optimal adaptability for ocular disease detection tasks and RETFound's superior performance in systemic disease prediction. These findings provide new insights into the optimal utilisation of general-purpose vision FM and retina-specific FM, highlighting the importance of aligning FM selection with task- specific requirements and resource considerations to achieve optimal clinical performance."}, {"title": "Method", "content": "Details of pretraining datasets for DINOv2 models\nAll three model versions of DINOv2 - Small, Base, and Large were pretrained on the LVD- 142M dataset, which is a large-scale unannotated dataset with a total of 142,109,386 natural images [14]. This massive dataset was curated from multiple open-source natural image databases, including the ImageNet-22k (56,788,344 images, 40.0%), the ImageNet-1K[22] (40,997,344 images, 28.8%), the Google Landmarks v2 (6,321,880 images, 4.4%), and Mapillary SLS (1,434,262 images, 1.0%) datasets, and the remaining 36,567,556 images (25.8%) obtained through retrieval and augmentation of multiple smaller datasets. To ensure data quality and diversity, researchers employed self-supervised image retrieval and clustering methods to filter and rebalance the curated dataset, which provided rich visual feature learning resources for pretraining of the DINOv2 models.\nTask-specific datasets for ocular disease detection.\nWe evaluated model performance on multiple publicly available ophthalmological datasets covering various eye diseases. For DR detection, we used three datasets, the IDRID (India)[23][24], MESSIDOR-2 (France)[25][26][27], and APTOS-2019 (India) datasets[28]. The DR grading for these datasets were based on the International Clinical Diabetic Retinopathy Severity Scale, which indicated five stages of severity from no DR to proliferative DR. For detection of glaucoma, we used the PAPILA (Spain)[29] and Glaucoma Fundus (Korea)[30] datasets. Both datasets have glaucoma labels of three classes, non-glaucoma, early glaucoma (suspected glaucoma), and late-stage glaucoma. For detection of multi-category eye diseases, we used two CFP datasets including the JSIEC dataset (China)[31] which comprises 1,000 images covering 39 classes of common referable fundus diseases and conditions, and the Retina dataset [32], comprising 601 images with normal, glaucoma, cataract, and retinal diseases annotations. An additional OCT dataset, the OCTID dataset (India)[33], containing 470 OCT scans with labels including normal, macular hole, age-related macular degeneration (AMD), central serous retinopathy, and DR was also used. For more detailed information about the datasets used, please refer to Supplementary Table 1.\nTask-specific datasets for incidence prediction of systemic diseases.\nFor incidence prediction of systemic diseases, we focused on three cardiovascular disease outcomes: heart failure, myocardial infarction, and ischemic stroke. We used retinal images to predict the risk of these three cardiovascular diseases occurring within the next 3 years. All models were fine-tuned on a curated private dataset from the Moorfields AlzEye[34] study (\u041c\u0415\u041d- AlzEye) and internally evaluated on a held-out test dataset. The MEH-AlzEye study is a large- scale retrospective cohort study that linked the ophthalmological data of 353,157 patients seen at Moorfields Eye Hospital between 2008 and 2018 with systemic health data across England. These"}, {"title": "DINOv2 model architecture and implementation", "content": "DINOv2 employs Vision Transformer (ViT) as its backbone network with three different model size scales: Small, Base, and Large. DINOv2-Small (21M parameters) is based on the ViT-S/14 architecture, with an embedding dimension of 384, with 6 attention heads, and 12 Transformer blocks. DINOv2-Base (86M parameters) utilized the ViT-B/14 architecture, featuring an embedding dimension of 768, with 12 attention heads, and 12 Transformer blocks. DINOv2-Large (303 M parameters) was built on the ViT-L/14 architecture, with the largest embedding dimension of 1024, along with 16 attention heads, and 24 Transformer blocks. All three model versions use a patch size of 14\u00d714. During training, DINOv2 employs the AdamW optimizer with an initial learning rate of le-3, a batch size of 2048, and a total of 625k iterations. To enhance performance, LayerScale technology was introduced with an initial value of le-5. Weight decay follows a cosine schedule ranging from 0.04 to 0.2. Learning rate warm-up lasts for 100k iterations, while the teacher network momentum also adopts a cosine schedule, gradually increasing from 0.994 to 1. The models were primarily trained in float16 precision, but the gradients for DINO heads were computed in float32 precision to ensure accuracy. The feed-forward network layers used a multilayer perceptron (MLP) structure with a dropout rate of 0 to maximize information transfer. These models were distilled from a larger ViT-G/14 model, effectively learning and extracting powerful visual feature representations while maintaining training efficiency."}, {"title": "Details on task-specific fine-tuning of the DINOv2 models.", "content": "When adapting to downstream tasks, we used all three versions of the DINOv2 model (Large, Base, or Small) to generate high-level features from the retinal images, and added an MLP block to each model. The MLP block takes these high-level features as input and generates probability scores (ranging from 0 to 1) as outputs in each ocular disease or systemic disease incidence prediction task. The number of neurons in the last layer of the MLP block is determined by the number of categories in each task. To enhance the models' generalization, we also applied various data augmentation techniques, including random cropping, horizontal flipping, vertical flipping, rotation, and adjustments to brightness and contrast. The batch size was set to 32 and training was conducted over 100 epochs. The first 10 epochs was a learning rate warm-up phase, increasing from 0 to 5\u00d710-3. For the subsequent 90 epochs, a cosine annealing scheduling was applied, gradually decreasing the learning rate from 5\u00d710-3 to 1\u00d710-6. We used the AdamW optimizer with weight decay set to 0.05. After each training epoch, the models were evaluated on the validation set and the model iteration with the highest AUROC on the validation set was used for all downstream evaluation. These model weights were then implemented for independent internal and external evaluations. For RETFound, we utilized the publicly available pre-trained weights of the model for each specific task.\nFor the cross-dataset evaluation of DR, glaucoma detection, and systemic disease prediction, we employed the same fine-tuning strategy but trained and tested the models on different dataset combinations. Specifically, the datasets used for DR include APTOS-2019, MESSIDOR-2, and IDRID; for glaucoma, Glaucoma_fundus and PAPILA; and for systemic diseases, Moorfields AlzEye and UK Biobank.\nIn the label efficiency analyses, we applied varying percentages of training data (from 10% to 100%) and finetuned the models for ocular and systemic disease prediction tasks. This label efficiency analysis was repeated across all datasets (eight open-source ocular disease datasets and the Moorfields AlzEye dataset), and all models were internally validated on held-out test sets. This enabled a comprehensive analysis of the models' performance in scenarios with limited labeled data for task-specific finetuning.\nPerformance evaluation and statistical analysis\nFor all tasks, we used the area under receiver operating characteristics curve (AUROC) to evaluate the models' diagnostic performance. For multi-class detection tasks, including detection of DR (5 classes), glaucoma (3 classes), multi-category eye diseases and systemic diseases, we calculated the AUROC for each class and then averaged across the number of classes to obtain the overall AUROC. We calculated the mean and standard deviation of the AUROC by randomly sampling 20% of the data in each iteration across 100 bootstrap replicates. The standard error was obtained by dividing the standard deviation by \u221a100, and the 95% confidence interval was computed as the mean \u00b1 1.96 \u00d7 standard error. Normality was evaluated using the Shapiro-Wilk test applied to the AUROC derived from 100 bootstrapped replicate samples, with the majority demonstrating a normal distribution. To establish statistical differences in the AUROC performance between the DINOv2 and RETFound models, we computed the p-value (P) using the two-sided t-test with Bonferroni correction. Statistical significance was defined as P less than 0.05/3 (P=0.017), accounting for multiple pairwise comparisons between RETFound and the DINOv2 models. To evaluate the model calibration performance, we calculated the Expected Calibration Error (ECE) and plotted the reliability diagrams to ensure that the model's predictive accuracy matches its"}, {"title": "confidence level. For external evaluations of ocular and systemic diseases, we also reported metrics such as Kappa, Fl-score, and accuracy.", "content": "Ethics statement\nThis study, approved by the London-Central Research Ethics Committee (18/LO/1163, 1 August 2018), the Advanced Statistical Modelling of Multimodal Data Project (20/HRA/2158, 5 May 2020), and the Confidential Advisory Group (18/CAG/0111, 13 September 2018), received final approval from the NHS Health Research Authority on 13 September 2018. De-identification was validated by Moorfields Eye Hospital NHS Foundation Trust, and only de-identified retrospective data were used, with no direct patient involvement.\nData availability\nThe AlzEye dataset is governed by contractual restrictions between NHS Digital, Moorfields Eye Hospital, and University College London, limiting access to the AlzEye research team. While collaborations are welcomed, individual-level systemic health data can only be analyzed by AlzEye researchers. Detailed information about"}]}