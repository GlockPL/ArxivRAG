{"title": "What Makes Cryptic Crosswords Challenging for LLMs?", "authors": ["Abdelrahman Sadallah", "Daria Kotova", "Ekaterina Kochmar"], "abstract": "Cryptic crosswords are puzzles that rely on general knowledge and the solver's ability to manipulate language on different levels, dealing with various types of wordplay. Previous research suggests that solving such puzzles is challenging even for modern NLP models, including Large Language Models (LLMs). However, there is little to no research on the reasons for their poor performance on this task. In this paper, we establish the benchmark results for three popular LLMs: Gemma2, LLAMA3 and ChatGPT, showing that their performance on this task is still significantly below that of humans. We also investigate why these models struggle to achieve superior performance. We release our code and introduced datasets at https://github.com/bodasadallah/decrypting-crosswords.", "sections": [{"title": "1 Introduction", "content": "A cryptic crossword is a type of crossword puzzle known for its enigmatic clues (Friedlander and Fine, 2016). Unlike standard crossword puzzles, where clues are straightforward definitions or synonyms of the answers, cryptic crosswords involve wordplay, riddles, and cleverly disguised hints that make solving them more challenging (Moorey, 2018).\nTo solve a cryptic clue, one must not only apply generic rules in the specific context of the clue but also use domain-specific knowledge to produce a reasonable answer. Therefore, tackling cryptic crosswords with modern NLP methods provides a novel and interesting challenge. It has been shown that NLP models' performance is far from that of humans: Rozner et al. (2021) and Efrat et al. (2021) report an accuracy of 7.3% and 8.6% for rule- and transformer-based models. Sadallah et al. (2024) and Saha et al. (2024) show similarly low results for LLMs. In contrast, expert human solvers achieve 99% accuracy and self-proclaimed amateurs reach 74% (Friedlander and Fine, 2009, 2020), however, there are still no official statistics for average human performance.\nTypically, a cryptic clue can be divided into two parts: the definition and the wordplay. The definition consists of one or more words in the clue that can be used interchangeably with the answer, and it usually appears either at the beginning or at the end of the clue. The wordplay can take many forms: the most popular ones include anagrams, hidden words, and double definitions, among others\nPast approaches to solving cryptic clues range from rule-based models to traditional machine learning models like KNN (Rozner et al., 2021) and transformers like T5 (Rozner et al., 2021; Efrat et al., 2021). However, all these models achieve only modest accuracy on the task (see Section 2). The fact that LLMs can develop emergent capabilities suggests that they may be able to solve cryptic puzzles if not on a par with human solvers, then at least somewhat successfully,"}, {"title": "2 Related Work", "content": "Although prior work looked into wordplay and traditional crosswords, much less attention has been paid to cryptic crosswords. The early work of Deits (2015) achieved 7.3% accuracy on the task with a rule-based solver, which applied hand-crafted probabilistic context-free grammar to generate all possible syntactic structures for clue words. Following this, Efrat et al. (2021) introduced Cryptonite, a dataset of 523,114 cryptic clues collected from The Times and The Telegraph. They fine-tuned a T5 model, which helped set the benchmark accuracy for Transformer models at 7.6%. Similarly, Rozner et al. (2021) introduced a dataset extracted from The Guardian and used a curriculum learning approach, which involved training a model on simpler tasks before progressing to more complex compositional clues. This increased the performance to 21.8%.\nRecently, Sadallah et al. (2024) and Saha et al. (2024) have evaluated multiple LLMs on the task of solving cryptic crossword puzzles using a range of prompting techniques, including zero and few-shot learning. While Sadallah et al. (2024) also explicitly fine-tune open-source LLMs for this task, Saha et al. (2024) use a combination of chain-of-thought (CoT) and self-consistency (SC) techniques, and achieve an accuracy score of 20.85% with GPT4-turbo Both conclude that LLMs' performance on this task is still far from that of human experts. However, neither work further analyzes the models' behavior or why they struggle with this task."}, {"title": "3 Data", "content": ""}, {"title": "3.1 The Guardian dataset", "content": "In our experiments, we primarily use the dataset introduced by Rozner et al. (2021), which was extracted from The Guardian. Most previous models were tested on this dataset, so we have chosen it for comparison purposes as well. In total, the dataset contains 142,380 clues. Rozner et al. (2021) introduced two different splits for it: naive (random) and word-initial disjoint. We evaluate our models on the test subset of 28,476 examples from the naive (random) split, as it has more diverse examples than the other split."}, {"title": "3.2 Times for the Times dataset", "content": "To test models' performance across datasets, we use the data collected by George Ho, where every clue has a marked definition. The original dataset contains around 600k clues from many sources, which would result in extremely expensive experimentation with LLMs. For our experiments, we have sampled 1,000 representative examples"}, {"title": "3.3 Small explanatory dataset", "content": "Unfortunately, there is no large-scale dataset that contains information about the wordplay types of the clues. To investigate whether our models can detect wordplay types, we have annotated 200 examples from the additional dataset including 40 clues for each major wordplay type (anagram, assemblage, container, hidden word, and double definition"}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Zero-shot setup", "content": "Base prompt We begin by defining a simple prompt that only includes the minimal information required to solve the task. We include the line \"you are a cryptic crosswords expert\", as it has been shown that this phrase can help the model performance\nAll-inclusive prompt In this prompt, we combine general information about cryptic crossword solving without adding examples or CoT. We include information about clue parts and their meanings. We also add information about the typical position of the definition in the clue. Finally, our preliminary experiments suggest that LLMs often struggle to understand the constraints of the answer length mentioned in the clue, so we explicitly tell the model that the number of letters in the answer is indicated in parentheses at the end of the clue. In addition, we experiment with solving a cryptic clue using the definition provided."}, {"title": "4.2 Dividing solution process into sub-tasks", "content": "Next, we investigate why the models struggle to solve the task. To do that, we design experiments to test the models' ability to (1) extract definition word(s) from the clue, (2) detect the wordplay type with varying levels of information, and (3) explain the solution process given the clue and the answer."}, {"title": "5 Experiments and Discussion", "content": "We choose two of the most recent and popular open-source LLMs, Gemma2 and LLAMA3 and one closed-source model, ChatGPT. The details are provided in Appendix A, and the results in Table 1."}, {"title": "5.1 Cryptic clue solving", "content": "The first four rows of Table 1 show the models' accuracy in solving cryptic clues on two different datasets for two different prompts. We can see that ChatGPT outperforms the open-source models. Also, we can conclude that providing the models with the definition improves their performance. To put these results into perspective, in Table 2, we compare our results with those obtained by Rozner et al. (2021). We observe that using ChatGPT in a zero-shot setting achieves results comparable to (but still lower than) those of T5 fine-tuning. One important thing to note is that Rozner et al. (2021) explicitly fine-tuned models on the task, while the models we used are general LLMs that were pre-trained on the generic language modeling task."}, {"title": "5.2 Understanding various aspects of the task", "content": ""}, {"title": "5.2.1 Definition extraction", "content": "We ask the models to extract the definition part of the clue with the prompt illustrated in Figure E3. We specify that the definition should be a synonym for the answer but do not indicate that the definition usually appears at the beginning or end of the clue. All models show higher results in the definition extraction task. One reason for this could be that the definition is explicitly included in the clue, making the task a matter of repeating part of the clue, which is generally easier than generating new words as an answer."}, {"title": "5.2.2 Wordplay detection", "content": "Determining the wordplay type We identify five major types of wordplay listed in Table B1. Then we investigate if our models could identify the wordplay type from the clues. Usually, professional solvers note indicator words that relate the clue to one type or another: for example, confused, mixed up, and mad usually indicate anagrams. To test the models' ability to identify the wordplay type, we design three experiments that gradually add information to the prompt. The specific design of the experiments is described in the Appendix C.\nThe results show that adding the definition for the wordplay and providing a model with the answer do not significantly improve the model's ability to extract the wordplay type except for Gemma, which has a performance increase of 10%. LLAMA3 only predicted one wordplay type (hidden word) using the \u2018wordplay types' prompt (see Figure E4), but providing more information in the other prompts helped the model predict other types. We hypothesize that a potential reason for LLaMA3's behavior is that the model seems to attend more to the task prompt than the clue itself.\nWe acknowledge that the small dataset size might constrain our ability to draw definitive conclusions. However, an important observation is that all 3 models over-predict some types (anagram and hidden word) while under-predicting others (assemblage). We include the full analysis with the models' confusion matrices on the most informative prompt shown in Figure E6 in Appendix C."}, {"title": "5.2.3 Explanation extraction", "content": "Finally, we ask the models to explain the solution, given the clue and the answer. Our analysis of the models' answers shows that: (1) All the models follow some kind of structure in their explanations, breaking the clue into parts of one to three words; however, this separation often does not seem to make sense, as it may combine both definition and wordplay parts together or use words that do not interact with each other. (2) LLAMA3 does not mention any wordplay operations and only works at a synonym level, which is insufficient for solving the clues. (3) Gemma shows the knowledge of some operation types (such as anagram and even homophones-related operations) but applies it incorrectly. (4) ChatGPT recognizes that something should be done with the characters and words in the clue and sometimes even gets it right, for example, suggesting taking an anagram of a given word or putting together words in an assemblage clue; however, it does not properly \"understand\" the procedure. For instance, one of the ChatGPT's outputs is: rearranging the letters of \"pan\" and adding \"to cook cheese\" results in \"parmesan\". This statement is incorrect, as one cannot get \"parmesan\" from the letters in \"pan\" and \"to cook cheese.\" (5) The easiest type to generate sensible explanations for are clues for the double definition type, where both parts of the clue are synonymous with the answer this aligns with how base LLMs were trained."}, {"title": "6 Conclusions and Future Work", "content": "In this work, we have focused on studying the inner workings of LLMs while solving cryptic crosswords rather than trying to improve their performance on this task. We began by evaluating the models under a zero-shot setting and then tried to gain insights into their understanding of cryptic clues through auxiliary tasks. The results suggest that although ChatGPT model overall outperforms open-source LLMs, solving cryptic crosswords remains a very challenging task for all tested LLMs, with a significant room for improvement. In addition, we conclude that splitting the task into sub-tasks helps the models to some extent, which indicates that models cannot break down the composite task by themselves. The performance of the models on the chosen subtasks still remains unsatisfactory: the models struggle to identify the definition and the wordplay type.\nWe believe the performance can be improved in future work using several possible research directions. Firstly, promising avenues for research in this area are chain-of-thought and tree-of-thought techniques. This is motivated by our current results that suggest that splitting the task into simpler subtasks helps improve the model performance: specifically, CoT-based methods can teach models how to arrive at the solution step-by-step by splitting the original complex task into such multiple simpler subtasks. Secondly, given the considerable performance increase achieved using curriculum learning with T5 we consider this direction worth exploring with LLMs as well. Finally, approaches such as a mixture of experts used to train open-source models like Mixtral can be applied to the task, as models may develop expert layers specializing in separate wordplay types."}, {"title": "Limitations", "content": "Limited set of LLMs experimented with Experiments with an extensive set of state-of-the-art LLMs can get quite expensive. Due to budget limitations, we have been selective in terms of the LLMs that we use in this study. Specifically, we chose only a few of the most popular open-source and closed-source LLMs. We believe that the obtained results shed light on the current LLMs' capabilities on this task. However, we acknowledge that the set of LLMs we tested here is limited, and our results cannot be extrapolated to other LLMs. In addition, in many experiments, we have observed that minor changes in settings do not bring substantial improvement to the results. This motivated us to perform only a limited set of experiments with the chosen models, as elaborated in the paper.\nLimitations of the datasets size Some datasets we used are not large in terms of the number of examples. The main reason for this is the lack of existing datasets with rich annotation, so we had to create one such dataset ourselves. We acknowledge that the results obtained on a larger dataset may be more reliable; however, we believe that the results reported here already provide us with useful insights.\nCloseness to the real-world scenario In this work, we have focused on solving one clue at a time. In the real-world scenario, human solvers encounter twenty to thirty clues in one grid. Solving one clue usually reveals letters of the other answers, which can be quite helpful in the solution process. In contrast, our goal is to investigate LLMs' abilities in cryptic crossword clue interpretation, and we do not try to solve the whole grid.\nDangers of data contamination Finally, we observe in our experiments that ChatGPT outperforms the open-source models. We acknowledge that we lack information about its training setup, as ChatGPT is a proprietary model, and therefore, we cannot guarantee that this model's training data is uncontaminated; in other words, it is not entirely clear whether the model could have been exposed to any of the crossword clues during its training. However, we note that all LLMs still struggle to solve cryptic clues, showing that even if some contamination took place, the models do not seem to be able to memorize and simply reproduce the answers from previously seen clues. As a side note, human experts also get exposed to a lot of clues in their practice, and their performance on the task is still much higher than that of LLMs."}, {"title": "Ethics Statement", "content": "We foresee no serious ethical implications from this study."}]}