{"title": "Towards a Law of Iterated Expectations for Heuristic Estimators", "authors": ["Paul Christiano", "Jacob Hilton", "Andrea Lincoln", "Eric Neyman", "Mark Xu"], "abstract": "Christiano et al. (2022) define a heuristic estimator to be a hypothetical algorithm that estimates the values of mathematical expressions from arguments. In brief, a heuristic estimator G takes as input a mathematical expression Y and a formal \"heuristic argument\" \u03c0, and outputs an estimate G(Y | \u03c0) of Y. In this work, we argue for the informal principle that a heuristic estimator ought not to be able to predict its own errors, and we explore approaches to formalizing this principle. Most simply, the principle suggests that G(Y \u2013 G(Y | \u03c0) | \u03c0) ought to equal zero for all Y and \u03c0. We argue that an ideal heuristic estimator ought to satisfy two stronger properties in this vein, which we term iterated estimation (by analogy to the law of iterated expectations) and error orthogonality.\nAlthough iterated estimation and error orthogonality are intuitively appealing, it can be difficult to determine whether a given heuristic estimator satisfies the properties. As an alternative approach, we explore accuracy: a property that (roughly) states that G has zero average error over a distribution of mathematical expressions. However, in the context of two estimation problems, we demonstrate barriers to creating an accurate heuristic estimator. We finish by discussing challenges and potential paths forward for finding a heuristic estimator that accords with our intuitive understanding of how such an estimator ought to behave, as well as the potential applications of heuristic estimators to understanding the behavior of neural networks.", "sections": [{"title": "1. Introduction", "content": "It is often possible to estimate a mathematical expression with a high degree of confidence, even when proving tight bounds on the expression is difficult or impossible. For example, let Y be the number of primes between eN and eN + 1000N, where N is 1 billion. The prime number theorem tells us that among integers of this size, roughly one in a billion are prime. Thus, it would be reasonable to estimate that Y \u2248 1000, and we can be quite confident that 500 < Y < 2000, barring some yet-undiscovered regularity in the pattern of prime numbers. By contrast, formally proving that 500 < Y < 2000 may not be possible without a computationally intractable brute-force verification.\nArguments like this one \u2013 known as heuristic arguments \u2013 analyze the structure of a problem in order to estimate a quantity. The estimate produced by a heuristic argument reflects a \u201cbest guess\u201d about the quantity after taking into account some considerations. Correspondingly, the estimate is uncertain \u2013 the argument is not a proof \u2013 and could be revised in light of additional considerations. Heuristic arguments are common in fields such as number theory, discrete math, and theoretical computer science (see Cornu and Hilhorst (2019), Erd\u0151s and Ulam (1971), and M\u00e9zard and Zecchina (2002) for examples).\nDespite the ubiquity of heuristic arguments in mathematics, there has been little prior work attempting to formalize this style of reasoning. To our knowledge, the first in-depth attempt at formalization was Christiano et al. (2022). The authors introduced the notion of a heuristic estimation algorithm (henceforth heuristic estimator), which takes as input a formally specified real-valued expression Y together with a set of formal \u201carguments\" \u03c01,...,\u03c0m, and estimates the value of Y by incorporating the information provided by \u03c01,...,\u03c0m. The authors suggested that a heuristic estimator should be guided by a presumption of independence: presuming two sub-parts of the expression Y can be treated independently unless an argument points out a relationship between them.\nChristiano et al. (2022) discuss properties that a heuristic estimator ought to satisfy, such as linearity and respect for proofs (see Section 1.3 below). In this work, we will suggest and study a new property, inspired by the law of iterated expectations from probability theory: a heuristic estimator should not be able to easily predict its own errors."}, {"title": "1.1. A running example", "content": "In this work, we will use G (for \u201cguesser\u201d) to denote a hypothetical heuristic estimator, which takes as input a formal mathematical expression Y and a set of arguments II = {\u03c01,...,\u03c0m}, and outputs an estimate of the value of Y based on the arguments in \u03a0. As we discuss below, I can be thought of as a \u201cstate of knowledge\u201d: a formal description of all facts considered by the heuristic estimator. Under this view, G(Y | II) is an estimate of Y in light of these facts. We will use the notation G(Y | II) to denote this estimate. In the case of \u03a0 = {\u03c0}, we will simply write G(Y | \u03c0).\nIn this section, we introduce a simple example in order to illustrate how we would like a heuristic estimator G to behave.\nGiven a positive irrational number x and a positive integer k, let dr(x) be the k-th digit of x past the decimal point, when written in base 10. (For example, since \u221a2 = 1.41 ..., we have d\u2081(\u221a2) = 4, d\u2082(\u221a\u221a2) = 1, and so on.) For our example, we will take Y to be the expression\n$$Y:= \\sum_{n=101}^{120} d_6(\\sqrt{n}).$$"}, {"title": "1.2.\nPerspectives on heuristic estimation", "content": "In this section, we clarify the purpose and desired behavior of a heuristic estimator by analogizing heuristic estimates to three other concepts: proof verification, conditional expected values, and subjective probabilities and estimates."}, {"title": "Analogy to proof verification.", "content": "As discussed in Christiano et al. (2022) and Neyman (2024), a heuristic estimator can be thought of as a generalization of a formal proof verifier. A proof verifier takes as input a formal mathematical statement and a purported proof, and checks whether the proof proves the statement. Similarly, a heuristic estimator takes as input a formal mathematical expression and an argument (or arguments) about the expression, and outputs an estimate of the value of the expression that reflects those arguments. Importantly, the purpose of a heuristic estimator is to incorporate the heuristic arguments that it has been given as input, rather than to generate its own arguments. The output of a heuristic estimator is only as sophisticated as the arguments that it has been given.\nAs discussed by Christiano et al. (2022), G would ideally respect proofs: roughly speaking, if \u03c0 contains a proof that l < Y < h, then l < G(Y | \u03c0) \u2264 h. This is the sense in which heuristic estimators generalize proof verifiers, rather than just being analogous to them."}, {"title": "Analogy to conditional expectations.", "content": "In some ways, a heuristic estimator is analogous to a conditional expected value. In probability theory, the expectation of a random variable X conditioned on an event A, E [X | A] is the average value of X over the subset of outcomes in which A occurs. Intuitively, it is the estimate of X given by an observer who does not know the exact outcome and instead only knows that A occurred. Similarly, if Y is a mathematical expression and II is a set of heuristic arguments, then G(Y | II) is an estimate of Y given by an observer who has not computed the exact value of Y and has instead only done the computations described in \u03a0. Although there is a particular correct value of Y, the observer does not know this value, and G(Y | II) is a subjective \u201cbest guess\" about Y given only \u03a0."}, {"title": "Analogy to subjective probabilities and estimates.", "content": "Perhaps most intuitively, G is a procedure that extracts a subjective expectation from a state of knowledge. Under this view, I formally describes a set of facts known by an observer, and G(Y | II) is an estimate of Y in light of those facts. To clarify this perspective, we recall the notion of subjective expectations.\nThe subjectivist view of probability interprets probability as the subjective credence of an observer. For example, suppose that a coin was created to have a bias (i.e. probability"}, {"title": "1.3. The principle of unpredictable errors", "content": "Christiano et al. (2022) suggest that in order for G to be a coherent, general-purpose estimator, it ought to satisfy some formal properties. For example:\n\u2022 Correctly estimating constants: If the expression Y is a hard-coded constant c, then G should estimate Y correctly. That is, for c\u2208 R, for all II, we have G(c | II) = c. For example, if G correctly estimates constants, then G(2 | I) = 2 for all I (but G(22 | II) is not necessarily 4).\n\u2022 Linearity: For a, b \u2208 R and expressions X, Y, G's estimate of the expression aX +bY is linear in its estimates of X and Y \u2013 that is, for all I, we have\n$$G(aX + bY | \u03a0) = aG(X | \u041f) + bG(Y | \u041f).$$ \n\u2022 Respect for proofs: Given a proof that Y > 0, the proof may be turned into a heuristic argument \u03c0 such that G(Y | I) \u2265 0 for all \u03a0\u044d\u03c0.\nAdditionally, the authors suggest some informal properties \u2013 such as presumption of independence and independence of irrelevant arguments \u2013 though no formal statements are provided.\nThe basis of this work is the following informal principle: a heuristic estimator should not be able to predict its own errors. We call this the principle of unpredictable errors. The main technical content of this work concerns the formalization of this principle."}, {"title": "The principle of unpredictable errors is motivated by our earlier analogy of heuristic esti-mates to subjective Bayesian expectations.", "content": "A Bayesian reasoner cannot predict the direction in which they will update their estimate in light of new information: if they could, then they would make that update before receiving the information. More formally, a Bayesian reasoner's subjective estimate of their own future subjective estimate of a given quantity should be equal to their current estimate of the quantity. This is known as the martingale property.\nWe will discuss two approaches to formalizing the principle of unpredictable errors. The first approach (introduced in Section 2) \u2013 which we call the subjective approach \u2013 involves two formal properties of G: iterated estimation and error orthogonality. Both properties are inspired by laws that govern conditional expected values. The iterated estimation property states that for all expressions Y and for all sets of arguments II and \u03a0' \u2286 \u03a0, we have\n$$G(G(Y | \u03a0) | \u03a0') = G(Y | \u03a0').$$\nIn other words, if G is given a small set of arguments \u03a0', its guess about its estimate of Y if it were to consider a larger set of arguments II should be its current estimate of Y. This law is inspired by the law of iterated expectations in probability theory.\nThe error orthogonality property is a more sophisticated version of the iterated estimation property (though not a strict generalization). Informally, error orthogonality states that the error of G's estimate of Y given I should not be predictable from G's estimate of any other quantity (given I or a subset thereof). Formally, error orthogonality states that for all expressions X, Y and for all sets of arguments I and I1, I2 \u2286 I, we have\n$$G((Y \u2013 G(Y | \u03a0)) \u00b7 G(X | \u03a0\u2081) | \u041f2) = 0.$$\nIn other words, the outer G is tasked with estimating the subjective covariance between two quantities: the error in G's estimate of Y given a set of arguments II, and G's estimate of X given a smaller set of arguments \u03a0\u2081. The error orthogonality property states that this subjective covariance must be zero.\nIf the error orthogonality property were to not hold, it would mean that adding some constant multiple of G(X | \u041f\u2081) to G(\u03a5 | \u03a0) would produce an improved estimate of Y. In other words: knowing G(X | \u041f\u2081) makes the error of G(\u03a5 | \u03a0) predictable.\nOur discussion of iterated estimation and error orthogonality will lead naturally to another approach to formalizing the principle of unpredictable errors, which we call the objective approach (Section 3). We define a new class of properties of G, which we call accuracy properties. Concretely, iterated estimation closely resembles 1-accuracy, while error orthogonality closely resembles self-accuracy. The key difference is that while iterated estimation and error"}, {"title": "1.4. Outline of this work", "content": "In Section 2, we motivate and discuss the subjective approach to formalizing the principle of unpredictable errors. We also discuss challenges with the approach. Concretely, the iterated estimation and error orthogonality properties are meant to help guide a search for a \u201creasonable\u201d heuristic estimator; however, the properties involve G's estimate of its own output, which poses two challenges. First, G might be able to satisfy the iterated estimation property simply by treating input expressions of the form G(Y | II) as a special case, effectively satisfying the iterated estimation property \u201cby fiat.\" Second, reasoning about G's estimate of its own output may be unwieldy, because G is likely to be a nontrivial algorithm.\nIn light of these challenges, in Section 3 we take inspiration from the subjective approach, and introduce the objective approach, which loosely speaking - requires that G have unpredictable errors over a distribution of possible inputs. Concretely, given a distribution Dover mathematical expressions Y and a predictor X, we define an estimator f(Y) to be X-accurate with respect to Y if E\u266d [(Y \u2212 f(Y))X] = 0: in other words, the error of f's estimate of Y is uncorrelated with X over D. We use this notion of accuracy to define accuracy properties of heuristic estimators over a distribution.\nWe explore accuracy in the context of two case studies. First, in Section 4, we consider the problem of estimating the expected product of jointly normal random variables. We ask whether it is possible to efficiently compute an estimator of the expected product that is X-accurate for a small and simple collection of predictors X. We present a formal computational barrier: we show that computing an accurate linear estimator is #P-hard. We further conjecture that computing any accurate estimator is computationally intractable. We then define and discuss approximately accurate estimation. We find that approximately accurate estimates are efficiently computable under certain assumptions about the predictors X, but leave the question open in the general case."}, {"title": "Our second case study (Section 5) is estimating the permanent of a matrix.", "content": "We consider three predictors for the permanent, and show that an accurate estimator with respect to the three predictors can be obtained via linear regression. However, this estimator fails certain sanity checks (it sometimes outputs negative estimates for the permanent of positive matrices). We find that, although there is a natural estimator in terms of the three predictors that does pass this sanity check, it is not accurate with respect to the predictors. Together, the results of our two case studies lead us to conclude that the objective approach to formalizing the principle of unpredictable errors is unlikely to succeed.\nIn Section 6, we put our discussion of accuracy in context by revisiting our desiderata for G. Our rejection of the objective approach leads us to pose the following question: is there a formalization of the principle of unpredictable errors that accords with our intuitive understanding of how heuristic estimators ought to behave?\nFinally, in Section 7, we discuss a potential application of heuristic estimation to understanding the behavior of neural networks. We also discuss the main challenges ahead for constructing a reasonable heuristic estimator and briefly describe a new perspective on heuristic explanation. In the context of neural networks, this perspective views heuristic arguments as distributional models of a neural network's internal activations."}, {"title": "1.5.\nRelated work", "content": "This work builds on Christiano et al. (2022), which asked whether it is possible to formalize the heuristic arguments and heuristic estimation. The authors posited that the foundational principle for heuristic estimation is a presumption of independence: two expressions should be presumed independent, unless there is an argument to the contrary. Our work is also closely related to Neyman (2024, Chapter 9), which explores desiderata for heuristic arguments (particularly linearity and respect for proofs) in the context of boolean circuits. By contrast, our work focuses on exploring and developing a new desideratum that aims to capture the notion that a heuristic estimator ought not to be able to predict its own errors.\nBesides this line of work, little prior work attempts to formalize heuristic estimation. Perhaps the closest is Tao (2012), which also explores the presumption of independence as the basis for heuristic argumentation in mathematics. The author uses the presumption of independence to heuristically justify the ABC conjecture from number theory.\nOur work also bears resemblances to Barak (2015, 2016). The author explores whether there is an estimation algorithm whose estimate for a given quantity appears reasonable to a broad class of formal \u201cobservers.\u201d These observers are analogous to heuristic arguments in our setting. However, while Barak explores estimation that appears reasonable to a large but pre-defined class of observers, we are interested in an estimation algorithm that appears reasonable to a particular set of observers (heuristic arguments) that are given as input. This narrower goal may allow the estimation algorithm to satisfy a much broader range of possible observers.\nGarrabrant et al. (2016) explore the assignment of probabilities to logical statements (such as the probability that the trillionth digit of \u03c0is 7). The authors construct a logical inductor, an algorithm that keeps track of probabilities for every logical statement. Each day, the logical inductor is presented with a proof of a logical fact, and the inductor updates its probabilities in light of the proof. The inductor's strategy resembles online learning: it"}, {"title": "2. The subjective properties: Iterated estimation anderror orthogonality", "content": null}, {"title": "2.1. Motivation and definitions", "content": "We begin by defining and motivating the iterated estimation property.\nDefinition 2.1. A heuristic estimator G satisfies the iterated estimation property if for all expressions Y and for all sets of arguments II and \u03a0' \u2286 II, we have\n$$G(G(Y | \u03a0) | \u03a0') = G(Y | \u03a0').$$\nExample 2.2. Let Y, \u03c0101, \u03c0102 be as in our running example from Section 1.1. Let I = {\u03c0101, 102} and \u03a0' = {\u03c0101}. We have G(Y | \u03c0101) = 90.5 (see equation 1). The iterated estimation property states that G(G(Y | \u03c0101, \u03c0102) | \u03c0101) is also equal to 90.5. In other words, after G has learned \u03c0101 (but not yet \u03c0102), its estimate of what its belief of Y will be after learning 7102 is its current estimate of Y, namely 90.5.\nThe name \"iterated estimation\u201d comes from the law of iterated expectations. In its simplest form, this law states that for random variables X, Y, we have E [E [X | Y]] = E [X]. More generally and more formally, the law states that on a probability space (\u03a9, F, P) with \u03c3-algebras \u0397' \u2286 H \u2286 F, for any integrable random variable Y we have\n$$E [E [Y | H] | H'] = E [Y | H'],$$\nwhere = denotes equality of random variables on all of \u03a9.\nIntuitively, H is finer than H', so E [Y | H'] is the expectation of Y given some partial information, and E [Y | H] is the expectation of Y given more information. Thus, the law of"}, {"title": "Proposition 2.3 (Projection law of conditional expectations, see e.g. Moshayedi (2022, Chapter 3)).", "content": "Let (\u03a9, F, P) be a probability space with \u03c3-sub-algebras H' \u2286 H \u2286 F. Let X,Y be random variables satisfying E [X2], E [Y\u00b2] < \u221e. Then\n$$E [(Y \u2013 E [Y | H]) \u00b7 E [X | H] | H'] = 0.$$\nNote that equation 5 simplifies to equation 4 in the case where X = 1. Moreover, the intuition for equation 5 is similar to the intuition for equation 4. Equation 5 states that if all you know is the partial information given by H', then Y \u2013 E [Y | H] and E [X | H] are uncorrelated over your uncertainty about the state of the world \u03c9 \u2208 \u03a9. This is true because Y \u2013 E [Y | H] is your error in estimating Y after learning the information given by H, while E [X | H] is a random variable that only depends on the information given by H. Thus, learning the value of E [X | H] cannot cause you to update your estimate of Y \u2013 E [Y | H] away from 0. Again, the core intuition is that it should be impossible to predict your error (i.e. Y \u2013 E [Y | H]) based on information that you already know (i.e. E [X | H]).\nJust as the iterated estimation property is inspired by the law of iterated expectations, the error orthogonality property is inspired by the projection law.\nDefinition 2.4. A heuristic estimator G satisfies the error orthogonality property if for all expressions X, Y and for all sets of arguments I and \u041f1, \u041f2 \u2286 \u03a0, we have\n$$G((Y \u2013 G(Y | \u03a0)) \u00b7 G(X | \u03a0\u2081) | \u041f2) = 0.$$\nExample 2.5. Let Y, \u03c0101 be as in our running example from Section 1.1. Let X be the expression d6(\u221a101) (whose value is 5). We consider a few situations:"}, {"title": "2.2. Challenges with the subjective approach", "content": "Although there are reasons to find the iterated estimation and error orthogonality properties compelling, there are challenges with using these properties as stated to seek a reasonable"}, {"title": "3. Accuracy as an objective measure of error unpre-dictability", "content": null}, {"title": "3.1. Motivation and definitions", "content": "As we have discussed, the iterated estimation and error orthogonality properties state that G's errors should be subjectively unpredictable, i.e. unpredictable to G itself. In this section, we discuss as an alternative property that we call accuracy with respect to a predictor X, or multiaccuracy with respect to a set of predictors S. (Multi)accuracy states that G\u2019s"}, {"title": "Definition 3.1.", "content": "Let Y be a space of real-valued mathematical expressions, and let D be a probability distribution over y such that Ey\u223cD [Y\u00b2] < \u221e. Let X : Y \u2192 R be a random variable such that Ey\u223cD [X2] < \u221e. An estimator f : Y \u2192 R is X-accurate over D if\n$$EY~D [(Y \u2212 f(Y))X] = 0.$$\nWe say that f is self-accurate over D if f is f-accurate over D. For a set S of random variables, we say that f is S-multiaccurate over D if f is X-accurate over D for all X \u2208 S."}, {"title": "Proposition 3.4.", "content": "Let D, S, f be as in Definition 3.1. The following are equivalent:\n(1) f is S-multiaccurate over D.\n(2) For all X \u2208 S, among all estimators that differ from f by a constant multiple of X, f itself is optimal. That is,\n$$arg \\min_C EY~D [(Y \u2212 (f(Y) + cX))\u00b2] = 0.$$\n(3) For all n, for all X1,..., Xm \u2208 S, among all estimators that differ from f by a linear combination of the Xi's, f itself is optimal. That is,\n$$arg \\min_{(C_1,...,C_m)} EY~D \\bigg[ Y \u2212 f(Y) + \\sum_{i=1}^m C_i X_i \\bigg]^2 = 0.$$"}, {"title": "In statistics, the ordinary least squares (OLS) estimator of a random variable Y in terms of predictors X1,..., Xm is the linear combination \u03a3\u2081\u03b2\u2081X\u1d62 that minimizes the ex-pected squared distance from Y.", "content": "In particular, the OLS estimator of Y in terms of X1, . . ., Xm satisfies property (3) above (by definition), and is thus S-multiaccurate (where S = {X1,..., Xm}). While there can be many S-multiaccurate estimators of Y over D (see Example 3.2), the OLS estimator is the only S-multiaccurate estimator that is a linear combination of X1, ..., Xm. Furthermore, the OLS estimator is always self-accurate: since the OLS estimator cannot be improved by adding any linear combination of the Xi's, then certainly it cannot be improved by adding a multiple of itself (as the OLS estimator is itself a linear combination of the Xi's).\nThe notions introduced above give a potential path forward for defining properties of heuristic estimators similar to iterated estimation and error orthogonality, but without the need for nested G's. The basic idea is to substitute G(\u03a5 | \u03a0) for f(Y) in equation 6:\n$$EY~D [(Y \u2013 G(Y | \u03a0))X] = 0.$$\nIn order to make sense of this expression, we need to somewhat reconceptualize the notion of a heuristic argument. In Section 2, we regarded the heuristic argument \u03c0 in the expression G(Y | \u03c0) as a particular computation that helps to estimate Y (perhaps a partially computes Y). Now that we are taking an expectation over different expressions Y, we must allow \u03c0to depend on Y. Thus, in this section, we will be thinking of \u03c0as a computation that depends"}, {"title": "Definition 3.5.", "content": "Let Y, D be as in Definition 3.1. Let G be a heuristic estimator and X : Y \u2192 R be a random variable. A set of heuristic arguments \u041fx makes G be X-accurate over D if for all I \u2283 Ix, G(Y | II) is an X-accurate estimator over D (in the sense of Definition 3.1) \u2013 that is,\n$$EY~D [(Y \u2013 G(Y | \u03a0))X] = 0.$$\nWe say that G is X-accurate over D if there exists a short \u041f\u03c7 that makes G be X-accurate over D. For a set S of random variables, we say that Gis S-multiaccurate over Dis Gis X-accurate over D for all X \u2208 S.\nIn the special case of X = 1, equation 9 is quite similar to equation 2 (iterated estimation), the key difference being that the outer G is replaced by an expectation over D.\nWe also define self-accuracy for G:\nDefinition 3.6. Let Y, D be as in Definition 3.1. A heuristic estimator G is self-accurate over D if for every set of heuristic arguments \u03a0, \u03a0 makes G be G(Y | \u03a0)-accurate - that is, if for all I'\u2283 I, we have\n$$EY~D [(Y \u2013 G(Y | \u03a0')) \u00b7 G(Y | \u03a0)] = 0.$$\nEquation 10 is quite similar to equation 3 (error orthogonality), again with the key difference being that the outer G is replaced by an expectation over D."}, {"title": "3.2. Multiaccuracy as a constraint on argument merges", "content": "It can be useful to think of multiaccuracy as a constraint on merging estimates based on different arguments. Concretely, given a set of predictors S = {X1,..., Xm}, suppose that G is S-multiaccurate over D; let Ix; be an argument that makes G be X\u1d62-accurate over D. Then G(Y | \u03a0x\u2081,..., \u03a0x\u2098) is an {X1, ..., Xm}-multiaccurate estimator over D: for each i, G's error is orthogonal to Xi.\nIs it possible for G to be S-multiaccurate for a rich set of predictors S, while still running in polynomial time in the size of the arguments provided? One reason for hope is that estimates may be merged using linear regression:"}, {"title": "Proposition 3.7.", "content": "Let Y, D be as in Definition 3.5. Let G be a heuristic estimator, and suppose that G merges arguments using OLS regression: that is, for all sets of arguments \u03a0 = {\u03c01,..., \u03c0m}, we have\n$$G(Y | \u03a0) = M^+ \\begin{pmatrix} ED [YG(Y | \u03c0_1)]\\\\ :\\\\ ED [YG(Y | \u03c0_m)], \\end{pmatrix} \\begin{pmatrix} G(Y | \u03c0_1)\\\\ :\\\\ G(Y|\u03c0_m) \\end{pmatrix},$$,\nwhere M+ is the pseudoinverse of the Gram matrix M := (Ep [G(Y | \u03c0\u2081)G(Y | \u03c0j)])ji=1.\nThen G is self-accurate over D.\nUnfortunately, Proposition 3.7 does not provide a straightforward, computationally efficient way to merge heuristic estimates. This is because computing the Gram matrix M can be #P-hard even in simple cases, as we will see in Section 4.1. Further, even approximating the covariances by sampling from D can require exponentially many samples.\nIn the next two sections, we will explore accurate estimation in two specific contexts: estimating the expected product of jointly normal random variables, and estimating the permanent of a matrix. Our goal will be to demonstrate some limits to producing estimates that satisfy the accuracy definitions that we presented in this section."}, {"title": "4. Estimating the product of jointly normal randomvariables", "content": "In this section, we consider the problem of estimating the expected product of n jointly normal random variables. Formally: given an n \u00d7 n covariance matrix \u2211, we would like to estimate\n$$\u0395_{(z_1,..., z_n)~N(0,\\Sigma)} \\prod_{i=1}^n z_i $$\\nThis problem is motivated by two considerations. On the one hand, it is one of the simplest estimation problems in which computing (or indeed approximating) the correct answer is"}, {"title": "Theorem 4.2", "content": "(Isserlis, 1918). Let Z1, ..., Zn be jointly normal, zero-mean random variables. Then\n$$E[Z_1 ... Z_n] = \\sum_{p \\in P_2(n)} \\prod_{(i,j) \\in p} Cov(Z_i, Z_j). $$\\nNote that if n is odd then P2(n) is empty, so E [Z1 ... Zn] 0. For the remainder of this section, we will assume that n is positive and even.\nRemark 4.3. For even n, the hafnian haf(A) of a symmetric n \u00d7 n matrix A is defined as\n$$haf(A) := \\sum_{p \\in P_2(n)} \\prod_{(i,j) \\in p} A_{i,j}. $$\\nTheorem 4.2 can be restated as follows: for Z1,..., Zn ~ N(0, \u03a3), we have E [Z1... Zn] = haf(\u03a3). The hafnian generalizes the permanent (we discuss permanents more in Section 5)."}, {"title": "Definition 4.4.", "content": "Let \u2211 be an n \u00d7 n PSD matrix and let K \u2286 [n]. For a pairing p of K, we define Xp := \u03a0{ij}ep Si,j. For a set of pairings S C P2(K), we define Xs := \u03a3\u03c1\u03b5\u03c2 \u03a7\u03c1.\nWe being with the following observation.\nClaim 4.5. For all S \u2286 P2(n), Xs is a 1-accurate and self-accurate estimator of haf(\u03a3) over D.\nProof. To see that Xs is 1-accurate, we must show that Ep [XS] = 0. This is indeed the case, as Xs is a sum of Xp for p \u2208 S, and each Xp is zero-mean (because Xp is a product of independent, zero-mean random variables).\nTo see that Xs is self-accurate, note that ED [XpXq] = 0 for all p \u2260 q \u2208 P2(n). This is because XpXq is a product of n not-necessarily-distinct elements of \u2211, at least one of which appears only once in the product. This element is independent of all other elements in the product, and is zero-mean. It follows that Xs is self-accurate, because we have\n$$E_D [(haf(\\Sigma) - X_S)X_S] = E_D [(\\sum_{p \\in P_2(n)} X_p - \\sum_{p \\in S} X_p) \\sum_{p \\in S} X_p] = \\sum_{p \\in P_2(n)} X_p \\sum_{p \\in S} X_p = 0.$$\nWe also note the following property of estimators Xs, which will be useful for proving some of our results.\nClaim 4.6. For all sets of pairings S1, S2 \u2286 P2(n), we have Edn [XS1XS2] = |S\u2081 \u2229 S2|.\nProof. We have\n$$E_D [X_{S_1}X_{S_2}] = \\sum_{p \\in S_1} \\sum_{q \\in S_2} E_D [X_p X_q] = \\sum_{p \\in S_1 \\bigcap S_2} E_D [X_p^2] = |S_1 \\bigcap S_2|.$$"}, {"title": "Definition 4.8.", "content": "A pairing structure (denoted as T) over an index set K is a structured representation of a subset of P2(K) (we will use S(T) to refer to the set of pairings that T represents). It is defined as any one of the following:\n\u2022 If K = {i", "S(T)": "nP2(K) = {{(i"}, {"S(T)": "S(T\u2081) US(T2).\n\u2022 If T\u2081 and T2 are pairing structures over disjoint index sets K\u2081 and K2"}, {"S(T)": {"p1Up2": "P1 \u2208 S(T1)", "recursively": "a base pairing structure has representation size 1; rsize(T\u2081 \u25ca T\u2082) = rsize(T\u2081) + rsize(T2) + 1; and rsize(T\u2081 U T\u2082) = rsize(T\u2081) + rsize(T2) + 1.\nFor convenience", "as\nT{1,2,3,4}": "T{1,2} \u2297 T{3,4}) \u222a (T{1,3} T{2,4}) \u222a (T{1,4} T{2,3}).\nThat is, S(T) = P"}}]}