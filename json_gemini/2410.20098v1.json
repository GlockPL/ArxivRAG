{"title": "SELF-NORMALIZED RESETS FOR PLASTICITY IN CONTINUAL LEARNING", "authors": ["Vivek F. Farias", "Adam D. Jozefiak"], "abstract": "Plasticity Loss is an increasingly important phenomenon that refers to the empirical observation that as a neural network is continually trained on a sequence of changing tasks, its ability to adapt to a new task diminishes over time. We introduce Self-Normalized Resets (SNR), a simple adaptive algorithm that mitigates plasticity loss by resetting a neuron's weights when evidence suggests its firing rate has effectively dropped to zero. Across a battery of continual learning problems and network architectures, we demonstrate that SNR consistently attains superior performance compared to its competitor algorithms. We also demonstrate that SNR is robust to its sole hyperparameter, its rejection percentile threshold, while competitor algorithms show significant sensitivity. SNR's threshold-based reset mechanism is motivated by a simple hypothesis test that we derive. Seen through the lens of this hypothesis test, competing reset proposals yield suboptimal error rates in correctly detecting inactive neurons, potentially explaining our experimental observations. We also conduct a theoretical investigation of the optimization landscape for the problem of learning a single ReLU. We show that even when initialized adversarially, an idealized version of SNR learns the target ReLU, while regularization based approaches can fail to learn.", "sections": [{"title": "INTRODUCTION", "content": "Plasticity Loss is an increasingly important phenomenon studied broadly under the rubric of continual learning (Dohare et al., 2024). This phenomenon refers to the empirical observation that as a network is continually trained on a sequence of changing tasks, its ability to adapt to a new task diminishes over time. While this is distinct from the problem of catastrophic forgetting (also studied under the rubric of continual learning (Goodfellow et al., 2013; Kirkpatrick et al., 2017)), it is of significant practical importance. In the context of pre-training language models, an approach that continually trains models with newly collected data is preferable to training from scratch (Ibrahim et al., 2024; Wu et al., 2024). On the other hand, the plasticity loss phenomenon demonstrates that such an approach will likely lead to models that are increasingly unable to adapt to new data. Similarly, in the context of reinforcement learning using algorithms like TD, where the learning tasks are inherently non-stationary, the plasticity loss phenomenon results in actor or critic networks that are increasingly unable to adapt to new data (Lyle et al., 2022). Figure 1 illustrates plasticity loss in the 'permuted MNIST' problem introduced by Goodfellow et al. (2013).\nOne formal definition of plasticity measures the ability of a network initialized at a specific set of parameters to fit a random target function using some pre-specified optimization procedure. In this sense, random parameter initializations (eg. Lyle et al. (2024)) are known to enjoy high plasticity. This has motivated two related classes of algorithms that attempt to mitigate plasticity loss. The first explicitly 'resets' neurons that are deemed to have low 'utility' (Dohare et al., 2023; Sokar et al., 2023). A reset re-initializes the neurons input weights and bias according to some suitable random initialization rule, and sets the output weights to zero; algorithms vary in how the utility of a neuron is defined and estimated from online data. A second class of algorithms perform this reset procedure"}, {"title": null, "content": "implicitly via regularization (Ash & Adams, 2020; Kumar et al., 2023). These latter algorithms differ in their choice of what to regularize towards, with choices including the original network initialization; a new randomly drawn initialization; or even zero. The aforementioned approaches to mitigating plasticity loss attempt to adjust the training process; other research has studied the role of architectural and optimizer hyper-parameter choices. Across all of the approaches to mitigating plasticity loss described above, no single approach is yet to emerge as both robust to hyper-parameter choices, and simultaneously performant across benchmark problems.\nGiven some point process consider the task of distinguishing between the hypotheses that this point process has a positive rate (the null hypothesis), or a rate that is identically zero with a penalty for late rejection or acceptance. An optimal test here takes the following simple form: we reject the null hypothesis as soon as the time elapsed without an event exceeds some percentile of the inter-arrival time under the null hypothesis and otherwise accept immediately upon an event. Viewing the firing of a neuron as such a point process, we propose to reset a neuron based on a rejection of the hypothesis that the the neuron is firing at a positive rate. We use the histogram of past inter-firing times as a proxy of the inter-arrival time distribution under the null hypothesis. This exceedingly simple algorithm is specified by a single hyper-parameter: the rejection percentile threshold. We refer to this procedure as self-normalized resets (SNR) and argue this is a promising approach to mitigating plasticity loss:\n1. We demonstrate superior performance on four benchmark problems classes studied in (Dohare et al., 2023; Kumar et al., 2023). Interestingly, there is no single closest competitor to SNR across these problems. Many competing approaches also show significant sensitivity to the choice of hyper-parameters; SNR does not. We introduce a new problem to elucidate similar plasticity loss phenomena in the context of language models, and show similar relative merits for SNR.\n2. We conduct a theoretical investigation of the optimization landscape for the problem of learning a single ReLU. We show that while (an idealized version of) SNR learns the target ReLU, regularization based approaches can fail to learn in this simple setting."}, {"title": "RELATED LITERATURE", "content": "The phenomenon of plasticity loss was discovered in the context of transfer learning (Ash & Adams, 2020; Zilly et al., 2021; Achille et al., 2017). Achille et al. (2017) showed that pre-training a network on blurred CIFAR images reduces its ability to learn on the original images. In a similar vein, Ash & Adams (2020) showed that pre-training a network on 50% of a training set followed by training on the complete training set reduces accuracy relative to a network that forgoes the pre-training step. More recent literature has focused on problems that induce plasticity loss while training on a sequence of hundreds of changing tasks, such as Permuted MNIST and Continual ImageNet in Dohare et al. (2021), capturing the necessity to learn indefinitely.\nCorrelates of Plasticity Loss. The persistence of plasticity loss across a swathe of benchmark problems has elucidated a search for its cause. Several correlates of plasticity loss have been well observed, namely neuron inactivity, feature or weight rank collapse, increasing weight norms, and loss of curvature in the loss surface (Dohare et al., 2021; Lyle et al., 2023; Sokar et al., 2023; Lewandowski et al., 2023; Kumar et al., 2020). The exact cause of plasticity loss remains unclear and Lyle et al. (2023) have shown that for any correlate an experiment can be constructed in which its correlation with plasticity loss is negative. Nonetheless, these correlates have inspired a series of algorithms and interventions with varying degrees of success in alleviating the problem. However, none is consistently performant across architectures and benchmark problems.\nReset Methods. Algorithms that periodically reset inactive or low-utility neurons have emerged as a promising approach (Dohare et al., 2023; Sokar et al., 2023; Nikishin et al., 2022). Continual Backprop (CBP) (Dohare et al., 2023) is one such method which tracks a utility for each neuron, and according to some reset frequency r, it resets the neuron with minimum utility in each layer. CBP's utility is a discounted average product of a neuron's associated weights and activation, a heuristic inspired by the literature on network pruning. Another algorithm is ReDO (Sokar et al., 2023), where on every 1/rth mini-batch, ReDO computes the average activity of each neuron and resets those neurons whose average activities are small relative to other neurons in the corresponding layer, according to a threshold hyperparameter. Two defining characteristics of CBP and ReDO are"}, {"title": "ALGORITHM", "content": "To make ideas precise, consider a sequence of training examples $(X_t, Y_t) \\in \\mathcal{X} \\times \\mathcal{Y}$, drawn from some distribution $\\mu_t$. Denote the network by $f: \\mathcal{X} \\times \\Theta \\to \\mathcal{Y}$, and let $l: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}$ be our loss function. Denote by $H_t \\in \\mathcal{H}_t$, the history of network weights and training examples up to time $t$, and assume access to an optimization oracle $O_t : H_{t-1} \\to \\Theta$ that maps the history of weights and training examples to a new set of network weights. As a concrete example, $O_t$ might correspond to stochastic gradient descent.\nLet $\\theta_*$ minimize $E_{\\mu_1}[l(f(X_1; \\theta), Y_1)]$, denote $\\Theta_t = O_t(H_{t-1})$, and consider average expected regret\n$\\sum_t E_{\\mu_t} [l(f(X_t; \\Theta_t), Y_t)] - E_{\\mu_t} [l(f(X_t; \\theta_*), Y_t)]$\nPlasticity loss describes the phenomenon where, for certain continual learning processes $O_t$, such as those corresponding to SGD or Adam, average expected regret increases over time, even for benign choices of $\\mu_t$.\nExample 2.1 (The Permuted MNIST problem). Consider a sequence of 'tasks' presented sequentially to SGD, wherein each task consists of 10000 images from the MNIST dataset with the pixels permuted. SGD trains over a single epoch on each task before the subsequent task is presented. The figure also shows a potential correlate of this phenomenon: the number of 'dead' or inactive neurons in the network increases as training proceeds, diminishing the network's effective capacity.\nOne hypothesis that seeks to explain plasticity loss is that the network weights obtained from minimizing loss over some task yield poor initializations for a subsequent task, leading to the inactive neurons we observe in the above experiment. On the other hand random weight initializations are known to work well (Glorot & Bengio, 2010), suggesting a natural class of heuristics: re-initialize inactive neurons. Of course, the crux of any such algorithm is determining whether a neuron is inactive in the first place, and doing so as quickly as possible.\nTo motivate our algorithm, SNR, consider applying the network $f(\\cdot; \\theta)$ to a hypothetical sequence of training examples drawn i.i.d. from $\\mu_t$. Let $\\mathcal{Z}^i_{h \\in t}$ indicate the sequence of activations of neuron $i$, and let let $\\Delta t^{th}$ denote the random time between two consecutive activations over this hypothetical sequence of examples. Now turning to the actual sequence of training examples, let $a^i_t$ count the"}, {"title": null, "content": "such as a sequence corresponding to multiple epochs on a random sub-sample of a dataset followed by multiple epochs on a second random sub-sample; or Example 2.1"}, {"title": "MOTIVATING SNR AND COMPARISON TO OTHER RESET SCHEMES", "content": "Here we motivate the SNR heuristic and compare it to other proposed reset schemes. Consider the following simple hypothesis test: we observe a discrete time process $Z_s \\in \\{0,1\\}$ which under the null hypothesis $H_0$ is a Bernoulli process with mean $p>0$. The alternative hypothesis $H_1$ is that the mean of the process is identically zero. A hypothesis test must, at some stopping time $T$, either reject ($X_T = 1$) or accept ($X_T=0$) the null; an optimal such test would choose to minimize the sum of type-1 and type-2 errors and a penalty for delays:\n$P(X_T = 1|H_0) + P(X_T = 0|H_1) + \\lambda (E[\\tau|H_0] + E[\\tau|H_1])$"}, {"title": null, "content": "Here $\\lambda > 0$ penalizes the delay in a decision. If $\\lambda < p/2$, the optimal test takes a simple form: for some suitable threshold $T$, reject the null hypothesis iff $Z_s=0$ for all times $s$ up to $T$:\nProposition 2.1. Let $T$ be the $1 - \\frac{\\lambda}{(p - \\lambda)}$ percentile of a $Geometric(p)$ distribution. Then the optimal hypothesis test takes the form $X_T = 1\\{Z_T = 0\\}$ where $\\tau = min(s: Z_s = 1) \\wedge T$"}, {"title": null, "content": "Notice that if $\\lambda \\ll p$, the percentile threshold above is independent of $p$. Applying this setup to the setting where under the null, we observe the firing of neuron $i$ under i.i.d. training examples from $\\mu_t$, imagine that $p= P(Z=1)$. Further, we assume $\\lambda= ap$; a reasonable assumption which models a larger penalty for late detection of neurons that are highly active. It is then optimal to declare neuron i 'inactive' if the length of time it has not fired exceed the $1 - \\frac{\\alpha}{(1 - \\alpha)^{-1}}$ percentile of the distribution of $\\Delta t^{th}$. This is the underlying motivation for the SNR heuristic.\nComparison with Reset Schemes: Neuron reset heuristics such as Sokar et al. (2023) define (sometimes complex) notions of neuron 'utility' to determine whether or not to re-initialize a neuron. The utility of every neuron is computed over every consecutive (say) r minibatches, and neurons with utility below a threshold are reset. To facilitate a comparison, consider the setting where neurons that do not fire at all over the course of the r mini batches are estimated to have zero utility, and that only neurons with zero utility are re-initialized.\nThis reveals an interesting comparison with SNR. The schemes above will re-initialize a neuron after inactivity over a period of time that is uniform across all neurons. In the context of the hypothesis testing setup above, this will result in sub-optimal error rates across neurons. On the other hand, SNR will reset a neuron after it is inactive for a period that is effectively normalized to the nominal firing rate of that neuron, while still only specifying a single hyperparameter for the network."}, {"title": "EXPERIMENTS", "content": "We evaluate the efficacy and robustness of SNR on a series of benchmark problems from the continual learning literature, measuring regret with respect to prediction accuracy $l(y, y') = 1\\{y \\neq y'\\}$. As an overview, we will seek to make the following points:\nInactive neurons are an important correlate of plasticity loss: This is true across several archi- tectures: vanilla MLPs, CNNs and transformers.\nLower average loss: Across a broad set of problems/ architectures from the literature, SNR consis- tently achieves lower average loss than competing algorithms.\nNo consistent second-best competitor: Among competing algorithms, none emerge as consistently second best to SNR.\nRobustness to hyper-parameters: The performance of SNR is robust to the choice of its single hyper parameter (the rejection percentile threshold). This is less so for competing algorithms."}, {"title": "EXPERIMENTAL SETUP", "content": "Each problem consists of tasks $T_1, T_2, ..., T_N$, each of which contains training examples in $\\mathcal{X} \\times \\mathcal{Y}$. A network is trained for a fixed number of epochs per task to minimize cross-entropy loss. We perform an initial hyperparameter sweep over 5 seeds to determine the optimal choice of hyperpa- rameters. For each algorithm and problem, we select the hyperparameters that attain the lowest average loss and repeat the experiment on 5 new random seeds. A random seed determines the network's parameter initialization, the generation of tasks, and any randomness in the algorithms evaluated. We evaluate both SGD and Adam as the base optimization algorithm, as earlier literature has argued that Adam can be less performant than SGD in some continual learning settings (Dohare et al., 2023; Ashley et al., 2021). We evaluate on the following problems:\nPermuted MNIST (PM) (Goodfellow et al., 2013; Dohare et al., 2021; Kumar et al., 2023): A subset of 10000 image-label pairs from the MNIST dataset are sampled for an experiment. A task consists of a random permutation applied to each of the 10000 images. The network is presented with 2400 tasks appearing in consecutive order. Each task consists of a single epoch and the network receives data in batches of size 16.\nRandom Label MNIST (RM) (Kumar et al., 2023; Lyle et al., 2023): A subset of 1200 images from the MNIST dataset are sampled for an experiment. An experiment consists of 100 tasks, where each tasks is a random assignment of labels, consisting of 10 classes, to the 1200 images. A network is trained for 400 epochs on each task with a batch size 16.\nRandom Label CIFAR (RC) (Kumar et al., 2023; Lyle et al., 2023): A subset of 128 images from the CIFAR-10 dataset are sampled for an experiment. An experiment consists of 50 tasks, where each tasks is a random assignment of labels, consisting of 10 classes, to the 128 images. An agent is trained for 400 epochs on each task with a batch size 16."}, {"title": null, "content": "Continual Imagenet (CI) (Dohare et al., 2023; Kumar et al., 2023): An experiment consists of all 1000 classes of images from the ImageNet-32 dataset (Chrabaszcz et al., 2017) containing 600 images from each class. Each task is a binary classification problem between two of the 500 classes, selected at random. The experiment consists of 500 tasks and each class occurs in exactly one task. Each task consists of 1200 images, 600 from each class, and the network is trained for 10 epochs with a batch size of 100.\nPermuted Shakespeare (PS): We propose this problem to facilitate studying the transformer ar- chitecture in analogy to the MNIST experiments. An experiment consists of 32768 tokens of text from Shakespeare's Tempest. For any task, we take a random permutation of the vocabulary of the Tempest and apply it to the text. The network is presented with 500 tasks. Each task consists of 100 epochs and the network receives data in batches of size 8 with a context widow of width 128. We evaluate over 9 seeds.\nThis experimental setup, for all but Permuted Shakespeare, follows that of (Kumar et al., 2023), with the exceptions of Permuted MNIST which has its task count increased from 500 to 2400, Random Label MNIST which has its task count increased from 50 to 100, and Random Label CIFAR which has its dataset reduced from 1200 to 128 images. Lyle et al. (2023) consider variants of the Random Label MNIST and CIFAR problems by framing them as MDP environments for DQN agents. During training, the DQN agents are periodically paused to assess their plasticity by training them on separate, randomly generated regression tasks using the same image datasets."}, {"title": "ALGORITHMS AND ARCHITETCURES", "content": "Our baseline in all problems consist simply of using SGD or Adam as the optimizer with no further intervention. We then consider several interventions to mitigate plasticity loss. First, we consider algorithms that employ an explicit reset of neurons: these include SNR, Continual Backprop (CBP) (Dohare et al., 2021), and ReDO (Sokar et al., 2023). Among algorithms that attempt to use regular- ization, we consider vanilla L2 regularization, L2 Init (Kumar et al., 2023), and Shrink and Perturb (Ash & Adams, 2020). Finally, as a potential architectural modification we consider the use of Layer Normalization (Ba et al., 2016).\nWe utilize the following network architectures:\nMLP: For Permuted MNIST and Random Label MNIST we use an MLP identical to that in Kumar et al. (2023) which in turn is a slight modification to that in Dohare et al. (2023).\nCNN: For Random Label CIFAR and Continual ImageNet we use a CNN architectures identical to that in Kumar et al. (2023) which in turn is a slight modification to that in Dohare et al. (2023).\nTransformer: We use a decoder model with a single layer consisting of 2 heads, dimension 16 for each head, and with 256 neurons in the feed forward layer with ReLU activations. We deploy this architecture on the Permuted Shakespeare problem using the GPT-2 BPE tokenizer (limited to the set of unique tokens present in the sampled text)."}, {"title": "RESULTS AND DISCUSSION", "content": "We separately discuss the results for the first four problems (PM, RM, RC, CI) followed by Permuted Shakespeare; we observe additional phenomena in the latter experiment which merit separate discussion."}, {"title": null, "content": "We see that resets are by themselves insufficient in mitigating plasticity loss, providing at most a marginal improvement over no intervention. This is unsurprising since neurons are only present in the feedforward layers, unlike the MLP and CNN architectures in the earlier experiments. As such, regularization appears necessary and we see that, over the last 50 tasks, L2 regularization attains an average loss of 0.3101 in contrast to 3.0147 and 3.0242 for no intervention and SNR. This improvement in performance coincides with stable weight norms and non-vanishing average entropy of self-attention probabilities for L2 regularization. In contrast to the earlier problems, L2 Init fares worse than L2 regularization and experiences substantial loss of plasticity, although to a lesser extent than the no intervention network.\nWhile L2 regularization addresses weight blowup, neuron death remains present; see Figure 3. The average loss with L2 increases from 0.1560, over the first 50 tasks, to 0.3101, over the final 50 tasks."}, {"title": null, "content": "This prompts us to consider using SNR in addition to L2 regularization. This largely eliminates neuron death (see the right panel of Figure 3), while stabilizing weight norms and maintaining entropy of self-attention probabilities, providing the lowest loss (0.2551) over the final 50 tasks."}, {"title": "SCALED PERMUTED SHAKESPEARE", "content": "While the scale of our Permuted Shakespeare problem serves as a simple benchmark problem for evaluating a series of continual learning algorithms and hyperparameter choices for language mod- els, it is also of interest to investigate the effect of model and dataset scale on plasticity. To this end, we scale the number of non-embedding weights in our transformer network by a factor of N = 16, increasing the number of heads to 8 and number of neurons to 1024. In line with scaling laws (Kaplan et al., 2020), we increase the size of our dataset by a factor of $16^{0.74}$, specifically to 254'976 tokens per task. The rest of the problem setup remains unchanged; we train the network for 100 epochs on 500 tasks in sequence. To facilitate the larger token count, we train on a sample of 254'976 tokens worth of text from the complete set of plays by William Shakespeare.\nWe limit our experiment to 4 random seeds, scales $N \\in \\{1,16\\}$, evaluating only SNR+L2- regularization and L2-regularization with hyperparameters $\\eta = 0.05$ and $\\lambda = 10^{-4}$, presenting our results in Table 3. We first note that as model and dataset size grow, the gap in average loss between L2-regularization and SNR+L2-regularization grows substantially. Simultaneously, we see a dramatic increase in the proportion of inactive neurons with L2-regularization. At any time step, on average 0.06% of neurons are inactive at scale N = 1 while 32.9% are inactive at scale N = 16. These results suggest that resets can play a critical role in maintaining plasticity in large-scale lan- guage models."}, {"title": "THEORETICAL ANALYSIS OF LEARNING A SINGLE RELU", "content": "We analyze the average expected regret for learning a single target ReLU in order to gain intuition as to why SNR achieves lower average expected regret relative to the regularization based algorithms in our set of continual learning experiments in Section 3. At a high level, our analysis provides the following conclusions:"}, {"title": "PRELIMINARIES", "content": "We aim to learn a single ReLU activated neuron, parameterized by the family $\\Theta = \\mathbb{R}^2$.\n$f(x, v) = \\sigma(v^T x) = \\begin{cases}\nv^T x \\text{ if } v^T x \\geq 0 \\\\\n0  \\text{ if } v^T x < 0\n\\end{cases}$\nWe refer to $v=(\\tilde{v}, b_v)$ as the target parameters and $w = (\\tilde{w}, b_w)$ as our network's parameters.\nAssumption 4.1. We sample data $x, y \\in \\mathbb{R} \\times \\mathbb{R}^+$ according to the distribution $\\mu$ where $x \\sim Uniform(-L, L)$, for some positive constant $L$, and $y|x = f(x, v)$.\nWe analyze the average expected regret with respect to the squared error.\n$\\frac{1}{T} \\sum_{t=0}^{T-1} E_{\\mu} [\\frac{1}{2} (\\sigma(w^T x + b_w) - \\sigma(v^T x + b_v))^2]$"}, {"title": null, "content": "For notational brevity, we define\n$F(w) = E_{\\mu}[\\frac{1}{2}(\\sigma(\\tilde{w} x + b_w) - \\sigma(\\tilde{v} x + b_v))^2]$\nFor convenience of analysis, we suppose that the optimization oracle $O_t$ performs gradient descent, rather than SGD, and has access to the oracle\n$\\nabla F(w) = E_{\\mu}[(\\sigma(\\tilde{w} x + b_w) - \\sigma(\\tilde{v} x + b_v))1\\{\\tilde{w} x + b_w \\geq 0\\}(x, 1)]$\nTherefore, for a learning rate $\\eta$, without any regularization or resets, $\\{w_t\\}_{t=0}^{T-1}$ satisfies\n$w_{t+1} = w_t - \\eta \\nabla F(w_t)$"}, {"title": "GRADIENT DESCENT WITH L2 INIT AND L2 REGULARIZATION", "content": "Given a regularization strength $\\lambda > 0$, we consider the optimization oracle $O_{t}^{L2 Init}$ that performs gradient descent on the on the L2 Init regularized objective\n$F(w) = E_{\\mu}[\\frac{1}{2} (\\sigma(\\tilde{w} x + b_w) - \\sigma(\\tilde{v} x + b_v))^2] + \\frac{\\lambda}{2} ||w - w_0||^2 = F(w) + \\frac{\\lambda}{2} ||w - w_0||^2$\nThen the gradient of F is simply\n$\\nabla F(w) = E_x[(\\sigma(\\tilde{w}^T x) - \\sigma(\\tilde{v}^T x))\u2161(\\tilde{w}^T x > 0)x] + \\lambda(w - w_0) = \\nabla F(w) + \\lambda(w - w_0)$\nThen the L2 Init gradient descent update is simply\n$w_{t+1} = w_t - \\eta \\nabla F(w_t) = w_t - \\eta \\nabla F(w_t) - \\eta \\lambda (w_t - w_0)$\nSimilarly, we can consider vanilla L2 regularization whose update is simply\n$w_{t+1} = w_t - \\eta \\nabla F(w_t) - \\eta \\lambda w_t$"}, {"title": null, "content": "Note, if $\\lambda = 0$ then we simply retain the update of unregularized gradient descent (10). Then for any sufficiently small learning rate $\\eta$ we attain non-vanishing average regret.\nTheorem B.1. Suppose that x is sampled according to x ~ Uniform(\u2212L, L) \u00d7 {1} \u2286 $\\mathbb{R}^2$ and that the target parameters v = (v, b) satisfy v > 0 and $b_v \\leq 0$. Then applying gradient descent with L2 Init (11) or L2 regularization (12), with regularization strength $\\lambda > 0$ and learning rate $\\eta > 0$ such that\n$\\eta \\leq \\frac{1}{L^2 + 1}$\n$\\eta \\lambda < 1$\nand with $w_0$ sampled uniformly from $([-1, -\\tilde{w}_{min}) \\cup (\\tilde{w}_{min}, 1]) \\times \\{0\\}$, for any $\\tilde{w}_{min} > 0$, then with probability $\\frac{1}{2}$ over random initializations of $w_0$, the average regret is non-vanishing\n$\\frac{1}{T} E [R_T] \\geq F(0)$"}, {"title": "GRADIENT DESCENT WITH RESETS", "content": "For any reset threshold $\\epsilon > 0$ we define a reset oracle $O_{\\epsilon}$ such that for any $w \\in \\mathbb{R}^2$\n$O_{\\epsilon}(w) = \\begin{cases}\nTrue  \\text{ if } \\sup_{x \\in [-L, L] \\times \\{1\\}} w^T x \\leq \\epsilon \\\\\nFalse  \\text{ if } \\sup_{x \\in [-L, L] \\times \\{1\\}} w^T x > \\epsilon\n\\end{cases}$\nWe consider the following gradient descent updates with resets\n$\\tilde{w}_{t+1} = w_t - \\eta \\nabla F(w_t)$"}, {"title": null, "content": "$w_{t+1} = \\begin{cases}\n\\tilde{w}_{t+1}  \\text{ if } O_{\\epsilon}(w_{t+1}) = False \\\\\nsample \\text{from } Uniform ([-1, -\\tilde{w}_{min}) \\cup (\\tilde{w}_{min}, 1]) \\times \\{0\\}  \\text{ if } O_{\\epsilon}(w_{t+1}) = True\n\\end{cases}$\nTheorem B.2. Suppose that x is sampled according to x ~ Uniform(\u2212L, L) \u00d7 {1} \u2286 $\\mathbb{R}^2$ and the target parameters v = (v, b) satisfy v \u2208 [1, wmax] and \u2212\\tilde{w}_{min} > \\frac{12 \\tilde{v} \\epsilon}{L}$\nand\n$5 \\frac{\\tilde{v}L}{2} - L \\geq \\epsilon$\nThen there exists a $\\delta > \\frac{\\tilde{w}_{min} L^6}{3 \\cdot 12^2 \\cdot 24^3 (2 + ||v_{max}||)^5(L^2 + 1)^6}$ and apply gradient descent with resets, according to (13) and (14), with learning rate $\\eta \\leq \\frac{\\tilde{w}_{min} L^6}{3 \\cdot 12^2 \\cdot 24^3 (2 + ||v_{max}||)^5(L^2 + 1)^6}$ we have that $O_{\\epsilon}(\\tilde{w}_{t+1})$ = False $\\forall t > 0$"}, {"title": "PROOF OF THEOREM B.2", "content": "Theorem B.3 (Restatement of Theorem B.2). Suppose that x is sampled according to x ~ Uniform(\u2212L, L) \u00d7 {1} \u2286 $\\mathbb{R}^2$ and the target parameters v = (v, b) satisfy v \u2208 [1, wmax] and \u2212$w_0$ such that\n$\\tilde{w}_{min} > \\frac{12 \\tilde{v} \\epsilon}{L}$\nand\n$5 \\frac{\\tilde{v}L}{2} - L \\geq \\epsilon$\nThen gradient descent with a constant learning rate of\n$\\eta \\leq \\frac{\\tilde{w}_{min} L^6}{3 \\cdot 12^2 \\cdot 24^3 (2 + ||v_{max}||)^5(L^2 + 1)^6}$\nand with resets, i.e. (13) and (14), attains an average regret of"}]}