{"title": "ControLRM: Fast and Controllable 3D Generation via Large Reconstruction Model", "authors": ["Hongbin Xu", "Weitao Chen", "Zhipeng Zhou", "Feng Xiao", "Baigui Sun", "Mike Zheng Shou", "Wenxiong Kang"], "abstract": "Despite recent advancements in 3D generation methods, achieving controllability still remains a challenging issue. Current approaches utilizing score-distillation sampling are hindered by laborious procedures that consume a significant amount of time. Furthermore, the process of first generating 2D representations and then mapping them to 3D lacks internal alignment between the two forms of representation. To address these challenges, we introduce ControLRM, an end-to-end feed-forward model designed for rapid and controllable 3D generation using a large reconstruction model (LRM). ControLRM comprises a 2D condition generator, a condition encoding transformer, and a triplane decoder transformer. Instead of training our model from scratch, we advocate for a joint training framework. In the condition training branch, we lock the triplane decoder and reuses the deep and robust encoding layers pretrained with millions of 3D data in LRM. In the image training branch, we unlock the triplane decoder to establish an implicit alignment between the 2D and 3D representations. To ensure unbiased evaluation, we curate evaluation samples from three distinct datasets (G-OBJ, GSO, ABO) rather than relying on cherry-picking manual generation. The comprehensive experiments conducted on quantitative and qualitative comparisons of 3D controllability and generation quality demonstrate the strong generalization capacity of our proposed approach. For access to our project page and code, please visit our project page.", "sections": [{"title": "1 INTRODUCTION", "content": "The potential of 3D content generation spans various sectors such as digital games, virtual reality/augmented reality (VR/AR), and filmmaking. Fundamental techniques in 3D content creation, such as text-to-3D and image-to-3D methods, offer substantial benefits by significantly reducing the need for laborious and costly manual work among professional 3D artists, thus enabling individuals without expertise to engage in the creation of 3D assets. Given the notable achievements in 2D content generation, exemplified by projects like DALL-E [1] and StableDiffusion [2], the community is increasingly focusing on advancements in 3D content generation. Recent progress in this field is credited to the advantageous characteristics of image diffusion models [2], [3], differentiable 3D representations [4], [5], and large reconstruction models [6], [7].\nAn appealing area of interest for 3D content creation is text-to-3D generation. Some groundbreaking advancements [8], [9] in text-to-3D synthesis have introduced methods to enhance a neural radiance field (NeRF) [4] through score distillation sampling (SDS) loss [8] for 3D asset generation. Building upon the influential work of DreamFusion [8], these SDS-based techniques aim to distill 3D information from pretrained large text-to-image generative models [1], [2]. Various strategies seek to elevate generation quality by expanding to multiple optimization phases [9], optimizing 3D representation and diffusion prior simultaneously [10], [11], and adjusting score distillation algorithms [12], [13].\nAnother crucial aspect of generating 3D content is the process of image-to-3D synthesis. The traditional approach to this challenge relies on 3D reconstruction methods such as Structure-from-Motion [15] and Multi-view Stereo [16], [17], [18], [19]. These techniques involve identifying 3D surface points by comparing similarities among point features extracted from source images, enabling the creation of highly precise surface and texture maps. Despite significant achievements in accurately reconstructing geometrical details, these methods still struggle to reproduce detailed view-dependent appearances. Consequently, recent advancements have focused on developing implicit 3D representations like neural radiance fields [4], [20] and neural implicit surfaces [21], [22]. These novel approaches explore volumetric representations that can be learned from dense multi-view datasets without explicit feature matching, offering more efficient and high-quality solutions [20], [23], [24]. Such efforts aim to move towards feed-forward models for radiance fields reconstruction, relaxing the need for dense views and per-scene optimization. Leveraging the capabilities and generalization power of large generative models like diffusion models, recent studies [25], [26], [27], [28], [29] have integrated pre-trained generative models with multi-view information to generate new views from sparse inputs. Additionally, the emergence of Large Reconstruction Models (LRM) [6], [30], [31] has emphasized learning internal perspective relationships through a triplane transformer [32] and cross-attention mechanisms with 2D visual features from single-view input images. Recent enhancements [7], [33] of LRM have focused on replacing triplane-based volume rendering with 3D Gaussian splatting [20] and extending single-view inputs to sparse multi-view configurations, facilitating comprehensive 3D object information.\nTo address the question of whether the current prompt-"}, {"title": "2 RELATED WORK", "content": "Building on the accomplishments of text-to-image diffusion models [2], [3], optimization-based approaches present a practical alternative by circumventing the necessity for extensive text-3D datasets. DreamFusion [8] is a seminal work that introduced the SDS loss to optimize a neural field using diffusion priors for 3D asset generation. Additionally, Score Jacobian Chaining [38] is a study that elevates pretrained 2D diffusion models for 3D creation, utilizing the chain rule and the gradients learned from a diffusion model to backpropagate scores through the Jacobian of a differentiable renderer. However, these optimization-based techniques commonly encounter a shared challenge known as the Janus problem. MVDream [29] tackles this issue by refining a multi-view diffusion model, which replaces self-attention with multi-view attention in Unet to produce consistent multi-view images. Introducing the concept of 3D Gaussian splatting [20], DreamGaussian [39] optimizes 3D Gaussians using the SDS loss. Nonetheless, it grapples with the Janus problem stemming from the uncertainties of 2D SDS supervision and rapid convergence. Addressing this, GSGEN [40] and GaussianDreamer [41] incorporate a coarse 3D prior to generate more cohesive geometries. Furthermore, GSGEN proposes the use of the 3D SDS loss from Point-E [42] for joint optimization in the geometry phase. Despite SDS's benefits in terms of data requirements, it necessitates optimization for each new 3D object and demands hours to reach convergence.\nThe extensive 3D datasets [35], [36] have unlocked new possibilities for training feed-forward models to generate 3D assets directly from text, single- or multi-view images. (1) 3D generation from single-view: LRM [6] first scales up the triplane transformer on a large dataset to predict a triplane neural radiance field (NeRF) from single-view images, showing high generalization ability. TripoSR [30] integrates significant improvements in data processing, model design, and training techniques, enhancing the efficiency and effectiveness. (2) 3D generation from multi-view: Methods based on multi-view are extensions designed to enhance the generation quality of single-view methods. Typically, multi-view images of an object are initially synthesized from a single image using a multi-view diffusion model [29]. Similar to single-view approaches, these methods can be broadly categorized as either diffusion-based or transformer-based architectures. Examples of diffusion-based architectures include SyncDreamer [27] and Wonder3D [28]. SyncDreamer necessitates dense views for 3D reconstruction, while Wonder3D employs a multiview cross-domain attention mechanism to process relatively sparse views. Transformer-based architectures like Instant3D [43] encodes multi-view images by a image encoder and concatenate the encoded results into a set of tokens for the image-to-triplane decoder. Additionally, LGM [7], GRM [44] and GS-LRM [33] enhance the generation quality using high-resolution features and increasing the number of surrounding views. (3) 3D generation from text: Point-E [42] and Shap-E [45] utilize complex prompts to generate point clouds and neural radiance fields respectively. Representing 3D data as volumes, 3DTopia [46] and VolumeDiffusion [47] train diffusion models by fitting volumetric modules. ATT3D [48] employs a feed-forward transformer to generate the 3D contents and train the model with amortized training via pretrained diffusion model. Latte3D [49] extends the amortization architecture of ATT3D, significantly improving the efficiency and generation quality."}, {"title": "3 METHOD", "content": "In this section, we present the ControLRM framework as depicted in Fig. 2. We commence by outlining the fundamentals of LRM in Sec. 3.1. Next, we delve into a comprehensive examination of the LRM framework from the perspective of the Variational Auto-encoder (VAE) in Sec. 3.2. Building on the insights from Sec. 3.2, we elucidate the process of enhancing the LRM to our proposed ControLRM in Sec. 3.3. Subsequently, we elaborate on the components of each module within ControLRM and expound on the training objectives in Sec. 3.4."}, {"title": "3.1 Preliminary of LRM", "content": "Large Reconstruction Model (LRM) is an advanced method that efficiently generates a 3D object from a single 2D image input. The LRM primarily consists of the following components:\nImage Encoder: Given an RGB image as input, we utilize a pre-trained visual transformer (ViT) [50] to encode the image into patch-wise feature tokens denoted by ${h_{i} \\in \\mathbb{R}^{D_{e}}}_{i=1}^{N_{p}}$, where $i$ represents the index of the image patch, $N_{p}$ is the total number of image patches, and $D_{e}$ signifies the dimension of the feature tokens. Specifically, the pretrained self-supervised model DINO (Caron et al., 2021) is used. The ViT incorporates a predefined [CLS] token $h_{cls} \\in \\mathbb{R}^{D_{e}}$, which is then concatenated with the feature sequence ${h_{i}}_{i=1}^{N_{p}}$ to form the output.\nCamera Features: The camera feature $c \\in \\mathbb{R}^{20}$ is comprised of the flattened vectors of camera extrinsic and intrinsic parameters. The 4-by-4 extrinsic matrix $E$ is flattened to a 16-dimensional vector $E_{1\\times16}$. The intrinsic parameters, including the camera focal length and principal points, are combined as a 4-dimensional vector: $[foc_{x}, foc_{y}, PP_{x}, PP_{y}]$. To embed the camera feature, a multi-layer perceptron (MLP) is employed to transform the camera feature $c$ into a 1024-dimensional camera embedding $\\tilde{c}$.\n$\\tilde{c} = MLP_{cam}(c) = MLP_{cam}([E_{1\\times16}, foc_{x}, foc_{y}, PP_{x}, PP_{y}])$ (1)\nModulation with Camera Features: The camera modulation incorporates an adaptive layer normalization (adaLN) [51] to adjust image features using denoising iterations and class designations. When provided with the camera feature $\\tilde{c}$ as input, a multi-layer perceptron (MLP) predicts the scaling factor $\\gamma$ and the shifting factor $\\beta$:\n$\\gamma, \\beta = MLP_{mod}(\\tilde{c})$ (2)\nSubsequently, the modulation function will process the sequence of vectors in the transformer ${f_{j}}$ as follows:\n$ModLN(f_{j}) = LN(f_{j}) \\cdot (1 + \\gamma) + \\beta$ (3)\nwhere LN is the layer Normalization [52].\nTransformer Layers: Each transformer layer consists of a cross-attention sub-layer, a self-attention sub-layer, and a multi-layer perceptron sub-layer (MLP), where the input tokens for each sub-layer are modulated by the camera features. The feature sequence $f_{in}$, serving as the input to the transformer layers, can also be viewed as triplane hidden features. As illustrated in Fig. 2 (b), the cross-attention module uses the feature sequence $f_{in}$ as the query and the image features ${h_{cls}, h_{i}}_{i=1}^{N_{p}}$ as the key/value pairs.\n$f^{cross-i} = Cross-I(ModLN(f_{in}); {h_{cls}, h_{i}}_{i=1}^{N_{p}}) + f_{i}$ (4)\nwhere Cross-I represents the cross-attention between the image features and the triplane features.\nSubsequent to the original transformer [53], the self-attention sub-layer denoted as Self(\u00b7) and the multi-layer perceptron sub-layer labeled as MLP(\u00b7) handle the input feature sequence in the ensuing manner:\n$f^{self} = Self(ModLN(f^{cross-i}); ModLN(f^{cross-i})) + f^{cross-i}$ (5)\n$f^{out} = MLP(ModLN(f^{self})) + f^{self}$ (6)\nwhere $f^{out}$ represents the triplane feature output. This final output undergoes upsampling via a trainable deconvolution layer and is subsequently reshaped into the final triplane representation $TP \\in \\mathbb{R}^{3\\times64\\times64\\times D_{t}}$, where $D_{t}$ signifies the dimension of the triplane.\nTriplane NeRF: The triplane TP comprises three axis-aligned feature planes: $TP_{xy}/TP_{yz}/TP_{xz}\\in \\mathbb{R}^{64\\times64\\times D_{t}}$. Given any 3D point $p = [p_{x}, p_{y}, p_{z}]^{T}$ within the NeRF object bounding box $[-1,1]^{3}$, the point's feature can be extracted from the triplane TP using bilinear sampling.\n$TP_{p} = Concat(TP_{xy}[p_{x}, p_{y}], TP_{yz}[p_{y}, p_{z}], TP_{xz}[p_{x}, p_{z}])$ (7)"}, {"title": "3.2 Understanding LRM in a Perspective of VAE", "content": "From the perspective of Variational Autoencoder (VAE) [55], the LRM can be viewed as an intricate architecture that encompasses certain fundamental principles akin to VAEs.\nSimilar to the encoder in a VAE, the image encoder of LRM processes an input image, transforming it into a series of feature tokens. These tokens serve as the encoded latent representation of the input image, mirroring the latent space in a VAE. The decoding component of LRM functions analogously to the decoder in a VAE by reconstructing images from the latent space. Specifically, LRM maps the latent trilinear representation to a 3D object within NeRF and subsequently generates images with new perspectives, akin to the generation or decoding process within a VAE framework. LRM employs a reconstruction loss to reduce the dissimilarity between the input image and the rendered images altered based on camera parameters. In the subsequent section, we will offer a theoretical overview of LRM, including a form of Evidence Lower Bound (ELBO).\nGiven the 3D representation $x_{3d}$, a set of projected 2D images ${x_{i}}_{i=1}^{N_{v}}$ with corresponding camera parameters ${T_{i}}_{i=1}^{N_{v}}$, where $N_{v}$ denotes the number of viewpoints. It is assumed that the ground-truth distribution of the 3D representation is represented by the density $p(x_{3d})$. In LRM, this 3D representation is characterized by a triplane Neural Radiance Field (NeRF). Under this assumption, one can write:\n$p(x_{3d}) = \\int_{z} p(x_{3d}, z)dz = \\int_{z} p(x_{3d}|z)p(z)dz$ (9)\n$z$ represents the latent variable associated with $x_{3d}$, following a simple distribution $p(z)$ referred to as the prior distribution. The primary objective of the VAE is to acquire a robust approximation of $p(x_{3d}|z)$ based on the provided data. This approximated distribution is denoted by $p_{\\theta}(x_{3d}|z)$, where $\\theta$ symbolizes the learnable parameters. Subsequently, we can compute the log likelihood $log p_{\\theta}(x_{3d})$ in the following manner:\n$log p_{\\theta}(x_{3d})$\n$= log \\int p_{\\theta}(x_{3d}|T)p(T)dT = log \\int \\frac{p_{\\theta}(x_{3d}|T)p(T)}{p_{\\theta}(x_{3d}|T)p(T)} d \\int p_{\\theta}(x_{3d}|T)p(T)dT \\geq \\int \\frac{p_{\\theta}(x_{3d}, z|T_{i})}{q_{\\phi}(z|x_{i}, T_{i})}dz$\n$\\geq \\frac{1}{N_{v}} \\sum_{i=1}^{N_{v}} log \\int \\frac{p_{\\theta}(x_{3d}, z|T_{i})}{q_{\\phi}(z|x_{i}, T_{i})}dz = \\frac{1}{N_{v}} \\sum_{i=1}^{N_{v}} log E_{q_{\\phi}} [\\frac{p_{\\theta}(x_{3d}, z|T_{i})}{q_{\\phi}(z|x_{i}, T_{i})}]$ (10)\nwhere $p_{\\theta}(x_{3d}|T_{i})$ indicates that the 3D representation $x_{3d}$ is conditioned on the camera parameters $T_{i}$ corresponding to viewpoint i. Given that our $x_{3d}$ embodies a triplane NeRF, when conditioned on $T_{i}$, it serves as a representation of the rendered image from viewpoint i. The final row in Eq. 10 denotes the Evidence Lower Bound (ELBO). By isolating the inner term of ELBO at viewpoint i, we obtain:\n$E_{q_{\\phi}} log \\frac{p_{\\theta}(x_{3d}, z|T_{i})}{q_{\\phi}(z|x_{i}, T_{i})}$\n$=E_{q_{\\phi}} log \\frac{p_{\\theta}(x_{3d} z, T_{i})p_{\\phi}(z)}{q_{\\phi}(z|x_{i}, T_{i})}$\n$=E_{q_{\\phi}} log p_{\\theta}(x_{3d}|z, T_{i}) - KL(q_{\\phi}(z|x_{i}, T_{i})||p_{\\phi}(z))$\n$=E_{q_{\\phi}} [log \\int p_{\\theta}(x_{3d}|z, T_{i}, T)p(T)dT - KL(q_{\\phi}(z|x_{i}, T_{i})||p_{\\phi}(z))]$\n$= \\frac{1}{M} \\sum_{j=1}^{M} E_{q_{\\phi}} [log p_{\\theta}(x_{3d}|z, T_{i}, T_{j}) - KL(q_{\\phi}(z|x_{i}, T_{i})||p_{\\phi}(z))]$ (11)\nNote that the extrinsic matrix of the input reference view is normalized to an identity matrix, while the extrinsic matrices of the other views are adjusted to the relative transformation matrix with respect to the normalized reference view. The intrinsic parameters remain constant across all views. Consequently, the input camera parameter $T_{i}$ is consistent and fixed within the LRM, thereby allowing for its exclusion from the formulas:\n$E_{q_{\\phi}} log \\frac{p_{\\theta}(x_{3d}, z|T_{i})}{q_{\\phi}(z|x_{i}, T_{i})}$\n$\\frac{1}{M} \\sum_{j=1}^{M} E_{q_{\\phi}} [log p_{\\theta}(x_{3d}|z, T_{j}) - KL(q_{\\phi}(z|x_{i})||p_{\\phi}(z))]$ (12)\nwhere $p_{\\theta}(x_{3d}|z, T_{j})$ represents the triplane decoder (depicted as purple modules in Fig. 2), while $q_{\\phi}(z|x_{i})$ denotes the image encoder (illustrated as orange modules in Fig. 2)."}, {"title": "3.3 Upgrading LRM to ControLRM", "content": "Eq. 10, 11, and 12 elaborate on the extension of LRM, interpreting it as a specialized variant of the Variational Autoencoder (VAE). By analogy, these expressions can be further expanded to cater to the objective of controllable 3D generation. Consider $e_{i}$ as indicative of the input 2D visual condition on view i and the associated textual prompt concerning the 3D object, the ELBO can be formulated as:\n$log p_{\\theta}(x_{3d})$\n$= log \\int p_{\\theta}(x_{3d}|T)p(T)dT$\n$\\tilde{}\\geq \\int p_{\\theta}(x_{3d}|T)p(T)dT = \\int_{i}^{i}$\n$\\frac{1}{N_{v}} \\sum_{i=1}log \\int p_{\\theta} (x_{3d}|T_{i})$\n$\\geq \\frac{1}{N_{v}} \\sum_{i=1}log \\int \\frac{p_{\\theta}(x_{3d}, z|T_{i})}{q_{\\phi'}(z|e_{i}, T_{i})}dz \\frac{1}{N_{v}} \\sum_{i=1}log E_{q_{\\phi'}} [\\frac{p_{\\theta}(x_{3d}, z|T_{i})}{q_{\\phi'}(z|e_{i}, T_{i})}]$ (13)"}, {"title": "3.4 ControLRM", "content": "In this section, we delve into the specific modules of ControLRM. The design of the conditional generator was detailed in Fig. 2 in Section 3.4.1. Depending on the chosen backbone for the conditional generator, ControLRM manifests in two variants: 1) ControLRM-T featuring a transformer-based conditional generator (Section 3.4.2); 2) ControLRM-D integrating a diffusion-based conditional generator (Section 3.4.3). Subsequently, we present the condition-to-triplane transformer decoder in Section 3.4.6. The training objectives encompassing adversarial loss, clip loss, and rendering loss are expounded upon in Section 3.4.7."}, {"title": "3.4.1 Design of Conditional Generator", "content": "As depicted in Fig. 2, the conditional generator utilizes the 2D condition and the text embedding of CLIP [56] as input to produce the 2D latents required for subsequent procedures. A naive design of this generator is a transformer-based backbone with cross-attention mechanism between the feature sequence extracted from condition image and the text feature. However, this design with only the cross-attention mechanism fails to generate a regular results but yielding meaningless results in the experiments. A similar issue was observed in [57], indicating that the main reason for this optimization failure stems from the notable disparity between the 2D renderings and the ground truth images. As noted by [58], the optimization gradient becomes unreliable when the generated distribution and the target distribution are disjoint. In contrast, the backward gradients to the 2D latents in our model must traverse a series of modules, including the condition encoder, triplane transformer, and NeRF modules. This complexity of pathways may significantly impede the optimization process, consequently resulting in unexpected failures. A straightforward remedy proposed in [57] involves the incorporation of randomness (e.g., Gaussian noise) into the network architecture. By increasing the overlap between the rendered distribution and the target distribution, the gradients during training become more meaningful, promoting convergence. In summary, the key considerations for designing the condition generator in ControLRM are: 1) Incorporation of randomness for improved training outcomes. 2) Emphasis on the efficiency of the generator for fast inference speed."}, {"title": "3.4.2 Transformer-based Conditional Generator", "content": "For ControLRM-T model, we have devised a lightweight transformer-based generator, illustrated in Figure 3 (a). Building upon the preceding discussion, we introduce randomness through a style injection module. Drawing inspiration from the original style injection concept in StyleGAN [59], where style features and random noise are integrated into the generator via Adaptive Instance Normalization (AdaIN), we adapt this approach by treating the text embedding as the style feature. This text embedding is concatenated with random Gaussian noise and passed through a 3-layer MLP within our style injection module. The resulting feature vector is then combined with the output of each convolution layer to incorporate the text feature. In Figure 3 (a), the convolution blocks and transformer blocks are stacked together, with residual connections applied to the convolution blocks in a U-Net configuration."}, {"title": "3.4.3 Diffusion-based Conditional Generator", "content": "For the ControLRM-D model, we have intricately integrated LoRA adapters [60] into the original latent diffusion model, incorporating small trainable weights. Leveraging the inherent randomness within the diffusion model, and aided by the pre-trained weights obtained from large-scale datasets, we aim to address the discrepancy issue highlighted in Section 3.4.4. In addressing efficiency concerns, we opt for the fast one-step diffusion model [61] as the foundational framework. Specifically, we initialize the Diffusion-based generator with the pre-trained weights of SD-Turbo [62]. To form the 2D latents for subsequent procedures, we concatenate the outputs of the last three layers of the decoder depicted in Figure 3 (b)."}, {"title": "3.4.4 Condition Encoder", "content": "In Figure 2 (a), the 2D latents are firstly interpolated to match the resolution of the input condition image, and then divided into the feature sequence ${g_{i}|g_{i} \\in \\mathbb{R}^{D_{e}}}_{i=1}^{N_{p}}$. Similar to the feature sequence ${h_{i}}$ extracted from the input image discussed in Sec. ??, $D_{e}$ denotes the feature dimension, while $N_{p}$ corresponds to the number of patches. Within the condition encoder, the feature sequence ${g_{i}}_{i=1}^{N_{p}}$ is passed through a sequence of transformer layers, each comprising a self-attention sub-layer and an MLP sub-layer.\n$g^{self} = Self(g_{i}; g_{i}) + g_{i}$ (16)\n$g^{out} = MLP(g^{self}) + g^{self}$ (17)\nwhere $g^{out}$ is the output feature. To integrate the random sampling process, the output $g^{out}$ of the final transformer layer is fed to another MLP to regress the mean and variance results:\n$\\mu_{g}, \\sigma_{g} = MLP(g^{out})$ (18)\nwhere $\\mu_{g}$ is the mean feature and $\\sigma_{g}$ represents the variance. Throughout training, the output feature sequence ${\\hat{g_{i}}}_{i=1}^{N_{p}}$ is stochastically sampled from a Gaussian distribution, where $\\hat{g_{i}} \\sim N(\\mu_{g_{i}}, \\sigma_{g})$."}, {"title": "3.4.5 Auxiliary Decoder", "content": "To boost the performance, we further introduce an auxiliary decoder for the 2D latents to enhance the training process. The generated 2D latents from the conditional generator (refer to Sections 3.4.2 and 3.4.3) are passed through a lightweight three-layer convolutional neural network. The resulting image $X_{aux}$ is combined with the 2D renderings to compute the loss function for the generated images. The inclusion of the auxiliary decoder offers direct guidance to the 2D generator, aiding in overall network convergence."}, {"title": "3.4.6 Triplane Transformer Decoder", "content": "The condition-to-triplane decoder receives the condition feature sequence ${\\hat{g_{i}}}$ and the triplane feature sequence $f_{in}$. Analogous to the image-to-triplane decoder discussed in Sec. 3.1, each transformer layer consists of a cross-attention sub-layer, a self-attention sub-layer, and an MLP layer. The input tokens for each sub-layer are influenced by the camera features $\\tilde{c}$. The operation of each transformer layer can be described as follows:\n$f^{cross-c} = Cross-C(ModLN(f_{in}); {\\hat{g_{i}}}) + f_{in}$ (19)\n$f^{self} = Self(ModLN(f^{cross-c}); ModLN(f^{cross-c})) + f^{cross-c}$ (20)\n$f^{out} = MLP(ModLN(f^{self})) + f^{self}$ (21)"}, {"title": "3.4.7 Training Objectives", "content": "In Fig. 2, the training objectives consist of three components: adversarial loss, CLIP loss, and rendering loss. For each sample, we designate one reference view and randomly select $V - 1$ side views. Denoting the rendered images of ControLRM as $\\hat{v}$ and the ground truth images as $x_{GT}^{v}$, the index of the reference view is designated as 0. The resultant image from the auxiliary decoder (refer to Sec. 3.4.5) is denoted as $X_{aux}$. The calculation of the loss can be expressed as follows:\nAdversarial Loss: To incentivize the alignment of the generated images with the corresponding ground truth domains, we apply an adversarial loss [63]. In line with the approach advocated by Vision-Aided GAN [64], the discriminator utilizes the CLIP model as its foundation. The adversarial loss is defined as follows:\n$L_{adv} = \\frac{1}{V+1} [ \\sum_{v=0}^{V} E[log D(x_{GT}^{v})] + \\sum_{v=0}^{V} E[log(1 - D(\\hat{v}))] + E[log D(x_{GT}^{v})] + E[log(1 - D(X_{aux}))]$ (22)"}, {"title": "4 EXPERIMENT", "content": "To assess the controllability of various 3D generation methods, we have developed metrics tailored to gauge the consistency of input 2D conditions following ControlNet++ [72]. Four distinct conditions are taken into account: edge (canny), sketch, depth, and normal. Specific metrics have been intricately designed for each condition to quantify the extent to which the condition is maintained throughout the generation process:"}, {"title": "5 LIMITATION", "content": "In this study, the quantitative and qualitative analysis prove the superiority of our proposed method, but we also realize that this work is still insufficient and discuss the following limitations: (1) Condition Expansion: While significant advancements have been made under four control conditions, it is crucial to extend this framework to encompass additional control conditions such as segmentations, pose, and others. (2) Generalization Bottleneck: The bottleneck of the proposed method is attributed to the utilization of the pre-trained Large Reconstruction Model (LRM). Although the proposed approach effectively aligns the controllable 2D generator with the pre-trained triplane decoder, failures in the pre-trained LRM could result in the failure of our ControLRM. Therefore, enhancing the performance by employing a more robust backbone can address this issue."}, {"title": "6 CONCLUSION", "content": "This paper introduces ControLRM, a novel controllable 3D generation framework characterized by high speed and superior generation quality. Our model offers support for four different types of controls: Edge (Canny), Depth, Normal, and Sketch. The architecture comprises an end-to-end feed-forward network that includes a 2D condition encoder based on transformer or diffusion models and a 3D triplane decoder leveraging a pre-trained LRM, where only the cross-attention layers are active during training. Additionally, we introduce an joint training pipeline encompassing adversarial loss, clip loss, and reconstruction loss. To ensure fair evaluation, we collect unseen evaluation samples from three different datasets: G-OBJ, GSO, and ABO. The comprehensive quantitative and qualitative evaluation findings demonstrate that our model surpasses existing state-of-the-art methods and achieves generation speeds significantly faster by an order of magnitude."}]}