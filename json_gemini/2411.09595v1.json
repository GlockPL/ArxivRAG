{"title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models", "authors": ["Zhengyi Wang", "Jonathan Lorraine", "Yikai Wang", "Hang Su", "Jun Zhu", "Sanja Fidler", "Xiaohui Zeng"], "abstract": "This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived from textual sources like 3D tutorials, and (2) enabling conversational 3D generation and mesh understanding. A primary challenge is effectively tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly. To address this, we introduce LLAMA-MESH, a novel approach that represents the vertex coordinates and face definitions of 3D meshes as plain text, allowing direct integration with LLMs without expanding the vocabulary. We construct a supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate 3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs as required, and (3) understand and interpret 3D meshes. Our work is the first to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge for 3D mesh generation in a text-based format, effectively unifying the 3D and text modalities. LLAMA-MESH achieves mesh generation quality on par with models trained from scratch while maintaining strong text generation performance.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) [5, 52] have demonstrated remarkable capabilities in understanding and generating human-like text, achieving success in applications such as conversational agents, code generation, and visual content reasoning [1, 16, 30]. Despite these advances, their generative abilities have primarily been limited to the textual content, restricting their utility for broader tasks.\nOur work seeks to extend LLMs into a new modality\u20143D mesh generation\u2014unlocking significant potential for fields like computer graphics, engineering, robotics, and virtual/augmented reality. By enabling LLMs to generate 3D meshes from textual descriptions, we unify language understanding with 3D content creation, expanding the functional scope of LLMs. This approach paves the way for more intuitive and efficient workflows in 3D content creation driven by language-based instructions.\nHowever, integrating a new modality into an LLM is challenging, particularly in the tokenization process for processing the new modality. To the best of our knowledge, there have been no attempts to unify 3D mesh and text generation in a single framework. Some studies explored unifying image and text generation. Among these works [37, 55], a common approach is to train a new to-"}, {"title": "2. Related Work", "content": "We overview approaches to make LLMs multi-modal in Section 2.1, 3D object generation more broadly in Section 2.2, and then specifically generating meshes auto-regressively in Section 2.3."}, {"title": "2.1. Enabling LLMs to be Multi-Modal", "content": "Extending LLMs to process and generate multiple modalities, such as vision and language, in a unified model is an active research area. Existing works allow an LLM to understand visual input for multimodal interactions [2, 3, 12, 21, 22, 26, 28, 31, 32, 67, 72] or unify image and text generation [37, 55, 61, 62, 68, 74] with a new visual tokenizer. Instead, we avoid modifying tokenization and focus on 3D by simply outputting (the text of) an OBJ file.\nClosely related are works where LLMs wield tools to generate 3D scenes [66, 73] by generating layouts to compose predefined objects. However, these methods do not enable LLMs to produce 3D meshes directly. To the best of our knowledge, we are the first to allow LLMs to directly generate 3D meshes as text instead of just wielding 3D object generation tools."}, {"title": "2.2. 3D Object Generation", "content": "DreamFusion [41], Magic3D [27], ProlificDreamer [56] and many other methods [6, 8, 11, 20, 25, 29, 35, 47, 48, 54, 63, 69, 75] use score-distillation to generate 3D objects from pretrained large-scale text-to-image diffusion model [42, 43]. Feed-forward methods including LRM [19, 23, 53, 65], CRM [57], InstantMesh [64], and other methods [24, 33, 34, 49, 60, 71, 76] generate 3D objects without test-time optimization. However, the above methods typically treat 3D objects as numerical fields and extract meshes using marching cubes or their variants [44, 45], which do not easily allow representation as discrete tokens."}, {"title": "2.3. Auto-Regressive Mesh Generation", "content": "Methods such as PolyGen [39], MeshGPT [46], MeshXL [7], instead model a 3D object as a discrete sequence of tokenized coordinates and use an auto-regressive transformer to generate an object with artist-created topology. MeshAnything [9, 10], PivotMesh [58] and EdgeRunner [50] take point cloud as input condition for better control. These works also treat meshes as discrete tokens generated using auto-regressive transformers, but they are trained from scratch and lack language capabilities."}, {"title": "3. Method", "content": "We now introduce LLAMA-MESH. First, in Section 3.1, we explain why and how we represent 3D meshes as plain text for easy processing by LLMs. Then, in Section 3.2, we detail the pretrained LLaMA model [16], an effective initialization of LLAMA-MESH. Finally, Section 3.3 describes how we create a 3D dialog SFT dataset to give LLMs 3D generation capabilities through fine-tuning. Our model structure is illustrated in Figure 2."}, {"title": "3.1. 3D Representation", "content": "To enable large language models (LLMs) to generate 3D meshes directly, a key challenge lies in tokenizing this new modality so that the LLM can process it effectively. We observe that pretrained LLMs can generate 3D objects in the OBJ file format\u2014a simple and widely used plain text format\u2014in a zero-shot manner, as illustrated in Figure 6. Although these generated shapes are simple and not immediately usable, they demonstrate that some 3D knowledge in OBJ format is inherently encoded in the LLMs. Additionally, since OBJ files describe 3D geometry in a plain text format, they are ideal candidates for integration with LLMs without requiring modifications to the tokenizer or vocabulary. These insights motivate us to represent 3D objects using the OBJ file format.\nAn OBJ file consists of a list of vertex coordinates and face definitions: Vertices (v): Each line starting with the letter v defines a vertex in 3D space with its x, y, and z coordinates, e.g., v 0.123 0.234 0.345. Faces (f): Each line starting with the letter f defines a face by listing vertex indices that form a polygon (typically a triangle or quadrilateral), e.g., f 1 2 3. By treating these numerical values as plain text, we convert the 3D mesh into a sequential text format that LLMs process natively. Figure 4 shows an example of a simple OBJ file and its corresponding 3D object rendering. Note that OBJ files from the internet may vary slightly in format. We adopt a standard that is widely used and straightforward."}, {"title": "3.2. Pretrained Models", "content": "Pretrained LLMs, like LLaMA [16] variants, are natural candidates for generating text for meshes as they are (a) strong tools for modeling arbitrary sequences and (b) may have encountered similar data during pre-training. Specifically, we use LLaMA3.1-8B-Instruct [16] as our base model.\nThis model is chosen for its balance between performance and computational efficiency. It has been instruction-tuned to follow prompts and generate coherent responses, which is advantageous for our application, where the model needs to interpret text prompts and generate corresponding 3D meshes. Notably, the model can generate simple (but not perfect) OBJ files without fine-tuning, as shown in Figure 6, likely because there are publicly available examples, e.g., on GitHub.\nDespite its strengths, the pretrained LLaMA model performs poorly on mesh generation tasks without fine-tuning, underscoring the need for fine-tuning the model on a specialized dataset that includes mesh representations in plain text. By fine-tuning LLaMA on our curated dataset of text-mesh pairs, we enable the model to learn the patterns and semantics of the OBJ format, allowing it to generate valid 3D meshes directly from textual descriptions."}, {"title": "3.3. 3D-task Finetuning", "content": "To equip LLMs with 3D capabilities, we construct a supervised fine-tuning (SFT) dataset for training. We use 3D meshes from Objaverse [14], a comprehensive 3D dataset for general objects. To build the chat dataset, we employ (1) a rule-based approach and (2) LLM-based augmentation.\nIn the rule-based approach, we design several simple patterns, such as \u201c(user) {obj} What is this? (assistant) {caption}.\u201d for mesh understanding, and \u201c(user) Create a 3D model of {caption}. (assistant) {obj}.\u201d for mesh generation. For each 3D object, we randomly select a pattern and replace placeholders with the mesh definition and caption. Although these conversations are straightforward, they provide the LLM with foundational knowledge of the correspondence between text and 3D representations.\nTo enable more sophisticated conversations, we create complex text-3D dialogues. We write sample dialogues in a text-3D interleaved format and use in-context learning to prompt the pretrained LLM to generate dialogues for each 3D object based on its textual description. We use a combi-\n\nDataset preparation We filter the Objaverse dataset [14] to select meshes with a maximum of 500 faces to maintain a manageable computational complexity, resulting in 31k total meshes. Each is converted to the OBJ format, and vertex coordinates are quantized into 64 bins to reduce the token sequence length without significantly compromising geometric detail. We use captions generated by Cap3D [38] as text descriptions accompanying each mesh. To avoid overfitting, we randomly rotate the meshes with degrees from {0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0}, resulting in around 125k meshes. Following prior works [58], we sort the vertices by z-y-x coordinates from lowest to highest. We also sort faces by lowest vertex indices. If two faces have the same lowest index, we sort by the next lowest, and so on. The LLM\u2019s context length is set to 8k tokens."}, {"title": "4. Experiments", "content": "We first provide implementation details in Section 4.1, including dataset preparation and training. Next, in Section 4.2, we include our method\u2019s results, showcasing the quality and diversity of generated meshes and the chatting ability preserved from the pretrained LLM. We compare LLAMA-MESH with baseline methods in Section 4.3."}, {"title": "4.1. Implementation Details", "content": "Dataset preparation We filter the Objaverse dataset [14] to select meshes with a maximum of 500 faces to maintain a manageable computational complexity, resulting in 31k total meshes. Each is converted to the OBJ format, and vertex coordinates are quantized into 64 bins to reduce the token sequence length without significantly compromising geometric detail. We use captions generated by Cap3D [38] as text descriptions accompanying each mesh. To avoid overfitting, we randomly rotate the meshes with degrees from {0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0}, resulting in around 125k meshes. Following prior works [58], we sort the vertices by z-y-x coordinates from lowest to highest. We also sort faces by lowest vertex indices. If two faces have the same lowest index, we sort by the next lowest, and so on. The LLM\u2019s context length is set to 8k tokens."}, {"title": "4.2. Results", "content": ""}, {"title": "4.2.1. Mesh Generation Results", "content": "Figure 3 shows our method generates high-quality meshes. Similarly to previous auto-regressive mesh generation methods [9, 46], our approach produces artist-like topology, as it learns mesh topology during training.\nWe evaluate the diversity of generated meshes by providing the same text prompt multiple times and observing the variations in the resulting meshes. Figure 10 demonstrates the model generates a variety of unique meshes that all satisfy the prompt, highlighting our ability to produce diverse and creative outputs. This diversity is essential for applications requiring multiple design options or variations."}, {"title": "4.2.2. Language and Conversational Abilities", "content": "Qualitative Results After fine-tuning for mesh generation, we evaluate whether LLAMA-MESH retains its language understanding capabilities. Figures 1 and 7 show the model engages in coherent and contextually appropriate dialogues, comprehending complex instructions, asking clarifying questions, and providing detailed responses, demonstrating its language proficiency remains intact.\nQuantitative Results Table 3 presents quantitative results evaluating language abilities. We report the performance of our model, LLAMA-MESH (8B), and compare it with baseline models of various sizes: LLaMA3.1 (8B), LLaMA3.2 (3B), and LLaMA3.2 (1B). The metrics include MMLU [18] (5-shot), PIQA [4] (0-shot), HellaSwag [70] (0-shot), and GSM8K [13] (8-shot), which assess the model's general knowledge, commonsense reasoning, and mathematical problem-solving skills. Our model, fine-tuned to generate OBJ files for 3D mesh generation, retains language understanding and reasoning capabilities comparable to the baseline models. This demonstrates that LLAMA-MESH successfully extends the LLM's functionality to 3D content generation while preserving its original language capabilities."}, {"title": "4.3. Comparison with Existing Methods", "content": "Qualitative Comparison To assess our 3D generation capability, we compare with state-of-the-art methods in 3D mesh generation, specifically MeshXL [7], an auto-regressive text-to-mesh generation approach, and Unique3D [59], a 3D generation method based on multi-view image diffusion. Since Unique3D produces dense meshes, we do not visualize their topology. Additionally, because Unique3D is originally designed for image-to-3D generation, we use SDXL [40] to generate an input image for Unique3D based on the text prompt. Figure 11 shows LLAMA-MESH generates meshes of comparable quality to existing methods when given the same text prompt, capturing fine details and complex geometries effectively. While Unique3D and MeshXL are specialized models trained exclusively for mesh generation, LLAMA-MESH achieves similar results while maintaining robust language understanding capabilities within a single model.\nTraining Efficiency and Model Size Table 2 compares training time, computational resources, and model sizes across different methods. MeshXL [7] trains large transformer models entirely on mesh data, demanding substantial computational resources. In contrast, leveraging a pretrained LLM makes our fine-tuning approach considerably more efficient, reducing training computational cost."}, {"title": "5. Discussion", "content": ""}, {"title": "5.1. Limitations", "content": "While LLAMA-MESH shows the LLM's potential for 3D mesh generation, there are several limitations to address in future work. Quantizing vertex coordinates into a limited number of bins can lead to a loss of geometric detail, affecting the generated meshes' fidelity. Also, the context length constraints the model's ability to generate highly complex or large-scale 3D structures. Currently, we only support a maximum of 500 faces, limiting the mesh detail level.\nWe observe a slight degradation in language ability after fine-tuning as in Table 3. We conjecture this is due to relying solely on UltraChat as our text instruction dataset. Incorporating more diverse and high-quality text instruction datasets could help preserve the language capabilities of LLAMA-MESH. Also, we only use 3D meshes from Objaverse [14] dataset for training. We believe incorporating more datasets could enrich the generated results. Additionally, we use only an 8B model due to limited computational resources; we believe that using a larger LLaMA model would further improve the results."}, {"title": "5.2. Conclusion", "content": "We introduced LLAMA-MESH, a novel approach that unifies 3D mesh generation with large language models by representing meshes as plain text. By fine-tuning LLaMA on a 3D dialog dataset we curated, we enabled it to generate 3D meshes directly from textual prompts without expanding the vocabulary or introducing new tokenizers. Our method preserves the language understanding capabilities of the base model while extending its generative abilities to the 3D domain. Experimental results show that LLAMA-MESH achieves mesh generation quality comparable to specialized models trained from scratch on 3D data. This work represents a significant step toward integrating multi-modal content generation within a cohesive language model."}, {"title": "5.3. Future Work", "content": "Future work could explore more efficient encoding schemes for 3D data within language models, methods to handle longer context lengths, and techniques to improve the geometric precision of generated meshes. Integrating additional modalities, such as textures or physical properties, and extending the model's capabilities to handle dynamic scenes are promising directions.\nIntegrating 3D mesh generation into LLMs opens up exciting possibilities for interactive design, where users can converse with a model to create and manipulate 3D objects in real time. Such advancements could revolutionize virtual reality, gaming, education, and manufacturing by making 3D content creation more intuitive. We envision a future where large language models are universal generative tools capable of seamlessly producing content across multiple modalities, including text, images, and 3D structures."}, {"title": "Reproducibility", "content": "We provide comprehensive implementation details in Section 3, including data preprocessing steps, model architecture specifications, and training procedures. All hyperparameters, training configurations, and evaluation protocols are detailed in Section 4.1."}, {"title": "Ethics Statements", "content": "Our work unifies 3D mesh generation with LLMs, and thus we may inherit ethical concerns from both language models and generative models in general. Potential risks include generating 3D content that could be misused for malicious purposes, reproducing biased or inappropriate content present in training data, and potentially impacting the livelihoods of 3D artists and designers due to automation."}]}