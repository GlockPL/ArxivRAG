{"title": "AdaResNet: Enhancing Residual Networks with Dynamic Weight Adjustment for Improved Feature Integration", "authors": ["Hong Su"], "abstract": "In very deep neural networks, gradients can become extremely small during backpropagation, making it challenging to train the early layers. ResNet (Residual Network) addresses this issue by enabling gradients to flow directly through the network via skip connections, facilitating the training of much deeper networks. However, in these skip connections, the input (ipd) is directly added to the transformed data (tfd), treating ipd and tfd equally, without adapting to different scenarios. In this paper, we propose AdaResNet (Auto-Adapting Residual Network), which automatically adjusts the ratio between ipd and tfd based on the training data. We introduce a variable, weightpd, to represent this ratio. This variable is dynamically adjusted during backpropagation, allowing it to adapt to the training data rather than remaining fixed. Experimental results demonstrate that AdaResNet achieves a maximum accuracy improvement of over 50% compared to traditional ResNet.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, deep learning has revolutionized numerous fields, ranging from computer vision and natural language processing to autonomous systems and beyond. Among the various architectures that have emerged, ResNet (Residual Network) has played a pivotal role in advancing the state of the art in these domains [1] [2]. Its innovative design has enabled the training of extremely deep neural networks by addressing a critical challenge faced in traditional deep architectures: the vanishing gradient problem.\nAs neural networks become deeper, gradients can diminish significantly during the backpropagation process. This issue hampers the effective training of the early layers, causing the network to stagnate and preventing it from learning meaningful representations. ResNet tackles this problem by introducing skip connections [3], which allow gradients to bypass intermediate layers and flow directly through the network. This mechanism facilitates the training of much deeper networks, making it possible to achieve unprecedented levels of accuracy and performance on complex tasks.\nDespite the success of ResNet, the standard implementation of skip connections involves directly adding the input (ipd) to the transformed data (tfd), i.e., they are combined in a fixed ratio of 1:1, as illustrated in Figure 1. This approach inherently assumes that ipd and tfd contribute equally to the network's output, which may not be optimal across all recognition scenarios. By treating ipd and tfd as identical in their contribution, the traditional ResNet architecture does not account for the varying importance of ipd and tfd across different layers or diverse training data distributions.\nIn this paper, we propose a novel architecture, AdaResNet (Auto-Adapting Residual Network), which enhances the flexibility of ResNet by automatically adapting the contribution of ipd and tfd during training. Specifically, we introduce a learnable parameter, denoted as weightind, which dynamically adjusts the ratio between ipd and tfd based on the training data. Unlike traditional ResNet, where the combination of ipd and t fd remains fixed, AdaResNet allows this ratio to be tuned throughout the training process, thereby improving the network's ability to generalize across diverse data distributions.\nThe contributions of this paper are threefold: (1) Introduction of AdaResNet: We present AdaResNet, a novel extension of the ResNet architecture that incorporates an adaptive mechanism to balance the contributions of skipped input (ipd) and processed data (tfd). This approach overcomes the limitations of the fixed 1:1 ratio combination used in traditional ResNet, allowing for more flexible and effective integration of ipd and tfd. (2) Learnable parameter weightpd: We propose a new learnable parameter, weightind, which is automatically optimized during training. This parameter enables the network to dynamically adjust the balance between ipd and tfd in response to varying data characteristics, improving the model's adaptability and performance. (3)Layer-specific and"}, {"title": "II. MODEL", "content": "In very deep networks, gradients can become extremely small during backpropagation, making it difficult to train the early layers. ResNet (Residual Network) addresses this challenge by allowing gradients to flow directly through the network via skip connections, facilitating the training of much deeper networks.\nThe process of transforming input data (x) to produce output (y) in a traditional ResNet can be described as in (1)\n$x \\rightarrow f_1(x) \\rightarrow f_2(f_1(x)) \\rightarrow\\ldots\\rightarrow f_n(\\ldots (f_1(x)\\ldots))$\n$x\\rightarrow f'(x)$ (skip connection)\n$y = fact(f_n(\\ldots (f_1(x) \\ldots )) + f'(x))$\nHere, the input x is successively transformed by functions $f_1, f_2,..., f_n$. The original input x or its less transformed format (f'(x)) is then added via a shortcut connection (identity mapping) to the output of the final transformation fn, producing the final output y through the corresponding activation function fact.\nIn this process, the sequence of transformations $f_n(\\ldots (f_1(x)\\ldots))$ constitutes the main computational pathway in ResNet, and we refer to its output as the transformed data (tfd). On the other hand, $f'(x)or f'(x)$, which is either less processed or directly the input, is utilized to facilitate the training of much deeper networks, and we refer to it as the input represent data (ipd) or simply the input.\nThe transformed data refers to the output generated after applying a series of operations such as convolution, batch normalization, and activation functions\u2014on an input within a residual block of a neural network. This data represents the modifications made to the input as it passes through various layers in the block, capturing the learned features and patterns.\nIn a residual block, the transformed data is the result of the main processing path, which typically involves several convolutional layers followed by normalization and activation. This data is then combined with the input represent data (often via addition) to form the output of the residual block, enabling the network to learn more complex functions by effectively adding incremental changes to the input.\nInput represent data. The input represent data refers to the data that is passed directly from the input of a residual block to its output, often without undergoing significant transformation. This data serves as a baseline or identity mapping, allowing the network to retain and propagate the original input features alongside the transformed features from the main processing path.\nIn a residual block, the input represent data typically bypasses the primary convolutional operations and is combined with the transformed data at the block's output. This bypass, or shortcut connection, helps mitigate issues like vanishing gradients by ensuring that gradients can flow more easily through the network, leading to more effective training of deep models.\nThe combination of $f_n(\\ldots (f_1(x)\\ldots)) + f'(x)$ not only facilitates easier propagation of gradients to earlier layers but also impacts the final results differently.\nHowever, the contributions of the input represent data and the transformed data may not be equal. To control the influence of each component, we introduce a weight between the input and the transformed data, referred to as the weight of transformed data and input represent data. This weight is denoted by the variable weighted, where tfd stands for the Transformed Data and ipd stands for the Input Represent Data.\nThis approach forms the foundation of the AdaResNet architecture, a variant of the ResNet architecture that incorporates the weighted to modulate the contribution of the input. AdaResNet is closely related to ResNet; for example, AdaRes-Net50 is based on the ResNet50 architecture but includes this weighted mechanism.\nIn the modified structure, the weight is introduced as shown in Equation (2)\n$x \\rightarrow f_1(x) \\rightarrow f_2(f_1(x)) \\rightarrow\\ldots\\rightarrow f_n(\\ldots (f_1(x)\\ldots))$\n$x\\rightarrow f'(x)$ (skip connection)\n$y = fact(f_n(\\ldots (f_1(x)\\ldots)) + weighted. f'(x))$\nThe parameter we weighted enables the network to learn the optimal influence of the input x on the final output y. If weighted is learned to be close to zero, the network emphasizes the transformed data d over the raw input. Conversely, a larger weighted indicates a greater influence of the raw input. When weighted equals 1, the model functions as a traditional ResNet.\nAdditionally, weighted is automatically adjusted based on the input data. Specifically, it changes dynamically in response to the loss function during training, being updated through the process of gradient descent. We analyze this process in detail in the following section."}, {"title": "A. Gradient Descent Algorithm", "content": "Assuming the loss function is L, the update formula for the parameter weighted during each training step is given by (3).\n$y = tfd + weight^{ipd}_{tfd} \\cdot ipd$\nTaking the partial derivative of y with respect to weighttfd gives:\n$\\frac{\\partial y}{\\partial weight^{ipd}_{tfd}} = ipd$\nAdditionally, we can assign a weight to tfd (the processed intermediary data). However, since this involves a relative relationship between tfd and ipd, we choose to set weightipd relative to ipd.\nBy applying the chain rule, the gradient of the loss function L with respect to weightind is given by:\n$\\frac{\\partial L}{\\partial weight^{ipd}_{tfd}} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial weight^{ipd}_{tfd}}$\nSubstituting the previously computed gradients:\n$\\frac{\\partial L}{\\partial weight^{ipd}_{tfd}} = \\frac{\\partial L}{\\partial y} \\cdot ipd$\nThis gradient demonstrates how changes in weightind will affect the loss function. It is used to update weighted during the optimization step, which will adjust the relative influence between tfd and ipd.\nAlthough this derivation is based on a simplified form of AdaResNet with a single layer contributing to the output (y = tfd + weighted \u00b7 ipd), the same principles apply to the full AdaResNet architecture, which may have multiple layers (e.g., $y = f_{u1}(f_{u2}(... f_{un}(tfd + weight^{ipd}_{tfd} \\cdot ipd))))$).\nDuring the parameter update step, an optimization algorithm (e.g., gradient descent or Adam) uses the computed gradients to update weightfd. For gradient descent, the update rule is:\n$weight^{ipd}_{tfd} \\leftarrow weight^{ipd}_{tfd} -\\eta \\cdot \\frac{\\partial L}{\\partial weight^{ipd}_{tfd}}$\nwhere \u03b7 is the learning rate. This update step is repeated for each batch of training data across multiple epochs, leading to an optimized result from the training data."}, {"title": "B. Training Neural Network with Custom Parameter weighted", "content": "Based on the proposed model and backpropagation mechanism, the training process of AdaResNet is as follows.\n- During the forward pass, the custom layer receives inputs ipd and the intermediate result tfd, and then calculates the output as tfd + weighted \u00b7 ipd. This output is then passed to subsequent layers or serves as the final model output.\n- The model output is compared with the true labels to compute the loss function (assumed to be L)."}, {"title": "C. Brief Explanation", "content": "In this subsection, we briefly explain the rationale for introducing the weight between the Transformed Data and the Input Represent Data.\nIn the equation fn(... (f1(x)...)) + f'(x), f'(x) inherently contributes equally to the output as fn(...(f1(x) . . . )), meaning that both have the same impact on the final output. However, in most cases, we cannot assume this equal contribution. Even within the same scenario, different training data can alter the relationship between these contributions.\nTo formalize this, we introduce a function contrib(p, r) to describe how much a parameter p contributes to the output r. In ResNet, both the input represent data ipd and the transformed data tfd contribute to the recognition target r. However, in general, we cannot assume that contrib(ipd, r) = contrib(tfd, r).\nWe use a counterexample to illustrate the need for variable weighting. Assume that the input data f'(x) has the same weight as the intermediate results. One key feature of ResNet is that it can be extended into many layers. Let us consider two consecutive layers, nand n + 1, and examine the contributions contrib(fn(...(f1(x)) ... and contrib(fn+1(... (f1(x)) . . . )).\nIf contrib(fn(... (f1(x))...)) = contrib(xn) in layer n, where In represents the input of the n layer, then when the process continues to the next layer n+1, the input data is now Xn+1, and the transformed data is fn+1(... (f1(x)) ...). The input data of layer n + 1 is derived from the processed results of layer n, and since 2n has undergone non-linear processing (e.g., through the ReLU activation function) in layer n, it is difficult to maintain a linear one-to-one relationship between the input data and the transformed data. Therefore, there is no guarantee that the contributions will remain equal in layer n + 1, as shown in (5). In fact, as the number of layers increases, it becomes more likely that their contributions will diverge.\nif contrib(fn(... (f1(x)) ... )) = contrib(xn),\nthen contrib(fn+1(... (f1(x)) ...)) \u2260 contrib(xn+1)\nWe conclude, as shown in (5), that in most cases, even if one layer exhibits equal contributions from the input and the transformed data, it is unlikely that all layers will maintain this equality. Consequently, the weights cannot be assumed to be equal across the network.\nTherefore, weighted must be adjusted during the learning process, meaning it should dynamically change throughout training. This dynamic adjustment is crucial for ensuring that the network can effectively capture and utilize relevant features while minimizing the impact of irrelevant or noisy data."}, {"title": "D. Factors influencing weighted", "content": "Several factors influence weighted, including:\n1) Dependency on Training Datasets: The first challenge is that weight+fa can vary significantly depending on the specific training dataset used. Different datasets possess unique distributions and characteristics, necessitating the adaptation of weighted to ensure optimal performance.\n$\n\\frac{contrib(f_n (\\ldots (f_1 (x)) \\ldots ))}{contrib(f_n (x))}\n$\nis related to datatrain\nwhere datatrain represents sub sets of a training dataset.\nMoreover, this ratio often differs when training on different datasets, such as MNIST and CIFAR-10.\n$\n\\frac{contrib(f_n (\\ldots (f_1 (x)) \\ldots ))}{contrib(f_n (x))}\n$\nis related to typetrain\nwhere datatrain represents type of the training datasets.\n2) Neural Network Architecture: The specific neural network architecture also plays a significant role in determining the optimal value of weighted. Networks with varying depths, widths, and connectivity patterns exhibit distinct learning behaviors, thereby affecting the sensitivity and responsiveness of weighted to changes in the training data. Consequently, the dynamic adjustment of weightpd must be tailored to the specific architecture of the neural network in question."}, {"title": "III. VERIFICATION", "content": "To validate the effectiveness of the proposed method, we conducted comparative experiments using three different approaches: (1) the proposed method based on ResNet 50 with a trainable weight, AdaResNet (2) the traditional ResNet 50, and (3) a method using a fixed weight (2x) instead of a trainable one. The results over 10 epochs are reported and discussed."}, {"title": "A. Accuracy", "content": "Experimental Setup: The model was trained and evaluated on the CIFAR-10 dataset with ResNet50, as the accuracy by this model is about 40%, which can have enough space to show whether there are some improvement or not (On the other hand, if we use MNIST dataset, its accuracy can achieve more than 99%, if there are some improvement, it still small). The dataset consists of 60,000 32x32 color images in 10 classes, with 50,000 training images and 10,000 test images. The images were normalized to a range of [0, 1] and the labels were converted to one-hot encoded vectors.\nIn this verification, ResNet and AdaResNet are compared. For ResNet, we use the Keras library of TensorFlow. AdaRes-Net are customed based on the Keras library of TensorFlow too. It is a custom ResNet model modified to incorporate a trainable parameter weightind that scales the input before adding it to an intermediate feature map. This modification is intended to examine the impact of dynamic feature scaling on the performance of the model when applied to the CIFAR-10 dataset. The model is constructed using the Keras framework, and the details of the implementation are outlined below.\nThe implementation includes creating a custom layer within the Keras framework that integrates the trainable parameter weighted. The setup process is summarized in Algorithm 1."}, {"title": "Algorithm 2 Integration of Trainable Parameter weightind in ResNet", "content": "Require: CIFAR-10 dataset D with training set Dtrain and test set Dtest\nEnsure: Trained ResNet model with dynamic parameter weightind f\nNormalize images in Dtrain and Dtest to [0, 1]\nConvert labels in Dtrain and Dtest to one-hot encoding\nDefine Custom Layer with Trainable weightfd\nDefine forward pass as $y = d + weight^{ipd}_{tfd} \\cdot x$\nImport ResNet50 architecture without the top classifica-tion layer\nExtract intermediate feature map d\nApply custom layer to compute y\nUse Adam optimizer with categorical cross-entropy loss"}, {"title": "B. Weights Impact", "content": "In this section, we aim to verify that weightind is a dynamic parameter rather than a fixed value, capable of adapting to different training tasks and varying even within the same task across different training iterations.\nFor a better comparison, we output the weightind after each training is done, i.e. to iterate to output the weightind of each layer, as shown in 3."}, {"title": "C. Limitations of Traditional Residual Networks", "content": "Despite the successes of ResNet and its variants, the uniform treatment of the input (ipd) and processed data (tfd) in skip connections remains a limitation. Traditional ResNet adds ipd and tfd without considering the varying importance of these components across different layers or training data conditions. This uniformity can lead to suboptimal performance, especially in cases where the relative importance of ipd and tfd differs significantly."}, {"title": "D. Our Contribution", "content": "Our proposed AdaResNet builds on this body of work by introducing an adaptive mechanism specifically for skip connections in residual networks. By allowing the ratio of ipd to tfd, represented by the learnable parameter weightind, to be adjusted dynamically during training, AdaResNet provides a more flexible and data-responsive architecture. This approach not only addresses the limitations of traditional ResNet but also leverages the strengths of adaptive learning to enhance performance across a range of tasks and datasets.\nIn summary, while significant progress has been made in the design and optimization of deep neural networks, the uniform treatment of skip connections in residual networks presents a limitation that has yet to be fully addressed. AdaResNet represents a novel contribution in this area, introducing a dynamic and adaptive approach to residual learning that we believe will offer significant benefits in terms of both accuracy and generalization."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced AdaResNet, a novel extension of the ResNet architecture that incorporates an adaptive mechanism for dynamically balancing the contributions of skipped input (ipd) and processed data (tfd). Traditional ResNet models rely on a fixed 1:1 ratio for combining ipd and tfd, which can be suboptimal in various training scenarios. AdaResNet addresses this limitation by introducing a learn-able parameter, weightied, which is automatically optimized during training. This allows the network to adjust the ratio between ipd and tfd in response to the specific characteristics of the data, thereby enhancing the model's adaptability and overall performance.\nOur experimental results demonstrate that AdaResNet consistently outperforms the traditional ResNet architecture, particularly in tasks where the relative importance of ipd and tfd varies across different layers and datasets. We also highlighted the critical insight that the optimal weights for skip connections differ across layers and tasks, challenging the conventional approach of using a uniform weight ratio across the entire network. By leveraging adaptive skip connections, AdaResNet not only improves accuracy and efficiency but also offers a more nuanced and flexible approach to deep network design. This work opens up new possibilities for further exploration of adaptive mechanisms in neural networks, with potential applications across various domains in deep learning.\nFuture work will focus on extending the AdaResNet frame-work to other network architectures and exploring the impact of adaptive mechanisms in different types of neural networks, such as those used in natural language processing and rein-forcement learning. Additionally, we plan to investigate the theoretical underpinnings of adaptive skip connections to bet-ter understand their role in improving network generalization and robustness."}, {"title": "IV. RELATED WORK", "content": "The development of deep neural networks has been one of the most significant advancements in artificial intelligence, with ResNet (Residual Network) standing out as a ground-breaking architecture. Since its introduction by He et al. in 2016 [4], ResNet has become a cornerstone in the design of deep networks, particularly for tasks in computer vision such as image classification, object detection, and segmentation.\nThe concept of residual learning was introduced to address the degradation problem in deep neural networks, where adding more layers to a network does not necessarily lead to better performance and often results in higher training error. ResNet's innovative use of skip connections allows the network to learn residual mappings instead of directly learning unreferenced functions [5]. This approach effectively mitigates the vanishing gradient problem, as gradients can propagate more easily through the network. The original ResNet paper demonstrated that networks with over 100 layers could be trained successfully [6], a feat previously unattainable with traditional deep architectures.\nWhile ResNet has achieved remarkable success, several extensions and modifications have been proposed to further enhance its performance. For example, Wide ResNet [1] [7] explores the effect of increasing the width of the network (i.e., the number of channels) instead of just depth, leading to improved performance on various datasets. Another variation, ResNeXt [8], introduces a cardinality dimension, allowing for a more flexible combination of feature maps, which has been shown to improve accuracy and efficiency.\nThe idea of incorporating adaptive mechanisms into neural networks has gained traction as researchers seek to make models more flexible and responsive to varying data distributions. Squeeze-and-Excitation Networks (SENet) [9], for instance, adaptively recalibrate channel-wise feature responses by explicitly modeling interdependencies between channels. This enables the network to focus on the most informative features, leading to significant performance gains in image recognition tasks.\nAnother line of research focuses on adaptive learning rates and weights within networks. For example, the use of adaptive learning rates in algorithms such as Adam [10] and RMSprop [11] has become standard practice in training deep networks, allowing for faster convergence and better generalization.\nHowever, adaptive mechanisms within the architecture itself, such as the one proposed in our AdaResNet, are less explored. Existing methods typically focus on global adjustments, such as learning rates, rather than on dynamically altering the flow of information within the network. The Dynamic Convolution [12] approach is a notable exception, where convolutional kernels are dynamically adjusted based on input features. However, it does not address the specific challenges posed by skip connections in residual networks."}]}