{"title": "AdaResNet: Enhancing Residual Networks with Dynamic Weight Adjustment for Improved Feature Integration", "authors": ["Hong Su"], "abstract": "In very deep neural networks, gradients can become extremely small during backpropagation, making it challenging to train the early layers. ResNet (Residual Network) addresses this issue by enabling gradients to flow directly through the network via skip connections, facilitating the training of much deeper networks. However, in these skip connections, the input (ipd) is directly added to the transformed data (tfd), treating ipd and tfd equally, without adapting to different scenarios. In this paper, we propose AdaResNet (Auto-Adapting Residual Network), which automatically adjusts the ratio between ipd and tfd based on the training data. We introduce a variable, $weight_{ipd}^{tfd}$, to represent this ratio. This variable is dynamically adjusted during backpropagation, allowing it to adapt to the training data rather than remaining fixed. Experimental results demonstrate that AdaResNet achieves a maximum accuracy improvement of over 50% compared to traditional ResNet.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, deep learning has revolutionized numerous fields, ranging from computer vision and natural language processing to autonomous systems and beyond. Among the various architectures that have emerged, ResNet (Residual Network) has played a pivotal role in advancing the state of the art in these domains [1] [2]. Its innovative design has enabled the training of extremely deep neural networks by addressing a critical challenge faced in traditional deep architectures: the vanishing gradient problem.\nAs neural networks become deeper, gradients can diminish significantly during the backpropagation process. This issue hampers the effective training of the early layers, causing the network to stagnate and preventing it from learning meaningful representations. ResNet tackles this problem by introducing skip connections [3], which allow gradients to bypass inter-mediate layers and flow directly through the network. This mechanism facilitates the training of much deeper networks, making it possible to achieve unprecedented levels of accuracy and performance on complex tasks.\nDespite the success of ResNet, the standard implementation of skip connections involves directly adding the input (ipd) to the transformed data (tfd), i.e., they are combined in a fixed ratio of 1:1. This approach inherently assumes that ipd and tfd contribute equally to the network's output, which may not be optimal across all recognition scenarios. By treating ipd and tfd as identical in their contribution, the traditional ResNet architecture does not account for the varying importance of ipd and tfd across different layers or diverse training data distributions.\nIn this paper, we propose a novel architecture, AdaResNet (Auto-Adapting Residual Network), which enhances the flex-ibility of ResNet by automatically adapting the contribution of ipd and tfd during training. Specifically, we introduce a learnable parameter, denoted as $weight_{ipd}^{tfd}$, which dynamically adjusts the ratio between ipd and tfd based on the training data. Unlike traditional ResNet, where the combination of ipd and t fd remains fixed, AdaResNet allows this ratio to be tuned throughout the training process, thereby improving the network's ability to generalize across diverse data distributions.\nThe contributions of this paper are threefold: (1) Introduction of AdaResNet: We present AdaResNet, a novel extension of the ResNet architecture that incorporates an adaptive mech-anism to balance the contributions of skipped input (ipd) and processed data (tfd). This approach overcomes the limitations of the fixed 1:1 ratio combination used in traditional ResNet, allowing for more flexible and effective integration of ipd and tfd. (2) Learnable parameter $weight_{ipd}^{tfd}$: We propose a new learnable parameter, $weight_{ipd}^{tfd}$, which is automatically optimized during training. This parameter enables the network to dynamically adjust the balance between ipd and tfd in response to varying data characteristics, improving the model's adaptability and performance. (3)Layer-specific and"}, {"title": "II. MODEL", "content": "In very deep networks, gradients can become extremely small during backpropagation, making it difficult to train the early layers. ResNet (Residual Network) addresses this challenge by allowing gradients to flow directly through the network via skip connections, facilitating the training of much deeper networks.\nThe process of transforming input data (x) to produce output (y) in a traditional ResNet can be described as in (1)\n$x \\rightarrow f_1(x) \\rightarrow f_2(f_1(x)) \\rightarrow\\ldots\\rightarrow f_n(. . . (f_1(x)...))$\n$x\\rightarrow f'(x) (skip connection)$\n$y = fact(f_n(... (f_1(x) . . . )) + f'(x))$    (1)\nHere, the input x is successively transformed by functions $f_1, f_2,..., f_n$. The original input x or its less transformed format ($f'(x)$) is then added via a shortcut connection (identity mapping) to the output of the final transformation $f_n$, produc-ing the final output y through the corresponding activation function fact.\nIn this process, the sequence of transformations $f_n(... (f_1(x)...))$ constitutes the main computational pathway in ResNet, and we refer to its output as the transformed data (tfd). On the other hand, $f'(x)or f'(x)$, which is either less processed or directly the input, is utilized to facilitate the training of much deeper networks, and we refer to it as the input represent data (ipd) or simply the input.\nDefinition 1: Transformed data. The transformed data refers to the output generated after applying a series of opera-tions\u2014such as convolution, batch normalization, and activa-tion functions\u2014on an input within a residual block of a neural network. This data represents the modifications made to the input as it passes through various layers in the block, capturing the learned features and patterns.\nIn a residual block, the transformed data is the result of the main processing path, which typically involves several convolutional layers followed by normalization and activation. This data is then combined with the input represent data (often via addition) to form the output of the residual block, enabling the network to learn more complex functions by effectively adding incremental changes to the input.\nDefinition 2: Input represent data. The input represent data refers to the data that is passed directly from the input of a residual block to its output, often without undergoing significant transformation. This data serves as a baseline or identity mapping, allowing the network to retain and propagate the original input features alongside the transformed features from the main processing path.\nIn a residual block, the input represent data typically bypasses the primary convolutional operations and is combined with the transformed data at the block's output. This bypass, or shortcut connection, helps mitigate issues like vanishing gradients by ensuring that gradients can flow more easily through the network, leading to more effective training of deep models.\nThe combination of $f_n(... (f_1(x)...)) + f'(x)$ not only facilitates easier propagation of gradients to earlier layers but also impacts the final results differently.\nHowever, the contributions of the input represent data and the transformed data may not be equal. To control the influence of each component, we introduce a weight between the input and the transformed data, referred to as the weight of transformed data and input represent data. This weight is denoted by the variable $weighted_{ipd}^{tfd}$, where tfd stands for the Transformed Data and ipd stands for the Input Represent Data.\nThis approach forms the foundation of the AdaResNet architecture, a variant of the ResNet architecture that incorpo-rates the $weighted_{ipd}^{tfd}$ to modulate the contribution of the input. AdaResNet is closely related to ResNet; for example, AdaRes-Net50 is based on the ResNet50 architecture but includes this weighted mechanism.\nIn the modified structure, the weight is introduced as shown in Equation (2)\n$x \\rightarrow f_1(x) \\rightarrow f_2(f_1(x)) \\rightarrow\\ldots\\rightarrow f_n(. . . (f_1(x)...))$\n$x \\rightarrow f'(x) (skip connection)$\n$y = fact(f_n(... (f_1(x)...)) + weighted_{ipd}^{tfd}\u00b7 f'(x))$    (2)\nThe parameter $weighted_{ipd}^{tfd}$ enables the network to learn the optimal influence of the input x on the final output y. If $weighted_{ipd}^{tfd}$ is learned to be close to zero, the network empha-sizes the transformed data d over the raw input. Conversely, a larger $weighted_{ipd}^{tfd}$ indicates a greater influence of the raw input. When $weighted_{ipd}^{tfd}$ equals 1, the model functions as a traditional ResNet.\nAdditionally, $weight_{ipd}^{tfd}$ is automatically adjusted based on the input data. Specifically, it changes dynamically in response to the loss function during training, being updated through the process of gradient descent. We analyze this process in detail in the following section."}, {"title": "A. Gradient Descent Algorithm", "content": "Assuming the loss function is L, the update formula for the parameter $weighted_{ipd}^{tfd}$ during each training step is given by (3).\n$y = tfd + weight_{ipd}^{tfd} \u00b7 ipd$\nTaking the partial derivative of y with respect to $weight_{ipd}^{tfd}$ gives:\n$\\frac{\\partial y}{\\partial weight_{ipd}^{tfd}} = ipd$\nAdditionally, we can assign a weight to tfd (the processed intermediary data). However, since this involves a relative relationship between tfd and ipd, we choose to set $weight_{ipd}^{tfd}$ relative to ipd.\n3) Gradient of the Loss Function with Respect to $weighted_{ipd}^{tfd}$: By applying the chain rule, the gradient of the loss function L with respect to $weighted_{ipd}^{tfd}$ is given by:\n$\\frac{\\partial L}{\\partial weight_{ipd}^{tfd}} = \\frac{\\partial L}{\\partial y} \u00b7 \\frac{\\partial y}{\\partial weight_{ipd}^{tfd}}$\nSubstituting the previously computed gradients:\n$\\frac{\\partial L}{\\partial weight_{ipd}^{tfd}} = \\frac{\\partial L}{\\partial y} \u00b7 ipd$\nThis gradient demonstrates how changes in $weight_{ipd}^{tfd}$ will affect the loss function. It is used to update $weighted_{ipd}^{tfd}$ during the optimization step, which will adjust the relative influence between tfd and ipd.\nAlthough this derivation is based on a simplified form of AdaResNet with a single layer contributing to the output (y = tfd + $weighted_{ipd}^{tfd}$ ipd), the same principles apply to the full AdaResNet architecture, which may have multiple layers (e.g., y = $fu_1(fu_2(... fun(tfd + weighted_{ipd}^{tfd}\u00b7 ipd))))$.\n4) Parameter Update: During the parameter update step, an optimization algorithm (e.g., gradient descent or Adam) uses the computed gradients to update $weight_{ipd}^{tfd}$. For gradient descent, the update rule is:\n$weight_{ipd}^{tfd} \\rightarrow weight_{ipd}^{tfd} -\\eta \\frac{\\partial L}{\\partial weight_{ipd}^{tfd}}$\nwhere \u03b7 is the learning rate. This update step is repeated for each batch of training data across multiple epochs, leading to an optimized result from the training data."}, {"title": "B. Training Neural Network with Custom Parameter $weighted_{ipd}^{tfd}$", "content": "Based on the proposed model and backpropagation mecha-nism, the training process of AdaResNet is as follows.\n1) Forward Pass of $weighted_{ipd}^{tfd}$: - During the forward pass, the custom layer receives inputs ipd and the intermediate result tfd, and then calculates the output as $tfd + weighted_{ipd}^{tfd} \u00b7 ipd$. This output is then passed to subsequent layers or serves as the final model output.\n2) Calculating the Loss Function: - The model output is compared with the true labels to compute the loss function (assumed to be L)."}, {"title": "C. Brief Explanation", "content": "In this subsection, we briefly explain the rationale for introducing the weight between the Transformed Data and the Input Represent Data.\nIn the equation $f_n(... (f_1(x)...)) + f'(x)$, f'(x) inher-ently contributes equally to the output as $f_n(...(f_1(x) . . . ))$, meaning that both have the same impact on the final output. However, in most cases, we cannot assume this equal contri-bution. Even within the same scenario, different training data can alter the relationship between these contributions.\nTo formalize this, we introduce a function contrib(p, r) to describe how much a parameter p contributes to the output r. In ResNet, both the input represent data ipd and the transformed data tfd contribute to the recognition target r. However, in general, we cannot assume that contrib(ipd, r) = contrib(tfd, r).\nWe use a counterexample to illustrate the need for vari-able weighting. Assume that the input data f'(x) has the same weight as the intermediate results. One key feature of ResNet is that it can be extended into many layers. Let us consider two consecutive layers, nand n + 1, and examine the contributions contrib($f_n(...(f_1(x)) ... and contrib($f_{n+1}(...(f_1(x)) . . . ))$.\nIf contrib($f_n(...(f_1(x))...)) = contrib($x_n$) in layer n, where $I_n$ represents the input of the n layer, then when the process continues to the next layer n+1, the input data is now $X_{n+1}$, and the transformed data is $f_{n+1}(...(f_1(x)) ...)$. The input data of layer n + 1 is derived from the processed results of layer n, and since $X_n$ has undergone non-linear processing (e.g., through the ReLU activation function) in layer n, it is difficult to maintain a linear one-to-one relationship between the input data and the transformed data. Therefore, there is no guarantee that the contributions will remain equal in layer n + 1, as shown in (5). In fact, as the number of layers increases, it becomes more likely that their contributions will diverge.\nif contrib($f_n(... (f_1(x)) ... )) = contrib($x_n$),\nthen contrib($f_{n+1}(...(f_1(x)) ...)) \u2260 contrib($x_{n+1}$).    (5)\nWe conclude, as shown in (5), that in most cases, even if one layer exhibits equal contributions from the input and the transformed data, it is unlikely that all layers will maintain this equality. Consequently, the weights cannot be assumed to be equal across the network.\nTherefore, $weighted_{ipd}^{tfd}$ must be adjusted during the learning process, meaning it should dynamically change throughout training. This dynamic adjustment is crucial for ensuring that the network can effectively capture and utilize relevant features while minimizing the impact of irrelevant or noisy data."}, {"title": "D. Factors influencing $weighted_{ipd}^{tfd}$", "content": "Several factors influence $weighted_{ipd}^{tfd}$, including:\n1) Dependency on Training Datasets: The first challenge is that $weight_{ipd}^{tfd}$ can vary significantly depending on the specific training dataset used. Different datasets possess unique distributions and characteristics, necessitating the adaptation of $weighted_{ipd}^{tfd}$ to ensure optimal performance.\n$\\frac{contrib(f_n (... (f_1(x)) ... ))}{contrib(f_n(x))}$ is related to $datatrain$  (6)\nwhere datatrain represents sub sets of a training dataset.\nMoreover, this ratio often differs when training on different datasets, such as MNIST and CIFAR-10.\n$\\frac{contrib(f_n (... (f_1(x)) ... ))}{contrib(f_n(x))}$ is related to $typetrain$   (7)\nwhere datatrain represents type of the training datasets.\n2) Neural Network Architecture: The specific neural net-work architecture also plays a significant role in determining the optimal value of $weighted_{ipd}^{tfd}$. Networks with varying depths, widths, and connectivity patterns exhibit distinct learning behaviors, thereby affecting the sensitivity and responsiveness of $weighted_{ipd}^{tfd}$ to changes in the training data. Consequently, the dynamic adjustment of $weight_{ipd}^{tfd}$ must be tailored to the specific architecture of the neural network in question."}, {"title": "III. VERIFICATION", "content": "To validate the effectiveness of the proposed method, we conducted comparative experiments using three different ap-proaches: (1) the proposed method based on ResNet 50 with a trainable weight, AdaResNet (2) the traditional ResNet 50, and (3) a method using a fixed weight (2x) instead of a trainable one. The results over 10 epochs are reported and discussed."}, {"title": "A. Accuracy", "content": "1) Experimental Setup: The model was trained and evalu-ated on the CIFAR-10 dataset with ResNet50, as the accuracy by this model is about 40%, which can have enough space to show whether there are some improvement or not (On the other hand, if we use MNIST dataset, its accuracy can achieve more than 99%, if there are some improvement, it still small). The dataset consists of 60,000 32x32 color images in 10 classes, with 50,000 training images and 10,000 test images. The images were normalized to a range of [0, 1] and the labels were converted to one-hot encoded vectors.\nIn this verification, ResNet and AdaResNet are compared. For ResNet, we use the Keras library of TensorFlow. AdaRes-Net are customed based on the Keras library of TensorFlow too. It is a custom ResNet model modified to incorporate a trainable parameter $weight_{ipd}^{tfd}$ that scales the input before adding it to an intermediate feature map. This modification is intended to examine the impact of dynamic feature scaling on the performance of the model when applied to the CIFAR-10 dataset. The model is constructed using the Keras framework, and the details of the implementation are outlined below.\nThe implementation includes creating a custom layer within the Keras framework that integrates the trainable parameter $weighted_{ipd}^{tfd}$. The setup process is summarized in Algorithm 1."}, {"title": "B. Weights Impact", "content": "In this section, we aim to verify that $weight_{ipd}^{tfd}$ is a dynamic parameter rather than a fixed value, capable of adapting to different training tasks and varying even within the same task across different training iterations.\nFor a better comparison, we output the $weight_{ipd}^{tfd}$ after each training is done, i.e. to iterate to output the $weight_{ipd}^{tfd}$ of each layer, as shown in 3."}, {"title": "C. Limitations of Traditional Residual Networks", "content": "Despite the successes of ResNet and its variants, the uniform treatment of the input (ipd) and processed data (tfd) in skip connections remains a limitation. Traditional ResNet adds ipd and tfd without considering the varying importance of these components across different layers or training data conditions. This uniformity can lead to suboptimal performance, espe-cially in cases where the relative importance of ipd and tfd differs significantly."}, {"title": "D. Our Contribution", "content": "Our proposed AdaResNet builds on this body of work by introducing an adaptive mechanism specifically for skip connections in residual networks. By allowing the ratio of ipd to tfd, represented by the learnable parameter $weight_{ipd}^{tfd}$, to be adjusted dynamically during training, AdaResNet provides a more flexible and data-responsive architecture. This approach not only addresses the limitations of traditional ResNet but also leverages the strengths of adaptive learning to enhance performance across a range of tasks and datasets.\nIn summary, while significant progress has been made in the design and optimization of deep neural networks, the uniform treatment of skip connections in residual networks presents a limitation that has yet to be fully addressed. AdaResNet represents a novel contribution in this area, introducing a dynamic and adaptive approach to residual learning that we believe will offer significant benefits in terms of both accuracy and generalization."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced AdaResNet, a novel exten-sion of the ResNet architecture that incorporates an adaptive mechanism for dynamically balancing the contributions of skipped input (ipd) and processed data (tfd). Traditional ResNet models rely on a fixed 1:1 ratio for combining ipd and tfd, which can be suboptimal in various training scenarios. AdaResNet addresses this limitation by introducing a learn-able parameter, $weight_{ipd}^{tfd}$, which is automatically optimized during training. This allows the network to adjust the ratio between ipd and tfd in response to the specific characteristics of the data, thereby enhancing the model's adaptability and overall performance.\nOur experimental results demonstrate that AdaResNet con-sistently outperforms the traditional ResNet architecture, par-ticularly in tasks where the relative importance of ipd and tfd varies across different layers and datasets. We also high-lighted the critical insight that the optimal weights for skip connections differ across layers and tasks, challenging the conventional approach of using a uniform weight ratio across the entire network. By leveraging adaptive skip connections, AdaResNet not only improves accuracy and efficiency but also offers a more nuanced and flexible approach to deep network design. This work opens up new possibilities for further exploration of adaptive mechanisms in neural networks, with potential applications across various domains in deep learning.\nFuture work will focus on extending the AdaResNet frame-work to other network architectures and exploring the impact of adaptive mechanisms in different types of neural networks, such as those used in natural language processing and rein-forcement learning. Additionally, we plan to investigate the theoretical underpinnings of adaptive skip connections to bet-ter understand their role in improving network generalization and robustness."}]}