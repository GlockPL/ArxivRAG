{"title": "End-to-end Semantic-centric Video-based Multimodal Affective Computing", "authors": ["Ronghao Lin", "Ying Zeng", "Sijie Mai", "Haifeng Hu"], "abstract": "In the pathway toward Artificial General Intelligence (AGI), understanding human's affection is essential to enhance machine's cognition abilities. For achieving more sensual human-AI interaction, Multimodal Affective Computing (MAC) in human-spoken videos has attracted increasing attention. However, previous methods are mainly devoted to designing multimodal fusion algorithms, suffering from two issues: semantic imbalance caused by diverse pre-processing operations and semantic mismatch raised by inconsistent affection content contained in different modalities comparing with the multimodal ground truth. Besides, the usage of manual features extractors make they fail in building end-to-end pipeline for multiple MAC downstream tasks. To address above challenges, we propose a novel end-to-end framework named SemanticMAC to compute multimodal semantic-centric affection for human-spoken videos. We firstly employ pre-trained Transformer model in multimodal data pre-processing and design Affective Perceiver module to capture unimodal affective information. Moreover, we present a semantic-centric approach to unify multimodal representation learning in three ways, including gated feature interaction, multi-task pseudo label generation, and intra-/inter-sample contrastive learning. Finally, SemanticMAC effectively learn specific- and shared-semantic representations in the guidance of semantic-centric labels. Extensive experimental results demonstrate that our approach surpass the state-of-the-art methods on 7 public datasets in four MAC downstream tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "ULTIMODAL Affective Computing (MAC) aims at predicting the sentiment polarity, emotion class, or behavioral intention by comprehensively integrating information from different modalities of speakers such as textual (utterance), acoustic (human voice) and visual (facial expression, head movement, body gesture) modality in a human-centric video [1], [2]. With the surge of human-spoken content on social media platforms, research on multimodal affective computing has become crucial in the community of multimodal learning [3]. Considering various application purposes, multimodal affective computing is divided into diverse specific tasks, including multimodal sentiment analysis [4]\u2013[6], multimodal emotion recognition [7]\u2013[9], multimodal humor and sarcasm detection [10], [11].\nAffective computing are originated from conventional Natural Language Processing (NLP) tasks referring to understanding the affection contained in human-spoken utterances and conversations [1], [12], [13]. The performance of affection-related algorithms highly relies on semantic information [14] and are mostly improved by exploring the abundant semantic context embedded in language models. Nevertheless, immoderate reliance on language may easily overfit on subjective affective components, resulting in biased prediction [15], [16]. Thus, auxiliary features from other modalities, such as audio and image, are introduced to enhance affective understanding with multimodal learning [3]. In previous MAC methods, unlike textual features learned by language models, acoustic and visual features are mostly extracted by manual pre-processing toolkit such as CMU-MultimodalSDK\u00b9 [5], [17]\u2013[20], due to the information sparsity and inherent noise in audio and image. However, conducting multimodal learning with manual features may raise issues as shown in Figure 1.\nOn the one hand, the vague description of pre-processing causes the extraction of manual features hard to reproduce [6], introducing inevitable gap between training and inference stages for multimodal learning. Besides, the manual feature extractors such as COVAREP [21] and Facet [22] are untrainable, which brings difficulty in developing end-to-end multimodal learning pipeline and affects the generalization of the pre-trained models in various downstream scenarios.\nOn the other hand, due to the demand of semantic context for MAC task, the manual features such as facial landmarks for visual features and Mel-frequency cepstral coefficients for acoustic features, are not efficiently suitable for affection-related tasks. Lack of semantic information, such low-level features lead to poor embedding performance comparing with textual modality [6], [23], [24] and bring semantic imbalance"}, {"title": null, "content": "issue in multimodal learning. Since the scale of language models is increasing rapidly, the number of trainable parameters for other modalities are much smaller than the ones for textual modality for MAC models, which further exacerbates the semantic imbalance for various modalities.\nTo better understand the issue of semantic imbalance for different modalities, we visualize the contribution of the unimodal features for the fusion multimodal representations in Figure 2. Inspired by but diverse from [25], [26], we compute the Precision-Recall (PR) curve for the feature distributions between the unimodal and multimodal representations, taking the representations from state-of-the-art multimodal sentiment analysis models [17], [23], [27]\u2013[29] as examples.\nAs shown in Figure 2(a), we can observe that the manual acoustic and visual features contribute similarly when utilizing low-level textual features such as Glove [30] which computes word vector based on global word co-occurrence counts statistics. However, when we substitute the textual features with BERT [31] which embeds high-level semantic context by pretrained language model in Figure 2(b)-2(f), the contributions of manual acoustic and visual features drop significantly to the multimodal representations compared with the one of textual features, no matter in which models. Although the existing fusion strategies [5] may adjust the contributions of different unimodal representations adaptively, they fail in balancing the contributions of different modalities, mostly due to the inherent discrepancy of semantic abundance from various unimodal representations.\nMoreover, we remove features from each modality input in traversal manner as MissModal [29] to construct unimodal, bimodal and trimodal representations, and then compute the PR-curve among the distributions of these representations as shown in Figure 2(f). The bimodal representations with textual features contribute more than the representations with acoustic and visual features solely or both, which further indicates that the introduction of textual features can effectively increase the semantic information to the fusion multimodal representations.\nFrom the visualization in Figure 2, we can conclude that existing low-level manual acoustic and visual features are no longer appropriate for high-level textual features embedded by context-based language model. The difference of semantic abundance from various modalities causes the issue of semantic imbalance and affects the multimodal fusion process, leading to an urgent need of new solutions for unimodal feature extraction of acoustic and visual modalities.\nIn addition, different modalities may bring diverse affective intensities or classes for MAC task [15], [19], [32], meaning that the affection semantics of various modalities may not remain consistent in the same video. Previous methods categorize unimodal features into modality-specific and shared features to deal with such semantic inconsistency circumstance [23], [24], [33], [34]. However, they utilize final multimodal ground truth labels to jointly supervise representations learning, confusing the training of modality-specific features with different affection as shown in Figure 1. We summarize this issue as semantic mismatch raised by inconsistent semantics among unimodal features and the corresponding multimodal ground truth labels. Moreover, as interpreted in Du et al."}, {"title": null, "content": "[35], multimodal joint training easily suffer from modality laziness, which makes the model neglect the learning of modality-specific features regardless of the paired features. Therefore, relying solely on annotated multimodal labels as the supervision is insufficient for multimodal learning [27]. Particularly for MAC task, it is crucial to explore the unimodal semantics contained in various modalities and enhance nuanced comprehension of fine-grained affection in multimodal learning, ensuring more precise prediction without bias [36]. The case study in Table I further reveals that the MAC task require individual supervision signals to capture the affective semantic information for various modalities.\nAiming at addressing the challenges of semantic imbalance and mismatch, we propose a novel Semantic-centric Multimodal Affective Computing framework, named SemanticMAC, to learn multimodal representations in the semantic space for various video-based MAC tasks in an end-to-end"}, {"title": null, "content": "manner. Firstly, we utilize powerful pre-trained Transformer models [37] instead of manual features to extract unimodal features of different modalities from the raw videos. Such pre-processing operation ensures the end-to-end training and inference of the multimodal learning model. In order to reduce the modality heterogeneity and generalize to various scenarios for MAC task, we unify the embedding form for diverse input modalities according to the temporal sequences. Inspired by the thought of positional embedding [38], we utilize learnable frame embedding to denote videos with different frame lengths, which enhances the performance when dealing with varying length human-spoken videos. To further collect taffective information, we design a module named affective perceiver to process the features into fixed number of learnable tokens in the latent space, meanwhile filtering the noisy content contained in the generic acoustic and visual features.\nThen, we conduct Semantic-centric Gated Feature Interaction (SGFI) inside and among the unimodal features from various modalities by bridge attention and gated mechanism to extract specific- and shared-semantic representations. The former representations explore the intra-modal dynamics for affection-specific knowledge and the latter ones integrates the"}, {"title": null, "content": "cross-modal interaction in the semantic space. Lastly, the task predictor utilize the final mutlimodal representations to output \u0177M, which serves as the sentiment scores in regression task or as the emotion classes in recognition and detection task."}, {"title": "B. Architecture Overview", "content": "The architecture of the proposed SemanticMAC processing raw video in an end-to-end manner is depicted as Figure 3. Aiming at avoiding the issue of semantic imbalance from root, we firstly take pre-trained Transformer models instead of manual toolkits to pre-process unimodal data into embeddings with consistent form for various modalities. The unimodal embeddings Xu are multiple tokens in $R^{f_u \\times d_u}$ where fu denotes the numbers of tokens and du denotes the representation dimension of modality u \u2208 {t,a,v}. Then, for acoustic and visual modalities, we design the affective perceiver to integrate affective information contained in the generic unimodal embeddings and transfer the knowledge into multiple learnable tokens $F_a$ and $F_v$ with fixed length, which enable the flexible handling of videos with different lengths at the same time. Besides, the acoustic and visual features are summed with learnable frame embedding $E_{fr}$ to attend by relative temporal order of various frames of the video. While for textual modality, the pre-trained language model is utilized to learn the affective textual representation Ft. Note that we freeze the pre-processed encoders for acoustic and visual modalities and the tokenizer for textual modality during the training stage while update the parameters of affective perceivers and language model as fine-tuning paradigm. Next, to make the model distinguish different modalities in latter modules, the unimodal representations Fu are attached with learnable modality embeddings $E_{md}$ according to the corre-"}, {"title": null, "content": "sponding type of modality. In addition, we conduct semanticcentric feature interaction among various unimodal representations to learn semantic-specific representations ($F_{t}^{sp}$, $F_{a}^{sp}$, $F_{v}^{sp}$) and semantic-shared representations ($F_{t}^{sh}$, $F_{a}^{sh}$, $F_{v}^{sh}$) for each modality, which further address semantic imbalance issue induced by overfitting on the dominant modality. Specifically, we design the interaction mechanism as gated multi-head intra- and cross-attention to efficiently capture intra-modal dynamics and explore cross-modal commonality. To focus on the learning of query modality at one time, we measure the attention score with multi-query setting in each interaction. Additionally, to reduce the impact of the modality gap introduced by modality heterogeneity, we utilize a set of bridge tokens to interact the information between query and key modalities with massively diverse distributions. Lastly, we concatenate the semantic-specific representations ($F_{t}^{sp}$, $F_{a}^{sp}$, $F_{v}^{sp}$) and semantic-shared representations $F_{sh}$ and then project them into multimodal representations FM, which are fed to the task predictor to output the final affective prediction \u0177M.\nTargeting at the issue of semantic mismatch raised by various contents of each modality, we tend to utilize different semantic-centric labels as the supervision for different features in a multi-task training manner, which competently guides the learning of unimodal and multimodal representations in the semantic space. According to semantic attributes, we divide the training of representations into five sub-tasks denoted as * \u2208 {M,S,T, A, V}, including the sub-tasks of multimodal representations (M), semantic-shared representations (S) and semantic-specific representations for each modality (T, A, V). Most datasets only manually annotate the multimodal groundtruth labels $y_{gt}$ for multimodal representations FM in sub-task"}, {"title": null, "content": "M [5], [36]. Due to this, we consider to generate pseudo labels p\u2217 according to the fine-grained level of semantics for other representations ($F_{t}^{sp}$, $F_{a}^{sp}$, $F_{v}^{sp}$, $F_{sh}$) as a weaklysupervised strategy compared with the ground-truth annotations. By calculating the similarity ranking matrix for each type of representation in the feature space, the pseudo labels are then generated by scaling and shifting the corresponding ground-truth labels of k-nearest neighborhood samples. Besides, we stabilize the generation process of the pseudo labels in a momentum-based updating policy as the training epoch increases. Supervised by the ground truth labels and pseudo labels, the multi-task predictors take the unimodal and multimodal representations as the input and output the affective prediction \u0177 for each sub-task. Moreover, we perform semanticcentric contrastive learning in the level of intra-sample and inter-sample at the unit hypersphere [65] to further enhance the convergence of multimodal representations learning. The former one pulls closer the semantic-shared representations of all modalities inside the same video sample and pushes away the semantic-specific representations for each modality, which encourage the decoupling of semantic information for unimodal features. While the latter one constructs positive and negative pairs based on the ground-truth affective category for the multimodal representations among different samples. More technical details are introduced in the following subsections."}, {"title": "C. Unimodal Feature Extraction", "content": "To unify the pre-processing of various modalities, we adopt Transformer-based models to extract unimodal features. As shown in Figure 3, the text data $I_t$ are firstly processed to tokens Xt by tokenizer according to the specific language models [31], [66], [67] in particular downstream task. Note that our framework is suitable for various language model, which is latter validated in the experiments. Then, we utilize the pre-trained language model to learn the textual representations Ft, which is formulated as:\n$X_t = Tokenizer(I_t) \\in \\mathbb{R}^{f_t}$\n$F_t = LanguageModel(X_t;\\theta_t) \\in \\mathbb{R}^{f_t \\times d_t}$                                                                                                        (1)\nThe pre-trained language model are set in a fine-tuning paradigm where the parameters $ \\theta_t $ are updated during the training stage.\nWhile for the audio data Ia and video data Iv, where the general upper limit of the micro-expression duration is observed as 1/2 seconds [68], we uniformly sample the audio and vision stream at 2 frames per second, efficiently reducing the input data volume and the model inference time. Next, the sampled audio and video frames are directly fed into the frozen pre-trained encoders of ImageBind [63] to jointly learn acoustic embedding Xa and visual embedding X, which stack all [CLS] tokens from each sampled frame of the stream, formulated as:\n$I'_u = UniformSample(I_u) \\in \\mathbb{R}^{f_u \\times c_u}, \\kappa \\in {\\alpha, \\upsilon}$\n$X_f = ImageBind(I'_u;\\theta_u)[CLS] \\in \\mathbb{R}^{d_u}, f \\in [1, f_u]$\n$X_u = Stack[X_1... X_f] \\in \\mathbb{R}^{ f_u \\times d_u}$             (2)"}, {"title": null, "content": "For acoustic and visual modalities, given video stream with fu frames, the unimodal embeddings extracted by the corresponding unimodal encoders of ImageBind are represented as $X_f^u$, where u \u2208 {a,v} and f\u2208 [1, fu]. Firstly, as the positional embedding in language model [31], we sum the unimodal embeddings $X_u$ with learnable frame embeddings $E_{fr} \\in \\mathbb{R}^d$ to increase relative temporal order to the module when conducting cross-frame attention, represented as:\n$X_u = X_u + E_{fr}, u \\in {a,v}$                                                                                                              (3)\nThen, we innovate unimodal learnable tokens $L_u \\in \\mathbb{R}^{n \\times d_u}$ with fixed length n for individual modality aiming at collecting affective features in the generic unimodal embeddings. Due to the excellent performance of attention mechanism [37],"}, {"title": null, "content": "we design a multi-layer Transformer-based module named as Affective Perceiver by adopting multi-head attention (MHA) and feed forward network (FFN) in each layer. For u \u2208 {a, v}, the Affective Perceiver gradually encourages the affective information of the unimodal embeddings Xu to flow to the learnable tokens Lu. By constructing query, key and value as Q = Lu, K = V = Concat[Xu, Lu], the computations of each Affective Perceiver layer are formulated as follows:\n$L_u = MHA(LN(Q, K, V)) + L_u$\n$L_u = FFN(LN(L_u)) + L_u$                                                                                                                    (4)\nwhere layer normalization (LN) and residual connections are employed around each of the sub-layers. Note that Lu is initialized randomly and the output of the last layer is taken as the unimodal representations $F_u \\in \\mathbb{R}^{n \\times d_u}$. The effectiveness of such fixed number of leanrbale tokens in content abstraction for various modalities have been proved in recent researches [54], [71], [72]. Moreover, the unimodal learnable tokens Lu in the Affective Perceiver can not only integrate the most useful information for downstream tasks while removing irrelevant noise, but empower the model with the ability to align acoustic and visual modalities with different language model in the feature space. With the introduction of Affective Perceiver, the proposed framework is capable of processing video with various frame length and extracting affective unimodal features competently, which further address the issue of semantic imbalance as shown in Figure 1."}, {"title": "D. Semantic-centric Feature Interaction", "content": "After the extraction of unimodal features, the essential question of multimodal learning has become how to interact various type of information from different modalities and conduct multimodal fusion with huge modality gap. In the perspective of semantic, we decouple the feature space into semantic-specific and semantic-shared features, where the former features focus on modal-specific semantic information according to the contents of diverse modalities while the latter ones integrates the invariant commonalities among all modalities. Such feature disentanglement strategy is intuitive and works successfully with theoretical interpretability [23], [34], [73]. Diverse from previous researches, we propose Semanticcentric Gated Feature Interaction (SGFI) to learn semanticspecific and -shared representations by the designed bridge attention and gated mechanism, which effectively transfer intra- and cross-modal knowledge through bridge tokens and filter the irrelevant features by weighted activation layer.\nAs shown in Figure 5, inspired by VilT [38], each unimodal representation Fu is firstly summed with a learnable modality embedding $E_{md} \\in \\mathbb{R}^d$, which indicates the modality type for the module to distinguish corresponding representation in latter interaction, which is formulated as:\n$F_u = F_u + E_{md}, u \\in {t,a,v}$                                                                              (5)\nSimilar as self-attention [37], cross-attention has been proved competent in aligning different input data as query and key/value, respectively [74]\u2013[76]. However, due to the huge modality gap and delicate modality relationship, the interaction"}, {"title": null, "content": "in multimodal fusion for multimodal affective computing is far more complicated than simply multimodal alignment [5]. Therefore, we improve the cross-attention mechanism in three ways for SGFI module, named Gated Bridge Attention (GBA), to adapt at the complex multimodal fusion:\n1) Multi-query Attention: We adopt multi-query attention [51], [77] to primarily excavate various semantics inside query vectors, which accelerate the convergence of mutlimodal learning and lower the memory-bandwidth requirements concurrently. Specifically, we utilize multi-head projection $W^q$ for query vectors while maintain a single head of key/value vectors which share the same weights in the linear projection $W^y$ for each head of query vectors, formulated as:\n$Q_h = QW^q \\in \\mathbb{R}^{n \\times d_{head}}, h \\in [1, head]$\n$K_h = V_h = KWY = VWY \\in \\mathbb{R}^{n \\times d_{head}}$                                                                                                             (6)\nwhere head denotes the number of heads, $d_{head} = d_c/head$ denotes the dimension of each head and de is set as the common dimension for each representation.\n2) Bridge Token: Aiming at bridging the modality gap among various modalities in the semantic space and conducting efficient feature interaction, we introduce Bridge Tokens with fixed m tokens (m < n) as bottleneck to restrict the intra- and cross-attention flow, inspired by the thought of information bottleneck [78]\u2013[80]. The Bridge Tokens B are obtained by aggregating features in adaptive"}, {"title": null, "content": "average pooling based on semantics from query vectors:\n$Q' = Concat[Q_1... Q_{head}] \\in \\mathbb{R}^{n \\times d_c}$\n$K' = V' = Repeat(K_h) \\in \\mathbb{R}^{n \\times d_c}$\n$B = AdaptiveAvgPool(Q') \\in \\mathbb{R}^{m \\times d_c}$                                         (7)\nThen, scaling down by $ \\sqrt{d_c} $, the attention matrix is computed as:\n$BridgeAttn(Q, K,V) = Softmax(\\frac{(Q'B^T)(BK')}{\\sqrt{d_c}}) V$                                                                       (8)\n3) Gated ReLU: To filter the redundancy according to the semantic of individual representations, we adopt the gated mechanism between each attention and feed forward sublayer by Rectified Linear Unit (ReLU) [81], which has been proved suitable for Transformer models due to its activation sparsity and inference efficiency [82], [83]. Thus, for u \u2208 {t, a, v}, the computation in GBA is finally formulated as:\n$F_u = ReLU(BridgeAttn(Q, K, V)) + F_u$\n$F_u = ReLU(FFN(F_u)) + F_u$                                                                                                                                                                                  (9)\nThe SGFI module is conducted by stacking multiple GBA attention layers and outputs semantic-centric representations according to the input modality, which are denoted as pu (\u00b7) for semantic-specific feature interaction and hu(\u00b7) for semanticshared feature interaction.\nOn the one hand, to capture intra-modal dynamics and filter affective-unrelated noise, we take unimodal representations from the same modality to construct the input query and key/value for the SGFI module, which are denoted as Q = K = V = Fu. Then, the semantic-specific representations $F_{sp}^u$ can be computed as:\n$F_{sp}^u = AvgPool(p_u(F_u)) \\in \\mathbb{R}^{d_c}, u \\in {t, a, v}$                                                                                                      (10)\nOn the other hand, to effectively fuse knowledge among different modalities and incorporate the affective commonalities, given the input query as Q = Fu from arbitrary modality, we set the key/value as K = V = Concat[Fu', Fu'", "as": "n$F_{sh} = AvgPool(h_u(F_t, F_a, F_v)) \\in \\mathbb{R}^{d_c}$\n$F_{sh} = Concat[F_{sh}^t, F_{sh}^a, F_{sh}^v] \\in \\mathbb{R}^{3d_c}$                                                                                                                                         (11)\nFinally, to summarize the semantic-specific and -shared information from various modalities, the multimodal representations FM are formulated as:\n$F_M = Concat[F_{sp}^t, F_{sp}^a, F_{sp}^v, F_{sh}^t, F_{sh}^a, F_{sh}^v] \\in \\mathbb{R}^{6d_c}$                 (12)"}, {"title": "E. Semantic-centric Label Generation", "content": "For the learning of various semantic-specific representations $F_{sp}^u$ and semantic-shared representations $F_{sh}$, the supervision should be produced according to the semantics information. However, due to the absence of unimodal labels in most datasets, most of previous work [23], [34] directly utilize the multimodal ground truth labels to supervise the learning process of features with various semantic, which are"}, {"title": null, "content": "essentially contrary with the thought of disentangled representation learning. Besides, the affection expressed through single modality can be quite diverse, which is concluded as the semantic mismatch issue as shown in Figure 1. Aiming at addressing this issue, we present Semantic-centric Label Generation (SCLG) to construct pseudo label space based on semantics as the weak supervision strategy to improve the learning of semantic-centric representations.\nSpecifically, we deem the learning processes of representations F with various semantics as distinct sub-tasks * \u2208 {S, T, A, V}, which denote the sub-task of semantic-shared representations $F_{sh}$ and semantic-specific representations $F_{up}$ for textual, audio and visual modalities. Each subtask should be trained under the guidance of the corresponding semnaticcentric pseudo labels. Inspired by Yu et.al [27], the semanticcentric labels p\u2217 are assumed to share the distribution space with multimodal ground truth labels Ygt. Thus, we utilize the common semantics contained in the representations across various samples and their ground truth labels to generate the pseudo specific- and shared-semantic labels as shown in Figure 3.\nGiven a query of representations $B = {F_i}^B_{i=1}$, we conduct k-Nearest Neighbor (k-NN) algorithm to find the K most nearest samples {$F_k$}$^K_{k=1}$ (K < B) for each representation $F_i$ by comparing the similarity in the feature space and then output the euclidean distance matrix D\u2217 between each sample and the nearest samples, denoted as:\n{$F_k$}$^K_{k=1}$ = k-NN($F_i$; $F_1$ ... $F_B$) \u2208 $R^{d_c}, i \\in [1, B], k \\in [1, K]$\nD\u2217 = ($D_{ik}$), where $D_{ik} = \\sqrt{\\frac{1}{d_c} \\sum^{d_c}_{j=1} (F_{ij}-F_{kj})^2}$                                                                             (13)\nwhere the dimension of the representations de is utilized as a scaling factor to mitigate the adverse effect of excessive distance. The distance matrix D\u2217 represents the similarity of various representations, indicating the relationship among different samples at the level of specific or shared semantics.\nFor each sub-task, to transfer the knowledge of multimodal ground truth labels ygt to the semantic-centric pseudo labels p*, we design a scaling map to control the transferring magnitude related to semantics abundance and a shifting map to decide the direction and value of the label movement \u0394. Therefore, we intuitively construct pseudo labels by considering the distance matrix D\u2217 in the Gaussian potential kernel form function [84] as the scaling map, and the difference between multimodal labels $y_{st}^k$ and p of the corresponding nearest samples as the shifting map, which is formulated as:\n$\\Delta_{i}^* = \\frac{1}{K} \\sum^{K}_{k=1} exp(\\frac{-D_{ik}^{scale}}{w})  (y_{st}^k - p^k)$                                                                         (14)\nwhere \u0394\u2217 denotes the varying value for pseudo label $p_i^*$ of sample i in sub-task *. Moreover, the pseudo labels are initialized as the corresponding multimodal ground truth labels and updated in a momentum manner by combining the computed"}, {"title": null, "content": "movement and history values with the increasing of training epochs z, represented as:\n$p_i^* \\mid^{0} = p_i^* \\mid^{1} = ... = p_i^* \\mid^{r-1} = Y_{gt}, r>1$\n$p_i^* \\mid^{iz} = \\frac{1}{2} - \\frac{1}{z-1} p_i^* \\mid^{iz-1} + \\frac{1}{z-1} (p_i^* \\mid^{iz-1} + \\Delta_i^*), z>r$     (15)\n$p_i^* \\mid^{iz} = \\frac{1}{2} - \\frac{1}{z-1} p_i^* \\mid^{iz-1} + \\frac{1}{z-1} (p_i^* \\mid^{iz-1} + \\Delta_i^*), z>r$\nwhere the momentum-based updating policy is intended to relieve the fluctuations caused by noisy samples and the updating process of pseudo labels is started after the rth epoch for more stable label generation and better convergence.\nFor each subtask, the specific-semantic labels are generated to reveal the intra-modal connections among different samples while the shared-semantic labels are expected to show the inter-modal commonality. Comparing with the multimodal ground truth labels, the generated pseudo labels are used to guide the learning of various semantic-centric representations in a weakly-supervised manner. Note that the semantic-centric pseudo labels are allowed to be zero for individual samples when there are few unimodal features or rare paired features related to the downstream prediction [14], [35]."}, {"title": "F. Semantic-centric Contrastive Learning", "content": "To promote the disentanglement of semantics and encourage the feature interaction of unimodal and multimodal sub-tasks, we conduct Semantic-centric Contrastive Learning (SCCL) among various modalities inside and across different samples. Previous works [39], [85] utilizes cross-modal contrastive learning directly on unimodal representations suffering from the huge modality gap [64]. Diversely, SCCL is presented in the perspective of intra- and inter-sample at the semantic space, where the modality gap has been mitigated by SGFI.\nAs suggested by Wang et al. [65], we firstly employ $L2$-normalization on all representations for both intra- and inter-sample contrastive learning to restrict the contrastive learning space on the unit hypersphere, as shown in Figure 3. We implement the intra-sample contrastive learning among semantic-specific representations $F_{up}^t$ and semantic-shared representations $F_{sh}^u$ for u \u2208 {t,a,v}, which efficiently decouple the semantic-specific features and the consistent information contained in each modality u. Given {$F_{up}^t$, $F_{up}^a$} from each sample i, the goals are pushing away $F_{up}^u$ and pulling closer $F_{sh}^u$ from different modalities, and pushing apart $F_{up}^t$ and $F_{sh}^a$ according to the semantics inside the corresponding representations. Thus, we construct positive pairs as {$F_{sh}^u$; $F_{sh}^u$}, and the negative pairs as {$F_{up}^t$; $F_{sh}^u$} and {$F_{up}^u$; $F_{up}^a$}, and adopt dot-product similarity between the query and key in the pairs, formulated as:\n$sim(F_{query}, F_{key}) = \\frac{1}{2} ( \\frac{F_{query} F_{key}}{\\parallel F_{query} \\parallel} + \\frac{F_{query} F_{key}}{\\parallel F_{query} \\parallel})$\n$\\S^+ = \\sum_{u \\ne u'} exp(sim(F_{sh}, F_{sh})/\\tau)$\n$\\S^- = \\sum_{u \\ne u'} exp(sim(F_{up}, F_{up})/\\tau) + \\sum exp(sim(F_{up}, F_{sh})/\\tau)$                                          (16)\nwhere \u03c4 serves as a temperature hyper-parameter for altering"}, {"title": null, "content": "the strength of penalties on hard samples due to the modality gap [86]. Then we take InfoNCE [56] form function to compute the intra-sample contrastive learning loss LintracL, which is formulated as:\n$L_{intraCL} = -E_{(F_{sp}, F_{sh}) \\thicksim B} log \\frac{S^+}{S^+ + S^-}$                                                                                                                 (17)\nSimultaneously, the inter-sample contrastive learning is adopted for the multimodal representations FM among diverse samples under the supervision of multimodal ground truth labels to further excavate the affective information inspired by SupCon [60]. Given a mini-batch of B = {F}$^B_{i=1}$, we divide the representations into positive and negative sets according to the labels annotated as sentiment scores or emotion classes. For sentiment analysis task, we categorize the representations based on sentiment classes with a label threshold which decide the class each sentiment scores belongs to. While for emotion recognition and detection classes, we treat the representations with the same class as the positive pairs while the other representations as the negative pairs. Note that such setting is suitable for multi-label emotion recognition dataset [4], where we treat the representations with non-empty intersection set of emotion annotations as the positive pairs. To make the representations from various classes more discriminative with the guidance of multimodal labels, denoting positive pairs sets as P \u2208 {F}, the inter-sample contrastive learning loss LintercL is computed as:\n$L_{interCL} = -E_{F_M \\thicksim B} log \\frac{\\sum_{j,j' \\in P} exp(sim(F_i, F_{i'})/v)}{\\sum_{j,q \\in B} exp(sim(F_i, F_{i'})/v)}$                                                                                    (18)\nwhere v is another temperature hyper-parameter to regulate the probability distribution over diverse instance samples [87].\nCombining the intra- and inter-sample contrastive learning losses, the final semantic-centric contrastive learning loss LCL is computed as:\n$L_{CL} = \\alpha L_{intraCL} + \\beta L_{interCL}$                                                                                                                                 (19)\nwhere \u03b1 and \u03b2 are hyper-parameters to adjust the contribution of each loss in the semantic-centric contrastive learning."}, {"title": "G. Optimization Objective", "content": "Regarding the learning of multimodal and semantic-centric representations as multi-task training paradigm where subtask * \u2208 {M,S,T, A, V}, we utilize various multi-layer perceptron (MLPs) as the mutli-task predictors to output the corresponding affective predictions, formulated as:\n$\\hat{y}_M = MLP(F_M;\\theta_M) \\in \\mathbb{R}$\n$\\hat{y}_* = MLP(F_*;\\theta_*) \\in \\mathbb{R}^r, F_* \\in {F_{sh}, F_{sp}^t, F_{sp}^a, F_{sp}^v}$                                      (20)\nwhere r = 1 denotes the sentiment scores as for sentiment analysis, r = class denotes the number of emotion classes for recognition and r = 2 denotes the binary classification for detection task.\nAlong with the guidance of multimodal ground truth labels"}, {"title": null, "content": "$y_{gt}$ and the supervision of the generated semantic-centic"}, {"title": null, "content": "pseudo labels p\u2217 for each sub-task", "as": "n$L_{task}^{L"}]}