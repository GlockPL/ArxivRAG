{"title": "End-to-end Semantic-centric Video-based Multimodal Affective Computing", "authors": ["Ronghao Lin", "Ying Zeng", "Sijie Mai", "Haifeng Hu"], "abstract": "In the pathway toward Artificial General Intelligence (AGI), understanding human's affection is essential to enhance machine's cognition abilities. For achieving more sensual human-AI interaction, Multimodal Affective Computing (MAC) in human-spoken videos has attracted increasing attention. However, previous methods are mainly devoted to designing multimodal fusion algorithms, suffering from two issues: semantic imbalance caused by diverse pre-processing operations and semantic mismatch raised by inconsistent affection content contained in different modalities comparing with the multimodal ground truth. Besides, the usage of manual features extractors make they fail in building end-to-end pipeline for multiple MAC downstream tasks. To address above challenges, we propose a novel end-to-end framework named SemanticMAC to compute multimodal semantic-centric affection for human-spoken videos. We firstly employ pre-trained Transformer model in multimodal data pre-processing and design Affective Perceiver module to capture unimodal affective information. Moreover, we present a semantic-centric approach to unify multimodal representation learning in three ways, including gated feature interaction, multi-task pseudo label generation, and intra-/inter-sample contrastive learning. Finally, SemanticMAC effectively learn specific- and shared-semantic representations in the guidance of semantic-centric labels. Extensive experimental results demonstrate that our approach surpass the state-of-the-art methods on 7 public datasets in four MAC downstream tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "ULTIMODAL Affective Computing (MAC) aims at predicting the sentiment polarity, emotion class, or behavioral intention by comprehensively integrating information from different modalities of speakers such as textual (utterance), acoustic (human voice) and visual (facial expression, head movement, body gesture) modality in a human-centric video [1], [2]. With the surge of human-spoken content on social media platforms, research on multimodal affective computing has become crucial in the community of multimodal learning [3]. Considering various application purposes, multimodal affective computing is divided into diverse specific tasks, including multimodal sentiment analysis [4]\u2013[6], multimodal emotion recognition [7]\u2013[9], multimodal humor and sarcasm detection [10], [11].\nAffective computing are originated from conventional Natural Language Processing (NLP) tasks referring to understanding the affection contained in human-spoken utterances and conversations [1], [12], [13]. The performance of affection-related algorithms highly relies on semantic information [14] and are mostly improved by exploring the abundant semantic context embedded in language models. Nevertheless, immoderate reliance on language may easily overfit on subjective affective components, resulting in biased prediction [15], [16]. Thus, auxiliary features from other modalities, such as audio and image, are introduced to enhance affective understanding with multimodal learning [3]. In previous MAC methods, unlike textual features learned by language models, acoustic and visual features are mostly extracted by manual pre-processing toolkit such as CMU-MultimodalSDK\u00b9 [5], [17]\u2013[20], due to the information sparsity and inherent noise in audio and image. However, conducting multimodal learning with manual features may raise issues as shown in Figure 1.\nOn the one hand, the vague description of pre-processing causes the extraction of manual features hard to reproduce [6], introducing inevitable gap between training and inference stages for multimodal learning. Besides, the manual feature extractors such as COVAREP [21] and Facet [22] are untrainable, which brings difficulty in developing end-to-end multimodal learning pipeline and affects the generalization of the pre-trained models in various downstream scenarios.\nOn the other hand, due to the demand of semantic context for MAC task, the manual features such as facial landmarks for visual features and Mel-frequency cepstral coefficients for acoustic features, are not efficiently suitable for affection-related tasks. Lack of semantic information, such low-level features lead to poor embedding performance comparing with textual modality [6], [23], [24] and bring semantic imbalance"}, {"title": "issue in multimodal learning. Since the scale of language models is increasing rapidly, the number of trainable parameters for other modalities are much smaller than the ones for textual modality for MAC models, which further exacerbates the semantic imbalance for various modalities.\nTo better understand the issue of semantic imbalance for different modalities, we visualize the contribution of the unimodal features for the fusion multimodal representations in Figure 2. Inspired by but diverse from [25], [26], we compute the Precision-Recall (PR) curve for the feature distributions between the unimodal and multimodal representations, taking the representations from state-of-the-art multimodal sentiment analysis models [17], [23], [27]\u2013[29] as examples.\nAs shown in Figure 2(a), we can observe that the manual acoustic and visual features contribute similarly when utilizing low-level textual features such as Glove [30] which computes word vector based on global word co-occurrence counts statistics. However, when we substitute the textual features with BERT [31] which embeds high-level semantic context by pre-trained language model in Figure 2(b)-2(f), the contributions of manual acoustic and visual features drop significantly to the multimodal representations compared with the one of textual features, no matter in which models. Although the existing fusion strategies [5] may adjust the contributions of different unimodal representations adaptively, they fail in balancing the contributions of different modalities, mostly due to the inherent discrepancy of semantic abundance from various unimodal representations.\nMoreover, we remove features from each modality input in traversal manner as MissModal [29] to construct unimodal, bimodal and trimodal representations, and then compute the PR-curve among the distributions of these representations as shown in Figure 2(f). The bimodal representations with textual features contribute more than the representations with acoustic and visual features solely or both, which further indicates that the introduction of textual features can effectively increase the semantic information to the fusion multimodal representations.\nFrom the visualization in Figure 2, we can conclude that existing low-level manual acoustic and visual features are no longer appropriate for high-level textual features embedded by context-based language model. The difference of semantic abundance from various modalities causes the issue of semantic imbalance and affects the multimodal fusion process, leading to an urgent need of new solutions for unimodal feature extraction of acoustic and visual modalities.\nIn addition, different modalities may bring diverse affective intensities or classes for MAC task [15], [19], [32], meaning that the affection semantics of various modalities may not remain consistent in the same video. Previous methods categorize unimodal features into modality-specific and shared features to deal with such semantic inconsistency circumstance [23], [24], [33], [34]. However, they utilize final multimodal ground truth labels to jointly supervise representations learning, confusing the training of modality-specific features with different affection as shown in Figure 1. We summarize this issue as semantic mismatch raised by inconsistent semantics among unimodal features and the corresponding multimodal ground truth labels. Moreover, as interpreted in Du et al.", "title2": "II. RELATED WORK", "content": "A. Multimodal Affective Computing\nAs a sub-field of multimodal learning, the key question of Multimodal Affective Computing (MAC) is summarized as how to extract semantic-rich unimodal features and effectively fuse the affective related information from each modality to learn multimodal representations [2], [42], [43]. Therefore, developing pipeline to conduct MAC contains two main aspects: unimodal feature extraction and multimodal fusion [1], [3].\nCompared with the traditional low-level hand-craft features [21], [44], [45], unimodal features extracted by deep learning based models have achieved impressive performance for diverse modalities when applied in different fields. Particularly, unimodal pre-trained models consist of Transformer [37], such as BERT [31] and GPT [46] for text in natural language processing, ViT [47] for image in computer vision, and HuBERT [48] for audio in speech processing, are capable of learning efficient unimodal representations and generalizing to various downstream tasks in the pre-train and fine-tuning paradigm. Additionally, multimodal fusion focuses on jointly integrating information from diverse modalities to perform affective prediction [2]. Gkoumas et al. [5] and Geetha et al. [9] have provided comprehensive surveys on the current multimodal fusion techniques for MAC, which have attained remarkable results while still suffered from the huge modality gap and the issues of semantic imbalance and mismatch.\nThe MAC task consists of multiple affective prediction downstream tasks, including 1) multimodal sentiment analysis [4]\u2013[6] to compute a continuous score as the sentiment polarity of utterance in a regression method; 2) multimodal emotion recognition [7]\u2013[9] to classify the emotion class of the utterance in monologue or conversation; 3) multimodal humor and sarcasm detection [10], [11] to identify whether the utterance contains the humorous or satirical intent. Previous methods address single task according to distinctive forms of input data and objective functions. Differently in this paper, we present one unifying framework to effectively adopt these downstream tasks, providing a unique insight for future research.\nB. Attention Mechanism\nCurrent multi-head attention mechanism is mostly based on Transformer [37], named self-attention, which presents normalized scaled dot-product among the input query, key and value from the same input sequence. Multiple variants of attention mechanism have been proposed to adapt in distinct scenarios, such as linear attention [49] to reduce the inference computation from quadratic complexity into linear one, cross-attention [50] to process different input, and mutli-query attention [51] to decrease the model parameters and the key/value cache and so on.\nMultimodal learning with attention mechanism has been exploited extensively in previous researches [52]. Whisper [53] trains a robust speech recognition model by cross-attention with a large-scale web text-audio data as a weakly supervised datasets in a multi-task training approach. Flamingo [54] presents perceiver resampler to convert varying-size large feature maps to fixed visual tokens and interact these tokens with textual data by masked attention layers in the vision-language pre-training. However, most of them leverage the attention mechanism to construct cross-modal connection instantly, ignoring the different interaction manners and various supervision of the fine-grained features in the semantic space.\nC. Contrastive Learning\nContrastive learning focuses on dividing the samples into positive and negative pairs sets and adjusting the similarity of the corresponding representations [55]. The most popular form of contrastive loss function is InfoNCE, which is utilized to encode underlying shared latents by maximizing the lower bound of the mutual information [56]. As a pretext task, contrastive learning is initially adopted at unimodal models in an unsupervised manner [57]\u2013[59], and then extended to supervised methods and multimodal models due to its great effectiveness and generality. Supcon [60] leverages label information to conduct contrastive learning in a fully-supervised setting. Recent works such as CLIP [39], ALIGN [61] and wav2vec2.0 [62] and so on have claimed the better cross-modal alignment performance with contrastive objectives. Particularly, ImageBind [63] extend contrastive learning into the joint embedding space across six modalities. Nevertheless, most of them lack exploration into multimodal fusion and reveal significant modality gap [64], which is imperatively needed to be addressed."}, {"title": "III. METHODOLOGY", "content": "The proposed SemanticMAC is presented in detail in this section. We first define the input and output of MAC task and clarify the corresponding notations. Then we introduce the end-to-end architecture of the proposed framework. Next, we put forward the extraction process of unimodal features for different modalities. Following the semantic-centric thought, the module of gated feature interaction and the strategy of label generation are described additionally. Finally, we formulate the total optimization objective with the semantic-centric contrastive learning loss and the individual task prediction losses.\nA. Problem Definition\nMultimodal Affective Computing (MAC) concentrates on learning efficient representations to conduct various regression or classification tasks for affective analysis from the multimodal signals contained in a human-spoken video. To unify diverse downstream MAC tasks, we formulate the multimodal input of the raw videos as $I_u \\in \\mathbb{R}^{l_u \\times c_u}$, where $l_u$ denotes the temporal length of utterance sequence and $c_u$ denotes various contents of unimodal signal at the sampled timestep of the video. Particularly, since each video clip contains at least an utterance spoken by one human with facial expression, head movement and body gesture, $u \\in \\{t, a, v\\}$ represents the textual, acoustic and visual modalities respectively [3]. According to the semantics contained in various modalities, the proposed SemanticMAC processes each modality of the raw multimodal data to unimodal representations $F_u$ and then integrates the affective information into multimodal representations $F_M$ by cross-modal interaction in the semantic space. Lastly, the task predictor utilize the final mutlimodal representations to output $\\hat{y}_M$, which serves as the sentiment scores in regression task or as the emotion classes in recognition and detection task.\nB. Architecture Overview\nThe architecture of the proposed SemanticMAC processing raw video in an end-to-end manner is depicted as Figure 3. Aiming at avoiding the issue of semantic imbalance from root, we firstly take pre-trained Transformer models instead of manual toolkits to pre-process unimodal data into embeddings with consistent form for various modalities. The unimodal embeddings $X_u$ are multiple tokens in $\\mathbb{R}^{f_u \\times d_u}$ where $f_u$ denotes the numbers of tokens and $d_u$ denotes the representation dimension of modality $u \\in \\{t,a,v\\}$. Then, for acoustic and visual modalities, we design the affective perceiver to integrate affective information contained in the generic unimodal embeddings and transfer the knowledge into multiple learnable tokens $F_a$ and $F_v$ with fixed length, which enable the flexible handling of videos with different lengths at the same time. Besides, the acoustic and visual features are summed with learnable frame embedding $E_{fr}$ to attend by relative temporal order of various frames of the video. While for textual modality, the pre-trained language model is utilized to learn the affective textual representation $F_t$. Note that we freeze the pre-processed encoders for acoustic and visual modalities and the tokenizer for textual modality during the training stage while update the parameters of affective perceivers and language model as fine-tuning paradigm. Next, to make the model distinguish different modalities in latter modules, the unimodal representations $F_u$ are attached with learnable modality embeddings $E_{md}$ according to the corresponding type of modality. In addition, we conduct semantic-centric feature interaction among various unimodal representations to learn semantic-specific representations ($F_{t}^{sp}$, $F_{a}^{sp}$, $F_{v}^{sp}$) and semantic-shared representations ($F_{t}^{sh}$, $F_{a}^{sh}$, $F_{v}^{sh}$) for each modality, which further address semantic imbalance issue induced by overfitting on the dominant modality. Specifically, we design the interaction mechanism as gated multi-head intra- and cross-attention to efficiently capture intra-modal dynamics and explore cross-modal commonality. To focus on the learning of query modality at one time, we measure the attention score with multi-query setting in each interaction. Additionally, to reduce the impact of the modality gap introduced by modality heterogeneity, we utilize a set of bridge tokens to interact the information between query and key modalities with massively diverse distributions. Lastly, we concatenate the semantic-specific representations ($F_{t}^{sp}$, $F_{a}^{sp}$, $F_{v}^{sp}$) and semantic-shared representations $F_{sh}$ and then project them into multimodal representations $F_M$, which are fed to the task predictor to output the final affective prediction $\\hat{y}_M$.\nTargeting at the issue of semantic mismatch raised by various contents of each modality, we tend to utilize different semantic-centric labels as the supervision for different features in a multi-task training manner, which competently guides the learning of unimodal and multimodal representations in the semantic space. According to semantic attributes, we divide the training of representations into five sub-tasks denoted as * \u2208 {M,S,T, A, V}, including the sub-tasks of multimodal representations (M), semantic-shared representations (S) and semantic-specific representations for each modality (T, A, V). Most datasets only manually annotate the multimodal ground-truth labels $y_{gt}$ for multimodal representations $F_M$ in sub-task"}, {"title": "M [5], [36]. Due to this, we consider to generate pseudo labels $p_*$ according to the fine-grained level of semantics for other representations ($F_{t}^{sp}$, $F_{a}^{sp}$, $F_{v}^{sp}$, $F_{sh}$) as a weakly-supervised strategy compared with the ground-truth annotations. By calculating the similarity ranking matrix for each type of representation in the feature space, the pseudo labels are then generated by scaling and shifting the corresponding ground-truth labels of k-nearest neighborhood samples. Besides, we stabilize the generation process of the pseudo labels in a momentum-based updating policy as the training epoch increases. Supervised by the ground truth labels and pseudo labels, the multi-task predictors take the unimodal and multimodal representations as the input and output the affective prediction $\\hat{y}_*$ for each sub-task. Moreover, we perform semantic-centric contrastive learning in the level of intra-sample and inter-sample at the unit hypersphere [65] to further enhance the convergence of multimodal representations learning. The former one pulls closer the semantic-shared representations of all modalities inside the same video sample and pushes away the semantic-specific representations for each modality, which encourage the decoupling of semantic information for unimodal features. While the latter one constructs positive and negative pairs based on the ground-truth affective category for the multimodal representations among different samples. More technical details are introduced in the following subsections.\nC. Unimodal Feature Extraction\nTo unify the pre-processing of various modalities, we adopt Transformer-based models to extract unimodal features. As shown in Figure 3, the text data $I_t$ are firstly processed to tokens $X_t$ by tokenizer according to the specific language models [31], [66], [67] in particular downstream task. Note that our framework is suitable for various language model, which is latter validated in the experiments. Then, we utilize the pre-trained language model to learn the textual representations $F_t$, which is formulated as:\n$X_t = Tokenizer(I_t) \\in \\mathbb{R}^{f_t}$\n$F_t = LanguageModel(X_t;\\theta_t) \\in \\mathbb{R}^{f_t \\times d_t}$ (1)\nThe pre-trained language model are set in a fine-tuning paradigm where the parameters $\\theta_t$ are updated during the training stage.\nWhile for the audio data $I_a$ and video data $I_v$, where the general upper limit of the micro-expression duration is observed as 1/2 seconds [68], we uniformly sample the audio and vision stream at 2 frames per second, efficiently reducing the input data volume and the model inference time. Next, the sampled audio and video frames are directly fed into the frozen pre-trained encoders of ImageBind [63] to jointly learn acoustic embedding $X_a$ and visual embedding $X_v$, which stack all [CLS] tokens from each sampled frame of the stream, formulated as:\n$I'_u = UniformSample(I_u) \\in \\mathbb{R}^{f_u \\times c_u}, \\quad \\kappa \\in \\{a, v\\}$\n$X_f = ImageBind(I'_u;\\theta_u)[CLS] \\in \\mathbb{R}^{d_u}, f \\in [1, f_u]$ (2)\n$X_u = Stack[X_1 ... X_{f_u}] \\in \\mathbb{R}^{f_u \\times d_u}$\nwhere $d_u$ denotes the encoders parameters and the [CLS] token are taken as the global embedding to aggregate the contents of each frame [39], [63], [69]. Note that we leverage the power of ImageBind for its excellent performance in aligning different multimodal data in the joint embedding space [70].\nAlthough ImageBind has been proved effectively in multimodal alignment, the extracted unimodal embeddings are coarse-grained and generic in the embedding space, containing massive task-unrelated noise and affective-irrelevant information. Besides, directly utilize [CLS] embeddings as the unimodal representations cause the model lack of temporal interaction for each modality. Thus, we design an extra module named Affective Perceiver to further learn fine-grained unimodal features and explore affective dynamics by interacting the [CLS] embeddings across frames as shown in Figure 4."}, {"title": "Affective Perceiver\nFor acoustic and visual modalities, given video stream with $f_u$ frames, the unimodal embeddings extracted by the corresponding unimodal encoders of ImageBind are represented as $X_u^f$, where $u \\in \\{a,v\\}$ and $f \\in [1, f_u]$. Firstly, as the positional embedding in language model [31], we sum the unimodal embeddings $X_u^f$ with learnable frame embeddings $E_{fr} \\in \\mathbb{R}^{d_u}$ to increase relative temporal order to the module when conducting cross-frame attention, represented as:\n$X_u^f = X_u^f + E_{fr}, u \\in \\{a,v\\}$ (3)\nThen, we innovate unimodal learnable tokens $L_u\\in \\mathbb{R}^{n\\times d_u}$ with fixed length $n$ for individual modality aiming at collecting affective features in the generic unimodal embeddings. Due to the excellent performance of attention mechanism [37], we design a multi-layer Transformer-based module named as Affective Perceiver by adopting multi-head attention (MHA) and feed forward network (FFN) in each layer. For $u \\in \\{a, v\\}$, the Affective Perceiver gradually encourages the affective information of the unimodal embeddings $X_u$ to flow to the learnable tokens $L_u$. By constructing query, key and value as $Q = L_u$, $K = V = Concat[X_u, L_u]$, the computations of each Affective Perceiver layer are formulated as follows:\n$L_u = MHA(LN(Q, K, V)) + L_u$\n$L_u = FFN(LN(L_u)) + L_u$ (4)\nwhere layer normalization (LN) and residual connections are employed around each of the sub-layers. Note that $L_u$ is initialized randomly and the output of the last layer is taken as the unimodal representations $F_u \\in \\mathbb{R}^{n\\times d_u}$. The effectiveness of such fixed number of leanrbale tokens in content abstraction for various modalities have been proved in recent researches [54], [71], [72]. Moreover, the unimodal learnable tokens $L_u$ in the Affective Perceiver can not only integrate the most useful information for downstream tasks while removing irrelevant noise, but empower the model with the ability to align acoustic and visual modalities with different language model in the feature space. With the introduction of Affective Perceiver, the proposed framework is capable of processing video with various frame length and extracting affective unimodal features competently, which further address the issue of semantic imbalance as shown in Figure 1.\nD. Semantic-centric Feature Interaction\nAfter the extraction of unimodal features, the essential question of multimodal learning has become how to interact various type of information from different modalities and conduct multimodal fusion with huge modality gap. In the perspective of semantic, we decouple the feature space into semantic-specific and semantic-shared features, where the former features focus on modal-specific semantic information according to the contents of diverse modalities while the latter ones integrates the invariant commonalities among all modalities. Such feature disentanglement strategy is intuitive and works successfully with theoretical interpretability [23], [34], [73]. Diverse from previous researches, we propose Semantic-centric Gated Feature Interaction (SGFI) to learn semantic-specific and -shared representations by the designed bridge attention and gated mechanism, which effectively transfer intra- and cross-modal knowledge through bridge tokens and filter the irrelevant features by weighted activation layer.\nAs shown in Figure 5, inspired by VilT [38], each unimodal representation $F_u$ is firstly summed with a learnable modality embedding $E_{md} \\in \\mathbb{R}^{d_c}$, which indicates the modality type for the module to distinguish corresponding representation in latter interaction, which is formulated as:\n$F_u = F_u + E_{md}, u \\in \\{t,a,v\\}$ (5)\nSimilar as self-attention [37], cross-attention has been proved competent in aligning different input data as query and key/value, respectively [74]\u2013[76]. However, due to the huge modality gap and delicate modality relationship, the interaction in multimodal fusion for multimodal affective computing is far more complicated than simply multimodal alignment [5]. Therefore, we improve the cross-attention mechanism in three ways for SGFI module, named Gated Bridge Attention (GBA), to adapt at the complex multimodal fusion:\n1) Multi-query Attention: We adopt multi-query attention [51], [77] to primarily excavate various semantics inside query vectors, which accelerate the convergence of mutlimodal learning and lower the memory-bandwidth requirements concurrently. Specifically, we utilize multi-head projection $W_q^h$ for query vectors while maintain a single head of key/value vectors which share the same weights in the linear projection $W_y$ for each head of query vectors, formulated as:\n$Q^h = QW_q^h \\in \\mathbb{R}^{n \\times d_{head}}, h \\in [1, head]$\n$K^h = V^h = KWY = VWY \\in \\mathbb{R}^{n \\times d_{head}}$ (6)\nwhere head denotes the number of heads, $d_{head} = d_c / head$ denotes the dimension of each head and $d_c$ is set as the common dimension for each representation.\n2) Bridge Token: Aiming at bridging the modality gap among various modalities in the semantic space and conducting efficient feature interaction, we introduce Bridge Tokens with fixed m tokens (m < n) as bottleneck to restrict the intra- and cross-attention flow, inspired by the thought of information bottleneck [78]\u2013[80]. The Bridge Tokens B are obtained by aggregating features in adaptive"}, {"title": "average pooling based on semantics from query vectors:\n$Q' = Concat[Q^1... Q^{head}] \\in \\mathbb{R}^{n \\times d_c}$\n$K' = V' = Repeat(K^h) \\in \\mathbb{R}^{n \\times d_c}$\n$B = AdaptiveAvgPool(Q') \\in \\mathbb{R}^{m \\times d_c}$ (7)\nThen, scaling down by $\\sqrt{d_c}$, the attention matrix is computed as:\n$BridgeAttn(Q, K,V) = Softmax(\\frac{(Q'B^T)(BK')}{\\sqrt{d_c}}) V'$ (8)\n3) Gated ReLU: To filter the redundancy according to the semantic of individual representations, we adopt the gated mechanism between each attention and feed forward sublayer by Rectified Linear Unit (ReLU) [81], which has been proved suitable for Transformer models due to its activation sparsity and inference efficiency [82], [83]. Thus, for $u \\in \\{t, a, v\\}$, the computation in GBA is finally formulated as:\n$F_u = ReLU(BridgeAttn(Q, K, V)) + F_u$\n$F_u = ReLU(FFN(F_u)) + F_u$ (9)\nThe SGFI module is conducted by stacking multiple GBA attention layers and outputs semantic-centric representations according to the input modality, which are denoted as $p_u(\\cdot)$ for semantic-specific feature interaction and $h_u(\\cdot)$ for semantic-shared feature interaction.\nOn the one hand, to capture intra-modal dynamics and filter affective-unrelated noise, we take unimodal representations from the same modality to construct the input query and key/value for the SGFI module, which are denoted as $Q = K = V = F_u$. Then, the semantic-specific representations $F_{u}^{sp}$ can be computed as:\n$F_{u}^{sp} = AvgPool(p_u(F_u)) \\in \\mathbb{R}^{d_c}, u \\in \\{t, a, v\\}$ (10)\nOn the other hand, to effectively fuse knowledge among different modalities and incorporate the affective commonalities, given the input query as $Q = F_u$ from arbitrary modality, we set the key/value as $K = V = Concat[F_{u'}, F_{u''}]$ which is the concatenation of the other unimodal representations. Thus, the semantic-shared representations $F_{sh}$ are formulated as:\n$F_{sh} = AvgPool(h_u(F_t, F_a, F_v)) \\in \\mathbb{R}^{d_c}$\n$F_{sh} = Concat[F_{sh}^{t}, F_{sh}^{a}, F_{sh}^{v}] \\in \\mathbb{R}^{3d_c}$ (11)\nFinally, to summarize the semantic-specific and -shared information from various modalities, the multimodal representations $F_M$ are formulated as:\n$F_M = Concat[F_{t}^{sp}, F_{a}^{sp}, F_{v}^{sp}, F_{sh}^{t}, F_{sh}^{a}, F_{sh}^{v}] \\in \\mathbb{R}^{6d_c}$ (12)\nE. Semantic-centric Label Generation\nFor the learning of various semantic-specific representations $F_{u}^{sp}$ and semantic-shared representations $F_{sh}$, the supervision should be produced according to the semantics information. However, due to the absence of unimodal labels in most datasets, most of previous work [23], [34] directly utilize the multimodal ground truth labels to supervise the learning process of features with various semantic, which are essentially contrary with the thought of disentangled representation learning. Besides, the affection expressed through single modality can be quite diverse, which is concluded as the semantic mismatch issue as shown in Figure 1. Aiming at addressing this issue, we present Semantic-centric Label Generation (SCLG) to construct pseudo label space based on semantics as the weak supervision strategy to improve the learning of semantic-centric representations.\nSpecifically, we deem the learning processes of representations $F$ with various semantics as distinct sub-tasks * \u2208 {S,T, A, V}, which denote the sub-task of semantic-shared representations $F_{sh}$ and semantic-specific representations $F_{u}^{sp}$ for textual, audio and visual modalities. Each subtask should be trained under the guidance of the corresponding semnatic-centric pseudo labels. Inspired by Yu et.al [27], the semantic-centric labels $p_*$ are assumed to share the distribution space with multimodal ground truth labels $y_{gt}$. Thus, we utilize the common semantics contained in the representations across various samples and their ground truth labels to generate the pseudo specific- and shared-semantic labels as shown in Figure 3.\nGiven a query of representations $\\mathbb{B} = \\{F_i\\}_{i=1}^B$, we conduct k-Nearest Neighbor (k-NN) algorithm to find the K most nearest samples $\\{F_k\\}_{k=1}^K (K<B)$ for each representation $F_i$ by comparing the similarity in the feature space and then output the euclidean distance matrix $D_*$ between each sample and the nearest samples, denoted as:\n$\\mathbb{F}_k = k-NN(F_i; F_1 ... F_B) \\in \\mathbb{R}^{d_c}, i \\in [1, B], k \\in [1, K]$\n$D_* = (D_{ik}), where D_{ik} = \\sqrt{\\sum_{j=1}^{d_c} (F_i^j - F_k^j)^2 }$ (13)\nwhere the dimension of the representations $d_c$ is utilized as a scaling factor to mitigate the adverse effect of excessive distance. The distance matrix $D_*$ represents the similarity of various representations, indicating the relationship among different samples at the level of specific or shared semantics.\nFor each sub-task, to transfer the knowledge of multimodal ground truth labels $y_{gt}$ to the semantic-centric pseudo labels $p_*$, we design a scaling map to control the transferring magnitude related to semantics abundance and a shifting map to decide the direction and value of the label movement $\\Delta_*$. Therefore, we intuitively construct pseudo labels by considering the distance matrix $D_*$ in the Gaussian potential kernel form function [84] as the scaling map, and the difference between multimodal labels $y_{gt}^j$ and $p_k$ of the corresponding nearest samples as the shifting map, which is formulated as:\n$\\Delta_i = \\frac{1}{K} \\sum_{k=1}^K exp(-w_{scale}*D_{ik})* (y_{gt}^k - \\hat{p}_{ik}^j)$ (14)\nwhere $\\Delta_i$ denotes the varying value for pseudo label $\\hat{p}_i$ of sample $i$ in sub-task *. Moreover, the pseudo labels are initialized as the corresponding multimodal ground truth labels and updated in a momentum manner by combining the computed"}, {"title": "movement and history values with the increasing of training epochs z", "as": "n$\\hat{p}*_{i\\text{z}}^j = \\hat{p}*_{i\\text{z-1}}^j = ... = \\hat{p}*_{i1}^j = y_{gt}^j, r\\ge 1$\n$\\frac{1}{z}\\hat{p}*_{i\\text{z}} = \\frac{z-1}{z} * \\hat{p}*_{i\\text{z-1}} + \\frac{1}{z}(\\hat{p}*_{i, r-1} + \\Delta_i), z<r$\n$\\frac{1}{z}\\hat{p}*_{i\\text{z}} = \\frac{z-1}{z} * \\hat{p}*_{i\\text{z-1}} + \\frac{1}{z}(\\hat{p}*_{i, r-1} + \\Delta_*), z>r$ (15)\nwhere the momentum-based updating policy is intended to relieve the fluctuations caused by noisy samples and the updating process of pseudo labels is started after the rth epoch for more stable label generation and better convergence.\nFor each subtask, the specific-semantic labels are generated to reveal the intra-modal connections among different samples while the shared-semantic labels are expected to show the inter-modal commonality. Comparing with the multimodal ground truth labels, the generated pseudo labels are used to guide the learning of various semantic-centric representations in a weakly-supervised manner. Note that the semantic-centric pseudo labels are allowed to be zero for individual samples when there are few unimodal features or rare paired features related to the downstream prediction [14", "35": ".", "39": [85], "64": ".", "65": "we firstly employ $L2$-normalization on all representations for both intra- and inter-sample contrastive learning to restrict the contrastive learning space on the unit hypersphere, as shown in Figure 3. We implement the intra-sample contrastive learning among semantic-specific representations $F_u^{sp}$ and semantic-shared representations $F_{sh}$ for $u \\in \\{t,a,v\\}$, which efficiently decouple the semantic-specific features and the consistent information contained in each modality $u$. Given $\\{F_{t}^{sh}, F_{a}^{sh}\\}$ from each sample $i$, the goals are pushing away $F_{u}^{sp}$ and pulling closer $F_{sh}$ from different modalities, and pushing apart $F_{u}^{sp}$ and $F_{u'}^{sh}$ according to the semantics inside the corresponding representations. Thus, we construct positive pairs as $\\{F_{t}^{sh}; F_{a}^{sh}\\}$, and the negative pairs as $\\{F_{u}^{sp}; F_{u}^{sp}\\}$ and $\\{F_{u}^{sp}; F_{sh}^{u'}\\}$, and adopt"}]}