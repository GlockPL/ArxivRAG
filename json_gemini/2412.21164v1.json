{"title": "Adversarial Attack and Defense for LoRa Device Identification and Authentication via Deep Learning", "authors": ["Yalin E. Sagduyu", "Tugba Erpek"], "abstract": "LoRa provides long-range, energy-efficient communications in Internet of Things (IoT) applications that rely on Low-Power Wide-Area Network (LPWAN) capabilities. Despite these merits, concerns persist regarding the security of LoRa networks, especially in situations where device identification and authentication are imperative to secure the reliable access to the LoRa networks. This paper explores a deep learning (DL) approach to tackle these concerns, focusing on two critical tasks, namely (i) identifying LoRa devices and (ii) classifying them to legitimate and rogue devices. Deep neural networks (DNNs), encompassing both convolutional and feedforward neural networks, are trained for these tasks using actual LoRa signal data. In this setting, the adversaries may spoof rogue LoRa signals through the kernel density estimation (KDE) method based on legitimate device signals that are received by the adversaries. Two cases are considered, (i) training two separate classifiers, one for each of the two tasks, and (ii) training a multi-task classifier for both tasks. The vulnerabilities of the resulting DNNs to manipulations in input samples are studied in form of untargeted and targeted adversarial attacks using the Fast Gradient Sign Method (FGSM). Individual and common perturbations are considered against single-task and multi-task classifiers for the LoRa signal analysis. To provide resilience against such attacks, a defense approach is presented by increasing the robustness of classifiers with adversarial training. Results quantify how vulnerable LoRa signal classification tasks are to adversarial attacks and emphasize the need to fortify IoT applications against these subtle yet effective threats.", "sections": [{"title": "I. INTRODUCTION", "content": "A cost-effective communication solution is provided by LoRa for extended connectivity range, reduced power usage, prolonged battery life, and scalability. This makes LoRa well-suited for diverse applications of Internet of Things (IoT) that rely on Low Power Wide Area Networks (LPWANs), including asset tracking, industrial automation, surveillance, smart city infrastructure, smart home systems, and supply chain management [2]-[6]. Although LoRa presents various benefits, it is also subject to security threats. A significant concern involves unauthorized access, as adversaries may attempt to breach the LoRa network, intercept or tamper with data, and prevent reliable device communications. LoRa security vulnerabilities are studied in [7]. LoRa signals are susceptible to interception anoverd eavesdropping over the air. In potential replay attacks, adversaries may capture signals from legitimate LoRa devices and resend them to deceive the LoRa network [8], [9]. Adversaries may also engage in spoofing or impersonation of legitimate LoRa device signals with the intent of gaining unauthorized access, injecting malicious data, or undermining network operations. Consequently, it is crucial to assess the attack surface associated with LoRa and design defense mechanisms [10].\nSignal classification is essential in identifying LoRa devices and detecting rogue device transmissions. Enabled by recent computational and algorithmic advancements, deep learning (DL) is known to effectively extract advanced features from raw signals, facilitating precise and efficient classification of wireless signals [11]-[13]. To that end, deep neural networks (DNNs) are promising to reliably perform LoRa signal analysis tasks. Through training on real-world I/Q data collected from LoRa devices, the DNNs can capture subtle signal characteristics and enhance classification accuracy [14]-[20]. The state of the art in the area of DL-based radio frequency fingerprinting identification for LoRa devices is surveyed in [21].\nThere are potentially multiple tasks to perform when analyzing wireless signals [22]. In this paper, we focus on two signal classification tasks within LoRa networks: (i) Task 1: distinguishing among LoRa devices, (ii) Task 2: detecting spoofed signals that imitate LoRa signal characteristics. We adopt convolutional neural network (CNN) and feedforward neural network (FNN) architectures to implement classifiers for these tasks. For signal spoofing, adversaries generate synthetic signals to deceive the spectrum monitors and potentially bypass the physical-layer authentication mechanisms [23], [24]. In this paper, we apply the Kernel Density Estimation (KDE) method to generate synthetic signals by learning probability distributions of legitimate LoRa signals. We show that the DNNs achieve high accuracy for the two classification tasks that we consider for LoRa networks. For that purpose, we consider either two single-task classifiers, one for each task, or a multi-task classifier that trains a common model for both tasks, leveraging shared information across tasks.\nThe intricate decision space inherent in classification of wireless signals exhibits sensitivity to variations in input samples during test (inference) time, rendering it susceptible to adversarial (evasion) attacks [25]-[28]. Adversaries can determine subtle yet harmful perturbations in the input signals during test time with the aim of misleading the DNN models used for LoRa device identification and rogue signal detection, compromising the advanced authentication mechanisms that rely on RF fingerprinting and facilitating unauthorized access"}, {"title": "II. LORA SIGNAL CLASSIFICATION", "content": "We utilize the I/Q data from [16], sourced from authentic LoRa (Pycom IoT) devices acting as the transmitters and software-defined radios (SDRs), specifically, USRP B210 radios, acting as the receivers. In an outdoor setting, signals of LoRa devices operating at a center frequency of 915MHz have been captured with sampling rate of 1MS/s. The LoRa configuration comprises Raw-LoRa mode with 125kHz as the channel bandwidth, 7 as the spreading factor, 8-preamble, 20 dBm as the transmit power, 4/5 as the coding rate, and 5 meter as the distance between the transmitter and the receiver.\nAfter reading the I/Q samples from the files, we have directly used them as input to the DNN classifiers. We have not used any data preprocessing on the data. We also have not used any data augmentation techniques since there are sufficient number of samples in the dataset to achieve high classification accuracy. The dataset is balanced across classes, and we have not observed any biases in the data. We process each data sample to become an input of dimension (2,32) to the DNN, corresponding to 32 I and Q pairs. This way, we generate a dataset of 5000 samples, with 80% and 20% allocated for training and testing, respectively.\nIn this paper, we consider two tasks of classifying LoRa signals, Task 1: classifying the received signals to 'Device 1' and 'Device 2' depending on whether the transmissions are originated from LoRa Device 1 or Device 2, and Task 2: classifying the received signals to 'legitimate' vs. 'rogue' depending on whether the transmissions are originated from legitimate or rogue LoRa devices. Classifiers for Task 1 and Task 2 are referred to as Classifier 1 and Classifier 2."}, {"title": "A. Spoofed Signal Generation by Rogue LoRa Devices", "content": "The adversary employs KDE to generate signals for LoRa device spoofing. KDE calculates the probability density function (PDF) of legitimate LoRa signals received by rogue devices, and can be used to produce synthetic signals that have similar statistical properties as original signals. Transmitting these synthetic signals may help the adversary infiltrate the authentication mechanism and gain access to the LoRa network.\nKDE involves applying kernel smoothing techniques to estimate probability density using the observed samples. During this procedure, a kernel function is centered at each observed data point, and its effect on the PDF estimate is computed for the given kernel and bandwidth. The form of the kernel is determined by the kernel function, while the bandwidth determines its width, influencing the smoothness of the estimate and achieving a balance between capturing intricate details and preventing excessive smoothing of the PDF estimate. The involvement of the kernel entails a proportionally adjusted version of the kernel function assessed at the precise data point. Subsequently, the specific contributions from the kernels are aggregated to derive the final PDF estimation, guaranteeing the smoothness and continuity of the estimated PDF. Utilizing KDE on observed data yields an estimate that represents the inherent probability distribution. With $(x_1,x_2,...,x_n)$ representing the set of observed data points, the computation of the KDE estimates $f(x)$ for the PDF at a particular point $x$ is given as\n$$f(x) = \\frac{1}{nh} \\sum_{i=1}^n K \\bigg( \\frac{x-x_i}{h} \\bigg),$$\nwhere $K$ denotes the kernel function, $h$ denotes the bandwidth, $x_i$ denotes individual observed data points, and $n$ denotes the total count of observed data points. The KDE estimate $f(x)$ for a specific point $x$ is computed by combining the scaled contributions of the kernel function, each assessed at the observed data points $x_i$. Applying the scaling factor $\\frac{1}{nh}$ guarantees that the estimated PDF integrates to 1 over range of $x$. For signal spoofing, we utilize the Gaussian distribution as the kernel function with a bandwidth set to $10^{-3}$. Rogue Device 1, mimicking legitimate Device 1, transmits with a difference exceeding 2dB from the legitimate counterpart, while Rogue Device 2, emulating legitimate Device 2, transmits with a difference of less than 1dB from the authentic Device 2. Additionally, each rogue device exhibits a phase difference of up to $\\frac{\\pi}{3}$ degrees from the corresponding legitimate device.\nWe assess the fidelity of KDE-generated synthetic data with the Jensen-Shannon divergence (JSD). JSD measures the similarity of two probability distributions. When $P_l$ and $P_r$ denote the probability distributions for legitimate and rogue device $l=1,2$, the JSD is given by\n$$JSD(P_l, P_r) = \\frac{1}{2}(KL(P_l||M_l) + KL(P_r||M_l)),$$\nwhere $M_l = \\frac{1}{2}(P_l + P_r)$ and the Kullback-Leibler (KL) divergence between probability distributions $P$ and $Q$ on the same sample space $X$ is given by\n$$KL(P||Q) = \\sum_{x\\in X} P(x) \\log \\frac{P(x)}{Q(x)},$$\nwhere $P$ is the true and $Q$ is the model probability distribution. The JSD is from the range of 0 to 1, where 0 denotes identical distributions, and 1 signifies complete dissimilarity. Hence, a reduced JSD value indicates greater fidelity between the synthetic data produced by the KDE and the original data. Regarding the test data, the JSD values are JSD($P_1, P'_1$) = 0.0092 for Device 1 and JSD($P_2, P'_2$) = 0.0099 for Device 2, leading to an average JSD of 0.0096 for both devices."}, {"title": "B. Deep Learning for LoRa Signal Classification", "content": "For each signal classification task, we consider either a CNN or an FNN classifier. These FNN and CNN classifiers feature 6,522 and 70,074 trainable parameters, respectively. The training process minimizes the categorical cross-entropy as the loss function by using the Adam optimizer [32]. We aim at optimally selecting the hyperparameters of the DNN classifiers to achieve a high classification accuracy, i.e., higher than 85%, for both of the DNN architectures and both of the tasks by increasing the number of neural network layers and number of neurons in each layer in different runs. We gradually increase these hyperparameters until achieving the best performance without overfitting, while keeping the deep neural network models small for resource-constrained implementation. During training, we set the number of epochs as 50 and during evaluation we used the model parameters that provide the highest classification accuracy.\nSupervised learning algorithms require labeled data for each output class during training. They do not offer the inherent capability to correctly classify a new device type that has not been seen during training. In this paper, we assume that the device types do not change during the operation time. Adaptation of deep learning in the classification process can be achieved through continual learning. Continual learning methods such as elastic weight consolidation (EWC) [33] or PackNet [34] can be adopted to enable the DNN classifiers to classify new device types without training from scratch."}, {"title": "III. MULTI-TASK LEARNING FOR LORA SIGNAL CLASSIFICATION", "content": "In traditional single-task learning, a separate model is trained independently for each task, as discussed in Sec. II. Multi-task learning (MTL) aims to jointly train a model on multiple related tasks, leveraging shared information across tasks to improve the overall performance. MTL optimizes a joint objective function that encompasses the loss functions of all the tasks involved and trains a model to learn shared representations that capture essential features for both classification tasks. The benefits of MTL include shared feature learning, transfer of knowledge, and regularization effect. We apply MTL for Tasks 1 and 2 considered in Sec. II. When we consider each Task $i = 1,2$, separately, the objective of DL is to minimize a loss function for Task $i$, given by\n$$L_i (X, Y_i, \\Theta_i) = \\frac{1}{N} \\sum_{j=1} N_i l_i (x_j, y_{i,j}, \\Theta_i),$$\nwhere $\\Theta_i$ is the set of model parameters, $N_i$ is the number of samples, $Y_{i,j}$ is the $j$th element of the ground truth labels, $x_j$ is the $j$th element of input features, $f_{\\Theta_i}$ is the neural network with parameters $\\Theta_i$, and $l_i$ is loss functions for Task $i$. In MTL, we introduce task-specific weighting factors denoted as $w_1$ and $w_2$ and construct the loss function to be minimized as follows:\n$$L_{multi} (X, Y_1, Y_2, \\Theta) = \\sum_{i=1}^2 w_i L_i (X, Y_i, \\Theta),$$\nwhere $w_i$ serves as weight controlling the importance of Task $i$ in the overall learning process, $0 \\leq w_i < 1$ for $i = 1,2$ and $w_1 + w_2 = 1$.\nThe DNN architecture for MTL consists of three parts, a common branch (shared layer) followed two individual branches (task-specific layers), one for each of Tasks 1 and 2. The common branch is the first layer of the DNN architecture shown in Table I and each of individual branches consists of the rest of layers of the DNN architecture shown in Table I. For MTL, FNN and CNN classifiers feature 8,884 and 140,020 trainable parameters, respectively.\nMulti-task classification accuracy results are shown in Table IV for CNN and Table V for FNN. In these results, transmissions of both Device 1 and Device 2 as well as both legitimate and rogue devices are used. When we compare the same case in single task classification results reported in Table II and Table III, MTL improves classification accuracy in general, when CNN remains more successful than FNN in classification tasks."}, {"title": "IV. ADVERSARIAL ATTACKS", "content": "FGSM method is chosen to generate adversarial examples since it is fast and computationally efficient [30]. It requires only a single gradient computation to generate adversarial examples. While there are other advanced, iterative defense methods available such as Projected Gradient Descent (PGD), Basic Iterative Method (BIM), Momentum Iterative Method (MIM), Carlini & Wagner (C&W) and DeepFool, since our goal is to demonstrate the feasibility of adversarial attacks on single-task and multi-task LoRa signal analysis, FGSM remains as an effective approach. This one-step attack strategically crafts adversarial perturbations by capitalizing on gradient information derived from the loss function concerning the input samples. For the untargeted attack, the objective of FGSM is to amplify the model's loss by altering the input data in the direction specified by the gradient sign.\nFGSM generates an adversarial perturbation for each input sample by computing the gradient of the loss function concerning the input data using model backpropagation and establishing the direction in which the input requires perturbation, by taking the sign of the gradient. A small magnitude (\\epsilon) is used to proportionally scale the gradient sign to govern the intensity of the perturbation. The choice of $\\epsilon$ serves as a parameter influencing the balance between the attack's strength and the detectability of the perturbation.\nThis attack adds the scaled perturbation to the original input sample, accomplished through element-wise addition. It is crucial to ensure that the resulting adversarial sample remains within the acceptable range of values, dictated by the data type and controlled by the adversary's maximum transmit power. In the untargeted adversarial attack, the goal is to determine a perturbation $\\delta$ in the presence of a DNN model with parameters $\\Theta$, an input sample $x$, and its true label $y$. Formally, this optimization can be expressed as\n$$\\max_{\\delta} L_i (x + \\delta, y, \\Theta)$$\nfor Classifier $i$ that is trained for Task $i = 1,2$ subject to C1: $|\\delta|_p \\leq \\epsilon_{max}$: the magnitude of perturbation $\\delta$ is upper-bounded by $\\epsilon_{max}$.\nC2: $x + \\delta$ stays within the acceptable input range based on the device's transmit power and phase shift.\nSince solving the non-convex optimization problem in (6) poses a significant challenge, FGSM addresses this by linearizing the loss function concerning the input perturbation since neural networks are hypothesized to be too linear to resist linear adversarial perturbations [30]. We can linearize the cost function around the current value of $\\Theta$, obtaining an optimal max-norm constrained perturbation of $\\delta$. An adversarial example $X_{adv}$ is crafted by manipulating the input in a way that amplifies the loss function relative to the true label. To achieve this for Classifier $i = 1, 2$, the loss function $L_i(x, y, \\Theta)$ is computed with respect to the true label $y$ for the input $x$. Subsequently, the gradient of the loss function is determined concerning the input, denoted as $\\nabla_xL_i(x, y, \\Theta)$. The normalization of this gradient involves obtaining the sign of its elements. The sign of the gradient is scaled by a small perturbation magnitude $\\epsilon$. The perturbation for the untargeted attack on Classifier $i = 1,2$ is computed as\n$$\\delta = \\epsilon sign(\\nabla_xL(x, y, \\Theta)).$$\nThe adversarial example $X_{adv}$ is created by incorporating the perturbation into the original input, resulting in $x_{adv} = x + \\delta$. An adversary transmits perturbation signal such that the LoRa receiver receives $X_{adv}$ and its DNN model is potentially deceived into producing incorrect classifications for any label. The attack success probability (ASP) refers to the probability of incorrectly classifying input signals without specifying a specific target label. We assess the ASP as a function of the perturbation-to-signal ratio (PSR), which dictates the upper limit on the magnitude of the perturbation denoted by $\\epsilon$.\nThe adversarial perturbations generated for one model may exhibit a tendency to generalize and maintain effectiveness across multiple models [31]. This transferrability may occur because the vulnerabilities targeted by adversarial attacks are not exclusive to a specific model. Instead, they can be present in multiple models due to shared characteristics in their decision boundaries or loss landscapes. The impact of surrogate models and wireless channels on transferability has been explored in [35] within the context of a single task. In this paper, we assess how adversarial attacks transfer between Classifiers 1 and 2 for Tasks 1 and 2. For that purpose, we consider individual and joint perturbation scenarios:\n1) 'Perturbation for Classifier i' is the perturbation computed by (7) to maximize the loss of Classifier 1 for Task i.\n2) 'Perturbation for Classifier 1+2' is the perturbation computed according to\n$$\\delta = \\epsilon sign(\\sum_{i=1}^2 \\gamma_i \\nabla_x L_i (X, Y_i, \\Theta_i)$$\nto maximize the weighted loss of Classifiers 1 and 2, where $L_i$ is the loss function for classifier $i$ with parameters $\\Theta_i$ and $\\gamma_i$ is the weight for Classifier $i$, $i = 1,2$ (such that $0 \\leq \\gamma_i \\leq 1$ and $\\gamma_1 + \\gamma_2 = 1$) and. We set $\\gamma_1 = \\gamma_2 = 0.5$ for numerical results."}, {"title": "B. Targeted Attack on Single-Task Classification", "content": "The optimization formulation for a targeted adversarial attack aims to determine a perturbation that maximizes the loss of the classifier under attack specifically for a desired target class. Given a DL model with parameters $\\Theta$, an input sample $x$, a desired target label $Y_{target}$, and its true label $y$, the goal of the targeted attack based on FGSM is to find a perturbation $\\delta$ that minimizes the loss function $L_i$ for Classifier $i = 1,2$ such as\n$$\\min_{\\delta} L_i(x + \\delta, Y_{target}, \\Theta)$$\nsubject to the same conditions C1 and C2 of the optimization problem in (6). The optimization problem in (9) is solved by FGSM that generates an adversarial example $X_{adv}$ by perturbing the input in the direction that minimizes the loss function with respect to the target label $Y_{target}$. The adversarial example $x_{adv}$ is generated by adding the perturbation\n$$\\delta = -\\epsilon sign(\\nabla_x L(X, Y_{target}, \\Theta))$$\nto the original input such that $x_{adv} = x + \\delta$. The resulting adversarial example $x_{adv}$ fools the model into incorrect mis-classification for $Y_{target}$. The ASP for targeted attack is the probability of classifying input signals from another label to $Y_{target}$. We consider two targeted attacks: (i) attack on Classifier 1 with $Y_{target}$ = 'Device 1' (fooling Classifier 1 classifying Device 2 signals to Device 1) (ii) attack on Classifier 2 with $Y_{target}$ = 'rogue' (fooling Classifier 2 classifying rogue signals as legitimate, namely bypassing an RF fingerprinting-based authentication."}, {"title": "C. Untargeted Attack on Multi-Task Classification", "content": "By applying the FGSM method to the multi-task classifier, the adversarial perturbation for the untargeted attack is computed as\n$$\\delta = \\epsilon sign(\\sum_{i=1}^2 \\gamma_i\\nabla_x L(X, Y_i, \\Theta)$$\nwhere $\\gamma_i$ is the weight for the contribution from Task $i$'s loss function to the perturbation (we assume $\\gamma_i = 0.5$, $i = 1, 2$, for numerical results. The adversarial example $X_{adv}$ is generated by adding the perturbation to the original input such that $x_{adv} = x + \\delta)."}, {"title": "D. Targeted Attack on Multi-Task Classification", "content": "The goal is to flip labels of Tasks 1 and 2 to target labels $Y_{target,1}$ and $Y_{target,2}$. The adversarial example $x_{adv}$ is generated by adding the perturbation\n$$\\delta = -\\epsilon sign (\\sum_{i=1}^2 \\gamma_i \\nabla_x L(X, Y_{target,i}, \\Theta))$$\nto the original input such that $x_{adv} = x + \\delta$. For numerical results, we assume that the goal is to flip decisions from detecting rogue Device 2 to legitimate Device 1, i.e., $Y_{target,1}$ = 'Device 1' and $Y_{target,2}$ = 'legitimate'."}, {"title": "V. DEFENSE AGAINST ADVERSARIAL ATTACKS", "content": "We use adversarial training as part of the DNN training process to make the DNN models robust against adversarial manipulations. Adversarial training aims to determine the model parameters $\\Theta$ for the DNN that can handle worst-case perturbations and achieve robustness against adversarial attacks [36]. Samples with adversarial perturbations are incorporated in the training set, expanding the dataset and exposing the model to adversarial instances.\nGiven a training set of labeled examples $(x_i, y_i)$, where $x_i$ represents the input sample and $y_i$ represents the corresponding true label, adversarial training as a defense against untargeted attacks aims to determine model parameters $\\Theta$ that minimize the expected loss on adversarial examples as follows:\n$$\\min_{\\Theta} E_{(x_i,y_i)} [\\max_{\\delta \\in S} L(x_i+\\delta, y_i, \\Theta)],$$\nwhere $\\delta$ represents the adversarial perturbation and $S$ is the set of allowable perturbations subject to the conditions C1 and C2 of the optimization problem in (6). The inner maximization term seeks the worst-case adversarial perturbation within the allowable set $S$ that maximizes the loss on the perturbed example that is generated with FGSM."}, {"title": "VI. CONCLUSION", "content": "In this paper, we have addressed security concerns related to LoRa networks, which offer long-range and low-power com-"}]}