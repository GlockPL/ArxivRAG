{"title": "The Role of Sparsity for Length Generalization in Transformers", "authors": ["Noah Golowich", "Samy Jelassi", "David Brandfonbrener", "Sham M. Kakade", "Eran Malach"], "abstract": "Training large language models to predict beyond their training context lengths has drawn much attention in recent years, yet the principles driving such behavior of length generalization remain underexplored. We propose a new theoretical framework to study length generalization for the next-token prediction task, as performed by decoder-only transformers. Conceptually, we show that length generalization occurs as long as each predicted token depends on a small (fixed) number of previous tokens. We formalize such tasks via a notion we call k-sparse planted correlation distributions, and show that an idealized model of transformers which generalize attention heads successfully length-generalize on such tasks. As a bonus, Our theoretical model justifies certain techniques to modify positional embeddings which have been introduced to improve length generalization, such as position coupling. We support our theoretical results with experiments on synthetic tasks and natural language, which confirm that a key factor driving length generalization is a \"sparse\" dependency structure of each token on the previous ones. Inspired by our theory, we introduce Predictive Position Coupling, which trains the transformer to predict the position IDs used in a positional coupling approach. Predictive Position Coupling thereby allows us to broaden the array of tasks to which position coupling can successfully be applied.", "sections": [{"title": "Introduction", "content": "Enabling large language models (LLMs) to generalize to contexts longer than their training context length has emerged as a key problem in recent years. Indeed, many factors limit the context length of sequences which can be used during training, including the increased computational cost of training on long sequences (Tay et al., 2022) as well as the fact that longer sequences may be less numerous in the training dataset. Nevertheless, many applications require LLMs to be accurate on extremely long context lengths at inference time: for instance, a popular technique recently has been to use scratchpads or Chain-of-Thought (CoT) to perform various logic and reasoning tasks, and the length of the scratchpad can become very large, especially when combined with search or reinforcement learning techniques (DeepSeek-AI et al., 2025; Team et al., 2025). Unfortunately, transformers struggle to length generalize on even very simple arithmetic and logic tasks, such as computing parities, integer addition, and variable assignment (Anil et al., 2022; Kazemnejad et al., 2023). Indeed, only recently have transformer models successfuly been trained to length-generalize to contexts many times their training length on integer addition tasks, using a technique known as position coupling (closely related to Abacus embeddings) (Cho et al., 2024a,b; McLeish et al., 2024). For many other simple problems, comparable length generalization remains a challenge. In light of this uneven progress, a natural question is whether there is a more principled way of understanding length generalization. In particular,"}, {"title": "Preliminaries", "content": "Our focus in this paper is on decoder-only transformers trained to predict the next token. We review the architecture of such transformers in more detail in Appendix B.1. In this section, we discuss a few aspects of the position encodings of transformers which have previously been proposed to improve length generalization. Initially, Vaswani et al. (2017) proposed to introduce positional information in a transformer model using absolute positional embeddings (APE) (see also Gehring et al. (2017)). The APE assigns to each token a position ID $i$ (typically the position of that token in the sequence) and adds an embedding vector $V^{\\text{APE}}_i$ depending on $i$ to the embedding vector for the corresponding token. Recently, it has become more common to use position encodings which encode relative positions. In particular, a popular positional encoding technique in many large-scale open-source transformers is the rotary positional encoding (RoPE) (Su et al., 2021), which adjusts the computation of attention scores as follows: it multiplies each key embedding vector with position ID $i$ by a rotation matrix depending on $i$, and each query embedding vector with position ID $j$ by a rotation matrix depending on $j$. The effect of these rotations is that the attention score for a (key, query) pair with position IDs $(i, j)$ depends only on $i - j$ (and not $i$ or $j$ individually)."}, {"title": "Position Coupling", "content": "The technique of position coupling (Cho et al., 2024a,b; McLeish et al., 2024) (similar to Abacus in McLeish et al. (2024)) works specifically for problems with structured input where there is a clear one-to-one relationship between certain tokens. In particular, it assigns each token in a sequence a particular position ID in a way so that tokens assigned the same position ID should have a (task-dependent) \u201cone-to-one correspondence\". For example, to solve string reversal, i.e., predict the last $L$ tokens of the sequence\n\\[X_1,..., X_L, \\langle SEP \\rangle, X_L, . . ., X_1\\]\nfor some token IDs $X_1, ..., X_L$, then since the $i$th-to-last reversed token is equal to the $i$th input token, we feed the following position IDs:\n\\[1,2,..., L, 0, L, ..., 2, 1.\\]\nWe provide the precise position coupling schemes for our experiments with synthetic data in Section 5.2."}, {"title": "Positional Skip-Wise (PoSE) Training", "content": "The Positional skip-wise (PoSE) technique (Zhu et al., 2023) (see also (Wu et al., 2024)) aims to ensure that: (a) the position IDs used during training cover all posible position IDs 1,..., $L_{\\text{max}}$ that could be observed at test-time (where $L_{\\text{max}}$ is the maximum length of a test-time sequence), and (b) the differences between different position IDs seen in training sequences is of similar magnitude to that seen in testing sequences. To do so, we fix an integer $c$ denoting a number of chunks, and given a sequence of tokens $X = (X_1,..., X_e)$ during training, we partition $X$ into $e$ contiguous chunks (i.e., subsequences) and assign to each chunk a random contiguous sequence of position IDs so that the first position ID of each chunk is greater than the last position ID of the previous chunk. At test time, one simply uses the true position IDs, namely $(1,2,..., L)$, corresponding to a sequence $X$ of length $L$.\nThe precise schemes to partition into chunks and assign position IDs that we use are as follows: in all of our experiments we take the number of chunks to be $c = 2$: during training, we split a sequence $X = (X_1,..., X_e)$ into two parts by choosing a uniformly random position to split at. We then choose 2 integers $J_0, J_1 \\sim \\text{Unif}([L - l])$, and let the position ID of the first chunk begin at $\\text{min}\\{J_0, J_1\\}$ and the position ID of the second chunk begin at $\\text{max}\\{J_0, J_1\\}+l_1$, where $l_1$ is the length of the first chunk. This is essentially the same as the scheme used in Zhu et al. (2023) with all $v_i = 0$."}, {"title": "Theoretical model", "content": "Overview. In this section, we formally define our theoretical model. Our primary inspiration is the class of decoder-only transformers, which are trained to predict each next token as a function of the preceding ones in a given sequence. Accordingly, our theoretical framework will focus on the next-token prediction task, where we fix lengths $L < \\overline{L}$, and attempt to show that models trained on sequences of length $< \\overline{L}$ to predict"}, {"title": "Distributional assumptions: sparse structure", "content": "Without distributional assumptions on $P_\\ell$, achieving length generalization per Definition 3.1 becomes degenerate in the following manner, even if we assume $H$-realizability. For any fixed (realizable) choice of the distributions $P_\\ell$ for $\\ell < \\overline{L}$, unless $\\widehat{h}$ is identically equal to $h^*$, we can choose $P_{\\overline{L}}$ to have all its mass on some sequence $X \\in V^{\\overline{L}}$ for which $\\mathcal{L}(h^*(X), \\widehat{h}(X)) > 0$. This choice prevents the loss of $\\widehat{h}$ defined in (3) from being small, thus ruling out length generalization, as formalized below.\nProposition 3.1. Fix any $\\overline{L},\\overline{L} \\in \\mathbb{N}$ with $\\overline{L} < \\overline{L}$, and a hypothesis class $H \\subset \\mathcal{Y}^{V^*}$. Let $P_1,...,P_{\\overline{L}}$ be $H$-realizable distributions, realized by $h^*$. Suppose that $\\epsilon > 0$ and $\\widehat{h}$ defined in (3) satisfies $\\sup_{X \\in V^{\\overline{L}}} \\mathcal{L}(h^*(X), \\widehat{h}(X)) > \\epsilon$. Then there is an ensemble $\\mathcal{P}$ extending $P_1,...,P_{\\overline{L}}$ so that $H$ does not have $(\\overline{L}, \\overline{L}, \\epsilon)$-length generalization with respect to $\\mathcal{P}$."}, {"title": "Defining the hypothesis classes", "content": "In this section, we define a hypothesis class which allows us to effectively model the salient aspects of transformers while permitting us to obtain provable length generalization. In particular, we aim to satisfy the following two desiderata:\n1.  First, we would like there to be many ensembles $\\mathcal{P}$ with sparse planted correlations (per Definition 3.2) which are realizable with respect to $H$.\n2.  Second, we would like the class $H$ to capture simple transformers (i.e., a single attention head)."}, {"title": "Theoretical results: provable length generalization for the sparse group attention class", "content": "In this section, we establish formal length generalization guarantees for sparse functional attention classes $\\mathcal{H}^{\\text{attn}}$ (Definition 3.3) for distribution ensembles $\\mathcal{P}$ with sparse planted correlation (Definition 3.2). To do so, we need to make a few assumptions on the ensemble $\\mathcal{P}$. The first assumption ensures that the ensemble $\\mathcal{P}$ is approximately $\\mathcal{H}^{\\text{attn}}$-realizable.\nAssumption 4.1 (Realizability). We assume that $\\mathcal{P}$ is $\\delta$-approximately $\\mathcal{H}^{\\text{attn}}(\\mathcal{G}^{\\text{key}}, \\mathcal{G}^{\\text{val}})$-realizable, i.e., there is $h^* \\in \\mathcal{H}^{\\text{attn}}(\\mathcal{G}^{\\text{key}}, \\mathcal{G}^{\\text{val}})$ so that $\\mathbb{E}_{(X,Y) \\sim P_\\ell} [\\mathcal{L}(h^*(X), Y)] < \\delta$ for all $\\ell \\in \\mathbb{N}$.\nOur second assumption states that the distributions $\\mathcal{Q}^{\\text{pos}}$ in the context of Definition 3.2 have bounded coverage at all locations in the sense that changing $\\ell$ or shifting $S$ by $\\Delta$ units does not significantly change $\\mathcal{Q}^{\\text{pos}}(S)$.\nAssumption 4.2 (Coverage of $\\mathcal{Q}^{\\text{pos}}$). Fix a $k$-sparse distribution ensemble $\\mathcal{P}$ specified by $\\mu, (\\mathcal{Q}^{\\text{pos}})_{\\ell \\in \\mathbb{N}}, \\mathcal{Q}^{\\text{voc}}$, as well as $L_{\\text{local}} \\in \\mathbb{N}$. We assume that for each $\\ell \\in \\mathbb{N}$ with $\\ell \\geq L_{\\text{local}}$, there is some positive value $\\eta_\\ell \\in \\mathbb{R}$ so that for all $\\ell' \\in [L_{\\text{local}}, \\ell], \\Delta \\geq 0$ and $S^* \\in \\text{Sets}_k([\\ell])$,\\\\(\\frac{\\mathcal{Q}^{\\text{pos}}_\\ell(S^*)}{\\mathcal{Q}^{\\text{pos}}_{\\ell'}(S^* - \\Delta)} < \\eta_\\ell,\\frac{\\mathcal{Q}^{\\text{pos}}_\\ell(S^*)}{\\mathcal{Q}^{\\text{pos}}_{\\ell'}(S^*)} < \\eta_\\ell,\\\\\\]\\where the first inequality is only required if $S^* - \\Delta \\in \\text{Sets}_k([\\ell'])$ and the second inequality is only required if $S^* \\in \\text{Sets}_k ([\\ell'])$.\nTo help interpret Assumption 4.2, note first that in order for the distribution ensemble $\\mathcal{P}$ to be realizable (Assumption 4.1) by a class $\\mathcal{H}^{\\text{attn}}(\\mathcal{G}^{\\text{key}}, \\mathcal{G}^{\\text{val}})$ satisfying $L_{\\text{local}}$-locality (Assumption 3.3), it will typically be the case that $\\max_{i \\in S^*} i - \\min_{i \\in S^*} i \\leq L_{\\text{local}}$ with probability at least $1 - \\delta$ for $S^* \\in \\mathcal{Q}^{\\text{pos}}_\\ell$, for any $\\ell \\in \\mathbb{N}$.\nThus, a natural choice for $\\mathcal{Q}^{\\text{pos}}_\\ell$ is to fix some distribution $\\mathcal{Q}^{\\text{pos}}_{L_{\\text{local}}}$ over sets in $\\text{Sets}_k([L_{\\text{local}}])$ and let $\\mathcal{Q}^{\\text{pos}}_\\ell$ be the distribution of a random shift of a sample from $\\mathcal{Q}^{\\text{pos}}_{L_{\\text{local}}}$. Formally, $\\mathcal{Q}^{\\text{pos}}_\\ell$ is the distribution of the"}, {"title": "Improving length generalization: positional coupling", "content": "As discussed above, one limitation of Theorem 4.3 is its reliance on Item 1 of Assumption 3.3, which leads to the following restriction on the $k$-sparse distribution ensemble $\\mathcal{P}$: in typical examples (modulo some degenerate ones where, e.g., all functions in $\\mathcal{G}^{\\text{val}}$ are constant), in order to satisfy realizability (Assumption 4.1), we will need the low-loss hypothesis $h^* \\in \\mathcal{H}^{\\text{attn}}(\\mathcal{G}^{\\text{key}}, \\mathcal{G}^{\\text{val}})$ to be of the form $h^* = h_{g_0^*, g_1^*}$ for some $g_0^* \\in \\mathcal{G}^{\\text{key}}$ which \u201cselects out\u201d the planted set $S^*$ and $g_1^* \\in \\mathcal{G}^{\\text{val}}$ which correctly evaluates the label $Y$ given $X_{S^*}$; formally, for each $\\ell \\in \\mathbb{N}$, with high probability under $(X, S^*, Y) \\sim P_\\ell$,\n\\[g^*_0(S^*, X_{S^*}) > -\\infty, g^*_0(S, X_S) = -\\infty \\forall S \\neq S^*, \\\\\\]\nand $g_1^*(X_{S^*}) = Y$. (We formally call this property strong realizability in Assumption D.1.) But by Item 1 of Assumption 3.3, this means that $\\max\\{S^*\\} - \\min\\{S^*\\} < L_{\\text{local}}$ with high probability over the draw from $P_\\ell$. Ideally, we would like to establish results for planted $k$-sparse ensembles $\\mathcal{P}$ for which the planted set $S^*$ is not local in this sense.\nWe now show how a theoretical abstraction of position coupling as discussed in Section 2.1 can allow us to remove this locality requirement. Roughly speaking, this abstraction of position coupling states that there is a joint distribution over $(S^*, \\psi_\\ell)$, where $\\psi_\\ell : [\\ell] \\rightarrow [\\ell]$ gives a way of \u201crewriting\u201d position indices, so that the \u201crewritten\u201d set $\\psi_\\ell(S^*)$ of indices in the planted set $S^*$ satisfies the locality condition of Assumption 3.3. In particular, for each position ID $i \\in [\\ell]$, the value of $\\psi_\\ell(i)$ should be interpreted as its \u201ccoupled position ID\u201d as discussed in Section 2.1. More precisely, we have:\nDefinition 4.1 (Local position coupling). Fix a distribution ensemble $\\mathcal{P}$ with $k$-sparse planted correlations per Definition 3.2 (defined by $\\mu, \\mathcal{Q}^{\\text{pos}}, \\mathcal{Q}^{\\text{voc}}$). A $L_{\\text{local}}$-local position coupling of $\\mathcal{P}$ is defined by, for each $\\ell \\in \\mathbb{N}$, a joint distribution $\\mathcal{Q}^{\\text{pos-c}}$ over $S^* \\in \\text{Sets}_k([\\ell])$ and a mapping $\\psi_\\ell : [\\ell] \\rightarrow [\\ell]$ so that the marginal of $S^*$ under $\\mathcal{Q}^{\\text{pos-c}}$ is $\\mathcal{Q}^{\\text{pos}}$ and with probability 1 under the draw of $(S^*, \\psi_\\ell) \\sim \\mathcal{Q}^{\\text{pos-c}}$:\n1.  $\\max\\{\\psi_\\ell(S^*)\\} - \\min\\{\\psi_\\ell(S^*)\\} < L_{\\text{local}}$.\n2.  For each $i \\not\\in S^*$, $|\\psi^{-1}_\\ell(\\psi_\\ell(i))| = 1$. (I.e., Indices not in $S^*$ are not coupled.)"}, {"title": "Experiments", "content": "Conceptually, we view the main takeaways of Theorem 4.3 and Proposition 4.4 to be the following:\n(T1) First, an important factor enabling length generalization is that the label Y depends on only k tokens of the input (in the sense of Definition 3.2 and in particular the relation $Y = g^*(X_{s^*})$ in (4)). In particular, the parameter k (which we refer to informally as the sparsity) must be the same for both the lengths on which we train (namely, lengths $l < \\overline{L}$) and the length $\\overline{L}$ to which we attempt to extrapolate, and sufficiently small compared to the maximum training length $\\overline{L}$.\n(T2) Second, locality of the hypothesis class (per Item 1 of Assumption 3.3) plays an important role as well: the maximum distance $L_{\\text{local}}$ between tokens which \"matter\" in predicting the label Y must be the same for lengths $l < \\overline{L}$ on which we train and the length $\\overline{L}$ to which we extrapolate. Moreover, as this requirement is quite strong (and unrealistic for many problems of interest), one way to mitigate it is the technique of position coupling (per Proposition 4.4).\nIn this section, we evaluate these conclusions for synthetic and natural language modeling data.\nRemark 5.1. One might wonder why we emphasize Item 1 but not Item 2 of Assumption 3.3 in takeaway (T2). In fact, the conceptual message of Item 2, namely that position IDs only influence attention scores by their relative information (i.e., the difference between different positions) is already captured by the fact that it is common to use relative positional embeddings and variants (e.g., ROPE (Su et al., 2021), FIRE (Li et al., 2024)) in many open-source transformer architectures. Due to the success of such embeddings, in this sense the constraint imposed by Item 2 can \"come for free\"."}, {"title": "Length generalization for sparse parity", "content": "First, we discuss a simple setting which measures the degree to which length generalization in transformers reflects the requirement from (T1) that the sparsity $k$ be sufficiently small and not grow as a function of the sequence's length. In particular, consider the following sparse parity task: given $k, \\ell \\in \\mathbb{N}$, the input is drawn from a distribution $\\mathcal{D}_{\\ell,k}^{\\text{sp}}$ over length-$2\\ell$ sequences, where tokens at even-numbered positions are bits, and tokens at odd-numbered positions belong to some set $\\Omega$. Exactly $k$ tokens at odd-numbered positions belong to some \"special subset\" $\\Omega' \\subset \\Omega$, and the goal is to find the parity of the $k$ tokens immediately following them. See Appendix E.1.1 for precise details.\nExperimental setup: data. For each value of $K_{\\text{train}} \\in \\{4,6,8,10,12\\}$, we train a transformer $h^{K_{\\text{train}}}$ to predict the last token of samples drawn from $\\mathcal{D}_{\\ell,k}^{\\text{sp}}$, where $\\ell$ is sampled uniformly subject to the length of the sample satisfying $2\\ell \\in [20,50]$ and $k \\sim \\text{Unif}([K_{\\text{train}}])$. We then evaluate the performance of each of the trained transformers $h^{K_{\\text{train}}}$ on samples drawn from $\\mathcal{D}_{\\ell,k_{\\text{test}}}^{\\text{sp}}$ of length $2\\ell \\in [20,500]$ and with sparsities $k_{\\text{test}} \\in \\{4, 6, 8, 10, 12, 14, 16\\}$.\nExperimental setup: model. Our model is based off of the GPT-NeoX (decoder-only) transformer (Andonian et al., 2023), and uses rotary positional embeddings (ROPE). To ensure nontrivial length generalization performance, we combined RoPE with POSE (Section 2.2). Full training and evaluation details may be found in Appendix E.\nRemark 5.2. Numerous other modifications to position IDs have been proposed for length generalization, such as position interpolation and various enhancements, which typically modify the way the transformer uses the position IDs at inference time, often after a small amount of fine-tuning (see Appendix A.2). We stick with PoSE in this paper (when position coupling is not applicable) because of its simplicity and since (a) it does not require modifying the transformer's computations at inference time; and (b) the fine-tuning for position interpolation requires sequences of length given by the testing context length, which fails to lie in our framework where we assume that any amount of training on such sequences is not allowed. Understanding the role of sparsity and locality for length generalization in transformers which make these inference-time modifications to position IDs is left for future work."}, {"title": "Scratchpad with Predictive Position Coupling", "content": "Many tasks, unlike sparse parity, have the property that the sparsity of the next-token prediction task (as formalized by, e.g., Definition 3.2) can be quite large and in fact increase from shorter to longer lengths. To help achieve length generalization, one approach which has been suggested in numerous existing works (Anil et al., 2022; Hou et al., 2024) is the chain-of-thought/scratchpad technique (Nye et al., 2022; Wei et al., 2022; Lewkowycz et al., 2022). It proceeds by computing tokens representing intermediate steps, such as intermediate parities when the task is to compute the parity of a sequence of $\\ell$ bits. It has been observed that the scratchpad technique alone is insufficient to ensure length generalization (Anil et al., 2022; Dziri et al., 2023; Hu et al., 2024; Kazemnejad et al., 2023; Lanchantin et al., 2023). However, a number of recent works (Cho et al., 2024a,b; McLeish et al., 2024) have shown that the technique of position coupling (see Section 2.1), when combined with a scratchpad (and even when used on its own, when appropriate), can allow significant length generalization on arithmetic tasks such as parity, addition, and multiplication. This development parallels our theoretical results in Section 4.1, where we showed that position coupling removes the stringent requirement of locality, which is not satisfied in practical settings.\nOne downside of position coupling is that it requires significant structure in the format of the data sequence, including the scratchpad: in particular, once the length is fixed, the coupled position IDs (see Section 2.1) in all tokens the transformer is trained to predict must be determined, as they must be fed into the model to predict each successive token. (For instance, for the string reversal example in Section 2.1, when $L$ denotes the length of the string to be reversed, the sequence of position IDs is given by (2).) As a consequence, the applications of position coupling are currently limited to a small number of synthetic tasks where the computation of each token in the sequence (including the scratchpad) can be coupled with a fixed position of the input.\nTowards relaxing this limitation to allow sequences where the coupled position IDs can be input-dependent, we propose Predictive Position Coupling (PPC), which modifies the transformer architecture to instead predict the coupled position ID for each next token. In particular, we add an additional output embedding module so that, at each step, the transformer predicts two IDs: the next token ID, as well as the coupled position ID for that token. At generation time, those two predicted tokens are fed in as the position ID and token ID at the following position. In our results, we report the fraction of examples on which the model correctly predicts all tokens and coupled position IDs.\nExperimental goals. In the experiments discussed below, we aim to: (a) show that Predictive Position Coupling can successfully be applied to improve length generalization on instances where position IDs in the scratchpad must be predicted; and (b) validate our theoretical takeaways emphasizing the importance of sparsity (T1) and locality (T2) in controlling length generalization for instances with a scratchpad. Towards"}, {"title": "Variable assignment with scratchpad", "content": "Next, we consider the more challenging task of variable assignment with a scratchpad, of which several slight variants have been studied in numerous prior works (sometimes under different names, like \"pointer chasing\") (Zhang et al., 2023, 2022; Lanchantin et al., 2023; Anil et al., 2022; Hsieh et al., 2024; Peng et al., 2024).\nExperimental setup: data overview. In the variable assignment problem, we fix a depth $d$, denoting the number of \"hops\" it takes to compute the final value. Roughly speaking, the goal is as follows: given a sequence of \"variable assignments\" of the form $v \\leftarrow v'$ (where $v, v'$ are token IDs), together with a starting variable $v_{\\text{init}}$, follow a sequence of $d$ assignments starting at $V_{\\text{init}}$.\nIn more detail, a problem instance consists of some number of \"chains\", each of the form $v_1 \\leftarrow v_2; v_2 \\leftarrow v_3; \u00b7 \u00b7 \u00b7 ; v_d \\leftarrow v_{d+1}$ for some depth parameter $d$. The variable assignments in each of these chains are interleaved randomly; exactly one of the chains has $v_1 = V_{\\text{init}}$. The goal of the task is to find the final variable $v_d$ on the chain for which $v_1 = V_{\\text{init}}$. To do so, we use a scratchpad which writes out in full the chain starting with $v_{\\text{init}}$, together with position coupling, which matches each element of this chain on the scratchpad with its corresponding position in the input. The data format and position coupling scheme are presented in detail in Appendix E.3. We mention here that, because the chains are interleaved randomly, the position IDs used in the position coupling scheme depend on the particular input instance. Thus, the use of Predictive Position Coupling is crucial in order to be able to feed in the correct coupled position IDs in the scratchpad at inference time. We train on input sequences of length $\\ell \\in [20,60]$, and test on lengths $\\overline{L} \\in [20, 200]$.\nResults. Figure 3a shows our results for the variable assignment problem with position coupling: the technique of Predictive Position Coupling allows good length generalization for test lengths up to 3 times the training sequence length, for depths $d\\in \\{2,3,4,5\\}$. In Figs. 3b to 3d, we present baselines that remove either just the position coupling or the scratchpad altogether; as predicted by takeaways (T2) and (T1), respectively, these modifications significantly harm length generalization. In particular, using RoPE with POSE (with or without scratchpad) performs significantly worse than PPC.\nWhile in this paper, we have discussed two examples in which Predictive Position Coupling can be effectively applied as a proof of concept, we believe that PPC can be (a) combined with other approaches, including improved positional embeddings such as FIRE (Li et al., 2024) and (b) be applied to several other synthetic (and potentially natural language) tasks. We leave these investigations as exciting directions for future work."}, {"title": "Sparsity & length generalization in natural language", "content": "Finally, we study the interplay between sparsity and length generalization in natural language. As is typical in experiments involving natural language, we use the perplexity of a model on test sequences to evaluate its performance. Two challenges that arise in this setting are: (1) a trivial way to achieve reasonably good length generalization from sequences of length $< \\overline{L}$ to those of length $\\overline{L} > \\overline{L}$ is to simply ignore tokens more than $\\overline{L}$ units in the past when given a sequence of length $\\overline{L}$, and (2) since there is not a \u201ccorrect\u201d functional form for the value of each token as a function of previous ones, it is unclear how to define \u201csparsity\u201d of the next-token prediction task.\nIn order to understand length generalization in models beyond the trivial baseline suggested in (1) above, we ask: given $\\overline{L} < \\overline{L}$, can a transformer trained on natural language sequences of length $\\overline{L}$ achieve smaller perplexity on sequences of length $\\overline{L}$ than on sequences of length $\\overline{L}$? Previous works have shown a positive answer to this question (as will our experiments) when one uses appropriate modifications to the positional embeddings, namely PoSE (Zhu et al., 2023) or position interpolation (Chen et al., 2023; emozilla, 2023;"}, {"title": "Related work", "content": "Theoretical guarantees for length generalization\nZhou et al. (2023); Huang et al. (2024) propose that transformers will length-generalize at tasks which can be solved using a short RASP program (Weiss et al., 2021). Theorem 7 of Huang et al. (2024), however, assumes that the transformer is chosen so as to minimize a certain regularizer tailored specifically for length generalization, and their result is asymptotic in nature (in contrast to ours). Sabbaghi et al. (2024) show that using gradient flow on a 1-layer linear transformer model with relative position embeddings on a simple linear regression task will converge to a model which length-generalizes, whereas the use of absolute position embeddings will fail to length-generalize. Ahuja and Mansouri (2024) show that a model class defined as a simple abstraction of attention heads that sums up pairwise dependencies of tokens experiences length generalization. Unlike our theoretical results, those of Sabbaghi et al. (2024); Ahuja and Mansouri (2024) are asymptotic in nature and cannot capture the (most common) case of a softmax attention head. Moreover, they proceed, roughly speaking, by showing that the learned model is equal to the ground-truth model on the entire domain, which is generally not the case with actual language models (Zou et al., 2023; Wei et al., 2023; Andriushchenko et al., 2024). As our theoretical setup incorporates distributional assumptions, it establishes"}, {"title": "Modifying positional embeddings for length generalization", "content": "In addition to position coupling (Cho et al., 2024a,b) (and the closely related Abacus embeddings (McLeish et al., 2024)), which is a focus in our paper, several other techniques have been developed to modify position embeddings during training and/or inference time to improve the length generalization performance of transformers. Zhu et al. (2023) developed the positional skip-wise technique (PoSE; see Section 2.2), which was later refined by modifying the distribution of position IDs used at training time in Wu et al. (2024). We remark that PoSE is conceptually similar to randomized position embeddings (Ruoss et al., 2023), which trains using a random set of position IDs from the test-length context window (with no guarantee on the contiguity of these IDs, unlike PoSE).\nAnother popular strategy to extend the context length of transformers which has seen traction for models at larger scales, such as as Code Llama (Rozi\u00e8re et al., 2024), is position interpolation. This technique scales down (\"interpolates\") the position IDs in the longer test-length context window to match the length of the shorter training-length context. Several such interpolation strategies have been proposed, including the canonical choice of linear interpolation (Chen et al., 2023), as well as NTK-RoPE (emozilla, 2023), YaRN (Peng et al., 2023), and CLEX (Chen et al., 2024); the latter strategies adjust the amount of interpolation done on a per-frequency basis, with different ROPE frequencies receiving different interpolation scales. One downside of these position interpolation strategies is that they generally require some fine-tuning on the longer test-length sequences in order to effectively use the longer context windows (e.g., to achieve decreased perplexity on longer sequences than those seen during training). Such fine-tuning complicates the theoretical setting of length generalization, where it is typically assumed that any amount of training on the longer test- length sequences is not allowed. We remark, however, that these position interpolation techniques can be combined with PoSE (Zhu et al., 2023; Wu et al., 2024); exploring such combinations in the context of our experiments is an interesting direction for future work.\nFinally, we remark that some strategies, such as LM-Infinite (Han et al., 2024) and Self-Extend (Jin et al., 2024), have been proposed to adjust the attention mechanism at inference time so as to achieve length generalization without any fine-tuning, though"}]}