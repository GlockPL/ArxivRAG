{"title": "UNEARTHING SKILL-LEVEL INSIGHTS FOR UNDERSTANDING TRADE-OFFS OF FOUNDATION MODELS", "authors": ["Mazda Moayeri", "Vidhisha Balachandran", "Varun Chadrasekaran", "Safoora Yousefi", "Thomas Fel", "Soheil Feizi", "Besmira Nushi", "Neel Joshi", "Vibhav Vineet"], "abstract": "With models getting stronger, evaluations have grown more complex, testing mul-\ntiple skills in one benchmark and even in the same instance at once. However,\nskill-wise performance is obscured when inspecting aggregate accuracy, under-\nutilizing the rich signal modern benchmarks contain. We propose an automatic\napproach to recover the underlying skills relevant for any evaluation instance, by\nway of inspecting model-generated rationales. After validating the relevance of\nrationale-parsed skills and inferring skills for 46k instances over 12 benchmarks,\nwe observe many skills to be common across benchmarks, resulting in the cu-\nration of hundreds of skill-slices (i.e. sets of instances testing a common skill).\nInspecting accuracy over these slices yields novel insights on model trade-offs:\ne.g., compared to GPT-40 and Claude 3.5 Sonnet, on average, Gemini 1.5 Pro is\n18% more accurate in computing molar mass, but 19% less accurate in applying\nconstitutional law, despite the overall accuracies of the three models differing by a\nmere 0.4%. Furthermore, we demonstrate the practical utility of our approach by\nshowing that insights derived from skill slice analysis can generalize to held-out\ninstances: when routing each instance to the model strongest on the relevant skills,\nwe see a 3% accuracy improvement over our 12 dataset corpus. Our skill-slices\nand framework open a new avenue in evaluation, leveraging skill-specific analyses\nto unlock a more granular and actionable understanding of model capabilities.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have seen benchmarks evolve to keep up with ever-advancing models. While classical\nbenchmarks tested specific capabilities, like recognizing digits (LeCun et al., 1998) or classifying\nsentiment (Bowman et al., 2015), modern benchmarks measure proficiency in numerous capabilities\nsimultaneously, drawing questions of increasing difficulty from more diverse domains (Mialon et al.,\n2024; Yue et al., 2024; Wang et al., 2024). As the questions we test models on have grown more\ncomplex, aggregate performance measures provide less understanding about model proficiency in\nspecific abilites. For example, as shown in Figure 1, we find that over a dozen benchmarks, GPT-\n40, Gemini 1.5 Pro, and Claude 3.5 Sonnet (OpenAI, 2024; Gemini-Team, 2024; Anthropic, 2024)\nachieve overall accuracies within 0.4% of one another, leaving an open question: Are these models\nall the same, or are valuable insights being averaged away?\nManual annotations of categories across instances (e.g. Ying et al. (2024) and Liu et al. (2023) in-\nclude 'ability' tags) enable going beyond accuracy, but at a cost for benchmark creators that rises\nwith the number and difficulty of test questions. This results in few (if any) and non-standardized an-\nnotations, restricting cross-benchmark aggregation, even though many benchmarks have large over-\nlap in the (implicit) skills they test (Miao et al., 2020; Cobbe et al., 2021). Prior works show promise"}, {"title": "2 CONSTRUCTING SKILL-SLICES USING MODEL-GENERATED RATIONALES", "content": "We now detail our method for inferring relevant skills for each evaluation instance and aggregating\nskill annotations across datasets to form skill-slices (i.e. a set of instances that share a relevant\nskill). We utilize rationales, or step-by-step solutions, generated by a strong model (e.g. GPT-40) to\nfacilitate the extraction of relevant skills. However, as evaluation instances are challenging by design\nand generating relevant skills requires a significant level of meta-reasoning, it is important to assess\nthe quality of produced skills. Thus, we additionally present a multifaceted automatic approach to\nvalidating the relevance of extracted skills, which we confirm aligns well with human judgements."}, {"title": "2.1 SKILL EXTRACTION VIA RATIONALE PARSING", "content": "Given an evaluation instance, we prompt GPT-40 to generate a detailed rationale (i.e. step-by-step\nsolution), where each step only involves the use of a single skill; figure 2 presents an example. We\nadditionally instruct the model to, after each step, list the applied skill with multiple names of cas-\ncading granularity. For example, the skill 'treble clef recognition' is prepended with 'perception,"}, {"title": "2.2 CURATING SKILL-SLICES ACROSS BENCHMARKS", "content": "With rationale-parsing, we tag relevant skills for 46k evaluation instances from 12 benchmarks\nthat are popular (i.e. commonly featured in evaluations for recent model releases) and unsaturated\n(SOTA under 90% accuracy), resulting in 690k total skills (128k unique) over 202k rationale steps.\nImportantly, we observe that skills cut across benchmarks. That is, many skill-slices of non-trivial\nsize are formed when including instances across benchmarks, because models re-use skills in diverse\ncontexts. Namely, 278 skill-slices have at least 100 unique instances each. This number rises to 332\nafter de-duplicating via a tight clustering on the text embeddings of the skills. Aggregating across\nbenchmarks enables us to analyze skills that are in the long tail for one benchmark, but get resur-\nfaced when many different benchmarks are joined together. Indeed, slice count increases by 51%\nwhen slicing across benchmarks instead of only drawing instances from one benchmark at a time."}, {"title": "2.3 AUTOMATED VALIDATION OF SKILL RELEVANCE", "content": "In the absence of fine-grained ground truth skill annotations that would confirm whether generated\nskills are relevant or not, we design an automatic validation approach. Namely, we propose two\nautomated methods to directly validate the relevancy of listed skills: post-hoc verification and\ninter-(skill)annotator agreement. The latter checks the overlap in skills listed for the same instance"}, {"title": "3 ANALYZING FOUNDATION MODELS WITH SKILL-SLICES", "content": "We now leverage the skill-slices obtained in the previous section to better understand and utilize\nfrontier models from OpenAI, Google, and Anthropic. First, we identify skill-slices with vastly\ndifferent accuracies across releases from the same family, as well as across families, which are\nobfuscated when inspecting overall accuracy. Then, having discovered models differ in their specific\nstrengths and weaknesses, we show how overall accuracy can be improved by choosing the best\nmodel per instance or dataset, leveraging inferred skills and computed model-wise skill accuracies."}, {"title": "3.1 FINER-GRAINED INSIGHTS FROM EXISTING EVALUATION INSTANCES", "content": "Understanding Model Evolution. Figure 4 shows average improvement, along with skill-slices\nwith greatest and least improvement, for the last two releases in the Open AI GPT, Google Gemini,"}, {"title": "3.2 GENERALIZATION OF SKILL-SLICE INSIGHTS", "content": "Our skill-slice analysis operates under the premise that the insights drawn from inspecting\nsufficiently-large slices from a wide range of sources can generalize to new instances. If the in-\nsights generalize well, then understanding skill trade-offs between models can enable employing\nthe models in a more calculated manner. That is, if we have knowledge of the skills relevant to a\nnew instance, as well as each model's skill-wise strengths, we can improve accuracy by routing that\ninstance to the model whose strengths are most aligned with those skills.\nTo test this, we route each instance in our corpus to one of GPT-40, Gemini 1.5 Pro, and Claude 3.5\nSonnet, based on the skill annotations for that instance and the skill-wise accuracies per model com-\nputed over the remaining corpus (i.e. without the test instance). To obtain a single score per instance\nper model, we take a weighted average of skill-wise accuracies, where the weight for each skill is the\ninverse of its slice size (so to upweight finer-grained, more specific skills). As shown in the left panel\nof figure 6, routing increases accuracy by up to 3.2% compared to each of the frontier models alone\non 12 datasets combined, including improvements of 3.5 to 6.8% for MMLU Pro (Wang et al., 2024).\nWe highlight the MMLU Pro results because the vast majority of the instances in the reference\ncorpus (over which skill-wise accuracies are computed) are from multimodal benchmarks, while\nMMLU Pro is language-only, making it a good candidate to showcase the generalization of our\napproach. To study this deeper, we now partition each skill-slice based on if the instance comes\nfrom MMLU Pro or one of our multimodal benchmarks. Then, for each model we obtain two paired\nsets of slice-wise accuracies: one accuracy score per skill-slice, per partition. As shown in the right\npanel of figure 6, we observe strong correlations ($\\r \\geq 0.79, p < 1e - 7$) between these two sets of\nscores for all three models. In addition to showing generalization, this result further validates our\nidea that skills pertain to a deeper property of a given instance (namely, what must be done to solve\nit) than surface level attributes, like its modality."}, {"title": "4 CORROBORATING SLICE ACCURACIES WITH PROBING QUESTIONS", "content": "While skill-slices allow for approximating a model's proficiency at a skill by averaging accuracy\nover any instance where the skill is relevant, our rationale parsing framework enables a second\nindependent analysis to more directly probe a single skill, without the effect of co-occurring skills.\nNamely, rationale parsing localizes each skill to a specific step of the generated solution, as well as\nthe resulting claim, which we instruct the rationale-generating model to include in its response. For\nexample, as shown on the right in figure 7, the resultant claim to a step where the skill \u201c[musical]\nnote identification\u201d is applied is \u201cThe notes are B and E\u201d. A claim can then be reframed as a question\n(e.g. \"What are the notes?\") probing a single skill, unlike the original question that often requires nu-\nmerous skills and steps, any one of which could cause an incorrect answer. We use the rate of incon-\nsistency, like in Wang et al. (2023), over multiple responses to each probing question as a measure\nof proficiency at the probed skill: if a model contradicts itself, it must have been wrong at least once.\nWe now probe three sets of 20 skills: those with lowest, median, and highest accuracy based on the\nanalysis in section 3, to sample skills with a variety of slice accuracies. For each skill, we (i) gener-\nate 20 probing questions, (ii) obtain 5 responses per probe per model, and (iii) evaluate consistency\nof the responses with GPT-40. As shown in figure 7, models contradict themselves at a significantly\nhigher rate for low accuracy skills, and inconsistency rises as slice accuracy falls, corroborating\nthe skill-slice analysis. Overall, inconsistency correlates well with slice accuracy (r = -0.675; see\nAppendix E.2). Further, we find skills where all models contradict themselves more often than not,\nsuch as \u201c[musical] note identification\u201d, \u201ceye direction analysis\u201d, and \u201cenumerating objects\u201d, the last\nof which is a well-documented limitation of VLMs (Yuksekgonul et al., 2023; Paiss et al., 2023)."}, {"title": "5 RETRIEVING EVALUATION INSTANCES FOR CUSTOM QUERY SKILLS", "content": "Finally, towards (a) direct comparison of our work to prior methods and (b) more flexible use of skill\nannotations, we introduce the task of skill-based retrieval: given an open-vocabulary query skill, can\nwe retrieve relevant evaluation instances, so to build a custom skill-slice for specialized evaluation?\nThis task boils down to defining an efficient metric that assigns a similarity score between a text\nquery and an evaluation instance. Here, we focus on multimodal evaluation instances.\nWe consider baselines that leverage a. embeddings of the instance (either of just the image using\nCLIP Radford et al. (2021), just the text question, or the average of both image and text), b.\nattributes of the input, either inferred by GPT-40 or provided as ground-truths by benchmark\ncreators. In this latter case, we embed text attributes, along with the query skill, using a text encoder,\nwith which we compute similarity of an attribute to the query skill as the cosine similarity of their\nembeddings. To obtain a single similarity score for the GPT-40 attributes baseline where multiple\nattributes exist per instance, we average the top 3 highest similarities. Our method is identical\nto the attribute-based approach, except that we leverage annotated skills instead of attributes. The\nbaselines encapsulate how prior methods propose to group inputs to go beyond overall accuracy\n(Eyuboglu et al., 2022; Rezaei et al., 2024), enabling direct comparison to our method."}, {"title": "6 REVIEW OF LITERATURE", "content": "Prior efforts to gain fine-grain insights often involve intensive manual efforts to construct multi-\ntask benchmarks (Liang et al., 2023; Wang et al., 2019; Srivastava et al., 2023; Al-Tahan et al.,\n2024; Balachandran et al., 2024) or datasets with annotations beyond ground-truth labels (White\net al., 2024; Fu et al., 2023; Yu et al., 2024b; Lu et al., 2024). On top of quantifying specific abili-\nties, extra annotations have illuminated biases (Buolamwini & Gebru, 2018; Moayeri et al., 2024b)\nand robustness issues (Idrissi et al., 2022; Koh et al., 2021). Automated alternatives include synthe-\nsizing benchmarks with known attributes (Zhang et al., 2024; Bordes et al., 2023) or automatically\nuncovering sub-groups within existing data (Luo et al., 2024; Murahari et al., 2023).The term slice\ndiscovery (or otherwise referred to as data cohort or subgroup) was coined (Chung et al., 2019;\nEyuboglu et al., 2022) to describe these methods which enable comparisons (Eyuboglu et al., 2024)\nand error analysis (Rezaei et al., 2024; Slyman et al., 2023; Nushi et al., 2018; Singla et al., 2021;\nDunlap et al., 2024). While these works focus on slices formed by surface-level attributes for visual\nrecognition tasks, we curate skill-slices, which pertain to the underlying ability each slice requires\nfrom a model, and as such extend to more tasks and modalities.\nSkills are akin to the 'ability' tags present in some modern benchmarks (Liu et al., 2023; Ying et al.,\n2024), though they can be finer (or coarser) grained (White, 1973) than what is typically anno-\ntated. The importance of acquisition of skills has been studied in human cognition (Gagne, 1962;\nKoedinger et al., 2023) as well as language models (Arora & Goyal, 2023). Yu et al. (2024a) utilizes\nskills as input to an LLM to generate challenging evaluation instances requiring a composition of the\nprovided skills. Recent (Murahari et al., 2023) and concurrent (Didolkar et al., 2024) works also ex-\nplore skill inference, albeit in more narrow domains, toward improved prompting or finetuning. This\nimportant shift of methods from attributes to skills is indeed motivated by the increasingly general-\npurpose nature of state-of-the art models. In this work, we scale up such analysis and insights by\nexpanding it to 12 benchmarks and by introducing rationale parsing as a means to infer more skills.\nRationales have been studied extensively in the context of prompting (Wei et al., 2022; Kojima et al.,\n2022; Yao et al., 2023). Other works show the value of rationales as richer training signal (Hsieh\net al., 2023; Mitra et al., 2023; Zelikman et al., 2022; Krishna et al., 2023) or model explanations\n(Ehsan et al., 2019; Hu & Yu, 2024; Huang et al., 2023), though some question their faithfulness to\nunderlying model processes (Madsen et al., 2024; Fayyaz et al., 2024). We utilize rationales not to\ninterpret a model directly, but instead to improve the relevance and diversity of the annotated skills\n(Singh et al., 2024), which become apparent in the solution steps present in rationales."}, {"title": "7 DISCUSSION AND FUTURE WORK", "content": "It is often said, \"We can only improve what we can measure.\" As models grow in complexity and\ncapability, traditional evaluation metrics like aggregate accuracy over entire datasets become insuffi-\ncient. Simply averaging accuracy across datasets hides important insights needed to understand and\nenhance both the datasets and the models themselves.\nTo address this gap, our work introduced a skill-slice analysis approach for model evaluation. By\nexamining model-generated rationales, we developed an automated method to extract the underlying\nskills required to solve individual evaluation instances. This shift from focusing on overall accuracy\nto conducting a more detailed analysis provides a more granular picture of a model's strengths and\nweaknesses. Furthermore, our reliance on a strong model to help humans understand evaluation data\nmay prove to be increasingly pertinent as models get so strong that only a small handful of people\nwill be qualified to provide manual annotations on evaluation instances. In this sense, our work"}, {"title": "A LIMITATIONS", "content": "A (somewhat surprising) key hypothesis our work hinges on is that models can articulate relevant\nskills, even for evaluation instances that they fail to answer correctly. While we empirically validate\nthis claim for (a subset of) the skill annotations we present, it is possible that on harder benchmarks,\nthe listed skills may no longer be accurate. Thus, we recommend always employing at least our\npost-hoc verification when collecting skill annotations on a new set.\nPerhaps what is more likely is that while listed skills are relevant, they may be incomplete. Note\nthat we do not present evidence for completeness in the main text. High overlap between two skill\nannotators is a promising sign (see Appendix C.2), though the fact that overlap is not perfect suggests\nsome skills may be missed. Moreover, most skills can always be broken down into smaller skills,\nmaking completeness perhaps an infeasibile and extraneous objective.\nLastly, generating rationales is more expensive than directly prompting a strong model to list skills,\nor simply using a encoder to embed an entire instance at once. We show rationales have some ad-\nvantages compared to each of these approaches in Appendix C.1 and Section 5 respectively, though\nthe added cost of our method is worth mention."}, {"title": "B COMPOSITION OF SKILL-SLICES IN OUR STUDY", "content": "Here, we enumerate the datasets we compute skill annotations for, as well as provide more details\non the resultant slices we form. All annotations and slices, which we call the Skill-Index, will be\nreleased to the broader community to enable skill-level analyses on models of their choice. We will\nalso release code to easily add to the Skill-Index.\nDatasets. We include datasets from 12 benchmarks in our study, consisting of 11 multimodal (image\nand text) datasets and 1 language-only dataset. These datasets are:\n\u2022 MMLU Pro, a language-only benchmark by Wang et al. (2024), intended to be a harder\nversion of MMLU (Hendrycks et al., 2021)\n\u2022 MMMU, a multimodal benchmark with college level questions from many academic sub-\njects intended to test 'expert' AI, by Yue et al. (2024)\n\u2022 MathVista, a mathematics visual undersanding benchmark by Lu et al. (2024)\n\u2022 MMC, a chart understanding multimodal benchmark by Liu et al. (2024)\n\u2022 MMVP, a benchmark specifically focusing on failure modes of VLMs, by Liang et al.\n(2024)\n\u2022 Many general multimodal benchmarks testing numerous abilities:\nMMBench, by Liu et al. (2023)\nMMTBench, by Ying et al. (2024)\nMME, by Fu et al. (2023)\nMMVet, by Yu et al. (2024b)\nSEEDBench, by Li et al. (2023)\n\u2022 Realworld-QA, a benchmark that claims to test many realworld visual understanding\nquestions, by xAI (2023)\n\u2022 VibeEval, by Padlewski et al. (2024) (also referred to as reka_vibe, as it was produced\nby the company Reka).\nThe two largest datasets in our benchmark suite are SEEDBench and MMLU Pro, which have 14k\nand 12k instances respectively.\nWe primarily sought out multimodal benchmarks, as many detailed benchmarks exist for language-\nonly tasks. We aimed to select popular benchmarks by prioritizing benchmarks that were featured\nin recent reports for model releases or those that had a large number of recent downloads on hug-\ngingface. We stress that benchmarks can easily be added to our corpus our experiments suggest\nour prompt is effective in diverse settings."}, {"title": "C DETAILS ON SKILL EXTRACTION AND VALIDATION", "content": "We present the prompt used to generate rationales below. It features detailed instructions and an in-\ncontext example. We generate this prompt by iteratively asking GPT-40 to refine the prompt based\non specific desiderata, along with manual tweaks along the way. The in-context example is also\ngenerated by GPT-40; it was produced using an earlier version of the prompt, and then modified\nslightly to correct some errors. Note that by GPT-40, we always mean GPT-40 2024-05-13."}, {"title": "C.1 ABLATIONS ON RATIONALE PARSING FOR EXTRACTING SKILLS", "content": "We now perform an ablation study to compare how skills inferred by rationale parsing (our pro-\nposed method) differ from those obtained by directly prompting a strong model to list relevant\nskills. Specifically, the 'direct prompting' alternate method consists of presenting an evaluation in-\nstance along with the prompt \u201cList skills that are relevant to the given question. Do not answer the\nquestion, only provide the list of skills. Be detailed, but only use short specific phrases throughout\nyour response \u2013 only use a few words per list item. I will provide you with an example first.\" As\nthe prompt suggests, we additionally provide an in-context example, primarily to ensure standard\nresponse format to facilitate automatic parsing of the skill annotator model's outputs. We compare\nthe annotated skills produced by GPT-40 over 1195 instances from all 12 benchmarks (roughly 100\nper benchmark) using direct prompting and rationale parsing."}, {"title": "C.2 DETAILS ON AUTOMATIC VALIDATION", "content": "We now detail our second automatic validation approach: inter-(skill)annotator agreement. As the\nname suggests, we simply extract skills by feeding the same prompt to a new rationale-generating\nmodel. Namely, we try Claude 3.5 Sonnet, Gemini 1.5 Pro, and GPT-4 Turbo 2024-04-09. We com-\npute agreement between two sets of skills listed by different models as the overall fraction of skills\nsuch that a match exists in the other set of skills. Here, we define two skills being matched if their\ntext embeddings have a cosine similarity of at least 0.85. Note that this is also the threshold we set\nfor T in sampling 'negative' skills for post-hoc verification. This threshold is lower than our cluster-\ning threshold because when sampling negative skills, we do not want to admit paraphrased versions\nof the same skill, where as in our clustering, we only wish to ensure extremely high similarity within\neach slice, since each slice ultimately only takes on one name"}, {"title": "D DETAILS ON ROUTING PROOF OF CONCEPT", "content": "We route each instance independently, based on skill-slice accuracies computed over the remaining\ncorpus (i.e. everything except for the current instance). For an instance, we first take all of its listed\nskills, and then remove any for which we do not have a skill-slice with at least 100 instances on the\nremaining corpus. If no skills remain, we default to Claude 3.5 Sonnet, which is (barely) the best"}, {"title": "E DETAILS ON PROBE CONSISTENCY ANALYSIS", "content": "We now provide complete details for our probe consistency analysis."}, {"title": "E.1 GENERATING PROBING QUESTIONS AND CHECKING CONSISTENCY", "content": "To generate probing questions and check consistency, we require multiple calls to an LLM \u2013 we use\nGPT-40. Importantly, neither task requires a multimodal LLM, thanks to skill localization.\nFirst, we rephrase a claim linked to a skill of interest with the below prompt. Notice that we form\nprobing questions in a few slightly varied way, including two yes or no questions testing contra-\ndictory questions. Qualitatively, this increases the chance that a model contradicts itself when it is\nunsure, as opposed to giving the same exact incorrect answer (which our consistency analysis can-\nnot detect). We omit prompts for answering probe questions and checking consistency, as these are\nstraightforward and can be viewed in our code release."}, {"title": "E.2 CORRELATION OF PROBE INCONSISTENCY AND SKILL-SLICE ACCURACY", "content": "In figure 11, we plot probe inconsistency rate vs slice accuracy directly and for each model\nseparately, as opposed to the consolidated violin plot we show in figure 7. We observe reasonably\nhigh correlations for all three models, especially given that probe inconsistency rate can only be a\nfactor of 0.05, as our implementation checks 20 claims per probed skill."}, {"title": "E.3 COMBINING SLICE AND PROBING ANALYSES TOWARDS AUTOMATIC DIAGNOSIS OF SKILL DEFICIENCIES", "content": "Lastly, we briefly discuss an end-to-end pipeline for automatically diagnosing skill deficiencies.\nAs shown in figure 12, a two-stage pipeline can be engineered that first flags skills with low slice\naccuracy, and then probes those skills. These two stages are complementary in two ways: first, they\nadmit different kinds of errors. If we interpret a deficient skill as a 'positive', low skill-slice accuracy\ncan admit false positives due to the presence of a highly co-occurring deficient skill. However, a\ndeficient skill will necessarily have low skill-slice accuracy. Probe inconsistency can help remove\nthese false positives, as a model will always answer consistently (i.e. correctly) to probe questions\nif the model is proficient at a skill.\nThe order of these components is also motivated by the fact that probing requires many more LLM\ncalls, and as such is more expensive than skill-slice analysis. Nonetheless, if cost is not a factor, this\nprocedure could potentially uncover skill-level deficiencies at scale."}, {"title": "F DETAILS ON SKILL-BASED RETRIEVAL", "content": "We use the same text encoder (SFR-Embedding-2_R) for any text features, including embedding\nskills and attributes. We use CLIP ViT-L/14 Radford et al. (2021) for image features. To generate\nattributes, we prompt GPT-40 to \u201cList attributes describing the content of given question. **Do not\nlist skills needed to solve the question**, just attributes of the content\". We additionally instruct\nGPT-40 to avoid answering the question we wish for it to attribute."}, {"title": "G PROMPT USED TO GENERATE MODEL OUTPUTS", "content": "We note that we employ a simple prompt in obtaining model outputs on the benchmarks we analyze.\nNamely, we ask models to \u201cBe concise. Write 'ANSWER: ' followed by your answer. If multiple\nchoices are given, only provide the correct letter.\" As is well documented, accuracies can be im-\nproved by engineering stronger prompts, including with model-specific strategies. We opt to choose\na single standard prompt for all models, so to avoid giving any single model an advantage due to\nunequal amounts of time spent on prompt optimization. We leave investigation of these for future\nwork and study models in a simple setting for now."}]}