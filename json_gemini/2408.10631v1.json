{"title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models", "authors": ["Yupeng Su", "Ziyi Guan", "Xiaoqun Liu", "Tianlai Jin", "Dongkuan Wu", "Graziano Chesi", "Ngai Wong", "Hao Yu"], "abstract": "Large language models (LLMs) have grown significantly in scale, leading to a critical need for efficient model pruning techniques. Existing post-training pruning techniques primarily focus on measuring weight importance on converged dense models to determine salient weights to retain. However, they often overlook the changes in weight importance during the pruning process, which can lead to performance degradation in the pruned models. To address this issue, we present LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a novel one-shot pruning framework that rebuilds the sparsity mask of pruned models without any retraining or weight reconstruction. LLM-Barber incorporates block-aware error optimization across Self-Attention and MLP blocks, ensuring global performance optimization. Inspired by the recent discovery of prominent outliers in LLMS, LLM-Barber introduces an innovative pruning metric that identifies weight importance using weights multiplied by gradients. Our experiments show that LLM-Barber can efficiently prune models like LLaMA and OPT families with 7B to 13B parameters on a single A100 GPU in just 30 minutes, achieving state-of-the-art results in both perplexity and zero-shot performance across various language benchmarks. Code is available at https://github.com/YupengSu/LLM-Barber.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have become a cornerstone in natural language processing (NLP) due to their impressive performance on various tasks. However, as these models increase in size and complexity, their deployment poses significant challenges due to extensive computational and storage demands. For instance, models such as GPT-175B (Brown et al. 2020), with 175 billion parameters, require vast resources, making it impractical for many applications. Therefore, efficient model compression strategies are crucial for deploying these powerful models in practical applications. Model compression techniques commonly employ quantization and pruning to enhance model efficiency. Quantization reduces the precision of model parameters, while pruning removes less critical parameters. Traditional pruning methods, such as magnitude-based pruning (Han et al."}, {"title": "Related Work", "content": "Network pruning reduces deep neural networks by removing unnecessary weights. For LLMs, pruning methods are divided into parameter efficient fine-tuning and post-training approaches. Parameter Efficient Fine-tuning (PEFT) begins with a initialized sparse network and refines it through iterative processes (Liu et al. 2019). LoRA (Hu et al. 2021) adapts pre-trained models to specific tasks or domains by injecting trainable rank decomposition matrices. Dynamic Spase No Training (Zhang et al. 2024) minimizes the reconstruction error by iteratively pruning and growing weights. However, fine-tuning requires ample data and often leads to performance decline. Recent studies advance toward one-shot post-training pruning, showing substantial improvements. Post-training pruning removes weights from a pre-trained model. SparseGPT (Frantar and Alistarh 2023) uses Hessian-based metrics and subsequent residual weight updates, while Wanda (Sun et al. 2023) introduces a first-order pruning metric using weight-activation products. LLM-Barber employs a block-aware reconstruction approach and rebuilds masks with a novel pruning metric."}, {"title": "Model Compression Strategy", "content": "Compression is key to reducing the memory and computational demands of model. This work highlights the block-aware compression strategy over traditional layer-aware approaches. Layer-aware compression strategy began with Optimal Brain Damage (LeCun, Denker, and Solla 1989) and Optimal Brain Surgeon (Hassibi, Stork, and Wolff 1993). Building on these foundations, recent works like GPTQ (Frantar et al. 2023) further enhance layer-aware quantization using second-order information. Block-aware compression strategy generally offer better accuracy recovery in pruned models compared to layer-aware methods. For instance, APTQ (Guan et al. 2024) applies global quantization to attention mechanisms, enhancing model robustness, while BESA (Xu et al. 2024) uses block-wise sparsity allocation. Our method leverages block-aware pruning to optimize global performance across Self-Attention and MLP blocks, effectively balancing efficiency and accuracy."}, {"title": "Block-Aware Rebuilder for Sparsity Mask", "content": "LLM pruning removes weights from dense networks to minimize output discrepancies, which is computationally intensive cross large-scale models, leading to address a layer-aware reconstruction problem (Hassibi, Stork, and Wolff 1993). This section reviews and reanalyses layer-aware reconstruction error and Taylor expansion at dense networks.\nLayer-aware Reconstruction Error. For linear projection layer weight W of shape $(C_{out}, C_{in})$, where $C_{out}, C_{in}$ indicates the output and input channels. With N calibration samples and sequence length L, the input activation is denoted as X with the shape of $(C_{in}, N \u00d7 L)$. Layer-aware reconstruction error E is defined as the $l_2$ norm difference between output of dense and sparse layers:\n$E(W, X) = ||WX \u2013 \\hat{W}X||_2$, (1)\nwhere $\\hat{W}$ is the element-wise product of W and a binary sparsity mask M(i, j) \u2208 {0, 1} of shape $(C_{out}, C_{in})$, in context with mask selection and without weight reconstruction:\n$E(M, X) = ||WX \u2013 (W \\odot M)X||_2$. (2)\nThe objective is to search for an optimal sparsity mask M where the pruning process reduces model complexity while preserving its predictive accuracy."}, {"title": "Taylor Expansion at Dense Networks", "content": "For a dense network $W_{dense}$ at a local minimum, the reconstruction error can be expanded into its Taylor series with respect to W, ignoring terms beyond second order:\n$E(W, X) = E(W_{dense}, X)+\\frac{\\partial E}{\\partial W} \\Delta W+\\frac{1}{2}\\Delta W^T H \\Delta W$. (3)\nWithout a sparsity mask in dense model, we can simply assign all-one matrix to the mask M, thereby yielding the zeroth-order terms $E(W_{dense}, X) = 0$. The first-order derivative $\u2202E/\u2202W$ vanishes when training converged (Frantar and Alistarh 2023; Hassibi, Stork, and Wolff 1993), leaving only the computationally expensive second-order terms involving a large Hessian matrix which are challenging for layer-wise reconstruction and channel-wise independence assumption."}, {"title": "Block-Aware Rebuilder for Sparsity Mask", "content": "In this work, we depart from existing post-training pruning methods in three key aspects: Firstly, to address the layer-aware reconstruction problem that leads to exponentially accumulating biases, we adopt a block-aware reconstruction error and apply a divide-and-conquer strategy to mitigate errors and computational costs. Secondly, inspired by the pruning-and-growing\u00b9 operation (Mocanu et al. 2018; Zhang et al. 2024), we address the limitations of mask selection of pruning in dense networks due to the changeable significance of weights, by re-evaluating weight importance score in sparse networks and rebuilding the sparsity mask through targeted growth of salient weights and pruning of non-salient weights. Thirdly, our analysis reveals that with the advancement of LLMs, mask selection becomes increasingly critical in weight reconstruction. Thus, we prioritize the rebuilding of the sparsity mask and strip the reconstruction of weights. Base on these insights, we propose LLM-Barber, a Block-Aware Rebuilder for Sparsity Mask in One-Shot without any need for fine-tuning or retraining."}, {"title": "Block-Aware Reconstruction Error", "content": "Building on the definitions in Eq. (1) and Eq. (2), we define the block-aware reconstruction error for a Self-Attention or MLP block:\n$E(M, X) = ||Block(W, X) \u2013 Block(W \\odot M, X)||_2$. (4)\nEvaluating reconstruction error across blocks, denoted as Block(), allows us to achieve a globally optimal solution in Self-Attention and MLP blocks rather than layer-wise. This new block-aware reconstruction formulation offers a more effective and cohesive strategy for selecting sparsity masks."}, {"title": "Taylor Expansion at Sparse Networks", "content": "Migrating Eq. (3) at sparse networks $W_{sparse}$ with an initialization sparsity mask Mi, we can obtain the Taylor series expansion as:\n$E(W, X) = E(W_{sparse}, X)+\\frac{\\partial E}{\\partial W} \\Delta W+\\frac{1}{2}\\Delta W^T H \\Delta W$. (5)\nThe zeroth-order term of the Taylor expansion represents the reconstruction error after mask initialization:\n$E(W_{sparse}, X) = ||Block(W, X)-Block(W \\odot M_i, X)||_2$. (6)\nAssuming non-negligible zeroth-order terms, the first-order gradient in sparse networks remains significant even after convergence and can be efficiently accessed via PyTorch's Autograd. First-order information provides computational efficiency and operates independently of any reconstruction error. Therefore, second-order terms can be omitted when significant gradients are present, leading to the following change in reconstruction error due to mask rebuilding:\n$\\Delta E = (\\partial E/\\partial W) \u00b7 \\Delta W$, (7)\nwhich delineates the importance score of weights during sparsity mask rebuilding."}, {"title": "Pruning Metric", "content": "For a Self-Attention or MLP Block with weights W, first-order information suffices for block-aware reconstruction error in sparse networks. The change in weight magnitude during sparsity mask adjustment matches the weight's original magnitude $(|\\Delta W_{ij}| = |W_{ij}|)$. We thus assess the impact of mask rebuilding on reconstruction error by computing the product of the weight's magnitude and its gradient. The importance score for $W_{ij}$ is:\n$S_{ij} = |W_{ij}|\u00b7|(\\partial E/\\partial W_{ij})|$, (8)"}, {"title": "Pruning Granularity", "content": "Choosing the right pruning granularity is crucial (Sun et al. 2023). Traditional methods operate on a layer-wise (Han et al. 2015), input-wise (Frantar and Alistarh 2023), or output-wise (Sun et al. 2023) basis, which mainly address the layer-aware reconstruction problem, known for its output channel independence. Wanda's output-wise ranking yields superior results compared to other methods. However, in LLM-Barber's block-aware framework, output channel independence is no longer applicable. Thus, LLM-Barber extends consideration to block-wise granularity, prioritizing all linear layers within a block. Our analysis of the four distinct granularity levels shows that optimal granularity depends on the specific sparse mask initialization, with detailed results discussed in the Ablation Study."}, {"title": "Mask Rebuilding", "content": "With the block-aware reconstruction error E, gradient information, sparsity metric, and granularity established, we proceed to rebuild the sparsity mask for each layer. Consider a cluster of weights $W^o$ under a specific sparsity granularity and its corresponding sparsity mask $M^o$. We define the growing criterion and the pruning criterion:\n$g_{i,j} = \\underset{i, j}{\\operatorname{argmax}} |W^o|\u00b7|(\\partial E/\\partial W^o)|, \\text{if } M^o_{g_{i,j}} = 0$, (10)\n$p_{i,j} = \\underset{i, j}{\\operatorname{argmin}} |W^o|\u00b7|(\\partial E/\\partial W^o)|, \\text{if } M^o_{p_{i,j}} = 1$. (11)\nThe growing and pruning weights form a mask rebuilding pair, representing the interchange within the sparsity mask. The value of each pair is defined as the difference between the importance scores of the growing and pruning weights. Our experiments show that LLM-Barber identifies varying proportions of salient weights depending on the sparse mask's initialization method. To control the extent of mask rebuilding, we introduce a hyperparameter $\\alpha$, called the mask rebuilding ratio. The number of mask rebuilding pairs N is calculated as:\n$N = {i | S^{grow}_{row} - S^{prune}_{row} > 0} \u00b7 \\alpha$, (12)\nwhere $S^{grow}_{row} - S^{prune}_{row}$ represents the value of the mask rebuilding pairs, where $S^{grow}$ is arranged in descending order, and $S^{prune}$ in ascending order. The subscript i denotes the number of values that exceeds zero, indicating that the growing weight is more important than the pruning weight."}, {"title": "Procedure", "content": "Like most post-training pruning methods (Han et al. 2015; Sun et al. 2023), LLM-Barber is executed within a single global LLM forward pass, with local backward passes for gradient computation in each block. Figure 2 illustrates the LLM-Barber workflow, which consists of four stages:\n\u2022 Sparsity Mask Initialization. We apply a post-training pruning technique to initialize a preliminary sparsity mask from the dense network.\n\u2022 Block-aware Reconstruction Error Computation. We use a block-aware reconstruction error to evaluate the discrepancy between the dense and sparse model outputs."}, {"title": "Experiment", "content": "Setup. LLM-Barber is implemented in Pytorch and utilized public model checkpoints from the HuggingFace library 2 on a single 80GB NVIDIA A100 GPU. After mask initilization, LLM-Barber uniformly rebuilds sparsity masks in sequence, performing in one-shot without any fine-tuning."}, {"title": "Conclusion", "content": "We propose LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a novel framework that rebuilds sparsity mask to optimize LLM post-training pruning. By integrating a block-aware approach across Self-Attention and MLP blocks, LLM-Barber effectively reallocates sparsity to improve accuracy without the need for extensive fine-tuning. Specifically, LLM-Barber identifies novel importance score after mask initialization and rebuilds the sparsity mask with mask rebuilding pairs, simultaneously applying new sparsity masks to weights that have become less critical, thereby optimizing overall model performance. By utilizing the novel pruning metric as the product of weights and gradients, our approach enables precise and efficient reallocation of sparsity mask. Extensive experiments on pruning LLaMA series models demonstrate that LLM-Barber achieves state-of-the-art results in both perplexity and zero-shot performance within the domain of post-training pruning."}]}