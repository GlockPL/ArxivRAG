{"title": "Pitfalls of defacing whole-head MRI: re-identification risk with diffusion models and compromised research potential", "authors": ["Chenyu Gao", "Kaiwen Xu", "Michael E. Kim", "Lianrui Zuo", "Zhiyuan Li", "Derek B. Archer", "Timothy J. Hohman", "Ann Zenobia Moore", "Luigi Ferrucci", "Lori L. Beason-Held", "Susan M. Resnick", "Christos Davatzikos", "Jerry L. Prince", "Bennett A. Landman"], "abstract": "Defacing is often applied to head magnetic resonance image (MRI) datasets prior to public\nrelease to address privacy concerns. The alteration of facial and nearby voxels has provoked\ndiscussions about the true capability of these techniques to ensure privacy as well as their\nimpact on downstream tasks. With advancements in deep generative models, the extent to\nwhich defacing can protect privacy is uncertain. Additionally, while the altered voxels are known\nto contain valuable anatomical information, their potential to support research beyond the\nanatomical regions directly affected by defacing remains uncertain. To evaluate these\nconsiderations, we develop a refacing pipeline that recovers faces in defaced head MRIs using\ncascaded diffusion probabilistic models (DPMs). The DPMs are trained on images from 180\nsubjects and tested on images from 484 unseen subjects, 469 of whom are from a different\ndataset. To assess whether the altered voxels in defacing contain universally useful information,\nwe also predict computed tomography (CT)-derived skeletal muscle radiodensity from facial\nvoxels in both defaced and original MRIs. The results show that DPMs can generate high-fidelity\nfaces that resemble the original faces from defaced images, with surface distances to the\noriginal faces significantly smaller than those of a population average face (p < 0.05). This\nperformance also generalizes well to previously unseen datasets. For skeletal muscle\nradiodensity predictions, using defaced images results in significantly weaker Spearman's rank\ncorrelation coefficients compared to using original images (p \u2264 104). For shin muscle, the\ncorrelation is statistically significant (p < 0.05) when using original images but not statistically\nsignificant (p > 0.05) when any defacing method is applied, suggesting that defacing might not\nonly fail to protect privacy but also eliminate valuable information. We advocate two solutions for\ndata sharing that comply with privacy: 1) share skull-stripped images along with measurements\nof facial and cranial features extracted before skull-stripping for public access, while\nacknowledging that this approach inherently compromises many research potentials; or 2) share\nthe unaltered images with privacy enforced through policy restrictions.", "sections": [{"title": "1. Introduction", "content": "The practice of defacing whole-head MRI has become increasingly widespread. This trend is\ndriven by multiple factors, including improved image quality, enhanced 3D reconstruction\ncapabilities, more powerful facial recognition techniques, growing efforts in data sharing, and\nincreased emphasis on data privacy due to public awareness and institutional or governmental\nrequirements. High-resolution structural MRIs can reveal facial features that can potentially be\nused to identify individuals.1,2 This type of information is protected by regulations such as the\nHealth Insurance Portability and Accountability Act (HIPAA) in the United States and General\nData Protection Regulation (GDPR) in Europe. Due to these regulations, and despite variations\nin specific requirements by each Institutional Review Board (IRB), more entities are applying\ndefacing techniques to MRI datasets before sharing. For instance, the UK Biobank\u00b3 uses\nFSL_deface\u2074 for T1-weighted (T1w) and T2-weighted (T2w) MRIs. Similarly, the Human\nConnectome Project (HCP) datasets use Face_Masking\u00ae for T1w and T2w MRIs.\nThere are concerns about defacing MRI. Thorough discussion in the literature has brought three\nkey issues to light. First, defacing itself can fail in two ways: it can be too conservative, failing to\nremove enough facial features and leaving the MRIs recognizable by facial recognition software,\nor it can be too aggressive, altering voxel intensities beyond the facial regions or, worse, within\nthe brain. 7-9 Second, defacing can also cause failures in other processing pipelines, including,\nbut not limited to image registration,10 whole-brain segmentation,7,11 white matter lesion\nsegmentation, 12 and glioblastoma segmentation.12 Third, defacing has impact on the\nreproducibility of analyses.7,8,11\u201313 In addition to these known issues, new concerns have\nemerged with recent progress of deep generative models, e.g., the diffusion probabilistic models\n(DPMs), as these emerging technologies might be able to recover faces from defaced images. If\ntrue, this capability would challenge the rationale of applying defacing in the context of modern\nAl advancements. Additionally, non-brain regions in whole-head MRIs contain valuable\ninformation that could be used for various studies. 14\u201322 Defacing removes or alters these voxels,\nthereby eliminating this research potential and limiting the scope of possible scientific inquiries.\nIn this paper, we delve into the two emerging concerns (Figure 1). First, we show that DPMs can\ngenerate 3D high-resolution MRIs with realistic faces that resemble the original faces from\ndefaced images, demonstrating a potential malicious privacy attack. Second, we investigate a\nspecific example of the valuable information contained within facial voxels that are typically\nremoved by defacing. Specifically, we perform skeletal muscle radiodensity predictions using\nfacial voxels in original and defaced MRIs and compare the discrepancy in performance."}, {"title": "1.1 High-Resolution 3D Refacing of Defaced MRI", "content": "Refacing can be considered an inpainting task, where a generative model, trained to\napproximate the probability distribution of the data, imputes missing regions by sampling from\nthe probability distribution based on the observed regions. In medical image analysis, previous\nstudies have demonstrated the possibility to extend imaging boundaries to restore anatomical\nstructures truncated by restricted field-of-view, 23,24 adding value to downstream clinically"}, {"title": "1.2 Predicting Skeletal Muscle Radiodensity from Original vs.\nDefaced Facial Voxels", "content": "Non-brain regions in whole-head MRI have been shown to contain valuable information in\nvarious contexts. For instance, Hitomi et al. demonstrated that gadolinium leakage into ocular\nstructures on postcontrast FLAIR MRI is associated with acute stroke.14 Wiseman et al. showed\nthat the axial length of the eye can be measured from MRI, providing a proxy for eye size in the\nabsence of biometry.15 Mi et al. quantified temporalis muscle area from MRI, providing a\nsarcopenia-related metric associated with oncological outcomes.16 These examples represent\nonly a small fraction of the anatomical and clinical insights that could be derived from non-brain\nregions. Other regions, such as the meninges17, tongue18, sinuses19, ear canals20, marrow21,\nand subcutaneous fat21,22, also hold significant research potential that could be compromised by\ndefacing.\nWe explore the value of non-brain regions from a new perspective by predicting abdomen,\nthigh, and shin muscle radiodensities, measured from body CT data, from facial voxels in head\nMRI. Specifically, we include subjects with paired body CT and T1w MRI and apply four\ndefacing methods to the T1w images. Using the remaining facial voxels from each type of\ndefaced images, we train separate 3D residual neural networks (ResNet) to predict the muscle\nradiodensity of the abdomen, thigh, and shin, respectively. To examine the impact of information\nloss caused by defacing, we compare the correlation between predicted and ground truth values\nderived from defaced images with the correlation derived from original (non-defaced) images."}, {"title": "2. Cascaded Diffusion Models for Refacing Defaced\nMRI", "content": "We consider a two-stage refacing pipeline that takes in defaced MRIs and generates high-\nresolution 3D MRIs with faces intended to resemble the original faces. We measure the\nsimilarity between the generated images and the original images from both a face re-\nidentification risk perspective and an image quality perspective."}, {"title": "2.1 Model Architecture", "content": "Inspired by the cascaded diffusion models described by Ho et al.29, we use a two-stage\napproach to achieve high-resolution 3D image generation (Figure 2). This approach offers\nadvantages over directly training a single 3D model in high resolution, including computational\nefficiency.\nOur pipeline consists of two diffusion models, one for each stage, both of which are Denoising\nDiffusion Implicit Models (DDIM)30 (Figure 2). The stage-1 model is 3D and imputes the missing\nfacial voxels, conditioned on the downsampled defaced image, to generate a low-resolution\nrefaced image. The stage-2 model is 2.5D and performs super-resolution enhancement,\nconditioned on the up-sampled, low-resolution refaced image from stage-1 and the defaced\nimage. This is done in a slab-wise manner, where each slab consists of a stack of 8 axial slices.\nTo mitigate border effects commonly observed in 2.5D approaches, adjacent slabs have 4\noverlapping slices, allowing them to share anatomical context. The high-resolution slabs from\nstage-2 are then merged to form the complete high-resolution 3D refaced image. Our\nimplementation of the diffusion models is adapted from Song et al.'s implementation30, available\nat https://github.com/ermongroup/ddim. We slightly modify the DDIM denoising scheme to let\nthe model to predict the ground-truth image $x_0$ at each denoising step $t$ instead of predicting the\nnoise $ \\epsilon_\\theta (x_t, t )$. $ \\epsilon_\\theta (x_t, t )$ is estimated using the predicted $x_0$ following the first section of formula (12) in\nSong et al. (2021)30. Then, we follow formula (12) to infer the less noisy image $x_{t-1}$ for the next\ndenoising step. This modification avoids the challenges in converging during the denoising\nprocedure, which is observed when directly applying DDIM denoising scheme."}, {"title": "2.2 Data", "content": "Since most MRI defacing tools work for T1w MRI, we use T1w images for our experiments. We\ninclude 21, 179, and 469 subjects from the Kirby-21 31, OASIS3 32, and BLSA datasets 33,\nrespectively, with one T1w image per subject. Of the 200 subjects from Kirby-21 and OASIS3,\n180 were used to train the diffusion models, while 20 were reserved for internal testing. Five of\nthese internal testing subjects were excluded because their faces were already not visible even\nbefore defacing. The 469 BLSA subjects, whose faces were visible, serve as an external testing\nset. We apply FSL_deface\u2074, MRI_Deface34, Pydeface35, and Quickshear36 to the T1w images. In\ntotal, there are 720 (180 subjects * 4 defacing methods) pairs of training data."}, {"title": "2.3 Similarity Metrics", "content": "To quantitatively measure the similarity between faces in two images, we use the mean absolute\nsurface distance (MASD). First, we generate a binary mask of the head in each image using\nOtsu's thresholding\u00b37 followed by morphological operations. To focus the comparison on the\nface, we crop out 10 axial slices at the inferior to exclude the neck and the posterior half of the\nhead to exclude the back of the head. Using these binary masks, we apply the marching cubes\nalgorithm to extract a mesh of vertices representing the surface of each face. We then use a K-\ndimensional tree (KDTree)38,39 to query the nearest neighbor on the surface of one face for\nevery vertex on the other face and compute the absolute distance. The MASD between the two\nfaces is obtained by averaging these distances.\nTo measure the similarity of two images from an image quality perspective, we report the\nstructural similarity index measure (SSIM) and peak signal-to-noise ratio (PSNR). SSIM and\nPSNR are computed for two regions: 1) the whole head, defined by the binary mask of the head\nin the original image; and 2) the area that has been changed (Supplementary Material),\nspecifically the intersection of areas removed by defacing, altered by mri_reface, and imputed\nby our refacing. The masks used for calculating these metrics were visually inspected for quality\nassurance."}, {"title": "3. Estimation of Skeletal Muscle Radiodensity from\nFacial Voxels", "content": "We estimate CT-derived abdomen, thigh, and shin skeletal muscle radiodensities from facial\nvoxels in T1w MRI. We perform these estimations using either original or defaced T1w images.\nBy comparing the results, we evaluate the impact of defacing on the accuracy of estimating\nbody measurements from head MRI."}, {"title": "3.1 Paired MRI-CT Data", "content": "We include subjects from BLSA33 who have paired whole-head T1w MRI and single-slice CT\ndata of the abdomen, thigh, and shin. For the T1w images, we apply defacing methods including\nFSL_deface\u2074, MRI_Deface34, Pydeface35, and Quickshear36 to generate four types of defaced\nimages from the original images. As a baseline reference, representing the most aggressive\ndefacing where all facial voxels are removed, we apply skull-stripping using segmentation\nmasks produced by SLANT-TICV40 to obtain skull-stripped images from the original images.\nEach original T1w image is affinely registered to the NMRI225 template\u2074\u00b9, a T1w MRI template"}, {"title": "3.2 Regressing Out Age and Sex", "content": "Age and sex are two confounding factors that are strongly correlated with skeletal muscle\nradiodensity. Since our goal is to investigate whether facial voxels in T1w images contain\ninformation specific to abdomen, thigh, and shin muscle radiodensity, we regress these two\nconfounding factors out of the muscle radiodensity measurements. Specifically, we fit three\nlinear mixed-effects models45, one for each body part (abdomen, thigh, or shin), using the\nformula:\n$Yi,j = \\beta_0 + \\beta_1 \\times Age_{i,j} + \\beta_2 \\times Sex_i + r_i + \\varepsilon_{i,j}$ (1)\nwhere $y_{i,j}$ is the muscle radiodensity of subject $i$ at visit $j$, $Age_{i,j}$ is the age of subject $i$ at visit $j$,\n$Sex_i$ is the sex of subject $i$, $r_i$ represents the random intercept for each subject, and $ \\varepsilon_{i,j}$ is the\nresidual error of the linear mixed-effects model. We then subtract the predicted muscle\nradiodensity $\\hat{y}_{i,j}$ by age and sex from the observed muscle radiodensity to obtain the residuals,\nusing the equation:\n$Y_{i,j} \u2014 \\hat{Y}_{i,j} = Y_{i,j} \\beta_0 \u2013 \\beta_1 \\times Age_{i,j} \u2014 \\beta_2 \\times Sex_i$ (2)\nThese residuals will be the target of our regression models, which are introduced in the next\nsection."}, {"title": "3.3 Regression Model", "content": "We use 3D residual neural networks (ResNet)46, specifically ResNet10, initialized with weights\npretrained on large medical image datasets47. The input to the ResNet is one of six types of\ncropped 3D images containing only facial voxels. Each image is intensity-normalized before\nbeing fed into the ResNet. The target of the ResNet is the muscle radiodensity in one of the\nbody parts (abdomen, thigh, or shin) with age and sex regressed out using equation (2). The\nvalues are normalized using the mean and standard deviation computed from the training set.\nFor each input-target pair, we train a separate ResNet. In total, there are 18 (6 types of images *\n3 body parts) ResNets. All ResNets are trained using the same hyperparameters. To mitigate\nthe models' bias towards subjects with more samples, the probability of selecting each sample\nis normalized based on the number of samples available for each subject."}, {"title": "4. Results", "content": "Figure 3 shows a testing subject from Kirby-21 whose face was removed by MRI_Deface and\nsubsequently recovered by our DPMs. The DPMs-refaced face closely resembles the original\nface, based on visual inspection, whereas the linearly aligned population average face appears\nless similar to the original face. This visual observation is consistent with the MASD\nmeasurements: for DPMs-refaced face, the MASD to the original face is 0.363 mm, compared\nto 0.792mm for the linearly aligned population average face. White patches were placed over\nthe eyes in the screenshots of the renderings to obscure subject identity, but all computations\nwere performed without such obscuring. For the same subject, we present 3D renderings of the\nimages refaced from each type of defaced image by our DPMs, ordered by ascending MASD to\nthe original face (Figure 4). The MASD generally aligns with the perceptual dissimilarity between\nthe two faces. On skull-stripped images, the DPMs (retrained on skull-stripped images) fail to\nreconstruct faces that resemble the original ones (Figure 5).\nOn the internal testing set of 15 subjects, MASDs between DPMs-refaced faces and original\nfaces are significantly smaller compared to MASDs between population average faces and\noriginal faces (Table 1). Among the DPMs-refaced faces, those generated from MRI_Deface-\ndefaced images have the smallest MASD (0.34 mm), while those generated from Quickshear-\ndefaced images have the largest MASD (0.63 mm). Similarly, on the external testing set of 469\nsubjects, MASDs between DPMs-refaced faces and original faces are significantly smaller\ncompared to MASDs between population average faces and original faces. Among the DPMs-\nrefaced faces, those generated from MRI_Deface-defaced images have the smallest MASD\n(0.38 mm), while those generated from Pydeface-defaced images have the largest MASD (0.67\nmm). In general, DPMs-refaced images have higher PSNR and SSIM in the whole head area,\nbut lower PSNR and SSIM in the facial area removed by defacing, compared to mri_reface\nprocessed images (Tables 2 and 3).\nFor all three body parts, defacing results in significantly lower Spearman's rank correlation\ncoefficients between the predicted and ground truth values of muscle radiodensity (p \u2264 10-4)"}, {"title": "5. Discussion", "content": "5.1 The cascaded DPMs can generate MRIs with high-fidelity\nfaces from defaced MRIS\nOur refacing pipeline based on DPMs can generate high-resolution 3D T1w MRIs with faces\nthat resemble the original faces, from defaced images. We used MASD as a metric to quantify\nthe similarity between two faces. This metric generally aligns with our perceptual impression of\nwhether two faces belong to the same person (Figure 4). The DPMs-refaced faces have a\nsmaller MASD to the original faces compared to the linearly aligned population average face.\nThis implies that the DPMs leverage the underlying anatomical information present in the MRIs,\nallowing the model to reconstruct the defaced regions in a way that is more personalized and\naccurate, reflecting individual anatomical features rather than just an averaged representation.\nAdditionally, we found that the more aggressive the defacing, the more challenging the refacing\nbecomes, as indicated by a larger MASD to the original faces (Figure 4 and Table 1). On the\nskull-stripped images, the DPMs fail to reconstruct the faces (Figure 5).\nOn the other hand, the DPMs-refaced faces do not achieve higher (and often even lower) SSIM\nand PSNR compared to the population average face within the facial voxels (Tables 2 and 3).\nWe attribute this to two main factors. First, the population average face applied by mri_reface\nwas registered to the original faces, whereas the DPMs generate the face without any access to\nthe original faces during inference. Consequently, the linearly aligned population average face\naligns the internal structure of the face better compared to the DPMs-refaced faces. Second, the\nDPMs were trained on a relatively small dataset of only 180 subjects. It is reasonable to expect\nthat using larger datasets that encompass a diverse range of cohorts for training would lead to\nhigher SSIM and PSNR values for the refaced images.\n5.2 Facial voxels removed by defacing contain valuable\ninformation\nAs mentioned in Section 1.2, studies have shown the value of non-brain regions in whole-head\nMRIs for various applications. These studies provide examples where defacing may\ncompromise the feasibility of certain research, as the regions of interest are corrupted by\ndefacing. To further illustrate the impact of defacing on studies beyond the regions directly\naltered, we explore the connections between MRI facial features and overall body compositions."}, {"title": "6. Conclusion", "content": "These concerning results indicate that defacing might not only fail to protect privacy in the face\nof DPMs but also eliminate valuable information, thereby compromising research potential. We\nadvocate two solutions for data sharing in compliance with privacy: 1) share skull-stripped\nimages along with measurements of facial and cranial features extracted before skull-stripping\n(e.g., intracranial volume) for public access, while acknowledging that this approach inherently\ncompromises many research potentials; or 2) share the unaltered images with privacy enforced\nthrough the use of policy restrictions."}, {"title": "Data Sharing Statement", "content": "Our models will not be released. Code is available upon request and agreement not to use or\ndisseminate it for re-identification purposes."}, {"title": "Declaration of generative Al and Al-assisted\ntechnologies in the writing process", "content": "During the preparation of this work the author, Chenyu Gao, used GPT-40 in order to check\ngrammar. After using this tool/service, the author reviewed and edited the content as needed\nand takes full responsibility for the content of the publication."}, {"title": "Author Contributions", "content": "Chenyu Gao: Conceptualization, Methodology, Software, Validation, Formal analysis,\nInvestigation, Data Curation, Writing - Original Draft, Writing - Review & Editing, Visualization.\nKaiwen Xu: Conceptualization, Methodology, Software, Validation, Writing - Review & Editing.\nMichael E. Kim: Validation, Data Curation, Writing - Review & Editing. Lianrui Zuo: Validation,\nWriting - Review & Editing. Zhiyuan Li: Writing - Review & Editing. Derek B. Archer: Writing -\nReview & Editing, Funding acquisition. Timothy J. Hohman: Writing - Review & Editing,\nFunding acquisition. Ann Zenobia Moore: Resources, Writing - Review & Editing. Luigi\nFerrucci: Resources, Writing - Review & Editing. Lori L. Beason-Held: Resources, Writing -\nReview & Editing. Susan M. Resnick: Resources, Writing - Review & Editing. Christos\nDavatzikos: Writing - Review & Editing. Jerry L. Prince: Writing - Review & Editing. Bennett\nA. Landman: Conceptualization, Validation, Resources, Writing - Review & Editing,\nSupervision, Project administration, Funding acquisition."}]}