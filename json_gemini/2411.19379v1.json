{"title": "MARCONI: PREFIX CACHING FOR THE ERA OF HYBRID LLMS", "authors": ["Rui Pan", "Zhuang Wang", "Zhen Jia", "Can Karakus", "Luca Zancato", "Tri Dao", "Ravi Netravali", "Yida Wang"], "abstract": "Hybrid models that combine the language modeling capabilities of Attention layers with the efficiency of Recurrent layers (e.g., State Space Models) have gained traction in practically supporting long contexts in Large Language Model serving. Yet, the unique properties of these models complicate the usage of complementary efficiency optimizations such as prefix caching that skip redundant computations across requests. Most notably, their use of in-place state updates for recurrent layers precludes rolling back cache entries for partial sequence overlaps, and instead mandates only exact-match cache hits; the effect is a deluge of (large) cache entries per sequence, most of which yield minimal reuse opportunities. We present Marconi, the first system that supports efficient prefix caching with Hybrid LLMs. Key to Marconi are its novel admission and eviction policies that more judiciously assess potential cache entries based not only on recency, but also on (1) forecasts of their reuse likelihood across a taxonomy of different hit scenarios, and (2) the compute savings that hits deliver relative to memory footprints. Across diverse workloads and Hybrid models, Marconi achieves up to 34.4\u00d7 higher token hit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix caching systems.", "sections": [{"title": "1 INTRODUCTION", "content": "The emergence of large language models (LLMs) has em- powered many applications including chatbots (cha, 2022), AI-powered search (per, 2023), coding assistants (git, 2022), and more. Owing to increasing workload complexity and the evolution of inference-time enhancements such as few- shot prompting (Brown, 2020) and chain-of-thoughts (Wei et al., 2022), recent years have witnessed a push towards longer context windows during serving. Indeed, the accu- racy improvements from multi-step reasoning (Khattab et al., 2023; Yao et al., 2022), detailed prompt templates (Liu et al., 2023), and increased samples in few-shot prompting (Brown, 2020) all require expanded context sizes.\nTo keep pace with these trends, traditional foundation mod- els like Transformers have been extended to support longer context windows, e.g., Gemini 1.5 (Team et al., 2023) and Claude 3 (cla, 2024b) both have 1M window size. How- ever, their intrinsic reliance on the Attention mechanism harms long-context serving efficiency, particularly due to its quadratic compute complexity and large memory footprint for housing KV cache states (Wu et al., 2024; Lee et al., 2024). As a result, new recurrent and subquadratic model architectures like State Space Models (SSMs) (Gu & Dao, 2023; Dao & Gu, 2024) have emerged to offer lower com- pute and memory costs for long-context serving (Fig. 1c),"}, {"title": "2 BACKGROUND", "content": "In this section, we provide background on the efficient re- current architectures (exemplified by SSMs), Hybrid mod- els that contain these alternative architectures, and prefix caching for efficient LLM inference."}, {"title": "2.1 State Space Models and Hybrid Models", "content": "Token generation in LLM inference involves two main phases. First, the prefill phase processes an input sequence, generating the model's internal states (e.g., KV cache/KVs in Attention layers) for each layer of an LLM, and outputs the first new token. The decoding phase then utilizes the internal states to perform autoregressive token generation.\nDuring prefill, generating the first token depends on all previous tokens. In Transformers, the self-attention mecha- nism calculates how each token in the sequence \u201cattends\u201d to every other token. Consequently, the Attention mecha- nism incurs quadratic computational complexity (Dao et al., 2022), which quickly bottlenecks GPU compute as sequence lengths scale (Agrawal et al., 2024). Furthermore, the size of the KVs in Attention layers grows linearly with sequence length, resulting in a large inference-time memory require- ment (Wu et al., 2024; Lee et al., 2024; Gao et al., 2024).\nState space models (SSMs) and more generally linear RNNs and linear attention, such as Mamba (Gu & Dao, 2023; Dao & Gu, 2024), address these inefficiencies by selec- tively \u201ccompressing\u201d previous context into a recurrent and compact representation. The recurrent representation is used alongside the previous token to update the recurrent representation in place, as shown in Fig. 1b. Because the SSM states maintain a constant size, memory consumption remains fixed regardless of sequence length, and the compu- tational complexity scales linearly, rather than quadratically, with the sequence length (Fig. 1c). Although pure SSM models outperform Transformers on many NLP tasks, they lag behind on certain workloads that require strong recall or in-context learning capabilities (Waleffe et al., 2024; nee, 2024). To balance inference efficiency and model capability, SSM-Attention Hybrid models (Fig. 1a) have been proposed. These models blend quadratic Attention and subquadratic SSM layers, typically interleaved in a specific ratio (com- monly 1 Attention layer for every 6-10 SSM layers (Waleffe et al., 2024; Lieber et al., 2024; Glorioso et al., 2024)). When compared to Transformers of equivalent scale trained on the same datasets, Hybrid models demonstrate superior performance across a wide range of tasks while preserving most of the efficiency advantages of SSM layers (up to 8\u00d7 faster) (Waleffe et al., 2024). Many Hybrid models have been productionized (car, 2024; Lieber et al., 2024; Team et al., 2024; Glorioso et al., 2024; zam, 2024a;b; Waleffe et al., 2024; Ren et al., 2024), with the largest being Jamba 1.5 at 398B parameters (Team et al., 2024)."}, {"title": "2.2 Prefix Caching", "content": "Shared prefixes across requests, including input tokens and sometimes output tokens, are common across many LLM ap- plications. For example, question-answering workloads of- ten share a detailed system prompt combined with few-shot examples that provide instructions and demonstrations (Yao et al., 2022). Similarly, coding agents interact with the environment in multiple rounds, where each new request consists of a trajectory of past environment interactions and new observations and actions (Yang et al., 2024a; Wang et al., 2024). Redoing the prefill of these shared prefixes for all requests leads to many redundant computations, hurt- ing both throughput and latency. Prefix caching mitigates this by caching and reusing the model's internal states that represent the common prefixes (Fig. 2), achieving a lower time to first token (TTFT) latency, lower tail time per token (TPT) latency2, and higher prefill throughput (measured in tokens/s).\nMany research and production systems have been proposed to reap the benefits of prefix caching in Transformer infer- ence (Kwon et al., 2023; Zheng et al., 2023b; Gao et al., 2024; Srivatsa et al., 2024; Abhyankar et al.; cla, 2024a; cha, 2024; Qin et al., 2024). On startup, these systems provision blocks of GPU/CPU memory for caching the prefix states. Before prefilling a new sequence, the inference engine looks up the prefix cache for the longest matching prefix. Upon a cache hit, the corresponding prefix is fetched before prefill- ing. After decoding the final token of the sequence, to favor recency, the system admits the model states of all tokens of the new sequence into the cache. To reduce memory frag- mentation, the KVs are usually partitioned into fixed-sized token blocks in the prefix cache, each housing the KVs of x tokens where x is the block size (Kwon et al., 2023), and existing token blocks are evicted to make room if the cache is full. Because KVs have a sequence dimension, managing and evicting model states is efficient and flexible. E.g., if we have the KVs of a sequence of tokens 1...q and want to retain the KVs of the prefix 1...p (p < q), tensor slicing can be performed on the sequence dimension of the full KVs."}, {"title": "3 CHALLENGES OF PREFIX CACHING WITH HYBRID LLMS", "content": "Compared to Attention, SSM's properties greatly benefit its per-request computational complexity and memory con- sumption. However, the very properties that make SSMs more efficient also complicate prefix caching, an important optimization for cross-request efficiency wins. The key reason lies in how SSM updates its internal states: SSMs"}, {"title": "4 DESIGN AND IMPLEMENTATION", "content": "Marconi is the first prefix caching system designed to ac- commodate the unique characteristics of Hybrid models. It is designed to support models with arbitrary layer compo- sitions, including Hybrid models, pure Transformers, and pure SSMs. Its primary goal is maximizing the cache utiliza-"}, {"title": "4.1 Cache Admission", "content": "Marconi aims to cache the states with high reuse likelihood during admission. Although prefix reuse patterns of future requests cannot be perfectly predicted, our key insight is that the reuse potential can be sufficiently estimated through a taxonomy of potential prefix reusing scenarios. Through extensive analysis of prefix reusing patterns in various real- world datasets and request traces (Qin et al., 2024; cha, 2024; Zheng et al., 2023b) that represent traffic of both dedicated inference deployments (Jimenez et al., 2023) and public-facing APIs (cha, 2022; sha, 2024), we classify the token composition of all reused prefixes into two types:\n1. Purely input: The prefix is a part of the input sequence from a previous input, such as system prompts (cha, 2022), instructions (Yao et al., 2022), few-shot exam-"}, {"title": "4.2 Cache Eviction", "content": "For eviction, our main observation is that KVs and SSMs in Hybrid model states exhibit different tradeoffs between memory and compute savings. Specifically, whereas the size of KVs for a sequence is linearly proportional to the sequence length and (approximately) the compute savings from reusing that sequence, SSM state sizes are fixed regard- less of sequence length and compute savings. To quantify this difference, we propose a new metric, FLOP efficiency, to measure the compute savings (measured in the number of floating operations) per unit of memory achieved by reusing a prefix cache entry:\n$\\flop-efficiency = \\frac{Total FLOPs across layers}{Memory consumption of all states}$ (1)\nHere, Total FLOPs across layers denotes the sum of re- dundant compute across different types of layers (i.e., At- tention, SSM, and MLP) circumvented by reusing the prefix entry, and Memory consumption of all states denotes the total size of all stateful layers' states (i.e., Attention and SSM). More details on FLOP efficiency are in Appendix A.\nIn addition to recency, Marconi accounts for the FLOP efficiency by computing a utility score S that represents the utility for each radix node n:\n$S(n) = recency(n) + a flop-efficiency(n)$ (2)\nThis metric favors cache entries with higher recency, save more compute, and take less memory. Both the recency"}, {"title": "4.3 Implementation details", "content": "Alongside the custom admission and eviction policies, we made the following changes to accommodate Hybrid model states: (1) During eviction, all nodes with < 1 children are considered for eviction, not just leaf nodes. The reasoning is that nodes with multiple children represent the common pre- fixes shared by multiple requests and should not be evicted (unless they become stale, at which point all their children will be evicted first and the ex-parent nodes will become leafless nodes themselves, which make them subject for eviction), whereas intermediate nodes with a single child are unlikely to be reused more than once and still incur a memory cost for their SSM states. When an intermediate node is evicted, its SSM states are released, and its KVs are absorbed by its child node. (2) When a cache hit occurs, only the accessed node's timestamp is updated, unlike in existing systems where timestamps for all ancestor nodes are updated. In Marconi, previous SSM states are not reused (Fig. 4c), and although ancestors' KVs are accessed, their KVs will be subsumed by child nodes if evicted. Thus, not updating ancestors' timestamps doesn't affect recency tracking."}, {"title": "5 EVALUATION", "content": "We evaluated the performance of Marconi under various workloads with different datasets, request arrival patterns, cache sizes, and model architectures. Our key findings are:\n1. Overall, Marconi improves token hit rate by an average of 4.5-34.4\u00d7 compared to fine-grained checkpointing, reducing the P95 TTFT by up to 71.1% (617.0 ms) compared to baseline prefix caching systems.\n2. Compared to LRU, Marconi's FLOP-aware eviction improves the token hit rate by 19.0-219.7%. It achieves higher token hit rates and FLOP savings by trading off hit rate of shorter sequences to boost hit rate for longer sequences, a desirable tradeoff given the efficiency of Hybrid models over Transformers.\n3. Marconi performs better when sequences are long, the SSM layer ratio is high, and the SSM state dimension is large - trends that align with recent model develop- ments."}, {"title": "5.1 Methodology", "content": "Baselines. We compare Marconi with the following base- lines. Note that neither vLLM nor SGLang natively supports prefix caching for Hybrid/SSM models, so we have extended both to support Hybrid models favorably.\n\u2022 Vanilla inference: This baseline prefills all requests with- out doing prefix caching.\n\u2022 VLLM+ (Kwon et al., 2023): This baseline performs fine- grained checkpointing and caches a state for every token block. We use a token block size of 32, the largest size that vLLM supports (vll, 2024), which favors vLLM+ by minimizing the number of low-utility SSM states admit- ted (Fig. 3a) while reducing memory fragmentation of KVs within a token block.\n\u2022 SGLang+ (Zheng et al., 2023b): While SGLang also uses a radix tree, it doesn't judiciously checkpoint states during admission. We enhance it by applying the same judicious admission policy as Marconi; however, the evic- tion policy remains LRU, which does not account for FLOP efficiency.\nMetrics. The main metric we evaluate is token hit rate"}, {"title": "5.3 Fine-Grained Analysis of FLOP-Aware Eviction", "content": "To understand the performance improvement of FLOP- aware eviction over LRU, we compare the caching decisions of Marconi with SGLang+ using a request arrival trace from SWEBench. On this trace, SGLang+ achieves a 16.4% over- all token hit rate, while Marconi achieves a significantly higher hit rate of 32.7%, an improvement of 99.4%.\nIn Fig. 10a, we categorize the requests by sequence length and plot the difference in average hit rate between SGLang+ and Marconi. Marconi shows a lower hit rate (up to 3.0%) for sequences with <7K tokens, while for sequences with >7K tokens, it outperforms SGLang+ with a hit rate improvement of up to 25.5%. This is due to Marconi's"}, {"title": "5.4 Microbenchmarks and Ablation Studies", "content": "In these microbenchmarks, we use different representative traces to dissect Marconi's performance improvements.\nImpact of cache contention. In Fig. 11, we analyze how cache contention affects Marconi's benefits. We vary the cache size from 60 GB (high contention) to 140 GB (low contention). Across the five cache sizes, Marconi achieves token hit rate improvements of 24.3%, 51.5%, 68.3%, 30.0%, and 10.0% over SGLang+, respectively. The most significant performance gains occur under moderate contention, where eviction decisions are critical. In high con- tention scenarios, limited cache capacity prevents caching many useful prefixes (resulting in a token hit rate of less than 10%). Conversely, in low contention scenarios, the cache has sufficient space to store a larger number of prefixes, and thus FLOP-unaware eviction decisions have a smaller impact on the hit rate.\nVarying layer compositions. As described in \u00a73, the ra- tio of SSM layers directly affects the memory footprint of model states. In Fig. 12a, as we increase the Attention:SSM ratio from 1:2 to 1:4 to 1:8, Marconi's token hit rate improve- ment over VLLM+ and SGLang+ increases from 13.5% and"}, {"title": "6 RELATED WORK", "content": "(Hybrid) recurrent subquadratic models. There has been a resurgence of recurrent/linear models in the recent years: RWKV (Peng et al., 2023), RetNet (Sun et al., 2023), GLA (Yang et al., 2023), Griffin (De et al., 2024), Recur-"}, {"title": "7 CONCLUSION", "content": "This paper proposes Marconi, the first prefix caching sys- tem designed to accommodate the unique characteristics of Hybrid models. Marconi proposes novel and judicious ad- mission and eviction policies, achieving up to 34.4\u00d7 higher token hit rates (71.1% or 617 ms lower TTFT) over extended versions of state-of-the-art systems."}]}