{"title": "Evaluating the Prompt Steerability of Large Language Models", "authors": ["Erik Miehling", "Michael Desmond", "Karthikeyan Natesan Ramamurthy", "Elizabeth M. Daly", "Pierre Dognin", "Jesus Rios", "Djallel Bouneffouf", "Miao Liu"], "abstract": "Building pluralistic AI requires designing models that are able to be shaped to represent a wide range of value systems and cultures. Achieving this requires first being able to evaluate the degree to which a given model is capable of reflecting various personas. To this end, we propose a benchmark for evaluating the steerability of model personas as a function of prompting. Our design is based on a formal definition of prompt steerability, which analyzes the degree to which a model's joint behavioral distribution can be shifted from its baseline behavior. By defining steerability indices and inspecting how these indices change as a function of steering effort, we can estimate the steerability of a model across various persona dimensions and directions. Our benchmark reveals that the steerability of many current models is limited \u2013 due to both a skew in their baseline behavior and an asymmetry in their steerability across many persona dimensions. We release an implementation of our benchmark at https://github.com/IBM/prompt-steering.", "sections": [{"title": "1 Introduction", "content": "A primary question underlying alignment research is: who are we are aligning to? The philosophy of Al/algorithmic pluralism [9, 8, 18, 19] states that we should design AI systems such that they are capable of representing various individuals/groups, rather than aligning to a single \u201caverage\" human preference \u2013 a practice that is unfortunately common in many current model training pipelines. One mechanism for enabling pluralism is by constructing steerable models, i.e., models that can be (easily) made to adopt various behaviors [19].\nIn this paper, we propose a methodology for evaluating a model's steerability with respect to prompting. We first propose a formal definition for prompt steerability \u2013 quantifying a model's behavior as a joint distribution, which we term a profile, computed via evaluation/score functions on the distribution of model generations as a result of (a set of) input prompts. Using a dataset of model personas [14], we design a benchmark that measures the extent to which a model can be prompted to adopt various personas. Furthermore, building on our definition of prompt steerability, we define steerability indices that enable comparative measures of how much a model's behavior can be influenced. While there are a (growing) number of methods for steering models \u2013 via prompting [3, 11, 12], fine-tuning [14, 1], activations [16, 21, 20, 10], and other methods [7, 5, 6] \u2013 prompting is one of the most straightforward ways in which a typical user can influence a model's behavior. Often it is not feasible for a user to fine-tune a model (either due to computational requirements or simply due to not having access to the weights) or steer a model via its activations (which requires being able to access/modify a model's internals during inference)."}, {"title": "Related work", "content": "Steerability is a closely related notion to model alignment, with much of the community treating steering and aligning as interchangeable concepts. We emphasize, however, that the notion of steerability describes the extent to which a model can be aligned/steered along a given dimension. Some models can be aligned to a specific behavior more readily than others this is precisely what steerability aims to quantify. There is a variety of recent research concerning steerability, ranging from theoretical to practical. Perhaps most prominent of the theoretical results is that of [22] in which the authors present an existence theorem stating that, under the assumption that LLMs perform Bayesian inference, there exists a prompt that can amplify any existing model behavior. It is worth emphasizing that the authors do not describe what this prompt looks like nor prescribe how to find this prompt, simply that it exists. Similar theoretical work [3] finds that there exist short prompt sequences that can significantly alter the probability of specific output tokens. On the practical side, many recent papers propose algorithms for steering models to specific behaviors [23, 14, 16, 11, 21, 12]. Of the algorithmic papers, that of [14] is most relevant to the present paper, with the fundamental difference being that the authors explore steerability with respect to fine-tuning (specifically via RL from human feedback) where our methodology studies prompting. Lastly, model steerability is related to the notion of model sycophancy [15, 17, 13] with the primary difference being that the latter studies the degree to which the models mirror input biases in their outputs."}, {"title": "Contribution", "content": "Our primary contribution is the development of a steerability benchmark for evaluating the degree to which a model can be prompted to take on various personas. We additionally introduce metrics, termed steerability indices, to quantify the degree of steering. Our results complement the fine-tuning setting of [14] by analyzing steerability of model personas via prompting."}, {"title": "2 Prompt Steerability", "content": "We first define what we mean by prompt steerability. Given a generative language model $M_\\theta$, where $\\theta$ is the set of model parameters, denote $p_\\theta$ as the probabilistic function that maps inputs/prompts $x \\in \\mathcal{X}$ to outputs $y \\in \\mathcal{Y}$ via $y \\sim p_\\theta(x)$. Let $\\mathcal{S} = \\{s_1,..., s_n\\}$ denote a set of score functions, i.e., metrics, where each $s_i \\in \\mathcal{S}$ is a probabilistic function $s_i : \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathcal{P}(\\mathcal{E}_i)$ from prompt-output pairs $(x, y)$ to a score in an evaluation space $\\mathcal{E}_i \\subseteq \\mathbb{R}$, i.e., the values that score $s_i$ can take.\nThe score functions $\\mathcal{S}$, along with a set of prompts $\\mathcal{X} \\subseteq \\mathcal{X}$, yield a measure of a given language model's outputs, termed an evaluation profile. Formally, an evaluation profile is a joint distribution $p_\\mathcal{X} \\in \\mathcal{P} = \\mathcal{P}(\\mathcal{E})$, $\\mathcal{E} = \\mathcal{E}_1 \\times \\cdots \\times \\mathcal{E}_n$, defined as\n$p_\\mathcal{X} = \\mathbb{E}[p(s(x, y)) | y \\sim p_\\theta(x), x \\in \\mathcal{X}]$\nwhere $p(s(x, y))$ is the joint distribution of scores $s(x, y) = (s_1(x, y), ..., s_n(x, y))$ for a given $(x, y)$ pair. In other words, a model's evaluation profile (or simply profile) $p_\\mathcal{X}$ is the model's expected behavior on $\\mathcal{X}$ as measured by the score functions $\\mathcal{S}$.\nA model's prompt steerability measures the degree to which the model's profile changes, as a function of prompting, along a set of steering dimensions. Define a prompt steering function $\\sigma : \\mathcal{X} \\rightarrow \\mathcal{X}$ as a function that generates modified prompts that influence the model's outputs via $y \\sim p_\\theta(\\sigma(x))$. Let $\\mathcal{D} = \\{d_1,..., d_m\\}$ denote the set of steering dimensions and define $\\sigma_i^+$ (resp. $\\sigma_i^\u2212$) as the positive (resp. negative) prompt steering function along steering dimension $d_i$. For example, directing a model to respond in a more positive or negative tone could be achieved by defining steering functions $(\\sigma_i^+, \\sigma_i^-)$ that appropriately modify the model's system prompt. Define the positively and negatively steered profiles along $d_i$ as\n$\\begin{aligned}\np_\\mathcal{X}^{i,+} &= \\mathbb{E}[p(s(x', y)) | y \\sim p_\\theta(x'), x' = \\sigma_i^+(x), x \\in \\mathcal{X}] \\\\\np_\\mathcal{X}^{i,-} &= \\mathbb{E}[p(s(x', y)) | y \\sim p_\\theta(x'), x' = \\sigma_i^-(x), x \\in \\mathcal{X}]\n\\end{aligned}$\nA model's prompt steerability along $d_i$ is the degree to which $(p_\\mathcal{X}^{i,+}, p_\\mathcal{X}^{i,-})$ can be pulled away from $p_\\mathcal{X}$ by construction of $(\\sigma_i^+, \\sigma_i^-)$.\nFurther quantification of a model's prompt steerability is dependent upon the specific setting, requiring a definition of both the precise steering functions as well as assigning an appropriate distance metric between profiles (distributions). We quantify these notions in the context of persona-based prompt steerability in the following section."}, {"title": "3 Steerability of Model Personas", "content": "Prompt steerability of model's persona describes the degree that a model can be made to adopt various personas by prompting alone. We design a benchmark that enables measurement of this property."}, {"title": "3.1 Benchmark Design", "content": "Persona data. Our benchmark is based on the evals/persona dataset\u00b9 which consists of model persona dimensions spanning personality, political views, ethical views, religious views, unsafe behaviors, and other topics [14]. The dataset contains multiple statements for each persona dimension (e.g., agreeableness, willingness-to-defer-to-experts, politically-liberal, etc.) and each direction (positive, negative). The statements are simple strings that are designed to align with a given persona dimension and direction (with their degree of alignment given by a label_confidence parameter). Additional details on the data can be found in Appendix A.1.\nMethodology. Both the steering and scoring of a model's outputs are done via the persona statements. Specifically, by decomposing the prompt as $x = (x_{sys}, x_{usr})$, where $x_{sys}$ is the system prompt and $x_{usr}$ is the user message, steering functions $(\\sigma_k^{i,+}, \\sigma_k^{i,-})$ operate on the system prompt only, that is, $\\sigma_k^{i,+}(x) = (\\sigma_k^{i,+}(x_{sys}), x_{usr})$, where $\\sigma_k^{i,+}(x_{sys})$ appends $k$ steering statements (i.e., principles) to the system prompt. Scoring of the model's outputs is done by asking the model (in the user message, $x_{usr}$) if it would generate a given statement. By comparing the model's (yes/no) answers to the user prompt with the statement's true direction (and label confidence), we can construct estimates of the steered profiles (see Appendix A.2 for details).\nNote that because the model is both steered and scored using persona statements, the steering and scoring dimensions coincide (m = n). Also note that each statement is contained within a single persona dimension split, i.e., a given statement is only labeled with respect to a single persona dimension. Thus, when evaluating a model's answer, we can only reason about its behavior along its corresponding dimension, independently of other dimensions. Formally, the consequence of this independence structure is that the representation of a model's profile collapses to a set of marginals (rather than a joint distribution), i.e., $p_\\mathcal{X} = (p^1_\\mathcal{X},...,p^n_\\mathcal{X})$ where $p^i_\\mathcal{X} \\in \\mathcal{P}(\\mathcal{E}_i)$ is the marginal on dimension i. Similarly, define $p^i_{\\mathcal{X},k} = \\mathbb{E}[p(s_i(x', y)) | x \\in \\mathcal{X}, x' = \\sigma_k^{i,+}(x), y \\sim p_\\theta(x')]$ as the positively steered profile on dimension $d_i$ under steering function $\\sigma_k^{i,+}$ (analogously for $p^i_{\\mathcal{X},k}$). The construction of the score functions in terms of the persona statements is detailed in Appendix A.2.\nMeasuring prompt steerability. Given the structure of the prompt steering function, we can further quantify the definition of prompt steerability. We define steerability indices $(\\mathcal{V}_{i,k}^+, \\mathcal{V}_{i,k}^-)$, $i \\in [n], k \\in \\mathbb{N}$, as\n$\\begin{aligned}\n\\mathcal{V}_{i,k}^+ &= \\frac{\\mathcal{W}(p^i_\\mathcal{X}, p^{i,+}_\\mathcal{X}) - \\mathcal{W}(p^i_\\mathcal{X}, p^{i,+}_{\\mathcal{X},k})}{\\mathcal{W}(p^i_\\mathcal{X}, p^{i,+}_\\mathcal{X})}, \\\\\n\\mathcal{V}_{i,k}^- &= \\frac{\\mathcal{W}(p^i_\\mathcal{X}, p^{i,-}_\\mathcal{X}) - \\mathcal{W}(p^i_\\mathcal{X}, p^{i,-}_{\\mathcal{X},k})}{\\mathcal{W}(p^i_\\mathcal{X}, p^{i,-}_\\mathcal{X})}\n\\end{aligned}$\nwhere $\\mathcal{W}(\\cdot, \\cdot)$ is the Wasserstein distance and $p^{i,+}_\\mathcal{X}$, resp. $p^{i,-}_\\mathcal{X}$, represents the maximally steered marginal under $k$ steering examples assuming all model responses were in the positive, resp. negative, direction. Intuitively, the steerability indices describe the extent to which the model's profile was"}, {"title": "3.2 Benchmark Results", "content": "Prompt steerability. Plotting the steerability indices over $k$ yields steerability curves, i.e., the extent to which the model can be steered as a function of the steering effort (number of steering statements). Generally, we observe that more steering examples yield a more steered model, with the resulting steered direction in agreement with the attempted steering direction. The shape of the steerability curves informs how easily the model is steered along a given dimension/direction. In particular, more advanced models tend to possess steerability curves that both yield higher values (higher degree of steering) and plateau sooner, indicating a greater ease of steering. This early flattening behavior is likely due to more sophisticated models having better internal representations, allowing them to infer what the user is asking of it from fewer examples.\nDiscussion and implications. While larger models are generally more steerable than smaller models, the limited extent to which (even current SoTA) models can be steered poses various challenges for building pluralistic AI. A model's steerability, as computed by its steerability indices, is necessarily relative to its base behavior. As shown in Appendix B, many model's unsteered (baseline) behavior across various dimensions is not centered around a neutral point. Additionally, the steerability from this baseline is often asymmetric, with models generally able to be steered more easily in one direction than the other. For instance, as shown in Fig. 2, many of the models we benchmarked were able to be steered more in the negative direction than the positive direction of the dimension: subscribes-to-utilitarianism. Similar asymmetries exist for many of the other dimensions we studied. Notably, many models were more easily steered in the negative direction than the positive direction, with some resisting positive steering on some dimensions altogether. Appendix B provides detailed benchmark results for a collection of models, namely: llama-3-8b-instruct, llama-3.1-8b-instruct, granite-7b-lab, granite-13b-chat-v2, phi-3-mini-4k-instruct, and phi-3-medium-4k-instruct. These results indicate that models possess internal baseline personas that are steerable, but noticeably resistant to steering along some dimensions. This rigidity limits a model's behavior to a constrained region, preventing models from adopting the range of personas necessary for a fully pluralistic AI."}, {"title": "4 Concluding remarks and ongoing efforts", "content": "We present an experimental methodology for evaluating a model's steerability with respect to prompting. We first constructed a principled definition of a model's prompt steerability and, using this definition, we designed a benchmark for evaluating a model's steerability across various personas. We observed that many models resist steering on various dimensions/directions indicating that models possess (rigid) internal personas. Despite the limited steerability of many current models, our benchmark provides an approach to evaluate the steerability of models, providing a signal to design models that are more steerable. Current efforts are focused on better understanding the underlying reasons why some models are more steerable than others, with the goal of enabling controllable generation for the design of pluralistic AI systems."}, {"title": "A Prompt Steerability of Personas: Experiment Design", "content": null}, {"title": "A.1 Data preparation", "content": "The persona steerability benchmarking experiment is based on Anthropic's evals/persona dataset.3. The evals/persona dataset consists of 133 dimensions spanning personality, political views, ethical views, religious views, unsafe behaviors, and other topics [14]. Each dimension consists of 500 examples (statements) in each of the positive and negative direction (1000 examples total per dimension). Associated with each example is a label confidence, in [0.5, 1], indicating the expected accuracy of the label (given by a preference model) for the current dimension and direction. Additional details of how these examples were constructed can be found in the original paper [14].\nFor the purposes of the benchmark, we filter the original dimensions based on a desired minimum number of sufficiently confident examples. We set a minimum confidence threshold of 0.85 and a minimum count of 300 examples in each direction. Dimensions that possess more than 300 examples are pruned down to 300. Filtering based on these requirements yields a pruned set of examples across 70 dimensions. For computational reasons, we further prune this set down to the 32 dimensions listed in Fig. 3. Some example statements from this data are illustrated in Table 1. We split this data into 100 steering examples and 200 profiling examples for each direction and dimension."}, {"title": "A.2 Evaluating Prompt Steerability", "content": "Profile estimation. The specific benchmark procedure naturally fits into the formalism for prompt steering described in Sec. 2. Specifically, for each persona dimension di, let the score si(x, y) represent the probability that the model's response y exhibits the positive valence of persona dimension di in response to the prompt x. By collecting answers to multiple (steered) profiling prompts with known valences, the probability that model will exhibit specific behavior along a particular persona dimension can be estimated. This estimate is precisely the model's profile. The construction of the model's profile from the collected answer-valence pairs (response data) is described in the following subsection."}, {"title": "A.3 Extracting model responses", "content": "Our benchmark design requires that we are able to extract a model's yes/no response to the profiling question. We implement two approaches for this.\nOutput parsing. Format instructions (as a JSON) are appended to the prompt. The output is then parsed to extract the desired key-value pairs. Given the imperfect instruction following rate of models, this method usually requires multiple calls before all outputs can be successfully parsed and thus can be inefficient (if the model does not follow instructions).\nComparing logprobs. Each of the yes/no completions is appended to a given input prompt (see Fig. 4) and each passed into the model. By comparing the logprobs of the completion token (similar to the method of constrained decoding), we can infer which answer the model prefers to the given question. This process is very efficient but requires that the API provides access to the model's logprobs."}]}