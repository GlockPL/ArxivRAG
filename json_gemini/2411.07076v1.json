{"title": "StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification", "authors": ["Yichen He", "Yuan Lin", "Jianchao Wu", "Hanchong Zhang", "Yuchen Zhang", "Ruicheng Le"], "abstract": "Existing large vision-language models (LVLMs) are largely limited to processing short, seconds-long videos and struggle with generating coherent descriptions for extended video spanning minutes or more. Long video description introduces new challenges, such as plot-level consistency across descriptions. To address these, we figure out audio-visual character identification, matching character names to each dialogue, as a key factor. We propose StoryTeller, a system for generating dense descriptions of long videos, incorporating both low-level visual concepts and high-level plot information. StoryTeller uses a multimodal large language model that integrates visual, audio, and text modalities to perform audio-visual character identification on minute-long video clips. The results are then fed into a LVLM to enhance consistency of video description. We validate our approach on movie description tasks and introduce MovieStory101, a dataset with dense descriptions for three-minute movie clips. To evaluate long video descriptions, we create MovieQA, a large set of multiple-choice questions for the MovieStory101 test set. We assess descriptions by inputting them into GPT-4 to answer these questions, using accuracy as an automatic evaluation metric. Experiments show that StoryTeller outperforms all open and closed-source baselines on MovieQA, achieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and demonstrating a +15.56% advantage in human side-by-side evaluations. Additionally, incorporating audio-visual character identification from StoryTeller improves the performance of all video description models, with Gemini-1.5-pro and GPT-40 showing relative improvement of 5.5% and 13.0%, respectively, in accuracy on MovieQA.", "sections": [{"title": "1. Introduction", "content": "Generating detailed video descriptions is a fundamental challenge in video understanding. While large vision-language models (LVLMs) have made significant progress [8, 22-24, 40, 41], they remain limited to processing seconds-long videos and generating low-level visual concepts like objects, scenes and atomic actions. Real-world videos, such as movies, are much longer and require high-level information for effective description, such as plot development. Recent research works address long videos by segmenting them, generating descriptions for each segment, and combining them using large language models [13, 14, 47]. However, there is still a gap to resolve the key challenge of long video description: maintaining consistency. This includes ensuring logical coherence in the storyline, consistency in character descriptions and motivations, and overall continuity and fluidity of the description.\nAccurate character identification is essential for ensuring consistency in long video descriptions. While existing studies [18, 32] have improved the quality of video descriptions by focusing on character identification and tracking through visual elements, audio information is equally critical, as dialogue significantly contributes to drive the story progressing. Nevertheless, a gap remains in linking dialogue with on-screen characters to identify speakers, and further improve the capability for long video descriptions.\nTo address these issues, we propose a new task: given a video and its cast list with character photos and names, identify the speaker for each dialogue line in the video using both visual and auditory cues. If the character is in the cast, provide their name; otherwise, give a descriptive label. We refer to this task as audio-visual character identification. This task is challenging as it requires integrating visual, auditory, and textual information to link dialogue lines to character identities. To tackle this, we developed a multimodal large language model equipped with both visual and auditory inputs. Due to the limitation of models with visual encoder in handling only short segments, it struggles to utilize global auditory information (e.g., some lines spoken by the same character through a long video). To address this, we introduce a global decoding algorithm that incorporates global information during inference to improve accuracy of audio-visual character identification.\nFurthermore, we propose StoryTeller, a novel system for generating dense descriptions of long videos, incorporating both basic visual concepts and high-level plot information with high consistency. The system consists of three components: a video segmentation module, an audio-visual character identification module, and an identity-aware description generation module, as illustrated in Figure 1.\nA major challenge in long video description is the lack of training and test data. To address this, we introduce MovieStory101, a new dataset focused on movies-long videos with high information density. The videos in MovieStory101 are from Movie101[45] and Movie101v2 [46], comprising 187 movies split into 5,982 three-minute clips. Each clip is annotated with storyline descriptions, subtitles, and character identity labels for each dialogue line. The storyline annotations ensure a complete understanding of the movie's plot. By training on this dataset, we aim to unleash the model's ability to generate plot-level descriptions. In adiition, evaluating long video descriptions is also challenging. Besides human evaluation, we propose an automated evaluation method for MovieStory101's test set: MovieQA. MovieQA has 38 multiple-choice questions in average for each video, covering visual actions, character relationships, and plot details. We evaluate a long video description by inputting it into GPT-4 [28] to answer these questions, using the accuracy as an evaluation metric.\nBased on MovieQA and human side-by-side evaluation, we demonstrate that StoryTeller outperforms Gemini-1.5-pro [35], GPT-4o [29], and several advanced open-source LVLMs [7, 20, 24, 41] in generating descriptions for long videos. Specifically, the 7B model-based Story-Teller achieves an accuracy in the MovieQA evaluation that is 9.5% higher than the best-performing baseline model, Gemini-1.5-pro. In human side-by-side evaluation, Story-Teller shows an advantage of +15.56% over Gemini-1.5-pro. Additionally, we verify that incorporating audio-visual character identification results from StoryTeller as input features consistently enhances the long video description capabilities of various LVLMs. In particular, Gemini-1.5-pro and GPT-40 achieve relative improvements of 5.5% and 13.0% in accuracy on MovieQA, respectively."}, {"title": "2. Related Work", "content": "Long Video Description. The rapid advancement of LVLMs has greatly accelerated progress in video description tasks [8, 22-24, 40, 41]. However, most LVLMs are constrained by the limited number of frames they can process, making them effective primarily for short video clips spanning only a few seconds. These models typically generate descriptions focused on low-level visual concepts, such as objects, scenes, and actions. There remains a gap in describing longer videos, which may span several minutes or more, and in capturing higher-level content like character interactions and plot progression. To address this challenge, some research works [13, 14, 47] treat a long video as a series of short clips, incorporating the descriptions of previous clips when describing the current one. However, the quality of earlier segment descriptions significantly affects the accuracy of the current description, leading to cumulative errors that compromise the consistency and coherence of the overall narrative [13]. Another approach, Video Re-Cap [17], introduces a recursive video captioning model capable of handling videos of varying lengths. Nevertheless, for longer videos, this method tends to produce summaries, limiting its ability to generate detailed descriptions.\nAudio-Visual Character Identification. Character identification is essential for generating high-quality video descriptions [18, 32], particularly to maintain consistency across different clips. Previous research has mainly focused on character identification at the visual level, identifying characters in individual frames [27, 38]. However, videos contain both audio and visual elements, and utilizing audio cues to identify which character is speaking can straightforwardly enhance video descriptions. A closely related area of research is audio-visual diarization, which integrates both visual and auditory features to segment speakers in audio [15, 42]. Nonetheless, current methods are still unable to identify the speakers' identities."}, {"title": "3. StoryTeller Long Video Description System", "content": "3.1. Overview\nStoryTeller is a fully automated system for generating descriptions of long videos, capable of processing videos that span several minutes. As illustrated in Figure 1, the system consists of three main modules. First, the video segmentation module divides the long video into multiple short clips, each lasting several seconds, while preserving the integrity and relative independence of these clips, as detailed in Appendix C. The second component is the audio-visual character identification module, which employs a dual mechanism, local and global, to accurately identify characters throughout the long video. Locally, a multimodal large language model (MLLM) integrates audio, visual, and text modalities to achieve character identification using both audio and visual cues within each short clip. Globally, during inference, a global decoding algorithm processes multiple short clips associated with the same character simultaneously, combining their local results from the MLLM to enhance the overall identification accuracy. Finally, the description generation module utilizes an LVLM to produce a detailed description. By incorporating the audio-visual character identification as input, the LVLM generates coherent video descriptions that span the entire long video, resulting in a plot-level dense narrative."}, {"title": "3.2. Audio-Visual Character Identification", "content": "The audio-visual character identification module determines speakers identity in video clips by integrating audio, visual and textual information. In particular, given a video clip (containing frames, audio and subtitles) and a cast list (with character photos and names), the module assigns each dialogue line to its corresponding character. If the speaker is in the cast list, their name is provided; otherwise, a descriptive identity (e.g., 'a police officer' or 'a little girl') is used. This task presents several challenges: 1) relying solely on visual information is often insufficient, especially when multiple characters are on screen or the speaker is not visible; 2) audio cues in seconds-long clips are typically isolated, making them less effective for character identification. However, in longer segments, audio can be leveraged to extract speaker features across different clips, such as identifying if the same person speaks in multiple segments. Models that are limited to short segments often struggle to incorporate such global features effectively.\nTo address the aforementioned challenges, we design a novel audio-visual character identification mechanism incorporating a MLLM equipped with both audio and video modalities. We also develop a global decoding algorithm to leverage global audio information for more accurate character identification. Specifically, as illustrated in Figure 1, we begin by performing audio diarization on the entire video, segmenting the dialogue, and assigning IDs to different speakers (e.g., C1, C2, etc.), as detailed in Appendix D. The subtitles with global IDs are then fed into the MLLM to identify the corresponding character names for each global ID. The global decoding mechanism processes short video clips in parallel, ensuring that global IDs in different clips are consistently mapped to the same name, thereby improving identification accuracy. Details on the implementation of the MLLM and the global decoding algorithm are provided below.\nModel Architecture and Training. We integrate a MLLM capable of processing both visual and auditory data, as depicted in Figure 2. For visual inputs, each image or video frame is encoded by CLIP-ViT [33], followed by mapping into the token embedding space of the downstream LLM through a multi-layer perceptron (MLP). These tokens are then fed into the LLM. The audio inputs are processed by dual auditory encoders [37], a speech encoder from Whisper-Large-v2 [34] and a non-speech BEATs audio encoder [4]. The outputs from the two audio encoders are concatenated at the frame level. Subsequently, a window-level Q-former [21] align the combined audio representation to the LLM's input space.\nWe initialize the visual module and language model from the Tarsier-7b [40], a large vision-language model demonstrating SOTA performance on various video understanding benchmarks. Our training protocol consists of three phases: (1) pre-training the audio module, (2) fine-tuning on tasks including audio diarization, character identification, recognition and tracking, and (3) fine-tuning for audio-visual character identification using MovieStory101 training set. Details on the dataset used in each phase and the complete training methodology are provided in Appendix E.\nGlobal Decoding. We use the MLLM to assign the names to each global character ID. First, these names come from a set \\( N \\), which includes all names from the cast list as well as a special item \u201cOthers\u201d, representing characters not in the cast list. When the model outputs \"Others\", it will further provide a descriptive name. Second, the model outputs a JSON dictionary, where each key is a global character ID within the input short clip, and each corresponding value is the assigned name. Keys are listed in ascending order of global character IDs.\nA given global character ID may appear simultaneously in multiple short video clips. If we independently assign names to the ID across different clips, conflicts may arise where the same character ID is assigned different names in different clips. To address this, we decode the results from all clips containing the same global character ID in parallel.\nSpecifically, we consider a multimodal LLM parameterized by \\( \\theta \\), which assigns names to each global character ID in a fixed ascending order (i.e., C1, C2, ...). For a given global character ID \\( x \\) appearing in \\( m \\) short video clips, denoted as \\( v_1,..., v_m \\), the probability that the model assigns a name \\( y \\in N \\) to \\( x \\) in clip \\( v_k \\) is given by \\( p_{\\theta}(y|v_k, x) \\), which can be computed as:\n\\[\np_{\\theta}(y|v_k, x) = \\prod_t p_{\\theta}(y_t|v_k, x, y_{<t}),\n\\]\nwhere \\( y_t \\) is the \\( t \\)-th token in the response and \\( y_{<t} \\) is tokens in the response before \\( y_t \\).\nThen, the global probability of \\( x \\) corresponding to name \\( y \\) is defined as\n\\[\np_{\\text{global}}(y|x) = \\prod_{k=1}^m p_{\\theta}(y|v_k, x).\n\\]\nThe name assigned to the global character ID \\( x \\) is then determined as \\( \\arg \\max_{y \\in N} p_{\\text{global}}(y|x) \\).\nFor example, as shown in Figure 2, both videos \\( v_1 \\) and \\( v_2 \\) contain lines from character C1. In \\( v_2 \\), although Xu Tailang is the one speaking, he is shown in profile while the camera focuses entirely on Director Jin, who is not speaking. As a result, the model assigns a higher probability to Director Jin being the speaker. However, the model still has over 30% confidence in identifying the speaker as Xu Tailang. By integrating information from the entire video globally, C1 is ultimately mapped to Xu Tailang, correcting the erroneous prediction in \\( v_2 \\).\nIn addition, when calculating the probability of the model generating the special item \u201cOthers\u201d, only the token probability for \"Others\" is considered, excluding any descriptive name tokens. This approach is based on our observation that the model consistently achieves high accuracy in generating descriptive names, but during inference, it may produce different yet valid descriptions for the same individual, such as 'A policeman' and 'A man in police uniform.' Although these descriptions vary, they do not impact the final video descriptions, as the language model can correctly identify that they refer to the same person."}, {"title": "3.3. Description Generation", "content": "After identifying the character, we proceed to generate the final description. At this stage, the video, cast list, and subtitles with character names are provided to the model. Audio input is excluded to ensure compatibility with most existing open-source or closed-source large vision-language models. For open-source models, we use video description data from the MovieStory101 training set for supervised fine-tuning. During this stage, both the visual adapter and LLM are trained."}, {"title": "4. MovieStory101 Dataset", "content": "We introduce MovieStory101, a long video description dataset based on movies. We choose movies because they contain rich and diverse information. A high-quality movie description should capture not only fine-grained details such as scenes, characters, and actions but also higher-level aspects like character relationships, plot development, and causal links between events (see Figure. 3). Furthermore, since movies are inherently long videos, their descriptions require linguistic coherence and logical consistency throughout the narrative. Consequently, the task of movie description presents significant challenges."}, {"title": "4.1. Movie Source", "content": "We build MovieStory101 dataset based on the publicly available datasets, Movie101 [45] and Movie101v2 [46]. Together, these datasets include 203 movies, providing audio descriptions for scenes without dialogue in the barrier-free versions.\nSince the movies in Movie101 and Movie101v2 contain audio descriptions integrated into the audio tracks, we collect the original editions of the movies to obtain the original audio tracks. After reviewing the content, we select 187 movies for our dataset. We exclude 16 movies because their original editions differed significantly from their barrier-free versions. Each selected movie is segmented into 3-minute clips. The final MovieStory101 dataset includes 5,350 / 140 / 492 clips in the training / development / testing split, totaling 300 hours. Each split contains video clips from different movies."}, {"title": "4.2. Annotation Process", "content": "The annotation process comprises three steps: movie descriptions, character photos, and audio-visual character identification.\nMovie description. We annotate each movie clip in the dataset with detailed description. To improve annotation efficiency, we present audio descriptions and ASR data from Movie101 and Movie101v2 as references to the annotators. Annotators are tasked with providing the descriptions of each event in the video, along with the corresponding start and end timestamps. The descriptions should include details about scenes, characters, actions, dialogues, and plot development. The aim is to create a coherent long-form description of the video. Figure 3 provides an example in MovieStory101 dataset. See annotation guidelines in Appendix A. In addition, annotators revise the ASR data in parallel to generate subtitles for each movie clip.\nCharacter photos. For each movie clip, annotators identify main characters present and select a face for each character. A cast list is then created, to record each character's name and corresponding face, as shown in Figure 3.\nAudio-visual character identification. For each line of dialogue in the video clips, annotators identify the speaker. If the speaker is included in the cast list, the corresponding character name is assigned. Otherwise, a descriptive label is used to identify the speaker. Figure 3 provides an example of audio-visual character identification in the MovieStory101 dataset."}, {"title": "4.3. Evaluation of Movie Description", "content": "Evaluating long video descriptions is challenging due to the numerous details they contain, which increases evaluation costs. To address this, we develop an automatic evaluation method called MovieQA for the MovieStory101 test set. On average, each 3-minute video in MovieQA is accompanied by 38 multiple-choice questions focusing on visual actions, character relationships, and plot development. During evaluation, we employ GPT-4 [28] to answer these questions given the long description as context, and the QA accuracy serves as an automated metric for assessing the quality of the description. The prompt template is shown in Appendix F. Table 1 shows examples in MovieQA.\nCreation for MovieQA We identify three key types of information that need to be highlighted in evaluation of long video descriptions: 1) Action: actions within the movie, including action recognition, the sequence of actions, and the causal relationships between them; 2) Character: character roles, their relationships with others, and how these relationships evolve; 3) Plot: causes and effects, character motivations, and event connections. Based on these categories, we generate question-answer pairs for each type. The detailed process is shown in Appendix B."}, {"title": "5. Experiments", "content": "5.1. Baselines\nWe compare our method against two types of baselines:\n\u2022 Closed-Source LVLMs: We compare our method with Gemini-1.5-pro [35] and GPT-40 [29]. For both models, we perform prompt engineering to generate detailed video descriptions that incorporate both fine-grained details and high-level plot information. The prompt templates are provided in Appendix F.\n\u2022 Open-Source LVLMs: We also compare our method with open-source LVLMs that demonstrate strong video understanding capabilities, including VILA1.5-8B [24], LLaVA-OneVision-7B [20], Qwen2-VL-7B [31], and InternVL2-8B [7, 8]. To ensure a fair comparison, we fine-tune each model using MovieStory101 training set. For all baselines, we provide the video, cast list and subtitles as input. Specifically, for Gemini-1.5-pro, we directly input the entire 3-minute video, while for the other models, we first divide the video into 10-second clips. We then sample 8 frames from each clip as input to generate descriptions for the individual segments. Finally, we concatenate the descriptions from each clip to form a complete description of the entire 3-minute video."}, {"title": "5.2. Experimental Results", "content": "First, we evaluate the descriptions generated by our method and the baselines on the MovieQA dataset. As shown in Table 2, StoryTeller outperforms all baselines on MovieQA. Notably, StoryTeller achieves a 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro. Table 3 shows a video example featuring multiple characters, where StoryTeller effectively tracks their relationships, resulting in description with fewer hallucination compared to Gemini-15-pro. When analyzing performance across different question types in MovieQA, StoryTeller outperforms all baselines on action, character and plot questions, indicating that StoryTeller provides more accurate descriptions of both low-level visual details and high-level plot information.\nWe continue to conduct a human side-by-side evaluation, considered the gold standard for assessing video descriptions. We compare StoryTeller against Gemini-1.5-pro and VILA1.5-8B, separately representing the best closed-source and open-source baselines. In this evaluation, we randomly selected 100 3-minute videos from the MovieStory101 test set and asked annotators to compare the descriptions generated by two models, collecting their preferences. Due to the difficulty of directly comparing 3-minute videos, annotators viewed them in 20-second segments and provided preferences for the corresponding descriptions. We calculated the win rate at the 20-second segment level. The results, presented in Table 4, align with our automatic evaluation findings. Descriptions generated by StoryTeller significantly outperforms those generated by Gemini-1.5-pro and VILA1.5-8B. Specifically, StoryTeller demonstrate a +15.56% advantage over Gemini-1.5-pro and a +42.25% advantage over VILA1.5-8B.\nThe system design of StoryTeller is a general framework applicable to various LVLMs to enhance their long video descriptions. Table 5 presents the additive contributions of each component for various LVLMs. First, replacing the direct division into 10-second clips with our video segmentation module consistently improves description accuracy across all models, resulting in an average relative increase of 6.3% in accuracy on the MovieQA. Second, utilizing the audio-visual identification results generated by our model as input for the LVLMs, we observe an average relative accuracy improvement of 6.4%. This demonstrates that our model's audio-visual identification results is effective for enhancing video descriptions across various LVLMs. Lastly, we observe that replacing the label \u2018Others' with descriptive names generated by our model further improves video descriptions, consistently benefiting all models. Notably, under our framework, Gemini-1.5-pro and GPT-40 achieve relative improvements of 5.5% and 13.0% in accuracy on MovieQA, respectively.\nAt last, we conduct an ablation study to evaluate the necessity of fine-tuning for audio and visual tasks (audio diarization, character recognition, identification, and tracking), as well as effect of global decoding within the audio-visual identification module. We test the accuracy of audio-visual character identification on the MovieStory101 test set, with the results shown in Table 6. Notably, fine-tuning the audio and visual tasks improves performance by 4.6%, while global decoding contributes 3.2% improvement."}, {"title": "6. Conclusion", "content": "In this paper, we figure out audio-visual character identification as a crucial step for generating detailed and consistent descriptions of long videos. Based on this insight, we propose StoryTeller, a system designed to generate dense, plot-level descriptions for long videos. The system is composed of three main modules: video segmentation, audio-visual character identification, and description generation. We also introduce a new dataset, MovieStory101, which includes 5,982 three-minute movie clips from 187 movies. To effectively and efficiently evaluate long video descriptions, we present MovieQA, a large-scale multiple-choice question-answering dataset aligned with the MovieStory101 test set. We assess descriptions by inputting them into GPT-4 to answer these questions, using accuracy as an automatic evaluation metric. Experiments demonstrate that StoryTeller outperforms all open-source and closed-source baselines on MovieQA. Specifically, StoryTeller surpasses Gemini-1.5-pro by 9.5% in accuracy on MovieQA. In human side-by-side evaluations, StoryTeller shows a +15.56% advantage over Gemini-1.5-pro. Additionally, we verify that incorporating audio-visual character identification significantly enhances video descriptions for all LVLMs. Notably, Gemini-1.5-pro and GPT-40 achieve relative improvements of 5.5% and 13.0% in accuracy on MovieQA, respectively. These results indicate that StoryTeller 's methodology provides a general approach for improving long video descriptions."}, {"title": "A. Annotation Guidelines for Movie Description", "content": "The task involves providing a detailed annotation of a 3-minute movie clip, including start and end times for each event. Describe the main events, scenes, characters, actions, dialogues, and plot development to create a cohesive narrative. Ensure the descriptions are logically coherent and enable full understanding of the video through text alone. Please follow the provided guidelines.\n1. When generating detailed descriptions, include the corresponding start and end timestamps for each event.\n2. When describing key dialogues in the movie clip, provide the content of the dialogue, the manner of speaking, and any associated activities.\n3. We provide the audio description and ASR of the movie clip. However, ensure that you label any missing key events, activities, or audio details not captured by these resources.\n4. Provide fine-grained descriptions by segmenting longer descriptions that encompass multiple events. Each shorter description should have its own corresponding timestamps.\n5. Label the subtitles of the movie clip, including their start and end times."}, {"title": "B. Creation of MovieQA", "content": "1. We prompt GPT-4 to generate as many diverse questions as possible and design distractors for each question.\n2. We prompt GPT-4 to evaluate whether the ground-truth description provides enough information to answer each question. If it does not, the question-answer pair is discarded.\n3. For each question, we add a new option, \"None of the Above\". We then prompt GPT-4 to answer the question based on the ground-truth description. If GPT-4 selects \"None of the Above\", the question-answer pair is discarded.\n4. We prompt GPT-4 to answer the question directly without referring to any description. If it can provide the correct answer, the question-answer pair is discarded.\n5. Finally, we employ GPT-4 to deduplicate similar questions.\nWe release the code and prompts used for the data generation process."}, {"title": "C. Long Video Segmentation Method", "content": "We segment long videos using two types of temporal cues: shot timestamps and the start and end times of character dialogues. The specific processing steps are as follows:\n1. Use the PySceneDetect[2] to identify a series of cut points in the video.\n2. For each cut point, verify if it occurs within the start and end time of a character dialogue. If so, discard that cut point to avoid splitting the dialogue.\n3. Examine each segment obtained from the cut points. If a segment contains more than two dialogues, randomly select a new cut point between the end of one dialogue and the start of the next to ensure each segment contains at most two dialogues, thereby preventing overly long segments.\nThe code for the video segmentation module is available in our codebase."}, {"title": "D. Global ID Generation", "content": "To generate global ID for each dialogue line in a long video, we first use an audio embedding model to create embeddings for each utterance. Subsequently, we perform clustering, assigning the same global ID to all lines whose embeddings belong to the same cluster, based on a cosine similarity exceeding a specified threshold.\nAudio Embedding. We fine-tune the ERes2NetV2 model [5] to obtain improved audio embeddings for our scenario. The training data is based on the MovieStory101 training set, from which we construct a series of pairwise samples: positive pairs consist of two audio segments from the same person, while negative pairs consist of segments from different individuals. In total, we collect 246,834 data pairs, with 5% randomly sampled as the development set, and the rest used for training. The ERes2NetV2 model is fine-tuned using CosineEmbeddingLoss as the loss function. We set the learning rate to 0.005 and the batch size to 32.\nGlobal ID Generation. To determine the optimal threshold for clustering, we utilize data from the MovieStory101 development set to create a validation dataset for threshold selection. We pair all utterances, labeling pairs as positive if both utterances were spoken by the same character, and negative otherwise. We evaluate thresholds ranging from 0.5 to 1.0 on this dataset, with the results presented in Table 7. A threshold of 0.85, which achieved the highest accuracy, was selected. In this context, high precision in identifying whether two utterances were spoken by the same person is crucial, while maintaining a balance with recall. The chosen threshold resulted in a precision of 0.900 and a recall of 0.408, satisfying these requirements.\nAdditionally, our audio embedding model outperforms all open-source models under this condition, as Table 8 shown."}, {"title": "E. Training Details", "content": "Our training process consists of three phases, as detailed below.\nPre-training the audio module. In the pre-training stage, we use the same pre-training dataset of SALMONN [37], consisting of both 960-hour LibriSpeech training set [30] and 1000-hour GigaSpeech M-set [3] for speech recognition, as well as 2800-hour WavCaps [26], AudioCaps [19] and Clotho [11] for audio captioning. In this phase, we only pre-train the window-level Q-former, while freezing the audio encoders and LLM. For the window-level Q-former, we use only one trainable query, and set L = 17, which corresponds to approximately 0.33 seconds per window. This configuration outputs 88 textual tokens from the Q-former for a 30-second audio segment. The model was trained on 16 H800 GPUs for 50,000 steps using a batch size of 6.\nFine-tuning on audio and visual tasks. To enhance the model's ability in audio and visual character identification, we collect training data for the following tasks: audio diarization, and character recognition, identification, and tracking. For audio diarization, we sampled data from AMI [25], AISHELL-4 [12], AliMeeting [43, 44], CallHome [1], DIHARD [36], VoxConverse [9], and LibriCSS [6], creating a training dataset consisting of 290,088 minute-level utterances. Additionally, using MovieNet [16], we construct training data for three tasks: (1) character recognition, which involves recognizing characters from frames; (2) character identification, where given a cast list, the model identifies the characters in the frames; and (3) character tracking, where the model tracks a character's position across all frames based on a given face. We collect 33,576 data for these three tasks.\nWe jointly trained on all four tasks by sampling training data from each task in equal proportions for each batch. During training, we fine-tuned the visual adapter, audio window-level Q-former, and the LLM. The model was trained on 16 H800 GPUs for 8,000 steps using a batch size of 6.\nFine-tuning for audio-visual character identification. In this stage for the audio-visual character identification task, we utilize audio-visual character identification data from the MovieStory101 training set, including 5,350 videos. During this stage, we train adapters for the visual and audio modules as well as the LLM. The fine-tuning was performed using 16 H800 GPUs for 4000 steps with a batch size of 4."}, {"title": "F. Prompt Templates", "content": "Table 9 shows the prompt templates for Gemini-1.5-pro and GPT-40. Gemini takes the entire 3-minute video in the MovieStory101 as input, but due to the frame rate limitation of the interface, we segment the long video into seconds-long clips, extract 8 frames from each short video and have GPT-40 generate descriptions separately.\nTable 10 shows the prompt templates for GPT-4 to answer the questuions in MovieStory101. GPT-4 takes the whole descriptions and a multiple-choice question as input and output the selected answer and reason."}]}