{"title": "Will Trump Win in 2024? Predicting the US Presidential Election via Multi-step Reasoning with Large Language Models", "authors": ["Chenxiao Yu", "Zhaotian Weng", "Zheng Li", "Xiyang Hu", "Yue Zhao"], "abstract": "Can Large Language Models (LLMs) accurately predict election outcomes? While LLMs have demonstrated impressive performance in various domains, including healthcare, legal analysis, and creative tasks, their ability to forecast elections remains unknown. Election prediction poses unique challenges, such as limited voter-level data, rapidly changing political landscapes, and the need to model complex human behavior. To address these challenges, we introduce a multi-step reasoning framework designed for political analysis. Our approach is validated on real-world data from the American National Election Studies (ANES) 2016 and 2020, as well as synthetic personas generated by the leading machine learning framework, offering scalable datasets for voter behavior modeling. To capture temporal dynamics, we incorporate candidates' policy positions and biographical details, ensuring that the model adapts to evolving political contexts. Drawing on Chain of Thought prompting, our multi-step reasoning pipeline systematically integrates demographic, ideological, and time-dependent factors, enhancing the model's predictive power. Also, we apply our framework to predict the outcome of the 2024 U.S. presidential election in advance, demonstrating the adaptability of LLMs to unseen political data.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including natural language understanding, content generation, etc. (Brown et al., 2020). Their potential extends far beyond mere text processing, including a broad spectrum of applications from medical diagnostics (Zhang et al., 2023) to legal analysis (Chalkidis et al., 2022) and creative domains (Yang et al., 2022). This versatility stems not only from LLMs' ability to understand and generate text but also from their capacity to leverage large amounts of common knowledge (Roberts et al., 2020), simulate diverse personas (Hu and Collier, 2024), and effectively model human behavior in complex social science tasks (Bommasani et al., 2021). Specifically, LLMs have shown promising results in capturing human-like common sense reasoning (Zhou et al., 2020; AlKhamissi et al., 2022) and have been successfully applied to simulate human decision-making in various contexts (Zhou et al., 2023; Ziems et al., 2024). These multifaceted capabilities position LLMs as potential tools for simulating human decision-making processes in complex contexts. Recent research has begun exploring LLMs' political science applications, analyzing policy documents, campaign speeches, and public sentiment (Xu, 2022; Haq et al., 2023). While the text-based nature of political data certainly aligns with LLMs' strengths, it is the models' holistic combination of language understanding, knowledge integration, and human-like reasoning that truly underscores their potential for simulating complicated dynamics of political-related decision-making (Argyle et al., 2023; Bisbee et al., 2024).\nMotivation. Despite LLMs' success in the above straightforward political science tasks, their capacity to handle more complex tasks like election prediction remains uncertain (Lerer et al., 2022). In-"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 LLMs in Political Science: A New and Emerging Field", "content": "The application of LLMs in political science represents a new and rapidly evolving field, with a limited but growing body of research. While LLMs have revolutionized natural language processing and various other domains, their potential in political science remains largely untapped. Initial studies have demonstrated promising results in areas such as election forecasting, policy analysis, and public opinion simulation (Smith and Doe, 2023; Johnson and Lee, 2024). However, political science often involves complex social dynamics and multi-layered causal relationships, posing significant challenges for effectively utilizing LLMs in this context (Brown, 2023)."}, {"title": "2.2 Political Election Research: Classical and Agent-Based Approaches", "content": "Traditional political science literature has long relied on survey data and statistical models to analyze voter behavior and predict election outcomes. Classical models like the Downsian spatial model and the Median Voter Theorem explain election results by assuming voters' positions in policy space (Downs, 1957; Black, 1948). Time-series regression models also play a key role in long-term election forecasting, analyzing the relationships between economic indicators, political events, and voter sentiment (Lewis-Beck and Rice, 1992; Erikson and Wlezien, 2016).\nIn recent years, Agent-Based Models (ABMs) have gained traction in election research. These bottom-up approaches simulate individual voter decisions and social interactions, providing a more granular view of electoral dynamics. For instance, Gao et al. (2022a) introduced an agent-based election prediction model that captures social networks and voter-candidate interactions to offer more accurate election forecasts. This approach builds upon earlier work by Lemos et al. (2019), who demonstrated the effectiveness of ABMs in modeling voter turnout and preference formation.\nABMs excel at modeling heterogeneity and dynamic processes in real-world voting scenarios, offering greater flexibility and granularity than traditional statistical models. The work of Collins and Martinez (2023) further illustrates how ABMs can incorporate complex factors such as media influence and peer effects in voter decision-making processes. As the field evolves, future election studies may integrate ABMs with LLMs to balance large-scale data analysis with individual voter behavior modeling, as proposed by Zhang et al. (2024) in their hybrid forecasting framework."}, {"title": "2.3 LLMs in Political Science: Current Research and Our Contributions", "content": "Although LLMs have seen rapid advancements, their application in political science remains lim-"}, {"title": "3 Using LLMs for Election Result Prediction", "content": "How can we effectively leverage LLMs to predict election results? In this work, we simulate each voter's decision-making process by providing LLMs with detailed voter information and asking them to predict voter preferences based on that data. To achieve this, we focus on two key aspects: (1) establishing an evaluation framework with appropriate datasets that contain voter-level information, and (2) designing an LLM-based pipeline for accurate election predictions. In \u00a73.1, we introduce the datasets used in this study and describe the details of our pipeline evaluation process. We then provide an overview of our design approach in \u00a73.2, with a discussion of three progressive pipelines in \u00a73.2.1,"}, {"title": "3.1 Datasets, Evaluation, and Settings", "content": "Before presenting the pipelines for election prediction, we first describe the datasets, establish the evaluation framework, and introduce the experimental settings. In this work, we use two data sources: (1) real-world American National Election Studies (ANES) 2016 and 2020 Time Series data (Studies, 2019, 2022), and (2) voter-level synthetic data generated using advanced machine learning techniques based on aggregated information (Li et al., 2020b). Both datasets provide non-personally identifiable voter-level information. The following sections offer detailed descriptions of these datasets, explain their role in the evaluation framework, and outline the experimental settings used for testing the pipelines."}, {"title": "3.1.1 Real-world Data by American National Election Studies (ANES)", "content": "For evaluation, we use pre-election data from the ANES 2016 and 2020 Time Series Studies (Argyle et al., 2022; Studies, 2022), which provide 4,270 and 8,280 real-world samples, respectively, from individuals who participated in the 2016 and 2020 elections. The dataset includes a wide range of variables: (1) racial/ethnic self-identification, (2) gender, (3) age, (4) ideological self-placement on a conservative-liberal scale, (5) party identification, (6) political interest, (7) church attendance, (8) frequency of discussing politics with family and friends, (9) patriotic feelings associated with the American flag (unavailable in 2020), and (10) state of residence (unavailable in 2020). Additionally, the dataset records how individuals voted in both the 2016 and 2020 elections. Previous studies, such as Argyle et al. (2023), have evaluated GPT-3 using this dataset. We apply our method directly to this established benchmark to assess its effectiveness and performance."}, {"title": "3.1.2 Synthetic Data for the US Population", "content": "In addition to the medium-sized benchmark dataset, we utilize synthetic demographic data derived from a 1:1 synthetic population dataset of the United States (Li et al., 2020b). Synthetic data plays a crucial role in social and applied sciences, with recent applications in water quality estimation (Chia et al.,"}, {"title": "3.1.3 Evaluation Using Real-world and Synthetic Datasets", "content": "We employ two evaluation methods to assess our proposed approaches. First, for the ANES 2016 and 2020 benchmarks (Studies, 2019, 2022), we follow the methodology of Argyle et al. (2023). We compare the average voting probabilities:\nProbability = Republican Votes / (Republican Votes + Democratic Votes)\nAccuracy is assessed by comparing the predicted winning party with the actual election outcome.\nSecond, for the synthetic dataset, we treat each state as an independent validation unit. We compare the predicted results\u2014both in terms of the winning candidate and vote share percentages\u2014against the actual 2020 election results for each state. Accuracy is evaluated based on: (1) the agreement between the predicted and actual winning candidate for each state, and (2) the aggregate performance across all states, ensuring that the model reflects overall election trends. This state-level evaluation uses voter-level information processed through LLMs to predict outcomes accurately."}, {"title": "3.1.4 Hardware and LLM Settings", "content": "Our experiments are deployed on a GPU server equipped with an AMD EPYC Milan 7763 processor, 1 TB (64x16 GB) DDR4 memory, 15 TB SSD storage, and 6 NVIDIA RTX A6000 Ada GPUs. For the LLM component, we primarily utilize OpenAI's GPT-40 model for election predictions. Additionally, Meta's LLaMA 3.1 405B model is used in intermediate steps to provide neutral summarization of time-dependent information, enhancing certain pipelines by capturing temporal dynamics more effectively."}, {"title": "3.2 Our Progressive Design of LLM Pipelines", "content": "In this section, we present our progressive design for making voter-level election predictions using LLMs. As illustrated in Fig. 2, the methodology evolves through three distinct versions, each addressing limitations of the previous approach and incorporating more advanced techniques.\nV1: Demographic-only Prompting (\u00a73.2.1):\nThis version uses static demographic personas to prompt the LLM for voter-level predictions. While straightforward, it cannot account for temporal shifts in candidates' political focus over time, limiting its predictive power.\nV2: Single-step Prompting with Time-based Information (\u00a73.2.2): To address temporal factors, this version enriches the prompts with election-year-specific information, such as candidates' policy positions and campaign focuses. However, packing all relevant info. into a single prompt creates cognitive overload, which can hinder effective reasoning and reduce prediction accuracy.\nV3: Multi-step Reasoning with Contextual Information (\u00a73.2.3): This version breaks down the prediction process into sequential steps to improve reasoning. Structuring the decision-making process allows the model to effectively incorporate voter information, candidates' profiles, and political context. Our experiments across all datasets show that this approach produces improved predictions closely aligned with real-world outcomes.\nThe subsequent sections provide detailed descriptions of each version and its development. The quantitative evaluation of the three pipelines is presented in \u00a73.3."}, {"title": "3.2.1 Version 1: Demographic-only Prompting", "content": "Building on prior research demonstrating LLMs' ability to simulate human behavior (Xie et al., 2024), this initial version directly prompts the LLM with a persona and asks how that persona would vote (Argyle et al., 2023). This method provides all relevant information simultaneously, making it the simplest approach for voter-level prediction.\nLimitations: While simple and intuitive, this approach is limited by its inability to account for temporal changes in candidates' political agendas and public opinion. As a result, predictions for different years (e.g., 2020 vs. 2024) may not reflect meaningful variation, reducing the method's effectiveness in dynamic election contexts."}, {"title": "3.2.2 Version 2: Single-step Prompting with Time-based Information", "content": "Capturing macro-level and time-dependent variables is essential for bottom-up agent-based modeling in election prediction (Gao et al., 2022b). To enhance the contextual relevance of our simulations, we extended our pipeline by integrating election-year data sourced from Ballotpedia, a well-regarded political information platform. It includes campaign agendas, key policy stances, and candidates' biographical and professional backgrounds. Delivering this time-based information neutrally"}, {"title": "3.2.3 Version 3: Multi-step Reasoning with Contextual Information", "content": "To overcome the limitations of Version 2 and leverage insights from the ANES dataset analysis, we developed a multi-step prompting pipeline. Inspired by the Chain of Thought prompting strategy (Wei et al., 2022), this approach divides the task into intermediate steps, allowing the model to process information more systematically and accurately.\nThe process consists of two main steps: (1) Conservative-Liberal Spectrum Placement: First, the LLM is provided with a specific persona along with the current policy positions of both parties. The model is then asked to place the persona on the conservative-liberal spectrum based on the provided information. (2). Extended Persona and Voting Simulation: The conservative-liberal spectrum placement is incorporated into the persona to create an extended persona. This extended persona, along with the time-based information, is used in the second step to simulate voting behavior. The overall prompts are structured as follows:"}, {"title": "3.3 Validation on the Proposed Pipelines", "content": null}, {"title": "3.3.1 Evaluations on Real-world Data (ANES)", "content": "Settings. We evaluate our pipelines using the public ANES 2016 and 2020 Time Series datasets (Studies, 2019) (Studies, 2022) to: (1) assess the overall performance of each pipeline and (2) validate the V3 pipeline's ability to generate the critical Conservative-Liberal Spectrum feature.\nFor the first two pipelines (V1: Demographic-only Prompting and V2: Single-step Prompting with Time-based Information), we manually excluded the Conservative-Liberal Spectrum feature during evaluation to simulate how these pipelines would perform based purely on demographic data. This step mimics the limitations of simpler prompts that lack deeper ideological alignment.\nThe V3 pipeline, as outlined in \u00a73.2.3, addresses the limitations of earlier versions by using a multi-step reasoning approach inspired by"}, {"title": "3.3.2 Evaluations on Synthetic Data for the 2020 US Population", "content": "In addition to the nationwide evaluation on the ANES datasets, we conducted state-level simulations using synthetic data to compare predictions with actual 2020 election outcomes. For each state, we performed random sampling based on population size to ensure a statistically meaningful number of personas. The simulation outcomes were then benchmarked against official 2020 Presidential General Election Results from the Federal Election Commission (FEC). As in the benchmark evaluations, we calculated the average voting probabilities to assess the alignment of predictions with real-world outcomes. We evaluated five red states, five blue states, and 11 swing and tipping-point states."}, {"title": "4 Prediction for the 2024 US Election", "content": null}, {"title": "4.1 Experiment Settings", "content": "We extend our multi-step reasoning pipeline to predict the outcome of the 2024 US Presidential Election, featuring Donald Trump and Kamala Harris as the primary candidates, which thus introduces differences from the 2020 results. As in the 2020 simulations, we incorporate only the candidates' policy positions and backgrounds as time-sensitive inputs, excluding real-time data from social media or news sources to maintain consistency and control within the model's input space. We follow the same simulation setup used in \u00a73.3, utilizing identical hardware and LLM APIs. Since GPT-40's training corpus extends only until October 2023, this experiment evaluates the model's ability to make predictions on future, unseen data. After the election, these predictions can be compared with the actual results, offering a unique opportunity to assess the LLM's predictive capabilities.\nConsistent with our prior methods, we simulate individual votes cast exclusively for the Democratic and Republican parties, omitting No-Preference votes from the analysis. The preference for Trump (Republican Party) in each state is calculated using Eq. (3). We applied the V3 pipeline to 49 states, excluding Rhode Island (RI) and Connecticut (CT) due to missing data. As in previous experiments, we employed random sampling with proportional extraction of synthetic personas for each state, ensuring statistical significance while optimizing computational efficiency. This state-level analysis tests the adaptability of our multi-step pipeline to unseen election data and provides"}, {"title": "4.2 Prediction Results", "content": "The 2024 forecast by the LLM reveals several notable shifts compared to the 2020 prediction. A key change is in Wisconsin (WI), where Trump is now projected to win with 54.90% of the vote, marking a significant shift from 2020. Additionally, Trump shows improved performance in other swing and tipping-point states, including Pennsylvania (PA) with 47.85%, Michigan (MI) with 48.87%, and New Hampshire (NH) with 46.39%. However, despite these gains, he is still forecasted to narrowly lose these states to Harris.\nFor other critical states, the LLM predicts Trump will carry Arizona (AZ) with 51.09% of the vote, flipping it back to the Republicans, but lose North Carolina (NC) with 45.69%, consistent with the 2020 forecast despite the incorrect actual result in that election. In deep red states such as Texas (TX) with 56.36% and Alabama (AL) with 67.20%, Trump retains strong support. Similarly, Kamala Harris is expected to maintain dominance in traditional Democratic strongholds like California (CA) with 19.18%, New York (NY) with 22.41%, and Illinois (IL) with 30.36%.\nThe overall results indicate competitive dynamics in key swing states, where narrow margins may determine the final outcome. As shown in Figure 6, Trump is projected to win several battleground states like Georgia (GA) and Florida (FL) with 44.36% and 53.62%, respectively, strengthening his position. However, Harris still holds the edge in certain critical regions like Nevada (NV) with 34.77%.\nPrediction Results. Based on the LLM's predictions and the winner-takes-all electoral vote allocation, Trump is projected to win 268 electoral votes compared to Harris's 259, securing a narrow victory. However, if we were to include the excluded states\u2014Connecticut (CT) and Rhode Island (RI)-using their 2020 results, Harris's total would rise to 270 electoral votes, resulting in a razor-thin margin in favor of the Democratic candidate.\nLimitations. The 2024 prediction pipeline relies solely on synthetic persona demographics, the policy positions of both parties, and the candidates' biographical information. While this controlled setup ensures consistency in inputs, it introduces several limitations. First, the model does not account for shifts in public opinion, media narratives,"}, {"title": "5 Conclusion and Future Directions", "content": "In this work, we present a novel framework for election prediction using large language models (LLMs) with a focus on multi-step reasoning. By leveraging both synthetic personas and real-world datasets, we demonstrated the potential of LLMs to capture individual voting behaviors and state-level election outcomes. Our iterative design highlights the importance of integrating temporal information and complex reasoning for accurate predictions. The 2020 and 2024 simulations reveal both the strengths and limitations of using LLMs in dynamic political environments, emphasizing the model's ability to generalize on unseen data while showing the challenges associated with static demographic assumptions and limited real-time data inputs.\nFuture research can extend this work by incorporating multiple LLMs to better understand their internal political tendencies, enhancing temporal modeling with public opinion data and real-time trends for improved accuracy, and developing stronger multi-step reasoning pipelines through refined Chain of Thought (CoT) designs to further enhance prediction performance and mitigate biases. This study lays the foundation for future applications of LLMs in political forecasting, offering promising directions for further development in both election prediction and the broader study of LLM behavior in social contexts."}]}