{"title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Zijun Yao", "Bin Xu", "Lei Hou", "Juanzi Li"], "abstract": "Reward models (RMs) are crucial for the train-ing and inference-time scaling up of large lan-guage models (LLMs). However, existing re-ward models primarily focus on human prefer-ences, neglecting verifiable correctness signalswhich have shown strong potential in trainingLLMs. In this paper, we propose agentic re-ward modeling, a reward system that combinesreward models with verifiable correctness sig-nals from different aspects to provide reliablerewards. We empirically implement a rewardagent, named REWARDAGENT, that combineshuman preference rewards with two verifiablesignals: factuality and instruction following,to provide more reliable rewards. We conductcomprehensive experiments on existing rewardmodel benchmarks and inference time best-of-nsearches on real-world downstream tasks. RE-WARDAGENT significantly outperforms vanillareward models, demonstrating its effectiveness.We further construct training preference pairsusing REWARDAGENT and train an LLM withthe DPO objective, achieving superior perfor-mance on various NLP benchmarks comparedto conventional reward models. Our codes arepublicly released to facilitate further research\u00b9.", "sections": [{"title": "1 Introduction", "content": "Reward models (RMs) are designed to score thequality of responses and are typically used in thepost-training of large language models (LLMs),such as RL (Ouyang et al., 2022) and DPO train-ing (Rafailov et al., 2024), and in inference-timescaling laws (Wu et al., 2024; Snell et al., 2024),such as best-of-n search (Brown et al., 2024). Reli-able RMs are key to the success of modern LLMs.Despite the success of reward models, existingRMs primarily focus on human preferences, whichmay be susceptible to subjective biases (Saito et al.,"}, {"title": "2 Preliminaries", "content": "In the LLM domain, a reward model is typically aregression model that takes an instruction and a re-sponse as input and outputs a reward score (Ouyanget al., 2022), which can be formulated as $r_{RM}(x, y)$,where x denotes an instruction and y represents aresponse. Reward models are typically trained on alarge set of preference pairs based on the Bradley-Terry (BT) model (Bradley and Terry, 1952).However, due to the subjectivity and complex-ity of human preferences and the capacity limita-tions of the BT model (Munos et al., 2023; Swamy"}, {"title": "3 REWARDAGENT", "content": "In this work, we empirically implement a rewardagent, named REWARDAGENT, which integratesthe base human preference reward model with veri-fiable correctness signals from two key aspects: fac-tuality, which assesses the correctness of claimedfacts, and instruction-following, which evaluateswhether the response satisfies the hard constraintsspecified in the instruction (Zhou et al., 2023). Bothaspects significantly impact reliability and user ex-perience in real-world applications and are chal-lenging to evaluate effectively with existing rewardmodels (Liu et al., 2024b). This section introducesthe overall model architecture (\u00a7 3.1) and the spe-cific modules (\u00a7\u00a7 3.2 to 3.4) of REWARDAGENT."}, {"title": "3.1 Model Architecture", "content": "Following the concept in Euqation 1, the overallarchitecture of REWARDAGENT is illustrated inFigure 2, which consists of three main modules:(1) Router, which analyzes the instruction and de-termines which agents to invoke, corresponding to"}, {"title": "3.2 Router", "content": "Given an instruction, the router analyzes its re-quirements to the response to select the appropri-ate verification agents. The router is powered byan existing LLM backbone. Specifically, we firstmanually provide a concise description for eachverification agent, explaining its functionality andspecifying the conditions for its usage. Then, weinput the instruction with all agent descriptions intothe LLM, prompting it to select appropriate veri-fication agents for correctness assessment. Moreimplementation details are placed in appendix A."}, {"title": "3.3 Verification Agents", "content": "Factuality Previous studies have proposed vari-ous methods to evaluate the factuality of responses,such as FactScore (Min et al., 2023), which canbe directly used as a verification agent. However,these methods typically require extensive searchengine queries to verify the correctness of eachatomic fact, which is costly and inefficient for re-ward scoring. Intuitively, pairwise scoring basedon only the differences between two responses caneffectively reduce search engine queries and timecosts. Therefore, we propose a pairwise factual-ity verification agent for efficiently evaluating thefactual correctness of response pairs. The agent isillustrated in Figure 2, which consists of four maincomponents: (1) Difference proposal, which identi-fies key differences in claimed facts between twogiven responses. (2) Query generation, which con-structs queries based on the identified differencesto retrieve evidence for distinguishing these differ-ences. (3) Evidence generation, which uses thegenerated queries to retrieve supporting evidenceusing either external search engines or parametricknowledge in LLMs. (4) Verification, which as-"}, {"title": "3.4 Judger", "content": "The judger integrates reward scores from verifi-cation agents and human preferences from basereward models. In our implementation, we use aweighted sum as the judger, where $\\Lambda$ and $w_i$ areall set to 1.0, to compute the final reward score inEquation 1. One can also adopt different $\\Lambda$ and $w_ifor better applicability in different scenarios. Addi-tionally, the judger can dynamically adjust $\\Lambda$ and $w_ibased on the instruction like gating network (Wanget al., 2024a), we leave it as future work."}, {"title": "4 Experiments", "content": "This section presents experiments on several re-ward model benchmarks, including experimentalsetup (\u00a7 4.1), results (\u00a7 4.2), and analyses (\u00a7 4.3)."}, {"title": "4.1 Experimental Setup", "content": "REWARDAGENT Implementation We adopt theadvanced and lightweight ArmoRM (Wang et al.,2024a) as the base reward model to compute humanpreference scores. As REWARDAGENT is agnostic"}, {"title": "4.2 Experimental Results", "content": "Table 1 presents the experimental results, and wecan observe that: (1) Existing reward models fallshort in selecting more factual responses or betteradhering to hard constraints in instructions, whichmay limit their reliability in real-world applications.(2) REWARDAGENT significantly outperforms thebase reward model AromRM and the correspond-ing LLM backbone GPT-40 mini and Llama3-8BInstruct. It demonstrates that designing an appro-priate reward agentic workflow can effectively en-hance reward model performance. (3) Even whenusing Llama3-8B Instruct as the LLM backbone,REWARDAGENTLLAMA outperforms reward mod-els with much more parameters and more advancedproprietary LLMs such as GPT-40, which suggeststhat REWARDAGENT is more cost-efficient with-out requiring additional reward modeling trainingdata or more parameters to achieve advanced per-formance. (4) Using a search engine as an externalknowledge source for factuality slightly reducesperformance in RM-Bench and JudgeBench. Onepossible reason is that the retrieved informationmay contain noise or irrelevant information (Chenet al., 2024a). We leave the detailed analysis and de-sign of retrieval-augmented agents for future work.(5) REWARDAGENT achieves significant improve-ments on IFBench, particularly in the hard subset.It suggests that while not perfectly solved, exist-ing LLMs can effectively analyze hard constraintsand generate verification code, which can help thetraining of advanced LLMs (Lambert et al., 2024a).In conclusion, incorporating additional verifica-tion agents for specific scenarios (Mu et al., 2024;Lambert et al., 2024a), particularly those with ver-ifiable correctness, can develop more reliable andadvanced reward systems, presenting a promisingdirection for future reward model development."}, {"title": "4.3 Analysis", "content": "We first conduct an ablation study on the verifica-tion agents in REWARDAGENT. Specifically, we"}, {"title": "5 Applications", "content": "This section explores applying REWARDAGENT toinference-time search (\u00a7 5.1) and the training ofLLMs (\u00a7 5.2) to further validate its effectiveness."}, {"title": "5.1 Best-of-N Search", "content": "One important application of reward models is toconduct the inference-time search to find a betterresponse (Brown et al., 2024; Zhang et al., 2024a),which unleashes the inference-time scaling laws ofLLMs (Snell et al., 2024; Wu et al., 2024). There-fore, we explore applying REWARDAGENT to thebest-of-n search on downstream tasks. Specifically,we evaluate the best-of-n performance searched byREWARDAGENT on factuality question answeringand constrained instruction following tasks."}, {"title": "5.2 DPO Training", "content": "Reward models are primarily used to train LLMsusing RL (Ouyang et al., 2022) or DPO (Rafailovet al., 2024). Considering RL training is resource-intensive, we explore employing REWARDAGENTto construct preference pairs for DPO training tovalidate its effectiveness in real-world applications."}, {"title": "6 Related Work", "content": "Reward models are typically employed to scoreresponses and are crucial to the success of modernLLMs. Since the emergence of RLHF (Ouyanget al., 2022), numerous studies have focused ondeveloping more advanced reward models to helptrain LLMs. The approaches mainly include design-ing model architectures (Wang et al., 2024a; Dorka,2024; Chen, 2025) and utilizing more high-qualitydata or new training objectives (Infly, 2024; Yuanet al., 2024; Park et al., 2024; Liu et al., 2024a; Caiet al., 2024; Cao et al., 2024; Lou et al., 2024; Li"}, {"title": "7 Conclusion", "content": "In this paper, we propose agentic reward modeling,a reward system that integrates the human prefer-ences from conventional reward models with veri-fiable correctness signals to provide more reliablerewards. We empirically implement a reward agent,named REWARDAGENT, which consists of a router,well-designed verification agents for factuality andinstruction-following, and a judger. We conductextensive experiments on reward modeling bench-"}, {"title": "Limitations", "content": "The main limitations of this work lie in the imple-mentation of REWARDAGENT: (1) The verificationagents are far from providing perfect rewards, asthe average score on reward modeling benchmarksonly reaches 72.5%. This suggests that achievingperfect rewards is challenging and requires furtherresearch efforts. (2) We only implement verifica-tion agents for factuality and instruction-following,which we believe are current weaknesses in rewardmodels (Liu et al., 2024b) and important factors af-fecting LLM applications and user experiences. Weencourage the community to explore more verifi-able correctness signals. In conclusion, we believethe contribution of agentic reward modeling con-cept is substantial, and we look forward to develop-ing more advanced reward systems in the future."}, {"title": "Ethical Considerations", "content": "We discuss the ethical considerations here: (1) In-tellectual property. We have strictly adhered to thelicenses of all utilized artifacts, including datasets,models, and code repositories. We will open-sourceREWARDAGENT, code, and IFBench under theMIT license\u00b2. (2) Intended use and potential riskcontrol. We propose agentic reward modeling, areward system that integrates human preferenceswith correctness signals. We implement a rewardagent named REWARDAGENT to provide more re-liable rewards. We believe that all data used iswell anonymized. Our model does not introduceadditional ethical concerns but may provide incor-rect rewards due to performance limitations. Usersshould not conduct reward hacking (Skalse et al.,2022) and should carefully check important infor-mation. (3) AI assistance. We have used ChatGPTto refine some sentences."}]}