{"title": "Improving Vietnamese Legal Document Retrieval using Synthetic Data", "authors": ["Pham Tien Son", "Nguyen Doan Hieu", "Nguyen Dai An", "Dinh Viet Sang"], "abstract": "In the field of legal information retrieval, effective embedding-\nbased models are essential for accurate question-answering systems. How-\never, the scarcity of large annotated datasets poses a significant chal-\nlenge, particularly for Vietnamese legal texts. To address this issue, we\npropose a novel approach that leverages large language models to gener-\nate high-quality, diverse synthetic queries for Vietnamese legal passages.\nThis synthetic data is then used to pre-train retrieval models, specifically\nbi-encoder and ColBERT, which are further fine-tuned using contrastive\nloss with mined hard negatives. Our experiments demonstrate that these\nenhancements lead to strong improvement in retrieval accuracy, validat-\ning the effectiveness of synthetic data and pre-training techniques in\novercoming the limitations posed by the lack of large labeled datasets in\nthe Vietnamese legal domain.", "sections": [{"title": "1 Introduction", "content": "The task of passage retrieval, which involves identifying relevant passages from\na large corpus in response to a query, has become increasingly important with\nthe rise of pre-trained language models like BERT [5]. Enhancements such as\nSentence-BERT [22] and SimCSE [8] have further improved text embedding tech-\nniques, leading to significant advances in Natural Language Processing (NLP).\nFor Vietnamese legal information retrieval, accurate and efficient retrieval\nsystems are essential, particularly in legal question answering (QA) systems.\nPrior research has explored various models for this task: [12] introduced a new\nattention-based architecture, [25] proposed combining BM25 with RoBERTa,\nand [18] used models like Sentence-BERT and coCondenser to enhance retrieval\nperformance while [19] utilized SimCSE and an ensemble reranker to produce\nstate-of-the-art results on their benchmark data. Despite these efforts, a key limi-\ntation remains the scarcity of high-quality annotated datasets in the Vietnamese\nlegal domain, hindering the development of robust retrieval systems."}, {"title": "2 Related Work", "content": "Text Embeddings. Recently, there has been a shift of interest towards the\nuse of neural retrieval techniques, which rely on dense vector representations to\ncapture the underlying semantics of both queries and documents. Unlike tra-\nditional keyword-based approaches like BM25 or TF-IDF, neural models such\nas Sentence-BERT [22] and SimCSE [8] produce embeddings that enable se-\nmantic matching, allowing for more accurate retrieval based on context rather\nthan exact term matching. These models leverage techniques like [CLS] token\npooling or mean-pooling to aggregate embeddings, enabling fast and scalable\ncomparisons via cosine similarity or dot product calculations. The contrastive\nloss is commonly used during training to fine-tune the models, aligning relevant\nquery-document pairs while distancing irrelevant ones [14].\nMulti-vector models like ColBERT [11], extend this approach by represent-\ning queries and documents with multiple vectors, providing more detailed in-\nteractions for improved retrieval accuracy. One notable development is the M3-\nEmbedding model [4], part of the BGE-M3 project, which supports dense, multi-\nvector, and sparse retrieval. This model's versatility allows it to adapt to various\nretrieval scenarios, making it particularly beneficial for applications like legal\ntext analysis. Additionally, M3-Embedding's multilingual capabilities, support-\ning over 100 languages, and its ability to handle input texts up to 8192 tokens,\nmake it ideal for cross-lingual and large-scale document retrieval tasks.\nSynthetic data. One of the major obstacles to the widespread adoption of\nneural retrieval models is their requirement for large supervised training sets\nto surpass traditional term-based techniques, which are constructed from raw\ncorpora [15]. Thakur et al. [24] find that BM25 remains a robust baseline for\nout-of-domain tasks. This highlights a critical challenge: in-domain performance\ncannot reliably predict how well an approach will generalize in a zero-shot setup.\nMany approaches that outperform BM25 on an in-domain evaluation perform\npoorly on the BEIR datasets.\nPrevious works have demonstrated that leveraging pretrained language mod-\nels to generate queries can significantly enhance the performance of retrieval\nmodels in the absence of in-domain labeled data [15,13,3]. Motivated by a line of\nwork on knowledge distillation from black-box LLMs through training on syn-\nthetic data generated from them, such as Orca [16] and Phi [9,27] use GPT-3.5/4\n[1] to generate query-document pairs across multiple tasks and languages. They\nthen train open-source decoder-only LLMs on this synthetic data using stan-\ndard contrastive loss, achieving state-of-the-art results in information retrieval\nbenchmarks.\nHowever, using large language models for information retrieval tasks as [27]\nis costly in practice, both in terms of computational resources and inference\nlatency. Thus, this work examines the prospect of distilling the knowledge from\nthese advanced LLMs into smaller language models such as BERT.\nPre-training tailored for information retrieval. Another approach to im-\nprove the performance of dense retrieval that has been studied is enhancing\nthe pre-training process specifically for dense retrieval tasks. Techniques like\nCondenser [6] involve adding a shallow decoder to the encoder, forcing it to re-\nconstruct masked texts, thereby enhancing the encoder's capacity to produce\nmeaningful embeddings. Based on this, coCondenser [7] further improves per-\nformance by incorporating contrastive loss, which ensures that embeddings of\nsimilar text spans are brought closer together while those of different spans are\npushed apart.\nAnother innovative approach is CoT-MAE [29], which introduces a contextu-\nalized masked auto-encoder structure. This model uses both the target text and\nits context for reconstruction, further enhancing the encoder's understanding of\nthe text within its broader context. Additionally, models like Query-as-Context\n[26] extend these ideas by generating queries from passages for pre-training,\nwhich has shown promising results in improving retrieval performance. These"}, {"title": "3 Methodology", "content": "In this subsection, we outline our comprehensive workflow for generating syn-\nthetic query datasets and fine-tuning retrieval models using Vietnamese legal\ntexts. This workflow is visually represented in Figure 1, which illustrates the\nsequential stages from data collection to model fine-tuning.\nOur methodology is divided into several key stages:\nStage 1: Collect legal text data. This initial stage involves the collec-\ntion and preprocessing of legal documents. The documents are split into smaller\npassages suitable for further processing.\nStage 2: Generate queries using Llama 3 prompt with legal pas-\nsages. Using the Meta Llama 3 model, we generate synthetic queries based on\naspects identified in the legal text passages. This involves generating aspects\nfrom the legal texts and then crafting queries for these aspects.\nStage 3: Filter low-quality queries. We remove low-quality queries that\nexplicitly refer to the input passage and those that are only shallowly relevant\nto the input passage. For queries that are only shallowly relevant, we use the\nBGE-M3 dense retriever to filter out synthetic queries that cannot recover their\ninput passage within the top 40 retrieved results.\nStage 4: Query-as-Context Pre-training. We employ the generated queries\nto further pre-train our language model, focusing on enhancing its ability to un-\nderstand and retrieve relevant passages.\nStage 5: Hard negatives mining for fine-tuning datasets. We mine\nhard negative examples to create a robust fine-tuning dataset, which is crucial\nfor improving the retrieval model's accuracy.\nStage 6: Fine-tuning. The final stage involves fine-tuning the pre-trained\nmodel with the generated data, optimizing it for the specific task of legal text\nretrieval."}, {"title": "3.2 Data Curation", "content": "Wang et al. [27] use GPT-3.5/4 to generate queries along with corresponding\npositive and hard negative passages by maintaining output diversity through\ntwo stages of generation: first generating retrieval tasks, then using them to\ngenerate query-passage pairs. As our objective focuses on creating a domain-\nspecific dataset for Vietnamese legal text retrieval, relying solely on prompt\nengineering for this task would be complex and inefficient. Therefore, we opted\nto use collected legal text passages as input for an LLM to generate queries\nrelated to the content of each passage.\nOur main source for collecting legal documents was thuvienphapluat.vn. The\nwebsite hosts a wide range of legal documents, including Laws, Decrees, Circu-\nlars, Joint Circulars, Resolutions, Ordinances, Decisions, and the Constitution.\nAfter scraping both the metadata and full-text content, we preserved the hier-\narchical structure of each document, which typically includes chapters, sections,\narticles, and clauses. This structure was essential for maintaining context and\nensuring that the text remained coherent after being split into smaller passages.\nEach passage retained crucial information, such as the document's domain, ti-\ntle, header, and main content, which provided the necessary context for accurate\nquery generation. This process resulted in 143,261 passages, which were used as\ninput for the LLM to generate high-quality, contextually relevant queries."}, {"title": "3.3 Synthetic Query Generation", "content": "For generating synthetic queries, we chose the open-source LLM Llama 3 70B [2]\ndue to its strong performance, particularly in Vietnamese. Llama 3, trained on\nover 15 trillion tokens, outperforms many LLMs with a similar parameter count\nand demonstrates robust multilingual capabilities, making it well-suited for our\nneeds.\nTo maintain diversity and relevance in query generation, we experimented\nwith different prompting techniques. A direct approach, where the model gen-\nerated questions without identifying distinct aspects of the passage, often led to\nless diverse and sometimes irrelevant queries. Through various prompt designs,\nwe discovered that instructing the model to identify 1 - 5 different aspects cov-\nered in the passage and then generate a question for each aspect yielded the\nmost relevant and diverse queries. In Figure 2, we show the prompt template\nused to generate synthetic queries from legal text passages. Examples of a gen-\nerated query and its corresponding passage are shown in Table 1. A quantitative\nanalysis of the generation method will be later discussed in Section 5.1.\nApplying this method, we generated over 620,000 legal queries from 140,292\npassages extracted from our curated Vietnamese legal text collection. We then\nemployed the BGE-M3 dense retriever [4], which demonstrated strong zero-shot\nperformance in our testing, to filter out queries whose corresponding passages\ndid not appear in the top 40 relevant results. Additionally, we excluded queries\nthat directly referred to the passage using terms like \"quy \u0111\u1ecbnh n\u00e0y\" or \"th\u00f4ng t\u01b0\nn\u00e0y\". This process results in a final dataset of 507,152 Vietnamese legal queries,\ncovering a wide range of legal domains, which is then used to pre-train and\nfine-tune our retrievers.\nWe generated these synthetic queries using Llama 3 70B through Together\nAI's free credits program, with the entire generation process costing approxi-\nmately 200 USD."}, {"title": "3.4 Pre-training", "content": "Query-as-Context pre-training [26] is based on the observation that text spans\nwithin the same document can vary significantly in semantics, potentially weak-\nening the effectiveness of traditional pre-training techniques. However, one lim-\nitation noted in this approach is that the T5 model used often produced a sub-\nstantial number of unrelated query-passage pairs, which could diminish its overall\neffectiveness. This aligns well with the first part of our work generating a legal\nquery dataset where we address this issue by utilizing an advanced LLM to\nproduce queries and implementing a filtering process to remove irrelevant pairs,\nas detailed in the previous subsection.\nUsing the pairs of collected passages xi and corresponding legal queries\nYi generated by Llama 3, we apply the loss function from the Contextualized\nMasked Autoencoder (CoT-MAE) framework [29]. Specifically, the encoder re-\nconstructs the passage xi using its unmasked tokens, while the decoder recon-\nstructs the query yi by leveraging both its unmasked tokens and the contextual\npassage xi. The training objective combines the encoder's masked language mod-\neling (MLM) loss and the decoder's context-supervised MLM loss, ensuring that\nthe model learns to effectively integrate the query and passage context during\npre-training."}, {"title": "3.5 Fine-tuning", "content": "To verify the effectiveness of our pre-training, we fine-tuned a bi-encoder and\nColBERT model on downstream retrieval tasks. Our fine-tuning process is based\non a single-stage pipeline with hard negative mining, utilizing a comprehensive\nset of datasets.\nFor both models, we utilized the MS-MARCO passage ranking dataset, SQUAD\n2.0, 80% of the Legal Text Retrieval Zalo 2021 training challenge dataset, and\nthe dataset collected from thuvienphapluat.vn. Additionally, our synthetic query\ndata generated for pre-training is also used in this fine-tuning process. For all\ndatasets, we create hard negative passages for each training query from the\nBGE-M3 dense retrieval model.\nFor each query $q^+$, the positive passage $p^+$ forms a pair ($h_{p^+}, h_{q^+}$). The\nnegative samples {$p^\u2212$} included hard negatives identified by the BGE-M3 dense\nretrieval and in-batch negative passages. The training objective for both models\nis to maximize the similarity between the query and the positive passage while\nminimizing the similarity between the query and the negative passages. This is\nachieved using the InfoNCE loss function:\n$L = -log \\frac{exp(sim(h_{q^+}, h_{p^+})/\\tau)}{\\sum_{p \\in {p^+, p^-}} exp(sim(h_q, h_p)/\\tau)},$\nwhere $ \\tau $ is a temperature hyper-parameter fixed to 1, and sim(\u00b7, \u00b7) represents the\ndot product similarity function."}, {"title": "4 Experiment", "content": "Table 2 presents the datasets used for fine-tuning and evaluation. Our primary\nfine-tuning datasets include MS-MARCO [17], SQUAD 2.0 [21], 80% of the train-\ning set from the Legal Text Retrieval Zalo 2021 challenge (Legal Zalo 21), and\nthe newly introduced TVPL dataset. Since MS-MARCO and SQUAD 2.0 are\noriginally in English, we translated them into Vietnamese using Google Trans-\nlate, following the approach of [20]. The use of large, translated datasets has\nproven beneficial for improving the performance of monolingual retriever models\ndue to the absence of comparably large annotated datasets for Vietnamese.\nWe developed TVPL\na new benchmark dataset for Vietnamese legal text\nretrieval. TVPL is named after the thuvienphapluat.vn website, from which its\nlegal QA articles were sourced. The dataset comprises a training set of 165,334\nqueries and a test set of 10,000 queries, along with a corpus of 224,006 legal\npassages. This dataset addresses previous limitations of scale and diversity, pro-\nviding a more comprehensive benchmark for evaluating retrieval models in the\nVietnamese legal domain.\nAdditionally, our 507,152 generated queries using the Llama3-70B model\nbased on passages extracted from Vietnamese legal texts are also used in this\nfine-tuning stage. This synthetic dataset spans a broad range of legal domains, as shown in Figure 3."}, {"title": "4.2 Model Pre-training and Fine-tuning", "content": "We used the synthetic dataset for pre-training, with text segmented by the Un-\nderthesea library. During pre-training, we select a batch of passages at each\nstep and randomly choose a candidate query as context for each passage to\nform a relevant pair. The encoder for CoT-MAE was initialized with a pre-\ntrained PhoBERT-base-v2 model, while the decoder was trained from scratch.\nPre-training was conducted for 2,000 steps using the AdamW optimizer with a\nlearning rate of 1e-4, a batch size of 1024, and a linear schedule on a TPU v4-8.\nAfter training, the decoder was discarded, and only the encoder was retained for\nfine-tuning."}, {"title": "4.5 Out-of-domain Evaluation", "content": "To further evaluate the robustness and generalization capabilities of our models,\nwe conducted an out-of-domain evaluation on the Vietnamese Wiki Question\nAnswering dataset from the Zalo AI Challenge 2019. This dataset contains query-\npassage pairs covering a wide range of topics beyond the legal domain, on which\nour models were specifically pre-trained and fine-tuned.\nDespite being trained solely in the legal context, our models demonstrated\nimproved performance on the out-of-domain dataset, as shown in Table 4. No-\ntably, the pre-trained and fine-tuned ColBERT model achieves scores close to\nthose of larger multilingual retrievers, such as mE5base and BGE-M3. These\nmodels, with significantly larger parameter counts (mE5base at 270M and BGE-\nM3 at 560M, compared to our models with 124M), are still strong candidates\nfor zero-shot retrieval tasks. The results suggest that our approach, though spe-\ncialized for legal text, maintains strong generalization capabilities in broader\nretrieval scenarios."}, {"title": "5 Analyses", "content": "We analyze the improvements brought by the aspect-guided query generation\nmethod (Section 3.3) compared to basic prompting, where the LLM generates\nqueries directly from input passages. Performance is evaluated using passage\nhit rate (the percentage of queries retrieving their corresponding passage) and\ndocument hit rate (the percentage of queries retrieving the correct document).\nWe use the BGE-M3 dense retriever [4] to rank the top-k relevant passages for\neach query."}, {"title": "5.2 Quality - Space Footprint Trade-off: ColBERT's Residual Compression", "content": "We examine the impact of increasing the number of bits for compression on\nboth retrieval accuracy and storage requirements. Experiments were conducted\non 224,006 passages from the TVPL dataset, with evaluation metrics on its\ntest set. ColBERT's residual compression, as proposed in [23], offers improve-\nments in both storage and retrieval performance. While higher bit sizes improve\nperformance slightly, they come with a significant increase in storage. In con-\ntrast, The 1-bit ColBERT configuration performs competitively with minimal\nstorage (647.33MB), even outperforming bi-encoder (672.02MB) in metrics like\nMRR@10 and MAP@10."}, {"title": "6 Conclusion and Future Work", "content": "In this work, we proposed methods to improve Vietnamese legal text retrieval\nusing synthetic data. Our key contributions include generating synthetic legal\nqueries from Vietnamese legal text passages with a pre-trained LLM, creat-\ning a dataset of 500K query-passage pairs, and significantly enhancing retrieval\naccuracy with bi-encoder and ColBERT models trained on this dataset. Our\nexperiments demonstrate the effectiveness of fine-tuning with synthetic data for\nimproving model performance. We also applied the query-as-context CoT-MAE\npre-training technique, which further boosted retrieval accuracy. The combina-\ntion of synthetic data and CoT-MAE pre-training consistently yielded superior\nperformance in both in-domain and out-of-domain evaluations.\nThe dataset generated in this work has been made publicly available on Hug-\nging Face under the CC BY 4.0 license\u00b9. Additionally, we are also publishing the\nTVPL benchmark dataset queries\u00b2. We hope that these datasets and the method\nintroduced can help advance the development of new language models for Viet-\nnamese, potentially extending beyond legal contexts, and support applications\nlike legal search and question-answering systems to benefit both public servants\nand citizens.\nIn future work, we want to explore the potential of using generated queries as\ninputs for large language models to create an artificial legal corpus. This corpus\ncould, in turn, produce additional queries, creating a cycle to expand the dataset\nfurther. A deeper qualitative analysis comparing synthetic data to real-world\ndata is necessary to understand this approach's strengths and limitations better.\nWe also aim to examine potential performance collapse in models primarily\ntrained on synthetic data and investigate strategies to mitigate this risk."}]}