{"title": "Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models", "authors": ["Chang-Jin Li", "Jiyuan Zhang", "Yun Tang", "Jian Li"], "abstract": "Personality assessment, particularly through situational judgment tests (SJTs), is a vital tool for psychological research, talent selection, and educational evaluation. This study explores the potential of GPT-4, a state-of-the-art large language model (LLM), to automate the generation of personality situational judgment tests (PSJTs) in Chinese. Traditional SJT development is labor-intensive and prone to biases, while GPT-4 offers a scalable, efficient alternative. Two studies were conducted: Study 1 evaluated the impact of prompt design and temperature settings on content validity, finding that optimized prompts with a temperature of 1.0 produced creative and accurate items. Study 2 assessed the psychometric properties of GPT-4-generated PSJTs, revealing that they demonstrated satisfactory reliability and validity, surpassing the performance of manually developed tests in measuring the Big Five personality traits. This research highlights GPT-4's effectiveness in developing high-quality PSJTs, providing a scalable and innovative method for psychometric test development. These findings expand the possibilities of automatic item generation and the application of LLMs in psychology, and offer practical implications for streamlining test development processes in resource-limited settings.", "sections": [{"title": "Introduction", "content": "Personality assessments are essential across various fields, including recruitment, education, psychological research, and clinical settings. These tools help capture individual differences in traits and behaviors, which can influence life outcomes and professional performance. However, the traditional development of personality tests is time-consuming, resource-intensive, and often susceptible to biases introduced by human item writers (Bejar, 2002; Gierl & Lai, 2012). Given the growing demand for more efficient and scalable assessments, developing automatic methods for test item generation has become a priority.\nAutomatic item generation (AIG) represents a promising solution to these challenges. By leveraging computational methods to generate test items based on specific frameworks and psychometric principles, AIG can significantly enhance the efficiency and precision of personality assessments (von Davier, 2018; Zickar, 2020). While previous research has applied AIG techniques to Likert-type scales, little attention has been paid to situational judgment tests (SJTs), which require the generation of complex, context-rich scenarios to assess personality traits.\nSituational judgment tests offer several advantages over traditional Likert-type self-report scales. They simulate real-world scenarios, requiring respondents to make judgments or decisions in context with more cognitive effort, thus reducing the likelihood of response biases, such as social desirability or deliberate falsification (Hooper et al., 2006; Kasten et al., 2020). Additionally, SJTs can capture the subtler judgment processes by linking specific behaviors to situational factors, offering a more accurate assessment of personality traits (Olaru et al., 2019). Furthermore, SJTs have less adverse impact on minorities, especially with low cognitive loading and behavioral tendency instructions (Lievens et al., 2008). Despite these advantages, developing SJTs remains a labor-intensive process, the most challenging of which is to ensure that scenarios are both realistic and psychometrically sound.\nWith the rapid advancement of large language models (LLMs), such as GPT-4, new opportunities have emerged for automating the generation of personality SJTs. LLMs can generate highly contextual and semantically rich text, offering a potential breakthrough in SJT development by producing diverse, complex scenarios with minimal human input. Furthermore, LLMs have demonstrated success in various fields, including healthcare (Jeblick et al., 2024), education (Kasneci et al., 2023), law (Choi et al., 2022), finance (Liu et al., 2021), and scientific research (Van Noorden & Perkel, 2023), making them a natural candidate for psychometric applications.\nThis research explores the potential of LLMs, particularly GPT-4, in addressing the challenges of developing personality situational judgment tests (PSJTs). The studies focus on three primary goals. Firstly, to evaluate the feasibility of using GPT-4 for generating PSJTs. Secondly, to optimize prompt strategies and parameter settings in GPT-4 to enhance the quality and validity of generated items. Thirdly, to empirically validate the psychometric properties of GPT-4-generated PSJTs (GPSJTs). By addressing these objectives, this research seeks to provide a new paradigm for automatic test development, with potential applications in personality assessment and other psychological and educational testing areas."}, {"title": "Literature Review", "content": ""}, {"title": "Automatic Item Generation", "content": "Automatic item generation (AIG) is a technique that has gained increasing attention for its ability to streamline the test development process. Initially proposed in the 1970s, AIG has recently gained wider adoption, primarily due to advancements in computational power and algorithm design (Li & Zhang, 2008). AIG uses specially designed algorithms to generate test items based on specific theoretical frameworks and psychometric principles, ensuring each item is customized to meet defined parameters (Embretson, 2005; Embretson & Yang, 2007). Unlike manually generated test items, which are often time-consuming and resource-intensive, AIG can quickly produce large volumes of items tailored to various test-taker abilities and needs.\nOne of AIG's key advantages is its capacity to enhance the efficiency and quality of test development. By specifying psychometric properties such as difficulty levels, AIG reduces the need for traditional trial-and-error processes, where poorly performing items are often discarded after pilot testing. Additionally, the automatic process ensures structural validity at the item level, allowing for more precise control over cognitive complexity (Embretson, 2005). This enhances the overall accuracy and reliability of the tests.\nHowever, despite its strengths, traditional AIG approaches like item modeling and cognitive design system approaches have limitations when generating non-cognitive assessments, such as personality tests. The item modeling approaches rely on templates to create new items by replacing irrelevant elements (e.g., names, numbers) while maintaining the item's core structure (Bejar et al., 2002). The cognitive design system approaches construct items based on mental models, mainly focusing on identifying critical features of task-solving processes (Bejar et al., 2002). While these approaches excel in measuring cognitive functions like mathematics (Gier & Lai, 2015) and spatial reasoning (Arendasy, 2005; Arendasy et al., 2010), where items can be modeled using clear, well-defined rules, they are less suited to generating items for personality assessments, especially situational judgment tests (SJTs) that require more nuanced, context-rich scenarios that capture complex human behaviors and personality traits (Hommel et al., 2022).\nSeveral approaches to AIG have been developed to address these challenges, such as semantic analysis and deep learning approaches. Semantic analysis approaches analyze the syntax and vocabulary of existing items to generate new ones. Nevertheless, these methods rely on template-like frameworks, limiting their ability to create the rich, context-specific items necessary for SJTs (Huang & He, 2016). Deep learning approaches, such as recurrent neural networks (RNNs) or transformer-based models like GPT, promise to overcome these limitations. These models can generate items based on large text corpora, producing more varied and nuanced items (von Davier, 2018). However, even these advanced methods require further refinement, particularly in distinguishing between specific constructs and maintaining the psychometric quality of generated items (Hernandez & Nie, 2023; Hommel et al., 2022).\nWhile AIG has brought significant advancements to cognitive testing, its application in non-cognitive domains like personality assessment remains challenging. Newer approaches, such as deep learning, offer a pathway to more sophisticated item generation, especially for SJTs."}, {"title": "Research on Generating Personality Tests Using Large Language Models", "content": "The rapid advancement of large language models (LLMs), such as GPT-2 and GPT-3, has transformed natural language processing tasks like text generation, translation, and summarization, enabled by deep learning techniques and transformer-based architecture. Trained on vast text corpora, LLMs offer significant advantages over traditional AIG methods, particularly in psychometrics. Unlike template-based or cognitively modeled AIG approaches, LLMs excel in generating semantically rich and contextually diverse items, making them ideal for non-cognitive assessments like personality tests. Their ability to capture nuanced language and behavior is especially valuable for creating SJTs that mirror complex interpersonal dynamics and real-world scenarios.\nThe application of LLMs in personality test generation has shown promising potential. Hommel et al. (2022) demonstrated that GPT-2 could generate items comparable to manually crafted ones, though internal consistency remained a challenge, highlighting the need for further fine-tuning. Hernandez and Nie (2023) improved GPT-2's performance through iterative fine-tuning, achieving high Cronbach's alpha coefficients and maintaining traditional factor structures, though issues with grammatical errors persisted. By contrast, G\u00f6tz et al. (2024) found that most items generated by a cloned GPT-2 model failed to meet psychometric criteria, emphasizing the limitations of insufficient fine-tuning. Later, research on GPT-3 revealed its superior ability to capture personality nuances, with prompt-based item generation producing favorable psychometric properties (Lee et al., 2023). However, Liu et al. (2023) noted that prompt design critically influenced item diversity, underscoring its importance in achieving valid results.\nDespite these advancements, several challenges remain in generating high-quality personality test items using LLMs. One significant issue is the balance between creativity and text quality. While LLMs can generate diverse and context-rich items, they sometimes produce text that lacks the coherence or precision necessary for psychometric assessments. This issue is often linked to the settings of model parameters (e.g., temperature, presence_penalty, top_p, etc.; OpenAI et al., 2024). With strong model performance and proper prompt design, the other parameters have less impact than the temperature parameter, which controls the randomness of the generated text. Higher temperatures lead to more creative outputs and increase the risk of generating nonsensical or inconsistent items. Conversely, lower temperatures produce more conservative and repetitive items, limiting the diversity needed for a robust assessment.\nAnother critical challenge is ensuring that the items generated by LLMs accurately reflect the intended psychological constructs. G\u00f6tz et al. (2024) found that although LLMs could generate vast quantities of items, many of these items lacked precise alignment with the underlying constructs, leading to poor psychometric performance. This underscores the importance of refining prompt strategies and model parameters to ensure the generated items are valid and reliable.\nTo address these challenges, recent research has focused on prompt engineering-carefully designing input prompts to guide LLMs in generating high-quality outputs. By following prompt instructions, LLMs can perform domain-specific tasks for which they have not been specifically trained. They can apply previously learned knowledge to new and diverse contexts, thus enhancing their overall effectiveness and practicality"}, {"title": "The Present Study", "content": "Despite advancements in AIG, traditional methods need help to generate nuanced, context-rich items required for SJTs. While LLMs like GPT-2 and GPT-3 offer a promising solution by developing more diverse and sophisticated items, challenges remain, particularly in conceptual relevance and linguistic clarity. For instance, while GPT-2-generated items have been found to mirror traditional test items in terms of factor structure, the presence of grammatical inconsistencies and irrelevant content remains a significant challenge (G\u00f6tz et al., 2024).\nThe release of GPT-4 by OpenAI in 2023 marked a major advance in natural language processing, showing significant improvements in text comprehension, generation, and robustness compared to earlier models. Its expanded architecture and diverse training data enable it to produce more fluent, coherent, and accurate outputs while reducing grammatical errors and irrelevant responses (OpenAI et al., 2024). Although GPT-4 has yet to be directly applied to AIG for SJTs, its enhanced text generation capabilities and understanding of complex psychological concepts indicate strong potential for such applications.\nTherefore, this study addresses the gaps in LLM-based AIG by using the state-of-art model, GPT-4, to ensure the content validity of PSJTs. The following hypotheses were thus proposed:\nH1: The GPT-4-generated PSJTs (GPSJTs) demonstrate good content validity, comparable to or not inferior to manually-generated PSJTs.\nIn the application of LLMs, model parameters\u2014particularly temperature-play a crucial role in shaping outputs. By adjusting the temperature setting, users can control the randomness, variability, and creativity of the generated text. A higher temperature increases the likelihood of the model selecting lower-probability words, enhancing the output's creativity and diversity (OpenAI et al., 2024). This increased diversity may allow the generated items to better capture behaviors relevant to the target personality constructs, thus improving content validity. However, if the temperature is set too high (e.g., higher than 1.0), the output may become excessively disorganized and difficult to interpret, ultimately reducing text quality. According to the API reference (n.d.) released by OpenAI, the default temperature parameter value for chat completion is 1.0, within a range of 0 to 2. This led to the following hypothesis:\nH2: The highest content validity of the GPSJTs is achieved when the temperature parameter is set to 1.0.\nIn addition to model performance and parameters, the syntax (e.g., length, whitespace, order of examples) and semantics (e.g., wording, instructions, example selection) of prompts can significantly impact the quality of model outputs (Marvin, Hellen, Jjingo, & Nakatumba-Nabende, 2023). By carefully tailoring prompts to meet specific output requirements, LLMs can apply previously acquired knowledge to new, diverse contexts, enhancing their effectiveness and practical utility (Ling et al., 2024). This approach introduces new possibilities for generating PSJTs. In previous research, the prompt strategy Lee et al. (2023) employed in generating Likert-type personality scales was relatively preliminary. It lacked detailed explanations of construct meanings or precise output requirements and relied too heavily on examples, leading to suboptimal generation outcomes.\nTo address these limitations, the present study seeks to optimize prompt design by incorporating detailed construct definitions, enriching context descriptions, and providing clear task instructions. These enhancements aim to ensure that the generated content is more closely aligned with the requirements of PSJTs. Thus, we proposed the following hypothesis:\nH3: The content validity of the GPSJTs generated by the optimized prompt was higher than that of the unoptimized version adapted from Lee et al. (2023).\nPrevious research has yet to thoroughly examine the stability of psychological scales generated by LLMs using identical prompts and parameters at different times. Given the inherent randomness of LLMs, it is possible that even with identical prompts and parameters, scales generated at other times may vary in both textual content and psychometric properties. For AIG to be practically valuable particularly at a level where the generated scales can be used directly\u2014it is essential to go beyond validating a single instance. Instead, it is necessary to assess whether multiple scales, generated under identical prompts and parameters at different times, consistently meet validity standards. Currently, there is a lack of evidence supporting such stability. Ensuring that the same methodology reliably produces equally valid scales across different times and contexts is critical for upholding the scientific rigor and practical applicability of psychometric assessments. To address this gap, we will examine the stability of content validity in GPSJTs. We proposed the following hypothesis:\nH4: The GPSJTs using identical prompts and parameters demonstrate stable content validity across different time points, showing no significant differences in content validity at various times.\nAfter validating the feasibility and stability of GPT-4 in automatically generating PSJT items, it is essential to provide empirical evidence that the items generated by GPT-4 can indeed form a fully functional scale, capable of withstanding psychometric evaluation and perform on par with traditional, well-established measurement methods. To this end, we used the optimal prompts and parameter settings for GPT-4 to automatically generate items, developing an SJT to assess the Big Five personality traits. The GPSJT then underwent rigorous psychometric scrutiny. We proposed the following hypotheses:\nH5: The reliability of GPSJT is equal to that of manually generated PSJTs.\nH6: The GPSJT has good validity."}, {"title": "Study 1: Content Validity and Stability of GPT-4-generated Personality Situational Judgment Tests", "content": "Study 1 utilized GPT-4 to generate an SJT focusing on the self-consciousness facet of the Big Five factor neuroticism (Costa & McCrae, 1995). An expert panel assessed the content validity of the GPSJT items and the SJT items manually designed by Mussel et al. (2018). Additionally, the study explored the stability of content validity by examining item generation over time while optimizing prompt strategies and temperature settings.\nThe study involved three comparisons: (1) temperature variations, comparing the content validity of manually-generated PSJT and GPSJT generated at various temperature settings; (2) prompt variations, comparing the content validity of manually-generated items and GPSJT items generated using different prompt strategies; (3) stability validation, comparing the content validity of GPSJT generated at different times using the same prompt strategies and temperature settings."}, {"title": "Participants", "content": "The expert panel consisted of eight doctoral students in psychology from Beijing Normal University, Central China Normal University, and Shaanxi Normal University, all of whom were familiar with the development of personality tests and situational judgment tests."}, {"title": "Materials", "content": ""}, {"title": "Personality Situational Judgment Test (Mussel et al., 2018)", "content": "The original German version of the test contains 110 items and uses the five facets of the Big Five personality instead of the five factors. The facets involved self-consciousness (from the Big Five factor neuroticism), gregariousness (from extraversion), openness to ideas (from openness to experience), compliance (from agreeableness), and self-discipline (from conscientiousness). Each facet contains 22 items, each offering four options-two representing a higher level of a specific trait (1 point) and two representing a lower level (0 points). In this study, 22 items on self-consciousness were used. One of the items was used as a scoring example, while the remaining 21 items were randomly divided into three groups, each containing seven items. These groups were labeled as Manually-generated Group 1 (MG1), Manually-generated Group 2 (MG2), and Manually-generated Group 3 (MG3), and were used for subsequent analysis. The original German version of the test was adapted into Chinese following the back-translation procedure with the assistance of AI translation tools and researchers specializing in German."}, {"title": "Personality Situational Judgment Test Generated by GPT-4", "content": "Utilizing the GPT-based AI tool, we selected OpenAI's \u201cgpt-4-1106-preview\" model to generate PSJT items for the personality facet of self-consciousness. Different temperature settings and prompts were tested to determine the optimal temperature settings and prompt strategies, with seven items generated for each condition.\nTemperature Setting. This study tested temperature values of 0.5, 0.7, 0.9, 1.0, 1.1 and 1.5. Lee et al. (2023) used a temperature of 1.0, G\u00f6tz et al. (2024) used a temperature of 0.9, and Hommel et al. (2022) used temperatures of 0.7, 0.9 and 1.1. To evaluate the effects of extreme temperature settings on item generation, we used temperature values of 0.5 and 1.5. However, during the item generation, a temperature of 1.5 resulted in chaotic outputs, suggesting that very high-temperature settings can lead to incoherent and nonsensical results. This finding is consistent with the research of G\u00f6tz et al. (2024), which led us to discontinue the use of this parameter setting.\nPrompt Strategy. Three versions of prompt strategies were employed. Prompt v0, adapted from the strategy used by Lee et al. (2023) for generating Likert-type personality scales, uses only strategies 1, 5, and 8 from Table 1 and lacks detailed explanations of construct meanings or precise output requirements. Prompt v1 enhances Prompt v0 by incorporating strategy 2, 4, 7 and 9, guiding GPT-4 to generate more diverse and context-rich items. Prompt v2 incorporated four key improvements\u2014integrating the persona with task objectives, clarifying output requirements, removing redundant behavior descriptions, and optimizing examples\u2014building on Prompt v1 to further enhance content validity. Unless otherwise noted, this study employed Prompt v1 to generate items. The complete content of Prompts v0, v1, and v2 can be found in Appendix A.\nStability. Two sets of test items were generated using the prompt v1 and a temperature of 1.0, with a 10-day interval, to examine the stability of content validity for GPSJTs generated at different time points using identical prompts and temperature settings."}, {"title": "Procedures", "content": "The GPT-4-generated items and manually-generated items were randomized and mixed. A panel of eight experts rated the content validity using four indicators. These include the necessity of the situation, the rationality of options, the rationality of scoring and the overall evaluation of the quality of items (Christian et al., 2010; Lee et al., 2023). The instructions first clarified that the items include both manually-generated and GPT-4-generated items. Then, experts were provided a detailed definition of the self-awareness facet of personality. They were subsequently asked to evaluate each item based on four indicators.\n\u2022 The necessity of the situation: Experts are required to evaluate the extent to which an individual's behavior, as described in the scenario, reflects the level of the target personality traits. The rating scale has three levels: Necessary and useful, which indicates that the scenario is indispensable or crucial for reflecting the level of the personality traits (scored as 1); Useful but not necessary, which means the scenario is helpful but not essential (scored as 2); Neither necessary nor useful, meaning the scenario does not contribute to reflecting the level of the personality traits (scored as 3). Based on these ratings, the content validity ratio (CVR; Lawshe, 1975) was calculated using the formula CVR = $\\frac{(n \u2212 N/2)}{(N/2)}$, where N is the total number of experts and n is the number of experts who consider the scenario necessary for reflecting the level of the target personality traits. The CVR ranges from -1 to +1, with positive values indicating that more than half of the experts agree that the context of the item is essential and negative values indicating that fewer than half of the experts hold this view. Based on the criteria proposed by Lawshe (1975), a CVR of at least 0.75 is recommended when evaluated by a panel of eight experts.\n\u2022The rationality of options: Experts were required to evaluate the rationality of each option. A reasonable option should be realistic and contextually relevant, meaning the behavior described could plausibly occur in the scenario. The rating scale is as follows: 0 for no options are reasonable, 1 for one option is reasonable, 2 for two options are reasonable, 3 for three options are reasonable, and 4 for four options are reasonable. The score ranges from 0 to 4. The average of the experts' ratings was used as the score of the rationality of options.\n\u2022The rationality of scoring: Experts are required to evaluate the rationality of scoring. An accurate scoring should award 1 point for options representing a high level of the trait and 0 points for options representing a low level of the trait. The rating scale is as follows: 0 for no options are scored accurately, 1 for one option is scored accurately, 2 for two options are scored accurately, 3 for three options are scored accurately, and 4 for four options are scored accurately. The score ranges from 0 to 4. The average of the experts' ratings was used as the score of the rationality of scoring.\n\u2022Overall quality: Experts are required to evaluate whether each item is suitable for directly measuring the target personality traits based on a comprehensive evaluation of its syntactic and linguistic correctness, contextual richness, psychological fidelity, and scoring method. No is scored as 0, and yes is scored as 1. The sum of the experts' ratings was used as the score of overall quality."}, {"title": "Statistical analysis", "content": "The first analysis compared the content validity of GPT-4-generated items using Prompt v1 at different temperature settings (labeled as Temp0.5, Temp0.7, Temp0.9, Temp1.0, and Temp1.1) with items in MG2. A Kruskal-Wallis H test was conducted using SPSS 26 to assess whether there were statistically significant differences between the item groups in terms of the four indicators of content validity. If significant differences were found, Dunn's post hoc tests were performed with the Bonferroni error correction.\nThe second analysis compared the content validity of GPT-4-generated items using different prompt strategies (labeled as Prompt v0, Prompt v1, and Prompt v2) at a temperature setting of 1.0 with items in MG1 and MG3. The statistical methods and the indicators of content validity were the same as those used in the first analysis.\nThe third analysis compared the differences between items generated at two time points, which were 10 days apart, using Prompt v1 at a temperature setting of 1.0. Data for time point 1 were from the first analysis mentioned above, and data for time point 2 are from the second analysis. Using SPSS 26, a Mann-Whitney U test was performed to determine whether there were statistically significant differences between the two item groups (labeled as Timel and Time2) across the four indicators of content validity."}, {"title": "Results", "content": ""}, {"title": "Comparison of content validity across different temperature settings", "content": "The differences of content validity among Temp0.5, Temp0.7, Temp0.9, Temp1.0, Temp1.1 and MG2 were examined. According to the results of the Kruskal-Wallis H test, significant differences were found among the groups in terms of the necessity of the situation, the rationality of options, the rationality of scoring, and the overall evaluation of the quality of items. Dunn's post hoc tests with the Bonferroni correction were used to follow-up these findings. Results of the Kruskal-Wallis and Dunn tests are presented in Table 2 and Figure 1. It indicated that the MG2 items consistently scored lower in all four indicators of content validity compared to the GPT-4-generated items. In terms of the necessity of the situation, Temp1.0 and Temp1.1 received higher mean ranks, indicating that higher temperature settings generated context descriptions that were more relevant and necessary to the target traits. However, at a temperature of 1.1, notable option scoring errors occurred, such as three options receiving a score of 1 point or similar statements where one received 1 point and another 0 points. Additionally, there were issues with unclear levels of target traits and poorly constructed sentences in the options. In contrast, items generated at a temperature of 1.0 performed better overall, achieving the highest mean rank in overall item quality."}, {"title": "Comparison of content validity across different prompt strategies", "content": "The differences in content validity among Prompt v0, Prompt v1, Prompt v2, MG1 and MG3 were examined. According to the results of the Kruskal-Wallis H test, significant differences were found among the groups in terms of the necessity of the situation and the rationality of options. In contrast, no significant differences were observed in the rationality of scoring and the overall evaluation of the quality of items. Dunn's post hoc tests with the Bonferroni correction were used to follow-up these findings. Results of the Kruskal-Wallis and Dunn tests are presented in Table 3 and Figure 2. Although there were no statistically significant differences between GPT-4-generated and manually generated items, Prompt v1 and Prompt v2 achieved higher mean ranks across all four indicators compared to Prompt v0, MG1 and MG3. Notably, Prompt v2 had the highest mean rank on three of the indicators, except for the rationality of options."}, {"title": "Comparison of content validity at different times", "content": "The results of the Mann-Whitney U test, presented in Table 4 and Figure 3, indicated no significant differences between the items of Timel and Time2 across the four indicators: The necessity of the situation, the rationality of options, the rationality of scoring, and the overall evaluation of the quality of items. These findings suggest that the content validity of the items generated using the prompt v1 and a temperature of 1.0 exhibits temporal stability."}, {"title": "Discussion", "content": "Study 1 investigated the effectiveness of GPT-4 in generating PSJTs and provided evidence for the hypotheses. First, GPT-4-generated items demonstrated content validity that surpassed that of manually generated items, thereby supporting H1. Second, items generated at a temperature setting of 1.0 achieved the highest content validity, striking an optimal balance between creativity and text quality. In contrast, higher temperature settings introduced inaccuracies in option scoring, and excessively high temperatures could produce incoherent and unintelligible output. This finding confirms H2. Third, items generated using optimized prompts (Prompt v1) significantly outperformed those generated with unoptimized prompts (Prompt v0), lending support to H3. Although improvements made in Prompt v2 resulted in minor, non-significant gains, it is noteworthy that the CVR of the necessity of the situation of Prompt v2 reached 0.79, meeting the criteria for good content validity of CVR larger than 0.75 proposed by Lawshe (1975). Finally, no significant differences in content validity were found across items generated at different times, indicating the temporal stability of GPT-4-generated PSJTs and supporting H4.\nStudy 1 demonstrates that GPT-4, with optimized prompts and temperature settings, is capable of producing PSJTs with content validity comparable to, or surpassing, manually generated PSJTs. These findings validate the potential of GPT-4 as an efficient and reliable tool for AIG. In Study 2, we utilized GPT-4 to generate a comprehensive PSJT that encompasses the five dimensions of the Big Five personality traits. We then empirically validate the psychometric properties of this test, specifically focusing on its reliability and validity."}, {"title": "Study 2: Reliability and Validity of GPT-4-generated Personality Situational Judgment Tests", "content": "This study utilized GPT-4 with the validated optimal settings of Prompt v2 and a temperature of 1.0 to generate a PSJT that covers the five dimensions of the Big Five personality traits. While Study 1 relied on expert evaluations to evaluate the content validity, Study 2 collected empirical data from the target population to evaluate the psychometric properties of the GPSJT in practice."}, {"title": "Participants", "content": "A total of 468 participants were recruited, yielding 443 valid questionnaires (264 females, 59.6%). Among these, 97.5% held a bachelor's degree or higher, and their ages ranged from 18 to 58 years, with a mean age of 28.25 years (SD = 7.92).\nAmong the retained participants, 130 college students (67 females, 51.5%) also completed the criterion measures for criterion validation. Their ages ranged from 18 to 28, with a mean age of 21.12 years (SD = 1.92). After two weeks of the first testing, 80 participants (51 females, 63.8%) responded to the invitation and attended the retest for reliability evaluation."}, {"title": "Measures", "content": ""}, {"title": "Personality situational judgment test generated by GPT-4", "content": "Using OpenAI's \u201cgpt-4-1106-preview\u201d model, with optimized prompts (Prompt v2) and a temperature of 1.0, a GPSJT was generated to measure five facets of the Big Five personality traits. The five facets are self-consciousness (from neuroticism), gregariousness (from extraversion), openness to ideas (from openness to experience), compliance (from agreeableness), and self-discipline (from conscientiousness). For each facet, eight items were generated, resulting in a total of 40 items. Each item featured four response options, with two options reflecting high levels of the trait scoring 1 point each and the other two representing low levels of the trait scoring 0 points. The score for each facet was determined by calculating the sum of the items."}, {"title": "NEO-PI-R scale", "content": "The same five facets were selected from the NEO-PI-R scale (Costa & McCrae, 2008). Each facet consists of eight items, resulting in a total of 40 items. A 5-point Likert scale was utilized for scoring (1 for strongly disagree and 5 for strongly agree). The Cronbach's a coefficients of the five facets of NEO-PI-R in this study are 0.72 for self-consciousness, 0.88 for gregariousness, 0.85 for openness to ideas, 0.58 for compliance, and 0.89 for self-discipline."}, {"title": "Criterion measures", "content": "Five constructs were selected as criteria, all of which have been shown to have significant correlations with one or more dimensions of the Big Five personality traits (e.g., Jones et al., 2011; Paulhus & Williams, 2002; Strauman, 2002). The criterion measures included five items measuring subjective well-being (Diener et al., 1985), five items measuring depression (Norton, 2007), seven items measuring game addiction (Lemmens et al., 2009), four items measuring aggression (Buss and Perry, 1992), and 12 items measuring the Dark Triad personality traits (Jonason & Webster, 2010). A total of 33 items were included, all employing a 5-point Likert scale for scoring (1 for strongly disagree and 5 for strongly agree). The Cronbach's a coefficients of the criterion measures in this study are 0.84 for subjective well-being, 0.89 for depression, 0.91 for game addiction, 0.63 for aggression, 0.76 for Machiavellianism of Dark Triad personality, 0.74 for psychopathy of Dark Triad personality, and 0.85 for narcissism of Dark Triad personality."}, {"title": "Procedures", "content": "Participants were recruited, and questionnaires were distributed through social media and Credemo, a questionnaire platform. Each participant got about 2 US dollars as a reward. They completed three demographic questions, the GPSJT and NEO-PI-R items personality facets (40 items each), and three attention check items designed to detect careless responding. Additionally, 130 participants completed the criterion questionnaire (33 items in total). The inclusion criteria for valid responses included: (1) participants aged between 18 and 60 years; (2) all attention check items answered correctly; and (3) the average response time for each item exceeded 2 seconds. Out of 468 questionnaires collected, 25 participants who failed the attention check were excluded, resulting in a total of 443 valid questionnaires. Two weeks later, 80 participants were recruited from the valid participant group to complete a retest of the GPSJT."}, {"title": "Statistical analysis", "content": "Item analysis, reliability analysis (internal consistency, split-half, and test-retest reliability), and validity analysis (convergent, discriminant, and criterion-related validity) were conducted using SPSS 26. A confirmatory factor analysis (CFA) using Mplus 8.3 was conducted for both the GPSJT and the NEO-PI-R. Given that the item scores for both the GPSJT and the NEO-PI-R are ordered-categorical variables, the Robust Weighted Least Squares (WLSMV) estimation techniques was used, as recommended by Flora and Curran (2004)."}, {"title": "Results", "content": ""}]}