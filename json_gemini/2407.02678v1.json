{"title": "Reasoning in Large Language Models: A Geometric Perspective", "authors": ["Romain Cosentino", "Sarath Shekkizhar"], "abstract": "The advancement of large language models (LLMs) for real-world applications\nhinges critically on enhancing their reasoning capabilities. In this work, we explore\nthe reasoning abilities of large language models (LLMs) through their geometrical\nunderstanding. We establish a connection between the expressive power of LLMs\nand the density of their self-attention graphs. Our analysis demonstrates that the\ndensity of these graphs defines the intrinsic dimension of the inputs to the MLP\nblocks. We demonstrate through theoretical analysis and toy examples that a\nhigher intrinsic dimension implies a greater expressive capacity of the LLM. We\nfurther provide empirical evidence linking this geometric framework to recent\nadvancements in methods aimed at enhancing the reasoning capabilities of LLMs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), such as GPT-4 [1], Llama 3 [2], have achieved impressive per-\nformance on a wide range of tasks. The search for better LLMs hinges critically on the reasoning\nperformance of these models. However, it is unclear what aspects of the language models are essential\nfor achieving this goal. Today the predominant approach, considered by the community, to advance\nreasoning involves (i) increased model size (where larger models have resulted in better reasoning\ncapabilities) [3-5] and (ii) increased context length [6], more tokens or text as input to the LLM,\nthrough chain of thought [7], retrieval augmented generation [8], or prompting with examples [9].\nWhile these approaches have been sufficient, they represent only part of the potential avenues for\nimprovement. Moreover, longer inputs and bigger models correspond to increased computational\ncost and inference latency for real-world use cases. In this work, we take a principled approach to\nunderstand and elucidate the properties of LLMs that allow for improved and better reasoning. Our\nstudy leverages the geometry of the transformer layer [10], a key component in LLMs, with empirical\nevidence on simulated as well as Llama 3 family of models [2] to justify our claims.\nIn particular, we characterize key properties of the transformer layer that are correlated with its\ncapacity or expressive power. We show that the (i) density of interaction between the tokens in the\nself-attention or multi-head attention (MHA) module of a transformer exemplifies the complexity of\nfunction representation achievable by the multi-layer perceptron (MLP) layer that follows it, and (ii)\nincreased model size and context length facilitates higher attention density and consequently better\nreasoning. Our analysis presents a path toward improving reasoning and advancing LLMs while\ndeepening our understanding of the models and their behaviors. We note that our accompanying work\n[11], presented an analysis as in this work where we showed the brittleness of toxicity guardrails\nobtained via RLHF through the lens of LLM geometry.\nIn this work, we are specifically interested in understanding how the geometry of the LLM is\ncorrelated with its reasoning capabilities. Besides, we are investigating how increasing the input"}, {"title": "2 Input Space Partitioning and Expressive Power", "content": "In this section, we delve into the geometrical intuitions that underpin a fundamental aspect of Deep\nNeural Networks (DNNs): the adaptive partitioning of the DNN input space. This process leads to the\nformation of regions within the input space, each associated with an affine map that characterizes how\nthe network processes the inputs in that region. We then leverage this perspective in conjunction with\nthe multi-head attention (MHA) layer in the transformer module to develop a novel geometric view\nof LLMs. This perspective allows us to hypothesize about the role of model size and context length\nin modern LLMs and presents a path toward alternate ideas that can lead to improved reasoning\ncapabilities."}, {"title": "2.1 Deep Neural Networks", "content": "We describe the continuous piecewise affine formulation of DNNs to elucidate the concept of\ntheir induced local linear mappings. In particular, we focus on the simple case of the multilayer\nperceptron (MLP) consisting of one hidden layer, typically employed in a transformer, from a spline\ngeometric viewpoint. Subsequently, we provide an intuitive depiction through simulated experiments\nof their approximation capabilities, emphasizing the significance of the adaptive partitioning property,\nand the role of input space dimension."}, {"title": "Continuous Piece-wise Affine Formulation of DNNs", "content": "The geometric characterization of MLPs\nemploying nonlinearities, such as (leaky-)ReLU, absolute value, and max-pooling, have been exten-\nsively studied from the lens of a continuous piecewise linear operator, resulting in a partition \u03a9 of the\ninput space [12-14]. As such, a DNN defined as $f_\\theta$ with parameters \u03b8 can be re-written as\n$f_\\theta(x) = \\sum_{\\Theta \\in \\Omega} 1_{\\{\\omega\\}} (A_\\omega x + B_\\omega)$,\nwhere 1 defines the indicator function, $A_\\omega$ and $B_\\omega$ the per region affine parameters associated with\nthe DNN layer, and x the input to the network. The indicator function is data dependent and subsumes\nthe affine parameters and the nonlinearity of the region \u03c9 \u2208 \u03a9. A depiction of the regions and the\npartition induced by an MLP having a 2-dimensional input is given in Figure 1."}, {"title": "Partitioning, Number of regions, and Function approximation", "content": "The approximation capability of\na DNN for a given interval in the input space is directly proportional to the number of regions and the\nmapping associated with that input space interval. As per the continuous piece-wise affine property\nof DNNs defined in Equation 1, consider the two possible scenarios in terms of approximation: (i)\nthe target function is linear in a given interval, in which case a single region is sufficient enough"}, {"title": "2.2 Large Language Models", "content": "In this section, we interpret the architectural\ncomponents of an LLM and its variations that\ncan help improve the expressive power of the\nLLMs. Concretely, we will study the impact\nof the LLM-induced partition concerning an in-\ncrease in the number of attention heads as well\nas the context length (the sequence of tokens\npassed as input). To do so, we will exploit re-"}, {"title": "Intrinsic dimension \u00d7 Multi-Head Attention graph density", "content": "We begin by introducing notation\nthrough the definition of a transformer layer in a causal LLM, as follows\n$Head^{(l)}(X) \\stackrel{\\rm def}{=} softmax_{causal} \\left( XQ^{(l)} (XK^{(l)})^T \\right) XV^{(l)}$,\n(single-head mapping of X)\n$MHA^{(l)}(X) \\stackrel{\\rm def}{=}  \\underset{h=1}{H} Head_h^{(l)}(X) O^{(l)}$,\n(combination of H heads)\n$Layer^{(l)}(X) \\stackrel{\\rm def}{=} MLP^{(l)} \\left( LayerNorm^{(l)} \\left( MHA^{(l)}(X) + X \\right) \\right) + X$, (single layer)\n$LLM(X) \\stackrel{\\rm def}{=} \\left( Layer^{(L)} \\circ .... \\circ Layer^{(1)} \\right) (X)$,\n(compose L layers)\nwhere we denote the attention map as follows\n$Attn^{(l)}(X) \\stackrel{\\rm def}{=} softmax_{causal} \\left( XQ^{(l)} (XK^{(l)})^T \\right)$.\nIt is evident from Equation 6, that the output of an attention layer is a right stochastic matrix that\ndefines a graph where the nodes of the graph are the sequence of tokens and the edges (weights)\nare defined by the attention values. We will usually refer to density of the self-attention graph when\nexpressing the level of connectivity of the graph, i.e., the number of tokens that have an edge."}, {"title": "LLM expressive power \u00d7 intrinsic dimension", "content": "Theorem 2.1 is consequential, specifically when\nwe consider subsection 2.1, and in particular with Figure 3. We showed that: (i) the higher the\nnumber of regions, the higher the approximation capability of DNNs, and (ii) the number of regions\ncan be increased by, not only having more neurons but by increasing the ID of the MLP's input.\nWe also know from the transformer architecture described in Equation 2 through Equation 5 and\nTheorem 2.1 that the intrinsic dimension of the input to the MLP is driven by the attention maps.\nTherefore, the higher the density of the attention graph, the higher the number of regions that will be\ninduced by the MLP, and thus, the higher its expressive power.\nIt is now clear that one can increase the expressive power of an LLM by (i) increasing the number of\nheads as per the additive nature of Equation 7, (ii) performing prompt modifications as to increase\nthe density of the attention graph. Note that both these approaches have commonly been employed in\nvarious aspects in the last couple of years."}, {"title": "3 Experiment: Increasing LLM expressive power does improve its reasoning\nability", "content": "In this section, we are analyzing the capabilities of LLMs to answer reasoning questions through the\nlens of the aforementioned geometric analysis. Specifically, we are questioning how the increase\nin a number of regions induced by the MLP can lead to better reasoning capabilities. In fact, it is\nclear that approximation capabilities and generalization are not equivalent notions. However, it is\nnot yet determined that the reasoning capabilities of LLMs are tied to their generalization. While"}, {"title": "4 Related Work", "content": "The success of transformer-based models [10] across various input modalities has spurred significant\nresearch into the understanding of their internal mechanisms. Our work follows the lead of several\nkey works on this topic. The difference, however, between these previous works and ours is the\nlens of analysis: we focus, fundamentally, on an end-to-end geometric perspective rather than a\nmechanistic framework [19] or pattern analysis through empirical results [20-22]. Our work is also\ndifferent from these prior works in that we study the impact of model size and context length in\ntransformer models and their role in reasoning capabilities, a critical aspect of modern LLMs whose\nunderstanding is largely absent."}, {"title": "5 Discussion and Open Questions", "content": "We presented here some aspects of DNNs and LLMs geometry, where in particular, we show the\nimportance of the input space partitioning induced by the MLPs exploiting their piece-wise affine\nformulation. The adaptive partitioning of DNN in general plays a huge role in their approximation\ncapability. In fact, as opposed to traditional spline, the regions induced by the MLP in their input space\nare data-dependent, and henceforth determined during training. We showed how such an interplay\nbetween approximation and the number of regions impacts the ability of LLMs to approximate\nfunctions. Then, we show that, while approximation power is not equivalent to generalization, it\nseems to be highly correlated to the reasoning capabilities of LLMs. In this work, we provided a brief\noverview of the underlying theory and a limited set of experiments related to these concepts. We\nbelieve that further exploration of this phenomenon is crucial to enhancing the reasoning capabilities\nof LLMs. Our hope is that through this, smaller LLMs can soon bridge the performance gap with\ntheir larger counterparts."}]}