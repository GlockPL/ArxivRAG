{"title": "Reasoning in Large Language Models: A Geometric Perspective", "authors": ["Romain Cosentino", "Sarath Shekkizhar"], "abstract": "The advancement of large language models (LLMs) for real-world applications hinges critically on enhancing their reasoning capabilities. In this work, we explore the reasoning abilities of large language models (LLMs) through their geometrical understanding. We establish a connection between the expressive power of LLMs and the density of their self-attention graphs. Our analysis demonstrates that the density of these graphs defines the intrinsic dimension of the inputs to the MLP blocks. We demonstrate through theoretical analysis and toy examples that a higher intrinsic dimension implies a greater expressive capacity of the LLM. We further provide empirical evidence linking this geometric framework to recent advancements in methods aimed at enhancing the reasoning capabilities of LLMs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), such as GPT-4 [1], Llama 3 [2], have achieved impressive performance on a wide range of tasks. The search for better LLMs hinges critically on the reasoning performance of these models. However, it is unclear what aspects of the language models are essential for achieving this goal. Today the predominant approach, considered by the community, to advance reasoning involves (i) increased model size (where larger models have resulted in better reasoning capabilities) [3-5] and (ii) increased context length [6], more tokens or text as input to the LLM, through chain of thought [7], retrieval augmented generation [8], or prompting with examples [9].\nWhile these approaches have been sufficient, they represent only part of the potential avenues for improvement. Moreover, longer inputs and bigger models correspond to increased computational cost and inference latency for real-world use cases. In this work, we take a principled approach to understand and elucidate the properties of LLMs that allow for improved and better reasoning. Our study leverages the geometry of the transformer layer [10], a key component in LLMs, with empirical evidence on simulated as well as Llama 3 family of models [2] to justify our claims.\nIn particular, we characterize key properties of the transformer layer that are correlated with its capacity or expressive power. We show that the (i) density of interaction between the tokens in the self-attention or multi-head attention (MHA) module of a transformer exemplifies the complexity of function representation achievable by the multi-layer perceptron (MLP) layer that follows it, and (ii) increased model size and context length facilitates higher attention density and consequently better reasoning. Our analysis presents a path toward improving reasoning and advancing LLMs while deepening our understanding of the models and their behaviors. We note that our accompanying work [11], presented an analysis as in this work where we showed the brittleness of toxicity guardrails obtained via RLHF through the lens of LLM geometry.\nIn this work, we are specifically interested in understanding how the geometry of the LLM is correlated with its reasoning capabilities. Besides, we are investigating how increasing the input"}, {"title": "2 Input Space Partitioning and Expressive Power", "content": "In this section, we delve into the geometrical intuitions that underpin a fundamental aspect of Deep Neural Networks (DNNs): the adaptive partitioning of the DNN input space. This process leads to the formation of regions within the input space, each associated with an affine map that characterizes how the network processes the inputs in that region. We then leverage this perspective in conjunction with the multi-head attention (MHA) layer in the transformer module to develop a novel geometric view of LLMs. This perspective allows us to hypothesize about the role of model size and context length in modern LLMs and presents a path toward alternate ideas that can lead to improved reasoning capabilities."}, {"title": "2.1 Deep Neural Networks", "content": "We describe the continuous piecewise affine formulation of DNNs to elucidate the concept of their induced local linear mappings. In particular, we focus on the simple case of the multilayer perceptron (MLP) consisting of one hidden layer, typically employed in a transformer, from a spline geometric viewpoint. Subsequently, we provide an intuitive depiction through simulated experiments of their approximation capabilities, emphasizing the significance of the adaptive partitioning property, and the role of input space dimension.\nContinuous Piece-wise Affine Formulation of DNNs: The geometric characterization of MLPs employing nonlinearities, such as (leaky-)ReLU, absolute value, and max-pooling, have been extensively studied from the lens of a continuous piecewise linear operator, resulting in a partition \u03a9 of the input space [12-14]. As such, a DNN defined as $f_\\Theta$ with parameters \u0398 can be re-written as\n$f_\\Theta(x) = \\sum_{\\Theta \\in \\Omega} 1_{\\{\\omega\\}}(A_\\omega x + B_\\omega),$ (1)\nwhere 1 defines the indicator function, $A_\\omega$ and $B_\\omega$ the per region affine parameters associated with the DNN layer, and x the input to the network. The indicator function is data dependent and subsumes the affine parameters and the nonlinearity of the region \u03c9. A depiction of the regions and the partition induced by an MLP having a 2-dimensional input is given in Figure 1.\nPartitioning, Number of regions, and Function approximation: The approximation capability of a DNN for a given interval in the input space is directly proportional to the number of regions and the mapping associated with that input space interval. As per the continuous piece-wise affine property of DNNs defined in Equation 1, consider the two possible scenarios in terms of approximation: (i) the target function is linear in a given interval, in which case a single region is sufficient enough"}, {"title": "2.2 Large Language Models", "content": "In this section, we interpret the architectural components of an LLM and its variations that can help improve the expressive power of the LLMs. Concretely, we will study the impact of the LLM-induced partition concerning an increase in the number of attention heads as well as the context length (the sequence of tokens passed as input). To do so, we will exploit results from [11], showing that the expressive power of an LLM increases as the intrinsic dimension of the self-attention layer increases.\nIntrinsic dimension \u00d7 Multi-Head Attention graph density: We begin by introducing notation through the definition of a transformer layer in a causal LLM, as follows\n$Head^{(l)}(X) = softmax_{causal} (XQ^{(l)} (XK^{(l)})^T) XV^{(l)},$ (single-head mapping of X) (2)\n$MHA^{(l)}(X) = \\underset{h=1}{H}Head_h^{(l)}(X)O^{(l)},$ (combination of H heads) (3)\n$Layer^{(l)}(X) \\equiv MLP^{(l)}(LayerNorm^{(l)}(MHA^{(l)}(X) + X)) + X,$ (single layer) (4)\n$LLM(X) \\equiv (Layer^{(L)} \\circ .... Layer^{(1)})(X),$ (compose L layers) (5)\nwhere we denote the attention map as follows\n$Attn^{(l)}(X) = softmax_{causal} (XQ^{(l)} (XK^{(l)})^T).$ (6)\nIt is evident from Equation 6, that the output of an attention layer is a right stochastic matrix that defines a graph where the nodes of the graph are the sequence of tokens and the edges (weights) are defined by the attention values. We will usually refer to density of the self-attention graph when expressing the level of connectivity of the graph, i.e., the number of tokens that have an edge."}, {"title": "3 Experiment: Increasing LLM expressive power does improve its reasoning ability", "content": "In this section, we are analyzing the capabilities of LLMs to answer reasoning questions through the lens of the aforementioned geometric analysis. Specifically, we are questioning how the increase in a number of regions induced by the MLP can lead to better reasoning capabilities. In fact, it is clear that approximation capabilities and generalization are not equivalent notions. However, it is not yet determined that the reasoning capabilities of LLMs are tied to their generalization. While"}, {"title": "5 Discussion and Open Questions", "content": "We presented here some aspects of DNNs and LLMs geometry, where in particular, we show the importance of the input space partitioning induced by the MLPs exploiting their piece-wise affine formulation. The adaptive partitioning of DNN in general plays a huge role in their approximation capability. In fact, as opposed to traditional spline, the regions induced by the MLP in their input space are data-dependent, and henceforth determined during training. We showed how such an interplay between approximation and the number of regions impacts the ability of LLMs to approximate functions. Then, we show that, while approximation power is not equivalent to generalization, it seems to be highly correlated to the reasoning capabilities of LLMs. In this work, we provided a brief overview of the underlying theory and a limited set of experiments related to these concepts. We believe that further exploration of this phenomenon is crucial to enhancing the reasoning capabilities of LLMs. Our hope is that through this, smaller LLMs can soon bridge the performance gap with their larger counterparts."}]}