{"title": "Hyperbolic Brain Representations", "authors": ["Alexander Joseph", "Nathan Francis", "Meijke Balay"], "abstract": "Artificial neural networks (ANN) were inspired by the architecture and functions of the human brain and have revolutionised the field of artificial intelligence (AI). Inspired by studies on the latent geometry of the brain we posit that an increase in the research and application of hyperbolic geometry in machine learning will lead to increased accuracy, improved feature space representations and more efficient models across a range of tasks. We look at the structure and functions of the human brain, highlighting the alignment between the brain's hierarchical nature and hyperbolic geometry. By examining the brain's complex network of neuron connections and its cognitive processes, we illustrate how hyperbolic geometry plays a pivotal role in human intelligence. Empirical evidence indicates that hyperbolic neural networks outperform Euclidean models for tasks including natural language processing, computer vision and complex network analysis, requiring fewer parameters and exhibiting better generalisation. Despite its nascent adoption, hyperbolic geometry holds promise for improving machine learning models and advancing the field toward AGI.", "sections": [{"title": "Introduction", "content": "Artificial neural networks (ANN), and more specifically, Deep Learning have led to great advancements in the field of AI in the last decade. The architecture of ANN with connected neurons that fire based on activation mechanisms was inspired by the brain's architecture, albeit with simplifications. This methodology has vastly outperformed other AI approaches on a wide range of tasks and has led to deep learning systems powering many of the products we use in day-to-day life.\n\nThe vast majority of ANN use Euclidean geometry as their latent geometry, with research on the use of other geometries being relatively niche, despite the latent geometry of various brain functions not being Euclidean.\n\nHyperbolic geometry, which is a non-Euclidean geometry with negative curvature, is often called the geometry of complex networks and has shown remarkable improvements compared to other geometries in modelling complex networks and hierarchical structures. A complex system is a system composed of many interacting parts which display collective behaviour that does not follow trivially from the behaviours of the individual parts. They commonly have a hierarchical modularity and can often be represented as a complex network. An example of a hierarchical complex system is the human brain. Recently, there has been an increase in research that has shown various brain functions are best described using hyperbolic geometry. Inspired by this connection, we investigate whether hyperbolic geometry can improve the performance of ANN.\n\nMachine learning models which use hyperbolic geometry often demonstrate significant performance improvements over their Euclidean counterparts, particularly when applied to hierarchical data. Notably, these hyperbolic models frequently achieve comparable or superior results with fewer parameters than traditional Euclidean networks. This efficiency in parameter use, combined with enhanced performance, makes hyperbolic geometry an attractive approach in AI, especially for tasks involving data with inherent hierarchical structures. However, there are barriers preventing researchers and data scientists from applying hyperbolic geometry in AI, such as numerical instability. We believe that research into hyperbolic geometry in AI should be encouraged, continuing to draw inspiration from human intelligence.\n\nThis paper is structured to provide a comprehensive"}, {"title": "Hyperbolic Geometry", "content": "Geometry is one of the oldest branches of mathematics and it is concerned with the shape of individual objects, spatial relationships among various objects, and the properties of surrounding space.\n\nIn geometry, there are three types of spaces with constant curvature: spherical (positive curvature), Euclidean (zero curvature), and hyperbolic (negative curvature) (see Table 1). Euclidean geometry has been the dominant geometry in machine learning, mainly because its vector-space structure allows the application of powerful techniques of computational linear algebra. Another factor contributing to the widespread use of Euclidean geometry in machine learning may be familiarity bias, as curved geometries are less intuitive and harder to visualise.\n\nThere are several models of hyperbolic geometry, each differing in their representation of lines (geodesics) and points of hyperbolic space. Common models are the Poincar\u00e9 model, the Lorentz model (also known as hyperboloid), the Klein model and the upper half-space model. In machine learning these models are applied based on their ease of implementation and the performance on a given task.\n\nTwo significant properties of hyperbolic geometry are its exponential expansion and its tree-like structure. This tree-like structure allows it to represent hierarchical structures with low distortion and also model complex networks very effectively, as complex networks often contain hierarchies. \n\nThe implication for machine learning \u2013 specifically representation learning \u2013 is that hyperbolic space naturally represents data with large-scale hierarchical structures. Hierarchical structures occur in numerous datasets, including those representing words, social networks, and biological neural networks. In particular, biological data is often represented using dendrograms or hierarchical tree structures. This widespread occurrence of hierarchical organisation in varied data types underscores the importance of methods capable of effectively handling such structures."}, {"title": "Hyperbolic geometry and the brain", "content": "Hyperbolic geometry has an important role in the physical architecture of the brain and the workings of various cognitive functions, both of which are integral to intelligence. This section first looks at the physical architecture of the brain, including the wiring of neurons and their interaction with each other. Secondly, we cover the latent geometry of cognition at both a macro and micro scale."}, {"title": "Physical brain structure", "content": "The physical architecture of the brain, which includes connections between neurons, affects information propagation through the brain, as well as the process of cognition.\n\nThe brain is a complex network and displays traits typical of complex networks, such as being organised hierarchically. Hyperbolic geometry models these complex networks most closely which suggests the brain may be best described in hyperbolic space. The maps of connections between neurons provide us with significant insight into the brain's structure. To our knowledge, any attempts to show that the node connectivity of the human brain is Euclidean have failed . When functions and structures of the brain are modelled using hyperbolic geometry instead of Euclidean geometry, this process has shown vast improvements, as shown in . This is consistent with the characterisation of hyperbolic geometry as the \u201ceffective geometry of complex networks\u201d.\n\nThe idea that complex systems have a hierarchical modular organisation originated in the early 1960s and results show that human brain functional networks have a hierarchical modular organisation . Brain regions tend to coordinate by forming a highly hierarchical"}, {"title": "Cognition", "content": "Cognition can be defined as the states and processes involved in knowing, which in their completeness include perception and judgement. Cognition includes all conscious and unconscious processes by which knowledge is accumulated, such as perceiving, recognising, conceiving, and reasoning . As the brain has a physically hierarchical structure, various cognitive functions of the brain may also be more accurately described in hyperbolic space as they rely on the interactions between neurons within its structure.\n\nThis section will consider cognition from a comprehensive perspective presenting the literature that can provide insight into cognition at the macro scale of the brain. We will then consider major cognitive processes such as smell, vision, and language."}, {"title": "Macroscopic Cognition", "content": "Mental states emerge from the interaction between physical and functional levels in the human brain. The human mind is what thinks, feels, perceives, imagines, remembers, and wills, encompassing our understanding of mental phenomena. It is a complex phenomenon built on the physical scaffolding of the brain.\n\nrevealed a hierarchical structure of cognitive processes in the brain (see Figure 2). Their research demonstrates that cognition operates through a series of levels. At the foundation lie primary sensory relay and processing functions. Moving upward, these sensory inputs are progressively integrated and abstracted. At the highest levels, we find complex cognitive functions such as reasoning and language. This hierarchical organisation suggests that higher-level cognitive processes emerge from the aggregation and abstraction of lower-level sensory information. Functions like language and reasoning reflect the brain's ability to construct complex understanding from basic inputs."}, {"title": "Smell", "content": "In a study of human smell perception , it was demonstrated that smell perception is best modelled with hyperbolic geometry. The study showed that both natural odours and human perceptual descriptions of smells are best described using a three-dimensional hyperbolic space. This match in geometries can avoid distortions that would otherwise arise when using geometries ill-suited to mapping odours to perception. These findings suggest that the brain groups odours according to how often they occur together, rather than according to their molecular makeup. When a map of these odour clusters was made, it was found that the distance between similarly structured molecules was best represented using ideas of distance from hyperbolic geometry, rather than Euclidean. The study also noted that hyperbolic perceptual organisation is likely to be general across different sensory modalities. This relationship between smell perception and hyperbolic geometry stems from the structure of odour production in the natural environment. Odour components are generated by hierarchical biochemical networks, which can be effectively represented as tree-like structures. These tree structures, in turn, are well-approximated by hyperbolic spaces."}, {"title": "Spatial Representation", "content": "Research into the brain's spatial encoding and navigation capabilities has been an active area of research for decades.\n\nSpatial representation, a fundamental cognitive function, refers to the brain's ability to construct and utilise internal maps of the physical world. Research has focused on the CA1 region of the hippocampus, particularly in rats, due to its critical role in spatial memory and navigation. The similarity between a rat and a human brain has been evidenced by which suggests that the knowledge generated from decades of rodent research is relevant to human neurophysiology. Building on this foundation, recent research by has unveiled new dimensions of complexity in spatial representation. It was discovered that neurons in the CA1 region of the rat hippocampus encode space through a non-linear hyperbolic geometry. This dynamic hyperbolic mapping allows neural circuits to efficiently adapt to changes in the environment and the rat's familiarity with it, offering a sophisticated mechanism for spatial perception.\n\nThe adoption of hyperbolic geometry in spatial navigation offers advantages that enhance both the efficiency and flexibility of neural computations. One of these advantages is its facilitation of hierarchical planning. This hierarchical organisation in neural networks enables a maximally informative representation of input signals allowing the brain to process and prioritise information in a way that supports efficient decision-making and problem-solving. Additionally, hyperbolic geometry supports efficient routing of signals, particularly in scenarios where network connections may change dynamically. This adaptability is crucial for navigating complex and ever-changing environments.\n\nThe dorsal CA1 region of the hippocampus is critical for spatial representation and analysis by revealed that the size of the hyperbolic representation in this brain region increases as an animal becomes more familiar with its environment. This finding suggests that the brain's spatial encoding mechanisms are not static, but instead dynamically expand with experience, thus enhancing the brain's ability to navigate and understand increasingly complex spaces."}, {"title": "Vision", "content": "The human visual system is a sophisticated network that allows for the perception and interpretation of the surrounding environment. At its core is binocular vision, which utilises the input from both eyes, enabling depth perception and a comprehensive field of view. Binocular vision depends on several factors, such as the motor system that coordinates eye movements . Its primary advantage is to help provide a sense of distance and relative depth as we move around in our environment. As early as 1947, Luneburg developed a mathematical theory of binocular vision, concluding that the space of binocular vision has constant negative curvature, favoring hyperbolic geometry over Euclidean or elliptic geometries . While his findings have since been contested, the visual space has been shown to have a hyperbolic geometry at least in part.\n\nStudies have evidenced hyperbolic geometry and hierarchical structures in vision and visual perception. In researchers posited a hyperbolic geometry framework for human vision, which demonstrated several important geometric features of ocular structures and visual functions. Using this framework the distribution of cone cells in the retina, responsible for color vision, was modeled in hyperbolic space using a method that aligns well with microscopic studies of the retinal structure .\n\nThe visual cortex plays a critical role in visual processes. Located in the occipital lobe, the visual cortex is the primary cortical region responsible for receiving, integrating, and processing visual information from the retinas. It is divided into five distinct areas (V1 to V5), each with specialized functions and structures. The primary visual area (V1) is the first stage of cortical processing and contains a complete map of the visual field as seen by the eyes . determined that the shape of area V1 is hyperbolic, describing it as a hyperbolic planform. They determined this by predicting brain activity patterns and designing experiments to test these hypotheses.\n\nGiven that hierarchies are best expressed in hyperbolic geometry, the link between hyperbolic geometry and the human visual system is further reinforced by the prevalence of hierarchical structures in visual motion perception."}, {"title": "Language", "content": "Language serves as a cornerstone for human cognition and evidence that suggests that some elements of human language are best modelled using hyperbolic geometry. For example, the semantic relations between words in the hierarchical WordNet dataset were shown to be best modelled using hyperbolic geometry, and research in the last two decades has suggested that hierarchical phrase structures characterise language. This is seen in the English language where noun and verb phrases are arranged within a clause in a hierarchical way rather than a linear order of words.\n\nPower laws, which are a functional relationship between two quantities, where a relative change in one quantity results in a relative change in the other quantity proportional to a power of the change, have been shown to appear in human language, and hyperbolic geometry can represent power laws well. An example of a power law in language is Zipf's law, which states that the frequency at which a word appears in a given corpus is inversely proportional to its rank. Zipf's law can be represented as a hierarchy with a cascade structure and has been noted as a signature of hyperbolic geometry .\n\nOther cognitive functions Studies have validated the claim that the brain also utilises hierarchical representations in other areas. Audio perception involves a complex, multi-stage process organised hierarchically in the human auditory cortex . This hierarchical structure is crucial for understanding how the brain processes and interprets sounds, ranging from simple tones to complex speech. There is further evidence in speech perception where the human brain continuously predicts a hierarchy of representations across multiple timescales ."}, {"title": "Hyperbolic Machine Learning", "content": "Research has demonstrated that hyperbolic geometry provides the most accurate model for representing the structural organisation of the brain and various cognitive functions, especially those involving hierarchical processes. As the field of AI has taken large inspiration from the human brain, we could use hyperbolic geometry as further inspiration. The use of hyperbolic geometry in machine learning and computer science, while limited, has been slowly growing in recent years. Hyperbolic machine learning models have demonstrated several advantages over their Euclidean counterparts: they require fewer parameters, can model tree-like structures without distortion, and more effectively fit complex, hierarchical data. The reduced number of parameters leads to shorter training times, providing an additional incentive for further research in this area. Moreover, deep learning models can struggle with capturing context and generalising to unseen data. Hyperbolic geometry, by better modelling the hierarchical and modular nature of many problems, may offer improved generalisation capabilities. This potential for enhanced performance and efficiency makes hyperbolic neural networks an increasingly important area of study in AI research.\n\nThe rest of this section will cover the existing literature in the field, the available tools practitioners can make use of and the issues practitioners currently face."}, {"title": "Existing research", "content": "Research showcasing the application of hyperbolic geometry on a range of tasks including computer vision, NLP, graph learning textual embeddings and symbolic learning have been undertaken in the field. These applications are particularly relevant for problems involving complex systems, hierarchical data structures, and cognitive functions typically associated with human intelligence.\n\nIn , image embeddings made using the Poincar\u00e9 model of hyperbolic geometry have been shown to improve the performance of image classification, one-shot, and few-shot learning, as well as person re-identification, when compared to embeddings using Euclidean geometry. The effectiveness of hyperbolic geometry in this context is primarily attributed to its capacity to capture and represent the inherent hierarchical structures present in images. This hierarchical representation is advantageous when developing machine learning models for image analysis and understanding. The inspiration for the work came from the hierarchical relations that exist between images that are common in computer vision tasks. To build hyperbolic image embeddings used a neural network and changed the operations in the model to use operations in hyperbolic space. They also introduced an approach to evaluate the hyperbolicity of a dataset based on the concept of Gromov-hyperbolicity; this approach is useful for determining if hyperbolic geometry is appropriate for a dataset. Other recent approaches to finding whether hierarchies exist in a dataset include which captures implicit hierarchical structures in data using hyperbolic embeddings and cone constraints.\n\nThe use of a hyperbolic distance metric in computer vision tasks is effective in Open World Object Detection (OWOD). OWOD is the task of detecting both known and unknown objects in a scene while integrating learned knowledge for future tasks. In a novel method for OWOD, used a hierarchical approach to detect unknown objects in a scene. The hyperbolic distance metric was used because of its effectiveness in modelling hierarchies and this method showed up to a 6% improvement compared to existing methods.\n\nThe ability to learn effective representations of symbolic data, including text, graphs, and multi-relational information, has become increasingly crucial in machine learning. These learned representations serve as foundational elements for various downstream tasks. Symbolic datasets that contain complex symbolic structures frequently exhibit latent hierarchies.\n\ndemonstrated the efficacy of hyperbolic embeddings for symbolic data, utilising the Poincar\u00e9 model to achieve greater performance compared to its Euclidean counterparts. Their approach employed Riemannian optimization techniques, notably Riemannian Stochastic Gradient Descent (RSGD), to minimise the distance between symbolic representations. The experiments primarily made use of WordNet noun hierarchies, employing the embeddings for link prediction and reconstruction tasks. Poincar\u00e9 embeddings demonstrated superior performance over their Euclidean counterparts, achieving comparable results with significantly fewer dimensions. Similar substantial improvements in performance were observed when applying these techniques to social network datasets for reconstruction and link prediction tasks. extended their previous work by using the Lorentz model. This approach led to an improvement on their earlier hyperbolic WordNet embeddings, which served to further demonstrate that hyperbolic embeddings outperform Euclidean embeddings. They also applied this approach to embedding cognates to find relationships between languages in modelling historical linguistics, and the hyperbolic embeddings outperformed the Euclidean counterpart once again.\n\nLarge Language Models (LLMs) have been at the forefront of a renewed interest in AI. Early research has looked at the effectiveness of LLMs which use hyperbolic geometry as their latent geometry. propose that pre-trained language models (PLMs) should operate entirely within hyperbolic spaces, as structured features within language are better encoded in hyperbolic than Euclidean spaces. Experiments demonstrated the superiority of hyperbolic PLMs over their Euclidean counterparts across a wide variety of tasks, indicating that the geometry of model representation is crucial for enhancement. introduced Hierarchy Transformer encoders, which re-train transformer encoder-based language models to interpret and encode hierarchies using two innovations; hyperbolic clustering, and centripetal losses. Hyperbolic Centripetal Loss ensures parent entities are positioned closer to the Poincar\u00e9 ball's origin than their child counterparts. Hyperbolic Clustering Loss aims at clustering related entities and distancing unrelated ones. This approach led to significant improvements for Multi-hop Inference and Mixed-hop Prediction tasks on WordNet compared to Euclidean counterparts.\n\nperformed molecular property prediction using a Hyperbolic Graph Convolutional Neural Network (HGCN) using the ZINC dataset. The model was developed by replacing Euclidean operations with hyperbolic ones, utilising an exponential map and incorporating both Lorentz and Poincar\u00e9 models. Experimental results showed that these hyperbolic approaches consistently outperformed Euclidean graph convolutional networks, with the Lorentz model outperforming Poincar\u00e9's across all experiments. Notably, these hyperbolic models also surpassed existing state-of-the-art deep learning molecular property prediction models developed by the wider community. The HGCN was further applied to a large-scale complex graph problem: predicting price fluctuations for the underlying asset of the Ethereum blockchain, where nodes represent addresses in the blockchain. This application showcased one of the key benefits of hyperbolic representations \u2014 the ability to inspect the hierarchy learned by the network. Consistent with previous findings, the Lorentz model significantly outperformed the Poincar\u00e9 model, while Euclidean representations yielded the poorest results. This pattern held across various experimental settings, reinforcing the advantages of hyperbolic geometry in capturing complex hierarchical structures in data.\n\nGiven the growing interest and complexity in hyperbolic machine learning, numerous comprehensive surveys have been published, offering broader perspectives on the application of hyperbolic geometry in machine learning. These reviews, including works by , provide valuable insights into the current state and future directions of this area of research."}, {"title": "Limitations and existing tools", "content": "Currently, there are four models used to perform calculations in hyperbolic geometry, but their optimal applications in machine learning have not been thoroughly studied. Most research tends to default to the Poincar\u00e9 model. Several challenges persist, including mathematical precision errors, the lack of linear algebra in hyperbolic geometry, and the absence of a natural mean. These issues complicate both theoretical work and practical implementation, contributing to the problem of vanishing and exploding gradients during neural network training.\n\nTo effectively build models that leverage hyperbolic geometry at scale, tools are needed to abstract complex mathematical computations for machine learning practitioners. Several such tools exist, varying in maturity: Hyperlib, geoopt, manifold.js , and HypLL. These libraries enable the implementation of machine learning components in hyperbolic space, and their continued development and use are encouraged."}, {"title": "Future Work", "content": "There are several avenues for future research exploring the theory and application of hyperbolic geometry. Important areas which aim to bridge gaps in current knowledge, address existing challenges, and leverage the properties of hyperbolic geometry, are detailed in this section.\n\nChoice of hyperbolic model: Four models of hyperbolic geometry are commonly used: including Poincar\u00e9, Lorentz, Klein and upper half-space model. However, a clear understanding on when each model is appropriate has not been developed. Future research should evaluate the strengths and weaknesses of each model, their use in machine learning problems and the problem cases they are best suited to.\n\nMaturation of tools: Currently, a few tools and libraries are available that allow users to create and use hyperbolic machine learning models, such as , geoopt , manifold.js , and HypLL. Most of these libraries implement multiple hyperbolic models, Riemannian gradient descent methods and the basic building blocks of a neural network such as linear layers and activation functions. However more complex components such as hyperbolic attention mechanisms are often found in repositories used by researchers that are not easy to disentangle from the rest of the code. The implementation of these components, as well as the implementation of popular and proven hyperbolic neural networks will help these libraries become more practical for practitioners.\n\nPrecision Errors: Mathematical precision errors can occur when implementing hyperbolic neural networks. This issue contributes to unstable training, the occurrence of NaN values in the network, vanishing gradients and exploding gradients. To mitigate this issue implementations of networks often have to clip values to prevent large values or use numerical types with a larger capacity. Further research on how to maintain mathematical precision is important so that future research can focus on the application of hyperbolic geometry rather than fixing mathematical issues in models.\n\nHyperbolic LLMs: The gains in LLM performance have primarily been driven by scaling up parameters and data, leading to significant improvements in performance. A promising new direction for enhancing LLMs without further scaling is to adopt a hyperbolic feature space. By leveraging hyperbolic spaces, these models could offer more efficient and powerful representations of linguistic structures such as syntactic trees or semantic hierarchies, leading to improved natural language understanding and generation. Future research should explore optimising LLMs within hyperbolic spaces, which could pave the way for more efficient, scalable, and robust language models that maintain high performance without excessive computational resources.\n\nHuman-Centric Tasks and AI Alignment: The alignment between hyperbolic geometry and the latent geometry of the brain makes it a good choice for tasks related to human cognition and perception, including but not limited to modelling language, and predicting visual and sensory preferences. The use of hyperbolic geometry in modelling the brain is established in neuroscience. Still, in the field of Al this could increase, particularly given the prominence of tasks in AI that relate to the human brain. Moreover, designing ANNs that further reflect the brain's structure and functions could make them more aligned with other aspects of human intelligence.\n\nModelling human language and understanding sensory perceptions-such as determining which smells people might find appealing-involves modelling how the brain represents and processes information. Performance in such tasks could improve by more closely mirroring the hyperbolic cognitive representations in the brain.\n\nWorks such as have developed quantitative measures that score the alignment between deep natural network representation spaces and those in the brain for object recognition. This work could be extended beyond object recognition and applied to other human-centric tasks.\n\nNeuromorphic Machine Learning: Neuromorphic computers are non-von Neumann computers whose structure and function are inspired by the brain and are composed of neurons and synapses . In a neuromorphic computer both processing and memory are governed by the neurons and the synapses. On the other hand, Von Neumann computers are composed of separate CPUs and memory units, where data and instructions are stored in the latter.\n\nThe application of machine learning on neuromorphic computers is still in its infancy, as is the development of neuromorphic computers themselves, however, due to the architecture of these types of computers they provide a good fit for machine learning tasks. As neuromorphic computers inherently implement neural network-style computation, neuromorphic computers are a natural platform for many of today's artificial intelligence and machine learning applications; with various approaches to implementing neural networks already shown to be successful. Implementations typically use the neurons in the computer's hardware as the neurons in the artificial neural network. Hyperbolic geometry could be applied to machine learning models trained on neuromorphic computers. In the context of hyperbolic geometry, neuromorphic computers and the AI algorithms run on them are complex networks inspired by the brain's network, and complex networks can be assumed to have a hyperbolic geometry. This similarity offers a compelling case for utilising hyperbolic geometry in neuromorphic computing."}, {"title": "Conclusion", "content": "With their architectures and functionalities initially inspired by the brain's neuronal networks and activation mechanisms, artificial neural networks and deep learning have profoundly transformed the landscape of artificial intelligence. Despite this biological inspiration, the latent geometry of neural networks seems to contrast that of the brain. The human brain's architecture shows both hierarchical organisation and complex network properties, characteristics that are more accurately represented with hyperbolic geometry. Various cognitive processes also exhibit patterns suggesting they may operate within a hyperbolic space. This observation is what prompted us to investigate the use of hyperbolic geometry in Al systems, particularly in tasks involving hierarchical data."}]}