{"title": "BIOMETRICS IN EXTENDED REALITY: A REVIEW", "authors": ["Ayush Agarwal", "Sushma Venkatesh", "Raghavendra Ramachandra", "S. R. Mahadeva Prasanna"], "abstract": "In the domain of Extended Reality (XR), particularly Virtual Reality (VR), extensive research has been devoted to harnessing this transformative technology in various real-world applications. However, a critical challenge that must be addressed before unleashing the full potential of XR in practical scenarios is to ensure robust security and safeguard user privacy. This paper presents a systematic survey of the utility of biometric characteristics applied in the XR environment. To this end, we present a comprehensive overview of the different types of biometric modalities used for authentication and representation of users in a virtual environment. We discuss different biometric vulnerability gateways in general XR systems for the first time in the literature along with taxonomy. A comprehensive discussion on generating and authenticating biometric-based photorealistic avatars in XR environments is presented with a stringent taxonomy. We also discuss the availability of different datasets that are widely employed in evaluating biometric authentication in XR environments together with performance evaluation metrics. Finally, we discuss the open challenges and potential future work that need to be addressed in the field of biometrics in XR.", "sections": [{"title": "1 Introduction", "content": "Extended Reality (XR) is an umbrella term that encompasses various immersive technologies, including Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR). XR combines real and virtual environments to create a blended experience that allows users to interact with digital content and the physical world. VR completely immerses users in a simulated environment, AR overlays digital content onto the real world, and MR blends digital and real-world elements to create interactive and dynamic experiences. The market value of XR has increased rapidly in recent years from USD 6.1 billion in 2016 to USD 42.83 billion in 2022. It is expected to grow further, reaching USD 345.09 billion by 2030\u00b9. This growth is driven by the increasing adoption of XR in various sectors as well as advancements in XR technology and hardware.\nThe evolution of AI has the potential to significantly enhance the usability of extended reality (XR) by making interac-tions within virtual environments more seamless and accessible. Through AI-powered natural language processing, users can rely on improved voice commands and real-time translations, enabling intuitive communication across"}, {"title": "2 Security in XR systems", "content": "In this section, we discuss the security aspects of XR systems, focusing on the different vulnerability points that can allow attackers to perform various types of attacks on XR systems. First, we present an overview of the XR system with the main working blocks and discuss different types of authentication mechanisms. We then discuss different types of attacks that can be performed on the different vulnerable parts of XR systems."}, {"title": "2.1 XR System", "content": "Extended reality (XR) systems are composed of four key components that collaborate harmoniously to craft an immersive experience for the user, as depicted in Figure 4. These components encompass the input processor, simulation processor, rendering processor, and the XR environment. The input processor captures and processes the user's movements and interactions within the XR environment, thereby enabling seamless interaction with the extended world in an intuitive manner. The simulation processor is responsible for fabricating an extended reality environment and dictating its responses to the user input. This entails simulating the orientation and locomotion of extended objects, along with other tangible phenomena, such as physics and illumination. This helps create a real and believable XR world that responds accurately to what users do. The simulation processor ensures that the XR environment behaves consistently and predictably. This provided users with a smooth and exciting experience.\nThe rendering processor creates sights and sounds that make up what a person experiences through their senses, such as seeing things in 3D, hearing sounds, and feeling touch-like sensations. This is important because it helps make an experience feel real, as if a person is actually inside a different world. The rendering processor works closely with another part called the simulation processor, which replicates the live experience. This makes the entire experience feel accurate and responsive to what a person is doing.\nThe XR environment is like a special place where all instructions (scripts) and things (entities) exist in the extended world. It also includes rules that control how these things operate. This special place defines how the extended world looks and how things act in it. It also demonstrates how different parts of the extended world work together. The XR environment is important because it helps to build a connected and believable extended world that makes sense and feels real to the person using it. In summary, the extended reality system has four main parts that work together smoothly to provide the user with a continuous and deeply engaging experience. It does this by taking into account what the user does, creating very realistic computer-made scenarios, showing things in both sight and sound, and setting up a consistent alternate world. This system is good at taking the user to another world where they can interact in a natural and easy manner."}, {"title": "2.2 Various modes of authentication in XR", "content": "Various modes of authentication have been developed over the years to protect XR users from attacks. These authentica-tions are broadly classified as knowledge-based, gesture-based, gaze/task-based, or rhythm-based. \n\u2022 Knowledge-based authentication refers to the traditional method of authentication where users have to enter a personal identification number (PIN) or password to unlock a device or access a system . Researchers have explored various transformations and innovations to enhance the security of this type of authentication. One approach is the use of shuffled virtual keyboards, which randomize the layout of the keys to prevent shoulder surfing attacks and password guessing. Rajarajan et al. proposed a shoulder-surfing-resistant authentication system that employs a shuffled virtual keyboard, making it challenging for an attacker to observe the user's keystrokes . Similarly, Holland and Morelli introduced a dynamic virtual keyboard that continuously changed its layout, further enhancing security against shoulder surfing attacks. Another interesting transformation is RubikAuth, proposed by Mathis et al. which integrates the concept of a Rubik's cube into the authentication process. Users manipulate the virtual Rubik's Cube to generate a password by adding an additional layer of complexity to the authentication process. RubikBiom, also proposed by Mathis et al., combines knowledge-based authentication with biometrics by using the orientation of the Rubik's Cube as a biometric feature [Mathis et al. [2020b]]. The fusion of knowledge-based and biometric authentication aims to enhance both security and usability.\n\u2022 Gesture-based authentication refers to a method of user authentication that utilizes specific hand or body movements to verify the identity of an individual. Instead of relying solely on traditional methods, such as passwords or PINs, gesture-based authentication leverages the unique patterns and characteristics of user gestures to grant access to systems or devices. However, such actions can be cumbersome and awkward, particularly when performed in public places. Future research on gesture-based biometric authentication should focus on developing techniques that capture distinctive biometric features in a non-intrusive and natural manner. This ensures that the authentication process is user-friendly and does not draw unnecessary attention, thereby promoting both usability and security in various contexts.\n\u2022 Task-based authentication is a novel approach that leverages the innate consistencies in human performance of everyday tasks to establish a unique signature for individuals in extended reality (XR) environments. Humans develop a certain level of proficiency and consistency in performing common tasks owing to their real-world experience. Activities such as throwing a ball, lifting a chair, swinging a golf club, or driving a car involve specific patterns of movement that are unique to each individual. In XR environments, task-based movements can be captured and analyzed to create a personalized authentication system. By recording and analyzing the performance of these tasks, a unique signature was established based on individual movement patterns, timing, and coordination. However, challenges remain in the implementation of task-based authentication systems. This requires the accurate tracking of user movements using sensors or cameras within the XR environment. The system must also be able to distinguish between genuine users and imposters attempting to mimic the task movements.\n\u2022 Rhythm-based authentication is a unique approach where users respond to a predefined rhythm or pattern during the enrollment process. This authentication method relies on the ability of the user to accurately reproduce rhythm to verify their identity. The response can be based on various actions such as tapping a finger, blinking an eye, or any other predefined gesture. During enrollment, users were prompted to synchronize their actions with a specific rhythm or sequence. The system captures and records the user response as a unique rhythm-based signature. When authentication is required, the user is asked to reproduce the same rhythm or pattern with which they initially enrolled. The system compares the user's response to the enrolled rhythm and determines whether it matches, grants, or denies access accordingly.\n\u2022 Biometric: Biometric characteristics are widely employed in the XR for authentication that includes:\nIris: The use of various functionalities of the iris as the biometric to detect the authentication of the user. The advantage of Iris-based biometrics is that they are not prone to shoulder surfing attacks. Zhu et al. proposed a two-factor authenticator called BlinKey. The first factor is blinking of the eye in a certain rhythm. The second factor is the variation in the pupil size at each blink. These blinks and size variations were captured by the eye tracker. BlinKey-based authentication relies on a blinking pattern, so an HMD-based solution that captures the iris and a partial periocular region is used for biometric authentication . Features for iris extraction for this purpose include ordinal measures (OM) [Sun and Tan [2008]] and Discrete Fourier Transforms (DFT) [Miyazawa et al. [2008]]. Ordinal Measures focus on the relative ordering of pixel intensities, making them robust to lighting variations, noise, and contrast changes in iris recognition. OM encodes these relationships into ordinal codes for feature extraction. DFT capture the frequency components of the iris texture, highlighting periodic patterns that aid in distinguishing irises despite variations in scale and orientation. In addition to this Convolution Neural Network (CNN) based deep learning feature extraction methods like DeepIrisNet [Gangwar and Joshi [2016]] and MobileNetV3 [Howard et al. [2019]] is utilized in [Boutros et al. [2020b]] for highly accurate iris recognition.\nBrain signal-based biometrics, specifically the use of electroencephalogram (EEG) signals, has gained interest as a non-intrusive input modality, particularly in conjunction with wearable headsets like virtual reality (VR) devices . EEG signals were collected using electrodes placed on the scalp to measure the electrical activity of the brain. One challenge in utilizing EEG data for Brain-Computer Interface (BCI) algorithms is the poor generalization performance across users. EEG signals exhibit inter-user differences owing to variations in the brain structure and activity patterns. However, these differences can be leveraged for user authentication. By studying and understanding the inter-user differences in EEG signals, researchers aim to develop authentication systems based on these unique brain activity patterns. Each individual's brain signal can serve as a distinctive biometric . This approach offers a non-intrusive and potentially more secure authentication method. In EEG signal analysis, feature extraction methods like the Wavelet Packet Transform (WPT) [Bong et al. [2017]] and Autoregressive (AR) models [Makhoul [1975], Agarwal et al. [2020]] are crucial. WPT offers detailed decomposition, capturing both time and frequency information, while AR models can predict future signal values, formulated either as a spectral estimation problem or a linear prediction in the time domain. Power Spectral Density (PSD) analysis, computed via the Fast Fourier Transform (FFT) and refined using Welch's method, characterizes signal power distribution over frequency. Additionally, Statistical Parameters of Signals (SPS) measure the distribution and complexity of EEG signals, extending concepts like cumulants and entropy is also used as feature of EEG signal. These statistical features, while useful for providing insights into its overall distribution, are generally not well-suited for capturing complex, time-varying components of signals like those found in EEG. EEG signals are inherently represented as time-series data, necessitating the extraction of nuanced temporal dependencies that characterize these signals. Gated Recurrent Units (GRUs) are particularly well-suited for this task, demonstrating the ability to capture and model the complex temporal relationships embedded within EEG data. By leveraging GRUs, the temporal dynamics of EEG signals can be effectively unraveled, allowing for a more comprehensive understanding of the underlying neural processes . \nContinuous authentication: Continuous authentication in Extended Reality (XR) refers to the ongoing process of verifying a user's identity and intent while they interact within virtual, augmented, or mixed reality environments. Unlike traditional authentication methods that occur only at login, continuous authentication employs real-time biometric and behavioral data analysis. This includes factors such as head movement patterns, gaze direction, and face and voice characteristics. By assessing these factors, XR systems can detect anomalies that might indicate unauthorized access or compromised interactions. This dynamic approach enhances security by adapting to user behavior changes and prevents unauthorized users from gaining extended access to sensitive XR applications and data. Continuous authentication thus safeguards user experiences and privacy, maintaining a secure environment throughout the user's XR session."}, {"title": "2.3 Various attacks in an extended reality", "content": "Figure 6 illustrates the vulnerabilities and potential points of attack across different levels of an XR system. The three primary levels, where user authentication can be performed, are visually represented within a dashed rectangular box in Figure 6. The first level of authentication can be performed on the device before using the device for XR applications. Second-level authentication is based on continuous authentication using gestures and actions that can be performed during interactions in an XR world. Third-level authentication is performed in virtual space rendering, where the authentication of rendered avatars in the XR space is carried out. Therefore, depending on the various functional blocks of the XR system, we identified four different vulnerability points. In the following, we discuss different types of attacks that can be foreseen at different vulnerability points."}, {"title": "2.3.1 Vulnerability Point 1", "content": "Vulnerability point 1 is mainly associated with early authentication, primarily performed to validate the XR devices to authenticate the legitimate user. Conventional authentication methods include PINs, cards, passwords, and biometrics. Possible sources of attacks include direct attacks on biometrics and shoulder-surfing attacks on passwords and PINs.\nThe Direct attacks or presentation attacks [Ramachandra and Busch [2017a]] on the XR devices include the presentation of the biometric artefacts of the legitimate user to gain malicious access. The attacker can generate biometric artifacts either by printing or by synthesizing and presenting them directly to the biometric sensors. Depending on the type of biometric characteristics, the attack artifacts and generation of Presentation Attack Instruments (PAI) for successful attacks also vary. For instance, print attacks involve using printed biometric data (such as a fingerprint or face image) to deceive the system. In 3D mask attacks, a three-dimensional mask resembling the legitimate user's face is created and presented to the sensor. Replay image, speech or video attacks involve playing back a recorded image, speech or video of the legitimate user to fool the device. \nSeveral defense mechanisms have been proposed for PAI attacks. Optical flow texture analysis, for instance, detects subtle motion inconsistencies in spoofed biometric data by tracking pixel-level movements, effectively distinguishing real subjects from static or fake presentation attacks [Li et al. [2022], Bhattacharjee et al. [2019], Damer et al. [2016], Raghavendra et al. [2015], Ramachandra and Busch [2017b]]. Attacks on speech involve generating deepfakes or replaying audio to deceive authentication systems. These attacks exploit weaknesses in speech features like pitch, formants, and phonemes [Kumar et al. [2022], Sadashiv TN et al. [2023]]. Deep learning models, such as ResNet [Magazine et al. [2022]] and ASSIST are employed to analyze these features, detecting subtle inconsistencies in the generated audio that distinguish authentic speech from manipulated or replayed versions.\nShoulder surfing attacks in XR are similar to their counterparts in the physical world. They involve an attacker trying to obtain sensitive information by watching the actions and movements of the target user. In the XR context, an attacker may be able to see the user's virtual keyboard or other input devices and use that information to steal passwords or other sensitive data. In addition, the attacker may be able to observe the user's virtual interactions and use that information to gain unauthorized access to restricted areas or sensitive information. These attacks can be especially dangerous in virtual environments that are used for sensitive tasks such as military training or healthcare."}, {"title": "2.3.2 Vulnerability Point 2", "content": "Vulnerability point 2 is mainly targeted at the communication channel that connects the user interaction to the XR environment. User interaction is captured in terms of head movement, hand movement, full-body movement, and user behavior based on the stimuli. The possible sources of attacks include the injection of human behavior through a side-channel attack, as detailed below.\nSide channel attacks could be used to extract sensitive information from the user, such as biometric data or other personal information. For example, an attacker could use a microphone placed near the user's HMD to capture the sounds of their hand movements and use that information to reconstruct their hand gestures and steal biometric data . Another example is the use of sensors to capture a user's body movements and infer sensitive information about their physical health or other personal characteristics. Additionally, inject attacks [Pant et al. [2024]] could be performed by overriding the biometric capture device. In such scenarios, an attacker could inject a biometric sample corresponding to the victim directly into the device, bypassing the actual user interaction. This could allow the attacker to impersonate the victim within the XR environment, gaining unauthorized access to sensitive areas or information. These injected biometric samples could be sourced from previously captured data or generated using advanced techniques like deepfakes, making the attack difficult to detect.\nTo counter inject attacks in Extended Reality (XR) environments, effective defense strategies include implementing tamper-resistant biometric capture devices to prevent unauthorized access and utilizing robust encryption protocols for secure data transmission. Incorporating multi-factor authentication methods, such as PINs or behavioral biometrics, enhances security. Additionally, deep fake detection algorithms can monitor user behavior and identify unusual patterns indicative of potential attacks, strengthening overall system integrity [Qamar et al. [2023]]."}, {"title": "2.3.3 Vulnerability Point 3", "content": "Vulnerbality point 3 is the target on the rendered virtual space where the attacks can impersonate the virtually rendered avatar. Advanced XR systems continuously authenticate users in virtual space, especially in confidential meetings [Sivasamy et al. [2020]]. However, secure authentication is assured based on biometric characteristics, including avatar verifications, voice authentication , and gaze movement . In the following section, we discuss the possible attacks at vulnerability point 4.\nMan-In-The-Room attack is a type of attack where the attacker joins the target's VR environment without their knowledge and remains invisible while extracting information or manipulating the environment. This attack is particularly concerning in VR environments because users tend to have a high level of immersion and may not notice the presence of an attacker. An attacker can use this attack to spy on the user's interactions, collect sensitive information, or even manipulate the environment in a way that could be harmful to the user. For example, by leveraging morphing techniques, the attacker can subtly alter avatars or objects within the environment, making changes that go unnoticed by the user but that could influence their behavior or decisions. Additionally, the use of synthetically generated images or videos can further enhance the attack, allowing the intruder to introduce realistic yet fake content, such as deepfake avatars or manipulated scenes, which can deceive or mislead the user. These manipulations can be carried out by exploiting vulnerabilities in the VR system or by using social engineering techniques to trick the user into granting access to the VR environment. As VR has become more widespread and integrated into various industries, it is important to address the potential security threats posed by man-in-the-room attacks, including the sophisticated use of morphing and synthetic data generation, to ensure the safety and privacy of VR users.\nLooking into the adverse affects that these fake avatars create, Bader and Amara [2014a] proposes a watermarking algorithm for securing avatar access in virtual worlds by embedding a 128-bit biometric fingerprint into the ordered facet-vertex rings of the avatar's face, ensuring imperceptibility and resistance to potential attacks during authentication [Bader and Amara [2016], Lin and Latoschik [2022]].\nCamera Stream and Tracking Exfiltration attack is a type of attack where the attacker gains unauthorized access to the head-mounted display's (HMD) live stream and the HMD front-facing camera stream. By gaining access to these streams, attackers can potentially monitor the user's actions and surroundings, compromising their privacy and security. In addition, the attacker can track the user's head movements and use this information to gain a better understanding of their environment. This type of attack can be particularly concerning in situations where the user is engaging in sensitive activities such as financial transactions or national security discussions. Therefore, it is essential to implement robust security measures to prevent unauthorized access to HMD streams."}, {"title": "3 Biometric verification Techniques in XR", "content": "In this section, we discuss the existing biometric verification techniques that are widely adopted in XR applications. Biometric characteristics can be classified into two categories: physiological and behavioral. Physiological biometrics involves the analysis of physical measurements of the human body. Examples of physiological biometrics include iris-recognition and fingerveins. On the other hand, behavioral biometrics analyzes body movements such as voice, gait, and keystrokes."}, {"title": "3.1 Physiological biometric", "content": "Physiological biometrics has been widely developed to secure XR systems. Fingerveins are located beneath the skin, which makes them more secure and adaptable . Because of their subcutaneous position, they are difficult to imitate and pose a lower risk of presentation/spoof attacks. Therefore, the adoption of fingerveins as a biometric measure in XR mitigates the limitations associated with traditional authentication methods, offering a promising solution for secure and reliable identity verification in the metaverse.\nTo enhance biometric authentication in Extended Reality (XR) systems, iris data-based biometrics have been integrated into the head-mounted display. However, this approach is not without its limitations. One challenge is the potential distortion of the iris data, which can result in misleading responses. To address this issue, a data-augmentation technique has been proposed by Varkarakis et al. [2018]. By augmenting iris data with off-axis images, as well as images with blur, contrast, and shadow, the accuracy and robustness of iris authentication can be improved. Another concern with iris-based biometrics is their vulnerability to presentation attacks owing to the static nature of iris authentication. To mitigate this risk, a real-time iris detection method was introduced by Adnan et al. . This method aims to detect and prevent presentation attacks by continuously monitoring the iris in real-time during the authentication process. By dynamically analyzing iris features, the system can detect anomalies and suspicious behaviors, enhancing the security of iris-based biometric authentication.\nIn addition to iris-based biometrics, the periocular region has been recognized as a valuable source of unique user information [Boutros et al. [2020a]]. The periocular region refers to the area surrounding the eye, including the eyelid, extraocular cells, muscles, and the surrounding nerves. By fusing the iris and periocular regions at the score level, researchers have demonstrated enhanced performance in biometric authentication [Boutros et al. [2020a]]. This fusion approach leverages the complementary information provided by the iris and periocular regions, thereby improving the overall accuracy and reliability of the authentication system."}, {"title": "3.2 Behavioural biometric", "content": "As discussed previously, knowledge-based biometrics are susceptible to attacks, highlighting the need for alternative approaches. One promising avenue is to leverage user behavior in response to stimuli or forced changes, as this behavior often contains identifiable information. \nFinger-movement-based behavior biometrics was proposed by Liebers et al. [2021] for reliable user authentication while using the XR system. Finger movement data were collected to identify users based on their interaction with buttons and sliders [Liebers et al. [2022]]. However, this method is limited in that it restricts the user behavior to specific input devices. Therefore, there is a need for an approach that can track the natural unrestricted movement of the hand. Hand movements are susceptible to mimicry attacks because they are exposed to the physical world [Khan et al. [2018], Parampalli et al. [2008]].\nSignatures have long been utilized as a traditional means of authentication, particularly in settings such as banks. However, signatures are vulnerable to forgery by trained individuals through practice. To address this limitation, a novel air handwriting-based verification system was proposed by Lu et al. [2018, 2019], Lu and Luo [2020], Lu [2021], Lu et al. [2021a]. In this system, users use controllers to sign into the VR environment, ensuring that fraudulent actors do not gain access to their signatures. In addition, the system incorporates the tracking of hand orientation and geometry, providing a multi-factor authentication (MFA) mechanism for enhanced security. By capturing air signatures and monitoring hand orientation and geometry, this approach offers a secure and robust authentication method for VR environments. The combination of air signatures and multifactor authentication provides a comprehensive approach for user verification, minimizing the risk of unauthorized access.\nRecent research by Asish et al. has focused on identifying the minimal set of features necessary for user identification with the aim of creating a task-agnostic VR authentication system. Additionally, investigations have been conducted to determine the most effective machine-learning models for user identification using eye-gaze data. Notably, these studies predominantly relied on stored data for the analysis. In contrast, the study presented by Lohr et al. proposed a real-time processing approach for eye movements in VR authentication. Researchers collect eye gaze data by tracking the movement of a sphere in 2D space, enabling the processing and analysis of eye gaze information in real time. The Gaze-based biometrics has also been developed for the authentication of XR devices [Luo et al. [2020]]. OcuLock system is based on gaze-based biometrics, which are secure and accurate. OcuLock utilizes multiple biometric modalities, including iris, eye movement, eyelid, extraocular cells, muscles, and surrounding nerves, to verify the identity of the user. Considering these various factors, OcuLock enhances the security and accuracy of biometric authentication in XR systems. In the context of VR authentication, eye movement- and gaze-based behavioral authentication methods are typically integrated into head-mounted displays. However, these approaches often require significant computational resources owing to the large number of eye-gaze features involved. Furthermore, these VR authentication systems are typically task dependent and identify users based on their performance in specific activities related to a particular task.\nThe use of full-body motion as a biometric for user verification has been explored by Prakash et al. . This approach can be considered as a multimodal approach for authentication, incorporating various behavioral biometrics. A related paper by Winkler et al. [2022] presented a reinforcement learning framework that enables the real-time tracking of human body motion using sparse signals obtained from wearable devices. The proposed framework can simulate physically valid full-body motions based on input signals, even in scenarios where observations of the lower body are unavailable. Through the use of a policy network, the framework learns to generate appropriate torques for activities such as balancing, walking, and jogging while closely aligning with the input signals. The framework exhibits robustness across different locomotion styles, body sizes, and environments, demonstrating its effectiveness in immersive experiences such as augmented reality (AR) and virtual reality (VR). These movement-based behavioral biometrics, including full-body motion, have demonstrated promising results for user authentication. However, there remains a need for biometric approaches that are less susceptible to vulnerability and potential attacks. Further research and development is necessary to explore additional modalities and techniques that can enhance the security and reliability of user verification in immersive environments."}, {"title": "4 Digital avatars in XR", "content": "Digital avatars provide social interaction among users in an Extended Reality (XR) environment. Avatar has evolved over the years based on the advancements of avatar generation techniques and now can be used for verification of the individuals in the XR environment. The taxonomy of the current section is shown in figure 8. These avatars were generated in five different ways, as shown in Figure 9.\n\u2022 Early VR Systems: In the early stages of VR, avatars are typically basic and lack detailed customization options. Users are often limited to selection from a small set of predefined avatars that resemble their real-world appearance [Liu and Steed [2021]].\n\u2022 Basic Customization: As VR technology improves, users are given the ability to customize their avatars to some extent. They could choose from a range of pre-designed attributes such as gender, hairstyles, clothing, and basic facial features. Although this provided a degree of personalization, options were still relatively limited.\n\u2022 Advanced Customization: With the advancement of VR systems and increasing demand for immersive experiences, more comprehensive customization options have been introduced. Users can modify the finer details of their avatars, including facial structure, skin tone, body shape, and accessories. This level of customization allows users to create avatars that closely resemble their real-world appearance or desired virtual identity [O'Brolch\u00e1in et al. [2016], Achenbach et al. [2017]].\n\u2022 Photorealistic Avatars: The introduction of advanced capture technologies, such as 3D scanning and pho-togrammetry, revolutionized avatar generation. Users can now create highly realistic avatars by capturing their own facial features and bodies using specialized hardware or even standard cameras. These techniques enable the creation of avatars that closely mirror the user's physical appearance, thereby enhancing the sense of presence in VR environments.\n\u2022 Real-time Motion Capture: Recent developments in motion capture technologies have enabled real-time tracking and mapping of user movements on avatars. This allows for a more accurate representation of body movements, facial expressions, and gestures in a virtual environment. Users can see their avatars to replicate their real-world actions, thereby enhancing their immersion and sense of embodiment in VR . Photo-realistic avatars serve as the identity of the user in the virtual world to uniquely identify the user."}, {"title": "4.1 Biometric-based photo-realistic avatars", "content": "In this section, we summarise the existing works on biometrics based of Photo-realistic avatar generation on verification to achieve reliable and secure XR environment. First, we discuss the different avatar generations, including both 2D and 3D, then we discuss the improvents in the quality of avatar with the evolution of generative deep learning architectures, and finally, a discussion on the avatar verification method is presented.\n\n4.1.1 Avatar generation:\nIn the initial stages of Extended Reality (XR), avatars were relatively simplistic, offering users a limited choice. However, advancements in technology have led to the introduction of customizable avatars, allowing users to personalize their virtual representation by selecting features, such as hairstyle, eye color, and skin tone. Although these avatars offered a degree of customization, they were not realistic and were unsuitable for biometric purposes. Consequently, the demand for photorealistic avatars has increased in recent years. Researchers have conducted numerous studies on the generation of photorealistic avatars using both 2D and 3D techniques.\n2D photo-realistic avatar: The 2D photorealistic avatar has been extensively studied for face biometrics. The generated avatar possesses biometric characteristics. A unified framework for generating a photorealistic interactive virtual environment (piVE) without relying on bluescreen techniques or specialized rendering hardware has been proposed by Kim et al. . The framework, as presented by Kim et al. , utilizes heterogeneous multiview cameras and stereo images/videos to render piVE in real time. This approach eliminates the need for expensive high-end computers and renders graphics objects based on camera parameters and user interaction.\n3D photo-realistic avatar: The 3D photo-realistic avatar generation has gained the attention of researchers owing to its high-quality generation that can reflect the biometric characteristics of the individual. Recent advancements in deep generative models have significantly improved the realism of synthetic human images. However, flexibly controlling the generative process, such as changing camera angles and poses while preserving biometric identity features, remains a challenge. On the other hand, deformable body models such as SMPL [Loper et al. provide pose and shape control, but rely on traditional rendering pipelines that have limitations. By Prokudin et al. , a network was proposed to bridge the gap between the geometry-based rendering and generative networks. The network directly converts a sparse set of 3D mesh vertices into photorealistic images, thereby eliminating the need for traditional rasterization. The model was trained on a large dataset of human 3D models and real photos, demonstrating superior photorealism and rendering efficiency compared to conventional differentiable renderers.\nBy Feng et al. , the generation of real-time avatars was shown in less than 20 min by scanning. The construction of a character involves the capture and creation of three distinct models: the body, face, and hands. A photogrammetric capture cage and software were used to create a 3D body model, which was then rigged to enable control over movement. To enable facial expressions and hand shapes, a blended shape model and hand rigging were integrated with the virtual avatar. A 3D face model was created by scanning facial expressions and constructing blended shapes. The face and body models were combined by replacing the face geometry and texture of the body model with the blend shape model, followed by color correction. Finally, a controllable hand and finger model were added to the 3D body model. In [Tewari et al. , a model-based deep convolutional autoencoder for reconstructing 3D human faces from single in-the-wild color images. The model combines a convolutional encoder network with a differentiable parametric decoder that encodes the detailed face pose, shape, expression, skin reflectance, and scene illumination. This allows the CNN-based encoder to extract meaningful parameters from a monocular image, thereby enabling end-to-end unsupervised training on large real-world datasets. Reconstructed faces demonstrated superior quality and representation.\nThe deep appearance model for a realistic rendering of the human face was presented by Lombardi et al. which utilizes a data-driven pipeline that learns a joint representation of facial geometry and appearance from multiview captures. A deep variational autoencoder is employed to model vertex positions and view-specific textures, enabling the capture of complex effects and correction for imperfect geometry. The system can be implemented in real-time interactive settings such as Virtual Reality (VR) using existing rendering engines. This offers a flexible and accurate approach without the need for precise geometric estimates.\nIn [Cao et al. [2022]] photorealistic avatars of individuals were generated using a simplified mobile phone capture. Unlike other existing methods, the architecture employed in [Cao et al. focuses on producing avatars that closely resemble individuals, while requiring minimal data. The approach involves utilizing a conditional representation and a universal avatar prior trained on the facial performance of a wide range of subjects. By employing inverse rendering techniques, the model was fine-tuned to enhance realism and personalize the avatar's range of motion. The technique proposed by Cao et al. can create a high-fidelity 3D head avatar that accurately depicts the facial shape and appearance of subjects, along with providing control mechanisms for gaze direction. Experimental results demonstrate that the avatars generated using this approach exhibit superior visual quality compared to existing methods [Wang et al. for lightweight avatar creation.\nThe generation of realistic head avatars from video sequences has been proposed in [Zheng et al. [2022]]. Existing methods rely on either fixed 3D morphable meshes (3DMM) or neural implicit representations, both of which have limitations in terms of topology, deformation, and rendering efficiency. In contrast, the approach in [Zheng et al. referred to as PointAvatar, introduces a deformable point-based representation that separates the source color into intrinsic albedo and normal-dependent shading. PointAvatar [Zheng et al. [2022]] combines high-quality geometry and appearance with topological flexibility, ease of deformation, and rendering efficiency. Therefore, it can successfully generate animatable 3D avatars using monocular videos from various sources, surpassing other existing approaches in challenging scenarios, such as thin hair strands, while also being more training-efficient than competing methods.\nTo enable immersive 3D experiences and create photo-realistic avatars using commodity hardware, such as mobile phone cameras, a novel landmark detector was proposed by Szymanowicz et al. It estimates the camera poses from 360-degree videos of a human head, allowing realistic rendering from any viewpoint. Synthetic data validation experiments demonstrate the effectiveness of the method, which was showcased with 360-degree avatars trained from mobile phone videos. In Lattas et al. , FitMe, a facial reflectance model, and a rendering pipeline for generating high-fidelity human avatars from single or multiple images were presented. It utilizes a multimodal style-based generator and PCA-based shape model to capture facial appearance and shape. The differentiable rendering optimization process achieves photorealistic shading and accurately preserves identity. FitMe outperforms existing methods in reflectance acquisition and identity preservation by generating relightable avatars from single \"in-the-wild\" images or multiple unconstrained images of the same person. Notably, FitMe is efficient, requiring only one minute to produce avatars with a mesh and texture suitable for end-user applications.\nA computationally efficient and adaptive deep generative model for 3D human faces, called Pixel Codec Avatars (PiCA), was introduced by Ma et al. . PiCA combines a fully convolutional architecture for spatially varying feature decoding with a rendering-adaptive per"}]}