{"title": "Deep Graph Anomaly Detection: A Survey and New Perspectives", "authors": ["Hezhe Qiao", "Hanghang Tong", "Bo An", "Irwin King", "Charu Aggarwal", "Guansong Pang"], "abstract": "Graph anomaly detection (GAD), which aims to identify unusual graph instances (e.g., nodes, edges, subgraphs, or graphs), has attracted increasing attention in recent years due to its significance in a wide range of applications. Deep learning approaches, graph neural networks (GNNs) in particular, have been emerging as a promising paradigm for GAD, owing to its strong capability in capturing complex structure and/or node attributes in graph data. Considering the large number of methods proposed for GNN-based GAD, it is of paramount importance to summarize the methodologies and findings in the existing GAD studies, so that we can pinpoint effective model designs for tackling open GAD problems. To this end, in this work we aim to present a comprehensive review of deep learning approaches for GAD. Existing GAD surveys are focused on task-specific discussions, making it difficult to understand the technical insights of existing methods and their limitations in addressing some unique challenges in GAD. To fill this gap, we first discuss the problem complexities and their resulting challenges in GAD, and then provide a systematic review of current deep GAD methods from three novel perspectives of methodology, including GNN backbone design, proxy task design for GAD, and graph anomaly measures. To deepen the discussions, we further propose a taxonomy of 13 fine-grained method categories under these three perspectives to provide more in-depth insights into the model designs and their capabilities. To facilitate the experiments and validation of the GAD methods, we also summarize a collection of widely-used datasets for GAD and empirical performance comparison on these datasets. We further discuss multiple important open research problems in GAD to inspire more future high-quality research in this area. A continuously updated repository for GAD datasets, links to the codes of GAD algorithms, and empirical comparison is available at https://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph anomaly detection (GAD) aims to identify graph instances (e.g., node, edge, sub-graph, and graph) that do not conform with the normal regime. It has been an active research area with wide application in detecting abnormal instances in a variety of graph/network data, e.g., abusive user behaviors in online user networks, fraudulent activities in financial networks, and spams in social networks. Furthermore, since the relations between data samples can be modeled as similarity graphs, one can also use GAD methods to discover anomalies in any set of data objects (as long as an appropriate pairwise similarity function is available).\nDue to the complex structure of graphs, traditional anomaly detection methods cannot be directly applied to graph data. In recent years, graph neural networks (GNNs) have shown promising capabilities in modeling and learning the representation of graphs by capturing structural patterns, inspiring a large number of GNN-based approaches for GAD. However, the popular GNN designs, such as aggregation of node representations and optimization objectives, may lead to over-smoothing, indistinguishable representations of normal and abnormal graph instances, which significantly limits their applications in real-world use cases. Many novel GNN-based approaches specifically designed for GAD have been proposed to tackle the these challenges. In this work, to summarize the current methodologies and findings, we provide a systematic and comprehensive review of current deep GAD techniques and how they may tackle various types of challenges in GAD. We also propose several important open research problems in GAD to inspire more future research in this area.\nRelated surveys. There have been several reviews on anomaly detection in recent years, e.g., [2], [4], [81], [89], [103], [113], but most of them are focused on non-deep-learning-based methods for GAD [2], [4], [103], or on general data rather than graph data [7], [89]. The studies [81], [113] are on deep GAD, but the reviews are limited to a relatively narrow point of view. For example, Ma et al. [81] focus on task-specific discussions, with limited reviews on the technical development, while Liu et al. [64] and Tang et al. [113] focus on establishing a performance benchmark for unsupervised and supervised GAD methods respectively. Another related work is Liu et al. [71], but it is restricted to supervised imbalanced graph learning. Although these surveys provide useful guidelines for the development of methods for GAD, it is difficult to understand the technical insights of existing methods and their limitations in addressing some unique challenges in GAD.\nOur work. To fill this gap, we aim to offer a distinctive review on GAD to discuss these insights, the limitations, and the future research opportunities in this crucial topic."}, {"title": "2 PROBLEMS AND CHALLENGES IN GAD", "content": "This section discusses some unique complexities and chal-lenges in GAD."}, {"title": "2.1 Major Problem Complexities", "content": "The complexities in GAD can be summarized in two ways. One source of the complexities lies in some inherent charac-teristics of graph data.\n\u2022 P1. Structural dependency. The samples are typically correlated/connected with each other instead of being independent. The connections are of different semantics, e.g., it could be a purchase relationship in a social network, or a citation relationship in a citation network. The complexity of graph structure is reflected in connectivity patterns, dependency, or influence at different levels of graph data, which play a significant role in defining what is abnormal on graphs [4]. For example, different from i.i.d. data, where the anomalies are independent of the context,\n\u2022 P2. Diverse types of graph. There are many types of graphs in the real world, each serving different purposes and applications. Graphs can be categorized into static and dynamic types, depending on whether they change over time. It can also be divided into heterophilic and homophilic graphs according to the type of connection [154], [163]. The definition of anomaly in one type of graph can differ significantly from that in other types of graph. In particular, a graph instance (e.g., node/edge/graph) that is clearly abnormal in a dynamic graph at a specific time step (i.e., a static graph) can demonstrate strong normality when looking at the evolution of the graph; similarly, we can have opposite abnormality of a graph instance in a homophilic graph vs. in a heterophilic graph. Dealing with diverse types of graphs requires the GAD methods to adapt its learning strategy based on its unique properties of graphs.\n\u2022 P3. Computational complexity in handling large-scale graphs. With the increasing amount of online data, modern applications can include very large-scale graph data with millions/billions of nodes and/or edges [43], [48], [98], [113], such as those in web-scale social networks, financial transaction networks, cyber networks, user-product e-commerce networks, and citation networks. To identify anomalies using global structural contexts, it is essential to consider the full graph structural information, or a large proportion of the structural relations. The key complexity here is to deal with the time and space complexities when loading such large-scale structural relation data.\nAnother source is from the variety of graph abnormalities.\n\u2022 P4. Diverse graph anomaly instances. In contrast to anomaly detection in other forms of data, anomalies within graph data can arise from different compo-nents, such as nodes, edges, sub-graphs, or the entire graph [81]. Moreover, graph anomalies can manifest themselves in diverse ways, depending on the struc-ture and attribute information of graph data. This highlights the need for GAD methods to incorporate a range of techniques focused on identifying irregular patterns across nodes, edges, subgraphs, and the entirety of the graph.\n\u2022 P5. Large variation in graph abnormality. Anomalies in graphs can manifest in different forms, including abnormality in graph attributes, graph structure, or the composition of graph attributes and structure [4]. Some exemplars include attribute anomalies (i.e., graph instances that are exceptional in a graph attribute set) [3], [89], structural anomalies (i.e., graph instances that connect different communities, forming dense connections with others) [18], [81], contextual anomalies (i.e., graph instances that have different attribute values compared to other nodes in the same community) [18], [81], and local affinity anomalies (i.e., graph instances that demonstrate significantly"}, {"title": "2.2 Major Challenges", "content": "The aforementioned problem complexities lead to the follow-ing largely unsolved challenges in GAD, which deep GAD approaches can tackle to various extent:\n\u2022 C1. Graph structure-aware GAD. As discuss in P1, graph anomalies are not solely determined by their own attributes but also by their structural context. Thus, the GAD methods are required to effectively capture those structural dependency in their anomaly scoring functions. The effect of this dependency in anomaly scoring may vary significantly from ho-mophilic graphs to heterophilic graphs, and from static graphs to dynamic graphs (P2). On the other hand, oversmoothing is a common issue when model-ing the graph structure information, which is referred to as a phenomenon in graph representation learning where the learned representations of different nodes become overly similar due to the iterative aggregation of representations of neighboring nodes to obtain the representations of the target nodes. In GAD, this can lead to node/subgraph representations that smooth out anomalies as well, making them indis-tinguishable between normal and abnormal graph instances [25], [98]. Therefore, it is challenging to model diverse structural influences in the anomaly scoring on graphs, while avoiding adverse effects like representation oversmoothing.\n\u2022 C2. GAD at scale. As discussed in P3, large-scale graphs with millions or even billions of nodes/edges presents a significant computational challenge to GAD methods that aim to model global or higher-order structure information [113]. Existing large-graph modeling methods are challenging to apply directly to GAD due to the extreme imbalance in the data (P7). While some subgraph and sampling techniques have been proposed to address this issue, they often fail to capture the full structural informa-tion, resulting in sub-optimal performance [25], [69], particularly for unsupervised GAD. Consequently, performing anomaly detection on large-scale graphs remains a long-standing challenge in the area.\n\u2022 C3. Generalization to different graph anomalies. As discussed in P4, there are various types of graph anomaly instances, making it hard to apply a one-for-all approach. Achieving this requires a combination of robust feature extraction and versatile detection models. Further, the anomalies can manifest in vari-ous forms, ranging from attributes, structure and their composition (P5). However, most existing methods are designed for a specific type of anomaly in an un-supervised manner, which typically have a low recall rate [18], [69], [98]. The challenge is amplified when the training data does not illustrate every possible\n\u2022 C4. Balanced GAD. As discussed in P7, since the number of normal instances is significantly larger than that of abnormal instances, the models tend to bias towards the majority class during the training, i.e., they perceive the normal patterns more frequently. Consequently, the models might be overly specialized in recognizing normal instances while generalizing poorly on the anomalies. Often the detection decision thresholds are crucial for making predictions for some methods [25], [113], and poorly chosen thresholds can worsen the effects of data imbalance. Thus, the challenge is to avoid biased GAD.\n\u2022 C5. Robust and interpretable GAD. GAD in real applications needs to be robust against various ad-verse conditions, such as abnormality camouflage (P8) [25], [37], [113] and unknown anomaly contamination [98], [99]. Addressing the abnormality camouflage or anomaly contamination may require models that can capture subtle differences between normal graph instances and camouflaged instances, and complex relationships within the graph as well. Besides, an explanation of why a graph instances is detected as anomaly can be crucial for the utility of the predic-tions in real applications [62], [106], [106], but it is a largely unexplored area. For example, in bank fraud detection, it is essential to provide a comprehensive explanation of the detected fraudulent activity for facilitating the subsequent investigation, but it is challenging to link the fraud to specific attributes of particular transactions (nodes) and their relations (edges) at a specific time period."}, {"title": "3 CATEGORIZATION OF DEEP GAD", "content": ""}, {"title": "3.1 Preliminaries", "content": "GAD aims to recognize the anomaly instances in graph data that may vary from nodes, edges to subgraphs by learning an anomaly scoring function. Traditional GAD methods achieve anomaly detection using matrix decomposition and residual analysis [4]. However, their performance is often bottlenecked due to the lack of representation power to capture the rich structure-attribute semantics of the graph data and to handle high-dimensional node attributes. In recent years, GNNs have been widely used in GAD due to their powerful representation learning ability. Some representative GNNs like GCN [54], GraphSage [39], and GCL [137] attract much attention in node representation learning in graphs. These GNNs can be leveraged to learn the expressive representation of different graph instances for GAD.\nDefinition and Notation. In this section, we introduce the definitions and notations used throughout the paper. We denote a graph by $G = (V, E)$ where $V$ and $E$ denote the node set and edge set respectively. For the graph $G$, we use $X\\in\\mathbb{R}^{N \\times M}$ to denote the matrix of node attributes and $x_i \\in \\mathbb{R}^M$ is the attribute vector of $v_i \\in V$, and $A \\in {0,1}^{N \\times N}$ is the adjacency matrix of G with $A_{ij} = 1$ iff $(v_i, v_j) \\in E$, where $N$ is the number of node.\nProblem Statement. GAD can be divided into anomaly detection at the node-level, edge-level, sub-graph level and graph-level settings. The node-, edge- and subgraph-level AD tasks are typically performed within a single large graph G, where the input samples are nodes $v \\in G$, edges $e\\in G$, and subgraphs $s \\subset G$, respectively. For the graph-level AD task, the input samples are a set of graphs $G = {G_1, G_2, ...}$. For the sake of simplicity and generality across different levels of GAD, we uniformly denote the input samples as $o$, i.e., $o$ can denote a node $v$, an edge $e$, a subgraph $s$, or a full graph $G$, depending on their use in specific algorithms or models. Then GAD aims to learn an anomaly scoring function $f:{o_1, o_2,...} \\rightarrow \\mathbb{R}$, such that $f(o) < f(o')$, $o \\in O_n$, $o' \\in O_a$, where $O_n$ and $O_a$ denote the set of normal and abnormal graph instances, respectively. Since anomalies are rare samples, it is typically assumed that $|O_n| >> |O_a|$."}, {"title": "3.2 Categorization of Deep GAD Methods", "content": "In order to facilitate a comprehensive understanding of the research progress in GAD, we introduce a new taxonomy that categorizes current GAD methods into three main groups,including GNN backbone design, GAD proxy task design, and graph anomaly measures, depending on the insights offered by each method. This enables us to review the GAD methods from three different technical perspectives. To elaborate the insights in each perspective, we further categorize the methods into fine-grained 13 groups. An overview of the taxonomy is shown in Figure 1.\nMore specifically, general GNNs can not be directly applied to GAD due to the aforementioned problem com-plexities, and thus, there is a group of studies that focus on designing suitable GNN backbones for GAD. The design of the GNN backbones can be divided into discriminative GNNs and generative GNNs according to the improvement of different modules in GNNs. The second main category of methods is on the GAD models constructed by optimizing a diverse set of well-crafted learning objective functions to form a proxy task that can guide the GAD models to capture diverse graph anomaly/normal patterns without the need for ground-truth labels. This category of methods can be further divided into five subcategories based on the modeling in the proxy tasks. Lastly, there is a group of methods that build GAD models based on anomaly measures that are designed specifically for graph data. These methods can be further grouped into four subcategories depending on the type of graph anomaly measures used. A summarization of representative algorithms for each type of GAD approaches is presented in Table 2 in Appendix A."}, {"title": "4 GNN BACKBONE DESIGN", "content": "This category of methods aims at leveraging GNNs to learn effective representations of graph instances for downstream anomaly detection tasks. Due to its strong capability to represent graph-structured data, GNNs can effectively obtain expressive node representations through aggregation among the connected nodes. However, unlike general node/graph classification datasets, GAD datasets are often extremely class-imbalanced, which prevents GNNs from being directly applied to GAD datasets. Therefore, several GNNs have been proposed to handle the imbalance problem for more effective GAD. Concretely, this type of methods can be"}, {"title": "4.1 Discriminative GNNS", "content": "Discriminative GNN-based GAD methods refer to a GNN architecture specifically designed for discriminating nor-mal graph instances from the abnormal ones, where the discrimination is typically achieved through a supervised learning manner. Thus, the discriminative GNNs are typically trained on the labeled graph dataset containing examples of both normal and abnormal instances. The core idea in these methods is to adapt the conventional GNN backbones in a way so that the message passing in GNNs can capture the majority patterns or the deviation patterns better.\nLet $h_i$ be the feature representation of a graph instance $o_i$ that is obtained through $l$ layers of feature aggregation (FAG) in a GNN, i.e., $h_l = \\text{FAG}_{1:l}(o_i, N_i)$ where $N_i$ represents the neighbor set of a graph instance $o_i$, these methods are typically optimized via a general cross-entropy loss to train the discriminative GAD model.\n$L_{cls} = \\sum_{o_i \\in G} [y_i \\log p_i + (1 - y_i) \\log (1 - p_i)],$ (1)\nwhere $y_i$ denotes the class label of the instance $o_i$ and $p_i = \\text{MLP}_{1:m}(h_l)$ is the output of a mapping function that goes through $m$ layers of multiple perceptrons (MLP) to project the feature $h_l$ to a probability of the sample being abnormal/normal. During inference, given a graph instance, $o_j$, $p_j$ or its inverse $1 - p_j$ can be used as its anomaly score.\nDifferent GNN-based encoders can be used to obtain the feature representation $h$, such as graph convolutional networks (GCNs) [54], graph attention networks (GAT) [117], or GraphSage [39]. Depending on which part of the learning"}, {"title": "4.1.1 Aggregation Mechanism", "content": "As a simple and effective way to obtain the representation of nodes in GNNs, feature aggregation plays a crucial role in learning node representations by aggregating information from neighboring nodes in a graph. Thus, to create GNN-based methods for GAD, one principled approach is to craft suitable feature aggregation FAG designs that are sensible for graph anomaly instances.\nAssumption. The GAD methods in this line assume that the connected instances from the same class in graphs have similar characteristics, from which we can perform feature aggregation to obtain discriminative normality/abnormality patterns.\nA widely-used FAG mechanism is as follows [54]:\n$h^{(l)}_i = \\sigma \\Big( W^{(l)}h^{(l-1)}_i + \\text{AGG} \\big( \\{h^{(l-1)}_j | o_j \\in N_i \\} \\Big) \\Big),$ (2)\nwhere $h^{(l)}_i$ is the feature representation of instance $i$ in the $l$-th layer, $\\sigma$ is an activation function, and $W^{(l)}$ is the training parameters in the $l$-th layer. The graph instance $o_i$ is often set as a node $v_i$, and $N_i$ is typically the 1-hop neighborhood of the node $v_i$. However, due to the oversmoothing representation issue (C1 in Sec. 2.2), directly applying such neighborhood aggregation mechanism can largely reduce the discriminability of of graph anomalies, especially for those whose abnormal behaviors are subtle (C3 in Sec. 2.2). Therefore, a variety of methods were proposed to enforce distinguishable representations for normal and abnormal graph instances throughout a number of feature aggregation iterations. These methods can be summarized via the following principled framework:\n$\\hat{h}^{(l)}_i = \\text{AGG} \\Big( \\{h^{(l-1)}_j | o_j \\in \\Phi \\big(N_i\\big) \\cup \\Psi (V) \\} \\Big),$\n$h^{(l)}_i = \\sigma \\big( W^{(l)} (h^{(l-1)}_i + \\hat{h}^{(l)}_i)\\big),$ (3)\nwhere $\\Phi(\\cdot)$ and $\\Psi(\\cdot)$ represent a filtering function on the neighborhood set $N_i$ and an edge synthesizer on the full node set $V$, respectively. Depending on how the methods specify the $\\Phi$ or $\\Psi$ function, we further categorize them into two fine-grained groups \u2013 hard/soft edge selection and edge synthesis \u2013 to gain better insights into these existing methods.\n\u2022 Hard Edge Selection. Popular aggregation mecha-nisms in GNN methods are built upon a homophily assump-tion that connected nodes come from the same class. Thus, the existence of non-homophily edges (i.e., edges that connect nodes of different classes, also referred to as heterophily edges below) in a GAD dataset can greatly hinders the discriminability of the learned feature representations. As shown in Fig. 2(a), one popular strategy is to instantiate $\\Phi(N_i)$ that prune the heterophily edges w.r.t. the normal class, referred to as hard edge selection. Below we review the methods in this line.\nIn order to enhance the homophily relations, CARE-GNN [25] devises a label-aware similarity measure to find informa-tive neighboring nodes during the aggregation where $\\Phi(N_i)$ is instantiated by a node selector that chooses the neighbors with high similarity. Moreover, a reinforcement learning module is also used in [25] to find the optimal amounts of neighbors to be selected. MITIGATE [12] implements $\\Phi(N_i)$ via a masked aggregation mechanism that utilizes the distance-based clustering algorithm to choose a subset of high-representative nodes, in which the nodes that are closest to the cluster centers are chosen. GmpaAD [82] takes a similar clustering-based approach as MITIGATE, but it uses a differential evolutionary algorithm to find the optimal mapping strategy and generate the representative nodes given the selected candidates from a clustering method. On the other hand, H2-FDetector [108] categorizes the edges into homophily and heterophily connections in the graph, and further designs a new information aggregation strategy ensure that the homophily connections propagate similar information while the heterophily connections propagate different information.\nIn addition to using distance, $\\Phi$ can also be specified via meta learning or reinforcement learning. BLS [22] is the rep-resentative method that enhances the FAG mechanism under imbalanced and noisy scenarios by selecting important nodes via a meta-learning gradient of the learning loss. AO-GNN [46] employs a reinforcement learning method supervised by a surrogate reward based on AUC performance to prune the heterophily edges. NGS [100] takes a meta-graph learning approach that devises a differentiable neural architecture to determine a set of optimized message passing structures and then combines multiple searched meta-graphs in FAG.\n\u2022 Soft Edge Selection. Another research line is adopt-ing an attention mechanism in GNNs by assigning the weights for each edge for soft edge selection for GAD, rather than hard edge selection, as demonstrated in Fig. 2(c). This weight is generally obtained through the relationship between node embeddings, which serves as an effective way to enforce the importance of some specific edge relations in the feature aggregation. GAT [117] is widely used as the basic backbone, on top of which a variety of designs is introduced in the methods of this category for GAD. Specifically, the general attention mechanism in GAT can be formulated as:\n$h_i^{(l)} = \\sigma \\Big(\\sum_{o_j \\in N(i)} \\Phi \\big( h_i, h_j; \\Theta \\big) \\mathbf{W}h_j^{(l-1)} \\Big),$ (4)\nwhere $\\Phi$ indicates a weight learning function with param-eters $\\Theta$ applied on the embedding of a graph instance $o_i$ and its neighbors $o_j$. It represents the contribution of relations/neighbors to the target instance $o_i$, where the instance $o_i$ is often specified as a node $v_i$. For instantiating $\\Phi$, GraphConsis [73] reveals an inconsistency phenomenon in node connections that abnormal nodes can have a high likelihood of being connected to normal nodes to camouflage their abnormality. It then introduces a consistency scoring-based method based on node embedding similarities and a self-attention mechanism to assign weights for different connections in the aggregation in $\\Phi$. FRAUDRE [142] extends the inconsistency-based scoring method to include three types of graph inconsistencies in features, topology, and"}, {"title": "4.1.2 Feature Transformation", "content": "In addition to the efforts on the aggregation mecha-nism, another popular approach to obtain discrimina-tive features for GAD is to perform feature transforma-tion on either the graph instance representations from GNNs, i.e., Transformation(hl), or raw attributes, i.e., Transformation(xi). This is crucial since datasets used in GAD can often contain a substantial proportion of feature in-formation that is irrelevant, or even noisy, to GAD. There are two popular approaches to instantiate this Transformation(\u00b7) function: one is to use gradient information and another is to use spectral graph filters.\nAssumption. It is assumed that there is irrelevant or noisy information in the raw attributes or graph structure w.r.t. GAD, which should be discarded during feature aggregation.\n\u2022 Gradient-based Feature Scoring. Extracting class-related features specific to the characteristics of a particular class is one straightforward way to obtain discriminative features. Inspired by variable decomposition [32], gradient information-based methods have been emerging as one main approach to obtain discriminative representations from GNNs [78]. The key idea here is to select features from the representation hl based on the gradient backpropagated from the softmax probability of being a specific class, as illustrated in Fig. 3(a). Let ak be the gradient score of an anomaly class c w.r.t. a feature k, then it represents the contribution of this feature to anomaly detection, which can be formulated as follows:\n$\\alpha_k = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial y_c}{\\partial h_{k,i}},$ (5)\nwhere $y^c$ is the predicted probability of being the anomaly class $c$ and $h_{k,i}$ is the $k$-th feature of the representation of node $i$ from a hidden layer. After obtaining a gradient score for each feature dimension, the top K features with the largest gradient scores are selected to represent the nodes in a reduced feature space. This gradient score-based approach is used in GDN [35] to select abnormal and normal features in a supervised manner, and these features are found often to be invariant to structure distribution shift. It helps reduce the negative influence of irrelevant features while preserving the extracted abnormal/normal graph patterns, thus enhancing the overall performance of GAD. Similarly, GraphENS [94] determines the importance of each node feature via a gradient score-based method. Apart from gradient score, existing feature selection methods for anomaly detection or imbalanced classification [57], [86]\u2013[88], [93] may be adapted for GAD.\n\u2022 Spectral Graph Filter. Spectral graph filter, which combines the strengths of spectral graph theory and GNNs, is widely applied to capture and analyze the structural prop-erties of graphs for GAD tasks. This approach utilizes a set of graph filters to transform the raw attributes to latent space to extract discriminative graph representations, as illustrated in Fig. 3(b). Each graph filter assumes that normal nodes tend to have similar features with their neighbors, which can be regarded as low-frequency information, whereas abnormal nodes in the graph are characterized by deviations from the norm, which are often accompanied by high-frequency infor-"}, {"title": "4.2 Generative GNNs", "content": "Generative GNN-based methods focus on synthesizing new graph instances to augment the existing graph data for enhancing model training on the new graph for GAD. This approach is motivated by the problem complexities like the scarcity of graph anomaly instances and their large variations (P5 and P7 in Sec. 2.2). The underlying key idea in these GAD methods is to synthesize outliers that can simulate graph anomalies in some specific properties to provide pseudo anomaly information for training the GAD models. In general, these methods can be summarized by the following formulation:\n$o^{new}_i = g_{\\theta}(o_i, X, A; \\epsilon),$ (8)\nwhere $o_i$ is an existing graph instance, $g_{\\theta}$ is a graph instance generator with parameters $\\theta$ that uses $o_i$, existing attribute information $X$, graph structure information $A$, and some auxiliary information $\\epsilon$ to generate the augmented graph instance, $o^{new}_i$. It is then followed by a one-class or binary classification loss $L_{cls}$ defined as\n$L_{gen} = \\sum l \\big( Y, f_{\\Theta} (X_{new}, A_{new}) \\big),$ (9)\nwhere $X_{new}$ and $A_{new}$ are the augmented version of $X$ and $A$, $Y$ represents the label set of the graph instances consisting of pseudo labels of the generated anomaly instances (and the labels in the original graph if any), and $f$ is a GAD model. Depending on the source of $\\epsilon$ in $g_{\\theta}$ in Eq. 8, these methods can be categorized into feature interpolation and noise perturbation generation methods. The former focuses on specifying $\\epsilon$ using data from existing graphs, while the latter focuses on utilizing prior distribution to specify $\\epsilon$."}, {"title": "4.2.1 Feature Interpolation", "content": "Feature interpolation is a commonly employed technique in imbalance learning for augmenting data, where the"}, {"title": "4.2.2 Noise Perturbation", "content": "Unlike the feature interpolation methods that generate new graph instances based on interpolation between rep-resentations of existing instances, the noise perturbation generation methods aim to generate graph instances using prior-driven noise perturbation, as shown in Fig. 4(b). This approach can incorporate prior knowledge of the graph normality/abnormality into the generation process for more effective GAD. The generated graph samples as abnormal graph instances, combined with the given labels for existing nodes, can then be leveraged to guide the training of a discriminator for GAD.\nThe approach can be generally formulated as follows\n$X_{new}, A_{new} = g_{\\Phi} (X, A; \\epsilon),$\n$L_{cls} = \\sum_{i=1}^N \\sum_{j=1}^M l \\big( y_i, f_{\\Theta} (X_i, A_i) \\big) + \\sum_{l=1}^M l \\big( 1, f_{\\Theta} (X^{new}_l, A^{new}_l) \\big),$ (11)\nwhere $\\epsilon$ is noise perturbation typically generated from a probability distribution, such as a Gaussian distribution, $g_{\\Phi}(\\cdot)$ is a generation function parameterized by $\\Phi$, and $M$ is the number of generated graph instances.\nAssumption. Certain prior distributions can be used as a source of noise perturbation to generate pseudo-abnormal graph instances and/or diversify normal graph instances.\nOne group of methods in this category [8], [14], [60], [99] takes a representation permutation approach, which focuses on applying permutation to graph representations to instantiate $g_{\\Phi}(\\cdot)$, i.e., Permutation(Z). It first utilizes a GNN to obtain the representations from the original graph data X and A. Then it applies the permutation to the representations to generate the representation of anomalous samples, denoted as $\\tilde{Z}$.\n$\\tilde{Z} = \\text{Permutation}(Z; \\epsilon), Z = \\text{GNN} (X, A)$ (12)\nwhere $\\epsilon$ are the hyperparameters of permutation. For ex-ample, DAGAD [60] employs the permutation and concate-nates on the representation learned from a limited number of labeled instances to generate the anomalous sample, thereby enriching the knowledge of anomalies captured in the training set. On the other hand, GGAD [99] aims to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations by leveraging the two priors of anomaly nodes, including asymmetric local affinity and egocentric closeness, to impose constraints the representations. SDGG [8] takes a similar approach as GGAD, but it is focused on generating abnormal graphs that closely resemble fringe normal graphs, which are then used to train graph-level anomaly detectors. Unlike GGAD and SDGG that work on exclusively normal training data, ConsisGAD [14] focuses on unlabeled nodes. It creates a noise version of the unlabeled nodes by injecting noise into their representations to synthesize instances that maintain high consistency with"}, {"title": "5 PROXY TASK DESIGN", "content": "The proxy task design-based approaches aim to capture di-verse normal/abnormal graph patterns by optimizing a well-crafted learning objective function that aids the detection of anomalies in the graph data without the use of human-annotated labels. One of the crucial challenges in proxy task design is to guarantee that i) the proxy task is associated with GAD, and ii) it can deal with rich structure information and complex relationships in graphs. We roughly divide the methods in this group into five categories according to the employed proxy tasks, including reconstruction, contrastive learning, knowledge distillation, adversarial learning, and score prediction. Their respective framework is shown in Fig. 5. Below we introduce each of them in detail."}, {"title": "5.1 Graph Reconstruction", "content": "Data reconstruction aims to learn low-dimensional feature representations of data for reconstructing given data in-stances, which is widely used in tabular data, and im-age/video data to detect anomalies [91", "41": "consists of a graph encoder and a graph decoder and it is easy to implement. Formally, given a graph with X and A, the graph reconstruction can be formulated as\n$Z = \\text{GNN}_{enc} (X, A; \\Theta_{enc}),$\n$\\hat{X} = \\text{GNN}_{dec} (Z, A; \\Theta_{dec}),$\n$\\hat{A_{ij}} = p \\big( A_{ij} = 1 | Z_i, Z_j \\big) = \\text{sigmoid} (z_i z^T_j),$ (14)\nwhere the GNN encoder and decoder are parameterized by $\\Theta_{Enc}$ and $\\Theta_{Dec}$ respectively, Z are the representations of the nodes in"}]}