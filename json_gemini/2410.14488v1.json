{"title": "ANT: Adaptive Noise Schedule for Time Series Diffusion Models", "authors": ["Seunghan Lee", "Kibok Lee", "Taeyoung Park"], "abstract": "Advances in diffusion models for generative artificial intelligence have recently propagated to the time series (TS) domain, demonstrating state-of-the-art performance on various tasks. However, prior works on TS diffusion models often borrow the framework of existing works proposed in other domains without considering the characteristics of TS data, leading to suboptimal performance. In this work, we propose Adaptive Noise schedule for Time series diffusion models (ANT), which automatically predetermines proper noise schedules for given TS datasets based on their statistics representing non-stationarity. Our intuition is that an optimal noise schedule should satisfy the following desiderata: 1) It linearly reduces the non-stationarity of TS data so that all diffusion steps are equally meaningful, 2) the data is corrupted to the random noise at the final step, and 3) the number of steps is sufficiently large. The proposed method is practical for use in that it eliminates the necessity of finding the optimal noise schedule with a small additional cost to compute the statistics for given datasets, which can be done offline before training. We validate the effectiveness of our method across various tasks, including TS forecasting, refinement, and generation, on datasets from diverse domains. Code is available at this repository: https://github.com/seunghan96/ANT.", "sections": [{"title": "1 Introduction", "content": "Diffusion models have demonstrated outstanding performance in generative tasks across diverse domains, and various methods have been proposed in the time series (TS) domain to address a range of tasks, including forecasting, imputation, and generation [26, 34, 38, 18, 30, 1]. However, recent works primarily focus on determining which architecture to use for a diffusion model, overlooking the useful information from the domain knowledge for other components, e.g., noise schedule.\nNoise schedules control the noise added to the data across the diffusion process, with the choice of schedule being crucial for performance [4]. Several works have explored the design of noise schedules [24, 21]; however, they do not consider the characteristics of data, resulting in suboptimal performance, especially in the TS domain. This highlights the importance of selecting an appropriate schedule for each dataset.\nIn this work, we propose Adaptive Noise schedule for Time series diffusion models (ANT), a method for choosing an adaptive noise schedule based on the statistics representing non-stationarity of the TS dataset. These statistics measure the patterns appeared in the TS, with TS from the real world exhibiting high non-stationarity while white noise TS exhibiting low non-stationarity. We argue that a desirable schedule should gradually transform non-stationary TS into stationary ones, in line with"}, {"title": "2 Background and Related Works", "content": "Denoising diffusion probabilistic model (DDPM). DDPM [12] is a well-known diffusion model where input x\u00ba is corrupted to a Gaussian noise during the forward process and x\u00ba is denoised from xT during the backward process with total T diffusion steps. For the forward process, xt is corrupted from xt-1 iteratively with Gaussian noise of variance \u03b2t \u2208 [0, 1] :\n$q (x^{t} | x^{t-1}) = N (x^{t}; \\sqrt{1 - \\beta_t}x^{t-1}, \\beta_t I), t = 1,...,T$.$\nUsing a property of Gaussian transition kernel, the forward process of multiple steps can be written as q (x | x\u00ba) = N (xt; \u221a\u0101tx\u00ba, (1 \u2013 \u0101t) I), where \u0101t = \u220f=1(1 \u2212 \u03b2s) and at = 1 \u2013 \u03b2t. For the backward process, xt-1 is denoised from xt by sampling from the following distribution:\n$p_\\theta (x^{t-1} | x^{t}) = N (x^{t-1}; \\mu_\\theta (x^{t},t), \\Sigma_\\theta (x^{t},t))$,$\nwhere \u00b5o (x, t) is defined by a neural network and E\u0473 (xt,t) is usually fixed as I. DDPM formulates this task as a noise estimation problem, where \u025b\u0259 predicts the noise added to x\u00b2. With the predicted noise \u025b\u04e9 (xt, t), \u03bc\u0473 (xt, t) can be obtained by\n$\\mu_\\theta (x^t,t) = \\frac{1}{\\sqrt{\\alpha_t}}(x^t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta (x^t,t))$$\nand \u025be is optimized using L\u025b = [\u03b5 \u2212 \u03b5\u03b8 (x, t)||2] , as a training objective.\nUnconditional TS diffusion models. Unlike most TS diffusion models which are conditional models [26, 38, 18, 30, 1, 6, 29, 40, 19], unconditional TS diffusion models do not use conditions (i.e. information of the observed values xobs) as explicit inputs during the training stage. Instead, they utilize them as guidance during the inference stage through a self-guidance mechanism. The backward process of unconditional models with the self-guidance term can be expressed as\n$p_\\theta (x^{t-1} | x^{t}, x_{obs}) = N (x^{t-1}; \\mu_\\theta (x^{t},t) + s\\sigma \\nabla_{x^t} log p_\\theta (x_{obs} | x^{t}), \\sigma^2_\\theta I)$,\nwhere s is the scale parameter controlling the self-guidance term. As the self-guidance mechanism helps avoid the need for architectural changes depending on the condition or task, we apply our method to TSDiff [15], which is the SOTA unconditional TS diffusion model. However, our method is not limited to the unconditional model; application to a conditional model, such as CSDI [34], is also discussed in Appendix M."}, {"title": "3 Methodology", "content": "In this section, we introduce our main contribution, ANT, an adaptive noise schedule for TS diffusion models. ANT proposes a noise schedule resembling an ideal one that gradually diminishes the non-stationarity of TS as the diffusion step progresses. Subsequently, we investigate the properties of commonly used noise schedules and argue that: 1) The diffusion step embedding (DE) is unnecessary for TS diffusion models when a linear schedule is employed, and 2) non-linear schedules are more robust to the change of the number of diffusion steps than linear schedules in terms of the performance and the scale parameter of the self-guidance mechanism of unconditional diffusion models."}, {"title": "3.1 ANT: Adaptive Noise Schedule for TS Diffusion Models", "content": "The overall framework of ANT is illustrated in Figure 2, where we aim to find a noise schedule that decreases the non-stationarity of TS on a linear scale. This difference can be explained by computing the non-stationarity curve of a TS for a given noise schedule as shown in Figure 2b. ANT proposes a noise schedule that minimizes the discrepancy between the ideal linear line and the non-stationarity curve of the schedule, whose x-axis and y-axis represent the progress of the step (%) and the (normalized) statistics of non-stationarity, respectively. As illustrated in Figure 2c, reduction in discrepancy with ANT yields better performance compared to without ANT.\nStatistics of non-stationarity. While various statistics can be used to quantify the non-stationarity of TS, we propose the integrated absolute autocorrelation time (IAAT). IAAT is a variation of IAT [22] that takes the absolute value of the autocorrelation to account for positive and negative correlations without canceling them out: TIAAT = 1+2\u2211k=1|pk|. Although IAAT shows the best performance, we note that ANT is robust across different statistics as shown in Table 8.\nAdaptive schedule. Different"}, {"title": "3.2 Diffusion Step Embedding for TS Diffusion Models", "content": "Diffusion models take the t-th step data (x\u00b9) and the step number t as inputs, where t is typically encoded in a DE. However, we argue that DE is not necessary for TS diffusion models employing a linear schedule, because information about the step is inherent in the data. To validate our claim, we conduct two experiments: 1) A proxy classification task to predict t with x\u00b9, and 2) visualization of the embeddings of x\u00b9 with various t.\nProxy classification. We design a proxy classification task that identifies the step number in the forward diffusion pass given a noisy TS. For the task, we sample multiple subseries from M4 [23], each with varying steps of up to 100 steps. We build a 1D convolutional neural network (CNN) classifier with the architecture of [Conv1D ReLU Flatten Linear], where Conv1D has the kernel size of 3 and 4 output channels. We argue that this is due to the variance of noise for each step (Bt), as a non-linear schedule adds most of the noise at the end of the steps, making a TS less distinguishable for most of the steps. In contrast, a linear schedule that gradually increases Bt makes a TS more distinguishable across steps, allowing us to eliminate the DE in the model."}, {"title": "3.3 Robustness of Non-linear Schedules", "content": "In diffusion models, the number of steps T determines the efficiency of overall process and sample quality. We argue that non-linear schedules are more robust to T than linear schedules in two aspects: 1) performance and 2) the optimal scale parameter s controlling the self-guidance in Eq. (4).\nRobustness of performance to T. Robustness of optimal s to T. Figure 7a shows that the optimal s is sensitive to T when employing linear schedules, whereas robust when using non-linear schedules. We discover that this robustness stems from the posterior variance (7) of a schedule, which affects the self-guidance term in Eq. (4)."}, {"title": "4 Experiments", "content": "Experimental setup. We demonstrate the effectiveness of our proposed method on three tasks: TS forecasting, refinement, and generation. For evaluation, we adopt ANT to TSDiff [15], where we use the official code to replicate the results. All experimental protocols adhere to that of TSDiff, where we utilize the CRPS [9] to assess the quality of probabilistic forecasts. We conduct experiments on eight TS datasets from different domains \u2013 Solar [17], Electricity [3], Traffic [3], Exchange [17], M4 [23], UberTLC [7], KDDCup [10], and Wikipedia [8]. We present the mean and standard deviations calculated from three independent trials."}, {"title": "4.1 Time Series Forecasting", "content": "For baseline methods in forecasting tasks, we compare our method with various approaches, including DeepAR [28], MQ-CNN [36], DeepState [25], Transformer [35], TFT [20], CSDI [34], and TSDiff [15]. We note that we do not aim to outperform all SOTA methods, but rather to demonstrate the efficacy of our proposed adaptive noise schedules when applied within the framework of TS diffusion models. Nonetheless, Table 2a indicates that TSDiff employing the noise schedule proposed by our method achieves the best performance on all datasets.\nTo show the effectiveness of our method across various prediction horizons, we conduct an experiment with different horizons using the same input window. Specifically, we multiply a by the standard horizon H used in Table 2a, where the range of a depends on the sequence length of the dataset. Table 2b shows that ANT enhances TSDiff with variable prediction lengths."}, {"title": "4.2 Time Series Refinement", "content": "We conduct an experiment refining the forecasting results of other base forecasters to evaluate the quality of the implicit probability density of TSDiff applied with ANT. For the experiment, we employ Gaussian (MS) and asymmetric Laplace negative log-likelihoods (Q) as regularizers for both energy (LMC) and maximum likelihood (ML)-based refinement, following TSDiff. Table 3 displays the results using a linear model as the base forecaster, showing that TSDiff, when applied with our method, yields performance gains across all datasets and refinement methods."}, {"title": "4.3 Time Series Generation", "content": "To assess the generation quality of TS diffusion models with ANT, we evaluate the forecasting performance of downstream forecasters trained on the synthetic samples generated by various models. For the baseline methods, we employ TimeGAN [39], TimeVAE [5], and TSDiff [15]. For the downstream models, we employ a simple linear (ridge) regression model, DeepAR [28], and Transformer [35]. The results are shown in Table 4, indicating that ANT improves the performance of TSDiff, leading to SOTA performance in most cases. Further details regarding the generation task with Electricity are discussed in Section O."}, {"title": "4.4 Analysis", "content": "Proposed noise schedule. Necesssity of DE. Tabele 6 displays the forecasting results for all datasets except for Traffic , models are better when a model better or trained with the model do trained. Table 5 shows the schedules proposed by ANT and the schedules that yield the best performance (oracle) among all candidate schedules, along with the forecasting performance obtained when using these schedules. The results indicate that the schedules selected by ANT are mostly consistent with the oracle, and even for an exceptional case that ANT fails to select the best schedule, e.g., on Traffic, the performance gap from the oracle is not significant.\n\n\nAblation study. The restlt shows that a that all those are the best of T with of that data and the and in Tablue 7 is the high. the results show that of the each ANT, we conduct an ablation study in Table 7.Table"}, {"title": "5 Conclusion", "content": "In this work, we introduce ANT, a method designed to select an adaptive noise schedule for TS diffusion models using the statistics of non-stationarity. ANT is practical in that it predetermines a noise schedule before training, as the statistics can be precomputed offline with minimal additional cost. The proposed ANT is a simple yet effective method for TS diffusion models across various tasks, e.g., ANT improves the SOTA method TSDiff by 9.5% on average across eight forecasting tasks, highlighting the importance of adaptive schedules tailored to the characteristics of TS datasets.\nLimitations and future work. We note that our contribution is not in finding the optimal schedule but in proposing a criterion for efficiently selecting a better noise schedule from a set of candidates based on the dataset characteristics. While our experiments search for an appropriate schedule from 35 candidates derived from linear, cosine, and sigmoid functions, any schedule can be considered a candidate for ANT, and incorporating additional schedules could lead to further performance gains at the expense of longer search times. We leave developing algorithms to determine the optimal schedule parameters based on our proposed criterion for future work. We hope that our research motivates further research across various domains to incorporate domain knowledge in the design of machine learning models, extending beyond noise schedules for diffusion models."}, {"title": "G Statistics of Non-stationarity", "content": "Integrated autocorrelation time (IAT) [22]. Integrated autocorrelation time (IAT) quantifies the efficiency of an MCMC sampler by estimating the effective number of independent samples produced by the sampler through calculating the autocorrelation (AC) within a chain. A sampler with a lower IAT is considered more efficient. Additionally, IAT can be used to diagnose the level of noise in a TS, as a TS with higher noise levels will exhibit smaller AC in their absolute values. The IAT and its absolute version (IAAT) are defined as\n$T_{IAT} = 1 + 2\\sum_{k=1}^{\\infty} \\rho_k, T_{IAAT} = 1 + 2\\sum_{k=1}^{\\infty} |\\rho_k|$,\nrespectively, where pk is an AC at lag k. Note that this computation needs to be truncated at a practical lag where the ACF values are essentially zero.\nLag-one autocorrelation (LagAC) [37]. LagAC (p1) is a measure of AC that quantifies the correlation between adjacent observations in a TS, which is defined as\n$\\rho_1 = \\frac{\\sum_{t=1}^{n-1} (x_t - \\bar{x}) (x_{t+1} - \\bar{x})}{\\sum_{t=1}^{n-1} (x_t - \\bar{x})^2}$,$\nwhere xt and xt+1 are consecutive observations in the TS, 2 is the mean of the TS observations, and n is the total number of observations in the TS.\nVariance of autocorrelation (VarAC) [37]. VarAC (\u03c32) measures the variance of the AC over different lags, which is defined as\n$\\sigma_\\rho^2 = \\frac{1}{m} \\sum_{k=1}^{m} (\\rho_k - \\bar{\\rho})^2$,$\nwhere p is the mean of the AC over m lags, and m is the total number of lags considered."}, {"title": "H Robustness to Statistics of Non-stationarity", "content": "Table H.1 presents the CRPS on forecasting tasks with eight datasets using various statistics representing non-stationarity. The result demonstrates our method's robustness across various statistics."}, {"title": "K Comparison with Other Noise Schedules", "content": "Table K.1 shows the comparison of our method with a cosine schedule [24] and a recently proposed method [21] in the computer vision domain that utilizes a schedule enforcing a zero signal-to-noise ratio (SNR) at the terminal step and employs v-prediction [27]. Note that we tune the hyperparameters for the above methods using the same range as our method. The results show that our method outperforms the two other methods across all eight datasets in terms of CRPS on forecasting tasks, emphasizing the importance of using an adaptive schedule that considers the rate of data corruption during the forward process."}, {"title": "L IAAT for Multivariate TS", "content": "IAAT for multivariate TS can be applied in two different ways: (1) by calculating IAAT for each variable individually, and (2) by calculating a single IAAT that considers all variables together, which we denote as mIAAT (multivariate IAAT).\nCalculation of mIAAT. First, the autocorrelation function is defined for each pair of variables at different lags. For a multivariate TS X\u2081 = (x11, X21, ..., Xdl) with d variables of length L, the pat lag k for variable pairs (i, j) is computed as\n$\\rho_k^{(i,j)} = \\frac{\\sum_{t=1}^{L-k} (x_{il} - \\bar{x_i}) (x_{j,l+k} - \\bar{x_j})}{\\sqrt{\\sum_{l=1}^{L-k} (x_{il} - \\bar{x_i})^2 \\sum_{l=1}^{L-k} (x_{jl} - \\bar{x_j})^2}}$,\nwhere xi and ; are the means of the TS for variables i and j, respectively.\nThen, for each variable X(d), IAAT is calculated by computing the sum of autocorrelations across all lags and variables:\n$T_{IAAT}^{(i)} = 1 + 2\\sum_{k=1}^{\\infty} \\sum_{j=1}^{d} \\rho_k^{(i)})$.$\nTo aggregate the IAAT across multiple variables, a weighted average of the IAAT values for individual variables can be employed. In this method, the weights are determined by the variances of the individual variables, thereby assigning greater importance to variables with higher variance. Consequently, the IAAT for the multivariate TS (mIAAT) can be calculated as follows:\n$T_{mIAAT}= \\frac{\\sum_{i=1}^{d} (T_{IAAT}^{(i)}*\\sigma_i^2)}{\\sum_{i=1}^{d} \\sigma_i^2}$,$\nwhere \u03c32 is the variance of the i-th variable."}, {"title": "E Pseudocode for ANT Score", "content": "Algorithm 1 shows the pseudocode for calculating the ANT score, which evaluates the suitability of schedules for a given dataset.\nAlgorithm 1 Calculation of ANT score\nN\nInput: Dataset D = {x}1, Schedule s = (schedule function: f, temperature: 7, number of steps: T)\n1: for i = 1 to N in parallel do\n2:\nx:= Xi\n3:\nfort = 1 to T do\n4:\nAdd noise: x ~ q(x | x\u00af\u00b9) based on s\n5:\nCalculate non-stationarity: g = G(x), where G is a non-stationarity metric\n6:\nend for\n7: end for\n8: Define non-stationarity curve: ls = [l(1), ..., IT)], where It)\n9: Normalize non-stationarity curve: \u00ces = MinMaxScale(ls)\nN\n19\n10: Calculate ANT score: ANT(D, s) = Alinear \u00b7 Anoise \u00b7 Astep,\nwhere linear = d(1*, \u00ces), Anoise = 1\n: 1 + 1)/11), Astep = 1 + 1/T, l* = Linspace(1,0, T)\nOutput: ANT score: ANT(D, s)"}, {"title": "FTS Refinement Methods", "content": "TSDiff offers a strategy to improve predictions from base forecasters, adaptable to any type of base forecaster as it only requires their initial outputs. These initial forecasts are iteratively enhanced through an implicit density learned by the diffusion model, which acts as a prior. The refinement process can be conducted using one of two methods: (a) sampling from an energy function (LMC), or (b) maximizing the likelihood to determine the most probable sequence (ML). Furthermore, the approach includes an option between two types of regularizers: (a) Gaussian (MS) and (b) Laplace negative log-likelihoods (Q).\nEnergy based sampling (LMC). LMC improves the initial forecast of a given base forecaster g, by framing the refinement as a task of sampling from a regularized energy-based model (EBM). This can be expressed as\nEe(y; y) = -log pe (y) + AR(y, \u1ef9),\nwhere y represents the time series formed by combining yobs with g(yobs), R denotes a regularizer, and A is a Lagrange multiplier controlling the strength of regularization.\nTo sample from the specified EBM, overdamped Langevin Monte Carlo (LMC) [33] is used, with y(0) initialized to \u1ef9. The iterative refinement process is outlined as follows:\ny(i+1) = y(i) - n\u2207y(4) \u0395\u03b8 (y(i); \u1ef9) + \u221a2\u03b7\u03b3\u03be; where \u03be\u2081 ~ N(0, I),\nwhere \u03b7 represents the step size, and y is the noise scale.\nMaximizing the likelihood (ML). The refinement procedure can alternatively be viewed as a regularized optimization problem aimed at identifying the most probable TS while adhering to specific constraints on the observed time steps. Formally, this is expressed as\narg min [- log po(y) + AR(y, \u1ef9)],\ny\nwhich can be optimized using gradient descent."}]}