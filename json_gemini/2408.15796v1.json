{"title": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large Language Models", "authors": ["H\u00e9di Zeghidi", "Ludovic Moncla"], "abstract": "This paper evaluates Few-Shot Prompting with Large Language Models for Named Entity Recognition (NER). Traditional NER systems rely on extensive labeled datasets, which are costly and time-consuming to obtain. Few-Shot Prompting or in-context learning enables models to recognize entities with minimal examples. We assess state-of-the-art models like GPT-4 in NER tasks, comparing their few-shot performance to fully supervised benchmarks. Results show that while there is a performance gap, large models excel in adapting to new entity types and domains with very limited data. We also explore the effects of prompt engineering, guided output format and context length on performance. This study underscores Few-Shot Learning's potential to reduce the need for large labeled datasets, enhancing NER scalability and accessibility.", "sections": [{"title": "1 Introduction", "content": "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP), involving the identification and classification of entities such as names, locations, and dates within text. Traditional NER systems typically require large, annotated datasets for effective training, which can be expensive and time-consuming to curate. This limitation hinders the rapid deployment and scalability of NER applications across diverse domains and languages. Few-Shot Learning (FSL) offers a promising alternative by enabling models to perform NER with minimal labeled examples. Leveraging the capabilities of Large Language Models (LLMs), FSL can significantly reduce the dependency on extensive annotated data. This paper investigates the performance of LLMs in NER tasks using FSL, comparing their efficacy to conventional fully supervised methods. Through this evaluation, we aim to highlight the potential of FSL to transform the landscape of NER, making it more accessible and scalable.\n\nDespite their impressive performance in many NLP tasks, NER remains challenging for LLMs (Lu et al., 2024; Gonz\u00e1lez-Gallardo et al., 2024). Mainly because NER is inherently a sequence labeling task, whereas LLMs are primarily designed for text generation. Existing methods utilizing LLMs typically extract only a list of entities from text along with their respective classes. We aim to improve the process by identifying each entity's position in the text and outputting it in JSON format, including both token and character positions.\n\nThis study aims to answer the following question: How effective are FSL techniques in LLMs for performing NER tasks? Our experiments are perform using the GeoEDdA dataset\u00b9 (Moncla et al., 2024) and several LLMs are evaluated and compared to BERT-like model (Devlin et al., 2018) baselines."}, {"title": "2 Related works", "content": "Some works study the zero-shot capabilies of GPT for NER. For instance, Xie et al. (2023) proposed the NER task decomposition into a set of simpler subproblems by labels and perform a decomposed-question-answering task in addition to a syntactic augmentation. However, most of works are investigating FSL. Li and Zhang (2023) investigate the performances of LLMs and smaller pre-trained language models (e.g., BERT, T5) for domain-specific NER. Ashok and Lipton (2023) propose to prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Wang et al. (2023) propose adding special tokens for marking the entities to extract. The evaluation shows that this method achieve comparable results as supervised methods.\n\nFor newly models with expanded context windows, (Agarwal et al., 2024) propose an experimen-"}, {"title": "3 Methodology", "content": "The objective of this preliminary work is to evaluate the capabilities of LLMs on the NER task at the token and span (or entity) levels. For these two tasks, the output must be a JSON format with information for each detected token or span. A span refer to an entity and can be composed of several tokens.\n\nOur methodology relies on a prompt containing descriptions of the task, tagset and one example (with both input and output). A preliminary experiment reveals that LLMs struggle to accurately retrieve or calculate token or span positions from raw input text. Although the output format was correct, the generated position values were inaccurate, resembling hallucinations or random numbers rather than referring to actual token positions, despite appearing as numerical values. To address this issue, we propose a solution that includes tokenization information\u2014specifically, token position details for span detection-within the input data."}, {"title": "4 Experiments and Results", "content": "The experiments\u00b2 are performed using the GeoEDdA dataset (Moncla et al., 2024) which contains semantic annotations (at the token and span levels) for named entities (i.e., Spatial, Person, and Misc), nominal entities, spatial relations, and geographic coordinates. Nested named entities (Gaio and Moncla, 2017) also present in this dataset were not considered in this experiment. One example\u00b3 from the training set containing at least one entity of each class is used for FSL (included in the"}]}