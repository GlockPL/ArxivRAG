{"title": "DOCS: QUANTIFYING WEIGHT SIMILARITY FOR DEEPER INSIGHTS INTO LARGE LANGUAGE MODELS", "authors": ["Zeping Min", "Xinshang Wang"], "abstract": "We introduce a novel index, the Distribution of Cosine Similarity (DOCS), for quantitatively assessing the similarity between weight matrices in Large Language Models (LLMs), aiming to facilitate the analysis of their complex architectures. Leveraging DOCS, our analysis uncovers intriguing patterns in the latest open-source LLMs: adjacent layers frequently exhibit high weight similarity and tend to form clusters, suggesting depth-wise functional specialization. Additionally, we prove that DOCS is theoretically effective in quantifying similarity for orthogonal matrices, a crucial aspect given the prevalence of orthogonal initializations in LLMs. This research contributes to a deeper understanding of LLM architecture and behavior, offering tools with potential implications for developing more efficient and interpretable models.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs), built on transformer architectures (Vaswani et al., 2017), have ushered in a new era in natural language processing (Brown et al., 2020). These complex models have demonstrated remarkable capabilities, but understanding their underlying mechanisms remains a challenge. Similarity analysis techniques (Raghu et al., 2017; Morcos et al., 2018; Kornblith et al., 2019) offer a promising approach for gaining insights into the learned representations and computational processes within these models.\nIn this work, we extend the application of similarity analysis by directly examining the weight matrices of various LLMs\u00b9, instead of focusing on representations. By analyzing the weights themselves, we aim to uncover deeper insights into the model's structure and functionality that are not apparent from representations alone.\nWhile prior research has explored many methods for characterizing the similarity of neural networks (Wu et al., 2020; Khosla & Williams, 2024; Kriegeskorte et al., 2008; Klabunde et al., 2023b; Wang et al., 2020; Barannikov et al., 2021; Hamilton et al., 2016; Rahamim & Belinkov, 2024; Tang et al., 2020; Camastra & Staiano, 2016; Wang et al., 2018; Raghu et al., 2017; Morcos et al., 2018; Kornblith et al., 2019), these methods are often not applicable to measuring similarity between weight matrices due to the following two key factors. For further discussion, see Appendix E.\n1. Focus on Representation, Not Weights: Similar representations across layers do not necessarily imply similar weight matrices. This discrepancy arises from the use of residual connections in transformer architectures (He et al., 2016), which create shortcuts that allow information to bypass layer transformations. Mathematically, a residual connection is represented as\n y = F(x, W) + x, (1)\nwhere x is the layer's input, W represents the weight matrices, F is the transformation function (including the feedforward network and attention), and y is the layer's output. As shown in equation 1, the output y directly depends on the input x. While residual connections mitigate issues such as vanishing gradients during training, this direct dependence"}, {"title": "MATHEMATICAL PROPERTIES OF SIMILARITY INDICES", "content": "When evaluating similarity indices for neural network weights, especially in the context of large language models, a critical property to consider is their ability to discriminate between orthogonal matrices. Orthogonal matrices play a significant role in neural network initialization and training. They are often used to improve training stability and preserve gradient flow. Even after optimization, weight matrices may retain orthogonality properties (Tian et al., 2023). Therefore, a similarity index that can distinguish between different orthogonal matrices is essential for capturing meaningful variations in weight matrices.\nWe categorize the behavior of similarity indices concerning orthogonal matrices into three classes:\nDefinition 1 (Constant on Orthogonal Matrices). An index S is constant on orthogonal matrices if there exists a constant C\u2208 R such that for all n \u2208 N and all orthogonal matrices X,Y \u2208 Rn\u00d7n, we have S(X, Y) = C."}, {"title": "DISTRIBUTION OF COSINE SIMILARITY", "content": "Our DOCS method operates by comparing the weight matrices of two components (e.g., feed- forward networks, attention heads) within a neural network. Each component is represented by a matrix where columns correspond to individual parameter vectors (e.g., neuron weights, attention patterns). The key idea is to quantify how well the parameters from one component align with those from another, based on their vector representations."}, {"title": "THEORETICAL JUSTIFICATION", "content": "We establish that DOCS can distinguish between orthogonal matrices, a capability that existing similarity indices lack (see Section 2). The following theorem demonstrates that DOCS not only meets the Definition 3 of being discriminative on orthogonal matrices but also achieves a stronger level of distinction through a constructive proof.\nTheorem 1. For n \u2265 2, there exist m = \u03a9(n) and column-orthogonal matrices X,Y \u2208 Rnxm such that their Frobenius norm difference and DOCS similarity satisfy:\n ||X \u2013 Y ||F = \u03a9(\u221am), and Spocs(X,Y) =  1/\u221am\nThe proof of the theorem is deferred to Appendix C. The intuition behind the proof is to construct the matrices X and Y with orthonormal columns but differing structures to highlight their dissimilarity. The matrix X is built from standard basis vectors, while Y leverages a normalized Hadamard matrix to ensure orthonormality. By calculating the Frobenius norm of the difference X \u2013 Y, we show that it scales as \u221am. Meanwhile, the DOCS value between X and Y is controlled by the normalized entries of the Hadamard matrix, demonstrating that their cosine similarity is small, on the order of 1/\u221am. This gap between the large Frobenius norm and small DOCS establishes the desired properties.\nThis theorem proves the existence of column-orthogonal matrices with significant differences ||X \u2013 Y||F = \u03a9(\u221am). Unlike existing methods, DOCS similarity effectively captures these differences Spocs(X,Y) = 1/\u221am, demonstrating its superior discriminative power for orthogonal matrices (see Table 1). An illustrative example is provided in Appendix A.5."}, {"title": "EXPERIMENTS", "content": "We conduct experiments to demonstrate the capabilities of DOCS and to gain insights into the internal structure of LLMs. In LLM implementations\u00b2, the rows of a weight matrix correspond to output dimensions, and the columns correspond to input dimensions. To align the column vectors with meaningful entities (e.g., neuron weights), we transpose Wu, Wk, Wq, and MLP-UP before computing DOCS scores. In the MoE experiment, W\u2081 and W3 (Jiang et al., 2024) are also transposed for consistency."}, {"title": "COMPARISON OF SIMILARITY INDICES", "content": "Figure 2 provides a visual comparison of eight different similarity indices applied to the MLP-UP layers of the Meta-Llama-3.1-8B-Instruct model.\nIn these visualizations, we observe that Linear Regression, Canonical Correlation Analysis (CCA), and CCA (Nuclear) indices fail to exhibit clear structural patterns, with their heatmaps appearing noisy. This suggests that they may have limitations in accurately capturing meaningful relationships between the parameters of different transformer layers. The observed noise could be indicative of their reduced sensitivity to the underlying layer similarities.\nOn the other hand, the similarity indices Singular Vector CCA (SVCCA), SVCCA (Nuclear), and Linear Centered Kernel Alignment (Linear CKA) display more discernible patterns. Specifically, they exhibit block-diagonal structures, which suggests that the layers in close proximity share higher similarity. This behavior aligns with our expectations, as neighboring layers within transformer models often exhibit greater parameter correlation due to their sequential processing of information. However, we also observe the presence of minor blocks and stripes in the off-diagonal regions, which could be attributed to biases or noise in the similarity indices. These artifacts may result from the indices' reduced ability to differentiate orthogonal matrices, as elaborated in Section 2.\nIn contrast, the heatmap for DOCS exhibits a clear structure along the diagonal, demonstrating its effectiveness in identifying similar layers with minimal interference from noisy or unrelated signals. Furthermore, we calculated the Gini coefficients of the similarity heatmaps obtained through each method (calculation details are provided in Appendix H). A higher Gini coefficient indicates a more uneven distribution of similarity scores, signifying a greater concentration of significant similarities in fewer layer pairs. This concentration potentially highlights structural characteristics of the model parameters. As shown in Table 2, DOCS achieves the highest Gini coefficient among all the evaluated similarity indices. This result suggests that DOCS excels in revealing structural characteristics of the model parameters."}, {"title": "NEIGHBORING LAYERS EXHIBIT SIMILAR WEIGHTS", "content": "We investigated the similarity patterns between neighboring transformer layers by analyzing various weight matrices (Wu, Wk, Wq, W., MLP-UP, MLP-DOWN) in various LLMs. We employed DOCS to compute and visualize these similarities. Figure 3 illustrates the results for Wk, Wq, and MLP-DOWN on gemma-2-27b-it.\nThe heatmaps clearly show that adjacent layers exhibit higher similarity scores. Scatter plots in Figures (d), (e), and (f) further support this, displaying a decreasing trend in similarity as the absolute layer index difference increases. The shaded areas, representing one standard deviation around the mean similarity, reinforce this observation.\nInterestingly, the first and last layers\u2014the most distant ones\u2014also display higher similarity, as indicated by upward trends at the ends of the scatter plots. We hypothesize that this is because both layers are closely connected to the token embeddings. Similar patterns were observed for other weight matrices and across different LLMs."}, {"title": "CLUSTERS OF SIMILAR LAYERS", "content": "We examined clusters of similar consecutive layers within LLMs. Figure 4 shows heatmaps of DOCS scores for the W\u2082 matrices. They reveal clusters represented by light regions\u2014each containing multiple layers with high mutual DOCS scores. The bottom row displays the average DOCS scores for diagonal blocks ranging from 3 \u00d7 3 to 7 \u00d7 7, providing a clearer view of cluster locations and their similarity levels.\nAn interesting pattern emerges: each figure shows two clusters of layers. The first cluster, located in the middle depths (centered around layer 19 in gemma-2-9b and gemma-2-27b, and around layer 10 in Llama-3.1-8B and Mixtral-8x7B), is the most distinct. The second cluster appears in the last layers (after layer 27 in gemma-2-9b, after 33 in gemma-2-27b, after 21 in Llama-3.1-8B, and after 23 in Mixtral-8x7B), though it is less pronounced in some models. This phenomenon, observed across different model sizes and even vendors, suggests a universal structural pattern resulting from LLM training. The consistent appearance of these clusters indicates that the training process leads to the formation of distinct functional groups of layers within the model.\nDifferent models, however, may exhibit a varying number of clusters. For example, Figure 5 shows that Llama-3.1-70B contains multiple clusters."}, {"title": "BASE VS. INSTRUCT MODELS", "content": "We used the DOCS index to measure changes in weight matrices between base and instruction-tuned models, examining LLM families including yi-1.5, Llama-3.1, and gemma-2. Figure 6 illustrates the results. Our analysis reveals that all DOCS scores are notably high, with values exceeding 0.7 for every matrix evaluated. This indicates that the base and instruction-tuned models largely retain the same foundational knowledge after the fine-tuning process.\nOn the other hand, a notable observation from our results is the tendency of the weight matrices to cluster into three distinct groups based on the trends observed in their DOCS scores. Specifically,"}, {"title": "MOE EXPERIMENT", "content": "We analyzed the weights of different experts in Mixtral-8x7B using DOCS to generate similarity heatmaps for the expert weight matrices (W1, W2, W3) in the MoE network. The outcomes are visualized in Figure 7.\nIn many model layers, typically just one expert stands out, indicated by dark grids that show clear separation from the others. Figures 7a, 7b and 7c illustrate this with the third expert's prominent dark intersecting lines, highlighting its uniqueness. This suggests some experts may have specialized roles, differing in function or behavior within the architecture. We hypothesize that this may be related to data load imbalance during the MoE training process, where a large portion of the data concentrates on a single expert (Dai et al., 2022; Zuo et al., 2021)."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "This work introduces DOCS, a novel index for quantifying weight similarity in large language models. Unlike existing similarity indices, DOCS effectively differentiates orthogonal matrices, addressing a key limitation and enabling deeper insights into the internal structures of LLMs. Our experiments uncovered meaningful patterns, including high similarity between neighboring layers and the presence of similar layer clusters. These findings underscore the potential of DOCS to guide applications in several ways. First, the identified cluster structures could be leveraged to introduce inductive biases during the supervised fine-tuning stage of parameter-efficient techniques. Second, the clustering results offer valuable information for designing sparsity patterns aimed at model compression. By targeting redundant connections within clustered layers, we may be able to significantly reduce model size and computational costs without compromising performance. Third, these findings can inform more efficient knowledge distillation strategies by identifying critical layers that deserve prioritization in the distillation process. A student model could focus on replicating the most representative layers within each cluster, thereby alleviating the computational overhead associated with emulating the teacher model's entire architecture."}, {"title": "A PROOFS OF MATHEMATICAL PROPERTIES OF DOCS SIMILARITY INDICES", "content": "A.1 PROOF OF PERMUTATION TRANSFORMATION INVARIANCE\nLemma 1 (Permutation Transformation Invariance). Let X, Y \u2208 Rn\u00d7m and let Px, Py \u2208 Rmxm be permutation matrices. Then the DOCS similarity index is invariant under permutation transformations:\n S(X,Y) = S(XPx,YPy).\nProof. We aim to show that the DOCS similarity index is invariant under permutation transformations:\n SDOCS (X, Y) = Spocs(XPx, YPY),\nwhere Px and Py are permutation matrices of size m \u00d7 m.\nFirst, recall that multiplying a matrix by a permutation matrix permutes its columns. Specifically, if \u03c3\u03c7 is the permutation associated with Px, then:\n XPx = [Xox (1), \u03a7\u03c3\u03c7 (2), ..., \u03a7\u03c3\u03c7(m)],\nwhere Xox(j) denotes the ox(j)-th column of X. Similarly, YPy permutes the columns of Y according to the permutation \u03c3\u03b3:\n YPy = [\u03a5\u03c3\u03b3 (1), \u03a5\u03c3\u03b3 (2), ..., \u03a5\u03c3\u03b3 (m)].\nNext, compute the cosine similarity matrix C \u2208 Rm\u00d7m between X and Y:\nCjk = XTYk / ||Xj || || Yk ||\nwhere X and Ye are the j-th and k-th columns of X and Y, respectively.\nWhen we compute the cosine similarity matrix C' between the permuted matrices XPx and YPY, we get:\nC'jk = (XPx)T (YPY)k / ||(XPx)j|| ||(YPY)k|| = Xox (j) \u03a5\u03c3\u03b3 (k) / ||Xox (j)|| ||Yoy (k) ||\u02d9\nThis shows that Cjk = Cox(j), \u03c3y (k). In other words, C' is a reordering of the entries of C based on the permutations \u03c3\u03c7 and \u03c3\u03b3.\nIn the MAXCOSSIM function of the DOCS algorithm 1, for each j, we compute:\nsx, = maxk |Cjk|.\nSimilarly, for the permuted matrices, we have:\nS(XPx)j = maxk |C'jk | = maxk |Cox (j), \u03c3y (k)|.\nSince \u03c3y is a permutation of {1, 2, . . ., m}, as k ranges over 1 to m, so does \u03c3y (k). Therefore:\nS(XPx); = max |Cox (j), \u03c3y (k) = max |Cox (j), k| = $Xox(j)*\nThis means that 8(XPx); is equal to 8Xox(j), indicating that the sequence {$(XPx); } is a permutation of {sx;} based on \u03c3\u03c7.\nSimilarly, for Y and its permutation YPy, we find:\nS(YPY)k = max |Ckj| = max | max |Cox (j), \u03c3y (k) | = max |Cj,oy (k)| = 8=SYoy (k)\nThus, 8(YPY)k is equal to SY\u00f8y (k), so the sequence {$(YPY)k } is a permutation of {syx } based on \u03c3\u03b3."}, {"title": "B PROOFS OF NON-DISCRIMINATIVE NATURE OF OTHER SIMILARITY INDICES", "content": "In this section, we present the proofs demonstrating that other common similarity indices are either constant or depend only on the dimensions of the matrices when applied to orthogonal matrices.\nB.1 LINEAR REGRESSION\nThe Linear Regression Similarity is defined as:\n SLR(X, Y) =  ||QFX || / ||X||\nwhere Qx and Qy are orthonormal bases for the columns of X and Y, respectively.\nLemma 6. For any orthogonal matrices X, Y \u2208 Rn\u00d7n,\n SLR(X,Y) = ||QFX || / ||X||\nis a constant that is independent of n.\nProof. Since X and Y are orthogonal matrices, their columns form orthonormal bases. Simply let Qx = X and Qy = Y.\nWe begin by evaluating the Frobenius norm squared of QFX :\n ||QFX || = || YTX || = trace ((YTX)T(YTX)) = trace (XTYYTX).\nSince Y is orthogonal, YYT I. Thus, the expression simplifies to:\ntrace (XX) = trace(I) = n.\nNext, we compute the Frobenius norm squared of X:\n||X|| = trace (XTX) = trace (I) = n.\nTherefore, the ratio is:\n ||QFX|| / ||X|| = n / n = 1."}, {"title": "E FURTHER DISCUSSIONS ON THE MOTIVATIONS OF DOCS", "content": "In this section, we provide additional justifications regarding (i) why we focus on weight similarity rather than representational similarity, (ii) the orthogonality of weight matrices in large language models, and (iii) the advantages of DOCS compared to other existing similarity indices.\nE.1 WEIGHT SIMILARITY VS. REPRESENTATIONAL SIMILARITY\nAs introduced in Section 1, while prior research has explored many methods for characterizing the similarity of neural networks (Wu et al., 2020; Khosla & Williams, 2024; Kriegeskorte et al., 2008; Klabunde et al., 2023b; Wang et al., 2020; Barannikov et al., 2021; Hamilton et al., 2016; Rahamim & Belinkov, 2024; Tang et al., 2020; Camastra & Staiano, 2016; Wang et al., 2018; Raghu et al., 2017; Morcos et al., 2018; Kornblith et al., 2019), these methods often focus on representational similarity rather than weight similarity.\nSimilar representations across layers do not necessarily imply similar weight matrices. This discrepancy arises from the use of residual connections in transformer architectures (He et al., 2016). This is evidenced by Figures 1a and 1b, which show that the input and output of the feedforward network have similar patterns of representational similarity\nHere, we present another example where weight similarity reveals an intriguing finding that would otherwise be overlooked. We plot both the representational and weight similarity indices for the 01-ai/Yi-1.5-9B-Chat model. On one hand, we use the Linear CKA method to calculate the similarity between the outputs of MLP-UP layers. On the other hand, we employ the DOCS method to calculate the weight similarity of the MLP-UP layers. The experimental results are shown in Figure 8.\nThe results underscore the strengths of DOCS in revealing the weight structure. Specifically, DOCS reveals an intricate pattern in Figure 8a, indicating that layer 9 is highly similar to layer 25, layer 10 is highly similar to layer 26, layer 11 is highly similar to layer 27, and so on. This suggests a repetition of a section of layers within 01-ai/Yi-1.5-9B-Chat, possibly due to a specific training strategy employed to save training costs.\nIn contrast, Figures 8b, 8c, and 8d present the representational similarity computed by Linear CKA using different input sequences. Despite using multiple input sentences for the analysis, the results provide a limited understanding of the weight structure. They exhibit relatively homogeneous patterns, lacking the fine-grained layer correspondence seen with DOCS. Therefore, in this example, focusing on weight similarity deepens our understanding of the model."}, {"title": "E.2 FURTHER EXPERIMENTS ON THE NON-DISCRIMINATIVE NATURE OF OTHER SIMILARITY INDICES", "content": "In Section 1, we emphasized that many existing similarity indices, such as Canonical Correlation Analysis (CCA) (Ramsay et al., 1984; Morcos et al., 2018), Singular Vector Canonical Correlation Analysis (SVCCA) (Raghu et al., 2017), and Linear Centered Kernel Alignment (Linear CKA)"}]}