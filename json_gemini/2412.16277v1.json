{"title": "Mapping the Mind of an Instruction-based Image Editing using SMILE", "authors": ["Zeinab Dehghani", "Koorosh Aslansefat", "Adil Mehmood Khan", "Ad\u00edn Ram\u00edrez Rivera", "Franky George", "Muhammad Khalid"], "abstract": "Despite recent advancements in instruction-based Image Editing models for generating high-quality images, they are known as black boxes and a significant barrier to transparency and user trust. To solve this issue, we introduce SMILE (Statistical Model-agnostic Interpretability with Local Explanations), a novel model-agnostic for localized interpretability that provides a visual heatmap to clarify the textual elements' influence on image-generating models. We applied our method to various Instruction-based Image Editing models like Instruct-pix2pix, Img2Img-Turbo and Diffusers-Inpaint and showed how our model can improve interpretability and reliability. In addition, we use stability, accuracy, fidelity, and consistency metrics to evaluate our method. These findings indicate the exciting potential of model-agnostic interpretability for reliability and trustworthiness in critical applications such as healthcare and autonomous driving, while encouraging additional investigation into the importance of interpretability in enhancing reliable image editing models.", "sections": [{"title": "I. INTRODUCTION", "content": "THE instruct image editing models have transformed digital content creation by enabling intuitive, text-based modifications to images [1], making complex adjustments accessible across fields such as creative industries [2], healthcare [3], marketing [4], education [5], and autonomous driving [6].\nThese tools, built on advanced machine learning models, particularly deep neural networks (DNNs), allow users to make detailed edits using simple natural language commands [7]. Systems such as Instruct-Pix2Pix [8] use natural language processing (NLP) approaches to match user input with image information. This is achieved using vision-language models, such as CLIP [9]. Lastly, they implement adjustments using generative models such as GANs or Diffusion Models [10], [11].\nThe problem is that these DNN-based editing systems are still mostly seen as \"black boxes\u201d and have limited interpretability [12]. This lack of transparency raises significant concerns, particularly in fields where the consequences of model decisions are critical. For example, doctors need to fully understand a model that suggests editing medical images to ensure that critical information is not lost or misrepresented [13]. Similarly, in autonomous driving, the decisions made by models that process visual input could directly affect safety, which makes it crucial to understand how and why specific edits or interpretations are made [14].\nThe main challenge with these models is their opacity in decision-making. For example, the black-box nature of these models makes it difficult to understand how they interpret and execute commands. For example, when a command like \"remove the pedestrian\" fails, it is unclear whether the model misunderstood the instruction, lacked capability, or encountered another issue [12].\nThis also leads to difficulty debugging, as with insight into the model's internal processes, identifying the source of errors becomes more accessible, making it easier to refine and improve performance effectively [15]. Additionally, there needs to be more efficiency in optimizing commands, as optimizing phrasing becomes a trial-and-error process. Knowing whether to use terms like \"make\" or \"must\" requires extensive testing because it is unclear what the model responds to best [16]. Moreover, the model's hidden internal logic of the model can result in unpredictable performance, causing inconsistency with new or similar commands [17].\nResearchers have developed various methods to address these challenges to improve model explainability. One used approach is LIME (Local Interpretable Model-agnostic Explanations), which approximates the model's behavior locally around a specific prediction using a more straightforward, interpretable model [18]. This method helps users determine which input features, like specific parts of an image, had the most significant impact on the model's decision.\nBayLIME takes things further by building on LIME and adding Bayesian inference. This means it offers multiple possible explanations while considering uncertainty in the model's predictions [19]. It is an upgrade that makes the whole interpretability process even more dependable.\nAnother popular technique is SHAP (Shapley et al.), which uses a game-theoretic framework to assign each feature a \"Shapley value.\u201dThis value represents the feature's contribution to the final prediction [20]. SHAP calculates these values by analyzing all possible combinations of characteristics, ensuring consistent and interpretable explanations [21].\nIn our work, we propose SMILE (Statistical Model-agnostic Interpretability with Local Explanations) [22], a novel interpretability method that builds on LIME by integrating Empirical Cumulative Distribution Function (ECDF) statistical distances, providing greater robustness and improved interpretability.\nSMILE generates visual heatmaps that highlight the influence of each word in a text command on the image editing process. When a model is given both an image and a textual instruction, the heatmap will visually display the weight or importance of each word in driving the specific modifications made to the image. This approach gives users a visual map showing how different text parts influence the model's editing choices. It is a great way to understand better the connection between the text input and the following visual changes."}, {"title": "II. LITERATURE REVIEW", "content": "This section explores key ideas and recent advancements in instruction-based image editing and explainable AI (XAI), focusing on how they can improve transparency in these models. The discussion is divided into three main areas:\nThe first area instructs image editing models that allow users to modify images using natural language commands. Here, we provide an overview of these models, including their core technologies, input types and some information. The second area, Explainable Artificial Intelligence (XAI), addresses the critical need for interpretability in complex AI methods. This section covers various explainability methods, focusing on LIME(Local Interpretable Model-agnostic Explanation) and recent extensions to LIME [18].\nThe third area, Explainable Instruction Image Editing, focuses on the unique challenges of making instruction-based image editing models interpretable [28], [29], [30].\nInstruction-based image editing modifies an input image according to specific user instructions [31]. This task involves providing an instruction that guides the transformation of the original image into a new design that aligns with the given directive [8]. Text-guided image editing enhances the controllability and accessibility of visual manipulation by allowing users to make changes through natural language commands [32]. Advanced models use large-scale training to perform precise edits efficiently [1], [31].\nFor example, Instruct-Pix2Pix [8] uses GANs and diffusion models to make detailed edits based on user input, making it a versatile tool for flexible and specific changes. Similarly, Adobe Firefly expands this capability by working with images, videos, and text to create visually enhanced content, often targeting professional use cases. On the other hand, tools like MagicBrush [33] focus on applying artistic styles and creative effects to images, guided by simple text instructions.\nFor example, SmartEdit [31]utilizes bidirectional interaction to manage complex edits, while MGIE [34] is specifically designed for large-scale, high-resolution input. Img2Img-Turbo [24] addresses significant challenges traditional diffusion-based models face, such as slow inference times and dependence on paired datasets. It employs a single-step diffusion process and a streamlined generator network, resulting in faster inference, reduced over-fitting, and improved preservation of the structures in the input images. The model excels in unpaired tasks like scene translation (e.g., day-to-night and weather"}, {"title": "III. PROBLEM DEFINITION", "content": "In complex deep learning models, extensive language and text-based image editing models, interpretability poses a significant challenge. Due to the Enormous number of parameters and complex processing mechanisms, these models essentially act as black boxes, making it problematic for users to understand why and how the model responds to certain inputs or shows specific behaviors [12], [44]. The need for interpretability poses a severe challenge to these models' safe deployment and control. It makes it harder to manage them effectively and increases the risk of unexpected or undesirable behavior.\nFor instance, to improve the interpretability of large language models, the Anthropic team applied Sparse Autoencoders to extract semantic and abstract features from these models. This technique enabled them to identify critical concepts and internal behaviors, thus providing a more transparent view of how these models operate [82].\nInspired by this approach and recognizing the importance of interpretability in text-based image editing models, we present an innovative, model-agnostic approach for evaluating and enhancing the interpretability of these models. We leverage SMILE (Statistical et al. with Local Explanations) as a foundational tool to generate localized explanations and visual heatmaps. These heatmaps illustrate the influence of specific text elements on the image generation process, helping users better understand the relationship between textual inputs and visual outputs [22]. This model-agnostic method is crucial for making instruction image editing models more transparent and trustworthy.\nIn addition, to further examine the model's interpretability, We incorporated a t-SNE scatter plot, as you can see in Fig. 3 to visualize the embeddings of images modified with various descriptive sentences, each embedding color-coded based on specific keywords. In this plot, images generated with sentences containing the keyword \"sunny\" are represented as orange points, \"rainy\" as green points, \u201cfoggy\" as yellow points, \"snowy\" as gray points, and \u201cnight\u201d as purple points. Additional points generated from sentences with perturbed texts lacking targeted keywords are represented as dark blue points.\nThis visualization shows that embeddings of each keyword create discrete clusters, clearly separated from perturbation points without specific keywords. This separation suggests that a surrogate model, like weighted linear regression, could effectively capture the model's behavior. Moreover, the regression coefficients provide a simple, interpretable measure of each keyword's importance, showing how they influence the model's output. This method enhances interpretability by offering a clear, quantitative view of how specific textual components impact the generated images."}, {"title": "IV. PROPOSED METHOD", "content": "The proposed method improves the interpretability of instruction image editing models by focusing on understanding how specific text inputs influence the generated images [8]. Analyzing the relationship between textual prompts and visual output makes the behavior of the model more transparent and predictable [32].\nIn the context of image classification, as you can see in Fig. 4, SMILE is a tool that explains why a model makes confident predictions by isolating specific parts of an image that influence the decision. SMILE starts by segmenting the image into \"super-pixels,\" cohesive clusters of pixels representing distinct image parts. It then creates multiple altered versions of the image by randomly keeping or removing different super-pixels as perturbations, generating images highlighting different regions of the original.\nFor each of these perturbed images, SMILE records the model's predictions. It calculates a similarity score with the original image using a statistical measure based on the Empirical Cumulative Distribution Function (ECDF), such as the Wasserstein distance. These similarity scores are then mapped to weights using a kernel function, which transforms the distances to fall within a specified range, allowing for a balanced weighting scheme.\nFinally, SMILE uses the perturbations, predictions, and computed weights to train a weighted linear regression model. This model identifies the superpixels that most strongly influence the model's final decision, pinpointing specific shapes, colors, or textures as key factors in the prediction. The resulting explanation clearly shows which parts of the image were vital in the model's prediction. Inspired by SMILE's approach, we adapted this methodology to interpret how text prompts influence image generation in our work.\nInspired by SMILE's capabilities, we propose a similar method for interpreting text-prompt-based image generation models as demonstrated in Fig. 5. Instead of perturbing the generated image, as done in SMILE, we modify the instruction by including or excluding certain words. We generate a corresponding image for each altered text prompt and compute the Wasserstein distance between these images and the one produced by the original text prompt. This distance is a similarity measure, allowing us to identify which words in the text prompt have the most significant impact on the generated image.\nThese similarity scores are then used as the outcome variable in a weighted linear regression model, with weights based on the text distances and perturbations. This regression model helps determine how each word affects the finished image. The coefficients derived from this model quantify the influence of each word, which we use to create a visual heatmap highlighting the most influential words in the text prompt. This heatmap provides an intuitive visual representation of which prompt elements contribute most to the generated output.\nBy employing this enhanced three-step process that builds on SMILE's methodology, we aim to help users understand how particular elements of text prompts impact image generation [31]. This technique improves user control and predictability in instruction-driven image editing models by offering insightful information about which elements of textual input-such as stylistic instructions or descriptive terms-impact the final visual outputs most.\nThe first step employs an image editing model based on input text. The image editing model receives the original image input and an corresponding text; only the input text changes each time the model renders the image. The image editing model is expected to generate images similar to the original image that vary only based on critical components of the input texts.\nDifferent text permutations are utilized from the individual words of the input prompt to create various text perturbation blocks after breaking down the prompt into individual words [83]. The instruction-based image editing model generates images based on the original input and the perturbation text.\nWhile the Instruction-Based Image Editing model faces challenges in counting objects and spatial reasoning, these issues can be mitigated by carefully selecting perturbation texts and restricting the input domain for perturbations [84]. Instruction-based image editing helps the proposed method create images that only conflict with the original text regarding keywords related to perturbation texts.\nSecond, To assess similarities between original and modified images, we use the DINOv2 model to generate high-quality, semantic embeddings [85]. Direct image comparisons often capture only surface features like color or pixel structure, overlooking more profound semantic shifts. Embedding images in DINOv2's space enables us to capture visual and conceptual content, allowing for more meaningful comparisons [86].\nThis project aims to develop a model-agnostic, explainable framework for interpreting image edits independent of specific models, such as Instruct-Pix2Pix [23] or DALL-E [26]. DINOv2 is particularly suitable for this purpose due to its self-supervised learning approach, which enables it to generate high-quality, general-purpose image embeddings without relying on labeled data or model-specific outputs. Its adaptability to diverse image types makes it ideal for model-agnostic explainability [85], [86].\nWe use the Wasserstein Distance to quantify these differences to compare the distributions in both image and text embeddings [87], [88]. We use the Wasserstein distance because it provides a more complete measure of the differences between image embeddings compared to other ECDF-based distances like Kolmogorov-Smirnov or Cram\u00e9r-von Mises. While those methods focus on specific aspects, Wasserstein distance captures both broad and subtle changes in the data [87].\nWasserstein Distance is also effective for text embeddings by capturing distributional geometry, enabling us to assess how text perturbations conceptually align with the original prompt [89]."}, {"title": "A. Image Generating and perturbed prompts", "content": "To begin, we generate variations of the original text prompt, known as \"perturbed prompts\" to examine how these subtle changes affect the output of an image-editing model [32]. The original text is first broken down into individual words. Multiple text versions are then created by selectively including or excluding words. Each subset is paired with an input image and provided to the Instruct Image Editing model, which generates a unique image for each prompt. In other words, the input image is repeatedly edited with different text prompts [31].In this process, the image edited with the original sentence is considered the reference, or baseline image, against which all other edited images will be compared.\nFor the original text prompt $P_{org}$, the Instruct Image Editing function $\u00d8_{edit}$ generates an output image $org$ based on the input image $a$, as shown in Eq. 1:\n$d_{org} = edit (P_{org}, C_{input})$\nSimilarly, for each perturbation text $P_{pert}$ within the set $S_{pert}$, the network produces a corresponding edited image $a_{pert}$ according to Eq. 2:\n$a_{pert} = edit (P_{pert}, C_{input}), Vi\u2208 S_{pert}.$\nHere, $P_{org}$ represents the original prompt, $P_{pert}$ describes a perturbation generated from the original text, and $C_{input}$ is the input image used by the Instruct Image Editing model function, $edit$, to generate images a' that reveal the influence of the prompt variations. The set of all perturbation texts is denoted by $S_{pert}$ [18], [8]."}, {"title": "B. Creating the interpretable space", "content": "In order to map the output image space of the Instruct Image Editing model to a one-dimensional space, the Wasserstein distance between each image generated from the perturbed texts and the original text(baseline image) is computed as a measure of similarity [88]. In all cases, the input image for editing remains consistent. A self-supervised feature extraction model, DINOv2, is employed to eliminate redundant information from the image, allowing focus on the primary components of the image [85], [91]. This reduces noise in the distance estimation, enabling the accurately calculated distance to serve as the output for the linear regression model.\nThe distance between the images generated by the perturbation texts and the original text is given by Eq. 3:\n$Wi(d_{org}, prom): = \\sqrt[p]{\\sum_{j=1}^{n} ||Dinov2(v_{org}^{j}) - Dinov2(v_{prom}^{j})||^{p}}$\n$Vi \u2208 S_{prom}$\nIn the above relation, the $Dinov2$ function represents the feature extraction model applied to the images before calculating the distance [85]. In Eq. 3, n represents the number of pixels, p denotes the norm order, and W($@_{org}$, $a_{pert}$) demonstrates the Wasserstein distance between the two images produced by the original and perturbed prompts [92].\nUsing p-values is crucial for validating the significance of the observed Wasserstein distances (WD) between images generated under different prompt perturbations. By calculating a p-value, we test whether the observed WD could have occurred by chance, determining if the perturbation-induced changes are meaningful [93]. To compute this p-value, we use the Bootstrap algorithm [94], [95], which estimates the probability of observing a WD as large as the observed one. This involves generating distributions of WDs through repeated sampling and comparing each sampled WD with the observed value. If a small proportion of sampled WDs exceed the observed WD, the resulting p-value confirms that the differences are significant, supporting the validity of the perturbation effects.\nThe Wasserstein-p metric allows us to adjust WD's sensitivity to different image variations by fine-tuning the norm order p. Smaller p values emphasize local differences, while larger p values capture broader distributional shifts [91]. By testing various p-values, we identify the norm that best captures meaningful variations caused by prompt changes, enhancing the reliability and interpretability of the results [91].\nWhen analyzing Wasserstein distances, measures with high p-values indicating non-significant differences are excluded from the procedure [92]. The Bootstrap algorithm, described in Algorithm 1, calculates these p-values for all relevant ECDF-based distance measures, including WD. The algorithm computes both the WD and its associated p-value.\nFor a univariate example, let X and Y be the inputs. The Bootstrap algorithm performs 1e5 iterations, where XY is the concatenated set of X and Y. In each iteration, two random samples are drawn from XY, and their Wasserstein distance (boostWD) is computed. If boostWD exceeds the observed WD, a counter (bigger) is incremented. After completing all iterations, the p-value (pVal) is calculated as the proportion of iterations where boostWD is greater than the observed WD, as shown in Algorithm 1."}, {"title": "A. Qualitative Results", "content": "The qualitative evaluation demonstrates the interpretability of the proposed framework using scenario-based testing and visualizations. Leveraging the Operational Design Domain (ODD) [109], we generated diverse testing scenarios encompassing environmental factors like weather transformations (e.g., rain, fog, snow) and non-environmental variables (e.g., object attributes, human-centric factors, temporal conditions).\nHeatmaps illustrate how specific keywords influence image edits, as shown in Tables IX and X. Further, we analyzed input prompt explainability by mapping word contributions to visual changes, highlighting the weight of keywords such as"}, {"title": "VI. CONCLUSION", "content": "Interpretability is a crucial aspect of machine learning and deep learning models. Human interaction with AI systems is essential for integrating these systems into the business, public, health, and education sectors. Since people are unlikely to adopt AI systems daily without understanding how decisions are made, developing interpretability solutions for these black box models is vital. Also, the extensive laws passed soon in Al will support the operation of a wide range of systems that could be more understandable to humans. Consequently, businesses that do not strive for interpretable models may face significant legal challenges in the future. The text discusses the importance of enhancing transparency in diffusion-based image editing models, particularly concerning model interpretability. SMILE has been utilized to generate heatmaps that help illustrate how specific textual elements affect image edits, thereby clarifying the model's decision-making process for users. This approach has been evaluated with various image editing models, including Instruct-Pix2Pix and Img2Img-Turbo, and has demonstrated reliable stability, accuracy, fidelity, and consistency. This framework has significant potential to build user trust and transparency, especially in critical fields such as healthcare and autonomous driving, where a clear understanding of model behavior is essential."}, {"title": "VII. FUTURE WORKS", "content": "1) Expanding to Other Generative Models: While our work has primarily focused on Instruct Image Editing models, future work could extend the principles and methodologies to other types of generative models, such as video generation. This could involve exploring both general-purpose video generation models, such as Sora [115], and domain-specific models, like Gaia [116].\n2) Linking the prompt explainability with attention maps proposed by [30]. By utilizing attention maps to visualize and interpret the model's response to specific prompt components, we could provide deeper insight into the relationship between prompt structure and generated output. This integration could help in building more transparent generative systems, as well as enhancing control over the generation process. Further exploration into how attention maps relate to semantic consistency across generated outputs could also improve model interpretability."}]}