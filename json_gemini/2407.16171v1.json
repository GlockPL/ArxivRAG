{"title": "Learning Trimodal Relation for Audio-Visual Question Answering with Missing Modality", "authors": ["Kyu Ri Park", "Hong Joo Lee", "Jung Uk Kim"], "abstract": "Recent Audio-Visual Question Answering (AVQA) methods rely on complete visual and audio input to answer questions accurately. However, in real-world scenarios, issues such as device malfunctions and data transmission errors frequently result in missing audio or visual modality. In such cases, existing AVQA methods suffer significant perfor-mance degradation. In this paper, we propose a framework that ensures robust AVQA performance even when a modality is missing. First, we propose a Relation-aware Missing Modal (RMM) generator with Relation-aware Missing Modal Recalling (RMMR) loss to enhance the ability of the generator to recall missing modal information by understanding the relationships and context among the available modalities. Second, we de-sign an Audio-Visual Relation-aware (AVR) diffusion model with Audio-Visual Enhancing (AVE) loss to further enhance audio-visual features by leveraging the relationships and shared cues between the audio-visual modalities. As a result, our method can provide accurate answers by ef-fectively utilizing available information even when input modalities are missing. We believe our method holds potential applications not only in AVQA research but also in various multi-modal scenarios. The code is available at https://github.com/VisualAIKHU/Missing-AVQA.", "sections": [{"title": "1 Introduction", "content": "In the era of artificial intelligence, research efforts aimed at understanding scenes by integrating multi-modal information have made significant progress. A no-table example in this field is Audio-Visual Question Answering (AVQA), which integrates video, audio, and text inputs to comprehend complex situations and generate responses by assimilating relevant video and audio information based on the questions. It involves extracting salient information from inputs and training networks to identify correlated features for accurate prediction."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Audio-Visual Question Answering", "content": "Recently, several works have used audio, visual, and text modalities for multi-modal scene understanding. Schwartz et al. [38] proposed a baseline for audio-visual scene understanding, consisting of feature extractors, a multi-modal at-tention module, and an answer generation module. Yun et al. introduced the Pano-AVQA network [46] for semantic scene understanding in panoramic videos, proposing spherical spatial embedding methods and using equirectangular and NFOV [10] projections to reduce visual distortion during feature extraction. Li"}, {"title": "2.2 Missing Modality in Multi-modal Learning", "content": "Recently, some works have addressed the missing modality problem in multi-modal learning [13, 17-19, 41, 42]. Woo et al. [42] investigated the effects of ar-chitecture, data augmentation, and regularization under missing modalities and proposed an Action Masked Auto Encoder (ActionMAE) that generates pseudo features of missing modalities for inference. Wang et al. proposed Shared-Specific Feature Modeling (ShaSpec) [41], which extracts shared and modality-specific features to enhance input data representation using shared and specific encoders. Lee et al. [19] introduced a missing-aware prompting method for transformer models that helps the model be aware of the missing modality by attaching missing-aware prompts at the input. Woo et al. [42] also proposed a transformer architecture to fuse features from all modalities into a comprehensive set using hybrid modality-specific encoders, intra-modal transformers, and inter-modal transformers. In the context of autonomous driving, Choi et al. [4] proposed a Shared Cross-modal Embedding method to encode features effectively, address-ing missing modality issues. Additionally, Wu et al. [43] addressed the missing modality problem using a knowledge distillation method with a vision teacher, an auditory teacher, and an audio-visual student. These studies have demonstrated significant success in addressing the missing modality problem. However, their applicability to the AVQA task is limited due to its unique complexities. Ex-isting methods primarily handle modality in one-to-one pairs, overlooking the interdependencies among different modalities. For example, [42] pairs input im-ages, depth images, and IR images to generate estimated features. In contrast, the AVQA task demands a nuanced understanding of the question context to produce relevant information, necessitating the flexible generation of pseudo fea-tures for missing modalities based on the specific question. Consequently, this paper proposes a method to tackle the missing modality problem more effectively by comprehending the context of questions and generating missing modality in-formation that corresponds to the context of the questions."}, {"title": "2.3 Diffusion Based Models", "content": "The diffusion model generates data through an iterative process of adding and removing noise. It consists of a forward process that adds noise at each time step and a reverse process that removes noise at each time step. Diffusion mod-els represent a promising group of generative models that simplify the data generation process into a step-by-step noise reduction technique [7]. Diffusion models have been shown to perform well in several image generation tasks [5], including super-resolution [32, 37, 47], effective image restoration [9], image pro-cessing [12, 28], text conditioning [6, 29, 35], image inpainting [36], and more. Stable diffusion [35] built a DPM (Diffusion Probabilistic Model) in latent space to reduce the number of pixels. Most of these previous works on diffusion take only one modality as input to the diffusion process and derive the output of the same modality. In contrast, our AVR diffusion model integrates audio and visual modalities simultaneously, ensuring effective fusion and thorough learning of the diffusion process for enhanced feature extraction."}, {"title": "3 Methodology", "content": "Fig. 2 shows the overall architecture of the proposed AVQA framework address-ing missing modalities during inference. The visual modal input, audio input, and question pass through their corresponding encoders to obtain fv, fa, ft. Our method consists of three major components: (1) Relation-aware Missing Modal (RMM) generator (see Fig. 2 (a)), (2) Audio-Visual Relation-aware (AVR) dif-fusion model (see Fig. 2 (b)), and (3) AVQA backbone. For example, in the case of audio being missing, the RMM generator generates a pseudo audio feature for the missing modality (e.g., audio) using the existing modalities (e.g., visual and question). Since our work addresses missing scenarios during the inference phase, the RMM generator is trained to learn pseudo audio features that closely"}, {"title": "3.1 Relation-aware Missing Modal Generator", "content": "The Relation-aware Missing Modal (RMM) generator is inspired by the remark-able ability of human brain to integrate and process different types of information from various sensory modalities. When humans encounter content, they can of-ten infer missing audio information by analyzing visual cues and context, and similarly, infer visual information from audio cues. This capability arises from shared cues between different modalities that come from the same object or scene. For instance, when only auditory information of piano sound is provided, humans can perceive the sound and recall the image of a piano (visual informa-tion). Conversely, when watching a muted video of a piano performance, humans can recall the piano sound (auditory information)."}, {"title": "3.2 Audio-Visual Relation-aware Diffusion Model", "content": "In this section, we introduce the proposed Audio-Visual Relation-aware (AVR) diffusion model. The goal of this model is to enhance the feature representation of both the missing modality (e.g., audio) as well as the original counterpart modality (e.g., visual). As illustrated in Fig. 4 (a), the process begins by com-bining the real audio feature (i.e., fa) and the visual feature (i.e., fv) through concatenation, resulting in the combined feature (i.e., fav). This combined fea-ture is then passed through a diffusion process, which includes a forward process q (adding noise) and a reverse process p\u03b8. In the reverse process, p\u03b8 estimates the steps of the forward process in reverse, using the weight parameter \u03b8 of the autoencoder [7]. This allows the AVR diffusion model to learn how to recover the original data from the noise effectively. The forward process q and reverse process p\u03b8 at time step t \u2208 [0, T] is defined as:\n$q(f_{av}^{t} | f_{av}^{t-1}) = N(f_{av}^{t}; \\sqrt{1 - \\beta_t} f_{av}^{t-1}, \\beta_t I), t \\in [1, T]$,\n$p_{\\theta}(f_{av}^{t-1} | f_{av}^{t}) = N(f_{av}^{t-1}; \\mu_{\\theta}(f_{av}^{t}, t), \\Sigma_{\\theta}(f_{av}^{t}, t))$,\nwhere \u03b2t indicates hyper-parameters to control amount of noise at time step t, $\\mu_{\\theta}(f_{av}^{t}, t)$ and $\\Sigma_{\\theta}(f_{av}^{t}, t))$ denote mean and variance, respectively. In this pro-cess, combining features of audio and visual modalities enables mutual informa-tion utilization, resulting in enhanced feature representations. This process is repeated T timesteps. As a result, our AVR diffusion has learned the ability to enhance feature representations for a given input.\nSince we aim to address the missing modalities (audio or visual) in the in-ference phase, we also combine $(f_{ap}, f_v)$ to generate $f_{av}^{p}$ for audio missing (see Fig. 4 (b) left) and $(f_a, f_{vp})$ to generate $f_{av}^{vp}$ for visual missing (see Fig. 4 (b) right). $f_{av}^{p}$ and $f_{av}^{vp}$ go through the reverse process of AVR diffusion to produce $f_{av}^{pu}$ and $f_{av}^{vpu}$. Finally, after leveraging cross-modal knowledge through AVR dif-fusion, the audio and visual features are separated, i.e., $(f_a, f_{vp})$ and $(f_{ap}, f_v)$ to use each individual feature as an input to the AVQA backbone. Note that the existing AVQA networks [20, 21, 38, 46] require individual inputs (audio, visual, question (text)) to answer the questions.\nThe role of our AVR diffusion can be highlighted as follows: (1) Through the diffusion process, AVR diffusion learns the ability to generate the enhanced features for both audio-visual modalities by jointly leveraging cross-modal knowl-edge of $f_{av}^{p}$. (2) Also, the features of the pseudo modality and the counterpart original modality are combined and passed through AVR diffusion to further en-hance their representations, considering the missing case in the inference phase."}, {"title": "Audio-Visual Enhancing Loss", "content": "To guide AVR diffusion can perform the aforementioned roles, we introduce the Audio-Visual Enhancing (AVE) loss Lave,\n$L_{ave} = E_{\\epsilon \\sim N(0, I)}[|| \\hat{\\epsilon}_{\\theta}(f_{av}^{uu}, t) - \\epsilon ||^2]$,\nwhere e denote noise in normal distribution N(0,I) and $\\hat{\\epsilon}_{\\theta}(f_{av}^{uu}, t)$ denotes the prediction of the autoencoder of AVR diffusion at the t-th time step."}, {"title": "3.3 Total Loss", "content": "To learn the AVQA network to perform robustly in missing modality scenarios, the total loss function is defined as follows:\n$L_{avqa} = L_{ce}(f_a, f_v, f_t) + L_{ce}(f_{av}^{u}, f_v, f_t) + L_{ce} (f_a, f_{vv}, f_t)$,\n$L_{Total} = L_{avqa} + \\lambda_1 L_{rmmr} + \\lambda_2 L_{ave}$,\nwhere \u03bb1,2 denote the balancing hyper-parameters, Lavqa denotes the loss func-tion of the AVQA predictors [20, 21, 38, 46], and Lce indicates the cross-entropy loss when (fa, fv, ft), $(f_{av}^{u}, f_v, f_t)$ and (fa, $f_{vv}, f_t)$ pairs are input to AVQA, respectively. With Lavqa and the proposed two loss functions (Lrmmr and Lave), our AVQA framework can provide more accurate answers to questions even in the missing modality situations."}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Experimental Settings", "content": "Dataset. In the experiments, two publicly available open-source datasets, namely MUSIC-AVQA [21] and AVQA [44], were utilized. The MUSIC-AVQA dataset serves as an AVQA benchmark for comprehensive scene understanding in musi-cal performance. It encompasses 45,867 question-answer pairs derived from 9,288 videos. To elaborate further, 32,087, 4,595, and 9,185 question-answer pairs were allocated for training, validation, and testing, respectively. Similarly, the AVQA dataset is a large-scale AVQA dataset designed for reasoning about multiple audio-visual relationships in real-life scenarios. It comprises 57,335 question-answer pairs sourced from 57,015 videos. In accordance with the specifications provided in [44], 34,401, 5,734, and 17,200 question-answer pairs were earmarked for training, validation, and testing, respectively.\nAVQA Network. To verify the capability of the proposed method, we adopted our method to four recently introduced AVQA networks (AVST [21]\u00b9, AVSD [38]\u00b2, Pano-AVQA [46]\u00b3, and PSTP-Net [20]\u2074) for which official code is available. Note that, all our implements are conducted by referring to the official codes. In"}, {"title": "4.2 Evaluation Under Missing Modality", "content": "Results on MUSIC-AVQA Dataset. We adopt the four state-of-the-art AVQA networks, i.e., AVSD [38], Pano-AVQA [46], AVST [21], and PSTP-Net [20] on MUSIC-AVQA dataset [21] to demonstrate the ability of our method in handling missing modalities. As shown in Table 1, when the visual modality is missing, existing AVQA methods struggle to estimate answers, achieving around 51~59%. On the other hand, applying our approach to the existing AVQA net-works significantly improves accuracy. Furthermore, our proposed method ex-hibits even more substantial improvements when the audio modality is missing. The results verify the effectiveness of our method in the missing modalities.\nResults on AVQA Dataset. Furthermore, we extended our experiments to the AVQA dataset [44]. Table 2 shows the results on the AVQA dataset. No-tably, our method remains effective even when one modality is missed. These results demonstrate the efficacy of our proposed approach in compensating for missing modalities. Furthermore, our method exhibits flexibility enough, as it can seamlessly integrate into various existing AVQA network architectures."}, {"title": "4.3 Comparison with Existing Missing Modality Handling Methods", "content": "We compare our method with the state-of-the-art methods [19, 41, 42, 45] that handle the missing modality on the MUSIC-AVQA dataset. We adopt AVST for the base AVQA backbone. As shown in Table 3, even in the visual modality missing and the audio modality missing, our method achieves the highest per-formance in overall accuracy (i.e., 'All Avg' metric). The results show that even in the missing scenario, our RMM generator effectively recall the missing modal information. Also our AVR diffusion further enhances the feature representation of the audio-visual modality by leveraging the cross-modal relation."}, {"title": "4.4 Ablation Study", "content": "Effect of the RMM Generator and AVR Diffusion Model. We con-ducted experiments to evaluate the effectiveness of pseudo features generated by our proposed RMM generator and AVR diffusion model. Table 4 presents the results when features for the missing modality are generated by each module. The absence of the visual modality leads to a decline in accuracy, particularly for questions related to visual information. However, leveraging pseudo features gen-erated by the RMM generator helps alleviate the impact of the missing modality,"}, {"title": "4.5 Discussions", "content": "Varying Missing Ratios. We also experimented with various missing rates to test aspects similar to real-world applications. In Fig. 5 (a), we can see that visual and audio perform consistently well even when one of them is missing with a higher ratio. In Fig. 5 (b), we can see that the probability of missing each modality is not significantly affected, and performs consistently well. Fig. 5 results prove that our method can work robustly in terms of real-world applica-tions such as various missing ratio situations.\nLimitation. We addresses the problem of missing modalities in only inference situations that are executed after training has taken. However, in terms of further real-world applications, it is possible that missing modalities may occur during learning. So in future work, this consideration will lead to the study of AVQA networks that can robustly cope with missing modalities in training situations."}, {"title": "5 Conclusion", "content": "In this work, we introduced a novel Audio-Visual Question Answering (AVQA) framework designed to tackle the challenge of missing modalities in real-world scenarios. Our framework incorporates the Relation-aware Missing Modal (RMM) generator and the Audio-Visual Relation-aware (AVR) diffusion model. The RMM generator generates the pseudo feature of the missing modality, while the AVR diffusion model enhances audio-visual representations. It effectively handles situations where audio or visual information is missing. Through our experiments and comparisons with state-of-the-art AVQA methods, we demon-strated the superior performance of our approach, even in scenarios where one modality is missing. This contributes to enhancing the robustness and accuracy of AVQA networks in real-world environments."}]}