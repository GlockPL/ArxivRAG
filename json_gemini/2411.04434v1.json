{"title": "SCALING LAWS FOR PRE-TRAINING AGENTS AND WORLD MODELS", "authors": ["Tim Pearce", "Tabish Rashid", "Dave Bignell", "Raluca Georgescu", "Sam Devlin", "Katja Hofmann"], "abstract": "The performance of embodied agents has been shown to improve by increasing model parameters, dataset size, and compute. This has been demonstrated in domains from robotics to video games, when generative learning objectives on offline datasets (pre-training) are used to model an agent's behavior (imitation learning) or their environment (world modeling). This paper characterizes the role of scale in these tasks more precisely. Going beyond the simple intuition that 'bigger is better', we show that the same types of power laws found in language modeling (e.g. between loss and optimal model size), also arise in world modeling and imitation learning. However, the coefficients of these laws are heavily influenced by the tokenizer, task & architecture \u2013 this has important implications on the optimal sizing of models and data.", "sections": [{"title": "INTRODUCTION", "content": "Much progress in AI in the early 2020's has been driven by increasing model size, dataset size, and training compute. Whilst conceptually simple, the importance of this practice has led to an emerging subfield studying the science of scaling. This field answers questions such as how to estimate the benefit of increased compute investment, or how to optimally trade-off model and dataset size.\n\nThe role of scale in pre-training is until now best understood in the context of large language models (LLMs). Following the observation that the empirical relationship between loss and key scaling quantities can be accurately described by power laws (Kaplan et al., 2020), ensuing work studied the precise trade-off between model and dataset size (Hoffmann et al., 2022), as well as considerations about inference compute (Sardana and Frankle, 2023), repeated training data (Muennighoff et al., 2024), parameter counting (Pearce and Song, 2024), and more (Section 2).\n\nIn comparison, less is understood about scaling in embodied AI. Recent high-impact works suggest increasing model and dataset size can lead to ever more capable agents for two pre-training objectives; behavior cloning (BC) (Reed et al., 2022; Baker et al., 2022; Brohan et al., 2023) and world modeling (Hafner et al., 2020; Hu et al., 2023; Yang et al., 2023; Bruce et al., 2024). Such works typically demonstrate the benefit of scale through ablations over only a few model sizes, shown in terms of downstream agent performance, confirming the intuition that 'bigger is better' (Sartor and Thompson (2024) provide an aggregated analysis). However, this leaves a large gap to the precise understanding of scale in LLMs, where for a given increase in compute, models can be sized optimally, and their expected performance accurately predicted.\n\nThis paper helps close this gap. Similar to the initial study of scale in LLMs, we focus on the effect of scaling on a generative pre-training loss (rather than on downstream agent performance, or reward- or representation-centric objectives), in the infinite data regime, on a fixed offline dataset. Under this setting, we train families of transformers on next-token prediction tasks using architectures popular in both world modeling and BC tasks. This leads to several contributions, summarized in Figure 1.\n\n\u2022 Scaling laws similar to those in LLMs can be observed in world modeling with tokenized observations and actions (Section 4.1, Figure 1a).\n\n\u2022 The optimal trade-off between model and dataset size in world modeling is influenced by the tokenizer's compression rate (number of tokens per observation) (Section 4.1, Figure la & b)."}, {"title": "RELATED WORK", "content": "Scaling laws origin. The term scaling laws is used throughout the engineering and physical sciences to denote power law relationships between two quantities, e.g. duration of a volcanic eruption and the probability of it continuing (Cannav\u00f2 and Nunnari, 2016). The name derives from the scale-invariant\u00b9 property of power laws. While early work suggested that power laws could be good empirical descriptors of important variables in deep learning (Hestness et al., 2017; Rosenfeld et al., 2019), it was Kaplan et al. (2020) who provided a comprehensive study of power laws in transformer LLMs, and popularized the usage of scaling laws in this context.\n\nScaling laws in LLMs. As the real-world value of LLMs was understood, scaling in LLMs became a high-priority research topic. Hoffmann et al. (2022) conducted a precise analysis into the trade-off of model and dataset size, finding they should be increased in equal proportions. This conflicted with Kaplan et al.'s suggestion that model size should be prioritized an incorrect conclusion that Pearce and Song (2024) showed largely arose from counting only non-embedding parameters.\n\nMany other aspects of LLM scaling analyses are beginning to be refined. Su et al. (2024) revisited the methodology used to find scaling coefficients. H\u00e4gele et al. (2024) found that multiple independent cosine schedules could be reproduced more efficiently through a constant learning rate with multiple short decays, or stochastic weight averaging. Pearce and Song (2024) & Porian et al. (2024) found that well-tuned constant learning rates were sufficient to recover certain coefficients. Bi et al. (2024) study the effect of various hyperparameters on scaling. Muennighoff et al. (2024) looked at repeated epochs, finding up to four epochs produce negligible departures from the infinite data regime. Sardana and Frankle (2023) factored in inference compute to the definition of what is compute-optimal. Isik et al. (2024) study the link between pre-training loss and downstream performance. A further line of research aims to explain why power laws are such a good descriptor of empirical deep learning (Hutter, 2021; Maloney et al., 2022; Bahri et al., 2024).\n\nScaling in embodied AI. Compared to LLMs, our understanding of scale in embodied settings is less advanced. Early successes in competitive games showed that running reinforcement learning (RL) at scale could surpass human performance, e.g. (Silver et al., 2017; Berner et al., 2019). In self-play RL, power laws were observed between certain quantities by Neumann and Gros (2022). Meanwhile, Hilton et al. (2023) noted that, in general, reward signals do not follow power laws, and defined a transformation of reward (intrinsic performance) that created self-consistent scaling laws.\n\nInspired by the effectiveness of scaling in LLMs, embodied AI research has recently begun to explore the effectiveness of generative pre-training objectives on offline datasets, when executed at scale. This includes behavior cloning objectives in video games (Baker et al., 2022; Raad et al., 2024), robotics (Brohan et al., 2022; 2023; Padalkar et al., 2023; Bousmalis et al., 2023), or multiple domains (Reed et al., 2022), as well as world modeling objectives (Hu et al., 2023; Yang et al., 2023; Bruce et al., 2024). In these studies, the benefit of scale is generally shown through increasing model size on a specific downstream task of interest (e.g. measured by completion rate) an aggregated survey is provided by Sartor and Thompson (2024).\n\nWhilst such studies provide strong evidence that scale can be effective in embodied domains, the complexity introduced by downstream task evaluation makes quantifying the effects, and asking more nuanced questions very challenging (e.g. how to trade-off model and dataset size). To address this, we argue that a better starting point for studying scale in these generative pre-training approaches, is to study the effect of scale on pre-training loss. Whilst downstream task performance may be of ultimate interest, LLM research evidences the utility of focusing on this clean intermediate signal, which is more straightforward to analyze. Few prior works have taken such an approach. In Appendix A we contrast our BC efforts with Tuyls et al. (2023) who provide a valuable initial study on the relationship between compute and pre-training loss for BC. We also discuss scaling analyses of image and video modeling, which are related to world modeling."}, {"title": "METHODOLOGY", "content": "This section provides details for our main experiments. We describe the pre-training tasks, architectures, and datasets considered. We also detail the methodology used in the scaling law analyses."}, {"title": "TASKS", "content": "We consider trajectories constructed as sequences of alternating observations ot and actions at for timestep t \u2208 N. In this work, observations are always images, ot \u2208 R3\u00d7w\u00d7h and any continuous actions are discretized during preprocessing leaving, at \u2208 {0,1}da.\n\nGiven this data format, we consider two tasks. World modeling (WM) (Ha and Schmidhuber, 2018) predicts future observations from previous observations and actions. This allows an agent to explicitly understand how its environment works, which can be used for planning, or dyna-style RL (Sutton, 2018). Behavior cloning (BC) predicts the future actions that the dataset's demonstrators take (Bakker et al., 1996). This creates a policy that can be directly used to act in the environment, either as-is or following further fine-tuning. Concretely, these two tasks require modeling the following quantities,\n\nWorld modeling: $P(o_{t+1}|o_t... o_{t-k}, a_t...a_{t-k})$,\nBehavior cloning: $P(a_t|o_t... o_{t-k}, a_{t-1}...a_{t-k-1})$.\n\nThis work focuses on generative pre-training aiming to model this full conditional probability distribution. We leave a study of scaling laws for alternative objectives, e.g., explicitly targeting representation learning (Nair et al., 2022) or reward-centric models (Hafner et al., 2020), to future work."}, {"title": "ARCHITECTURES", "content": "All experiments revolve around GPT-2 style causal transformers (Radford et al., 2019) as the core of the model. However we consider two different methods for inputting image observations, summarized in Figure 2. Section 3.4 details how we measure the model size of each.\n\nTokenized architecture. Our first architecture tokenizes each image observation into multiple discrete tokens. This is done with a frozen VQGAN encoder $Ence(o_t) \\rightarrow z_t$, where $z_t \\in \\{1, 2, ..., V_o\\}^{d_z}$, for vocabulary size $V_o$ and latent dimension $d_z$. Discretized actions are mapped to a non-overlapping vocabulary. Following tokenization, training sequences take the form,\n\n$[z_t^1, z_t^2, ..., z_t^{d_z}, a_t^1, a_t^2, ..., a_t^{d_a}, z_{t+1}^1, z_{t+1}^2,..., z_{t+1}^{d_z}, a_{t+1}^1, a_{t+1}^2,..., a_{t+1}^{d_a}, ...]$,\n\nwhere each item of the sequence is an integer within our vocabulary. A transformer is then trained to maximize the likelihood of either the latent image tokens (world modeling), or action tokens (BC).\n\nThis tokenized architecture is widely used both in world modeling (Micheli et al., 2022) and BC tasks (Bousmalis et al., 2023). Gato (Reed et al., 2022) used a similar design but with continuous patches rather than discrete tokens. Our implementation tests both a \u2018small' (28M parameters, dz = 256) and 'large' (150M parameters, dz = 540) VQGAN \u2013 further details in Appendix B."}, {"title": "CNN architecture", "content": "Our second architecture differs in two ways. 1) Each image observation is input into the transformer as a single continuous embedding, extracted from a small trainable convolutional neural network (CNN). 2) Action dimensions are predicted independently (rather than in series), assuming $P(a_t ...) \\approx \\Pi_{i=1} P(a^i_t|...)$. A single forward pass of the transformer is needed per action prediction.\n\nThis produces an architecture similar to Baker et al. (2022) (VPT additionally used a transformer-XL and a refined hierarchical action space). Our implementation uses an Impala-style (Espeholt et al., 2018) CNN with 0.6M parameters for embedding image observations."}, {"title": "DATASETS", "content": "This paper focuses on the effect of scaling on the pre-training loss over an offline dataset. To study this cleanly, datasets must meet two criteria.\n\n1. Dataset size. Repeated training on the same data alters the effect of scaling. Therefore, datasets should be large enough that all model sizes use a low number of training epochs.\n\n2. Dataset diversity. Both the behavior and environment must contain enough richness and variety that pre-training loss does not saturate across the model sizes tested.\n\nMany existing benchmark datasets fail to fulfill these criteria \u2013 if not due to limited size, then because behavior is generated from a pre-trained fixed policy, or the environment is too simple.\n\nOur work instead focuses on a dataset of human behavior collected in a video game named Bleeding Edge. This is a fast-paced 4 vs 4 multiplayer game, with a range of characters, abilities and maps. Game play is highly complex due to the cooperative and competitive dynamics. Success requires selecting high-level strategies (e.g. choosing which map regions to fight for), as well as fine-grained reactive control during combat. Figure 9 shows example sequences from our dataset.\n\nSupported by the game's developer Ninja Theory, we compiled a dataset of 8.6 years of anonymized game play, containing both image observations and controller actions. We refer to this as the 7 map dataset. We also use a subset of this for some experiments, of around 1.1 years from a single map, which we name the Sky Garden dataset. Appendix B.3 provides further technical details."}, {"title": "SCALING ANALYSIS METHODOLOGY", "content": "We are interested in studying the relationship between several quantities defined below.\n\n\u2022 Model size N, the total number of trainable parameters (ignoring VQGAN parameters for WM-Token & BC-Token, but including the fixed-size CNN for BC-CNN). Embedding parameters are included in the count following Pearce and Song (2024).\n\n\u2022 Dataset size D, the total number of inputs the transformer sees during training. For WM-Token and BC-Token this is dz + da per observation & action pair, and for BC-CNN this is one per observation & action pair.\n\n\u2022 Compute C, the number of floating point operations (FLOPs) used during training. The common approximation of C = 6ND (Kaplan et al., 2020) is used.\n\n\u2022 Loss L, the standard classification cross-entropy loss (all targets are discretized). We assume training loss is an accurate proxy for test loss (Appendix B.3.1 analyzes further).\n\nMore specifically, we are interested in 'compute-optimal' versions of each quantity. For loss, this is defined as the minimal loss possible for a given FLOPs budget,\n\n$L_{optimal}(C) = \\min_{N, D} L(N, D)$,\ns.t. C=6ND\n\nwhere L(N, D) is the empirical loss achieved with an N parameter model trained on D tokens. We further define optimal model and dataset sizes as the configuration that produce this minimal loss given a FLOPs budget,\n\n$N_{optimal}(C), D_{optimal}(C) = \\argmin_{N,D \\ s.t. C=6ND} L(N, D)$."}, {"title": "SCALING ANALYSIS IN EMBODIED AI", "content": "This section presents our main results. We begin by considering the scaling laws for the task of world modelling in Section 4.1 with two different tokenizers (turning image observations into 256 and 540 tokens for the small and large variants respectively). Section 4.2 then considers the task of BC both with tokenized and CNN architectures. Finally, Section 4.3 tests the extrapolation capability of these scaling laws for the task of world modeling."}, {"title": "SCALING ANALYSIS IN WORLD MODELING", "content": "Figures 3 & 4 present our results for the task of world modeling, with the scaling law coefficients summarised in Table 1. For WM-Token-256 we find that the optimal coefficients for model and"}, {"title": "SCALING ANALYSIS IN BEHAVIOR CLONING", "content": "We present our results on the scaling law coefficients for BC-Token in Figure 5. Despite sharing an architecture with WM-Token-540 we now observe the opposite dependence on model and dataset sizes. The coefficients skew heavily towards dataset size; Noptimal = 0.32, Doptimal = 0.68 (compared to Noptimal = 0.62, Doptimal = 0.37 \u2013 explained in Section 5.1). Furthermore, under the same compute budget the compute-optimal model sizes are significantly smaller. For a compute budget of 1018 and 1019 FLOPs we find that model sizes of 2M and 11M are compute-optimal for BC-Token-540 compared to 27M and 110M for WM-Token-540. In our experiments, we observe the losses for the BC-Token models take much longer to plateau leading to less overlap between model sizes."}, {"title": "EXTRAPOLATION IN WORLD MODELING", "content": "To test the extrapolation accuracy of our derived scaling laws, we train a 894M parameter WM-Token-256 model with an order of magnitude more compute than used for the scaling law analyses. Figure 7 presents both the learning curve as well as the extrapolated lines derived from the Frontier fit method. We take the point with the loss value closest to our extrapolated loss curve (~ 1.58 \u00d7 1021FLOPs), and mark it on the Frontier fit extrapolations. We observe very good agreement between that point and our compute-optimal predictions for both model and dataset size, demonstrating the accuracy of our derived scaling laws. The gap between our prediction and the actual training run suggests we could further optimize the hyperparameters (learning rate and batch size in particular) for the 894M parameter model, which was not extensively tuned due to compute requirements."}, {"title": "FURTHER ANALYSIS", "content": "Section 4 made several observations about the effect of scale in the pre-training of embodied agents. This section aims to understand these results further, and provide intuition for why they occur. Specifically we target three questions.\n\n\u2022 Q1: Why does BC-Token produce training curves that do not plateau, while WM-Token does, given an identical architecture and dataset? (Section 5.1)\n\n\u2022 Q2: Why does moving from BC-Token to BC-CNN resolve this issue? (Section 5.2)\n\n\u2022 Q3: Why does increasing the amount of tokens per image observation (256 to 540) lead to an increase in the optimal model size coefficient (0.49 to 0.62)? (Section 5.3)"}, {"title": "Q1: BC-TOKEN VS. WM-TOKEN", "content": "The lack of saturation of BC-Token models compared to WM-Token models can be attributed to two factors. The first is a sparser loss. A single observation-action pair is discretized into dz + da total tokens. With the large VQGAN tokenizer, world modeling receives supervision for dz/(dz + da) = 540/556 \u2248 97% tokens, while BC is supervised for da/(dz + da) = 16/556 \u2248 3% of tokens.\n\nThe second factor is the granularity of the targets. The large tokenizer creates a world modeling vocabulary size of 4096. Each vocabulary item roughly corresponds to a specific color and texture for an image patch. Many vocabulary items may only be used to model specific map regions or special abilities. Hence, the world modeling loss is very granular. On the other hand, a player can take the same action in multiple different situations \u2013 continue straight could be used to escape an enemy, chase an enemy, or navigate to a checkpoint. Hence, the supervision for BC is more vague and abstracted. We can think of this as a super-classed label.\n\nTo demonstrate the effect of these two factors on optimal model size coefficients, we run a set of tiny-scale experiments in language modeling. Transformers are trained on next-character prediction, on a dataset of Shakespeare text\u00b2 using a single character for each token. Model sizes are varied from 4k parameters to 17M parameters. Context length is fixed at 16 characters/tokens."}, {"title": "Q2: BC-TOKEN VS. BC-CNN", "content": "Despite the same non-granular loss signal, why does switching architecture from BC-Token to BC-CNN makes the loss of similar model sizes plateau under a much smaller compute budget?\n\nConsider each architecture using a transformer with 1M parameters. Observe from Figure 2 that BC-Token receives dz + da = 556 inputs for every action at it predicts, while BC-CNN receives just one input for every action predicted. Hence, BC-Token uses around 556 times more compute in its action prediction (556 \u00d7 2 \u00d7 1M \u2248 1 \u00d7 10\u00ba FLOPs) than BC-CNN (1 \u00d7 2 \u00d7 1M \u2248 2 \u00d7 106 FLOPs). This means that even with the same number of parameters, BC-Token can learn a far more expressive function than BC-CNN. Hence, BC-Token requires far more tokens to match this expressivity, and training curves for a given model size plateau much later."}, {"title": "Q3: WM-TOKEN-256 vs. WM-TOKEN-540", "content": "Finally, we seek to understand why the optimal model size coefficient increases when moving from the 256 to the 540 token VQGAN. As the number of tokens per image observation are increased, the compression rate of the tokenized representation decreases. We would expect that each individual token becomes easier to predict in this less compressed representation. This would mean a less expressive function is needed (smaller model size), but also a smaller number of examples would need to be seen (smaller dataset size). It is less clear what ratios these ingredients decrease in, and hence what effect a lower compression rate has on the optimal model size coefficient.\n\nFor further insight into how compression affects the optimal model size coefficient, we ran a small scale experiment in language modeling using two text representations; 1) ASCII character-level tokenization. (low compression) 2) GPT-2 tokenizer (high compression). We used the BookCorpus dataset (Zhu et al., 2015), and trained models past their compute-optimal point, so the Frontier fit method could be used for coefficient estimation."}, {"title": "DISCUSSION & CONCLUSION", "content": "This paper establishes a deeper understanding of scaling laws for world modeling and behavior cloning, two tasks that underpin embodied AI applications in domains such as video games and robotics. Focusing on generative pre-training of such models, we show that it is possible to recover scaling laws similar to those established in the LLM literature. Establishing such a link is key to making efficient use of available resources, and to training compute-optimal models.\n\nConsidering the task of world modeling, we find that models can be smoothly scaled following best practices and insights from the LLM literature. Surprisingly, the scaling coefficients for our WM-Token-256 architecture very closely match those established for LLMs. Comparing to our WM-Token-540 model and additional analysis, we further establish that scaling is affected by the tokenizer's compression rate.\n\nTurning to pre-training BC policies for agents, the choice of architecture is extremely important in determining optimal scaling behavior. When using architectures with tokenized image observations, dataset size should be increased much more rapidly than model size. Meanwhile, for BC-CNN architectures, model size should be increased faster than dataset size.\n\nLimitations. While we show that scaling laws can be precisely described in the infinite data regime and for appropriate architectures, future work is needed to establish scaling laws for alternative models and under varying dataset quality. In addition, we focus on loss as an intermediate quantity that can be effectively optimized in pre-training. Many additional considerations are required for effective AI models, such as downstream task performance and model inference times. How valuable scaling laws can be in providing insights relevant to those choices remains an open question."}, {"title": "EXTENDED RELATED WORK", "content": "Here we contrast more granular details of several related works.\n\nTuyls et al. (2023) provide (amongst other things) a valuable initial study on the relationship between compute and pre-training loss for BC. They focus on Atari and Nethack games, where the dataset is generated by a fixed pre-trained agent. They test one architecture a single-layer LSTM network, scaling width-wise to increase parameter count. They find the model size coefficient $N_{optimal} \\propto C^a$ varies between 0.58 to 0.79 for different games.\n\nBy contrast, our work focuses on transformer-based models, and we consider two variants of architecture. We also train on human behavior, rather than pre-trained agents (which could be viewed as model distillation). Additionally, we go beyond BC and also study scale in world modeling.\n\nScaling laws in other domains. Scaling laws have also been observed in auto-regressive modeling of other modalities. Most relevant to our world modeling task are those of video and images. Henighan et al. (2020) found the optimal trade off between model and dataset size to match their reported LLM coefficient ($N_{optimal} \\propto C^{0.7}$) and was not affected by tokenizer. Our experiments offer different findings in the domain of world modeling on human data we use updated methodologies to measure this trade-off (Hoffmann et al., 2022), finding it is affected by the tokenizer. Tian et al. (2024) show scaling laws emerge in image modeling with non raster-ordered next-token prediction."}, {"title": "SCALING EXPERIMENTS FURTHER DETAILS", "content": "HYPERPARAMETERS\nWe trained three VQGANs from scratch with reconstruction losses.\n\n\u2022 BE-Small. Based on Esser et al. (2021), uses $d_z$ = 256, $V_o$ = 4096, $h$ = $w$ = 128, with 28M parameters, and a CNN design. It was trained on a single SkyGarden Bleeding Edge map.\n\n\u2022 BE-Large. Based on Yu et al. (2022), uses $d_z$ = 540, $V_o$ = 4096, $h$ = 180, $w$ = 300, with 150M parameters, and a vision transformer design. It was trained on all seven Bleeding Edge maps.\n\nBC-CNN details. We use $h$ = $w$ = 128. The 0.6M paramter CNN is similar to that used by (Baker et al., 2022), however it uses ConvNext blocks (Liu et al., 2022). The CNN produces an embedding of size 1024 which is then put through a linear layer to obtain a vector matching the transformer's embedding dimension.\n\nTransformer configurations are given in Table 2. We describe the parameters for the WM-Token architecture. Note that MLP layers are four times the width of embed dim."}, {"title": "TRAINING DETAILS", "content": "All transformers are trained with a variant of nanoGPT (Karpathy, 2022) using PyTorch Lightning (Falcon and The PyTorch Lightning team, 2019)."}, {"title": "ON THE INFINITE DATA REGIME", "content": "We wish to study scaling in the infinite data regime, where training loss is not significantly effected by models repeatedly training on the same datapoints which can lead to overfitting effects. This section calculates the number of training tokens allowed for each model family trained in this work. Viewing Figure 1 alongside these numbers confirms that models remain in the infinite data regime for all our experiments.\n\nWM-Token-540, BC-Token-540. We trained on the 7 maps dataset, with 1.63B observation-action pairs. Models used the tokenized architecture with the large VQGAN, so each observation-action pair creates 540 + 16 = 556 transformer inputs, for a total of 1.63B\u00d7556 = 906B training tokens. Muennighoff et al. (2024) observe that tokens may be reused up to four times with negligible departure from the infinite data regime. This produces 3.6T tokens. For a 200M parameter model the compute allowed by the infinite data regime is C = 6ND = 6 \u00d7 200M \u00d7 3.6T = 4.3 \u00d7 1021 FLOPs.\n\nWM-Token-256. This is trained on the Sky Garden dataset, with 355M observation-action pairs. Each pair is split into 256 + 16 = 272 tokens, for 97B training tokens, or 97B\u00d74 = 386B effective tokens. For a 200M parameter model the compute allowed by the 'infinite data regime' is C = 6ND = 6 \u00d7 200M \u00d7 386B = 4.6 \u00d7 1020 FLOPs."}]}