{"title": "Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning", "authors": ["Xingchen Zeng", "Haichuan Lin", "Yilin Ye", "Wei Zeng"], "abstract": "Emerging multimodal large language models (MLLMs) exhibit great potential for chart question answering (CQA). Recent efforts primarily focus on scaling up training datasets (i.e., charts, data tables, and question-answer (QA) pairs) through data collection and synthesis. However, our empirical study on existing MLLMs and CQA datasets reveals notable gaps. First, current data collection and synthesis focus on data volume and lack consideration of fine-grained visual encodings and QA tasks, resulting in unbalanced data distribution divergent from practical CQA scenarios. Second, existing work follows the training recipe of the base MLLMs initially designed for natural images, under-exploring the adaptation to unique chart characteristics, such as rich text elements. To fill the gap, we propose a visualization-referenced instruction tuning approach to guide the training dataset enhancement and model development. Specifically, we propose a novel data engine to effectively filter diverse and high-quality data from existing datasets and subsequently refine and augment the data using LLM-based generation techniques to better align with practical QA tasks and visual encodings. Then, to facilitate the adaptation to chart characteristics, we utilize the enriched data to train an MLLM by unfreezing the vision encoder and incorporating a mixture-of-resolution adaptation strategy for enhanced fine-grained recognition. Experimental results validate the effectiveness of our approach. Even with fewer training examples, our model consistently outperforms state-of-the-art CQA models on established benchmarks. We also contribute a dataset split as a benchmark for future research.", "sections": [{"title": "1 INTRODUCTION", "content": "Multimodal large language models (MLLMs), such as GPT4-Vision [3], have made remarkable strides in understanding and interpreting natural images, enabling breakthroughs in various vision-language tasks (e.g., visual question answering [4]). These models excel by aligning the image representation of pre-trained vision encoders with the powerful linguistic understanding of LLMs. Thereby, MLLMs show great potential for visualization tasks that involve interpreting charts using natural language, such as chart question answering (CQA), chart summarization [70], and chart reverse-engineering [61]. CQA poses intricate challenges, requiring both the comprehension of complex natural language, along with the recognition of information from charts and the reasoning ability to derive accurate answers [24].\nBuilding MLLMS tailored for CQA necessitates high-quality training datasets and benchmarks. Recent research [21, 42, 54] in this realm primarily concentrates on scaling up training datasets encompassing charts, data tables, and QA pairs, using manual labeling and data synthesis techniques. These efforts have enhanced the performance of MLLMs in traditional CQA benchmarks [51,55]. However, bottlenecks have emerged, posing challenges for further improving performance and adapting to real-world scenarios. Simply scaling up the training dataset without implementing quality control measures poses significant challenges in training efficiency and the feasibility of integrating these data into general MLLM training.\nRecent research [18] emphasizes the impact of different QA types on model performance, finding that reasoning-oriented [18] and complexity-enhanced [60] instruction sets are particularly useful in improving the performance of MLLMs. In the context of CQA, existing MLLMs for CQA encompass visual instructions in the format of <chart, question, answer>. The quality of the chart and question-answering (QA) pairs is pivotal for the effectiveness and generalizability of MLLMs. However, the utilization of visual instruction data to enhance CQA remains largely under-explored, leaving unanswered questions about what makes good visual instructions and how to improve the dataset from the perspectives of visual instructions.\nTo address these inquiries, we conduct a comprehensive evaluation (Sect. 4) of MLLMs on CQA, aiming to pinpoint deficiencies and identify visual instructions that enhance MLLMs' performance. The study utilizes the ChartQA dataset [51], a widely adopted benchmark for CQA. Through empirical analysis (Sect. 4.1.1), we uncover notable distribution bias in both chart and QA pairs within the ChartQA dataset, as compared to practical datasets such as the Beagle image dataset [6] and visual literacy assessment datasets [19,33,57]. Thorough experiments (Sect. 4.1.2) uncover significant impacts of the distribution bias on MLLMs' performance in CQA, highlighting the necessity of incorporating more instructions for compositional and visual-compositional questions. Ablation studies (Sect. 4.2) further confirm that incorporating more reasoning-oriented QAs can significantly enhance model performance compared to including data retrieval QAs.\nDrawing inspiration from the results, we introduce a novel data engine (Sect. 5) aimed at generating instruction-enhanced CQA datasets. This engine comprises a data-filtering component (Sect. 5.1), utilizing a classifier with fine-grained chart features to reveal distributions and filter existing chart datasets. To mitigate the bias in the chart distributions and generate unavailable chart tasks, we further design a data generation component (Sect. 5.2) employing a chart space-guided data augmentation strategy to ensure the inclusivity of real-world possible charts. We further enrich reasoning-oriented QAs for the generated charts, contributing to a new CQA dataset and benchmark (Sect. 5.3) that features a wider variety of chart types and more QAs with effective visual instructions.\nExisting MLLMs, mostly relying on CLIP encoders trained on natural images, are not optimally suited for visualization charts due to inherent differences. Recognizing the limitations, we develop a new MLLM (Sect. 6) that unfreezes the vision encoders in CLIP to better adapt to chart-specific features. Our MLLM is trained using the newly curated CQA dataset with more effective visual instructions. Additionally, we incorporate a mixture-of-resolution adaptation strategy [49] to enhance the fine-grained recognition capabilities of chart elements. Quantitative experiments (Sect. 7) demonstrate that even trained on a dataset with 80 times less CQA data, our model consistently outperforms state-of-the-art CQA models on established benchmarks.\nIn summary, our contributions are three-fold:\n\u2022 An empirical study that identifies limitations of current MLLMS and ChartQA dataset and key factors (i.e., recognition and reasoning) that contribute to effective visual instructions for MLLMs' chart understanding.\n\u2022 A novel data engine encompassing data filtering and data generation, producing a high-quality dataset and benchmark using visualization-referenced instruction tuning.\n\u2022 An MLLM that outperforms existing open-source CQA models on existing CQA benchmarks and comparable to the best commercial models on our proposed benchmark."}, {"title": "2 BACKGROUND OF MLLMS", "content": "Recently, LLMs [8,63] have showcased powerful text generation and comprehension capabilities. However, native LLMs live in the pure-text world and cannot process other common modalities such as images and videos, thereby limiting their application scope [5]. To break this limitation, a group of MLLMs (e.g., LLaVA [45], Qwen-VL [5], and GPT4-Vision [3]) have emerged to endow LLMs with the ability to perceive and understand visual images.\nInspired by LLaVA [45], current open-source MLLMs adopted a similar architecture to align the visual and textual features. Figure 2 illustrates the typical MLLM architecture that comprises three modules: Vision Encoder, Projection Layer, and Large Language Model. Particularly, the Vision Encoder (e.g., CLIP-Vit [58]) extracts a sequence of visual features from the input image. Then, the Projection Layer (e.g., multiple linear layers [45] and querying transformer [37]) transforms the visual features into the LLM word embedding space, resulting in compatible visual tokens Ximg for the subsequent LLM (e.g., Vicuna-1.5 [73]). Finally, the LLM processes the concatenated visual Ximg and text tokens Xtext, i.e., [Ximg, Xtext], and then autoregressively generates responses Y. Formally, the language model predicts the response Y = {yi}_{i=1}^L conditioned on the multimodal input Ximg, Xtext, where L means the number of tokens in the response. Therefore, the response is predicted by maximizing\n$p (Y | X_{img}, X_{text}) = \\prod_{i=1}^{L}p_{\\theta} (y_i | X_{img}, X_{text}, y_{<i}),$ (1)\nwhere \u03b8 is the trainable parameters.\nDespite the architectural harmonization, the biggest challenge in training generic MLLMs is collecting high-quality visual instruction data, i.e., [Ximg, Xtext, Y]. Visual instructions facilitate the alignment of the multimodal (i.e., language-image) space, thus preserving and fusing the knowledge and abilities in the pre-trained vision encoder and LLM, empowering the MLLM with image-based conversation capabilities. In a general form, visual instructions are composed of , namely  in CQA."}, {"title": "3 RELATED WORK", "content": "Vision-Language Models for Chart Understanding. Researchers have long been committed to developing vision-language (VL) models with strong capabilities in chart-related tasks (e.g., CQA and chart summarizing). Previous works fall into two categories: 1) two-stage approaches that employ vision models to convert charts into data tables for subsequent processing with language models [15, 17, 40, 51]; and 2) unified VL models that directly process and interpret the fused chart and text features in a single integrated phase [41, 52, 54].\nThe two-stage pipeline struggles with preserving visual information (e.g., color and spatial location) [40] when performing the chart-to-table transformation, which inherently limits their applicability to specific scenarios. For unified models, Matcha [41] integrates mathematical reasoning and chart data extraction tasks into a pre-trained generic VL model, Pix2Struct [32], thus excelling at CQA and chart summarizing. UniChart [52] follows Matcha while collecting more data to undergo multitask instruction tuning for more chart-related tasks. However, their limited language model performances pose challenges, especially in reasoning problems that necessitate numerical calculations [51].\nThe advent of MLLMs has shifted the paradigm, achieving breakthroughs in visual question answering [4]. Notably, the open-source generic model Qwen-VL [5] demonstrates superior performance over all specialized chart models in the ChartQA benchmark [51], especially those posed by humans as opposed to machine-generated questions.\nDespite these advancements, our extensive empirical study has uncovered limitations in the current MLLMs' ability to handle real-world CQA tasks, especially those that fall outside of the training data distribution. Rectifying these limitations necessitates the consideration of the visualization reference model [9] when constructing training data, which elucidates the pratical mapping process from raw data to final graphical representations. Accordingly, this study contributes to enhancing the performance of MLLMs in CQA by integrating knowledge from the visualization reference process into training data generation and augmentation.\nEnhancing Capabilities of MLLMs. The enhancement of MLLMs in specific scenarios, such as medicine images and text-dense images, can be categorized into two primary approaches: model-centric works that aim to improve the performance and efficiency of vision encoders or projectors; data-centric works that try to improve the model performance by boosting the number and quality of training data. In data-centric advancements, several studies employ powerful LLMs (e.g., GPT-4 [3]) to generate various instruction-format VL tasks, like caption generation [45]. Another line of studies has explored converting classical VL task datasets (e.g., COCO [39]) into an instruction-following format with pre-defined templates. Within this context, to enhance chart comprehension, ChartLLaMA [21] finetunes LLaVA with 160K instruction data generated by GPT-4. Similarly, ChartAst [54] crawls a huge amount of tables from arXiv and then uses tables to generate charts for large-scale chart-to-table pre-training. ChartAst also generates QA pairs based on the tables they collected. Despite these efforts, the factors that contribute to efficient instruction data for chart understanding are still unclear.\nOur research seeks to investigate this gap with an empirical study that revisits the differences in improving model performance using different types of CQA task data. The results underscore the significance of integrating complex chart reasoning questions, prompting us to develop a data engine enriched with real-world chart tasks. Moreover, we have also made improvements to the model-centric side by tailoring the training methodology of base MLLMs, initially tailored for natural images, to suit visualization contexts.\nVisualization Datasets and Benchmarks. Datasets form the foundation of model training, and well-structured benchmarks help researchers evaluate and choose appropriate models for downstream tasks. Specific to visualization scenarios, current benchmarks mainly focus on evaluating chart understanding performance via chart-to-table transformation [51,55], CQA tasks [51,55], and chart summarizing [27,59,62]. ChartQA [51] and PlotQA [55] are representative of the QA datasets and benchmarks. ChartQA features partially high-quality human-annotated QA pairs, while PlotQA offers a more voluminous collection of lower-quality items crafted using templates. Beyond QA tasks, VisText [62] introduces a comprehensive benchmark, which incorporates multi-level and fine-grained chart labeling, covering aspects such as chart construction, summary statistics, relations, and complex trends. The primary strength of these datasets is their expansive size and the carefully crafted templates used for data generation. However, they have limitations, including a restricted range of chart types, the challenge of maintaining high-quality questions and answers, and a tendency to focus excessively on basic data retrieval from the charts.\nIn the visualization field, real-world image datasets like Beagle [6], VisImage [16], Vis30K [12], and multi-view visualizations [14], together with practical QA benchmarks for visual literacy test [19, 33,57], have been introduced. The challenge lies in converting them into high-quality instruction data due to sparse label annotations. Our research draws upon methodologies that utilize GPT to generate code-format charts and associated instruction data. Specifically, we aim to guide the data generation process with the well-defined chart-task space [33] to contribute a dataset encompassing the real-world spectrum of chart features and QA tasks, thereby improving current MLLMs' chart understanding capability."}, {"title": "4 EMPIRICAL STUDY: REVISITING MLLMS FOR CQA", "content": "We conduct an empirical study to revisit the effectiveness of existing MLLMS for CQA, aiming to identify limitations and glean insights for further improvements. The study is informed by the CQA leaderboard\u00b9 and recent research [3, 5, 44], where highlights that ChartQA [51] serves as the primary training and testing dataset for MLLM in chart understanding. ChartQA encompasses large-scale real-world charts sourced from online platforms, accompanied by data tables and both human-authored and machine-generated QA pairs. Nevertheless, in-depth analyses are imperative to ensure that MLLMs exhibiting good benchmark performance on ChartQA can reliably transition to real-world scenarios. In particular, this empirical study aims to address the following research questions:\n\u2022 RQ1: How can ChartQA be enhanced to reflect real-world scenarios better? We aim to refine ChartQA to align more closely with real-world contexts. While the charts in ChartQA are sourced from online platforms, they do not encompass the entire spectrum of chart designs, as a recent study [71] identifies a biased distribution of online charts. Specifically, we will explore the diversity of chart design and QA pairs, both essential aspects for enhancing the effectiveness of CQA models.\n\u2022 RQ2: What makes effective visual instructions for CQA? While QA pairs inherently serve as instruction data, they include various question types (e.g., data retrieval and visual). Exploring which specific QA features can better improve the effectiveness of visual instructions is notably under-explored. Furthermore, previous studies [40,51] suggest that incorporating the chart-to-table translation task improves VL models' general chart understanding performance, while its effect within the context of MLLMs merits deeper investigation.\n4.1 Computational Analysis of ChartQA Dataset\nTo address RQ1, we conduct computational analyses of ChartQA's distribution in terms of chart and QA pairs. We identify distribution bias by comparing them with practical charts and visual literacy data. Subsequently, we assess the performance of various MLLMs on ChartQA and contrast these results with performances in real-world scenarios, emphasizing the impacts of distribution bias on model performance.\n4.1.1 Distribution Biases in Chart and QA Pairs\nChart distribution. ChartQA primarily consists of bar, line, and pie charts sourced from online platforms. These charts have similar visual styles (e.g., color themes) and lack the coverage of the diverse range of chart types such as area charts and scatterplots. Moreover, even within their included chart types, there can be significant differences in fine-grained chart features compared to practical charts. To investigate these differences, we utilize Beagle [6] as the control group to compare their distributions of chart features. Beagle crawls visualizations from the web using keyword searches and is considered to be comparatively diverse among available visualization datasets [71], encompassing charts from various visualization tools and libraries (e.g., D3 [7] and Chartblocks [11]). Specifically, we use the pre-trained CLIP-Vit [58], a commonly used vision encoder of MLLMs, to extract high-dimensional features from the images. We then project the features into two dimensions with t-SNE [64].\nQA pair distribution. ChartQA comprises two testing QA datasets: ChartQA-H for human-authored QAs and ChartQA-M for machine-generated QAs. These QAs are categorized into data retrieval, visual, compositional, and visual-and-compositional types, as defined in [28].\n\u2022 Data retrieval: finding the value of the corresponding elements through the entity name in the chart.\n\u2022 Visual: leveraging visual channels, such as color identification, comparison between entities using visual attributes (e.g., which is rightmost, highest, or largest)\n\u2022 Compositional: requiring mathematical operations like sum, difference, and average.\n\u2022 Visual-and-compositional: blending of visual and compositional.\nHowever, ChartQA does not annotate the question type for each QA pair, hindering the fine-grained accuracy analysis based on question types. To address the issue, we manually labeled the questions in the ChartQA-H and ChartQA-M test sets, each containing 1250 QAs. Statistics reveal that data retrieval task (1035/1250) dominates the ChartQA-M set. This distribution likely stems from the limited performance of the language model used for generation, which restricts ChartQA-M to specific question templates. In contrast, the human-authored ChartQA-H set features a more diverse distribution, containing (251) data retrieval, (476) visual, (251) compositional, and (272) visual-and-compositional types. The diversified distribution motivates us to conduct a more comprehensive evaluation of the model's chart understanding ability across different question types, as detailed in the subsequent section.\n4.1.2 Impacts of Distribution Bias\nWe further study how the distribution bias identified in the above section affect the model performance.\nModels. Our selected MLLMs include open-source models explicitly trained on ChartQA: LLaVA-1.6-13b [44], LLaVA-1.6-34b [44] and Qwen-VL-Chat [5]; and mainstream commercial models: Qwen-VL-Plus [5], and GPT-4-vision-preview [56]. The commercial models are accessed through their official APIs.\nEvaluation metric. Following existing research [21,51,52], we adopt the widely-used relaxed correctness [51], which requires exact matches for text responses but allows 5% error for numerical responses.\nPrompt settings. The CQA evaluation requires the model to answer with a single word or short phrase. Following the LLaVA setup for short answers [43], we prompt the model with \"Please answer with a single word or phrase\" for metric evaluation and \"Please think step by step\" for zero-shot chain-of-thought (CoT) [66] to investigate the key and error steps in the model's reasoning process.\nDatasets. Besides ChartQA-H and ChartQA-M test sets, we have mixed QA pairs from studies on visual literacy [19,33,57], resulting in the creation of a new dataset comprising 131 QA pairs. Visual literacy QAs are designed to assess an individual's ability to read, comprehend, and interpret data visualizations. These are representative examples of real-world QAs covering most of the chart-task space [33].\nResult analysis. Table 1 presents the experimental results, showing that all MLLMs exhibit a performance disparity between ChartQA-M and visual literacy. A plausible hypothesis is the uneven distribution of question types in ChartQA-M. To validate this hypothesis, we disaggregate the performances on different question types in ChartQA-H. The results unveil significant discrepancies among various question types. Specifically, all models demonstrate high performances on data retrieval and visual questions, while their performances notably decline on compositional and visual-compositional questions. Typically, data retrieval and visual questions mainly require the ability for chart recognition. In contrast, the compositional questions need chart recognition followed by calculation and reasoning, heavily relying on MLLM's reasoning ability. This confirms the validity of the hypothesis.\nTo gain deeper insights into the underlying reasons for the issue, we examine the responses generated by MLLMs equipped with CoT. Figure 4 illustrates three typical cases, highlighting deficiencies in three categories: recognition errors, inference errors for numerical calculations, and inference errors regarding chart knowledge. Multiple factors contribute to these errors. First, errors often occur for chart types common in visual literacy but rare in ChartQA, such as normalized stack bar charts. Additionally, uncommon questions in ChartQA, such as accurately determining a range of data values, may lead MLLMs to struggle to identify the correct range.\nSummary. These insights highlight a crucial issue with ChartQA: while it includes a wide range of real-world images and QAs, biases in chart and QA distributions constrain its generalizability. This emphasizes the need for a dataset incorporating a broader variety of chart types and QAs. Such a dataset can potentially enhance MLLM's ability to tackle the complex challenges inherent in real-world scenarios."}, {"title": "4.2 Instruction Tuning Ablations", "content": "To address RQ2, we design a series of ablation studies to examine the effect of different question types and chart-related tasks on CQA, aiming to identify effective visual instructions.\n4.2.1 Experiment settings\nBackbone MLLM: We select LLaVA-1.5 [43] as the baseline because its training data does not contain a specific chart dataset, making it easier to study the effect of different training data composition. We follow the official fine-tuning settings of LLaVA-1.5, where we freeze the vision encoder and only update the parameters of the projector and the LLM. Specifically, we employ the Low-Rank Adaptation (LORA) [25] strategy to train LLM to reduce the training workload.\nDataset Control: Despite the biased chart distribution with ChartQA, we utilize it for instruction tuning ablation tests due to its suitability for examining how MLLMs learn from and react to specific data distributions. In addition to ChartQA-H and ChartQA-M, each chart in ChartQA is associated with its data table, constituting a chart-to-table translation task, denoted as Chart2Table. Studies [40, 51] reveal that Chart2Table has the potential to enhance chart recognition capabilities, which justifies its inclusion in our ablation study. Specifically, the instruction data for Chart2Table are structured as  .\nAblation Models: We use the backbone MLLM without fine-tuning as the baseline. Furthermore, we fine-tune the backbone model with individual and different combinations of ChartQA-H, ChartQA-M, and Chart2Table, resulting in a total of six fine-tuned MLLMs.\n4.2.2 Results and Analysis\n shows the results of the ablation experiment of baseline and the fine-tuned MLLMs on different question types in ChartQA-H test set. Overall, models fine-tuned with more training data (individual vs. combinations datasets) achieve higher accuracy. Specifically, the inclusion of human-generated ChartQA-H dataset substantially enhances model performance across all question types. In contrast, ChartQA-M dataset is less effective and mainly improves data retrieval and visual questions. This difference further underscores the limited impact of data retrieval questions for tackling CQA challenges and the critical role of diverse, reasoning-intensive questions over simple recognition questions. Moreover, Chart2Table serves as an accompanying effective instruction task if the data tables are available.\nIn summary, enhancing MLLM's chart understanding necessitates focusing on diversity, especially in question types demanding reasoning, over expanding the volume of data retrieval-focused training examples."}, {"title": "5 DATA ENGINE", "content": "Collecting all available CQA data for training an MLLM is inefficient and cannot address inherent distribution flaws. First, research has revealed the importance of data balance in training a generic MLLM [10]. Without precise labeling, aggregating all data produces a massive dataset, causing learning inefficiency and training expensiveness of MLLMs [23]. For instance, LLaVA [45] as a leading generic MLLM only requires 1223K instruction data, whilst UniChart [52] and ChartAssistant [54] use about 6900K and 39.4M chart-related instruction data. This disparity highlights the impracticality of incorporating all available chart data into generic MLLMs' training data. Furthermore, our empirical study has demonstrated the distribution flaws in existing CQA data, underscoring the necessity of generating new data.\nTo this end, we opt to design a data engine for a dataset of appropriate size while encompassing the real-world spectrum of chart features and QA tasks. The data engine consists of two modules: data filtering (Sect. 5.1) for efficiently utilizing the existing data and also ensuring appropriate training cost; and data generation (Sect. 5.2) for optimizing the data distribution. Finally, we present the obtained visualization-referenced dataset and benchmark (Sect. 5.3).\n5.1 Data filtering\nThis module is designed to filter representative data from existing CQA datasets. We first establish principles for what constitutes an appropriate chart distribution. Specifically, drawing on the taxonomy of the chart and corresponding task types outlined by [33] , our methodology involves analyzing the distribution across the following:\n\u2022 chart types summarized in visualization literacy papers [19,33] ; \n\u2022 fine-grained chart attributes identified in visualization retrieval tasks [68], e.g., color, trends and layouts; and\n\u2022 chart attributes that significantly affect MLLMs' chart understanding, i.e., number annotations (existent or absent) and data grouping (single or multiple).\nGiven that studies have shown common pretrained visual feature extractors (e.g., CLIP-Vit [58]) are not sensitive to fine-grained chart attributes [68], conventional filtering approaches that sample data in the pretrained feature space lead to inhomogeneity in these attributes. Additionally, most existing datasets lack detailed annotations beyond coarse-grained chart types (e.g., bar, line, and pie), posing challenges for stratified sampling. To mitigate this issue, we construct classifiers to learn those attributes in a supervised manner and then perform stratified sampling based on the labels predicted by the classifiers.\n5.1.1 Image Classifier\nAs shown in Figure 5, we build probing classifiers (i.e., two-layer perceptron) based on the frozen ConvNeXt [67] backbone to accurately assess the distribution of these fine-grained chart types and attributes. We collect training data sourced from [68] alongside a manually collected subset. To mitigate the issue of lacking some attribute annotations, we manually labeled each image for missing attributes. Due to the unbalanced nature of chart type and attribute distribution (e.g., number annotations), we choose to use focal loss [38] as the loss function, designed to focus on unbalanced image type. Focal loss is defined as:\n$L_{FL} (p_t) = - (1 - p_t)^{\\gamma} log(p_t),$ \nwhere pt \u2208 [0,1] represents the estimated probability of class t, is the scaling factor, and \u03b3 represents the modulating factor. Among them, \u03b1 is set by inverse class frequency. Thus, learning parameters tend to contribute to classes with fewer samples, and \u03b3 assists in up-weighting the loss assigned to poorly-classified examples, avoiding the possibility that the amount of well-classified samples dominates the training process. We empirically compare several design alternatives of backbone models (e.g., CLIP-Vit [58] and ResNet50 [22]) and trainable modules (e.g., linear probe [58]) in Table 5.\nNote that not all chart types possess the same set of fine-grained attributes. For instance, pie charts do not exhibit a trend attribute, so the trend classifier training will not consider pie charts. We leverage the trained classifiers to label our collected existing data, providing clear inspections of the chart attributes and laying the foundation for data balancing in the subsequent sampling.\n5.1.2 Image Sampling and Instruction Data Sampling\nFigure 5 illustrates the image sampling process. We employ the CLIP-Vit [58] and a color extractor [1] to extract the overall feature and color feature of each image and then concatenate the two feature vectors to formulate a joint embedding space. Inspired by Bunny [23] and SemDeDup [2], we cluster images into k clusters via k-means within the joint embedding space, aiming to group charts with similar features. To ensure chart attribute balancing, we incorporate stratified sampling within each cluster. Specifically, we create strata within each cluster according to predicted chart attributes and further perform sampling in each stratum. We identify duplicates by constructing an undirected graph, where edges connect image pairs with cosine similarity above a specified threshold \u03b5, indicating high feature similarity. We streamline the process by directly retaining only the image with the lowest cosine similarity to the stratum's centroid from each set of semantic duplicates, thereby effectively reducing dataset size while preserving diversity. Finally, we manually adjust \u03b5 to obtain 100K charts, ensuring an appropriate training cost.\nFor instruction data, Sect. 4.2 summarizes the effects of different components of existing datasets. Drawing on the insights from the empirical study, we keep the table data of all sampled images for Chart2Table task and further sample numerical and visual reasoning questions in their attached QA pairs."}, {"title": "5.2 Data Generation", "content": "This module is designed to generate a dataset encompassing real-world chart types and QA tasks, thus alleviating the distribution bias issue of existing datasets. Specifically, we refer to the chart-task space types summarized in visualization literacy research [33] .\nWith collected tables, former works have explored generating charts and QA pairs using LLMs [21,54]. However, they overlook potential quality distortions arising from the instability inherent in language model outputs, nor has it considered guiding the generative process through an informed understanding of the chart space. We harness LLMs' in-context learning ability to follow the visualization reference process [9], ensuring the variety of the resulting charts and thus covering the chart space. Figure 6 outlines our chart generation pipeline, which encompasses two phases: Retrieval-Augmented Chart Generation and Visualization-Referenced Encoding Augmentation. Templates of prompts constructed for LLM input in this section can be found in Supplementary Section S1.\n5.2.1 Collection and Expansion of Seed Charts\nGenerating charts begins with aggregating a diverse and high-quality set of seed examples that cover a wide representation of styles and chart types. These examples are table-code pairs collected from a variety of authoritative chart libraries, such as Vega-Lite2, Matplotlib\u00b3, Seaborn4, and ECharts5. Moreover, we collect high-quality table-code pairs from previous studies [31] and handpick select examples from the web. To further expand our seed examples, we also gather high-quality table data from various sources [50,71]. Notably, charts filtered from existing datasets are not used here, as most of them are not in code format.\nAs depicted in Figure 6 (left), the expansion process employs the retrieval-augmented generation (RAG) method [34], which enhances the accuracy and quality of the generated charts by providing LLMs with contextually relevant examples during the generation process. To implement this, we first extract table features to identify and match each collected table with the most similar tables among existing table-code pairs. Specifically, following visualization recommendation research [26,36,65], we extract 30 cross-column data features that capture the relationships between columns and 81 single-column data features that quantify the properties of each column. These features allow us to represent the table features of the seed examples in a vector space, enabling the retrieval of nearest neighbors based on cosine similarity. When constructing prompts for seed chart expansion, the corresponding codes of these matched seed examples serve as few-shot examples alongside the new tables.\n5.2.2 Enhancement Through Visual Mapping Variations\nTo broaden the collection of seed examples, we leverage the LLM to introduce variations in the visual mappings or encodings of charts. This phase follows the visualization reference process [9] and is crucial to encompass a broader array of possible chart presentations and to align with the diverse distributions of real-world data. We guide the LLM by specifying which visual mappings each chart type can adopt and incorporating instances featuring diverse visual encodings in the input context for reference. For instance, as illustrated in Figure 6 (right), the LLM is instructed to modify chart elements like number annotations, groupings, and bar widths and to truncate or invert axes in bar charts."}, {"title": "5.2.3 Generation of Question and Answer Pairs", "content": "We further generate QA pairs based on the enriched set of table-code pairs, which are expected to be accurate and cover the chart-task space. Specifically, for each type of chart, we employ the LLM to generate Q&A pairs by prompting it with tables for numerical information, code for encoded visual information, and the corresponding chart-task space as context. We also require the LLM to classify generated Q&A pairs with category labels following the chart-task taxonomy (e.g., data retrieval and find extremum), balancing the distribution of different tasks. During the generation process, we randomly select some QA pairs for manual checking and ensure they are as accurate as expected.\nReasoning process. Recent studies show that unnecessary step-by-step training annotation leads to downgraded generalizability and instruction-following ability. For simple questions (e.g., data retrieval of bar charts), the reasoning process does not provide useful information compared to single-word answers. We only attach reasoning processes to questions that need numerical calculations and visual references. For visual references, we mainly consider visual channels that are less used in former work, such as the point area of the bubble chart and the truncated or inverted axis of line charts."}, {"title": "5.3 Visualization-referenced Dataset and Benchmark", "content": "Overview of Dataset. Our generated dataset comprises 11 chart types and 8 task categories, as outlined in visual literacy research [19,33,57", "46": ".", "51": "."}]}