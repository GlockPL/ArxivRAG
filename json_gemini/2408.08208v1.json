{"title": "LLM4DSR: Leveraing Large Language Model for Denoising Sequential Recommendation", "authors": ["Bohao Wang", "Feng Liu", "Jiawei Chen", "Yudi Wu", "Xingyu Lou", "Jun Wang", "Yan Feng", "Chun Chen", "Can Wang"], "abstract": "Sequential recommendation systems fundamentally rely on users' historical interaction sequences, which are often contaminated by noisy interactions. Identifying these noisy interactions accurately without additional information is particularly difficult due to the lack of explicit supervisory signals to denote noise. Large Language Models (LLMs), equipped with extensive open knowledge and semantic reasoning abilities, present a promising avenue to bridge this information gap. However, employing LLMs for denoising in sequential recommendation introduces notable challenges: 1) Direct application of pretrained LLMs may not be competent for the denoising task, frequently generating nonsensical responses; 2) Even after fine-tuning, the reliability of LLM outputs remains questionable, especially given the complexity of the task and th inherent hallucinatory issue of LLMs.\nTo tackle these challenges, we propose LLM4DSR, a tailored approach for denoising sequential recommendation using LLMs. We constructed a self-supervised fine-tuning task to activate LLMs' capabilities to identify noisy items and suggest replacements. Furthermore, we developed an uncertainty estimation module that ensures only high-confidence responses are utilized for sequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing the corrected sequences to be flexibly applied across various recommendation models. Extensive experiments validate the superiority of LLM4DSR over existing methods across three datasets and three recommendation backbones.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in content comprehension, generation, and semantic reasoning, thereby catalyzing a revolution in artificial intelligence [1]. Recently, LLMs have been extensively applied in the field of Recommender Systems (RS) [54]. Some researchers have directly utilized LLMs by prompting or fine-tuning them to act as specialized recommenders [3, 14, 29]. Meanwhile, others have employed LLMs as auxiliary tools to enhance traditional recommendation models e.g., serving as encoders for user/item features [53], supplementary knowledge bases [55], advanced recommendation explainers [38], and conversational agents [12].\nGiven the powerful capabilities of LLMs, this work explores a innovative question: Can LLMs effectively function as denoisers for sequential recommendation? Sequential recommendation critically relies on the accuracy of users' historical interaction sequences to predict subsequent items. However, in practice, these sequences often contain noise due to various factors - e.g., being attracted by clickbait [46], influenced by item prominent positions [6], or from accidental interactions [30]. Such noise can significantly mislead recommendation models and degrade their performance, thus underscoring the necessity for effectively denoising.\nThe task of denoising is inherently difficult. A primary issue is the absence of labels for noisy data, resulting in a lack of clear signals regarding the nature of the noise. Recent approaches to denoising in sequential recommendation have either relied on human-designed"}, {"title": "2 Preliminary", "content": "In this section, we introduce key definitions in this work.\nSequential Recommendation Let \\(V = \\{o_1, o_2,\\dots, o_{|V|}\\}\\) be the set of items. We use \\(S^i = [s_1, \\dots, s_{|S^i|}]\\) to denote \\(i\\)-th historical temporal interaction sequence where \\(s_t \\in V\\) is the \\(t\\)-th item interacted within sequence \\(S^i\\), and \\(|S^i|\\) represents the sequence's length. We denote a subsequence \\([s_j, \\dots, s_k]\\) of \\(S^i\\) as \\(S_{j:k}^i\\). For each sequence \\(S^i\\), sequential recommendation takes \\(S^i\\) sequence data as input and outputs the item that the user is likely to interact with next, which is expected as \\(S_{|S^i|+1}^i\\). For the sake of brevity, we will use \\(S\\) to replace \\(S^i\\) in the following text.\nDenoise Task Formulation For a given sequence \\(S\\), the objective of the denoising task is to identify and remove the noise within the sequence and ultimately generate a noiseless sequence \\(S' = [s_1,\\dots, s_{|S'|}]\\). Previous explicit denoising tasks simply performed deletion operations on the sequence, resulting in a subsequence that is shorter than the original sequence, i.e., \\(|S'| \\le |S|\\) and \\(s' \\in S\\).\nIn this paper, the denoising task is accomplished by substituting the identified noise with a more appropriate item. The reason is it can effectively reduce the impact of erroneous deletions, and the"}, {"title": "3 Method", "content": "In this section, we outline the methodology of LLM4DSR. Section 3.1 details the training process for the LLM to enable its denoising capabilities. In Section 3.2, we present the uncertainty estimation module, which is designed to enhance the accuracy of the denoising process. Finally, Section 3.3 describes the procedure for replacing noise samples with the suggested items provides by LLMs."}, {"title": "3.1 Training LLM as a Denoiser", "content": "Due to the significant discrepancy between the original training objectives in pre-training phase and denoising tasks, LLMs inherently lack the capability for denoising. Previous studies have indicated that fine-tuning with a small amount of data can enable LLMs to quickly adapt to new tasks [4]. In this section, we design a self-supervised task and employ supervised fine-tuning (SFT) to empower LLMs with denoising ability.\nFor an original sequence \\(S\\) in the training set, we randomly select an element \\(s_t\\) and replace it with an item \\(\\hat{s}_t\\) randomly sampled from the item set \\(V\\), generating a modified sequence \\(\\hat{S}\\). The training objective of the LLM is to identify the replaced item \\(\\hat{s}_t\\) given the modified sequence \\(\\hat{S}\\) and predict the original item \\(s_t\\) at that position. This self-supervised task can be expressed in natural language as follows:\nTraining on the aforementioned self-supervised tasks empowers the LLMs to effectively understand and perform denoising tasks. The prompt \"The user has interacted with the following items before:\" provides the model with information about the user's historical interactions. \"Noise Items:\" guides the model to identify noise within the sequence, as these noise items typically deviate significantly from other items. \"Suggested Items:\" encourages the model to infer the user's interests based on the most items in the sequence, thereby suggesting items that align more closely with the user's preferences."}, {"title": "3.2 Uncertainty Estimation", "content": "Relying exclusively on responses from LLMs for denoising is both imprecise and inflexible. We developed an uncertainty estimation module designed to gauge the model's confidence in its response.\nFor a given sequence \\(S\\), we define the probability of item \\(s_i\\) being noise as \\(P_{\\text{noise}} (s_i|S)\\). Considering that LLMs are capable of modeling text probabilities, we utilize the LLM to provide the probability of \\(\\text{text}(s_i)\\) appearing after the prompt \"Noise Items:\" in the context of a user-provided sequence \\(S\\). This probability serves as an indicator of the LLM's confidence in identifying the item as noise and can be treated as an estimation of \\(P_{\\text{noise}} (s_i|S)\\). Formally,\n\n\\begin{equation}\nP_{\\text{noise}} (s_i|S) = P(T_i|x, z_{\\text{noise}}) = \\prod_{j=1}^{T_i} P(T_{i,j}|x, z_{\\text{noise}}, T_{i,<j})\n\\end{equation}\n\nwhere \\(T_i\\) denotes the token vector of the item \\(s_i\\) and \\(T_i\\) is the total length of \\(T_i\\).\nThe calculation of \\(P(T_i|x, z_{\\text{noise}})\\) necessitates a forward pass for each individual item in the sequence, which can be computationally expensive for LLMs. Based on the observation that the first token of different items often varies for most sequences, we can simplify the probability computation. For such sequences, once the first token \\(T_{i,1}\\) is given, the subsequent tokens \\(T_{i,>1}\\) are fully determined. This is because the generated noise item must be included among the\n\n\\begin{equation}\nP(T_{i,2},..., T_{i, T_i}, z_{\\text{noise}}, T_{i,1}) = 1\n\\end{equation}\n\nThus we can get,\n\n\\begin{equation}\nP_{\\text{noise}} (s_i|S) = \\prod_{j=1}^{T_i} P(T_{i,j}|x, z_{\\text{noise}}, T_{i,<j})\n\\end{equation}\n\n\\begin{equation}\n= P(T_{i,1}|x, z_{\\text{noise}})P(T_{i,2},..., T_{i, |T_{i}|} |x, z_{\\text{noise}}, T_{i,1})\n\\end{equation}\n\n\\begin{equation}\n= P(T_{i,1} |x, z_{\\text{noise}})\n\\end{equation}\n\nGiven the aforementioned simplification, we only need to perform a single forward pass using the prompt \\([x, z_{\\text{noise}}]\\) to obtain the probability values of all items in the sequence being noise, no need for performing one forward pass for each item in the sequence individually.\nBy modeling text probabilities using a LLM, we are able to determine the likelihood of each item in a sequence being noise. Ideally, a higher probability indicates a greater likelihood of the item being noise. We validated this hypothesis through experiments conducted on a dataset with artificial noise (noise label are available), the details of datasets can be found in Section 4.1.1. As shown in Figure 3, both the number and proportion of noise samples increase as the probability values increase, while the number of clean samples decreases. This experiment confirms the reliability of the probability provided by the LLM.\nWe propose setting a threshold \\(\\eta\\) to classify items with probability values exceeding this threshold as noise. Formally, for a sequence \\(S\\), we denote the probability values for each item as \\(P = [p_1,\\dots, p_{|S|}]\\). We introduce a hyperparameter \\(\\eta\\) as the threshold and classify items with a probability greater than \\(\\eta\\) as noise, i.e.,\n\n\\begin{equation}\ny_i = \\begin{cases}\n1, & \\text{if } p_i > \\eta \\\\\n0, & \\text{if } p_i \\le \\eta\n\\end{cases}\n\\end{equation}\n\nUncertainty estimation of leveraging model-generated probability, rather than relying solely on response-based discrimination, enhances the accuracy of noise detection while also increasing the algorithm's flexibility. In certain sequences, the model may exhibit high confidence that multiple items are noise, resulting in several items being modified, whereas in others, the confidence levels across all items may be relatively low, leading to no modifications being made."}, {"title": "3.3 Replace Noise with Suggested Items", "content": "Instead of simply removing noise, it is more logical to replace it with an item that closely aligns with the user's preferences. Replacing rather than simply deleting can mitigates the issue of information loss resulting from erroneous deletions. By leveraging the powerful reasoning capabilities and extensive knowledge of LLMs, we can enable the LLM to select an item that better matches the user's historical interaction patterns to replace the noise sample.\nFormally, for a sequence \\(S\\), if \\(s_t\\) is identified as noise (i.e., \\(y_k = 1\\)), we use the prompt \\([x, z_{\\text{noise}}, T_k, z_{\\text{suggest}}]\\) to generate a suggested item, referred to as the oracle. Due to the hallucination phenomenon inherent in LLMs, the oracle may be a fake item that does not necessarily exist in the item set. Inspired by BIGRec [2], we employ\n\n\\begin{equation}\ns_k' = \\arg \\min_{u_j \\in V} ||\\text{emb}_{\\text{oracle}} - \\text{emb}_{u_j} ||_2\n\\end{equation}\n\nwhere \\(\\text{emb}_{u_j}\\) denotes the embedding of the item \\(v_j\\) and \\(\\text{emb}_{\\text{oracle}}\\) denotes the embedding of the suggested item's title generated by the LLM. The grounding operation maps the generated item oracle to the actual items with the most similar embedding."}, {"title": "4 Experiments", "content": "We aim to answer the following research questions:\n*   RQ1: How does LLM4DSR perforem compare to existing state-of-the-art sequential recommendation denoising methods?\n*   RQ2: What are the impacts of the components (e.g., uncertainty estimation module, replace with suggested items) on LLM4DSR?\n*   RQ3: How does LLM4DSR identify noise and complete the sequence?"}, {"title": "4.1 Experimental Settings", "content": "4.1.1 Datasets. Three real-world datasets with 10-core setting: Amazon Video Games, Amazon Toy and Games, and Amazon Movies 2 were utilized in our experiments, which have been used in the previous works [7]. To ensure a fair comparison, we adopted the data preprocessing used in recent studies [2]. For each user sequence longer than 11 interactions, a sliding window of length 11 is used to segment the sequences. The sequences were then organized in ascending order of timestamps to partition each dataset into training, validation, and testing sets with ratios of 8:1:1. We randomly retained 20,000 items for Amazon Movies due to its large size. The dataset statistics are presented in Table 1.\nTwo noise settings are used in the experiment. The first setting uses the original real-world datasets, which are assumed to inherently contain natural noise. The second setting involves artificial noise, where we randomly replace items in the user interaction sequences according to a specified probability. The noise ratio \\(\\alpha\\) indicates that each item is randomly replaced with a probability of \\(\\alpha\\). Note that the artificial noise is introduced before segmenting the sequences using the sliding window, thus preventing any information leakage between sequences.\n4.1.2 Baselines. We conducted experiments on three backbones, including the traditional sequential recommendation models SASRec [23] and BERT4Rec [35], as well as a LLM-based recommendation model LLaRA [29]. The methods compared in the study fall into two categories:\n*   Explicit Denoising Methods (HSD [61], STEAM [30]): HSD identifies noise based on inconsistency signals, whereas STEAM employs traditional models to train a discriminator and generator"}, {"title": "4.2 Performance Comparison (RQ1)", "content": "In this section, we analyze the superior of LLM4DSR under two different noise setting as compared with other baselines.\n4.2.1 Evaluations on Real-world Datasets. In Table 2, we present the test results of various methods on real-world datasets. Notably, LLM4DSR consistently achieved the best performance across almost all datasets.\nFor explicit denoising methods (i.e., STEAM and HSD), we observed that while these methods perform well on BERT4Rec, their effectiveness diminishes when retraining on SASRec using the denoised dataset they generated. For instance, on the Games dataset, the STEAM method improves NDCG@20 from 0.0351 to 0.0373 on BERT4Rec. However, when the same denoised dataset is evaluated on SASRec, NDCG@20 drops from 0.0299 to 0.0233. This suggests that the denoised dataset generated by STEAM lacks good transferability. Additionally, the performance of HSD significantly declined. We argue that this issue may arise from HSD's inability to effectively capture sequence information in short sequence scenarios. This limitation can result in the loss of critical information when incorrect items are deleted. LLM4DSR also demonstrated impressive performance when applied to LLM-based architectures (i.e., LLaRA). Notably, HSD demonstrates better performance on LLM-based backbones compared to traditional models. This may be due to the fact that the shortened sequences generated by HSD are more compatible with LLMs [19], and the inherent reasoning capabilities of LLMs make them relatively insensitive to erroneous deletions.\nFor implicit denoising methods (i.e., FMLP and CL4SRec), we found that their performance is unstable and can even decline on certain datasets. For example, on the Toy dataset, both FMLP and CL4SRec experienced performance drops, with NDCG@20 decreasing from 0.256 to 0.242 and 0.246, respectively. We hypothesize that for CL4SRec, the substantial difference from the original sequence during the sequence enhancement phase may degrade the quality of positive samples, and using contrastive loss for alignment might introduce additional noise. FMLP, which performs denoising at the"}, {"title": "4.3 Ablation Study (RQ2)", "content": "To verify the contributions of various components of LLM4DSR, we conducted an ablation study on different variants under SASRec, using both real and artificially noisy datasets from the Game and Toy datasets. The variants tested include: (1) \"w/o uncertainty,\" which omits uncertainty estimation and directly uses the LLM output for denoising; (2) \"w/o completion,\" which excludes the completion capability of LLM4DSR and performs only deletion operations on sequences; and (3) \"w/ popularity completion,\" which replaces noise with items randomly sampled from the item set based on popularity weights. As presented in Table 4, the consistently superior performance of LLM4DSR across all datasets highlights the effectiveness of each component. In particular, the performance decline observed in the \"w/o completion\" and \"w/ popularity completion\" emphasize the critical role of the replacement operation, demonstrating that the LLM4DSR can effectively extract user interests and provide items that better align with the sequence."}, {"title": "4.4 Case Study (RQ3)", "content": "In this case study, we illustrate how LLM4DSR leverages open knowledge to achieve denoising. As depicted in Figure 6, we analyze a user's viewing history from the Movie dataset, which predominantly consists of films in the drama, romance, and comedy genres. A notable outlier in this collection is the science fiction film \"Escape from the Planet of the Apes\". Utilizing prior knowledge about movie classifications, the LLM identifies this film as noise with a probability of 0.967. To enhance the sequence information, LLM4DSR recommends \"The Man from Snowy River\", a film that aligns with the user's preferred drama/romance genres, as a suitable replacement for the identified noise."}, {"title": "5 Related Work", "content": "5.1 Sequential Recommendation\nThe sequential recommendation infers the next item of interest to the user based on their historical interactions. Compared to collaborative filtering, sequential recommendation takes into account the temporal order of interactions to better capture the dynamic changes in user interests. Benefiting from the development of deep learning models in recent years, sequential recommendation employs various deep models to model users' historical interactions and capture their interests. For instance, GRU4Rec [16] uses RNN, while Caser [37] uses CNN. SASRec [23] and BERT4Rec [35] are based on the self-attention mechanism [40], which automatically learns the weight of each interaction. DROS [59] introduces distributionally robust optimization (DRO) [32] to enhance the model's ability to handle out-of-distribution (OOD) problems. The readers may refer to the excellent survey [10, 44] for more details. However, the aforementioned methods do not consider the negative impact of noisy interactions on modeling the sequence, which is quite common in real-world scenarios.\n5.2 Denoising Recommendation\nThe primary objective of denoising sequential recommendation is to enhance the performance of recommendation systems by mitigating the impact of noisy data. Denoising methods for sequential recommendation can be broadly categorized into implicit denoising and explicit denoising. Implicit denoising methods primarily reduce the impact of noise without explicitly removing them. Some works reduce the influence of noisy interactions to the sequence representation by increasing the sparsity of attention weights in attention-based models [5, 60]. FMLP and END4Rec [13, 66] consider using filters to remove noise signals from representations. The drawback of these methods is that noise still affect the model, and their high coupling with the model architecture makes it difficult to transfer them to other backbones or downstream tasks, including the widely researched LLM-based recommendation systems in recent years.\nExplicit denoising methods directly modify the sequence content, effectively addressing the limitations of implicit denoising techniques. These methods often rely on heuristically designed denoising modules or metrics. BERD [36] models the uncertainty of each interaction and eliminates modules with high loss but low uncertainty to achieve denoising. HSD [61] assesses the similarity of each sample to other items in the sequence and its alignment with user interests to detect noise. STEAM [30] trains a discriminator model and a generator model, which are used to identify noise and complete sequences, respectively.\nRegardless of whether explicit or implicit denoising methods are used, the most significant challenge stems from the lack of effective supervision signals from noise labels to train the denoising model. Existing methods often rely on heuristic designs inspired by prior knowledge to develop denoising algorithms, which requires substantial expert knowledge and lacks precision. Additionally, since these methods usually utilize supervision information from recommendation loss, they are still adversely affected by noise as the labels of the recommendation label (i.e., the next item) could also be contaminated. To address these issues, LLM4DSR leverages the extensive open knowledge of Large Language Models (LLMs) to aid in more effective denoising.\nAdditionally, some works focus on denoising under collaborative filtering scenarios [9, 11, 15, 24, 39, 45, 49, 50, 52]. Compared to denoising in sequential recommendation, these methods do not take the temporal information of interactions into account. Readers can refer to the survey [64] for more details.\n5.3 LLMs for Recommendation\nLarge Language Models (LLMs), with their extensive knowledge, generalization capabilities, and reasoning abilities, have achieved remarkable results across numerous fields. LLMs are utilized in various recommendation tasks, including collaborative filtering [47, 67], sequential recommendation [2, 17, 18], graph-based recommendation [53], and CTR (Click-Through Rate) tasks [3, 42, 57]. Additionally, LLMs have been utilized for tasks that were challenging for traditional model to achieve, such as explaining item embeddings [38], explaining recommendation results [12, 48], and conversational recommendations [12].\nThe initial attempts directly use pre-trained LLMs as the backbone for recommendations to explore their zero-shot capabilities. [12, 19, 31, 43, 51]. These methods perform not well due to the significant gap between recommendation tasks and the training tasks of LLMs. Subsequent research has organized recommendation data into prompt formats and then fine-tuned LLMs to enhance their"}, {"title": "6 Conclusion", "content": "In this paper, we propose LLM4DSR, a novel method for sequence recommendation denoising using Large Language Models (LLMs), which is the first exploration of LLM performance on this task. Leveraging the extensive open knowledge embedded in LLMs, LLM4DSR offers significant advantages over traditional denoising methods. We have designed a self-supervised task that endows LLMs with enhanced denoising capabilities and integrated the Uncertainty Estimation module to improve both the accuracy and flexibility of the denoising process. Furthermore, we utilized the generative capabilities of LLMs to complete the denoised sequences, thereby enriching sequence information and ensuring robust performance in downstream tasks. Comprehensive experiments demonstrate that LLM4DSR outperforms existing state-of-the-art sequence denoising techniques.\nDespite the notable performance of LLM4DSR, it incurs higher computational costs compared to traditional methods due to the use of LLMs. A promising direction for future research is to employ techniques such as knowledge distillation to transfer the denoising capabilities of LLMs to smaller models, thereby achieving superior performance without increasing computational overhead."}, {"title": "Ethical Considerations", "content": "Denoising methods essentially transfer the model's prior knowledge to the data. If the model's outputs are biased, for example, if it tends to recommend items of a specific category, it will further amplify the biases present in the data. In the context of using LLMs for denoising as discussed in this paper, it is crucial to ensure that the outputs of the pre-trained LLM are inherently fair. Special attention must be paid to bias testing when selecting the backbone model."}]}