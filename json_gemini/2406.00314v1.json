{"title": "CASE: Curricular Data Pre-training for Building Generative and Discriminative Assistive Psychology Expert Models", "authors": ["Sarthak Harne", "Monjoy Narayan Choudhury", "Madhav Rao", "TK Srikanth", "Seema Mehrotra", "Apoorva Vashisht", "Aarushi Basu", "Manjit Sodhi"], "abstract": "The limited availability of psychologists necessitates efficient identification of individuals requiring urgent mental healthcare. This study explores the use of Natural Language Processing (NLP) pipelines to analyze text data from online mental health forums used for consul-tations. By analyzing forum posts, these pipelines can flag users who may require immediate professional attention. A crucial challenge in this domain is data privacy and scarcity. To address this, we propose uti-lizing readily available curricular texts used in institutes specializing in mental health for pre-training the NLP pipelines. This helps us mimic the training process of a psychologist. Our work presents two models: a discriminative BERT-based model called CASE-BERT that flags po-tential mental health disorders based on forum text, and a generative model called CASE-Gemma that extracts key features for a preliminary diagnosis. CASE-BERT demonstrates superior performance compared to existing methods, achieving an fl score of 0.91 for Depression and 0.88 for Anxiety, two of the most commonly reported mental health disor-ders. CASE-Gemma can achieve a BERT Score of 0.849 on generating diagnoses based on forum text. The effectiveness of CASE-Gemma is evaluated through both human evaluation and qualitative methods, with the collaboration of clinical psychologists who provide us with a set of annotated data for fine-tuning and evaluation. Our code is available at https://github.com/sarthakharne/CASE", "sections": [{"title": "1 Introduction", "content": "Mental health plays a vital role in the lifestyle of individuals worldwide. As living situations become complex, individuals around the world have expressed signs of disorders more frequently than before. The demography of mental health"}, {"title": "2 Background and Related Work", "content": ""}, {"title": "2.1 BERT and Mask Language Modeling", "content": "Devlin et al. [4] introduced Bidirectional Encoder Representations from Trans-formers (BERT), a transformer-based encoder architecture that is used to ob-tain representational embedding for natural language input. It uses deep bidi-rectional representation and trains on unlabeled text using a self-supervised"}, {"title": "2.2 MentalBERT and Psych-Search", "content": "MentalBERT by Ji et al. [1] leverages a pre-trained BERT model (bert-base-uncased) and fine-tunes it on a dataset of 13 million sentences scraped from Reddit [3] communities specifically focused on mental health issues like depres-sion, anxiety, and bipolar disorder. Psych-Search an open-sourced model is based on SciBERT [8], a model pre-trained on a collection of scientific paper abstracts. Psych-Search refines SciBERT further by specifically training it on 3.2 million abstracts related to psychology and psychiatry obtained from PubMed."}, {"title": "2.3 Textbooks are all you need", "content": "Gunasekar et al. [6] in their work introduce the notion of how the quality of data affects the performance of text generation of Large Language Models. In this work, they rely highly on the hypothesis that involves the usage of \"text-book quality\" data from the web (6B token) as well as synthetically generated data from GPT3.5. This small-scale data compared to large web-crawled cor-pora gave a respectable performance, especially for a significantly lower number of parameters. They pre-train on such data and then attempt to fine-tune on \"textbook-exercise\" like data. In philosophy, this encapsulates the domain learn-ing process for humans. A human obtains knowledge and a basic understanding of the domain from textbooks and similar reading materials and then applies that understanding to solve various exercises in the case of an evaluation-based method of learning. We aim to verify this ideology in the case of mental health disorder identification which in comparison to Python programming is a strong field"}, {"title": "2.4 LORA and QLORA", "content": "Low-rank adaptation of large language models (LoRA) is an optimization tech-nique for reducing memory requirements by using lesser training parameters termed adapters. This technique was proposed by Hu et al. [14] which made the finetuning of LLMs much easier as the number of parameters to train re-duced drastically. During stochastic gradient descent, gradients are sent to the adapter that is modified to maximize the loss function via the fixed pre-trained model weights. By using a second factorized projection, LoRA enhances a linear projection in a transformer layer."}, {"title": "2.5 Google Gemma", "content": "Gemma is a family of open-source models that are based on Gemini [11] a transformer-based decoder-only model. It has 2 variants based on the number of parameters viz. the 2B and 7B parameters. We use the 2B instance of the instruct model for our work. The base Gemma model is pre-trained on 2T token data primarily in the English language using web documents, code files, and mathematical documents."}, {"title": "3 Data", "content": "Due to the sensitive nature of the data required for building models for mental health disorder diagnosis, relevant good quality counseling data, like transcripts of sessions between therapists and patients is difficult to find and to the best of our knowledge, not available publicly. Moreover, as protecting the confidentiality of the patient is paramount, the use of such data is unethical unless explicit consent is received from the patient.\nDue to the above-mentioned reasons, creating a counseling dataset is difficult and costly. Hence, we limit our use case to screening patients on mental health forums or social media as publicly available data of this form is available.\nThe dataset used for this work involved publicly available datasets and data curated by psychology professionals. We use three different datasets in this method.\nMental Health is a sensitive domain that has a severe lack of open-source well-annotated data as well as unsupervised datasets to be used as corpora. This makes our approach of using curricular data critical in general domain understanding using Large Language Models(LLMs) even more important as the same strategy can be adapted to other such sensitive domains and can be enhanced. We discuss the type of data used in detail below."}, {"title": "3.1 Curricular Data", "content": "We obtained a private curricular text dataset from the clinical psychologist who collaborated in this work. The psychologists were a combination of people work-ing in academia as well as professional practitioners. This dataset consisted of about 110 curricular text materials that are used to train Psychology students in"}, {"title": "3.2 Reddit Mental Health Subreddits Dataset", "content": "The Reddit dataset [3] contains posts from 28 subreddits (15 mental health sup-port groups) from 2018-2020. We obtain data from subreddits like \"r/anxiety\", \"r/depression\", \"r/addiction\" etc.\nDiagnosis Annotations A subset of this Reddit dataset was sent to clini-cal professionals to annotate. Out of the given dataset, we obtained annotated diagnoses for 632 posts. All the columns and their descriptions are given in the Appendix. These columns represent the criteria on which the posts were judged and deemed fit by the psychologists. These annotations provide a com-plete breakdown of the conclusions that can be drawn from the provided text."}, {"title": "3.3 CounselChat Dataset", "content": "CounselChat [2] is an expert community platform. It serves as a platform to assist counselors in establishing connections with possible clients. Therapists answer queries from customers on the website. It's a good concept that produces some fascinating data. The biggest advantage that this dataset has over other datasets like Reddit is that the feedback text is from a professional as well as the tags used are reliable in this case as the queries are first verified by a professional and then a reply is provided by the expert. We use the query text here for our model fine-tuning along with the tags for the discriminative model. It consists of 1374 rows with an average passage size of 140 words. These sentences are tagged with multiple labels ranging from depression, anxiety, addiction, marriage, relation-ships, and so on. Out of these, we aim to use the data points tagged depression and anxiety as the classes for our discriminative model, as these are the most common disorders in the world by a large margin as reported by WHO [17] and Institute of Health Metrics and Evaluation [18]. This involves extracting all texts marked with the above-mentioned tags as positive examples while the other samples that lack the tag are considered to be negative examples."}, {"title": "4 Method", "content": "As mandated by APA [16] clinical psychology student undergoes years of training where they refer to various curricular documents to learn the mandated"}, {"title": "4.1 Discriminative Model", "content": "We use an instance of bert-base-uncased and bert-large-uncased [4] available on Hugging Face as our base model to build CASE-BERT.\nPre-training We employ the mask language modeling task as discussed in [4] on the curricular data where we masked tokens randomly with a probability of 0.15. We trained this on a Nvidia RTX3090Ti workbench for 60 epochs which took about 8.3 hours to train. Gradient accumulation is applied to achieve an effective batch size of 128. Learning rate of 1 \u00d7 10^{-5} was used.\nFine-tuning A simple Multi-Layer Perceptron with 2 layers was used as a sequence classification head for the binary classification problem of detecting whether certain posts exhibit the possibility of the above-mentioned disorders. This model was fine-tuned on the CounselChat Dataset [2]. We train this on a workbench with two Nvidia T4 GPUs in parallel publicly available on Kaggle for 3 epochs with a batch size of 32. The learning rate used was the same as that of the pretraining step.\nEvaluation We use the fl-score and accuracy to evaluate our model(s) against the baseline(s) - BERT [4], ROBERTa [5], MentalBert [1] and Psych-Search [7]. We finetune these models for using above dataset with the same hyperparameters to ensure uniformity in comparison. We report the metrics on the same test split on all the models."}, {"title": "4.2 Generative Model", "content": "We use Google's Gemma [10] as our off-the-shelf generative model for generating the assistive diagnosis from the posts on the forum. Specifically, we use the 2 billion instruct checkpoint of the Gemma model from huggingface. The genera-tive pipeline is shown in Fig. 2. Next token prediction loss is used to train the Gemma model as is regularly done in training generative language models as used by Radford et al. in [19] and [20]. We use QLORA [15] for both pre-training"}, {"title": "5 Experiments and Results", "content": "We perform ablation experiments on the amount of curricular data required for pre-training, the number of steps of pre-training required, and their correspond-ing results."}, {"title": "5.1 Discriminative Model", "content": "To judge the utility of the curricular data pre-training, we perform ablation studies with an increasing number of epochs, starting at the base model (BERT [4]) and then going up to 60 epochs - the resulting graph can be seen in Fig. 5. We then use this model and compare it against other similar models - BERT [4]ROBERTa [5], Psych-Search [7] and MentalBERT [1] and report it in Table"}, {"title": "5.2 Generative Model - CASE-Gemma", "content": "For the human evaluation of the generative model, we report the defined metrics Strict-Accuracy and Loose-Accuracy for the columns - Main Theme, Predomi-nant Mood, Positive Emotion, and Diagnosed Syndrome. We also mention the number of instances (out of 30) where the expert agreed, partially agreed, and disagreed with the model outputs. For the Severity of Negative Emotion and the"}, {"title": "6 Discussion", "content": "In this work, we present a pre-training method for both discriminative as well as generative styles of models. By using curricular data for pre-training, we present two models CASE-BERT, a discriminative BERT-based model that flags potential mental health disorders based on forum texts, and CASE-Gemma a generative model that extracts key features for a preliminary diagnosis. We"}, {"title": "6.1 Ethical Concerns", "content": "Our work attempts to provide an assistive diagnosis from forum texts. Mental health is a privacy-sensitive domain. However, CASE-BERT and CASE-Gemma are fine-tuned on data obtained publicly from open domains that are created after anonymizing all personally identifiable information. We would also like to highlight that our models do not aim to replace professional psychologists but rather aim to assist a psychologist in screening potential patients faster"}, {"title": "6.2 Further Scope", "content": "Our work opens up the possibility of creating preliminary screening pipelines that can be deployed as web applications. For discriminative models we can use variants of BERT models, like CASE-BERT, that take relatively less memory and various on-device assistive applications can be created as a consequence of this. The generative models, like CASE-Gemma, however, are difficult to adopt on-device due to the large model sizes but, inference using these models is still possible on small servers as it requires less memory as compared to commercial LLMs. In general, we believe curricular data can be used in other domains where high-quality data for a well-defined task is not present. One similar example can be in the domain of legal issue analysis where one can create such assistive screen-ing pipelines and assign the severity of issues which would help in the allocation of lawyers and accelerating the legal process for minor cases. Lastly, we hope that our work opens up the avenue for the creation of more curricular training-based specialized assistive experts that can leverage existing LLMs and can be trained on relatively accessible hardware and obtain respectable performance with a significantly lower amount of fine-labeled data."}, {"title": "7 APPENDIX", "content": ""}, {"title": "7.1 Annotated column descriptions", "content": "The annotated columns and their descriptions can be found in Table. 5"}, {"title": "7.2 Mathematical Definitions of Metrics", "content": "Let the set of data be D, the ith verdict is $v\u017c$ where 1 \u2264 i \u2264 |D|. We also define S: {0,1,2} \u2192 {0,1}, a function such that:\n$S(x) = \begin{cases} 1 & \\text{if } x = 1\\\\ 0 & \\text{otherwise} \\end{cases}$\nWe define another function L: {0,1,2} \u2192 {0,1}, such that:\n$L(x) = \begin{cases} 1 & \\text{if } x \u2265 0\\\\ 0 & \\text{otherwise} \\end{cases}$\nThe metrics are then defined as\n$Strong-Accuracy = \\frac{\\sum_{i}\\delta(v_i)}{|D|}$"}, {"title": null, "content": "$Loose-Accuracy = \\frac{\\sum_iL(V_i)}{|D|}$\nWe define an indicator function GTE: $R^2$ \u2192 0,1 such that:\n$GTE(x,y) = \\begin{cases} 1 & \\text{if } x \u2265 y\\\\ 0 & \\text{otherwise} \\end{cases}$\nLet the predicted magnitude of risk or severity be $p_i$ and let the actual value be $g_i$. Using this, we define the Cumulative-Accuracy as:\n$Cumulative-Accuracy = \\frac{\\sum_iGTE(P_i, g_i)}{|D|}$"}]}