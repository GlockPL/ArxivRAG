{"title": "Estimating Causal Effects of Text Interventions Leveraging LLMs", "authors": ["Siyi Guo", "Myrl G. Marmarelis", "Fred Morstatter", "Kristina Lerman"], "abstract": "Quantifying the effect of textual interventions in social systems, such as reducing anger in social media posts to see its impact on engagement, poses significant challenges. Direct interventions on real-world systems are often infeasible, necessitating reliance on observational data. Traditional causal inference methods, typically designed for binary or discrete treatments, are inadequate for handling the complex, high-dimensional nature of textual data. This paper addresses these challenges by proposing a novel approach, CAUSALDANN, to estimate causal effects using text transformations facilitated by large language models (LLMs). Unlike existing methods, our approach accommodates arbitrary textual interventions and leverages text-level classifiers with domain adaptation ability to produce robust effect estimates against domain shifts, even when only the control group is observed. This flexibility in handling various text interventions is a key advancement in causal estimation for textual data, offering opportunities to better understand human behaviors and develop effective policies within social systems.", "sections": [{"title": "1 Introduction", "content": "Causal inference enables us to move beyond mere correlations to identify and quantify the factors driving system behavior [40]. This helps to isolate the true effects of policy decisions and develop effective interventions [5, 31, 41] to change individual human [46] and social [2] behavior. Modern social systems, especially on digital platforms, are largely based on language data. Therefore, quantifying the causal effects of language on the outcomes of interest is an important task [15, 19, 47, 53].\nHowever, incorporating language, as expressed in text, directly into the causal inference framework introduces unique challenges in identification and estimation [12, 16, 49]. We use the following example to illustrate the challenges. Suppose it appears that angrier posts on social media receive more attention. A straightforward analysis of this hypothesis would attempt to estimate the levels of anger in a sample of posts, record how much attention they receive, and then assess the relationship between these two variables. One could adjust for confounding variables, such as topics and author demographics, in an attempt to mitigate the bias in causal estimation. However, a critical issue is that the treatment variable-anger-is embedded in the language of the post but never directly observed. Therefore, the causal inference task involves predicting the treatment variable anger, and then also predicting the outcome variable as a function of treatment prediction, not the treatment itself (such as in [44]). Inaccuracies or systematic biases in predicted anger would contaminate the downstream estimates of the causal effect. This is the fundamental challenge of identifying causal effect of a treatment variable that is itself inferred, as attributes of observed texts often are. In addition, identifying and adjusting for all confounders is also extremely challenging in observational studies.\nTo address the challenges in causal estimation of the effects of latent text attributes, we propose a framework called CAUSALDANN. This framework first crafts a hypothetical intervention on the observed text, which can be implemented as a text transformation using a large language model (LLM), and then estimates the causal effect by predicting the outcomes for both observed and transformed data. The text transformation forms the class of interventions that scopes our causal inference problem. It allows the treatment variable in a potential-outcomes setting [45] to be an entire piece of text, and the causal estimands to be formulated as potential outcomes under intervened text versus potential outcomes under original (observed) text. This framework allows us to conduct causal studies even in scenarios where some intervention group is missing, which would be a significant issue under the conventional causal problem setup.\nOur first contribution is to demonstrate how LLMs can be utilized to formulate the intervention. For instance, if we are interested in the effect of anger in a social media post on the attention the post receives, we propose an intervention that rephrases posts with more or less anger, and then robustly estimate the effect of those rephrasings. Here we substitute the causal question of how anger A affects engagement Y with how an anger-transformation on the text $Wg_A(W)$ affects Y, for some $g_a$ parameterized by a prompted LLM. If the transformation function $g_a$ faithfully reflects a change in anger A without manipulating other attributes like topic or writing style, then we argue that this more specific causal question is a good proxy for the original causal question of A \u2192 Y.\nThanks to the massive model sizes and training data, LLMs have"}, {"title": "2 Related Works", "content": "The field of causal inference in machine learning continues to grow rapidly, as does its intersection with language modeling. Prior literature that focuses on causal effect estimation with text [49] does not usually consider the text as the treatment variable. Rather, text embeddings are used as covariate information [52] or some discrete coding of the text is used as the treatment [15, 44]. To our knowledge, we are the first to propose a method for estimating the effects of direct interventions on text.\nWork on controlling spurious correlations in text classifiers [6, 51] mirrors some of the techniques used for causal effect estimation. The statistical implications of spurious correlations are similar to those of confounders in a causal context. Generally, spurious correlations are of concern for model generalization [17], and causal effect estimation entails a special case of generalizing a model to the interventional domain.\nOur method is strongly motivated by the apparent lack of overlap between the control and treatment groups (Section 4.1). Previous work has considered the similar case of overlap violations due to the treatment being a deterministic function of text [21]. In that case, for different pieces of text, there is no overlap in the treatment variable. In our case, an apparent lack of overlap is induced by the way we define the potential outcomes from the intervention. We do not assume to have observed any intervened text. Lack of overlap is a practically significant problem, but does not receive the same attention as violations to the other standard causal assumptions [57]."}, {"title": "2.2 LLMs for Causal Inference", "content": "Large language models promise to enhance computational understanding of text in causal inference [4, 29, 30, 55]. However, the ubiquitous spurious correlations in large text corpora tend to hinder an LLM's understanding of causal relations in the text-generating world [54]. The problem of weak causal understanding in LLMs might stem from a fundamental limitation of their training. Causal inference generally entails extrapolating to a system's behavior under novel conditions, like an intervention [28, 42]. Asking an LLM about interventions on text can place it in a regime of low probability in its training data. Recent work suggests that LLM reasoning struggles in such data-starved regimes [50].\nNew estimation methodology is therefore needed for using LLMs in causal inference without relying on them for tasks that are still challenging for them. One promising area of research is using LLMs for counterfactual data generation [8, 25, 33, 38]. In our work, we utilize the power of LLMs in two ways. First, we use LLMs to impose interventions on text, such as rephrasing or transforming it with respect to some linguistic property like sentiment. This does not rely on LLMs for any sort of causal reasoning, but only for transforming text-a task on which they excel. Second, due to the lack of ground truth causal effect in observational studies, synthetic or semi-synthetic data are often needed for evaluating causal inference. We perform counterfactual generation with LLMs to simulate the outcomes to build evaluation datasets."}, {"title": "2.3 Domain Adaptation", "content": "In the era of large language modeling, the two steps of pre-training then fine-tuning on specific tasks has become one of the more common paradigms [11, 13, 14]. However, many works have shown that fine-tuning on specific data easily leads to performance drop when predicting on unseen data due to data shift [7, 35, 48], since the training and test data may have different distributions. To address the data shift problem, researchers have developed many approaches for domain adaptation, such as sample reweighting [32], structural correspondence learning [9], joint distribution matching using max mean discrepancy [34], and mixture of experts [23].\nAnother popular approach is Domain Adversarial Neural Network (DANN) [18]. In addition to a standard feature extractor and label classifier, this model incorporates a domain classifier that acts as an adversary, encouraging the feature extractor to produce domain-invariant features, thereby enhancing domain adaptation. This architecture has demonstrated success across various NLP tasks, including aspect-based text classification [56], stance detection [3, 26], and morality detection [24].\nDomain adaptation is also tightly related to causal inference. When predicting potential outcomes, data shifts often exist between the control and the treated groups. One way to tackle this is to correct the bias through aligning the distribution of the treated and control groups in the form of domain adaptation [1]. In fact, some classic causal methods to adjust for confounders, such as inverse propensity weighting (IPW) [27], can also be viewed as a form of domain adaptation by aligning the distributions. However, a gap remains in utilizing more advanced deep learning domain adaptation methods in causal estimation. In our work, we will evaluate both IPW and DANN methods in prediction of potential outcomes."}, {"title": "3 Causal Inference Preliminaries", "content": "We formalize the observational causal inference problem using the framework of potential outcomes [28, 45]. For text W, any covariates X, outcome Y, we construct causal estimands through an intervention on the text defined by a transformation $W\u2192 g(W)$. Central to the motivation of our method is being able to handle cases where interventional outcomes have not been observed. Specifically, with potential outcomes defined as Y(w) for any text w, we only ever observe $Y = Y(W)$ and not $Y(g(W))$. The structure of the potential outcomes is characterized in Assumption 1.\nAssumption 1 (Potential Outcomes). The standard setting of potential outcomes adapted for interventions on text.\n(a) Stable unit treatment value assumption (SUTVA). The potential outcomes for a unit do not depend on the text variables for any other unit.\n(b) Overlap or positivity. All relevant text values w \u2208 W have a nonzero probability of occurring for every unit.\n(c) Ignorability or no hidden confounding. Potential outcomes are independent of the realized text variable W after conditioning on covariates."}, {"title": "4 Methods", "content": "We tackle the challenge of causal estimation for textual data where the treatment variable (e.g., emotion) is not directly observed and where the treated group or the control group is hard to construct. We propose a framework CAUSALDANN (Figure 1) in which we (1) apply a transformation on observed text (the non-intervened group) to construct the intervened group, (2) predict the potential outcomes using a model trained with domain adaptation on observed non-intervened data in order to predict the unobserved intervened outcomes, and (3) estimate the causal effects from predicted outcomes."}, {"title": "4.1 Interventions on Text using an LLM", "content": "When the intervention acts on the language of text and is not observed separately, we propose to define the treatment variable for causal-effect estimation implicitly through a text transformation. For example, when anger is the treatment-that is, we are interested in the effect of anger on some outcome from the text, we define an intervention where we rephrase the observed text with higher or lower anger level while keeping all other linguistic properties the same. The mapping from observed text to transformed text forms a binary distinction that appears to resemble causal inference with a binary treatment variable. After all, the causal estimand can be phrased similarly, as the intervened (transformed) outcomes contrasted with the non-intervened (observed) outcomes. However, this line of reasoning leads to a violation of a crucial identifiability assumption. There is no overlap, since we never observe the outcome of the transformed arm in this binary-treatment setting. We only ever observe the outcomes for the non-intervened arm. This complete lack of overlap is catastrophic from an identification point of view.\nTherefore, we need to look beyond a binary dichotomy of the treatment variable, and leverage the high-dimensional nature of language (and interventions on it.) Surprisingly, lifting the treatment variable into an abstract space of text, or a high-dimensional embedding space thereof, induces overlap. Interventions on text can be viewed as point-wise mappings (transformations) in that space. The estimation problem for text transformations is a straightforward generalization of the additive nudging approach that was recently proposed by Marmarelis et al. [36]. Replacing the nudge $W\u2192 W + $ with some $W\u2192 g(W)$ leads to effect estimators for a general class of text transformations, like rephrasing with more anger, or anything else that an LLM can be prompted to do.\nWe propose different approaches of transformation $W\u2192 g(W)$ on observed texts W to generate data in the intervened group:\n(1) Textual transformation with LLMs. Their outstanding capability to manipulate, summarize, and generate text allows for high quality counterfactual data generation. This lifts the burden of controlling for latent confounding parts in the texts and is generalizable to many different scenarios. One mentioned example is to generate counterfactual data by rephrasing texts with higher anger level but keeping all other linguistic properties the same (Section 5.4).\n(2) Sampling from other observed data, corresponding to the same covariates. For example, to study the effect of positive reviews on product sales, we can sample 5-star reviews as the positive ones and 1 or 2-star reviews as the non-positive ones while keeping the same covariates such as product type (Section 5.2). This is a convenient method if observations are available and covariates are known and easy to control for."}, {"title": "4.2 Counterfactual Generation with LLMS", "content": "Even when we apply the above transformation techniques to construct the intervened group, outcomes are still not observed, and ground truth causal effects are not available. Therefore, constructing synthetic or semi-synthetic data for causal estimation evaluation is an important task. We can further utilize the power of LLMs to generate synthetic outcomes for unobserved data. The great amount of training data and reinforcement learning from human feedback (RLHF) in terms of safety and bias gives LLMs good ability to simulate human social norms, especially when studying social data [11, 14]. In Section 5.3 and 5.4 we will use LLMs to simulate social judgement on posts from Reddit's /r/AmITheAsshole subreddit.\nOne concern is that using LLMs for text transformation and outcome data generation is susceptible to potential LLM biases. To mitigate, we manually check a part of the generations to ensure the quality. Second, other than performing the text transformation defined as the intervention (e.g., increasing anger), we also rephrase the original textual data using an LLM. In this way, both non-intervened and intervened groups are LLM-generated. Third, the estimated effect is a relative difference between the two groups. With text and outcomes in both groups being LLM-generated, the biases should cancel out to a large extent."}, {"title": "4.3 Outcome Prediction", "content": "Since the outcomes of the generated (intervened) data are never observed, we need a prediction model to estimate the outcomes for all the data in order to estimate the causal effects."}, {"title": "4.3.1 BERT", "content": "The BERT [13] model offers an easy and cheap way to learn textual representations and generally performs well in supervised classification tasks. Therefore we decide to build our baseline outcome predictor with BERT. We assume that conditioning on the textual embeddings learned by BERT effectively adjust for the latent confounding parts of the texts.\nA naive approach is to train a vanilla BERT model with only the non-intervened textual data. Their outcomes are observed and can be used as labels for supervised training. The trained predictor can then be used to predict the potential outcomes for the intervened text data with unobserved outcomes. This serves as our first baseline method.\nWe build the outcome predictor by appending a linear classification layer to the BERT embedding model, with cross-entropy loss and sigmoid normalization for the binary classification task. We add a dropout layer with a rate of 0.3. Adam optimizer with an initial learning rate of 5e-5 is used, along with a scheduler $lr = lr_{init}/((1 + \\alpha \\cdot p)^{\\beta})$, where $p = \\frac{current \\ epoch}{total \\ epoch}$ and $\\alpha = 10$ and $\\beta = 0.25$, following [24].\nTo obtain the predicted outcome for all data, we randomly split the data into five folds. Each time we train the outcome predictor with four-fifths of the data, which is further split into training and validation sets by 80%-20% ratio. We train for 20 epochs and stop early when the model achieves the best validation F1 score. The batch size is 64. Finally, we predict on the remaining one-fifth data to obtain their predicted outcomes. Given the i-th data point $W_i$, the predicted outcome is\n$Y_i = \\mu_{BERT}(W_i)$ (1)\nwhere $\\mu_{BERT}$ is the BERT-based classification model. The Average Treatment Effect (ATE) can be estimated as\n$\\uparrow = \\frac{1}{N}\\sum_{i=1}^{N} \\mu_{BERT}(g(W_i)) - \\mu_{BERT}(W_i)$ (2)\nwhere N is the total number of data points and g() is the text transformation function.\nIn real life applications, we often observe some covariates C and are interested in obtaining the heterogeneous treatment effects in different groups with respect to C. For example, when studying whether angrier posts receive more attention, we might be interested in estimating the effects with respect to different topics discussed. We can obtain the Conditional Average Treatment Effect (CATE) as\n$\\tau(c) = \\frac{1}{N_c} \\sum_{i:C_i=c} \\mu_{BERT}(g(W_i), c) - \\mu_{BERT}(W_i, c) \\forall c \\in C$ (3)\nOne common situation is that C, such as the topic, is already latently embedded in the BERT textual representation. In such cases, we can simplify the CATE as\n$\\tau(c) = \\frac{1}{N_c} \\sum_{i:C_i=c} \\mu_{BERT}(g(W_i)) - \\mu_{BERT}(W_i) \\forall c \\in C$ (4)"}, {"title": "4.3.2 CAUSALDANN", "content": "Many prior works have shown that the vanilla BERT, when finetuned on a specific dataset with supervision, is susceptible to performance drop when predicting on unseen data [35, 48]. As mentioned in Section 4.1, the outcome predictor needs to be trustworthy for both non-intervened data with observed outcomes and data obtained from the defined transformation. In order to better predict on the unseen transformed data, we adopt the classic domain adaptation framework Domain Adversarial Neural Network (DANN) instead of the vanilla BERT. [24] has shown the outstanding performance of DANN for domain adaptation in textual data.\nDANN aims to map the text embeddings from labeled source domain and the unseen target domain onto a common embedding space, thus mitigating the problem of data shift. CAUSALDANN has three modules (Figure 1b): (1) the BERT encoder that aims to learn the textual representation, (2) an outcome predictor that is a linear classification layer with cross-entropy loss, same as in the vanilla BERT outcome predictor, and (3) a domain predictor, also a linear classification layer with cross-entropy loss, but the loss to be maximized during the adversarial training, because we want to fool this domain classifier. This is achieved by connecting the domain classifier to the other parts of the model with a gradient reversal layer. The loss term is:\n$L = L_{outcome} \u2013 \\lambda D \\cdot L_{domain}$ (5)\nFollowing [24], we balance the loss terms between the outcome predicting module and the domain predicting module by controlling $\\lambda_D$ indirectly by\n$\\lambda_D = \\frac{2}{1 + e^{-\\gamma p}} \u2013 1$ (6)\nwhere $p = \\frac{current\\ epoch - epochs \\ trained \\ w/o \\ adversary}{total \\ epochs}$, and $\\gamma$ is now the hyperparameter. We performed a grid search for $\\gamma$ in [0.1, 1, 10] and decide to set $\\gamma = 1$.\nIn our case, the source domain is the non-intervened (observed) data, and the target domain we want to adapt to is the intervened (unobserved) data. We train the model in a semi-supervised way we provide both the labeled non-intervened training data and the unlabeled intervened data in each training batch, balanced in size. Both of them pass through the BERT encoder, so that their textual representations can be effectively learned. Next, the labeled non-intervened data pass through both the outcome predictor and the domain predictor, but the intervened data do not have outcome labels, and only pass through the domain classifier, which works as the adversary, pushing the BERT encoder to learn domain-invariant embeddings to fool the domain classifier. In this way, we aim to align the feature distributions of intervened data closer to the non-intervened labeled data.\nSame as in Section 4.3.1, we perform training and prediction by randomly splitting the data into five folds. We train with 20 epochs in total. However, during the first three epochs the model is trained without the domain adversarial module activated, the model to better learn the outcome labels first. The batch size is 32. The Adam optimizer, the learning rate and the scheduler are the same as in Section 4.3.1.\nAfter obtaining predicted outcome by CAUSALDANN, the ATE and CATE can be caculated in the same way as in Section 4.3.1."}, {"title": "4.4 Inverse Propensity Weighting", "content": "We compare the proposed CAUSALDANN with another baseline Inverse Propensity Weighting (IPW). IPW is a classic causal estimation method and has been commonly used. In the traditional causal problem setup, there are observed data in both intervened and non-intervened groups, but the effect is confounded. Thus we typically need to adjust for all the confounders when predicting the outcomes for data in both groups. The idea of IPW is that, to adjust for high-dimensional confounders, e.g. text W, we do not need to condition on the entire confounder vector, but it is enough to condition on the propensity score $\\pi = P(T|W, C)$, as long as the system satisfies SUTVA.\nIn our scenario, traditional IPW would not work, because the outcomes of our intervened data are unobserved. Nevertherless, IPW can also be seen as a sample reweighting technique, a commonly used domain adaptation technique [32] that pulls the distributions of the observed non-intervened data and the unobserved intervened data closer. When the propensity scores can be accurately measured, IPW can help to unbias the outcomes on the unobserved intervened data. We can therefore apply IPW on top of the outcomes predicted by vanilla BERT or CAUSALDANN.\nWe estimate the propensity score $\\pi = P(T|W, C)$ by training a separate BERT model. The architecture is the same as in Section 4.3.1, a BERT encoder with a linear classification layer. We train this propensity predictor on all the data, including intervened and non-intervened, using the texts as the input and $T = I_{transformed}$ as the training labels. To obtain the propensity score as a probability, we calibrate it on the validation set with temperature scaling [22]. The training procedure and hyperparameters are the same as in Section 4.3.1.\nOnce the propensity scores are obtained, we compute the ATE as\n$\\uparrow = \\frac{1}{N_1} \\sum_{i:T_i=1} \\frac{\\mu_{BERT} (g(W_i))}{\\pi(W_i)} - \\frac{1}{N_0} \\sum_{i:T_i=0} \\frac{\\mu_{BERT}(W_i)}{1 - \\pi(W_i)}$ (7)\nwhere the first term includes all data points that are intervened ($T_i = 1$) and the second term includes all data points that are non-intervened ($T_i = 0$).\nThe CATE can be computed as\n$\\tau(c) = \\sum_{i:T_i=1, C_i=c} \\frac{\\mu_{BERT} (g(W_i))}{\\pi(W_i)} - \\frac{1}{N_{0, c}} \\sum_{i:T_i=0, C_i=c} \\frac{\\mu_{BERT}(W_i)}{1 - \\pi(W_i)}$ (8)"}, {"title": "5 Experiments", "content": "We use real-world data, Amazon product reviews [39] and Reddit r/AmITheAsshole discussions of social dilemmas and user judge-ments on them, 1 to form three research questions for model eval-uation: (1) how much does a positive product review affect sales, (2) does seeing a top-upvoted comment change people's judge-ment about a social dilemma, and (3) does increasing anger in texts change people's social judgements."}, {"title": "5.1 Baselines and Setups", "content": "We compare two baselines (1) BERT, (2) BERT+IPW, and our proposed method (3) CAUSALDANN. We also evaluate (4) CAUSALDANN+IPW to see if IPW can further improve domain adaptation on top of DANN.\nIn addition, we compare these models with (5) TextCause [44], a method for causal estimation from textual data. Similar to our method, TextCause deals with the confounding in texts by adjusting on BERT embeddings. However, in contrast to our approach, this method assumes some observations are available in both control and treated groups. Therefore they use data and observed outcomes in both control and treated groups as the training data for BERT, and use this model to estimate the potential outcomes and treatment effects. This approach is good in scenarios when outcome observations are easy to collect for both control and treated groups. However, in scenarios when the treatment variable cannot be directly observed, or when one of the treated and control groups are not observed, this method fails. On the other side, our method, more"}, {"title": "5.2 Amazon Reviews", "content": "5.2.1 Data. We first evaluate our method on a semi-synthetic dataset based on Amazon reviews [39], a benchmark also used in [44]. This dataset consists of 5.6K reviews on products in the categories of mp3, CD, or Vinyl. Reviews for products worth more than $100 or fewer than 5 words are excluded.\nFollowing [44], we focus on estimating the effect of the positive sentiment of a product review on its sales. The hypothesis is that a product with positive reviews can have a higher chance of getting clicked and sold, but this effect might be confounded by the type of product. The treatment is the positive sentiment of a review, which is not directly observed but latently embedded in the review texts. Hence in a conventional causal setting, the treatment variable is hard to define.\nBased on our framework, we define a transformation function g() that intervenes on the positive sentiment of the reviews without manipulating other attributes like content or style. In this experiment, we do not use an LLM for text transformation, but instead we sample the positive and non-positive reviews from the observed data itself. That is, we define a review to be positive if it gets 5 stars and not positive if it gets 1 or 2 stars. The intervention is $T = 1_{positive}$. This setting (1) allows us to evaluate our framework without the impact of any bias from LLM generations, and (2) is closer to a traditional causal estimation setup and follows [44] exactly, allowing us to fairly compare our framework with TextCause.\nThe observed covariate C is a binary indicator for whether the associated review is a CD or not. The outcome Y, a binary variable representing whether a product received a click or not, is simulated based on whether the textual intervention $T = 1_{positive}$ is applied and the covariate C as following:\n$Y ~ Bernoulli(\\sigma(\\beta_c (\\pi \u2013 \\beta_0) + \\beta_T T + N(0, \\gamma)))$ (9)\nwhere $\\sigma(\\cdot)$ is the sigmoid function, $\\pi = P(T|C)$ is the propensity, $\\beta_c$ controlling confound strength is set to 4.0, $\\beta_T$ controlling treatment strength is set to 0.8, $\\beta_0$ controlling propensity is set to 0.8. All these hyperparameters are set as the same as a harder-to-estimate scenario in [44].\nWe then follow the framework described in Section 4 and train the outcome and propensity predictors. We assume that we do not observe the outcomes in the intervened (positive) group. Therefore, we only use the non-intervened (non-positive) reviews and their labels Y(T = 0) to train the outcome predictors. For the propensity predictor in the IPW method, we use T as the training labels and"}, {"title": "5.2.2 Results", "content": "Table 1 shows that CAUSALDANN gives us the ATE and CATE estimation closest to the ground truth, outperforming other baselines including BERT, BERT+IPW and also CAUSALDANN+IPW. The fact that CAUSALDANN achieves lower error than the vanilla BERT baseline tells us the effectiveness of domain adaptation by DANN.\nIn addition, the results show that using IPW on top of either BERT or DANN lowers the performance significantly. This infers that the propensity score estimation might be problematic. From the propensity predictor outputs, we find that it is very easy for BERT to classify whether a review is positive or negative, and hence the probability outputs are either very close to 1 or very close to 0, even after calibration with temperature scaling. This leads to the ill behavior when we perform inverse propensity weighting.\nLast, the performance of TextCause tells us how close we can get to ground truth effects even training with more observations in the intervened group. Therefore it is no surprise that TextCause performs better. Nevertheless, CAUSALDANN has a \u0394\u0391\u03a4\u0395 very close to TextCause and has the MSE of CATE same as TextCause, indicating very good debiasing ability in causal estimation."}, {"title": "5.3 Reddit AITA Comments", "content": "5.3.1 Data. Next we evaluate our methods on another dataset, the Reddit r/AmITheAsshole (AITA) data. In this subreddit, users post personal stories of social dilemmas, and other people give a verdict on whether the author is at fault or the other party is. This data offers insights about human perspectives [10] and moral judgments [43] and has been used as a benchmark for causal estimation [37]. We select posts that are shorter than 140 words to reduce computational complexity.\nThe verdict can be determined by the upvote mechanism on comments. The most upvoted comment, which most subreddit members agree with, usually stays at the top, where it is more visible to others. An interesting research question is - does seeing the top comment affect one's judgement? This infers how easily people are affected by the \"mainstream\" opinion. Such an experiment is hard and expensive to conduct in real life. However, our framework offers an opportunity to estimate such causal effects with the help of LLMs.\nMarmarelis et al. [37] used GPT-42 to act as moral judge on these real-world posts, avoiding problems about the real-world salience of the data. Here we follow the same approach. The assumption is that"}, {"title": "5.3.2 Results", "content": "Table 2 shows that again CAUSALDANN has an outstanding performance on this dataset. We notice that CAUSALDANN and BERT have similar performance. This is because the top and random comments are sometimes similar in terms of perspectives (see examples in Appendix Table 5), and comments are usually succinct and have similar writing styles as well. Thus there is less data shift from the training (non-intervened) data to the unseen intervened data, and a vanilla BERT can also give good outcome predictions in this case.\nThe performance of IPW-based methods is still significantly lower in this experiment, but this time for a different reason. Because it is relatively hard to classify whether a comment is a top comment or a random comment with respect to the post, the propensity predictor performance is low. The classification F1-score is only 0.52 \u00b1 0.01 over 5-fold cross validation. When the propensity score"}, {"title": "5.4 Anger in AITA Posts", "content": "5.4.1 Data. On the same AITA dataset, we formulate another research question: does the anger level in the posts affect people's verdicts. If a post contains a high anger level or aggression, there might be a higher chance that people would consider the author to be at fault. Anger is a latent attribute in texts and not a directly observed treatment variable. Under a conventional setup, we would need to use a proxy treatment variable which could be susceptible to more bias [44]. We define an intervention $T = 1_{anger-transformed}$. We take the original AITA dataset and ask an LLM to transform each post into a version with higher anger level, but keeping the writing style, the semantics and the perspectives exactly the same. To mitigate the bias brought by LLM generations, we also ask LLM to rephrase the original post and keeping the anger level the same. Then we use the rephrased posts and the posts transformed to higher anger as pairs of control and intervened data. We choose to use Claude 3.5 Sonnet 3 instead of GPT, because GPT has more conservative behaviors and does not raise the anger level in texts significantly. See Appendix A for procedure and example generations.\nNext, we also synthesize the outcome Y with Claude. We ask Claude to separately provide a verdict based on the rephrased post and the anger-transformed post. Therefore Y is a binary variable that equals to 1 when Claude decides someone in the post was at fault, and 0 when Claude decides no one was at fault (i.e. we categorize YTA (you are the asshole) and ESH (Everyone sucks here) into Y = 1 and NTH (not the asshole) and NAH (no asshole here) into Y = 0). We can then compute the ground truth causal effect for each post.\nSimilar to previous experiments, we only use the non-intervened (rephrased) posts and their corresponding Claude-generated verdicts to train the outcome predictor. To train the propensity predictor, we use all data and $T = 1_{anger-transformed}$ as the label. The input to both predictors are the post texts. In addition, we also have the covariates C to be the binary vector of the top 30 frequent topics obtained from BERTopic for computing CATE."}, {"title": "5.4.2 Results", "content": "From Table 3, we can see that CAUSALDANN outperforms other methods including TextCause (using more training data from the intervened group) on estimating CATE. For ATE, CAUSALDANN+IPW is the best performing one. The fact that methods using DANN architecture are better than methods based on vanilla BERT again shows the effectiveness of domain adaptation in predicting potential outcomes. Interestingly, applying IPW does not significantly hurt the ATE estimations in this dataset, although does"}, {"title": "6 Conclusions", "content": "In this work, we target the problem of causal estimation for textual data especially when the treatment variable is not directly observed. We propose a novel framework CAUSALDANN that utilizes LLMs for counterfactual outcome generation and text interventions through transformations. Following this, we utilize text-level classifiers equipped with domain adaptation capabilities to produce robust effect estimates, effectively handling domain shifts against domain shifts. We demonstrate the outstanding performance of CAUSALDANN in three different experiments."}, {"title": "Limitations", "content": "LLM data generation is susceptible to potential biases and lack of diversity. However, we try to mitigate the"}]}