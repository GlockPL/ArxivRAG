{"title": "Alt-MoE:Multimodal Alignment via Alternating Optimization of Multi-directional MoE with Unimodal Models", "authors": ["Hongyang Lei", "Xiaolong Cheng", "Dan Wang", "Qi Qin", "Huazhen Huang", "Yetao Wu", "Qingqing Gu", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "abstract": "Recent Large Multi-Modal Models (LMMs) have made significant advancements in multi-modal alignment by employing lightweight connection modules to facilitate the representation and fusion of knowledge from existing pre-trained uni-modal models. However, these methods still rely on modality-specific and direction-specific connectors, leading to compartmentalized knowledge representations and reduced computational efficiency, which limits the model's ability to form unified multi-modal representations. To address these issues, we introduce a novel training framework, Alt-MoE, which employs the Mixture of Experts (MoE) as a unified multi-directional connector across modalities, and employs a multi-step sequential alternating unidirectional alignment strategy, which converges to bidirectional alignment over iterations. The extensive empirical studies revealed the following key points: 1) Alt-MoE achieves competitive results by integrating diverse knowledge representations from uni-modal models. This approach seamlessly fuses the specialized expertise of existing high-performance uni-modal models, effectively synthesizing their domain-specific knowledge into a cohesive multi-modal representation. 2) Alt-MoE efficiently scales to new tasks and modalities without altering its model architecture or training strategy. Furthermore, Alt-MoE operates in latent space, supporting vector pre-storage and real-time retrieval via lightweight multi-directional MoE, thereby facilitating massive data processing. Our methodology has been validated on several well-performing uni-modal models (LLAMA3, Qwen2, and DINOv2). achieving competitive results on a wide range of downstream tasks and datasets.", "sections": [{"title": "1 Introduction", "content": "Human perception is inherently multi-modal, seamlessly integrating diverse sensory inputs from vision, hearing, touch, and other senses to comprehend the world. Inspired by this capability, multi- modal learning aims to develop Artificial Intelligence (AI) systems that can process and interpret multiple types of input simultaneously, thereby mimicking human-like cognition. multi-modal large- scale models can process and integrate information from multiple modalities such as text, images, audio and video, and have become an important way to solve complex tasks involving heterogeneous data sources and achieve general artificial intelligence Alayrac et al. [2022], Radford et al. [2021], Wang et al. [2022a, 2023], Gao et al. [2024]."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 End-to-end multi-modal Learning", "content": "Recently, end-to-end multi-modal models employing various architectures have achieved outstanding performance. These architectures can be broadly categorized into several main types: Dual-encoder Radford et al. [2021], Jia et al. [2021]. Fusion-encoder Li et al. [2020], Jia et al. [2021], Chen et al. [2020]. Combining dual encoder and fusion encoder architectures integrate specialized layers into multi-modal models to enable deep cross-modal interactions Li et al. [2021, 2022].\nThe majority of multi-modal learning method employ large-scale multi-modal datasets for end-to-end pre-training. However, as model scale continues to increase, several potential challenges emerge: Firstly, the pre-training process may incur prohibitively high computational costs. Secondly, these models often struggle to adapt to novel modalities or tasks without extensive retraining. Moreover,"}, {"title": "2.2 Multi-modal Learning with uni-modal Models", "content": "Recent trends in multi-modal learning have increasingly focused on integrating high-performance uni-modal models to achieve effective multi-modal capabilities. Flamingo Alayrac et al. [2022] integrates visual information into each layer of a frozen Large LLM through the use of cross-attention. BLIP-2 Li et al. [2023] introduces an additional vision-to-language adaptation module, Q-former, and proposes a two-stage training process to mitigate the challenges associated with learning vision- language alignment. However, both methods require substantial parameters and multi-modal data for cross-modal alignment.\nRecent studies indicate a convergence of representations across modalities, providing evidence for the feasibility of developing advanced multi-modal models by connecting high-performance uni-modal models with lightweight parameters and data-efficient techniques Huh et al. [2024]. For instance, LLaVA Liu et al. [2024] achieved state-of-the-art performance by employing a two-layer multilayer perceptron (MLP). Similar architectures have subsequently proliferated across various domains Li et al. [2024], Zhang et al. [2024], Lin et al. [2024, 2023]. Alt-MoE further advances this concept by interconnecting diverse high-performance uni-modal models through a shared multi-directional MoE. We conducted extensive experiments focused on alignment efficacy, demonstrating the effectiveness of joint training across multiple modalities and directions."}, {"title": "2.3 Multi-modal learning with MoE and AGD", "content": "Prior studies have investigated AGD-based multi-modal multi-task alternating training, revealing that the integration of diverse modalities, tasks, and resolutions can yield mutual benefits, thereby effectively enhancing the model's generalization capabilities and cross-domain performance Akbari et al. [2023], Likhosherstov et al. [2021]. we further extend this approach to integrate existing pre-trained high-performance uni-modal models, achieving overall alignment through alternating bidirectional alignment.\nMoE-LLaVA Lin et al. [2024] proposes MoE-Tuning, a strategy for Large Vision-Language Models that creates a sparse model with constant computational cost. While both MoE-LLaVA and Alt- MoE employ sparse MoE to connect high-performance uni-modal large models, Alt-MoE not only scales this approach to large audio-visual-language models but also achieves pairwise bidirectional alignment across modalities."}, {"title": "3 Problem Formulation", "content": "In this section, we will first introduce the overall optimization objective of multi-modal bidirec- tional alignment, followed by the optimization objective of unidirectional alignment, and finally the alignment loss and theoretical explanation. It is worth noting that by integrating uni-modal models, Alt-MoE can effectively align multiple modalities. To clearly articulate the design rationale, we illustrate this with the example of image-text alignment.\nMulti-modal alignment objective: Alt-MoE decomposes multi-modal alignment into multi- ple unidirectional alignment subtasks, where MoE serves as a knowledge representation and"}, {"title": "Alternating unidirectional alignment", "content": "Alt-MoE maximizes mutual information and mini- mizes conditional entropy to obtain independent and shared information for unidirectional alignment. Specifically, Alt-MoE adds embeddings as prior information to the input repre- sentations to guide the MoE in performing different pre-training tasks.\nWe provide an information-theoretic explanation to elucidate the rationale behind the design of the training objective.\nNext, we will describe the optimization process at two levels, including the optimization objectives and parameter updates."}, {"title": "3.1 Multi-modal Alignment objective:", "content": "In this section, we will introduce the decomposition of the multi-modal bidirectional alignment objective. multi-modal alignment aims to align diverse modalities in a latent space by finding optimal parameters @ that minimize an alignment loss $\\mathcal{L}_{align}$. This can be formulated as:\n$\\theta^* = \\arg \\min_\\theta \\mathcal{L}_{align}(\\theta)$.\nBy combining MoE $f_\\theta(\\cdot)$ parameterized by $\\theta$ and AGD, which can decompose the optimization objectives for multi-modal alignment $\\mathcal{L}_{align}$ into multiple unidirectional alignment subtasks, and then alternately execute each subtask at various time step to achieve overall alignment. Specifically, the image-text alignment can be decomposed as follows in the following Equation 2:\n$\\mathcal{L}_{i-t}(\\theta) = \\mathcal{L}_{i\\rightarrow t}(\\theta_{i\\rightarrow t}) \\oplus \\mathcal{L}_{t\\rightarrow i}(\\theta_{t\\rightarrow i}),\\newline \\theta = \\theta_{i\\rightarrow t} \\cup \\theta_{t\\rightarrow i}$,\nwhere $\\mathcal{L}_{i\\rightarrow t}, \\theta_{i\\rightarrow t}$ represents the image-to-text ($t \\rightarrow i$) alignment objective and parameter subset, $\\mathcal{L}_{t\\rightarrow i}, \\theta_{t\\rightarrow i}$ represents the text-to-image ($i \\rightarrow t$) alignment objective and parameter subset, $\\oplus$ denotes an alternating optimization operation at various time step. Therefore, we decompose the image-text alignment into multiple unidirectional alignment optimizations and parameter subsets."}, {"title": "3.2 Alternating Unidirectional Alignment", "content": "In this section, we delineate the optimization objectives and parameter update procedures across various time steps $t$. By introducing AGD, we can alternately optimize unidirectional alignment at each time step $t$ with the goals of maximizing mutual information between image $I$ and text $T$, denoted as $I(I; T)$, and minimizing conditional entropies, denoted as $H(T|I)$ and $H(I|T)$.\nGiven time step $t$, the objective function is updated as Equation 3:\n$\\mathcal{L}_{i-t} =\\begin{cases} \\mathcal{L}_{i\\rightarrow t}(t) = -I(I;T) + H(T|I), & \\text{if } t = 2k \\\\ \\mathcal{L}_{t\\rightarrow i}(t) = -I(I;T) + H(I|T), & \\text{if } t = 2k + 1, \\end{cases}$\nwhere $k$ is a non-negative integer. Based on Equation 1 and 3, the overall optimization objective can be formulated as shown in Equation 4:\n$\\theta^* \\triangleq \\arg \\min_\\theta (-I(I; T) + \\lambda(H(T|I) + H(I|T)))$, \nwhere $\\lambda$ is a weight parameter.\nGiven a set of parameters $\\theta$, we alternate between image-to-text and text-to-image unidirectional alignment at different time steps. At different time steps, we update only a subset of the parameters: $\\theta_{i\\rightarrow t}$ for $i\\rightarrow t$ alignment and $\\theta_{t\\rightarrow i}$ for $t \\rightarrow i$ alignment. Ultimately, this process ensures that all parameters are updated, such that $\\theta = \\theta_{i\\rightarrow t} \\cup \\theta_{t\\rightarrow i}$. The overall update process can then be described by the following Equation 5:\n$\\theta^{t+1} = \\begin{cases} \\theta^t - \\eta \\nabla_{\\theta_{i\\rightarrow t}} \\mathcal{L}_{i\\rightarrow t}^t, & \\text{if } t = 2k \\\\ \\theta^t - \\eta \\nabla_{\\theta_{t\\rightarrow i}} \\mathcal{L}_{t\\rightarrow i}^t, & \\text{if } t = 2k + 1, \\end{cases}$\nwhere $\\eta$ is the learning rate, $k$ is a non-negative integer."}, {"title": "3.3 Information Decomposition and Alignment", "content": "For image to text alignment, conditional entropy $H(T|I)$ and $H(I|T)$ represent modality-specific information in text and image respectively, measuring uncertainty in one modality after observing the other. Mutual information $I(T; I)$ quantifies shared information between image and text modalities, indicating how much knowing one reduces uncertainty about the other. For accurate image-text alignment, high mutual information (more shared content) and low conditional entropy (less modality- specific information) are desirable, ensuring strong semantic coupling between modalities.\nAlt-MoE leverages the MoE router to automatically select different experts, optimizing for these two objectives. This approach helps decouple modality-specific information from shared informa- tion, potentially improving the balance between capturing unique modal features and cross-modal relationships."}, {"title": "4 Methodology", "content": "In this section, we will provide a detailed introduction to the architecture of Alt-MoE. As shown in Figure 2, Alt-MoE is divided into three modules: the visual model (VM), the language model (LM), and the fusion module MoE. We input the paired images and text $(I, T)$ into VM and LM respectively to obtain latent representations $z_i, z_t$, and then perform multi-modal interaction in the fusion module."}, {"title": "4.1 Image and Text Encoding", "content": "Given a pair of image and text inputs $(I, T)$, we employ separate encoders to process each modality. The visual model (VM) $f_v(\\cdot)$ encode the image $I$, while the language model (LM) $f_l(\\cdot)$ encode the text $T$. This process results in latent representations $z_i$ and $z_t$ for the image and text, respectively. The encoding can be formally expressed as:\n$z_i = f_v(I), \\quad z_i \\in \\mathbb{R}^{d_i},\\newline z_t = f_l(T), \\quad z_t \\in \\mathbb{R}^{d_t}$,\nwhere $z_i \\in \\mathbb{R}^{d_i}$ is the image representation, and $z_t \\in \\mathbb{R}^{d_t}$ is the text representation."}, {"title": "4.2 Unidirectional Alignment", "content": "At different time steps $t$, Alt-MoE performs unidirectional alignment tasks using different parameter updates and optimization objectives according to Equations 3 and 5. To guide MoE in selecting different experts for different modalities and tasks, we set up trainable modality encodings \u0415\u0442, \u04151 and trainable task encodings Ece, Emi. Figure 3 illustrates the cross embedding process, where these embeddings are combined and added to $z_t, z_i$ to guide the execution of different tasks. Specifically,"}, {"title": "4.3 Loss Function", "content": "We adopt a unidirectional prediction approach at each time step to minimize conditional entropy. This method involves predicting either text features from image features or image features from text features, alternating between time steps. We define the prediction loss function using the L2 distance as follows:\n$\\mathcal{L}_{CE} = ||\\hat{z_t} - z_t||_2 + ||\\hat{z_i} - z_i||_2$,\nwhere $|\\cdot |$ denotes the squared L2 norm. $\\hat{z_t}$ and $\\hat{z_i}$ are the predicted text and image features, respectively, and $z_t$ and $z_i$ are the corresponding target features. At each time step, only one of these terms is active, depending on the prediction direction.\nHere, we set $\\hat{z}^{mi}_t$ as $\\hat{z}^{T}$ and $\\hat{z}^{mi}_i$ as $\\hat{z}^{I}$ and contrastive loss can be formulated as follows:\n$\\mathcal{L}_{MI} = - \\frac{1}{2N} \\sum_{i=1}^N \\left[ \\log \\frac{\\exp(\\text{sim}(z_i, \\hat{z_i}) / \\tau)}{\\sum_{j=1}^N \\exp(\\text{sim}(z_i, \\hat{z_j}) / \\tau)} + \\log \\frac{\\exp(\\text{sim}(z_i, \\hat{z_i}) / \\tau)}{\\sum_{j=1}^N \\exp(\\text{sim}(z_j, \\hat{z_i}) / \\tau)} \\right]$,\nwhere:\n* N is the number of image-text pairs in a batch.\n* $z_i$ and $\\hat{z_i}$ are the latent representations of the $i$-th image and its corresponding text, respec- tively.\n* $\\text{sim}(z_i, \\hat{z_i})$ is the cosine similarity between the latent representations $z_i$ and $\\hat{z_i}$.\n* $\\tau$ is a temperature parameter that controls the sharpness of the similarity distribution."}, {"title": "5 Experiments and Results", "content": "We present the results of Alt-MoE on multiple modalities (audio, text, image) and various tasks and utilize Low-Rank Adaptation (LoRA) Hu et al. [2021] for fine-tuning the final layers of uni-modal models. First, we perform image-text retrieval on COCO Lin et al. [2014] and Flickr30K Plummer et al. [2015] by integrating LLMs such as LLAMA3 Dubey et al. [2024] and Qwen2 Oquab et al. [2023] with LVMs such as Dinov2 Oquab et al. [2023]. Then, we perform audio-text retrieval by integrating LLMs and LAMs such as Qwen2 Yang et al. [2024] and Whisper Radford et al. [2023]."}, {"title": "5.1 Image-Text Retrieval", "content": "For the image-text retrieval task, since Alt-MoE connects two uni-modal models through a multi- directional MoE, we train the multi-directional MoE on the training sets of COCO and Flickr30K, and then test it on the test sets.\nFurthermore, by comparing different architectures of multi-modal models, Alt-MoE demonstrates superior data efficiency and parameter efficiency. Specifically, Alt-MoE has only 140M trainable parameters, which is significantly smaller than BLIP-2's 1.2B trainable parameters. Furthermore, BLIP-2 requires pre-training on 129M images before fine-tuning on COCO, whereas Alt-MoE is trained on COCO and Flickr30K.\nIn conclusion, the high efficiency in both parameters and training data further demonstrates that the representations are converging across modalities. This efficiency underscores the potential of a modality-agnostic multi-modal alignment strategy that can achieve alignment across various modalities using a lightweight model."}, {"title": "5.2 Audio-text Retrieval", "content": "As part of our ongoing research, we are currently exploring the potential of Alt-MoE in audio-text retrieval tasks. This extension aims to validate Alt-MoE's scalability to new tasks and modalities while maintaining its architecture and training strategy. In this work-in-progress, we are in the process of integrating existing high-performance Audio Models and Language Models, into the Alt-MoE framework. Our goal is to assess its performance on audio-text retrieval tasks. This investigation is"}, {"title": "5.3 Validation of Alignment Objectives", "content": "Alt-MoE utilizes the MoE router to automatically select different experts, optimizing for both ob- jectives. This approach helps decouple modality-specific information from shared information, potentially improving the balance between capturing unique modal features and cross-modal relation- ships.\nTo validate this conclusion, we applied the predicted representations to retrieval tasks. The results in Table 2 indicate that training solely on the prediction alignment objective yields significantly lower performance compared to jointly training on both prediction and contrastive learning objectives.\nBy jointly optimizing for prediction and contrastive learning, the model maximizes the mutual information, while minimizing the conditional entropy. This ensures that Alt-MoE capture the most relevant and informative features, leading to better alignment and retrieval performance. The mutual reinforcement of the two tasks helps in effectively balancing the trade-off between capturing modality-specific features and learning cross-modal relationships."}, {"title": "5.4 Sensitivity Analysis", "content": "Figure 4 shows the sensitivity analysis of loss weights on average recall 1. The figure compares the average recall rates of contrastive learning and prediction models across different loss weights (0.4, 0.6, 0.8, 1.0). Both models achieve the highest recall at a loss weight of 1.0."}, {"title": "5.5 Ablation Study", "content": "Our model achieves excellent performance through the use of MoE. To validate its effectiveness, we conducted ablation experiments by replacing MoE with MLP and comparing the results. Additionally, we examined the impact of alternating optimization versus non-alternating optimization.\nThe results of the ablation study, as shown in Table 3, indicate that replacing MoE with MLP leads to a significant drop in performance, demonstrating the critical role of MoE in our model's success. Furthermore, the comparison between alternating optimization and non-alternating optimization reveals that alternating optimization contributes to better model performance, highlighting its importance in the training process.\nIn summary, the ablation experiments confirm that both the MoE architecture and the alternating optimization strategy are essential components for achieving SoTA performance in our model."}, {"title": "6 Current Limitation", "content": "In this work, we propose a new paradigm for multi-modal alignment, which has been applied on several multi-modality matching tasks and achieved state-of-the-art on retrieval metrics. This approach temporarily has not been applied on generative missions. However, by appending the multi-modal connector into an off-the-shelf decoder, we expect this framework can also help improve generative performance.\nAlt-MoE is a typical self-supervised learning framework which can theoretically help build the world model. The input and output signals (x and y in Figure 1) can be any observed and unobserved sample parts of real world information, spanning over arbitrary combination of modalities, as well as different time and space slots. Among currently studied tasks, each of x and y is a uni-modality sample, and we focus on the cross-modality matching problem. In the future, we will study a more general problem and aim to provide a general framework for self-supervised world modeling."}, {"title": "7 Conclusion", "content": "In this study, we introduced Alt-MoE, a novel modality-agnostic training strategy and architecture designed for multi-modal learning. We implement Alt-MoE based on the joint-embedding predictive architecture, to achieve the multi-modal alignment in the latent-variable space. Alt-MoE effectively integrates high-performance uni-modal models using lightweight connection modules, facilitating the alignment of modality pairs in multiple directions and enabling generalization to new tasks and modalities. Application of lightweight multi-directional MoE ensures both training and data efficiency. To validate the modality and task scalability of Alt-MoE, we conducted extensive experiments focused on alignment performance. The experimental results demonstrate that Alt-MoE can easily generalize to new modalities, tasks, and datasets while maintaining the same training strategy and architecture. Furthermore, Alt-MoE offered a simple, efficient, and scalable solution for multi-modal alignment and large-scale retrieval. By capitalizing on the strengths of existing high-performance uni-modal models, Alt-MoE provides a practical approach to achieving state-of-the-art performance across various tasks and datasets, underscoring its generalizability and effectiveness."}]}