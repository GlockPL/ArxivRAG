{"title": "Understanding Adam Requires Better Rotation Dependent Assumptions", "authors": ["Lucas Maes", "Tianyue H. Zhang", "Alexia Jolicoeur-Martineau", "Ioannis Mitliagkas", "Damien Scieur", "Simon Lacoste-Julien", "Charles Guille-Escuret"], "abstract": "Despite its widespread adoption, Adam's advantage over Stochastic Gradient Descent (SGD) lacks a comprehensive theoretical explanation. This paper investigates Adam's sensitivity to rotations of the parameter space. We demonstrate that Adam's performance in training transformers degrades under random rotations of the parameter space, indicating a crucial sensitivity to the choice of basis. This reveals that conventional rotation-invariant assumptions are insufficient to capture Adam's advantages theoretically. To better understand the rotation-dependent properties that benefit Adam, we also identify structured rotations that preserve or even enhance its empirical performance. We then examine the rotation-dependent assumptions in the literature, evaluating their adequacy in explaining Adam's behavior across various rotation types. This work highlights the need for new, rotation-dependent theoretical frameworks to fully understand Adam's empirical success in modern machine learning tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities as their scale grows (Brown et al.,"}, {"title": "2 PRELIMINARIES", "content": "matching Definition 1.\n2.1 Notations and Settings\nLet f: Rd \u2192 R be the loss of a neural network with d trainable parameters. Stochastic optimization algorithms approximate arg minw\u2208rd f(w) by only accessing independent stochastic functions fB that depend on a stochastic minibatch B following some data distribution D such that \u2200w \u2208 Rd, EB~D[fB(w)] = f(w).\nOur study examines the optimization process under rotations of the parameter space. More formally, let SO(d) be the set of rotation matrices,\nSO(d) = {R\u2208Rdxd: RTR = RRT= I, det(R)=1}. (1)\nInstead of directly optimizing f, we consider its rotated counterpart f(R) : w \u2192 f(Rw), R \u2208 SO(d). This transformation rotates the coordinate system while preserving the geometry of the optimization landscape.\nUnless specified otherwise, we use the popular AdamW variant of Adam (see Appendix D).\n2.2 Rotational Equivariance of SGD\nWe say that an optimizer is rotation equivariant if, after a rotation of the parameter space, its trajectories are equally rotated.\nDefinition 1 (Rotational equivariance). Consider an optimization algorithm A applied to the function f, generating iterates as follow:\nWt+1 = A({Wi}i=0...t, f, t).\nwe say that the optimization algorithm is rotation equivariant if it satisfies, VR \u2208 SO(d),\nRWt+1 = A({Rwi}i=0...t, f(R), t).\nProposition 1. Stochastic Gradient Descent is rotation-equivariant.\nProof. Using the same notation as in Section 2.1, we consider SGD* with learning rate n and a fixed sequence of batches Bt:\nZt+1 = A(zt, f, t) = zt - n\u2207fB\u2081(zt),\nFor any wo \u2208 Rd, we have Rwo = A(Rwo, f(R),0). Moreover, we have VfBr) (wt) = RVfB\u2081 (RTwt), hence\nf(R)\nBt\nA(Rzt, f(R), t) = Rzt \u2013 nRVfB\u2084(RTRzt)\n= Rzt - NRVfB\u2081(zt)\n= RA(zt, f, t) = R2t+1.\n*For the sake of simplicity we are not considering momentum here, but the results similarly hold for heavy ball (Polyak, 1964) and Nesterov Accelerated Gradient (Nesterov, 1983).\nIn contrast, as illustrated in Figure 2, Adam is not rotation equivariant. This dependence stems from its element-wise division, see Algorithm 2, step 9 in Appendix D.\n2.3 Training Neural Networks in Rotated Parameter Spaces\nA crucial aspect of our study is the empirical evaluation of Adam's performance under rotations of the parameter space.\nOur approach, described in Figure 3, maintains the weights wt in the standard basis while performing Adam's optimization steps in the rotated space. This method allows us to leverage existing neural network frameworks for forward and backward propagation while still examining Adam's behavior under rotation.\nRotations in high dimension. It is computationally intractable to operate with full d \u00d7 d rotation matrices due to the size of modern neural networks.\nWe employ a composite approach that combines block-diagonal rotations with strategic permutations to circumvent this limitation while preserving the essential characteristics of uniformly sampled rotations. This method effectively emulates the statistical properties of full-scale rotations. A detailed description of our sampling process and ablation studies are provided in Appendix A.\nNumerical considerations. Given the sensitivity of neural network training to numerical precision (Li et al., 2018; Wang et al., 2018; Sun et al., 2022), it is crucial to ensure that rounding errors from applying"}, {"title": "3 ROTATIONS' INFLUENCE ON ADAM'S EFFICIENCY", "content": "This examines the effects of random rotations at different scopes, from neuron-wise to global rotations (Section 3.1) and reveals that specific, structured rotations enhance Adam's performance (Section 3.2).\n3.1 Random Rotations\nThis section examines the impact of four random rotations in the parameter space on Adam's performance.\nGlobal Rotation: Applies the rotation to the entire parameter space.\nLayer-wise Rotation: We partition the parameter space by layers (for transformers, the keys, queries, and values are considered separate layers), independently rotating each corresponding subspace.\nOutput-wise: Similar to layer-wise, except we partition the parameter space by output neuron. We apply rotations to the weight dimensions corresponding to connections terminating at the same neuron in the subsequent layer.\nInput-wise: Same as output-wise, except we rotate weight dimensions originating from a same neuron.\nExperimental setting. Our approach to sampling high-dimensional rotations is detailed in Appendix A, and Figure 4 illustrates the different scopes of rotations. We conduct experiments across three distinct settings, and technical details such as hyperparameters are provided in Appendix B.\n1. Language Model (GPT-2, Fig. 1a): We use a 124M-parameters decoder-only transformer (Radford et al., 2019) model on the OpenWebText dataset (Gokaslan and Cohen, 2019), a canonical setup for evaluating large language models.\n2. Image Classification (Transformer, Fig. 1b): We use a small Vision Transformer (ViT/S, 22M parameters) (Dosovitskiy et al., 2021) for image classification on ImageNet-1K (Deng et al., 2009), exemplifying modern, attention-based computer vision architectures.\n3. Image Classification (ResNet, Fig. B): We employ a ResNet-50 (He et al., 2016) for image classification on ImageNet-1K, a setting in which SGD is more competitive than Adam (Keskar and Socher, 2017; Wilson et al., 2017).\nResults. We make several key observations.\n1. Adam's performance degrades under global rota-"}, {"title": "3.2 SVD Rotations Improve Performances", "content": "Inspired by GaLore (Zhao et al., 2024), which uses low-rank Singular Value Decomposition (SVD) to compress optimizer states, we extend this concept to full-rank SVD to rotate the parameter space. Our approach decomposes the gradient matrix G of each layer into G = USVT. This decomposition yields a natural rotation of the parameter space through the transformation M \u2192 U\u00afMV, which combines an output-wise rotation (via UT) and an input-wise rotation (via V). \u03a4\u03bf evaluate this method, we train a GPT2 model under the same conditions as in Section 3.1, but in this SVD-rotated space. We update the SVD decompositions every 200 steps.\nOur experiments (see Fig. 5) demonstrate that Adam's performance improves under SVD rotations. Given its relatively small computational overhead, future research could show that variations of this approach can lead to consistent, cost-effective improvements across different settings.\nOur findings underscore the importance of capturing the properties of the chosen basis to understand Adam's"}, {"title": "4 EXAMINING ROTATION DEPENDENT ASSUMPTIONS", "content": "While rotation-invariant assumptions dominate optimization literature, some frameworks incorporate"}, {"title": "4.1 $L_\\infty$ bounded gradients", "content": "Reddi et al. (2018); Kingma and Ba (2015) assume a bound on the Lo norm of stochastic gradients,\nVw \u2208 Rd, ||\u2207fB(W)||\u221e < C almost surely. (2)\nThe constant C depends on the basis as the Loo norm is not preserved under rotations. To evaluate this assumption's relevance to Adam's performance, we compute the empirical bound on the rotated gradients:\nCR(R) := max ||VfBR) (WR')||\u221e,\nBi\nwhere wr denotes the last checkpoint obtained by running Adam under rotation R'. The maximum is over 1000 stochastic minibatches Bi, across different rotations R (see Section 3). Table 1 reveals that \u0108 significantly decreases under random global rotations, predicting better performance for Adam, but we observe degradation in Adam's convergence. This discrepancy shows that the Lo gradient bound fails to capture the beneficial properties of the basis for Adam."}, {"title": "4.2 Block-diagonality of the Hessian", "content": "A common hypothesis for understanding Adam's behavior is that the Hessian is well-approximated by a block-diagonal matrix (Zhang et al., 2024a). This property might correlate with Adam's performance. Indeed, random rotations likely disrupt block-diagonality, hence degrading Adam's convergence. However, rotations within parameter spaces corresponding to diagonal blocks would preserve this structure, potentially explaining the consistent performance under random output-wise rotations.\nTo examine this assumption's validity, we sample rows of the Hessian of f(R) at a checkpoint wr':\nri (R, R') =\n1k\n1k\nk\nf(R)\n\u2211\u00b2) (WR') [i:]\nj=1\nk\nB\n\u2211RV2fB\u2081 (WR'). Rei,\nj=1\nwhere e\u00a1 denotes the ith canonical basis vector. The vector ri represents the average of the i-th row of the stochastic Hessian over k minibatches. As k increases, ri converges to the true Hessian row. We set k = 5000 in the experiments. We use efficient Hessian-vector products (hvp) (Dagr\u00e9ou et al., 2024) for this computation.\nWe partition the indices of the Hessian row ri corresponding to weight w\u2081 into three disjoint subsets:\nri = ri[In] + ri[LL] + Fi{1,},\nwhere\n\u2022 IN are indices of weights leading to the same output neuron as wi,\n\u2022 IL are indices of other weights from the same layer,\n\u2022 If are indices of weights of other layers,\nand ri[IN] = ri (resp. IL and I\u2081) in the indices in In (resp. IL and I\u2081) and zero elsewhere.\nFigure 7 presents the distribution of absolute values for each subset. Our findings show that entries in In and, to a lesser extent, IL, are significantly larger than those from I\u2081, supporting the notion of an approximately block-diagonal Hessian.\nTo assess the practical implications of this structure, we evaluate how each block contributes to gradient variations for a given small direction \u03b4\u03c9. This local variation can be approximated and decomposed as:\n[\u2207f(w + \u03b4\u03c9) \u2013 \u2207 f(w)]i \u2248 e\u2207\u00b2 f(w)\u03b4\u03c9\n= ri[IN] \u00b7 W + Fi[IL] \u00b7 \u03b4\u03c9 + ri[11] \u00b7 \u03b4\u03c9.\nTable 2 quantifies these contributions in either a random direction or Adam's update direction. Surprisingly, our results reveal that Hessian values outside the block diagonal are the primary drivers of gradient evolution, despite their smaller magnitude."}, {"title": "4.3 $L_\\infty$-smoothness and (1,1)-norm", "content": "Lo-smoothness was recently shown to guarantee the convergence of Adam, and presented as a potential key property of the basis in Xie et al. (2024). We first remind its definition.\nDefinition 2. A function f is C-smooth wrt. || ||\u221e if || \u2207 f(x) - \u2207 f(y)||1 \u2264 C||x \u2212 y ||\u221e \u2200 x, y \u2208 Rd.\nGiven the challenges in directly estimating the Lo smoothness constant, Xie et al. (2024) proposed using the (1,1)-norm of the Hessian as a surrogate measure. This norm is defined as:\nMN\n||H|| (1,1) := \u03a3\u03a3 Hmn,\nm=1n=1\nWhere Hmn represents the element at the m-th row and n-th column of the Hessian matrix. Notably, they observed a degradation in their estimate of ||H||(1,1) under global random rotations.\nFigure 8 illustrates the variations in ||H||(1,1) under global, svd, and output-wise rotations, and yields several significant findings:\n\u2022 Under global rotations, we confirm the findings of Xie et al. (2024) that the (1, 1)-norm also degrades.\n\u2022 Interestingly, we find that SVD rotations lead to an improvement in the (1,1)-norm. This enhancement correlates positively with the observed boost in Adam's performance under these rotations, sug-"}, {"title": "5 RELATED WORK", "content": "Adam under rotations. We consider the concurrent work Xie et al. (2024) to be the closest related study, showing that Adam converges more slowly with a randomly rotated loss landscape. They provide convergence analysis based on L\u221e geometry, demonstrating that this yields a better empirical smoothness constant for GPT-2 models. While their work offers valuable theoretical insights, our study takes a more experimental stance. We aim to paint a comprehensive picture of Adam's behavior under a spectrum of rotations, from random to structured transformations, and evaluate how existing rotation invariant assumptions correlate with Adam performance.\nUnderstanding Adam. Our work casts light on the critical interactions between Adam and the coordinate system, contributing to a growing body of research on Adam's behavior and convergence. Recent works have attributed Adam's success to the heterogeneous block-diagonal structure of its Hessian (Zhang et al., 2024a), though we find this assumption to be unrealistic. Others have improved convergence guarantees: D\u00e9fossez et al. (2022) and Guo et al. (2022) offered simplified and novel derivations, Zhang et al. (2022) argued that vanilla Adam converges without modification, Zhou et al. (2024) provided a general convergence analysis for adaptive methods in non-convex settings, and Li et al. (2023b) proposed a convergence proof for Adam without relying on globally bounded gradients. Li et al. (2023a) developed a convergence analysis based on generalized smoothness conditions, and H\u00fcbler et al. (2023)"}, {"title": "6 DISCUSSION", "content": "6.1 Limitations\nWhile our study demonstrates the critical need for a proper characterization of the beneficial properties of the Hessian in understanding Adam's performance, it is important to note that we do not propose a specific property that fully captures these benefits. Instead, we lay the groundwork by providing a detailed blueprint for this missing theoretical tool, outlining the key characteristics it should possess, and provide novel insights into the relationship between Adam and the standard base.\n6.2 Conclusion\nIn this work, we have conducted a comprehensive investigation into Adam's sensitivity to rotations of the parameter space, revealing key insights into its optimization dynamics. We demonstrated that some rotations possess advantageous properties, opening new avenues for algorithmic contributions to adaptive algorithms. Our study demonstrates that Adam's performance is intricately tied to the choice of basis, a relationship that existing theoretical frameworks struggle to capture adequately. This investigation highlights the limitations of current rotation-invariant assumptions in explaining Adam's behavior and underscores the need for new, basis-dependent theoretical tools. As the field continues to evolve, we anticipate that these findings will spark new avenues of research, potentially leading to more robust optimization algorithms and deepening our understanding of the fundamental principles underlying successful deep learning optimization."}, {"title": "A SAMPLING RANDOM ROTATIONS IN HIGH DIMENSION", "content": "This section explains our method of sampling random rotations for high-dimensional spaces and the implementation details.\nA.1 High-Dimensional Rotations\nEven small modern machine learning models typically have millions of parameters. Consequently, storing a dxd rotation matrix is often intractable, let alone performing the dot product required to rotate the gradient vector. To address this issue, we sample an\u00d7n rotation matrix Rn with n < d uniformly (in the sense of the Haar measure) from the special orthogonal group SO(n), and a random permutation \u03c0\u03bff 0, . . ., d \u2013 1. For now, we assume\u2208 N, see appendix A.3 for a general case. To rotate a gradient g, we compute:\ng(R\u03b7,\u03c0) := \u03c0\u22121 \u3002\n(\n(\nd/n\n\u2295\ni=1\nR\u03b7 (\u03c0\u03bfg)),\n(3)\n= \u03c0-1 \u03bf\n(\nRn\nRn\n0\n(\u03c0\u03bfg),\n(4)\n0\nRn\nRn\nwhere denotes the direct sum operation, producing a block-diagonal matrix with d/n blocks Rn. This procedure effectively computes a rotation by blocks of size n picked from a random partition of indices, constituting a valid rotation.\nIntuitively, if n is sufficiently large, we expect this procedure to approximate well the effect of random rotations sampled uniformly from SO(d), due to the law of large numbers homogenizing geometric properties across coordinates. To confirm this intuition, we perform an ablation study in Figure 9, finding that the impact on Adam's performance saturates well below our operational values.\nOur approximation reduces the memory cost from O(d\u00b2) to O(n\u00b2 + d), and the computational cost from O(d2) to O(nd). Since batch matrix multiplications required for the rotation can be performed efficiently on modern GPUs, the final overhead of applying rotations is extremely small.\nA.2 Reflections and Sampling From The Haar Measure\nTo sample Rn uniformly from SO(n) with respect to the Haar measure, we employ the QR decomposition trick (Mezzadri, 2007; Ozols, 2009), which samples from the Haar measure \u03bc of the orthogonal group O(n). Let us consider the projection \u03c0 : O(n) \u2192 SO(n), such that \u03c0(R) is R when R \u2208 SO(n), and \u03c0(R) simply multiplies the first column of R by -1 when R \u2208 O(n) \\ SO(n). The push forward of \u03bc by \u03c0is the Haar measure on SO(n). Since Adam is reflection equivariant, rotating with \u3160(R) and with R will lead to identical performance for any R\u2208 O(n). Thus we can omit to apply \u03c0, and simply sample from u using the QR decomposition method.\nSimilarly, Adam is permutation equivariant, thus we omit to apply the inverse permutation before providing the rotated gradients to Adam, and to apply the permutation before rotating the update, as removing these two steps do not affect performances.\nA.3 Rotation Residual\nBased on the type of rotation and the chosen dimension n, the number of blocks may not divide evenly, i.e.,"}, {"title": "A.4 Overall Validation and Impact of Flash Attention", "content": "In Figure 10, we present the training loss when training GPT-2 with SGD without rotations, with global random rotations using flash attention, and with global random rotations without flash attention. In particular, we confirm two important observations:\n\u2022 Without flash attention (the setting we use for our experiments) the performances of SGD under global random rotation and under no rotations are identical. This validates that our experimental setting is behaving as expected.\n\u2022 When we use flash attention with rotations, we observe a slight difference in performance. As explained in Section 2.3, this is due to flash attention amplifying numerical errors from the application of the rotation. Interestingly, likely due to a slight regularization effect, it slows down performance at first but actually provides a small improvement to the loss at the end of training."}, {"title": "\u0412 EXPERIMENTAL DETAILS", "content": "This section provides additional details about the hyperparameters used for the architecture mentioned in the paper, as well as their optimizer and rotations.\nB.1 Rotations Design Choices\nBy default, for random rotations we fix the dimension of our rotation matrix Rn at 768 (which is the hidden dimension and thus makes residual rotations unnecessary for most rotation types). The matrix is sampled at the start of training, remains fixed throughout, and is shared across blocks the entire training process.\nSVD Rotation. Following Zhao et al. (2024), this is the only rotation that is dynamic rather than static. Specifically, we compute the full-rank SVD decomposition of the gradient for each layer every 250 steps (recommended frequency in Zhao et al. (2024)).\nRotation in Transformers. By default, many implementations store the query, key, and value parameters within a single linear layer. Thus, we split them to treat them as separate layers, reflecting the fundamental differences in how their parameters are involved in forward computations. Additionally, PyTorch stores parameters as tensors in the shape (output_dim, input_dim), but embeddings are stored as lookup tables in the shape (input_dim, output_dim). For output neuron and input neuron rotations to behave intuitively, we thus transpose embedding layers before and after rotations.\nB.2 Architectures\nGPT2 (Transformer). We trained a GPT-2 model with 124M parameters on the OpenWebText dataset (Gokaslan and Cohen, 2019) using a configuration designed for efficient pretraining. The model architecture includes 12 layers, 12 attention heads, and a 768-dimensional embedding space, with no bias in LayerNorm or Linear layers. We employed the AdamW optimizer with a peak learning rate of 6e-4, \u03b2\u2081 = 0.9, \u03b22 = 0.95, and a weight decay of le-1, applying gradient clipping of 1.0. Training ran for 100,000 iterations (or 30,000 for some smaller ablations), with learning cosine rate decay starting after a 2,000-iteration warm-up, decaying to a minimum of 6e-5. We used batch size of 12 with gradient accumulation steps simulating an effective batch size of 40. All experiments were performed on four A100 80GB GPUs, leveraging mixed precision. Unless otherwise specified, all optimizer hyperparameters were shared across experiments and set to the default values specified in Karpathy (2022).\nViT (Vision Transformer). We trained a Vision Transformer (ViT) model on the ImageNet-1K dataset (Deng et al., 2009) using the SimpleViT architecture (Beyer et al., 2022). The model consists of 12 layers, 6 attention heads, a hidden dimension of 384, and an MLP dimension of 1536, with a patch size of 16 and input image size of 224. The AdamW optimizer was employed with a learning rate of 0.001, \u03b2\u2081 = 0.9, \u03b22 = 0.999, \u20ac = 1e - 8, and a weight decay of 0.1. We used a cosine learning rate schedule with 5 warm-up epochs. The training was conducted for 100 epochs with a batch size of 1024. All experiments were performed with mixed precision.\nResNet-50 (CNN). We trained a ResNet-50 model (He et al., 2015) on the ImageNet-1K dataset (Deng et al., 2009) using the AdamW optimizer. The optimizer was configured with a learning rate of 0.001, \u03b2\u2081 = 0.9, \u03b22 = 0.999, \u20ac = 1e-8, and a weight decay of 0.0001. We employed a cosine learning rate schedule with 5 warm-up epochs. The training ran for 100 epochs with a batch size of 256.\nB.3 Assumptions Estimation\nWe now outline how we computed empirical estimations of assumptions in Section 4.\nLx-bounded gradient. Algorithm 1 describes the process we use to estimate the bound constant C of stochastic gradients under L\u221e norm, as detailed in section 4.1.\n(1,1)-Norm. Using the Hessian rows sampled from GPT-2 checkpoints that were trained under various rotations in Section 4.2, we estimate ||H||(1,1) by averaging the L\u2081 norm of sampled rows. While this could induce a large"}, {"title": "CADDITIONAL RESULTS", "content": "C.1 Main Experiments\nWe provide additional results from our main line of experiments.\nViT/S (ImageNet). Figure 11 extends the results from Figure 1b with validation loss and accuracy\nResNet50 (ImageNet). Figure 12 demonstrates that Adam maintains its performance well under rotational transformations for ResNets. This robustness to rotation implies that Adam gains little advantage from the standard basis structure in this setting. This finding aligns with the fact that SGD with extensive tuning can outperform Adam when training these networks."}, {"title": "C.2 Architecture Aware Rotation.", "content": "We seek to identify whether certain transformer layer types are more sensitive to rotations than other, and contribute more to the overall performance degradation observed in Figure la when using layer-wise rotations.\nFigure 13 shows the loss curves when rotating only one layer type at a time. We find that the performance degradation induced by layer-wise rotations is small for most layer types, seemingly balanced across these layers, with the exception of value and embedding layers.\nLayerwise rotation of value layers seem to impact the loss more noticeably than with other layer types. In Figure 15, we find that reducing the scope of rotations to output neuron wise does not improve the performance when rotating value layers.\nThe biggest drop of performances is observed for embedding layers, which we conjecture to be linked to the discrepancy in frequency across tokens. Figure 14 shows indeed that when rotating the embedding layer by output neuron (i.e., within weights corresponding to a same token) the degradation becomes unnoticeable."}, {"title": "C.3 Hessian Rows", "content": "We use the end checkpoint of GPT2 to sample rows from the Hessian in different rotated parameter spaces (see Section 4.2).\nFrom Figure 16 to Figure 22, we present the same figure as in Figure 7, but for rows taken from different layer types, confirming that the behavior we observed is consistent across parameter types. Except for embeddings, rows are always taken from the second Transformer block.\nFigure 23 shows a row in the attention projection layer of the 8-th transformer block, showing our observations seem also consistent across depth.\nFigure 24 uses checkpoints trained with the same rotations as the one applied to the Hessian. We find the same behavior for no rotations, global and output-wise, but we find that with the SVD-rotated checkpoints, there is increased variance in the Hessian values outside of the layer."}, {"title": "DADAMW ALGORITHM", "content": "We remind here the AdamW algorithm (pseudocode) in Algorithm 2", "Algorithm\nRequire": "a: stepsize\nRequire: B1", "0,1)": "exponential decay rates for moment estimates\nRequire: \u03bb: weight decay coefficient\nRequire: e: small constant for numerical stability\nRequire: f(0): stochastic objective function with parameters \u03b8\n1: Initialize \u03b8\u043e", "0\n2": "while \u03b8t not converged do\n3:\nt-t+1\n4:\ng"}]}