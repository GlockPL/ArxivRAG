{"title": "NT-VOT211: A Large-Scale Benchmark for Night-time Visual Object Tracking", "authors": ["Yu Liu", "Arif Mahmood", "Muhammad Haris Khan"], "abstract": "Many current visual object tracking benchmarks such as OTB100, NfS, UAV123, LaSOT, and GOT-10K, predominantly contain day-time scenarios while the challenges posed by the night-time has been less investigated. It is primarily because of the lack of a large-scale, well-annotated night-time benchmark for rigorously evaluating tracking algorithms. To this end, this paper presents NT-VOT211, a new benchmark tailored for evaluating visual object tracking algorithms in the challenging night-time conditions. NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. To the best of our knowledge, it is the largest night-time tracking benchmark to-date that is specifically designed to address unique challenges such as adverse visibility, image blur, and distractors inherent to night-time tracking scenarios. Through a comprehensive analysis of results obtained from 42 diverse tracking algorithms on NT-VOT211, we uncover the strengths and limitations of these algorithms, highlighting opportunities for enhancements in visual object tracking, particularly in environments with suboptimal lighting. Besides, a leaderboard for revealing performance rankings, annotation tools, comprehensive meta-information and all the necessary code for reproducibility of results is made publicly available. We believe that our NT-VOT211 benchmark will not only be instrumental in facilitating field deployment of VOT algorithms, but will also help VOT enhancements and it will unlock new real-world tracking applications. Our dataset and other assets can be found at: https://github.com/LiuYuML/NV-VOT211", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of Visual Object Tracking (VOT) has undergone substantial progress, characterized by the proliferation of diverse tracking algorithms [1,5,11,8,16,18]. This advancement has been fueled by the creation of several benchmark datasets, each playing a crucial role in the evaluation and comparison of tracking methodologies. Prominent among these datasets are OTB100 [51], NfS [25], GOT-10k [24], TNL2K[48], TrackingNet [40], LaSOT [15], AVisT [41], and the VOT competition [29]. However, all of these datasets lack the very important tracking attributes required for improving night-time VOT. Despite the fact that any deployed VOT system has to handle night-time scenarios almost 50% of the time, which is quite crucial for safety and security; existing VOT benchmarks significantly lack this aspect.\nVOT is a crucial task with applications in security surveillance, autonomous driving, and wildlife conservation. Despite the significance of these applications, existing tracking algorithms have not effectively addressed the challenges posed by night-time conditions, which are prevalent in real-world scenarios. It is mainly because the current datasets fall short of significant amount of night-time scenarios for benchmarking SOTA trackers. This could compromise the effectiveness of the tracking process during night-time. The research progress towards night-time robust, practical, and accurate visual object tracking has been significantly hindered by the absence of any well-"}, {"title": "2 Existing Low-light VOT Benchmarks", "content": "Majority of existing visual object tracking (VOT) benchmarks have focused on tracking well-illuminated scenes while largely ignoring the need for tracker deployment in the low-light conditions or night-time VOT. In the darkness of night, the tracking conditions are much more adverse and pose unique and severe tracking challenges. This may cause inferior tracking performance and even tracking failure. To enable trackers to achieve adequate performance for all day tracking, some researchers have proposed low-light datasets as discussed below.\nUAVDark135 [33] dataset was built on the motivation of enabling Unmanned Aerial Vehicles (UAVs) equipped with real-time robust visual trackers for night-time aerial maneuver. It comprises of 125K annotated frames in UAV videos. Along similar lines, DarkTrack2021 [56] was proposed to facilitate tracking advancements in night-time conditions. It offers 110 challenging sequences with over 100K frames in total. Likewise, NAT2021 [57] dataset was proposed for unsupervised domain adaptive nighttime tracking. It comprises a test set of 180 manually annotated tracking sequences and a train set of over 276K unlabelled nighttime tracking frames. To asses tracking performance under adverse visibility conditions, recently AVisT [41] dataset was proposed. AVisT contains 120 sequences with 80k annotated frames. The adverse scenarios include heavy rain, dense fog, sandstorm, fire, sun glare, splashing water, low-light, small targets and distractor objects along with camouflage.\nDespite some efforts, achieving robust night-time tracking is still a formidable challenge for existing SOTA methods. This is primarily because of a lack of a large-scale night-time benchmark containing diverse and generic night-time scenarios. To this end, we introduce a novel benchmark, NT-VOT211, which comprises diverse content, distinctive low-brightness attributes, and is of large size. Compared to aforementioned datasets that mainly focus on UAV-based night-time scenarios, our proposed NT-VOT211 offers much bigger diversity in terms of number of attributes; it is more challenging in difficulty index and cause significant performance degradation for existing SOTA. Finally, it is the largest dataset exclusively focused at night-time scenarios."}, {"title": "3 The Proposed Night-time Benchmark", "content": "In this section, we compare our proposed night-time benchmark NT-VOT211 with existing benchmarks (sec. 3.1), we then present the details of various attributes in night-time benchmark (sec. 3.2), and finally we explain the protocol and steps used while collecting and annotating the proposed night-time benchmark NT-VOT211 (sec. 3.3)."}, {"title": "3.1 Comparison with other benchmarks", "content": "We compare our benchmark with existing low-light benchmarks, in terms of illumination level and attribute distribution.\nIllumination Level: Our statistical analysis considers the mean brightness and brightness variance across five tracking benchmarks including NT-VOT211. The results, shown in Figure 3, indicate that our proposed night-time benchmark (NT-VOT211) has the lowest mean illumination level. It also exhibits the highest brightness variance, highlighting substantial variations in illumination between frames. Furthermore, our dataset stands as the largest benchmark in terms of the number of annotated frames among these benchmarks.\nUnified attributes: Differing from existing benchmarks where different attributes are labeled by annotators, we introduce a unified criterion including six computer-labeled attributes in addition to two human-labeled attributes. The six computer-labeled attributes can be determined promptly after annotators finish the bounding box annotation, with further details discussed in section 3.2. We initially analyze the distribution of these attributes for all mentioned benchmarks (Table 1). It is important to note that, since GOT-10k and TrackingNet did not provide their ground truth, they are not included. Additionally, it's important to note that certain benchmarks lack frame by frame attribute-labels for occlusion and out-of-view, and this is denoted as \"-\" in the table."}, {"title": "3.2 Attributes", "content": "NT-VOT211 presents a specialized dataset that encompasses a diverse array of real-world scenarios, mirroring everyday situations where object tracking may occur. Our dataset stands out for its comprehensive coverage of six attributes labeled through automated processes. These attributes comprise camera motion, deformation, fast motion, distractors, motion blur, and tiny target. Beyond these automatically labeled attributes, our dataset incorporates two attributes labeled by human annotators: occlusion and out-of-view. These attributes are commonplace in other benchmarks as well. The automated attributes are closely related to either the hand-labeled bounding boxes or the properties of the entire frame. In other words, these attributes are generated through automated algorithms that take into account the annotated bounding boxes and the corresponding frame, rather than being subjectively determined by human annotators.\nCamera Motion: Camera motion refers to the detectable motion of the camera during recording. In many benchmarks, camera motion is often overlooked, and it is frequently associated with motion blur[51,25,15]. However, in our dataset, we have found that camera motion is nearly unavoidable using hand held video recording devices. Particularly in complex scenarios with low brightness (see Figure 4), the presence of camera motion can significantly degrade tracking performance. To detect camera motion, we use an optical flow based method by Park et al. [42]. In our dataset 8.73% frames have camera motion (Table 1)."}, {"title": "Deformation", "content": "Dealing with deformation poses a common challenge in Visual Object Tracking (VOT) tasks, primarily because the ground truth is often limited to the bounding box of the target object from the initial frame. It is unrealistic to expect the target to maintain its appearance perfectly throughout the entire sequence. In typical VOT tasks, substantial appearance variation is often associated with out-of-plane rotation [51]. However, in our dataset, this variation is more likely attributed to factors such as partial occlusion by other objects (as shown in Figure 4) or illumination changes, which are more pronounced during night-time. According to the statistics in Table 1, deformation is quite common, in 32.98% of frames in our dataset."}, {"title": "Fast motion", "content": "In the initial implementations of Correlation Filter-based trackers, such as MOSSE [7], CSK [22], and KCF [23], a fixed searching region was employed for target localization, making them susceptible to challenges posed by fast motion. However, the advent of deep neural networks (DNNs) has significantly mitigated this issue, due to the larger receptive fields. Recent research conducted by Vincent et al. [45] highlights that multi-head self-attention mechanisms have a tendency to reduce high-frequency signals. Given that fast variations in the image are often associated with high-frequency components, this poses challenges for modern Vision Transformer (ViT)-based trackers like Mixformer [11], SeqTrack-L384 [8], and others. To identify frames with rapid movement, we classify those where the target's displacement between two consecutive frames exceeds 0.5 times the frame's height or width as instances of fast motion. Table 1 shows that 1.29% of frames in our dataset exhibit fast motion."}, {"title": "Distractors", "content": "The presence of the distractors attribute indicates that some background regions may have cross-correlation exceeding 0.90 with the target region. Illustrated in Figure 4, there is virtually no noticeable distinction between the sunflower within the bounding box and the sunflower in the background. As the camera keeps moving, it's possible for the tracker to occasionally position the bounding box on the sunflower in the background rather than consistently anchoring its prediction on the target. In Table 1, it is evident that our dataset has a higher proportion of distractors compared to the other datasets, constituting 39.62% frames."}, {"title": "Motion blur", "content": "Motion blur is widely recognized as a challenging attribute in various benchmarks, such as OTB100[51], NfS [25], and LaSOT[15]. In this dataset, we detect motion blur specifically for the target using the sharpness estimation algorithm proposed by Zhengzi et al. [60]. According to Table 1, 18.21% of frames in our dataset exhibit motion blur attributes."}, {"title": "Tiny target", "content": "Tiny target is a condition where the width or height of the target is less than 3.00% that of the frame's width or height. As in Figure 4, tracking tiny targets is exceptionally challenging, even for human observers, and this challenge is further amplified in low-light conditions. Our dataset contains 28.65% frames with tiny targets (Table 1).\nTo conclude this section, we present an evaluation based on different attributes for the top-performing trackers in various categories. Specifically, we select the leading Correlation Filter tracker, Siamese Tracker, and Vision Transformer (ViT)-based tracker from our leaderboard."}, {"title": "Attribute wise Evaluation:", "content": "In our evaluation, we consider 3 Correlation Filter trackers, 3 Siamese trackers, 4 Discriminative Correlation Filter (DCF) trackers, and 4 transformer based trackers. Unlike traditional benchmarks that analyze entire video sequences with all frames labeled with a particular attribute, our methodology selectively assesses specific frames with an attribute labels to report a more precise evaluation. The exact count of frames is explicitly specified in the title above the graph. This tailored evaluation approach ensures high precision in gauging tracker performance within our unique context, deviating from the broader assessments conducted by conventional benchmarks."}, {"title": "", "content": "ment are visualized in Figure 5. The presented results indicate that challenges associated with attributes can be ordered in ascending order of difficulty: camera motion, distractors, tiny target, motion blur, occlusion, fast motion, deformation and out-of-view. We can calculate the Difficulty Index (D1) for all benchmarks using following formula:\nD\u2081 = \u2211pi log (1 + (1 \u2212 pi)(1 \u2212 si)),\ni\nwhere pi is the percentage of frames with i-th attribute and si is the maximum score on that attribute. The results are listed in the Table 1, Our method has a difficulty index of 0.42, the highest among all benchmarks."}, {"title": "3.3 Collection and Annotation", "content": "In the dataset capturing process, 10 persons were involved. Specifically, 5 collectors were assigned to record the video, 3 annotators were utilized in both the coarse and refined annotation phases, and 2 professional examiners conducted the final review of each stage from collection to annotation.\nCollection: All the videos in our dataset were recorded using handheld devices to capture real-world scenarios occurring after sunset. In the interest of ethical and legal considerations, and to respect privacy rights, our data collectors ensured that individuals present during the recordings were informed of the video capture. Their explicit consent was obtained for using this footage in the creation of a publicly available dataset. The result was a collection of 450 videos, comprising an impressive 600,000 frames. To ensure the fundamental quality of the coarse annotation, we employ cross-correlation values to assess the accuracy of the current annotation. If the value falls below 0.7, it signifies a lower quality annotation, and the annotator will need to re-annotate.\nFollowing that, as part of our effort to create a challenging test set for evaluating tracking algorithms, we carefully create a compilation of videos where target tracking presented a substantial challenge, even for human observers. To achieve that, we conducted initial assessments on all 450 coarse annotated sequences utilizing various state-of-the-art tracking algorithms, including, Mixformer[11] (SOTA on LaSOT[15]), ARTrack-L[49] (SOTA on GOT-10k[24]), SiamMask_E (SOTA on VOT2018[28]), STMTrack[16] (SOTA on OTB100[51]), NeighborTrack-OSTrack[10](SOTA on UAV123[38]) and, AiATrack[18] (SOTA on NfS[25]). To ensure a thorough evaluation of the test set, we initially assessed and computed average performance scores for the mentioned trackers across the entire set. Subsequently, we ranked the videos based on their area-under-the-curve (AUC) scores in descending order. From the top 400 videos, encompassing 500,000 frames, we randomly sampled a subset of 209,560 frames distributed among 210 videos where the target remained consistently trackable. To enhance scenario diversity, we introduced a video capturing a tiny skyborne object at dawn, introducing an additional layer of difficulty due to the target's reduced scale. The finalized dataset comprises 211 videos spanning 211,000 frames, with individual sequence lengths varying from 70 to 10,372 frames."}, {"title": "Annotation:", "content": "To annotate the video dataset, we developed custom Python software for both coarse and fine bounding box annotation. As mentioned above, coarse annotations were required to select the top 400 videos from the pool of 450 candidates. In the coarse annotation phase, we meticulously annotated only the first frame of each sequence. Subsequently, we efficiently annotate the successive frames, drawing bounding boxes rapidly without imposing strict requirements for perfect centering on the target. To ensure a basic level of quality for the initial assessments of all 450 sequences, we implemented a check based on the normalized cross-correlation between successive frames. If this value dropped below 0.7, the annotator will be prompted to re-examine the current frame before proceeding. Following the initial coarse bounding box annotation, we advanced to a fine annotation process involving iterative refinement and careful examination to ensure high-quality results. Three annotators carefully adjusted the coarse boxes utilizing specialized software that provided flexibility in modifying labels as necessary. Subsequently, an examiner conducted a comprehensive review of the refined annotations employing a preview tool, identifying frames requiring additional revision. To avoid potential mistakes, the fine annotations underwent 2 cycles of refinement by annotators, each followed by auditing and feedback from the examiner. This collaborative annotation pipeline, with iterative checks, proved instrumental in upholding stringent labeling quality standards.\nAfter that, we intentionally introduced a small degree of label noise to better simulate real-world ambiguity. Approximately 5% of the box annotations, excluding the first frame, underwent slight random perturbations. Specifically, we applied shifts of min{max{[x,y,w,h], 1}, 10} pixels from the original annotations. As in the GOT-10k dataset[24], we opted not to explicitly identify the perturbed frames, requiring trackers to inherently handle annotation noise during training and evaluation."}, {"title": "4 Benchmarking NT-VOT211", "content": "We evaluated NT-VOT211 by benchmarking it against 43 state-of-the-art trackers gathered from 42 papers. These trackers were implemented in various codebases such as PySot, Video Analyst, and PyTracking. To facilitate this evaluation, we created specific dataloaders for each of these three toolkits, and the corresponding code is made publicly available. To ensure a fair comparison, all trackers were run in their original environments, producing raw results on NT-VOT211. Following the execution, predictions were post-processed and assessed using standardized metrics within PyTracking. These metrics include AUC[15], precision[51], normalized precision[25], OP50, and OP75[24]. The detailed results can be found in Table 2, where \"Rank(A|P)\" indicates the global rank of each method based on its performance in terms of AUC Precision."}, {"title": "4.1 Analysis", "content": "Correlation Filter Trackers: MOSSE[7] represents a seminal work in the field of correlation filter tracking. Interestingly, it is the lowest-ranking method in our benchmark, which is a statistics-based approach. This result is understandable because in many sequences, the background consists of purely black pixels with a value of 0, which can impede statistical processing. A similar issue is observed with Staple-CA[39]. Conversely, we observe significant improvement with methods like KCF (HOG) [23] and CN[13], which indicates the potential of feature diversity. Staple[1] argues that combining the final score map of correlation filter (CF) methods with the appearance model generated by DAT[61] leads to substantial enhancements. MKCFup[44] suggests that considering multiple results from different CF trackers simultaneously can be beneficial. BACF [26] adopts a different approach by focusing on the target itself rather than modeling the background, resulting in improved success. On the other hand, STRCF [34] and CSRDCF [12] emphasize spatio-temporal constraints, which have proven to be effective for tracking in low-light conditions.\nDiscriminative Trackers: The DCF trackers ensure feature diversity through pretrained backbones. KSY [4] leverages information across different frames to distinguish foreground from background, achieving a notable 16-th rank in AUC on NT-VOT211. KeepTrack [37] demonstrates that gradient-descent-based methods are quite advanced compared to plain appearance models. RTS[43] attempts to fuse the score map with segmentation results in a more complex manner. However, it turns out to be less efficient than KSY, possibly due to the challenges of segmentation posed by low brightness conditions.\nSiamese Trackers: SiamFC[2] primarily focused on the target itself. SiamRPN[31] achieves better performance. Surprisingly, the appearance-model-based DaSiamRPN[61] does not outperform its baseline SiamRPN on NT-VOT211. SiamMask[47] and its advanced iteration SiamMask_E[28] have integrated a binary segmentation objective into their loss functions, thereby achieving enhanced tracking accuracy. SiamDW[58] investigates the depth of the backbone network to balance performance and processing speed, although the improvement on NT-VOT211 is limited. SiamBAN-ACM[19] introduces Asymmetric Convolution to accommodate different feature map sizes, proved to be effective for nighttime tracking. Ocean[59] opts for direct prediction of the position and scale of target objects in an anchor-free manner. Interestingly, it yields degraded results on NT-VOT211, suggesting that traditional Siamese methods may have advantages. This could be due to the challenges posed by the high amount of random noise in nighttime frames when estimating the position instead of anchor offsets. STMTrack [16] concentrates on the target itself and achieves a 13-th rank on AUC in NT-VOT211. LightTrack [54] introduces an efficient approach to developing a lightweight and effective tracker. However, it falls behind SiamFC in frames per second (FPS), and its performance improvement on our dataset is marginal compared to SiamFC, which might be explained by the unique distribution pattern compared to other benchmarks."}, {"title": "Transformer-based Trackers:", "content": "TransT[9] is one of the earliest transformer-based tracking methods. SLT-TransT[27] recognizes visual tracking as a sequence-level task and introduces a sequence-level training strategy, ranking 11-th in AUC on NT-VOT211. STARK [53] focuses on learning spatio-temporal transformers for visual tracking and achieves a 10-th ranking in AUC on NT-VOT211. TrDiMP[46] is ranked 14-th by incorporating temporal context. Mixformer[11] proposes a Mixed Attention Module to replace the standard Siamese architecture's classification and regression branches, benefiting from the contributions of the masked convolution autoencoders (ConvMAE) module[50]. AiATrack[18] introduces efficient feature reuse and target-background embeddings to make full use of temporal references, achieving the 6-th rank in NT-VOT211. OSTrack[55] adopts a multi-feature fusing strategy, ranking 8-th on NT-VOT211. However, based on OSTrack, NeighborTrack-OSTrack[10] models the region around the target and experiences a slight decrease in performance on NT-VOT211. ToMP-50[36], utilizing the ResNet-50[21] backbone, extends the model predictor to estimate a second set of weights that are applied for accurate bounding box regression, similar to the Siamese approach. It achieves a rank of 4 in AUC on NT-VOT211. Interestingly, ToMP-101 with the ResNet-101 backbone performs less effectively, suggesting that deeper backbone is not necessarily better for NT-VOT211. ARTrack-L[49] conducts regression on position and ranks 17-th in AUC. E.T.Tracker [6] is a lightweight tracker, while Unicorn[52] is a multi-task classifier using the same network with the same parameters. ProContEXT[30], leverages both temporal and spatial contexts, instead of sole reliance on the target area in the initial frame, significantly enhancing tracking precision and achieve SOTA performance achieved 1-st rank on NT-VOT211.\nFine-tuning performance: To demonstrate the impact of fine-tuning in NT-VOT211, we extract sequences with over 20% of frames containing specific attributes as test sequences. We created test sequences for camera motion, deformation, fast motion, motion blur, tiny targets, and distractors features. We use the remaining sequences as the training set (test: training 37%:63%). \nNight-time trackers on NT-VOT211: We observe the performance of trackers specifically targeted at night-time scenarios and that reveal the best performance on existing night-time datasets tend to perform poorly on ours NT-VOT211 benchmark"}, {"title": "4.2 Insights for Future Research", "content": "Focus on the target: Trackers like DAT [61], Staple-CA [39], DaSiamRPN [61], and NeighborTrack-OSTrack [10] exhibit degraded performance under low illumination due to over-reliance on background context. Aggressively extracting information from the surroundings hinders these methods in dim settings with less discriminative context. On the other hand, target-focused trackers like CN [13], KCF [23], and SiamRPN [31] prove more robust in low light. While background context remains valuable, as demonstrated by KeepTrack's architecture [37], which distinguishes the target from the background, achieving optimal tracking robustness requires balancing target-centric and contextual reasoning, especially in handling illumination variation. Target-only approaches excel in invariance but lack discrimination, while context-driven methods enable classification at the cost of sensitivity to environmental changes.\nSpatio-temporal information is critical: Methods that incorporate spatio-temporal priors[53,12,30,46,18] consistently outperform their baselines, emphasizing the significance of motion and temporal consistency in low-light tracking. By leveraging target dynamics and coherence across frames, trackers extract discriminative cues without depending on rich visual appearance in low light settings. The reliability of frame-to-frame variations in inferring spatio-temporal patterns persists even under changes in illumination. These findings underscore the importance of encoding target movement and temporal consistency for achieving invariance to environmental conditions.\nTransformer based Trackers are better: The fact that 9 of the top 10 trackers on NT-VOT211 utilize a vision transformer architecture highlights the benefits of this approach for generalization. The inherent ability of transformers to model long-range dependencies appears well-suited for tracking under challenging new conditions not seen during training.\nRegression on offset: Regression on offset is necessary, as evidenced by the success of SOTA ProContEXT[30] and TATrack[20]. However, the failure of Ocean[59] and the performance gap between ARTrack[49] and ProContEXT suggest that directly regressing on location may not be a suitable choice.\nThe deeper may not the better: The poor performance of ToMP-101[36] and SiamDW[58] on the proposed dataset suggests that these methods may be overfitting to their training datasets and may not generalize well to the proposed NT-VOT211 dataset."}, {"title": "5 Conclusion", "content": "In this work, a novel night-time visual object tracking benchmark, NT-VOT211, is proposed. It is a new large-scale benchmark for night-time visual tracking, featuring 211 challenging videos with 211K well-annotated frames. Comprehensive evaluation and analysis indicates the complementary nature of this benchmark to the existing ones for evaluation and deployment of the trackers in real-world systems. The state-of-the-art performance on NT-VOT211 highlights significant room for improvement in the task of nighttime visual object tracking. Besides, we provide a leaderboard, showcasing performance rankings, and annotation tools."}]}