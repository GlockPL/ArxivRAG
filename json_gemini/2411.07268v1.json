{"title": "Target-driven Attack for Large Language Models", "authors": ["Chong Zhang", "Mingyu Jin", "Dong Shu", "Taowen Wang", "Dongfang Liu", "Xiaobo Jin"], "abstract": "Current large language models (LLM) provide a strong foundation\nfor large-scale user-oriented natural language tasks. Many users can\neasily inject adversarial text or instructions through the user inter-\nface, thus causing LLM model security challenges like the language\nmodel not giving the correct answer. Although there is currently a\nlarge amount of research on black-box attacks, most of these black-\nbox attacks use random and heuristic strategies. It is unclear how\nthese strategies relate to the success rate of attacks and thus effec-\ntively improve model robustness. To solve this problem, we propose\nour target-driven black-box attack method to maximize the KL diver-\ngence between the conditional probabilities of the clean text and the\nattack text to redefine the attack's goal. We transform the distance\nmaximization problem into two convex optimization problems based\non the attack goal to solve the attack text and estimate the covari-\nance. Furthermore, the projected gradient descent algorithm solves\nthe vector corresponding to the attack text. Our target-driven black-\nbox attack approach includes two attack strategies: token manipu-\nlation and misinformation attack. Experimental results on multiple\nLarge Language Models and datasets demonstrate the effectiveness\nof our attack method.", "sections": [{"title": "1 Introduction", "content": "As large language models (LLMs) [21, 6] continue to advance in\narchitecture and functionality, their integration into complex systems\nrequires a thorough review of their security properties. Since the use\nof most LLMs relies on interface interaction, it is difficult to avoid\nthe hidden danger of generative adversarial attacks. Therefore, it is\nsignificant to study adversarial attacks on large language models to\nimprove the security and robustness of LLMs [28].\nPrevious attacks on LLMs mainly include white-box attacks and\nblack-box attacks. White-box attacks assume that the attacker has\nfull access to the model weights, architecture, and training workflow\nso that the attacker can obtain the gradient signal. The main method\nis gradient-based attacks, for example, Guo et al. [11] proposed a\ngradient-based distributed attack (GBDA), which, on the one hand,\nuses the Gumbel-Softmax approximation technique to make the ad-\nversarial loss optimization differentiable and, on the other hand, uses\nBERTScore and perplexity to enhance perceptibility and fluency.\nThese methods can only attack open-source large language mod-"}, {"title": "2 Related Work", "content": "Gradient-based Attack\nGradient-based Distributional Attack\n(GBDA) [11] uses the Gumbel-Softmax approximation technique\nto make the adversarial loss optimization differentiable while us-\ning BERTScore and perplexity to enhance perceptibility and fluency.\nHotFlip [7] maps text operations into vector space and measures the\nloss derivatives with respect to these vectors. AutoPrompt [30] uses\na gradient-based search strategy to find the most effective prompt\ntemplates for different task sets. Autoregressive Stochastic Coordi-\nnate Ascent (ARCA) [14] considers a broader optimization problem\nto find input-output pairs that match a specific pattern of behavior.\nWallace et al. [34] propose a gradient-guided search of markers to\nfind short sequences, with 1 marker for classification and 4 addi-\ntional markers for generation, called Universal Adversarial Trigger\n(UAT), to trigger the model to produce a specific prediction. How-\never, most of the widely used LLMs are not open source, so gradient-\nbased white-box attacks are unsuitable for these large language mod-\nels. Our work belongs to the black-box attack method.\nToken Manipulation Attack Given a text input containing a se-\nquence of tokens, we can apply simple word operations (such as re-\nplacing them with synonyms) to trigger the model to make incorrect\npredictions. Ribeiro et al. [26] manually define heuristic Semantic\nEquivalent Adversary Rules (SEAR) to perform minimal labeling\noperations, thereby preventing the model from generating correct an-\nswers. In contrast, Easy Data Augmentation (EDA) [36] defines a set\nof simple and more general operations to enhance text including syn-\nonym replacement, random insertion, random exchange, or random\ndeletion. Given that context-aware prediction is a very natural use\ncase for masked language models, BERT-Attack [17] replaces words\nwith semantically similar words via BERT. Unlike token manipu-\nlation attacks, our attack attempts to insert a generated adversarial\nprompt rather than just modifying certain words in the prompt.\nPrompt Injection Attack Larger LLMs have superior instruction-\nfollowing capabilities and are more susceptible to these types of\noperations, which makes it easier for an attacker [19] to embed\ninstructions in the data and trick the model into understanding it.\nPerez&Ribeiro [23] divides the targets of prompt injection attacks\ninto goal hijacking and prompt leaking. The former attempts to redi-\nrect LLM's original target to a new target desired by the attacker,\nwhile the latter works by convincing LLM to expose the application's\ninitial system prompt. However, system prompts are highly valuable\nto companies because they can significantly influence model behav-\nior and change the user experience. Liu et al. [18] found that LLM\nexhibits high sensitivity to escape and delimiter characters, which ap-\npear to convey an instruction to start a new range within the prompt.\nTherefore, they provide an efficient mechanism for separating com-\nponents to build more effective attacks. Our generative prompt in-\njection attack method does not attempt to insert a manually specified\nattack instruction but attempts to influence the output of LLM by\ngenerating a confusing prompt based on the original prompt.\nMisinformation Attack In the realm of information, the pro-\nliferation of misinformation-encompassing fake news and ru-\nmors poses a severe threat to public trust. Such misinformation,\ndisseminated through various media channels, often results in sig-\nnificantly divergent narratives of the same events, complicating the"}, {"title": "3 Methodology", "content": "3.1 Threat Model with Target-driven Attack\nAdversarial scope and goal. Given a text t containing multiple\nsentences, we generate a text t' to attack the large-scale language\nmodel (LLM) similar to ChatGPT, ensuring that the meaning of the\noriginal text t is preserved. We use its semantic representation s(t)\nusing a LLM M, S (t', t) to represent the distance between the se-\nmantics of text t and t'. To attack the language model, we replace t\nwith a new text t', ensuring that the meaning of the original text t is\npreserved. If the outputs M(t) and M(t') differ, then t' is identified\nas an adversarial example or an attack input for M. We aim to select\na text t' that maximizes the likelihood of M producing an incorrect\noutput compared to t. Our objective is formulated as follows:\n$$M(t) = r, M (t') = r', S(r,r') > \\varepsilon, S (t', t) < \\varepsilon,$$ (1)\nwhere the texts r and r' are the outputs of model M on text t and\nt' respectively, and r is also the groundtruth of text t. We introduce\na distance function S(,) and a small threshold \u03b5 to measure the\nrelationship between two texts. In our problem, we aim to provide\nthe following attack characteristics\n\u2022 Effective: Problem 1 shows that the attack model ensures a high\nattack success rate (ASR) with S(M(t'), r) \u2265 \u03b5 on one hand and\nmaintains high benign accuracy with S(M(t), r) < \u03b5 on the other\nhand.\n\u2022 Imperceptible: Text perturbations are often detected easily by in-\nserting garbled code that can disrupt large models. We try to en-\nsure that the text perturbations fit better into the problem context\nso that the model's active defense mechanisms make it difficult to\ndetect the presence of our prompt injections.\n\u2022 Input-dependent: Compared to fixed triggers, input-dependent\ntriggers are imperceptible and difficult to detect from humans in\nmost cases [35]. It is obvious the adversarial text (or trigger) t' is\ninput-dependent by Eqn. (1).\n3.2 Analysis on Objective\nBelow, we first discuss the necessary conditions for the LLM model\nto output different values under the conditions of clean text t and\nadversarial text t', respectively.\nAs can be seen from the problem (1), the input t and output r of\nthe model M are both texts. To facilitate analysis, we discuss the"}, {"title": "3.3 Solving Optimization Problem (9)", "content": "When \u03a3 is known, we can solve for the optimal zi for each xi sep-\narately. For the convenience of discussion, for each x, we find the\noptimal z\n$$\\max_{z} (z-x)^T\\Sigma^{-1}(z-x), \\text{ s.t. } ||z||_2 = 1.$$ (10)\nNote that the above problem is a non-convex problem, which also\nequivalent to\n$$\\max_{z\\neq0} \\frac{(z-x)^T\\Sigma^{-1}(z-x)}{||z||_2^2}$$ (11)\nFurthermore, we transform (11) into the following convex optimiza-\ntion problem\n$$\\min_{z} ||z||_2^2, \\text{ s.t. } (z - x)^T\\Sigma^{-1} (z - x) \\leq 1.$$ (12)\nNext, we will optimize the following loss function with the constraint\n$$||t||_2 \\leq 1$$ through projected gradient descent shown in Alg. 1. There\nare two main steps: gradient-based update and projection to the con-\nstraint.\nLet\n$$L(t) = ||Lt + x||$$ (16)\nThe gradient of L with respect to t is directly computed\n$$\\nabla_t L(t) = 2L^T (Lt + x),$$ (17)\nSo we update t with\n$$t \\leftarrow t - \\alpha L^T (Lt + x).$$ (18)\nAs for the projection operation onto the constraint $$||z||_2 \\leq 1$$, we\nhave\n$$\\text{arg }\\min_{||z||_2\\leq1} ||z - t||_2 \\rightarrow \\frac{t}{||t||_2}$$ (19)\nFor a detailed discussion on the projected gradient solution problem\n(15), see subsection A.4."}, {"title": "3.4 Estimating Covariance \u03a3 with Know zis", "content": "In problem (12), the covariance \u03a3 only appears in the constraints. We\nlook for \u03a3 that satisfies the constraints $$(z_i - x_i)^T\\Sigma^{-1}(z_i - x_i) \\leq 1$$\nfor each xi and zi. However, we notice that when det (\u03a3) tends to\n$$\\infty$$, the value of $$(z_i - x_i)^T\\Sigma^{-1}(z_i - x_i)$$ tends to 0 (See A.3 for\na more detailed discussion). In order to avoid the value of det(\u03a3)\nbeing too large, we also minimize log(det (\u03a3)), that is, we minimize\nthe following function to find the covariance \u03a3\n$$\\min_{\\Sigma} [\\sum_{i=1}^{N} (z_i - x_i)^T\\Sigma^{-1}(z_i - x_i) + \\log(\\text{det}(\\Sigma))].$$ (20)\nSee subsection A.3 for a more detailed discussion of the optimization\nproblem (20).\nWe represent the objective function of problem (20) as\n$$f(\\Sigma^{-1}) = \\sum_{i=1}^{N} (z_i - x_i)^T \\Sigma^{-1} (z_i - x_i)$$\n$$- N \\log(\\text{det}(\\Sigma^{-1})).$$ (21)\nLet H = \u03a3\u207b\u00b9, then we directly take the derivative of the function f\nwith respect to H and let the derivative be 0, and with $$\\frac{\\partial \\log(\\text{det}(X))}{\\partial X} = (X^T)^{-1}$$ we get\n$$\\frac{\\partial f(H)}{\\partial H} = \\sum_{i=1}^{N} (z_i - x_i) (z_i - x_i)^T - NH^{-1} = 0.$$ (22)"}, {"title": "3.5 Discussion on Convergence", "content": "We note that the optimization problem (15) is a convex optimiza-\ntion problem, where the constraints are convex sets and the objective\nfunction is a convex function and thus has a global minimum.\nFor the function f(H) in Eqn. (21), we continue to calculate the\nsecond derivative of H based on Eqn. (22)\n$$\\nabla_H f(H) = NH^{-2} \\geq 0.$$ (25)\nTherefore, the function f (H) is a convex function of H. We directly\nsolve the stationary point of the function (21) with $$\\frac{\\partial f(H)}{\\partial H} = 0$$ to\nobtain a new estimate (23) of \u03a3."}, {"title": "3.6 Goal-guided Attack Strategy", "content": "Below, we will use two black box attack methods to generate the at-\ntack text t' corresponding to the vector z* while approximately sat-\nisfying the semantic constraints for the clean input t and the attack\ntext t', which is essentially the reverse process of the embedding op-\neration. Our basic idea is to use the keywords in the sentences to\nregenerate a series of sentences that satisfy semantic constraints and\nfind the sentences closest to z* from these sentences.\n3.6.1 Token Manipulation Attack\nWe will extract the keywords in the sentence, that is, the subject,\npredicate, and object of the sentence. We obtain a new sentence by\nreplacing these keywords and select the closest sentence to z* from\nmany candidate samples as the attack sentence.\nIn a paragraph of text, usually, the most important text information\nis stated first. Therefore, the subject, predicate, and object that appear"}, {"title": "4 Experimental Results", "content": "4.1 Experimental Details\n4.1.1 Victim Models\n\u2022 ChatGPT. ChatGPT is a language model created by OpenAI that\ncan produce conversations that appear to be human-like [24]. The\nmodel was trained on a large data set, giving it a broad range of\nknowledge and comprehension. In our experiments, we choose\nGPT-3.5 Turbo and GPT-4 Turbo as our victim models in the Ope-\nnAI series.\n\u2022 Llama-2. Llama-2 [33] is Meta AI's next-generation open-source\nlarge language model. It is more capable than Llama 1 and outper-"}, {"title": "4.1.2 Implementation Details", "content": "In our experiment, we randomly selected 300 questions from each\ndataset to test our strategy, where the evaluation criteria include clean\naccuracy, attack accuracy, and attack success rate (ASR), as shown\nin evaluation metrics 4.1.3. The shapes of x and z are in the shape\nof ([1,768]). Notice that the size of the covariance matrix \u03a3 is 300 \u00d7\n300. For Alg. 1 and 2, we set the maximum number of iterations to\n1000, \u03b5 = 0.2 and \u03b1 = 0.05. The other details can be found in the\nexperiment section."}, {"title": "4.1.3 Evaluation Metrics", "content": "Assume that the test set is D, the set of all question answer pairs\npredicted correctly by the LLM model f is T, and a(x) represents\nthe attack sample generated by the clean input x. Then we can define\nthe following three evaluation indicators\n\u2022 Clean Accuracy The Clean Accuracy measures the accuracy of\nthe model when dealing with clean inputs $$\\text{A}_{\\text{clean}} = \\frac{|T|}{|D|}$$"}, {"title": "4.3 Comparison to Other Mainstream Methods", "content": "Below, we compare our method with the current mainstream black-\nbox attack methods in zero-sample scenarios on two data sets:\nSQUAD2.0 dataset [25] and Math dataset [12]. Microsoft Prompt\nBench [38] uses the following black box attack methods to attack\nthe ChatGPT 3.5 language model, including BertAttack [16], Deep-"}, {"title": "4.4 Transferability", "content": "We use the adversarial examples generated by model A to attack\nmodel B, thereby obtaining the transferability [37] of the attack on\nmodel A. The attack success rate obtained on model B can demon-"}, {"title": "5 Conclusion", "content": "We propose a target-driven attack on LLMs, which transforms the at-\ntack problem into a KL-divergence maximization problem between\nthe posterior probabilities of two conditions, the clean text and the\nattack text. Subsequently, the KL-divergence maximization problem\nis transformed into a well-defined convex optimization problem. At\nthe same time, it is proved theoretically that maximizing the KL-\ndivergence is approximately equivalent to maximizing the Maha-\nlanobis distance between normal text and attack text. The projected\ngradient algorithm is used to find the optimal solution to the problem,\nwhich is the vector corresponding to the attack text. Two attack al-\ngorithms are designed including token manipulation attack and mis-\ninformation attack. Experimental results on multiple public datasets\nand LLM models verify the effectiveness of the proposed method.\nFor our attack, an intuitive defense method (or adversarial training)\nis to find attack samples in a certain epsilon neighborhood of the\nclean samples, so that the Mahalanobis distance between the clean\nsamples and the attack samples is minimized."}, {"title": "A Theoretical discussion", "content": "A.1 Discussion of Conditions or Assumptions of Theorem 1\nWe cannot confirm whether these assumptions or preconditions hold or not for the LLM model since our algorithm is a black-box attack, and\nwe do not know the form and parameters of the LLM model. However, to ensure that our optimization problem is a well-defined problem, we\nmake reasonable assumptions and constraints on the variables so that these theoretical results can guide our algorithm design:\n\u2022 z is a unit vector: If we do not limit the length of z ($$||z||_2$$), then our optimization problem will be a trivial optimization problem: our goal\nis to maximize the Mahalanobis distance between vector z and vector x, then $$||z||_2$$ tends to infinity, the objective function (Mahalanobis\ndistance) will tend to infinity.\n$$\\lim_{||z|| \\to \\infty} (z-x)^T\\Sigma^{-1}(z - x)$$\n$$\\lim_{||z|| \\to \\infty} \\frac{||z||^2( \\frac{x}{||z||} - \\frac{z}{||z||} )^T\\Sigma^{-1}( \\frac{x}{||z||} - \\frac{z}{||z||} )}{1}$$\n$$\\geq \\lim_{||z|| \\to \\infty} ||z||^2(t^T\\Sigma^{-1}t)$$\n$$\\geq \\lim_{||z|| \\to \\infty} ||z||^2 \\lambda_{\\min}(\\Sigma^{-1}) = +\\infty,$$"}, {"title": "A.3 Discussion on Problem (20)", "content": "We restate the problem (15) as follows\n$$\\min_{z} ||z||^2, \\text{ s.t. } (z-x)^T\\Sigma^{-1} (z - x) < 1.$$\nHere, note that the variable \u03a3 only appears in the constraints, we hope to find \u03a3 with $$(z-x)^T\\Sigma^{-1} (z - x) < 1$$ for each xi and zi. Below we\ntry to transform the problem of finding constraints into the minimization problem of $$(z-x)^T\\Sigma^{-1} (z - x)$$. However, this is not a well-defined\nproblem with respect to \u03a3.\nAssume that the eigenvalue decomposition of \u03a3 is $$ \\Sigma = \\sum_{i=1}^{n} \\lambda_i p_i p_i^T$$, then we can get\n$$\\Sigma^{-1} = \\sum_{i=1}^{n} \\frac{p_i p_i^T}{\\lambda_i}$$\nTherefore the Mahalanobis distance will become\n$$(z-x)^T\\Sigma^{-1}(z-x) = \\sum_{i=1}^{n} \\frac{[p_i^T(z-x)]^2}{\\lambda_i}$$\nSince \u03a3 defines a non-trivial Mahalanobis distance, \u03a3 is a non-singular matrix. When the minimum eigenvalue \u03bbmin tends to \u221e, so the\nMahalanobis distance will tend to 0\n$$\\lim_{\\lambda_{min} \\to \\infty} (z-x)^T\\Sigma^{-1}(z-x)$$\n$$<=\\lim_{\\lambda_{min} \\to \\infty} \\sum_{i=1}^{n} \\frac{[p_i^T(z-x)]^2}{\\lambda_{min}}$$\n$$\\leq  \\lim_{\\lambda_{min} \\to \\infty} \\frac{\\sum_{i=1}^{n} [p_i^T(z-x)]^2}{\\lambda_{min}}= 0.$$\nTherefore, we limit the determinant value det (\u03a3) not to be too large, since we have\n$$det(\\Sigma) = \\prod_{i=1}^{d} \\lambda_i \\geq \\lambda_{min} \\implies (det(\\Sigma))^{1/d} \\geq \\lambda_{min},$$"}]}