{"title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "authors": ["Weijia Shi", "Jaechan Lee", "Yangsibo Huang", "Sadhika Malladi", "Jieyu Zhao", "Ari Holtzman", "Daogao Liu", "Luke Zettlemoyer", "Noah A. Smith", "Chiyuan Zhang"], "abstract": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content, and data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models, leading to the development of many approximate unlearning algorithms. Evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations, because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations.", "sections": [{"title": "1 Introduction", "content": "Training language models (LMs) often involves using vast amounts of text data, which may inadvertently contain private and copyrighted content (Carlini et al., 2021; Henderson et al., 2023; Min et al., 2023). In real-world applications, data owners may demand that their data be removed from a trained language model due to privacy or copyright concerns, as mandated for example by the General Data Protection Regulation (GDPR, European Parliament & Council of the European Union). Moreover, recent copyright lawsuits (DOE 1 v. GitHub, Inc., N.D. Cal. 2022; Tremblay v. OpenAI, Inc.,, 2023) emphasize the need for removing copyrighted data from the model."}, {"title": "2 Machine Unlearning: Preliminaries and Notations", "content": "Machine unlearning (Ginart et al., 2019; Liu et al., 2020; Izzo et al., 2021; Sekhari et al., 2021; Gupta et al., 2021; Ye et al., 2022b; Liu et al., 2024) has emerged as an important capability to accommodate data removal requirements that arise from scenarios with privacy or copyright concerns.\nWe briefly describe the machine unlearning setting. Consider a dataset Dtrain and a model ftarget trained on Dtrain. Suppose we design an algorithm U to unlearn a specific subset (i.e., the forget set) Dforget \u2286 Dtrain from ftarget. We want to preserve performance on a retain set Dretain = Dtrain \\ Dforget, and we also evaluate the model on an in-distribution but disjoint hold-out set Dholdout which the model has never been trained on. So, the unlearning algorithm U takes ftarget, Dforget, and, optionally, Dretain and outputs an unlearned model funlearn. Exact unlearning ensures funlearn is behaviorally identical to the model resulting from retraining from scratch, denoted ftarget, but such retraining is usually too costly in real world deployment, so we focus on evaluating approximate unlearning algorithms."}, {"title": "3 The MUSE Evaluation Benchmark", "content": "MUSE evaluates a comprehensive set of desirable properties of machine unlearning across six facets. We detail the evaluation metrics in \u00a73.1 and describe the evaluation corpus in \u00a73.2."}, {"title": "3.1 Evaluation Metrics", "content": "Ideally, an unlearned model should behave as if it had never seen the forget set, exhibiting similar behavior to a retrained model on any corpus D such that m(funlearn, D) \u2248 m(fretrain, D), where m represents any evaluation metric. Prior evaluations on LM unlearning focus on performance of specific tasks like question answering (e.g., Eldan & Russinovich, 2023; Maini et al., 2024). However, these metrics do not faithfully reflect data owner expectations and real-world deployment considerations when performing unlearning. To address this, we propose comprehensive evaluation metrics that consider both data owner and deployer expectations. A comparison between MUSE and the prior benchmark is shown in Table 3.\nData owner expectations. When removing a forget set from a model, data owners typically have three main expectations regarding the unlearned model: (C1) No verbatim memorization: The model should not exactly replicate any details from the forget set. (C2) No knowledge memorization: The model should be incapable of responding to questions about the forget set. (C3) No privacy leakage: It should be impossible to detect that the model was ever trained on the forget set. For example, if a patient's records are unlearned from a medical diagnosis model, in addition to verbatim"}, {"title": "C1. No verbatim memorization", "content": "When a model has unlearned a medical record, it should not output its contents verbatim. We quantify the verbatim memorization VerbMem by prompting the model with the first l tokens from a sequence x[:l] \u2208 Dforget and comparing the continuation outputted by the model f to the true continuation x[l+1:] \u2208 Dforget using the ROUGE-L F1 score (Lin, 2004).\nVerbMem(f, D) := 1/|Dforget| \u2211x\u2208Dforget ROUGE(f(x[:l]), x[l+1:])"}, {"title": "C2. No knowledge memorization", "content": "When a model has unlearned a medical record, it should no longer be able to answer questions about that record. We measure a model f's memorization of knowledge from the forget set Dforget as follows: for each example x \u2208 Dforget associated with a question-answer pair (q, a), we gather the model's answer to the question q, denoted f(q). We then average the ROUGE scores for all question-answer pairs in Dforget to compute the knowledge memorization score KnowMem:\nKnowMem (f, Dforget) := 1/|Dforget|\u2211(q,a)\u2208Dforget ROUGE(f(q), a)"}, {"title": "C3. No privacy leakage", "content": "As discussed previously, it is desirable that the unlearned model does not leak membership information indicating that Dforget was part of Dtrain. To determine if a given example was used during training, membership inference attack (MIA) exploits distributional differences in certain statistics (e.g., loss) between training (member) and non-training (non-member) data: if the loss on the example is low, then it was likely used for training. Unlearning typically increases the loss on the example, but there are two possible ways that unlearning can fail to prevent privacy leakage: (1) under-unlearning, when the loss is not made large enough; and (2) over-unlearning, when the loss is made abnormally large. To accurately measure the privacy leakage, we employ Min-K% Prob (Shi et al., 2024a), a state-of-the-art MIA method for LMs based on the loss, and compute the standard AUC-ROC score (Murakonda et al., 2021; Ye et al., 2022a) of discriminating Dforget (members) and Dholdout (non-members). By comparing the AUC score with that of the retrained model, we define\nPrivLeak := AUC(funlearn;Dforget,Dholdout) - AUC(fretrain;Dforget,Dholdout)/AUC(fretrain;Dforget,Dholdout)\nThe PrivLeak metric for a good unlearning algorithm should be close to zero, whereas an over/under-unlearning algorithm will get a large positive/negative metric."}, {"title": "Deployer expectations", "content": "Model deployers have their own considerations for using unlearning algorithms in the real world. Unlearning specific datapoints can unpredictably degrade model capabilities in ways that are difficult to recover. Moreover, deployers are expected to effectively accommodate somewhat large-scale forget sets and successive unlearning requests from data owners. As such, we consider three key metrics: (C4) utility preservation on the retain set, (C5) scalability to handle large-scale content removal, and (C6) sustainability to maintain performance over sequential unlearning requests."}, {"title": "C4. Utility preservation", "content": "Model capabilities are often hard-won through expensive training procedures, so deployers would want an unlearning algorithm that preserves performance on the retain set. To quantify this, we evaluate the unlearned model's performance on the retain set using the knowledge memorization metric KnowMem (funlearn, Dretain)."}, {"title": "C5. Scalability", "content": "We assess the scalability of unlearning methods by examining their performance on forget sets of varying sizes. Let D denote a forget set of size c, and fo be the corresponding unlearned model. For any data owner-valued metric such as utility preservation, we measure scalability by analyzing the trend of this metric as c increases from small to large values."}, {"title": "C6. Sustainability", "content": "Machine unlearning operations often need to be applied sequentially, as data removal requests may arrive at different times. We denote the unlearned model after processing the k-th request as fu,k. To measure sustainability, we analyze the trend of any data owner-valued metric as the number of sequential unlearning requests k increases."}, {"title": "3.2 Evaluation Corpus", "content": "MUSE considers two representative types of textual data that may frequently involve unlearning requests: news articles (Tremblay v. OpenAI, Inc.,, 2023) and books (Eldan & Russinovich, 2023). These datasets are detailed as follows:\n\u2022 NEWS consists of BBC news articles (Li et al., 2023b) collected after August 2023. All articles are randomly divided into (disjoint) forget, retain, and holdout sets.\n\u2022 BOOKS consists of the Harry Potter book series. To simulate a real-world setting for testing utility preservation (C4), we include different types of materials in the forget and retain sets. The forget set contains the original books, while the retain set contains related content from the Harry Potter FanWiki, representing domain knowledge that should be retained after unlearning."}, {"title": "4 Unlearning Methods", "content": "We evaluate eight efficient approximate unlearning methods belonging to four families of algorithms.\nFour families of unlearning methods. We first introduce four families of unlearning methods, which serve as the basis for the eight methods we evaluate.\n\u2022 Gradient Ascent (GA) minimizes the likelihood of correct predictions on Dforget by performing gradient ascent on the cross-entropy loss (the opposite of conventional learning with gradient descent). GA has achieved mixed results: while Jang et al. (2023) found it effective for unlearning examples from the Enron email dataset (Klimt & Yang, 2004) with minimal performance degradation, Ilharco et al. (2023) reported that GA significantly harms general model utility when unlearning a high-toxicity subset of the Civil Comments dataset (Borkan et al., 2019).\n\u2022 Negative Preference Optimization (NPO; Zhang et al., 2024b) treats the forget set as negative preference data and adapts the offline DPO objective (Rafailov et al., 2023) to tune the model to assign low likelihood to the forget set without straying too far from the original model ftarget.\nLNPO(\u03b8) = \u2212 E x\u223cDforget log \u03c3(-\u03b2 log fo(x)/ftarget(x))\nwhere f\u03b8 refers to the model that undergoes unlearning, \u03c3 is the sigmoid function, and \u03b2 is a hyperparameter that controls the allowed divergence of f\u03b8 from its initialization ftarget. Following Rafailov et al. (2023); Zhang et al. (2024b), we fix \u03b2 = 0.1 in our experiments.\n\u2022 Task Vectors (Ilharco et al., 2023) derived from straightforward arithmetic on the model weights can effectively steer neural network behavior. We adapt task vectors to perform unlearning in two stages. First, we train ftarget on Dforget until the model overfits, yielding a reinforced model freinforce. We then obtain a task vector related to Dforget by calculating the weight difference between ftarget and freinforce. To achieve unlearning, we subtract this task vector from ftarget's weights, intuitively moving the model away from the direction it used to adapt to Dforget \u2013 i.e., funlearn = ftarget \u2212 (freinforce \u2212 ftarget).\n\u2022 Who's Harry Potter (WHP; Eldan & Russinovich, 2023) defines the unlearned model funlearn as the interpolation between the target model ftarget and the reinforced model freinforce. Let pf(\u00b7|x) denote the token distribution parametrized by the model f when given a prompt x as input. Then, concretely, for any input x, WHP samples the next token from\npfunlearn(\u00b7|x) = pftarget(\u00b7|x) \u2212 \u03b1(pfreinforce(\u00b7|x) \u2212 pftarget(\u00b7|x))\nwhere \u03b1 is a hyperparameter that controls the interpolation between the two models.\nTwo regularizers for utility preservation. GA and NPO are not explicitly designed for utility preservation, so we discuss several regularization strategies that either improve the performance on the retain set or ensure the unlearned model remains close to the target model during unlearning.\n\u2022 Gradient Descent on the Retain Set (GDR; Liu et al., 2022; Maini et al., 2024; Zhang et al., 2024b) augments the unlearning objective with a standard gradient descent learning objective on the cross-entropy of the retain set Dretain to more directly train the model to maintain its performance on Dretain.\n\u2022 KL Divergence Minimization on the Retain Set (KLR; Maini et al., 2024; Zhang et al., 2024b) encourages the unlearned model's probability distribution pfunlearn(\u00b7|x) to be close to the target model's distribution pftarget(\u00b7|x) on inputs from the retain set x \u2208 Dretain.\nList of methods. We combine GA and NPO with the two regularizers GDR and KLR, which yields four new combinations. Hence, we end up with a total of 8 candidate unlearning methods: GA,"}, {"title": "5 Experiments", "content": "We evaluate the eight representative unlearning methods using the experimental setup described in \u00a75.1. We present the results for data owner expectations in \u00a75.2 and for deployer expectations in \u00a75.3."}, {"title": "5.1 Experimental Setup", "content": "Retrained and target models. We start with a general pretrained base model fo, and finetune two models: ftarget on Dforget \u222a Dretain, and fretrain on Dretain only. See Appendix B.2 for details about finetuning. For each unlearning algorithm U, we further generate the unlearned model funlearn = U(ftarget, Dforget, Dretain). We ensure that fo has no access to Dforget, Dretain, Dholdout. Therefore, for NEWS, we use fo = LLaMA-2 7B (Touvron et al., 2023), which was released before the BBC news articles we use to construct our benchmarks; and for BOOKS, we use fo = ICLM-7B (Shi et al., 2024b), which does not contain the Harry Potter books in its pretraining data.\nUnlearning experimental configuration. Following prior work (Maini et al., 2024), we run GA, NPO, and their regularized variants using the AdamW optimizer (Loshchilov & Hutter, 2017) with a constant learning rate of 10\u22125 and a batch size of 32. We employ the stopping criteria as follows: if the utility (i.e., KnowMem on Dretain) of a model undergoing unlearning drops below that of fretrain within 10 epochs of unlearning, we stop at the first epoch where this condition holds; otherwise, we take a checkpoint from the 10th epoch. For Task Vector and WHP, to obtain the reinforced model for unlearning, we fine-tune the target model for 10 epochs using the same learning rate and batch size. Further details on the model fine-tuning and unlearning can be found in Appendix B.2."}, {"title": "5.2 Results: Data Owner Expectations", "content": "We first analyze how eight unlearning methods meet data owner expectations (C1, C2 & C3 in \u00a73.1)."}, {"title": "C1&C2. Most methods are effective for unlearning memorization.", "content": "As shown in Table 3, most unlearning methods perform exceptionally well in [C1. No verbatim memorization] and [C2. No knowledge memorization], often reducing VerbMem and KnowMem even beyond the levels achieved by the retrained model. Notably, some methods, such as GA and NPO, achieve a score of 0 for both VerbMem and KnowMem, meaning that these methods completely prevent the unlearned models from producing any text related to the forget set. However, as we will see later, these reductions often come at the cost of significant utility loss on the retain set."}, {"title": "C3. Unlearning leads to privacy leakage.", "content": "Most unlearning methods reveal the membership of Dforget in Dtrain through under-unlearning (PrivLeak \u226a 0) or over-unlearning (PrivLeak \u226b 0), as shown in Table 3. We further examine the effectiveness of membership inference by plotting ROC curves in Figure 4. The deviation from the diagonal line indicates the attacker's advantage over random guessing. We observe that the Min-K% Prob based attack achieves AUC \u2248 0 on ftarget, confirming its effectiveness. Meanwhile, the ROC curve for fretrain closely follows the diagonal line (AUC = 0.47), suggesting that perfect unlearning ensures MIA is no more effective than random guessing. Among the approximate unlearning methods, GA and NPOGDR without regularizers consistently over-unlearn (AUC > 0.7), whereas KLR-regularized methods (NPOKLR and GAKLR) tend to under-unlearn and barely improve privacy leakage over ftarget. WHP also deviates from the diagonal significantly."}, {"title": "5.3 Results: Deployment Considerations", "content": "C4. Unlearning significantly degrades model utility. Table 3 [C4 Utility Preserv.] shows that all unlearning methods compromise the model's utility by 24.2% ~ 100%. Notably, several methods (GA, GAGDR, NPOGDR) lead to complete utility loss, rendering the unlearned models practically unusable. Figure 5 illustrates the trade-offs between utility preservation on Dretain and knowledge memorization on Dforget. An ideal unlearned model should mimic the behavior of fretrain (desired region) by achieving a low level of memorization on Dforget while maintaining its utility. However, most methods, such as GAKLR, NPOKLR, and WHP, unlearn the knowledge on D\u03bc at the cost of utility.\nC5. Unlearning methods scale poorly with forget set sizes. To evaluate the robustness of the unlearning methods to larger forget sets, we collect additional news articles from the same distribution to scale our NEWS corpus from 0.8M tokens to 3.3M tokens and observe the utility preservation at four different forget set sizes. As shown in Figure 6 (a), the model utility decrease with the size of the forget set and achieves a minimum at the largest size.\nC6. Unlearning methods cannot sustainably accommodate sequential unlearning requests. To evaluate the robustness of these unlearning methods to more than one unlearning requests, we sequentially apply k unlearning processes, each with respect to a different forget set. To simulate sequential unlearning, we partition the extended NEWS forget set (comprised of 3.3M tokens) into four disjoint folds (each containing 0.8M tokens) and apply the unlearning methods to each fold in a sequential manner."}, {"title": "6 Related Work", "content": "Machine unlearning for non-language model applications. Machine unlearning is a long-running, well-studied topic. Several studies have explored exact unlearning, aiming to make the unlearned model (funlearn) exactly identical to the reference model (fretrain). As expected, this can only be accomplished in simple models like SVMs (Cauwenberghs & Poggio, 2000; Tveit et al., 2003; Romero et al., 2007; Karasuyama & Takeuchi, 2010) or naive Bayes models (Cao & Yang, 2015). Another approach is to ensure that the unlearned model funlearn is probabilistically indistinguishable from fretrain (Ginart et al., 2019; Guo et al., 2020), and this view of certifiable unlearning is closely related to differential privacy (Dwork et al., 2006b,a). This rigorous definition of unlearning has inspired several theoretical works that characterize the feasibility of unlearning in convex and non-convex models, but those proposed algorithms are too computationally costly to operate on modern-day LMs (Izzo et al., 2021; Neel et al., 2021; Ullah et al., 2021; Sekhari et al., 2021; Gupta et al., 2021). Several more tractable unlearning algorithms have been proposed (Borkan et al., 2019; Ginart et al., 2019; Thudi et al., 2022; Chourasia & Shah, 2023) with broader applications such as image classification (Ginart et al., 2019; Golatkar et al., 2020a), text-to-image generation (Gandikota et al., 2023; Zhang et al., 2023; Fan et al., 2023), Federated Learning (Liu et al., 2020; Che et al., 2023; Halimi et al., 2022) and Recommender Systems (Li et al., 2024b).\nMachine unlearning for language models: methods and applications. Machine unlearning has recently found its way into language model applications. In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance. Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022; Wu et al., 2023; Wei et al., 2024), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units."}, {"title": "In-context unlearning", "content": "In-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.\nMachine unlearning has also been applied to various downstream language model tasks, though the unit of machine unlearning may differ from what we study in this work. Our evaluation focuses on unlearning specific examples or datasets, aiming to make LMs forget either the phrasing or the content knowledge of targeted data, while preserving their utility for data not targeted for removal. This is crucial for ensuring privacy and copyright compliance. In addition to this specific unlearning, there's also a broader application similar to model editing, where outdated information is replaced with new knowledge (Pawelczyk et al., 2023; Yu et al., 2023; Belrose et al., 2024). Moreover, efforts have been made to eliminate harmful behaviors in language models by creating toxicity benchmarks and enhancing safety measures (Lu et al., 2022; Yao et al., 2023; Li et al., 2024a; Zhang et al., 2024b). Despite these varied approaches to unlearning at different operational and knowledge levels, the evaluation principles we propose such as preserving utility, ensuring scalability, and maintaining sustainability-are relevant across these contexts.\nMachine unlearning for language models: evaluation. Evaluating machine unlearning methods for language model applications is also critical. Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion. For example, Eldan & Russinovich (2023) experiment with unlearning to forget Harry Potter books and demonstrate the effectiveness of their methods by showing that familiarity scores, measured through completion-based, token-probability-based, and question-answering evaluations, significantly decline post-unlearning. Lynch et al. (2024) further suggest comparing unlearned models with perfectly retrained models. Their evaluation finds that while familiarity scores with the forget set may drop post-unlearning, they still remain higher than those of the retrained model. The closest work to ours is TOFU (Maini et al., 2024), a benchmark featuring 200 synthetic author profiles, each with 20 question-answer pairs, divided into forget and retain sets. However, TOFU is small-scale (0.15M tokens) and lacks separation of original text and knowledge, which conflates the evaluation of unlearning verbatim text and knowledge (C1 and C2 in MUSE). Additionally, current evaluations focus on limited aspects of data owner expectations and do not adequately reflect real-world deployment considerations, such as scalability and potential sequential unlearning requests. In contrast, MUSE formally defines different unlearning scopes and corresponding metrics, resulting in a systematic six-way evaluation featuring both data owners' and deployers' expectations. The evaluation uses a large-scale corpus of over 6 million tokens, separated into verbatim text and knowledge sets. We also note that some of our findings align with previous evaluations. For example, our observation that over- or under-unlearn can exacerbate privacy leakage (\u00a75.2) is consistent with the recent work by Hayes et al. (2024)."}, {"title": "Survey papers", "content": "We direct readers to several insightful survey papers for further reading. For non-LLM applications, notable surveys include Shintre et al. (2019); Nguyen et al. (2022); Thudi et al. (2022); Xu et al. (2023). Additionally, the NeurIPS 2023 machine unlearning competition for image classification is a valuable source of empirical methods tailored for this specific application (Triantafillou et al., 2023). For language model applications, Si et al. (2023) categorize unlearning methods into different families and summarize datasets for evaluating unlearning. Liu et al. (2024) review LM unlearning algorithms by targets and methods, discuss the effectiveness and efficiency of existing approaches and emphasize the importance of clearly defining the unlearning scope."}, {"title": "7 Conclusion", "content": "In this work, we propose MUSE, a comprehensive machine unlearning evaluation benchmark that highlights six desirable properties from the perspectives of both data owners and model deployers. We find that current unlearning methods successfully prevent the model's memorization of content at a significant cost to utility on data not intended for removal. They also lead to severe privacy leakage and cannot sustainably accommodate successive unlearning requests or large-scale content removal. These findings highlight the need for future research into more robust unlearning methods.\nLimitations. While MUSE provides a systematic benchmark for evaluating unlearning algorithms, it does not consider all possible considerations. For example, data owners may have additional expectations, such as ensuring their information cannot be probed from intermediate activations (Song & Raghunathan, 2020) or receiving formal guarantees of unlearning success (Sekhari et al., 2021;"}, {"title": "A Broader Impact", "content": "As LMs are deployed broadly and publicly, there is mounting legal and social pressure on deployers to release models that permit effective unlearning when requested by data owners (European Parliament & Council of the European Union; DOE 1 v. GitHub, Inc., N.D. Cal. 2022; Tremblay v. OpenAI, Inc.,, 2023). These incentives have prompted a flurry of new unlearning algorithms stemming from different technical perspectives. As such, systematic evaluation of the strengths and weaknesses of these methods when executing realistic unlearning requests on popular models is essential. MUSE disentangles several desirable properties of unlearning algorithms and finds that no existing algorithm is able to satisfy all of the data owner and deployer considerations. We hope that our fine-grained, multi-faceted framework facilitates the improvement of unlearning algorithms. Moreover, we expect that the general approach of designing metrics to balance the considerations of various stakeholders is flexible and can adapt to the rapidly shifting legal, social, and economic landscape.\nWe also acknowledge the potential negative impacts of our study. One limitation of our evaluation benchmark is that we do not have comprehensive study of how unlearning would impact the model performance for different user bases, especially underrepresented groups. However, we note proper handling and evaluation of fairness issues in unlearning is still an active ongoing research area (Zhang et al., 2024a; Oesterling et al., 2024), therefore we leave it as future work. Additionally, our work may be misinterpreted towards skepticism regarding the broader use of machine unlearning, as our current evaluation reveals that existing unlearning methods are not yet ready for effective real-world deployment. However, machine unlearning, especially for large language models, is a young and active research area and new algorithms are constantly being proposed. We emphasize that our results is not a criticism of the paradigm of machine unlearning, but a study of the potential downsides of existing methods and a call for better algorithms. We believe our benchmark is an important step towards guiding future algorithm design of machine unlearning research towards more realistic deployment scenarios."}, {"title": "B Experimental Details", "content": "B.1 Compute Configurations\nAll experiments are conducted on 8 NVIDIA A40 GPU cards in a single node.\nB.2 Experimental Setup\nFinetuning details. As described in \u00a75.1, for NEWS, we start from fo = LLaMA-2 7B (Touvron et al., 2023) and finetune the model on the BBC news articles for 5 epochs with a constant learning rate of 10\u22125 and a batch size of 32. For BOOKS, we start from fo = ICLM 7B (Touvron et al., 2023) and finetune the model on the Harry Potter books with same set of hyperparameters.\nUnlearning details. For all the unlearning methods in Table 3, we use a constant learning rate of 10\u22125 and a batch size of 32. For freinforced used in WHP and Task Vector, we fine-tune ftarget for 10 epochs.\nBefore evaluation, for each unlearning method, we select its optimal epoch or \u03b1 (both of which are parameters that control a degree of unlearning) by using our unlearning stopping criteria based on the unlearned model's utility on Dretain compared to that of fretrain. The chosen epochs or \u03b1's for each method are listed below.\nB.3 Efficiency of Unlearning Methods\nWe report the efficiency of unlearning methods in Table 5, measured by the wall-clock time for a single gradient update step of unlearning. The time measurements were conducted using 8 NVIDIA A40 GPUs on a single node, with a batch size of 32 and an input length of 2048 tokens. Each step corresponds to one gradient update processing a total of 65,536 tokens (32 \u00d7 2048 tokens). For Task Vector and WHP, each step represents one iteration of fine-tuning to create the reinforced model."}, {"title": "C More Experimental Results", "content": "C.1 Confidence Intervals for C1, C2 and C4 in Table 3\nWe compute confidence intervals for C1, C2, and C4 (Mean ROUGE-L F1) using bootstrapping. For each mean ROUGE-L score reported in Table 3, we draw 9,999 bootstrap resamples and calculate a two-tailed 95% confidence interval using the \u201cpercentage\u201d method."}, {"title": "D Dataset Details", "content": "GPT-generated QA pairs. We begin the generation by partitioning the Verbatim text of each corpus into a set of 2048-token excerpts using LLaMA-2's tokenizer. For each QA pair to generate, we randomly sample an excerpt from this set and prompt GPT-4 (gpt-4o-2024-05-13) to create a JSON object with two fields: \u201cquestion\u201d (a question that can only be answered using specific information from the excerpt) and \u201canswer\" (an answer to the \"question\u201d extracted verbatim from the excerpt). We validate and exclude any pairs whose answers cannot be found verbatim in their corresponding excerpts. This verbatim requirement ensures that our Knowledge set is used precisely to evaluate the model's ability to correctly associate questions with relevant portions of the training data.\nFor each QA pair to generate, we initiate a new conversation with GPT-4 with its corresponding excerpt. The instruction begins with a system prompt that specifies the desired format of generated QA pairs as follows:\nDataset segmentation. Table 7 shows examples from MUSE and Table 8 presents detailed statistics for MUSE. For both the NEWS and BOOKS datasets, we include the type of documents along with the number of tokens in each dataset. Additionally, MUSE incorporates Dretainreg, a distinct retain set which is seen by ftarget but not included in Dforget. This set is used exclusively with the GDR and KLR regularizers discussed. To ensure that regularized methods do not directly optimize towards the evaluation set Dretain, Dretainreg is kept disjoint from Dretain."}]}