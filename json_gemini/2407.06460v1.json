{"title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "authors": ["Weijia Shi", "Jaechan Lee", "Yangsibo Huang", "Sadhika Malladi", "Jieyu Zhao", "Ari Holtzman", "Daogao Liu", "Luke Zettlemoyer", "Noah A. Smith", "Chiyuan Zhang"], "abstract": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content, and data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models, leading to the development of many approximate unlearning algorithms. Evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations, because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations.", "sections": [{"title": "1 Introduction", "content": "Training language models (LMs) often involves using vast amounts of text data, which may inadvertently contain private and copyrighted content (Carlini et al., 2021; Henderson et al., 2023; Min et al., 2023). In real-world applications, data owners may demand that their data be removed from a trained language model due to privacy or copyright concerns, as mandated for example by the General Data Protection Regulation (GDPR, European Parliament & Council of the European Union). Moreover, recent copyright lawsuits (DOE 1 v. GitHub, Inc., N.D. Cal. 2022; Tremblay v. OpenAI, Inc.,, 2023) emphasize the need for removing copyrighted data from the model."}, {"title": "2 Machine Unlearning: Preliminaries and Notations", "content": "Machine unlearning (Ginart et al., 2019; Liu et al., 2020; Izzo et al., 2021; Sekhari et al., 2021; Gupta et al., 2021; Ye et al., 2022b; Liu et al., 2024) has emerged as an important capability to accommodate data removal requirements that arise from scenarios with privacy or copyright concerns.\nWe briefly describe the machine unlearning setting. Consider a dataset $D_{train}$ and a model $f_{target}$ trained on $D_{train}$. Suppose we design an algorithm $U$ to unlearn a specific subset (i.e., the forget set) $D_{forget} \\subset D_{train}$ from $f_{target}$. We want to preserve performance on a retain set $D_{retain} = D_{train} \\setminus D_{forget}$, and we also evaluate the model on an in-distribution but disjoint hold-out set $D_{holdout}$ which the model has never been trained on. So, the unlearning algorithm $U$ takes $f_{target}$, $D_{forget}$, and, optionally, $D_{retain}$ and outputs an unlearned model $f_{unlearn}$. Exact unlearning ensures $f_{unlearn}$ is behaviorally identical to the model resulting from retraining from scratch, denoted $f_{target}^*$, but such retraining is usually too costly in real world deployment, so we focus on evaluating approximate unlearning algorithms."}, {"title": "3 The MUSE Evaluation Benchmark", "content": "MUSE evaluates a comprehensive set of desirable properties of machine unlearning across six facets. We detail the evaluation metrics in \u00a73.1 and describe the evaluation corpus in \u00a73.2."}, {"title": "3.1 Evaluation Metrics", "content": "Ideally, an unlearned model should behave as if it had never seen the forget set, exhibiting similar behavior to a retrained model on any corpus $D$ such that $m(f_{unlearn}, D) \\approx m(f_{retrain}, D)$, where $m$ represents any evaluation metric. Prior evaluations on LM unlearning focus on performance of specific tasks like question answering (e.g., Eldan & Russinovich, 2023; Maini et al., 2024). However, these metrics do not faithfully reflect data owner expectations and real-world deployment considerations when performing unlearning. To address this, we propose comprehensive evaluation metrics that consider both data owner and deployer expectations. A comparison between MUSE and the prior benchmark is shown in Table 3.\nData owner expectations. When removing a forget set from a model, data owners typically have three main expectations regarding the unlearned model: (C1) No verbatim memorization: The model should not exactly replicate any details from the forget set. (C2) No knowledge memorization: The model should be incapable of responding to questions about the forget set. (C3) No privacy leakage: It should be impossible to detect that the model was ever trained on the forget set. For example, if a patient's records are unlearned from a medical diagnosis model, in addition to verbatim"}, {"title": "C1. No verbatim memorization", "content": "When a model has unlearned a medical record, it should not output its contents verbatim. We quantify the verbatim memorization VerbMem by prompting the model with the first $l$ tokens from a sequence $x[:l] \\in D_{forget}$ and comparing the continuation outputted by the model $f$ to the true continuation $x[l+1:] \\in D_{forget}$ using the ROUGE-L F1 score (Lin, 2004).\n$VerbMem(f, D) := \\frac{1}{|D_{forget}|} \\sum_{x \\in D_{forget}} ROUGE(f(x[;:l]), x[l+1:])$"}, {"title": "C2. No knowledge memorization", "content": "When a model has unlearned a medical record, it should no longer be able to answer questions about that record. We measure a model $f$'s memorization of knowledge from the forget set $D_{forget}$ as follows: for each example $x \\in D_{forget}$ associated with a question-answer pair $(q, a)$, we gather the model's answer to the question $q$, denoted $f(q)$. We then average the ROUGE scores for all question-answer pairs in $D_{forget}$ to compute the knowledge memorization score KnowMem:\n$KnowMem (f, D_{forget}) := \\frac{1}{|D_{forget}|} \\sum_{(q,a) \\in D_{forget}} ROUGE(f(q), a)$"}, {"title": "C3. No privacy leakage", "content": "As discussed previously, it is desirable that the unlearned model does not leak membership information indicating that $D_{forget}$ was part of $D_{train}$. To determine if a given example was used during training, membership inference attack (MIA) exploits distributional differences in certain statistics (e.g., loss) between training (member) and non-training (non-member) data: if the loss on the example is low, then it was likely used for training. Unlearning typically increases the loss on the example, but there are two possible ways that unlearning can fail to prevent privacy leakage: (1) under-unlearning, when the loss is not made large enough; and (2) over-unlearning, when the loss is made abnormally large. To accurately measure the privacy leakage, we employ Min-K% Prob (Shi et al., 2024a), a state-of-the-art MIA method for LMs based on the loss, and compute the standard AUC-ROC score (Murakonda et al., 2021; Ye et al., 2022a) of discriminating $D_{forget}$ (members) and $D_{holdout}$ (non-members). By comparing the AUC score with that of the retrained model, we define\n$PrivLeak := \\frac{AUC(f_{unlearn}; D_{forget}, D_{holdout}) - AUC(f_{retrain}; D_{forget}, D_{holdout})}{AUC(f_{retrain}; D_{forget}, D_{holdout})}$\nThe PrivLeak metric for a good unlearning algorithm should be close to zero, whereas an over/under-unlearning algorithm will get a large positive/negative metric."}, {"title": "Deployer expectations", "content": "Model deployers have their own considerations for using unlearning algorithms in the real world. Unlearning specific datapoints can unpredictably degrade model capabilities in ways that are difficult to recover. Moreover, deployers are expected to effectively accommodate somewhat large-scale forget sets and successive unlearning requests from data owners. As such, we consider three key metrics: (C4) utility preservation on the retain set, (C5) scalability to handle large-scale content removal, and (C6) sustainability to maintain performance over sequential unlearning requests."}, {"title": "C4. Utility preservation", "content": "Model capabilities are often hard-won through expensive training proce- dures, so deployers would want an unlearning algorithm that preserves performance on the retain set. To quantify this, we evaluate the unlearned model's performance on the retain set using the knowledge memorization metric $KnowMem (f_{unlearn}, D_{retain})$."}, {"title": "C5. Scalability", "content": "We assess the scalability of unlearning methods by examining their performance on forget sets of varying sizes. Let $D^c$ denote a forget set of size $c$, and $f^c$ be the corresponding unlearned model. For any data owner-valued metric such as utility preservation, we measure scalability by analyzing the trend of this metric as $c$ increases from small to large values."}, {"title": "C6. Sustainability", "content": "Machine unlearning operations often need to be applied sequentially, as data removal requests may arrive at different times. We denote the unlearned model after processing the $k$-th request as $f_{u,k}$. To measure sustainability, we analyze the trend of any data owner-valued metric as the number of sequential unlearning requests $k$ increases."}, {"title": "3.2 Evaluation Corpus", "content": "MUSE considers two representative types of textual data that may frequently involve unlearning requests: news articles (Tremblay v. OpenAI, Inc.,, 2023) and books (Eldan & Russinovich, 2023). These datasets are detailed as follows:\n\u2022 NEWS consists of BBC news articles (Li et al., 2023b) collected after August 2023. All articles are randomly divided into (disjoint) forget, retain, and holdout sets.\n\u2022 BOOKS consists of the Harry Potter book series. To simulate a real-world setting for testing utility preservation (C4), we include different types of materials in the forget and retain sets. The forget set contains the original books, while the retain set contains related content from the Harry Potter FanWiki, representing domain knowledge that should be retained after unlearning.\nFor each corpus, we construct: 1) Verbatim text: the original text to assess the unlearning methods to remove verbatim memorization (C1), and 2) Knowledge set: a set of derived (question, answer) pairs based on the original texts to evaluate the unlearning method's effectiveness in purging learned"}, {"title": "4 Unlearning Methods", "content": "We evaluate eight efficient approximate unlearning methods belonging to four families of algorithms.\nFour families of unlearning methods. We first introduce four families of unlearning methods, which serve as the basis for the eight methods we evaluate.\n\u2022 Gradient Ascent (GA) minimizes the likelihood of correct predictions on $D_{forget}$ by performing gradient ascent on the cross-entropy loss (the opposite of conventional learning with gradient descent). GA has achieved mixed results: while Jang et al. (2023) found it effective for unlearning examples from the Enron email dataset (Klimt & Yang, 2004) with minimal performance degradation, Ilharco et al. (2023) reported that GA significantly harms general model utility when unlearning a high-toxicity subset of the Civil Comments dataset (Borkan et al., 2019).\n\u2022 Negative Preference Optimization (NPO; Zhang et al., 2024b) treats the forget set as negative preference data and adapts the offline DPO objective (Rafailov et al., 2023) to tune the model to assign low likelihood to the forget set without straying too far from the original model $f_{target}$.\n$\\mathcal{L}_{NPO}(\\theta) = - \\mathbb{E}_{x \\sim D_{forget}} log \\sigma\\left(\\beta (log \\frac{f_{\\theta}(x)}{f_{target} (x)} )\\right],$\nwhere $f_{\\theta}$ refers to the model that undergoes unlearning, $\\sigma$ is the sigmoid function, and $\\beta$ is a hyperparameter that controls the allowed divergence of $f_{\\theta}$ from its initialization $f_{target}$. Following Rafailov et al. (2023); Zhang et al. (2024b), we fix $\\beta = 0.1$ in our experiments.\n\u2022 Task Vectors (Ilharco et al., 2023) derived from straightforward arithmetic on the model weights can effectively steer neural network behavior. We adapt task vectors to perform unlearning in two stages. First, we train $f_{target}$ on $D_{forget}$ until the model overfits, yielding a reinforced model $f_{reinforce}$. We then obtain a task vector related to $D_{forget}$ by calculating the weight difference between $f_{target}$ and $f_{reinforce}$. To achieve unlearning, we subtract this task vector from $f_{target}$'s weights, intuitively moving the model away from the direction it used to adapt to $D_{forget}$ \u2013 i.e., $f_{unlearn} = f_{target} - (f_{reinforce} - f_{target})$.\n\u2022 Who's Harry Potter (WHP; Eldan & Russinovich, 2023) defines the unlearned model $f_{unlearn}$ as the interpolation between the target model $f_{target}$ and the reinforced model $f_{reinforce}$. Let $p_f(\\cdot|x)$ denote the token distribution parametrized by the model $f$ when given a prompt $x$ as input. Then, concretely, for any input $x$, WHP samples the next token from\n$p_{f_{unlearn}} (\\cdot|x) = p_{f_{target}} (\\cdot|x) - \\alpha (p_{f_{reinforce}} (\\cdot|x) - p_{f_{target}} (\\cdot|x))$\nwhere $\\alpha$ is a hyperparameter that controls the interpolation between the two models."}, {"title": "Two regularizers for utility preservation", "content": "GA and NPO are not explicitly designed for utility preservation, so we discuss several regularization strategies that either improve the performance on the retain set or ensure the unlearned model remains close to the target model during unlearning.\n\u2022 Gradient Descent on the Retain Set (GDR; Liu et al., 2022; Maini et al., 2024; Zhang et al., 2024b) augments the unlearning objective with a standard gradient descent learning objective on the cross-entropy of the retain set $D_{retain}$ to more directly train the model to maintain its performance on $D_{retain}$.\n\u2022 KL Divergence Minimization on the Retain Set (KLR; Maini et al., 2024; Zhang et al., 2024b) encourages the unlearned model's probability distribution $p_{f_{unlearn}} (\\cdot|x)$ to be close to the target model's distribution $p_{f_{target}} (\\cdot|x)$ on inputs from the retain set $x \\in D_{retain}$."}, {"title": "5 Experiments", "content": "We evaluate the eight representative unlearning methods using the experimental setup described in \u00a75.1. We present the results for data owner expectations in \u00a75.2 and for deployer expectations in \u00a75.3."}, {"title": "5.1 Experimental Setup", "content": "Retrained and target models. We start with a general pretrained base model $f_0$, and finetune two models: $f_{target}$ on $D_{forget} \\cup D_{retain}$, and $f_{retrain}$ on $D_{retain}$ only. See Appendix B.2 for details about finetuning. For each unlearning algorithm $U$, we further generate the unlearned model $f_{unlearn} = U(f_{target}, D_{forget}, D_{retain})$. We ensure that $f_0$ has no access to $D_{forget}, D_{retain}, D_{holdout}$. Therefore, for NEWS, we use $f_0$ = LLaMA-2 7B (Touvron et al., 2023), which was released before the BBC news articles we use to construct our benchmarks; and for BOOKS, we use $f_0$ = ICLM-7B (Shi et al., 2024b), which does not contain the Harry Potter books in its pretraining data.\nUnlearning experimental configuration. Following prior work (Maini et al., 2024), we run GA, NPO, and their regularized variants using the AdamW optimizer (Loshchilov & Hutter, 2017) with a constant learning rate of $10^{-5}$ and a batch size of 32. We employ the stopping criteria as follows: if the utility (i.e., KnowMem on $D_{retain}$) of a model undergoing unlearning drops below that of $f_{retrain}$ within 10 epochs of unlearning, we stop at the first epoch where this condition holds; otherwise, we take a checkpoint from the 10th epoch. For Task Vector and WHP, to obtain the reinforced model for unlearning, we fine-tune the target model for 10 epochs using the same learning rate and batch size. Further details on the model fine-tuning and unlearning can be found in Appendix B.2."}, {"title": "5.2 Results: Data Owner Expectations", "content": "We first analyze how eight unlearning methods meet data owner expectations (C1, C2 & C3 in \u00a73.1)."}, {"title": "C1&C2. Most methods are effective for unlearning memorization.", "content": "As shown in Table 3, most unlearning methods perform exceptionally well in [C1. No verbatim memorization] and [C2. No knowledge memorization], often reducing VerbMem and KnowMem even beyond the levels achieved by the retrained model. Notably, some methods, such as GA and NPO, achieve a score of 0 for both VerbMem and KnowMem, meaning that these methods completely prevent the unlearned models from producing any text related to the forget set. However, as we will see later, these reductions often come at the cost of significant utility loss on the retain set."}, {"title": "C3. Unlearning leads to privacy leakage.", "content": "Most unlearning methods reveal the membership of $D_{forget}$ in $D_{train}$ through under-unlearning (PrivLeak \u226a 0) or over-unlearning (PrivLeak \u226b 0), as shown in Table 3. We further examine the effectiveness of membership inference by plotting ROC curves in Figure 4. The deviation from the diagonal line indicates the attacker's advantage over random guessing. We observe that the Min-K% Prob based attack achieves AUC \u2248 0 on $f_{target}$, confirming its effectiveness. Meanwhile, the ROC curve for $f_{retrain}$ closely follows the diagonal line (AUC = 0.47), suggesting that perfect unlearning ensures MIA is no more effective than random guessing. Among the approximate unlearning methods, GA and NPOGDR without regularizers consistently over-unlearn (AUC > 0.7), whereas KLR-regularized methods (NPOKLR and GAKLR) tend to under-unlearn and barely improve privacy leakage over $f_{target}$. WHP also deviates from the diagonal significantly."}, {"title": "5.3 Results: Deployment Considerations", "content": "C4. Unlearning significantly degrades model utility. Table 3 [C4 Utility Preserv.] shows that all unlearning methods compromise the model's utility by 24.2% ~ 100%. Notably, several methods (GA, GAGDR, NPOGDR) lead to complete utility loss, rendering the unlearned models practically unusable. Figure 5 illustrates the trade-offs between utility preservation on $D_{retain}$ and knowledge memorization on $D_{forget}$. An ideal unlearned model should mimic the behavior of $f_{retrain}$ (desired region) by achieving a low level of memorization on $D_{forget}$ while maintaining its utility. However, most methods, such as GAKLR, NPOKLR, and WHP, unlearn the knowledge on $D_u$ at the cost of utility.\nC5. Unlearning methods scale poorly with forget set sizes. To evaluate the robustness of the unlearning methods to larger forget sets, we collect additional news articles from the same distribution to scale our NEWS corpus from 0.8M tokens to 3.3M tokens and observe the utility preservation at four different forget set sizes. As shown in Figure 6 (a), the model utility decrease with the size of the forget set and achieves a minimum at the largest size.\nC6. Unlearning methods cannot sustainably accommodate sequential unlearning requests. To evaluate the robustness of these unlearning methods to more than one unlearning requests, we sequentially apply $k$ unlearning processes, each with respect to a different forget set. To simulate sequential unlearning, we partition the extended NEWS forget set (comprised of 3.3M tokens) into four disjoint folds (each containing 0.8M tokens) and apply the unlearning methods to each fold in a sequential manner."}, {"title": "6 Related Work", "content": "Machine unlearning for non-language model applications. Machine unlearning is a long-running, well-studied topic. Several studies have explored exact unlearning, aiming to make the unlearned model ($f_{unlearn}$) exactly identical to the reference model ($f_{retrain}$). As expected, this can only be accomplished in simple models like SVMs (Cauwenberghs & Poggio, 2000; Tveit et al., 2003; Romero et al., 2007; Karasuyama & Takeuchi, 2010) or naive Bayes models (Cao & Yang, 2015). Another approach is to ensure that the unlearned model $f_{unlearn}$ is probabilistically indistinguishable from $f_{retrain}$ (Ginart et al., 2019; Guo et al., 2020), and this view of certifiable unlearning is closely related to differential privacy (Dwork et al., 2006b,a). This rigorous definition of unlearning has inspired several theoretical works that characterize the feasibility of unlearning in convex and non-convex models, but those proposed algorithms are too computationally costly to operate on modern-day LMs (Izzo et al., 2021; Neel et al., 2021; Ullah et al., 2021; Sekhari et al., 2021; Gupta et al., 2021). Several more tractable unlearning algorithms have been proposed (Borkan et al., 2019; Ginart et al., 2019; Thudi et al., 2022; Chourasia & Shah, 2023) with broader applications such as image classification (Ginart et al., 2019; Golatkar et al., 2020a), text-to-image generation (Gandikota et al., 2023; Zhang et al., 2023; Fan et al., 2023), Federated Learning (Liu et al., 2020; Che et al., 2023; Halimi et al., 2022) and Recommender Systems (Li et al., 2024b).\nMachine unlearning for language models: methods and applications. Machine unlearning has recently found its way into language model applications. In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance. Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022; Wu et al., 2023; Wei et al., 2024), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units."}, {"title": "In-context unlearning", "content": "(Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.\nMachine unlearning has also been applied to various downstream language model tasks, though the unit of machine unlearning may differ from what we study in this work. Our evaluation focuses on unlearning specific examples or datasets, aiming to make LMs forget either the phrasing or the content knowledge of targeted data, while preserving their utility for data not targeted for removal. This is crucial for ensuring privacy and copyright compliance. In addition to this specific unlearning, there's also a broader application similar to model editing, where outdated information is replaced with new knowledge (Pawelczyk et al., 2023; Yu et al., 2023; Belrose et al., 2024). Moreover, efforts have been made to eliminate harmful behaviors in language models by creating toxicity benchmarks and enhancing safety measures (Lu et al., 2022; Yao et al., 2023; Li et al., 2024a; Zhang et al., 2024b). Despite these varied approaches to unlearning at different operational and knowledge levels, the evaluation principles we propose such as preserving utility, ensuring scalability, and maintaining sustainability are relevant across these contexts.\nMachine unlearning for language models: evaluation. Evaluating machine unlearning methods for language model applications is also critical. Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion. For example, Eldan & Russinovich (2023) experiment with unlearning to forget Harry Potter books and demonstrate the effectiveness of their methods by showing that familiarity scores, measured through completion-based, token-probability-based, and question-answering evaluations, significantly decline post-unlearning. Lynch et al. (2024) further suggest comparing unlearned models with perfectly retrained models. Their evaluation finds that while familiarity scores with the forget set may drop post-unlearning, they still remain higher than those of the retrained model. The closest work to ours is TOFU (Maini et al., 2024), a benchmark featuring 200 synthetic author profiles, each with 20 question-answer pairs, divided into forget and retain sets. However, TOFU is small-scale (0.15M tokens) and lacks separation of original text and knowledge, which conflates the evaluation of unlearning verbatim text and knowledge (C1 and C2 in MUSE). Additionally, current evaluations focus on limited aspects of data owner expectations and do not adequately reflect real-world deployment considerations, such as scalability and potential sequential unlearning requests. In contrast, MUSE formally defines different unlearning scopes and corresponding metrics, resulting in a systematic six-way evaluation featuring both data owners' and deployers' expectations. The evaluation uses a large-scale corpus of over 6 million tokens, separated into verbatim text and knowledge sets. We also note that some of our findings align with previous evaluations. For example, our observation that over- or under-unlearn can exacerbate privacy leakage (\u00a75.2) is consistent with the recent work by Hayes et al. (2024).\nSurvey papers. We direct readers to several insightful survey papers for further reading. For non-LLM applications, notable surveys include Shintre et al. (2019); Nguyen et al. (2022); Thudi et al. (2022); Xu et al. (2023). Additionally, the NeurIPS 2023 machine unlearning competition for image classification is a valuable source of empirical methods tailored for this specific application (Triantafillou et al., 2023). For language model applications, Si et al. (2023) categorize unlearning methods into different families and summarize datasets for evaluating unlearning. Liu et al. (2024) review LM unlearning algorithms by targets and methods, discuss the effectiveness and efficiency of existing approaches and emphasize the importance of clearly defining the unlearning scope."}, {"title": "7 Conclusion", "content": "In this work, we propose MUSE, a comprehensive machine unlearning evaluation benchmark that highlights six desirable properties from the perspectives of both data owners and model deployers. We find that current unlearning methods successfully prevent the model's memorization of content at a significant cost to utility on data not intended for removal. They also lead to severe privacy leakage and cannot sustainably accommodate successive unlearning requests or large-scale content removal. These findings highlight the need for future research into more robust unlearning methods.\nLimitations. While MUSE provides a systematic benchmark for evaluating unlearning algorithms, it does not consider all possible considerations. For example, data owners may have additional expectations, such as ensuring their information cannot be probed from intermediate activations (Song & Raghunathan, 2020) or receiving formal guarantees of unlearning success (Sekhari et al., 2021;"}, {"title": "8 Acknowledgements", "content": "We thank Eric Wallace, Robin Jia, Howard Chen, and anonymous reviewers of the GenLaw workshop for the valuable feedback and discussions."}, {"title": "D Dataset Details", "content": "GPT-generated QA pairs. We begin the generation by partitioning the Verbatim text of each corpus into a set of 2048-token excerpts using LLaMA-2's tokenizer. For each QA pair to generate, we randomly sample an excerpt from this set and prompt GPT-4 (gpt-40-2024-05-13) to create a JSON object with two fields: \u201cquestion\u201d (a question that can only be answered using specific information from the excerpt) and \u201canswer\u201d (an answer to the \"question\u201d extracted verbatim from the excerpt). We validate and exclude any pairs whose answers cannot be found verbatim in their corresponding excerpts. This verbatim requirement ensures that our Knowledge set is used precisely to evaluate the model's ability to correctly associate questions with relevant portions of the training data.\nFor each QA pair to generate, we initiate a new conversation with GPT-4 with its corresponding excerpt. The instruction begins with a system prompt that specifies the desired format of generated QA pairs as follows:\nWe then present the excerpt as a user prompt to the model and collect the generated QA pairs. Here are two example generated QA pairs from the Knowledge set of NEWS:\nDataset segmentation.  is disjoint from the standard Retain Set used in evaluation and is employed in unlearning training to preserve utility through regularizers."}]}