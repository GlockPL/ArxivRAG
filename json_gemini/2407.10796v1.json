{"title": "Mammographic Breast Positioning Assessment via Deep Learning", "authors": ["Toygar Tanyel", "Nurper Denizoglu", "Mustafa Ege Seker", "Deniz Alis", "Esma Cerekci", "Ercan Karaarslan", "Erkin Aribal", "Ilkay Oksuz"], "abstract": "Breast cancer remains a leading cause of cancer-related deaths among women worldwide, with mammography screening as the most effective method for the early detection. Ensuring proper positioning in mammography is critical, as poor positioning can lead to diagnostic errors, increased patient stress, and higher costs due to recalls. Despite advancements in deep learning (DL) for breast cancer diagnostics, limited focus has been given to evaluating mammography positioning. This paper introduces a novel DL methodology to quantitatively assess mammogram positioning quality, specifically in mediolateral oblique (MLO) views using attention and coordinate convolution modules. Our method identifies key anatomical landmarks, such as the nipple and pectoralis muscle, and automatically draws a posterior nipple line (PNL), offering robust and inherently explainable alternative to well-known classification and regression-based approaches. We compare the performance of proposed methodology with various regression and classification-based models. The CoordAtt UNet model achieved the highest accuracy of 88.63% \u00b1 2.84 and specificity of 90.25% \u00b1 4.04, along with a noteworthy sensitivity of 86.04% \u00b1 3.41. In landmark detection, the same model also recorded the lowest mean errors in key anatomical points and the smallest angular error of 2.42 degrees. Our results indicate that models incorporating attention mechanisms and CoordConv module increase the accuracy in classifying breast positioning quality and detecting anatomical landmarks. Furthermore, we make the labels and source codes available to the community to initiate an open research area for mammography, accessible at https://github.com/tanyelai/deep-breast-positioning.", "sections": [{"title": "1 Introduction", "content": "Breast cancer remains the most common cancer and leading cause of cancer-related deaths among women globally [3]. Mammography screening is the most effective method for early detection, significantly reducing mortality rates [12]. Thus, many countries have adopted national screening programs [4].\nA standard mammogram includes craniocaudal (CC) and mediolateral oblique (MLO) views, with the MLO view being crucial as it captures nearly the entire breast tissue, especially the upper quadrant where cancer frequently occurs. Proper positioning in mammography is vital, as poor positioning can lead to diagnostic errors and necessitate repeat exams, increasing costs and causing stress for patients [5,9,11,17]. There is a pressing need for automated systems that can instantly evaluate the quality of mammogram positioning, allowing technologists to take immediate corrective action if necessary.\nRecent advancements in deep learning (DL) have shown promising results in breast cancer diagnostics, often matching or surpassing radiologists in accuracy [7, 14]. However, less focus has been given to using DL for assessing mammography positioning. This gap presents an opportunity to improve the evaluation process right after image acquisition. Traditionally, studies have relied on classification-based DL approaches involving qualitative expert assessments [2, 18] or by dividing related tasks into separate regression processes [8], which can introduce additional complexity and affect the explainability and objectivity of the results.\nOur work introduces a novel deep learning methodology that quantitatively evaluates image positioning quality in MLO views. By identifying key anatomical features such as the nipple and pectoralis muscle, and automatically drawing a perpendicular posterior nipple line (PNL) to the pectoralis muscle or film edge, our methodology provides a robust and superior alternative to traditional classification-based approaches. We demonstrate the effectiveness of our method on existing models, showcasing its potential to enhance mammography positioning assessments."}, {"title": "2 Materials", "content": "In this section we provide information on the dataset and the ground truth criteria for correct MLO positioning."}, {"title": "2.1 Study Sample", "content": "We used the VinDr Mammography dataset [13], an open-access collection of 5000 exams from two hospitals in Vietnam (2018-2020). From this, we selected 1000 exams, each with two MLO view mammograms from both breasts, totaling 2000 images. Exams were split into training, validation, and testing sets with an 80%/10%/10% split, ensuring a balanced representation of clinical outcomes. According to the PNL criteria, MLO-view positioning in the datasets was classified as 967 good and 633 poor for training, 108 good and 92 poor for validation, and 123 good and 77 poor for testing."}, {"title": "2.2 Image Positioning Quality Criterion", "content": "Several international systems assess the quality of mammography images in MLO views based on criteria like the angle, width, and length of the pectoral muscle, its border angulation, and the distance between the pectoral muscle and the nipple. The primary goal is to ensure maximum breast tissue coverage. Some criteria, such as the distance from the pectoral muscle to the nipple, are subjective and impractical [16]. The angle and dimensions of the pectoral muscle lack universal standards. A consistent criterion is that the PNL, drawn from the nipple to the pectoralis muscle at a right angle, intersects the pectoralis muscle. This method is endorsed by the American College of Radiology and the Royal Australian and New Zealand College of Radiologists [1, 10, 15, 16, 19] and is adopted as our study's reference standard."}, {"title": "2.3 Ground Truthing Process", "content": "Ground truth annotations were performed by a board-certified breast radiologist (N.D.) with over five years of experience in breast imaging. The radiologist used a specialized workstation, featuring a browser-based annotation tool (https://matrix.md.ai) and a 6-megapixel diagnostic monitor (Radiforce RX 660, EIZO), to annotate mammograms. All mammograms were examined in the Digital Imaging and Communications in Medicine (DICOM) format. The radiologist marked the nipple and pectoralis muscle line on MLO views."}, {"title": "3 Methods", "content": "In this section we provide details of our pre-processing operation, loss function, model architecture and experimental setup."}, {"title": "3.1 Pre-processing Steps", "content": "The pre-processing steps for mammography images are designed to prepare the data for analysis while preserving anatomical features and spatial relationships. Initially, the midpoint of the nipple bounding boxes and the endpoints of the pectoralis muscle are extracted, yielding three critical landmarks for orientation and scale adjustments. Next, the endpoints of the pectoralis muscle are standardized by extending each line to the image boundary with a 10-pixel margin to minimize variability from radiologists' arbitrary line terminations. Significant breast regions are then isolated by removing extensive black pixel areas around the periphery and below the breast. This involves thresholding the image to create a binary version, applying morphological opening, labeling connected regions, and cropping the image to the bounding box of the largest region. Zero-padding is applied to make the images square, preventing distortion during resizing and maintaining uniformity across the dataset. Subsequently, all images are resized to 512x512 pixels to facilitate computational efficiency and model training while preserving necessary detail."}, {"title": "3.2 Landmark-Aware Wing Loss", "content": "Landmark-Aware Wing Loss is tailored to improve the model's accuracy in predicting landmark coordinates. It employs a piecewise function that combines the Wing Loss's sensitivity to small errors with a linear part to moderate the response to larger errors. The Wing Loss [6] formula is given by:\n\nLWing(y) =\\begin{cases} w \\cdot log(1+\\frac{|y|}{\\epsilon}), & \\text{if } |y| < w \\\\\\  |y| - C, & \\text{otherwise} \\end{cases}\n\nwhere y represents the absolute error between the predicted and target coordinates, w is the parameter that defines the width of the non-linear region,  controls the curvature within this region, and C is a continuity constant, defined as $C = w - w \\cdot log(1+\\frac{w}{\\epsilon})$.\nPractically, the LLAW (y) is calculated for each coordinate of the landmarks, leading to a comprehensive loss for each landmark by summing up the mean losses of the coordinates, i.e., mean of x and y, expressed as:\n\n$L_{LLAW} = \\alpha \\cdot L_{Wing}(L_1) + \\beta \\cdot L_{Wing}(L_2) + \\gamma \\cdot L_{Wing}(L_3)$\n\nwhere \u03b1, \u03b2, and y are the weights for the landmarks."}, {"title": "3.3 Model Architectures and Techniques for Landmark Detection", "content": "We used U-Net as the backbone with coordinate convolution module (CoordConv) and attention mechanisms for landmark detection (Fig. 1). CoordConv and attention refine feature maps, improving spatial information. We also used ResNeXt50 as the backbone for landmark regression, adjusting it for single-channel input and comparing its classification and regression results. This section details these components and the landmark regression process.\nCoordConv Integration To improve spatial awareness, we replaced the initial convolutional layer with a CoordConv layer, integrating spatial coordinates into the convolution operation. CoordConv augments the input by adding normalized spatial coordinates across height (H) and width (W), improving the model's ability to learn spatial hierarchies:\n\nCC(I) = R (B (C (A(I, {x,y}))))\n\nwhere A(I, {x,y}) adds coordinate channels to input tensor I. The x-coordinates {x} and y-coordinates {y} are normalized in the range [0, 1], calculated as:\n\n$X_j = \\frac{j}{W-1}, Y_i = \\frac{i}{H-1}$\n\nfor each pixel (i, j) in the feature map. These coordinates are repeated across the batch size and stacked to form two new channels appended to I.\nC performs convolution with these channels, and B and R represent batch normalization and ReLU activation.\nAttention Mechanism Attention mechanisms in the U-Net model selectively focus on important features. The attention block integrates features from both encoder and decoder paths by computing attention coefficients (\u03c8) using the following formulations: g = Wgate * G and x = Wx * X. Here, Wgate and Wx are convolutional filters, with G representing the gating signal from the decoder and X the feature map from the encoder. The attention coefficients \u03c8 are computed as \u03c8 = \u03c3(ReLU(g + x)), where \u03c3 denotes the sigmoid activation. The attended output A is then calculated as A = X\u00b7\u03c8. This mechanism ensures that only the most relevant features are propagated through the network to improve the precision of the output.\nResNeXt50 We used ResNeXt50, a 50-layer deep residual network with a cardinality of 32, featuring grouped convolutions for enhanced feature learning. For landmark regression, we modified ResNeXt50 to accept single-channel input and output the required landmark coordinates. Additionally, we used the raw classification model to compare image-level classification with regression results, evaluating differences in accuracy and robustness, and demonstrating the model's versatility and effectiveness."}, {"title": "3.4 Evaluation", "content": "The evaluation checks our model's accuracy in predicting the nipple and endpoints of the pectoral muscle line. We verify if the perpendicular intersection from the nipple to the pectoral muscle (i.e., PNL) falls within the image boundaries to indicate image quality. We also measure the angular error between the original and predicted pectoral muscle lines. Additionally, we compute accuracy, sensitivity, and specificity for the model's decisions. Euclidean distance between predicted and original landmarks in millimeters is calculated, ensuring real-world measurement accuracy. The angle error is normalized to 0-180 degrees to represent the deviation from vertical. Performance metrics are based on image quality classification, with accuracy as the proportion of correct predictions, sensitivity as the proportion of correctly identified bad quality images, and specificity as the proportion of correctly identified good quality images."}, {"title": "3.5 Experimental Setup", "content": "Regression We employed the R-ResNeXt50, UNet, Attention UNet, and CoordAtt UNet models, each configured with a single input channel and six output channels corresponding to the x, y coordinates of three landmarks. Training was conducted on an NVIDIA L4 GPU for 300 epochs. The Adam optimizer was used with an initial learning rate of 1 \u00d7 10-4, dynamically adjusted by a CyclicLR scheduler oscillating between 1\u00d710-5 and 5\u00d710-4 using a triangular policy without cycle momentum. The loss function integrated Wing Loss (w = 3,  = 1.5) and additional parameters (\u03b1 = 1.0, \u03b2 = 1.0, \u03b3 = 1.0) to capture both precision and geometric intricacies in landmark detection. The model with the lowest validation loss was preserved for subsequent evaluation. As an exception, the R-ResNeXt50 model was initially fine-tuned with pretrained ImageNet weights using a batch size of 8 for 150 epochs.\nClassification For classification, we utilized a ResNeXt50 model to classify images into two positioning quality classes. Training was conducted on an NVIDIA L4 GPU with a batch size of 8 for 30 epochs, fine-tuning the model with pre-trained ImageNet weights. The same optimizer and learning rate progression as in the regression setup were applied. Categorical Cross-Entropy Loss was used for loss calculations. The best-performing model, determined by a range of metrics, was preserved for subsequent evaluation."}, {"title": "4 Results", "content": "In this section, we provide results on landmark detection and binary image quality assessment.\nModels' Performance on Landmark Detection.\nTable 1 details models' performance on landmark detection, focusing on distance errors. Direct landmark regression with ResNeXt50 (R-ResNeXt50) and vanilla UNet regression (UNet) showed higher errors for pectoral and nipple landmark detection. Attention UNet considerably improved performance both in terms of median errors, and an angular errors. CoordAtt UNet outperformed others with mean errors of 4.99mm (Perp), 5.62mm (Pec1), 6.49mm (Pec2), 2.97mm (nipple), and the smallest angular error of 2.42 degrees.\nModels' Performance on Breast Positioning Labels.\nThe models' performance on breast positioning labels is summarized in Table 2. The raw ResNeXt50 model used for binary classification without landmark regression achieved the lowest performance measures. Addition of landmark regression and rule-based binary classification (R-ResNeXt50), improved the performance dramatically. Vanilla UNet regression of the landmark points showed poor performance, where Attention block addition (Attention UNet) outperformed previous models. Addition of coordinate points (CoordAtt UNet) achieved comparable performance to Attention UNet showcasing superior performance in accuracy and specificity."}, {"title": "5 Discussion", "content": "In this study, we presented a novel deep learning methodology for assessing the quality of mammogram positioning, focusing on the MLO views. Our method quantitatively evaluates image positioning by identifying key anatomical features and drawing a perpendicular PNL, providing a robust alternative to traditional classification-based approaches. The evaluation of various deep learning models, including ResNeXt50, UNet, Attention UNet, and CoordAtt UNet, demonstrated considerable improvements in accuracy, specificity, and sensitivity. Notably, the CoordAtt UNet model achieved the highest performance, highlighting the effectiveness of incorporating attention mechanisms and CoordConv module (Fig. 2). This study addresses a critical unmet need in mammography screening, offering an automated, objective, and explainable solution for assessing breast positioning quality, which is crucial for accurate breast cancer diagnosis.\nDespite the promising results, several limitations must be acknowledged. Our study focused exclusively on MLO views, which, while comprehensive, do not cover all diagnostic perspectives. Future work will extend the model to include CC views to provide a more holistic evaluation of mammogram positioning. Additionally, our primary criterion for evaluating positioning quality was the PNL. While robust for MLO views, the models' effectiveness might be limited when considering other criteria, such as the angle and shape of the pectoral muscle. Future studies will aim to incorporate these additional criteria to improve the models' versatility. The clinical impact of this research is significant, as it paves the way for more reliable and efficient mammography screening, ultimately improving early breast cancer detection and patient outcomes."}]}