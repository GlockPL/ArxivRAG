{"title": "MORAL ALIGNMENT FOR LLM AGENTS", "authors": ["Elizaveta Tennant", "Stephen Hailes", "Mirco Musolesi"], "abstract": "Decision-making agents based on pre-trained Large Language Models (LLMs) are increasingly being deployed across various domains of human activity. While their applications are currently rather specialized, several research efforts are un-der way to develop more generalist agents. As LLM-based systems become more agentic, their influence on human activity will grow and the transparency of this will decrease. Consequently, developing effective methods for aligning them to human values is vital.\nThe prevailing practice in alignment often relies on human preference data (e.g., in RLHF or DPO), in which values are implicit and are essentially deduced from relative preferences over different model outputs. In this work, instead of relying on human feedback, we introduce the design of reward functions that explicitly encode core human values for Reinforcement Learning-based fine-tuning of foun-dation agent models. Specifically, we use intrinsic rewards for the moral alignment of LLM agents.\nWe evaluate our approach using the traditional philosophical frameworks of Deon-tological Ethics and Utilitarianism, quantifying moral rewards for agents in terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD) environ-ment. We also show how moral fine-tuning can be deployed to enable an agent to unlearn a previously developed selfish strategy. Finally, we find that certain moral strategies learned on the IPD game generalize to several other matrix game en-vironments. In summary, we demonstrate that fine-tuning with intrinsic rewards is a promising general solution for aligning LLM agents to human values, and it might represent a more transparent and cost-effective alternative to currently predominant alignment techniques.", "sections": [{"title": "1 INTRODUCTION", "content": "The alignment problem is an active field of research in Machine Learning (Christian, 2020; Wei-dinger et al., 2021; Anwar et al., 2024; Gabriel et al., 2024; Ji et al., 2024; Ngo et al., 2024). It is gaining even wider importance with the advances and rapid deployment of Large Language Models (LLMs, Anthropic 2024; Gemini Team 2024; OpenAI 2024). The most common practices in the alignment of LLMs today involve Reinforcement Learning from Human Feedback (RLHF - Glaese et al. 2022; Ouyang et al. 2022; Bai et al. 2023) or Direct Preference Optimization (DPO - Rafailov et al. 2024). Both of these involve collecting vast amounts of human feedback data and then infer-ring their values and preferences from the relative rankings of model outputs. In doing so, human values are implicitly represented.\nThis approach poses certain challenges (Casper et al., 2023). Specifically, collecting preference data is very costly and often relies on potentially unrepresentative samples of human raters. Indeed, the values derived through this process are strongly dependent on the selection criteria of the pool of individuals. Furthermore, human preferences are notoriously complex and inconsistent. In RLHF, the values that are ultimately incorporated into the fine-tuned models are learned by a reward model from data in a fully bottom-up fashion, and are never made explicit to any human oversight. One might argue that current LLMs fine-tuned with these methods are able to provide \u201chonest, harmless and helpful\" responses (Glaese et al., 2022; Bai et al., 2023) and already display certain moral values"}, {"title": "2 BACKGROUND", "content": "2.1 LLM AGENTS\nAgency refers to the ability of a system to decide to take actions in the world (Swanepoel & Corks, 2024). In this paper, we equate agency with strategic decision-making - i.e., making a choice in an environment in which multiple actions are available and lead to different outcomes. In the case of LLMs, this view assumes that model outputs will be interpreted as actions in some environment. The simplest way of implementing this is through the use of specific tokens to represent the actions. Particular tokens can be reserved or fine-tuned from the model's vocabulary to represent actions, and planning and reasoning ability can be improved via action-driven prompting strategies (Yao et al., 2023). Other ways of implementing LLM agents can involve generation of executable code for a"}, {"title": "2.2 FINE-TUNING LLM AGENTS WITH REINFORCEMENT LEARNING", "content": "Proximal Policy Optimization (PPO, Schulman et al. 2017) is the most commonly used technique for fine-tuning LLMs with RL (Stiennon et al., 2022). This on-policy method is often deployed in conjunction with a Kullback-Leibler (KL) penalty to prevent the new model from shifting too far away from the original underlying token distribution and thus losing other capabilities such as producing coherent linguistic output (Jaques et al., 2017; Ziegler et al., 2020; Stiennon et al., 2022). Offline fine-tuning methods have also been developed (Snell et al., 2023) and may provide a more sample-efficient alternative in the future. The reward signal for RL-based training in existing implementations tends to be derived from preference data provided by human raters (Glaese et al., 2022; Ouyang et al., 2022; Bai et al., 2023) or a constitution of other human and/or artificial agents (Bai et al., 2022; Huang et al., 2024). In this study we propose a new methodology for RL-based fine-tuning with intrinsic moral rewards.\nCompared to non-linguistic RL agent training, the pre-trained LLM in this case can be viewed as providing a common-sense model \u00b2 of the world (Wong et al., 2023), equipping an LLM-based agent with some intuition about potential dynamics of various environments. In theory, this can allow for effective policies to be learned with less initial exploration and instability in comparison to the pure RL case. Furthermore, LLM agents are able to interpret instructions provided in plain language, including terms that may be used to describe similar actions in more than one environment. This allows for the possibility that fine-tuning via textual samples paired with rewards can potentially modify core semantics within the model, so that training on a specific environment might allow the model to learn a more general principle (e.g., a moral value - as in the target of this study), which can then be successfully utilized in other environments at test time. Early results from text-instructed"}, {"title": "2.3 SOCIAL DILEMMA GAMES", "content": "A prominent social dilemma game is the Iterated Prisoner's Dilemma (IPD), in which a player can Cooperate (C) with their opponent for mutual benefit, or betray them - i.e., Defect (D) for individual reward (Rapoport, 1974; Axelrod & Hamilton, 1981). The payoffs in any step of the game are determined by a payoff matrix, presented for the row player versus a column player in Figure 1. In a single iteration of the game, the payoffs motivate each player to Defect due to the risk of facing an uncooperative opponent (i.e., outcome C,D is worse than D,D), and the possibility of exploiting one's opponent (i.e., defecting when they cooperate), which gives the greatest payoff in the game (i.e., D,C is preferred over C,C). Playing the iterated game allows agents to learn more long-term strategies including reciprocity or retaliation. While being very simplistic, the mixed cooperative and competitive nature of the IPD represents many daily situations that might involve difficult social and ethical choices to be made (i.e., moral dilemmas). This is why it has been extensively used for studying social dilemmas in traditional RL-based agents (Bruns, 2015; Hughes et al., 2018; Anastassacos et al., 2020; McKee et al., 2020; Leibo et al., 2021) and, more recently, utilized as a training environment for moral alignment of agents in particular (Tennant et al., 2023a; 2024).\nThe behavior of LLM agents on decision-making and game-theoretic scenarios has been extensively debated in recent literature (Gandhi et al., 2023; Fan et al., 2024; Zhang et al., 2024). LLM agents have been found to act differently to humans, and in ways that are still not fully \u201crational\" in terms of forming goals from a prompt, refining beliefs, or taking optimal actions based on those goals and beliefs (Fan et al., 2024; Macmillan-Scott & Musolesi, 2024). Large-scale state-of-the-art models playing the IPD have been observed to deploy sensible yet \u201cunforgiving\u201d strategies (Akata et al., 2023), but some benchmark datasets suggest that these models lack true strategic reasoning in games including the IPD (Duan et al., 2024). New developments in in-token reasoning capabilities of state-of-the-art LLM-based platforms (OpenAI, 2024) as well as prompting strategies specifically centered around reasoning and acting (Wei et al., 2022; Shinn et al., 2023; Yao et al., 2023) are likely to improve these capabilities, though existing results suggest that the benefits of these methods are more likely to arise for very large foundation models (Bubeck et al., 2023). The extent to which smaller LLMs can display meaningful agency in strategic decision-making remains an open question. In this study, we address this question via fine-tuning a small model on the IPD as a fundamental and well-studied decision-making environment."}, {"title": "2.4 INTRINSIC REWARDS FOR MORAL ALIGNMENT", "content": "In this work, we directly specify alignment goals for agents by defining intrinsic rewards in terms of actions in a social dilemma environment. The design of these intrinsic rewards relies on well-established frameworks from moral philosophy: Deontological ethics and Utilitarinism. Deonto-logical ethics (Kant, 1785) considers an agent moral if their actions conform to certain norms. A prominent example of a norm is conditional cooperation (i.e., \"it is unethical to defect against a cooperator\"). This norm forms an essential component of direct and indirect reciprocity, a poten-tially essential mechanism for the evolution of cooperation in human and animal societies (Nowak, 2006). Utilitarian morality (Bentham, 1780), on the other hand, is a type of consequentialist rea-soning, according to which an agent is deemed moral if their actions maximize collective \"welfare\" for all agents in their society (or, in this case, collective payoff for all players in the game), and less attention is paid to whether current actions adhere to norms. Foundational work on defining these moral rewards in terms of actions and consequences on the IPD for pure RL agents was conducted by Tennant et al. (2023a) and Tennant et al. (2024). In this paper, we evaluate the extent to which this framework can be applied to align the behavior of LLM-based ones."}, {"title": "3 FINE-TUNING METHODOLOGY", "content": "3.1 AGENT AND ENVIRONMENT\nThe LLM agent and an opponent play a repeated Iterated Prisoner's Dilemma game. At each time step, the model receives a prompt containing a description of the IPD game, including a state that contains the history of the opponent's single previous move. Within the MDP framework, each player's current action affects the game's state at the next time step.\nWe evaluate fine-tuning in two settings: an LLM agent learning by playing against a fixed-strategy Tit-for-Tat (TFT) opponent (LLM vs TFT), and an LLM agent learning by playing another learning LLM agent (LLM vs LLM). We chose TFT as a specific type of fixed-strategy opponent from the literature given its characteristics, i.e., being forgiving, defensive and, at the same time, interpretable (Axelrod & Hamilton, 1981; Binmore, 2005). Thus, it may act as a good \u201cteacher\u201d for the LLM agent to \"understand\" concepts such as retaliation, reciprocity, and cooperation. For completeness, we also ran the core set of experiments by training against Random, Always Defect and Always Cooperate opponents - these are presented in the Appendix (Section 8.5). The LLM vs LLM case is a more complex scenario that may lead to non-stationarity due to two separate models being updated continuously, but which also presents great interest due to the difficulty in predicting the outcomes from multi-agent learning (Busoniu et al., 2008).\nThe aim of this study is to enable moral decision-making capabilities in LLM agents. We perform fine-tuning based on a single environment - the IPD. However, we aim to mobilize the general decision-making elements of the model in playing the game, rather than allowing it to retrieve mem-orized responses for the Prisoner's Dilemma that were present in its pre-training data. For this reason, in our prompt we use a structured, implicit representation of the IPD as a general decision-making game, without actually stating the terms \"Prisoner's Dilemma\u201d, \u201ccooperation\" or \"defec-tion\". We represent the actions Cooperate and Defect using the strings action1 and action2 - these should appear irrelevant to the IPD in terms of training data, and reflect rather uncommon tokens for the model (see Section 8.2 in the Appendix for an illustration of the prompt). Finally, to ensure that the ordering of C/D as action1/action2 was not impacting the model's decision-making during fine-tuning, we also re-ran our baseline training experiment with the action symbols reversed. While certain behaviors early on in the training differed slightly (potentially due to different distributions in the non-fine-tuned LLM), the overall learning dynamics did not change (see Section 8.4 in the Appendix for the results)."}, {"title": "3.2 MORAL FINE-TUNING PROCEDURE", "content": "We run training in $T$ episodes: each episode begins with a random state being incorporated into the IPD prompt. The LLM-based agent M then plays $N$ repetitions of the IPD game against an"}, {"title": "3.3 IMPLEMENTATION DETAILS", "content": "We use Gemma2-2b-it (Gemma Team, 2024) as our core agent model to be fine-tuned, being one of the most popular and performant small open-source models. The small model footprint allows us to run computationally feasible experiments through the use of LoRA (Hu et al., 2022) and 4-bit quantization. We use the TRL library (von Werra et al., 2020) to fine-tune the LLM with PPO. We run training for $T = 1000$ episodes for each fine-tuning variation. In our PPO implementation, we use batch sizes of $N = 3$ and $N = 5$ for LLM vs LLM and LLM vs TFT training, respectively, which strikes a nice balance between not running out of available CUDA memory, yet providing sufficient experience for stable and efficient training We use reward scaling and normalization (Engstrom et al., 2020), as well as gradient accumulation with 4 steps, and employ LoRA (Hu et al., 2022) with rank 64, so that the total number of parameters we train is only around 5% of the original model. Otherwise, we keep all PPO parameters as their default values in the TRL package, including the optimizer's learning rate and adaptive KL control (Jaques et al., 2017). In terms of reward parameters, we set $\u00a7 = 3$ and $R_{\\text{illegal}} = \u22126$. We select the tokens action1 and action2 as the only \"legal\" tokens in response to the IPD prompt ($C_{\\text{legal}} = \\text{action1}, D_{\\text{legal}} = \\text{action2}$). The action symbols are each encoded as two tokens in the model's tokenizer, so during training we restrict the maximum output length for model generations to two tokens. For detail on parameter selection, please refer to Appendix 8.1."}, {"title": "4 EVALUATING THE EFFECTIVENESS OF FINE-TUNING: MORAL CHOICES ON THE IPD", "content": "4.1 EVALUATION APPROACH\nFirst of all, we analyze the learning dynamics observed as models develop the ability to meet the moral goals set in their rewards (Section 4.2). We analyze learning against a static opponent and a learning opponent. We then assess the effectiveness of moral \u201cunlearning\" (Section 4.3). Beyond measuring success on IPD fine-tuning itself, we evaluate the generalization of the moral fine-tuning from one matrix game environment onto four other matrix games (Section 5.1): Iterated Stag Hunt, Iterated Chicken, Iterated Bach or Stravinsky and an Iterated Defective Coordination game. The payoffs and further details around these games are presented in the Appendix (Section 8.6). Finally, we evaluate the extent to which fine-tuning on the IPD alters the models' behavior on more general prompts, as well as the explicit IPD game (Section 5.2). For each experiment, we report average results across five random seeds."}, {"title": "4.2 LEARNING DYNAMICS", "content": "In general, we find that it is possible to fine-tune the LLM agents to choose actions that are consistent with certain moral and/or game rewards in the IPD. We analyze learning dynamics over the four core types of fine-tuning in Figure 2. Learning against a fixed-strategy opponent (panel a), fine-tuning on Game rewards (i.e., rewards assigned through the payoff matrix of the game), the agent learns a defective policy, which forms a Nash Equilibrium versus a TFT opponent (Axelrod & Hamilton, 1981). In the case of Deontological fine-tuning, the agent quickly learns to avoid defecting against a cooperator nearly 100% of the time, thus never violating the moral norm encoded in the respective reward function. In practice, this agent also learns to prefer cooperation in general, though this was not directly encouraged by the Deontological norm (in terms of Deontological reward, defecting against a defector is just as valid as cooperating against a cooperator - see reward definition in Table 1). On Utilitarian fine-tuning, the agent learns to achieve mutual cooperation against a TFT oppo-nent, which is expected given that this strategy offers the optimal way to obtain the highest collective reward on the IPD. Finally, in the case of fine-tuning with a multi-objective Game+Deontological reward, the agent learns a 50%-50% Cooperate-Defect strategy, but also learns to avoid defecting against a cooperator. Thus, this agent does not violate their moral norm even as they work to obtain high payoffs on the game itself. An analysis of moral reward obtained during learning is presented in the Appendix (Section 8.3)."}, {"title": "4.3 LEARNING AND UNLEARNING THE SELFISH STRATEGY ON THE IPD", "content": "In addition to the moral fine-tuning on a single type of reward, we also evaluate the extent to which fine-tuning with intrinsic moral rewards can allow for an agent to unlearn a previously developed selfish strategy on the game. As shown in Figure 3, we find that fine-tuning with purely prosocial (i.e., Deontological and Utilitarian) moral rewards on the second half of training allows the LLM agents to unlearn the selfish strategy to some extent (panel a), even in the case of two LLM agents being trained against one another (panel b). Given the shorter moral fine-tuning period on any one reward type (only 500 episodes vs 1000 in the core experiments), the training does not converge to levels of cooperation as high as in the purely prosocial fine-tuning (Figure 2). Nevertheless, as we discuss in Section 5 below, at test time the agents based on \"unlearned\" models play similarly to those fine-tuned purely on the prosocial moral rewards (see Figure 4)."}, {"title": "5 GENERALIZATION TO MORAL CHOICES IN OTHER ENVIRONMENTS", "content": "After fine-tuning the models with moral reward, we evaluate each one through 10 episodes: each episode begins with a randomly generated state and proceeds through 5 interaction steps. We average the results across the 5 runs of each fine-tuned model. In this section, we present evaluations of models which were fine-tuned versus a static opponent. The results for models trained against another LLM show similar patterns - these are reported in the Appendix (Section 8.7)."}, {"title": "5.1 GENERALIZATION TO MORAL CHOICES IN OTHER MATRIX GAMES", "content": "We are interested in analyzing the generalization of moral strategies developed during fine-tuning from the IPD to other matrix game environments. To ensure that we evaluate the model's response to the semantics of the tokens and payoffs in the prompt, rather than evaluating memorization of the particular training action tokens, we run this evaluation using a new pair of action tokens: ac-tion3=Cooperate, action4=Defect.\nIn Figure 4, we analyze the extent to which the moral strategies learned while fine-tuning on the IPD game generalize to other matrix games with a similar format but a different set of equilibria: the Iterated Stag Hunt, Iterated Chicken, Iterated Bach or Stravinsky and an Iterated Defective Coordination game. We are particularly interested in the extent to which actions taken according to the two core moral frameworks (i.e., Deontological and Utilitarian morality) can be consistently observed across the games by each agent type. For example, with regards to the Utilitarian goal (i.e., maximizing collective payoff), unconditional cooperation may not be the best strategy on the Iterated Bach or Stravinsky or the Iterated Defective Coordination game. (For a further discussion of the games, please refer to the Appendix, Section 8.6.) We additionally seek to cross-compare how the actions of agents trained on one type of moral value align to those based on other values. Therefore, we conduct evaluations in terms of moral regret, defined as the difference between the maximum possible moral reward that could have been attained on a game and the moral reward that was actually received by the agent. During this test phase, we evaluate each fine-tuned model playing the matrix games against a Random opponent - this allows us to observe the agent responding to a variety of states. To aid interpretation, we also analyze the types of action-state combinations played by each agent in each case (see Figure 5).\nIn terms of moral regret with respect to Deontological norms (Figure 4, panel a), we find that all fine-tuned models are able to reasonably translate the moral strategy learned from the IPD to other matrix games. For any one model, performance in terms of reward (Figure 4) and action choices (Figure 5) is generally similar across the five games. Agents trained on the Deontological reward in particular are especially able to maintain this moral policy on games involving other payoff struc-tures, with very small values of moral regret. An analysis of their action choices (Figure 5) shows"}, {"title": "5.2 IMPACT OF FINE-TUNING BEYOND MATRIX GAMES", "content": "Given the fine-tuning process based on rewarding particular action tokens in certain states, it is important to understand the extent to which fine-tuning on a matrix game might have made the models learn a certain \"meaning\" of the action tokens more generally. To test this, we presented the models with three unrelated prompts involving a \"call to action\" of a similar format (and using the same action tokens), as well as an explicit IPD prompt, but all without a payoff matrix being provided. Our results show that, especially when responding to prompts mentioning a \"game\" or involving a previous action of another agent (i.e., a state), the LLM agents based on fine-tuned modes are likely to choose actions in a similar pattern to that seen on the IPD and in a way that is consistent with their learned moral values. For detailed results, see Section 8.8 of the Appendix."}, {"title": "6 DISCUSSION", "content": "In this work, we present a method for fine-tuning LLM agents to adhere to a specific moral strategy in matrix games by employing RL with intrinsic rewards.\nThe two different moral payoff structures used in this study have different advantages and disad-vantages in terms of implementation in real-world systems. Our definition of the consequentialist (Utilitarian) moral agents is a function of the payoffs given by the environment to both players. Thus, its implementation in practice requires that the LLM agent will has observability of the re-wards received by both players from the environment on a given time step (or a reliable estimate). For Deontological morality, on the other hand, a norm may be easier to define in any environment without direct access to game payoffs or the opponent's current actions. The definition of the Deon-tological norm used in this study (\"do not defect against a cooperator\") only required a memory of one previous move of an opponent. This makes such a norm-base reward function easy to implement in cases in which the developer of an LLM agent only has access to their own agent's observations of the environment and not anyone else's. In future work, the intrinsic rewards approach can be applied to modeling a variety of other moral values.\nAn extension of this method to other environments would be of great interest, including fine-tuning agents with other payoff structures, more complex games or longer history lengths (for example, to aid the development of continually-learning LLM agents in practice), as well as text-based scenarios that tap into a variety of moral values. Furthermore, the method of intrinsic rewards could also be applied in a multi-objective manner to address the issue of pluralistic alignment (Sorensen et al., 2024) - in particular, a single agent could be trained with a combination of rewards representing different moral values. This may provide a promising direction for building agents that are able to satisfy the moral preferences of a wide range of individuals, which currently remains an open problem in alignment (Anwar et al., 2024; Ji et al., 2024). Finally, agents trained via intrinsic rewards as proposed in this study could also form the basis for a Constitutional AI architecture composed of artificial agents characterized by different moral frameworks (Bai et al., 2022)."}, {"title": "7 CONCLUSION", "content": "In this paper we have demonstrated that fine-tuning with intrinsic rewards is a promising general solution for aligning LLM agents to human moral values. We have evaluated the approach by quan-tifying moral rewards for agents in terms of actions and consequences on a matrix social dilemma game, and we have shown that both unlearning of undesirable behaviors and generalization to other environments are possible. We have identified promising future directions in using this methodology for advancing LLM agent alignment, and we hope that other researchers will be able to build upon the ideas presented in this work."}, {"title": "8 APPENDIX", "content": "8.1 IMPLEMENTATION DETAILS FOR REPRODUCIBILITY\nOver the course of the experiments, we tried various values for key parameters in the TRL library and in our reward definitions - these are are presented in Table 2. We chose the combination of values that resulted in the most stable fine-tuning."}, {"title": "8.2 TRAINING AND EVALUATION PROMPTS", "content": "During training, we used a prompt describing the IPD game with a history of one previous move as the state. This is presented in Figure 6. At the evaluation stage, we used matrix games other than the IPD game. We presented these in the exact same format a the IPD training prompt, except with a different payoff matrix - see Figures 7. As a final step in our evaluation, we also conducted analysis on four more general and unrelated prompts - these are presented in Figure 9.\nThroughout all prompts, we always randomized the order in which the action tokens are presented within the text (this is not reflected in the example prompts presented, where we show one example ordering only)."}, {"title": "8.3 MORAL REWARD DURING FINE-TUNING", "content": "In Figure 10, we visualize moral reward obtained by the LLM agent over the course of fine-tuning to complement the action types observed during training, which were presented in Figures 2 and 3 in the main paper. An interesting observation is the high variance in moral rewards of the Game, then Utilitarian agent - we hypothesize that this is caused by the slower convergence rate of the Utilitarian moral policy in general (c.f. the pure Utilitarian learner in Figure 2), so converting from a selfish to a Utilitarian reward function leads to instability in the model's behavior before convergence."}, {"title": "8.4 FINE-TUNING VARIATION WITH C & D SYMBOLS REVERSED", "content": "As a robustness check, we ran a core baseline experiment (fine-tuning on Game reward versus a TFT opponent) with the meaning of the action tokens reversed: here action2=Cooperate, action1=Defect. Compared to the original type of fine-tuning, we observe slightly more cooperation early on in the trailing process, but the end point is similar to the results presented in the main paper, with the LLM agent learning to Defect nearly 100% of the time (see comparison in Figure 11)."}, {"title": "8.5 ALL FINE-TUNING RESULTS VS TFT, RANDOM, AD, AC OR LLM OPPONENT", "content": "To complement the results in the paper, where we fine-tune an LLM agent versus a TFT or another LLM opponent, in Figure 12 we add the results for fine-tuning versus three additional fixed-strategy opponents: Random, Always Defect (AD), Always Cooperate (AC). We present the results for fine-tuning versus a TFT and ann LLM opponent once again for comparability."}, {"title": "8.6 FIVE MATRIX GAMES USED IN THE GENERALIZATION ANALYSIS", "content": "As discussed in the paper, when evaluating the generalization of the learned policies, in addition to the IPD, which was used in training, we relied on four other matrix games of a similar format, each of which presented a different set of strategies and theoretical equilibria. The payoff matrices for any one step of these iterated games are presented in Table 3. The associated prompts are presented in Figure 7.\nFor example, in terms of Utilitarian reward, these games differ in meaningful ways from the IPD. In the IPD, the highest collective payoff on any one step (which is equivalent to the Utilitarian moral reward in our definition) can be achieved via mutual cooperation. This is also the case on the Iterated Stag Hunt game. However, on the Iterated Chicken game greater collective payoff is obtained by unilateral defection (C,D or D,C), and on the Iterated Bach of Stravinsky game, equivalent collective rewards are received under mutual cooperation (C,C) or mutual defection (D,D). Finally, on the Iterated Defective Coordination game, the greatest collective payoff is obtained by mutual defection.\nDue to these differences, these games provide an interesting test-bed for the generalization of the moral policies learned by the LLM agents, which were fine-tuned in our experiments with Deonto-logical and Utilitarian moral rewards."}, {"title": "8.7 ANALYSIS OF GENERALIZATION FOR MODELS FINE-TUNED AGAINST ANOTHER LLM", "content": "The analyses in Figures 13 and 14 complement the results for models fine-tuned versus a TFT opponent presented in the main paper, presenting generalization analysis for models that were fine-tuned against another LLM opponent. The patterns of results are similar to those for fine-tuning against the static TFT opponent."}, {"title": "8.8 ANALYSIS OF THE IMPACT OF FINE-TUNING BEYOND MATRIX GAMES.", "content": "As discussed in Section 5.2 of the paper, we conduct a further evaluation of the behavior of fine-tuned models on four unrelated prompts without a payoff matrix. Figure 9 presents the four extra prompts used in this analysis. In this evaluation, we used the original action tokens actionl and action2. In the prompts, we varied three elements in particular: an action choice (\u201cYou must choose either action actionl or action action2", "game": "escription (\u201cYou are playing a game with another agent A", "You have played with this opponent before. Last time, they played action1. What action would you take this time?\"). Finally, we also tested responses to an explicit IPD prompt (\\\"You are playing a repeated Prisoner's Dilemma game with another agent A. You must choose either action action1 or action action2. Assume traditional payoffs from the Prisoner's Dilemma. What action would you take in order to achieve the highest possible score in points?\\\" we note that the explicit IPD prompt we use does not involve a payoff matrix). Again, we randomize the order in which the action tokens are presented in the textual part of the prompt.\"\n    },\n    {\n      \"title\"": "8.9 ANALYSIS OF GENERALIZATION ACROSS FIVE GAMES USING THE ORIGINAL ACTION TOKENS IN THE TEST-TIME PROMPT"}, {"content": "To complement the analysis in the main paper done with new action tokens at test time, we also run the evaluation using the same action tokens as in training (action1=Cooperate, action2=Defect - see Figure 8b for prompts, and Figure 17 for results), and with the meaning of these tokens swapped (action2=Cooperate, action1=Defect - see Figure 8c for prompts, and Figure 18 for results)."}]}