{"title": "Multimodal Emotion Recognition with Vision-language Prompting and Modality Dropout", "authors": ["Anbin QI", "Zhongliang Liu", "Xinyong Zhou", "Jinba Xiao", "Fengrun Zhang", "Qi Gan", "Ming Tao", "Gaozheng Zhang", "Lu Zhang"], "abstract": "In this paper, we present our solution for the Second Multimodal Emotion Recognition Challenge Track 1 (MER2024-SEMI). To enhance the accuracy and generalization performance of emotion recognition, we propose several methods for Multimodal Emotion Recognition. Firstly, we introduce EmoVCLIP, a model fine-tuned based on CLIP using vision-language prompt learning, designed for video-based emotion recognition tasks. By leveraging prompt learning on CLIP, EmoVCLIP improves the performance of pre-trained CLIP on emotional videos. Additionally, to address the issue of modality dependence in multimodal fusion, we employ modality dropout for robust information fusion. Furthermore, to aid Baichuan in better extracting emotional information, we suggest using GPT-4 as the prompt for Baichuan. Lastly, we utilize a self-training strategy to leverage unlabeled videos. In this process, we use unlabeled videos with high-confidence pseudo-labels generated by our model and incorporate them into the training set. Experimental results demonstrate that our model ranks 1st in the MER2024-SEMI track, achieving an accuracy of 90.15% on the test set.", "sections": [{"title": "1 INTRODUCTION", "content": "As an important part of human-computer interactions (HCI), Multi-modal Emotion Recognition (MER) has garnered increasing interest. Incorporating an understanding of human emotional states into HCI is critical not only elevates the quality of the interaction ex-perience but also fosters a deeper connection between users and technology. There are many technologies for studying human emo-tions, including speech, image, text, video, and other modalities.\nAlthough multimodal can boost emotion recognition, collecting annotation data with high quality is challenging. The performance of the model degrades easily when only limited data is feasible. To address the above issue, methods that leverage pre-trained models predominate in current research. In Natural Language Process-ing (NLP) tasks, RoBERTa [13] outperforms BERT [6] on several benchmarks via a larger training dataset, longer training duration, and dynamically masking strategy. XLNet [22] incorporates the mechanisms of Transformer-XL [5] and integrates bidirectional context conditioning. ELECTRA [4] replaces the widely used mask prediction task with a new pre-training task called \"Replaced To-ken Detection\". By now, Baichuan [21] has achieved outperforming results on several benchmarks owing to the utilization of extensive high-quality data and effective training methodologies, especially in the Chinese language. GPT4[1] has better language understand-ing due to more languages and a wider training set. Therefore, we attempt to combine GPT4 and Baichuan to enhance emotional representation in language.\nIn Speech Signal Processing (SSP) tasks, Wav2Vec2.0 [2] uses contrastive learning to learn robust audio representations from large-scale speech data pre-training. HuBERT [7] further improves"}, {"title": "2 RELATED WORKS", "content": "Semi-Supervised Learning with Self-training: Self-training is a simple semi-supervised method. The main idea is to augment the labeled training dataset with unlabeled data [3] [24] [20]. Self-training-based semi-supervised learning methods have demon-strated promising performance in MER2023 challenge. Both Un-labeled data with generated pseudo-labels and labeled data are fed into the model during training and models can benefit from increasing the amount of training data [3].\nModality competition in multimodal fusion: The superior-ity of multimodal over single-modal lies in its capability to integrate and exploit complementary information from multiple modalities, thereby enriching the representation for downstream tasks. How-ever, due to modality competition [9], only a subset of modalities effectively learn and contribute to the final model, while others"}, {"title": "3 METHOD", "content": null}, {"title": "3.1 Model Architecture", "content": "The overall architecture of the proposed model is illustrated in Figure. 1, which consists of multiple single-modal feature extrac-tors and a multimodal feature fusion network. In brief, the input x \u2208 {xs, x1, XT, Xxy } are fed into the corresponding single-modal fea-ture extractor to obtain high-dimensional feature f \u2208 {fs, fr, fr, fv}. The meanings represented by the subscripts are as follows: S for speech, I for image, T for text, and V for video. For video modality, we propose EmoVCLIP with a strong capacity for emotion extrac-tion. For image modality, we use CLIP [17] to extract image features by slicing the video into frames of images. For speech modality, we extract speech features by feeding the raw speech signals into Hubert [7], which is widely adapted for speech-based tasks. For text modality, we propose to use Baichuan with input text aug-mented by GPT4 to explicitly obtain textual features along with the corresponding emotion states. Then two dimensional inputs f \u2208 RL,C are pooled into corresponding single-modal embeddings e \u2208 {es, ex, et, ev} via temporal average pooling. Finally, multi-modal embeddings are concentrated in channel dimensional and fed into the fusion networks to output the prediction of emotion labels in neutral, anger, happiness, sadness, worry, and surprise. Next, we will provide a detailed introduction to our proposed methods."}, {"title": "3.2 EmoVCLIP", "content": "As is discussed in [18], although simple fine-tuning of CLIP exhibits competitive performance, it is not always feasible, particularly in downstream tasks with little available data. Research [12] has found that fine-tuning CLIP does not improve the performance on the MER2023 dataset but instead degrades the generalization perfor-mance. We attribute this performance degradation to the lack of labeled data and the inefficiency of the fine-tuning strategy. Inspired by Vision-Language prompt learning [18], we use prompt learning to effectively improve the generalization ability of the model on specific emotion datasets instead of fine-tuning CLIP."}, {"title": "3.3 Modality Dropout", "content": "In u-HuBERT [8], modality dropout is proposed to learn modality-agnostic representations by randomly dropping certain modalities during training, thereby improving the model's generalization ca-pabilities across various modalities.\nIn this paper, we propose random modality dropout to promote the integration of features from different modalities and alleviate modality competition. Specifically, in our model, each modality has its own extracted embeddings before multimodal fusion: es for speech, er for image, et for text, and ev for video. Embeddings are concentrated and then fed into fusion networks g to predict the emotion labels. This process can be dropped as Eq. 1.\npred = g(concat(es, er, et, ev)) (1)\nIn our method, each modality's embedding is replaced by zero vectors with a certain probability before concentration. The real multimodal fusion can be described as Eq. 2.\n{\\begin{cases} pred = g(concat(e_s, e_f, e_t, e_v)) \\\\ e_i = 0, i \\in {S, I, T, V}, p = p_1\\end{cases}}(2)\nwhere p\u2081 denotes the modality dropout rates."}, {"title": "3.4 GPT4-Baichuan", "content": "Although Baichuan excels in Chinese-based tasks, we found that GPT4[1] surpasses Baichuan in emotion recognition. To enhance the emotional feature extraction capabilities of Baichuan, we have integrated GPT4's emotion extraction ability with Baichuan's Chi-nese language processing ability via text augment. Considering that it is impossible to obtain the feature embeddings by GPT4, we input the text and prompt words into GPT4, let GPT4 pay attention to the emotional information in the text, and sort by the possibility of six emotion labels according to the text. Finally, the output of GPT4 is spliced with the text and input into Baichuan to obtain the text information and the corresponding emotional state more clearly."}, {"title": "3.5 Self-training", "content": "Following [3], we adopt self-training as it exhibits promising capa-bilities in semi-supervised learning by effectively leveraging unla-beled data. Initially, we train a foundational multimodal emotion recognition model using labeled data. Then, we engage in an iter-ative self-training strategy which involves expanding the labeled dataset with pseudo-labeled data, ending with the retraining of the model."}, {"title": "4 EXPERIMENTS AND RESULTS", "content": null}, {"title": "4.1 Datasets", "content": "MER2024-SEMI provides two sets, Train & Val with labels and Test without labels, containing 5030 and 115595 videos respectively. The final test set contains 1169 videos, including 115595 unlabeled videos. Since Train & val is not divided into train set and validation set, similar to the baseline system, we choose to perform five-fold cross-validation, and then weigh the best results on the five valida-tion sets equally to obtain the final result. The evaluation indicator is the weighted average F-score(WAF) defined by the challenge organizers."}, {"title": "4.2 Implementation Details", "content": "We used four feature extractors, Chinese-Hubert-large\u00b9, CLIP \u00b2, EmoVCLIP, and GPT4-Baichuan. The 24k speech signals extracted from the video are fed into the Chinese-Hubert-large and the output of the last four layers are added as the frame-level features of the speech modality. We used the open-face toolkit\u00b3 to extract the facial features of the main faces in each frame of the video and aligned them to 128*128 pixels, and finally obtained the frame-level features. We directly input each frame of the video into EmoVCLIP without using open-face to obtain video-level features. When fine-tuning EmoVCLIP, we set the number of learned prompts of each layer to 4 and the layers are the first 12 layers of CLIP. After inputting the text and prompt into GPT4, we obtained the probability ranking of the six categories of emotion labels and then concatenated the ranking and text into Baichuan to obtain frame-level features. For the frame-level features, we calculated the mean and variance and concatenated them during training to obtain sentence-level features. Finally, we combined modality dropout with the attention used by the baseline to fuse different modalities. In the self-training process, we select top k = 10 pseudo-labeled samples from each class and set the maximum number of steps Nr = 10. In each fold of 5 folds, we set the maximum number of epochs to 30 and the batch size to 64. The Adam [12] optimizer is utilized with a learning rate of 3e-4. Dropout [24] and modality dropout rate are set to 0.3."}, {"title": "4.3 Results and Analysis", "content": "Table 1 shows our experiments in different modalities. First, in the single modal, we try to reproduce the baseline system under the same conditions. Then we show the model results obtained by all the methods we proposed. The experiment shows that our method exceeds the best baseline result by 3%. At the same time, to verify the effectiveness of our proposed method, we conducted an ablation experiment and compared the performance of EmoVCIIP and CLIP single modal on the test set. It can be found that EmoVCIIP has better emotional features than CLIP, and EmoVCLIP and CLIP have certain complementary characteristics, which means that although EmoVCLIP pays more attention to the time change information of the frame sequence after fine-tuning CLIP, it will lose some of its focus on facial features due to the lack of open-face processing. We will also show the comparison between GPT4-Baichuan and Baichuan, but there is no complementarity between GPT4-Baichuan features and Baichuan. As can be seen from the table, using modality dropout is more conducive to the fusion of different modalities than not using it, and alleviates the dependence and competition effects between modalities."}, {"title": "5 CONCLUSIONS", "content": "We proposed a method to use prompt learning to fine-tune CLIP to adapt to emotion recognition in the video domain. We used GPT4 to strengthen Baichuan's attention to the order of sentiment clas-sification of text. When fusing different modalities, we proposed using modality dropout to alleviate modal dependence and modal competition and used self-training to utilize unlabeled data. Finally, our model achieved a weighted F1 score of 90.15% in the emotion recognition results of the MER2024-SEMI challenge, and we ver-ified the effectiveness of the proposed method through ablation experiments."}]}