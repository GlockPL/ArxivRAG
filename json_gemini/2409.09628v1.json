{"title": "Can Large Language Models Grasp Event Signals? Exploring Pure Zero-Shot Event-based Recognition", "authors": ["Zongyou Yu", "Qiang Qu", "Xiaoming Chen", "Chen Wang"], "abstract": "Recent advancements in event-based zero-shot object recognition have demonstrated promising results. However, these methods heavily depend on extensive training and are inherently constrained by the characteristics of CLIP. To the best of our knowledge, this research is the first study to explore the understanding capabilities of large language models (LLMs) for event-based visual content. We demonstrate that LLMs can achieve event-based object recognition without additional training or fine-tuning in conjunction with CLIP, effectively enabling pure zero-shot event-based recognition. Particularly, we evaluate the ability of GPT-40 / 4turbo and two other open-source LLMs to directly recognize event-based visual content. Extensive experiments are conducted across three benchmark datasets, systematically assessing the recognition accuracy of these models. The results show that LLMs, especially when enhanced with well-designed prompts, significantly improve event-based zero-shot recognition performance. Notably, GPT-40 outperforms the compared models and exceeds the recognition accuracy of state-of-the-art event-based zero-shot methods on N-ImageNet by five orders of magnitude. The implementation of this paper is available at https://github.com/ChrisYu-Zz/Pure-event-based-recognition-based-LLM.\nIndex Terms-large language model, event camera, object recognition.", "sections": [{"title": "I. INTRODUCTION", "content": "Event-based cameras [1], due to their high temporal resolution, high dynamic range, and low power consumption, have garnered widespread attention in fields such as computer vision [2], [3], achieving promising results in tasks like object recognition [4], [5]. Event-based object recognition methods can be classified into traditional and zero-shot approaches. As illustrated in Figure 1, traditional methods require extensive training and are limited in the categories they can recognize due to the constraints of convolutional neural networks [6].\nTo address these limitations, researchers have proposed zero-shot methods that leverage large-scale pre-trained models [7]. Among these approaches, CLIP [8], a groundbreaking model that leverages a contrastive learning strategy, aligns visual and textual representations within a shared embedding space. This alignment is particularly effective for zero-shot recognition tasks, where the model can associate images with textual descriptions of unseen categories. However, due to the domain gap between events and the associated textual labels, existing zero-shot methods are inherently constrained by the characteristics of CLIP. Additionally, existing zero-shot methods rely on an additional event encoder to process raw event data before classification, further complicating the recognition process.\nRecent research has extensively explored the advanced"}, {"title": "II. RELATED WORK", "content": "Exploration of LLMs. Recent studies have conducted in-depth explorations of LLMs' capabilities across various tasks, including zero-shot recognition [13], the 2023 BioASQ challenge [14], zero-shot classification of point cloud data [15], emotion recognition [16], and visual recognition and localization [17]. Although substantial research has explored various visual tasks, there is still a lack of quantitative analysis on the performance of LLMs in event-based pure zero-shot recognition. Furthermore, to the best of our knowledge, only a limited number of studies have evaluated the effectiveness of open-source LLMs in visual tasks.\nEvent-based zero-shot visual recognition. The web-scale image-text pre-training model CLIP [8] has bridged visual and textual domains, inspiring extensions into event-based scene understanding. EventCLIP [7] adapted CLIP for event-based recognition by converting event data into 2D representations and refining feature alignment. ECLIP [4] and EventBind [5] further improved this approach by incorporating an event encoder and a Hierarchical Triple Contrastive Alignment (HTCA) module. However, these methods heavily rely on extensive training and CLIP's limitations, requiring specialized event encoders."}, {"title": "III. DATASETS AND TASK SETUPS", "content": "We select three representative event datasets for recognition experiments. To ensure a fair comparison, these datasets undergo the same pre-processing steps before being fed into the LLMs. After obtaining the model's outputs, we apply post-processing to the results.\nDataset. We use three experimental datasets: N-ImageNet [12], N-Caltech101 [18], and N-MNIST [19]. N-ImageNet is derived from ImageNet-1K [20], where RGB images are shown on a monitor and captured by a moving event camera, resulting in 1,781,167 event streams at 480 \u00d7 640 resolution across 1,000 object classes. N-Caltech101 consists of 8,246 event streams, each 300 ms long, recorded by an event camera capturing images from Caltech101 [19] displayed on an LCD monitor, covering 101 object categories. N-MNIST includes 70,000 event streams, each representing one of the 10 handwritten digits from the MNIST [21] dataset.\nDataset processing. As shown in Figure 2, we convert the raw event stream from three experimental datasets into two formats: event frames and reconstructed frames. First, we transform the event streams into event frames. Specifically, we process the raw event data by projecting the voxel grid onto a 2D histogram, applying logarithmic scaling for normalization, and mapping the result to an 8-bit event frame to represent"}, {"title": "IV. EXPERIMENTS", "content": "Models\nWe explore four LLMs, including two of the most powerful models from OpenAI and two representative open-source models that can process visual content. All dataset representations are evaluated in the same sequence across these models.\nGPT-40, GPT-4turbo. We utilize two OpenAI models as pure zero-shot models. We provide a task description to each model, and the model returns the corresponding answer. For GPT-40, we use the latest version, chatgpt-4o-latest [22], which was released on August 15, 2024. For GPT-4-Turbo, we use theHardware and software details\nWe run LLaVA and MiniGPT-4-v2 on an RTX 3090. Since GPT models only require API calls, we set up the environment and call the GPT API on an RTX 3050.\nB. Task Setups\nWe evaluate the pure zero-shot recognition capabilities of several LLMs for event-based visual content.\nTask. As illustrated in Figure 2, our task is to present event frames or reconstructed frames generated from raw event streams to a LLM and determine whether it can accurately identify the category to which each frame belongs. The model is given its task along with multiple options (e.g., prompts), but only one option is correct. The number of options corresponds to the total number of categories in each dataset: 1,000 options for N-ImageNet, 101 options for N-Caltech101, and 10 options for N-MNIST.\nEvaluation metrics. The metric used to evaluate the model's performance is accuracy, which is the proportion of correctly classified instances out of the total number of instances pre-sented. This metric provides a straightforward assessment of the model's overall effectiveness in the classification task.\nPost-processing. Since different models may produce varying responses, we need to apply post-processing to match the correct answer, as shown in Figure 2. This includes removing all characters except for the answer, converting all letters to lowercase, and performing character matching. For GPT-40, due to its superior performance, we can add constrained prompts to ensure it returns the desired response format, eliminating the need for post-processing.\nLLaVA. We use LLaVA as one of our pure zero-shot models. Similarly, we provide the model with a task description, and it returns the corresponding answer. Specifically, we use the LLaVA-v1.5-7b version [23].\nMiniGPT-4-v2. We also use MiniGPT-4-v2 as one of our pure zero-shot models. Similarly, we provide it with a task description, and it returns the corresponding answer. We specifically use the MiniGPT-4-v2 [24] model based on LLaMA2 Chat 7B [25], with stage 3: Multi-modal instruction tuning [24], as the pretrained weights."}, {"title": "A. Results", "content": "Q1: Can LLMs understand and recognize raw event streams? Test: GPT-4o. Answer: No. When given raw event streams and asked to identify their corresponding category, GPT-40 responds by stating that these are raw event streams and that it cannot directly analyze them. These models are designed to handle natural language, code, or structured data, while event camera outputs low-level, asynchronous data (e.g., pixel position, timestamp and polarity), representing"}, {"title": "V. CONCLUSION", "content": "In this paper, we evaluate a pure zero-shot event-based recognition method using LLMs and demonstrate its superiority over existing state-of-the-art methods in zero-shot tasks. We also explore how different event representations impact LLM recognition capabilities, finding that converting event streams into reconstructed frames can improve accuracy. These findings highlight the potential of LLMs in event-based visual content understanding, underscore their promising applications in this domain, and provide a foundation for future research in event-based visual content recognition. In the future, we will continue to explore advanced methods to enhance the accuracy of pure zero-shot event-based recognition."}]}