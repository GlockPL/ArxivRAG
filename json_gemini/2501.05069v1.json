{"title": "Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning", "authors": ["Huabin Liu", "Filip Ilievski", "Cees G. M. Snoek"], "abstract": "This paper proposes the first video-grounded entailment tree reasoning method for commonsense video question answering (VQA). Despite the remarkable progress of large visual-language models (VLMs), there are growing concerns that they learn spurious correlations between videos and likely answers, reinforced by their black-box nature and remaining benchmarking biases. Our method explicitly grounds VQA tasks to video fragments in four steps: entailment tree construction, video-language entailment verification, tree reasoning, and dynamic tree expansion. A vital benefit of the method is its generalizability to current video- and image-based VLMs across reasoning types. To support fair evaluation, we devise a de-biasing procedure based on large-language models that rewrites VQA benchmark answer sets to enforce model reasoning. Systematic experiments on existing and de-biased benchmarks highlight the impact of our method components across benchmarks, VLMs, and reasoning types.", "sections": [{"title": "1. Introduction", "content": "This paper proposes a video-grounded reasoning method for commonsense video question answering (VQA). VQA has a long tradition in computer vision [11, 22], with remarkable recent progress obtained through video- and image-language models [10, 12-14, 29] (throughout this paper collectively abbreviated as VLMs). Yet, there are growing concerns that their improved performance is based on learning shortcut associations between videos and likely answers, as opposed to reasoning [23]. Such concerns are reinforced by the black-box nature of these models [12, 13], which prohibits a deeper understanding of their decision-making process.\nWe are inspired by recent work in natural language processing, where entailment trees have emerged as a mechanism to explicitly analyze answer candidates, using LLMs to recursively decompose a candidate into hypotheses and natural language inference formalisms to evaluate the hypotheses [4]. Entailment trees provide an explicit reasoning chain that explains the model's decision-making process and enables verification of each step, thus addressing concerns about shortcut learning. Recently, Sanders et al. [16] have devised a mechanism to apply entailment trees to videos. However, their work assumes video transcripts are explicitly provided to evaluate answers, thus avoiding the complexity of grounding hypotheses into video content.\nIn this paper, we propose the first video-grounded entailment tree reasoning method for commonsense VQA.\nOur method explicitly grounds VQA tasks to video fragments in four steps: (i) entailment tree construction, (ii) video-language entailment verification, (iii) tree reasoning, and (iv) dynamic tree expansion. As shown in Fig. 1, given a video and a multiple-choice question, we generate a statement for each answer candidate that acts as a first-level hypothesis. We decompose each statement iteratively, aiming to produce sub-statements that can be confidently verified in the video. The video is itself decomposed into partitions, consisting of sets of frames. Verifying each statement is then a matter of aligning it to a video partition. A vital benefit of the method is its generalizability to current video and image-based VLMs across reasoning types, including temporal and causal. To demonstrate its video reasoning ability, we develop an answer-set de-bias procedure supported by an LLM that ensures that VQA benchmarks [11, 22] are adequate for reasoning in videos without relying on spurious correlations. Our experiments show that our video-grounded entailment tree method consistently improves video- and image-based baselines on both the existing and de-biased benchmarks. Further ablations show that the method benefits from considering both textual and video information and that its performance is especially strong on causal and temporal questions."}, {"title": "2. Related work", "content": "Video question answering. Recent research has shown that while video-based VLMs can achieve state-of-the-art performance, their answers are sensitive to object size, positioning, and speed [1, 24]. Moreover, when answering temporal and spatial questions, VLMs rely on textual biases to \"guess\" answers rather than performing genuine understanding and reasoning over visual-text information [28].\nTo improve the robustness and interpretability of VLMs, one line of research enriches VLMs with visual grounding functionality during QA, which enables VLMs to localize relevant video moments [15, 23, 26] or key frames [19] to support answers. However, while these methods localize visual evidence, the process by which VLMs use it to deduce answers remains opaque. Another approach leverages external LLMs as reasoners or agents to enhance interpretability in textual modality. For instance, LLoVi [27] converts VQA to a text-based QA task via video captioning, then prompts an LLM to provide answers. Similarly, VideoAgent [18] uses an LLM to recursively determine if the current frames can answer the given question based on their textual descriptions. However, these methods heavily rely on the reasoning capabilities of LLMs. Like VLMs, the LLM reasoning process remains a black box, and hallucinations are common. Recently, TV-trees [16] attempted to perform explicit reasoning over both visual and textual modalities using a neuro-symbolic system. However, their work assumes video transcripts are explicitly provided to evaluate answers, thus avoiding the complexity of grounding hypotheses into video content. Instead, we contribute a general framework for explicit reasoning in commonsense VQA, fueled by a grounding component that aligns question components with video fragments.\nBeyond methodology, some works focus on providing fair and comprehensive VLM evaluations in VQA tasks by creating new benchmarks [3, 6, 9]. These benchmarks contain videos with diverse scenarios and durations, with carefully crafted questions and options designed to prevent textual shortcuts that VLMs might exploit. Video-specific questions (e.g., compositional action reasoning) [1], which require insights beyond textual associations, are included to test commonsense reasoning in VLMs. Addressing concerns of remaining biases in such benchmarks [6], we contribute an LLM-based answer-set de-biasing procedure to ensure that VQA benchmarks [11, 22] are adequate to evaluate reasoning in videos rather than spurious correlations.\nSystematic language reasoning. As LLMs demonstrate great potential in reasoning, there has been considerable interest in using LLMs to generate systematic explanations to support their answers. The series of Chain-of-Thought prompting [2, 20, 25] encourages LLMs to think step-by-step to perform explicit multi-hop reasoning, providing free-form reasoning steps before arriving at an answer. However, such implicit explanations are not grounded in external knowledge or evidence, which may lead to unverifiable and unfaithful reasoning. Since the development of EntailmentBank [4], research has increasingly focused on constructing explanation trees [17, 21] and graphs [8], encouraging models to generate step-wise entailment proofs of a statement using a set of supporting facts. Entailer [17] introduced this systematic explanation framework into language-based multiple-choice QA, performing explicit reasoning by generating entailment trees grounded in the model's internal beliefs. REFLEX [8] extends the entailment tree to form a belief graph for QA models, aiming to address consistency issues by intervening in the intermediate reasoning steps. Instead of grounding facts in predefined rules or model beliefs, NELLIE [21] adopts Prolog-based inference engines and external natural language corpora to build entailment trees as explainable reasoning for multiple-choice QA tasks. While such techniques for natural language processing inspire our framework, we generalize entailment trees to VQA, contributing a novel grounding method that aligns entailment trees with video fragments."}, {"title": "3. Video-grounded entailment tree reasoning", "content": "This paper devises a novel explainable framework for grounded commonsense VQA. It derives the answers through systematic reasoning over video-text information with entailment trees. Specifically, in the entailment tree (Fig. 2a), each candidate answer is decomposed into statements that entail the answer, explaining why each answer could be plausible. These statements are then grounded in relevant visual evidence from the video to prove or refute them (Fig. 2b). While entailment trees in natural language processing are constructed based on a model's internal knowledge or corpora [8, 17, 21], we ground entailment trees into video fragments. Finally, backtracking through the entailment tree leads to systematic reasoning over the statements (Fig. 3). Thus, answers can be deduced by a systematic structure with explicit reasoning paths and explanations rather than relying on opaque, black-box models."}, {"title": "3.1. Entailment tree construction", "content": "Initial statement generation. Given a question and its answer candidates, we first convert each question-answer pair into a declarative sentence that preserves the semantic meaning of the original QA pair. As a result, an N-way multiple-choice QA problem produces a set of statements, denoted as D=d1, ..., dy. For example, the two-way question \"What did the boy in white do after he first took the balloon? (A) resting on a chair (B) carries it toward the hula hoop\" is transformed into: D : {d\u2081= \u201cThe boy in white resting on a chair after first taking the balloon.\u201d, d2= \u201cThe boy in white carries it toward the hula hoop after first taking the balloon.\u201d }. Thus, selecting the best answer equals identifying the correct statement for a given video.\nRecursive statement decomposition. For each initial statement in D, we generate two sub-statements as proofs that support the statement: Statement \u4ecb Sub-statement1, Sub-statement2. The statement is True if and only if these two sub-statements are proved to be True, i.e., the sub-statements entail the statement. Proving the original statements is thus translated into proving two simpler sub-statements. This procedure is recursive: the sub-statements can be further decomposed into further sub-statements that entail them. Therefore, to construct an entailment tree, we recursively decompose these sub-statements as new statements in the next tree layer until reaching the maximum depth or meeting the stop criterion. Fig. 2(a) presents an example of entailment tree generation. For both the initial statement generation and the statement decomposition, we leverage LLM prompting (see implementation details)."}, {"title": "3.2. Video-language entailment verification", "content": "Given the entailment tree, the framework then verifies language statements based on the grounded video content as evidence. Specifically, each statement in the entailment tree must be proven or refuted by analyzing the video. A straightforward solution is to encode the whole video to collect information that can be used to verify the statement. However, the critical visual evidence that accurately verifies a statement tends to exist in a local moment instead of the whole video. Therefore, we develop a novel video grounding that guides the verification process to the moments with relevant visual evidence.\nQuestion-aware video captioning. Given a video, we convert its visual information into detailed textual information. Specifically, we input video frames into a VLM-based captioner Cap(\u00b7) to obtain a caption $c_i$=Cap($f_i$) for each frame. However, captioning frames individually can overlook essential details or introduce irrelevant information for VQA. In commonsense VQA, questions often focus on specific facts already observed in the video. For example, a typical temporal reasoning question is \u201cWhat happened before/after Event-A?\u201d where Event-A refers to a fact statement about an event in the video. The fact referenced by the question can be leveraged to guide understanding of video content. To this end, we first extract the prior fact indicated by the question and provide it to Cap(\u00b7) as prior knowledge, encouraging the generation of relevant captions. Moreover, for each current frame, captions from all previous frames are also provided to ensure Cap(\u00b7) captures the temporal context from the past. This process is formulated as:\n$C_i$ = Cap($f_i$ | F, ($c_1$,\u00b7\u00b7\u00b7, $c_{i\u22121}$)),\nwhere F indicates the fact statement.\nVideo evidence grounding. For commonsense VQA, depending on how the question reasons around the fact statement, the necessary evidence for answers can be gathered from specific video moments. For instance, in the case of temporal reasoning (e.g., before or after questions), the answer should be inferred from moments occurring either before or after the time of the relevant fact. Following this intuition, we design a two-step evidence-grounding strategy to localize the critical moments for answering.\nFirst, given the frame-wise captions, we retrieve a keyframe deemed most relevant to the fact statement, which we refer to as the anchor frame. A straightforward retrieval approach would involve comparing each $c_i$ with the fact description using specific metrics to identify the anchor frame. However, we enhance retrieval accuracy by adopting a structured semantic retrieval strategy. Specifically, the textual descriptions of each frame and fact statement are converted into structured triplets. These triplets capture the attributes and relationships of objects in each frame through structured semantics. As shown in Fig. 2(a), rather than directly comparing raw textual descriptions of frames and fact statements, we use these triplets for retrieval. Inspired by the success of using LLMs for retrieval tasks, we prompt an LLM to conduct anchor frame retrieval using the triplets of the fact statement as the query. The LLM then identifies and returns the most relevant frame ID, i.e., its timestamp.\n$t_{anchor}$ = Rtv($c_i$, F),\nwhere $t_{anchor}$ is the time stamp of the anchor frame, Rtv(\u00b7) denotes the retrieval process. Second, we determine the final moment where we should look centered on the $t_{anchor}$."}, {"title": "3.3. Dynamic entailment tree expansion", "content": "So far, we have performed statement decomposition recursively to construct an entailment tree with pre-defined depth. However, not all statements need to be verified recursively, especially those easily determined to be true or false by VLMs. Moreover, as the depth increases, some statement sentences are atomic and directly verifiable. Thus, to improve the efficiency of the reasoning process, we further adopt a strategy to expand the entailment tree dynamically. Specifically, each statement d is tied with two confidence scores provided by the Prv(\u00b7):"}, {"title": "3.4. Reasoning over the entailment tree", "content": "Finally, we perform a backtrace through the entailment tree to calculate the confidence score of each top statement. Specifically, the final score for each statement is produced by comparing its direct score $s_d$ and proof score $s_p$, i.e., s=max($s_d$, $s_p$) during backtrace (as shown in Fig. 3). The overall framework selects the answer corresponding to the statement at the top layer with top-scoring proof."}, {"title": "4. De-biasing commonsense VQA answer sets", "content": "To demonstrate the reasoning ability of video-grounded entailment trees, it is essential to evaluate using commonsense VQA benchmarks that enforce model reasoning. Recent work [15, 23] has provided evidence that shortcuts are present in VQA datasets which enables VLMs to solve these tasks based on textual associations rather than video-grounded reasoning. While VQA benchmarks increasingly focus on commonsense reasoning skills, such as temporal (e.g. after, before) or causal (how, why, what if) relationships in video content, reasoning shortcuts affect the validity of their evaluation. This is illustrated in Fig. 4 (top), where the correct answer (D) is much more relevant to the question and also aligns best with real-world expectations. Consequently, a VLM (VideoLLaVA [13] used in this example) can answer this question correctly by leveraging such associations and without analyzing the video content. Meanwhile, replacing the answer set distractors with other commonsensical answer candidates, as illustrated in Fig. 4 (bottom), makes this task challenging for VLMs. Here, a VLM switches its answer incorrectly to option C, which confirms the impact of commonsense associations and the lack of grounded reasoning by these models.\nTo this end, we devise a de-biasing procedure that mitigates reasoning shortcuts in commonsense VQA answer sets. Our de-biasing procedure transforms multiple-choice VQA benchmarks (e.g., NEXT-QA) by rewriting their answer distractors while keeping their question and ground-truth answer intact. We prompt an LLM (LLaMA-3) to implement the rewriting procedure for each original QA set. Fig. 5 shows the detailed prompt we used for LLaMA-3 on NEXT-QA dataset. This procedure ensures that (1) the answers cannot be easily derived from the QA set associations and (2) the answer remains consistent with the original QA pair. Thus, our procedure enables the scalable construction of de-biased QA sets by leveraging the commonsense associations in LLMs. The next section, focusing on experimental evaluation, analyzes the application of de-biasing to various datasets and its impact on the performance of VLMs, with and without entailment tree reasoning."}, {"title": "5. Experiments", "content": "5.1. Experimental setup\nDatasets. We test our framework on three VQA benchmarks: (1) NEXT-QA [22], a VQA benchmark for causal and temporal reasoning. (2) IntentQA [11], which focuses on video intent reasoning from both causal and temporal aspects. (3) Video-MME [6], which is a recently proposed comprehensive evaluation benchmark for video analysis; we use its \"short-term\" split (video length < 2 mins) and 4 question types (temporal, spatial, action, and object reasoning) highly related to commonsense reasoning are selected.\nEvaluation. We report model performances on our rewritten test set for each dataset and its original test set. We evaluate our framework on all datasets under the multiple-choice QA setting, using a standard accuracy metric.\nBaselines. Our baselines represent three categories:\n\u2022 Video-based VLMs: Video-based VLMs are widely used for VQA tasks, so we include Video-LLaVA [13], VideoChat2 [12], and VideoLLaMA [29]. To test the effectiveness of our framework, we integrate these VLMs by replacing our Prv(\u00b7) with specific VLM models.\n\u2022 Image-based VLMs: We include BLIP-2 [10] and LLaVA-1.5 [14] as Image-LLM baselines.\n\u2022 State-of-the-art VQA approaches: Recent works in VQA, such as VideoTree [19], VideoAgent [18], and LLoVi [27], are included as strong baselines.\nImplementation details. We use LLaMA-3-8B [5] as the default LLM to handle basic functionalities, including (1) converting original QA into declarative statements, (2) statement decomposition, (3) structured semantic extraction and retrieval, and (4) guiding evidence grounding. Detailed prompts for each functionality are provided in the supplementary material. For frame-wise captioning across all datasets, we use LLaVA-1.5 [14] as our default captioner. When comparing with state-of-the-art VQA methods (e.g., VideoAgent), we follow their setup by replacing the captioner with the stronger CogAgent [7] model for fair comparison. When integrating our framework with VLMs, the VLM itself serves as the Prover, Prv(\u00b7). For video-LLMs, frames are uniformly sampled from the grounded video moment to meet their input requirements (VideoChat2:16 frames; VideoLLaVA & VideoLLaMA:8 frames). For image-language models like LLaVA, we sample 8 frames from the grounded moment and process them individually; the final confidence score is obtained by averaging scores across frames. During dynamic tree generation, we also set a max depth of 5 for the overall entailment tree to improve the efficiency. All code will be released upon acceptance."}, {"title": "5.2. Main results", "content": "Benefit for image and video-based VLMs. Tab. 1 summarizes the results of our method compared to baselines on the three original datasets. Integrating entailment tree reasoning brings consistent improvement across the video- and image-based VLMs for all datasets (1-4% on average). This finding includes the recently proposed benchmark VideoMME, which poses much more challenging videos and questions. The benefits are particularly regular for temporal reasoning (improvement in 14 out of 15 cases), which illustrates how our explicit reasoning process enhances temporal commonsense QA. Image-based VLMs, which initially lack temporal modeling capabilities, perform poorly when directly applied to video QA tasks. However, our framework provides a significant performance boost for these models up to 8% for LLAVA-1.5. By reasoning over multiple sub-problems rather than tackling the entire complex question simultaneously, our reasoning method makes the task more manageable for both video- and image-based VLMs.\nResults on de-biased QA sets. As shown in Tab. 2, all models experience considerable performance drops on the de-biased set of the same VQA dataset, which aligns with our observation that current VLMs often rely on textual bias in commonsense reasoning tasks. Notably, video-based VLMs show an 8%-10% decrease in the de-biased set even though the question and the correct answer remain unchanged. In contrast, our proposed framework, which derives answers through an explicit reasoning process based on specific visual evidence, demonstrates much greater robustness on the de-biased set. The improvement brought by our framework on the de-biased set is even higher than on the original test sets. In turn, our framework compensates for the performance loss of the VLMs on the de-biased set. This analysis underscores our framework's potential to mitigate textual bias in commonsense reasoning. Furthermore, the performance differences between the original and de-biased QA sets highlight VQA benchmarks' limitations in evaluating VLMs' true reasoning abilities.\nComparison with state-of-the-art. Tab. 3 compares our framework's results on the de-biased sets to state-of-the-art VQA approaches. The table shows that, next to the consistent benefit our framework provides to various VLMs, it is also competitive with state-of-the-art VQA methods. When applying an advanced captioner and reasoner that aligns with VideoAgent and VideoTree, our framework yields new state-of-the-art results in some cases. In particular, our framework performs best on temporal reasoning questions for all three benchmarks and outperforms all methods on the IntentQA dataset."}, {"title": "5.3. Ablation studies", "content": "In this section, ablation experiments are conducted using VideoLLaVA's baseline with our framework. Results are reported on the test set of the NExT-QA dataset. We provide more ablations in the supplemental material.\nImpact of LLMs for statement decomposition. Entailment generation in our framework relies on prompting an external LLM to recursively decompose statements (cf. Sec. 3.1), which is crucial in guiding reasoning paths. Consequently, we tested various LLMs for entailment tree generation, including open-source models (LLama-3 and Mistral) of different sizes and proprietary LLMS (GPT-4 and Gemini-1.5). The results are summarized in Tab. 4. As expected, the proprietary model GPT-4, known for its strong step-by-step reasoning capabilities, delivers the best performance across all settings. Scaling up LLaMA-3 to 70B offers improvements over the 8B model, though with a notable increase in inference time. As the overall performance difference between models is within 1%, we select LLaMA-3-8B as the default for integrating our framework into VLMs due to its free availability and efficiency.\nAblation on grounding components. Next, we test the effectiveness of each component in our grounding module (Sec. 3.2). The results, summarized in Tab. 5, indicate that both fact-conditional captioning and structure-guided retrieval enhance overall performance by improving grounding accuracy. However, using only structure-guided retrieval results in a slight performance drop, possibly because Cap() introduces irrelevant semantic information that doesn't align with the question's focus, and the structured representation can make identifying anchor frames more challenging. In contrast, fact-conditional captioning alone yields substantial improvement, demonstrating that this straightforward approach can yield an effective and more controllable textual description for videos by conditioning on prior knowledge or relevant facts.\nImpact of length of video frames. We further ablate the impact of input video frame length in our framework to determine the optimal number of frame-wise captions to generate per video. The results, summarized in Tab. 6, show that ideal performance is achieved only when sampling a sufficient number of frames (at least 16 for NExT-QA). When fewer frames are used (e.g., 4 or 8), key anchor frames may be missed, reducing the accuracy of grounded visual evidence. Additionally, while increasing the frame count to 32 yields the best performance, it also increases the calls required for Cap(.) to generate frame-wise captions. Balancing efficiency with performance gains, we set 24 frames as the default in our implementation.\nEffectiveness of evidence grounding. Our method grounds relevant video fragments to support statements in the entailment tree (Sec. 3.2). To validate its effectiveness, we compare it with two other variations as sources of visual evidence: (1) a version without evidence grounding, using the full video as evidence, and (2) upper-bound results: manually annotated temporal boundaries provided in the NEXT-GQA dataset, indicating where the QA models should focus when producing correct answers. The results are shown in Tab. 7. Compared to the baseline, our video-grounded method provides consistent improvements across the original and de-biased sets. The improvement is more apparent in the de-biased set, where the answer options are more semantically similar and require more precise, discriminative visual evidence. Using the ground-truth fragment can further boost our approach, suggesting that enhancing grounding accuracy could further improve our framework.\nEffectiveness of dynamic tree expansion. The depth of the entailment tree determines the granularity of reasoning (Sec. 3.3). This ablation analyzes how tree depth impacts overall reasoning performance and compares a fixed-depth approach with our dynamic tree generation strategy. Increasing the depth of reasoning yields significant improvements, as complex, long statements are broken down into concise sub-statements that VLMs can understand more effectively. However, extending reasoning beyond the 4th layer offers diminishing returns; for NExT-QA, the original statements' complexity constrains the task, and some 5th-layer sub-statements become overly simplistic and less effective for reasoning. This finding highlights the necessity of our dynamic strategy. By applying the dynamic tree generation strategy, we can see that the performance outperforms the fixed-depth paradigm. In the meantime, the dynamic strategy increases the reasoning efficiency over the entailment tree, more details about efficiency comparison can be found in our supplementary material."}, {"title": "6. Conclusion", "content": "This paper provided the first video-grounded entailment tree framework for commonsense VQA. We also contributed a de-biasing procedure to avoid spurious correlations in the evaluation and applied it to enhance representative benchmarks. Experiments with five video- and image-based VLMs show consistent benefits of our method on these benchmarks, especially for temporal reasoning. While de-biasing hurts VLM performance, our framework regains the accuracy losses and is competitive with state-of-the-art VQA baselines. Ablations confirm the added value of our fact-conditioned and structure-based grounding and the utility of dynamic tree expansion."}]}