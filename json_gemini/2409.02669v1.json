{"title": "Causality-Aware Transformer Networks for Robotic Navigation", "authors": ["Ruoyu Wang", "Yao Liu", "Yuanjiang Cao", "Lina Yao"], "abstract": "Recent advances in machine learning algorithms have garnered growing interest in developing versatile Embodied AI systems. However, current research in this domain reveals opportunities for improvement. First, the direct adoption of RNNs and Transformers often overlooks the specific differences between Embodied AI and traditional sequential data modelling, potentially limiting its performance in Embodied AI tasks. Second, the reliance on task-specific configurations, such as pre-trained modules and dataset-specific logic, compromises the generalizability of these methods. We address these constraints by initially exploring the unique differences between Embodied AI tasks and other sequential data tasks through the lens of Causality, presenting a causal framework to elucidate the inadequacies of conventional sequential methods for Embodied AI. By leveraging this causal perspective, we propose Causality-Aware Transformer (CAT) Networks for Navigation, featuring a Causal Understanding Module to enhance the models's Environmental Understanding capability. Meanwhile, our method is devoid of task-specific inductive biases and can be trained in an End-to-End manner, which enhances the method's generalizability across various contexts. Empirical evaluations demonstrate that our methodology consistently surpasses benchmark performances across a spectrum of settings, tasks and simulation environments. Extensive ablation studies reveal that the performance gains can be attributed to the Causal Understanding Module, which demonstrates effectiveness and efficiency in both Reinforcement Learning and Supervised Learning settings.", "sections": [{"title": "1 Introduction", "content": "Navigation is a fundamental task in the research of Embodied AI, and the methods for Navigation can be broadly grouped into Supervised Learning methods and Reinforcement Learning methods, distinguished by the nature of their training processes [11,32,8]. Supervision in Navigation generally refers to a demonstration of a possible solution to a problem, and the methods are typically focused on matching the behaviour of their demonstrator. In Reinforcement Learning (RL) settings, an agent learns the policy by interactions with an environment."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Motivation", "content": "Across diverse settings and paradigms within Embodied AI tasks, many necessitate the modelling of sequential data accumulated from preceding time steps. Predominantly, research endeavours utilize Recurrent Neural Networks (RNNs) or Transformers for this purpose, given their established efficacy in handling sequential data. Nevertheless, these methods were primarily proposed for other scenarios such as NLP, our question is, are these methods well-suited for the Navigation tasks?\nWe argue that directly adopting methods such as RNNs or Transformers to navigation tasks may result in specific limitations. This is due to the intrinsic differences between navigation tasks and conventional sequential data modelling tasks such as NLP, particularly when viewed through the framework of Causality.\nFor instance, if we consider the following sentence in NLP:\nThe doctor asked the nurse a question, \u201c...?\", She said, ...\nIt is obvious that the token She refers to the nurse, which was mentioned several time steps earlier. This indicates that direct causal relationships in such scenarios can be long-term or short-term, and do not adhere to a universal pattern.\nIn Navigation tasks, however, based on our empirical understanding of the physical world, the direct causal relationship can only be one-step, because it is generally believed that the transition from one state to another is typically attributed to the actions undertaken in the interim. For example, the current standing position of the agent and the action performed in that position serves as the direct determinants of its subsequent position, as demonstrated in Figure 1.\nThis fundamental difference underscores why methods such as RNNS or Transformers may not be ideally suited for Embodied AI tasks, because they are inherently designed to capture long-term associations across time steps. While this feature is advantageous for tasks that exhibit long-term causal connections, it proves counterproductive for navigation tasks, where such associations may not be desirable. Therefore, modifications to the architecture are necessary to address these disparities.\""}, {"title": "2.2 Motivation Formulation from a Causal Perspective", "content": "We formalize the motivation introduced above into a causal framework. First, based on the nature of the Navigation tasks, we make the following assumptions, which closely align with our empirical observations in the real world, as elaborated in Section 2.1.\nAssumption 1 At any given time step t, the state \\(S_t\\) and the action \\(a_t\\) are the only direct causal parent influencing the subsequent state at time step t + 1, denoted as \\(S_{t+1}\\).\nAssumption 2 At any given time step t, the state \\(S_t\\) and the Objective are the only causal parents of the action \\(a_t\\).\nDifferences with Markov Property While the intuition of these assumptions may appear similar to the Markov Property [24], they are distinct concepts from three perspectives: 1) Our assumptions pertain to causality, whereas the Markov property concerns transitional probability. 2) The Markov property underpins problem formulations such as Markov Decision Processes (MDPs), rather than serving as a specific modeling technique. However, our focus is on the practical aspects, and we discuss the appropriate architecture for modelling the scenario. 3) Our method is broadly applicable in both Reinforcement Learning and Supervised Learning contexts, thus not restricted to MDPs.\nWe depict these causal assumptions in a causal graph, as illustrated in the enlarged Causal Understanding Module in Figure 2, where the edge \\(S_{t-1} \rightarrow S_t \\leftarrow a_{t-1}\\) corresponds to the Assumption 1, and the edge \\(S_t \rightarrow a_t\\) corresponds to the Assumption 2. Further, we derive Proposition 1 based on these assumptions, which suggests that there exist NO direct causal relationships between \\(S_{t-1}\\) and \\(S_{t+1}\\), SO \\(S_{t-1} \rightarrow S_{t+1}\\) is marked by a light dotted line, indicating the causation between them is indirect and weak.\nProposition 1. At any given time step t, and for any integer \\(d \\geq 2\\), there exist no direct causal relationships between \\(S_t\\) and \\(S_{t-d}\\), the causal relationships between states \\(S_t\\) and \\(S_{t-d}\\) are indirect and must be mediated by states \\(S_{t'}\\) for all \\(t'\\) where \\(t - d \\leq t' < t\\).\nAs discussed in Section 2.1, we identify Proposition 1 as a crucial distinction between Navigation tasks and other sequential data tasks such as NLP. Therefore, we aim to make the direct causal relationship \\(S_{t-1} \rightarrow a_{t-1}\\), \\(S_{t-1} \rightarrow S_t\\) and \\(a_{t-1} \rightarrow S_t\\) stand out from the associations in other forms. While \\(S_{t-1} \rightarrow a_{t-1}\\) is inherently highlighted by the data collection process in the task, \\(S_{t-1} \rightarrow S_t\\) and \\(a_{t-1} \rightarrow S_t\\) need to be addressed in the historical data modelling step. Building on these concepts, we propose our method to make these causal associations stand out from the associations in other forms."}, {"title": "3 Causality-Aware Transformer Networks", "content": "The architecture of our proposed methodology is depicted in Figure 2 and encompasses multiple components: Visual Encoder, Goal/Action Embedding, Feature Post-Processing, Multi-modal Transformer, and the Causal Understanding Module. The functions and operations of these individual modules are delineated in the subsequent sections."}, {"title": "4 Experiments", "content": "In Section 4.1-4.2, we evaluate the performance of our method in the Reinforcement Learning setting. In Section 4.3, we conduct ablation studies and found the Causal Understanding Module contributes most of the performance gain. In Section 4.4, we demonstrate that the Causal Understanding Module is also generally applicable and effective in the Supervised Learning setting."}, {"title": "4.1 Experiment Setting", "content": "Task Descriptions We evaluate our method over three tasks in the Reinforcement Learning setting: (1) Object Navigation in RoboTHOR [6], which requires an agent to navigate through its environment and find an object of a given category. For example, \"Find an apple\". The task consists of 12 possible goal object categories, and the agent is allowed to MoveAhead, RotateRight, RotateLeft, LookUp, and LookDown. The agent is considered to have completed the task if it takes a special Stop action and one goal object category is visible within 1 meter of the agent. (2) Object Navigation in Habitat, which is defined similarly to the task in RoboTHOR, but Habitat has 21 objects and does not require the agent to be looking at a target object to succeed. Habitat uses scenes from the MatterPort3D [5] dataset of real-world indoor spaces. (3) Point Navigation in Habitat, which requires an agent to navigate from a random initial position to polar goal coordinates. For example, \"Navigate to (X, Y)\". The agent is allowed to do three actions, which include MoveAhead, RotateRight, and RotateLeft. The agent should perform a special Done action when it reaches its goal coordinates. We train within the Gibson Database [40].\nEvaluation Metrics Various metrics are employed to assess an agent's performance across different tasks. Following the previous works, we evaluate the performance of an agent on Object Navigation in RoboTHOR by Success Rate (SR) and Success weighted by Path Length (SPL). Success Rate measures the frequency with which an agent successfully completes a task, while Success weighted by Path Length (SPL) takes into account the length of the path traversed by the agent to accomplish the task. On the other hand, for the two tasks in Habitat, apart from SR and SPL, we also evaluate the agent's performance on Goal Distance (GD), which quantifies the agent's proximity to the goal upon task completion. In general, an agent is considered to be better if it achieves a higher SR and SPL, or lower GD.\nBaselines (1) We compare our method with EmbCLIP, because both methods do not contain any task-specific designs and can be trained in an End-to-End manner, thus lying in the same category and can be compared directly. For both methods, we train the model for 20M steps. And during the training process, we select the checkpoint with the highest SR for evaluation and comparison.\n(2) We compare our method with baseline methods specifically designed for each task. However, as discussed earlier, it is worth noting that these methods are not directly comparable with us because they are not in the same setting as our method. For example, these methods employ either task-specific hand-crafted logic, undergo training across multiple stages, or leverage extensive offline datasets to facilitate the training process. But for a comprehensive understanding of the performance of our method, we report the validation results of these methods as a reference. Specifically, we compare our method with Action Boost, RGB+D, ICT-ISIA, and ProcTHOR [7] on RoboTHOR ObjNav; and compare"}, {"title": "4.2 Results", "content": "We provide the results of our experiments for each task in Table 1 and Table 2. As introduced in Section 4.1, we compare our method with two types of baselines, thus our findings similarly fall into two aspects:\n(1) Compared with EmbCLIP, our method outperforms the baseline significantly on all the tasks across all evaluation metrics, as illustrated in Table 1-2. This shows the effectiveness of our method. In particular, our method achieves"}, {"title": "4.3 Ablation Studies", "content": "Our proposed method differs from EmbCLIP mainly from three perspectives: (1) We propose a Causal Understanding Module to reinforce the model's capability for environmental understanding; (2) We implement a Multi-modal transformer for feature encoding; (3) We fine-tune the CLIP visual encoder in the training process instead of holding it fixed. Therefore, we conduct ablation studies to examine the impact of these components. We conduct the studies on all three tasks and present the result in Table 3.\nImpact of Causal Understanding Module We first examine the impact of the Causal Understanding Module by implementing this module on EmbCLIP [15]. We keep all the configurations in EmbCLIP unchanged and compare the cases with/without the causal understanding module. We refer to this model as Causal-RNN in Table 3 for simplicity.\nBesides, we also plot a curve of Success Rate in Figure 4 for direct comparison of the training progress, where the x-axis denotes the steps of training, and the y-axis denotes the Success Rate. We ran each case 10 times randomly and plotted the average Success Rate at each step. Due to the space limitation, we only plotted the curve for the first 10M steps.\nWe observe two clear benefits of implementing the Causal Understanding Module: (1) It significantly improves the performance of the baseline model without computational overhead. Specifically, we improve the success rate by more than 50% by adding one simple linear layer; (2) It significantly reduces the time required for training a satisfactory agent. Specifically, we trained the Causal-RNN model for 20M steps and achieved a success rate of 0.48. In contrast, as claimed in [15], EmbCLIP achieved a success rate of 0.47 after training for 200M steps. Therefore, we significantly reduce the training time 10 times by adding one simple Linear layer.\nImpact of Multi-modal Transformer To examine the effectiveness of the Multi-modal Transformer Encoder, we simplify the architecture by removing the Causal Understanding Module from Figure 2, and holding CLIP fixed across the training process while keeping all other settings unchanged. In other words, we replace the encoder in EmbCLIP with our Multi-modal Transformer. We"}, {"title": "4.4 Causal Understanding Module in Supervised Learning", "content": "While our primary focus in the earlier sections is on experiments within the Reinforcement Learning setting, we also assess the effectiveness of our proposed method in Supervised Learning. Specifically, we implemented the Causal Understanding Module on existing methods including Seq2Seq [1], Speaker Follower [12] and EnvDrop [35], while maintaining all other settings as proposed in the respective papers. The experiments are conducted on the R2R dataset [1], and we evaluate the performance of the methods by the commonly used metrics for the task including Navigation Error (NE), Oracle Success Rate (OSR), and SR introduced in earlier sections. The results of the validation unseen split are presented in Table 4. Overall, the results have demonstrated the effectiveness, consistency and generalizability of the proposed Causal Understanding Module."}, {"title": "4.5 Computational Cost", "content": "Our proposed architecture requires more computational resources than the baseline method. However, our experiments reveal that our method is not computationally expensive. We trained the model on our machine with a single NVIDIA TITAN X GPU, and it occupies only 5GB of Memory to train the model. It takes around 40 GPU hours to train the model for 20M steps, which is nearly identical to the baseline (EmbCLIP). Besides, our Causal-RNN model in Section 4.3 is proven to be effective without any extra resources."}, {"title": "5 Related Work", "content": "Many simulators and tasks have been proposed in recent years for Embodied AI. Simulators such as [6,9,17,28,30,34,40] enables the interaction between agents and their environments, and tasks such as Navigation [3,6,9], Rearrangement [2,37], Interpreting Instructions [31] are proposed to evaluate the performance of the agent. While fast progress has been made on some of the tasks, most of these tasks remain challenging.\nWhile various methods have been proposed in recent years, most methods are specifically designed for certain tasks aiming for better results for the challenges [22,20,10,14,42,44,21,13,4,43,33,7,23]. However, although these methods achieve state-of-the-art performances, they cannot be generalized to other scenarios due to the dataset-specific architecture, inductive bias and hand-crafted logic [8,15]. Recently, [15] addressed these issues. They found that CLIP [25] makes an effective visual encoder and proposed a novel method that does not require any task-specific architectures and inductive bias."}, {"title": "6 Conclusion", "content": "In conclusion, this paper addresses key challenges in the domain of Navigation tasks in Embodied AI, particularly the limitations associated with prevalent vanilla sequential data modelling methods and task-specific designs, which often hinder generalizability and performance. By elucidating the intrinsic disparities between Navigation tasks and conventional sequential data modelling tasks, we introduce a novel causal framework to explain the necessity of the causal environment understanding module and proposed Causality-Aware Transformer (CAT), an End-to-End transformer-based method that exhibits notable performance improvements across diverse tasks and simulators, surpassing baseline approaches on multiple evaluation metrics. Furthermore, comprehensive ablation studies reveal that most of the performance gain of our method can be attributed to the Causal Understanding Module, which is proven to be effective and can be implemented in other methods across the paradigm of Reinforcement Learning and Supervised Learning without computational overhead."}]}