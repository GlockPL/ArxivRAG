{"title": "Exploring the design space of deep-learning-based weather forecasting systems", "authors": ["Shoaib Ahmed Siddiqui", "Jean Kossaifi", "Boris Bonev", "Christopher Choy", "Jan Kautz", "David Krueger", "Kamyar Azizzadenesheli"], "abstract": "Despite tremendous progress in developing deep-learning-based weather forecasting systems, their design space, including the impact of different design choices, is yet to be well understood. This paper aims to fill this knowledge gap by systematically analyzing these choices including architecture, problem formulation, pretraining scheme, use of image-based pretrained models, loss functions, noise injection, multi-step inputs, additional static masks, multi-step finetuning (including larger stride models), as well as training on a larger dataset. We study fixed-grid architectures such as UNet, fully convolutional architectures, and transformer-based models, along with grid-invariant architectures, including graph-based and operator-based models. Our results show that fixed-grid architectures outperform grid-invariant architectures, indicating a need for further architectural developments in grid-invariant models such as neural operators. We therefore propose a hybrid system that combines the strong performance of fixed-grid models with the flexibility of grid-invariant architectures. We further show that multi-step fine-tuning is essential for most deep-learning models to work well in practice, which has been a common practice in the past. Pretraining objectives degrade performance in comparison to supervised training, while image-based pretrained models provide useful inductive biases in some cases in comparison to training the model from scratch. Interestingly, we see a strong positive effect of using a larger dataset when training a smaller model as compared to training on a smaller dataset for longer. Larger models, on the other hand, primarily benefit from just an increase in the computational budget. We believe that these results will aid in the design of better weather forecasting systems in the future.", "sections": [{"title": "1 Introduction", "content": "Deep learning for weather forecasting has been a very active area of research in the recent past, with many newer deep-learning-based models approaching the performance of numerical weather prediction systems while being orders of magnitude faster and energy efficient (Pathak et al., 2022; Bi et al., 2022; Lam et al., 2022; Price et al., 2023). Despite these advances, the justification for the design decisions, including model architectures, is rarely described (Pathak et al., 2022; Bi et al., 2022; Lam et al., 2022; Price et al., 2023). While it is likely that most of these decisions are based on"}, {"title": "2 Related Work", "content": "Weyn et al. (2020) established some of the initial results on deep-learning-based weather forecasting. Their model was trained on 2\u00b0 \u00d7 2\u00b0, and consumed two previous input states to produce a forecast for the next two states, each with a 6h stride. Their architecture was based on UNet, borrowing motivation from Larraondo et al. (2019) who compared three different encoder-decoder segmentation models to establish the utility of the UNet model for predicting precipitation fields given only geopotential height field on small input size of 80 \u00d7 120 (focusing only on Europe). Rasp & Thuerey (2021) used a ResNet model on WeatherBench dataset (Rasp et al., 2020), achieving state-of-the-art performance for a data-driven method at a small resolution of 5.625\u00b0 and a forecasting horizon of 5 days. These works showed that deep-learning-based models fall short of the performance of the operational IFS system.\nFourCastNet (Pathak et al., 2022) was one of the first deep-learning-based weather forecasting systems to match the performance of the operational IFS system on spatially high-resolution inputs (0.25\u00b0 \u00d7 0.25\u00b0) while being orders of magnitude faster at inference time. FourCastNet was based on (Adaptive) Fourier Neural Operator (Li et al., 2020a; Guibas et al., 2021) which is particularly designed to learn solution operators of differential equations, owing to its ability to be grid invariant. The model was trained for 6h prediction and fine-tuned for multi-step auto-regressive prediction. They trained the model using latitude-weighted MSE and summed up the losses from the two-step rollout during fine-tuning. They explored using additive Gaussian noise to the input state to generate an ensemble prediction (Graubner et al., 2022).\nPangu-weather (Bi et al., 2022) was based on the Swin transformer architecture (Liu et al., 2021), replacing the general position embedding with an earth-specific positional embedding. They trained 4 models with different prediction horizons including 6h, 12h, 18h, and 24h. The final prediction was based on a greedy combination of these fixed-stride models. The model was trained for 100 epochs on hourly sampled data. An ensemble of model predictions was constructed by adding Perlin (Perlin, 2002) noise to the input states.\nSpherical Fourier Neural Operator (SFNO) (Bonev et al., 2023) proposed an extension of the Fourier neural operator based on spherical harmonics, taking into account the geometrical properties of Earth, which ultimately resulted in more stable prediction over longer horizons. Such a model is discretization agnostic, meaning it can be trained on various discretization of input functions, and the output functions can be queried at any discretization. The model was again pretrained for 6h prediction, and subsequently fine-tuned for multi-step auto-regressive predictions.\nFuXi (Chen et al., 2023) focused on a 15-day forecasting task, and proposed a cascaded ML weather forecasting system that produced 15-day global forecasting on par with the ECMWF ensemble mean at a temporal resolution of 6h and spatial resolution of 0.25\u00b0 \u00d7 0.25\u00b0. FuXi ensemble was created by perturbing both the initial states using Perlin noise (Perlin, 2002) as well as the model parameters using dropout (Srivastava et al., 2014). FuXi used the two previous time-steps to forecast the next step. Due to the 6h forecasting horizon, it required 60 auto-regressive steps for the model to generate a 15-day forecast. Focusing on previous work which argued that a single model is sub-optimal for both short-term and long-term horizons, they proposed a cascade model architecture"}, {"title": "3 Methods", "content": "Our evaluation considers a range of different architectures. We use custom UNet implementation (Ronneberger et al., 2015), UNO (Ashiqur Rahman et al., 2022) implementation from Neural Operator package\u00b9, official SFNO implementation from the Makani codebase\u00b2, FourCastNet (Pathak et al., 2022), and GraphCast (Lam et al., 2022) from Modulus package\u00b3, official pseudo-code translated to PyTorch (Paszke et al., 2019) for Pangu-weather (Bi et al., 2022), official code for Point Transformer v3 (Wu et al., 2023), Octformer (Wang, 2023), ISNet (Qin et al., 2022), and Efficient ViT L2 (Cai et al., 2022), and MMSeg (Contributors, 2020) implementation of SETR (Zheng et al., 2021), ResNet-50 with fully-convolutional decoder (Long et al., 2015), and Segformer (B5) (Xie et al., 2021). Note that some architectures operate on patches, hence producing a lower resolution output (one prediction per patch rather than per pixel). We add a very lightweight upsampling network in these cases to upsample the outputs to match the input resolution.\nWe emphasize that prior work based on neural operators learn the operator map between function spaces and are capable of training and testing on various resolutions, a capability unique to these models. In this work, we mainly focus on a fixed resolution and do not explore the operator learning aspect of weather forecast."}, {"title": "3.2 Dataset", "content": "We use 0.25\u00b0 \u00d7 0.25\u00b0resolution ERA-5 reanalysis dataset (Hersbach et al., 2020) from 1980-2018 sub-sampled with a 6-hour stride following prior work with snapshots at 0000, 0600, 1200, and 1800 UTC (Pathak et al., 2022; Bi et al., 2022; Lam et al., 2022). We use data from 1980-2016 for model training, 2017 for validation, and 2018 for out-of-sample test set. All evaluations presented in the paper are based on the 2018 out-of-sample test set. We used 73 input variables including 8 surface variables, and 5 variables (q, t, u, v, z) sampled at 13 pressure levels, making the final input size for each timestamp to be: 721 \u00d7 1440 \u00d7 73. We standardize each input channel based on the per-channel mean and standard deviation computed on the entire dataset."}, {"title": "3.3 Additional channels", "content": "Zenith angle is appended as an extra channel to the input when enabled. Therefore, during auto-regressive rollouts, we append a new zenith angle for each timestamp. The input location is similarly appended as a separate input channel. We evaluate the use of three constant masks, including land mask, soil type, and topography following Bi et al. (2022). For multi-step inputs, multiple steps are concatenated in the channel dimension. Therefore, the number of input channels is multiplied"}, {"title": "3.4 Multi-step fine-tuning", "content": "We backpropagate through multiple auto-regressive steps when fine-tuning. By default, we supervise only the last step in the auto-regressive rollout. We also evaluate intermediate-step supervision, where for each intermediate step, we calculate a (discounted) loss with a discount factor \u03b3 of 0.9 starting from 1.0 for the first step i.e., the weight for step i is computed as $w_i = \\gamma^{i-1}$ for i > 1. Furthermore, when using intermediate step supervision, we also experiment with the use of scheduled sampling (Bengio et al., 2015) i.e., the input steps are a convex combination of the previous model prediction as well as the actual target in the dataset with a linear schedule starting at 0.9 for the target, and annealing it to 0 for the last epoch i.e., the weight for the prediction at different epochs is 0.1, 0.2, 0.3, ..., 1.0 for the 10 epochs."}, {"title": "3.5 Optimization", "content": "All models were trained via synchronous updates for 10 epochs on 128 H100 GPUs using a batch size of 1 per GPU (effective batch size of 128). We used AdamW for optimization with an initial learning rate of 0.001, cosine learning rate decay, and weight decay of 0.0001. Note that we did not tune the hyperparameters for each of the different architectures separately."}, {"title": "3.6 Loss functions", "content": "We evaluated the use of different loss functions for training including $L_2$ i.e., Mean-Squared Error (MSE), $L_1$, along with their linear composition, and their combinations such as Huber loss (Huber, 1992). For the ground truth next time-step weather state y and the model forecast \u0177, $L_2$ and $L_1$ losses are defined as:\n\\begin{align}\n\\mathcal{L}_2(\\hat{y}, y) &= \\frac{1}{HWC} \\sum_{h=1}^H \\sum_{w=1}^W \\sum_{c=1}^C (\\hat{y}_{h,w,c} - y_{h,w,c})^2  \\tag{1}\\\\\n\\mathcal{L}_1(\\hat{y}, y) &= \\frac{1}{HWC} \\sum_{h=1}^H \\sum_{w=1}^W \\sum_{c=1}^C |\\hat{y}_{h,w,c} - y_{h,w,c}| \\tag{2}\n\\end{align}\nHuber loss is defined as:\n\\begin{align}\n\\mathcal{L}_H(\\hat{y}, y) = \\frac{1}{HWC} \\sum_{h=1}^H \\sum_{w=1}^W \\sum_{c=1}^C\n\\begin{cases}\n(\\frac{(\\hat{y}_{h,w,c} - y_{h,w,c})^2}{2}) & \\text{if } |\\hat{y}_{h,w,c} - y_{h,w,c}| \\leq \\delta \\\\\n(\\vert \\hat{y}_{h,w,c} - y_{h,w,c} \\vert - \\frac{\\delta}{2}) & \\text{otherwise}\n\\end{cases} \\tag{3}\n\\end{align}\nFollowing the function view of the problem, i.e., the variables are in fact functions defined everywhere on the sphere for which we only have access to their pointwise evaluation on a grid, we also explored training using geometric $L_2$ and $L_1$ losses. For a given quadrature rule w:\n\\begin{equation}\nw(i) = \\frac{\\cos(\\text{lat}(i))}{\\sum_{j=1}^H \\cos(\\text{lat}(j))}.\n\\tag{4}\n\\end{equation}"}, {"title": "3.7 Metrics", "content": "We report geometric (latitude-weighted) Anomaly Correlation Coefficient (ACC), and Root Mean-Squared Error (RMSE), where the latitudes are weighted according to the latitude value (Rasp et al., 2020; Nguyen et al., 2023b) (see Eq. 5). ACC is defined as:\n\\begin{equation}\nACC(\\hat{y}, y) = \\frac{1}{C} \\sum_{c=1}^C \\frac{\\frac{1}{HW} \\sum_{h=1}^H \\sum_{w=1}^W w(h) (\\hat{y}_{h,w,c} \\times y_{h,w,c})}{\\sqrt{\\frac{1}{HW} \\sum_{h=1}^H \\sum_{w=1}^W w(h) (\\hat{y}_{h,w,c})^2 \\times \\frac{1}{HW} \\sum_{h=1}^H \\sum_{w=1}^W w(h) (y_{h,w,c})^2}}.\n\\tag{6}\n\\end{equation}\nThese two metrics allow for assessing the quality of the trained model. In particular, ACC considers the climatology-corrected correlation between the ground truth and model forecast, and RMSE considers the quantitative differences between the two fields. Since each channel is normalized, we report the mean RMSE and ACC across all 73 input channels."}, {"title": "4 Experiments", "content": "Our evaluation focuses on analyzing different aspects of the design space, starting from evaluating the impact of different problem formulations (Section 4.1), model architecture (Section 4.2), self-supervised pretraining objectives (Section 4.3), image-based pretraining (Section 4.4), multi-step inputs (Section 4.5), zenith angle as an additional input channel (Section 4.6), padding schemes for the convolutional layers (Section 4.7), noise addition to the input states (Section 4.8), auxiliary information regarding the 3D coordinates on the sphere (Section 4.9), loss functions (Section 4.10), additional auxiliary static channels such as soil type, land-sea mask, and topography (Section 4.11), wider models by increasing the hidden dimension (Section 4.12), multi-step fine-tuning including models operating at a larger prediction horizon (Section 4.13), comparison of different models with multi-step fine-tuning (Section 4.14), and training on a larger hourly dataset (Section 4.15)."}, {"title": "4.1 Impact of formulation", "content": "Direct prediction refers to the case where we directly generate the next weather state conditioned on the previous state(s). More concretely, $s_t = \\Phi(S_{t-1})$ where $\\Phi$ represents the deep-learning-based weather forecasting system. This has been a common choice for prior high-resolution models (Pathak et al., 2022; Bonev et al., 2023; Bi et al., 2022; Lam et al., 2022).\nDelta prediction refers to the case where we predict the change to be made to the previous state considering the weather forecasting to be discretization of a continuous-time process (Chen et al., 2018). More concretely, $s_t = S_{t-1} + \\Phi(S_{t-1})$ where $\\Phi$ represents the deep-learning-based weather forecasting system. Note that this residual prediction is distinct from residual layers used within the model architecture. Residual prediction for weather forecasting has also been explored in the recent past (Nguyen et al., 2023b). However, we attempt to disentangle the precise impact of residual prediction instead of directly using it as our proposed approach.\nThe geometric ACC and RMSE for direct vs. delta prediction on selected architectures are"}, {"title": "4.2 Evaluation of different model architectures", "content": "Prior work on high-resolution weather forecasting explored different architecture, ranging from Adaptive Fourier Neural Operators (Pathak et al., 2022), Spherical Fourier Neural Operators (Bonev et al., 2023), Vision Transformer (Bi et al., 2022; Nguyen et al., 2023a), and message-passing GNNs (Lam et al., 2022; Price et al., 2023). Note that the large input size used for high-resolution weather forecasting makes some architectures more applicable to this problem due to memory constraints. Therefore, we further expand on the list of architectures explored in the last section. Given the established utility of delta prediction, we only focus on delta prediction in this case.\nIn particular, we consider three different model families. The first type of models considered"}, {"title": "4.3 Impact of self-supervised pretraining", "content": "A wide range of different pretraining objectives has been used for pretraining deep-learning models in the past (He et al., 2022; Brempong et al., 2022; El-Nouby et al., 2024). Man et al. (2023) applied mask-autoencoding objective (He et al., 2022) for the pretraining of weather-forecasting systems. However, their analysis confounded the impact of the pretraining objective, model architecture, and the training budget that our analysis attempts to disentangle. Similarly, ClimaX (Nguyen et al., 2023a) trained using heterogeneous sources, evading the possibility of understanding the marginal contribution of the pretraining objective itself. We explore the use of four simple pretraining techniques in this work:\nPretraining directly on the forecasting task. Note that fine-tuning using this task is equivalent to training the model for longer with a restart in the learning rate schedule.\nAuto-encoder pretraining where the model is trained to reconstruct the input. In this formulation, we remove the skip connections as it introduces a trivial solution, and initialize these skip connections randomly in the fine-tuning phase.\nMasked auto-encoder (He et al., 2022) where we mask complete channels of the input, and"}, {"title": "4.3.1 UNet (4 Blocks)", "content": "The geometric ACC and RMSE for the different pretraining objectives on 4-block UNet are presented in Table 4 (Fig. 5). The results highlight that pretraining using the supervised objective (pretraining and fine-tuning tasks are the same in this case) achieves better performance in comparison to the evaluated self-supervised objectives. Despite not observing gains in performance with self-supervised objectives in our case, it is likely that improvements in the objectives used would result in significant improvement in forecasting capabilities in the future. Furthermore, we only evaluate a single set"}, {"title": "4.3.2 UNet (5 Blocks)", "content": "The geometric ACC and RMSE for the different pretraining objectives on 5-block UNet are presented in Table 5 (Fig. 6). These results are consistent with the results on the 4-block UNet, where we observe supervised pretraining to outperform self-supervised objectives."}, {"title": "4.4 Impact of using image-based pretraining", "content": "Weather models process different resolutions and channels in contrast to their image-based counter-parts. Despite this, image-based pertaining has been looked at as a form of careful initialization that is useful even in the presence of a task mismatch (Kolesnikov et al., 2020; Bommasani et al., 2021). Therefore, we aim to evaluate the impact of using pretrained weights obtained after image-based pretraining and finetune them for the weather forecasting task by reinitializing the first and the last layer to match the input and output channels. Note that not all parameters are loaded i.e., some parameters are initialized randomly such as the decoder head that we attach to the original model. This includes the input and output features as the image models are trained with just 3 input channels, while we have a significantly larger number of channels in our case. In particular, we use the Segformer B5\u2075 pretrained backbone from MMSeg as our Segformer pretrained model and use the torchvision pretrained ResNet-50\u2076 (He et al., 2016) as our ResNet-50 with conv-decoder pretrained model (which is only trained for classification).\nThe geometric ACC and RMSE for image-based pretraining are presented in Table 6 (Fig. 7). There is a conflicting trend where we see performance improvement for the Segformer model while we observe performance degradation for ResNet-50. We consider this to be primarily an artifact of the loaded model as only the base residual modules were successfully loaded for ResNet-50, leaving the decoder as well as the input stem to be initialized randomly. Furthermore, the model is only trained for classification as compared to Segformer, which is trained directly for segmentation. It would be interesting as a future step to train perfectly aligned models (except the input and the output layers which would be different) and redo this evaluation to get a clear sense of the transfer"}, {"title": "4.5 Impact of multi-step inputs", "content": "All our prior comparisons just took a single step as input. However, it is possible to condition the model prediction on multiple previous input steps as commonly done in the past (Lam et al., 2022; Price et al., 2023; Chen et al., 2023). This is particularly useful when doing long auto-regressive rollouts where the model can look at the previous steps to decide the right delta to be used for the next step. The problem formulation in this case is $S_t = S_{t-1} + \\Phi(\\lbrace S_{t-n}, ..., S_{t-2}, S_{t-1}\\rbrace)$ where n specifies the number of input time-steps.\nThe geometric ACC and RMSE for multi-step inputs are presented in Table 7 (Fig. 8). These results highlight that model performance is nearly maintained for 1 or 2 input steps but degrades significantly when using 4 input steps. We consider the difference in performance between the 1 and"}, {"title": "4.6 Impact of zenith angle", "content": "Solar zenith angle specifies the angle of the Sun to the vertical, providing a notion of time to the model. Therefore, this has been commonly used in prior models (Pathak et al., 2022). We also evaluate the impact of adding zenith angle as an additional input channel to the model. For the auto-regressive rollout, we append the next zenith angle to the model's prediction before feeding it back to the model. We only add the zenith angle for the last input step, even when using multi-step inputs as the delta is computed w.r.t. the last input step.\nThe geometric ACC and RMSE for zenith angle comparison are presented in Table 8 (Fig. 9). It is clear from the results that the zenith angle always improves performance for both short-horizon"}, {"title": "4.7 Impact of padding scheme", "content": "We evaluated the impact of different padding schemes. The default UNet padding scheme is zero padding on both sides. As we have continuity along the longitudinal direction (x-axis), we also evaluate the impact of using a circular padding scheme in the longitudinal direction. Furthermore,"}, {"title": "4.8 Impact of noise addition", "content": "Noise addition has been used as an effective strategy for ensemble generation in the past (Pathak et al., 2022; Bi et al., 2022), where FourCastNet (Pathak et al., 2022) added Gaussian noise while Pangu-weather (Bi et al., 2022) and FuXi (Chen et al., 2023) applied Perlin noise (Perlin, 2002) to the initial states. Given these evaluations, it is useful to understand the impact of noise addition on model performance during training as a regularization technique (Lopes et al., 2019). Therefore, we evaluate the impact of additive Gaussian and Perlin noise during model training.\nThe geometric ACC and RMSE for noise addition are presented in Table 10 (Fig. 11). The figure highlights that adding noise improves model performance in the long-horizon prediction"}, {"title": "4.9 Impact of appending 3D coordinates", "content": "As a fixed-sized grid is not natural for weather forecasting due to the underlying spherical structure of the globe, and limits querying the model on any arbitrary grid point, resolution invariant models that use the query location coordinates as input to generate predictions have been a major focus (Pathak et al., 2022; Bonev et al., 2023; Lam et al., 2022).\nThis 3D coordinate information is useful for the model to understand the proximity in 3D space (specifically close to the poles due to oversampling when projecting to a fixed-sized lat-long grid). Latitude-weighted loss functions have been used to circumvent this issue partially due to this oversampling. In our architecture search, we also used several 3D methods that directly use this 3D coordinate information. This coordinate information can also potentially be useful for even image-based models as used in prior models such as CoordConv (Liu et al., 2018)."}, {"title": "4.10 Impact of loss functions", "content": "Most prior works experimented with different loss functions, such as Geometric MSE (Pathak et al., 2022; Bonev et al., 2023; Lam et al., 2022; Nguyen et al., 2023b) or geometric $L_1$ (Chen et al., 2023). We attempt to evaluate the impact of different loss functions on the predictive performance of the resulting model, which includes regular MSE which is equivalent to $L_2$ (the default loss function that we use to train all our models), geometric MSE, $L_1$ loss, geometric $L_1$, Huber (smooth L1), and L1-L2 loss (with a weight of 0.05 assigned to $L_1$ loss and 0.95 assigned to $L_2$ loss). Note that we did not explore the impact of weighing the variables or the pressure levels differently as explored"}, {"title": "4.10.1 UNet (4 Blocks)", "content": "The geometric ACC and RMSE for different loss functions on the 4-block UNet model are presented in Table 12 (Fig. 13). The figure highlights the superiority of L1-L2 loss when considering short-horizon predictions. However, when considering long-horizon predictions, geometric $L_1$ loss dominates. In general, we observe better performance with $L_1$ variants in contrast to $L_2$ variants potentially due to its capacity to ignore outliers, which is particularly helpful during long auto-regressive rollouts."}, {"title": "4.10.2 UNet (5 Blocks)", "content": "The geometric ACC and RMSE for different loss functions on the 5-block UNet model are presented in Table 13 (Fig. 14). In contrast to the 4-block UNet, L2 (MSE) variants dominate for the short-horizon forecast, while variants of L1 loss dominate in the long-horizon prediction. Interestingly, L1-L2 loss diverges in this case, highlighting a potential optimization instability with L1 variants. As seen previously, this highlights that different models might be best suited to different loss functions and hence, require a model-specific evaluation."}, {"title": "4.11 Impact of constant masks", "content": "Constant masks are static masks that provide additional information to the model that might be useful for prediction. Following Bi et al. (2022), we consider three constant masks including topography, soil type, and land-sea mask. As these masks are static, we only append them once even during the auto-regressive rollout. Since all our input channels are standardized based on the dataset"}, {"title": "4.12 Impact of wider models", "content": "The interplay between the depth and width of the model has been an interesting point of discussion in the past (Zagoruyko & Komodakis, 2016). In order to understand the gain in performance with model width, we experiment with increasing the starting embedding dimension of our model from 64. Note that we have an hourglass architecture, which results in doubling the number of channels after every spatial downsampling stage following the original UNet architecture (Ronneberger et al., 2015). In this case, we increase the number of channels from 64 which was the default for all our prior experiments to 128.\nThe geometric ACC and RMSE with an increased model width are presented in Table 15 (Fig. 16). The results highlight that increasing the hidden dimension of the model from 64 to 128 boosts model performance for both the 4-block and 5-block variants. However, the gain is more pronounced for the 5-block UNet, despite having an extremely large number of parameters in contrast to all other"}, {"title": "4.13 Impact of multi-step fine-tuning", "content": "With the notable exception of Bi et al. (2022) where they used multiple models operating at different temporal resolutions, and Price et al. (2023) where a diffusion model was employed, all prior works rely on fine-tuning the base model for multi-step forecasting to enable the model to generate smooth long-horizon predictions (Pathak et al., 2022; Lam et al., 2022; Bonev et al., 2023). There are different choices involved in this case, where a complicated combination of temporal resolution, number of auto-regressive steps, and appropriate fine-tuning supervision are potential choices that one encounters.\nWe therefore attempt to understand the marginal impact of each of these complex design choices. We evaluate two variants of fine-tuning supervision in this case i.e., intermediate-step supervision and only last-step supervision. When using intermediate-step supervision, we use a discount factor of 0.9 for all the subsequent prediction steps starting with a weight of 1.0, in order to represent higher uncertainty as commonly used in reinforcement learning (Sutton, 2018). Even in the case of just supervising the model on the last step, the gradient is computed through the auto-regressive rollout trajectory. Note that the initial input is the same in all cases i.e., two input steps with a stride of 6h are concatenated regardless of the prediction temporal resolution of the model for consistency between results.\nWe only consider delta prediction formulation for our experiments. However, delta prediction might not be well-suited for larger stride models beyond the 6h horizon considered here. Therefore, there is a chance that these experiments greatly undermine the actual potential of larger stride models. We consider resolving this discrepancy and doing a more detailed analysis as an important"}, {"title": "4.13.1 UNet (4 Blocks)", "content": "The results in terms of geometric ACC and RMSE for the 4 block UNet are presented Table 16 (Fig. 17). The results show a clear trend where fine-tuning the model on subsequently larger strides while only supervising the last step provides the best performance, surprisingly even better than supervising the intermediate steps of the model. This reaffirms the common practice of sequentially fine-tuning a small stride model (such as 6h) on increasingly larger prediction horizons (Pathak et al., 2022; Bonev et al., 2023; Lam et al., 2022).\nNote that there is also a confounding variable in this case in the form of compute time. As each of our runs has a fixed budget of 10 epochs, sequential fine-tuning results in a direct increase in the allocated computational budget. The obtained conclusions might differ if each of these runs is trained till convergence."}, {"title": "4.13.2 UNet (5 Blocks)", "content": "The results for the 5-block UNet are presented in Table 17 (Fig. 18). Similar to the 4-block UNet, we see that the model benefits from sequential fine-tuning. However, in contrast to the 4-block"}, {"title": "4.14 Comparing best models with multi-step fine-tuning", "content": "We initially picked one model i.e., UNet (Ronneberger et al., 2015) out of multiple other promising models including ResNet-50 with fully-convolutional decoder (Long et al., 2015), Segformer (Xie et al., 2021), and SETR (Zheng et al., 2021) from our initial architecture search in Section 4.2. To understand the full potential of these models, we evaluate them again with the inclusion of the uncovered important design choices i.e., using 2-input steps, zenith angle, and multi-step fine-tuning.\nWe compare the performance between these models in Table 18 (Fig. 19). We adopt the sequential fine-tuning scheme in this case, supervising only the last step. Since this is ideal for the 4-block UNet, but not the 5-block variant, we see that the performance of our 4-block model outcompetes the 5-block variant in this case. Furthermore, Segformer achieves performance comparable to our 4-block UNet, while ResNet-50 with convolutional decoder follows the performance of our 5-block UNet. Note that Segformer without fine-tuning is the worst model in the plot with a large margin, again highlighting the importance of multi-step fine"}]}