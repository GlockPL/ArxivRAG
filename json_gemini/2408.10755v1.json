{"title": "Generating Synthetic Fair Syntax-agnostic Data by Learning and Distilling Fair Representation", "authors": ["Md Fahim Sikder", "Resmi Ramachandranpillai", "Daniel de Leng", "Fredrik Heintz"], "abstract": "Data Fairness is a crucial topic due to the recent wide usage of AI powered applications. Most of the real-world data is filled with human or machine biases and when those data are being used to train AI models, there is a chance that the model will reflect the bias in the training data. Existing bias-mitigating generative methods based on GANs, Diffusion models need in-processing fairness objectives and fail to consider computational overhead while choosing computationally-heavy architectures, which may lead to high computational demands, instability and poor optimization performance. To mitigate this issue, in this work, we present a fair data generation technique based on knowledge distillation, where we use a small architecture to distill the fair representation in the latent space. The idea of fair latent space distillation enables more flexible and stable training of Fair Generative Models (FGMs). We first learn a syntax-agnostic (for any data type) fair representation of the data, followed by distillation in the latent space into a smaller model. After distillation, we use the distilled fair latent space to generate high-fidelity fair synthetic data. While distilling, we employ quality loss (for fair distillation) and utility loss (for data utility) to ensure that the fairness and data utility characteristics remain in the distilled latent space. Our approaches show a 5%, 5% and 10% rise in performance in fairness, synthetic sample quality and data utility, respectively, than the state-of-the-art fair generative model.", "sections": [{"title": "Introduction", "content": "With the availability of vast amounts of data, the use of Artificial Intelligence powered systems has increased expo- nentially. These systems are being used to ease our daily life tasks, e.g. language translation, health-care diagnosis. The enormous amount of available data is being used to train these AI-based systems. Unfortunately, these available datasets contain machine or human bias (Liu et al. 2022). There is a chance that a trained model using these biased data will also give biased outcomes for certain demograph- ics (Caton and Haas 2024). For example, in the COMPAS case, the future risk assessment software gives high-risk re- sults towards African-Americans due to false-positive out- comes (Angwin et al. 2016). Recently, large language mod- els have become increasingly popular. However, these mod- els were shown to discriminate against women and people with dark skin (Fang et al. 2024). In another example of bias in decision-making, the Dutch Tax and Customs Adminis- tration in 2021 unlawfully leveraged residents' citizenship information to assess the risk of fraud in childcare benefits applications. This resulted in selection bias in the decision- making process and caused a lot of agony to the affected people (Heikkil\u00e4 2021; Weerts, Theunissen, and Willemsen 2023). So, considering these incidents, the necessity of bias-free dataset or model that can mitigate bias is high. Over the years, researchers have proposed various bias mitigation techniques for improving the model performance towards fairness and data utility. These techniques work ei- ther by pre-processing the data in a way before feeding it to a model that the dataset becomes fair (pre-processing), changing the model architecture in a way that the output of the model will satisfy fairness criteria (in-processing) or by changing the model outcome after training in a way that the outcome is fair towards demographics (post-processing) (Caton and Haas 2024). Representation learning refers to the process of learning latent features which can be useful for downstream tasks. Different approaches have been proposed to learn fair repre- sentation from biased datasets over time (Zemel et al. 2013; Xu et al. 2021; Knobbout 2023). Besides creating fair repre- sentation, researchers also use generative models to generate fair synthetic samples (Rajabi and Garibay 2022; Li, Ren,"}, {"title": "Related Works", "content": "In recent years, various techniques have been proposed to learn fair representation from biased data. LFR (Zemel et al. 2013) presents representation learning approach as a opti- mization task and showed their work achieve fair perfor- mance in the downstreaming task. FairDisco (Liu et al. 2022) learns fair representation by minimizing the corre- lation between the sensitive attributes and non-sensitive at- tributes by using a variance correlation loss. Adversarial ap- proach has been also used to learn fair representation (Gao et al. 2022; Madras et al. 2018; Xu et al. 2021) where the adversary critiques probable unfair outcome. Knowledge distillation is a technique to transfer the knowledge of one model to another. This is usually done by comparing both the teacher (the model we want to distil) and student model's (the model where we want to transfer the knowledge) last layer output and using it to optimize the student model. Recently some work has been done in the field of distillation on data fairness. RELIANT (Dong et al. 2023) and FairGKD (Zhu et al. 2024) uses Graph Neural Network and knowledge distillation to lean the fair represen- tation of the data. However, one of the drawbacks of these approach is the data label is required for distillation, which makes it challenging to distill latent space. So, in this work, we present a novel approach to distil the latent space which eliminates the necessity of data label. Over the years, various (fair) generative models have been proposed to generate synthetic fair data. Generative Adver- sarial Networks (GANs) based TabFairGAN (Rajabi and Garibay 2022) tries to generate fair data by adding fair- ness constraints during the training process, FLDGMs (Ra- machandranpillai, Sikder, and Heintz 2023) produces syn- thetic fair data by generating latent space using GANs and Diffusion models. However, these models are expensive to train due to their architectural design, while our presented architecture is lightweight and takes less computational re- sources and less time to train."}, {"title": "Background", "content": "In this section, we present the background information to follow the paper. First, we present what we mean by fairness, then the background of knowledge distillation, followed by information about synthetic data generation."}, {"title": "Problem Formalization and Data Fairness", "content": "For a given dataset $D := (\\{X_i, Y_i, S_i\\})\\}_{i=1}^n$, here $(x_1,x_2,..X_n) \\in \\mathcal{X}$ are non-sensitive attributes and i.i.d random variables, $s \\in \\mathcal{S}$ are the sensitive attributes, e.g: race, gender, etc. and $y \\in \\mathcal{Y}$ is the target variables. The goal here is to create a fair generative model $G$ that generates synthetic dataset $D'$ by creating and distilling fair representa- tion. The fair representation can be obtained using a model (which can be an Encoder), $z = f_{\\theta}(x,s)$, takes the input $(x, s) \\in (\\mathcal{X}, \\mathcal{S})$ and produce a lower dimensional represen- tation $z \\in \\mathcal{Z}$. Here, $\\theta$ is the trainable parameters for the model $f$. The latent representation $z$ needs to be fair, so that for any downstreaming task, the output $\\hat{y} \\in \\hat{\\mathcal{Y}}$ would be independent of the sensitive attribute $s$. In the rest of the pa- per, we use \u201cFair Representation\u201d and \u201cFair Latent Space\" alternatively. Fairness is a very broad term, and it can mean different things in different discipline. In this paper, when we present fairness, we mean algorithmic fairness. A model will be al- gorithmic fair if it satisfies definition 1 and/or 2:"}, {"title": "Knowledge Distillation", "content": "Knowledge distillation is a transfer learning process where the knowledge of one trained model (teacher model) is being transferred to another smaller models (student model) (Hin- ton, Vinyals, and Dean 2015). Knowledge can be distilled from one model to another model by various manner, e.g. in the response-based knowledge distillation process, the knowledge can be transferred by mimicing the output layer of the teacher model (Hinton, Vinyals, and Dean 2015; Jung et al. 2021; Dai et al. 2021). Usually, the Cross-entropy-loss (CE) is used on the final layer output added with the Kl- divergence (KL). \n$\\mathcal{L}_{distillation} = \\mathcal{L}_{CE}(Y, \\hat{Y}) + \\mathcal{L}_{KL} (p, q)$ (1)\nEquation 1 shows an example of distillation loss where $y, \\hat{y}$ are the label and predicted label respectively and p, q are two different distributions. Sometimes in the loss function a parameter T is used to control probability distribution from both the teacher and student model (Hinton, Vinyals, and Dean 2015). In the feature-based distillation, the knowledge can be transferred by mimicing the intermediate layer of the teacher model (Zhang and Ma 2020). All of the aforementioned distillation, learns to mimic the teacher model based on the data label or using the output of the intermediate layer. But, in the representation learning, the model learns the latent feature so, the label-based distil- lation process is not directly applicable to the representation learning. Also, for the case of fair representation distillation, we need to balance the distillation process in a way that the distillation performance should be maximized for both fair- ness and data utility."}, {"title": "Synthetic Data Generations", "content": "Generative models are used to create synthetic samples that has the same statistical properties of the original data. Most of the recent work in generative models are based on Vari- ational AutoEncoder (VAE), Generative Adversarial Net- works (GANs) and Diffusion model (Oussidi and Elhas- souny 2018). Variational AutoEncoder learns the data distri- bution by compressing the input data into lower dimensional latent space and reconstructing it back to the data space by using two architectures called Encoder and Decoder. Gen- erative Adversarial Networks use adversarial techniques to learn data distribution by using noise as input in an architec- ture called generator and create some fake samples, while another architecture called Discriminator tries to distinguish between the real and fake samples and over time genera- tor learns to fool the discriminator. In the diffusion model, noise is being added gradually to the input data itself until the data becomes pure noise then a neural network architec- ture is used to remove the noise from the data and in that process the neural network learns the data distribution."}, {"title": "Fair Representation Distillation and Synthetic Data Generation", "content": "In this work, we present a novel synthetic data generation technique by learning and distilling fair representation. We take the biased data D and change the distribution $p(D)$ to fair distribution $p(D')$. The process involves training fair representation, which minimizes the correlation between the sensitive and non-sensitive attributes, then distilling the fair representation and finally reconstruction of the high-fidelity data. We divide the whole process of the distillation and data generation into three stages:\n1. We take the biased data $D \\in p(D)$ and encoded it into a fair latent space using variational auto encoder, in this process, the correlation between the sensitive and non- sensitive attributes is being minimized.\n2. We distill the fair latent space into a smaller model, this transfer the knowledge of learned fair representation from the VAE to the smaller model.\n3. We reconstruct high-fidelity fair synthetic data $D'\\in p(D')$ using the distilled fair latent space and trained de- coder from stage 1."}, {"title": "Fair Representation Learning", "content": "Over the years, different approaches to learn the fair repre- sentation learning has been proposed and majority of them are based on the adversarial learning or using some sort of fairness constraints, e.g. demographic parity, equalized odds etc. (Zemel et al. 2013; Zhang, Lemoine, and Mitchell 2018; Ramachandranpillai, Sikder, and Heintz 2023). In these works, fair representation is being learned by encod- ing the data into latent space Z in a way that the corre- lation between the data features and sensitive attribute be- comes lower. So, while doing a classification tasks, the out- come will be free from the sensitive attribute influence. To minimize the correlation between the sensitive attributes and other non-sensitive features, and to optimize the model for fair representation learning, mutual-information minimiza- tion (Rodr\u00edguez-G\u00e1lvez, Thobaben, and Skoglund 2021; Nan and Tao 2020; Moyer et al. 2018) and distance corre- lation minimization techniques (Liu et al. 2022; Guo et al. 2022) has been proposed. In this work, we use the distance correlation minimiza- tion approach equipped with variational autoencoder to learn the fair representation (Liu et al. 2022). Given the dataset $(\\{x_i, y_i, s_i\\})_{i=1}^n \\in D$, first we encode all the features using the encoder $E_{\\Phi}$, into a low dimensional latent space $z \\in \\mathcal{Z}$, where, $z = E_{\\Phi}(x, s)$. Here, $x \\in \\mathcal{X}$ and $s \\in \\mathcal{S}$ are the non- sensitive and sensitive attributes respectively. Then, we train the decoder $(D_{\\Theta})$, and reconstruct the data $x' = D_{\\Theta}(z,s)$. Here, $\\Phi$ and $\\Theta$ are the parameters of the encoder and decoder respectively. To produce fair representation, we minimize the correlation between the latent space (z) and the sensi- tive attribute (s) using\n$\\mathcal{V}_{a}(z, s) = \\int_{ZJS} |\\mathcal{P}_{o(z,s)} - \\mathcal{P}_{o(z)}p(s)|^2 dz ds.$ (2)\nThe final optimization problem of fair representation learning is denoted by (Liu et al. 2022): arg max{log $p_{\\phi}$ (x|s) \u2013 \u03b2$V_z$(z,s)}\n\u03a6,0 (3)\nHere, $V_{\\beta}(z, s)$ is the distance covariance between the la- tent space z and sensitive attribute s (Liu et al. 2022), and $\\beta \\epsilon \\{1,2,3,..., 10\\}$ is the hyper-parameter for balancing the level of fairness."}, {"title": "Distillation of Fair Latent Space", "content": "The main contribution of our work is to enabling the oppor- tunity to distill the fair latent space. Most of the knowledge distillation works need data labels to transfer the knowledge to another model which is the reason they are not useful to learn representation learning. Here we present a distribu- tion matching distillation process where we use the trained encoder $E_{\\Phi}$ to distill its knowledge to a smaller model $E_{\\psi}$. $\\psi$ is the trainable parameter for the smaller model $E_{\\psi}'$. Dur- ing the distillation process, the smaller model $\\psi$ takes the biased data D, as input and produce biased latent space $z' = E_{\\psi}'(x, s)$. Then we transform the biased latent space by measuring the difference between the fair latent space and the biased latent space using a loss function $L_{distillation}$, i.e. $\\mathcal{L}_{distillation} (z, z') = \\sum_{j=1}^k \\mathcal{L}_{distillation} (z_j, z_j')$. (4) Here, the choice of $\\mathcal{L}_{distillation}$ can be L1 loss, MSE loss, mean-difference, Huber loss. Besides ensuring fairness, to enforce the data utility features in the latent space, which helps to maximise the accuracy in the downstream task, we employ utility loss along the quality loss. We calculate the utility loss by calculating the KL divergence as follows:\n$L_{KL} = D_{KL}(q(z') || p(z'))$ (5) Here, q(.) is an approximate model. So, the overall objec- tive of our distillation process is\narg min $V (E_{\\psi}) = \\mathcal{L}_{quality loss} + \\lambda \\times \\mathcal{L}_{utility loss}$ (6) Here, \u03bb is a tuning parameter to tweak the utility perfor- mance. In the experiment, we use \u03bb \u2208 {1, 2, 3, ..., 10}. Algorithm 1 gives the overall process for fair distillation."}, {"title": "Fair Synthetic Data Generation", "content": "After successfully distilled the fair latent space in $E_{\\psi}$, we can use the distilled fair latent space and trained decoder $D_{\\Theta}$ to reconstruct synthetic fair data, $X_{fair} \\sim D_{\\Theta}(z')$, where $z' \\sim E_{\\psi}(x, s)$. Due to usage of quality loss ($L_{distillation}$) and utility loss ($L_{KL}$), the generated synthetic data have increase in both fairness and data utility in the downstreaming task. We show the result in section 5."}, {"title": "Experiments", "content": "In this section, we present the experimental setup of our work. We use four benchmarking datasets (two tabular and two image) for this experiment and a wide range of eval- uation metrics to verify the performance of the model. We compare the performance of our model with several state- of-the-art methods for the tabular data, e.g. Decaf, Tab- FairGAN, FairDisco, FLDGMs, pre-processing technique Correlation Remover and post-processing technique Thresh- old Optimizer. For the CelebA (Liu et al. 2015) and Color MNIST (Lee et al. 2021) datasets, we perform the visualiza- tion analysis as well as the utility and fairness evaluation of our model. While training the benchmark, we use the same hyperparameters as in their respective original publication. For loading the dataset, training and comparing work with other state-of-the-art models, we use a fairness benchmark- ing tool called FairX (Sikder et al. 2024)."}, {"title": "Datasets", "content": "In our experiment, we use two benchmarking tabular datasets (Adult income, COMPAS (Angwin et al. 2016)) and two image dataset (CelebA, Color MNIST), which are widely used in the data fairness field. Details of the dataset can be found in the supplementary section 1."}, {"title": "Evaluation Metrics", "content": "We evaluate the performance of our model with the follow- ing criteria:\n\u2022 Fairness: We evaluate the fairness performance of our model by using the \"Demographic Parity Ratio (DPR)\" and \"Equalized Odds Ratio (EOR)\" in a downstream task (see definition 3 and 4). We use Random Forest as the classifier for the downstream task. The higher score in both ratios means high fairness.\n\u2022 Data Utility: Beside fairness, we also evaluate the qual- ity of our synthetic samples for data utility. Here, we also train a random forest classifier and measure the Accu- racy, Recall and F1-score for the downstream task.\n\u2022 Visual Evaluation: To evaluate the distillation quality visually, we use \"t-SNE\" (Van der Maaten and Hinton 2008) and \"PCA\" (Bryant and Yarnold 1995) plots. These dimension reduction techniques, plot the latent space in two-dimensional space and show how closely the fair la- tent space and distilled fair latent space match each other.\n\u2022 Synthetic Data Quality: We also evaluate the quality of the synthetic data. We check if the synthetic data is fol- lowing the same distribution of the original data or not. We use \"Density\" and \"Coverage\" metrics (Naeem et al. 2020; Alaa et al. 2022) to see how identical the two dis- tributions are and how diverse the synthetic samples are respectively. The \u201cCoverage\u201d metrics also works as a de- tector of mode-dropping problem which is common issue for the generative models, especially for GANS (Good- fellow et al. 2020).\n\u2022 Explainability Analysis: We show the feature impor- tance on downstream tasks using an explainable algo- rithm (Lundberg et al. 2020). This helps us to understand the importance of each feature when making a decision."}, {"title": "Experimental Setup", "content": "In our experiment, for the model design, we follow the same model setup as presented in the FairDisco (Liu et al. 2022). For the distilled encoder, we use the smaller model than the one presented in FairDisco. More about the hyperparameter settings can be found in the supplementary section 3. While"}, {"title": "Tabular Data Results", "content": "First, we present the performance of our architecture for the tabular data. We show the result of our experiment using two sensitive attributes {sex, race}. Also, for the downstream task, we predict the income class for the individuals. Besides the benchmarking model, we show the result using different variations of loss functions on our model (described in section 4.2). We observe that L1- loss with KL divergence achieves best performance in terms of fairness, data utility and synthetic quality. Our distilled model achieves better performance than the base model as well as with other architectures. We get near-perfect results for both of the protected attributes in terms of fairness in both metrics (DPR & EOR score 0.99 and 0.98). We achieve this with the help of our proposed quality and utility loss, which helps us gain better performance in fairness and util- ity than the fair base model and other competing models. We achieve a 5% and 10% rise in the fairness and utility per- formance compared to state-of-the-art (FLDGMs). We also show the effect of the tuning parameter \u03bb in supplementary where \u03bb = 1 exhibits the best performance."}, {"title": "Image Data Results", "content": "We train the CelebA dataset with the target \"Gender\" and sensitive attribute \"Smiling-status\", where \"Smilling\" and \"Not smilling\" are the sub-groups. Figure 2 shows the synthetic data generated by our model, where we use the {Gender, Smiling}, {Gender, Not-Smiling} combination. For the Color MNIST, we use the \"digit number\" as the tar- get and \"colors\" as sensitive attributes. We use the {Digit, Red}, {Digit, Blue}, {Digit, red} combinations for this par-"}, {"title": "Visual Evaluation", "content": "To show if the smaller distilled model is, in fact, distilling the fair representation or not, we use PCA and t-SNE plots to project both latent space (fair and distilled fair) into two- dimensional space. Figure 3 shows the PCA and t-SNE plots of the fair-representation and distilled representation for the \"Compas\" dataset, sensitive attribute: \u201csex\u201d. Each dot repre- sents a data point. Here, we observe both data points from the fair representation and distilled representation almost overlap with each other; this means the distilled represen- tation has captured the fair data distribution."}, {"title": "Synthetic Data Analysis", "content": "Beside the fairness analysis and data utility analysis, we also perform the synthetic data quality analysis. Where we check the quality of the synthetic data. Generative model should follow the distribution of the original data and the gener- ated samples should be diverse. We use the \"Density\u201d and \"Coverage\" metrics to check if the data generated by the generative model follows the original data distribution and if the synthetic samples are diverse. We observe from Table 1, that different variations of our model achieve better scores for both of the synthetic quality metrics, especially our \u201cL1- KL\" variation achieves better performance in all three crite- ria (fairness, utility and synthetic quality). So this signifies"}, {"title": "Explainability Analysis", "content": "For this analysis, we take the fair synthetic data generated from our model, then train a tree-based classifier for a down- stream task, and show the feature importance of both biased data and synthetic data. We use the \"Compas\" dataset for this task and we predict the recidivism rates of the defen- dants in two years. Figure 4 shows the explainability anal- ysis, where the sensitive attribute is \"Sex (Gender)\". We observe, while doing the downstream task on the biased dataset, the importance of the sensitive feature is too high (F Score) in the prediction task. On the contrary, when using our generated synthetic data, the feature importance of the sensitive feature is negligible. This signifies the fair quality of our generated data."}, {"title": "Run-time Analysis", "content": "For the run time analysis of different generative model in our study (TabFairGAN, FLDGMs, Decaf and ours), we mea- sure the training time for the models. We use the same ma- chine and dataset to train these models and record the time. Our experiment shows, our model use 23% less time than the FLDGMs, 105% less than TabFairGAN and 153% less time than Decaf. This happen due to the usage of simple ar- chitecture than the other models also for training in the latent"}, {"title": "Discussion", "content": "To improve the fairness quality, data quality in the synthetic data also reduce the computational overhead in the gener- ative model, in this work, we present a distillation based fair generative model. We present our approach in the sec- tion 4, where we use quality and utility loss to distill the latent space. We use a small model to distill the both tab- ular and image datasets, this aids to reduce the computa- tional overhead while training the model and take less time We also show in the Table 1 that our model is performing better than the distillation base model (FairDisco) as well as all other benchmarking mod- els present in this study. We use wide range of evaluation metrics to evaluate the quality of our generated samples, in terms of fairness, data utility, synthetic data quality, explain- ability analysis and visual evaluation. We show the distilled model is able to capture the data distribution of the fair base model and \u201cDensity\u201d metrics in Table 1). We also show the synthetic samples of Image dataset in the Figure 2.  Amidst doing distillation in the feature space, reducing computational cost and gener- ating high-quality fair data, while creating the fair represen- tation, we are considering single protected attribute. In the future work, we will focus on extending the work for multi- ple sensitive attribute. However, it is noteworthy that, as we are distilling from the latent space, so the same architecture and approach we present here can be used to distill the latent space for multiple sensitive attributes.\nAs a fair generative model, our work takes the biased data and creates both fair representation and fair synthetic data. Using biased data while decision-making might make the decision biased towards some demograph- ics, e.g. the Dutch scandal for fraud detection we discuss in the section 1 (Heikkil\u00e4 2021). Synthetic fair data and fair representation can mitigate the gap, as these synthetic sam- ples and representations do not take sensitive attributes into consideration while making any decision."}, {"title": "Conclusion", "content": "With the advancement of AI-enabled decision making sys- tem, the necessity of fair model has increased due to their vulnerability towards bias decision. Over the times, re- searchers present various bias-mitigation techniques includ- ing generative models to create fair representation and/or fair synthetic samples. However, these generative models need extensive computational resources. One way to miti- gate this issue is to use knowledge distillation to transfer the knowledge from one model to another smaller model. But existing distillation methods works on labelled-dataset which makes it challenging to distill representation. From these motivation, we present a novel fair generative model, where we present an approach to distill fair latent space and use that to generate high-fidelity fair synthetic samples on"}]}