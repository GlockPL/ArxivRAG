{"title": "Voting or Consensus? Decision-Making in Multi-Agent Debate", "authors": ["Lars Benedikt Kaesberg", "Jonas Becker", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "abstract": "Much of the success of multi-agent debates depends on carefully choosing the right parameters. Among them, the decision-making protocol stands out. Systematic comparison of decision protocols is difficult because studies alter multiple discussion parameters beyond the protocol. So far, it has been largely unknown how decision-making addresses the challenges of different tasks. This work systematically evaluates the impact of seven decision protocols (e.g., majority voting, unanimity consensus). We change only one variable at a time (i.e., decision protocol) to analyze how different methods affect the collaboration between agents and test different protocols on knowledge (MMLU, MMLU-Pro, GPQA) and reasoning datasets (StrategyQA, MuSR, SQUAD 2.0). Our results show that voting protocols improve performance by 13.2% in reasoning tasks and consensus protocols by 2.8% in knowledge tasks over the other decision protocol. Increasing the number of agents improves performance, while more discussion rounds before voting reduces it. To improve decision-making by increasing answer diversity, we propose two new methods, All-Agents Drafting (AAD) and Collective Improvement (CI). Our methods improve task performance by up to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the importance of decision-making in multi-agent debates beyond scaling.", "sections": [{"title": "1 Introduction", "content": "Humans are inherently social, and collaboration has been key to innovation and progress. We know that generating solutions together is only beneficial if we can effectively select, agree, and commit to them. History, sociology, and psychology have long demonstrated how different decision-making processes influence collective outcomes (Jones, 1994; List, 2022). Multi-agent systems form a parallel to human behavior by solving problems collectively via debate. However, so far, few"}, {"title": "2 Related Work", "content": "LLMs as Agents. An agent differs from an LLM in that it has a defined planning behavior, can use tools, and maintains a state or memory across interactions. Techniques such as Chain-of-Thought (CoT) prompting (Wei et al., 2022), self-refinement (Madaan et al., 2023), and self-consistency (Wang et al., 2023) improve models' ability to plan, critique, and refine responses. Persona-based prompting (Jiang et al., 2024) enables LLMs to adopt specialized roles, improving answer diversity. A single-agent system operates as one entity with an internal state, while a multi-agent debate consists of multiple agents with a private state that persists across calls and may operate asynchronously, leading to emergent, independent behaviors (Du et al., 2023a; Zhao et al., 2023; Xu et al., 2023; Suzgun and Kalai, 2024; Goldberg, 2024). In multi-agent debates, many parameter choices have to be made, such as in which turn order they communicate (Yin et al., 2023), and which tools they can use (Yao et al., 2023). Yet one choice is inevitable: How to make decisions between agents.\nDecision-Making. Finding collective solutions markedly impacts human decision-making (Jones, 1994). While consensus promotes shared decisions and allows everyone to contribute to the final solution, it can be time-consuming and lead to power concentration of individuals with \"vetos\". Conversely, voting can lead to a faster final decision because it streamlines decision-making, but is susceptible to manipulation and does not take into account all opinions. For example, participants can vote for a less preferred but more viable alternative to block an undesired outcome, leading to outcomes that do not reflect the collective will of the group (List, 2022).\nResearch in multi-agent debate has implemented various human decision protocols (Yin et al., 2023; Chen et al., 2023; Yang et al., 2024b). Exchange-of-Thought (Yin et al., 2023) employs a consensus-based approach, where agents iteratively refine answers through discussion in reasoning tasks, but they do not compare consensus to other decision protocols. Yang et al. (2024b,a) introduce multiple voting protocols (e.g., approval voting) but do not compare the performance of these voting decision protocols across different tasks, focussing more on a comparison of how humans vote compared to LLMs. ReConcile (Chen et al., 2023) integrates a hybrid voting and consensus approach"}, {"title": "3 Methodology", "content": "In the following, we explain the multi-agent environment, decision protocols, response generators, and datasets used in our experiments."}, {"title": "3.1 Setup", "content": "We run multi-agent debates based on three key components: a discussion paradigm, a decision protocol, and an agent response generator. Each discussion consists of three automatically generated expert personas for the task following Kim et al. (2024). While the number of personas can vary, Yin et al. (2023) found this number to be the most efficient. Using personas is important as it generates agents with expertise in different domains, incorporating diverse viewpoints for the final decision. After that, the agents discuss the problem for multiple turns. The number of turns varies from one to five, depending on the experiment setup and decision protocol. The decision protocol defines what criteria must be met for the discussion to end and how the final solution will be created. Each agent can generate one answer per turn and, based on the agent's turn order, respond to the other agents as a default behavior. If not explicitly stated, all agents exchange messages with one another. These discussion characteristics are defined by the discussion paradigm. For consensus decision protocols, each agent also indicates whether they agree with the previous message. We repeat this process each round until a certain level of agreement (defined by the decision protocol) is reached, which leads to the final solution. For voting decision protocols, the agents start voting after the third turn. If they successfully agree on a solution in any turn after the third, the discussion ends. Agents can only memorize messages from up to two turns to limit the context length provided to the agent. We use a response generator to define how agents respond to previous messages. It determines how the discussion history is presented, what additional information is provided, how the persona is introduced, and in which tone they should respond (e.g., neutral or critical). Additional details on the experimental setup can be found in Appendix C.\nThe LLMs used for the experiments are Llama 3 8B and 70B (META, 2024). Within one discussion, all prompted agents use the same base model. The smaller model uses two NVIDIA RTX5000 with 16 GB of VRAM (for ~ 250 hours), and the larger model uses eight NVIDIA A100 with 40GB VRAM (for ~ 40 hours). A list of experiment-specific parameters is available in Appendix E."}, {"title": "3.2 Agent Prompts and Decision Protocols", "content": "Prompt design has a marked impact on multi-agent debate. For this work, we propose three response generators. The Simple Response Generator is our default in which agents are prompted to answer neutral and unbiased to previous messages. The Critical Response Generator encourages agents to critically assess prior answers and propose new solutions, countering sycophantic tendencies (Sharma et al., 2023). The Reasoning Response Generator restricts agents to sharing only reasoning paths, thus avoiding bias towards agreeing with other agents' final solutions.\nDecision protocols then determine the final solution of the discussion by selecting the most promising solution based on predefined mechanisms. We use two classes of protocols in this work.\nConsensus decision protocols decide on an answer by prompting the agents to converge on one shared solution. The solution is selected when a required level of agreement among agents is reached. We explore three major agreement levels: majority consensus - more than 50% agreement, supermajority consensus - more than 66% agreement, and unanimity consensus - all agents have to agree. We extend the majority consensus of Yin et al. (2023) by higher agreement levels, i.e., supermajority (66%), unanimity (100%).\nVoting decision protocols allow several possible solutions to be presented in parallel during the discussion, and ultimately all agents must vote on a final solution. If there is a tie in the voting, all agents will discuss for another round and then vote again. Our work includes four different voting decision protocols inspired by Yang et al. (2024b):"}, {"title": "3.3 Datasets", "content": "We evaluate our decision protocols using six datasets: three knowledge tasks (i.e., MMLU, a broad-topic test; MMLU-Pro, a domain-specific test with challenging questions; GPQA, a specialized question set difficult for web search) and three reasoning tasks (i.e., StrategyQA, a multistep reasoning task; MuSR, a long-context murder mystery; SQUAD 2.0, a reading comprehension task with questions that may or may not have answers in the context). Because of computational constraints, we calculate the task performance on a subset of samples with three runs and report their standard deviation. More details about the datasets, the number of samples, and the sampling strategy can be found in Appendix B."}, {"title": "4 Experiments", "content": "We use a set of diverse reasoning and knowledge tasks to explore the effectiveness and limits of decision protocols in multi-agent debate."}, {"title": "4.1 Performance of Decision Protocols", "content": "Multi-agent debates for task solving have shown promise in recent research, but they rely on a varying choice of decision protocols hindering systematic comparison (Yin et al., 2023; Chen et al., 2023). This experiment systematically compares a set of seven decision protocols, changing only the decision protocol used and nothing else. Specifically, we determine whether some protocols have advantages over others and how they behave in specific edge cases, such as for unanswerable questions.\nWe experiment with four voting-based (Simple, Ranked, Cumulative, Approval) and three consensus-based (Majority, Supermajority, Unanimity) methods and test them on three knowledge tasks (MMLU, MMLU-Pro, GPQA) and three reasoning tasks (SQUAD 2.0, StrategyQA, MuSR). We use the Llama 3 8B model with and without CoT prompting to generate baseline results for comparison with multi-agent debate. We compare answerable and unanswerable questions of the SQUAD 2.0 dataset to inspect how decision protocols behave in edge cases.\nNotably, consensus protocols perform better on knowledge-based tasks with average improvements of 2.3% on MMLU, 4.9% on MMLU-Pro, and 1.3% on GPQA over the voting average. Voting"}, {"title": "4.2 Number of Agents and Discussion Rounds", "content": "Research by Yin et al. (2023) suggests that increasing the number of rounds and agents participating in a multi-agent debate is beneficial because of test-time compute scaling. In our experimental setup, agents communicate for three rounds before voting, following Du et al. (2023b). As our first experiment showed, consensus reaches a decision much faster than voting. Here, we investigate how the number of discussion rounds before voting impacts task performance, as fewer rounds with the same accuracy would improve computational efficiency. Becker (2024) suggests that extended discussions may lead to decreased accuracy because of agents drifting away from the original task. Wang et al. (2024a) shows increasing the number of agents may have positive effects.\nWe conduct this experiment using the StrategyQA dataset because multi-agent debate has consistently mitigated errors of single agents using CoT as shown in the previous experiment. The experiment is structured in two parts. First, we fix the number of agents to three and increase the number of discussion rounds from one to ten. Second, we fix the number of rounds to three and increase the number of agents from one to ten. Both experiments use the simple voting decision protocol, and each condition is evaluated across three independent runs."}, {"title": "4.3 Answer Diversity", "content": "In multi-agent debates, answer diversity plays an important role in improving decision-making and task performance. A more diverse set of answers is beneficial because selecting the correct solution from multiple options is often easier than relying on a single agent's solution (Zheng et al., 2023). This principle is also key to self-consistency approaches (Wang et al., 2023) and explains why multiple-choice tests are often easier than open-ended ones. The goal of this experiment is to exploit this property and explore ways to increase answer diversity to optimize task performance.\nIn typical multi-agent debates, the first agent starts generating a possible solution for the given task. The next agent can either propose a new solution or improve on the previous solution. Throughout our experiments, we observe that agents are agreeable and often only improve the answer from the first agent without proposing an idea based on their own expertise. We propose three methods to counteract this issue and to improve answer diversity. First, we introduce All-Agents Drafting (AAD), a method that forces each agent to generate an independent solution based on their own expertise in the first round. Second, we propose Collective Improvement (CI) which starts similarly to AAD with each agent generating an independent solution, but unlike AAD where each agent can communicate with another after the first turn, CI prohibits communication and only shows the solutions from the previous turn to the agent in the next"}, {"title": "5 Conclusion", "content": "We systematically evaluated the role of consensus and voting decision protocols across three knowledge and three reasoning tasks. Our study assessed how the number of discussion rounds and agents influences task performance. We propose two new methods to improve answer diversity during multi-agent discussions and decisions, i.e., All-Agents Drafting (AAD) and Collective Improvement (CI). AAD requires each agent to contribute draft ideas at the beginning of the discussion, and CI encourages independent reasoning steps by limiting communication between agents and only allowing them to exchange possible solutions after each turn.\nOur findings show that voting performs well on reasoning tasks, outperforming consensus by up to 13.2%, and outperforming a single CoT baseline by 10.4%. This is likely because voting-based protocols allow agents to explore multiple reasoning paths instead of a single one, as in consensus. In comparison, consensus outperforms voting in knowledge tasks by up to 2.8%, because it improves fact-checking by requiring at least the agreement of the majority of agents. Increasing the number of agents in the discussion improved task performance, while increasing the number of discussion rounds before voting decreased performance. AAD improved performance by up to 3.3%, and CI by up to 7.4% over default multi-agent debate baseline, and 6.1% and 10.2% over single model CoT baseline respectively. Our new methods enhance answer diversity and reveal a connection between answer diversity and task performance.\nFuture work could explore other characteristics influencing decisions, such as power relations between managers and employees. This could also involve examining personas within this hierarchical structure to investigate whether dominant or affectionate leaders are more effective in leading discussions (Ames, 2009).\nWe recommend using voting in reasoning tasks and consensus in knowledge tasks, scaling up the number of agents instead of the number of rounds, and increasing the diversity of answers between agents using AAD and CI."}, {"title": "Limitations", "content": "Multi-agent debates are computationally expensive because they require a message from each agent in each round, quickly leading to hundreds of forward passes per model. Because of the high computational cost and the range of decision protocols and tasks in our work, we used sampled subsets of the datasets, which can lead to some variance. To control for that variance, we sampled with a 95% confidence level and calculated the standard deviation of three independent runs.Overall, the results were markedly higher than what could be explained by the standard deviation. More details about the dataset and other parameters can be found in Appendix B. Despite efforts to improve answer diversity, agents often converged on similar responses, suggesting that more advanced techniques to encourage independent solutions are needed in the future."}, {"title": "A Additional Results", "content": "Additional results for the first experiment to provide further information."}, {"title": "A.1 Task Performance with Llama 3 70B", "content": "Compared to the results of the Llama 3 8B model, the larger Llama 3 70B model performs much better overall, as seen in Table 3. Most of the results are a bit better than the baseline, but the multi-agent discussions are only in a few cases able to outperform the CoT baseline. This model does not fail to follow the prompt for the MuSR baseline and consensus-based decision protocols. Therefore, the big performance gain from the smaller model cannot be observed here. SQUAD 2.0 and StrategyQA had the largest performance gains, even outperforming the CoT baseline, similar to the results from the smaller model. This difference in task performance can have many reasons. As Li et al. (2024) showed, smaller models are more likely to hallucinate, which reduces task performance. This can be mitigated by using multiple agents because it is less likely that two agents hallucinate the same things. Larger models tend to hallucinate less, reducing this effect for the Llama 3 70B model (Li et al., 2024). In general, Llama 3 70B has a much higher baseline for task performance, making it more difficult to improve baseline results. Many of the improvements by the Llama 3 8B model are quite small, except for the ones where the Llama 3 70B model also outperforms the CoT baseline. This can be taken as evidence that these multi-agent discussions require specific problem structures, or else the agents are just talking about the same results for multiple rounds and agreeing with each other. If these discussions continue too long, they can drift away from the original task, which reduces task performance. This has also been observed by Becker (2024) and an example can be seen in Appendix F.3. A positive example of how discussion can help task performance can be seen in Appendix F.1."}, {"title": "A.2 Termination percent", "content": "The data in Table 4 shows the number of turns that are needed for each decision protocol to reach a final decision for the MMLU dataset. Most of the voting decision protocols are able to vote for a final answer already in the first round in which they are allowed to vote. Simple voting has the"}, {"title": "B Additional Details on Datasets", "content": "The dataset selection is very important for this work. It needs to be tested whether decision protocols perform well in multiple domains and whether some protocols perform better with specific tasks than others. Therefore, we selected datasets from different domains and divided them into two groups:\n\u2022 Knowledge-based Datasets: MMLU, MMLU-Pro, and GPQA. These still require some reasoning and domain knowledge.\n\u2022 Reasoning-based Datasets: StrategyQA, MuSR, and SQUAD 2.0. These emphasize multistep reasoning and textual comprehension.\nAn overview of all these datasets can be found in Table 5 with a description and the number of samples used for evaluation."}, {"title": "B.1 Sampling Strategy", "content": "Because multi-agent discussions are expensive, we use a small subset of each dataset that still represents the dataset effectively. This follows approaches used by Yin et al. (2023); Chen et al. (2023); Becker (2024) and ensures a 95% confidence level with a 5% margin of error:\n\\(n_0 =  \\frac{Z^2 \\cdot p \\cdot (1-p)}{d^2}\\)"}, {"title": "C Multi-Agent Framework", "content": "For our experiments, we use the Multi-Agent Large Language Models (MALLM) framework7."}, {"title": "C.1 Architecture Overview", "content": "To better understand the different modules, we take a closer look at each component and what role it plays in creating multi-agent discussions. An overview can be found in Figure 10 as it provides an example workflow for the framework and how a discussion is created. The discussion starts with generating personas relevant to the given task and assigning them to the participating agents. The personas are generated using the same LLM which is later used for the agents. After that the agents start to generate solutions and improve the suggestions from the other agents. The turn order of the agents is defined by the discussion paradigm. This also defines which answers are visible to other agents and who can talk to whom. The response generator defines how an agent receives the other answers and also the way it responds. After a certain number of rounds or when enough agents agree, a decision protocol is used to select the best answer either via voting or just by looking for a certain consensus threshold. If the decision protocol fails, for example, due to a tied vote, the discussion continues for another round. In the framework a parameter can be defined to terminate discussions after a certain number of rounds to make sure they do not communicate forever."}, {"title": "Agent Personalities", "content": "The first step of the discussion is the generation of agent personas. Each of the agents participating in the discussion has a certain persona assigned to them. This can unlock more knowledge for the LLM on a specific topic (Kim et al., 2024). To get the best results, we want as diverse personas as possible while still maintaining them to be relevant to the task. The default setting for the framework is to prompt a LLM and ask for a persona relevant to the given task (Wang et al., 2024c). After each generation it also provides the generated personas to avoid duplication. This way of generating personas provides a good starting point, but as this is built as a modular component, it can be swapped out with another function, which, for example, generates half of the agents with this method and initializes the other ones as neutral agents without a persona."}, {"title": "Response Generators", "content": "Another important part of multi-agent discussions is how each agent responds to the previous responses. Do we use CoT to improve performance, or does this result in too long answers? By changing the way an agent is prompted, a lot of performance can be gained or lost. Therefore, it is key to make this as customizable as possible. The researcher has the possibility to change the default behavior (neutral answers), for example, by prompting the agent to be more critical or changing the way the discussion history is presented. The system prompt for the agent's persona can also be adjusted. MALLM already has many different built-in response generators. The ones relevant for this work are the following.\n\u2022 Free Text is the most basic form of the agent prompt. Each agent gets a predefined number of discussion history rounds as memory. The prompt language is neutral, and the task is presented each round to mitigate the potential drift from the topic of the discussion (Becker, 2024). In addition, the agent is always asked to agree or disagree with the answer of the previous agent.\n\u2022 Simple behaves very similar to the Free Text response generator, but the prompt is a bit simpler to make it easier to understand for the LLM and reduce the context length.\n\u2022 Critical forces the agent to respond very critically to the previous answer and try to find new solutions. Some studies have shown that"}, {"title": "Discussion Paradigms", "content": "These paradigms define the discussion format for the entire task. They can control the order in which the agents communicate with each other, and which answers are visible only to certain agents. Currently, all the built-in discussion paradigms are static, meaning that the turn order is predefined and cannot be changed based on specific events during the discussion. However, due to the modular nature of MALLM, a new discussion paradigm can be added, for example using an LLM as a moderator to dynamically decide which agent should respond next. Current research by Yin et al. (2023) and Becker (2024) shows that discussion protocols have little impact on downstream task performance. MALLM includes the following discussion paradigms, which are illustrated in Figure 11. The first four paragdims are inspired by the work of Yin et al. (2023), while the fifth was developed as part of this work.\n\u2022 Memory is the most basic discussion paradigm. The agents respond to the solution of the previous agents with feedback or an improved solution. All answers are visible to the other agents.\n\u2022 Relay behaves similarly to the memory paradigm. The turn order is the same, but each agent can only see the answer from the previous agent.\n\u2022 Report introduces one agent as a moderator that can communicate with other agents. The"}, {"title": "Decision Protocols", "content": "These are crucial for the framework as they decide which answer gets presented as the final answer to the problem. Multi-agent discussions produce multiple results for the same problem because each agent has its own reasoning and ideas on how to solve the problem. Therefore, some process is needed to decide which answer looks the most promising. We divide these decision protocols into three subtypes that we want to analyze. An overview of how each of these decision protocols works theoretically can be found in Figure 1 and all prompts used for them can be found in Appendix D.\nConsensus Based Decision Protocols. These are the simplest kinds of decision protocols. After each answer, the next agent has to agree or disagree with the previous statement. Depending on the response generator, this happens in the same message, and the agreement is extracted with a regular expression, or this is split into multiple answers. If enough agents agree in order, there is a consensus. The final answer is extracted by instructing the last agent to solve the given task with the information available in the latest messages. The prompt used for this can be found in Appendix D.1. There are several types of consensus decision protocols available in MALLM. Majority consensus requires 50% of the agents to agree. Supermajority con-"}, {"title": "D Prompts", "content": ""}, {"title": "D.1 Final Answer Extraction", "content": ""}, {"title": "D.2 Voting Prompts", "content": ""}, {"title": "D.3 Challenge Prompt", "content": ""}, {"title": "E MALLM Setup", "content": ""}, {"title": "F Example Discussions", "content": ""}, {"title": "F.1 Successfull Voting Discussion", "content": "All decision protocols are attached as an example to this discussion. The original discussion was created using simple voting."}, {"title": "Overview", "content": "Task: Answer the following question. If the question is not answerable with the provided information, write '[UNKNOWN]'.\nInput: What choir from the area has sung at the Guildhall?\nContext: Southampton has two large live music venues, the Mayflower Theatre (formerly the Gaumont Theatre) and the Guildhall. The Guildhall has seen concerts from a wide range of popular artists including Pink Floyd, David Bowie, Delirious?, Manic Street Preachers, The Killers, The Kaiser Chiefs, Amy Winehouse, Lostprophets, The Midnight Beast, Modestep, and All Time Low. It also hosts classical concerts presented by the Bournemouth Symphony Orchestra, City of Southampton Orchestra, Southampton Concert Orchestra, Southampton Philharmonic Choir and Southampton Choral Society."}, {"title": "Turn 1", "content": ""}, {"title": "Turn 2", "content": ""}, {"title": "Turn 3", "content": ""}, {"title": "Result", "content": ""}, {"title": "F.2 Agents Tricked Discussion", "content": "In this discussion, the agents were tricked by information provided in the context."}, {"title": "Overview", "content": "Task: Answer the following question. If the question is not answerable with the provided information, write '[UNKNOWN]'.\nInput: What provides critical support for drug discovery and the availability of economic resources?\nContext: Biodiversity provides critical support for drug discovery and the availability of medicinal resources. A significant proportion of drugs are derived, directly or indirectly, from biological sources: at least 50% of the pharmaceutical compounds on the US market are derived from plants, animals, and micro-organisms, while about 80% of the world population depends on medicines from nature (used in either modern or traditional medical practice) for primary healthcare. Only a tiny fraction of wild species has been investigated for medical potential. Biodiversity has been critical to advances throughout the field of bionics. Evidence from analysis and biodiversity science indicates that the decline in output from the pharmaceutical sector since the mid-1980s can be attributed to a move away from natural product exploration (\"bioprospecting\") in favor of genomics and synthetic chemistry, indeed claims about the value of undiscovered pharmaceuticals may not provide enough incentive for companies in free markets to search for them because of the high cost of development; meanwhile, natural products have a long history of supporting significant economic and health innovation. Marine ecosystems are particularly important, although inappropriate bioprospecting can increase biodiversity loss, as well as violating the laws of the communities and states from which the resources are taken."}, {"title": "Turn 1", "content": ""}, {"title": "Turn 2", "content": ""}, {"title": "Turn 3", "content": ""}, {"title": "Result", "content": ""}, {"title": "F.3 Bad Voting Discussion", "content": "In this discussion, the agents were tricked by one agent who came up with a solution not provided in the context."}, {"title": "Overview", "content": "Task: Answer the following question. If the question is not answerable with the provided information, write '[UNKNOWN]'.\nInput: Where must enclitics appear in an English sentence?\nContext: Because Czech uses grammatical case to convey word function in a sentence (instead of relying on word order, as English does), its word order is flexible. As a pro-drop language, in Czech an intransitive sentence can consist of only a verb; information about its subject is encoded in the verb. Enclitics (primarily auxiliary verbs and pronouns) must appear in the second slot of a sentence, after the first stressed unit. The first slot must contain a subject and object, a main form of a verb, an adverb or a conjunction (except for the light conjunctions a, \"and\", i, \"and even\" or ale, \"but\")."}, {"title": "Turn 1", "content": ""}, {"title": "Turn 2", "content": ""}, {"title": "Turn 3", "content": ""}, {"title": "Result", "content": ""}, {"title": "F.4 Majority Consensus Discussion", "content": "In this discussion, the agents discussed only one round, as they already had a high enough agreement score."}, {"title": "Overview", "content": "Task: Answer the following question. If the question is not answerable with the provided information, write '[UNKNOWN]'.\nInput: What choir from the area has sung at the Guildhall?\nContext: Southampton has two large live music venues, the Mayflower Theatre (formerly the Gaumont Theatre) and the Guildhall. The Guildhall has seen concerts from a wide range of popular artists including Pink Floyd, David Bowie, Delirious?, Manic Street Preachers, The Killers, The Kaiser Chiefs, Amy Winehouse, Lostprophets, The Midnight Beast, Modestep, and All Time Low. It also hosts classical concerts presented by the Bournemouth Symphony Orchestra, City of Southampton Orchestra, Southampton Concert Orchestra, Southampton Philharmonic Choir and Southampton Choral Society."}, {"title": "Turn 1", "content": ""}, {"title": "G Al Usage Card", "content": ""}]}