{"title": "MIGT: Memory Instance Gated Transformer Framework for Financial Portfolio Management", "authors": ["Fengchen Gu", "Angelos Stefanidis", "\u00c1ngel Garc\u00eda-Fern\u00e1ndez", "Jionglong Su", "Huakang Li"], "abstract": "Deep reinforcement learning (DRL) has been applied in financial portfolio management to improve returns in changing market conditions. However, unlike most fields where DRL is widely used, the stock market is more volatile and dynamic as it is affected by several factors such as global events and investor sentiment. Therefore, it remains a challenge to construct a DRL-based portfolio management framework with strong return capability, stable training, and generalization ability. This study introduces a new framework utilizing the Memory Instance Gated Transformer (MIGT) for effective portfolio management. By incorporating a novel Gated Instance Attention module, which combines a transformer variant, instance normalization, and a Lite Gate Unit, our approach aims to maximize investment returns while ensuring the learning process's stability and reducing outlier impacts. Tested on the Dow Jones Industrial Average 30, our framework's performance is evaluated against fifteen other strategies using key financial metrics like the cumulative return and risk-return ratios (Sharpe, Sortino, and Omega ratios). The results highlight MIGT's advantage, showcasing at least a 9.75% improvement in cumulative returns and a minimum 2.36% increase in risk-return ratios over competing strategies, marking a significant advancement in DRL for portfolio management.", "sections": [{"title": "1 Introduction", "content": "Portfolio management involves determining the allocation of funds to multiple financial assets and continuously changing the portfolio weights to maximize the investment returns (Avramov, 2002). The financial investment market is volatile because it is influenced by economic, political, and technological factors (Poon & Granger, 2003). The financial market data are complex, often consisting of a large volume of historical information such as price, volume, and technical indicators (X.-Y. Liu et al., 2021). The varying conditions of the market environment together with its inherent data complexity, makes it difficult for traditional investment methods to effectively analyze and inform investor decisions (Z. Gao, Gao, Hu, Jiang, & Su, 2020; Ren, Jiang, & Su, 2021).\nDeep learning is a branch of machine learning that uses multiple layers of neural networks to learn representations of data with higher levels of abstraction (Lecun, Bengio, & Hinton, 2015). It has revolutionized the fields of computer vision, speech recognition, and natural language processing (Bengio & Courville, 2013; Xia, Yin, Dai, & Jha, 2020). Reinforcement learning is a machine learning technique in which an agent learns how to operate in an environment by trial and error while maximizing rewards and minimizing penalties (Charpentier, Elie, & Remlinger, 2020; Sutton & Barto, 1998). It allows machines and software agents to discover optimal strategies within a specific context automatically (Bello, Zoph, Vasudevan, & Le, 2016).\nDeep reinforcement learning (DRL), combining the perception capabilities of deep learning with decision-making using current knowledge of reinforcement learning, is a practical approach to automated portfolio management (Jiang, Xu, & Liang, 2017). The DRL framework learns using feedback from a complex environment and is well suited to addressing dynamic decision-making problems. In portfolio management, stock trading is about making dynamic trading decisions in a complex stock market environment, i.e., deciding when and what to trade, at what price, and the transaction volume (Jiang & Liang, 2018; Yao, Ren, & Su, 2022).\nRecently, researchers applied DRL to portfolio management in the cryptocurrency markets and the stock markets. The cryptocurrency market is particularly volatile in the short term, and high-frequency trading can lead to short-term gains. Jiang and Liang (2018) used the Ensemble of Identical Independent Evaluators (EIIE) and policy gradients for cryptocurrency portfolio management. Others subsequently constructed the EIIE framework by modifying the neural networks and adding features as input (Gu, Jiang, & Su, 2021; Qin, Gu, & Su, 2022; R. Sun, Jiang, & Su, 2021; X. Yang et al., 2022). The stock market tends to trade on a single day or over a longer period of time, requiring a long-term portfolio management strategy (Z. Gao et al., 2020). The relatively early framework Investor-Imitator (IMIT) uses reinforcement learning to mimic investor behavior to extract trading knowledge rather than directly using reinforcement learning for portfolio management (Z.T.Y. Liu, 2018). For compatibility with more reinforcement learning algorithms, the FinRL framework presented by X.-Y. Liu et al. (2021) provides reinforcement learning-based stock trading strategies that support single stock trading and portfolio management containing multiple stocks. It supports a variety of reinforcement learning algorithms, e.g., Deep Deterministic Policy Gradient (DDPG) (Paul et al., 2018), Soft Actor-Critic (SAC) (Haarnoja, Zhou, Abbeel, & Levine, 2018), and Proximal Policy Optimization (PPO) (Schulman, Wolski, Dhariwal, Radford, & Klimov, 2017). Based on FinRL framework, the Ensemble Strategy (ES) uses the DDPG, SAC, and PPO that are selectively applied to different time intervals according to some decision rule (H. Yang, Liu, Zhong, & Walid, 2020). In addition to FinRL, there is also TradeMaster, an open-source platform for quantitative trading through reinforcement learning, which covers the entire process of designing, implementing, evaluating, and deploying reinforcement learning-based algorithms (S. Sun et al., 2023). Other works such as SARL augment asset information with price movement prediction and are based on reinforcement learning (Ye et al., 2020).\nWhile existing frameworks demonstrate the efficacy of reinforcement learning in portfolio management, some limitations remain. First, the stock market is a complex and volatile environment, with significant fluctuations occurring during trading. This makes training challenging in converging to obtain a reliable policy and update asset portfolio weights effectively (Guan & Liu, 2022; V\u00e1zquez-Canteli & Nagy, 2019). This may lead to the inability of DRL to train effective strategies to rationally allocate portfolios, resulting in large fluctuations in returns. Second, the existing DRL portfolio management frameworks have insufficient generalization capabilities (Cui, Sun, & Su, 2022; Koziarski, 2020). Portfolio management should focus more on long-term trends and withstand the impact of short-term volatility, such as \"Black swan\" events that lead to sharp fluctuations in stock prices. Outliers from such events and other causes can disrupt reinforcement learning's modeling of the normal pattern of stock price movements, leading to the learning of inaccurate associations. Reinforcement learning models may fail to deal with the different features of different market situations, reducing the generalization ability of the strategy (Kandanaarachchi, Mu\u00f1oz, Hyndman, & Smith-Miles, 2020; Onireti et al., 2016). Third, the profitability of existing portfolio management models targeting stock markets is not optimal, primarily due to the lack of processing measures for the characteristics of stocks (R. Gao et al., 2022; Soleymani & Paquet, 2020). They contain simple networks with fewer layers or existing networks from other domains without sufficient optimization for portfolio management. In either case, they are unable to adequately handle complex stock data, leading to limited feature extraction capabilities for capturing complex and non-linear relationships in stock data (C. Liu, Ventre, & Polukarov, 2022; R. Zhang et al., 2022).\nThe key contributions of our work are threefold. First, the stability of DRL training is enhanced by using the newly constructed Lite Gate Unit (LGU) gating layer as the fan-in layer (Dai et al., 2020), which allows the training to converge smoother and faster, resulting in more effective portfolio management strategies. Second, we propose the use Instance Normalization to balance the scale difference between different feature dimensions of each state, avoiding certain feature dimensions that dominate the model training. Such an approach can reduce the negative impact of single sample outliers and short-term volatility, allowing the network to learn more discriminative feature representations, and improve the generalization ability of the model (Kandanaarachchi et al., 2020; Ulyanov, Vedaldi, & Lempitsky, 2014). Third, we construct a new Transformer variant (Gated Instance Attention module) to handle complex stock data by leveraging the ability of the model to capture long-range dependencies as well as dependencies among different periods in stock data, which improves the profitability of the DRL portfolio management strategies (Vaswani et al., 2017). It achieves parallel computation through a multi-headed self-attention mechanism, which enables it to efficiently process high-dimensional features of stock data, and eventually improve the profitability of the portfolio management strategies.\nThe remainder of this paper is organized as follows. In Section 2, we define the trading period, make some assumptions about the experimental setting, and give the portfolio management objectives. Section 3 presents the DRL environment for portfolio management. In Section 4, the Memory Instance Gated Transformer (MIGT) policy network is presented. In Section 5, we conduct the comparative and ablation experiments as well as offer an interpretation of the results. Finally, conclusions and future work are given in Section 6."}, {"title": "2 Definition", "content": null}, {"title": "2.1 Definition of Data Input,\nTrading Period and Process", "content": "Under the DRL framework, the agent reallocates capital into different asset classes at each period t, t \u2208 N\u207a. We first consider the process of portfolio management in Figure 1. The input to the portfolio management model is a high-dimensional tensor with dimensions of historical time, stock assets, and historical data including price and technical indicators. We set the length of each trading period t to one day. At the end of each period t, the agent trades on the portfolio with Vt, the vector of closing prices of all assets in period t, and the portfolio value is Pt. Portfolio value is the total value of all the securities in the portfolio and the cash sum, i.e., to obtain the total value of the entire portfolio."}, {"title": "2.2 Assumptions about the\nExperimental Environment", "content": "In order for the constructed trading environment to be as realistic as possible, the following assumptions are made:\n\u2022 Because the trading simulation is based on historical data, we assume our transactions will not affect the price. This is due to the amount traded being tiny relative to the market's overall size, and as such, the impact on stock prices is negligible (Jiang & Liang, 2018; Soleymani &\nPaquet, 2020).\n\u2022 Since the trading frequency is set to one day, the price of each trade is the previous day's adjusted closing price (X.-Y. Liu et al., 2021). Based on the actions in period t, we define the trading"}, {"title": "2.3 Portfolio Management Objective", "content": "The objective of our strategy is to maximize the return of the final portfolio. We define Pt \u2208 R+ to be the portfolio value at the end of period t. The variable Pt contains the available cash At and the stock values Et. The available cash in the portfolio at this period is:\n$A_t = A_{t-1}+ M_tV_t(1-c) \u2013 B_tV_t(1 + c)$, (1)\nwhere Mt is the share of stock sold in period t, Bt represents the proportion of stock bought in period t, and Vt is the closing price vector in period t. The dimensions of the vectors Vt, Mt and Bt are the number of stocks n in the portfolio. The portfolio share vector Wt represents the share of each asset in the portfolio in period t. At each time period t, the portfolio share vector Wt is Wt\u22121 from the previous time period adds the shares purchased Bt and subtracts the shares sold Mt,\n$W_t=W_{t-1}+ B_t- M_t$. (2)\nThe value of stock assets Et at period t is the product of the transposition of the portfolio share vector Wt, where the share of each stock is non-negative, and the vector of all assets' closing prices Vt:\n$E_t = W_t^TV_t = (W_{t-1}+ B_t - M_t)^TV_t$. (3)\nThe portfolio's value in period t, Pt, can be obtained by summing the equations (1) and (3), i.e.,\n$P_t = A_t + E_t = A_{t-1}+ W_{t-1}^TV_t \u2212 c (B_t + M_t)^T V_t$. (4)\nFrom equation (1), we obtain the value of the stocks in the portfolio in the last period t \u2212 1,\n$P_{t-1}= A_{t-1}+ E_{t-1} = A_{t-1}+ W_{t-1}^TV_{t-1}$. (5)\nSubtracting equation (5) from equation (4), we obtain the change in the portfolio value in period t, i.e.,\n$\u0394P_t= P_t -P_{t-1} = W_{t-1}^T(V_t \u2013 V_{t\u22121}) \u2212 c (B_t + M_t)^T V_t$. (6)\nThe change in portfolio value in period t is the increase or decrease in returns at that period, and the goal of portfolio management is to maximize positive returns \u25b3Pt or minimize negative losses during each period t."}, {"title": "3 DRL Environment", "content": null}, {"title": "3.1 The Markov decision process of\nportfolio management", "content": "The stock market is stochastic, i.e., the movement of stock prices is random and unpredictable (M., Harikrishnan, & Ambika, 2022). We model the portfolio management task as a Markov decision process problem (Figure 2) to choose an action based on the current state and then ran-domly move to a new state (Baxter & Puterman,"}, {"title": "3.2 State, Action, Reward Function,\nand transaction agent", "content": "The state st is a representation of a particular situation or context that describes the environment (Naeem, Rizvi, & Coronato, 2020). Transaction agents observe many features in an interactive market environment to make sequential decisions (Arulkumaran, Deisenroth, Brundage, & Bharath, 2017). To make the model adaptive to stock trends and volatility data, we use different features to help agents to make informed decisions and use them to construct the state. Previous frameworks have generally used the following items as states:\n\u2022 At, available cash assets of stocks in period t;\n\u2022 Wt, the share vector of each stock in the portfolio in period t; and\n\u2022 Vt, the closing price vector in period t.\nTo help portfolio management strategies identify uptrends and downtrends in each stock, the following trend indicators are included as part of the state:\n\u2022 Bollinger bands (BOLL) can be used to determine the range of stock price fluctuations and future movements, and to show the safe high and low-price levels of a stock (Murphy, 1999);\n\u2022 Commodity Channel Index (CCI) mainly measures the variability out of the normal range of prices (Altan & Karasu, 2022);\n\u2022 Relative Strength Index (RSI) is a momentum indicator that measures the speed and magnitude of price movements (Altan & Karasu, 2022); and\n\u2022 True Range of Trading (TR) is used to measure the intensity of market volatility (Chang, Li, & Zeng, 2019).\nOverbought and oversold indicators can reflect the short-term volatility of each stock and help portfolio management strategies perceive risk. Therefore we also include the following volatility indicators:\n\u2022 Directional Movement Index (DMI) is used to determine the movement of stock prices by analyzing the change in the equilibrium point between buyers and sellers during the rise and fall of stock prices (CAVDAR & AYDIN, 2020);\n\u2022 Moving Average Convergence Divergence (MACD) is a technical indicator that uses the convergence and divergence between the short-term exponential moving average and the long-term exponential moving average of the closing price to make a judgment on the timing of a trade (Hung, 2016); and\n\u2022 Money Flow Index (MFI) is a technical indicator that uses trading volume and price to determine overbought and oversold (Singleton, 2014).\nThe state tensor in period t is $S_t = [f_{1,t}, f_{2,t}, ..., f_{n,t}]$, where n the number of stocks in the portfolio. Each element of st, fk,t = [Ak,t, Wk,t, Vk,t, BOLLk,t, MFIk,t], is the feature vector of k-th stock in period t, k = 1,2,3,..., n.\nTo allow the model to deal directly with portfolios rather than individual stock trades, the process of transaction is the portfolio in period t for each asset in Et after a buy, sell or hold operation (Kumar, Yadav, Gupta, & Mehlawat, 2021; Poon & Granger, 2003). The action at is a portfolio share vector Wt that reflects the buying and selling behavior of the transaction agent.\nThe role of the reward function r (st, at, st+1) is to define the goals of DRL and evaluate the transaction agent's behavior (Arulkumaran et al., 2017; Lehnert, Littman, & Frank, 2020). The change in the portfolio value in period t from the"}, {"title": "4 Policy Network", "content": null}, {"title": "4.1 Research Problem", "content": "DRL is successfully applied to many domains, especially those with restricted state and action spaces that are conducive to exploration. The environments of most DRL tasks are relatively stable, with rules that persist and do not change frequently, i.e., chess and other confrontational games (Chasparis & Shamma, 2012; Khader, Yin, Falco, & Kragic, 2021). The stock market is challenging for DRL due to its instability and dynamics (Parisi, 2020). Portfolio management tasks face a more dynamic and volatile envi-ronment than typical DRL application domains, being influenced by many external factors such as the interplay between corporate earnings, macroeco-nomics, and policies (Byrne & Sakemoto, 2021; Nasir & Soliman, 2014; Sharif, Aloui, & Yarovaya, 2020). Irrational investor behaviors and information asymmetry further destabilize the market, causing drastic fluctuations in individual stock prices (Paule-Vianez, G\u00f3mez-Mart\u00ednez, & Prado-Rom\u00e1n, 2020).\nIn such a complex and dynamic environment, DRL faces three main challenges:\n\u2022 The stock market environment is always changing, but DRL relies on a stable environment to accumulate experience (Khader et al., 2021). Existing DRL models can hardly deal with high-dimensional, non-linear, and dynamic stock data. They usually use fixed-length information, which can lead to truncation or zeroing of the sequence, destroying the original time-series structure and potentially losing valuable historical data (Cao, Li, & Fair, 2019). In addition, basic DRL models have difficulty effectively learning and modeling the long-term dependen-cies present in stock time series data, which makes it challenging to understand underlying market trends, and the profitability of portfolio strategies is compromised (Gershman, Blei, &\nNiv, 2010; Hu, Zhao, & Khushi, 2021).\n\u2022 In the context of the stock market, company information, policy environment, investment psychology, etc. in the stock market are changing constantly, making the training of DRL models challenging. The frequent emergence of new information in the stock market leads to the continuous need for adaptation of prices and trading strategies. However, typical DRL is inefficient in adapting to new environments, making it difficult to train a stable and reliable strategy (Dayan & Niv, 2008; Gupta, Singal, &\nGarg, 2021). This is manifested in the difficulty of convergence of the training process and the eventual under-training that leads to poor strat-egy results (W. Zhang, Chen, Yan, Zhang, & Xu, 2021).\n\u2022 There are various types of short-term market disturbances and noisy trades in the stock market, and these disturbances flood the data, interfering with the DRL algorithm's identifi-cation of effective signals. Normal fluctuations and \"Black swan\" incidents can expose DRL susceptible to overfitting problems, resulting in strategies that excel at short-term stage but have poor generalization ability (Ale, Hartford, & Slater, 2020; Song, Jiang, Tu, Du, &"}, {"title": "4.2 MIGT Framework", "content": "To address these challenges, we propose the MIGT Framework (Figure 3), as the neural network for PPO input. The input to the portfolio management model is historical data in a high-dimensional tensor with historical period t, stock assets number n and and historical data f comprising price and technical indicators. Each period t is a trading day, so the historical time dimension of the input tensor is the number of periods t, which is the number of trading days. The stock assets dimension defines the number of stocks n. The input data first passes through a standard FC layer, followed by memory trajectory processing. Subsequently, the tensor is processed by the Gated Instance Attention module (Figure 4). The next module is a Position-wise Multilayer Perceptron (PW-MLP) with normalization and fan-in layer. Finally, the data from the PW-MLP are fed into the Logits Multilayer Perceptron (Logits MLP)."}, {"title": "4.3 Gated Instance Attention", "content": "Figure 4 gives the structure of the Gated Instance Attention. The Transformer's self-attention mechanism can weigh each position in the input sequence to focus on the information of other posi-tions so that the multi-dimensional information in the stock history data can be processed (Vaswani et al., 2017). As given in Figure 5, the input to the scaled dot-product attention module consists of the query vector Q and key vector K of the dimension and the value vector V of the dimension dy (Vaswani et al., 2017). We compute the dot product of the query and all keys, dividing each key by the root dk, and use the SoftMax function to derive the weights of the values. In this network, we compute the attention function for a set of queries and store them in the query vector Q. The keys and values are also stored in a key vector K and a value vector V. We compute the output vector as (Vaswani et al., 2017):\n$Attention (Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_K}}) V$ (9)\nwhere \u221adk represents the scaling factor that prevents inflation in calculations from causing unstable values. The correlation between query and key is calculated by using nonlinear match-ing functions including dot product, which can deal with the nonlinear relationship and the complex dynamic stock market environment. The model receives all available data in a scaled dot-product attention layer, but it only accesses one representation space. We divide the model into multiple heads, forming multiple subspaces so that the model can learn relevant information in different representation subspaces. Through the multi-head mechanism, different features of multi-dimensional stock data can be learned from different subspaces, which helps to deal with high-dimensional inputs. In order to focus on the different parts of the input sequence, we use four heads to perform the attention computation simultaneously, without sharing parameters before each other and eventually stitching the results together (Vaswani et al., 2017). Simultaneous computation of multiple attentions can focus on different parts of the input, which can help to capture the key characteristic information of the stock time series to deal with changes in its dynamics.\nTransformers use layer normalization in Natural Language Processing and other fields after calculating self-attention and after the feedforward portion of each encoder block, which is usually applied to the same sample (Jurafsky & Martin, 2009; Vaswani et al., 2017). In contrast,"}, {"title": "4.4 PW-MLP, Logits MLP and\nMemory Trajectory", "content": "The PW-MLP retains the dimensionality of the inputs and outputs while increasing the expressiveness of the model. As given in Figure 6, it applies two standard FC layers to each input position individually (Dai et al., 2020). The input size of the first layer is attention dimension size 64, and the output dimension is PW-MLP dimension 32. The input and output sizes of the second layer are the opposite of that of the first layer, i.e., its input and output sizes are PW-MLP dimension, and attention dimension, respectively. This allows the PW-MLP to learn non-linear transformations at each position independently, which enhances the model's ability to capture complex relationships in the data, i.e., increasing the expressiveness of the model. The final layer used to output values is the Logits MLP in Figure 6, which consists of two FC layers (Goodfellow et al., 2014). The first layer has the same input dimension as the attention module, and the output outputs have the same size as the attention. The output of the final layer will be used by the PPO algorithm to update the model.\nThe basic attention operation does not explicitly consider the sequence order because it is reciprocally invariant (Vaswani et al., 2017). As such, we incorporate a novel recursive mechanism into the Transformer architecture to address the limitation of using fixed time length information. In each step of the recursion, the model uses the information of the current time step and the previous time steps to generate output, and the output of the previous step is used as the input of the next step, which continues to generate the output until the end of the sequence. To allow the model to fuse current input and historical memory to deepen its learning ability, we cache a predetermined length of old hidden states across multiple segments and refer to them as memory. In the memory trajec-tory scheme, there is an additional t-step memory tensor Mt(I, D), where I is memory inference and D refers to past memory, whose value is the attention dimension. Memory is used as an add-tional input to the attention layer during each training session. By using memory states as addi-tional input to the neural network, the attention mechanism can consider current and historical information, improving the model's inference and comprehension."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Data Selection", "content": "To assess the performance of the MIGT framework, we conduct experiments using historical data, using 30 stocks from the Dow Jones Industrial Average (DJIA) (Dow Jones Industrial Aver-age, 2023) as the portfolio. We use three datasets of calendar years 2019, 2020, and 2021, as the test sets for Experiments 1, 2, and 3, respectively. The training set for each experiment is the data from the three years prior to their test set. The years of these datasets represent the pre-COVID-19, early, and mid-term COVID-19 years, respectively. The specific time composition of the training set and test set are given in Table 1."}, {"title": "5.2 Performance Measures", "content": "The most direct and effective way to evaluate portfolio management frameworks is to compare cumulative rates. We use the cumulative rate of return as the metric, i.e.,\n$Cumulative \\space rate \\space of \\space return = \\frac{P_T - P_0}{P_0}$ (13)"}, {"title": "5.3 Comparative Strategies", "content": "To assess the performance of our strategy, traditional statistical strategies and DRL strategies are used as comparative strategies. The traditional statistical strategies we use are based on mean reversion and trend following.\nThe mean reversion strategies (Fil & Kristoufek, 2020; Mousavi & Shen, 2021) aim to take advantage of stock price fluctuations, as stock prices constantly fluctuate over a certain period. The strategies based on mean reversion are:\n\u2022 Confidence Weighted Mean Reversion (CWMR) (Li, Hoi, Zhao, & Gopalkrishnan, 2011) models the portfolio vector as a Gaussian distribution and updates the distribution sequentially by following the mean reversion trading principle.\n\u2022 Online Moving Average Reversion (OLMAR) (Li & Hoi, 2012) uses multi-period moving average regression.\n\u2022 Passive Aggressive Mean Reversion (PAMR) (Li, Zhao, Hoi, & Gopalkrishnan, 2012) relies on the mean reversion relationship of financial markets and utilizes online passive-aggressive learning techniques in machine learning.\n\u2022 Robust Median Reversion (RMR) (Huang, Zhou, Li, Hoi, & Zhou, 2016) takes advantage of the mean reversion properties of financial markets and uses a technique called \"robust L1-median estimation\" to solve the outlier problem in mean reversion.\n\u2022 Transaction costs optimization (TCO) (Li, Wang, Huang, & Hoi, 2018) is a strategy for non-zero transaction costs that combines the L1 parametrization of the difference between two consecutive allocations with the principle of maximizing expected logarithmic returns.\n\u2022 Weighted Moving Average Mean Reversion (WMAMR) (L. Gao & Zhang, 2013) is calculated by taking a weighted moving average of stock prices to predict the upward or downward trend of stock prices.\nTrend-following strategies (Fousekis & Tzaferi, 2021; Takada & Kitajima, 2022), on the other hand, aim to take advantage of stock price trends. Strategies based on the trend following are:\n\u2022 The algorithm selects the asset with the best performance on the last day (BEST) (Jiang et al., 2017),\n\u2022 Nearest neighbor-based strategy (BNN) (Gy\u00f6rfi, Lugosi, & Udina, 2006) uses proximity to classify or predict groups of individual data points.\n\u2022 Correlation-driven nonparametric learning approach (CORN) (Li, Hoi, & Gopalkrishnan, 2011) is a correlation-based nonparametric learning approach that uses correlations to infer relationships between variables.\n\u0395\u0399\u03a0\u0395, IMIT, FinRL, ES, TradeMaster, and SARL are used in our experiments as DRL comparative frameworks. EIIE has performed well in the cryptocurrency portfolio management space (Jiang et al., 2017), and we have migrated their framework to the stock portfolio space and optimized it for the stock market. IMIT formalizes trading knowledge by mimicking investor behav-ior using a set of logical descriptors and introduces a Rank-Invest model that learns to optimize dif-ferent evaluation metrics to maintain the diversity of logical descriptors (Z.T.Y. Liu, 2018). FinRL is a well-structured and effective basic framework for automated trading using DRL (X.-Y. Liu et"}, {"title": "5.4 Comparative Results", "content": "The results of the experiments are given in Table 2, where the best results for each group are given in bold. The experimental results show that MIGT performs best in all three experiments. The MIGT outperforms the comparative strategies by at least 9.75%, suggesting that our strategy has a stronger ability to capture returns. In terms of the Sharpe ratio, our framework is at least 0.2072 higher than the comparative strategy, indicating that our framework can generate higher risk-adjusted returns per unit level of risk. The Sortino ratio of our strategy is at least 0.3858 higher than the comparative strategy, suggesting that our framework can generate higher excess returns per unit of downside risk. MIGT's Omega ratio remains the highest of the three experiments, with an mar-gin of at least 0.0286 in the experiment, although it has a slight advantage over the other metrics. This demonstrates that, for the same task and dataset, our framework has a higher probability of obtaining positive returns and stronger sustained profitability."}, {"title": "5.5 Ablation Study", "content": "Section 5.4 illustrates the performance of the MIGT framework. To analyze the impact of the individual modules on the effectiveness of our framework, we perform ablation experiments. The experiments provide a principled approach to understanding how and why the proposed framework works. To investigate the effective-ness of the Gated Instance Attention module, we modify it for the following version branches: the version with instance normalization removed (MIGT_w/o_Norm), the version with the gat-ing layer removed (MIGT_w/o_Gating), and the"}, {"title": "5.5.1 Overall ablation experiments\nwith cumulative returns", "content": "The results of the ablation experiments in Table 4 show that the return and ratio indicators for each group that underwent ablation are worse than MIGT. MIGT_w/o_Norm's impact on cumulative returns averaged -5.79%, which suggests that the use of instance normalization to mit-igate the impact of outliers has an obvious effect. However, this gap narrows to 1.61% in the experiments with 2021 data, suggesting that the outliers all have different degrees of impact on return capacity under different data sets. MIGT_w/o_Gating has a higher impact (-8.67%), where the cumulative return in the experiment for the dataset 1 was 30.14%, the difference with the full framework amounted to -8.39%. This suggests that improving the stability of DRL training enhances the portfolio strategy's profitability. MIGT_w/o-Transformer model reduced both the return and the risk-return ratio to the lowest levels in the ablation experiment. The three-year aver-age annual rate of return decreased by 22.76%. The strategy applying Attention and Memory Trajectory increases the return by an average of 5.25% compared to the basic reinforcement learn-ing framework. This shows that using the Gated Instance Attention module with a Transformer"}, {"title": "5.5.2 Ablation experiments for\ninstance normalization", "content": "We introduce pseudo-data into the training dataset to test the effectiveness of instance nor-malization in mitigating the impact of outliers. Figure 12 shows that when pseudo-data is not added, the average cumulative return of the MIGT"}, {"title": "5.5.3 Ablation experiments for LGU\nGating Layer", "content": "We conducted ablation experiments, as given in Figure 13, to determine if the LGU Gating Layer improves the training of DRL. The cumulative return of the MIGT strategy increases steadily with the increase of training steps, reaching a peak of 0.238166 at 10,000 steps, and then remains stable at about 0.2429. The cumulative return of the MIGT_w/o_Gating strategy also rises with the increase of training steps but with a much smaller increase. The cumulative return of the MIGT strategy is significantly higher than that of the MIGT_w/o_Gating strategy, especially at the early stage when the number of training steps is small, and the difference between the two strate-gies is even larger. This illustrates the effect of using the LGU Gating Layer in the MIGT strat-egy. At the later stage, when there are more training steps, the cumulative returns of the two strategies stabilize, and the growth slows down, indicating that the strategies have basically con-verged. However, MIGT can converge to a higher level of cumulative returns than the strategy with-out a gating layer, suggesting that using the LGU gating layer results in a faster and more efficient output of the strategy."}, {}]}