{"title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation", "authors": ["Daniel Fleischer", "Moshe Berchansky", "Moshe Wasserblat", "Peter Izsak"], "abstract": "Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG FOUNDRY, an open-source framework for augmenting large language models for RAG use cases. RAG FOUNDRY integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have emerged as a transformative force in the field of AI, demonstrating an impressive ability to perform a wide range of tasks that traditionally required human intelligence (Brown et al., 2020; Kojima et al., 2022). Despite their impressive capabilities, LLMs have inherent limitations. These models can produce plausible-sounding but incorrect or nonsensical answers, struggle with factual accuracy, lack access to up-to-date information after their training cutoff and struggle in attending to relevant information in large contexts (Huang et al., 2023; Liu et al., 2023).\nRetrieval-Augmented Generation (RAG) enhances LLMs performance by integrating external information using retrieval mechanisms. Combining retrieval that leverages vast knowledge-bases outside the knowledge of the model, effectively addresses knowledge limitations, can reduce hallucinations, improve the relevance of generated content, provide interpretability and could be vastly more cost-efficient (Lewis et al., 2021; Mallen et al., 2022; Gao et al., 2023; Asai et al., 2023; Borgeaud et al., 2021; Peng et al., 2023; de Jong et al., 2023). Furthermore, recent research indicates that fine-tuning LLMs for RAG can achieve state-of-the-art performance, surpassing that of larger, proprietary models (Yu et al., 2024b; Liu et al., 2024).\nHowever, the implementation of RAG systems is inherently complex and requires a series of intricate decisions that can significantly impact the performance of the system. This process de-"}, {"title": "2 Related Work", "content": "There are numerous open-source tools related to the different aspects of RAG, namely inference, training and evaluation. LlamaIndex (Liu, 2022), LangChain (Chase, 2022) and Haystack (Pietsch et al., 2019) are well known libraries for composing RAG pipelines; however they are not focused on evaluation and their training capability is underdeveloped.\nHoshi et al. (2023) proposes a framework for developing RAG-based LLMs; while our processing may be similar in the sense of being comprised of custom individual steps, they do not introduce any form of training. Khattab et al. (2023, 2022) presents a different approach, where LLM prompting is represented as a programming language, to be optimized and compiled; a rather unique and general approach that could benefit RAG but has a high level of complexity due to the abstractions introduced. Saad-Falcon et al. (2024) focuses more on the evaluation aspect, by creating synthetic data and training an LLM critic to evaluate the RAG system. Hsia et al. (2024) studies aspects of retrieval on the performance of RAG; our RAG Foundry library is general and enables experimentation on all aspects of RAG: retrieval, text-processing, prompt design, model selection, inference and evaluations.\nRecently, a concurrent work by Jin et al. (2024) proposes a RAG building framework, including some RAG implementations and datasets; we focus on extensibility, letting users define custom types of pipelines with custom components. Rau et al. (2024) presents a framework, sharing a similar design-principle of extensibility-through-configuration as ours; their library imposes a specific workflow structure (retriever, ranker, LLM) while our library is more general and does not imposes any specific paradigm."}, {"title": "3 RAG Foundry", "content": "The RAG FOUNDRY framework facilitates rapid prototyping and experimentation with various RAG settings and configurations. The library is composed of four modules: dataset creation, training,"}, {"title": "3.1 Data Creation and Processing", "content": "The processing module facilitates the creation of context-enhanced datasets by persisting RAG interactions, which are essential for RAG-oriented training and inference (Berchansky et al., 2024; Liu et al., 2024; Yu et al., 2024b). These interactions encompass dataset loading, column normalization, data aggregation, information retrieval, template-based prompt creation, and various other forms of pre-processing. The processed data can be saved in a consistent, model-independent format, along with all associated metadata, ensuring compatibility and reproducibility across different models and experiments.\nThe processing module is comprised of an abstract pipeline with multiple steps, each defined by Python classes that implement specific data processing functionalities. These steps are categorized into two types:\n\u2022 Global Steps: Can act on the dataset as a whole, making them useful for operations such as aggregations, group-by, examples filtering, join operations, and more.\n\u2022 Local Steps: Operate on individual examples, making them suitable for tasks such as retrieval, text processing, and field manipulation.\nThe modular design allows for building flexible and efficient data processes, tailored to the needs of RAG-oriented training and inference. Steps can be categorized into the following non-exclusive categories:\n\u2022 Loaders: Load datasets from the Hugging Face\u00b9 hub or from local sources.\n\u2022 Selectors: Filter examples, shuffle datasets, and select subset datasets.\n\u2022 Retrievers: Integrate information from external databases, tools, libraries and pipelines.\n\u2022 Samplers: Collect random examples or features from any dataset to compile few-shot or negative examples.\n\u2022 Prompters: Format prompts using custom templates and keyword mappings.\nThe processing module supports the handling of multiple datasets at once, through global dataset sharing. This feature allows each step of the pipeline to access any of the loaded datasets, enhancing flexibility and allowing for complex processing procedures. Furthermore, the module includes step caching, which caches each pipeline step locally. This improves compute efficiency, and facilitates easy reproduction of results."}, {"title": "3.1.1 Example: Enhancing a Q&A Dataset", "content": "To showcase the effectiveness of the processing module, we demonstrate how to enrich a question-answering dataset with external informa-"}, {"title": "3.2 Training", "content": "We provide a training module to fine-tune models given the datasets created by the previous processing module. The training module relies on the well established training framework TRL2 and sup-"}, {"title": "3.3 Inference", "content": "The inference module generates predictions given the processed datasets created by the processing module. Inference is conceptually separated from the evaluation step, since it is more computationally demanding than evaluation. Additionally, one can run multiple evaluations on a single, prepared inference results file."}, {"title": "3.4 Evaluation", "content": "The goal of the framework is augmenting LLMs for RAG. The evaluation module allows users to run collections of metrics to evaluate RAG techniques and tuning processes. The evaluation module loads the output of the inference module and runs a configurable list of metrics. Metrics are classes implemented in the library. These classes can be as simple as wrappers around other evaluation libraries, or can be implemented by the user. Local metrics can be run on individual examples, like Exact Match (EM), while Global metrics run on the entire dataset as a whole, e.g. Recall (for classification-based metrics). Metrics can use any field and metadata in the dataset, not just the input-output pairs. Some of the metrics implemented in the library include: a wrapper for the Hugging Face evaluate library, EM, F1, classification metrics, BERTScore (Zhang et al., 2019), Semantic Similarity and a wrapper for DeepEval (for using"}, {"title": "4 Experiments: RAG Tuning", "content": "To illustrate the usage and usefulness of the RAG FOUNDRY library, we experiment with several possible RAG improvements to LLMs, and evaluate the results on three knowledge-intensive tasks."}, {"title": "4.1 RAG Augmentation Techniques", "content": "We explore several techniques for RAG augmentation, and use RAG FOUNDRY to easily implement and evaluate their benefit. As an initial step, we evaluate unmodified models; we set Baseline as a configuration that is defined by running unmodified models and without any external knowledge. We define a RAG setting that introduces top-relevant documents in a consistent prompt template format with a system instruction, and a CoT scheme which guides the model to use the retrieved context, explain the steps, quote relevant parts and produce a final answer. Complementing that, we explore fine-tuning recipes. We fine-tune the model in the RAG setup and denote is as RAG-sft. To complement CoT, we implemented a fine-tuning recipe, denoted as CoT-sft, introduced in (Zhang et al., 2024), where gold documents and purely distractor documents are used in the prompt, determined by probability, in conjunction with a CoT prompt. All prompt templates are included in appendix A.1."}, {"title": "4.2 Datasets", "content": "We evaluate our models on TriviaQA (Joshi et al., 2017), PubmedQA (Jin et al., 2019), and ASQA (Stelmakh et al., 2022) which are knowledge intensive question-answering datasets which benefit from external sources. The TriviaQA and PubmedQA datasets contain relevant context; for ASQA, retrieval was done over a Wikipedia corpus using a dense retriever4. Dataset sources and sizes are included in appendix A.2."}, {"title": "4.3 Models", "content": "We experiment with two representative models: Llama-35 (Touvron et al., 2023; AI@Meta, 2024) and Phi-36 (Abdin et al., 2024) as they represent robust capabilities and are ideal candidate models for RAG use case deployments."}, {"title": "4.4 Evaluation", "content": "We measure and report Exact Match (EM) for TriviaQA, STR-EM for ASQA, accuracy and F1 for PubmedQA. Additionally, we evaluate two RAGAS metrics (Es et al., 2024): Faithfulness and Relevancy. Faithfulness measures the relation between the generated text and the context. Relevancy measures the relation between the generated text and the query. These two metrics use the context as input for the LLM critic, so are only relevant in the RAG settings. The critic LLM used is GPT4-32k, version 0613. An embedder is required for the relevancy evaluation."}, {"title": "4.5 Results", "content": "We present a comparative study of RAG augmentation techniques, on the TriviaQA, ASQA and PubmedQA datasets. Results are presented in table 1:"}, {"title": "5 Conclusion", "content": "We introduced RAG FOUNDRY, an open-source library dedicated to the task of RAG-augmentation of LLMs, namely fine-tuning LLMs to become better at RAG settings. The library is designed to serve as an end-to-end experimentation environment, enabling users to quickly prototype and experiment with different RAG techniques. We demonstrated the usefulness of the library by augmenting two models with RAG configurations, evaluating on three Q&A datasets and showing the benefit of RAG techniques, as well as of using multi-aspect metrics relevant for RAG systems evaluation."}, {"title": "Limitations and Future Plans", "content": "Our hope is that the library will be useful to as many people and use-cases as possible. However, due to time and resource constraint, we were able to demonstrate its usefulness on a subset of tasks and datasets. Future work can expand the evaluation to other tasks, as well as implementing other RAG techniques and evaluations.\nAlthough we designed the library to be general and customizable, there might be specific workflows which will be difficult to run as-is and some code changes may be required. The library proved useful for our own research projects on a diverse set of datasets and tasks and extending it is easy and straightforward.\nFinally, despite our best efforts to offer detailed documentation in the library, there could be some missing details regarding some functionality or specific use-cases. The code repository will accept suggestions, bug-fixes and pull requests."}, {"title": "Ethics Statement", "content": "In conducting our research we strive abiding to the highest ethical standards, including integrity, fairness, and societal benefit of our work. We prioritized data privacy and security throughout our research; any data used in our experiments was publicly available and did not contain any private information. We are committed to the principles of transparency and reproducibility; the methodologies, including data pre-processing, model training, and evaluation are documented in order to enable others to replicate our findings. Code is made available in an open repository. We advocate for the responsible use of LLMs and RAG augmentation. It is essential to exercise caution and verify the accuracy and reliability of generated text produced by LLMs. Hallucinations can have negative implications, and even when RAG methods can ameliorate some of these aspects, verification and inspections are needed."}, {"title": "A Implementation Details", "content": "A.1 Prompts\nYou are a helpful question answerer who can provide an answer given a question and relevant context.\nListing 5: System instruction used in the experiments.\nQuestion: {query}\nContext: {docs}\nListing 6: Template for inserting relevant documents as context.\nQuestion: {query}\nContext: {docs}\nAnswer this question using the information given in the context above. Here is things to pay attention to:\nFirst provide step-by-step reasoning on how to answer the question.\nIn the reasoning, if you need to copy paste some sentences from the context, include them in ##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and ##end_quote## are not directly copy paste from the context.\nEnd your response with final answer in the form <ANSWER>: $answer, the answer should be succinct.\nListing 7: Template for Chain-of-Thought reasoning.\nA.2 Datasets\nDatasets used:\n\u2022 TriviaQA\n\u2022 ASQA\n\u2022 PubmedQA\nContext size was k\n5, unless indicated otherwise.\nDataset sizes are:\nDataset\nTraining\nEvaluation\nTriviaQA\n6000\n1000\nASQA\n4353\n948\nPubmedQA\n10000\n500\nA.3 Training Details\nParameter\nValue\nLORA r\n16\nLORA a\n16\nLORA Dropout\n0.1\nLORA Bias\nNone\nLORA Modules\nqkv_proj, Phi-3\nq/v_proj, Llama-3\nLR\n1e-4\nLR Scheduler\ncosine\nWarmup Ratio\n0.03\nWeight Decay\n0.001\nBatch Size\n1\nEpochs\n1"}]}