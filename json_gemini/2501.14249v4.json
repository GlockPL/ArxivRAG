{"title": "Humanity's Last Exam", "authors": ["Long Phan", "Alice Gatti", "Ziwen Han", "Nathaniel Li", "Josephina Hu", "Hugh Zhang", "Chen Bo Calvin Zhang", "Mohamed Shaaban", "John Ling", "Sean Shi", "Michael Choi", "Anish Agrawal", "Arnav Chopra", "Adam Khoja", "Ryan Kim", "Richard Ren", "Jason Hausenloy", "Oliver Zhang", "Mantas Mazeika", "Summer Yue", "Alexandr Wang", "Dan Hendrycks"], "abstract": "Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce HUMANITY'S LAST EXAM (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 2,700 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.", "sections": [{"title": "1 Introduction", "content": "The capabilities of large language models (LLMs) have progressed dramatically, exceeding human performance across a diverse array of tasks. To systematically measure these capabilities, LLMs are evaluated upon benchmarks: collections of questions which assess model performance on tasks such as math, programming, or biology. However, state-of-the-art LLMs [3, 14, 16, 34, 37, 49, 56] now achieve over 90% accuracy on popular benchmarks such as MMLU [21], which were once challenging frontiers for LLMs. The saturation of existing benchmarks, as shown in Figure 1, limits our ability to precisely measure AI capabilities and calls for more challenging evaluations that can meaningfully assess the rapid improvements in LLM capabilities at the frontiers of human knowledge.\nTo address this gap, we introduce HUMANITY'S LAST EXAM (HLE), a benchmark of 2,700 extremely challenging questions from dozens of subject areas, designed to be the final closed-ended benchmark of broad academic capabilities. HLE is developed by academics and domain experts, providing a precise measure of capabilities as LLMs continue to improve (Section 3.1). HLE is multi-modal, featuring questions that are either text-only or accompanied by an image reference, and includes both multiple-choice and exact-match questions for automated answer verification. Questions are original, precise, unambiguous, and resistant to simple internet lookup or database retrieval. Amongst the diversity of questions in the benchmark, HLE emphasizes world-class mathematics problems aimed at testing deep reasoning skills broadly applicable across multiple academic areas.\nWe employ a multi-stage review process to thoroughly ensure question difficulty and quality (Section 3.2). Before submission, each question is tested against state-of-the-art LLMs to verify its difficulty - questions are rejected if LLMs can answer them correctly. Questions submitted then proceed through a two-stage reviewing process: (1) an initial feedback round with multiple graduate-level reviewers and (2) organizer and expert reviewer approval, ensuring quality and adherence to our submission criteria. Following release, we plan to further conduct a public review period, welcoming community feedback to correct any points of concern in the dataset.\nFrontier LLMs consistently demonstrate low accuracy (less than 10%) across all models, highlighting a significant gap between current capabilities and expert-level academic performance (Section 4). Models also provide incorrect answers with high confidence rather than acknowledging uncertainty on these challenging questions, with RMS calibration errors above 80% across all models.\nAs AI systems approach human expert performance in many domains, precise measurement of their capabilities and limitations is essential for informing research, governance, and the broader public. High performance on HLE would suggest expert-level capabilities on closed-ended academic questions. To establish a common reference point for assessing these capabilities, we publicly release a large number of 2,700 questions from HLE to enable this precise measurement, while maintaining a private test set to assess potential model overfitting."}, {"title": "2 Related Work", "content": "LLM Benchmarks. Benchmarks are important tools for tracking the rapid advancement of LLM capabilities, including scientific [10, 12, 21, 29, 30, 44, 47, 53, 61] and mathematical reasoning [13, 17-19, 22, 31, 45, 50], code generation [6, 9-11, 20, 26, 60], and general-purpose human assistance [1, 7, 8, 25, 40, 42, 43, 47, 54]. Due to their objectivity and ease of automated scoring at scale, evaluations commonly include multiple-choice and short-answer questions [15, 42, 51, 52, 58], with benchmarks such as MMLU [21] also spanning a broad range of academic disciplines and levels of complexity.\nSaturation and Frontier Benchmark Design. However, state-of-the-art models now achieve nearly perfect scores on many existing evaluations [3, 14, 16, 34, 37, 49, 56], obscuring the full extent of current and future frontier AI capabilities [27, 32, 38, 39]. This has motivated the development of more challenging benchmarks which test for multi-modal capabilities [2, 10, 26, 28, 31, 46, 48, 53, 57, 59], strengthen existing benchmarks [24, 43, 45, 48, 53], filter questions over multiple stages of review [18, 27, 30, 33, 44], and employ experts to write tests for advanced academic knowledge [5, 18, 30, 34, 41, 44]. HLE combines these approaches: the questions are developed by subject-matter experts and undergo multiple rounds of review, while preserving the broad subject-matter coverage of MMLU. As a result, HLE provides a clear measurement of the gap between current AI capabilities and human expertise on closed-ended academic tasks, complementing other assessments of advanced capabilities in open-ended domains [10, 35, 36, 55]."}, {"title": "3 Dataset", "content": "HUMANITY'S LAST EXAM (HLE) consists of 2,700 challenging questions across over a hundred subjects. A high level summary is provided in Figure 3. We publicly release these questions, while maintaining a private test set of held out questions to assess model overfitting."}, {"title": "3.1 Collection", "content": "HLE is a global collaborative effort, with questions from nearly 1000 subject expert contributors affiliated with over 500 institutions across 50 countries \u2013 comprised mostly of professors, researchers, and graduate degree holders."}, {"title": "Mathematics", "content": "The set of natural transformations between two functors F,G: CD can be expressed as the end  Nat(F,G) A Homp(F(A), G(A)). Define set of natural cotransformations from F to G to be the coend  CoNat(F,G) = 1=  A Homp (F(A), G(A)).Let:\n - F = B. (24)*/ be the under-category of the nerve of the delooping of the symmetric group 24 on 4 letters under the unique 0 -simplex * of B.\u03a34.\n - G = \u0392. (27)*/ be the under-category nerve of the delooping of the symmetric group 7 on 7 letters under the unique 0-simplex * of \u0392. \u03a37. \nHow many natural cotransformations are there between F and G?"}, {"title": "Chemistry", "content": "The reaction shown is a thermal pericyclic cascade that converts the starting heptaene into endiandric acid B methyl ester. The cascade involves three steps: two electrocyclizations followed by a cycloaddition. What types of electrocyclizations are involved in step 1 and step 2, and what type of cycloaddition is involved in step 3?\nProvide your answer for the electrocyclizations in the form of [\u03b7\u03c0]-con or [\u03b7\u03c0]-dis (where n is the number of \u3160 electrons involved, and whether it is conrotatory or disrotatory), and your answer for the cycloaddition in the form of [m+n] (where m and n are the number of atoms on each component)."}, {"title": "A Linguistics", "content": "I am providing the standardized Biblical Hebrew source text from the Biblia Hebraica Stuttgartensia (Psalms 104:7). Your task is to distinguish between closed and open syllables. Please identify and list all closed syllables (ending in a consonant sound) based on the latest research on the Tiberian pronunciation tradition of Biblical Hebrew by scholars such as Geoffrey Khan, Aaron D. Hornkohl, Kim Phillips, and Benjamin Suchard. Medieval sources, such as the Karaite transcription manuscripts, have enabled modern researchers to better understand specific aspects of Biblical Hebrew pronunciation in the Tiberian tradition, including the qualities and functions of the shewa and which letters were pronounced as consonants at the ends of syllables.\n?(104:7 Psalms) \u05de\u05b4\u05df\u05be\u05d2\u05b7\u05bc\u05e2\u05b2\u05e8\u05b8\u05ea\u05b0\u05da\u05b8 \u05d9\u05b0\u05e0\u05d5\u05bc\u05e1\u05b7\u05d5\u05bc\u05df \u05de\u05b4\u05df\u05be\u05e7\u05b8\u05d5\u05b9\u05dc \u05e8\u05b8\u05e2\u05b7\u05de\u05b0\u05da\u05b8 \u05d9\u05b5\u05d7\u05b8\u05e4\u05b5\u05d6\u05b0\u05d5\u05bc\u05df"}, {"title": "Accuracy", "content": "All frontier models achieve low accuracy on HLE (Table 1), highlighting significant room for improvement in narrowing the gap between current LLMs and expert-level academic capabilities on closed-ended questions. These low scores are partially by design \u2013 the dataset collection process (Section 3.1) attempts to filter out questions that existing models can answer correctly. Nevertheless, we notice upon evaluation, models exhibit non-zero accuracy. This is due to inherent noise in model inference \u2013 models can inconsistently guess the right answer or guess worse than random chance for multiple choice questions. We choose to leave these questions in the dataset as a natural component instead of strongly adversarially filtering. However, we stress the true capability floor of frontier models on the dataset will remain an open question and small inflections close to zero accuracy are not strongly indicative of progress."}, {"title": "Calibration Error", "content": "Given low performance on HLE, models should be calibrated, recognizing their uncertainty rather than confidently provide incorrect answers, indicative of confabulation/hallucination. To measure calibration, we prompt models to provide both an answer and their confidence from 0% to 100% (Appendix C.1.1), employing the setup from Wei et al. [54]. The implementation of"}, {"title": "5 Discussion", "content": "Future Model Performance. While current LLMs achieve very low accuracy on HLE, recent history shows benchmarks are quickly saturated \u2013 with models dramatically progressing from near-zero to near-perfect performance in a short timeframe [12, 44]. Given the rapid pace of AI development, it is plausible that models could exceed 50% accuracy on HLE by the end of 2025. High accuracy on HLE would demonstrate expert-level performance on closed-ended, verifiable questions and cutting-edge scientific knowledge, but it would not alone suggest autonomous research capabilities or \u201cartificial general intelligence.\u201d HLE tests structured academic problems rather than open-ended research or creative problem-solving abilities, making it a focused measure of technical knowledge and reasoning. HLE may be the last academic exam we need to give to models, but it is far from the last benchmark for A\u0399.\nImpact. By providing a clear measure of AI progress, HLE creates a common reference point for scientists and policymakers to assess AI capabilities. This enables more informed discussions about development trajectories, potential risks, and necessary governance measures."}, {"title": "C.1 Prompts", "content": "We use the following system prompt to judge the model answers against the correct answers for our evaluations in Table 1. We used 03-mini-2025-01-31 with structured decoding enabled to get an extracted_final_answer, reasoning, correct, confidence extraction for each output."}]}