{"title": "Detecting the Undetectable: Combining Kolmogorov-Arnold Networks and MLP for AI-Generated Image Detection", "authors": ["Taharim Rahman Anon", "Jakaria Islam Emon"], "abstract": "As artificial intelligence progresses, the task of distinguishing between real and AI-generated images is increasingly complicated by sophisticated generative models. This paper presents a novel detection framework adept at robustly identifying images produced by cutting-edge generative AI models, such as DALL-E 3, MidJourney, and Stable Diffusion 3. We introduce a comprehensive dataset, tailored to include images from these advanced generators, which serves as the foundation for extensive evaluation. we propose a classification system that integrates semantic image embeddings with a traditional Multilayer Perceptron (MLP). This baseline system is designed to effectively differentiate between real and AI-generated images under various challenging conditions. Enhancing this approach, we introduce a hybrid architecture that combines Kolmogorov-Arnold Networks (KAN) with the MLP. This hybrid model leverages the adaptive, high-resolution feature transformation capabilities of KAN, enabling our system to capture and analyze complex patterns in AI-generated images that are typically overlooked by conventional models. In out-of-distribution testing, our proposed model consistently outperformed the standard MLP across three out of distribution test datasets, demonstrating superior performance and robustness in classifying real images from AI-generated images with impressive F1 scores.", "sections": [{"title": "I. INTRODUCTION", "content": "THE he rapid advancement in artificial intelligence (AI) has led to significant progress in image generation technologies, resulting in highly realistic synthetic images [1]. While advancements in image generation [6] technologies bring numerous benefits, they also present significant challenges in misinformation and digital forensics [2]. Maintaining the integrity of visual media relies crucially on the ability to differentiate between AI-generated and real images. Lu et al. [3] underscore this challenge, revealing in their study significant difficulties even among human observers. Utilizing a large-scale fake image dataset named Fake2M and conducting a human perception evaluation called HPBench, they found a notable misclassification rate of 38.7%, indicating that humans struggle considerably to distinguish real photos from AI-generated ones.\nSeveral studies have focused on developing methods to detect AI-generated images, each with its own strengths and limitations. Epstein et al. [4] evaluate classifiers trained incrementally on 14 AI generative models arranged by release date, to discern real from AI-generated images. The study highlights the classifiers' generalization capabilities to newer AI models but notes a limitation: advance generative models like DALL-E and MidJourney, especially those without public APIs, are not included. This exclusion leads to observed performance drops with major architectural changes in emerging models, emphasizing the need for continual updates and retraining of these classifiers.\nChen et al. present a novel detection method using noise patterns from a single simple patch (SSP) to distinguish between real and AI-generated images, capitalizing on the noise discrepancies that AI-generated images often overlook [5]. Their methodology includes the SSP Network, which extracts and analyzes noise patterns using SRM filters and a ResNet50 classifier. Despite these advancements, the study notes significant challenges with very low-quality images, where performance deteriorates, particularly with images having compression quality below 90. However, a notable limitation, as observed, is the exclusion of advanced generative models in the datasets used, which might impact the method's applicability to newer or more sophisticated AI image generators.\nWang et al. proposed DIRE for detecting diffusion-generated images, demonstrating strong generalization and robustness, but noted the computational cost of their inversion and reconstruction process [7]. Yan et al. developed the Chameleon dataset to challenge existing detectors and proposed the AIDE model, which showed competitive performance but highlighted ongoing detection challenges and higher computational costs [9]. Martin-Rodriguez et al. used Photo Response Non-Uniformity (PRNU) and Error Level Analysis for pixel-wise feature extraction combined with CNNs, achieving high accuracy but noted the limitations with JPEG format dependency and potential manipulation of PRNU [10]. Chai et al. focused on local artifacts for detection, showing strong generalization but acknowledged preprocessing artifacts and adversarial vulnerability [11]. Park et al. provided a comprehensive comparison and visualization of various detection methods, demonstrating the strengths and weaknesses of each, particularly their robustness to augmentations like JPEG compression and Gaussian blur [12].\nBuilding upon the existing research landscape, this study introduces a robust approach to tackle the challenge of generator variance effectively distinguishing between images generated by diverse AI technologies. Our method not only addresses common limitations found in previous works, such as the inability to handle newer or more sophisticated generative models, but also demonstrates enhanced adaptability across a wide range of image qualities and generator architectures. The key contributions of our work are detailed as follows:\n\u2022 We have compiled a dataset that includes images generated by the latest models, such as DALL-E 3 [15], MidJourney 6, and Stable Diffusion 3 Ultra [14]. This dataset is crucial for developing a model that is well-versed in the nuances of the most recent AI image generators, ensuring robust detection capabilities across a spectrum of generative technologies.\n\u2022 We proposed a classification system as a baseline approach that integrates semantic image embeddings with a MLP. This combination is specially designed to effectively differentiate between real and AI-generated images, even under challenging conditions such as low image quality or subtle image features.\n\u2022 We proposed a hybride architecture that seamlessly integrates Kolmogorov-Arnold Networks (KAN) with a traditional MLP to further enhance the classification efficacy. This approach leverages the unique capabilities of KAN for adaptive, high-resolution feature transformation, allowing our system to capture and analyze intricate patterns and complexities in AI-generated images that are often missed by conventional models.\n\u2022 In out-of-distribution testing, proposed model consistently outperformed the standard MLP model across three test datasets: Real vs. DALL-E 3, Real vs. Midjourney 5, and Real vs. Adobe Firefly. The F1-Scores for the Hybrid KAN MLP were 0.94, 0.94, and 0.91, respectively, demonstrating superior performance and robustness in discriminating real from AI-generated images."}, {"title": "II. DATA PREPARATION", "content": "To develop our AI-generated image detection framework, we utilize a subset of the RAISE dataset [13]. Designed specifically for digital forgery detection algorithms, the RAISE dataset consists of 8,156 high-resolution, uncompressed RAW images that are guaranteed to be camera-native. This characteristic ensures no prior processing has altered the images, making it ideal for authenticity testing."}, {"title": "A. Data Generation for Training", "content": "To create a robust model for distinguishing real from AI-generated images, we implemented an extensive data generation process utilizing paid image generation services to address the research gap in generated image detection tasks. Our approach involved the following steps as outlined in figure 1. For this study, we selected 1000 images from the RAISE dataset, referred to as RAISE 1k. This subset provides a diverse range of real-world imagery, enabling robust testing of our detection framework under varied conditions. For each real image, we generated a detailed description using a large language model (OpenAI's GPT-4.) These descriptions encapsulate the key visual elements and context of the images. The generated descriptions were split into three folds and used to create synthetic images through three advanced image generation models: Stable Diffusion 3 Ultra (SD 3 Ultra), DALL-E 3, and MidJourney 6. Each model adds its unique characteristics to the generated images, enhancing the diversity of the dataset as shown in figure 2. After that real images and the AI-generated images were combined to form a comprehensive dataset. This process ensures our model is exposed to a balanced mix of real and synthetic images during training, as summarized in Table I."}, {"title": "B. Out of Distribution Test Data for Model Validation", "content": "To ensure the robustness and reliability of our model, particularly its ability to generalize beyond the training data, we have incorporated a comprehensive Out of Distribution (OOD) test dataset. This dataset is specifically designed to challenge the model with new, unseen examples that were not present during the training phase. The purpose of this OOD test data is to evaluate the model's invariant classifications its ability to maintain consistent and accurate predictions across varying inputs that deviate from the typical distribution of the training set. A set of 500 real images collected randomly provides a diverse basis for testing. These images ensure that the model can effectively recognize authentic visuals that have not been previously encountered during training. To test the model's ability to detect newly emerging generative models, we include 500 images generated by Adobe Firefly, a different generative model from those used in training. This tests the adaptability of our detection framework. Images generated by DALL-E 3 and MidJourney 5, amounting to 1000 images (500 each), serve to further diversify the test conditions. This inclusion helps ascertain the model's performance against a variety of generative techniques it has seen but under new parameters. This strategy enhances our model's ability to generalize and provides a stringent test of its effectiveness across a broad spectrum of real and synthetic images. The OOD test dataset plays a critical role in validating the model under varied and challenging conditions."}, {"title": "III. METHODOLOGY", "content": "Our methodology is designed within a comprehensive framework aimed at effectively distinguishing real images from Al-generated ones. This structured approach comprises four critical stages: data generation, embedding generation, classifier training, and rigorous model evaluation. Initially, we create a dataset with a combination of real images and generated images by leveraging a large language model and image generation models. This dataset forms the foundation for generating semantic image embeddings, which capture the essential visual and contextual information of the images. These embeddings serve as the input for training two types of classifiers: a standard MLP as our baseline and our proposed Hybrid KAN-MLP. We then undertake a thorough evaluation of these classifiers using out-of-distribution data. This evaluation focuses on key metrics such as the AUC ROC, F1 score, precision, and recall to assess the classifiers' performance comprehensively. Our methodology not only highlights the potential applications of these classifiers in real-world scenarios but also ensures a robust analysis of their effectiveness in distinguishing between real and generated image."}, {"title": "A. Semantic Image Embedding Generation", "content": "In our study, image embeddings [8] play a pivotal role in distinguishing between real and AI-generated images. An image embedding is a compact, high-dimensional vector representation of an image, denoted as:\n$X \\in R^{512}$                                                                                                                                 (1)\nwhere x represents the embedding vector with a dimensionality of 512. This vector transforms the raw visual content of the image its colors, textures, shapes, and semantic meanings into a numerical format that deep learning algorithms can process. The process of generating these embeddings can be mathematically represented as:\n$x = f_{CLIP}(image)$                                                                                                                       (2)\nwhere $f_{CLIP}$ denotes the embedding generation function implemented by the CLIP model, which maps an input image to its corresponding embedding in a structured vector space. This transformation captures the essence of the image's visual information, allowing for efficient comparison and analysis of images on a scale that is not feasible with raw pixel data."}, {"title": "B. Designing of the Hybrid KAN-MLP Classifier", "content": "The proposed Hybrid KAN-MLP Classifier represents a synthesis of the adaptive feature transformation of Kilimon-grove Arnold Networks (KAN) and the structural integrity of a MLP. This innovative classifier is anchored by the KANLinear module, a cornerstone of our approach, which utilizes advanced spline-based techniques for precise and high-resolution feature mapping. This module not only enhances the model's ability to capture and analyze complex nonlinear relationships in the data but also significantly boosts its accuracy and generalizability across varied imaging contexts. The mathematical representation of the transformation function applied by the KANLinear module is given by:\n$f(x) = \\sum_{i=1}^{N} w_{i} B_{i}(x, \\theta),$                                                                                                                                                                                                                                                                                                                                                                (3)\nwhere $B_{i}(x, \\theta)$ are adaptive spline basis functions, parameterized by $\\theta$, and $w_i$ are the learned weights.\nThe spline basis functions within the KANLinear module are constructed using spline interpolation, mathematically defined as follows:\n$B_{i}(x, \\theta) = \\sum_{j} \\theta_{ij} spline\\_basis(x, grid, order),$                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (4)\nwhere spline_basis denotes the spline basis function applied at points defined by grid; with a spline order specified by order. Here, the grid and spline order are configured to 10 and 3, respectively, to balance flexibility and robustness, enabling the model to fit a variety of data patterns without overfitting. A uniform grid is established across the input feature space, ranging from -1 to 1. The transformation across this grid involves a small epsilon value to ensure smooth transitions:\n$\\epsilon = min(\\Delta grid), \\  where \\Delta grid = grid_{i+1} - grid_i$.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (5)\nFurthermore, the module incorporates both base and spline weights for linear and nonlinear transformations, respectively, enhancing adaptability to diverse input distributions. Initial transformations use a Sigmoid Linear Unit (SiLU) for nonlinear activation:\n$SiLU(x) = \\frac{x}{1+ e^{-x}}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (6)\nRegularization within the module is managed through a 50% dropout rate, and batch normalization is utilized to stabilize the learning process:\n$BatchNorm(x) = \\gamma \\frac{(x - \\mu)}{\\sigma} + \\beta,$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (7)\nwhere $\\mu$ and $\\sigma$ are the mean and standard deviation of the batch, and $\\gamma$, $\\beta$ are learnable parameters.\nAfter the KANLinear transformation, the classifier structure includes a fully connected layer that maps transformed features to the output space, accompanied by a ReLU activation to introduce additional non-linearity. A final sigmoid activation function produces output probabilities suitable for binary classification tasks:\n$\\sigma(z) = \\frac{1}{1+e^{-z}}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (8)"}, {"title": "C. MLP Classifier", "content": "In addition to our hybrid model, we employ a standalone MLP classifier as a comparative baseline within our study. The MLP is designed with multiple dense layers, formulated to learn and classify images directly from the embeddings generated by the CLIP model. The architecture of the MLP can be mathematically represented as follows:\n$h_1 = \\sigma(W_1x + b_1)$                                                                                                                                                            (9)\n$h_2 = \\sigma(W_2h_1 + b_2)$                                                                                                                                                           (10)\n$\n                                                                                                                                     (11)\n$y = softmax(W_nh_{n-1}+b_n)$                                                                                                                                                         (12)\nWhere:\n\u2022 x is the input embedding vector derived from the CLIP model.\n\u2022 $h_1$ represents the hidden layer outputs.\n\u2022 $W_i$ and $b_i$ are the weights and biases of the $i^{th}$ layer, respectively.\n\u2022 $\\sigma$ denotes the activation function, typically ReLU (Rectified Linear Unit) for intermediate layers.\n\u2022 softmax is the activation function used in the output layer to produce the probability distribution over the classes.\nThe simplicity of the MLP structure not only allows us to validate the added value of the more complex Hybrid KAN-MLP model but also provides a straightforward and effective benchmark for performance comparison. This approach allows us to highlight the capability of the hybrid model in handling more intricate image data complexities, underlining the enhancements brought about by the integration of KAN techniques within the neural network architecture."}, {"title": "D. Training", "content": "To train the both classifier, we utilize binary cross-entropy as the loss function. The Adam optimizer is chosen for its robustness in managing sparse gradients and optimizing the training process, particularly when dealing with the non-linear transformations and high-dimensional feature space."}, {"title": "E. Evaluation Metrics", "content": "To assess the performance of our trained model, we employ a variety of metrics that provide insights into different aspects of model accuracy and effectiveness. These metrics are crucial for understanding how well the model discriminates between real and AI-generated images, ensuring robustness and reliability in practical applications. Below, we detail the specific metrics used to evaluate our model:\n\u2022 Confusion Matrix: The confusion matrix is used to describe the performance of the classification model on a set of test data for which the true values are known. It includes the values for true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN), giving a comprehensive understanding of the model's performance across different classes. The matrix can be represented as:\n$ \\left(  \\begin{array}{cc}  TP  & FP\\\\ FN  & TN \\end{array} \\right)$                                                                                                                                                                                                                                                                              (13)\nwhere the rows represent the actual classes and the columns represent the predicted classes.\n\u2022 Precision and Recall: Precision measures the proportion of true positive predictions among all positive predictions made by the model, while recall (sensitivity) measures the proportion of true positives identified out of all actual positives. These metrics are calculated as follows:\n$Precision = \\frac{TP}{TP+FP}$                                                                                                                                                                                                                                                                                                                         (14)\n$Recall \\frac{TP}{TP + FN}$                                                                                                                                                                                                                         (15)\n\u2022 F1-Score: The F1-Score is the harmonic mean of precision and recall, providing a balance between the two metrics. It is calculated as:\n$F1-Score = 2x \\frac{Precision \\times Recall}{Precision + Recall}$                                                                                                                                                                                                                                                                                                                                                                                                                                                            (16)\n\u2022 ROC Curve and AUC: The Receiver Operating Characteristic (ROC) curve is a graphical plot used to illustrate the diagnostic ability of a binary classifier as its discrimination threshold is varied. The curve is created by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. The TPR and FPR are defined as:\n$TPR= \\frac{TP}{TP + FN}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (17)\n$FPR= \\frac{FP}{FP+TN}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (18)\nThe Area Under the Curve (AUC) of the ROC represents the measure of the ability of the classifier to discriminate between the classes and is used as a summary of the ROC curve. The higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. An AUC of 0.5 suggests no discrimination ability (random guessing), while an AUC of 1.0 indicates perfect discrimination."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "In this section, we evaluate the performance of the Proposed Hybrid KAN-MLP and the Baseline MLP classifiers across three challenging Out of Distribution (OOD) datasets-Real vs. DALL-E 3, Real vs. Midjourney 5, and Real vs. Adobe Firefly. Each dataset consists of 1000 images, split evenly between real and AI-generated images, providing a comprehensive basis for assessing the robustness of our classifiers against OOD test data. Precision, recall, and F1-scores for each condition are tabulated in Table IV, demonstrating the quantitative outcomes of our experiments. These metrics indicate the high level of accuracy achieved by the Proposed Hybrid KAN-MLP classifier, particularly in terms of F1-score, which is crucial for balancing the trade-off between precision and recall. Notably, the Proposed classifier consistently outperforms the Baseline across all datasets, affirming the effectiveness of the KANLinear module in enhancing feature discriminability and classification reliability. The results from these evaluations are visually represented through confusion matrices and summarized in terms of precision, recall, and F1-score, which are key metrics for classifier performance assessment. The confusion matrices for each classifier and dataset combination are illustrated in Figure 4, which depicts the classifiers' ability to distinguish between real and generated images under varying conditions.\nAdditionally, the AUC ROC curves presented in figure 5 further substantiate the robust discriminatory power of our model. The curves indicate a significant separation between the true positive rate and false positive rate, underscoring the model's efficiency in classification tasks under challenging conditions imposed by high-quality AI-generated images.Overall, the results corroborate the Hybrid KAN-MLP's potential as a formidable tool in fields requiring precise image validation, such as digital forensics and media integrity. The comparative analysis with the baseline also emphasizes the advancements incorporated through the KAN architecture, which significantly enhance the classifier's performance and adaptability. Future work will focus on scaling these approaches to accommodate larger datasets and integrating real-time processing capabilities, ensuring that the model remains effective against evolving generative image technologies."}, {"title": "V. CONCLUSION", "content": "This study has successfully demonstrated the efficacy of the Hybrid KAN MLP model in distinguishing between real and AI-generated images, a crucial capability in the era of advanced digital image synthesis technologies. The robust performance of our classifier across diverse datasets underscores its potential utility in critical applications such as digital forensics and media integrity verification. The significant capability of our model to accurately classify images supports its potential integration into systems aimed at combating digital misinformation and ensuring the credibility of visual content in digital media. This is particularly relevant in today's context, where the authenticity of digital content is frequently questioned.\nDespite its strong performance across various datasets featuring the latest AI-generated models like DALL-E 3, Mid-Journey 6, and Stable Diffusion 3 Ultra, our model faces a notable challenge: the high cost of using advanced AI services limits our ability to gather large datasets. This limitation affects our ability to capture a wide range of behaviors from different AI models, making it important to manage dataset size carefully without compromising the model's effectiveness. This is crucial as we plan to scale up the assessment of our classifier. The model's potential in areas like digital forensics, media verification, and content moderation remains significant. These fields rely on the accuracy of image analysis to maintain the trustworthiness of digital content. Moving forward, we aim to improve how our model handles large datasets cost-effectively and efficiently, preparing it to adapt quickly to new Al technologies while staying reliable and accurate."}]}