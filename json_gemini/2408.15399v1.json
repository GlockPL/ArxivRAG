{"title": "A Statistical Framework for Data-dependent Retrieval-Augmented Models", "authors": ["Soumya Basu", "Ankit Singh Rawat", "Manzil Zaheer"], "abstract": "Modern ML systems increasingly augment input instances with additional relevant information to enhance final prediction. Despite growing interest in such retrieval-augmented models, their fundamental properties and training are not well understood. We propose a statistical framework to study such models with two components: 1) a retriever to identify the relevant information out of a large corpus via a data-dependent metric; and 2) a predictor that consumes the input instances along with the retrieved information to make the final predictions. We present a principled method for end-to-end training of both components and draw connections with various training approaches in the literature. Furthermore, we establish excess risk bounds for retrieval-augmented models while delineating the contributions of both retriever and predictor towards the model performance. We validate the utility of our proposed training methods along with the key takeaways from our statistical analysis on open domain question answering task where retrieval augmentation is important.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in machine learning (ML) have not only led to breakthroughs on long-standing challenging tasks across various fields, but they have also inspired a great deal of interest to develop ML models that can solve even harder tasks [Meinhardt et al., 2022, Lewkowycz et al., 2022, Cramer, 2021] or focus on completely new fields [Austin et al., 2021, OpenAI, 2023, Singhal et al., 2023]. While scaling the size of parametric ML models, such as neural networks, is becoming the predominant approach to meet such demands [Brown et al., 2020, Chowdhery et al., 2022, Touvron et al., 2023, Dosovitskiy et al., 2021, Dehghani et al., 2023], the excellent performance realized by this approach is marred by drawbacks such as high computational cost, inefficient storage of world knowledge in parameters, lack of transparency in model behavior, and reduced grounding/factuality of model predictions.\nRecognizing these shortcomings, retrieval-augmented models (RAMs) have emerged as a promising alternative. Such models typically employ two components, namely retriever and predictor, during inference on a given input instance: The retriever first identifies instance-specific relevant information from a data-store, and then the predictor jointly processes the retrieved information and the input instance to make a final prediction. In practice, RAMs have enjoyed favorable performance vs. compute trade-off [Borgeaud et al., 2021, Das et al., 2021, Thai et al., 2023] as employing moderate-size parametric models as retriever and predictor in a RAM often matches or exceeds the performance of a much larger standalone ML model that directly maps input instances to predictions. Similarly, conditioning prediction on the retrieved information has shown to exhibit improved grounding [Shuster et al., 2021, Lin et al., 2023, Asai et al., 2023]. Furthermore, having access to an external corpus can obviate the need to store task-specific world knowledge in model parameters and enable incorporating dynamically evolving knowledge [Izacard et al., 2022, Liska et al., 2022].\nDespite these desirable characteristics, training RAMs presents multiple challenges. The natural approach of independently training retriever and predictor can be sub-optimal [Izacard et al., 2022]. Moreover, it requires collecting intermediate supervision on the instance-dependent relevant information to retrieve, which"}, {"title": "2 Problem setup", "content": "In this paper, we focus on developing a systematic understanding of RAMs with learned retrievers in a classification setting where the model has access to a data-store. Towards this, we begin by formally defining the problem setup and providing the necessary background along with the notations used.\nLet's first consider the standard classification setting which requires predicting a class in y for a given instance x \u2208 X. Assume that \\(D_{XY}\\) captures the underlying data distribution and one has access to n training examples \\(S_n = \\{(x_i, Y_i)\\}i\\in[n]\\) that are independent and identically distributed (i.i.d.) according to \\(D_{XY}\\). Given \\(S_n\\), one hopes to learn a classifier \\(f : X \\rightarrow R^{|Y|}\\) that minimizes the miss-classification error:\n\\begin{equation}\nR(f) = P_{(X,Y)\\sim D_{XY}} [\\mathop{\\text{arg max}} \\limits_{Y\\in Y} f_y (X) \\neq Y],\n\\end{equation}\nwhere \\(f_y(x)\\) denotes the score that f assigns to the y-th class, given the input instance x. Since directly optimizing the miss-classification error or 0/1-loss poses computational challenges, one typically selects the classifier that minimizes the empirical risk associated with a well behaved surrogate loss function"}, {"title": "3 Joint training and excess risk", "content": "Recall that training a RAM involves training both the retriever \\(r_{\\theta}: X\\times Z \\rightarrow \\mathbb{R}\\) and the predictor \\(h_{\\xi}: X\\times I\\rightarrow \\mathbb{R}^{|Y|}\\) components of the model without access to intermediate supervision on retrieval, which is infeasible to obtain in most practical settings. Thus, it becomes critical to devise methods to jointly train \\(r_{\\theta}\\) and \\(h_{\\xi}\\) with access to only labeled instances \\(S_n = \\{(X_i, Y_i)\\}i\\in[n] \\subseteq X \\times Y\\) with the predictor guiding the retriever training based on how valuable the retriever-provided evidences are towards the correct final prediction.\nTowards this, we leverage the empirical risk from (8) along with the log-loss \\(l(h_{\\xi}(x, z), y) = - \\log p_{\\xi}(y|x, z)\\), where \\(p_{\\xi}(y|x, z)\\) is defined in (7). In particular, this leads to the following joint end-to-end training objective:\n\\begin{equation}\n\\mathcal{L}_n(f,\\theta;\\mathcal{J}) = R_{\\text{log},I,n}(\\xi, \\theta) = -\\frac{1}{n} \\sum_{i\\in[n]} \\sum_{z\\in\\mathcal{I}} P_{\\theta, \\mathcal{I}}(z|x) \\cdot \\log P_{\\xi} (Y_i|x_i, z).\n\\end{equation}\nNote that the objective in (13) aims to improve the end-to-end performance of a RAM in deployment in the sense that the objective aims to minimize the expected loss given the selected evidences as per the retriever-induced distribution. One can use gradient-based methods to jointly minimize the objective in (13) with respect to (\\(\\xi, \\theta\\)); however, its efficient implementation is non-trivial due to the sum over entire data-store \\(\\mathcal{J}\\). In App. C.1, we discuss some approximate design choices. Lastly, please refer to Sec. 3.6 for connections between our proposed objective in (13) and some of the existing end-to-end training approaches for RAMs.\nNext, to study the generalization and expressive power of RAMs, we want to bound the excess risk \\(\\Delta_{\\xi,\\theta}(\\xi, \\theta)\\) as defined in (12). We consider \\(X\\) to be a compact subspace of \\(\\mathbb{R}^d\\) and, for simplicity, take"}, {"title": "5 Discussion and related work", "content": "Several works have proposed some form of retrieval augmented models. Here, we provide a brief account of the evolution of RAMs and discuss how our proposed joint-learning objective and the framework for excess risk analysis compare with existing end-to-end training methods.\nAugment with local neighborhood The first approaches dating back to 1970s employed just augmenting training instance in the local neighborhood of the input space [Stone, 1977, 1980]. Such approaches gained a lot of attention as parametric regression was not adequate in various practical applications of the time. This line of work aims to fit a low-degree polynomial at each point in the data set based on a subset of data points, which resulted in a rich literature on local polynomial regression in low dimensions [Katkovnik and Kheisin, 1979, Cleveland, 1979, Pinsker, 1980, Donoho and Liu, 1988, Ruppert and Wand, 1994, Ibragimov and Has Minskii, 2013]. These classical ideas have found their application in many ML algorithms such as face recognition [Jain and Learned-Miller, 2011], dimensionality reduction via local linear embeddings [Roweis and Saul, 2000], domain adaptation [Yang et al., 2021], test time training on neighboring points [Sun et al., 2020, Gandelsman et al., 2022], etc. Recently, Basu et al. [2023] generalized this setup of augmenting with a local neighborhood of the input instance in the context of modern ML models like neural networks and proposed a statistical framework to study such retrieval-based models. However, they do not consider a learned or a specialized distance metric to find the augmenting set, which is critical for realizing good performance in practice [Schonberger et al., 2017, Karpukhin et al., 2020b] and studied in the present work.\nFixed retriever augmentation Next generation retrieval augmented models started to deploy either a hand crafted or a learned retriever. Zhang et al. [2006] employed SIFT [Lowe, 1999] based retrieval followed by a SVM [Cortes and Vapnik, 1995] classifier to improve performance on multiple vision tasks. Chen et al. [2009] studied generalization bounds for SVM-kNN methods one of the limited works in this domain with formal analysis. For natural language understanding, methods like TF-IDF [Sparck Jones, 1972] were employed in the tasks like case based reasoning [Leake et al., 1996] and open-domain question answering (ODQA; Voorhees et al. 1999). Unlike many previous methods, one retrieves relevant text passages in ODQA settings as opposed to retrieving labelled training pairs. With introduction of transformers [Vaswani et al., 2017], both retriever and predictor models based on encoder and decoder, respectively, have become popular across various domains, including image classification [Long et al., 2022, Iscen et al., 2023], text classification [Wang et al., 2022,"}, {"title": "6 Conclusion", "content": "In this work, we initiate the development of a theoretical framework to study the statistical properties of RAMs with data-dependent retrieval. Our excess-risks analysis allows us to highlight how retriever and predictor components play complementary roles in reducing approximation error as we increase their respective function class complexity. We surface both theoretically and empirically a Pareto surface achieving the same performance with different size predictors and retrievers. As future work, it would be interesting to study the effect of dynamically updatable data-store and multi-step retrievals for making predictions."}, {"title": "A Preliminaries", "content": "Definition A.1 (Rademacher complexity). Given a sample \\(S_n = \\{(X_i, Y_i)\\}i\\in[n] \\subset X \\times Y\\) and a real-valued function class \\(F : X \\times Y \\rightarrow \\mathbb{R}\\), the empirical Rademacher complexity of \\(F\\) with respect to \\(S_n\\) is defined as\n\\begin{equation}\n\\Re_{S_n}(F) = \\frac{1}{n} \\mathbb{E}_{\\sigma} \\Big[\\mathop{\\text{sup}}\\limits_{f\\in F} \\sum\\limits_{i=1}^n \\sigma_i f(x_i, Y_i)\\Big]\n\\end{equation}\nwhere \\(\\sigma = \\{\\sigma_i\\}i\\in[n]\\) is a collection of n i.i.d. Bernoulli random variables. For \\(n \\in \\mathbb{N}\\), the Rademacher complexity \\(R_n(F)\\) and worst case Rademacher complexity \\(\\hat{R}_n(F)\\) are defined as follows.\n\\begin{equation}\nR_n (F) = \\mathbb{E}_{S \\sim D_n} [\\Re_{S_n} (F)], \\quad and \\quad \\hat{R}_n(F) = \\mathop{\\text{sup}}\\limits_{S_n\\sim(X\\times Y)^n} \\Re_{S_n} (F).\n\\end{equation}\nDefinition A.2 (Covering number). Let \\(\\epsilon > 0\\) and \\(\\|\\cdot\\|\\) be a norm defined over \\(\\mathbb{R}^n\\). Given a function class \\(F : X \\times Y \\rightarrow \\mathbb{R}\\) and a collection of points \\(S_n = \\{(x_i, Y_i)\\}i\\in[n] \\subset X \\times Y\\), we call a set of points \\(\\{u_j\\}j\\in[m] \\subset \\mathbb{R}^n\\) an \\((\\epsilon, \\|\\cdot\\|)\\)-cover of \\(F\\) with respect to \\(S\\), if we have\n\\begin{equation}\n\\mathop{\\text{sup}}\\limits_{f\\in F} \\mathop{\\text{min}}\\limits_{j\\in[m]} \\| f(S_n) - u_j \\| \\leq \\epsilon,\n\\end{equation}\nwhere \\(f(S_n) = (f(x_1,y_1),..., f(x_n, Y_n)) \\in \\mathbb{R}^n\\). The \\(\\|\\cdot\\|\\)-covering number \\(N(F, \\epsilon, \\|\\cdot\\|; S_n)\\) denotes the cardinality of the minimal \\((\\epsilon, \\|\\cdot\\|)\\)-cover of \\(F\\) with respect to \\(S_n\\). In particular, if \\(\\|\\cdot\\|\\) is a \\(l_p\\) norm (e.g., \\(\\|\\nu\\|_2 = (\\sum_{j=1}^d |v_j|^p)^{1/p}\\) for \\(v \\in \\mathbb{R}^d\\)), then we simply use \\(N(F, \\epsilon, \\|\\cdot\\|_{l_p}; S_n)\\) to denote the corresponding \\(l_p\\)-covering number.\nWhen \\(S_n\\) is unambiguous we may drop it, i.e., we use \\(N(F, \\epsilon, \\|\\cdot\\|_{l_p})\\) to represent the covering number.\nDefinition A.3 (Multi-layer perceptron (MLP)). We consider for both retrieval and predictor, the class of multi-layer-perceptron, aka fully connected Deep Neural Network, with Relu nonlinearity \\(\\sigma(x) = \\text{max}(x, 0)\\). An MLP is specified by the number of layers \\(L\\), and the width \\(W\\). We define with weight \\(W\\in \\mathbb{R}^{d_2 \\times d_1}\\) and bias \\(b \\in \\mathbb{R}^{d_2}\\), an affine transform \\(A_{W,b}(\\mathbb{R}^{d_1}, \\mathbb{R}^{d_2}) : x \\rightarrow Wx + b\\). Let \\(\\sigma \\circ A_{W,b}(\\mathbb{R}^{d_1}, \\mathbb{R}^{d_2})\\) define the elementwise application of the Relu non-linearity on the affine transform. The class of \\(L\\) layers and \\(W\\) width MLP is defined as\n\\begin{equation}\nMLP(\\mathbb{R}^d, \\mathbb{R}^k; W, L) = \\{A_{W_L,b_L} \\circ \\sigma \\circ A_{W_{L-1},b_{L-1}} \\circ ... \\circ \\sigma \\circ A_{W_0,b_0}\\},\n\\end{equation}\nwhere \\(W_L \\in \\mathbb{R}^{k \\times W}\\) and \\(b_L \\in \\mathbb{R}^{k}\\); \\(W_i \\in \\mathbb{R}^{W \\times W}\\) and \\(b_i \\in \\mathbb{R}^{W}\\), for \\(1 \\leq i \\leq (L - 1)\\); and \\(W_0 \\in \\mathbb{R}^{W \\times d}\\) and \\(b_0 \\in \\mathbb{R}^{W}\\).\nDefinition A.4 (Sobolev space). For \\(p > 1\\), we denote the set of functions with finite \\(L_p\\) norm over \\(\\Omega\\) as \\(L_p(\\Omega)\\), i.e., for any \\(f \\in L_p(\\Omega)\\), \\(\\|f\\|_{L_p(\\Omega)} = (\\int_{\\Omega} |f(s)|^pds)^{1/p} < \\infty\\). Note that for \\(p = \\infty\\), we have \\(\\|f\\|_{L_{\\infty}(\\Omega)} = \\text{ess sup}_{s\\in\\Omega} |f(s)|\\). Let \\(\\alpha \\in \\mathbb{N}^d\\) denote a multi-index, and \\(|\\alpha| = \\sum_{i=1}^d \\alpha_i\\) be it's degree. We denote by \\(D^\\alpha\\) the weak-derivative with respect to multi-index \\(\\alpha\\) for any function.\nFor an integer \\(\\kappa > 0\\), the Sobolev semi-norm \\(W^\\kappa(L_p(\\Omega))\\) for a function f that has weak-derivatives of order \\(\\kappa\\) is defined as\n\\begin{split}\n\\forall 1 \\leq p < \\infty, |f|_{W^{\\kappa}(L_p(\\Omega))} = (\\sum_{\\alpha: |\\alpha| = \\kappa} \\| D^\\alpha f \\|_{L_p(\\Omega)}^p )^{1/p} and |f|_{W^{\\kappa}(L_{\\infty}(\\Omega))} = \\mathop{\\text{max}}\\limits_{\\alpha: |\\alpha| = \\kappa} \\| D^\\alpha f \\|_{L_{\\infty}(\\Omega)}.\n\\end{split}\nThe Sobolev norm \\(W^{\\kappa}(L_p(\\Omega))\\) for the same function f is defined as \\(\\|f\\|_{W^{\\kappa}(L_p(\\Omega))} = \\|f\\|_{L_p(\\Omega)} + |f|_{W^{\\kappa}(L_p(\\Omega))}\\). A function f with all weak-derivatives of order k, and a finite \\(W^{\\kappa}(L_p(\\Omega))\\) norm lies in the Sobolev space with \\(\\kappa\\) derivatives and \\(L_p(\\Omega)\\) norm.\nIn our approximation guarantees for MLP retreiver and predictor classes later, we use [Siegel, 2023, Theorem 1]. We restate the result here for completeness."}, {"title": "B Derivations of main result", "content": "As discussed in Section 2, the objective here is to study the excess risk in Eq. (12) which has three main components, generalization error, retriever approximation error, and predictor approximation error (cf. (15)). In this section, we structure our results somewhat differently than the main body to capture the general setting of learning retriever with a fixed predictor, and vice versa. We first prove excess risk bounds for learning the retriever, then excess risk bounds for learning the predictor. Finally, we combine the results to obtain the guarantees for jointly learning the retriever and the predictor presented in the paper. For the rest of the analysis we need to specify the space of retrieved examples to define the complexity of the gap function (cf. 3.1). We recall that our retrieved samples are embedded in a compact subspace of \\(\\mathbb{R}^{d_z}\\), and X is a compact subspace of \\(\\mathbb{R}^d\\). In particular, for simplicity, we assume that \\(X \\subseteq [-1,1]^{d_x}\\) and \\(Z \\subseteq [-1,1]^{d_z}\\).\nB.1 Learning the retriever\nWe first study learning the retriever over class \\(\\Theta\\) when the predictor \\(\\xi\\) is fixed. The task of learning the retriever corresponds to minimizing the following over \\(\\theta\\in\\Theta\\),\n\\begin{split}\n&\\mathbb{E}_{(X,Y)\\sim D} [\\mathbb{E}_{Z\\sim P_{\\theta}(\\cdot|X)}l(h_{\\xi}(X,Z), Y)] = \\mathbb{E}_{X} [\\mathbb{E}_{Z\\sim P_{\\theta}(\\cdot|X)} \\mathbb{E}_{Y|X}l(h_{\\xi}(X, Z), Y)|X]] \\\\\n&\\mathbb{E}_{X} [\\mathbb{E}_{Z\\sim P_{\\theta}(\\cdot|X)}g_{\\xi}(X,Z)],\n\\end{split}\nwhere \\(g_{\\xi}(X, Z) = \\mathbb{E}_{Y|X}l(h_{\\xi}(X, Z), Y)\\). We have a closed form for the optimal retriever when not restricted within a function class. The optimal retriever is \\(p^*_{\\xi}(z|x) = \\mathbb{1}_{\\arg \\min_{z'\\in\\mathcal{I}} g_{\\xi}(x,z')}(z)\\), where a tie is broken arbitrarily.\nFor the fixed predictor \\(\\xi\\), let \\(\\theta(\\xi)\\) minimize the empirical risk given, and \\(\\theta(\\xi)\\) minimize the population risk over the class \\(\\Theta\\), i.e.\n\\begin{equation}\n\\theta(\\xi) = \\arg \\min_{\\theta\\in\\Theta} \\frac{1}{n} \\sum_{i\\in[n]} \\sum_{z\\in \\mathcal{I}} P_{\\theta}(z|x_i)l(h_{\\xi}(x_i, z), Y_i),\n\\end{equation}\n\\begin{equation}\n\\theta(\\xi) = \\arg \\min_{\\theta\\in\\Theta} \\mathbb{E}_{X} [\\mathbb{E}_{Z\\sim P_{\\theta}(\\cdot|X)}g_{\\xi}(X,Z)].\n\\end{equation}\nHere, the probability is defined using the softmax operator for a given \\(\\theta\\in\\Theta\\) as follows:\n\\begin{equation}\nP_{\\theta,\\mathcal{I}} (z|x) = \\frac{\\exp (r_{\\theta}(x, z))}{\\sum_{z'\\in\\mathcal{I}} \\exp (r_{\\theta}(x, z'))},\\qquad \\forall z \\in \\mathcal{I}, x \\in X.\n\\end{equation}\nHardness of retrieval: We recall the Sobolev space with \\(\\kappa\\) derivatives as defined in Section A. The following is the restatement of Assumption 3.1 but for any \\(\\xi\\in \\Xi\\) and not just the optimal one \\(\\xi^*\\).\nAssumption B.1 (Complexity of \\(g_{\\xi}\\)). For any \\(\\xi\\in \\Xi\\), there exists a baseline \\(b_{\\xi}: [-1,1]^{d_x} \\rightarrow \\mathbb{R}\\) such that the function \\(gap_{\\xi}: [-1,1]^{d_x+d_z} \\rightarrow \\mathbb{R}\\) with baseline \\(b_{\\xi}\\), as defined by \\(gap_{\\xi}(x, z) = (g_{\\xi}(x, z) - b_{\\xi}(x))\\) lies in the Sobolev space with \\(\\kappa\\) derivatives and \\(L_{\\infty}([-1,1]^{d_x+d_z})\\) norm.\nAs noted in the main text this means that the predictor loss has a possibly \u2018complex' component \\(b_{\\xi}(x)\\), and a relatively \u2018smooth' component \\(gap_{\\xi}(x, z)\\) that ensures two retrieved examples that are close leads to similar loss for the predictor \\(\\xi\\) for any \\(x \\in X\\). As \\(gap_{\\xi}(x, z)\\) solely determines the optimal retrieved set, it's smoothness defines the hardness of underlying retrieval task.\nExcess risk decomposition: With the fixed predictor \\(\\xi\\), excess risk in (12) takes the following form"}, {"title": "C More experiments", "content": "C.1 Implementation details\nComputing the objective (13), let alone its gradient, requires evaluating the reader and predictor over the entire data-store \\(\\mathcal{J}\\) making it prohibitively expensive. We explore two ways to approximately compute the objective:\nTop-K approximation This approach involves constraining the summation to a specific subset. Periodically we compute \\(P_{\\theta}(z|x)\\) for all items \\(z \\in \\mathcal{I}\\) based on the current value of \\(\\theta\\). We use this to obtain a set of \\(K\\) documents \\(Z(x_i)\\) with the highest (stale) scores, i.e. \\(T_K(P_{\\theta}(\\cdot|x_i))\\) and evaluate the sum on this.\n\\begin{equation}\n\\mathcal{L}_{\\text{RCE+TopK}}(\\theta; \\mathcal{E}, \\mathcal{I}) = - \\frac{1}{n} \\sum_{i\\in[n]} \\sum_{z\\in Z(x_i)} P_{\\theta, \\mathcal{I}}(z|x) \\cdot \\log P_{\\xi} (Y_i|x_i, z)\n\\end{equation}\nThis methodology is akin to those adopted by EMDR2 and PDist, with the set being refreshed every 500 training steps and the selection of \\(K = 64\\).\nPolicy gradient Based on connection to RLHF/RLAIF, we propose to use policy gradient method [Sutton and Barto, 2018] to obtain an unbaised estimate of gradient with respect to \\(\\theta\\) efficiently. However, as"}]}