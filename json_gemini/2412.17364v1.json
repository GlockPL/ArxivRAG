{"title": "EFFICIENT FINE-TUNING METHODOLOGY OF TEXT EMBEDDING\nMODELS FOR INFORMATION RETRIEVAL: CONTRASTIVE\nLEARNING PENALTY (CLP)", "authors": ["Jeongsu Yu"], "abstract": "Text embedding models play a crucial role in natural language processing, particularly in information\nretrieval, and their importance is further highlighted with the recent utilization of RAG (Retrieval-\nAugmented Generation). This study presents an efficient fine-tuning methodology encompassing data\nselection, loss function, and model architecture to enhance the information retrieval performance of\npre-trained text embedding models. In particular, this study proposes a novel Contrastive Learning\nPenalty function that overcomes the limitations of existing Contrastive Learning. The proposed\nmethodology achieves significant performance improvements over existing methods in document\nretrieval tasks. This study is expected to contribute to improving the performance of information\nretrieval systems through fine-tuning of text embedding models. The code for this study can be found\nat https://github.com/CreaLabs/Enhanced-BGE-M3-with-CLP-and-MoE, and the best-performing\nmodel can be found at https://huggingface.co/CreaLabs.", "sections": [{"title": "Introduction", "content": "Text embedding models play a crucial role in natural language processing, particularly in information retrieval, by\nmapping text data into a semantically rich vector space. The importance of information retrieval has been further\nhighlighted with the recent utilization of RAG (Retrieval-Augmented Generation) (Lewis et al., 2020) to address the\nissues of hallucination and outdated information in large language models (LLMs). Pre-trained text embedding models\non a massive corpus have significantly improved the quality of text representation. BGE M3-Embedding (Chen et al.,\n2024) is a representative model that shows outstanding performance in multilingual text embedding and information\nretrieval.\nThis study proposes an efficient fine-tuning methodology to enhance the information retrieval performance of pre-trained\ntext embedding models by specializing them to a specific domain:\n1. Efficient Training Data Selection Technique: Applies ANCE (Approximate Nearest Neighbor Negative Contrastive\nEstimation) (Xiong et al., 2020) for selecting negative samples in the training data.\n2. Contrastive Learning Penalty (CLP): Analyzes the limitations of existing Contrastive Learning functions and proposes\na novel loss function to overcome them.\n3. Mixture of Experts (MoE) Technique (Gupta et al., 2022): Generates optimized embeddings based on the characteris-\ntics of the input text to effectively respond to various types of queries.\nTo validate the effectiveness of the proposed methodology, we evaluated its performance on document retrieval tasks in\nthree languages using the nDCG metric. The experimental results demonstrate that the proposed methodology achieves\nsignificant performance improvements compared to existing methods."}, {"title": "Related Work", "content": "In the past, keyword-matching based retrieval methods like BM25 were mainly used for document retrieval. BM25\ncalculates a similarity score considering term frequency, inverse document frequency, and document length (Robertson\net al., 1995; Crestani et al., 1998). Although BM25 is an efficient and widely used method, it has the following\nlimitations:\n1. Vocabulary mismatch: If different words with the same meaning are used in the query and the document, BM25 may\nnot accurately capture the relevance. For example, BM25 recognizes \"car\" and \"vehicle\" as different words even though\nthey have the same meaning.\n2. Difficulty in understanding meaning: Since BM25 only considers the frequency and distribution of words, it does not\ncapture the meaning or context of words. Therefore, it may have difficulty accurately matching the intent of the query\nwith the content of the document.\n3. Ignoring syntactic information: BM25 does not consider syntactic information such as word order or sentence\nstructure. Therefore, \"how to get from Seoul to Busan\" and \"how to get from Busan to Seoul\" can be treated the same\nin BM25.\nThese limitations can be overcome with the advent of text encoders, i.e., text embedding models, from pre-trained\nlanguage models (Reimers and Gurevych, 2019; Ni et al., 2021).\nDense Retrieval. Dense Retrieval (DR) using text embedding models enables search results that understand the\nmeaning of text, not just word matching. DR maps text to a high-dimensional vector space using deep learning and\nperforms search by measuring similarity based on distances in this vector space (Karpukhin et al., 2020).\nContrastive Learning. The learning of text embedding models for DR has been advanced by the advent of Contrastive\nLearning techniques (Hadsell et al., 2006; Chen et al., 2020; Gao et al., 2022). Contrastive Learning (CL) aims to\nlearn effective representations by pulling together semantically close neighbors and pushing away irrelevant ones. This\nassumes a dataset D = {(q_i, p_i^+, P)_{i=1}^N} composed of semantically related pairs, where q_i denotes a query, p_i^+ denotes a\npositive sample for q_i and P denotes a set of negative samples for q_i. Following the Contrastive Learning framework\nof Chen et al. (2020), we use a cross-entropy objective function with negative samples from within a mini-batch.\nSpecifically, when h_i, h_i^+, H' are the representations of q_i, p_i^+, P respectively, the training objective function on a\nmini-batch of N pairs is:\nL_{ITER} = -log \\frac{exp(sim(h_i, h_i^+)/\\tau)}{\\sum_{H \\in {h_i^+, H'}} exp(sim(h_i, H)/\\tau)}"}, {"content": "where \\tau is a temperature hyperparameter and sim(h_1, h_2) is the cosine similarity $\\frac{h_1\\cdot h_2}{||h_1||_2 ||h_2||_2}$.\nTo perform CL, we need to construct pairs D = {(q_i, p_i^+, P)_{i=1}^N}. In this process, effective negative sampling has a\nsignificant impact on learning performance. The ANCE (Xiong et al., 2020) study revealed that using uninformative\nnegative samples that are distant from the query leads to problems such as decreased gradient values, increased stochastic\ngradient variance, and slow learning convergence. Therefore, utilizing informative negative samples that are closer to\nthe query is more effective."}, {"title": "Fine-tuning Methods", "content": null}, {"title": "Contrastive Learning Penalty (CLP)", "content": "CL simply minimizes the distance between q_i and its semantically related counterpart $p_i^+$, while maximizing the distance\nto the unrelated $p^-$. However, this learning method does not consider the distance between H' and its semantically\nrelated queries, $Q^*$, where $Q^*$ is the set of positive queries for each document in $H'$. For efficient learning, while\nminimizing the distance between q_i and $p_i^+$ and maximizing the distance to $H'$, we need to ensure that this does not\nnegatively affect the distance between H' and $Q^*$. In this study, we propose a novel CLP to address the negative\nimpact on the distance between H' and $Q^*$ in existing CL.\nCLP imposes a penalty that increases as the distance between H' and $Q^*$ grows in the existing Contrastive Learning\nLoss function. The specific formula is as follows:"}, {"title": "Negative Sampling", "content": "In this study, we utilize the top 10 most similar samples from the dense retrieval results of the training corpus of the\nmodel being trained, excluding the positive sample, as negative samples, referring to the ANCE methodology. This\nprovides information-rich samples to induce effective learning and prevents problems caused by using uninformative\nnegative samples (Xiong et al., 2020)."}, {"title": "Mixture of Experts (MoE)", "content": "Existing methodologies utilized dense networks that use the same weights for all tasks, regardless of input characteristics.\nHowever, this caused a problem where inputs with different characteristics interfered with each other in model training.\nTo address this issue, Mixture of Experts (MoE) was introduced. MoE shares only some weights and trains the remaining\nweights according to the input characteristics. By assigning tasks to suitable experts for the input through a task-aware\ngating function that analyzes input characteristics, it improves transfer learning to low-resource tasks, enables efficient\ngeneralization, and prevents capacity loss in existing models (Gupta et al., 2022).\nThis study leverages the advantages of MoE by applying the MoE structure to the intermediate layer of the existing text\nembedding model. We froze and trained the remaining parameters excluding the MoE layer."}, {"title": "Experiment", "content": null}, {"title": "Evaluation Setup", "content": null}, {"title": "Metric", "content": "In this study, we fine-tuned the BGE M3-Embedding model for specific languages (Korean, Hindi, and Persian) using\nthe multilingual document retrieval dataset MIRACL (Table 1) and evaluated its performance. Korean is an agglutinative\nlanguage, Hindi is an inflectional language, and Persian belongs to the Indo-European language family. By selecting\nthese languages, we aimed to comprehensively analyze various language types.\nThe dense retrieval results of the fine-tuned models are evaluated using the nDCG metric. nDCG is an evaluation metric\nthat considers the ranking of search results, assigning higher scores when more relevant documents are ranked higher,\nand is widely used in the field of information retrieval. The BGE M3-Embedding model achieved higher nDCG scores\nthan previous models on MIRACL (Table 2)."}, {"title": "Training details", "content": "This model was trained on a single 3090 GPU with the following hyperparameters:\n\u2022 Learning rate: 1e-5\n\u2022 Mixed precision training (fp16)\n\u2022 Number of training epochs: Table 3\n\u2022 Per-device training batch size: 1\n\u2022 Gradient accumulation steps: 4\nFor CLP, the penalty weight was set to 0.1. For the MoE layers, the number of experts was set to 2 and the number of\nexperts per token was set to 1."}, {"title": "Performance Comparison", "content": null}, {"title": "Train Dataset", "content": "This study compared and analyzed negative sampling techniques in CL. Comparing the conventional method of random\nnegative sampling with the ANCE methodology, the following performance differences were observed:\n\u2022 random dataset: This method employed random negative sampling. This approach resulted in lower perfor-\nmance (52.92) compared to the baseline model (55.95). This is likely because random sampling has a higher\nchance of selecting negative samples with low information content, making it less effective for model training.\n\u2022 ANCE dataset: This method utilized the ANCE methodology for negative sampling. This approach achieved\nhigher performance (57.43) than the baseline model (55.95). It is analyzed that ANCE contributed to the\nperformance improvement by effectively selecting hard negative samples, which provide useful information\nfor model learning.\nThese experimental results show that the negative sampling technique in CL can significantly influence model perfor-\nmance. In particular, it was confirmed that a technique that effectively selects hard negative samples, such as ANCE, is\neffective in improving the performance of the model."}, {"title": "Loss Funtion", "content": "This study compared and analyzed the performance of the proposed CLP and existing CL. Applying CLP requires\npositive queries corresponding to negative samples, but the MIRACL dataset used in the experiment does not include\nthis information.\nTherefore, we utilized Gemini 1.5 Pro to generate synthetic data for positive queries. Specifically, we generated negative\nsample's positive query through the Gemini 1.5 Pro model using the prompt in Table 5.\nAs shown in Table 4, the experimental results demonstrate a performance improvement when applying CLP (ANCE-\nCLP) with a score of 58.06, compared to the existing CL (ANCE dataset) which scored 57.43. Notably, in the case of\nPersian, applying the existing CL resulted in a performance decrease (50.88) compared to the baseline model (51.13).\nHowever, applying CLP showed a significant performance improvement (52.39), demonstrating the superiority of CLP."}, {"title": "Model Architecture", "content": "To determine the optimal location for applying Mixture of Experts (MoE), we conducted an experiment where only\nthe intermediate layer of the model was trained, while keeping other layers frozen. Interestingly, training only\nthe intermediate layer (ANCE-CLP-intermediate) resulted in higher performance (59.45 -> 59.89) than training all\nparameters of the model (ANCE-CLP). In this experiment, the intermediate layer serves to expand the dimension from\n1024 to 4096.\nBased on these experimental results, we applied MoE to the intermediate layer (ANCE-CLP-moe-intermediate) while\nfreezing other parameters. This ultimately yielded the best performance of 59.89.\nHowever, applying MoE has the drawback of increased inference time. Embedding the MIRACL Korean corpus dataset\n(371,688 instances) took 3 hours and 7 minutes for the baseline model, while the MoE model required 5 hours and 1\nminute."}, {"title": "Conclusion", "content": "This study proposes an efficient fine-tuning methodology for specializing pre-trained text embedding models to a\nspecific domain, thereby enhancing information retrieval performance. We aimed to maximize model performance\nthroughout the entire fine-tuning process, from training data construction to loss function and model structure.\nOur key improvements include:\n1. Efficient Training Data Selection: We utilized ANCE to select informative negative samples, leading to more effective\nmodel learning."}, {"title": null, "content": "2. Contrastive Learning Penalty (CLP): We introduced a novel loss function, CLP, which addresses limitations of\nexisting contrastive learning by considering the relationship between negative samples and their corresponding positive\nqueries.\n3. Mixture of Experts (MoE) Application: We applied MoE to the intermediate layer of the text embedding model to\ngenerate optimized embeddings tailored to diverse input text characteristics.\nExperiments conducted on a multilingual document retrieval dataset (MIRACL) across Korean, Hindi, and Persian\ndemonstrated the effectiveness of the proposed methodology. By applying all proposed methods, we achieved a final\nperformance improvement of approximately 5 points compared to the baseline.\nThe main contributions of this study are as follows:\n\u2022 We demonstrate the effectiveness of combining ANCE, CLP, and MoE for fine-tuning pre-trained text\nembedding models in information retrieval tasks.\n\u2022 The newly proposed CLP significantly improved performance, especially for Persian, where conventional CL\nexhibited performance degradation.\nThis study presents a novel approach to fine-tuning text embedding models and is expected to contribute to improving\nthe performance of information retrieval systems. Future research will focus on validating the applicability of our\nmethod to various languages and domains and further enhancing the efficiency of CLP.\nThe code for this study is available at https://github.com/CreaLabs/Enhanced-BGE-M3-with-CLP-and-MoE, and the\nfinal model can be found at https://huggingface.co/CreaLabs."}]}