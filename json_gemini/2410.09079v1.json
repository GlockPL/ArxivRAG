{"title": "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models", "authors": ["Aofei Chang", "Jiaqi Wang", "Han Liu", "Parminder Bhatia", "Cao Xiao", "Ting Wang", "Fenglong Ma"], "abstract": "Parameter Efficient Fine-Tuning (PEFT) offers an efficient solution for fine-tuning large pretrained language models for downstream tasks. However, most PEFT strategies are manually designed, often resulting in suboptimal performance. Recent automatic PEFT approaches aim to address this issue but face challenges such as search space entanglement, inefficiency, and lack of integration between parameter budgets and search processes. To overcome these issues, we introduce a novel Budget-guided Iterative search strategy for automatic PEFT (BIPEFT), which significantly enhances search efficiency. BIPEFT employs a new iterative search strategy to disentangle the binary module and rank dimension search spaces. Additionally, we design early selection strategies based on parameter budgets, accelerating the learning process by gradually removing unimportant modules and fixing rank dimensions. Extensive experiments on public benchmarks demonstrate the superior performance of BIPEFT in achieving efficient and effective PEFT for downstream tasks with a low parameter budget.", "sections": [{"title": "1 Introduction", "content": "Large pre-trained models (PTMs) (Devlin et al., 2019; Radford et al., 2019) based on Transformer architectures (Vaswani et al., 2017) have achieved significant success across a variety of downstream tasks through fine-tuning, including applications of healthcare (Wang et al., 2024; Luo et al., 2024). However, the computational and storage demands of PTMs limit the feasibility of full fine-tuning. To address this, parameter-efficient fine-tuning (PEFT) methods have garnered considerable attention. Existing PEFT approaches have demonstrated superior performance on downstream tasks (Xu et al., 2023). However, most methods, such as adapters (Houlsby et al., 2019; Lin et al., 2020; R\u00fcckl\u00e9 et al., 2021), BitFit (Ben Zaken et al., 2022), and LoRA (Hu et al., 2022a; Zhang et al., 2023b; Zi et al., 2023; Zhang et al., 2023a; Dettmers et al., 2023) require manual design of fine-tuning strategies. Configuring PEFT for different layers of Transformers can result in varying performance outcomes.\nTo mitigate this issue, automatic PEFT approaches (Hu et al., 2022b; Zhou et al., 2024) have been proposed to automatically search for the optimal PEFT configuration. S\u00b3Delta (Hu et al., 2022b) is the first differential neural architecture search (NAS)-based PEFT approach, which automatically searches for the optimal modules to include in the configuration. AutoPEFT (Zhou et al., 2024) employs Bayesian optimization to conduct the PEFT search across a large space. Although these two automatic approaches are effective, they still suffer from the following issues:\n(1) Search Space Entanglement. As shown in Figure 1, the search space of S\u00b3Delta (Hu et al.,"}, {"title": "2 Related Work", "content": "Parameter Efficient Fine-Tuning (PEFT). Generally, PEFT is designed based on a Transformer architecture and only optimizes a small portion of parameters and leaves the vast majority of parameters frozen for efficient adaptation to downstream tasks. The PEFT methods can be broadly classified into four categories (Han et al., 2024): (1) Additive PEFT such as Adapter (Houlsby et al., 2019) and Prefix-Tuning (Li and Liang, 2021) inserts new trainable modules or parameters in the model. (2) Selective PEFT aims to optimize model performance by selectively fine-tuning a subset of the model's parameters by masking (Guo et al., 2021; Fu et al., 2023; Liao et al., 2023; Sung et al., 2021) or manual design (Ben Zaken et al., 2022). (3) Reparameterized PEFT like LoRA (Hu et al., 2022a), constructs a reparameterization of the original model parameters for training, then equivalently transforms it back at the inference stage (Zhang et al., 2023b; Luo et al., 2023). (4) Hybrid PEFT focuses on combining the benefits of diverse PEFT modules from the last three categories (Mao et al., 2021; Chen et al., 2023). A series of automated hybrid PEFT methods are developed to automatically search for an effective hybrid PEFT structure, and our work falls into this category.\nAutomated Configuration Search for PEFT. AutoPEFT (Zhou et al., 2024) using Bayesian optimization and S\u00b3Delta (Hu et al., 2022b) utilizing differential neural architecture search (NAS) techniques to search for optimal architectures for natural language processing (NLP) tasks. In contrast, PrunePEFT (Lawton et al., 2023) adopts a straightforward pruning approach to identify essential PEFT parameters. In the vision domain, NOAH (Zhang et al., 2022b) applies an evolutionary NAS strategy to tailor PEFT configurations specifically for Vision Transformer (Dosovitskiy et al., 2021). Despite these advancements, significant challenges remain in optimizing search space design and improving search efficiency.\nEarly Stopping in Neural Architecture Search. Existing early stopping strategies in neural architecture search address the overfitting issues on selecting many skip connections in convolutional neural networks (CNNs) when using methods like DARTS (Liu et al., 2019). OLES (Jiang et al., 2023) introduces an operation-level early stopping method to mitigate this issue. DARTS+ (Liang et al., 2019) implements a manual threshold for stopping the search. SGAS-es utilizes an indicator for stabilizing search results on CNNs. Notably, early stopping techniques have not yet been adapted for the PEFT search, which is characterized by faster convergence compared to CNNs."}, {"title": "3 Methodology", "content": "Our approach aims to automatically search for the optimal PEFT structure from N modules of LLMs through a novel budget-aware search strategy named BIPEFT. This strategy iteratively searches from two distinct spaces: a binary position search space and a dimension search space, with module and dimension selection mechanisms. As shown in Figure 2, at each training step t, BIPEFT will first optimize the architecture weights $\\Theta_t \\in \\mathbb{R}^{N \\times 2}$ by fixing the weights of $\\Phi_{t-1} \\in \\mathbb{R}^{N \\times K}$ for the dimension search space, where K is the number of dimension candidates. Subsequently, BIPEFT updates $\\Phi_t$ using the optimized $\\Theta_t$.\nAfter each training step, BIPEFT checks whether the budget-aware selection mechanisms are triggered according to module sensitivity scores and the targeted budget B detailed in \u00a7Sec. 3.2. If triggered, BIPEFT will first estimate the number of reduced parameters $R_z$ at this trigger stage and then discard unimportant modules according to the estimated $R_z$ with a designed module selection strategy in \u00a7Sec. 3.3. BIPEFT also selects an appropriate dimension size for certain modules in \u00a7Sec. 3.4. These selection operations lead to further updates $\\Theta_t$ and $\\Phi_t$. BIPEFT stops when the maximum trigger count Z is reached, ensuring that the model size approximates the targeted budget B. The search process of BIPEFT is outlined in Algorithm 1, with the details of the designed strategy discussed in the following subsections."}, {"title": "3.2 Budget-aware Trigger Generation", "content": "A naive solution of automatically searching for the optimal PEFT structure is to gradually reduce the number of parameters during model fine-tuning. However, if the fine-tuned model is unstable, we cannot decide which modules will be pruned. Thus, evaluating model stability is important. To achieve this goal, we design a new budget-aware model stability strategy according to module-level sensitive scores and the targeted budget B."}, {"title": "3.2.1 Module Sensitivity Estimation", "content": "In our setting, we apply the differential neural architecture search (NAS) for optimal PEFT configuration. If a module $M_n$ plays an important role in the PEFT model, it should be more stable and contribute greatly to the loss function. In the NAS-based model training, we have both training and validation data, denoted as $D_{tra}$ and $D_{val}$. The module sensitivity can be evaluated on these two kinds of data as follows:\n$\\begin{aligned}\ns_n &= f_\\theta(D_{tra}) + a_n f_\\theta(D_{val}),\\\\\nf_\\theta(D) &= \\frac{1}{|M_n|} \\sum_{w \\in M_n} |w \\cdot G(w, D)|, \\\\\na_n &= \\text{cos}(G_\\theta(D_{tra}), G_\\theta(D_{val})).\n\\end{aligned}$\nFollowing (Molchanov et al., 2019), we use $f_\\theta$ to calculate the parameter-level average magnitude of the gradient-weight product. $|M_n|$ is the number of fine-tuned parameters of the n-th module. $G(w)$ denotes the gradient of weight w. $a_n$ denotes the gradient cosine similarity of the module $M_n$ on both training and validation data.\nThe sensitivity score $s^t_n$ of each module can be measured after each training step t. To mitigate the impact of stochastic data batch sampling, we propose to smooth the sensitivity score using an exponential moving average following (Zhang et al., 2022a) as follows:\n$s_n^t = \\gamma s_n^{t-1} + (1-\\gamma) s_n^t,$\nwhere $\\gamma$ is a predefined hyperparameter."}, {"title": "3.2.2 Trigger Generation", "content": "Determining the optimal timing for starting parameter reduction is pivotal. To address this challenge, we propose an adaptive trigger generation approach to automatically estimate the optimal timing according to module sensitivity scores learned by Eq. (4) along with the targeted budget B.\nModule Importance Indicator. Ideally, the finally selected modules should be greatly yet continuously contributed to the model fine-tuning. Moreover, the total parameters of the finally selected modules should be close to the targeted budget B in our setting.\nBased on these motivations, we design a module importance indicator $I_t \\in \\{0,1\\}^N$ for each training step t to record the estimated importance of modules. Specifically, we initialize $I_t = 0$ and rank the module sensitivity scores in descending order. We accumulate the parameters from top-ranked modules one by one. If the current sum is smaller than the targeted budget B, we set the indicator value as 1 for that module. Otherwise, all the remaining indicator values are 0.\nTrigger. Intuitively, the model performs stably if the important modules change slightly within a time window H. We use an average cosine similarity between two consecutive module importance indicators within H steps to evaluate the model stability as follows:\n$\\beta_t = \\frac{1}{H} \\sum_{j=t-H}^t \\sum_{i=1}^N \\text{cos}(I_j, I_{j-1}),$\nwhere $\\beta_t \\geq \\tau$ means that the model is stable now, and the trigger can be generated, where $\\tau$ is a hyperparameter. After triggering the parameter reduction, BIPEFT then uses two strategies to reduce the number of training parameters, i.e., binary module selection and multiple dimension selection."}, {"title": "3.3 Binary Module Selection", "content": "The majority of parameters will be reduced in the binary module selection stage. In the design of BIPEFT, we aim to gradually obtain the optimal PEFT configuration after triggering the reduction Z times. Thus, estimating the number of parameter reductions at the trigger step z is essential. Let $E_t$ denote the expected number of parameters at the t-th training step when the trigger counter is z. We can estimate the expected number of reductions is\n$R_z = \\frac{E_t - B}{Z - z}.$\nHere, the challenge is how can we estimate $E_t$."}, {"title": "3.3.1 Expected Parameters Estimation", "content": "We use the current status of the PEFT model at the t-th training step (i.e., the z-th trigger counter) and the selection status at the $(z - 1)$-th trigger stage to estimate the expected parameters $E_t$. The PEFT model contains both architecture weights $\\Theta_t$ and $\\Phi_t$. The selection status at z 1 contains a binary module selection indicator denoted as $b_{z-1} \\in \\{0,1\\}^N$ and a module dimension selection indicator vector $d_{z-1} \\in \\{0,1\\}^N$. $b_{z-1} = 1$ means that the module $M_n$ is kept in the PEFT training. Otherwise, the module has been removed. $d_{z-1} = 1$ indicates that $M_n$ has a selected or fixed dimension value with index k, but $d_{z-1} = 0$ means the dimension is undetermined. Based on these notations, we can estimate $E_t$ as follows:\n$E_t = \\sum_{n=1}^N p_n \\sum_{k=1}^K q_n C_k^n,$\nwhere $p_n = b_{z-1} * \\text{softmax}(\\Theta)[1]$ is the probability of the kept module, and $C_k^n$ is the estimated number of parameters of each module, which is defined as follows:\n$C_k^n = \\begin{cases} q_n [k_n], & \\text{if } d_{z-1} = 1, \\\\q_n \\cdot \\text{softmax}(\\Phi), & \\text{if } d_{z-1} = 0, \\end{cases}$\nwhere $q_n \\in \\mathbb{R}^K$ is the parameter vector for all candidate dimensions, which can be precalculated. When the dimension is fixed, we use the corresponding parameter values directly. Otherwise, we use a weighted sum over the estimated probabilities."}, {"title": "3.3.2 Adaptive Module Selection", "content": "Using Eq. (7), we can obtain the expected reductions $R_z$. To prune unimportant modules, we still use the module-level sensitivity scores calculated by Eq. (4) at step t. We rank all sensitivity scores of the currently kept modules and remove the modules with the lowest scores if their total parameter size is smaller than $R_z$. Correspondingly, we update $b_{z-1}$ to obtain $b_z$ as the new module indicator."}, {"title": "3.4 Multiple Rank Dimension Selection", "content": "We can also determine the dimension size for each module $M_n$ if there is a clearly stable pattern on learned architecture weights $\\Phi$. Since evaluating the sensitivity score for each dimension as each module is hard, we propose a new strategy to measure the weight distributions between two trigger counters, z 1 and z. Note that there are several training steps between the trigger gap. Assume that at the j-th training step, BIPEFT triggers the reduction z 1, and at t (t > j), the z-th trigger happens. We use the historical architecture weights $[\\Phi^j, \\dots, \\Phi^t]$ to evaluate the stability of each dimension."}, {"title": "3.4.1 Dimension Stability Estimation", "content": "Intuitively, a stable dimension needs to satisfy two conditions. On the one hand, the intra-dimension weight at each training step may not change much, i.e., the standard deviation of dimension-level weights $[\\sigma_j^k, \\sigma_{j+1}^k, \\dots, \\sigma_t^k]$ should be as small as possible. On the other hand, the cross-dimension weights, i.e., the weight distributions, should also be as similar as possible. Specifically, we can use the Kullback-Leibler (KL) divergence to evaluate the similarity of two distributions. Based on these motivations, we design a dimension stability indicator as follows:\n$\\lambda_n = \\frac{1}{K} \\sum_{k=1}^K \\sum_j^{} KL(\\Phi_k^j || \\Phi_k^t).$"}, {"title": "3.4.2 Adaptive Dimension Selection", "content": "Similar to the module selection, we need to determine which modules' dimensions should be selected automatically according to the calculated dimension stability scores $[\\lambda_1, \\dots, \\lambda_n]$ using Eq. (9). However, the challenge here is estimating the expected number of modules for dimension reduction.\nExpected Reduction Module Size Estimation. Intuitively, we can fix more modules' dimensions if the potential dimension selections at z 1 and z are similar, which motivates us to use the similarity score to potential dimension vectors, i.e., $v_{z-1}$ and $v_z$. The potential dimension of the n-th module that is not fixed can be obtained through its architecture weights, i.e., $v_n = \\text{Dim}[\\text{argmax}(\\Phi)]$, where Dim is all the possible dimension vector. Note that if the module is removed, then $d_{z-1} = 1, v_n = 0$, and $v_n = \\text{Dim}[k]$ for the fixed module. Finally, we can estimate the number of modules for dimension reduction as follows:\n$Y_z = \\text{sum}(1 - d_{z-1}) * \\text{cos}(v_{z-1}, v_z),$\nAdaptive Dimension Selection. After obtaining the expected number of reduction modules, we then use the dimension stability scores $[\\dots]$ to select dimension-unfixed modules among the top $Y_z$ lowest scores, whose dimensions are then fixed using the corresponding values in v. Finally, we can update $d_z$ based on $d_{z-1}$ and the newly dimension-fixed modules."}, {"title": "3.5 Iterative Optimization", "content": "The proposed BIPEFT is a differential NAS-based PEFT model, which can be optimized as DARTS (Liu et al., 2019). The optimization objective is defined as follows:\n$\\begin{aligned}\n&\\min_{\\Theta*, \\Phi*} \\mathcal{L}_{val} (D_{val}; \\Theta, \\Phi, W_0 + \\Delta W*),\\\\\n&s.t. \\Delta W* = \\arg \\min_{\\Delta W} \\mathcal{L}_{tra} (D_{tra}; \\Theta*, \\Phi*, W_0 + \\Delta W),\n\\end{aligned}$\nwhere the network parameters contain two parts fixed pre-trained LLM weights $W_0$ and trainable PEFT parameters $\\Delta W$. However, we have two distinct search spaces with architecture weights $\\Theta$ and $\\Phi$. These two kinds of architecture weights depend on each other, making optimizing them simultaneously hard.\nTo address this issue, we propose an iterative optimization approach. We first use the first-order approximation in DARTS to optimize $\\Delta W$, which improves the search efficiency by considering that $\\Delta W$ converges fast based on the pre-trained $W_0$. After obtaining $\\Delta W*$, we then fix the dimension parameters $\\Phi*$ and optimize the module parameters $\\Theta$ first. Subsequently, we use the optimized $\\Theta*$ to learn the optimal dimension parameters $\\Phi*$. We repeat the previous steps until BIPEFT converges.\nBesides, to bridge the gap between the search and validation stages, we follow (Chang et al., 2019) by employing the Gumbel-Softmax (Jang et al., 2017) function to normalize $\\Theta*$ and $\\Phi*$, where the nodes with maximal weight will have the highest probability to be selected. In addition, during the search, we implement weight entanglement, as utilized in AutoFormer (Chen et al., 2021), allowing shared weights across different dimensions to enhance stability, promoting faster convergence and reducing memory costs."}, {"title": "4 Experiments", "content": "We use two widely used natural language processing (NLP) benchmarks: GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) in our experiments. All datasets are downloaded from the HuggingFace Datasets (Lhoest et al., 2021).\nWe follow S\u00b3Delta (Hu et al., 2022b) to generate the training, validation, and test splits. For larger datasets, including QQP, QNLI, ReCoRD, SST-2, and MNLI, we allocate 2,000 random samples from the training set to form a new validation set, use the remaining samples as the training set, and repurpose the original validation set as the test set. For smaller datasets, we equally split the original validation set into new validation and test sets, while the original training set remains unchanged. Each dataset is split using different random seeds to introduce variability in the dataset configurations.\nNote that we remove the COPA dataset in SuperGLUE since its performance varies dramatically following S\u00b3Delta (Hu et al., 2022b).\nBaselines. We follow existing studies and use the following models as baselines: (1) Automatic PEFT methods: AutoPEFT (Zhou et al., 2024) uses multi-objective Bayesian optimization to discover a Pareto-optimal set of PEFT configurations. S\u00b3Delta (Hu et al., 2022b) automatically searches sparse PEFT structure by determining the usage of each module using differential NAS. PrunePEFT (Lawton et al., 2023) implements a simple unstructured pruning strategy to search for optimal PEFT structures. (2) Manually designed PEFT methods: We use LoRA (Hu et al., 2022a), Adapter (Houlsby et al., 2019), BitFit, and LNFit as baselines following S\u00b3Delta (Hu et al., 2022b).\nPretrained Backbone Model. Our experiments utilize the T5 large model for all the baselines and BIPEFT, which contains approximately 770 million parameters. We freeze the pretrained parameters Wo across all PEFT settings.\nSearch Spaces. Since the search space of existing work is different, for a fair comparison, we follow existing work and make comparisons within the space that they use. Setting 1 (S1): For AutoPEFT, we use a mixture of Serial Adapter (Houlsby et al., 2019), Parallel Adapter (He et al., 2022), and Prefix-Tuning (Li and Liang, 2021). Setting 2 (S2): When comparing with S\u00b3Delta and PrunePEFT, we use a mixture of LoRA, Adapter-LR, BitFit (Ben Zaken et al., 2022), and LNFIT modules as the search space. For both S1 and S2, each module has a binary selection space {0,1}. For baseline S\u00b3Delta, its dimension size is fixed,"}, {"title": "4.3 Ablation Study", "content": "The proposed BIPEFT mainly considers the disentanglement search spaces and uses both binary module and dimension rank early selection strategies to enhance efficiency and boost performance. We use four baselines in the ablation study to validate the effectiveness of our model design. The experimental setting follows (Hu et al., 2022b), and the results of these baselines are shown in Table 2. We can observe that the iterative search approaches (the last two rows) outperform the entanglement and non-iterative ones, indicating the effectiveness of our proposed iterative search solution. Besides, using the designed selection strategies, BIPEFT further enhances its performance and reduces the number of parameters. This comparison validates the usefulness of our early selection strategies."}, {"title": "4.4 Efficiency Analysis", "content": "In addition to the promising performance, we also evaluate the search efficiency of our method by comparing the time consumed during the search stage against the re-training duration, as detailed in Table 3. All the training time is tested with the same batch size on the same GPU devices. The time is averaged from three datasets, RTE, STSB, and CoLA, under two search space settings, S1 and S2. It clearly shows that BIPEFT achieves a high search efficiency with the design of early selection."}, {"title": "4.5 Different Parameter Budgets", "content": "We also explore the influence of the parameter budget. Ideally, a larger budget will lead to better performance. Figure 3 shows the results with different ratios of parameter budgets, and the models are searched in space S1 on the RTE dataset in the SuperGLUE benchmark, compared with S\u00b3Delta and PrunePEFT. We can observe that BIPEFT consistently sustains a higher accuracy and saturates to full fine-tuning performance by preserving essential PEFT modules even at very low budget levels."}, {"title": "4.6 Generalization Ability Analysis", "content": "We evaluate the task generalization capability of the structures searched within search space S1, as presented in Table 4. The results indicate that the structures searched for source datasets such as MRPC, MNLI, and QQP exhibit robust generalization to various target datasets, which encompass different types of NLP tasks. This demonstrates the ability of our searched structures in maintaining high performance across diverse NLP downstream applications."}, {"title": "5 Conclusion", "content": "In this paper, we introduce BIPEFT, a highly efficient search framework for parameter-efficient fine-tuning (PEFT) modules on large pretrained language models. BIPEFT operates within a specifically designed disentangled search space using an iterative search strategy, incorporating novel selection mechanisms that significantly accelerate the search process while delivering promising performance outcomes. Our extensive experiments on two widely used NLP benchmarks demonstrate the superiority of the PEFT structures identified by BIPEFT. Despite requiring limited parameters and incurring minimal search costs, BIPEFT outperforms both manually designed and other automated PEFT methods across a variety of NLP tasks. Additionally, the searched structures exhibit excellent generalization ability, proving effective for other downstream tasks as well. Overall, BIPEFT represents a substantial advancement in the field of automatic PEFT optimization, providing an efficient and effective solution for fine-tuning large pretrained language models."}, {"title": "Limitations", "content": "While we conduct our PEFT within the S1 and S2 search space settings, which include many popular PEFT modules, there remains the potential to integrate additional existing or new PEFT modules into our framework to further evaluate search performance and efficiency. Nonetheless, the proposed BIPEFT framework is inherently flexible, allowing for the seamless integration of new PEFT modules as they become available. In addition, although the early stopping design requires manual hyperparameters like the maximum step Z, this mechanism will sustain high efficiency and strong performance because it could effectively prioritize the most critical PEFT modules within the given budget."}, {"title": "A Evaluation Metrics", "content": "For both GLUE and SuperGLUE benchmarks, we employ various evaluation metrics: Accuracy is reported for the SST-2, MNLI, QNLI, BoolQ, CB, RTE, and WIC tasks. We utilize the F1 score to assess performance on MRPC, QQP, MultiRC, and ReCoRD. Additionally, Matthew's Correlation is used to evaluate CoLA, and the Pearson Correlation coefficient is applied to the STSB task."}, {"title": "B Experiment Settings and Hyperparameters", "content": "For each experiment setting, we report the average performances and standard deviations using results from three different seeds on the final test sets. We configure the maximum sequence lengths as 128 for GLUE tasks and 256 for SuperGLUE tasks, maintaining a consistent batch size of 32 across both benchmarks. For the ReCoRD task, we adjust the settings to a maximum sequence length of 512 and a batch size of 16. We utilize the AdamW optimizer with a linear learning rate decay schedule to optimize our model. All experiments maintain a consistent learning rate of 3 \u00d7 10-4 for PEFT training, while the learning rate for architecture parameters in BIPEFT is set at 0.01. For BIPEFT, following DARTS (Liu et al., 2019), we equally split the original training set into two parts. One part is used for optimizing the model parameters, and the other for optimizing the architecture parameters. The original validation set serves to evaluate and save the searched structures. For the default values of the model hyperparameters used in BIPEFT, we set Z = 100, \u03b3 = 0.85, \u0397 = 5, and \u03c4 = 0.85."}, {"title": "C Additional Experiment Results", "content": "We perform a detailed hyperparameter sensitivity analysis using the same datasets as in our ablation study. Specifically, we test the following hyperparameters: (1) maximum trigger count Z, (2) \u03b3, used for the smoothed sensitivity in Eq. (4), (3) time window H, employed in the trigger design to measure model stability, and (4) stability threshold \u03c4. All these hyperparameters are configured for the sub-modules of our early selection module, which is designed to enhance search efficiency. The parameter budget ratios are uniformly set to 1.39%. A comprehensive analysis of each hyperparameter is presented below.\nThe maximum trigger count Z controls the speed of the early selection process. We report the average search time across three datasets, demonstrating that while larger Z values ensure smoother selection, they may marginally reduce efficiency. As shown in Table 6, increasing Z beyond 100 diminishes marginal returns in performance improvement. The parameter \u03b3 is used for smoothed sensitivity in Eq. (4), capturing the ratio of historical sensitivity information retained during the training process. The default value for \u03b3 is set to 0.85. Our additional experiments, presented in Table 7, indicate that setting it to 1 prevents the sensitivity from being updated with new data. Stable performance is observed when \u03b3 remains 0.7 to 0.9. The time window H is used in the trigger design to measure model stability. As indicated by the results in Table 8, varying H does not produce significant performance differences across the three datasets. The default value of H is set to 5. The stability threshold \u03c4 influences trigger generation, with higher values enforcing stricter stability requirements, leading to smoother trigger generation and improved performance of the searched structures, as demonstrated in Table 9. By default, we set \u03c4 to 0.85."}, {"title": "C.2 Higher Sparsity Ratio of Parameters", "content": "As demonstrated in the main experiments, BIPEFT outperforms all baselines at a parameter ratio of 1.39%. To further showcase the effectiveness of our method across diverse budget settings, we provide additional results on the GLUE benchmark at a parameter ratio of 5.56% under setting S2, compared to PrunePEFT and S\u00b3Delta. The average score for full fine-tuning is 87.88. As shown in Table 5, BIPEFT continues to outperform other baselines given a higher budget."}, {"title": "C.3 Zero-Cost PEFT Configuration", "content": "As introduced in (Su et al., 2023), their zero-cost PEFT configuration method, APET, initially freezes key factors such as the number of trainable parameters. Under this constraint, APET arbitrarily selects tunable parameters using different random seeds, each representing a distinct parameter distribution, and trains them on the tasks. As shown in Table 10, BIPEFT outperforms APET even at a lower parameter ratio, albeit with a small search cost."}]}