{"title": "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "authors": ["Yifan Song", "Weimin Xiong", "Xiutian Zhao", "Dawei Zhu", "Wenhao Wu", "Ke Wang", "Cheng Li", "Wei Peng", "Sujian Li"], "abstract": "Fine-tuning on agent-environment interaction trajectory data holds significant promise for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we introduce AGENTBANK, by far the largest trajectory tuning data collection featuring more than 50k diverse high-quality interaction trajectories which comprises 16 tasks covering five distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are able to scale the annotated trajectories and generate a trajectory dataset with minimized difficulty bias. Furthermore, we fine-tune LLMs on AGENTBANK to get a series of agent models, SAMOYED. Our comparative experiments demonstrate the effectiveness of scaling the interaction trajectory data to acquire generalized agent capabilities. Additional studies also reveal some key observations regarding trajectory tuning and agent skill generalization.", "sections": [{"title": "1 Introduction", "content": "An agent is an entity that possesses the capability for volition, decision-making, action-taking and, most critically, environment perception (Jennings et al., 1998). In the realm of cognitive science, previous literature has suggested that interaction with environment derives an agent's generalized intelligence, and intelligent behavior emerges from a synergistic blend of simpler behaviors, including reasoning, programming, and game playing (Brooks, 1991). The proprietary large language models (LLMs), such as GPT-3.5 (OpenAI, 2022) and GPT-4 (OpenAI, 2023), have demonstrated strong capabilities in instruction following, reasoning, and planning, which encourage many attempts to build autonomous agent systems utilizing LLMs as core controllers (Richards, 2023; Song et al., 2023). However, comprehensive evaluations have shown that the majority of open-sourced LLMs fall short in agent capabilities when compared with GPTs (Liu et al., 2023; Wang et al., 2023).\nPrevious research pointed out that learning from gold interaction trajectories, a process we term Trajectory Tuning, could enhance the capabilities of weaker agents (Brooks, 1991; Hussein et al., 2017). Early studies heavily focus on specialized agents designed for particular tasks. Existing attempts are exemplified by Chen et al. (2023a), Yin et al. (2023), and Song et al. (2024), who build agent trajectory data from teacher agents (e.g., GPT-4) and fine-tune open-source LLMs to improve specific agent abilities like reasoning. Taking a step further, Zeng et al. (2023) adopt a multi-task tuning approach called AgentTuning. However, trained on a small trajectory dataset comprising six tasks with 1.8k trajectories, Zeng et al. (2023) struggle to enhance the generalized agent capability, especially in the case of 7B and 13B models.\nTo explore the impacts of incorporating interaction trajectory data on agent ability generalization, we construct AGENTBANK, the largest agent interaction trajectory dataset to date. AGENTBANK features 16 distinct tasks across five agent skill dimensions and contains over 50,000 trajectories, each annotated with high-quality chain-of-thought (CoT) rationale for every step of action. Leveraging a novel annotation pipeline that fully exploits the capability of LLMs, the trajectory collection process is highly scalable and adaptable to diverse agent environments. In contrast to prior studies that have relied on successful trajectories of GPTs for training data (Chen et al., 2023a; Zeng et al., 2023), AGENTBANK stands out with its exceptional quality and mitigated susceptibility to the difficulty bias issue.\nWe further develop SAMOYED, a suite of models with enhanced agent capabilities, through the trajectory tuning of Llama-2 (Touvron et al., 2023) using AGENTBANK. Our evaluations on both held-in and unseen held-out tasks suggest that by fine-tuning on extensive multi-task trajectories, our models exhibit remarkable agent intelligence in comparison with untuned ones. Specifically, SAMOYED outperforms GPT-3.5-Turbo on average on held-in tasks, which can be attributed to the in-domain trajectory tuning. Furthermore, our models also demonstrate superior performance on held-out tasks, underscoring the efficacy of large-scale trajectory tuning in acquiring generalized agent capabilities.\nTo trace the emergence of agent capabilities generalization, we follow the initial evaluation with a systematic analysis across various dimensions. Initially, we delineate the scaling trends of tasks alongside the quantity of trajectories. Next, we conduct an ablation study that merges generalist instruction data and code data to examine the benefits of hybrid training. This study uncovers further enhancements in the agent capabilities and mitigates catastrophic forgetting. Furthermore, our findings underscore the pivotal role of CoT rationale in the acquisition of generalized agent capability.\nOur contributions are summarized as follows:\n\u2022 The release of AGENTBANK, a dataset of over 50,000 high-quality agent interaction trajectories, spanning 16 tasks across five skill dimensions. We also present a novel annotation pipeline, offering scalability and a marked reduction in difficulty bias, surpassing previous methods.\n\u2022 The development of SAMOYED, the most powerful open-source LLM suite at the 7B/13B scale optimized for agent tasks. Trained through trajectory tuning, SAMOYED demonstrates exceptional performance, showcasing transferable agent intelligence on unseen tasks.\n\u2022 We conduct comprehensive experiments and in-depth analysis on agent intelligence acquisition, including the relations with instruction following and code capability, scaling law of interaction trajectories, and the effectiveness of training with CoT."}, {"title": "2 Related Work", "content": "2.1 Instruction Tuning\nInstruction tuning is a simple yet powerful approach to align LLMs with human preferences (Zhang et al., 2023). Previous studies have primarily focus on improving general-purpose instruction following capabilities of LLMs. FLAN series (Wei et al., 2021; Chung et al., 2022), TO (Sanh et al., 2021), and NaturalInstruction (Wang et al., 2022b) scale up the instruction datasets to activate the generalized instruction following capabilities of LLMs. More recently, utilizing synthetic instruction following data distilled from GPTs to align open-source LLMs has also been proposed (Taori et al., 2023; Chiang et al., 2023). Furthermore, multiple works have shown the promise of instruction tuning in enhancing the specialized abilities of LLMs, such as math (Yu et al., 2023; Yue et al., 2023), reasoning (Lee et al., 2023), and agent tasks (Chen et al., 2023a; Zeng et al., 2023).\n2.2 LLM-based Agent\nModern LLMs have demonstrated various emergent abilities that encourage researchers to build agent systems based on LLMs. ReAct (Yao et al., 2022b) combines CoT reasoning with agent actions to accomplish tasks such as QA. Auto-GPT (Richards, 2023) harnesses LLMs as the core controllers to constitute powerful agent frameworks capable of solving real-world complex problems. While advanced proprietary models exampled by GPT-3.5/4 have shown strong performances on agent tasks, their open-source counterparts still lag far behind (Liu et al., 2023; Wang et al., 2023). In response, recent studies including FireAct (Chen et al., 2023a), AgentTuning (Zeng et al., 2023) and AgentOhana (Zhang et al., 2024) collect agent trajectory data from teacher agents (e.g., GPT-4) and fine-tune open-source LLMs (e.g., Llama series) with the data. However, limited by the number of tasks and expert trajectories, existing research has not yet exhaustively explored whether open-source LLMs can acquire generalized agent abilities, a gap that this study aims to bridge."}, {"title": "3 Preliminary", "content": "3.1 Agent Task Formulation\nGiven an agent task described by the instruction u, an LLM agent generates an action a\u2081 based on its policy. Next, an environment receives the action, transfers to a new latent state, and provides an observation o\u2081 in natural language format. Subsequently, the agent generates another action for the next step, ai+1, and repeats this circle of interaction with the task environment until either the task is completed or the maximum number of steps is reached. This \u201cconversation\" between the agent with the environment is denoted as the interaction trajectory (u, a\u2081, o\u2081, ..., an). Finally, a final reward r \u2208 [0, 1] is returned depending on the task completion status.\nChain-of-Thought (CoT) (Wei et al., 2022; Kojima et al., 2022) is an effective approach to enhance the inferential capabilities of LLMs by a step-by-step reasoning process. We employ ReAct (Yao et al., 2022b) as the agent tasking framework, which outputs rationale before the action.\n3.2 Challenges in Trajectory Collection\nPrevious works (Chen et al., 2023a; Zeng et al., 2023) have employed GPT-4 as teacher agents to interact with the environment and collect successful interaction trajectories. To ensure the quality of generated data, a failure filtering mechanism is used to remove the cases where GPT failed. However, this GPT-exploration pipeline automates the trajectory construction at some significant cost.\nHard to Scale-Up The quality of data is essential for agent training, and training with failure trajectories will lead to performance degradation (Zeng et al., 2023). Therefore, scaling up this process to a larger trajectory amount is challenging due to the low success rate of GPT-4. For instance, AgentInstruct (Zeng et al., 2023) discards more than 90% generated trajectories due to GPT failures.\nDifficulty Bias Even worse, GPT-exploration pipelines will inevitably introduce difficulty bias to the final training data. Essentially, a trajectory filtering strategy can be regarded as grouping the instances based on whether GPT is capable of solving them. Discarding failed trajectories leads to a skewed distribution of \u201cdifficulty\", resulting in a training set with much easier instances than those in the test set. This violation of the i.i.d. assumption may hurt the generalization ability of the trained agents. In Appendix B, we conduct an experiment to show this bias."}, {"title": "4 AGENTBANK", "content": "In response to the challenges of previous trajectory collection pipeline, we propose a new trajectory annotation pipeline and construct AGENTBANK trajectory dataset.\n4.1 Task and Instruction Collection\nA generalized agent needs to possess a wide range of capabilities across various dimensions. To this end, as shown in Table 2, we curate 16 publicly available agent datasets to lay the foundation of AGENTBANK and categorize specific tasks into five skill dimensions: reasoning, math, programming, web navigation, and embodied tasks. Additionally, some tasks aggregated in AGENTBANK involve the usage of external tools, such as search engine, calculator, and code interpreter, as the ability to effectively operate tools is also a crucial aspect for generalized agents. From the perspective of action space, tasks in AGENTBANK can be classified into two types: those with a continuous action space (including natural language and code) and those with a predefined discrete action space. Our dataset also covers a broad range of interaction turns, ranging from 1 to 30. Note that some tasks are originally evaluated in a single-turn QA style, such as HotpotQA (Yang et al., 2018) and MATH (Hendrycks et al., 2021). Following Wang et al. (2023), we modify these datasets to accommodate multi-turn interaction environments with tool usage.\nSince most of the original benchmarks have a training set, we use them to construct our dataset. To balance data sources, we down-sample some tasks which have a huge training set. See Appendix A for detailed descriptions of each dataset.\n4.2 Action Annotation\nTo tackle the challenges in trajectory collection, unlike previous methods that generate action and CoT simultaneously, we separate the annotation of gold actions and their corresponding rationales, fully leveraging of the capability of LLMs.\nSpecifically tailored to the specific nature of different tasks, our approach involves several techniques to obtain high-quality action sequences accordingly.\nAnswer Forcing For tasks characterized by a continuous natural language or code action space, such as IC-SQL, we introduce an answer forcing action annotation strategy as an extension to GPT-exploration pipeline. This strategy aims to mitigate the bias introduced by failure filtering. Initially, we use GPT-4 to interact with the environment and gather interaction trajectories. For failed trajectories, rather than directly discarding them, we prompt GPT with the failed trajectory and the gold final answer to generate a new interaction trajectory. Then we validate the correctness of new trajectories by executing the actions within real agent environments. This answer forcing process is used in an iterative manner to re-annotate failure trajectories and generate a substantial number of gold action sequences. See Appendix H for the re-annotation prompt.\nHeuristic Action Search For tasks with a discrete action space, exemplified by embodied AI tasks (Weihs et al., 2021; Gordon et al., 2018), we are able to access both the environment's source code and its complete execution state. Leveraging this access, we employ the heuristic depth-first search algorithm to efficiently get the optimal action sequences.\nReformat Some tasks have already provided official solving trajectories. For instance, GSM8K (Cobbe et al., 2021) offers ground-truth intermediate reasoning steps. For these tasks, following (Yin et al., 2023), we exploit GPT as a style transfer tool to transform reasoning process into agent interaction action sequences.\n4.3 Rationale Annotation\nGive the instructions and gold action sequences, we directly prompt GPT to generate the corresponding CoT rationale of each action step. Since providing explanation for gold actions is relatively easy task, we employ GPT-3.5-Turbo as the primary LLM in the rationale annotation process. The rationale generation prompt is shown in Appendix H. We also compare rationales generated by different LLMs in Appendix C.\nFor tasks with a huge number of instructions and GPT-4 have a high success rate, such as StrategyQA (Geva et al., 2021) and WebShop (Yao et al., 2022a), we directly use the GPT-exploration pipeline as Zeng et al. (2023)."}, {"title": "5 Train SAMOYED with AGENTBANK", "content": "To initialize the training of SAMOYED, we formulate agent interaction trajectories in AGENTBANK into a chatbot-style schema (u, a1, o1..., ai, oi, \u2026\u2026\u2026, an), where u is the task instruction, oi and ai denote the observation from the task environment and the corresponding action with rationale generated by the agent in the i-th round. During the training process, we feed the entire interaction trajectory into a decoder-only LLM, where only the auto-regressive loss on tokens of ground-truth responses Y = {a1, ..., an} is counted. We mask all tokens belonging to the instruction and observations from the environment to prevent them from loss computation. Concretely, the loss function is defined as:\n$\\mathcal{L} = -\\sum_{j} \\log p_{\\theta}(t_{j} | t_{<j}) \\times \\mathbb{1}(t_{j} \\in Y),$(1)\nwhere tj denotes the j-th input token and 1 is the indicator function.\nRecent studies (Yang et al., 2024; Zeng et al., 2023) suggest that hybrid training with generalist instruction data and code data may improve the generalized ability of LLM agents. Following them, we adopt a mixture of AGENTBANK \\(D_{\\text{agent}}\\), the general domain instruction dataset \\(D_{\\text{general}}\\), and the code dataset \\(D_{\\text{code}}\\) for fine-tuning. We perform detailed ablation experiments to explore the effectiveness of generalist and code data in Section 7.2."}, {"title": "6 Experiments", "content": "6.1 Experimental Setup\nBase LLMs and Baselines We use several LLMs to conduct experiments, including Llama-2-Chat (Touvron et al., 2023), CodeLlama (Roziere et al., 2023), Mistral (Jiang et al., 2023), and Llama-3-Instruct (Meta, 2024). However, since most baselines, including AgentLM (Zeng et al., 2023) and Agent-FLAN (Chen et al., 2024) are tuned from Llama-2-Chat, we mainly use Llama-2-Chat as our base model for a fair comparison. Due to our limited resources, we use 7B and 13B models for our experiments, leaving the comparison at a larger scale (e.g., Lemur-70B (Xu et al., 2023b) and xLAM-8\u00d77B (Zhang et al., 2024)) for the future work. We also select GPT-3.5-Turbo (OpenAI, 2022) and GPT-4 (OpenAI, 2023) as strong baselines. For all LLMs, the decoding temperature is set to 0 for the most deterministic generation.\nTraining Setup We use AdamW optimizer with a learning rate of 5e-5 and a cosine scheduler. The models are trained for 3 epochs with 3% warm-up steps. The batch size is set to 128 and the sequence length is 2048. We choose ShareGPT\u00b9 as the generalist instruction data, and Evol-CodeAlpaca (Luo et al., 2023) as the code data. The mixture ratio of \\(D_{\\text{agent}}\\), \\(D_{\\text{general}}\\), and \\(D_{\\text{code}}\\) is 80%, 10%, 10%. A corresponding data contamination analysis can be found in Appendix E. All experiments are conducted on 8 NVIDIA A100 80G GPUs. We use FastChat (Zheng et al., 2023a) and PyTorch FSDP (Paszke et al., 2019) for efficient training.\nHeld-in/out Tasks In an effort to balance the reliability and efficiency of the evaluation, we select nine tasks from AGENTBANK to form the held-in test set. For tasks with a huge test set, following Wang et al. (2023), we randomly sample a subset from the original test set. To evaluate the generalized agent intelligence of SAMOYED, we additionally compile five unseen held-out tasks that do not exist in AGENTBANK but still fall into the five skill dimensions of a foundation agent. The held-in and held-out evaluation tasks used in the experiments are listed in Table 3. For all evaluated tasks, 1-shot in-context example is provided in prompts. We use average scores on held-in/out tasks to measure the overall capability of different agents. We also report the results on AgentBench (Liu et al., 2023), another agent benchmark, in Appendix G.\n6.2 Main Results\nTable 4 shows the results of different models on held-in and held-out tasks. Due to the space constraint, we grouped the held-in tasks according to skill dimensions and report the average scores. In Figure 2, we show the results of trajectory tuning on different base LLMs.\nMassive trajectory tuning enables generalization to unseen tasks The performance of SAMOYED has a remarkable improvement on held-out unseen tasks, which demonstrates a substantial boost in agent capabilities through large-scale trajectory tuning. Surprisingly, SAMOYED-7B exhibits an even greater enhancement compared to SAMOYED-13B. Our models also outperform AgentLM and Agent-FLAN which are tuned on less trajectories, demonstrating the effectiveness of scaling up the tuning trajectories.\nComparison among baselines The experiment yields several noteworthy model-wise observations. We find that CodeLlama, benefiting from code pre-training, excels in web browsing tasks. Vicuna exhibits strong abilities through fine-tuning on generalist instruction data, demonstrating impressive performance on both held-in/out tasks. Remarkably, the performance of Vicuna-13B even surpasses AgentLM-13B. It is important to highlight that AgentLM's training set comprises 80% generalist instruction data, suggesting that the held-out task performance of AgentLM largely comes from the enhanced capability of instruction following.\nEffectiveness of trajectory tuning on different base models As illustrated in Figure 2, after large-scale trajectory tuning, all LLMs yield significant performance improvements on held-in and held-out tasks. We also notice some interesting outcomes. CodeLlama's superior performance indicates that code training can enhance agent capabilities. As for Mistral and Llama-3, although fine-tuning on AGENTBANK also yields improvements, the performance gain is relatively modest compared with the substantial improvement seen on Llama-2. This finding indicates that weaker LLMs may benefit more from massive trajectory tuning than their stronger counterparts."}, {"title": "7 Further Analysis", "content": "7.1 Scaling Trends of Generalization\nWe investigate the generalization performance of trajectory tuning with respect to two scaling factors: the number of training tasks and the number of training trajectories. Figure 3 illustrates the performance changes on held-out tasks when scaling each of these factors.\nTo explore the impact of task scaling, we modify the number of tasks in each skill dimension while ensuring that the skill coverage of the subsets remains consistent. We observe that increasing the number of tasks used for training results in improved performance on held-out tasks. This finding suggests that by scaling the number of distinct tasks for trajectory tuning, the model can enhance its generalized agent capabilities.\nAs shown in Figure 3b, a comparison between the performance using 1k trajectories and that with 50k+ cases reveals a marked decrease in the generalized ability of the agent, highlighting the importance of scaling the amount of interaction data for better performance. However, the trajectory of performance improvement is gradually plateauing, particularly noticeable with the 13B model, suggesting the necessity for more advanced agent training techniques beyond SFT.\n7.2 The Effect of Data Mixture\nMixture Training leads to better generalization. When training SAMOYED, we mix 10% generalist instruction data and 10% code data. Here we conduct ablation study to investigate the effect of mixture training. Specifically, we vary the mixture ratio of ShareGPT and code data and train Llama-2-7B-Chat for 1000 steps. As shown in Figure 4a, a relatively low proportion of generalist data leads to improved agent performance on unseen tasks. Nevertheless, as the amount of generalist data continues to increase, the performance on held-out tasks dramatically degrades. Moreover, disagreed with Zeng et al. (2023) who find that training with only interaction trajectory data will lead to performance degradation on held-out tasks, SAMOYED trained on solely AGENTBANK shows performance improvement on held-out tasks instead.\nThe ablation on code data also shows a lower ratio of code data will benefit the generalization ability of the agents. Code data, comprising standard syntax and logical abstraction, has the potential to enhance the planning and decision-making capabilities of LLM agents (Yang et al., 2024).\nCode pretraining benefits web tasks. As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, and abstraction. To further analyse the effect of code training, in Table 5, we compare the distinctions between agents based on Llama-2-Chat and CodeLlama. Unsurprisingly, due to its extensive code training, CodeLlama demonstrates excellent performance in programming tasks. Training with extensive interaction trajectories can further elevate its coding proficiency. Additionally, CodeLlama shows exceptional competence in web navigation tasks, likely attributed to the abundance of web pages present in its pretraining datasets.\nMixture training alleviates catastrophic forgetting. Supervised fine-tuning LLMs on downstream tasks will lead to catastrophic forgetting on general capabilities. Here, we select three widely used benchmarks, MMLU (Hendrycks et al., 2020), MT-Bench (Zheng et al., 2023a), AlpacaEval 2 (Li et al., 2023), to evaluate the general capabilities of the trained agents. As shown in Table 6, since the agent trajectory often presented in specific ReAct formats, the models are easily to get overfitting on this style when training solely on agent data. Simply incorporating generalist instruction data during training proves to be an effective strategy in mitigating catastrophic forgetting.\n7.3 The Effect of CoT Rationale\nChain-of-Thought (CoT) plays an vital role in LLM reasoning and planning (Wei et al., 2022; Kojima et al., 2022). In our experiments, agents are trained with GPT-generated rationales for each action step and are deployed under ReAct framework (Yao et al., 2022b). In this section, we conduct an ablation study to examine the effectiveness of CoT.\nAs shown in Table 7, when it comes to held-in tasks, training without rationales has a minimal impact on performance. Mistral-based agent without CoT even slightly surpasses the one with CoT. Nonetheless, for unseen held-out tasks, training without rationale results in a significant performance decline. Explanation traces provide a detailed step-by-step thought processes, enabling agents to learn from the underlying and planning process (Mukherjee et al., 2023). Moreover, without rationale, the agents tend to mimic the style and get overfitting on held-in tasks.\n7.4 Skill-Level Transfer\nTo explore the potential transferability across different agent skills, we fine-tune Llama-2-Chat on held-in tasks corresponding to a specific agent skill and evaluate on held-out tasks. The compared baseline is fine-tuning Llama-2-Chat using a mixture of generalist instruction and code data. All models are trained for 300 steps to ensure a fair study.\nAs depicted in Figure 5, most skills, with the exception of embodied skill, exhibit the ability to transfer across different skill dimensions. This can be attributed to the unified agent interaction format in AGENTBANK. The transferability of programming and web tasks further confirms the findings from Section 7.2. Notably, embodied AI skill is particularly challenging, for it receives negative impact from all other skills."}, {"title": "8 Conclusion", "content": "In this work, we explore the acquisition of generalized agent capabilities through fine-tuning open-source LLMs on massive interaction trajectories. We introduce by far the largest interaction trajectory dataset AGENTBANK, comprising over 50k trajectories that encompass 16 tasks across five distinct agent skill dimensions. Building upon AGENTBANK, we fine-tune Llama-2 to develop SAMOYED, an open-source LLM series specialized for agent tasks. Evaluations on both held-in and held-out tasks show that SAMOYED significantly outperforms strong baselines in terms of generalized agent capabilities. Comprehensive analysis also reveals the effectiveness of data mixture and plots the scaling law of trajectories. We hope this work to serve as a catalyst for further exploration in the development of more powerful agents.\nLimitations\nWe conclude the limitations of this work as follows:\n\u2022 Due to the resource constraints, we only conduct experiments and analysis on 7B and 13B models. The extent to which larger models can benefit from large-scale trajectory tuning remains unknown.\n\u2022 We have not fully explored the potential of equipping our SAMOYED with more sophisticated agent mechanisms, such as Reflexion (Shinn et al., 2023) and ReWOO (Xu et al., 2023a). Further investigation into these mechanisms could yield valuable insights.\n\u2022 This study primarily focuses on improving the agent's performance via supervised fine-tuning on expert trajectories. How to exploit exploration-based methods (Song et al., 2024; Xiong et al., 2024) to further optimize the agents is left for future investigation.\n\u2022 This work is centered around building strong ReAct-style single-agent models. However, multi-agent collaboration framework has demonstrated impressive performance in handling realistic tasks. The development of strong generalized multi-agent systems based on open-source LLMs is still an under-explored area."}, {"title": "A Details of Tasks in AGENTBANK", "content": "Reasoning Tasks HotpotQA (Yang et al., 2018) is a question answering dataset featuring multi-hop reasoning. StrategyQA (Geva et al., 2021) is another question answering task where the required reasoning steps are implicit in the question and should be inferred using a strategy. TriviaQA (Joshi et al., 2017) is a dataset consisting of complex compositional questions that require multi-evidence reasoning. In our work, we repurpose these three datasets to interaction environments by incorporating a search engine tool. We employ the GPT-exploration pipeline and filter out failed cases to build the gold trajectories.\nFor our held-out evaluation, we use Bamboogle (Press et al., 2022), which is made up of questions that need compositional reasoning and are unable to be directly answered by Google.\nMath Tasks GSM8K (Cobbe et al., 2021) is a dataset of diverse grade school math problems created by humans. Each problem in GSM8K comes with an official solution path. In our work, we leverage the power of GPT-3.5-Turbo to transform these solution paths into interaction trajectories.\nMathQA (Amini et al., 2019) is a large-scale multiple-choice math problem dataset covering multiple math domains. MATH (Press et al., 2022) contains challenging mathematics problems from high school math competitions. To adapt these two datasets into interaction environments, we employ a Python interpreter and employ the GPT-exploration pipeline to construct the trajectories.\nFor the held-out task, we use TheoremQA (Chen et al., 2023b), a theorem-driven question answering dataset composing of high-quality questions from math, physics, EE&CS, and finance. We implement Python interpreter and Wikipedia tools to construct the corresponding interactive environment.\nProgramming Tasks InterCode (Yang et al., 2023) is a benchmark for evaluating language models on interactive programming tasks. In this task, agents are required to respond to natural language requests by interacting with a software system, such as a database or terminal. Our work focuses on evaluating the programming ability of agents using two environments: IC-Bash and IC-SQL. IC-Bash is specifically used for the held-out evaluation of agents.\nAPPS (Hendrycks et al., 2021) is a benchmark focused on Python code generation, encompassing a range of difficulty levels from introductory to competition level. We utilize GPT-3.5-Turbo to reformat the instances in this dataset and construct the trajectories.\nHumanEval (Chen et al., 2021) is a dataset designed to measure functional correctness for synthesizing programs from docstrings. MBPP (Austin et al., 2021) consists of around 1,000 crowd-sourced Python programming problems. For both of these datasets, we employ the GPT-exploration pipeline to annotate the interaction trajectories. Subsequently, we employ the answer forcing method to re-annotate the cases where GPT failed.\nWeb Tasks Mind2Web (Deng et al., 2023) is a dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. WebArena (Zhou et al., 2023) builds realistic web environments for agents to execute tasks. Even GPT-4 struggles with these tasks, so we utilize a teacher forcing and break down the complete interaction trajectory into multiple single steps. Then GPT-3.5-Turbo is employed to annotate the rationales.\nWebShop (Yao et al., 2022a) is a simulated e-commerce website environment with real-world products and crowd-sourced text instructions. For 1571 official human annotated trajectories, we employ GPT-3.5-Turbo to reformat them and annotate rationales. Additionally, we incorporate trajectories generated through GPT-exploration, which have final rewards exceeding 0.3.\nFor our held-out task, we utilize MiniWoB++ (Kim et al., 2023), a diverse collection of over 100 web interaction environments, to formulate our benchmark.\nEmbodied AI Tasks ALFWorld (Shridhar et al., 2020b) contains interactive TextWorld environments that parallel embodied worlds in the AL-FRED dataset (Shridhar et al., 2020a). This dataset provides human-annotated gold trajectories for imitation learning. RoomR (Weihs et al., 2021) is an embodied AI dataset which requires agents to restore the initial configurations of all objects within a room. IQA (Gordon et al., 2018) is a question answering task that requires an agent to interact with a dynamic visual environment. In our work, we utilize the text versions of RoomR and IQA developed by Zheng et al. (2023b). We employ a depth-first-search algorithm to build the gold action sequences for RoomR and IQA. We then leverage GPT-3.5-Turbo to annotate the corresponding rationales.\nFor the held-out evaluation, we utilize Science-World (Wang et al., 2022a), a text-based virtual environment which encompasses various elementary science experiment tasks, including thermodynamics and electrical circuits."}, {"title": "B Difficulty Bias in Trajectory Collection", "content": "In this section, we conduct a experiment to verify the existence of difficulty bias introduced by the trajectory annotation pipeline widely used in recent studies (Chen et al., 2023a; Zeng et al., 2023). Specifically, we choose WebShop trajectories in AGENTBANK and AgentInstruct (Zeng et al., 2023) to conduct the experiment. For AgentInstruct and AGENTBANK, we select 300 instances as the training set \\(D_{\\text{train}}\\), 50 instances as the pseudo test set \\(D_{\\text{pseudo}}\\). We also include the original WebShop test set \\(D_{\\text{test}}\\).\nFor a dataset conforming to the i.i.d. assumption, the instances in \\(D_{\\text{train}}, D_{\\text{pseudo}}, D_{\\text{test}}\\) are sampled from the same distribution. Therefore, the expected behavior is that the evaluation results on \\(D_{\\text{pseudo}}\\) and \\(D_{\\text{test}}\\) should be consistent. Furthermore, an agent trained on \\(D_{\\text{train}}\\) should ideally perform better on \\(D_{\\text{train}}\\) compared to \\(D_{\\text{pseudo}}\\) and \\(D_{\\text{test}}\\).\nTable 8 illustrates the performance of untrained Llama-2-7B-Chat and the trained agent on different sets. For AgentInstruct, both models exhibit worse performance on \\(D_{\\text{test}}\\) compared to \\(D_{\\text{pseudo}}\\), indicating that instances in AgentInstruct are considerably easier than those in the original test set. Conversely, for AGENTBANK, the agents have close performance on \\(D_{\\text{pseudo}}\\) and \\(D_{\\text{test}}\\), aligning with our expectations. The agent trained on our dataset also outperforms the agent trained on AgentInstruct when evaluated on \\(D_{\\text{test}}\\). These experiments highlight that the GPT-exploration trajectory annotation pipeline can introduce difficulty bias in the training set, potentially compromising the generalizability of trained agents."}, {"title": "C CoT Rationales Generated by Different LLMS", "content": "Since providing explanation for gold actions is relatively easy task, we employ GPT-3.5-Turbo as the primary LLM in the rationale annotation process for AGENTBANK. Here we compare the difference of rationale generated by different LLMs. Specifically, we select IC-SQL and WebShop to conduct the experiments. As shown in Table 9, agents training with rationale generated by GPT-4 and GPT-3.5-Turbo have little performance gap."}, {"title": "D Quality Control of AGENTBANK", "content": "In Section 4.2, we incorporate heuristic and GPT-based methods to construct AGENTBANK, which can mitigate the difficulty bias problem in the previous annotation pipeline. In this section, we propose to perform a human evaluation to assess the quality of AGENTBANK. To achieve this, we employ 5 human annotators who are instructed to choose the better trajectory from two anonymous candidate options. Here, we select two representative tasks"}]}