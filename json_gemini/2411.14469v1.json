{"title": "Popular LLMs Amplify Race and Gender Disparities in Human Mobility", "authors": ["Xinhua Wu", "Qi R. Wang"], "abstract": "As large language models (LLMs) are increasingly applied in areas influencing societal outcomes, it is critical to understand their tendency to perpetuate and amplify biases. This study investigates whether LLMs exhibit biases in predicting human mobility-a fundamental human behavior based on race and gender. Using three prominent LLMS-GPT-4, Gemini, and Claude we analyzed their predictions of visitations to points of interest (POIs) for individuals, relying on prompts that included names with and without explicit demographic details. We find that LLMs frequently reflect and amplify existing societal biases. Specifically, predictions for minority groups were disproportionately skewed, with these individuals being significantly less likely to be associated with wealth-related points of interest (POIs). Gender biases were also evident, as female individuals were consistently linked to fewer career-related POIs compared to their male counterparts. These biased associations suggest that LLMs not only mirror but also exacerbate societal stereotypes, particularly in contexts involving race and gender.", "sections": [{"title": "Introduction", "content": "As the application of large language models (LLMs) broadens, understanding their potential for perpetuating societal biases becomes increasingly crucial. Recent research"}, {"title": "Results", "content": ""}, {"title": "Experiment 1", "content": "In this study, we developed a series of prompts to examine how LLMs understand pat- terns of POI visits across different demographic groups. We categorized four distinct types of POIs into two groups: (1) career-related and everyday-needs POIs, and (2) wealth-related and poverty-related POIs. This categorization enabled us to investigate travel predictions made by three LLMS-GPT-40, Gemini-1.5 pro, and Claude-3.5- sonnet for eight subgroups based on race and gender. Detailed information on the specific POIs in each category is provided in Table S1.\nIn Experiment I, we selected 2,675 common first names and obtained the racial and gender composition associated with these names (see Methods). We first designed questions based solely on the individual's name and asked the LLM to select the most likely POI this individual visited. The selection set for POIs consisted of two randomly"}, {"title": "Career-Related and Everyday-Needs POIs", "content": "Figure 1b shows the distribution of GPT-40's predictions for POI visits across dif- ferent demographic subgroups. Predictions from Gemini and Claude can be found in Figure S1, both of which show slightly different values but similar patterns. When demographic information is not explicitly provided, the LLM predicts that only 15.7% of visits are to career-related POIs, while the vast majority (84.3%) are to everyday- needs POIs. This suggests a default assumption by the model that individuals are more likely to engage in day-to-day activities, such as shopping, rather than career-related activities when no demographic information is specified."}, {"title": "Wealth- and Poverty-Related POIs", "content": "Without explicit demographic information, the LLM assigns the vast majority of individuals (96.8%) to wealth-related POIs and none to poverty-related POIs. This suggests that, in the absence of demographic cues, the model assumes a strong like- lihood of individuals frequenting affluent locations, perhaps reflecting a neutral or optimistic bias in the model's default predictions.\nOnce demographic information is provided, significant disparities emerge. White individuals both male and female continue to be strongly associated with wealth- related POIs, with little variation between genders (95.2% for males and 95.3% for females). The predictions are also optimistic for the Asian demographic. Asian males are predicted to visit Wealth-related POIs 93.0% of the time, second only to White individuals. Asian females are assigned even higher wealth-related predictions, at 94.3%. This racial group, particularly Asian females, enjoys a strong association with wealth-related locations, with only a small percentage (7% for males and 5.7% for females) of predictions tied to Poverty-related POIs.\nThese predictions stand in stark contrast to Black individuals, especially Black males, who are predicted to visit wealth-related POIs only 32.9% of the time. Black females, although linked to wealth-related locations more often than their male counterparts (49.1%), are still significantly less likely to be associated with wealth than their White counterparts. Hispanic individuals also show a marked disparity in wealth-related predictions. Both Hispanic males and females are predicted to visit"}, {"title": "Comparison with Survey Data", "content": "Following the observation of disparities in the LLMs' predictions, an important ques- tion emerges: do these disparities accurately reflect real-world patterns, or are they amplified by the model? To answer this question, we compare the predictions to the 2022 National Household Travel Survey (NHTS) in the U.S. [20] shown in Table 1. Note the comparison is limited to work-related trips due to data availability.\nWe observe that the overall proportion of work-related travel for men is 12.8% with only small variations among different racial groups: Hispanic men show the highest proportion at 13.6%, closely followed by White and Black men at 12.8% and 12.7%, respectively. Asian men report the lowest proportion of work-related travel, at 11.9%. For women, the overall proportion of work-related travel is lower, at 9.6%. Among the racial groups, Asian women have the highest proportion of work-related travel at 11.5%, followed by Black women at 10.7%. In contrast, White women show the lowest proportion of work-related travel, at 9.4%, while Hispanic women have a slightly lower proportion at 8.8%.\nDespite the differences between men and women shown in the NHTS data, the LLMs substantially amplify the existing gender gap in travel for career-related POIs. While the NHTS shows that men are 3.5% (Asian) to 54.5% (Hispanic) more likely to travel for work than women within the same racial group, the model's predictions significantly exaggerate these gaps, ranging from 65.0% (Black) to 610% (White). Additionally, the model suggests that Black and Asian populations, especially the males, have a greater inclination towards career-related POIs than other demographic groups. This discrepancy is not observed in Table 1, which shows no significant racial/ethnic differences in work-related travel purposes. Similar results were observed for other LLMs (see Figure S1)."}, {"title": "Logistic Regression Analysis of POI Predictions", "content": "To further quantify the relationship between the POI visits predicted by the LLM and an individual's name, race, and gender, we developed logistic regression models. These models used White males as the reference category, and the results are shown in Figure 2. Notably, even when race and gender were not explicitly provided, the LLMs inferred demographic characteristics based on cues from the individual's name (see first three columns of Figure 2). For instance, all LLMs predicted that individuals with more feminine-sounding names (higher Female_ratio (Name)) were less likely to visit career-related POIs. Both Gemini and Claude associated individuals with Asian- sounding names (higher Asian_ratio (Name)) with a greater likelihood of being linked to career-related POIs, while suggesting that those with traditionally Black names (higher Black_ratio (Name)) were less likely to visit wealth-related locations.\nWhen demographic information was explicitly provided, the influence of the name diminished (see the last three columns of Figure 2), and the provided race and gen- der data became critical. The LLMs consistently predicted that: (1) minority groups (Black, Hispanic, Asian) were more likely to visit career-related POIs but less likely to visit wealth-related locations compared to White individuals, and (2) females (Female) were less likely to visit career-related POIs and more likely to visit wealth-related POIs compared to males. In some cases, these disparities were stark. For example, in Gemini's regression model predicting visits to wealth-related POIs, the parameter for Race_black was below -6, implying that Gemini estimated a Black individual to be less than 1/400 as likely as a White individual to visit a wealth-related POI, suggesting extreme bias in the LLM."}, {"title": "Experiment 2", "content": "To further explore the differential treatment of various demographic groups by LLMs, we designed Experiment II (see Figure 3a). The goal of this experiment is to explore how the inclusion of demographic information, such as race and gender, impacts the predictions made by LLMs when assigning likely POIs to individuals. To investigate this, the experiment compared predictions made for two individuals (e.g., John and Maricela), simultaneously, with and without demographic information. The model was tasked with determining which two locations each person was more likely to visit from a randomized list of four potential POIs, two wealth-related and two poverty-related.\nIn the first part of the experiment, no demographic details are provided. The prompt simply asks the LLM to assign two locations for each individual from the list of POIs. In this case, a possible outcome can be that the model predicts that John visits the resort and soup kitchen, while Maricela visits the art gallery and thrift store. This result shows the model's neutral assignment without any explicit cues related to race or gender.\nIn the second part, the prompt was modified to include explicit demographic details. The inclusion of these demographic markers can lead to a shift in the LLM's predictions. In one of our experiments shown in Figure 3a, with demographic infor- mation, the model predicts that John visits the resort and art gallery, while Maricela visits the soup kitchen and thrift store. The change in the predicted locations suggests"}, {"title": "Distribution of the Predictions from Experiment II", "content": "In this heatmap shown in Figure 3b, the distribution of predicted visits to different categories of POIs is shown across various demographic groups, with rows representing POI categories and columns representing demographic subgroups. The predictions are made by GPT-40, and the color intensity reflects the proportion of individuals from each group visiting particular types of locations, ranging from 0% (blue) to 25% (red).\nNote that Figure 3b represents the aggregated results of prompts with demographic information (the right panel of Figure 3a). Each subgroup was mentioned equally in"}, {"title": "Amplified Gender and Race Disparities", "content": "Figures 4a and 4b provide detailed insights into the disparities in how LLMs predict visits to career-related and wealth-related POIs when provided with gender and race information. These findings illustrate how the models reflect and potentially reinforce societal biases based on both demographic categories.\nFigure 4a demonstrates that the explicit inclusion of gender in the prompt reduces the association between females and career-related POIs by 2% to 8%, depending on the racial group. This decrease is consistent across all racial categories. When gender is not specified (blue bars), White females are predicted to visit career-related POIs 39.5% of the time when comparing to White males, but this drops to 36.5% when the individual's gender is explicitly defined as female. Similarly, for Black individuals, the predicted probability decreases from 40.8% to 38.8%, and for Hispanic individuals, it drops from 45.0% to 41.0%. The most substantial reduction is observed in the Asian group, where the probability falls from 40.2% to 32.2% when gender is specified as female.\nFigure 4b highlights how race impacts the LLM's predictions of visits to wealth- related POIs. When race is not specified (red bars), LLMs do not exhibit strong biases, with minority individuals predicted to visit wealth-related POIs at high probabilities"}, {"title": "Rejection of Responses and Bias Mitigation in LLMs", "content": "Throughout the experiments, we noticed instances where LLMs refused to provide answers or did not generate outputs based on the prompt. This occurs when the model judges the question to be harmful. For example, Gemini responded, \"I cannot fulfill your request. It is harmful to make assumptions about a person's behavior or the places they visit based on their race or gender.\" Claude responded, \"Unable to determine without additional context-specific information.\"\nThe rejection rates for each race in Experiment I are summarized in Table 2. GPT- 40 consistently shows low rejection rates, all under 0.5%, regardless of the demographic group. In contrast, Gemini and Claude demonstrate significantly higher rejection rates, especially when questions involve specific subgroups. Notably, when Black individuals are mentioned, Gemini's rejection rate peaks at 87.5% for wealth-related POIs com- pared to poverty-related POIs, while Claude's reaches 37.5%. For Hispanic and Asian groups, both models show consistently lower rejection rates than for Black and White individuals. Although the exact training processes for these models are not public, it is reasonable to assume that Gemini and Claude have been fine-tuned to be more cau- tious in addressing questions related to certain racial groups. However, as the above experiments reveal, in cases that these models do provide responses, race and gen- der disparities in human mobility are greatly amplified. This underscores that current"}, {"title": "Discussion", "content": "Our study investigated the inherent biases of LLMs in predicting human mobility, with a particular focus on how these biases manifest based on race and gender. The results revealed that while LLMs can be powerful tools for modeling human behavior, they are"}, {"title": "Methods", "content": "Large language model employment. We employed three large language models (LLMs) through their official APIs: GPT-40-2024-08-06 [21], Gemini-1.5-pro [22], and Claude-3-5-sonnet-20240620 [23]. System prompts were used to instruct the LLMs to output responses in a specified JSON format, and simple natural language process- ing techniques were applied to extract the name and the corresponding POI visit from the output. Each question was input into the LLM independently to avoid any potential influence from prior context on the LLM's responses. In some instances, the LLMs rejected to provide responses (details in Supplementary Information), and these refusals were excluded from the analyses presented in this paper.\nNames and their racial and gender composition. In Experiment I, we included 2,675 popular first names. The racial and ethnic information for these names was obtained from [24], and the gender distribution for each name was sourced from [25]."}]}