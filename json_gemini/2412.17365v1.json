{"title": "Boosting LLM via Learning from Data Iteratively and Selectively", "authors": ["Qi Jia", "Siyu Ren", "Ziheng Qin", "Fuzhao Xue", "Jinjie Ni", "Yang You"], "abstract": "Datasets nowadays are generally constructed from multiple sources and using different synthetic techniques, making data de-noising and de-duplication crucial before being used for post-training. In this work, we propose to perform instruction tuning by iterative data selection (ITERIT). We measure the quality of a sample from complexity and diversity simultaneously. Instead of calculating the complexity score once for all before fine-tuning, we highlight the importance of updating this model-specific score during fine-tuning to accurately accommodate the dynamic changes of the model. On the other hand, the diversity score is defined on top of the samples' responses under the consideration of their informativeness. ITERIT integrates the strengths of both worlds by iteratively updating the complexity score for the top-ranked samples and greedily selecting the ones with the highest complexity-diversity score. Experiments on multiple instruction-tuning data demonstrate consistent improvements of ITERIT over strong baselines. Moreover, our approach also generalizes well to domain-specific scenarios and different backbone models. All resources will be available at https://github.com/JiaQiSJTU/IterIT.", "sections": [{"title": "1. Introduction", "content": "Instruction tuning (Ouyang et al., 2022) is an important stage for large language models (LLMs) after the knowledge-centric pre-training. It tailors pre-trained LLMs from a massive knowledge reservoir to a useful knowledge provider by training on instruction-following data (Wang et al., 2022), endowing a significant boost on the LLMs' performance. Data synthesis techniques (Wang et al., 2023) play a key role in constructing such data. These techniques facilitate post-training in different domains due to their scalable nature,\nbut meanwhile introduce more noises and duplications into the training data. Zhou et al. (2024) find that models fine-tuned with only around 1K manually curated high-quality samples already demonstrate a strong generalization ability in downstream tasks. This finding encourages research into identifying the valuable subset for instruction tuning, which can lead to competitive or even superior performance while significantly reducing training costs.\nTo automate the data selection process, certain studies (Chen et al., 2023; Lu et al., 2023) employ powerful proprietary large language models, such as ChatGPT, to assess the quality of each sample based on predefined prompts. Nonetheless, these prompting methods incur additional costly expenses and lack interpretability. Liu et al. (2024b) developed scoring models using labels gathered from ChatGPT, and various approaches have been proposed to evaluate quality, either by leveraging a range of foundation models (Liu et al., 2024a) or by focusing on a single model itself (Li et al., 2024c;b;d). Meanwhile, some research (Shen, 2024; Zhao et al., 2024) also reveals that choosing samples by"}, {"title": "2. Approach", "content": "The goal of supervised fine-tuning (SFT) is to endow pre-trained foundation models with the ability to comprehend and follow user instructions. Let D denote the complete dataset containing N samples {X\u1d62, Y\u1d62}\u1d62\u208c\u2081\u1d3a\u207b\u00b9, where X\u1d62 = {X\u1d62,\u2080, ..., X\u1d62,\u2090\u208b\u2081} refers to the instruction and Y\u1d62 = {Y\u1d62,\u2080, ..., Y\u1d62,\u0432\u208b\u2081} refers to the answer. A and B denote the corresponding number of tokens. The process of supervised fine-tuning is to maximize the model's likelihood of generating answer Y\u1d62 based on instruction X\u1d62, i.e., P\u03b8(Y\u1d62|X\u1d62), where \u03b8 represents the LLM's parameters.\nWe propose ITERIT, the core of which lies in a novel iterative data selection algorithm, with enhancements on measuring samples from both the complexity and diversity aspects. Our approach relies solely on the model being trained itself with affordable computation for data selection. An illustration is shown in Fig. 1(c), with further details elaborated in the following sections."}, {"title": "2.1. Complexity Measurement", "content": "We characterize the complexity of a sample through instruction-following difficulty (IFD), as proposed by Li et al. (2024b). This metric quantifies the connection between X\u1d62 and Y\u1d62 by assessing the perplexity of Y\u1d62 using the model's prior knowledge directly, which is expressed as:\nPPL\u1d62,\u209a\u02b3\u2071\u1d52\u02b3 = exp(- 1/B \u2211\u2096\u208c\u2080\u1d2e\u207b\u00b9 log p\u03b8 (Yi,\u2096|Yi,<\u2096)),  (1)\nand further conditioned on the corresponding instruction, as shown below:\nPPL\u1d62,\u1d9c\u1d52\u207f\u1d48 = exp(- 1/B \u2211\u2096\u208c\u2080\u1d2e\u207b\u00b9 log p\u03b8 (Yi,\u2096|Yi,<\u2096, Xi)).  (2)\nThe complexity score is then defined as the ratio of PPL\u1d62,\u1d9c\u1d52\u207f\u1d48 to PPL\u1d62,\u209a\u02b3\u2071\u1d52\u02b3:\nS\u1d9c\u1d3c\u1d39 = PPL\u1d62,\u1d9c\u1d52\u207f\u1d48 / PPL\u1d62,\u209a\u02b3\u2071\u1d52\u02b3.   (3)\nThis score serves to evaluate the effectiveness of instruction X\u1d62 in facilitating the generation of Y\u1d62. A lower value of S\u1d9c\u1d3c\u1d39 suggests that the model can successfully follow the user's instruction without the need for additional training on that specific sample, while a higher value signifies greater complexity. Ideally, the following condition should be held:\n\u2200\u1d62, \u03b8, PPL\u1d62,\u1d9c\u1d52\u207f\u1d48 < PPL\u1d62,\u209a\u02b3\u2071\u1d52\u02b3.  (4)\nThe rationale behind this is that the entropy of Y\u1d62 should decrease when provided with more relevant information. If this condition is not met, the instruction-response pair is considered to be unaligned for the model."}, {"title": "2.2. Diversity Measurement", "content": "We base the diversity score of a selected subset on the informativeness of individual samples and the degree of information overlap among these samples. Specifically, the informativeness of a sample is predominantly manifested through its response, which conveys not only the topics of the sample but also the attitude in the response. A response rich in detailed information is generally more valuable and preferred by humans. Drawing inspiration from Wu et al. (2024a), we quantify the diversity score using the TF-IDF score of n-grams present in the response Y\u1d62. Mathematically, the TF-IDF for a single n-gram g is defined as:\nTF-IDF(g, Yi) = TF(g, Yi) \u00d7 IDF(g, Yi) = f_g/ \u03a3\u2096\u2208Yi fk \u00d7 log N'/ Ng. (5)\nHere, f denotes the count. N' represents the size of the candidate set. Ng refers to the number of samples in which the n-gram g appears.\nSubsequently, the diversity score of a sample {Xi, Yi} is:\nS\u1d30\u1d35\u1d5b = \u2211 g\u2208Yi  \u03b1_g \u00d7 TF-IDF(g, Yi).   (6)\n\u03b1 is the weight newly introduced to measure the importance of the n-gram, which is initialized to 1. The higher the diversity score, the more informative the response is."}, {"title": "2.3. Iterative Data Selection Algorithm", "content": "Building upon the complexity and diversity scores defined earlier, we introduce an iterative data selection algorithm, dubbed ITERIT, designed to effectively integrate both metrics for the purpose of identifying high-quality samples. Our approach adheres to the complexity-first and diversity-aware principle, which views the complexity-reflecting the pairwise relationship between the instruction and the response of a sample\u2014as the foundational requirement. Simultaneously, it strives to maximize diversity among the candidates that already exhibit high complexity.\nIn alignment with the intuition to adapt to the evolving dynamics of the model during fine-tuning and to prevent the introduction of a substantial computational burden, we implement the complexity score in a coarse-to-fine, epoch-wise fashion. The complexity score for all samples is calculated at the beginning of the SFT process, and only the top-(\u03b1\u00d7M) samples are reserved for re-calculation and fine-grained selection after each epoch iteratively. M refers to the number of samples to be selected for each epoch and \u03b1 > 1 is the coefficient for candidate re-calculation. Therefore, the time complexity for score calculation can be reduced from O(#steps \u00d7 N) to O(N + \u03b1\u00d7M\u00d7 (#epochs \u2013 1)), where both #epochs < #steps and M < \u039d.\nBefore each training epoch, we filter out the samples with S\u1d9c\u1d3c\u1d39 \u2265 1 among the \u03b1 \u00d7 M candidates, and calculate the comprehensive score for each sample as follows:\nS = S\u1d9c\u1d3c\u1d39 \u00d7 S\u1d30\u1d35\u1d5b   (8)\nThe sample with the highest S' will be selected. Subsequently, we update the S\u1d30\u1d35\u1d5b according to Eq. 7, which will in turn impact S\u00b2. ITERIT repeats this process greedily, until a total of M samples are collected for the instruction-tuning phase of the current epoch.\nOverall, our data selection algorithm, designed to balance the dual objectives of effectiveness and efficiency in instruction tuning, is achieved in an iterative manner. The algorithm is elaborated in Algorithm 1."}, {"title": "3. Experimental Setup", "content": "We conduct instruction tuning upon LLMs with four different instruction-tuning datasets. Alpaca (Taori et al., 2023) contains 52,000 samples that are created by leveraging text-davinci-003 model under the self-instruct framework (Wang et al., 2023). Alpaca-GPT4 (Peng et al., 2023) contains higher quality responses generated by GPT-4 given the same instructions from Alpaca. WizardLM (Xu et al., 2023) refers to the 70K evolved samples collected by the Evol-Instruct algorithm that rewrites the initial instruction from Alpaca step by step into more complex instruction by ChatGPT. Dolly (Conover et al., 2023) consists of 15K human-generated prompt-response pairs for instruction tuning."}, {"title": "3.2. Evaluation Benchmarks", "content": "To evaluate the capabilities of instruction-tuned LLMs comprehensively, we selected widely-adopted benchmarks across a spectrum of targeted abilities. They include GSM8K (Cobbe et al., 2021) for arithmetic reasoning, MMLU (Hendrycks et al.) for factual knowledge, TruthfulQA (Lin et al., 2022) for safety, BBH (Suzgun et al., 2023) for multi-step reasoning and HumanEval (Chen et al., 2021) for coding capability, ARC (Clark et al., 2018) for scientific questions and Hellaswag (Zellers et al., 2019) for commonsense understanding. To guarantee the fairness of evaluation, all of the models are evaluated using publicly available code bases, including open-instruct \u00b9 and Open LLM Leaderboard \u00b2.\nMoreover, we employ MixEval (Ni et al., 2024), which adeptly captures the breadth and subtlety of real-world user queries, demonstrating a 0.96 correlation with Chatbot Arena. Specifically, we conducted our evaluation using MixEval-hard-0601 to gauge the model's proficiency in handling general user queries."}, {"title": "3.3. Baselines", "content": "We compared our approach with five representative baselines as follows:\n\u2022 Vanilla refers to supervised fine-tuning with the whole dataset.\n\u2022 Longest (Zhao et al., 2024; Shen, 2024) is a rule-based method that selects the samples with the longest response.\n\u2022 Deita (Liu et al., 2024b) trains scoring models based on labels collected from ChatGPT for complexity and quality assessments, and measures diversity via distances of model embeddings. The samples are selected greedily with the highest complexity-quality scores while not being redundant with the others.\n\u2022 Superfiltering (Li et al., 2024b) improves the model-specific IFD score (Li et al., 2024c) and proposes to select samples with the highest IFD score.\n\u2022 GraphFilter (Wu et al., 2024a) utilize the IFD score for complexity and TF-IDF scores based on instructions for diversity. The data are selected greedily by the priority score defined as the multiplication of both metrics.\nFor fair comparisons, we calculate the IFD scores using the target model itself instead of a smaller model such as GPT-2 (Radford et al., 2019). Besides, the same number of samples are selected by different approaches and all of the models are updated for the same steps during fine-tuning."}, {"title": "3.4. Implementation Details", "content": "We mainly carried out experiments on the LLaMA-3-8B pre-trained language model (Dubey et al., 2024). All of the models are trained with batch size equaling 32 for 3 epochs. The learning rate is set to 2e-5 following the training"}, {"title": "4. Results", "content": "We first present the overall comparisons with competitive baselines on different instruction-following datasets, followed by generalization performance on domain-specific datasets and other backbone models."}, {"title": "4.1. Performance on General Instruction-following Datasets", "content": "We compared models trained with different data selection approaches on various datasets. The results on Dolly are in Appendix A due to space limitation.\nAccording to the average scores across the seven benchmarks and the MixEval benchmark, we have the following observations:\nPrevious data selection methods struggle to achieve consistent improvements over Vanilla among different datasets. Deita outperforms Vanilla by 1.25% on MixEval trained on the Alpaca dataset, while lags behind Vanilla by 1.05% and 4.2% on Alpaca-GPT4 and WizardLM correspondingly, where the instruction-response pairs are of better quality."}, {"title": "4.2. Performance on Domain-Specific Data", "content": "We further investigate the effectiveness of various data selection approaches on domain-specific datasets. Specifically, we select Code Alpaca (Chaudhary, 2023), an instruction tuning dataset collected in a manner similar to Alpaca, which is designed to improve the code generation capabilities of LLMs. We utilize the HumanEval (Chen et al., 2021) and MBPP+ (Liu et al., 2023) benchmarks, which are evaluated using the pass@k metrics, indicating the success rate of a model within k attempts.\nAccording to the results presented in Table 2, ITERIT exhibits superior performance in code generation scenarios. It consistently outperforms Longest, with the exception of pass@1 on HumanEval. Furthermore, among all approaches, ITERIT and GraphFilter are the only two that consistently surpass Vanilla across all evaluation metrics."}, {"title": "4.3. Performance on Other Backbone Model", "content": "To assess the generalization capability across different backbone pre-trained models, we conducted additional experiments on Qwen-2.5-7B (Team, 2024), and compared ITERIT with Vanilla and the strongest baseline, Longest. The overall results are listed in Table 3, with more details elucidated in Appendix B. ITERIT consistently outperforms both Longest on most metrics and Vanilla."}, {"title": "5. Ablations and Analysis", "content": "In this section, we analyze the rationale behind the design of ITERIT with ablation studies and data visualization. First, we show the effectiveness of iterative selection. Following that, we conduct ablations to establish the significance of incorporating diversity in our complexity-first data selection algorithm. Next, we analyze the characteristics of the selected data. Lastly, we assess the sensitivity of hyperparameters newly introduced in ITERIT."}, {"title": "5.1. Ablation for Iterative Selection", "content": "We verify the necessity of performing iterative selection by comparing the performance of our complete approach (w-iteration) with the ablation (wo-iteration), which uses a fixed subset selected at the beginning of the fine-tuning process. Figure 2 indicates that the average performance over 7 benchmarks consistently drops when removing the iterative selection operation, demonstrating the importance of catering to the dynamic changes of the model by updating model-specific complexity scores during the fine-tuning process."}, {"title": "5.2. Ablation for Introducing Diversity", "content": "To analyze the importance of incorporating the diversity measurement into the complexity-first iterative data selection algorithm, we conduct experiments on the Alpaca dataset with the following ablations:\n\u2022 \"w/o div\" refers to the algorithm that does not incorporate S\u1d30\u1d35\u1d5b and selects samples based on S\u1d9c\u1d3c\u1d39\n\u2022 \u201cw div(\u00b7)\u201d refers the algorithm that utilizes S\u1d30\u1d35\u1d5b calculated on different part of the data. i, o, i + o represents Xi, Yi and the concatenation of (Xi, Yi), respectively.\nResults are listed in Table 4. \"w/o div\" lags behind most of the other ablations considering the diversity metric, indicating the significance of making a balance between complexity and diversity. Using S\u1d30\u1d35\u1d5b based solely on the instruction does not help enhance the model's overall performance, as evidenced by the lowest average score. Our approach ITERIT, i.e., \u201cw div(o)\u201d, shows superior performances among ablations, highlighting the importance of defining the diversity of a sample based on its response. It achieves a notable improvement on GSM8K, MMLU, BBH and HumanEval compared to \"w/o div\"."}, {"title": "5.3. Analysis on Selected Data", "content": "We dive deeper into analyzing the characteristics of the selected instruction tuning data by different approaches. Considering the strong performance of Longest, we first analyze the response length of different approaches measured by the number of words, as well as the three subsets selected by our ITERIT for each epoch. The statistics for Alpaca are shown in Fig. 3, with additional results in Appendix C.\nThe overall trend among different approaches is Longest>ITERIT>Superfiltering>GraphFilter>Vanilla>Deita, similar to the performance trend show in Table 1. Nevertheless, the reversed pairwise relations (Longest, ITERIT) and (Deita, Vanilla) show that length is not the only golden criterion for data selection. Our approach selects data based on complexity and diversity, outperforming Longest with smaller response lengths.\nWe further analyze the similarity between the selected subsets by different approaches. The Jaccard similarity between the subset of Longest and the ones used in each epoch by ITERIT is all around 50%. Besides, the lower quartile of ITERIT is much lower than that of Longest in Fig. C. These observations suggest that samples with shorter responses can also be valuable for instruction tuning, which is not only a key difference between the selected data but also contributes to the favorable performance of ITERIT. In addition, the similarity between the subsets in our approach is listed in Table 5. The Jaccard similarity is much lower between Epoch 1 and Epoch 3, showing the model's preference to the instruction-tuning data changes as more update steps are carried out. ITERIT made adjustments to the data selection during fine-tuning for catering to the dynamics of the model."}, {"title": "5.4. Sensitivity of Hyper-parameters", "content": "We discuss the effects of hyper-parameters in ITERIT, including the selected data size for each epoch, the candidate size for re-calculation controlled by \u03b1, and the weight decay coefficient b for diversity measurement.\nData Size The model's performance with selection size M set to different percentages of the original dataset is illustrated in Fig. 4(a). The performance does not exhibit a positive correlation with the amount of selected data. When only 1% of the data is selected, ITERIT requires a greater number of training epochs to achieve convergence, resulting in frequent recalculation processes and a reduction in training efficiency. Meanwhile, the performance does not improve even when the model is trained on more data with additional steps. At this point, the recalculation process cannot be executed in a timely manner, which hinders the synergy between the model and the data, leading to suboptimal performance. Therefore, we suggest setting M in the range of 1K to 5K considering the balance between training efficiency and the model's performance. For fair comparisons, we set M = 0.05N across different datasets in this paper.\nCandidate Pool Performance under different candidate sizes calculated by the multiplication of \u03b1 and M for recalculation, is shown in Fig. 4(b). Enlarging the candidate size not only lifts the computational load, but also increases the likelihood of low-quality data being selected due to the imperfect complexity or diversity score. Therefore, we propose a coarse-to-fine way for updating the complexity score and suggest setting \u03b1 = 3. In this way, low-quality samples recognized at the beginning will be removed directly, and sufficient high-quality samples will be considered for diversity consideration.\nWeight Decay The rationale behind introducing weight decay in the diversity score Sory is illustrated in Fig. 4(c). We hypothesize that removing the TF-IDF scores for already selected n-grams may not be appropriate for mathematical data and code data. These samples contain reserved words that have a high repetition rate. Setting b to 0 reduces the likelihood of such data being selected, which may adversely affect the reasoning capabilities of LLMs. This can be further verified by the 2.50%, 2.59% and 4.76% improvement of b = 0.1 on GSM8K, BBH and HumanEval, respectively, compared with b = 0.0. On the other hand, adopting a large b will have a negative impact on the diversity of data. Therefore, we set b = 0.1 for most instruction-tuning scenarios involving data from various tasks, and suggest b = 0.0 for task-specific scenarios where reserved words are either undefined or shared across all samples."}, {"title": "6. Related Work", "content": "Instruction tuning teaches LLMs to perceive the intent of users and provide helpful responses, standing as a core component in the deployment of LLMs (Ouyang et al., 2022). Collecting high-quality data for instruction tuning has caught great attention. Early works (Wei et al., 2022; Longpre et al., 2023) merge existing NLP datasets to obtain a diverse collection across different tasks. Subsequently, Wang et al. (2023) proposed the Self-Instruct framework, which is an automated algorithm gathering instruction-response data by bootstrapping LLMs' generations. Building on this work, a number of data synthesis and evolution techniques (Xu et al., 2023; Ding et al., 2023; Wu et al., 2024b; Li et al., 2024a) have emerged, leveraging powerful proprietary LLMs to significantly enhance the quality and size of candidate datasets for supervised alignment. Nonetheless, according to the superficial alignment hypothesis proposed by Zhou et al. (2024), LLMs have acquired abundant knowledge and abilities during pre-training, and the focus of instruction tuning is all about style learning for providing a helpful response. They verified their hypothesis with around 1K elaborately selected samples, highlighting data selection for instruction tuning as a promising research direction."}, {"title": "6.2. Data Selection for Instruction Tuning", "content": "Although it's widely accepted that the quality of instruction tuning data is more significant than the quantity, the criteria for quality measurement remains mysterious. AlpaGasus (Chen et al., 2023) relies on ChatGPT's understanding and prompts it to score samples based on its quality. Cao et al. (2023) relies on a bag of indicators measuring quality from different aspects, such as length, coherence, understandability, etc., trying to estimate the models' performance trained on a selected subset. Nuggests (Li et al., 2024d) leverages one-shot learning performance to discern the quality of the data, and SelectIT (Liu et al., 2024a) utilizes the intrinsic uncertainty of LLMs from different levels, including token, sentence and model, to make a collaborative decision. Meanwhile, research from Shen (2024) and Zhao et al. (2024) argue that selecting the longest responses is a simple but tough-to-best baseline, which aligns with the superficial alignment hypothesis. They find that the longest samples are more challenging, high-quality and preferred by humans.\nMore works consider complexity and diversity for assessing data quality. InsTag (Lu et al., 2023) queries proprietary models to tag samples based on semantics and intentions, and defines diversity and complexity measurements regarding the number of tags. Li et al. (2024b) measures the complexity from the aspect of instruction-following difficulty and proposes the model-specific IFD score (Li et al., 2024c). Deita (Liu et al., 2024b) trains an additional scoring model based on labels collected from ChatGPT from the aspects of complexity and quality, and considers diversity via distance between samples' semantic representations during the selection process. Wu et al. (2024a) considers lexical diversity based on bipartite graph and IFD score at the same time, outperforms a number of baselines (Wang et al., 2024; Arthur & Vassilvitskii, 2006; Li et al., 2024b) that mainly consider a single dimension. Our approach ITERIT is in line with these works and aims at advancing instruction tuning performance in an efficient way."}, {"title": "7. Conclusion", "content": "We introduce ITERIT for improving LLMs' instruction tuning by iterative data selection. Our approach successfully unleash the power of quality metrics by perceiving the model dynamics for complexity calculation and exploiting response informativeness for diversity calculation. By integrating both aspects, our approach demonstrates superior performance on multiple instruction tuning datasets and generalizes well to domain-specific scenarios. We underscore the importance of model-data collaboration towards powerful LLMs and will devote into more complicated scenarios and different training stages of LLMs in the future."}]}