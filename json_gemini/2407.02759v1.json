{"title": "Multi-Scenario Combination Based on Multi-Agent\nReinforcement Learning to Optimize the Advertising\nRecommendation System", "authors": ["Yang Zhao", "Jin Cao", "Shaobo Liu", "Xingchen Li", "Chang Zhou", "Yi Zhao", "Chiyu Cheng"], "abstract": "This paper explores multi-scenario optimization on\nlarge platforms using multi-agent reinforcement learning\n(MARL). We address this by treating scenarios like search,\nrecommendation, and advertising as a cooperative, partially\nobservable multi-agent decision problem. We introduce the\nMulti-Agent Recurrent Deterministic Policy Gradient (MA-\nRDPG) algorithm, which aligns different scenarios under a\nshared objective and allows for strategy communication to boost\noverall performance. Our results show marked improvements in\nmetrics such as click-through rate (CTR), conversion rate, and\ntotal sales, confirming our method's efficacy in practical settings.", "sections": [{"title": "I. INTRODUCTION", "content": "In the dynamic landscape of e-commerce, platforms host a\nmultitude of interconnected scenarios including search,\nrecommendation, and advertisements. Each of these sub-\nscenarios further divides into specialized categories, default\nranking and store search within the search scenario. Current\noptimization techniques typically treat these scenarios in\nisolation, leading to disjointed user experiences and suboptimal\noverall performance. Users frequently navigate between\ndifferent scenarios, and independent optimization can result in\nconflicts and inefficiencies. To address these issues, we\nexplores using Multi-Agent Reinforcement Learning to achieve\na cohesive and enhanced user experience across multiple\nscenarios. We develop a system where each scenario's strategy\nconsiders the overall platform performance. This paper\ninvestigates the application of the MA-RDPG algorithm,\nexamining its impact on key performance indicators (KPIs) and\noverall platform efficiency."}, {"title": "II. THEORETICAL FRAMEWORK", "content": "The theoretical foundation of this research is based on the\nprinciples of reinforcement learning (RL) and multi-agent\nsystems. The core concept involves agents interacting with an\nenvironment to maximize cumulative rewards. In a multi-\nagent setup, agents can be cooperative, competitive, or mixed.\nFor our study, we adopt a fully cooperative model where all\nagents share the same objective function, aiming to optimize\nthe global performance of the platform."}, {"title": "A. Deep Recurrent Q-Networks (DRQN)", "content": "Traditional RL approaches assume fully observable\nenvironments. However, in practical applications like e-\ncommerce, the environment is partially observable. Deep\nRecurrent Q-Networks (DRQN) address this by utilizing\nrecurrent neural networks (RNNs) to encode historical\nobservations, thereby maintaining a memory of past states and\nactions."}, {"title": "B. Actor-Critic Methods", "content": "Our approach is inspired by the Deterministic Policy\nGradient (DPG) method, specifically the Deep Deterministic\nPolicy Gradient (DDPG), which combines the strengths of Q-\nlearning and policy gradient methods. The actor-critic\nframework, comprising a central critic that evaluates actions\nand multiple actors that propose actions, serves as the basis for\nour model."}, {"title": "III. LITERATURE REVIEW", "content": "Reinforcement learning (RL)[2] has advanced significantly\nand is utilized in areas such as natural language processing[3-6],\ncomputer vision[7-12], deep learning[13-16], and machine\nlearning[17-22]. Building on foundational research in RL and\nmulti-agent systems, Learning to Rank (L2R) algorithms have\nevolved in online systems from point-wise to list-wise methods.\nIn multi-agent RL (MARL), there have been substantial\ndevelopments in both cooperative and competitive dynamics,\napplied in fields from robotics to resource management.\nHowever, the use of MARL in e-commerce to optimize\ninterrelated scenarios is still underexplored. Our study\naddresses this by proposing a unified framework that promotes\nscenario cooperation, aiming to prevent conflicts and enhance\noverall performance. The findings of Li et al.[1] demonstrated\nthat combining data from two distinct components significantly\nimproves model performance. This inspired us to consider\nintegrating inputs from various sources in our current research."}, {"title": "IV. METHODOLOGY", "content": "The methodology tackles the multi-scenario optimization in\ne-commerce as a cooperative, partially observable multi-agent\ndecision problem using the MA-RDPG algorithm, which\nmerges DRQN and DPG. This allows agents to recall past\ninteractions and optimize actions in continuous spaces,\nenhancing robust predictions. We follow Li et al.[1] approach,\nutilizing multiple rounds of majority voting to ensure reliable\nresults with a limited dataset."}, {"title": "A. Data Collection and Preprocessing", "content": "We gather data from an unpublished e-commerce dataset,\nincluding search queries, click data, and purchase histories.\nThis data is preprocessed to generate training samples for the\nreinforcement learning model."}, {"title": "B. Algorithm Implementation", "content": "The MA-RDPG is implemented with a global critic for\nperformance evaluation, scenario-specific actors for action\ngeneration, and an LSTM-based communication module to\nfacilitate information sharing. The critic estimates the Q-value\nfunction, while actors decide on actions based on the current\nstate and inter-agent communications."}, {"title": "C. Training and Evaluation", "content": "The training process involves continuous interaction with\nthe environment, using a replay buffer to store experiences and\nupdate the model via mini-batch gradient descent. The\nperformance of the algorithm is evaluated using standard A/B\ntesting, comparing key metrics such as CTR, conversion rate,\nand gross merchandise volume (GMV) against baseline models.\nWe train the centralized critic network Q using the Bellman\nformula as in Q-learning. We minimize the following loss\nfunction:\n\n$L($) = E_{h_{t-1},o_t}[(Q(h_{t-1}, o_t, a_t; $) - \u2013 y_t)^2]$\n\nThe update of the private actor network is based on\nmaximizing the overall expectation. Assuming that at time t,\nAit is active, then the objective function is:\n\n$J(0^{i_t}) = E_{h_{t-1},o_t}[Q(h_{t-1}, o_t, a; $)|a=\\mu'_{t} (h_{t-1},o;0^{i_t})]$ (2)\n\nAccording to the chain rule, the goal of module training is\nto minimize the following function:"}, {"title": "", "content": "$L(\\psi) = E_{h_{t-1},o_t}[(Q(h_{t-1}, o_t, a_t; \\Phi) \u2013 Y_t)^2|h_{t-1}=LSTM(h_{t-2},[o_{t-1};a_{t-1}];\\psi)]$\n$E_{h_{t-1},o_t} [Q(h_{t-1}, o_t, a_t; \\Phi)|h_{t-1}=LSTM(h_{t-2},[o_{t-1};a_{t-1}];\\psi)]$\n(3)\nThe training process, detailed in Table 1, involves using a\nreplay buffer to store interactions between agents and the\nenvironment, which are then updated in minibatches. During\neach training session, we select and train several minibatches\nin parallel, simultaneously updating the actor and critic\nnetworks."}, {"title": "A. Experiment Analysis", "content": "The results indicate that our MA-RDPG algorithm\noutperforms existing algorithms, particularly the L2R+L2R\nalgorithm commonly used by e-commerce platforms. While\nL2R+L2R focuses solely on individual scenario optimization,\nMA-RDPG enhances overall benefits by fostering cooperative\ninteractions between scenarios, which significantly improves\nGMV. Key findings include:\n1) MA-RDPG yields a notable increase in GMV, especially\nin store search, with moderate improvements in the main\nsearch. This is primarily because MA-RDPG directs more users\nfrom the main search to the store search rather than vice versa,\nbenefiting the store search more substantially.\n2) Comparative results with L2R+EW further validate the\nnecessity of scenario cooperation, as optimizing the main\nsearch alone using L2R+EW negatively impacts the\nperformance metrics of the in-store search."}, {"title": "B. Behavior Analysis", "content": "In our study, each agent displays continuous behavior,\nallowing us to monitor behavior changes over time across\ndifferent search scenarios, as illustrated in Figure 5. We\nrepresent each behavior as a real vector and depict the average\nbehavior across dimensions in our graphs.\nThe main search scenario graph shows that the most\nsignificant feature is the CTR estimation score (Action_1),\naffirming its relevance to ranking efficacy. Surprisingly, the\nsecond most influential feature, indicated by Action_6, is the\npopularity of a product's corresponding store. This feature,\nthough not typically pivotal in Learning to Rank (L2R) models,\nproves crucial here for directing traffic from the main to the\nstore search, enhancing cooperation between scenarios.\nThe following sub-graph describes the behavior change\nover time of the store search. Action_0 is the most important\nfeature, which represents the sales volume of a product; this\nmeans that in a store, hot-selling products tend to be more\nlikely to be sold.\nDespite initial fluctuations, the action distribution stabilizes\nafter 15 hours of training, aligning with observations from\nFigure 4."}, {"title": "VI. DISCUSSION", "content": "The results highlight the potential of cooperative multi-\nagent systems in optimizing complex, interconnected\nenvironments like e-commerce platforms. The MA-RDPG\nalgorithm effectively balances individual scenario goals with\nthe overall platform objective, leading to improved global\nperformance. The use of recurrent neural networks in the\ncommunication module enables agents to maintain contextual\nawareness, further enhancing decision-making accuracy.\nWe further analyze a typical example to illustrate how MA-\nRDPG makes the main search and in-store search work\ntogether. Considering that there are too many changes in online\nsystems, we focus on analyzing some typical cases here and\ncompare the ranking results of MA-RDPG and L2R+L2R\nalgorithms. For example, how can the main search help in-store\nsearch to get more overall benefits. We assume such a scenario:\na female user with strong purchasing power clicks on many\nhigh-priced and low-conversion products, and then searches for\na keyword \"dress\". Obviously, MA-RDPG is more likely to\nreturn some high-priced and low-sales products in large stores,\nwhich makes it easier for users to enter the store. Compared\nwith the L2R+L2R algorithm, MA-RDPG can sort from a more\nglobal perspective. It not only considers the current short-term\nclicks and transactions, but also considers potential transactions\nthat will be made in the store."}, {"title": "VII. CONCLUSION", "content": "This study presents a novel approach to multi-scenario\noptimization in e-commerce using the MA-RDPG algorithm.\nBy treating multiple scenarios as cooperative agents with\nshared goals, we achieve significant improvements in key\nperformance metrics. Future research can explore the\napplication of this framework to other domains and further\nrefine the communication mechanisms between agents."}]}