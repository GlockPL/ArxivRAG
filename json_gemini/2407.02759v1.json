{"title": "Multi-Scenario Combination Based on Multi-Agent Reinforcement Learning to Optimize the Advertising Recommendation System", "authors": ["Yang Zhao", "Jin Cao", "Shaobo Liu", "Xingchen Li", "Chang Zhou", "Yi Zhao", "Chiyu Cheng"], "abstract": "This paper explores multi-scenario optimization on large platforms using multi-agent reinforcement learning (MARL). We address this by treating scenarios like search, recommendation, and advertising as a cooperative, partially observable multi-agent decision problem. We introduce the Multi-Agent Recurrent Deterministic Policy Gradient (MA-RDPG) algorithm, which aligns different scenarios under a shared objective and allows for strategy communication to boost overall performance. Our results show marked improvements in metrics such as click-through rate (CTR), conversion rate, and total sales, confirming our method's efficacy in practical settings.", "sections": [{"title": "I. INTRODUCTION", "content": "In the dynamic landscape of e-commerce, platforms host a multitude of interconnected scenarios including search, recommendation, and advertisements. Each of these sub-scenarios further divides into specialized categories, default ranking and store search within the search scenario. Current optimization techniques typically treat these scenarios in isolation, leading to disjointed user experiences and suboptimal overall performance. Users frequently navigate between different scenarios, and independent optimization can result in conflicts and inefficiencies. To address these issues, we explores using Multi-Agent Reinforcement Learning to achieve a cohesive and enhanced user experience across multiple scenarios. We develop a system where each scenario's strategy considers the overall platform performance. This paper investigates the application of the MA-RDPG algorithm, examining its impact on key performance indicators (KPIs) and overall platform efficiency."}, {"title": "II. THEORETICAL FRAMEWORK", "content": "The theoretical foundation of this research is based on the principles of reinforcement learning (RL) and multi-agent systems. The core concept involves agents interacting with an environment to maximize cumulative rewards. In a multi-agent setup, agents can be cooperative, competitive, or mixed. For our study, we adopt a fully cooperative model where all agents share the same objective function, aiming to optimize the global performance of the platform."}, {"title": "A. Deep Recurrent Q-Networks (DRQN)", "content": "Traditional RL approaches assume fully observable environments. However, in practical applications like e-commerce, the environment is partially observable. Deep Recurrent Q-Networks (DRQN) address this by utilizing recurrent neural networks (RNNs) to encode historical observations, thereby maintaining a memory of past states and actions."}, {"title": "B. Actor-Critic Methods", "content": "Our approach is inspired by the Deterministic Policy Gradient (DPG) method, specifically the Deep Deterministic Policy Gradient (DDPG), which combines the strengths of Q-learning and policy gradient methods. The actor-critic framework, comprising a central critic that evaluates actions and multiple actors that propose actions, serves as the basis for our model."}, {"title": "III. LITERATURE REVIEW", "content": "Reinforcement learning (RL)[2] has advanced significantly and is utilized in areas such as natural language processing[3-6], computer vision[7-12], deep learning[13-16], and machine learning[17-22]. Building on foundational research in RL and multi-agent systems, Learning to Rank (L2R) algorithms have evolved in online systems from point-wise to list-wise methods. In multi-agent RL (MARL), there have been substantial developments in both cooperative and competitive dynamics, applied in fields from robotics to resource management.\nHowever, the use of MARL in e-commerce to optimize interrelated scenarios is still underexplored. Our study addresses this by proposing a unified framework that promotes scenario cooperation, aiming to prevent conflicts and enhance overall performance. The findings of Li et al.[1] demonstrated that combining data from two distinct components significantly improves model performance. This inspired us to consider integrating inputs from various sources in our current research."}, {"title": "IV. METHODOLOGY", "content": "The methodology tackles the multi-scenario optimization in e-commerce as a cooperative, partially observable multi-agent decision problem using the MA-RDPG algorithm, which merges DRQN and DPG. This allows agents to recall past interactions and optimize actions in continuous spaces, enhancing robust predictions. We follow Li et al.[1] approach, utilizing multiple rounds of majority voting to ensure reliable results with a limited dataset."}, {"title": "A. Data Collection and Preprocessing", "content": "We gather data from an unpublished e-commerce dataset, including search queries, click data, and purchase histories. This data is preprocessed to generate training samples for the reinforcement learning model."}, {"title": "B. Algorithm Implementation", "content": "The MA-RDPG is implemented with a global critic for performance evaluation, scenario-specific actors for action generation, and an LSTM-based communication module to facilitate information sharing. The critic estimates the Q-value function, while actors decide on actions based on the current state and inter-agent communications."}, {"title": "C. Training and Evaluation", "content": "The training process involves continuous interaction with the environment, using a replay buffer to store experiences and update the model via mini-batch gradient descent. The performance of the algorithm is evaluated using standard A/B testing, comparing key metrics such as CTR, conversion rate, and gross merchandise volume (GMV) against baseline models.\nWe train the centralized critic network Q using the Bellman formula as in Q-learning. We minimize the following loss function:\n$\\L(\\$) = E_{h_{t-1},o_t}[(Q(h_{t-1}, o_t, a_t; \\$) -  \u2013 y_t)^2]$\n(1)\nThe update of the private actor network is based on maximizing the overall expectation. Assuming that at time t, Ait is active, then the objective function is:\n$J(\\theta_{it}) = E_{h_{t-1},o_t}[Q(h_{t-1}, o_t, a; \\$)|a=\\mu'_{t}(h_{t-1},o;\\theta_{it})]$\n(2)\nAccording to the chain rule, the goal of module training is to minimize the following function:"}, {"title": "", "content": "$\\L(\\psi)\n= E_{h_{t-1},o_t}[(Q(h_{t-1}, o_t, a_t; \\Phi) \u2013 Y_t)^2|h_{t-1}=LSTM(h_{t-2},[o_{t-1};a_{t-1}];\\psi)]\nE_{h_{t-1},o_t} [Q(h_{t-1}, o_t, a_t; \\Phi)|h_{t-1}=LSTM(h_{t-2},[o_{t-1};a_{t-1}];\\psi)]$\n(3)\nThe training process, detailed in Table 1, involves using a replay buffer to store interactions between agents and the environment, which are then updated in minibatches. During each training session, we select and train several minibatches in parallel, simultaneously updating the actor and critic networks."}, {"title": "V. RESULTS", "content": "The results indicate that our MA-RDPG algorithm outperforms existing algorithms, particularly the L2R+L2R algorithm commonly used by e-commerce platforms. While L2R+L2R focuses solely on individual scenario optimization, MA-RDPG enhances overall benefits by fostering cooperative interactions between scenarios, which significantly improves GMV. Key findings include:\n1) MA-RDPG yields a notable increase in GMV, especially in store search, with moderate improvements in the main search. This is primarily because MA-RDPG directs more users from the main search to the store search rather than vice versa, benefiting the store search more substantially.\n2) Comparative results with L2R+EW further validate the necessity of scenario cooperation, as optimizing the main search alone using L2R+EW negatively impacts the performance metrics of the in-store search."}, {"title": "B. Behavior Analysis", "content": "In our study, each agent displays continuous behavior, allowing us to monitor behavior changes over time across different search scenarios, as illustrated in Figure 5. We represent each behavior as a real vector and depict the average behavior across dimensions in our graphs.\nThe main search scenario graph shows that the most significant feature is the CTR estimation score (Action_1), affirming its relevance to ranking efficacy. Surprisingly, the second most influential feature, indicated by Action_6, is the popularity of a product's corresponding store. This feature, though not typically pivotal in Learning to Rank (L2R) models, proves crucial here for directing traffic from the main to the store search, enhancing cooperation between scenarios.\nThe following sub-graph describes the behavior change over time of the store search. Action_0 is the most important feature, which represents the sales volume of a product; this means that in a store, hot-selling products tend to be more likely to be sold.\nDespite initial fluctuations, the action distribution stabilizes after 15 hours of training, aligning with observations from Figure 4."}, {"title": "VI. DISCUSSION", "content": "The results highlight the potential of cooperative multi-agent systems in optimizing complex, interconnected environments like e-commerce platforms. The MA-RDPG algorithm effectively balances individual scenario goals with the overall platform objective, leading to improved global performance. The use of recurrent neural networks in the communication module enables agents to maintain contextual awareness, further enhancing decision-making accuracy.\nWe further analyze a typical example to illustrate how MA-RDPG makes the main search and in-store search work together. Considering that there are too many changes in online systems, we focus on analyzing some typical cases here and compare the ranking results of MA-RDPG and L2R+L2R algorithms. For example, how can the main search help in-store search to get more overall benefits. We assume such a scenario: a female user with strong purchasing power clicks on many high-priced and low-conversion products, and then searches for a keyword \"dress\". Obviously, MA-RDPG is more likely to return some high-priced and low-sales products in large stores, which makes it easier for users to enter the store. Compared with the L2R+L2R algorithm, MA-RDPG can sort from a more global perspective. It not only considers the current short-term clicks and transactions, but also considers potential transactions that will be made in the store."}, {"title": "VII. CONCLUSION", "content": "This study presents a novel approach to multi-scenario optimization in e-commerce using the MA-RDPG algorithm. By treating multiple scenarios as cooperative agents with shared goals, we achieve significant improvements in key performance metrics. Future research can explore the application of this framework to other domains and further refine the communication mechanisms between agents."}]}