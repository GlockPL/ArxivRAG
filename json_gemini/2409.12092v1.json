{"title": "IMRL: Integrating Visual, Physical, Temporal, and Geometric Representations for Enhanced Food Acquisition", "authors": ["Rui Liu", "Zahiruddin Mahammad", "Amisha Bhaskar", "Pratap Tokekar"], "abstract": "Robotic assistive feeding holds significant promise for improving the quality of life for individuals with eating disabilities. However, acquiring diverse food items under varying conditions and generalizing to unseen food presents unique challenges. Existing methods that rely on surface-level geometric information (e.g., bounding box and pose) derived from visual cues (e.g., color, shape, and texture) often lacks adaptability and robustness, especially when foods share similar physical properties but differ in visual appearance. We employ imitation learning (IL) to learn a policy for food acquisition. Existing methods employ IL or Reinforcement Learning (RL) to learn a policy based on off-the-shelf image encoders such as ResNet-50. However, such representations are not robust and struggle to generalize across diverse acquisition scenarios. To address these limitations, we propose a novel approach, IMRL (Integrated Multi-Dimensional Representation Learning), which integrates visual, physical, temporal, and geometric representations to enhance the robustness and generalizability of IL for food acquisition. Our approach captures food types and physical properties (e.g., solid, semi-solid, granular, liquid, and mixture), models temporal dynamics of acquisition actions, and introduces geometric information to determine optimal scooping points and assess bowl fullness. IMRL enables IL to adaptively adjust scooping strategies based on context, improving the robot's capability to handle diverse food acquisition scenarios. Experiments on a real robot demonstrate our approach's robustness and adaptability across various foods and bowl configurations, including zero-shot generalization to unseen settings. Our approach achieves improvement up to 35% in success rate compared with the best-performing baseline. More details can be found on our website https://ruiiu.github.io/imrl/.", "sections": [{"title": "I. INTRODUCTION", "content": "Robotic assistive feeding [1-8], involves the use of robotic systems to assist individuals with feeding disabilities to eat, presents a great opportunity to enhance the quality of life by providing autonomy in daily eating tasks. One of the critical challenges in this domain is the acquisition of diverse food items from various containers under different conditions. Unlike robotic manipulation tasks [9-11] that often involve rigid, well-defined objects, food acquisition introduces complexities such as variable food types, non-rigid and deformable food, different physical properties (e.g., solid, semi-solid, granular, liquid, and mixture), and different containers. These factors complicate the development of effective robotic feeding systems.\nWhile there has been progress in food acquisition for robotic assistive feeding, several limitations persist. Some existing systems [1-4] rely on predefined, heuristic-based, or hard-coded primitive actions that lack generalization across different food types and configurations. Other works [5\u20138, 12-14] incorporate visual perception to handle certain aspects of food acquisition; however, these methods mainly focus on learning information like bounding boxes and food poses from visual cues. They do not capture the food's physical properties, which are crucial for effective manipulation. Additionally, the acquisition of semi-solid and liquid foods remains relatively underexplored in assistive feeding, with most research focusing on solid or granular foods [5, 8, 15].\nWe use imitation learning (IL) and select Behavior Cloning (BC) as the IL algorithm to learn a policy for food acquisition. BC has been employed in previous works [13\u201315] for similar tasks. However, standard BC often lacks robustness and struggles to generalize across diverse acquisition scenarios. The effectiveness of BC is heavily dependent on the quality of the representations used. Although we only focus on BC, since our advancement is in the representations, we expect that this will be just as useful in other downstream approaches such as RL [16, 17].\nTo address these gaps, our approach focuses on enhancing the robustness and generalizability of BC by developing"}, {"title": "II. RELATED WORK", "content": "Food Acquisition for Assistive Feeding: While recent studies have explored various methods for food acquisition for assistive feeding, achieving robust and adaptive food acquisition is still challenging. Several commercially available devices for mealtime assistance such as [3, 4], often rely on pre-defined trajectories or require user teleoperation, limiting their adaptability. Feng et al. [5], Gordon et al. [6, 7] and Sundaresan et al. [12] leveraged visual information to plan fork skewering motions. Similarly, Grannen et al. [8] planned bimanual food scooping and grouping actions. However, these approaches primarily focus on learning geometric information (e.g., bounding box, food pose) from visual cues for food acquisition; they do not capture the physical properties of the food, or they rely on haptic sensors to determine the necessary force for interacting with food. Tai et al. [15] introduced a food scooping framework, but their work focused solely on granular foods like beans and rice, neglecting other food types such as semi-solids and liquids, and did not account for variations in bowl configurations (e.g., different sizes or shapes).\nVisual Representation Learning: Visual Representation learning has been explored in the context of robotic manipulation and imitation learning, where visual features are crucial for understanding object properties and guiding actions. Previous works such as Dense Object Nets (DON) [18] focus on learning dense visual object descriptors for robotic manipulation by leveraging 3D information, including depth data, to capture fine-grained object details and correspondences. However, these approaches either assume the availability of depth data or depend on substantial supervision to capture the object's state and properties effectively. By using exclusively RGB data, we avoid the complexities associated with 3D reconstruction and depth data while still achieving robust and informative representations that are applicable to downstream imitation learning tasks.\nOther methods, like DINO-v2 [19], SwAV [20], MoCo-v3 [21], and SimSiam [22] utilize self-supervised or unsupervised learning to learn representations. However, representations learned by these methods often focus on surface-level features such as color, shape, and texture, and have limited understanding of the physical properties of objects. For example, a pretrained ResNet-50 [23] can distinguish between water and milk based on their color differences but lacks the understanding that both are liquids. Similarly, it might recognize that milk and yogurt are both white, but it does not comprehend that one is a liquid while the other is semi-solid. In contrast, our approach learns not only to classify food types, but also learns the physical properties of food (e.g., liquid, solid, granular, semi-solid, and mixture), as well as captures temporal dynamics for food acquisition tasks, distinguishing it from other methods that focus mainly on visual surface features.\nImitation Learning: Imitation learning [24-26] has been widely utilized in robotic manipulation to enable robots to mimic human demonstrations and perform complex tasks. Traditional approaches like Behavior Cloning (BC) [27] focus on learning a policy that maps observations directly to actions by minimizing the error between the predicted and demonstrated actions. While effective, standard BC often lacks robustness and generalizability across diverse contexts, as its performance is highly reliant on the quality of the learned representations. Our approach addresses these limitations by developing richer and more informative representations, which include awareness of food types, physical properties, temporal dynamics, optimal scooping points, and bowl fullness. This allows the robot to adapt its actions based on the specific attributes of the food and the environment, resulting in more robust and generalizable BC for food acquisition."}, {"title": "III. APPROACH", "content": "We formulate the food acquisition task as a policy learning problem to learn a visuomotor policy $\\pi_{\\theta}$, parameterized by $\\theta$. The input observation space is defined as $O_t = (I_t,P_t)$, where $I_t \\in \\mathbb{R}^{3\\times H\\times W}$ represents the RGB images, and $p_t \\in \\mathbb{R}^{6}$ denotes the robot proprioception, representing the 6D joint angles. The state $s_{t-k:t} = (I_t, I_{h-k:t}, P_{t-k:t})$ includes an environment RGB image and a sequence of the last $k$ steps of eye-in-hand RGB images and joint angles up to the current timestamp $t$. The output action consists of the predicted joint angles for the next step."}, {"title": "B. Representation Learning", "content": "Representation learning is essential as it provides compact, rich representation that is critical for downstream tasks such as BC, directly influencing BC's performance. We begin by adding image data augmentation, including random center crop, horizontal flip, color jittering, and Gaussian blur. Next, we use the Segment Anything Model (SAM) [28] to segment the desired food. We then extract features from a pretrained model, such as ResNet-50 [23], pretrained on ImageNet [29].\nAfter extracting features, it is essential to fine-tune these features to make them aware of not only food types but also physical properties to improve performance for our downstream BC tasks. To achieve this, we collect a dataset of food images featuring various types with different physical properties, including liquids (e.g., milk and water), granular substances (e.g., rice and cereals), semi-solids (e.g., jello and yogurt), and mixture items (e.g., soup with both liquid and solid components)."}, {"title": "1) Visual Semantics", "content": "To make the features visually food type aware and encourage the model to learn better visual semantic representations, as shown in Fig. 3, we finetune the model by minimizing the following cross-entropy loss: $\\mathcal{L}_{CE} = -\\mathbb{E}_D[\\sum_{i=1}^C y_i \\log(p_i)]$, where $C$ is the total number of classes in the food dataset, $y_i$ is the ground truth label, and $p_i$ is the predicted probability of an image $x$ belongs to class $i$."}, {"title": "2) Physical Properties", "content": "As previously mentioned, understanding the physical properties of food is crucial. Therefore, instead of just learning visual semantics, we also focus on learning the physical properties of food, as illustrated in Fig. 3. The goal is to develop representations that not only differentiate food types but also capture physical properties. Foods with similar physical properties should have closer representations, while those with different properties should be more distinct. For example, milk and water, which are both liquids, should have closer representations. In contrast, milk and rice should have more distinct representations despite both being white, and rice and cereals should be closer due to their shared granular property.\nWe apply contrastive learning with triplet loss to ensure that representations of food items with similar physical properties are pulled closer together, while those with differing properties are pushed further apart.\nWe define the triplet as following: A (anchor), a sample whose representation that we want to anchor (e.g., milk); P (positive), a sample that is similar to the anchor in terms of the physical property (e.g., water, which shares the liquid property with milk); N (negative), a sample that is different from the anchor (e.g., rice, which is granular).\nWe use the L2 norm to measure the dissimilarity between feature representations: $d(f(x_i), f(x_j)) = ||f(x_i)-f(x_j)||_2$, where $f$ is the feature extractor that maps an input image $x$ to its feature representation $f(x)$.\nMathematically, we define the triplet loss for a triplet (A,P,N) as: $\\mathcal{L}(A,P,N) = \\max\\left(0, d(f(A), f(P)) - d(f(A), f(N)) + \\alpha\\right)$, where $\\alpha$ is the margin, a hyperparameter that defines how much closer the positive should be compared to the negative.\nThe objective is to make the distance between the anchor and positive $d(f(A), f(P))$ smaller than the distance between the anchor and negative $d(f(A), f(N))$ by at"}, {"title": "3) Temporal Dynamics", "content": "While visual and physical representation provides features that capture food types and physical properties, which is advantageous for downstream BC tasks. However, food acquisition tasks involve predicting future actions based on current and past observations, making them sensitive to temporal dynamics.\nGiven video sequences with frames ${x_1,x_2,...,x_N}$ of the robot scooping food from bowls, we shuffle these frames and then aim to predict the original order for each frame $x_j$. We formulate this temporal dynamics learning task as a classification problem with the following cross entropy loss: $\\mathcal{L}_{temp} = -\\mathbb{E}_D[\\sum_{j=1}^N y_j \\log(p_j)]$, where $N$ is the number of frames in a video sequence, $y_j$ is the ground truth label for the correct order of frame $x_j$, and $p_j$ is the predicted probability that the frame $x_j$ is in position $j$. This loss to predict the original order of shuffled video frames helps learn temporal dynamics by ensuring that the model captures and understands the sequential relationships between frames."}, {"title": "4) Geometric Information", "content": "Visual, physical, and temporal representations capture food types, properties, and dynamics but miss critical details like food position and bowl fullness. Incorporating geometric information, such as position and depth, addresses these gaps: position-aware learning identifies optimal scooping points, depth-aware learning adjusts the scooping strategy based on bowl fullness. Integrating these information allows the robot to adapt its scooping strategy to the food's location and the bowl's fullness, improving its ability to scoop and clear the bowl effectively.\nPosition-Aware Learning. Position information enables the robot to focus on specific food items and determine effective scooping strategies. To identify the optimal scooping points, we propose the interior point search with local density method to determine optimal scooping points within segmented food regions. This method leverages local density information to identify key points that are not only within the food area but also avoid sparse regions and minimize collisions with the bowl.\nWe first use SAM [28] to segment the desired food item and generate a binary mask over the food. SAM is known for its high-quality segmentation and adaptability without retraining.\nWe then compute the expectation over the mask to determine the centroid position of the food, which serves as a reference point to initialize the search for the optimal scooping point.\nTo find the optimal scooping point, we calculate local density for each point $(x_i, Y_i)$ inside the food mask $(\\mathcal{M}(x_i, Y_i) = 1)$, we define a local neighborhood $\\mathcal{N}_r(x_i, Y_i)$ of size $r \\times r$ centered around $(x_i, Y_i)$. The local density $\\rho(x_i, Y_i)$ for that point is given by: $\\rho(x_i, Y_i) = \\frac{\\sum_{(x_j,y_j)\\in \\mathcal{N}_r(x_iY_i)} \\mathcal{M}(x_j,Y_j)}{|\\mathcal{N}_r(x_i,Y_i)|}$ where $|\\mathcal{N}_r(x_i, Y_i)|$ is the total number of pixels in the neighborhood $\\mathcal{N}_r (x_i, Y_i)$. This equation computes the propertion of food pixels in the local region, effectively providing a meaure of food density around point $(x_i, Y_i)$."}, {"title": "The optimal scooping point", "content": "$(x^*, y^*)$ is the point inside the mask that maximizes the local density: $(x^*, y^*) = \\arg \\max_{(x_i,y_i)} {\\rho(x_i, Y_i)|\\mathcal{M}(X_i, Y_i) = 1}$.\nWe also ensure that the chosen point $(x^*, y^*)$ lines within the interior of the mask. We achieve this by defining a condition that points selected are at least a margin distance $m$ away from the mask boundary. This constraint also helps avoid collision with the bowl's edges when scooping: $\\min \\text{distance}((x^*, y^*), \\mathcal{B}(\\mathcal{M})) > m$, where $\\mathcal{B}(\\mathcal{M})$ represents the boundary of the mask $\\mathcal{M}$.\nDepth-Aware Learning. To estimate the fullness of the bowl, we employ regression with Mean Squared Error (MSE) loss: $\\mathcal{L}_{full} = \\mathbb{E}_D [||f(x) \u2013 l^*||^2]$, where $\\mathcal{L}_{full}$ is the loss for bowl fullness, $f$ is the feature extractor, $x$ is an RGB image, $l^*$ is the ground truth bowl fullness, which we manually label as values such as 0.8 or 0.5.\nThe overall loss for the representation learning is the following: $\\mathcal{L}_z = \\lambda_{CE}\\mathcal{L}_{CE} + \\lambda_{tri}\\mathcal{L}_{tri} + \\lambda_{temp}\\mathcal{L}_{temp} + \\lambda_{full}\\mathcal{L}_{full}$, where $\\lambda_{CE}$, $\\lambda_{tri}$, $\\lambda_{temp}$, and $\\lambda_{full}$ are the coefficients for visual, physical, temporal, and geometric representation loss, respectively."}, {"title": "C. Behavior Cloning", "content": "As mentioned earlier, the primary objective of this paper is to learn a policy $\\pi : S \\rightarrow A$ that guides a robot's actions for food acquisition with BC.\nThe BC loss is a CE loss and is expressed as following: $\\mathcal{L} = -\\mathbb{E}_D [\\log \\pi_{\\theta}(a_t|s_{t-k:t})]$, where $\\theta$ is the parameters of the policy, $a_t$ is the expert action. Minimizing the BC loss $\\mathcal{L}$ allows us to optimize the scooping actions for effective food manipulation.\nUnlike naive BC, we develop a richer and more informative multi-dimensional representation $z$ to enhance the robustness and generalizability of BC. This representation $z$ ensures that the learned policy can tailor scooping strategies appropriately based on different contexts."}, {"title": "IV. EXPERIMENTS", "content": "To validate our approach, we test it on a real robot. We design our experiments to answer the following key questions: (1) Performance Comparison: How well does our approach perform against other baselines for food manipulation, specifically in scooping tasks? (2) Generalization Ability: How well does our approach generalize to different contexts, including unseen food items with various colors and physical properties, and unseen bowls of different colors, shapes, and sizes? (3) Advantage of Learned Representations: How effectively does our approach leverage visual, temporal, and geometric learned representations for improved food manipulation? (4) Critical Design Choices: What design choices in our method are essential for achieving good performance?"}, {"title": "A. Experimental Setup", "content": "We use a UR3e robot arm with RealSense cameras to caputure RGB images, and a spoon affixed to the arm for scooping tasks. To address question (1), we conduct"}, {"title": "D. Experimental Results", "content": "We first qualitatively analyze our learned visual and physical representations using 2D t-SNE visualization [31] of representation embeddings for various food types. Fig. 4a shows embeddings from a ResNet-50 model pretrained on ImageNet, which roughly separate food types but tend to cluster similar-colored items together, like black beans and jello, milk and yogurt. In contrast, as shown in Fig. 4b, embeddings based solely on visual semantics provide better separation between food types compared to ResNet-50 but still ignore physical properties, like milk and yogurt, jello and soup. When incorporating both visual semantics and physical properties, our learned representations achieve better differentiation between food types and group items with similar physical properties, such as yogurt and jello; milk and water; beans, rice and cereals, as in Fig. 4c. Foods with different physical properties are more distinctly separated, such as the mixture property of soup.\nWith the learned representations, we employ IMRL and compare it with other baselines on the real robot UR3 to answer question (1). As shown in Fig. 5a and Fig. 5b, IMRL achieves the highest success rate and the lowest spillage or failure rate across granular cereals, semi-solid jello, and liquid water. IMRL outperforms SCONE [15] by 20%, 30%, and 35% success rate for cereals, jello, and water, respectively. Vis [12] does not perform well without haptic sensors. The performances of pretrained feature extractors without representation learning are worse than IMRL since they capture only visual information, lacking detailed physical properties, temporal dynamics, and geometric information of optimal scooping points and bowl fullness.\nWe also evaluate the quantity of food scooped after a certain number of scooping attempts. We scoop cereals for 10 sequential scooping actions. As shown in Fig. 5c, IMRL successfully scoops 18% of the total bowl volume (0.74 qt for the white circular bowl). While other methods fall short, due to spillage or failure."}, {"title": "E. Generalization Testing", "content": "We conduct zero-shot generalization testing on unseen scenarios to answer question (2). These scenarios involve various bowl types, including a large blue circular bowl, a small blue circular bowl, and a transparent square bowl, as well as different food items such as granular rice, black beans, green beans, yellow beans, and milk. To further assess the generalization capability of IMRL in handling foods with similar physical properties despite their differing color appearances, we mix black, green, and yellow beans and compare the success rate of IMRL with other baselines for acquiring unseen food. As shown in Fig. 5d, IMRL performs comparably well on unseen foods, while the performance of other baselines decreases significantly, highlighting the limitation of standard BC methods in terms of robustness and generalizability.\nFor qualitative comparison, we present the spoon trajectories from zero-shot generalization testing shown in Fig. 6. The left illustrates IMRL scooping cereals from a white"}, {"title": "F. Ablation Studies", "content": "We conduct ablation studies to address questions (3-4) and validate our model's design by assessing the contributions of visual, physical, temporal, and geometric representations. We compare the performance of IMRL full approach with three ablated versions: one without visual and physical representations, one without temporal representation, and one without geometric representation. The results in Fig. 5e demonstrate that removing any of these components leads to a decrease in success rate, confirming the effectiveness of each representation in enhancing overall performance."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we introduce IMRL (Integrated Multi-Dimensional Representation Learning), a novel approach to enhance the robustness and generalizability of behavior cloning (BC) for food acquisition in robotic assistive feeding. By integrating visual, physical, temporal, and geometric representations, IMRL enables adaptive food acquisition across diverse foods and bowl configurations, addressing limitations in existing methods that rely on surface-level geometric information. Our approach showes a 35% improvement in success rate over the best-performing baseline, highlighting its effectiveness, including zero-shot generalization to unseen food and bowl combinations.\nHowever, there are some limitations to our approach. Our framework relies solely on visual cues, limiting performance in scenarios where haptic or tactile feedback could offer"}]}