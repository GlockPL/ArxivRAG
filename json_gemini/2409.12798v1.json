{"title": "Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL", "authors": ["Eduardo Pignatelli", "Johan Ferret", "Tim Rockt\u00e4schel", "Edward Grefenstette", "Davide Paglieri", "Samuel Coward", "Laura Toni"], "abstract": "The temporal credit assignment problem is a central challenge in Reinforcement Learning (RL), concerned with attributing the appropriate influence to each actions in a trajectory for their ability to achieve a goal. However, when feedback is delayed and sparse, the learning signal is poor, and action evaluation becomes harder. Canonical solutions, such as reward shaping and options, require extensive domain knowledge and manual intervention, limiting their scalability and applicability. In this work, we lay the foundations for Credit Assignment with Language Models (CALM), a novel approach that leverages Large Language Models (LLMs) to automate credit assignment via reward shaping and options discovery. CALM uses LLMs to decompose a task into elementary subgoals and assess the achievement of these subgoals in state-action transitions. Every time an option terminates, a subgoal is achieved, and CALM provides an auxiliary reward. This additional reward signal can enhance the learning process when the task reward is sparse and delayed without the need for human-designed rewards. We provide a preliminary evaluation of CALM using a dataset of human-annotated demonstrations from MiniHack, suggesting that LLMs can be effective in assigning credit in zero-shot settings, without examples or LLM fine-tuning. Our preliminary results indicate that the knowledge of LLMs is a promising prior for credit assignment in RL, facilitating the transfer of human knowledge into value functions.", "sections": [{"title": "Introduction", "content": "The Credit Assignment Problem (CAP) [Minsky, 1961, Sutton, 1984, Pignatelli et al., 2024] is a fundamental challenge in RL. It typically involves determining the contribution of each action to the final outcome, a process crucial for accurate policy evaluation. Effective Credit Assignment (CA) enables agents to learn useful associations between actions and outcomes, and provides useful directions to improve the policy.\nHowever, when rewards are dispensed only at the end of a task [Efroni et al., 2021], as it is often the case, the feedback becomes sparse and delayed, making CA particularly challenging. In such scenarios, rewarding events are rare, and Deep Reinforcement Learning (Deep RL) agents often struggle to convert occasional successes into a robust decision-making process. To exacerbate the issue, RL agents typically begin with no prior knowledge (tabula rasa) and must learn the nuances and intricacies of complex tasks from scratch. The lack of controlled experimental conditions, such as the ability to observe counterfactuals, makes it difficult for them to distinguish between correlation and causation. As a result, tasks that are usually easy to solve for humans become hard to address for an RL agent.\nTo address these challenges, many methods incorporate prior human knowledge into RL systems. Two techniques are canon: reward shaping [Ng et al., 1999, Gupta et al., 2022] and Hierarchical Reinforcement Learning (HRL) [Al-Emran, 2015, Sutton et al., 1999] via options [Sutton et al., 1999]. Reward shaping involves providing an additional synthetic reward to guide the agent's actions when natural rewards are uninformative. HRL decomposes complex tasks into simpler ones (options), training agents to achieve intermediate objectives that provide a signal while the Markov Decision Process (MDP) would not. Despite their effectiveness, these methods require extensive human input, making them costly and difficult to scale across different environments.\nRecently, LLMs have emerged as a useful tool to transfer human knowledge into computational agents, either through planning [Dalal et al., 2024], expressing preferences [Klissarov et al., 2023], or grounding their abstract knowledge into practical solutions [Huang et al., 2023, Carta et al., 2023]. Notably, these models have produced strong results in causal reasoning tasks [Jin et al., 2023] with performances comparable to humans [K\u0131c\u0131man et al., 2023]. These results suggest that LLMs could be an effective, supplementary tool to distinguish between correlation and causation more effectively than traditional methods used in early stages of RL training.\nWith these results, a natural question arises: \u201cCan the knowledge encoded in LLMs serve as a useful prior for CA in RL?\" Inspired by the successes of LLMs, we introduce CALM, a general method to perform CA with LLMs using reward shaping. We hypothesize that the prior knowledge of a LLM can provide valuable signals that improve CA in RL, and propose a way to transfer these priors into the agent's value function. On this assumption, CALM leverages a pretrained LLM to break down tasks into smaller, composable subgoals and determine if a state-action-state transition achieves a subgoal. This provides an additional reward signal to enhance RL algorithms, and effectively automates reward shaping by substantially reducing the involvement of humans in the training loop.\nWe present a preliminary evaluation of the efficacy of CALM in zero-shot settings, with no examples and no finetuning. We collect a dataset of demonstrations from MiniHack [Samvelyan et al., 2021] and use it to compare the performance of LLMs against human annotations. Our results indicate that LLMs are a viable means to transfer common human knowledge into value functions, and can be effective in automating reward shaping. This bodes well for the prospect to improve CA in the full RL problem.\""}, {"title": "Related work", "content": "LLMs for RL. Recent advancements have shown the potential of pretrained LLMs in enhancing RL agents. Paischer et al. [2022, 2024] used CLIP encodings to improve the state representations of Partially-observable MDPs (POMDPs). Yao et al. [2020], Du et al. [2023] investigated the ability of pretrained LLMs to improve exploration. Huang et al. [2023], Carta et al. [2023] grounded the abstract knowledge of these models and their capabilities into practical RL tasks. LLMs have been used for planning, either directly as world models [Huang et al., 2022, Wang et al., 2023, Singh et al., 2023, Brohan et al., 2023, Dasgupta et al., 2023, Shah et al., 2023, Zhong et al., 2020, 2022] or by writing code [Liang et al., 2022]. Unlike these methods we use pretrained LLMs as a critic: the LLM provides an evaluation of an action for how useful it is to achieve a goal in the future. Among the methods above, Du et al. [2023] is the only method to use subgoals, but these are used to condition a goal-oriented policy, rather than as a critic.\nLLMs for reward shaping. Carta et al. [2022], Goyal et al. [2019] explore the advantages of using pure language abstractions for reward shaping, but do not use a pretrained LLMs and its prior knowledge. Kwon et al. [2023] use the responses of LLMs as a reward signal, but the investigation is limited to conversational environments.\nLLMs for knowledge transfer. Another set of studies used intrinsic rewards to transfer the prior knowledge of an LLM to a value function. Wu et al. [2024] used LLMs to provide an auxiliary reward signal in Atari [Bellemare et al., 2013], based on the information contained in a game manual. Unlike this study, we use subgoals to extract the reward signal, and we do not focus on incorporating external knowledge material, but rely on the LLM's prior knowledge to solve the task. Klissarov et al. [2023] constructed a reward function from the LLM's preferences over NetHack [K\u00fcttler et al., 2020] in-game messages only. Instead, our method incorporates the full observation, does not use preferences, and does not require a separate stage to fit the preference set, but uses the LLM's output directly.\nIn short, none of these methods proposes to generalise reward shaping with hierarchical skills using pretrained LLMs. Unlike the methods above, we use pretrained LLMs as a critic: we aim to uncover cause-effect relationships between actions and goals by both breaking down a task into valuable subgoals and then acting as a reward function for them. This provides an intermediate signal to shape the agent's behaviour when rewards are sparse and delayed."}, {"title": "Preliminaries", "content": "We consider the problem of learning to solve POMDPs. A POMDP is defined by a tuple $M = (S, A, R, \\mu, O, O, \\gamma)$. Here S is the state space with elements s. A is the action space of elements a. $R:S\\times A\\times S \\rightarrow [0, 1]$ is a deterministic, bounded reward function. $\\mu : S \\times A \\rightarrow S$ is the state transition function. O is the space of all observations, and $O : S \\rightarrow O$ is an observation function, mapping a state s to a partial observation o. $\\gamma \\in [0, 1]$ is the discount factor.\nTo best isolate the CAP from other problems, we focus only on environments with a discrete action space, and deterministic state transitions. To evaluate the capabilities of LLMs in environments where the CAP is hard, we only consider tasks where the reward signal is delayed. Here, the reward function is 0 everywhere, and 1 when a goal state is reached.\nTo start the investigation, we evaluate the LLM only in language settings, and do not consider multimodal (text, image, audio, video) settings. For this reason, we consider only environments with an observation kernel that maps to a textual codomain, $O: S \\rightarrow T$, where T is a set of sequences of characters.\nFinally, we consider a black box, pretrained LLM, that takes an input text and maps it to a finite set of output characters. We consider only open-weights models that can fit an NVIDIA A100 80Gb in either 16 bits floating point or 4 bits integer mode. We assume that the LLM has enough preliminary knowledge of the MiniHack environment to recognise valuable actions that progress towards a win."}, {"title": "Methods", "content": "We set out to design a general method to assign credit in RL using LLMs that can generalise to multiple tasks with little human input. Next, we formalise the method, discuss its assumptions and provide details on the protocols we use to evaluate it."}, {"title": "Reward shaping", "content": "Among the available CA techniques, we focus on reward shaping [Ng et al., 1999], due to both its effectiveness in assigning credit and its limitations to generalisation related to the costs of human involvement in the training loop. Reward shaping aims to address the scarcity of learning signals by introducing an auxiliary reward function, the shaping function:\n$r_{t+1} = R(s_t, a_t, s_{t+1}).$\nHere, $s_t$ is the state at time t, $a_t$ is the action taken in that state, $s_{t+1}$ is the resulting state, and $r_{t+1}$ is the auxiliary reward collected after taking $a_t$ in $s_t$. This reward is added to the original reward signal $R(s_t, a_t, s_{t+1})$ to obtain the new, shaped reward\n$r_{t+1} = R(s_t, a_t, s_{t+1}) + \\hat{R}(s_t, a_t, s_{t+1}).$\nIf there exist a function $\\phi : S \\rightarrow R$ such that $\\hat{R}(s_t, a_t, s_{t+1}) = \\phi(s_{t+1}) \u2013 \\phi(s_t)$, then the set of optimal policies is preserved, and the shaping function is also a potential function [Ng et al., 1999]. In the following, we consider the more general case of non-optimality preserving functions.\nFor example, in key-door environments, a common testbed for CA methods [Hung et al., 2019, Mesnard et al., 2021], the agent must reach a reward behind a locked door, which can only be opened if the agent possesses a key. Here, the agent has clear subgoals: (i) go to the key, (ii) pick it up, (iii) go to the door, (iv) unlock it, (v) go to the reward. Achieving these subgoals sequentially leads to optimal behaviour. However, the agent struggles to recognise this hierarchical pattern due to the lack of immediate feedback from the environment. This is particularly true in the early stages of training, when behaviour is erratic, and two optimal actions can be separated by a long sequence of random ones. Providing intermediate feedback for each achievement often improves the agent's performance [Gupta et al., 2022], and the ability of $\\hat{R}$ to produce an instantaneous signal indicating progress is crucial for better CA. Thus, reward shaping can significantly accelerate the learning process in environments with sparse or delayed rewards.\nHowever, designing an effective shaping function is challenging. The function should be carefully designed to provide useful guidance without leading to unintended behaviours. This often calls for incorporating domain knowledge or heuristic information about the task, and requires deep task and environment knowledge. Such knowledge may not be readily available or easily codifiable, limiting the applicability of reward shaping in diverse or unknown environments. This process is complex and time-consuming, and it might not always be possible to devise a reward function that incentivizes learning, is computationally cheap, and general enough to adapt to various tasks. Improving this limitation could enable broader use of reward shaping and enhance CA in deep RL."}, {"title": "LLMs as shaping functions", "content": "Encouraged by the recent successes of LLMs in RL [Klissarov et al., 2023] and of using language to abstract skills [Jiang et al., 2019, Jacob et al., 2021, Sharma et al., 2021, Mu et al., 2022], we explore whether these models can offer a valid alternative to humans in the reward shaping process. Our goal is to produce a function that, given a description of the task and a state-action-state transition, produces a binary signal indicating whether the action makes progress towards solving the task or not:\n$LLM : desc(M) \\times desc(S \\times A \\times S) \\rightarrow B.$\nHere, LLM is a pretrained LLM; desc(M) is a natural language description of the POMDP (the task); desc(S \u00d7 A \u00d7 S) is a textual representation of the transition, not necessarily in natural language (for example, a grid-arranged text), and B = {0, 1} is the Boolean domain. In this scenario, the LLM acts as a critic: its role is to evaluate the action at in the transition $(s_t, a_t, s_{t+1})$ based on the heuristics that we describe next.\nWe operationalise the idea using the notion of options [Sutton et al., 1999]. An option is a temporally extended action and consists of two elements: an intra-option policy $\\pi: S \\rightarrow \\Delta(A)$, and a termination condition $\\beta: S \\rightarrow B$.\nTo develop an intuition of options, it is useful to visualise one as a macro-action: a set of actions that, taken together, have precise semantics. For example, in our key-to-door example, one useful option to consider is to pick up the key. This macro action includes a set of primitive actions \u2013 the set of actions to navigate to the key and the action pickup \u2013 and a termination condition \u2013 whether the key is picked up. For the purpose of our analysis, this termination is crucial, as it signals that the subtask has been successfully achieved.\nWe exploit this idea to build our shaping function, set up a single-turn conversation, and prompt the model to perform two subtasks:\n(i) To identify a set of useful options in the environment, by breaking down the task into a sequence of shorter subgoals. These options, and more specifically their termination, effectively constitutes our set of subgoals, since a subgoal is achieved when the option terminates (a key is picked up).\n(ii) Determine whether an option terminated (thus, if a subgoal is achieved) in the transition $(s_t, a_t, s_{t+1})$.\nEvery time an option terminates, we augment the task reward with the subtask reward as according to our reward shaping rule, $R(s_t, a_t, s_{t+1}) = \\beta(s_{t+1}).$"}, {"title": "Experimental protocol", "content": "The viability of CALM in online RL settings depends on the quality of the assignments provided by the LLM. Good quality assignments \u2013 signals that reinforce optimal actions \u2013 can improve the performance of an RL algorithm. Thus, we provide a preliminary evaluation of CALM on an offline dataset of demonstrations.\nEnvironment. We focus on the KeyRoom environment, a canonical testbed for CA methods [Hung et al., 2019, Mesnard et al., 2021, 2023] originally proposed in Minigrid [Chevalier-Boisvert et al., 2018]. We choose its MiniHack version, for it provides a textual representation of the observations that can be fed to a language system. The game presents a top-down view of a grid-like environment with two rooms. The agent starts in the first room, where a key is located. It must pick up the key and use it to unlock the door to the second room, where a reward is located. We consider two types of observations:\n1. Cropped observations. a top-down, north-facing, 9x9 crop around the agent, which is known to improve the performance in standard RL benchmarks on Nethack [K\u00fcttler et al., 2020].\n2. Game screens. A top-down, north-facing, 21x79 grid showing the entire game scene, including an in-game message and a set of statistics of the current state. We also refer to these as human observations, since they reproduce the conditions of human game play.\nBoth observations are partial, despite containing different amounts of information. We consider a discrete action set: go north, go east, go south, go west, pickup, apply. The reward function is deterministic, providing a reward of 1 if the agent reaches the goal tile and 0 otherwise. Transitions are also deterministic.\nDataset. We collect 256 one-step transitions $d_t = (s_t, a_t, s_{t+1})$ using a random policy. Given a set of subgoals $G \\subset (S \\times A \\times S)$, a transition $d_t$ can then be classified as either achieving a subgoal $g \\in G$ or not. This produces categories $C = {c_i: 0 \\leq i \\leq |G + 1|}$, one for each subgoal, and an additional one when no subgoal is achieved. To characterise the abilities of an LLM to assign credit accurately, we produce a balanced dataset where each goal appears with equal probability.\nComposing the prompt. For each transition we then compose a prompt using the following structure:\n1. <ROLE> specifies the role we ask the LLM to simulate.\n2. <ENVIRONMENT-DESCRIPTION> describes the RL environment, the source of the gameplay.\n3. <SYMSET> is a list reporting Nethack wiki entries\u00b2 of what each symbols in the grid represents.\n4. <TASK-DESCRIPTION> specifies the overall goal of the agent, and does not contain information about subgoals.\n5. <SUBGOALS> contains either a list of subgoals to achieve, or asks the LLM to produce one.\n6. <INSTRUCTIONS> tasks the agent to determine whether a subgoal is achieved in the trajectory presented in <TRANSITION>."}, {"title": "Can LLMs understand goal specifications and verify option termination?", "content": "This experiment aims to assess whether a pretrained LLM can function as a reward function when subgoals are provided externally. We provide the LLM with the environment name, MiniHack, and a list of two subgoals: pick up the key and unlock the door. We specify that the goal of the agent is simply to win the game [Jeurissen et al., 2024], and ask it to determine if each subgoal has been achieved in the transition. Prompt 4 shows an example prompt for this experiment.\nWe present results for multiple pretrained LLMs, using both cropped observations and full game screens. The purpose of the comparison is not to determine a winning model. It is, instead, to understand whether the ability to assign credit to single transitions is in the spectrum of capabilities of existing open-weights LLMs. This will lay the foundation for applying the method in full RL settings.\nWe report results in Tables 1 and 2, and draw the following two insights. First, LLMs, except gemma-1.1-2b-it, probably due to its small size, are generally effective in recognising when an instruction has been successfully completed in a state-action-state transition. This shows their ability to understand goal specifications and to recognise when an option terminates due to completion. We also noticed that c4ai-command-r-plus degenerates into outputting false for most transitions, most probably due to quantisation.\nSecond, restricting the field of view of the observation helps improve performance. This is most likely due to observations being more concise, and avoiding the information to drown among a high number of tokens. This also seems to increase the lower bound, and the performance of models drastically failing with human observations greatly improves, especially gemma-1.1-2b-it."}, {"title": "Can LLMs suggest effective options?", "content": "In this experiment, we evaluate whether LLMs can autonomously suggest effective options. Instead of providing a predetermined list, we ask the LLM to break down the task into subgoals and verify whether these subgoals have been achieved. Despite only a small change on the surface, removing some key information from the prompt intensively tests the LLM's knowledge of NetHack. More importantly, it stresses the ability of the models to come up with a viable and effective hierarchy of subgoals such that, if reinforced, produces useful signals for progress.\nThis setting is more complex but also more general, as it replicates the amount of information typically available to a human player. Prompt 5 shows an example prompt for this experiment. As for the previous experiment, we evaluate the performance of different models using both cropped and human observations.\nResults in Table 3 indicate that LLMs can effectively suggest subgoals when presented with game screen observations, and that these subgoals align with those identified by humans. Models like Meta-Llama-3-70B-Instruct and Meta-Llama-3-8B-Instruct come close to human performance, suggesting that LLMs can effectively use the additional information to suggest and validate subgoals."}, {"title": "Conclusions, limitations, and future work", "content": "In this study, we explored whether LLMs can be a useful means to transfer human knowledge into the value function of RL agents. By focusing on reward shaping, we highlighted its limitations in scalability due to the cost of human involvement. To mitigate these costs, we proposed replacing humans with LLMs, leveraging their ability to decompose tasks into shorter subgoals. Preliminary results from an offline dataset of MiniHack demonstrations suggest that LLMs are effective in verifying subgoal achievement and align with those proposed by humans. This suggests the potential of using LLMs to enhance CA in RL.\nLimitations. While preliminary results are promising, they are limited by the scope of the current evidence. We did not conduct RL experiments to validate the method in online RL settings. The dynamic nature of online RL could pose unique challenges not present in offline settings. Additionally, despite KeyRoom being representative of the CA challenges, and a common testbed for CA, evaluating the method in a broader range of environments would provide more comprehensive evidence of its robustness and applicability.\nThe method also has inherent limitations. Environments must provide observations in the form of text. The LLM must hold enough knowledge of the game to evaluate actions. While this can be a mild assumption for MiniHack, it can be an obstacle for environments requiring more specialised knowledge, such as Nethack [K\u00fcttler et al., 2020] or Crafter [Hafner, 2021, Matthews et al., 2024]. Finally, the LLM relies solely on their prior knowledge and does not incorporate new knowledge while assigning credit, limiting their adaptability and accuracy over time.\nFuture work. Future work should focus on addressing these limitations. Validating the approach in online RL settings and exploring its applicability to a broader range of environments can tell if CALM can enhance the learning process of RL agents in practice. A natural extension of this work is to generalise the method beyond text-only observations. Baumli et al. [2023] follows this line of research, testing the capability of Vision Language Models (LLMs) to evaluate the completion of an instruction from pixels alone. The instruction completion question corresponds to ours in the LLMs domain. Finally, a closed feedback loop where CALM helps improve the policy, the policy provides new information to the LLM, and the LLM incorporates this information to improve its CA ability could help scale to more complex problems requiring specialistic knowledge."}, {"title": "Prompting", "content": "To develop an intuition of the LLM's task, we show examples of prompts for each configuration used in the experiments of Section 5.1. In particular, we show examples of prompts for both option termination verification and option discovery, and both game screen and cropped observations. Finally, we present prompts where two options terminated (a subgoal is achieved): a key pick-up and a door unlock."}, {"title": "Cropped vs gamescreen observations", "content": null}, {"title": "Option termination vs option discovery", "content": null}, {"title": "Examples of different subgoals", "content": null}, {"title": "Tokenisation", "content": null}, {"title": "Actions", "content": "In this section we investigate the impacts of explicitly adding the action $a_t$ to the transition $(s_t, a_t, s_{t+1})$, which was left implicit in the main experiments in Section 5."}]}