{"title": "The Finite Element Neural Network Method: One Dimensional Study", "authors": ["Mohammed Abda", "Elsa Piollet", "Christopher Blake", "Fr\u00e9d\u00e9rick P. Gosselin"], "abstract": "The potential of neural networks (NN) in engineering is rooted in their capacity to understand intricate patterns and complex systems, leveraging their universal nonlinear approximation capabilities and high expressivity. Meanwhile, conventional numerical methods, backed by years of meticulous refinement, continue to be the standard for accuracy and dependability. Bridging these paradigms, this research introduces the finite element neural network method (FENNM) within the framework of the Petrov-Galerkin method using convolution operations to approximate the weighted residual of the differential equations. The NN generates the global trial solution, while the test functions belong to the Lagrange test function space. FENNM introduces several key advantages. Notably, the weak-form of the differential equations introduces flux terms that contribute information to the loss function compared to VPINN, hp-VPINN, and cv-PINN. This enables the integration of forcing terms and natural boundary conditions into the loss function similar to conventional finite element method (FEM) solvers, facilitating its optimization, and extending its applicability to more complex problems, which will ease industrial adoption. This study will elaborate on the derivation of FENNM, highlighting its similarities with FEM. Additionally, it will provide insights into optimal utilization strategies and user guidelines to ensure cost-efficiency. Finally, the study illustrates the robustness and accuracy of FENNM by presenting multiple numerical case studies and applying adaptive mesh refinement techniques.", "sections": [{"title": "1. Introduction", "content": "The Finite Element Method (FEM) remains the cornerstone of numerical simulation in engineering and applied mathematics due to its accuracy and reliability [1, 2]. However, FEM requires well-posed problems with predefined parameters, loading, and boundary conditions. Moreover, its results are typically compared to experimental measurements only after the simulation. In contrast, a Physics-Informed Neural Networks (PINN) [3, 4] seeks a solution by training a neural network on the appropriate physical laws, with the possibility to also use measurement data for the training. This versatility makes PINNs well-suited for solving ill-posed problems with incomplete, sparse, or noisy data while ensuring consistency with the underlying physics. Given the widespread success of FEM in handling complex physics and geometries, we aim to combine its strengths with the data-driven and inverse problem-solving capabilities of PINNs.\nNeural netowrks (NNs) have been recognized since the 1990s [5] as universal function approximators for solving differential equations. This is because of their ability to approximate a wide range of nonlinear functions using a relatively small number of parameters [5, 6, 7, 8, 9]."}, {"title": "2. Theoretical Background and Previous Work", "content": ""}, {"title": "2.1. Finite Element Method (FEM)", "content": "The FEM is a mathematical technique that allows one to obtain numerical approximations for differential equations that represent physical systems that are usually subjected to external loads [1]. One of the strengths of FEM is its ability to provide approximations to complex problems that are difficult to solve using other methods. This is because the finite element solution can be used repeatedly for all elements in the same mesh and adapted to different problems with minimal modifications [1]. Consider the following illustration problem shown in equation (1), also called the equilibrium problem\n\\frac{d}{dx}(\\frac{dU(x)}{dx}) = \\frac{2}{x^2}, x \\in [1,2],\\tag{1}\nwith x as the independent variable. The essential and natural boundary conditions are, respectively\nU(1) = 2, (\\frac{dU}{dx})|_{x=2} = \\frac{1}{2}\\tag{2}\nIn FEM, the solution to a differential equation is approximated by choosing a trial function from a finite-dimensional space and reducing the residuals of the equation by weighting them with a set of test functions. The trial functions and the test functions belong to a linear space where they are the same in the Galerkin framework and are distinct in the Petrov-Galerkin framework, producing different numerical schemes depending on the choice of the trial functions [1, 10, 2]. The FEM formulation of equation (1) using the Galerkin method for one element is [1]\n\\int_{x_n}^{x_{n+1}} \\Phi_k(x) x \\frac{d^2 \\hat{U}}{dx^2} dx =  -\\int_{x_n}^{x_{n+1}} \\frac{d \\Phi_k(x)}{dx} \\frac{d \\hat{U}(x)}{dx} dx +  \\Phi_k(x) \\frac{d \\hat{U}(x)}{dx}|_{x_n}^{x_{n+1}} - \\int_{x_n}^{x_{n+1}} \\frac{2}{x^2} \\Phi_k(x) dx = 0,\\tag{3}\nwhere $\\Phi_k(x)$ is the kth test function, and $\\hat{U}(x)$ is the trial solution. The flux term is evaluated at the element left boundary $x_n$ and the right boundary $x_{n+1}$. Equation (1) will be used as an example to demonstrate the concepts discussed in the current work.\nIn a more general setting, nonlinear trial functions extend the approximations to a nonlinear space, resulting in a more robust estimation with sparser representation and reduced computational cost [17, 18]. Nonlinear approximation approaches can include radial basis functions [19], dictionary learning [20], neural networks [6], and adaptive splines [17]. However, while nonlinear approximation introduces additional capabilities, its nonlinear nature brings additional complications, and achieving an optimal approximation rate can become challenging, especially in high-dimensional spaces [10]."}, {"title": "2.2. Physics Informed Neural Networks (PINN)", "content": "NNs transform high-dimensional input into output through algebraic operations and nonlinear mapping [6, 7]. They function as an optimization method by iteratively adjusting their parameters to minimize a loss function that measures the discrepancy between the network output and high-fidelity data. A key advantage of NNs is their ability to represent a wide range of nonlinear functions using a relatively small number of parameters [10].\nAlthough NNs are not inherently data-driven, when used to fit data, they are unaware of the mathematical model expressing physical laws. Hence, they require a large amount of high-fidelity data to achieve accurate and reliable predictions. This becomes problematic"}, {"title": "2.3. Variational Physics Informed Neural Networks (VPINN)", "content": "Incorporating the weighted residual of the differential equation to construct a variational loss function results in VPINNs. The loss function is developed within the Petrov-Galerkin framework, where the test functions belong to a linear space and are a combination of Legendre polynomials, while the nonlinear approximation of the NN represents the trial solution [10]. The weak formulation reduces the regularity required in the network output by lowering the operator orders in the loss function. This approach reduces automatic differentiation computations when"}, {"title": "2.4. Convolutional Variational Physics Informed Neural Network (cv-PINN)", "content": "The computational cost in hp-VPINN is addressed by applying convolution operations to compute the strong-form weighted residual loss in cv-PINN. The product of the test functions and the quadrature weights form convolution filters passing over the strong-form residuals [15]. By evaluating the loss function with convolution operations in TensorFlow, rather than sequentially looping over elements for each test function as done in hp-VPINN, cv-PINN gains"}, {"title": "3. The Finite Element Neural Network Method (FENNM)", "content": "To overcome the challenges mentioned above, the current work presents the finite element neural network method FENNM using the convolution operations introduced in cv-PINN. The NN provides the global nonlinear space of solutions, while the test functions belong to the Lagrange test function space, and have at least one nonvanishing value at the element boundaries. Consequently, the information of the flux terms across the elements is now implemented inside the weak-form loss function. Figure 1 presents a schematic of the one-dimensional FENNM solver. In the automatic differentiation step, the NN output generates the fluxes, weak-form residuals of the differential equation including the forcing term using automatic differentiation termed signals. The predefined filters comprising the test functions and their derivatives and quadrature weights then pass over these signals in the convolution process to construct the residual loss. Finally, the total residual loss is evaluated and the NN parameters are updated iteratively to minimize the total loss.\nConsider equation (8); the weak-form per element for the $k^{th}$ test function after integrating by parts is\nL_R^{(n)} = \\int_{x_n}^{x_{n+1}} \\Phi_k(x) x \\frac{d^2 U_{NN}}{dx^2}dx = \\Phi_k(x) \\frac{d U_{NN}(x)}{dx}|_{x_n}^{x_{n+1}} - \\int_{x_n}^{x_{n+1}} \\frac{d \\Phi_k(x)}{dx} \\frac{d U_{NN}(x)}{dx} dx - \\int_{x_n}^{x_{n+1}} \\frac{2}{x^2} \\Phi_k(x) dx\\tag{9}\nAs convolution filters are fixed and pass over the NN output, it is essential to define the test functions in local coordinates. This approach allows for the generalization of the formulation to accommodate meshes with adaptive element sizes similar to FEM. Hence, equation (9) becomes\nL_R^{(n)} = \\Phi_k(\\xi) x \\frac{d U_{NN}}{dx} |_{-1}^{+1} -  \\int_{-1}^{+1} \\frac{d \\Phi_k(\\xi)}{d \\xi} \\frac{d U_{NN}(x)}{dx} \\frac{1}{J_x} d\\xi  - \\int_{-1}^{+1} \\Phi_k(\\xi)  \\frac{2}{x^2} J_x d\\xi,\\tag{10}\nwhere $\\xi$ denotes the local coordinate within the interval [-1,1] such that $x = x_n + J_x(1 + \\xi)$, and $J_x = (x_{n+1} - x_n)/2$ is the one-dimensional Jacobian. The integral terms in equation (10)"}, {"title": "4. Numerical Experiments and Discussions", "content": "In the current study, the penalty terms $\\tau_R$ and $\\tau_B$ in the total loss function in equation (4) are nondecreasing variables and are updated simultaneously during the training process using ADAM optimizer [30] and remain constant during optimization using L-BFGS [31]. Loss terms with increasing errors are automatically weighted more, forcing the network to minimize them. Hence, the network seeks to find a saddle point where it optimizes its parameters during training using gradient descent to minimize total loss and updates the penalty terms using gradient ascent to maximize their weights [13, 32].\nTable 1 provides the NN architecture, parameters, and the choice of test functions for case studies from Section 4.1 to Section 4.3.4. Section 4.1 explores the residual loss function of FENNM in detail, aiming to establish a link between the necessary quadrature points per element and the components of the residual loss function. The relation between the order of the test function and the convergence rate of FENNM is demonstrated in Section 4.2."}, {"title": "4.1. The Influence of The Test Function Order on The Element Error", "content": "The domain of equation (12) is discretized into an element to investigate the relationship between the average absolute error and the number of quadrature points used for a certain"}, {"title": "4.2. Rate of Convergence of FENNM", "content": "The convergence rate (CR) of FENNM was examined through a set of numerical experiments involving equation (12), using varying mesh densities and test function orders. Figure 5 shows the relative absolute error of FENNM at x = 1.5 displayed on a log-log scale for linear, quadratic, and cubic test functions, using mesh densities ranging from [1, 1000] elements.\nThe CR in Figure 5 is defined as the slope of the curve before it plateaus. An adequate quadrature rule order was employed for each test function based on the study conducted in Section 4.1. The results show that the CR decreases for high-order test functions. A similar trend was observed in a previous study conducted in VPINN to understand the role of the order of the quadrature rule and the test functions [28]. The authors concluded counterintuitively that for a fixed quadrature rule, the best CR is achieved using test functions of the lowest order for smooth solutions [28].\nA high-order quadrature rule is necessary when employing high-order test functions, as discussed in Section 4.1. Conversely, applying a high-order quadrature rule to low-order test"}, {"title": "4.3. Numerical Case Studies", "content": "This section investigates four case studies that evaluate the performance and accuracy of the FENNM. The details of each NN are summarized in Table 1. The loss function was initially optimized using the ADAM optimizer [30] for a specified number of iterations to adjust its penalty terms. Subsequently, the L-BFGS optimizer took over the optimization with another specified number of iterations to refine the network parameters [31]."}, {"title": "4.3.1. Cantilever Beam Subjected to An Intermediate Static Force", "content": "Consider an Euler-Bernoulli cantilevered beam of length L, bending rigidity EI, and subjected to a point force F at its midpoint. Its deflection w(x) varies along the position x to obey the differential equation\nEI \\frac{d^4 w(x)}{dx^4} = F \\delta(x - \\frac{L}{2}),\\tag{13}\nwith Dirichlet boundary conditions\nw|_{x=0} = \\frac{dw}{dx}|_{x=0} = 0,\\tag{14}\nand Neumann boundary conditions\n\\frac{d^2 w}{dx^2}|_{x=L/2} - \\frac{d^3 w}{dx^3}|_{x=L} = \\frac{d^2 w}{dx^2}|_{x=L} = 0.\\tag{15}\nConsider the domain of equation (13) to be discretized into four elements as shown in Figure 6 (a). The residual loss function is\n\\frac{1}{N_e K} \\sum_{n=1}^{N_e} \\sum_{k=1}^{K(n)} (  \\int_{x_{n-1}}^{x_n}EI (\\frac{d^4 w(x)}{dx^4} - v_k(\\xi)) d\\xi)^2 + \\int_{x_{n-1}}^{x_n}(\\frac{d^4 w(x)}{dx^4} - v_k(\\xi)) J_x d\\xi = 0\\tag{16}\nwhere v(\\xi) are the cubic Hermite interpolators in the local coordinates, typical of beam elements [2]. Breaking down the total loss per element to\nL_R = \\sum_{n=1}^4 v_n\\tag{17}\nthe Neumann boundary conditions in equation (15) can be imposed into the residual loss function equation (16) as follows\nL^{(2)}_R = EI  (\\frac{E I}{EI}   \\frac{d^3 w}{dx^3}|_{x_1} - \\frac{-F}{EI}   \\upsilon_k| _{\\xi=1}) \\frac{\\partial \\upsilon_k}{\\partial \\xi}|_{\\xi=1} + EI  (0 \\frac{\\partial \\upsilon_k}{\\partial \\xi}|_{\\xi=-1} - \\int_{-1}^{0} \\frac{1}{J} \\frac{\\partial^2 w(x_2)}{\\partial x^2}  \\frac{\\partial  \\upsilon_k(\\xi^{(2)})}{\\partial \\xi^2} J^2 d \\xi)^2,\\tag{18a}\nL^{(4)}_R =   (EI    \\frac{d^3 w}{dx^3}|_{x_3} - 0 \\upsilon_k |_{\\xi=1})   \\frac{\\partial \\upsilon_k}{\\partial \\xi}|_{\\xi=1} - EI    (0 - \\frac{d^2 w}{dx^2} |_{\\xi=-1}   \\frac{\\partial  \\upsilon_k}{\\partial \\xi}|_{\\xi=-1} )\\tag{18b}\nThe intermediate Neumann boundary conditions are imposed at the second element in equation (18a), and the end Neumann boundary conditions are imposed at the fourth element in equation (18b). Ultimately, the loss function contains the Neumann boundary conditions at their corresponding location, similar to the FEM formulation. However, unlike the FEM, Dirichlet boundary conditions are defined in the boundary loss function in the FENNM.\nFigure 6 shows the solution of the cantilevered beam subjected to an intermediate static force with FENNM as described in equation (13). The deflection w(x) of the beam is compared with the exact solution in Figure 6 (a), where the PWE is illustrated in Figure 6 (b). The PWE accumulates over the length of the beam because the Neumann boundary conditions are weakly enforced within the residual loss function and remain in order O(-4).\nThe penalty terms $\\tau_R$ for residual loss and $\\tau_B$ for Diriclet boundary conditions in Figure 6 (c) increased during the first 5000 iterations with ADAM optimizer. Approximately 3000 iterations later, the network found a saddle point where the penalty values saturated, and simultaneously the residual loss $L_R$ started to decrease. A direct relation was found between the saturation of the penalty terms and the number of layers l used in the NN. When the NN is not deep enough, the penalty terms do not saturate regardless of the number of ADAM iterations used. Additionally, it was found that the ADAM optimizer struggles with stiff problems and a similar pattern was observed. After that, training was continued with the L-BFGS optimizer for 10,000 iterations to fine-tune the network parameters and converge to the local minimum with the penalty terms remaining constant.\nEmploying nonvanishing test functions allows FENNM to integrate intermediate loads into the residual loss function using a single network while maintaining high accuracy across the entire domain. Solving the problem of equation (13) with a vanilla PINN is not straightforward as considering a punctual force would require a piecewise solution with two coupled PINNs."}, {"title": "4.3.2. Nonlinear Pendulum", "content": "The FENNM approach can be extended to the temporal dimension in transient problems using the same formulation by treating time as a spatial dimension [9]. Consider the nonlinear pendulum equation\n\\ddot{\\theta} + c \\dot{\\theta} + \\frac{g}{L} sin(\\theta) = 0,\\tag{19}\nwith initial conditions\n\\theta|_{t=0} = \\frac{3 \\pi}{8}, \\dot{\\theta}|_{t=0} = 0,\\tag{20}\nwhere $\\theta$, $\\dot{\\theta}$, and $\\ddot{\\theta}$ are angular displacement, angular velocity, and angular acceleration, respectively. With a damping coefficient c, length L, and gravitational acceleration g. The first-order initial condition can be imposed inside the flux term of the weak-form residual loss function as shown in equation (21). In contrast, the zeroth-order initial condition is separately imposed inside the boundary loss function.\nL_R = \\frac{1}{N_e K} \\sum_{n=1}^{N_e} \\sum_{k=1}^{K(n)} (   \\sum_{q=1}^Q J W_q [\\dot{\\theta}_{n+1} - (\\frac{\\theta\\upsilon_k}{\\upsilon_k} +  \\sum_{q=1}^Q J W_q (c \\dot{\\theta} + \\frac{g}{L} sin(\\theta))^2,\\tag{21}\nThe solutions of the damped and undamped pendulum in equation (21) are shown in Figure 7 computed via FENNM. Figure 7 (a) shows the different oscillations for both pendulums and (d) shows their phase-space plots. The FENNM approximation was compared with the Runge-Kutta method of precision of the 4th order with steps taken from the 5th order (RK45) in Python. Thus, the numerical errors inherent in the RK45 solution prevent an accurate"}, {"title": "4.3.3. The Transport Equation", "content": "The nondimensional transport equation sometimes referred to as Convection-Diffusion equation is given by\nPe \\frac{du}{dx} - \\frac{d^2 u}{dx^2} = f(x), \\text{ in } \\Omega,\\tag{22a}\nu(x) = 0, \\text{ on } \\partial \\Omega,\\tag{22b}\nwhere x = x/L is the nondimensional spatial coordinate, and L is the characteristic length of the domain. The P\u00e9clet number Pe = Lv_x/a describes the relative importance of convection represented by the velocity of fluid flow $v_x$ to diffusion indicated by the diffusion coefficient a, which measures the rate of spreading of the physical quantity [33, 34]. The nondimensional forcing term f(x) = f(x)/a is defined in the domain $\\Omega$ with zero boundary conditions at both ends of the domain $\\partial \\Omega$. Hence, for small values of a, small perturbations of f(x) lead to large local values of du/dx, which introduce thin regions near the boundaries known as layers where the solution undergoes sharp changes [34]. The weak-form loss function takes the form\n\\frac{1}{N_e K} \\sum_{n=1}^{N_e} \\sum_{k=1}^{K(n)} (\\sum_{q=1}^Q J W_q (Pe \\frac{\\partial u}{\\partial x} - \\frac{\\partial^2 u}{\\partial x^2} - f(x)) \\upsilon_k)^2 + (Pe (\\frac{\\partial u \\upsilon_k}{\\upsilon_k})_{x_{n-1}}^{x_n}) +  \\sum_{q=1}^Q J W_q (Pe \\frac{\\partial u}{\\partial x} - \\frac{\\partial^2 u}{\\partial x^2} - f(x)) \\upsilon_k^2),\\tag{23}\nTaking into account various orders of test function and mesh sizes, the solution to equation (23) is derived by FENNM as illustrated in Figure 8. A comparison between the FEM and FENNM solutions compared to the analytical solution using a mesh of 22 elements using linear test functions is presented in Figure 8 (a). As expected, the FEM solution introduces oscillations within the computational domain because the element size is larger than the diffusion coefficient [34]. However, FENNM captures the sharp change in solution caused by the layer at the boundary with a uniform PWE except for the last element, as illustrated in Figure 8 (d). In FEM, the solution is based on solving for the nodal values, in which, for coarse meshes and large P\u00e9clet number Pe, is more influenced by the convective term which shares the information of the solution between every other node [34]. Conversely, FENNM relies on reducing"}, {"title": "4.3.4. One-Dimensional Poisson's Equation with Asymmetric Steep Solution", "content": "Consider the following Poisson's equation taken from [9]\n\\frac{d^2 U}{dx^2} = f(x), \\text{ in } \\Omega = [-1,1],\\tag{24a}\nU(-1) = g, U(1) = h,\\tag{24b}\n\\frac{1}{N_e K} \\sum_{n=1}^{N_e} \\sum_{k=1}^{K(n)} ( \\sum_{q=1}^Q J W_q  (\\frac{d^2 U}{dx^2} -f(x)) \\upsilon_k )^2 + (\\frac{\\partial U \\upsilon_k}{\\partial x} - \\sum_{q=1}^Q J W_q f(x)  \\upsilon_k^2,\\tag{25}\nFor asymmetric steep solution, a manufactured solution of equation (24) can take the form\nU(x) = 0.1sin(8\\pi x) + tanh(80(x + 0.1)),\\tag{26}\nwhere the forcing term is obtained by substituting the manufactured solution (26) into equation (24). Figure 9 presents the FENNM solution of equation (24) for two different meshes with their corresponding PWE and training histories. The FENNM solution is evaluated against the analytical solution employing Lagrange quartic test functions with a mesh of 30 elements, as depicted in Figure 9 (a). The PWE illustrated in Figure 9 (b) shows higher errors in the elements corresponding to the location of the sharp asymmetric change. FENNM is trained on the weak-form weighted residuals, while the DE residuals are not used during training. The residuals of the DE shown in Figure 9 (b) correlates well with the PWE and can thus be a good proxy to inform mesh refinement."}, {"title": "5. Conclusion", "content": "The finite element neural network method (FENNM) is developed within Petrov-Galerkin method, where the NN provides the global nonlinear space of solutions, while the test functions belong to the Lagrange test function space and have at least one nonvanishing value at the element boundaries. Consequently, the flux information across the elements is now implemented inside the weak-form loss function. FENNM offers several key advantages:\n*   Convolutions are a fast and parallel way to evaluate the Gauss integration in each element.\n*   Solving for the weak-form of the differential equations reduces the order of the derivatives in the loss function and the associated error. In addition, it reduces the number of *backpropagation* processes, which speeds up the training process compared to vanilla PINN.\n*   Clear lower and upper limits for the number of quadrature points were established. These limits ensure the approximation of the integral terms within the loss function with the lowest computation cost.\n*   Having nonvanishing values at the elements' boundaries enables imposing natural boundary conditions and intermediate boundary conditions such as point force in the residual loss function with just one network, reducing the number of competing terms in the total loss function. This capability can be extended to include domains with different properties within the loss function for the same inputs rather than adding these properties as additional inputs to the network, which would enhance the optimization and generalization of FENNM.\n*   The strong-form of the residual is not used during training and can be evaluated as a posterior to provide a proxy to guide mesh refinement.\n*   It was shown that, in contrast to FEM, FENNM can be used to discretize and solve in the time dimension, which opens opportunities for approximating solutions for space-time problems in higher dimensions.\n*   FENNM takes a step closer to classical FEM, narrowing the gap between machine learning and traditional numerical methods.\nFuture developments may involve broadening this approach to cover two and three dimensions, integrating time and parameter spaces, handling unstructured meshes, and addressing parametric identification challenges."}]}