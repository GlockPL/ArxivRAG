{"title": "Facial Wrinkle Segmentation for Cosmetic\nDermatology: Pretraining with Texture\nMap-Based Weak Supervision", "authors": ["Junho Moon", "Haejun Chung", "Ikbeom Jang"], "abstract": "Facial wrinkle detection plays a crucial role in cosmetic der-\nmatology. Precise manual segmentation of facial wrinkles is challenging\nand time-consuming, with inherent subjectivity leading to inconsistent\nresults among graders. To address this issue, we propose two solutions.\nFirst, we build and release the first public facial wrinkle dataset, 'FFHQ-\nWrinkle', an extension of the NVIDIA FFHQ dataset. This dataset in-\ncludes 1,000 images with human labels and 50,000 images with auto-\nmatically generated weak labels. This dataset can foster the research\ncommunity to develop advanced wrinkle detection algorithms. Second,\nwe introduce a training strategy for U-Net-like encoder-decoder models\nto detect wrinkles across the face automatically. Our method employs a\ntwo-stage training strategy: texture map pretraining and finetuning on\nhuman-labeled data. Initially, we pretrain models on a large dataset with\nweak labels (N=50k) or masked texture maps generated through com-\nputer vision techniques, without human intervention. Subsequently, we\nfinetune the models using human-labeled data (N=1k), which consists of\nmanually labeled wrinkle masks. During finetuning, the network inputs\na combination of RGB and masked texture maps, comprising four chan-\nnels. We effectively combine labels from multiple annotators to minimize\nsubjectivity in manual labeling. Our strategies demonstrate improved\nsegmentation performance in facial wrinkle segmentation both quantita-\ntively and visually compared to existing pretraining methods.", "sections": [{"title": "1 Introduction", "content": "With the growing interest in dermatological diseases and skin aesthetics, pre-\ndicting facial wrinkles is becoming increasingly significant. Facial wrinkles serve\nas critical indicators of aging [2,19,20], and are essential for evaluating skin"}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Deep learning-based facial wrinkle segmentation", "content": "Deep learning-based methods for facial wrinkle segmentation aim to enable neu-\nral network models to learn the features necessary for accurate wrinkle detection\nautonomously. Kim et al. [14] introduced a semi-automatic labeling strategy to\nenhance performance by extracting texture maps from face images and combin-\ning them with roughly labeled wrinkle masks, utilizing a U-Net architecture [23]\nfor segmentation. In a subsequent study [15], they further improved segmentation\naccuracy by implementing a weighted deep supervision technique, which employs\na weighted wrinkle map to more precisely calculate the loss for the downsampled\ndecoder, outperforming traditional deep supervision methods. Yang et al. [34]\ndeveloped Striped WriNet, which integrates a Striped Attention Module com-\nposed of Multi-Scale Striped Attention and Global Striped Attention within a\nU-shaped network. This approach applies an attention mechanism across multi-\nple scales, effectively segmenting both coarse and fine wrinkles."}, {"title": "2.2 Weakly supervised learning", "content": "Weakly supervised learning is a methodology that trains models using incomplete\nor inaccurate labeled data instead of fully labeled data in situations where strong\nsupervision information is lacking [36]. Xu et al. [33] proposed CAMEL, a weakly\nsupervised learning framework that uses a MIL-based label expansion technique\nto divide images into grid-shaped instances and automatically generate instance-\nlevel labels, enabling histopathology image segmentation with only image-level"}, {"title": "3 Dataset", "content": ""}, {"title": "3.1 Dataset specifications", "content": "The first public facial wrinkle dataset, 'FFHQ-Wrinkle', comprises pairs of face\nimages and their corresponding wrinkle masks. We focused on wrinkle labels\nwhile utilizing the existing high-resolution face image dataset FFHQ (Flickr-\nFaces-HQ) [12], which contains 70,000 high-resolution (1024x1024) face images\ncaptured under various angles and lighting conditions. The dataset we provide\nconsists of one set of manually labeled wrinkle masks (N=1,000) and one set of\n\"weak\" wrinkle masks, or masked texture maps, generated without human labor\n(N=50,000). We selected 50,000 images from the FFHQ dataset, specifically\nimage IDs 00000 to 49999. We used these 50,000 face images to create the weakly\nlabeled wrinkles and randomly sampled 1,000 images from these to create the\nground truth wrinkles. The methods for generating weakly labeled wrinkles and\nground truth wrinkles are discussed in Section 4.2. As illustrated in Fig. 2, the\ndataset includes individuals of diverse ages, races, and backgrounds, featuring a\nvariety of skin conditions such as freckles, acne, and pigmentation. This diversity\nmakes the dataset particularly suitable for training models to handle the wide\nrange of skin conditions encountered in clinical settings. The dataset is publicly\navailable\u00b3."}, {"title": "3.2 Ground truth wrinkle annotation", "content": "For ground truth wrinkles, we manually annotated the face images. The an-\nnotation process involved three annotators with extensive experience in image\nprocessing and analysis. Wrinkles can be categorized into two types dynamic\nwrinkles and static wrinkles [31]. Dynamic wrinkles are formed by facial mus-\ncles and appear with expressions but disappear when the face is at rest. Static\n(permanent) wrinkles are visible even when the face is at rest and result from\nthe repeated formation of dynamic wrinkles over time. Treatments for these\ncategories also differ: dynamic wrinkles are often treated with muscle relaxants\nlike Botox [28], while static wrinkles may require dermal fillers [8] or resur-\nfacing treatments [7]. We annotated both types of wrinkles. Given the subjec-\ntivity inherent in wrinkle data, a consistent standard for wrinkle assessment\nwas established prior to the commencement of labeling. The annotators con-\nducted three synchronization sessions to minimize inter-rater variability. The\nannotation primarily targeted the forehead, crow's feet, and nasolabial folds,\nencompassing the overall facial area. Due to the high resolution and diversity"}, {"title": "4 Method", "content": ""}, {"title": "4.1 Model architecture", "content": "We evaluated our proposed method using the U-Net [23] and Swin UNETR [9]\narchitectures, with U-Net serving as the base model for ablation studies and\nadditional experiments. As depicted in Fig. 1, the U-Net model features a stan-\ndard architecture comprising four encoder blocks and four decoder blocks. The\nSwin UNETR model employs an encoder with a window size of 16 and patches\nof size 4x4, projecting the input patch into a 48-dimensional embedding space.\nThis model includes four encoder blocks, each consisting of two successive Swin\nTransformer blocks [16], and four decoder blocks."}, {"title": "4.2 Training strategy", "content": "We train the segmentation model using a substantial number of masked texture\nmaps in a weakly supervised manner, followed by finetuning with a smaller set"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Implementation details", "content": "In both the weakly supervised pretraining and supervised finetuning stages, we\nutilize the original 1024x1024 image-label pairs as inputs without resizing. The\nAdamW optimizer [18] is employed, configured with a weight decay of 0.05, \u03b2\u2081\nset to 0.9, and \u03b2\u2082 set to 0.999. We also implement the SGDR scheduler [17]. \u03a4\u03bf\nmaintain dataset diversity, we randomly apply various augmentations, including"}, {"title": "5.2 Evaluation metrics", "content": "To evaluate the performance of the final finetuned model in wrinkle segmenta-\ntion, we use the Jaccard Similarity Index (JSI), F1-score, and Accuracy (Acc).\nThe Jaccard Similarity Index measures the overlap between the predicted\nwrinkle regions and the ground truth regions, defined as follows:\nJSI = $\\frac{A \\cap B}{A \\cup B}$\nwhere A is the predicted segmentation, and B is the actual label.\nThe F1-score is the harmonic mean of precision and recall, while accuracy\nmeasures the proportion of correctly predicted pixels out of the total pixels.\nThey are defined as follows:\nPrecision = $\\frac{TP}{TP + FP}$\nRecall = $\\frac{TP}{TP+FN}$\nFl-score=2\u00d7 $\\frac{Precision \u00d7 Recall}{Precision + Recall}$\nAcc = $\\frac{TP+TN}{TP+TN+FP+ FN}$\nwhere TP is the number of true positives, FP is the number of false positives,\nFN is the number of false negatives, and TN is the number of true negatives."}, {"title": "5.3 Results", "content": "To evaluate the performance of our proposed method, we first compare it with\nthe latest methods: the semi-automatic labeling and weighted deep supervision\nmethod [15], and the Striped WriNet method [34]. Because the primary contri-\nbution of this work is the pretraining strategy, we also compare it with other\npretraining techniques. They include using ImageNet pretrained models and self-\nsupervised learning methods. For the ImageNet pretrained models, we replace\nthe encoder part of the U-shape architecture with models pretrained on the\nImageNet-1K dataset [24]; specifically, we use ResNet-50 [10] for U-Net and\nSwin-T [16] for Swin UNETR. For the self-supervised learning methods, we use\ndenoising self-supervised learning [3] for pretraining U-Net, setting the Gaussian\ndistribution's standard deviation to 0.2, and masked image prediction [32] for\npretraining Swin UNETR, using 32x32 masked patches and a 60% masking ra-\ntio. All training hyperparameters follow those specified in Section 5.1. To assess\nperformance in scenarios with very limited labeled data, we train our model on\nthe full training set (100%, N=800) and on a randomly sampled subset (5%,\nN=40).\nThe proposed method outperforms the latest wrinkle segmentation meth-\nods and the ones using the same model architectures with different pertaining\nmethods. The performance gap is much larger in data-limited situations-i.e.,\nfine-tuned on 5% of the manually-labeled data. Table 2 shows quantitative com-\nparisons of wrinkle segmentation performance for each method using U-Net and\nSwin UNETR architectures. Our method consistently achieves the highest per-\nformance across both datasets and architectures. Fig. 7 presents a qualitative\ncomparison of our method with denoising pretraining using U-Net, which is the\nnext best performing method in experiments using 100% of the data."}, {"title": "5.4 Ablation study", "content": "Incorporating the masked texture map as an additional input during the fine-\ntuning stage led to significant improvements in wrinkle segmentation, demon-\nstrating the effectiveness of our approach. Table 3 presents quantitative compar-\nisons using the U-Net architecture to assess the benefits of including a 1-channel\nmasked texture map as an additional input during finetuning. We compare our\npretraining method (Texture map pretraining) with a conventional approach\n(No pretraining), which is trained solely on manually labeled data, both with\n(RGB+Texture) and without (RGB) the additional masked texture map input."}, {"title": "6 Discussion", "content": "Our approach achieves state-of-the-art performance when compared to two pub-\nlicly released models specifically designed for wrinkle segmentation, in addition\nto outperforming ImageNet pretrained models and self-supervised learning meth-\nods. We demonstrate that our two-stage training strategy significantly enhances"}, {"title": "7 Conclusion", "content": "We propose a two-stage learning strategy for facial wrinkle segmentation using\ndeep learning. Our approach leverages the knowledge gained from extracting\nfacial features for transfer learning in facial wrinkle segmentation. This allows\nus to achieve high segmentation performance even with limited manually labeled\ndata, thereby establishing a state-of-the-art training strategy. This significantly\nreduces the time and cost required for manual labeling of wrinkles, thus offering\npotential benefits in cosmetic dermatology. Furthermore, to support ongoing\nresearch and enhance reproducibility, we have made our facial wrinkle dataset\npublicly available."}]}