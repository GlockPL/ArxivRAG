{"title": "Facial Wrinkle Segmentation for Cosmetic Dermatology: Pretraining with Texture Map-Based Weak Supervision", "authors": ["Junho Moon", "Haejun Chung", "Ikbeom Jang"], "abstract": "Facial wrinkle detection plays a crucial role in cosmetic dermatology. Precise manual segmentation of facial wrinkles is challenging and time-consuming, with inherent subjectivity leading to inconsistent results among graders. To address this issue, we propose two solutions. First, we build and release the first public facial wrinkle dataset, 'FFHQ-Wrinkle', an extension of the NVIDIA FFHQ dataset. This dataset includes 1,000 images with human labels and 50,000 images with automatically generated weak labels. This dataset can foster the research community to develop advanced wrinkle detection algorithms. Second, we introduce a training strategy for U-Net-like encoder-decoder models to detect wrinkles across the face automatically. Our method employs a two-stage training strategy: texture map pretraining and finetuning on human-labeled data. Initially, we pretrain models on a large dataset with weak labels (N=50k) or masked texture maps generated through computer vision techniques, without human intervention. Subsequently, we finetune the models using human-labeled data (N=1k), which consists of manually labeled wrinkle masks. During finetuning, the network inputs a combination of RGB and masked texture maps, comprising four channels. We effectively combine labels from multiple annotators to minimize subjectivity in manual labeling. Our strategies demonstrate improved segmentation performance in facial wrinkle segmentation both quantitatively and visually compared to existing pretraining methods.", "sections": [{"title": "1 Introduction", "content": "With the growing interest in dermatological diseases and skin aesthetics, predicting facial wrinkles is becoming increasingly significant. Facial wrinkles serve as critical indicators of aging [2,19,20], and are essential for evaluating skin"}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Deep learning-based facial wrinkle segmentation", "content": "Deep learning-based methods for facial wrinkle segmentation aim to enable neural network models to learn the features necessary for accurate wrinkle detection autonomously. Kim et al. [14] introduced a semi-automatic labeling strategy to enhance performance by extracting texture maps from face images and combining them with roughly labeled wrinkle masks, utilizing a U-Net architecture [23] for segmentation. In a subsequent study [15], they further improved segmentation accuracy by implementing a weighted deep supervision technique, which employs a weighted wrinkle map to more precisely calculate the loss for the downsampled decoder, outperforming traditional deep supervision methods. Yang et al. [34] developed Striped WriNet, which integrates a Striped Attention Module composed of Multi-Scale Striped Attention and Global Striped Attention within a U-shaped network. This approach applies an attention mechanism across multiple scales, effectively segmenting both coarse and fine wrinkles."}, {"title": "2.2 Weakly supervised learning", "content": "Weakly supervised learning is a methodology that trains models using incomplete or inaccurate labeled data instead of fully labeled data in situations where strong supervision information is lacking [36]. Xu et al. [33] proposed CAMEL, a weakly supervised learning framework that uses a MIL-based label expansion technique to divide images into grid-shaped instances and automatically generate instance-level labels, enabling histopathology image segmentation with only image-level"}, {"title": "3 Dataset", "content": ""}, {"title": "3.1 Dataset specifications", "content": "The first public facial wrinkle dataset, 'FFHQ-Wrinkle', comprises pairs of face images and their corresponding wrinkle masks. We focused on wrinkle labels while utilizing the existing high-resolution face image dataset FFHQ (Flickr-Faces-HQ) [12], which contains 70,000 high-resolution (1024x1024) face images captured under various angles and lighting conditions. The dataset we provide consists of one set of manually labeled wrinkle masks (N=1,000) and one set of \"weak\" wrinkle masks, or masked texture maps, generated without human labor (N=50,000). We selected 50,000 images from the FFHQ dataset, specifically image IDs 00000 to 49999. We used these 50,000 face images to create the weakly labeled wrinkles and randomly sampled 1,000 images from these to create the ground truth wrinkles. The methods for generating weakly labeled wrinkles and ground truth wrinkles are discussed in Section 4.2. As illustrated in Fig. 2, the dataset includes individuals of diverse ages, races, and backgrounds, featuring a variety of skin conditions such as freckles, acne, and pigmentation. This diversity makes the dataset particularly suitable for training models to handle the wide range of skin conditions encountered in clinical settings. The dataset is publicly available\u00b3."}, {"title": "3.2 Ground truth wrinkle annotation", "content": "For ground truth wrinkles, we manually annotated the face images. The annotation process involved three annotators with extensive experience in image processing and analysis. Wrinkles can be categorized into two types dynamic wrinkles and static wrinkles [31]. Dynamic wrinkles are formed by facial muscles and appear with expressions but disappear when the face is at rest. Static (permanent) wrinkles are visible even when the face is at rest and result from the repeated formation of dynamic wrinkles over time. Treatments for these categories also differ: dynamic wrinkles are often treated with muscle relaxants like Botox [28], while static wrinkles may require dermal fillers [8] or resurfacing treatments [7]. We annotated both types of wrinkles. Given the subjectivity inherent in wrinkle data, a consistent standard for wrinkle assessment was established prior to the commencement of labeling. The annotators conducted three synchronization sessions to minimize inter-rater variability. The annotation primarily targeted the forehead, crow's feet, and nasolabial folds, encompassing the overall facial area. Due to the high resolution and diversity"}, {"title": "4 Method", "content": ""}, {"title": "4.1 Model architecture", "content": "We evaluated our proposed method using the U-Net [23] and Swin UNETR [9] architectures, with U-Net serving as the base model for ablation studies and additional experiments. As depicted in Fig. 1, the U-Net model features a standard architecture comprising four encoder blocks and four decoder blocks. The Swin UNETR model employs an encoder with a window size of 16 and patches of size 4x4, projecting the input patch into a 48-dimensional embedding space. This model includes four encoder blocks, each consisting of two successive Swin Transformer blocks [16], and four decoder blocks."}, {"title": "4.2 Training strategy", "content": "We train the segmentation model using a substantial number of masked texture maps in a weakly supervised manner, followed by finetuning with a smaller set"}, {"title": "Weakly supervised pretraining stage", "content": "In the pretraining stage, we utilized weakly labeled wrinkle data automatically extracted through computer vision techniques without human intervention as the ground truth. Fig. 4 illustrates the pipeline for generating weakly labeled wrinkles for the weakly supervised pretraining stage. Utilizing Equation (1), we extracted the texture map [14] from the face image through a Gaussian kernel-based filter.\n\n$T(x, y) = (1 - \\frac{I(x,y)}{1+ I_{G(\\sigma)} (x, y)}) \\times 255$ (1)\n\nwhere G represents the Gaussian kernel, $\\sigma$ denotes its standard deviation, $I_{G(\\sigma)}$ is the Gaussian filtered image, and (x, y) are the pixel coordinates in the image. Following the methodology in [14], we set the Gaussian kernel's standard deviation to 5 and its size to 21x21 for texture map extraction. The extracted texture map contains detailed information about the contours, curves, and skin textures of the face image. However, as the texture map includes numerous false positives from the background, we employ a BiSeNet [35] architecture-based facial parsing deep learning model to mask non-facial regions, resulting in the final masked texture map used as ground truth. We avoid converting the masked texture map into a binary mask by setting a threshold due to the variability in the size, shape, and depth of wrinkles, which makes determining an appropriate threshold"}, {"title": "Supervised finetuning stage", "content": "For the ground truth in the finetuning stage, we utilized human-labeled wrinkle data generated as described in Section 3.2. Fig. 6 illustrates the pipeline of the ground truth generation of the wrinkle mask. To produce a reliable ground truth wrinkle mask, we used majority voting to retain only the pixels that were labeled by at least two groups, thereby reducing variability among the annotators. Fig. 2-(c) displays the manual wrinkle mask used as the final ground truth in the supervised finetuning stage. As model"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Implementation details", "content": "In both the weakly supervised pretraining and supervised finetuning stages, we utilize the original 1024x1024 image-label pairs as inputs without resizing. The AdamW optimizer [18] is employed, configured with a weight decay of 0.05, $\\beta_1$ set to 0.9, and $\\beta_2$ set to 0.999. We also implement the SGDR scheduler [17]. To maintain dataset diversity, we randomly apply various augmentations, including"}, {"title": "5.2 Evaluation metrics", "content": "To evaluate the performance of the final finetuned model in wrinkle segmentation, we use the Jaccard Similarity Index (JSI), F1-score, and Accuracy (Acc). The Jaccard Similarity Index measures the overlap between the predicted wrinkle regions and the ground truth regions, defined as follows:\n\n$JSI = \\frac{A \\cap B}{A \\cup B}$ (4)\n\nwhere A is the predicted segmentation, and B is the actual label.\n\nThe F1-score is the harmonic mean of precision and recall, while accuracy measures the proportion of correctly predicted pixels out of the total pixels. They are defined as follows:\n\n$Precision = \\frac{TP}{TP + FP}$ (5)\n\n$Recall = \\frac{TP}{TP+FN}$ (6)\n\n$Fl-score=2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$ (7)\n\n$Acc = \\frac{TP+TN}{TP+TN+FP+ FN}$ (8)\n\nwhere TP is the number of true positives, FP is the number of false positives, FN is the number of false negatives, and TN is the number of true negatives."}, {"title": "5.3 Results", "content": "To evaluate the performance of our proposed method, we first compare it with the latest methods: the semi-automatic labeling and weighted deep supervision method [15], and the Striped WriNet method [34]. Because the primary contribution of this work is the pretraining strategy, we also compare it with other pretraining techniques. They include using ImageNet pretrained models and self-supervised learning methods. For the ImageNet pretrained models, we replace the encoder part of the U-shape architecture with models pretrained on the ImageNet-1K dataset [24]; specifically, we use ResNet-50 [10] for U-Net and Swin-T [16] for Swin UNETR. For the self-supervised learning methods, we use denoising self-supervised learning [3] for pretraining U-Net, setting the Gaussian distribution's standard deviation to 0.2, and masked image prediction [32] for pretraining Swin UNETR, using 32x32 masked patches and a 60% masking ratio. All training hyperparameters follow those specified in Section 5.1. To assess performance in scenarios with very limited labeled data, we train our model on the full training set (100%, N=800) and on a randomly sampled subset (5%, N=40).\n\nThe proposed method outperforms the latest wrinkle segmentation methods and the ones using the same model architectures with different pertaining methods. The performance gap is much larger in data-limited situations-i.e., fine-tuned on 5% of the manually-labeled data. Table 2 shows quantitative comparisons of wrinkle segmentation performance for each method using U-Net and Swin UNETR architectures. Our method consistently achieves the highest performance across both datasets and architectures. Fig. 7 presents a qualitative comparison of our method with denoising pretraining using U-Net, which is the next best performing method in experiments using 100% of the data."}, {"title": "5.4 Ablation study", "content": "Incorporating the masked texture map as an additional input during the finetuning stage led to significant improvements in wrinkle segmentation, demonstrating the effectiveness of our approach. Table 3 presents quantitative comparisons using the U-Net architecture to assess the benefits of including a 1-channel masked texture map as an additional input during finetuning. We compare our pretraining method (Texture map pretraining) with a conventional approach (No pretraining), which is trained solely on manually labeled data, both with (RGB+Texture) and without (RGB) the additional masked texture map input."}, {"title": "6 Discussion", "content": "Our approach achieves state-of-the-art performance when compared to two publicly released models specifically designed for wrinkle segmentation, in addition to outperforming ImageNet pretrained models and self-supervised learning methods. We demonstrate that our two-stage training strategy significantly enhances"}, {"title": "7 Conclusion", "content": "We propose a two-stage learning strategy for facial wrinkle segmentation using deep learning. Our approach leverages the knowledge gained from extracting facial features for transfer learning in facial wrinkle segmentation. This allows us to achieve high segmentation performance even with limited manually labeled data, thereby establishing a state-of-the-art training strategy. This significantly reduces the time and cost required for manual labeling of wrinkles, thus offering potential benefits in cosmetic dermatology. Furthermore, to support ongoing research and enhance reproducibility, we have made our facial wrinkle dataset publicly available."}]}