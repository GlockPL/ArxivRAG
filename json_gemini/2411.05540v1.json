{"title": "CRepair: CVAE-based Automatic Vulnerability Repair Technology", "authors": ["Penghui Liu", "Yingzhou Bi", "Jiangtao Huang", "Xinxin Jiang", "Lianmei Wang"], "abstract": "Software vulnerabilities are flaws in computer software systems that pose significant threats to the integrity, security, and reliability of modern software and its application data. These vulnerabilities can lead to substantial economic losses across various industries. Manual vulnerability repair is not only time-consuming but also prone to errors. To address the challenges of vulnerability repair, researchers have proposed various solutions, with learning-based automatic vulnerability repair techniques gaining widespread attention. However, existing methods often focus on learning more vulnerability data to improve repair outcomes, while neglecting the diverse characteristics of vulnerable code, and suffer from imprecise vulnerability localization. To address these shortcomings, this paper proposes CRepair, a CVAE-based automatic vulnerability repair technology aimed at fixing security vulnerabilities in system code. We first preprocess the vulnerability data using a prompt-based method to serve as input to the model. Then, we apply causal inference techniques to map the vulnerability feature data to probability distributions. By employing multi-sample feature fusion, we capture diverse vulnerability feature information. Finally, conditional control is used to guide the model in repairing the vulnerabilities.Experimental results demonstrate that the proposed method significantly outperforms other benchmark models, achieving a perfect repair rate of 52%. The effectiveness of the approach is validated from multiple perspectives, advancing AI-driven code vulnerability repair and showing promising applications.", "sections": [{"title": "1. INTRODUCTIO", "content": "Software vulnerabilities refer to flaws in the computational logic or code of a software system, which can be exploited by malicious actors to cause significant harm to the system[1]. With the rapid development of the internet, both the complexity of software and the number of vulnerabilities have surged, becoming a major threat to cybersecurity. Traditional vulnerability repair methods require extensive human intervention, are inefficient, and often lead to new vulnerabilities[2]. Due to the diverse root causes and repair methods for different vulnerabilities, researchers have classified vulnerabilities based on their intrinsic characteristics to facilitate understanding and repair. This has led to the formation of various vulnerability types. Currently, the international Common Weakness Enumeration (CWE) system includes 939 types of vulnerabilities[3], aiding researchers in vulnerability analysis and remediation. However, the vast number and complexity of vulnerability types make it difficult to analyze repair mechanisms for each one. As a result, general vulnerability repair methods, which can be applied to multiple types of vulnerabilities, have garnered significant attention from researchers.The current mainstream approach for general vulnerability repair is a history-driven deep learning model. This method takes vulnerable code as input and repair patches as output, focusing on learning from historical repair versions to achieve automatic vulnerability repair. However, existing public vulnerability datasets are often small in size and low in quality, and vulnerable code is typically lengthy, making both vulnerability localization and repair challenging. The key challenge for automatic vulnerability repair lies in how to use limited datasets to accurately repair vulnerabilities and quickly locate them.\nTo address the issue of limited vulnerability datasets, Nong et al.[4] attempted to generate realistic vulnerabilities by injecting them into real code data using neural code techniques. However, there remains a significant difference between synthetic data and real vulnerability data. To focus on the repair of actual vulnerability data, Chen et al.[5] proposed VRepair, an automatic vulnerability repair technique based on transfer learning. VRepair col-"}, {"title": "2. RELATED WORK", "content": "Code vulnerabilities refer to serious flaws in the computational logic or code of a software system, which can be easily exploited to cause harm. Common examples include data breaches, system intrusions, and unauthorized modifications of privileges, all of which pose severe threats to system security. Timely repair of these vulnerabilities can prevent potential attacks, making vulnerability repair a core task in cybersecurity.To ensure system security, numerous researchers have conducted studies on vulnerability repair. For instance, Lee et al. [8] proposed MemFix, an automated patch generation technique based on a two-step process of detection and repair. For each memory allocation statement, MemFix generates patches and combines them to repair vulnerabilities. However, this approach sacrifices efficiency and scalability, as it is limited to repairing specific types of vulnerabilities and results in a high false positive rate. To address this issue, researchers have proposed vulnerability repair based on dynamic analysis, where vulnerabilities are located and repaired while the code is being executed.\nWang et al.[9] addressed the limitations of static analysis in vulnerability detection by proposing a hybrid approach that combines both static and dynamic methods. They also employed taint analysis techniques to validate security vulnerabilities, thereby improving the efficiency of vulnerability remediation. However, the implementation of these methods is often complex and can lead to issues where the remediation does not meet expectations, making it difficult to ensure that vulnerabilities are properly fixed.As machine learning and deep learning technologies have matured, they have seen widespread application in fields such as image recognition, speech recognition, and natural language processing, achieving significant breakthroughs. In response to the challenges posed by traditional static and dynamic analysis-based vulnerability remediation\u2014such as high false-positive rates and the considerable time and effort required from researchers\u2014researchers have framed automatic vulnerability repair as a neural machine translation (NMT) task[10]. The goal of NMT is to learn the mapping between vulnerable code and its correct, post-repair version. This is similar to the sequence-to-sequence (seq2seq) approach, which learns the mapping between two sequences[11], a method commonly used in neural machine translation, text summarization[12], and other natural language processing tasks.However, due to the limited availability of public vulnerability datasets and the large amounts of data required for training machine learning models, these methods struggle to learn effec-"}, {"title": "3. BACKGROUND KNOWLEDGE", "content": "The method proposed in this paper primarily employs Conditional Variational Autoencoders (CVAE) as the overall framework. It utilizes prompt engineering and the Byte-Pair Encoding (BPE) tokenization algorithm for preprocessing vulnerability data. The following sections will introduce the relevant concepts and foundational knowledge."}, {"title": "3.1. Variational inference and conditional variational autoencoders", "content": "Conditional Variational Autoencoders (CVAE) are an extension of Variational Autoencoders (VAE)[17], designed as generative models that introduce conditional variables to precisely control specific features of the generated data. CVAE utilizes an encoder-decoder architecture, combined with sampling mechanisms for deep learning, which has led to its widespread application in fields such as image generation and natural language processing (NLP)[18]. In the realm of text generation, CVAE is particularly significant as it not only enhances the model's ability to understand context but also improves the quality and generalization of the generated content. By employing conditional controls, CVAE ensures that the generated text meets specific conditions, resulting in more accurate and relevant outputs. Specifically, CVAE can implement variational inference through parameterized neural networks, learning the latent structure of data and constructing corresponding probability distributions. By sampling from these distributions, it captures the diverse features of the data, helping to mitigate issues of data scarcity and the singularity of feature information.\nIn this paper, vulnerability data can be viewed as generated by unobservable latent variables that satisfy a certain specific distribution through random processes. This distribution represents the intrinsic features of the vulnerability code or the organizational structure between contexts. Let us denote the vulnerability code dataset as {(xi, Yi)|Xi \u2208 X,Yi \u2208 Y}1,Here, X represents the vulnerability dataset, while Y denotes the corresponding repaired dataset. N is the total number of samples in the dataset. For the conditional variable c, this paper will extract feature information from the repaired code to guide the model in repairing the vulnerable code. Based on the aforementioned conditions, we employ variational inference to optimize the variational lower bound of the marginal likelihood, specifically the Evidence Lower Bound (ELBO), to enhance the model:\n$\\mathcal{L} = E_{q(z|x,c)} [log p(y|z, c)] \u2013 D_{KL}(q(z|x, c)||p(z|c))$\nThe first term represents the reconstruction error, which is the expected log-likelihood of the data generated by the decoder under the distribution of the latent variable z. Since the true posterior distribution p(z|x, c) is often intractable to compute directly, the second term utilizes variational inference to minimize the Kullback-Leibler (KL) divergence between the posterior distribution q(z|x, c) from the encoder and the prior distribution q(z|c), thereby driving the posterior distribution closer to the prior distribution. Here, q(z|x, c) indicates the distribution parameters of the latent variable z outputted by the encoder, which receives the input data x and the conditional variable c. This can be expressed using the following formula:\nq(z|x, c) = N(y; \u03bc' (z, c), \u03c3'\u00b2(z, c))\nN indicates that a Gaussian distribution is used for calculations, where \u03bc' (z, c) and o'\u00b2(z, c) represent the computed mean and variance, respectively. In this context, the distribution of z is an approximate probability distribution that aims to closely approximate the true posterior distribution by minimizing the Kullback-Leibler (KL) divergence with respect to the prior distribution p(z|c). This process helps construct a better representation of the distribution for the latent variable z, which is essential for subsequent sampling and decoding operations.\nIn the first term, p(y|z, c) represents the distribution of the output y generated by the decoder, which receives the latent variable z and the conditional variable c. This distribution can also be computed using a Gaussian"}, {"title": "3.2. Prompt Engineering", "content": "Prompt engineering is a technical approach that utilizes prompts to facilitate knowledge transfer during task execution, similar to a software pattern. This method enhances the model's understanding of tasks by providing rich feature information specific to the context[19]. During model training, prompts can be added to guide the model to learn particular tasks or behaviors. As shown in Figure 2-c, this paper introduces prompt engineering during the data preprocessing phase. Specifically, CWE vulnerability prompt information is embedded at the beginning of the vulnerable source code. This vulnerability type prompt helps the model quickly identify the type of vulnerability and develop a corresponding repair strategy. Special prompt markers such as <StartLoc >and <EndLoc >are then added around the start and end locations of the vulnerability, enabling the model to more accurately pinpoint the vulnerability and focus on learning its features.By applying prompt engineering, the CRepair model can tailor its learning to different types of vulnerabilities, and quickly and precisely locate vulnerabilities using prompts. This enhances the accuracy of feature extraction and improves the effectiveness of vulnerability repair."}, {"title": "3.3. BPE Algorithm", "content": "Byte Pair Encoding (BPE) is an unsupervised subword tokenization technique widely used in neural machine translation and natural language processing tasks [20]. This technique reduces the vocabulary size by iteratively merging the most frequent byte pairs in the text, effectively addressing the issue of out-of-vocabulary (OOV) words. Specifically, BPE operates in two key steps: character pair merging and vocabulary construction. First, the algorithm breaks down longer words, selects and merges the most frequent character pairs to form new words. These newly formed words are then added to the vocabulary. For instance, when tokenizing an unfamiliar function name like \"calculate_total,\" the output might be the list [\u201ccalculate\u201d, \"total\u201d]. By reorganizing common subwords, BPE reduces vocabulary size, enhancing the model's generalization and flexibility. In this paper, since code text often contains a large number of repetitive structures and patterns, it is highly likely to encounter new identifiers or variable names that are not in the vocabulary. Therefore, BPE is employed to process the code, splitting unfamiliar words into smaller subunits, thereby reducing sparsity and making the model's training process more efficient while improving the generalization of the learned representations."}, {"title": "4. CREPAIR: CVAE-BASED AUTOMATIC VULNERABILITY REPAIR TECHNOLOGY", "content": ""}, {"title": "4.1. Problem Definition", "content": "The goal of this research is to develop a model that can automatically repair vulnerable code by extracting and analyzing the intrinsic features of vulnerability code diversity. Specifically, the vulnerable code (highlighted in red in Figure 2-a) is preprocessed and used as input to the model, with the expectation that the model will output the repaired code, as shown in Figure 2-d. This output is then reviewed by security analysts, who can further refine the vulnerable code into the correct version, as illustrated in Figure 2-b, with the green parts representing the repaired content. In this paper, the vulnerability code dataset is represented as {(xi, Yi)|Xi \u2208 X, Yi \u2208 Y}1, where X denotes the set of vulnerable code, Y represents the corresponding set of repaired code, and N is the total number of samples in the dataset."}, {"title": "4.2. Model Building", "content": "This paper proposes a vulnerability auto-repair technique, CRepair, based on Conditional Variational Autoencoders (CVAE), aimed at addressing the limitations of existing methods that overlook the impact of intrinsic diversity in vulnerability code on repair effectiveness, as well as the issue of large search space caused by slow vulnerability localization. The overall model architecture and repair process are shown in Figure 1. Specifically, the vulnerability repair process is divided into three stages: data preprocessing, model training, and vulnerability repair.\nIn the data preprocessing stage, code block-level vulnerabilities are first expanded into sequences, followed by embedding prompt markers. \u0412\u0420\u0415 (Byte Pair Encoding) is then applied to tokenize the code, which serves as input to the model. During the model training phase, CRepair utilizes the T5 encoder to encode the data and extract vulnerability features. It then computes the mean and variance of these features to construct a probability distribution of the vulnerability characteristics. By sampling from this distribution multiple times, different vulnerability feature samples are obtained, which are then fused to generate the latent variable z. This latent variable, combined with the conditional variable c, is passed to the T5 decoder for decoding. The conditional variable c is derived by extracting features from the true repair samples y. After decoding, linear expansion and maximum probability calculations are used to obtain potential repair patches. The primary goal of the training process is to deeply learn the diverse features of vulnerability code, enabling precise control over the generated repairs through the conditional variable. The model is optimized by maximizing the variational lower bound of the marginal likelihood, which reduces both cross-entropy and KL divergence. In the vulnerability repair phase, the trained CRepair model receives the vulnerable code as input and outputs a ranked set of repair patches through beam search, which serves as a candidate repair set for security analysts to evaluate and implement.The following sections provide a detailed explanation of these three stages."}, {"title": "4.3. Data Preprocessing", "content": "The data preprocessing stage is primarily aimed at reducing noise and outliers in the data, preparing it into a format that the model can accept and use. This ensures that the model can effectively learn from the data, enhancing its generalization capability, performance, and practical utility. The preprocessing in this paper focuses on the following aspects:\nCode Serialization: By converting code blocks into a sequence, it reduces the overhead of control loops and reveals the complete structure and logic of the code. This allows the model to better understand the code's context and execution flow, improving its comprehension of code semantics. Specifically, the original block-level vulnerability (as shown in Figure 2-a) is transformed into a continuous text sequence, removing comments and other distracting noise so that the model can focus on learning the key vulnerability features.\nPrompt Insertion: Adding prompt information to the code sequence helps the model quickly identify the critical features and intent of the code, guiding it to focus on the most important parts of the code. This accelerates the localization of vulnerabilities and prioritizes their learning. Specifically, vulnerabilities are first classified according to their type, and a corresponding CWE vulnerability category is inserted at the beginning of the code sequence as a type prompt. Then, markers indicating the vulnerability's start and end, such as <StartLoc >and <EndLoc >(as shown in Figure 2-c), are embedded to help the model locate the vulnerability. This approach allows the model to tailor different repair strategies for various types of vulnerabilities and quickly focus on them, improving the precision of vulnerability feature extraction.\nTokenization: Given that the syntax structure of code is significantly different from regular text, traditional tokenization methods often mark complex or custom long words as unknown [UNK]. By applying Byte Pair Encoding (BPE), the code text is split into smaller subword units, which addresses the out-of-vocabulary (OOV) issue. This helps the model capture both the semantic and syntactic features of the code, enhancing the accuracy of feature extraction. In this study, the vulnerability code sequences are tokenized us-"}, {"title": "4.4. Model Training", "content": "In the model training phase, CRepair is built on the Conditional Variational Autoencoder (CVAE) framework, utilizing the encoder and decoder modules from the T5 model. Specifically, CRepair first uses the encoder to extract vulnerability features, including the mean and variance of the vulnerability distribution, which are then used to construct a probabilistic distribution of the vulnerability characteristics. Next, by sampling from this distribution, the model captures the feature information of the vulnerability and uses this as the input vector for the decoder. Finally, the decoder processes these features to generate repair patches. In this work, a multi-sample feature fusion technique is introduced during the sampling process, where-\n$\\mathcal{L}_{CVAE} = L_{RC} + L_{KL}$"}, {"title": "4.5. vulnerability repair", "content": "After the model training is complete, this paper employs a beam search strategy [26] during the generation phase. Specifically, beam search initially selects multiple candidates, and at each time step, it retains the top few tokens with the highest probabilities as candidates. Based on the set beam"}, {"title": "6. CONCLUSION", "content": "Automated vulnerability repair aims to help developers effectively address vulnerabilities in systems, reducing labor costs and achieving automatic fixes, thereby enhancing data security and mitigating the risks associated with cyber intrusions, which can lead to severe losses. This paper presents CRepair, an automated vulnerability repair technique based on Conditional Variational Autoencoders (CVAE), designed to tackle the issues in existing repair technologies that neglect code semantic diversity due to fixed encoding and struggle with large search spaces resulting from the inability to quickly locate vulnerabilities.Utilizing the CVAE architecture, we preprocess vulnerable code using prompts as model inputs and employ multi-sampling feature fusion techniques to diversify the extraction of vulnerability features. Concurrently, we utilize conditional control to guide the model in accurately repairing vulnerabilities. The generated candidate repair sets provide developers with faster and more precise support for vulnerability fixes. Through extensive experimentation, this study demonstrates that CRepair not only effectively repairs vulnerabilities but also significantly improves upon current mainstream repair technologies, showcasing promising application prospects."}]}