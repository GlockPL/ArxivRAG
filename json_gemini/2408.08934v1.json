{"title": "A FACTORED MDP APPROACH TO MOVING TARGET DEFENSE\nWITH DYNAMIC THREAT MODELING AND COST EFFICIENCY", "authors": ["Megha Bose", "Praveen Paruchuri", "Akshat Kumar"], "abstract": "Moving Target Defense (MTD) has emerged as a proactive and dynamic framework to counteract evolving\ncyber threats. Traditional MTD approaches often rely on assumptions about the attacker's knowledge\nand behavior. However, real-world scenarios are inherently more complex, with adaptive attackers and\nlimited prior knowledge of their payoffs and intentions. This paper introduces a novel approach to\nMTD using a Markov Decision Process (MDP) model that does not rely on predefined attacker payoffs.\nOur framework integrates the attacker's real-time responses into the defender's MDP using a dynamic\nBayesian Network. By employing a factored MDP model, we provide a comprehensive and realistic\nsystem representation. We also incorporate incremental updates to an attack response predictor as new\ndata emerges. This ensures an adaptive and robust defense mechanism. Additionally, we consider the\ncosts of switching configurations in MTD, integrating them into the reward structure to balance execution\nand defense costs. We first highlight the challenges of the problem through a theoretical negative result\non regret. However, empirical evaluations demonstrate the framework's effectiveness in scenarios marked\nby high uncertainty and dynamically changing attack landscapes.", "sections": [{"title": "1 Introduction", "content": "An inherent information asymmetry exists between attackers and defenders in cyber systems. This imbalance arises from\nthe attackers' capability to conduct reconnaissance [1] on a static system. Attackers have the freedom to systematically\nprobe a computer network or a system and gain insights into its vulnerabilities to plan targeted attacks over a period of\ntime. Moving Target Defense (MTD) [2] addresses this advantage held by attackers by introducing an increased amount of\nuncertainty for the attacker. This is achieved via dynamically altering the system configuration, rendering the attempts to\nexecute successful attacks more challenging for the attacker. However, implementation of MTD solutions can introduce\ncertain overheads like costs associated with maintaining multiple configurations and service disruptions incurred during\nthe process of switching the system configurations. Various MTD solutions focus on altering the system with respect\nto what, when, and how the changes are made [3]. In the \"what\" aspect, solutions shuffle elements of the system, like\nIP addresses and software programs, or implement different versions of the system using varying technological stacks,\noperating systems, virtualization, etc., to create distinct system configurations. In the \"when\" aspect, some works explicitly\naddress the timing of configuration changes, while others assume that the defender regularly takes switching actions on the\nsystem at predefined time intervals [4, 5]. In the latter case, at the start of each time step, the defender decides whether to\nswitch one or more adaptive aspects [3, 6] of the system, and concurrently, the attacker may launch an attack. The attack is\nexecuted on the configuration that results from the switching, and the attacker gains knowledge about the outcome of the\nattack at the end of the time step. Our work also operates under this temporal framework. The third aspect of \"how\" to"}, {"title": "2 Related Work", "content": "The problem of devising effective strategies for the defender in the context of MTD when facing strategic adversaries, has\nlong garnered interest. Previous research has predominantly adopted the Stackelberg game formulation and its variations\n[4, 5], to model the interactions between the attacker and defender. In this model, both the players, namely the attacker(s)\nand defender, strive to maximize their respective rewards.\nStackelberg Security Games (SSG) [21, 22], have received extensive attention across a range of domains. The prevailing\nsolution concept for these games is the Strong Stackelberg Equilibrium (SSE) [23], where it is assumed that the defender is\naware of the defender as well as the attacker side payoffs and uses them to select an optimal mixed strategy, anticipating\nthat the attacker will learn the mixed strategy of the defender and respond optimally. Extensions of SSG, adapted for\nMTD include approaches like Bayesian Stackelberg games (BSG) [4] where a Bayesian framework is employed to capture\nthe uncertainty regarding potential attackers. However, this approach fails to consider the multi-stage nature of the\ninteractions. Markov game models [24, 25] have been proposed to address MTD scenarios, enabling state-dependent\ndefense strategies but these models often assume a fixed attacker type. To address these limitations, Sengupta and\nKambhampati [5] introduced the Bayesian Stackelberg Markov game (BSMG) framework. In BSMG, the attacker type\nadheres to a predefined distribution, and Q-learning is utilized to iteratively update value functions for both the attacker and\ndefender and calculate the SSE at each stage, converging to the SSE of the BSMG.\nIn Li and Zheng [11], a meta-reinforcement learning (meta-RL) approach has been applied to simplify the bi-level\noptimization problem of identifying the optimal defender solution in the presence of a strategic attacker. This simplification\nis achieved by introducing specific assumptions in a zero-sum Markov game, thereby reducing the complexity of finding\nthe Strong Stackelberg Equilibrium to solving a single-agent MDP. Subsequently, meta-RL techniques have been employed\nto enhance the defender's policy through a training phase involving known attack scenarios, a short adaptation phase in\nresponse to real-world attacks, and a testing phase.\nViswanathan et al. [26] adopted a multi-armed bandit approach to address the uncertainty over attacks and adapts the\nFollow-the-Perturbed-Leader (FPL) algorithm with Geometric Resampling to the MTD problem. This allows for the\ngeneration of effective strategies despite having limited information about the attacker and potential exploits in the system.\nWhile Arora et al. [27] demonstrates that no bandit algorithm can ensure sublinear regret against adaptive adversaries,\nempirical evidence suggests that the FPL-based method [26] consistently outperforms other bandit algorithms. Furthermore,\nit demonstrates comparable performance to state-of-the-art methods that require prior knowledge of attacker payoffs and\nintentions. However, when choosing an action, the bandit model lacks the ability to account for the impact of that action on\nfuture outcomes.\nGame-theoretic models are known to grapple with issues stemming from assumptions such as player rationality or the full\nknowledge of the payoff functions of all parties, which may not hold in many real-world MTD scenarios. Reinforcement\nlearning approaches are frequently plagued by sample inefficiency, leading to higher exploration-exploitation costs,"}, {"title": "3 Problem Setting", "content": "We view the MTD problem from the defender's perspective as an infinite horizon deterministic Markov Decision Process\n(MDP) < S, A, P, R, \u03b3 >. The state space S of the MDP contains all possible system configurations. The action space A\ncontains the actions available to the defender corresponding to switching actions between configurations. Each action a can\nbe treated as the subsequent configuration reached after switching, as the transitions are deterministic in nature. When the\ndefender selects a switching action at the start of a time step, the attacker within the environment chooses an attack. Our\nmodel assumes knowledge of only the defender's rewards but not of the attacker's. Assumptions like zero-sum reward\nstructures may not always hold as attacks that are most harmful to the system might not be in the best interests of or even\nbe known to the attackers, who have their own intentions and abilities. In this framework, the primary source of uncertainty\nlies in the reward function, which is contingent on the attacker's actions. Consequently, we want to model the attacker's\nbehavior and incorporate it into the MDP so that the defender can solve for the policy with the best-expected reward over\ntime while considering the attacker's behavior."}, {"title": "3.1 Threat Model", "content": "To effectively model the attacker, we use the concept of attacker types [11]. Let T denote the set of all attacker types. Each\nattacker type \u0442\u2208\u0422 characterizes a distinct category of attackers with defined capabilities and the ability to execute a set\nof exploits in each configuration (see Figure 1, Table 4b) denoted by v. These attacker types may or may not be in the\ndefender's knowledge. An attacker aligns with a unique attacker type, and all attackers within the same type exhibit similar\nbehaviors. We consider that these attackers can demonstrate strategic behavior and may possess the knowledge of all the\npast configurations deployed by the defender until the current time step."}, {"title": "3.2 Integrating Attacker Response", "content": "We employ a dynamic Bayesian Network (DBN) to incorporate the attacker response into the defender's MDP. This enables\nus to model the dependencies that exist among the attacker's response variable, the states, and the actions within the MDP.\nThe success of an attack at timestep t + 1, denoted by \u03c6t+1, depends on the configuration st+1 achieved through switching\naction at on configuration st. It also dictates the reward rt at t."}, {"title": "3.3 Extending to Factored MDP", "content": "A cyber system comprises numerous attack surfaces in the adaptive aspects [3, 6] which contribute to the system's\nconfiguration variability and the attacker response. Zhuang et al. [6] presents the notion of a configuration state in MTD that\ncaptures the specific configuration of a system. This configuration state is characterized by sub-configuration parameters\n(or adaptive aspects of the system) that take on values from their respective domains. Examples of sub-configuration\nparameters can be the host memory size, hard disk size, CPU type, operating system, application software, database,\nprogramming language, IP address, open ports, etc. Building on this concept, we use a factored MDP to model these\nconfiguration parameters as state factors within the MDP. By breaking down the state and action spaces, we can enhance\nstate representation, improve the quality of our inferences, gain finer control over switching actions, and define more\ncomplex dependencies among the adaptive aspects modeled as the state and action factors. Let S = {S1, S2, \u2026\u2026\u2026, Sn}\nbe the finite set of state variables and A = {A1, A2, ..., An} be the finite set of action variables. The defender's MDP\nis characterized by attacker-type probability estimates Patt(r|s, a) \u2200r, s, a (from Algorithm 1) and domain information\ncontaining attack success rates \u03bc(\u03c4, s) \u2200t, s and unit time system losses l(t, s) \u2200t, s. These are utilized to create the MDP\n(Algorithm 1 step 5) and set the reward values (See Eq. 1).\nFactored States - Each state factor corresponds to an adaptive aspect or sub-configuration parameter within the system.\nState s is represented as a tuple (s1, s2, ..., s\u2033) \u2208 Dom(S) where si represents the value of the jth factor of the system's\nconfiguration Vj \u2208 [n]. For example, let the states of a system have two factors corresponding to language (S\u00b9) and\ndatabase (S2). The domain of language factor is Dom(S\u00b9) = {Python, PHP} and Dom(S2) = {MySQL}. An\nexample state s can be (PHP, MySQL) where s\u00b9 = PHP and s\u00b2 = MySQL.\nFactored Actions - Each action factor represents the value to which the corresponding state factor is switched to on\ntaking the action. Similar to the state space, the action space is also factored, and each action a is represented as a\ntuple (a1, a2, ..., an) \u2208 Dom(A) where a\u00b9 represents the value of the jth factor of the new configuration that the action\nmakes the current state switch to \u2200j \u2208 [n]. In the previous example, if action a = (PHP, MySQL) is taken on state\ns = (Python, MySQL), it means that the new state after switching becomes s' = (PHP, MySQL).\nFactored Rewards - The reward consists of three parts:\n1. Attack Loss (al) represents the expected loss incurred by the defender from successful attacks on the system. Given\nprobability estimate of Patt(T|s, a) over attacker types attacking state s on taking action a, the attack loss is:"}, {"title": "4 Solution Framework", "content": "We present our solution framework for addressing the defender's problem using an approximate linear programming (ALP)\napproach for solving the FMDP. Since the FMDP is unaware of the attacker type probability estimates Patt(T|s, a)\u2200r, s, a\n(from Algorithm 1) and the domain information containing attack success rates \u03bc(\u03c4, s) \u2200t, s and unit time system losses\nl(t, s) VT, s, these are passed into the solver (Algorithm 1 step 6) to generate constraints in the primal formulation of the\nALP."}, {"title": "4.1 Algorithm", "content": "To solve the defender's factored MDP augmented with the attacker's response, we use an ALP formulation for the FMDP\nwith the constraints constructed by taking an expectation over the possible values of the binary response variable, similar to\nLogistic MDP [28] but using our probability estimates instead of the logistic regression model predictions. See Appendix\nA for more details. We estimate Patt (7 | s, a) using Algorithm 1 that calculates the estimates Patt.\nIn dynamic settings with adaptive adversaries, traditional definitions of external pseudo-regret often become inadequate.\nArora et al. [27] highlights this inadequacy and presents alternative definitions better suited for such scenarios and introduces\nthe concept of policy regret for use in dynamic environments with adaptive adversaries. In our scenario of Moving Target\nDefense with switching costs, we adopt the policy regret definition and propose the following theorem. The theorem\nprovides a negative result that guarantees that achieving sublinear regret is impossible."}, {"title": "5 Empirical Evaluation", "content": "We look at a web application and a network-based environment to showcase the performance of MTD algorithms. (See\nAppendix G for diagrams)"}, {"title": "5.1 Environmental Setup", "content": "Web Application Environment Inspired by previous works [4", "29": "we employ the National Vulnerability Database\n(NVD) data from the years 2020 to 2022 and Common Vulnerability Scoring System (CVSS) scores to establish the\nexperimental framework for a web application. We define the set of system configurations as {(PHP", "11": "for a given state", "10": ".", "0,10": ".", "considered": "Mainstream Hacker (MH),\nDatabase Hacker (DH) and unknown. The switching costs (sc) used are given in Table 4a and the attacker capabilities,\nattack success rates (\u03bc) and unit time system losses (l) are given in Table 4b.\nNetwork Environment For simulating and testing network security in general, we have tools like Microsoft's CyberBat-"}]}