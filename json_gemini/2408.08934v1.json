{"title": "A FACTORED MDP APPROACH TO MOVING TARGET DEFENSE\nWITH DYNAMIC THREAT MODELING AND COST EFFICIENCY", "authors": ["Megha Bose", "Praveen Paruchuri", "Akshat Kumar"], "abstract": "Moving Target Defense (MTD) has emerged as a proactive and dynamic framework to counteract evolving\ncyber threats. Traditional MTD approaches often rely on assumptions about the attacker's knowledge\nand behavior. However, real-world scenarios are inherently more complex, with adaptive attackers and\nlimited prior knowledge of their payoffs and intentions. This paper introduces a novel approach to\nMTD using a Markov Decision Process (MDP) model that does not rely on predefined attacker payoffs.\nOur framework integrates the attacker's real-time responses into the defender's MDP using a dynamic\nBayesian Network. By employing a factored MDP model, we provide a comprehensive and realistic\nsystem representation. We also incorporate incremental updates to an attack response predictor as new\ndata emerges. This ensures an adaptive and robust defense mechanism. Additionally, we consider the\ncosts of switching configurations in MTD, integrating them into the reward structure to balance execution\nand defense costs. We first highlight the challenges of the problem through a theoretical negative result\non regret. However, empirical evaluations demonstrate the framework's effectiveness in scenarios marked\nby high uncertainty and dynamically changing attack landscapes.", "sections": [{"title": "1 Introduction", "content": "An inherent information asymmetry exists between attackers and defenders in cyber systems. This imbalance arises from\nthe attackers' capability to conduct reconnaissance [1] on a static system. Attackers have the freedom to systematically\nprobe a computer network or a system and gain insights into its vulnerabilities to plan targeted attacks over a period of\ntime. Moving Target Defense (MTD) [2] addresses this advantage held by attackers by introducing an increased amount of\nuncertainty for the attacker. This is achieved via dynamically altering the system configuration, rendering the attempts to\nexecute successful attacks more challenging for the attacker. However, implementation of MTD solutions can introduce\ncertain overheads like costs associated with maintaining multiple configurations and service disruptions incurred during\nthe process of switching the system configurations. Various MTD solutions focus on altering the system with respect\nto what, when, and how the changes are made [3]. In the \"what\" aspect, solutions shuffle elements of the system, like\nIP addresses and software programs, or implement different versions of the system using varying technological stacks,\noperating systems, virtualization, etc., to create distinct system configurations. In the \"when\" aspect, some works explicitly\naddress the timing of configuration changes, while others assume that the defender regularly takes switching actions on the\nsystem at predefined time intervals [4, 5]. In the latter case, at the start of each time step, the defender decides whether to\nswitch one or more adaptive aspects [3, 6] of the system, and concurrently, the attacker may launch an attack. The attack is\nexecuted on the configuration that results from the switching, and the attacker gains knowledge about the outcome of the\nattack at the end of the time step. Our work also operates under this temporal framework. The third aspect of \"how\" to"}, {"title": "2 Related Work", "content": "The problem of devising effective strategies for the defender in the context of MTD when facing strategic adversaries, has\nlong garnered interest. Previous research has predominantly adopted the Stackelberg game formulation and its variations\n[4, 5], to model the interactions between the attacker and defender. In this model, both the players, namely the attacker(s)\nand defender, strive to maximize their respective rewards.\nStackelberg Security Games (SSG) [21, 22], have received extensive attention across a range of domains. The prevailing\nsolution concept for these games is the Strong Stackelberg Equilibrium (SSE) [23], where it is assumed that the defender is\naware of the defender as well as the attacker side payoffs and uses them to select an optimal mixed strategy, anticipating\nthat the attacker will learn the mixed strategy of the defender and respond optimally. Extensions of SSG, adapted for\nMTD include approaches like Bayesian Stackelberg games (BSG) [4] where a Bayesian framework is employed to capture\nthe uncertainty regarding potential attackers. However, this approach fails to consider the multi-stage nature of the\ninteractions. Markov game models [24, 25] have been proposed to address MTD scenarios, enabling state-dependent\ndefense strategies but these models often assume a fixed attacker type. To address these limitations, Sengupta and\nKambhampati [5] introduced the Bayesian Stackelberg Markov game (BSMG) framework. In BSMG, the attacker type\nadheres to a predefined distribution, and Q-learning is utilized to iteratively update value functions for both the attacker and\ndefender and calculate the SSE at each stage, converging to the SSE of the BSMG.\nIn Li and Zheng [11], a meta-reinforcement learning (meta-RL) approach has been applied to simplify the bi-level\noptimization problem of identifying the optimal defender solution in the presence of a strategic attacker. This simplification\nis achieved by introducing specific assumptions in a zero-sum Markov game, thereby reducing the complexity of finding\nthe Strong Stackelberg Equilibrium to solving a single-agent MDP. Subsequently, meta-RL techniques have been employed\nto enhance the defender's policy through a training phase involving known attack scenarios, a short adaptation phase in\nresponse to real-world attacks, and a testing phase.\nViswanathan et al. [26] adopted a multi-armed bandit approach to address the uncertainty over attacks and adapts the\nFollow-the-Perturbed-Leader (FPL) algorithm with Geometric Resampling to the MTD problem. This allows for the\ngeneration of effective strategies despite having limited information about the attacker and potential exploits in the system.\nWhile Arora et al. [27] demonstrates that no bandit algorithm can ensure sublinear regret against adaptive adversaries,\nempirical evidence suggests that the FPL-based method [26] consistently outperforms other bandit algorithms. Furthermore,\nit demonstrates comparable performance to state-of-the-art methods that require prior knowledge of attacker payoffs and\nintentions. However, when choosing an action, the bandit model lacks the ability to account for the impact of that action on\nfuture outcomes.\nGame-theoretic models are known to grapple with issues stemming from assumptions such as player rationality or the full\nknowledge of the payoff functions of all parties, which may not hold in many real-world MTD scenarios. Reinforcement\nlearning approaches are frequently plagued by sample inefficiency, leading to higher exploration-exploitation costs,"}, {"title": "3 Problem Setting", "content": "We view the MTD problem from the defender's perspective as an infinite horizon deterministic Markov Decision Process\n(MDP) < S, A, P, R, \u03b3 >. The state space S of the MDP contains all possible system configurations. The action space A\ncontains the actions available to the defender corresponding to switching actions between configurations. Each action a can\nbe treated as the subsequent configuration reached after switching, as the transitions are deterministic in nature. When the\ndefender selects a switching action at the start of a time step, the attacker within the environment chooses an attack. Our\nmodel assumes knowledge of only the defender's rewards but not of the attacker's. Assumptions like zero-sum reward\nstructures may not always hold as attacks that are most harmful to the system might not be in the best interests of or even\nbe known to the attackers, who have their own intentions and abilities. In this framework, the primary source of uncertainty\nlies in the reward function, which is contingent on the attacker's actions. Consequently, we want to model the attacker's\nbehavior and incorporate it into the MDP so that the defender can solve for the policy with the best-expected reward over\ntime while considering the attacker's behavior."}, {"title": "3.1 Threat Model", "content": "To effectively model the attacker, we use the concept of attacker types [11]. Let T denote the set of all attacker types. Each\nattacker type \u0442\u2208\u0422 characterizes a distinct category of attackers with defined capabilities and the ability to execute a set\nof exploits in each configuration (see Figure 1, Table 4b) denoted by v. These attacker types may or may not be in the\ndefender's knowledge. An attacker aligns with a unique attacker type, and all attackers within the same type exhibit similar\nbehaviors. We consider that these attackers can demonstrate strategic behavior and may possess the knowledge of all the\npast configurations deployed by the defender until the current time step."}, {"title": "3.2 Integrating Attacker Response", "content": "We employ a dynamic Bayesian Network (DBN) to incorporate the attacker response into the defender's MDP. This enables\nus to model the dependencies that exist among the attacker's response variable, the states, and the actions within the MDP.\nThe success of an attack at timestep t + 1, denoted by $\u03c6_{t+1}$, depends on the configuration $s_{t+1}$ achieved through switching\naction $a_t$ on configuration $s_t$. It also dictates the reward $r_t$ at t. Figure 2 illustrates these dependencies."}, {"title": "3.3 Extending to Factored MDP", "content": "A cyber system comprises numerous attack surfaces in the adaptive aspects [3, 6] which contribute to the system's\nconfiguration variability and the attacker response. Zhuang et al. [6] presents the notion of a configuration state in MTD that\ncaptures the specific configuration of a system. This configuration state is characterized by sub-configuration parameters\n(or adaptive aspects of the system) that take on values from their respective domains. Examples of sub-configuration\nparameters can be the host memory size, hard disk size, CPU type, operating system, application software, database,\nprogramming language, IP address, open ports, etc. Building on this concept, we use a factored MDP to model these\nconfiguration parameters as state factors within the MDP. By breaking down the state and action spaces, we can enhance\nstate representation, improve the quality of our inferences, gain finer control over switching actions, and define more\ncomplex dependencies among the adaptive aspects modeled as the state and action factors. Let S = {$S_1, S_2, \u2026\u2026\u2026, S_n$}\nbe the finite set of state variables and A = {$A_1, A_2, ..., A_n$} be the finite set of action variables. The defender's MDP\nis characterized by attacker-type probability estimates $Patt(\u03c4|s, a) \\forall \u03c4, s, a$ (from Algorithm 1) and domain information\ncontaining attack success rates $\u03bc(\u03c4, s) \\forall \u03c4, s$ and unit time system losses $l(\u03c4, s) \\forall \u03c4, s$. These are utilized to create the MDP\n(Algorithm 1 step 5) and set the reward values (See Eq. 1).\nFactored States - Each state factor corresponds to an adaptive aspect or sub-configuration parameter within the system.\nState s is represented as a tuple ($s^1, s^2, ..., s^n$) \u2208 $Dom(S)$ where $s^j$ represents the value of the jth factor of the system's\nconfiguration $\\forall j$ \u2208 [n]. For example, let the states of a system have two factors corresponding to language (S\u00b9) and\ndatabase (S2). The domain of language factor is $Dom(S^1)$ = {Python, PHP} and $Dom(S^2)$ = {MySQL}. An\nexample state s can be (PHP, MySQL) where s\u00b9 = PHP and s\u00b2 = MySQL.\nFactored Actions - Each action factor represents the value to which the corresponding state factor is switched to on\ntaking the action. Similar to the state space, the action space is also factored, and each action a is represented as a\ntuple ($a^1, a^2, ..., a^n$) \u2208 Dom(A) where a\u00b9 represents the value of the jth factor of the new configuration that the action\nmakes the current state switch to $\\forall j$ \u2208 [n]. In the previous example, if action a = (PHP, MySQL) is taken on state\ns = (Python, MySQL), it means that the new state after switching becomes s' = (PHP, MySQL).\nFactored Rewards - The reward consists of three parts:\n1. Attack Loss (al) represents the expected loss incurred by the defender from successful attacks on the system. Given\nprobability estimate of $Patt(\u03c4|s, a)$ over attacker types attacking state s on taking action a, the attack loss is:"}, {"title": "4 Solution Framework", "content": "We present our solution framework for addressing the defender's problem using an approximate linear programming (ALP)\napproach for solving the FMDP. Since the FMDP is unaware of the attacker type probability estimates $Patt(\u03c4|s, a) \\forall \u03c4, s, a$\n(from Algorithm 1) and the domain information containing attack success rates $\u03bc(\u03c4, s) \\forall \u03c4, s$ and unit time system losses\nl(\u03c4, s) $VT, s$, these are passed into the solver (Algorithm 1 step 6) to generate constraints in the primal formulation of the\nALP."}, {"title": "4.1 Algorithm", "content": "To solve the defender's factored MDP augmented with the attacker's response, we use an ALP formulation for the FMDP\nwith the constraints constructed by taking an expectation over the possible values of the binary response variable, similar to\nLogistic MDP [28] but using our probability estimates instead of the logistic regression model predictions. See Appendix\nA for more details. We estimate Patt (7 | s, a) using Algorithm 1 that calculates the estimates Patt.\nIn dynamic settings with adaptive adversaries, traditional definitions of external pseudo-regret often become inadequate.\nArora et al. [27] highlights this inadequacy and presents alternative definitions better suited for such scenarios and introduces\nthe concept of policy regret for use in dynamic environments with adaptive adversaries. In our scenario of Moving Target\nDefense with switching costs, we adopt the policy regret definition and propose the following theorem. The theorem\nprovides a negative result that guarantees that achieving sublinear regret is impossible.\nTheorem 1. For any MTD defense strategy on n > 1 configurations, \u2203 an adaptive adversary such that the defender's\npolicy regret compared to the best static configuration in hindsight is $\u03a9(\u03a4)$.\nThe proof of Theorem1 is delegated to Appendix C.1. See Appendix B and C.2 for analysis on the attacker type probability\nestimates and average regret respectively. Figure 3 shows the scheme of our solution framework.\nThe algorithm starts with the initialization of hyperparameters and variables and the loading of domain information. Then,\nwe run the algorithm for T timesteps. In each timestep, if anomalous activity is detected, we run the re-optimization\nprotocol in Step 6 to update the FMDP policy based on the current estimate of attacker type probabilities. The defender\ntakes an action according to the last calculated policy (Step 8), and based on that, it receives the attacker response value 4,\nthe attacker type (unknown type if the attacker type is unknown to the defender), and the next state (Step 9). Using the\nattack response and unit-time system loss l(t, (s, a)) due to the attacker type \u0442 (tuple (s, a) and next state s' have the\nsame meaning as transitions are deterministic) and the switching cost incurred in executing the current switching action, we\ncalculate the defender's reward. We maintain a temporally weighted value n for each attacker type, state, and action tuple,"}, {"title": "5 Empirical Evaluation", "content": "We look at a web application and a network-based environment to showcase the performance of MTD algorithms. (See\nAppendix G for diagrams)\nWeb Application Environment Inspired by previous works [4, 11, 29], we employ the National Vulnerability Database\n(NVD) data from the years 2020 to 2022 and Common Vulnerability Scoring System (CVSS) scores to establish the\nexperimental framework for a web application. We define the set of system configurations as {(PHP, MySQL), (Python,\nMySQL), (PHP, PostgreSQL), (Python, PostgreSQL)}. Here, the first factor specifies the programming language,\nand the second factor represents the database used in the application. Hence, we have a domain with 2 state factors each\nwith a domain size of 2 and 2 action factors each with a domain size of 2, which leads to 8 sparse binarized features (sbfs)\nand induced state and action spaces of size 4 each. Actions correspond to configurations to which the current state will\ntransition to, as defined earlier.\nRewards are computed using the Common Vulnerability Scoring System (CVSS) scores. Each attacker type can exploit a\nspecific set of vulnerabilities. Following Li and Zheng [11], for a given state, the attack success rate (\u03bc) for a particular\nattacker type in a given state is determined by the average exploitability score (ES) of the vulnerabilities that the attacker\ntype can exploit in that state, calculated by 0.1 * ES where ES \u2208 [0, 10]. The unit time system loss (l) is calculated based\non the average impact score (IS) of the vulnerabilities that the attacker type can exploit in the configuration, calculated by\n-10 * IS where IS \u2208 [0,10]. Based on the probability distribution over attacker types that can attack the current state,\none of the attacker types attempts an attack. The outcome of the attack depends on the configuration switched to and the\nattacker's known success rate for that attacker type. Here, three attacker types are considered: Mainstream Hacker (MH),\nDatabase Hacker (DH) and unknown. The switching costs (sc) used are given in Table 4a and the attacker capabilities,\nattack success rates (\u03bc) and unit time system losses (l) are given in Table 4b."}, {"title": "5.2 Defense Methods Compared", "content": "Our approach, which we call 'Adaptive Threat-Aware Factored MDP' (ATA \u2013 FMDP), is compared to other methods\nthat utilize a similar amount of prior information. Specifically, we compare it against the following three approaches: (a) A\nbandit-based approach (FPL \u2013 MTD) [26] - FPL \u2013 MTD has proved as a strong baseline when compared with other\napproaches that do not consider prior knowledge regarding attackers. (b) EPS \u2013 GREEDY approach - this approach\nuses epsilon-greedy based exploration-exploitation where for exploitation it greedily selects the action that has the highest\naction-value estimate, and (c) a uniform random defense (URS) policy that selects next actions in a uniformly random\nmanner. We conduct our experiments in 10 iterations, each with 1000 timesteps. \u1e9e is 2, M is 200 and discount factor is 0.9.\nEpsilon of EPS \u2013 GREEDY is 0.2. The hyperparameters of FPL \u2013 MTD are taken from Viswanathan et al. [26]"}, {"title": "5.3 Response to Evolving Attack Landscapes", "content": "In the real-world dynamic attack landscape, attackers acquire knowledge gradually, learning when their attacks are most\nlikely to yield rewards. They may also observe the defender's actions over time and decide when to launch attacks on the\nsystem. In this case, the distribution of attacker types within each system state is not uniform across all states; it evolves\ncontinuously. For instance, a state with robust defenses capable of thwarting most attacks will not attract attacks from\nrational attackers. Conversely, say in the web application environment, if a state has vulnerabilities in the database, the\nprobability of a database hacker launching an attack on that state is expected to be higher.\nWe compare the average defender reward received using each of the defense strategies. We analyze the rewards averaged\nover 10 iterations, each consisting of 1000 timesteps. In the figures, the y-axis represents this average reward, while\nthe x-axis represents the defense strategy. The dotted lines indicate the average defender rewards that could have been\nachieved if the best and worst static defenses in hindsight were deployed. These lines provide a benchmark for assessing the\neffectiveness of the switching strategy against the best possible performance a static defense could achieve, had it known\nthe optimal configuration in advance, and against the worst-case performance if the least effective configurations were\ndeployed without any switching. We also present the evolution of the defender rewards averaged over the 10 iterations\nthrough 1000 timesteps. The y-axis represents this average reward and the x-axis represents the timesteps.\nIn Web Application Environment We consider an evolving attack landscape defined by initial probabilities of\n{0.5, 0.35, 0.15} for MH, DH, and unknown attackers, which change to {0.1, 0.0, 0.9} for MH, DH, and unknown\nattacker types between timesteps 330 and 660 and then back to initial probabilities. This kind of situation can occur if\nan unknown attacker enters the landscape with targeted attacks. The unknown attacker type can cause significant harm\nif the defender's strategy doesn't adapt to the landscape in time. From figures 5a, 5b, 5c, we see that on average, our\nATA \u2013 FMDP approach performs 13%, 34% and 56% better than FPL \u2013 MTD, EPS \u2013 GREEDY and URS,\nrespectively. From Figures 5d, 5e, 5f, we observe that our approach quickly adapts to the new scenario and maintains a\nhigh defender reward, while the other methods experience a drastic drop in the defender reward when faced with the strong\nunknown attacker. This demonstrates the robustness of our approach in evolving attack landscapes.\nThe attack landscape might not just be evolving but can also contain strategic attackers that observe the defender's actions\nover time, and keep changing their strategy based on observations. We consider such an evolving attack landscape with\nstrategic attackers where the attackers execute actions that cause the most harm to the defender at each timestep as the most\nadverse scenario for the defender. This is different from the best response attack strategy since we do not assume knowledge\nabout the attacker's side intentions and do not consider the scenario to be zero-sum. In the adverse attack scenario, the\nattackers use the estimated defender policy they have observed until now, and then based on the current configuration and\nthe estimated defender policy, they deploy the attack most adverse to defender. From Figure 6a, we derive that on average,\nour ATA - FMDP approach performs 15%, 20% and 52% better than FPL \u2013 MTD, EPS \u2013 GREEDY and URS,\nrespectively. From Figure 6b, we see that it gives better rewards than other methods across the 1000 timesteps."}, {"title": "5.4 Impact of Switching Cost", "content": "It is known that in certain systems, switching costs tend to be generally lower, while in others, the process of making\nswitches can be considerably challenging. We vary the value of the switching cost weighing factor a among {0.0, 0.5, 1.0}\nin our analysis and show the results on the web application environment in Figure 5. We observe that with an increased\nweightage (a) given to switching costs, URS performs much worse than other methods. This is expected because URS is"}, {"title": "5.5 Adaptability and Other Insights", "content": "The model's significant feature lies in its capacity to effectively handle unknown attacks. In cases where a system encounters\nan unfamiliar and potentially stealthy attack characterized by a potentially high attack rate and impact, the model is designed\nto detect and respond to it promptly. The defender's policy adapts to mitigate the threat until more information becomes\navailable about the nature of the attack and its intentions. Additionally, an intriguing analysis pertains to the prevalence of\nspecific actions chosen within the system by the defender. This exploration provides insights into the underlying dynamics\nof the system configurations and the broader attack landscape. Prolonged avoidance of a specific configuration may signal\nan underlying issue or an inherent vulnerability within that configuration. We also analyze an unknown attack landscape\ninvolving a specialized database hacker targeting only PostgreSQL in Appendix E. A detailed analysis of switching patterns\nin the action factors indicated potential vulnerability in PostgreSQL as expected."}, {"title": "6 Conclusions and Future Work", "content": "In this work, we modeled two broad real-world Moving Target Defense (MTD) scenarios with minimal assumptions\nabout attackers and no prior knowledge of the attacker's rewards and intentions. Our approach introduces an attacker\nmodel that learns from real-time attacks and continuously updates attack success probabilities, seamlessly integrating this\ninformation into the defender's Markov Decision Process (MDP). The defender determines an optimal policy by solving\nthe MDP formulation using the latest attacker response predictions. These estimates undergo periodic updates to maintain\nthe model's accuracy. Our method compares favorably to the other approaches, assuming a similar level of information\nregarding the evolving and adaptive attacker settings in two different domains. Notably, our approach demonstrates the\ncapability to model unknown attacker types and adapt to new and unfamiliar attack patterns. Insights into the system and\nattack landscape can be extracted from the calculated policies. We also look at the sensitivity of our approach to different\nimportance weights assigned to switching costs and draw pertinent conclusions. Even with limited initial knowledge of the\nattack landscape, the defender acquires intelligent insights and adapts favorably to reduce losses from attacks and avoidable\ncostly configuration changes.\nFuture Work Following directions can be pursued based on this work: Incorporation of more advanced attack success\npredictors, such as Artificial Neural Networks designed to handle sequential data, can enhance the learning of attacker"}, {"title": "A Approximate Linear Program Formulation", "content": "The primal LP formulation [16] to solve for the optimal policy in a finite MDP goes as follows:\n$\\min \\sum_{\\substack{s \\in S}} \\theta[s]v_s$\n(7)\ns.t. 0 \u2265 $Q_\\pi(s, a) \u2013 v_s, \\forall s \\in S, a \\in A$\n(8)\nwhere $Q_\\pi(s, a)$ denotes the action-value backup, $v_s$ denotes the optimal value function at s and \u03b8 denotes the initial state\ndistribution. In the factored MDP model, the value function is approximated using a linear combination of basis functions\ndenoted as B = {$\u03b2_1, ..., \u03b2_k$} over state variables X. Each $\u03b2_i$ is a function of a small set $B_i$ \u2286 S, including a bias factor\n$B_{bias}$ = 1 in B and $Par_{B_i}$ = \u222asi\u2208Bi $Par_i$ [19]. The value function is parameterized by a weight vector w:\nV(s; w) = $\\sum_{i<k} w_i\u03b2_i(s[B_i]) = B_s w,$\nwhere B = [$\u03b2_1, ..., \u03b2_k$] is the basis matrix, and $B_s$ denotes the row of B corresponding to state s. When \u03b8 is factored\nsimilarly and $g_i$ denotes the backprojection [19] of basis function $B_i$, the primal LP formulation in the MTD scenario can\nbe expressed as:\n$\\min \\sum_{i<k} \\sum_{b_i \\in Dom(B_i)} \\theta [b_i] w_i\u03b2_i (b_i)$\ns.t. 0 \u2265 C(s, a; w) $\\forall s, a$ where\nC(s, a; w) = $\\sum_{\\phi \\in {0,1}} c(s, a, \u03c6; w).$\n(9)\nFor 4 = 1,\nc(s, a, ; w)\n= $\\sum_{\\tau} P(\\phi |s, a, \\tau) Patt (\\tau | s, a) [M \u2013 l(\\tau, (s, a))\n a. sc(s, a) + $\\sum_{i<k} w_i (\u03b3\u22c5 g_i (s [Par_{B_i}], a [Par_{B_i}]) \u2013 \u03b2_i (s [B_i]))]$\n= $\\sum_{\\tau} \u03bc(\\tau, (s, a)) Patt (\\tau | s, a) [M \u2013 l(\\tau, (s, a)) \u2013 \u03b1\u22c5 sc(s, a) + $\\sum_{i<k} w_i (\u03b3g_i (s [Par_{B_i}], a [Par_{B_i}]) \u2013 \u03b2_i (s [B_i]))]$\n(10)\n= $\\sum_{\\tau} \u03bc(\\tau, (s, a)) Patt (\\tau | s, a) [M \u2013 l(\\tau, (s, a))\n a. sc(s, a) + $\\sum_{i<k} w_i g_i (s [Par_{B_i}], a [Par_{B_i}])]$\n- $\\sum_{\\tau} \u03bc(\\tau, (s, a)) Patt (\\tau | s, a) [$\\sum_{i<k} Wifi (s[i])]$](11)\nas $\u03bc(\u03c4, (s, a))$ is the attack success rate of t given (s, a). One can see that the constraint Eq. 11 is the difference between\nthe action-value backup and the value function as seen in Eq. 8. Similarly, the constraints for 4 = 0 are given below.\nHowever, here, since loss from attack is 0 and switching cost doesn't depend on the attacker response, we have\nc(s, a, ; w) = P(\u03c6 | s, a) [M \u2013 a sc(s, a) + $\\sum_{i<k} w_i (\u03b3\u22c5 g_i (s [Par_{Bi}], a [Par_{Bi}]) \u2013 \u03b2_i (s [B_i]))]$(12)\nNotice that,\nP(p = 0 | s, a) = 1 \u2013 P(\u03c6 = 1 | s, a) = 1 \u2212 $\\sum_{\\tau} P(\\phi = 1 | s, a, \u03c4) Patt (\u03c4 | s,a)$(13)"}, {"title": "B Attacker Type Probability Estimation", "content": "Let the success of an attack executed by attacker type 7 for each state-action pair (s, a) be characterized by independent\nBernoulli random variables with parameter $p_{\u03c4,s,a}$ over T timesteps. We denote them by {$X_1^{\u03c4,s,a}$, $X_2^{\u03c4,s,a}$, ..., $X_T^{\u03c4,s,a}$}. Let\nus define random variable $N_T^{\u03c4,s,a}$ as follows:\n$N_T^{\u03c4,s,a} = \\sum_{t=1}^T \\beta^{T-t}X_t^{\u03c4,s,a}$\nwhere \u03b2 is the hyper-parameter from ATA \u2013 FMDP. Taking expectation of $N_T^{\u03c4,s,a}$, we get\nE[$N_T^{\u03c4,s,a}$] = E[$\\sum_{t=1}^T \\beta^{T-t}X_t^{\u03c4,s,a}$] = $\\sum_{t=1}^T \\beta^{T-t}$E[$X_t^{\u03c4,s,a}$] = $\\sum_{t=1}^T \\beta^{T-t}$P(\u03c4,s,a)\nWe can write $P_{\u03c4,s,a}$ as P(\u03c6 = 1|\u03c4, s,a)P(\u03c4|s,a)P(s, a) as it is the joint probability of an attack success when the defender\nis in state s, takes action a and is attacked by attacker type \u0442. Here, P(\u03c6 = 1|\u03c4, s, a) is the attack success rate \u03bc(\u03c4, (s, a))\nof attacker type 7 for state-action pair (s, a). We aim to estimate P(T|s, a) or $Patt(T|s, a)$. Let us assume that the true\nvalue of P(rs, a) remains fixed in these T timesteps. Hence, we have\nE[$N_T^{\u03c4,s,a}$] = $P_{\u03c4,s,a}$E[$\\sum_{t=1}^T \\beta^{T-t}$] = P(s,a)$Patt(\u03c4|s,a)$\u03bc(\u03c4, (s, a))$\\sum_{t=1}^T \\beta^{T-t}$\n = P(s,a)$Patt(\u03c4|s,a)$\u03bc(\u03c4, (s, a))$\\frac{1 - \\beta^T}{1-\\beta}$\nFrom above, we that value of $P_{\u03c4,s,a}=\\frac{E[N_T^{\u03c4,s,a}]}{P(s,a)Patt(\u03c4|s,a)\u03bc(\u03c4, (s, a))} = $\\frac{1 - \\beta^T}{1-\\beta}$ acts as an unbiased estimator of a constant($\\frac{1-\\beta}{1 - \\beta^T}$) times P(T, s, a). The value\ntaken by $N_T^{\u03c4,s,a}$ is calculated as $nr,s,a$ in ATA \u2013 FMDP. For state-action pair (s, a), estimate Patt is calculated from\n$\\frac{n_{\u03c4,s,a}}{\u03bc(\u03c4,(s,a))}$ values normalized over the attacker types."}, {"title": "C Regret Analysis", "content": "To argue whether an algorithm performs well across different problem instances, a standard approach is to compare how\n\"badly\" the chosen actions perform compared to the scenario where the optimal action is chosen in each timestep. Hence,\nregret is a quantity defined to quantify the \"regret\" an algorithm has for choosing the actions taken compared to the\nmaximum that it could have achieved if it had known the best action in advance. Regret can take various forms, depending\non the context in which it is applied."}, {"title": "C.1 Proof of Theorem 1", "content": "For any MTD defense strategy on n > 1 configurations, \u2203 an adaptive adversary such that the defender's policy regret\ncompared to the best static configuration in hindsight is $\u03a9(T)$."}, {"title": "D Comparing MTD Approaches", "content": "Game-theoretic methods leverage prior knowledge about attacker side payoffs, whereas other methods operate without\nsuch knowledge.\nThe Markov Games model can capture the multi-stage nature of defender-attacker interactions. However, multi-armed\nbandit (MAB) based approaches lack the capability to model long-term temporal dependencies when selecting actions,\nunlike MDP-based approaches, which can capture the impact of current actions on future rewards and transitions.\nSwitching costs are dependent on the current configuration and switching action, a feature that can be incorporated into the\npayoff structure in game-theoretic settings. Conversely, simple MAB formulations struggle to model the dependency of\nswitching costs on the current configuration. Employing a contextual MAB can address this limitation by integrating the\ncurrent configuration into the context. However, it still suffers from a lack of modeling long-term dependencies, similar to\nother MAB methods. MDP-based approaches do not encounter this issue, as rewards inherently depend on the state and\naction.\nFinally, while effective real-time adaptation to evolving attack landscape is feasible in online bandit algorithms, it remains\nchallenging in game-theoretic and MDP based models. However, our method ATA-FMDP can handle this non-stationarity\nin the environment."}, {"title": "E Learning Properties of Unknown Attackers", "content": "We consider an attack landscape in the"}]}