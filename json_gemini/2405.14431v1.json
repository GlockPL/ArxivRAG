{"title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG", "authors": ["Shengyu Mao", "Yong Jiang", "Boli Chen", "Xiao Li", "Peng Wang", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "abstract": "As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG) techniques have evolved, query rewriting has been widely incorporated into the RAG system for downstream tasks like open-domain QA. Many works have attempted to utilize small models with reinforcement learning rather than costly LLMs to improve query rewriting. However, current methods require annotations (e.g., labeled relevant documents or downstream answers) or predesigned rewards for feedback, which lack generalization, and fail to utilize signals tailored for query rewriting. In this paper, we propose RaFe, a framework for training query rewriting models free of annotations. By leveraging a publicly available reranker, RaFe provides feedback aligned well with the rewriting objectives. Experimental results demonstrate that RaFe can obtain better performance than baselines.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated strong capacities to solve a variety of tasks (Zhao et al., 2023). However, they still encounter the challenges of hallucinations (Ji et al., 2023; Zhang et al., 2023; Huang et al., 2023) or outdated knowledge (Yao et al., 2023; Zhang et al., 2024). Recently, Retrieval Augmentation Generation (RAG) (Gao et al., 2023) has become an important technology to enhance LLMs' abilities, by incorporating external knowledge. For instance, in open-domain QA, LLMs can firstly retrieve related documents and then generate answers. Nonetheless, directly retrieving by original query does not always achieve correct and relevant documents. Therefore, query rewriting (Efthimiadis, 1996; Carpineto and Romano, 2012) has been widely employed to reformulate the query to expand the retrieved documents for a better response as illustrated in Figure 1."}, {"title": "2 Method", "content": ""}, {"title": "2.1 Task Formulation", "content": "Within the process of Retrieval Augmented Generation (RAG), when inputting an original query q, a set of relevant documents D = [do, d1, ..., dk] will be retrieved through a search engine, and the retrieved documents are utilized to enable the model to better accomplish the corresponding task (in this paper, we discuss the task of Open-domain Question Answering). Query rewriting is to reformulate the original query q into another form to better retrieve relevant passages. We aim to obtain a better rewrite model M\u03b8 that can rewrite q as:\nq' = M\u03b8(q),\n(1)\nhere q' is the rewritten query which is used to retrieve documents D' for completing subsequent task. Figure 2 shows the overview of our proposed framework, RaFe for query rewriting training."}, {"title": "2.2 Initial Supervised Fine-Tuning", "content": "Before leveraging the ranking feedback, we first initialize the rewrite model with a cold start supervised fine-tuning to gain the rewrite ability. Specifically, we prompt the LLMs to produce the rewrite data, The details of the datasets we used to produce the training rewrite can be found in Sec 3.1. The rewrites generated from LLMs are denoted as Tall = {(q, q')|q' \u2208 Q'}, here Q' is the rewrite set of original query q. We split the training instances into two parts Tall = [Tsft : Tf], here Tsft and Tf indicates the instances we use for SFT and feedback training, respectively. We train the rewrite model M\u03b8 with standard SFT loss as follows:\nLsft = - \\sum_{q \\in Q} \\sum_{q' \\in Q'} \\sum_{t} log M_{\\theta}(q'_t | q_{<t}, q),\n(2)\nNote that for each query, we mix all corresponding rewrites together in the dataset for training, to enhance the diversity of generation by our trained model, since in real-world applications, different rewrites are required for a single search query to address different aspects or interpretations."}, {"title": "2.3 Feedback Training", "content": "The evaluation of query rewriting is notoriously difficult due to the absence of direct quality assessment methods (Zhu et al., 2023), so previous feedback for QR typically rely on the annotated passages (Nogueira and Cho, 2017; Wu et al., 2022)."}, {"title": "3 Experimental Setup", "content": "As we attempt to improve query rewriting for better RAG, we conduct our experiments on the typical RAG scenarios, Open-Domain Question Answering (ODQA). The process of RAG for ODQA can be formulated as F([D : q]), where F denotes the LLMs, q is the original query from datasets and D is the documents concatenated for augmentation."}, {"title": "3.1 Dataset", "content": "To comprehensively validate the effectiveness and generalizability of our method, we conduct cross-lingual experiments. Specifically, we evaluate ReFe on both English and Chinese datasets.\nEnglish Datasets For English data, we use several open-domain QA datasets including NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018). For NQ and TriviaQA, we follow the split from previous work (Karpukhin et al., 2020), and default split for HotpotQA. We randomly gather 60k instances from the training set of the three datasets to conduct Tall for training rewrite models. As for evaluation, we collect the test set of NQ and TriviaQA, and the development set of HotpotQA as the held-in evaluation datasets. Additionally, we use FreshQA (Vu et al., 2023) for out of domain evaluation.\nChinese Datasets For Chinese data, we gather a bunch of open-source queries to conduct the query set, the sources are listed in 6. We use WebQA (Li et al., 2016) for the in-domain evaluation, while FreshQA (Vu et al., 2023) (translated) for the out-of-domain evaluation. The process of translation can be found in Appendix A.2.2."}, {"title": "3.2 Evaluation Settings", "content": "In practical retrieval scenarios, query rewriting is commonly used as a technique to expand the retrieved documents based on the original query, followed by a re-ranking of the expanded documents. Thus, we validate RaFe in two experimental settings.\nSUBSTITUTE Directly use the documents D' retrieved by rewrite q' for evaluation instead of the documents D retrieved by query q.\nEXPAND Employing both D and D' for evaluation. We generate two rewrites q1, q2 for the EXPAND setting with their retrieved D'1, D'2.\nTo further simulate the role of query rewriting in real-world scenarios, our experiments also include the performance under two following settings:\nRaw Concatenating top-5 retrieved documents in the default order. For EXPAND setting, the raw documents order is determined by sequentially and cyclically selecting the top documents from D, D'1, D'2.\nRanked Concatenating top-5 documents after re-ranking all the retrieved documents. As regard to EXPAND setting, all retrieved documents from both the query and rewrites are merged for ranking."}, {"title": "3.3 Baseline", "content": "Original Query Retrieval (OQR) Retrieve with the original query and utilize the documents by the default returned ranking from the search engine.\nLLM Rewrite Directly enable the LLMs to rewrite the original query with a few-shot prompt. In our experiment, we prompt Qwen-max to rewrite the original query.\nQuery2Doc (Wang et al., 2023) A method creates pseudo-documents through few-shot prompting of LLMs and then the query is expanded with the generated pseudo-documents for retrieving.\nSFT Use the pre-generated rewrites to directly train the rewrite model. SFT(Tsft) represents the rewrite model trained specifically on the Tsft, while SFT(Tall) denotes the model trained on Tall."}, {"title": "3.4 Implementation", "content": "Retriever We use an anonymous internal search engine for open domain to retrieve documents for the Chinese datasets, and Google Search for the English datasets. Specifically, we utilize the title and the summary snippet of the searched page as the retrieved documents for retrieval augmentation.\nBase Model We employ Qwen-max (Bai et al., 2023) to generate responses and conduct the evaluation with Qwen1.5-32b-chat. Query rewriting models are trained with the Qwen-7b-base.\nReranker For a general RAG task like open-domain QA, we believe that if our approach yields positive results with a general reranker, when transferring to a specific domain (where a domain-specific reranker is available), it will perform even better. Thus, we employ a publicly available bge-reranker (Xiao et al., 2023) to conduct open-domain QA experiments, which serves to demonstrate the effectiveness of the methods we designed."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Main Result", "content": "From Table 1 and Table 2, we can observe that RaFe outperforms other query rewriting baselines"}, {"title": "4.2 Compared with Other Types of Feedback", "content": "Previous work on training query rewrite models for the RAG (Ma et al., 2023) has leveraged LLMs performance on QA tasks as the feedback signal."}, {"title": "5 Analysis", "content": ""}, {"title": "5.1 How RaFe makes rewriting better?", "content": "In this section, we present illustrative case studies to intuitively compare different rewrites and the original query in Figure 3. The benifits of RaFe can be summarized into three types.\n(A): RaFe performs better in preserving the semantics of the original query. As shown in Figure 3 (A), it can be observed that RaFe, after alignment through reranker, can rewrite queries in a way that better preserves the semantics of the original query. In contrast, the rewrite by SFT directly shifts the focus of the query from which athlete to which competition.\n(B): RaFe's rewrites improve the format of the query for retrieval purposes. RaFe's rewrite is capable of transforming an uncommon term \u201crecipient\u201d into \u201cwinner\u201d. Although SFT rewrites also replace \u201crecipient\u201d with \u201cwinner\u201d, it changes \u201cteam\u201d from a sports competition context to \u201csquad\u201d, a term commonly used in military, police, or other contexts, thereby introducing potential ambiguity.\n(C): RaFe's rewrites sentences for better understanding. This kind of case is not easily discernible as good or bad based on intuition; however, RaFe's rewrite demonstrates better performance in retrieval results. Such cases show why we require feedback to enhance the QR effectiveness, as we always fail to articulate how a query could be formatted to better suit a retriever."}, {"title": "5.2 How does the Reranker Feedback Work?", "content": "To investigate how reranker works for query rewriting, we first ascertain the ability of the publicly available reranker to rank on unseen datasets.\nThe comparing results are presented in Figure 4. It can be clearly seen that all methods yield better QA performance after documents are ranked on all the datasets. This indicates that the reranker's pattern for document sorting acts as a positive signal for the retrieval system. Meanwhile, we can observe that RaFe performs the better improvements after ranked, which further demonstrates the effectiveness of reranker feedback.\nMoreover, we validate the effectiveness of reranker in constructing good and bad pairs within Tf. We compare the precision of documents retrieved by different queries in Table 5. It is obvious that the documents retrieved by good rewrites exhibit significantly higher precision compared to those retrieved by the original query, which indicates that the reranker is capable of effectively distinguishing between rewrites that can retrieve high-quality documents and those that cannot. We also provide some examples in Appendix A.4.2."}, {"title": "5.3 How Many Rewrites is Optimal for RAG?", "content": "In this section, we delve deeper into the impact that varying numbers of rewrites have on the final performance, since in practical applications of query rewriting, a balance must be struck between the quantity of generated rewrites and performance efficiency, given that generating more rewrites could potentially result in more response time. We generate different numbers of rewrites, the results are depicted in Figure 5. The QA results peak when there are 4-5 rewrites, suggesting that employing more rewrites can yield considerable benefits by retrieving more relevant top documents. However, Prec@5 nearly approaches the best around 2-3 rewrites. When ranking all passages, the performance ceiling is attained with merely 2 rewrites. Considering the time cost, 2-3 rewrites may benefit the most for practical RAG."}, {"title": "6 Related Work", "content": ""}, {"title": "6.1 Query Rewriting", "content": "Query rewriting is a critical technique within the retrieval domain (Carpineto and Romano, 2012; Zhu et al., 2023). With the groundbreaking advancements in scaling-up model capabilities, query rewriting has also played a pivotal role in enhancing the abilities of LLMs in RAG (Khattab et al., 2022; Press et al., 2023; Yan et al., 2024). Many works (Wang et al., 2023; Shen et al., 2023; Ye et al., 2023) directly leverage LLMs' strong capabilities to expand or rewrite queries. Nonetheless, in practical application scenarios, a smaller rewriting model is preferred to avoid the costly requests for LLMs. At the same time, feedback training is the most commonly employed method to enhance the smaller rewriting models. Nogueira and Cho (2017) incorporates the ranking signals from annotated passages for better results, as well as previous works on conversational query rewrite (Wu et al., 2022; Mo et al., 2023; Chen et al., 2022). Ma et al. (2023) first generates answers from LLMs and then uses the QA evaluation results as the training signals. Peng et al. (2023) leverages search scoring functions intrinsic to the e-commerce framework to assess rewrite quality, informing feedback signals, which is exceedingly domain-specific, limiting its applicability to other domains.\nThese works depend on using particularly designed scores or annotated labels for feedback signals, while our proposed method can generically deliver feedback based on ranking results, without needing annotated passages."}, {"title": "6.2 Learning From Feedback", "content": "Recent advancements in Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) have been instrumental in aligning the generative capabilities of large models with human preferences, significantly prompting the creation of strong LLMs (OpenAI, 2023). Therefore, a large number of studies about feedback alignment have been emerging (Zheng et al., 2023; Wang et al., 2024; Rafailov et al., 2023; Yuan et al., 2023; Dong et al., 2023; Kawin et al., 2023). Some research efforts are concentrated on devising methods to provide new forms of feedback (Lee et al., 2023; Shinn et al., 2023; Madaan et al., 2023; Pang et al., 2023; Liu et al., 2023a; Aky\u00fcrek et al., 2023; Nathani et al., 2023). Xu et al. (2023) propose to train models from judgment language feedback. Li et al. (2023) designs two types of ranking feedback drawing from LLMs, to improve the performance.\nDespite all these works, the exploration of feedback in rewriting is currently limited to direct feedback from LLMs (Ma et al., 2023) and domain-specific scoring (Peng et al., 2023). Such feedback approaches are costly and fail to utilize the effective signals from the IR system. While Le et al. (2022) and Liu et al. (2023b) effectively leverage the feedback from Unit Test in the domain of code generation, we investigate more appropriate feedback signals for query rewriting in this paper, the reranker feedback."}, {"title": "7 Conclusion and Future Work", "content": "This paper proposes a novel feedback training framework named RaFe for query rewriting, based on the effectiveness of the reranker in enhancing document ranking during the information retrieval process. By leveraging the feedback signals from reranker, RaFe is capable of effectively and generally conducting feedback training for rewrite models, yielding great improvements. Experimental results indicate that our method achieves exemplary performance across cross-linguistic datasets. In the future, we plan to conduct the joint training of reranker and rewrite models, which may yield substantial benefits for RAG."}, {"title": "Limitations", "content": "Although our experiments employ a general reranker as the source of feedback signals, there are still some limitations. (1) The Lack of Cross-Domain Validation. As constrained by the lack of domain-specific data, we lack the validation of separately trained rerankers on datasets pertinent to a specific domain. (2) Reliance on the Effectiveness of Rewriting as a Bottleneck. Although we can achieve some improvements by using publicly available rerankers, this enhancement may be limited by the capability of the reranker."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Feedback Training Loss", "content": ""}, {"title": "A.1.1 DPO Loss", "content": "Ldpo = -E(q,qg,q%)~T; [log \u03c3\n(\\frac{M_{\\theta}(q_g|q)}{M_{ref}(q_g|q)}\n) -Blog\n(\\frac{M_{\\theta}(q_b|q)}{M_{ref}(q_b|q)}\n)], (5)\nwhere \u03b2 is the temperature parameter for DPO, M\u03b8 is the rewrite model to be updated, and Mref is the fixed model during the training phase."}, {"title": "\u0391.1.2 \u039a\u03a4O Loss", "content": "The KTO (Kawin et al., 2023) (Kahneman-Tversky Optimization) method is based on prospect theory (Tversky and Kahneman, 1992), which tells how human decides according to uncertain outcomes. The theory is proposed by the economists Kahneman & Tversky. Compared to DPO, the training based on KTO only needs the signal that whether a rewrite q' is good or not, formulated as (q, q\u0384; \u03c1), \u03c1\u2208 [good, bad]. And the Lkto is computed as follows:\nLkto = E(q,q';p)~T, [w(q')(1 \u2013 \u0125(q, q'; \u03c1))],\ng(q, q'; p) = \u03b2log\n(\\frac{M_{\\theta}(q'|q)}{M_{ref}(q'|q)}\n)-Eq'~T [BKL(M\u03b8||Mref)],\nh(q, q'; \u03c1) = \\\n{\n(g(q,q';p)) if p is good\n\u03c3(-g(q, q'; \u03c1)) if p is bad\n,\nw(d') = \\\n{\nAgood if p is good\nAbad if p is bad\n(6)\nThe default values for Agood and Abad are set to 1. When there is an imbalance between the number of good and bad samples, specific values are determined as the following formula:\nAgood/ngood\n\u2208 [\\frac{1}{4}, \\frac{3}{4}],\nAbad/nbad\n(7)"}, {"title": "A.1.3 PPO Loss", "content": "When implementing PPO training, we indicate the action at at step t as generating the next token qt, while the current state st = (q, q<t) is composed of the original query and generated rewrite tokens. Here we directly use the ranking score as a reward, and by adding a KL-divergence regularization (Ramamurthy et al., 2023; Carpineto and Romano, 2012), the reward is computed as follow:\nR(st, at) = Sreranker(q|q) \u2013 BKLKL(M\u03b8||Mref)\n(8)\nand then with a value network V initialized from M\u03b8, the advantages function follows GAE (Schulman et al., 2016) can be formulated as:\n-dt = R(st, at) + V\u03c6(st + 1) \u2013 V\u03c6(st),\nA(st, at) = \\sum_{t'=0}^{\\infty} \\gamma^{t'} dt+t',\n(9)\nand the final objective function is composed of value loss and policy loss (Zheng et al., 2023).\nL\u03b8 = E(stat)~M\u03b8 [min\n(M_{\\theta}(s_t, a_t)\n(M_{ref}(s_t, a_t)\n-A(st, at),\nclip(\n(M_{\\theta}(s_t, a_t)\n(M_{ref}(s_t, a_t)\n),1 \u2212 \u03f5, 1 + \u03f5)A(st, at))],\nL\u03c6 = E(stat)~M\u03b8(V\u03c6(st) \u2013 Rt)\u00b2,\nLppo = L\u03b8 + L\u03c6\n(10)"}, {"title": "A.2 Training Details", "content": ""}, {"title": "A.2.1 Implementation", "content": "All model training are completed on a single machine with 4\u00d7A100 GPUs. And the training prompt for the rewrite is listed in Table 11.\nSFT We train the rewrite model with 2 epochs and set the learning rate to 5e-5.\nPPO The PPO implementation is carried out according to the TRL repo (von Werra et al., 2020). In line with the empirical configurations in previous work (Zheng et al., 2023), we set the batch size"}, {"title": "A.3 Additional Experimental Results", "content": ""}, {"title": "A.3.1 The Retrieval Results", "content": "We report complete retrieval results of Prec@10 and MRR in this section. The results of SUBSTITUTE-Raw and EXPAND-Raw are shown in Table 7, while the results of SUBSTITUTE-Ranked and EXPAND-Ranked are in Table 9.\nComparing the results between SUBSTITUTE and EXPAND, it can be found that methods with lower retrieval results under the SUBSTITUTE setting tended to show greater improvement under EXPAND. However, the retrieval results for RaFe do not exhibit great improvement under the EXPAND-Raw setting. Further comparison between the QA results and retrieval metrics reveals that, generally, the trends of improvement in retrieval results align with those in QA performance."}, {"title": "A.3.2 QA Results of Qwen-32b", "content": "To further demonstrate the results of our methods, we conduct experiments on different sizes of models. Specifically, we choose Qwen1.5-32b-chat for evaluation. The results are shown in Table 8."}, {"title": "A.3.3 Top-k Documents Results", "content": "Additionally, we explore the performance of our proposed method when concatenating a different number of documents. We carry out the experiment on the Chinese version of the FreshQA. The results presented in Figure 6 reveal that when solely the first document is utilized, the retrieval using the original query yields the best results. As the number of concatenated documents increases, RaFe consistently outperforms both SFT and the original query results."}, {"title": "A.4 Additional Analysis", "content": ""}, {"title": "A.4.1 The Relatively Weak Performance", "content": "From the results, it can be observed that there are only marginal improvements in some datasets, especially in SUBSTITUTE-Raw setting. Taking the NQ dataset as an example, we attempt to investigate the difference between. The NQ dataset is a quite hard dataset, so for challenging cases, the minor reformulation of key phrases could cause tion or confuse with other types of crosses, thereby reducing the precision of the search results.\nAdditionally, the results on smaller models revealed that RaFe could achieve noteworthy improvements even in SUBSTITUTE-Raw results. Thus, we obtain the cases answered both correctly and wrong by different size models. As shown in Table 10, the average prec@5 on 'good' cases is comparable between models of different sizes. However, in 'bad' cases, smaller models exhibit higher average precision. In contrast, when comparing the the results between Qwen-max and Qwen-32b, the improvements from RaFe diminish. This suggests that the benefits RaFe brings in simple cases are reduced as the model's parameter increases. Meanwhile, the deviations in more challenging cases are retained, which could lead to less impressive results. This further implies that query rewriting for RAG might be better suited for the EXPAND setting, to broaden the scope of the query to increase the chances of retrieving relevant information."}, {"title": "A.4.2 Good-Bad Pairs Cases", "content": "In this section, we delve deeper into how rerankers take effect by presenting case studies. We investigate cases of how rerankers distinguish between good-bad pairs. Figure 7 provides two examples.\nIn the first example, the original query pertains to the only European country where wild porcupines reside. The good rewrite simplifies to a more direct question: \u201cIn which unique European country do"}, {"title": "A.4.3 Additional Case for Better Format Rewriting", "content": "We provide one more case in this section. The original question used the phrase \u201cwoman in music\u201d to inquire about the highest-earning female musician in Forbes' Celebrity 100 list in 2020, which may not have been as intuitive for search engines, resulting in a failure to retrieve the documents. While the RaFe rewrite directly refines \u201cwoman in mu"}, {"title": "A.5 Prompts", "content": "In this section, we list the prompt we used in this paper. The instruction prompt for rewrite model is shown in Table 11, and the prompt for evaluation is in Table 12. The few-shot prompts used for Query2Doc are derived from Wang et al. (2023), and we use the same prompts for the LLMs Rewrite."}]}