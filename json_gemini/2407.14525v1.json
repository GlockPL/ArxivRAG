{"title": "Morse Code-Enabled Speech Recognition for Individuals with Visual and Hearing Impairments", "authors": ["Ritabrata Roy Choudhury"], "abstract": "The proposed model aims to develop a speech recognition technology for hearing, speech, or cognitively disabled people. All the available technology in the field of speech recognition doesn't come with an interface for communication for people with hearing, speech, or cognitive disabilities. The proposed model proposes the speech from the user, is transmitted to the speech recognition layer where it is converted into text and then that text is then transmitted to the morse code conversion layer where the morse code of the corresponding speech is given as the output. The accuracy of the model is completely dependent on speech recognition, as the morse code conversion is a process. The model is tested with recorded audio files with different parameters. The proposed model's WER and accuracy are both determined to be 10.18% and 89.82%, respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "The process of converting spoken language into text is known as speech recognition. It is a type of technology that enables computers to recognize and respond to spoken commands or questions. Speech recognition is classified into two types: command and control and dictation. Command-and-control systems are intended to recognize specific commands, whereas speech-to-text systems are intended to transcribe spoken language into text. Speech recognition is divided into two different approaches: rule-based and statistical. To interpret speech, rule-based systems use a set of predefined rules and grammar, whereas statistical systems use machine learning algorithms to learn patterns in speech and translate them into text. Statistical systems are more complex than rule-based systems but are also more accurate and flexible. They can adapt to new vocabularies and speaking styles over time and can handle a wide range of accents, dialects, and speaking styles. Speech recognition technology is widely used in various applications, such as voice assistants, voice search, voice-controlled devices, and transcription services.\nThere are several drawbacks to existing speech recognition models, including limited accuracy, limited language support, and so on. But one of the major drawbacks of speech recognition is limited accessibility for people with disabilities. People with hearing, speech, or cognitive impairments may find it difficult to use speech recognition models, and they may not be fully accessible to people with disabilities. This is where our model has tried to fill this gap with a technology that will make speech recognition fully accessible to people with disabilities. Our model has incorporated morse code with speech recognition, thereby making the speech(audio) interpretable by disabled people.\nMorse code is a method of transmitting text information as a series of on-off tones, lights, or clicks that a skilled listener or observer can directly understand without the use of special equipment. It was invented in the 1830s and 1840s by Samuel Morse and Alfred Vail for use with the telegraph, the first form of electrical communication. The basic Morse code unit is the dot (short signal), which lasts one unit of time, and the dash (longer signal), which lasts three units of time. A unique sequence of dots and dashes represents the letters of the alphabet, numbers, and punctuation marks. For instance, the letter \"A\" is represented by a single dash, the letter \"N\" by a single dot followed by a single dash, and the number \"1\" by a single dot. Morse code was widely used for telegraph communication for many decades, and it remained in use for some time after the invention of the telephone. It is now primarily used by amateur radio operators as a form of signaling and occasionally in emergency situations. Morse code is not a human-readable text.\nMorse code is sometimes used as an alternative mode of communication for the challenged of hearing and deaf individuals, or who have difficulty speaking or using their hands. Morse code can also be used as an assistive technology for people with mobility issues, such as those who have difficulty typing on a keyboard with their hands. They can communicate via a morse code interface, such as a simple switch or a sip-puff device activated by blowing or inhaling. In all of these cases, using morse code allows people with disabilities to communicate and interact with the world around them, even when other forms of communication are difficult."}, {"title": "A. Relate Work", "content": "Yang Xu conducted a measuring study using Google+, i-Chat, and Skype [1]. They looked into these programmers' architectural nuances. The authors revealed some performance information on these applications using passive and active tri-als, including video generation and adaptation methods, packet loss recovery approaches, and end-to-end delays. According to their research, the location of the server dramatically impacted both user performance and loss recovery in server-based apps. The use of Forward Error Correction (FEC), an error management approach for streaming across unpredictable network connections, was also refuted in favor of batched re-transmissions as a better option for real-time applications.\nA measuring investigation on the effectiveness of Skype's Forward Error Correction (FEC) mechanism was carried out by Te-Yuan Huang [2], [3]. They looked at the trade-offs between the quality of the user experience, the redundancy caused by the Forward Error Correction (FEC) mechanism, and the redundancy caused by the Forward Error Correction (FEC) mechanism, as well as the amount of redundancy added by the Forward Error Correction (FEC) process. To maximize the quality of the user experience, they looked for the right amount of redundancy.\nA study on Skype's speech rate adaptation in various network scenarios was also carried out by Te-Yuan Huang [4]. The results of this investigation showed that implementing public-domain codecs was not the optimal choice in terms of user satisfaction. To execute their experiment in this study, the researchers took into account various packet loss levels. They then developed a model to manage redundancy under various packet loss scenarios.\nA measurement methodology for users' QoE was put forth by Kuan-Ta Chen [5]. OneClick, the framework they suggested, offered a specific key that users could hit if they weren't happy with the network settings for stream-ing video. Two applications\u2014instant messaging and shooter games-have OneClick installed.\nKuan-Ta Chen [6] offered yet another methodology for quantifying the quality of a user's experience. The proposed technology supported crowd-sourcing since it could validate contributors' inputs. This framework facilitates participation and yields interval-scale scores. They contend that this ap-proach can be used by researchers to gauge user experience quality without degrading the quality of the research findings and to increase user participation diversity at a minimal cost.\nLukasz Budzisz [7] has conceived and developed a delayed-based congestion control. In homogeneous networks, the sug-gested system delivers low-standing queues and delays, while in heterogeneous networks, it enables balanced loss- and delay-based flows. They contend that this system outperforms TCP traffic and can accomplish these qualities with a range of loss values. Analyses and experiments show that this system ensures the aforementioned features.Hayes suggested a technique that accepts packet loss unrelated to congestion [8]. They demonstrated experimentally that the suggested approach increases throughput by 150% with 1% packet loss and increases capacity sharing by more than 50%.\nAkhshabi suggested doing an experimental analysis of rate adaptation techniques for HTTP streaming [9], [10]. Three popular video streaming applications were experimentally as-sessed over a range of bandwidth ranges. The findings of this study demonstrated that the performance of such streaming applications is not always impacted by TCP congestion man-agement and its reliability requirement. It is yet unknown how rate adaption logic and TCP congestion management interact.\nChen conducted an experimental study to examine how multi-path TCP performed across wireless networks [11]. They calculated the latency brought on by various cellular data carriers. According to the study's findings, Multi-path TCP provides a reliable data transfer under a variety of network-traffic scenarios. It should be thought about as a potential expansion of this study to examine the trade-offs between energy costs and performance.\nGoogle is actively developing QUIC (Quick UDP Internet Connections), a new transport protocol for the Internet [12], [13]. QUIC employs UDP to address packet delay issues in TCP connections with varying packet loss values. QUIC uses multiplexing and FEC to resolve this issue. Cicco and colleagues carried out an experimental examination on the Google Congestion Control (GCC) in the RTCWeb IETF WG [14]. For their experiment, they used a controlled testbed. The experimental investigation findings show that the suggested approach works successfully, however it does not use the bandwidth evenly when it is shared by two GCC flows or a GCC and a TCP flow.\nCicco has also conducted experimental research into Aka-mai's High Definition (HD) video distributions [15], [16]. They gave specifics on the client-server protocol used by Aka-mai to implement the quality adaptation algorithm. According to their research, the suggested method encodes any video at five distinct bit rates and stores them all on the server. Based on the signal it receives from the silent, the server chooses the bit rate that corresponds to the bandwidth measurement. Depending on the available bandwidth, the bitrate level adapts. The authors of the research also assessed the algorithm's dynamics in three scenarios.\nTo evaluate the quality of the user experience on televi-sion and mobile applications, Winkler [17], [18] conducted a series of studies. Their suggested experiment takes into account various bitrates, contents, codecs, and network traffic situations. The authors of the research used Double Stimulus Impairment Scale (DSIS) [19] and Single Stimulus Continous Quality Evaluation (SSCQE) on the identical collection of materials. They contrasted the two approaches and looked at the experiment outcomes in relation to codec performance.\nOh Hyung Rai [20] suggests mesh-pull-based P2P video streaming with fun-train coding. The recommended method offers streaming that is easy, rapid, and smooth. The proposed system outperforms existing buffer-map-based video streaming systems in packet loss scenarios, according to experimental testing. This study might be extended by looking at jitters as another crucial component and evaluating the behavior of the suggested system in light of jitters values.\nSmith [21] takes into account the use of Fountain Multiple Description Coding (MDC) in video streaming via diverse peer-to-peer networks. They come to the conclusion that Fountain MDC codes are advantageous in these circumstances, however, there are some limitations in actual P2P streaming systems.\nFinally, Vukobratovic [22], [23] suggested a unique real-time multicast Expanding Window Fountain (EWF) code-"}, {"title": "B. Motivation and Contribution", "content": "In [24], Kuldeep Kumar, R.K. Aggarwal, and Ankita Jain develop their own voice recognition system using the Mel fre-quency cepstral coefficient (MFCC) approach and the hidden Markov model toolkit (HTK). In order to deliver real-time voice-based machine translation, authors examine existing speech recognition technologies. Using Dragon Medical 11.0, Hanna Suominen, Liyuan Zhou, Leif Hanlen, and Gabriela Ferraro [25] propose using voice recognition to prevent errors in information flow in healthcare. In [13], Siri, Google Speech Recognizer, and Dragon were used to analyze cloud-based speech recognition systems. Numerous authors contrast their own systems. Using open-source code, Belenko M.V. and Bal-akshin P.V. [26] examine systems, enter evaluation coefficients for various parameters, and offer suggestions for recognition systems.\nIt is advised to employ HTK and Julius in voice recognition instructional activities. Research tasks can be successfully conducted with Kaldi [26]. Examine deep neural networks (DNNs) with many hidden layers as well as the methods used to train them in [27]. In [28], authors use Deep Belief Networks to pre-train a context-dependent artificial neuron net (ANN)/HMM system that was learned on two datasets. To enhance its voice recognition capabilities, Google created a front-end for neural networks. For low-resource languages, the authors of [29] try to use Google Speech Recognition. Authors of [30] from Google Inc. report the creation of an effective, compact, and extensive voice recognition system for mobile devices. HTK is used by Shelza Dua, Mohit Dua, R.K. Aggarwal, Virender Kadyan, among others [31] for Punjabi Automatic Speech Recognition.\nIt is advised to employ HTK and Julius in voice recognition instructional activities. Research tasks can be successfully conducted with Kaldi [26]. Examine deep neural networks (DNNs) with many hidden layers as well as the methods used to train them in [27]. In [28], authors use Deep Belief Networks to pre-train a context-dependent artificial neuron net (ANN)/HMM system that was learned on two datasets. To enhance its voice recognition capabilities, Google created a front-end for neural networks. For low-resource languages, the authors of [29] try to use Google Speech Recognition. Authors of [30] from Google Inc. report the creation of an effective, compact, and extensive voice recognition system for mobile devices. HTK is used by Shelza Dua, Mohit Dua, R.K. Aggarwal, Virender Kadyan, among others [31] for Punjabi Automatic Speech Recognition.\nFor Arabic phonemes, [32] authors utilize CMU Sphinx. Convolutional neural networks are used for error rate reduction by the authors of [33]. Techniques for creating noise-resistant voice recognition systems are analyzed by Jinyu Li, Li Deng, Yifan Gong, and Reinhold Haeb-Umbach [34]. Oliver Lemon [35] and Jerome R. Bellegarda [36], as well as Li Deng and Xiao Li [37], investigate speech recognition interfaces, and from these articles, we may distinguish Siri. Silnov Dmitry Sergeevich [38] uses Yandex Speech-Kit and Google Speech to decode radio conversations.\nThe authors of [39] present their CMU Sphinx-based recog-nition system. The authors of [40] suggest using Microsoft Speech API to automate your home. The usage of Microsoft Speech SDK to provide an online resource for students to train their speaking abilities is covered by Howard Hao-Jan Chen [41]. Kaldi Speech Recognition Toolkit is described by Povey, D. el. [42]. Microsoft Speech Engine is used in Ivan Tashev's [43] and R. Maskeliunas, K. Ratkevicius, and V. Rudzionis' [44] analysis of human-machine interaction. Microsoft Speech API is a suggestion made by Y. Bala Krishna, S. Nagendram [45], Faisal Baig, Saira Beg, and Muhammad Fahad Khan [46] for usage with smart home equipment. The authors of [47] suggest using Microsoft Speech API to create assistive technology that will enable communication between two phys-ically impaired people the blind and the deaf. The writers of [48] employ the Microsoft Speech API to assess audience response.\nThe Microsoft Speech API is utilized by the authors [49] to create an examination system that is accessible to students with disabilities. The use of Microsoft Speech API for discussion systems is also examined by the authors in [50]. A language model on Holy Quran recitations was trained and evaluated using CMU Sphinx tools [51]. The CMU Sphinx is used by the creators of [52] to train the system and decipher voice data. Using Sphinx technologies, Hassan Satori and Fatima ElHaoussi [53] create their own continuous automatic speech recognition system that is speaker-independent.\nThe same authors demonstrate their ability to define Smok-ers and Nonsmokers utilizing their CMU Sphinx-based system. Sphinx is used by authors to create subtitles in [54] through a three-stepstep procedure that combines audio extraction, speech recognition, and subtitle synchronization. In [55] CMU Polish speech was recognized using Sphinx. I. Medennikov and A. Prudnikov [56] Speech by Kaldi Russian voice recog-nition tests were conducted using the recognition tool-set. The [57] authors employ Kaldi for speech recognition in Africa. Kaldi is used by the authors in [58] for speech recognition.\nThe Kaldi voice recognition system is being developed by authors [48] to be compatible with Julius. For the creation of a robot voice recognition system in [59], the authors employ Julius. Julius is used by creators [60] of mobile applications. HTK is used by the authors of [61] for recognition whisper. HTK was utilized in [62] for Telugu language recognition. HTK was also utilized in [63] for their own speech-to-text system."}, {"title": "C. Novelty", "content": "Most of the current speech recognition models have sev-eral drawbacks for people with hearing, speech, or cognitive impairment. Speech recognition models may have trouble accurately identifying the speech of some people with speech impairments because they have trouble speaking. Using speech"}, {"title": "II. PROPOSED WORK", "content": "Our speech recognition model focuses on making it useful for people with hearing, speech, or cognitive impairments. The proposed model collects speech from the user via a microphone. Microphones collect the speech and transmit the audio to the proposed model. After the proposed model receives the audio, it first converts it into text format. Then that text format is converted to morse code. The processing of the audio to the morse code and the workflow of the proposed model are shown in Figure 1.\nThe audio file entering the proposed model is first passed through the speech recognition model (Section II-A) which converts the speech to text format. Within the speech recog-nition model (Section II-A); the audio is passed through a different neural network (section II-A1 and II-A2) which in steps converts the speech to text. Once the speech is converted to text it is then transmitted to the morse code converter (Section II-B) which with the help of morse code dictionary (Section II-B1) converts the text received to morse code"}, {"title": "A. Speech Recognition : Speech to Text", "content": "The Speech Recognition technology utilized in the proposed model is a speech-to-text technology created by Google. In more than 80 languages and variants, it can convert spo-ken words into written text. Our proposed model's Speech Recognition accurately records speech using a combination of machine learning techniques and a lot of data. The technology transcribes speech using both acoustic and language models. A detailed working of this speech recognition technology is shown in Figure 2"}, {"title": "1) Acoustic Model", "content": "The deep neural network (DNN) uti-lized in the speech recognition technology of our model was trained on a lot of audio data. The model's precise architecture and training information are confidential and not made available to the public by Google. However, in general, the model is most likely to be a multi-layered Time-Delay Neural Network (TDNN) or a variation of a Long Short-Term Memory (LSTM) network. A spectrogram or Mel-Frequency Cepstral Coefficients (MFCCs) of the audio are commonly used as the network's input, and its output is a probability distribution over a collection of potential phonemes or sub-words. To improve the network's parameters for precise voice recognition, the model is trained using a combination of supervised and unsupervised learning methods, including backpropagation and Connectionist Temporal Classification (CTC)."}, {"title": "2) Language Model", "content": "The speech recognition technology used in the proposed model uses a recurrent neural network (RNN) variation that has been trained on a lot of text data. However, in general, the model is to be a multi-layer trans-former network or a variation of an Long Short-Term Memory (LSTM) network. The network typically receives a series of phonemes or sub-words produced by the acoustic model mention in II-A1 as input, and the result is a probability distribution over the set of potential words or phrases. To opti-mize the network's parameters for precise speech recognition, the model is trained using supervised learning methods like backpropagation. The acoustic model's sequence of sub-words is evaluated using the trained model to assign probabilities, and the word sequence with the highest probability is selected as the transcription."}, {"title": "B. Morse Code Converter: Text to Morse Code", "content": "The suggested model's morse code converter is a function that accepts text input as input and outputs the appropriate morse code. The morse code dictionary and the function are both functional. The dictionary links every character and num-ber in the English alphabet to its appropriate representation in Morse code. Initially, the converter initialises a blank string to hold the generated Morse code.\nEvery letter in the given text is tokenized by the converter. It determines whether a given letter is a space or not for each one. In the event that it isn't a space, it adds the appropriate Morse code representation to the string of Morse code after locating it in the dictionary. It only adds a space to the morse text string if the letter is a space.\nAfter tokenizing through all the letters in the input text, the function returns the morse code string, which contains the complete Morse code representation of the input text.\nLet's consider an example, if the speaker says \"HELLO\" then the transmitted text to the converter is \"HELLO\", the converter will first tokenize each letter and look up the corresponding Morse code representation from the morse code dictionary and add it to the morse code string. So, the final output will be \" . -... -.. . --- \""}, {"title": "1) Morse Code Dictionary", "content": "The morse code dictionary is used in the morse code converter to convert each character of the transmitted text to its corresponding morse code represen-tation. The key in the dictionary is the letters and numbers that are present in the transmitted text, and the values are the corresponding Morse code representations of those letters and numbers. By using this dictionary, the morse code converter is able to convert each character in the transmitted text to its corresponding Morse code representation, store the final result and give it as output."}, {"title": "III. RESULTS AND ANALYSIS", "content": "The accuracy of the proposed model depends only on the accuracy of the speech recognition layer of our model. This dependency is because the morse code converter is a process and thus its accuracy is always 100%. The morse code conversion layer's accuracy depends completely on the output generated by the speech recognition layer.\nThe unit used in this study to measure the accuracy of the voice recognition layer is the word error rate (WER) [64], [65]. The ratio of transcription errors to total words spoken is known as WER. A lower WER in speech-to-text indicates better voice recognition accuracy. The Levenshtein distance, frequently referred to as the edit distance, is the source of WER. It determines the minimum number of edit operations required to change one string to obtain another [66].\nFrom the tables I and II, we can conclude that the average WER of the speech recognition is 10.18% and therefore the accuracy of the speech recognition layer of the proposed model is 89.82%."}, {"title": ":", "content": "However, the assessment parameters and therefore the pro-posed model's accuracy can change in a variety of ways [67]\n1) Confusable words and vocabulary size: A smaller vo-cabulary makes it simpler for the system to identify the right term than a bigger one. With a larger vocabulary, error rates inevitably rise. For instance, it is possible to recognize the numerals 0 through 10 properly, but mistake rates rise with larger vocabulary sets or the inclusion of confusable terms, or words with similar sounds. For instance, the phrases dew and you sound extremely similar yet have very different meanings.\n2) Speaker independence vs. dependence: Depending on the speaker and training, a speaker-dependent system is usually more accurate than a speaker-independent system. Furthermore, there are speaker-adaptive systems and multi-speaker systems that are intended for small user groups and that need very little speech data for training and understanding any speaker.\n3) Isolated, discontinuous, or continuous speech : The easi-est to identify are isolated, which refers to single words, and discontinuous, which refers to complete sentences with artificially separated words by silence. Because of co-articulation and hazy borders, continuous speech is the most challenging to identify, but it's also the most fascinating because it lets us speak normally.\n4) Task and language constraints : The restrictions may be job-specific, admitting only statements that are pertinent to the activity at hand, such as \"The car is blue\" being rejected by a business that sells tickets. Others may be syntactic, rejecting \"Car sad the is,\" or semantic, reject-ing \"The car is sad.\u201d Grammar represents constraints by removing absurd statements, and the perplexity of a sentence-a measure of the grammar's branching factor, or the number of possible words after a given word-measures how perplexing a sentence is.\n5) Read speech vs. Spontaneous: Compared to sponta-neous speech, which can include words like \"uh\" and \"um,\" as well as stuttering, coughing, and laughter, read speech from a text is simple to understand.\n6) Recording conditions: Background noise, acoustics"}, {"title": "IV. BENEFIT TO THE SOCIETY", "content": "The proposed model takes audio speech from the user's au-dience as input. This audio speech can be in the form of spoken words, phrases, sentences, or any vocal communication. The model processes and analyzes this input for various purposes, such as transcription, sentiment analysis, speech recognition, or any other relevant task. The user's audience refers to the individuals or group of people who are speaking or delivering the audio content.\nThe model is designed to work with this audio data and derive meaningful information or insights from it, depending on the specific application or use case. In essence, the model serves as a tool for processing and making sense of spoken language in audio form, providing valuable functionality and analysis based on the input it receives from the user's audience.\nThe proposed model is also designed to provide a unique and innovative way of conveying information to the user. Specifically, it takes spoken speech as its input and converts it into Morse code, a series of dots and dashes representing letters and numbers in a symbolic fashion. However, instead of displaying this Morse code output through traditional means such as sound or visual cues, it utilizes a sensory interface located on the lower surface of the user's arm.\nIn this context, the lower surface of the user's arm serves as a tactile communication platform. As the model processes the spoken speech, it translates it into the corresponding Morse code sequences, which are then conveyed to the user through a series of tactile sensations or vibrations on their arm. Each dot and dash in Morse code is represented by a distinct tactile signal, allowing the user to feel and interpret the converted message through their sense of touch.\nThe proposed model offers a myriad of significant ad-vantages for individuals facing challenges related to hearing, speech, or cognitive disabilities. Its core strength lies in providing an accessible, versatile, and universally comprehen-sible mode of communication, which can result in heightened independence, bolstered self-confidence, and improved self-esteem.\nThe Morse code model transcends barriers and offers a transformative approach to communication for individuals with hearing, speech, or cognitive disabilities, with the po-tential to significantly enhance their quality of life and partic-ipation in society.\nIn summary, the Morse code model holds profound societal advantages, particularly in its capacity to enhance inclusiv-ity and accessibility for individuals grappling with hearing, speech, or cognitive disabilities. It serves as a transformative tool that has the potential to bridge longstanding gaps between those with disabilities and the wider community."}, {"title": "V. CONCLUSION", "content": "We created a speech recognition system and reviewed it. The primary contribution of our work is that the proposed model focus on making speech recognition more accessible to people with hearing, speech, or cognitive impairments. Our method tries to develop a technology completely useful for only hearing, speech, or cognitive disabled people.\nThe proposal is mainly divided into two parts: Speech recog-nition and morse code conversion. The speech recognition receives the speech from the user. Then it converts the audio to text. This text produced acts as the input of the morse code conversion layer. It is where the text is converted to morse code and it is the final output of the proposed model.\nOther than helping people with hearing, speech, or cognitive disabilities, our model can also be used for military pur-poses. Morse code can employed in the military for telegraph transmission as well as for communication between ships and ground forces and headquarters. It was widely taught to military troops and became a necessary instrument for com-munication during military operations because to its simplicity and dependability. Morse code can also be beneficial when verbal communication is impossible because it can be sent through a variety of techniques (e.g., flashing lights, horns)."}, {"title": "VI. FUTURE SCOPE", "content": "By raising the speech recognition layer's accuracy, the suggested model may be enhanced. Our model can be rebuilt and developed further in the future and put it in work in a better manner. Now; if the morse code conversion layer is replaced with the Braille script conversion layer which converts the text produced from the speech recognition layer to braille script. Thereby it will come handy for the people with hearing and visual disabilities and also it will come as a helpful tool for people who are already trained or going to get trained in braille script.\nFurther more, as mentioned earlier in Section V, morse code can play a very helpful role in various military purposes. Therefore the proposed model can be rebuild and redesigned for any such specific purposes. There might be other scopes where morse can be put to use, and therefore the model can be redesign for that purpose."}]}