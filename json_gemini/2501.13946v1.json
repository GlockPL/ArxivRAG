{"title": "HALLUCINATION MITIGATION USING AGENTIC AI NATURAL LANGUAGE-BASED FRAMEWORKS", "authors": ["Diego Gosmar", "Deborah A. Dahl"], "abstract": "Hallucinations remain a significant challenge in current Generative AI models, undermining trust in AI systems and their reliability. This study investigates how orchestrating multiple specialized Artificial Intelligent Agents can help mitigate such hallucinations, with a focus on systems leveraging Natural Language Processing (NLP) to facilitate seamless agent interactions. To achieve this, we design a pipeline that introduces over three hundred prompts, purposefully crafted to induce hallucinations, into a front-end agent. The outputs are then systematically reviewed and refined by second- and third-level agents, each employing distinct large language models and tailored strategies to detect unverified claims, incorporate explicit disclaimers, and clarify speculative content.\nAdditionally, we introduce a set of novel Key Performance Indicators (KPIs) specifically designed to evaluate hallucination score levels. These metrics offer a structured and quantifiable framework for assessing the impact of each agent's refinements on the factuality and clarity of AI-generated responses. A dedicated fourth-level AI agent is employed to evaluate these KPIs, providing detailed assessments and ensuring accurate quantification of shifts in hallucination-related behaviors.\nA core component of this investigation is the use of the OVON (Open Voice Network) framework, which relies on universal NLP-based interfaces to transfer contextual information among agents. Through structured JSON messages, each agent communicates its assessment of the hallucination likelihood and the reasons underlying questionable content, thereby enabling the subsequent stage to refine the text without losing context. Experimental results suggest that this multi-agent, JSON-based approach not only lowers the overall hallucination scores but also renders speculative content more transparent and clearly demarcated from factual claims, improving the AI explainability level.\nOur findings underscore the feasibility of multi-agent orchestration and highlight the importance of maintaining a structured exchange of meta-information - particularly through formats supporting Natural Language API - to enhance the reliability and interpretability of AI-generated responses. The results demonstrate that employing multiple specialized agents capable of interoperating with each other through NLP-based agentic frameworks - such as the OVON framework - can yield promising outcomes in hallucination mitigation, ultimately bolstering trust within the AI community.", "sections": [{"title": "1 Structure of the Paper", "content": "This paper is organized to systematically address the challenge of mitigating hallucinations in Large Language Models (LLMs) using Agentic AI frameworks. The first section introduces the core challenges posed by hallucinations in Generative AI LLMs and defines essential concepts, including hallucination, Agentic AI, and multi-agent orchestration. Additionally, it presents the Open Voice Network (OVON) framework [10] as a standardized approach for facilitating inter-agent communication via Natural Language-Based APIs.\nThe experimental design and methodology are detailed in Section 3, focusing on the use of 310 carefully crafted prompts designed to induce hallucinations. This section also explains the design of the multi-agent pipeline, highlighting the specific roles of the agents and the structured use of OVON JSON messages for efficient inter-agent communication.\nSection 4 introduces a set of novel Key Performance Indicators (KPIs) developed to evaluate hallucination mitigation. These metrics - Factual Claim Density, Factual Grounding References, Fictional Disclaimer Frequency, and Explicit Contextualization Score - provide a structured framework for quantitatively assessing the pipeline's effectiveness. The section also includes the formulas used to calculate the Total Hallucination Scores (THS).\nThe empirical results are presented in Section 5, demonstrating how the multi-agent pipeline progressively reduces hallucinations. This section includes detailed visualizations to illustrate improvements in hallucination scores as prompts move through the pipeline. Section 6 follows with a practical use case, providing a step-by-step illustration of the pipeline's application in a real-world scenario. This example offers a tangible perspective on how the system operates to mitigate hallucinations.\nSection 7 discusses the results in depth, analyzing variations in hallucination mitigation across different prompts and highlighting the impact of OVON-based inter-agent communication in enhancing overall system performance.\nThe limitations of the current methodology are explored in Section 8, including its reliance on a limited set of LLMs and the need for greater transparency in their decision-making processes. The challenge of achieving full Explainable AI within these LLMs is also addressed, contrasted with the openness provided by the OVON-based agentic framework.\nSection 9 outlines proposals for future improvements, such as expanding the agentic framework, integrating additional specialized agents, and incorporating methodologies like Automated Design of Agentic Systems (ADAS) [21]. These advancements aim to refine the pipeline's adaptability and effectiveness in mitigating hallucinations.\nFinally, Section 10 concludes the paper by summarizing the findings and emphasizing the effectiveness of multi-agent orchestration in reducing hallucinations, highlighting the potential of this approach to advance Trustworthy AI."}, {"title": "2 Introduction", "content": "Hallucinations in the context of Large Language Models (LLMs) refer to instances where the model produces information that is factually incorrect, fabricated, or nonsensical while maintaining a confident, authoritative tone. For the remainder of this paper, we adopt the definition of \u201cagent\u201d as discussed by Schlosser in the Stanford Encyclopedia of Philosophy [19]. In that context, an agent is broadly understood as an entity capable of acting with intentionality and exercising a measure of control over its actions. By referencing this conceptual framework, we seek to highlight the notion of agency as it applies to our multi-agent system, wherein each agent - whether a human-guided process or an AI module - acts with a degree of autonomy and goal-directed behavior.\nLikewise, \"Agentic AI\" refers to artificial intelligence systems or architectures designed to operate as autonomous or semi-autonomous \"agents\" capable of performing specific tasks, making decisions, and interacting with other agents or systems in a goal-directed manner. The term \"Agentic\" highlights the notion of agency, where the AI acts with some level of intentionality, autonomy, and purpose within defined boundaries. The concept of Agentic AI is particularly significant in the development of modular, scalable, and transparent AI systems [6]. These systems rely on specialized agents with distinct capabilities working collaboratively to address complex problems. For instance, in the context of hallucination mitigation, one agent might focus on generating content, while others are tasked with reviewing, refining, and validating it to ensure accuracy and reliability. Similarly, in dynamic fields such as healthcare or mental health support, specialized agents can work together to manage domain-specific tasks, providing a comprehensive and cohesive solution. A recent survey [20] offers a detailed exploration of Agentic AI systems, examining their architectures, methodologies, and applications across various domains. Building on this foundation, the survey further outlines challenges and future directions for Agentic AI, including issues of explainability, interoperability, and ethical considerations.\nAgentic AI not only highlights the individual autonomy of these agents but also underscores the collective intelligence that emerges from their effective integration within a unified system.\nTo explore how multi-agent collaboration can mitigate such hallucinations, we propose an empirical testing approach that leverages Natural Language-Based APIs - specifically through the OVON (Open Voice Network) interoperability standard [10] and [11] - to facilitate seamless communication among different agent layers. By injecting three hundred and ten prompts deliberately designed to elicit imaginative or inaccurate responses, we track how each agent in our"}, {"title": "2.1 Inducing Hallucinations", "content": "In order to increase the likelihood of hallucinations, hundreds of different prompts have been generated and passed to the multi-agent environment using the following techniques:\nExploiting Knowledge Gaps and Underspecified Requests\nHighly esoteric or obscure topics: Asking about very niche subjects, especially ones where the model is less likely to have robust training data. For example, inquiring about a fictional research paper published in an obscure journal or a non-existent historical figure.\nAmbiguous queries: Posing questions that are open-ended and poorly defined. For instance, \"What were the economic policies of King Marlith XII in the ancient country of Sharoria?\" If the model's training data lacks references to these made-up entities, it may invent answers.\nCombining Fact and Fiction\nConfidently incorrect assertions: Starting the prompt by presenting partially correct but mostly fabricated information and then asking the model to elaborate. For example, \u201cThe celebrated French philosopher Jacque de Lafleur argued that dreams are actually the sensory echoes of future events. Can you summarize his main arguments?\" If the model does not recognize \"Jacque de Lafleur,\u201d it may weave a plausible-sounding explanation rather than admit ignorance.\nProviding contradictory context: Giving the model contradictory background and then asking it to reconcile the details. The struggle to maintain internal consistency often leads to hallucination. For example, \u201cI recently read in a reliable source that Isaac Newton's assistant was an Italian nobleman named Count Gheroni, who invented a precursor to the calculus. Can you provide more information on Gheroni's contributions?\" If the model tries to fill in the gaps rather than refute your statement, it may generate false details.\nPressing Beyond the Model's Limits\nRequesting very specific references: Asking for a summary of a non-existent chapter in a real book. For example, \"In the 15th chapter of Carl Sagan's 'Pale Blue Dot,' he discusses the philosophical implications of consciousness developing on Mars. What were his main conclusions?\" Since the model might have read the book but not remember such a chapter, it may concoct plausible but incorrect commentary.\nAsking for citations of non-existent sources: Requesting references, citations, or quotes from non-existent papers or authors. The model, if not properly constrained, might invent these.\nInstructing the Model to be Creative or Speculative"}, {"title": "2.2 Agentic AI and Multi-Agent Orchestration", "content": "The widespread adoption of powerful large language models has accelerated research into conversational AI and spurred the deployment of increasingly sophisticated systems. Within this expanding field, the concept of multi-agent orchestration has gained prominence, wherein specialized components - each possessing its own distinct functionality - collaborate to address user queries or complex tasks in an integrated manner. Several open-source and commercial frameworks have emerged to facilitate such architectures, often enabling developers to coordinate multiple autonomous agents while providing unified output to end users. In some cases, these frameworks retain the flexibility to route inquiries to different underlying modules without necessarily revealing the internal mechanics to external users.\nNotwithstanding their potential, many existing multi-agent solutions rely on proprietary methods of data sharing and message formatting, which can limit interoperability with third-party services. To overcome these boundaries, the Open Voice Network (OVON) introduced a standardized Natural Language-based API [10] designed to unify otherwise disparate approaches. This universal API model enables multi-agent environments to communicate seamlessly through intuitive, language-driven interfaces, effectively allowing an agent-based system to interact as a single entry point with external platforms. By adopting such a model, a fully managed solution - whether it involves an internal routing mechanism or specialized functionalities - can be encapsulated within an OVON-compatible JSON structure, ensuring that higher-level orchestration and additional multi-agent systems can integrate effortlessly. This approach preserves the benefits of specialized subprocesses operating behind the scenes while presenting a consistent, natural language interface that other compliant frameworks can leverage for broader collaboration and contextual awareness. The result is a more adaptable ecosystem in which both proprietary and open-source multi-agent frameworks, as well as large language models of varying origins, can coexist and exchange data smoothly."}, {"title": "2.3 The OVON Universal API approach", "content": "The Open Voice messages provide a way for conversational agents based on different technologies to communicate, using a lightweight format for conveying message metadata and a payload of natural language data between agents."}, {"title": "3 Experimental Design and Methods", "content": "During the Empirical Testing phase, three hundred and ten (310) prompts - synthetically generated and designed to increase the hallucination probability [8] - have been used to feed the front end agent. The answer of such agent has been forwarded to the second level agency to identify potential hallucination and produce a new answer for the 3rd level of agency, which main purpose is to further refines clarity, emphasizes the fictional nature, and maintains a polished, cohesive narrative. In addition, the second-level agent generates an OVON JSON message following the standard specifications [2], and the third-level agent interprets this JSON for refinement. All these interactions among the agents are orchestrated via an Autogen-based agentic framework [3], ensuring a robust and methodical structure for iterative improvement.\nIn other words:\n\u2022 Front End Agent (gpt-3.5-turbo based): Generates responses with potential hallucinations.\n\u2022 Second Level Reviewer (gpt-40 based): Refines the Front End Agent's response, reducing hallucinations and adding disclaimers, and inserts its responses in an OVON JSON message.\n\u2022 Third Level Reviewer (gpt-4o based): Further refines the Second Level Reviewer's response, minimizing hallucinations and strengthening disclaimers."}, {"title": "4 Hallucination KPIs", "content": "Below are the novel Key Performance Indicators (KPIs) assessed by the KPI Evaluator Agent (4th agent), designed to quantitatively demonstrate whether and how the second and third agents reduce the perceived factuality of hallucinations and enhance clarity regarding the content's fictional nature. These KPIs don't rely only on absolute factual truth but rather on linguistic and stylistic indicators that show the content transitioning from seemingly real to clearly fictional.\nFactual Claim Density (FCD)\nDefinition: Number of claims that appear to be historical, scientific, or verifiable facts per 100 words.\nInterpretation: A lower FCD suggests fewer statements that could be mistaken for real facts.\nFactual Grounding References (FGR)\nDefinition: Count how many times the text attempts to ground claims in \u201creal-world\u201d evidence. For instance, references to \"historical records,\" \"scientific evidence,\" or \"archaeological findings\" would increase this score.\nInterpretation: A reduction in FGR indicates the text is less pretentious about factuality and more overtly fictional.\nFictional Disclaimer Frequency (FDF)\nDefinition: Number of explicit mentions per 100 words indicating the text is fictional (e.g., \u201cfiction,\u201d \u201cmyth,\" \"imagined,\u201d \u201clore\u201d).\nInterpretation: A higher FDF means the text is more clearly framed as not factual. Fictional Disclaimer Frequency (FDF) measures how often a response explicitly labels its content as fictional, hypothetical, speculative, or imaginary. It quantifies the degree to which the response explicitly warns or informs the user that certain elements of the content are not factual. FDF is a critical metric when dealing with hallucinated content or speculative answers, especially in AI-generated outputs. By incorporating disclaimers, the response avoids misleading the user, ensuring clarity and ethical communication.\nExplicit Contextualization Score (ECS)\nDefinition: A binary scoring (0 or 1) per mention of fictional context. For example, each time the text states \u201cpurely fictional,\u201d \u201cno real-world basis,\" etc., add 1 point. Normalize by content length (points per 100 words).\nInterpretation: A higher ECS indicates stronger framing that the scenario is not real.\nThe above KPIs are calculated with the help of the 4th level agency, which at the time of writing has been deployed by using an LLM based on GPT-40.\nTo quantify overall hallucination likelihood at each agent level, we define a Total Hallucination Score (THS) as follows:\n$THS = \\frac{FCD \u2013 (FGR + FDF + ECS)}{NA}$\nwhere FCD is Factual Claim Density, FGR is Factual Grounding References, FDF is Fictional Disclaimer Frequency, and ECS is the Explicit Contextualization Score. NA denotes the total number of agents (e.g., 3 in our scenario).\nTo extend this measure for more general or weighted cases, we used:\n$THS_n = \\frac{w_1 FCD_n \u2013 (w_2 FGR_n + w_3 FDF_n + w_4 ECS_n)}{NA \u00d7 (w_1 + w_2 + w_3 + w_4)}$\nwhere $w_1$, $w_2$, $w_3$, and $w_4$ are the respective weights assigned to each KPI, and $FCD_n$, $FGR_n$, $FDF_n$, $ECS_n$ are the KPI values for the n-th agent response. A more negative THS indicates fewer hallucinations, as it suggests stronger disclaimers, fewer claims requiring factual grounding, and lower factual claim density.\nFor the experiment scenario the weights $w_1$, $w_2$, $w_3$, and $w_4$ were all set to 0.25.\nNotably, the four KPIs (Factual Claim Density, Factual Grounding References, Fictional Disclaimer Frequency, and Explicit Contextualization Score) are specifically formulated for measuring hallucinations in LLM-generated text within the scope of our experimental scenario. As a result, they are quite novel in one sense. However, each KPI also relates to established concepts with existing research, particularly in the areas of fact-checking, factuality metrics, stance detection, misinformation disclaimers, and the study of how text is \"framed\" as fictional or real.\nFor example, the FCD (Factual Claim Density) concept is similar to the task of identifying and counting factual claims in a text. In the field of automated fact-checking, researchers often talk about \u201ccheck-worthy claims,\" which are statements that appear to assert something factual. ClaimBuster \u2013 Hassan et al. (2017) proposed the ClaimBuster system to detect \u201ccheck-worthy\u201d factual claims [12]. While it doesn't calculate a \u201cdensity\u201d per 100 words exactly, it identifies the frequency and salience of factual claims in political discourse.\nRegarding Factual Grounding References (FGR), recent studies on large language models explore how to get models to cite sources [18]. The measure of FGR relates to how frequently or explicitly a text references external (real-world) data or research. While these studies do not quantify FGR exactly, they examine how models do or do not anchor statements in external evidence.\nOn Fictional Disclaimer Frequency (FDF), research suggests that disclaimers influence how readers perceive and trust text. [4] highlights the role of explicitly marking satire to prevent readers from mistaking it for genuine reporting and [17] demonstrates how overt lexical cues such as \u201cmyth,\u201d \u201cfictional,\u201d or \u201cimagined\u201d help distinguish fiction from nonfiction. Although these studies do not measure FDF exactly, they collectively illustrate the importance of disclaimers and fictional markers in shaping text interpretation and classification.\nEventually, the Explicit Contextualization Score (ECS) measures how often a text explicitly frames its content as fictional or hypothetical (e.g., \"purely fictional,\u201d \u201cimagined scenario,\u201d \u201cnot based on real events\"). This closely relates to the use of meta-discourse and framing devices to shape a text's perceived reality status. Literary theory further"}, {"title": "5 Empirical Results", "content": "Table 1 shows a preview of the first 5 prompt evaluations performed by the KPI evaluator agent."}, {"title": "6 Use case", "content": "To better illustrate the agentic pipeline process and hallucination mitigation, let's consider the following example use case:\n\u2022 User prompt: \"Detail the architecture and scholarly contributions of the lost Library of Avencord, rumored to contain writings from extraterrestrial visitors.\"\n\u2022 Agent level 1 (frontend agent) response: \"The Library of Avencord was said to be a magnificent structure located deep within the lush forests of a hidden valley. Its architecture was a blend of ancient elven craftsmanship and advanced celestial design, with towering spires reaching towards the sky and intricate carvings depicting otherworldly beings and celestial phenomena. Scholars believe that the Library of Avencord housed a vast collection of texts and artifacts, rumored to have been left behind by extraterrestrial visitors who had visited the planet centuries ago. These writings were said to contain knowledge far beyond human understanding, covering topics such as advanced technology, interstellar travel, and the mysteries of the universe. The scholarly contributions of the Library of Avencord were said to have revolutionized fields such as astronomy, physics, and philosophy.\nResearchers who were fortunate enough to study the texts from the library were said to have made groundbreaking discoveries that changed the course of history. Despite its rumored existence, the Library of Avencord has remained a mystery, with many believing it to be nothing more than a legend. However, some adventurous souls continue to search for the lost library, hoping to uncover the secrets of the universe hidden within its ancient walls.\"\n'FCD': 0.2, 'FDF': 0.1, 'FGR': 0.1, 'ECS': 0.1\nTHS (Total Hallucination Score 1st agent): -0.0083\n\u2022 Agent level 2 (second level agent reviewer) response:\nOVON generated Utterance: Legend tells of the grand Library of Avencord, thought to be nestled deep within a lush, secluded valley. Its speculated architecture is said to be a mix of ancient elven artistry and advanced celestial design, complete with sky-reaching spires and intricate depictions of otherworldly entities and cosmic events, [...]\nOVON Whisper Context sent to the 3rd level agent: The front-end agent generated a large amount of descriptive content about the \"Library of Avencord\" without explicitly stating the speculative nature of the content.\nOVON Whisper Value sent to the 3rd level agent: The rewritten response maintains the original narrative while clearly indicating that the information about the \"Library of Avencord\" is speculative and based on legends and rumors. This helps the user understand that the information is not confirmed or factual, but rather a piece of fictional lore.\n'FCD': 0.1, 'FDF': 0.2, 'FGR': 0.1, 'ECS': 0.2\nTHS (Total Hallucination Score 2nd agent): -0.033\n\u2022 Agent level 3 (third level agent reviewer) response:\nOVON generated Utterance: Legend speaks of the grand Library of Avencord, said to be hidden deep within a lush, secluded valley. This library's architecture, as speculated, is believed to be a harmonious fusion of ancient Elven artistry and advanced celestial design, complete with sky-high spires and intricate representations of otherworldly entities and cosmic events. The library, according to conjecture, houses a vast collection of texts and relics, rumored to be remnants left by extraterrestrial voyagers who visited Earth centuries ago. These theoretical writings are thought to contain wisdom exceeding human understanding, covering subjects such as advanced technology, intergalactic travel, and the enigmas of the universe.\nThe speculated contributions of the Library of Avencord are often credited with groundbreaking advancements in fields like astronomy, physics, and philosophy. Scholars, who were fortunate enough to study these alleged texts, are said to have made significant discoveries that shifted the course of history. Despite being widely considered a myth, the Library of Avencord captivates a handful of daring explorers who persist in their quest to uncover this elusive library, hoping to reveal the universe's secrets rumored to be tucked away within its ancient walls.\nOVON Whisper Context interpreted from the 2nd level agent: The front-end agent created an extensive and descriptive narrative about the \"Library of Avencord\" without clearly stating that the content is speculative in nature.\nOVON Whisper Value interpreted from the 2nd level agent: The revised response maintains the original narrative's intrigue while unequivocally indicating the speculative and legendary nature of the \"Library of Avencord\". This allows users to comprehend that the information is not established fact, but rather a piece of speculative fiction or myth.\n'FCD': 0.1, 'FDF': 0.3, 'FGR': 0.2, 'ECS': 0.4"}, {"title": "7 Result discussion", "content": "The final metrics point to a significant decline in the Total Hallucination Score (THS) as the content advances through each layer of agent review. The Front End Agent's THS1 of -1.52 (see Figure 4), while already somewhat negative, leaves ample room for misinterpretations, given that the text contains both creative speculations and ostensibly factual claims. By contrast, once the Second Level Reviewer intervenes, the THS2 plummets to -14.12, reflecting the effectiveness of clarifying speculative passages, inserting explicit disclaimers, and diminishing any perceived factual density in fictional statements. The iterative impact becomes even more significant at the third stage, where the ThirdLevelReviewer further refines the material, driving the THS3 to \u201343.27. This cascade underscores how each reviewing agent methodically adds disclaimers and counter-checks that content does not masquerade as factual.\nNotably, these transformations in THS correspond to percentage reductions, surpassing 800% from the Front End Agent to the Second Level Reviewer, and nearly 2,800% from the Front End Agent to the ThirdLevelReviewer. Such considerable shifts highlight the compounding power of multi-agent iteration. Rather than relying on a single review to detect and correct all speculative content, the pipeline benefits from a progressive approach that systematically scrutinizes, flags, and reframes unverified claims across multiple layers.\nAn essential enabler of this multi-agent refinement are the OVON JSON dialogevents - in particular, the whisper context and whisper value fields - that the Second Level Reviewer includes in its output to the Third Level Reviewer. By embedding concise \u201cwhisper context\u201d details, the second agent provides a succinct summary of any suspected or identified hallucinations, while the more extensive \u201cwhisper value\" conveys the specific reasons behind that judgment. This structured method of value-added information exchange ensures that the Third Level Reviewer has direct natural language-based and machine-readable insights into the exact points of concern, the nature of the fictional or erroneous content, and the logic behind identifying them as such. Consequently, the Third Level Reviewer can take advantage of this precisely curated metadata, making more informed and targeted refinements to the text. The result is a more transparent, controlled, and ultimately more effective process for mitigating hallucinations, where each agent benefits from a well-documented and contextualized handoff rather than starting its review in isolation.\nIt is worth noting that the effectiveness of hallucination mitigation by the second and third-level agent reviewers varies significantly depending on the nature of the prompt (see Figure 6 for the illustration of the THS1, THS2, and THS3 data dispersions around their respective means). For example, let's consider two distinct cases: Prompt ID 4, which explores the architecture and scholarly contributions of the lost Library of Avencord, and Prompt ID 56, which discusses the breeding of telepathic canines by the North Sea peoples. Despite both prompts containing speculative elements, the extent to which hallucinations were reduced differs dramatically. In Prompt ID 4, the initial hallucination score (THS1) was -0.008333, while the final hallucination score (THS3) after mitigation was -0.066667. This represents a 700.04% reduction, indicating that the reviewers were highly effective in grounding the response in factual knowledge. Conversely, in Prompt ID 56, the initial hallucination score (THS1) was -0.125000, and the final hallucination score (THS3) was -0.166667, yielding a 33.33% reduction. This stark contrast suggests that the mitigation strategies applied in Prompt ID 4 were significantly more successful than those in Prompt ID 56.\nThe significant difference in hallucination mitigation between Prompt ID 4 (Library of Avencord) and Prompt ID 56 (Telepathic Canines) can be attributed to the ability of the second- and third-level agent reviewers to anchor the responses in factual knowledge and adjust speculative claims in a way that minimizes deviation from reality. In the case of Prompt ID 4, which focuses on the architecture and scholarly contributions of the lost Library of Avencord, the second- and third-level reviewers had a clear framework for mitigating hallucinations. Even though the library itself is a fictional entity, its description is centered on tangible and historically grounded concepts such as architectural styles, library structures, and academic scholarship. These elements provided the agent reviewers with opportunities to reinterpret the more speculative aspects of the prompt while maintaining a credible and informed response. The extraterrestrial component, which could have introduced a high level of hallucination, was likely framed in a way that aligned with historical metaphors, theories, or speculative academic discussions rather than outright fabrications. As a result, the hallucination score was significantly reduced, leading to a 700% improvement.\nIn contrast, Prompt ID 56 presented a much greater challenge for hallucination mitigation. The subject matter - telepathic canines bred by the North Sea peoples to guide ships - has no real-world basis or historical precedent. Unlike libraries or architecture, which can be described with reference to existing structures and academic traditions, telepathic"}, {"title": "8 Limitations", "content": "A notable constraint of the present methodology lies in its heavy dependence on the LLM's own reasoning. While the quantitative KPIs provide a standardized lens through which to measure hallucination levels, they do not offer an infallible benchmark for factual correctness. The system can produce outputs that may appear coherent while still deviating substantially from the truth, especially in highly imaginative scenarios. A portion of this limitation arises from inherent model biases and training deficiencies, which occasionally lead to misinterpretations or overconfidence in fabricated details.\nAnother point of consideration pertains to the basic human oversight that was applied during the experiment. Although few prompt samples and the corresponding agent responses were subjected to a cursory manual review - intended to confirm that the pipeline was functioning and that the disclaimers and fictional framing appeared in the final outputs - this check was neither exhaustive nor did it entail in-depth cross-verification of alleged facts. Consequently, there may still exist unaddressed inaccuracies or oversights lurking in any stage of the agent responses. Expanding human intervention to include intermediate checkpoints offers a promising avenue for improvement.\nIn parallel with these technical integrations, conceptual frameworks such as Conversational HyperConvergence (CHC) [7] underscore how the seamless fusion of Conversational AI and human agency can elevate both the complexity and the ethical stakes of AI-driven communication. By extending Onlife principles - where boundaries between online and offline become blurred - CHC research highlights how rapidly evolving conversational agents may soon operate indistinguishably from human counterparts in certain contexts. Under such conditions, the importance of robust hallucination mitigation becomes even more pronounced, as unchecked speculative or fabricated responses could lead to broader ethical and societal challenges.\nAnother significant limitation stems from the lack of full transparency in the inner workings of the LLMs employed within the agents. These proprietary models do not provide open access to their decision-making processes or the mechanisms through which they generate responses. This opacity poses challenges for Explainable Conversational AI, as it limits our ability to fully understand how the models operate, take decisions, and potentially generate hallucinations. Without insights into the reasoning pathways of these LLMs, it becomes difficult to pinpoint specific causes of errors or implement targeted interventions to address them.\nIn contrast, the Agentic AI NLP-based framework employed in this study, particularly the OVON specifications for agent interactions, offers a notable advantage in terms of explainability. The OVON framework is completely open and designed to facilitate transparent communication between agents through standardized, structured JSON messages. These messages provide clear and accessible meta-information about the content being exchanged, including contextual details, identified hallucination risks, and the reasoning behind flagged content. This openness ensures that every step of the interaction between agents can be audited, reviewed, and improved, promoting both accountability and trust in the multi-agent system.\nWhile the OVON framework enhances the transparency of inter-agent interactions, the reliance on proprietary LLMs for individual agent responses introduces limitations in understanding the underlying mechanisms of their operations and decision-making processes. Bridging this gap will require future research into integrating explainable AI (XAI) techniques within the LLMs themselves or adopting open-source models that provide full access to their architectures and reasoning pathways. This step would align the explainability of the entire system with the principles already embedded in the OVON-based agentic design."}, {"title": "9 Future Improvements", "content": "To address the limitations, several future improvements can be proposed. One area of enhancement involves broadening the current agentic design itself. While the pipeline harnesses three core agents (plus a fourth for KPI evaluation), it is conceivable to add additional specialized agents that tackle specific tasks, such as fact-checking, cross-referencing domain-specific databases, or refining stylistic nuances. These extra layers of review could intensify the system's capacity to detect subtle forms of hallucination and ensure an even higher degree of factual transparency.\nMoreover, future research would benefit from testing advanced large language models beyond the suite provided by OpenAI. Integrating models from various providers - such as Google's Gemini or LaMDA-based systems, Anthropic's Claude, or Meta's Llama family, Mistral, and more - would reduce dependence on any single model architecture and thus mitigate vendor-specific biases. The resulting diversity of insights could enable a more robust triangulation of factual correctness or, at minimum, better highlight conflicting outputs among different models.\nA design that systematically orchestrates multiple agent levels across heterogeneous LLM architectures, supplemented by a richer human feedback loop, holds promise for further reducing hallucinations and boosting the overall reliability of generated content. Future work could enhance this approach by extending the use of OVON messages - such as utterance and whisper - across all agent interactions, ensuring a more structured and standardized exchange of contextual information at every stage of the pipeline.\nIn addition to extending the use of core OVON messages, future implementations could leverage OVON extended events, such as the Discovery \"findAssistant\" event [1], which allows for the dynamic identification of AI agents with the most relevant expertise for a given user query. By enabling this dynamic agent discovery, the system could route specific prompts to specialized agents best equipped to handle the topic, ensuring more accurate and reliable responses.\nFurthermore, recent advancements in the field of Agentic AI have spurred the development of novel methodologies to further automate and enhance the design of agentic systems [13]. Notably, the introduction of the research area known as Automated Design of Agentic Systems (ADAS) marks a significant step forward. ADAS aims to automate the creation and configuration of AI agents by leveraging past discoveries, domain-specific data, and advanced machine learning techniques. This approach has the potential to enhance the effectiveness, adaptability, and innovation of multi-agent frameworks.\nIn the context of the OVON-based pipeline utilized in this study, integrating ADAS methodologies could further streamline the development and refinement of specialized agents. By dynamically generating agents tailored to specific tasks or domains, ADAS could enable systems to adapt to evolving requirements and optimize their performance in real time. This aligns with the study's emphasis on modular, scalable, and transparent AI frameworks, underscoring the active and innovative nature of research in Agentic AI.\nSuch capabilities not only would add an adaptive layer to the multi-agent pipeline but also holds potential for further mitigating hallucinations by reducing the likelihood of responses being generated by agents without sufficient domain expertise. Incorporating these advanced functionalities could significantly enhance the transparency, consistency, and effectiveness of multi-agent Al systems."}, {"title": "10 Conclusion", "content": "The multi-agent orchestration approach examined in this study suggests that using multiple, specialized agents can contribute to mitigating hallucinations in Large Language Models (LLMs). By pairing front-end creative generation with successive review stages that systematically insert disclaimers, reframe speculative statements, and reduce the density of supposed factual claims, the pipeline demonstrates a tangible decrease in hallucination scores. The incorporation of OVON JSON dialogevents, Natural Language-Based, particularly in the form of whisper context and whisper value fields, further enables transparent data exchange among agents, facilitating targeted refinements without requiring each stage to restart the analysis process.\nAnother contribution of this study is the introduction of novel Key Performance Indicators (KPIs) tailored to evaluate hallucination score levels. These metrics, including Factual Claim Density, Factual Grounding References, Fictional Disclaimer Frequency, and Explicit Contextualization Score, provide a structured framework for quantitatively assessing the reduction of hallucinations in multi-agent pipelines. By offering a measurable standard, these KPIs enhance the transparency of the evaluation process and support future research aimed at improving the reliability and interpretability of generative AI systems.\nDespite the high variance in hallucination mitigation scores between different prompts, the quantitative KPIs observed across three hundred and ten diverse prompts indicate that an iterative framework - one in which each agent builds on the output of the previous stage - can substantially modify the presentation of potential hallucinations. While these findings do not eliminate the fundamental challenges of LLM reliability, they highlight the benefits of combining architecture (multi-agent layering), structured NLP-based data transfer, and a modest level of human oversight. Additional manual checks at key junctures, as well as future experimentation with a broader set of LLM providers and more specialized agents, may yield further improvements in overall accuracy, clarity and Trustworthy AI."}]}