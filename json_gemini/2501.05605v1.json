{"title": "Advancing Personalized Learning Analysis via an Innovative Domain Knowledge Informed Attention-based Knowledge Tracing Method", "authors": ["Shubham Kose", "Jin Wei-Kocsis"], "abstract": "Emerging Knowledge Tracing (KT) models, particularly deep learning and attention-based Knowledge Tracing, have shown great potential in realizing personalized learning analysis via prediction of students' future performance based on their past interactions. The existing methods mainly focus on immediate past interactions or individual concepts without accounting for dependencies between knowledge concept, referred as knowledge concept routes, that can be critical to advance the understanding the students' learning outcomes. To address this, in this paper, we propose an innovative attention-based method by effectively incorporating the domain knowledge of knowledge concept routes in the given curriculum. Additionally, we leverage XES3G5M dataset, a benchmark dataset with rich auxiliary information for knowledge concept routes, to evaluate and compare the performance of our proposed method to the seven State-of-the-art (SOTA) deep learning models.", "sections": [{"title": "Introduction", "content": "In recent years, due to the growth and popularity of online learning platforms such as Massive Open Online Courses (MOOCs) (Wulf et al. 2014), Intelligent Tutoring Systems (ITS) (Anderson, Boyle, and Reiser 1985), Khan Academy, and Coursera, online learning has becomes one popular learning method. To support online learning, intelligent tutoring systems have merged as effective techniques, which focus on analyzing students' knowledge states and predicting their performance on future related tasks.\nWhile traditional approaches, such as Bayesian Knowledge Tracing (BKT) (Corbett and Anderson 1994) and Performance Factor Analysis (PFA) (Pavlik, Cen, and Koedinger 2009) have provided foundational frameworks, these methods face limitations in capturing complex learning patterns and relationships between knowledge concepts (KCs) (Shen et al. 2024). Recent advances in deep learning have led to significant improvements in KT and resulted in deep learning-based KT methods like Deep Knowledge Tracing (DKT) (Piech et al. 2015) and Memory-Aware Knowledge Tracing (DKVMN) (Zhang et al. 2017; Abdelrahman and Wang 2019). These models leverage deep learning techniques, such as Recurrent Neural Networks (RNNs), to capture temporal dependencies in student interactions. However, they have been criticized for their lack of interpretability and tendency to overfit sparse data (Gervet et al. 2020). In recent years, the introduction of transformers with attention mechanisms (Vaswani 2017) has revolutionized deep learning across multiple domains, including natural language processing and computer vision. These models excel at capturing long-range dependencies and contextual relationships within data, making them particularly well-suited for tasks involving sequential information. This innovation has led to breakthroughs in large language models (LLMs) such as GPT-x (Brown 2020) and Llama (Touvron et al. 2023), which excel at capturing long-range dependencies and contextual relationships within data. Inspired by this success, various research efforts have been conducted to develop attention-based KT methods that aim to incorporate attention mechanisms into KT models to enhance their ability to handle sparse datasets and long-term dependencies between KCs. The established methods include Self-Attentive Knowledge Tracing (SAKT) (Pandey and Karypis 2019), SAINT (Separated Self-Attentive Neural Knowledge Tracing) (Choi et al. 2020) and Attentive Knowledge Tracing (AKT) (Ghosh, Heffernan, and Lan 2020). These methods enhanced the capability of traditional KT methods by focusing on ability to focus on relevant past interactions, while downweighing irrelevant ones. However, existing studies show that these attention based KT models primarily focus on immediate past interactions or treating questions independent, posing challenges in capturing hierarchical dependencies between knowledge concepts (Yudelson, Koedinger, and Gordon 2013). This limitation can result in suboptimal predictions, as learning process of students normally involves building upon prior knowledge in a sequential and hierarchical manner (Woolf 2010). Our work aims to address this gap by introducing a novel approach that incorporates domain knowledge-informed hierarchical knowledge concept routes of the given curriculum into the attention mechanism of KT models. By doing so, our approach is able to enhance the capability of capturing long-term dependencies between knowledge concepts and thus efficiently advance the performance. Our approach aims to improve the model's ability to capture long-term dependencies between knowledge concepts, leading to improved predictive performance compared to existing state-of-the-art models. To achieve this, we propose an innovative domain knowledge-informed Learning Relevance Matrix that tracks relationships between questions based on their shared concept paths. This matrix is designed to mask attention scores for unrelated questions, ensuring the model does not attend to questions involving concepts that will not aid the student in answering the current question. The main contributions of our work include:\n\u2022 A novel domain knowledge-informed Learning Relevance Matrix mechanism for tracking relationships between questions based on shared knowledge concept routes.\n\u2022 An improved attention mechanism that masks attention scores for unrelated questions\n\u2022 Extensive experimental validation demonstrating significant improvements in AUC and accuracy over SOTA KT methods.\nOur experiments on the benchmark dataset demonstrate that the proposed approach effectively captures hierarchical dependencies between knowledge concepts, offering improved interpretability of the underlying learning process and a promising direction for future research in personalized learning systems."}, {"title": "Literature Review", "content": "KT has evolved significantly over the years, with earliest model like Bayesian Knowledge Tracing (BKT) (Corbett and Anderson 1994), that relied on probabilistic frameworks like Hidden Markov Models (HMMs) to update the probability of mastery for each KC for modelling student knowledge as a binary variable representing mastery or non-mastery of a concept. BKT assumes that once a student masters a concept, they do not forget it, which limits its ability to model real-world learning scenarios where forgetting is common (Baker, Corbett, and Aleven 2008). Logistic models such as Learning Factor Analysis (LFA) (Cen, Koedinger, and Junker 2006) and Performance Factor Analysis (PFA) (Pavlik, Cen, and Koedinger 2009) which uses logistic regression to evaluate the effects of instructional factors and learning opportunities on student performance. However, these models face challenges in handling complex sequential learning dependencies and require substantial historical data.\nThe introduction of deep learning models, such as Deep Knowledge Tracing (DKT), marked a significant shift by leveraging Recurrent Neural Networks (RNNs) specifically Long Short-Term Memory (LSTM) networks (Hochreiter 1997) to capture temporal dependencies in student interactions. Despite its huge success, DKT's lack of interpretability and inability to capture concept-specific mastery (Yeung and Yeung 2018), led to various extensions. For example, Memory-Aware Knowledge Tracing (DKVMN) introduced external memory structures to track mastery levels for individual KCs, improving both interpretability and performance (Zhang et al. 2017). Recent advancements have also explored hierarchical structures within KT models. Graph-based Knowledge Tracing (GKT) (Nakagawa, Iwasawa, and Matsuo 2019) uses Graph Neural Networks (GNNs) to model relational dependencies between KCs, improving both prediction accuracy and interpretability. However, the complexity and computational demands of these models pose a challenge for practical deployment.\nAttention mechanisms have proven highly effective in addressing some of these issues. Self-Attentive Knowledge Tracing (SAKT) introduced a purely attention-based approach that focuses on relevant past interactions without relying on temporal order, improving performance on sparse datasets (Pandey and Karypis 2019). However, SAKT struggles with capturing long-term dependencies between distant interactions. To address this, models such as SAINT (Choi et al. 2020) and SAINT+ (Shin et al. 2021) incorporated deeper self-attentive layers and temporal features like elapsed time and lag time to enhance prediction accuracy by modeling the time dynamics of student interactions. To reduce the impact of noise and overfitting, ATKT (Guo et al. 2021) introduces an adversarial loss that helps model learn to capture more stable and reliable patterns in student responses and generate predictions that are less sensitive to small perturbations. Additionally, context-aware Attentive Knowledge Tracing (AKT) (Ghosh, Heffernan, and Lan 2020) integrates psychometric principles with attention mechanisms to prioritize recent activities while diminishing the influence of earlier events, and at the same time giving enough weightage to the earlier events that are relevant. AKT's use of Rasch model-based embeddings accounts for question difficulty variations, further enhancing interpretability (Rasch 1993). Furthermore, extraKT introduced by (Li et al. 2024) is effective where students' interaction sequences vary in length. It effectively models short-term forgetting behaviors and maintains robust predictive performance even with longer context windows. However, it requires careful tuning, and its scalability to longer sequences remains a challenge.\nDespite these advancements, existing KT models have limitation in fully capturing hierarchical dependencies between knowledge concepts. Most models focus on immediate past interactions or treat questions independently without considering how knowledge builds sequentially over time, which is highly required to provide actionable feedback and personalized learning experience to the student. This gap highlights the need for more sophisticated attention mechanisms that can track relationships between KCs across longer sequences. Incorporating auxiliary information into KT models has also shown promise. For instance, Relation-aware Self-Attention for Knowledge Tracing (RKT) (Pandey and Srivastava 2020) captures relational dependencies between exercises using self-attention mechanisms, while Exercise-Aware Knowledge Tracing (EKT) (Liu et al. 2019) embeds both student interaction data and exercise features to improve performance predictions.\nIn summary, while significant progress has been made in KT through deep learning and attention-based models, challenges remain in effectively capturing long-term dependencies and hierarchical relationships between KCs. This paper builds upon these advancements and address the remaining challenges of the SOTA models by innovatively incorporating domain knowledge-informed knowledge content routes in KT systems to further improve the performance of personalized learning analysis."}, {"title": "Methodology", "content": "The modeling of the KT problem is stated as:\nFor a particular learner at time step t, we denote their interaction with a question as a tuple $(q_t, c_t, r_t)$, where $q_t \\in N^+$ is the question index, $c_t \\in N^+$ is the concept index, and $r_t \\in \\{0,1\\}$ is the binary-valued response (1 for correct, 0 for incorrect). Given a learner's past history up to time t - 1 as $\\{(q_1, c_1, r_1), ..., (q_{t-1}, c_{t-1}, r_{t-1})\\}$, our goal is to predict their response $r_t$ to question $q_t$ on concept $c_t$ at the current time step t."}, {"title": "Attention Mechanism", "content": "The attention-based models operate by computing attention scores between the current exercise $e_{t+1}$ and all previous exercises $\\{e_1, e_2, ..., e_t\\}$. The attention mechanism assigns weights based on the relevance of each past exercise to the current one. Mathematically, the self-attention score for each pair of interactions (i,j)in the sequence is computed as (Vaswani 2017):\n$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}})V$\nwhere $Q = W_Q e_{t+1}$ is the query vector for the current exercise, $K = W_k e_i$ are the key vectors for past exercises, $V = W_v e_i$ are the value vectors for past exercises, $W_Q, W_K$, and $W_v$ are the learned projection matrices, d is the dimensionality of the latent space."}, {"title": "Attention-based KT Method", "content": "The attention-KT method (Ghosh, Heffernan, and Lan 2020) consists of four main components described below. The real-valued embedding vectors $x_t \\in R^D$ and $y_t \\in R^D$ represent each question and each question-response pair ($q_t, r_t)$, respectively.\n1. The Question Encoder outputs context-aware question embeddings:\n$x_t = f_{enc1}(x_1, ..., x_t)$\n2. The Knowledge Encoder outputs context-aware knowledge embeddings:\n$\\hat{y}_{t-1} = f_{enc2}(y_1, ..., y_{t-1})$\n3. The Knowledge Retriever computes the current knowledge state:\n$h_t = f_{kr}(x_1,..., x_t, \\hat{y}_1, ..., \\hat{y}_{t-1})$\n4. The Response Prediction network outputs the probability of a correct response:\n$\\hat{r}_t = \\sigma(f_{pred}([h_t; x_t]))$\nwhere o is the sigmoid function and $f_{pred}$ is a fully-connected network."}, {"title": "Monotonic Attention Mechanism", "content": "To account for temporal dynamics, the attention-based KT method uses a modified monotonic attention mechanism (Ghosh, Heffernan, and Lan 2020) that introduces an exponential decay term to downweight distant past interactions. The the scaled dot-product attention values $a_{t,\\tau}$, are then calculated as:\n$a_{\\tau, t} = \\frac{exp(s_{\\tau, t})}{\\sum_{\\tau'=1}^t exp(s_{\\tau', t})}$\nwhere:\n$s_{\\tau, t} = \\frac{exp(- \\theta \\cdot d(t, \\tau)) \\cdot Q_t^T K_{\\tau}}{\\sqrt{d_k}}$\n$\\theta > 0$ is a learnable decay rate parameter and $d(t, \\tau)$ is a temporal distance measure between time steps t and $\\tau$.\nThis ensures that more recent interactions have higher influence on predicting future performance while still considering concept similarity."}, {"title": "Context-Aware Representations", "content": "The context-aware distance measure between the different question pair is defined as:\n$d(t, \\tau) = |t - \\tau| \\cdot \\prod_{t'=\\tau+1}^t \\gamma_{t,t'}$\n$\\gamma_{t,t'} = \\frac{exp(k_t)}{exp(k_t) + \\sum_{k \\neq t} exp(k_k)}$"}, {"title": "Rasch Model-Based Embeddings", "content": "The attention-based KT method uses Rasch model-based embeddings (Ghosh, Heffernan, and Lan 2020) as the raw embeddings for questions and responses:\nQuestion embedding:\n$x_t = c_{c_t} + \\mu_{q_t} \\cdot d_{c_t}$"}, {"title": "Domain Knowledge-informed Knowledge Concept Route-based Self-attention Mechanism", "content": "The essential idea of our proposed domain-knowledge-informed knowledge concept route-based self-attention mechanism is to track relationships between KCs across sequences of interactions and use this information to modify attention scores within the model. Specifically, we introduce a domain knowledge-informed Learning Relevance Matrix, $(F_{i,j})$, that identifies whether two questions in a sequence share the same concept route. In other words, the proposed Learning Relevance Matrix aims to characterize, whether they are part of the same hierarchical knowledge structure. To achieve this, this matrix is designed to capture the relationship between questions in an interaction sequence based on their full knowledge concept routes, ensuring that the attention mechanism focuses only on relevant related questions. The detailed workflow of our proposed attention-based KT method is presented in Algorithm 1. The main procedure of formulating and applying the proposed Learning Relevance Matrix can be summarized in three steps, which are stated below:\nStep 1: Formulating the Learning Relevance Matrix\nLet $\\{(q_1, c_1), (q_2, c_2), ..., (q_t, c_t)\\}$ be a student interaction sequence, where $q_i$ is the question and $c_i$ is the knowledge concept at time i. Then the knowledge concept route can be formulated by the hierarchical structure of progression of knowledge concepts to the current concept $c_i$ that the student encounters, achieved according to the educational domain knowledge of the given curriculum. Now for each pair of questions, $q_i$ and $q_j$, where $i < j$, we compare their concept routes. If the concept route of $q_i$ share at least one common element with that of $q_j$, we set $F_{i,j} = 1$. Otherwise, $F_{i,j} = 0$. Therefore, we can formulate $F_{i,j}$ as follows:\n$F_{ij} = \\begin{cases}\n1 & \\text{if $q_i$ and $q_j$ share at least one common element,}\\\\\n0 & \\text{otherwise}\n\\end{cases}$\nStep 2: Attention Score Masking with Learning Relevance Matrix\nTo incorporate the knowledge concept routes into the attention mechanism, we will modify the attention score computation. Let $a_{\\tau,t}$ be the attention score between the question at time t and a past question at time $\\tau$, the original monotonic attention score can be computed as:\n$a_{\\tau,t} = \\text{Softmax}(\\frac{q_t^T k_{\\tau}}{\\sqrt{d_k}})$\nThe attention score between two questions is then modified by applying this Learning Relevance Matrix as a mask to the attention scores, which can be represented as:"}, {"title": null, "content": "$\\alpha_{\\tau,t}^{masked} = \\alpha_{\\tau,t} \\times F_{t,\\tau}$\nBy doing so, we are able to ensure that only scores from related questions, which is determined by the full knowledge concept route, are considered in the final attention output. By incorporating the Learning Relevance Matrix into the self-attention mechanism of our attention-based KT method, the context-aware embedding can be represented as:"}, {"title": null, "content": "$x_t = \\sum_{\\tau < t} \\alpha_{\\tau,t}^{masked} v_{\\tau}$\nAdditionally, the knowledge state update can be formulated as:\n$h_t = f_{retriever}(x_1,...,x_{t-1})$\nStep 3: Response Prediction\nThe masked attention scores are then applied in the knowledge retriever component of our attention-based KT method. The predicted probability $r_t$ that a student answers the current question $q_t$ correctly can be computed using a feed-forward neural network that is formulated as:\n$\\hat{r_t} = \\sigma(W_o \\cdot [h_t; x_t])$"}, {"title": null, "content": "where $h_t$ is the retrieved knowledge state, $x_t$ is the question embedding, and $W_o$ are the learnable parameters of the output layer. Furthermore, the model's parameters are trained by minimizing a binary cross-entropy loss function that is formulated as:\n$L = - \\sum_{i=1}^N \\sum_{t=1}^T (r_i \\cdot \\log(\\hat{r_i}) + (1 - r_i) \\cdot \\log(1 - \\hat{r_i}))$\nwhere $r_i \\in \\{0,1\\}$ refers to the ground truth of whether the student answers the question correctly or incorrectly, $\\hat{r_t} \\in [0, 1]$ is the predicted probability of whether student i answers the question correctly at time step t."}, {"title": "Performance Evaluations", "content": "To evaluate the performance of our proposed method, we conduct experiments by using XES3G5M dataset (Liu et al. 2024), a comprehensive KT benchmark dataset with auxiliary information. We will compare the proposed model against various SOTA KT models, including DKT (Piech et al. 2015), DKVMN (Zhang et al. 2017), Adversarial training based Knowledge Tracing (ATKT) (Guo et al. 2021), SAKT (Pandey and Karypis 2019), SAINT (Self AttentIve Neural Knowledge Tracing) (Choi et al. 2020), extraKT (Extended Knowledge Tracing) (Li et al. 2024) and standard AKT with Rasch Model Embeddings (Ghosh, Heffernan, and Lan 2020). We use 5-fold cross-validation to assess model performance and tune hyperparameters using the validation set.\nAll models are trained using the Adam optimizer with a learning rate of 0.0001. We use a batch size of 64 and trained each model for 200 epochs. Early stopping was applied based on validation AUC to prevent overfitting. The embedding dimension D is set to 256 for all attention-based models (Liu et al. 2022)."}, {"title": "XES3G5M Dataset", "content": "The XES3G5M dataset is a large-scale benchmark dataset for KT, collected from a real-world K-12 online math platform in China. It includes 7,652 math questions, categorized as 6,142 fill-in-the-blank and 1,510 multiple-choice, each with standardized answers, detailed solutions, and textual content for complexity analysis. It has 865 Knowledge Concepts (KCs) organized in a hierarchical tree structure, enabling modeling of prerequisite relationships and dependencies. It has a total of 5,549,635 interaction records from 18,066 students, capturing question IDs, KC IDs, binary response correctness, and timestamps. Each question includes its KC routes, which are the paths from root to leaf in the KC hierarchy, providing insights into conceptual dependencies and progression. Additionally, personal identifiers are encrypted to ensure anonymity, preserving the integrity of the dataset while protecting user privacy."}, {"title": "Dataset Preprocessing", "content": "Data filtering is applied to the dataset to remove interactions that had missing student ID or any information from the 4-tuple interaction representation (question, knowledge components, response, timestamp). Students with less than 3 interactions in their sequence are filtered out. Additionally, 20% of the interaction sequences are used as test set and the remaining 80% randomly split into 5 folds: 4 for training, 1 for validation. The original question-response sequences are expanded to knowledge component level by repeating responses for questions associated with multiple knowledge components and then truncated the expanded sequences to maximum length of 200 interactions, while padding those sequences which are shorter with -1."}, {"title": "Evaluation Metrics", "content": "To evaluate the model's performance, we use the Area Under the Receiver Operating Characteristic Curve (AUC) as the primary metric as it measures the ability of the model to distinguish between correct and incorrect responses across different thresholds. We also use accuracy for measuring the percentage of correct predictions across all interactions."}, {"title": "Evaluation Results and Discussions", "content": "The experimental results, as summarized in Table 1, provide a comprehensive comparison between the performance of our proposed method and those of other seven SOTA KT methods by using the XES3G5M dataset. Our method significantly outperforms all baseline models in both AUC and accuracy.\nAs shown in Table 1, DKT achieves an AUC of 0.835 and an accuracy of 84.12%, which is lower compared with SAINT, ATKT, extraKT, AKT, and our proposed models. This is reasonable since this method lacks of ability to account for hierarchal and contextual information. DKVMN performs slightly less effectively compared to DKT, with an AUC of 0.828 and an accuracy of 83.88%. SAINT further enhances performance with an AUC of 0.851 and accuracy of 84.54%. ATKT improves over SAINT slightly with an AUC of 0.857 and an accuracy of 84.58%. Amongst these existing SOTA methods, extraKT and AKT achieve better performance. The good performance of AKT illustrates the effectiveness of using Rasch model-based embedding and monotonic attention mechanism, which are also applied in our proposed method. The evaluation result also validates that our proposed method significantly outperforms all the considered SOTA approaches, achieving an AUC of 0.975 and an accuracy of 92.93%. These results highlight the effectiveness of the proposed domain knowledge-informed knowledge concept route-based self-attention mechanism of our attention-based KT model. The substantial improvement in AUC demonstrates enhanced predictive capabilities and the higher accuracy suggests better alignment with ground truth student performance data."}, {"title": "Conclusions and Future Work", "content": "In this paper, we present our initial work on developing a new domain knowledge-informed attention-based KT method to effectively improve the effectiveness of predicting the learning progress of individual students. The core component of our proposed KT method is a domain knowledge concept route-based self-attention mechanism with knowledge-informed Learning Relevance Matrix, which leverages the educational domain knowledge of the given curriculum to enhance the interpretation of the correlation between the questions and provide better insights into the learning trajectories of the individual students. The simulation results validate the significant improvement in both AUC and accuracy on predicting students' performances compared with SOTA methods.\nFuture work will focus on further optimizing the model and exploring its applicability across diverse datasets and learning scenarios. Additionally, incorporating interpretability features will be essential to provide actionable insights for educators and learners."}]}