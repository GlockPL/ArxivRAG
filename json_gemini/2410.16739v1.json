{"title": "Corrected Soft Actor Critic for Continuous Control", "authors": ["Yanjun Chen", "Xinming Zhang", "Xianghui Wang", "Zhiqiang Xu", "Xiaoyu Shen", "Wei Zhang"], "abstract": "The Soft Actor-Critic (SAC) algorithm is known for its stability and high sample efficiency in deep reinforcement learning. However, the tanh transformation applied to sampled actions in SAC distorts the action distribution, hindering the selection of the most probable actions. This paper presents a novel action sampling method that directly identifies and selects the most probable actions within the transformed distribution, thereby addressing this issue. Extensive experiments on standard continuous control benchmarks demonstrate that the proposed method significantly enhances SAC's performance, resulting in faster convergence and higher cumulative rewards compared to the original algorithm.", "sections": [{"title": "Introduction", "content": "Reinforcement Learning (RL) has become a cornerstone of modern artificial intelligence, underpinning advancements in domains ranging from autonomous systems and robotics to strategic game playing. Among the plethora of RL algorithms, the Soft Actor-Critic (SAC) algorithm introduced by Haarnoja et al. Haarnoja et al. [2018] stands out due to its integration of policy gradient techniques with entropy maximization, which together promote efficient exploration and robust learning in complex environments. SAC has been widely adopted in diverse and challenging tasks, as demonstrated in various applications Kalashnikov et al. [2018].\nHowever, despite its broad adoption and proven success, the SAC algorithm's action sampling method, particularly the tanh transformation employed to constrain actions within bounded spaces, has not been sufficiently scrutinized. This transformation, while essential for ensuring that actions remain within valid bounds, introduces a significant non-linear distortion to the underlying Gaussian distribution from which actions are sampled Fujimoto et al. [2018]. This distortion can lead to a mismatch between the sampled actions and the most probable actions in the original distribution, a problem that becomes increasingly pronounced in high-dimensional action spaces.\nIn high-dimensional environments, even minor distortions can compound, resulting in substantial errors in action selection. Such errors are particularly detrimental in tasks that require precise action control, such as those involving intricate dynamic interactions or high-dimensional action spaces. The sampling errors induced by the tanh transformation can thus severely hinder the learning process, leading to suboptimal policy performance and slower convergence.\nTo address this critical issue, this paper proposes an optimized action sampling method specifically designed to mitigate the distortive effects of the tanh transformation. The proposed method involves explicitly computing the likelihood of actions within the transformed bounded space and selecting actions that maximize this likelihood. This approach aligns the sampling process more closely with the underlying Gaussian distribution, thereby enhancing the accuracy of action selection and improving the overall performance of the SAC algorithm.\nThe main contributions of this work are threefold:"}, {"title": "Related Work", "content": "This section provides a comprehensive review of the literature related to the Soft Actor-Critic (SAC) algorithm, action sampling methodologies in reinforcement learning (RL), advancements in RL, and the impact of nonlinear transformations in these contexts. The discussion highlights the challenges and innovations that shape the current landscape of RL, with a particular focus on the distortions introduced by nonlinear transformations like the tanh function."}, {"title": "Soft Actor-Critic (SAC)", "content": "The Soft Actor-Critic (SAC) algorithm, introduced by Haarnoja et al. Haarnoja et al. [2018], represents a significant advancement in reinforcement learning due to its unique integration of entropy maximization with off-policy learning. SAC optimizes a stochastic policy using a maximum entropy objective, encouraging exploration by promoting diverse actions, thereby effectively balancing exploration and exploitation. This balance is crucial for enhancing the algorithm's stability and sample efficiency, particularly in complex environments. SAC has been successfully applied across various domains, including robotics Levine et al. [2018], autonomous systems Kalashnikov et al. [2018], and gaming Espeholt et al. [2018], demonstrating its robustness and adaptability. However, despite these successes, SAC faces limitations in handling environments with high-dimensional action spaces, particularly due to the tanh transformation used to map actions to a bounded space. This transformation, while necessary, introduces significant distortions to the Gaussian action distribution, potentially leading to suboptimal action selection, as highlighted by Fujimoto et al. Fujimoto et al. [2018]. Further comparative studies, such as those by Schulman et al. Schulman et al. [2017], suggest that while SAC excels in stability, Proximal Policy Optimization (PPO) may offer superior"}, {"title": "Action Sampling Methods", "content": "Action sampling is a fundamental component of reinforcement learning algorithms, directly influencing the effectiveness of the learned policies. Several sampling techniques have been developed to enhance the accuracy and efficiency of action selection. Importance sampling Precup [2000], for instance, adjusts the probability distribution of actions to better reflect their expected value under a given policy, thus improving learning outcomes. Moreover, adaptive sampling methods, such as those proposed by Mnih et al. Mnih et al. [2016], dynamically adjust the sampling strategy based on the evolving policy, thereby improving convergence rates and overall performance. Another important strategy is the application of Thompson Sampling, which has shown promise in balancing exploration and exploitation by incorporating uncertainty in action selection Russo et al. [2018]. However, these methods typically assume that the action space is either unbounded or not subject to nonlinear transformations, an assumption challenged in high-dimensional environments where transformations like tanh are applied. In SAC, these distortions can lead to suboptimal action selection, particularly in complex environments. Studies by Fujimoto et al. Fujimoto et al. [2018] have explored these challenges, yet the need for methods that directly address these distortions remains. Our research fills this gap by proposing a refined action sampling technique that accurately models the transformed action distribution, ensuring optimal action selection within bounded spaces."}, {"title": "Nonlinear Transformations", "content": "Nonlinear transformations, such as the tanh and sigmoid functions, play a critical role in neural network-based reinforcement learning by ensuring that actions remain within a valid range. However, these transformations introduce significant challenges, particularly in high-dimensional action spaces. The primary issue arises from the distortion of the original action distribution when mapped through these nonlinear functions. For instance, the tanh function compresses the output range to [-1,1], leading to a non-uniform distribution that distorts the original Gaussian distribution typically assumed in many RL algorithms, as discussed by Bengio et al. Bengio et al. [1994] and further analyzed in the context of reinforcement learning by Dinh et al. Dinh et al. [2017]. The impact of these distortions is particularly pronounced in high-dimensional spaces, where even small deviations can lead to significant errors in policy optimization. For example, in the SAC algorithm, the tanh transformation can result in the underestimation of the probability density near the boundaries of the action space, leading to suboptimal action selection and slower convergence. Kingma et al. Kingma and Welling [2013] explored similar challenges in the context of variational autoencoders, where nonlinear transformations affect the latent space distribution, a concept that parallels the challenges faced in reinforcement learning. Recent research has proposed several approaches to mitigate these issues, including reparameterization of the action space Jang et al. [2016] and the application of regularization techniques Ioffe and Szegedy [2015] that counterbalance the distortive effects. These strategies have been particularly effective in stabilizing learning processes in the presence of nonlinearities, and our work extends these ideas by focusing specifically on the SAC algorithm. We propose a novel action sampling method that directly addresses the distributional distortions caused by the tanh transformation, ensuring that the selected actions are representative of the most probable outcomes, leading to improved policy performance and more stable learning, particularly in high-dimensional and complex environments."}, {"title": "Advancements in Reinforcement Learning", "content": "The field of reinforcement learning has seen substantial progress in improving algorithmic stability, sample efficiency, and overall performance. Notable advancements include curiosity-driven exploration strategies Pathak et al. [2017], which incentivize agents to explore novel states, and value function approximation techniques, such as those used in Deep Deterministic Policy Gradient (DDPG) Lillicrap et al. [2015], which provide more accurate estimates of expected returns. Moreover, hybrid methods that integrate model-free and model-based approaches, like those proposed by Nagabandi et al. Nagabandi et al. [2018], have demonstrated significant potential in enhancing both learning efficiency and policy robustness. Beyond these, the field is witnessing the emergence of"}, {"title": "Background", "content": "The Soft Actor-Critic (SAC) algorithm integrates policy optimization with entropy regularization, providing a robust framework for reinforcement learning, particularly in environments requiring a balance between exploration and exploitation. As a leading method in model-free, off-policy reinforcement learning, SAC has been widely recognized for its ability to maintain a stochastic policy throughout training, which is crucial for avoiding premature convergence to suboptimal strategies. The use of twin Q-functions to estimate the value of state-action pairs further enhances its robustness, reducing the overestimation bias commonly encountered in value-based reinforcement learning methods. However, despite these strengths, SAC is not without its challenges. One significant issue arises from the tanh transformation applied to the action outputs, which, while ensuring actions remain within a bounded range, introduces specific challenges that impact policy performance. Understanding these challenges necessitates an in-depth examination of how action sampling and distributional distortions affect reinforcement learning algorithms like SAC."}, {"title": "Soft Actor-Critic Algorithm Details", "content": "SAC, a leading method in model-free, off-policy reinforcement learning, integrates policy optimization with entropy regularization. The algorithm employs two Q-functions to evaluate state-action pairs and a policy network to generate actions. These Q-functions minimize Bellman error, while the policy network maximizes expected rewards combined with an entropy term. The entropy term, modulated by a temperature parameter (a), is essential in balancing exploration and exploitation, ensuring the policy remains stochastic and preventing convergence to suboptimal strategies. The SAC optimization objective is:\n\n$J(\\pi) = \\sum_{t=0}^{T}E_{(s_t, a_t) \\sim p_{\\pi}} [r(s_t, a_t) + \\alpha H(\\pi(\\cdot|s_t))],$\n\nThis equation underscores the dual objectives of maximizing cumulative rewards and maintaining high policy entropy, crucial for robust exploration and stable learning in complex environments. Notably, SAC's ability to operate in continuous action spaces with high dimensionality has made it particularly effective in robotics and other domains where precise control is paramount."}, {"title": "Impact of the tanh Transformation", "content": "In SAC, actions are typically sampled from a Gaussian distribution to support unbounded exploration. However, the tanh transformation is applied to ensure actions remain within the feasible range [-1,1]. While necessary, this transformation introduces significant distortions, particularly at the distribution's extremes Chou et al. [2017], Kim et al. [2019]. The probability density function (PDF) of transformed actions is expressed as:\n\n$\\pi(a|s) = \\mu(u|s) \\cdot |det(\\frac{da}{du})^{-1}|,$"}, {"title": "Methodology", "content": "This section introduces two optimal action sampling methods designed to correct the distributional distortions caused by the tanh transformation in the Soft Actor-Critic (SAC) algorithm. The first method is applied during the inference phase, where precise action selection is critical, and the second method is employed during the training phase to maintain necessary stochasticity for effective exploration."}, {"title": "Inference Phase: Optimal Action Sampling", "content": "The optimal action sampling approach during the inference phase addresses the distortions introduced by the tanh transformation when mapping actions sampled from a Gaussian distribution to the bounded space [-1,1]. The goal is to identify the most probable action that best represents the underlying distribution after transformation."}, {"title": "Deriving the Action Probability Density Function", "content": "Given a Gaussian-distributed action u sampled from the SAC policy network, represented as:\n\n$u \\sim N(\\mu, \\sigma^2),$\n\nwhere \u03bc and \u03c3\u00b2 denote the mean and variance of the distribution, respectively. The action u is transformed using the tanh function to confine it within the bounded space [-1,1]:\n\n$y = tanh(u).$\n\nTo derive the probability density function (PDF) of the transformed action y, the change of variables technique is employed. The original Gaussian PDF is given by:\n\n$p(u) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp{(-\\frac{(u - \\mu)^2}{2\\sigma^2})}.$\n\nu can be expressed as a function of y as:\n\n$u = \\frac{1}{2} log{(\\frac{1 + y}{1 - y})}.$"}, {"title": "Identifying the Most Probable Action", "content": "To maximize the probability density function derived in Equation 9, the objective during the inference phase is to find the action y* that satisfies:\n\n$y^* = arg\\underset{y\\in(-1,1)}{max} p(y).$\n\nThis is achieved by discretizing the action space y, calculating p(y) at each point, and selecting the action with the highest probability."}, {"title": "Training Phase: Inverse Transform Sampling", "content": "During training, maintaining stochasticity in action selection is essential for effective exploration. Instead of selecting the most probable action, the inverse transform sampling method is used to sample actions according to their likelihood, preserving the necessary randomness for robust learning."}, {"title": "Inverse Transform Sampling Method", "content": "The cumulative distribution function (CDF) is computed from the probability density function:\n\n$C(y) = \\int_{-1}^{y}p(y') dy'.$"}, {"content": "A random value r uniformly sampled from [0, 1] is used to determine the sampled action $y_{sampled}$, where C(y) = r."}, {"title": "Discussion of the Two Sampling Methods", "content": "These two sampling methods are designed to cater to the distinct needs of SAC during inference and training. The inference phase requires precise action selection, which is achieved through optimal"}, {"title": "Experiments", "content": "This section rigorously evaluates the proposed optimal action sampling methods within the MuJoCo environment, renowned for its high-fidelity physics simulation. The experiments aim to assess the impact of these methods on the performance of the Soft Actor-Critic (SAC) algorithm, particularly focusing on key metrics such as convergence speed and cumulative rewards. By selecting a diverse set of continuous control tasks, this study ensures a comprehensive evaluation across varying levels of complexity and control precision."}, {"title": "Experimental Setup", "content": ""}, {"title": "Benchmark Tasks", "content": "The evaluation is conducted on six continuous control tasks within the MuJoCo environment: HalfCheetah-v4, Hopper-v4, Walker2d-v4, Humanoid-v4, HumanoidStandup-v4, and Reacher-v4. These tasks were selected for their ability to challenge different aspects of reinforcement learning algorithms:\n\u2022 HalfCheetah-v4: A task emphasizing speed and balance in a planar cheetah model, often used to evaluate algorithms' efficiency in optimizing forward motion.\n\u2022 Hopper-v4: This task tests the stability and precision of a single-legged robot's hopping motion, highlighting the importance of balance and control.\n\u2022 Walker2d-v4: A bipedal robot tasked with walking forward, providing a moderate level of complexity that requires both stability and dynamic control.\n\u2022 Humanoid-v4: One of the most complex tasks, involving a humanoid robot that must maintain balance while walking, representing a high-dimensional control challenge.\n\u2022 HumanoidStandup-v4: A highly challenging task where a humanoid must stand up from a lying position, requiring significant coordination and control.\n\u2022 Reacher-v4: A simpler task where a robotic arm must reach a target, used to assess precision in low-dimensional action spaces.\n\nThese tasks were chosen not only for their varying levels of difficulty but also for their relevance to real-world applications, such as robotics and autonomous systems, where precise control and optimization are critical."}, {"title": "Training Configuration", "content": "To ensure fair and consistent evaluation across all tasks, the experiments were conducted using a standardized set of configurations, as detailed in Table 1. Each parameter was meticulously selected to optimize the stability and learning efficiency of the SAC algorithm:"}, {"title": "Compared Methods", "content": "The proposed optimal action sampling methods were rigorously compared against a baseline configuration of the Soft Actor-Critic (SAC) algorithm under identical experimental conditions. The methods evaluated include:\n\u2022 Original-deterministic (Baseline): This method represents the standard SAC algorithm, where actions are sampled from a Gaussian distribution during training. During inference, deterministic action selection is performed by using the mean of the Gaussian distribution, followed by a tanh transformation to map the actions into the bounded action space.\n\u2022 Original-refineSampling: In this configuration, the standard SAC approach is maintained during training, employing Gaussian sampling as described in the Baseline method. However, during inference, this method utilizes the Sampling for Inference Phase algorithm, which refines action selection by considering the likelihood of actions within the transformed space.\n\u2022 RefineT-refineSampling: This method introduces a significant change by applying the Sampling for Training Phase algorithm during the training process, which incorporates the proposed inverse transform sampling technique. This approach maintains the stochastic nature of the action selection process while simultaneously correcting the distortions intro-"}, {"title": "Results and Analysis", "content": ""}, {"title": "Cumulative Rewards", "content": "Cumulative rewards represent the total accumulated rewards over a defined training period in reinforcement learning. Mathematically, it is expressed as:\n\n$Cumulative Reward (CR) = \\sum_{t=0}^{T} r_t,$\n\nwhere rt denotes the reward at time step t, and T is the total number of time steps. This metric provides a comprehensive evaluation of the policy's effectiveness in maximizing returns during its interactions with the environment.\nFigure 2 illustrates the cumulative rewards across various MuJoCo tasks, highlighting the effectiveness of the proposed methods in different environments.\nIn the complex Humanoid-v4 task, both RefineT-refineSampling and Original-refineSampling methods surpass the Original-deterministic method, achieving a 10% to 20% improvement, with RefineT-refineSampling exhibiting the highest stability, as indicated by narrower standard deviation bands.\nSimilarly, in the HumanoidStandup-v4 task, RefineT-refineSampling outperforms the baseline by 5% to 10%, demonstrating the benefits of advanced sampling techniques in complex environments.\nFor tasks of moderate complexity, such as Hopper-v4, RefineT-refineSampling slightly exceeds the other methods, with reduced variability, underscoring its consistency.\nIn contrast, for simpler tasks like Reacher-v4, the Original-deterministic and Original-refineSampling methods perform marginally better than RefineT-refineSampling, reflecting the diminished need for complex sampling strategies in less demanding environments."}, {"title": "Convergence Speed", "content": "Convergence speed is redefined as the average rate of change in reward values across training epochs. Instead of merely measuring the number of epochs required to reach a fixed percentage of the maximum reward, this refined definition captures the overall learning efficiency by evaluating the consistency and pace of reward improvement during training.\nMathematically, convergence speed is calculated as the mean difference in reward values between successive epochs. This approach provides a more granular assessment of the learning process, reflecting not only the time to reach a specific performance level but also the stability and uniformity of the learning progression.\nFormally, the convergence speed S can be expressed as:\n\n$S = \\frac{1}{N} \\sum_{i=2}^{N} (R_i - R_{i-1}),$\n\nwhere Ri denotes the reward at the ith epoch, and N is the total number of epochs. This metric highlights the average improvement in reward per epoch, offering a more comprehensive view of the model's learning dynamics and optimization efficiency.\nFigure 3 illustrates the convergence speed across various tasks, highlighting significant performance differences among the proposed methods.\nIn the most complex task, Humanoid-v4, RefineT-refineSampling outperforms both Original-refineSampling and Original-deterministic, demonstrating its effectiveness in high-dimensional action spaces. In contrast, for the HumanoidStandup-v4 task, both refined methods exhibit slower convergence compared to the baseline, likely due to the task's inherent difficulty, which constrains further reward improvements.\nFor moderately complex tasks such as Hopper-v4 and Walker2d-v4, RefineT-refineSampling consistently surpasses the other methods, reflecting its capability to effectively balance exploration and exploitation. However, in the HalfCheetah-v4 task, Original-refineSampling achieves faster convergence, suggesting that in specific scenarios, a simpler refinement may be more suitable.\nIn the simplest task, Reacher-v4, the baseline method achieves the fastest convergence, as anticipated, due to the task's lower complexity, which reduces the necessity for advanced sampling strategies."}, {"title": "Discussion", "content": "The experimental results provide compelling evidence of the advantages offered by the RefineT-refineSampling method, particularly in high-dimensional action spaces. As demonstrated in the Humanoid-v4 and HumanoidStandup-v4 tasks, RefineT-refineSampling not only achieves significant improvements in cumulative rewards-ranging from 5% to 20%-but also exhibits faster convergence compared to the Original-deterministic methods. This enhancement is largely attributed to the method's ability to accurately model the action distribution post-tanh transformation, thereby mitigating the distortive effects that often impede policy optimization in complex environments.\nIn the Humanoid-v4 task, RefineT-refineSampling's ability to refine the action selection process during both training and inference phases ensures that the selected actions align more closely with the most probable outcomes within the bounded action space. This alignment is crucial in high-dimensional tasks where even minor distortions can significantly affect policy performance. The narrow standard deviation bands associated with RefineT-refineSampling further underscore its stability, making it a robust choice for tasks that demand precise control and optimal exploration.\nHowever, the performance of the Original-deterministic method in simpler tasks such as Reacher-v4 reveals an important consideration: the complexity of the task and the dimensionality of the action space play a pivotal role in determining the effectiveness of advanced sampling techniques. In low-dimensional tasks where the action space is less complex and the potential for distortion is reduced, the overhead introduced by methods like RefineT-refineSampling may not provide substantial benefits. Here, the simplicity and efficiency of the baseline SAC approach suffice, achieving optimal performance without the need for additional refinements.\nThese findings emphasize the necessity of selecting an appropriate sampling strategy based on the task's complexity and the dimensionality of the action space. RefineT-refineSampling excels in high-dimensional and challenging environments where precision in action selection is paramount. In contrast, simpler tasks may not require such advanced techniques, allowing traditional methods to perform effectively with lower computational overhead. Future research should focus on developing hybrid approaches that dynamically adjust the sampling strategy based on the task characteristics, optimizing both performance and computational efficiency across a broader range of environments."}, {"title": "Conclusion", "content": "This study addresses a critical challenge in the Soft Actor-Critic (SAC) algorithm- the distributional distortions introduced by the tanh transformation during action sampling. By deriving an accurate probability density function and developing an optimized sampling method, this research significantly enhances SAC's performance across various continuous control tasks. The proposed RefineT-refineSampling method not only improves convergence speed in high-dimensional tasks like Humanoid-v4 but also achieves higher cumulative rewards, demonstrating its effectiveness in complex environments. These findings underscore the method's potential for broader applications in reinforcement learning, particularly in tasks requiring precise action control in high-dimensional spaces."}]}