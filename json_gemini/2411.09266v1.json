{"title": "How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative Study of ChatGPT, AI Models and Human Perception", "authors": ["Sahibzada Adil Shahzad", "Ammarah Hashmi", "Yan-Tsung Peng", "Yu Tsao", "Hsin-Min Wang"], "abstract": "Multimodal deepfakes involving audiovisual manipulations are a growing threat because they are difficult to detect with the naked eye or using unimodal deep learning-based forgery detection methods. Audiovisual forensic models, while more capable than unimodal models, require large training datasets and are computationally expensive for training and inference. Furthermore, these models lack interpretability and often do not generalize well to unseen manipulations. In this study, we examine the detection capabilities of a large language model (LLM) (i.e., ChatGPT) to identify and account for any possible visual and auditory artifacts and manipulations in audiovisual deepfake content. Extensive experiments are conducted on videos from a benchmark multimodal deepfake dataset to evaluate the detection performance of ChatGPT and compare it with the detection capabilities of state-of-the-art multimodal forensic models and humans. Experimental results demonstrate the importance of domain knowledge and prompt engineering for video forgery detection tasks using LLMs. Unlike approaches based on end-to-end learning, ChatGPT can account for spatial and spatiotemporal artifacts and inconsistencies that may exist within or across modalities. Additionally, we discuss the limitations of ChatGPT for multimedia forensic tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Synthetic multimedia content has become both innovative and a significant threat in recent years. Deepfake images and videos created using artificial intelligence (AI) and deep learning (DL) techniques have attracted public and academic attention. This synthetic content is generated by generative adversarial networks (GANs) [1] and more sophisticated AI techniques such as diffusion models [2]. While deepfake technology has many innovative applications in education, entertainment, and other fields [3], it is a double-edged sword that can be used for unethical purposes, such as pornography, political defamation, identity theft, fraud, misinformation, and disinformation [4]-[6]. Unethical use of this technology can lead to political instability and social violence [6]. On the one hand, deepfake technology continues to evolve to create more convincing and realistic fake multimedia content. Social media, on the other hand, plays a catalytic role in spreading such content. Therefore, timely detection of deepfake content is crucial to avoid any damage and loss to human society [4]. Audiovisual deepfakes that involve multimodal manipulation are a more convincing type of forgery, with attackers attacking audio, video, or both modalities. Unimodal video forgery detectors [7]\u2013[10] and spoofed audio detectors [11]\u2013[14] are generally unable to identify forgeries across multiple modalities, although they may be good at detecting forgeries in the specific modality they focus on. To address this challenge, the research community has developed sophisticated tools and algorithms to detect audiovisual forgeries in videos. These specialized tools require knowledge of multimedia forensics as well as knowledge of deep learning. Furthermore, these tools do not generalize well to other unseen datasets and manipulations. Large language models (LLMs) are a major advancement in the field of artificial intelligence. They are trained on a large amount of data and can perform well in various natural language processing (NLP) tasks such as text generation, summarization, classification, completion, sentimental analysis, machine translation, and question answering. Their applications even go beyond the aforementioned NLP tasks and can be used as writing assistants, learning tools, productivity tools, coding assistants, software development, healthcare, legal assistance, entertainment, and more. Despite being primarily designed for NLP tasks, OpenAI's ChatGPT can analyze image, audio, and video content. Taking advantage of its support for multimodal input, we studied the potential and limitations of ChatGPT for audiovisual deepfake detection. The research questions we aimed to address in this study are as follows:\nCan ChatGPT perform multimedia forensic tasks?\nIs ChatGPT capable of detecting forgery based on artifacts in audio and visual modalities?\nWhat is the role of prompt engineering in using ChatGPT to detect audiovisual deepfakes?\nWhich performs better at identifying forgeries in audiovisual deepfakes, ChatGPT, humans, or AI models?\nHow interpretable is ChatGPT for forgery detection?\nWhat are the limitations of ChatGPT in detecting multimodal deepfakes?\nThe main contributions of our work are threefold:\nWe explore for the first time the potential of ChatGPT for audiovisual forgery detection tasks.\nWe compare the performance of ChatGPT with human and state-of-the-art AI models on audiovisual forgery detection tasks.\nWe highlight the strengths and limitations of ChatGPT on audiovisual forgery detection tasks."}, {"title": "II. RELATED WORK", "content": "The societal impact of synthetic media content has prompted research from multiple angles within the forensic science community. In [15], the authors investigated synthetic content from multiple perspectives such as multimedia content production, representation, media audience dynamics, gender, politics, law, and regulation, and concluded that the intersection between media and deepfake content can have multiple impacts on individuals and society. A study in [16] on the impact of unreliable deepfake information on voter behavior in US elections and democracy suggests multi-stakeholder partnerships and technological approaches for identifying and mitigating manipulated content on public platforms. In [17], the balance between innovation and ensuring fair protection under existing laws is explored, particularly as generative AI blurs the line between human and machine-generated works. The US FDA's regulation of AI in medical devices and the European AI Act, which classify AI applications based on potential harm, are initiatives aimed at addressing challenges and aligning AI-generated content and applications with human-centered values. This analysis is essential for developing a legal framework that addresses the ethical and practical implications of the creativity of generative AI in the legal domain. A recent study in [18] investigated the risk of deepfakes in legal proceedings, where altered audiovisual evidence could compromise the integrity of justice. It highlights how deepfake technology can influence the outcome of cases based on subjective human judgment. The findings point to the need for changes to the legal framework to ensure that key judicial principles such as the presumption of innocence and the right to a fair trial are protected. Furthermore, research on human perception of audiovisual deepfake videos [19] shows that it is difficult for people to accurately distinguish deepfake content from real videos, mainly due to the realistic visual and acoustic manipulations involved. Audiovisual deepfakes can be broadly categorized into three types, as shown in Fig. 1. The first type is \u201cFake Video Real Audio\" (FVRA), in which the visual frames are manipulated using techniques such as Faceswap [20], Fsgan [21], and Wav2lip [22], while the audio modality remains unaltered. The second type is \"Real Video Fake Audio\" (RVFA), where the video frames are authentic, but the audio modality is manipulated using techniques such as SV2TTS [23], a real-time voice cloning tool that can synthesize fabricated audio content. The third type is \"Fake Video Fake Audio\" (FVFA), where both modalities are manipulated. In this type, face manipulation can be done using methods like Faceswap and Fsgan, while lip synchronization/manipulation can be achieved using Wav2lip. Additionally, cloned or synthesized audio can be integrated with visually manipulated frames to produce more realistic and convincing deepfake videos. The multimedia forensics community has developed several data-driven audiovisual deepfake detection methods based on multimodal feature fusion [24], ensemble learning [25], [26], and synchronization features [27], [28]. Models based on Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and Transformers [28]\u2013[31] have been widely used to detect forgeries in either modality and are trained on multimodal deepfake datasets. These methods provide a binary output for the input video, indicating whether the input video is genuine or spoofed. Disadvantages of these end-to-end learning-based methods include reliance on large datasets, heavy training, and lack of interpretability and generalization. Bias, imbalance, and lack of diversity in training data can lead to fairness, generalization, and security issues for detection models [32]. Recently, with the emergence of LLMs, the research community has begun to utilize these models to solve various tasks in different fields, beyond their original purpose. For example, while ChatGPT is primarily designed for NLP tasks, the multimodal mode in ChatGPT-4 enables it to handle multimodal inputs and analyze content from a multimodal perspective [33]\u2013[35]. Many studies have investigated the performance of LLMs in various challenging tasks, such as image forensics [36], facial biometrics [37], tampered image detection [38], fake news detection [39], NLP [40], cheap-fake detection [41], global warming [42], education [43], public health [44], and medical applications [45]. These studies highlight the strengths and limitations of LLMs, focusing"}, {"title": "III. METHODOLOGY", "content": "Fig. 2 shows an LLM-based approach for multimodal forgery detection, where the text prompts and video with corresponding audio are used as inputs. Based on the given prompts, the model works as a black box and produces multiple analysis results on the input video, such as visual, acoustic, and audiovisual analysis and their corresponding predictions. Our goal is to evaluate the detection capabilities of ChatGPT. This model is trained on multimodal data and can be used for audiovisual forgery detection tasks. Deepfake attacks usually target high-level facial features and voice identities; therefore, we choose frontal face videos with voices to evaluate the detection performance of ChatGPT. Unlike traditional end-to-end models that leverage low-level features, LLMs provide high-level features and descriptions to analyze multimodal inputs. In this study, we used OpenAI's GPT-4 to conduct audiovisual analysis of deepfake videos. Unlike other deep learning-based models, it provides interpretability by explaining the reasoning behind the final decision, thereby increasing the transparency of the decision-making process.\nInspired by previous research on LLM-based image forensics [36], we proposed the following custom prompts, ranging from simple, binary-answer prompts to advanced, context-rich prompts designed to account for artifacts and manipulations in audiovisual content:\nP1: Tell me if this is an AI-generated video by analyzing both audio and video modalities. Answer yes or no.\nP2: Tell me if this is a real video by analyzing both audio and video modalities. Answer yes or no.\nP3: Tell me the probability of this video being real by analyzing both audio and video modalities. Answer a probability score between 0 and 100.\nP4: Tell me the probability of this video being AI-generated by analyzing both audio and video modalities. Answer a probability score between 0 and 100.\nP5: Tell me if this is a real or AI-generated video by analyzing both audio and video modalities.\nP6: Tell me if synthesis artifacts are in the visual frames or audio. Must return with 1) yes or no only; 2) if yes, explain where the artifacts exist by answering in [region, artifacts] form.\nP7: I want you to work as a multimedia forensic expert for audiovisual deepfake detection. Check if the video has the artifact attribute listed in the following list, and ONLY return the attribute number in this video. The artifact list is [1-unnatural face edges; 2-inconsistent lighting and shading; 3-irregular teeth shape or texture; 4-irregular lip movement; 5-inconsistent skin texture; 6-spectral artifacts; 7-phoneme artifacts; 8-inconsistencies in speech patterns; 9-voice quality issues; 10-lack of synchronization between audio and video].\nWe feed the videos directly into the LLM-based ChatGPT model without performing any preprocessing or transcribing the videos for analysis. The model extracts audio from the video and performs visual and acoustic analysis based on input prompts. Let X = (xv, xa, xt) denote the entire input, where xv represents the video, xa represents the audio, and xt represents the custom text prompt. The model outputs its final prediction as:\n$\\hat{y} = f_{LLM}(X) = f_{LLM}(x_v, x_a, x_t),$\\\nwhere fLLM is the underlying function.\nBased on the given input text prompts, the model performs audio analysis by analyzing spectral features, zero crossing rate, mel-frequency cepstral coefficients (MFCC), amplitude envelope, amplitude range, median amplitude, spectral centroid, spectral rolloff, and silence ratio. For video, the model performs analysis such as blurriness, pixelation, lighting, frame difference mean, frame difference standard deviation, unnatural expression, skin texture, lip-syncing, and structural similarity index (SSIM). It also performs multimodal analysis to verify consistency between visual and audio modalities through synchronization checks. By combining unimodal acoustic and visual analyses with multimodal analysis, joint analysis is performed to reach a final prediction or suggest further manual inspection."}, {"title": "D. Prediction Assignment", "content": "The final prediction \u0177 for each input video is determined based on the following factors: yes/no response, probability score, overall conclusion, artifact-free versus artifact list, and estimated likelihood of the input video being real or fake. Fake classes are assigned label 1, while label 0 represents real classes."}, {"title": "E. ChatGPT vs Human vs AI Models", "content": "To understand the detection capabilities of ChatGPT, humans, and Al models, we follow the study in [19], where the authors reported a comparative analysis between humans and deep-learning-based multimodal forensic models. Their results concluded that state-of-the-art AI models surpass humans in detecting multimodal deepfakes. Furthermore, participants often showed overconfidence in their detections, with their average accuracy being lower than their confidence level. To evaluate the detection performance of ChatGPT, we selected the same video subset used in [19] from the FakeAVCeleb dataset [46] for a fair comparison."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "Following the study in [19], we selected the same 40 videos from the FakeAVCeleb dataset [46] to allow for a fair comparison with humans and state-of-the-art multimodal forensic models. The 40 videos contain 20 real videos and 20 fake videos, representing each class equally. To eliminate gender bias, each class contains an equal number of male and female videos."}, {"title": "B. Evaluation Metrics", "content": "For evaluation, we calculate precision, recall, F1-score, and accuracy, which are defined as,\nPrecision = $\\frac{TP}{TP + FP},$\nRecall = $\\frac{TP}{TP + FN},$\nF1 = $\\frac{2 \\times Precision \\times Recall}{Precision + Recall},$\nAccuracy = $\\frac{TP+TN}{TP+TN+FP+FN},$\nwhere TP, TN, FP, and FN stand for True Positive (fake videos correctly detected as fake), True Negative (real videos correctly detected as real), False Positive (real videos incorrectly detected as fake), and False Negative (fake videos incorrectly detected as real), respectively. Additionally, we calculate the rejection rate to evaluate the effectiveness of input text prompts:\nRejection Rate = $\\frac{Number \\space of \\space Rejected \\space Prompts}{Total \\space Number \\space of \\space Prompts} \\times 100.$\n1) Comparing Different Text Prompts: Table I lists the performance of different text prompts (P1-P7 in Section III-A). Fig. 3 shows a bar graph comparison of TP, TN, FP, and FN (see Section IV-B) for different text prompts. Additionally, Fig. 4 shows an example of custom text prompts, ChatGPT audiovisual analysis, corresponding predication, and ground truth label. Next, we analyze each text prompt in detail and discuss the results.\nPrompt P1: As can be seen from Table I, the accuracy of P1 reaches 54.0%, which is only slightly higher than the 50% of random guessing, indicating that it is less effective in guiding the model to make accurate predictions. Its simplicity and lack of necessary information resulted in a rejection rate as high as 7.50%, preventing the model from making predictions for every video input. The numbers of TP, FP, TN, and FN are 7, 5, 13, and 12 respectively. In the deepfake video detection task, a higher TP value is desirable. However, instead of obtaining higher TP, P1 produced more TN, resulting in a lower recall of 0.368."}, {"title": "V. ABLATION STUDY", "content": "Initially, we tested some basic prompts by mentioning only \"video\" and no mention of \"audio\", and executed custom prompts sequentially within one session. The following are the custom text prompts:\nTell me if this is an AI-generated video. Answer yes or no.\nTell me if this is a real video. Answer yes or no.\nTell me the probability of this video being real. Answer a probability score between 0 and 100.\nTell me the probability of this video being AI-generated. Answer a probability score between 0 and 100.\nTell me if this is a real or AI-generated video.\nTell me if synthesis artifacts are in the face. Must return with 1) yes or no only; 2) if yes, explain where the artifacts exist by answering in [region, artifacts] form.\nI want you to work as a video forensic expert for AI-generated faces. Check if the video has the artifact attribute in the following list and ONLY return the attribute number in this image. The artifact list is [1-asymmetric eye iris; 2-irregular reflection; 3-irregular teeth shape or texture; 4-irregular ears or earrings; 5-strange hair texture; 6-inconsistent skin texture; 7-inconsistent lighting and shading; 8-strange background; 9-unnatural edges; 10-lack of synchronization between audio and video].\nAs can be seen from Table III, the rejection rate is higher and the accuracy is lower, compared with the results in Table I. Based on these results, we made several observations. First, only mentioning the video in the prompt causes the model to focus mainly on visual frames without analyzing the audio modality. To obtain the desirable output from an LLM-based model, prompts need to be specific and context-rich. Second, when prompts are fed sequentially, the model takes into account the context of the results of previous prompts, affecting its response to the current prompt. To obtain independent and unbiased results from prompts, we must feed the input video and prompt independently within a session to eliminate the effects of contextual bias from previous prompts. Third, our experiments show that prompts must contain terms relevant to acoustic analysis in order for the model to effectively analyze the audiovisual content in a given video."}, {"title": "B. Failure Case Study", "content": "The reasons for detection failure vary depending on both the input prompt and video content. Through our experiments and careful analysis, we observed several factors that lead to inaccurate decisions in the multimodal ChatGPT model. One factor is a high silence ratio in the speech content, which may indicate robotic/synthetic audio since the speech generation pipeline excludes environmental noise. However, videos with clean/enhanced speech do not always indicate synthesis or voice spoofing. Conversely, adding synthetic environmental noise to clean audio can mislead the model, leading to inaccurate predictions. The high silence rate combined with unnatural pauses in the acoustic modality can lead to an increased number of false positives in the model. While unimodal/multimodal deep features and audiovisual correlation features are effective in various multimodal tasks, ChatGPT mainly relies on hand-crafted features and traditional functions in computer vision and speech processing libraries, including OpenCV, librosa, numpy, wav, and skimage, for visual and acoustic analysis. Furthermore, existing deep learning-based pretrained foundation models [47]-[50] and frameworks (such as Tensorflow or Pytorch) are not used to analyze video content for possible artifacts and forgeries. These two shortcomings limit the ChatGPT method from effectively analyzing video content and result in poor performance compared to state-of-the-art forensic models. In the context of audiovisual video forgery detection, if any modality (audio or video) is fake, the final prediction should be classified as fake. However, we observed that the overall probability score, calculated as the average of the audio and video scores, can lead the model to make incorrect predictions. If the score of one of the modalities dominates, the final prediction tends to reflect that modality, compromising the overall accuracy and classification results."}, {"title": "VI. LIMITATIONS", "content": "Although LLM-based models are superior to end-end learning-based black box models in terms of generalization, interpretability, and intuitive user interface for end users, they still have limitations. LLM-based models require domain knowledge for multimedia forensics tasks to design more effective prompts to exploit their underlying multimodal capabilities. Simple binary prompts are ineffective and yield lower accuracy and higher rejection rates. Furthermore, ChatGPT uses traditional techniques to analyze forgery in audiovisual content and has no access to pretrained models specifically trained for multimodal forgery detection tasks, resulting in lower accuracy even when the text prompts are effective and contextually rich. Given these limitations, the multimedia forensics community must focus on cutting-edge, end-to-end learning-based techniques to develop more robust, generalizable, and explainable audiovisual deepfake detectors."}, {"title": "VII. CONCLUSIONS", "content": "In this study, we investigated the detection capabilities of a large language model (LLM) (i.e., ChatGPT) in the multimodal forgery detection task. We compared its performance with that of end-to-end multimedia forensic methods and human capabilities. Our results showed that, although ChatGPT was not explicitly designed for multimedia forgery detection tasks, its performance was comparable to human detection performance, demonstrating its potential in this field. A notable advantage of using LLMs in video forensics is their ability to generalize effectively because these models are learned from a wide range of datasets, unlike end-to-end models that are typically learned from specific video deepfake datasets. Additionally, LLMs provide superior interpretability compared to deep learning-based forensic methods, which, while capable of identifying specific visual and acoustic artifacts, typically serve as black-box models with limited interpretability. In future work, we aim to combine LLM-based models with deep learning-based forensic models to enhance interpretability and further contribute more interpretable and transparent deepfake detection tools to the forensics community."}]}