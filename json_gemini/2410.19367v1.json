{"title": "BitPipe: Bidirectional Interleaved Pipeline Parallelism for Accelerating Large Models Training", "authors": ["Houming Wu", "Ling Chen*", "Wenjie Yu"], "abstract": "With the increasing scale of models, the need for efficient distributed training has become increasingly urgent. Recently, many synchronous pipeline parallelism approaches have been proposed to improve training throughput. However, these approaches still suffer from two major issues, i.e., pipeline bubbles caused by periodic flushing and extra communication due to the increasing number of pipeline stages. To this end, we propose BitPipe, a bidirectional interleaved pipeline parallelism for accelerating large models training. Specifically, a hybrid scheme of fusing interleaved pipelines with bidirectional pipelines is proposed to reduce the computational time of each single micro-batch and multiply the number of devices executing simultaneously. A V-shaped schedule with eager gradient synchronization is introduced to reduce and overlap the communication between devices. Experiments conducted on up to 32 GPUs show that BitPipe improves the training throughput of GPT-style and BERT-style models by 1.05x-1.28x compared to the state-of-the-art synchronous approaches.", "sections": [{"title": "Introduction", "content": "Scaling the number of parameters in contemporary deep learning models has yielded remarkable the state-of-the-art (SOTA) results. Training these large models is challenging, as the limited memory and computational capacity of a single device (e.g., GPU) pose obstacles to accommodating them within realistic timeframes. For instance, training a GPT-3 175B model demands over 3,000 GiB for storing model parameters and optimizer states, requiring an impractical 288 years with a single NVIDIA V100 GPU (Kim et al. 2023; Narayanan et al. 2021b).\nThe urgency for parallel and distributed training (e.g., data parallelism and model parallelism) has become increasingly pronounced. While data parallelism (Li et al. 2014) allows for ideal speedup, it falters when confronted with large models that exceed the capacity of a single device. Model parallelism (Dean et al. 2012; Lee et al. 2014; Wang, Huang, and Li 2019) addresses this limitation by distributing the weight parameters of a model across multiple devices, which mitigates the memory usage per device but suffers from severe resource under-utilization. Pipeline parallelism improves resource utilization, which splits a batch into smaller micro-batches and divides a model into stages within a pipeline, allowing simultaneous execution of different micro-batches across multiple devices. Pipeline parallelism can be categorized into synchronous and asynchronous schemes based on weight update semantic. Synchronous approaches flush periodically at the end of each iteration to guarantee strict optimizer semantics, which causes device idle times (also called pipeline bubbles). Asynchronous approaches do away with flushes completely by delaying weight updates, but at the expense of strict model convergence and thus are not within the scope of our work.\nEarly synchronous approach (e.g., GPipe (Huang et al. 2019)) focuses on reducing pipeline bubbles by increasing the number of concurrent batches in the pipeline (as shown in Figure 1(a)). As a direct consequence, there is an increase in peak activation memory demands. Subsequently, encouraged by the success of the 1F1B schedule (as shown in Figure 1(b)), researchers have proposed memory-efficient approaches (e.g., DAPPLE (Fan et al. 2021) and PipeDream-Flush (Narayanan et al. 2021a)), which further adjusts the number of micro-batches injected into devices at the beginning of pipelines.\nRecently approaches attempt to increase the number of devices executing simultaneously (i.e., bidirectional pipeline"}, {"title": "Related Work", "content": "Model Parallelism\nModel parallelism is a solution to train large models by partitioning the weight parameters of a model among available devices in two ways: tensor (intra-layer) model parallelism (Wang, Huang, and Li 2019; Shoeybi et al. 2019) and interlayer model parallelism (Krizhevsky, Sutskever, and Hinton 2012; Dean et al. 2012). The former is trapped in requiring all-to-all communication, while the latter suffers from underutilized resources.\nPipeline Parallelism\nPipeline parallelism can effectively improve resource utilization. In this scenario, a batch is further partitioned into smaller micro-batches, which allows each device to commence processing the subsequent micro-batch immediately after completing the preceding one. Pipeline parallelism approaches can be categorized into synchronous and asynchronous schemes based on the weight update semantics. For synchronous approaches, the magnitude of pipeline bubbles can be quantified as bubble ratio, which is defined as the bubble overhead divided by the overall pipeline runtime. GPipe (Huang et al. 2019) reduces the bubble ratio by increasing the number of concurrent batches in the pipeline, which increases the peak activation memory demands as a direct consequence. DAPPLE (Fan et al. 2021) and PipeDream-Flush (Narayanan et al. 2021a) lower the activation memory usage by adjusting the number of micro-batches injected into devices at the beginning of pipelines and performing the 1F1B schedule. Recently efforts have led to bidirectional pipeline parallelism and interleaved pipeline parallelism.\nBidirectional Pipeline Parallelism combines two pipelines in different directions, which doubles the number of devices executing simultaneously. GEMS (Jain et al. 2020) is a memory-efficient pipeline approach that first schedules micro-batches among two model replicas. Since GEMS is mainly designed for small batch sizes and executes at most two micro-batches simultaneously, its bubble ratio is much higher than that of the other approaches. Chimera (Li and Hoefler 2021) implements two pipelines in opposite directions simultaneously (named down and up pipeline, respectively), and the pipeline utilization can be better than that of vanilla pipeline parallelism with a single pipeline, as shown in Figure 2(c). MixPipe (Zhang et al. 2023) flexibly regulates the number of micro-batches injected into the bidirectional pipelines at the beginning, which achieves a better balance between pipeline utilization and device utilization. However, these approaches impose an increased burden on each device, requiring more weight memory and data-parallel communication.\nInterleaved Pipeline Parallelism splits the original pipeline stage into smaller non-consecutive stages and schedules in a loop way, which makes the bubble size reduce with the decrease of the calculation time of each micro-batch. 1F1B-Int (Narayanan et al. 2021b) effectively reduces the bubble ratio without incurring additional memory consumption for model weights, at the cost of extra pipeline-parallel communication overhead. WPipe (Yang et al. 2022) integrates 1F1B-Int with PipeDream-2BW (Narayanan et al. 2021a), which achieves better memory efficiency and fresher weight updates. Breadth-First (Lamy-Poirier 2023) generalizes 1F1B-Int and combines it with data parallelism, which shows a better overlap of communication with computation."}, {"title": "Methodology", "content": "Overview\nBitPipe is a hybrid schedule of integrating interleaved pipelines with bidirectional pipelines, which makes the bubble ratio smaller and exhibits a more balanced activations memory consumption, as shown in Figure 2(d). The key idea of BitPipe is to seamlessly merge two V-shaped interleaved pipelines in opposite directions (as shown in Figure 3), which partially transforms the cross-device communication to local copying and reduces the communication overhead. The symbols used by the following sections are defined in Table 1.\nV-shaped Interleaved Schedule\nPipeline parallelism typically splits the model layers into a single stage per device. 1F1B-Int assigns multiple smaller and nonconsecutive stages to each device, with cross-device communication between stages, as illustrated in Figure 2(b).\nIn contrast to this looping schedule, we introduce the V-shaped interleaved schedule that swaps the order and sequentially allocates stages to devices, starting from the first device and progressing to the last, then reversing the order from the last device back to the first, creating a \"V\" shape (i.e., \\(stage1\\) \\(stage2\\) are mapped to \\(P1\\)~\\(P2\\), and \\(stage3\\) \\(stage4\\) are mapped to \\(P2\\)~\\(P1\\), as illustrated in Figure 4(b)). Since the sequence of computation remains unchanged and the communication overhead is decreased by local copying (between consecutive stages in P2), we can deduce that the efficiency of this V-shaped schedule is at least on a par with, if not superior to, the original looping schedule.\nMotivated by Chimera that involves two pipelines and combines them together, we initially contemplate scaling the"}, {"title": "Bidirectional Interleaved Pipelines", "content": "number of the V-shaped interleaved pipelines in BitPipe. It should be noted that the V-shaped interleaved schedule can be generalized to a greater number of stages while maintaining the unchanged number of pipelines (discussed in Appendix A), and this further reduces the bubbles but at the expense of higher communication overhead.\nThe core concept of BitPipe is seamlessly integrating two V-shaped interleaved pipelines in opposite directions. Figure 3 presents an example with four pipeline devices (i.e., \\(D=4\\)). Herein, we assume that each device executes D micro-batches within a training iteration (i.e., \\(N=D\\)), which is the minimum to maintain the activeness of all stages. In the V-shaped interleaved down pipeline, \\(stage1\\) \\(stage4\\) are mapped to \\(P1\\)~\\(P4\\), and \\(stage5\\) \\(stage8\\) are mapped to \\(P4\\)~\\(P1\\). The stages in the V-shaped interleaved up pipeline are mapped in strikingly opposite order. Each pipeline schedules N/2 (assuming N is an even number) micro-batches using 1F1B-Int strategy, as shown in the left part of Figure 3. Subsequently,"}, {"title": "Bubble Ratio.", "content": "For the synchronous approaches, the bubble ratio is defined as the ratio of the bubble overhead to the overall runtime of the pipeline. By counting the number of injected micro-batches on each device of BitPipe in Figure 3, it can be observed that BitPipe incurs \\((3D-6)/4\\) bubbles (i.e., \\((D-2)/2\\) bubbles in the forward passes and \\((D-2)/4\\) bubbles in the backward passes). Given the assumption that the workload of a backward pass is about two times of a forward pass, the bubble ratio of BitPipe is \\((D-2)/(3N+D-2)\\). Table 2 presents the bubble ratio of the five pipeline approaches, among which BitPipe is the lowest. The bubble ratio of BitPipe can be further reduced to \\((D-2)/(4N+D-2)\\) by removing the middle bubbles (detailed in Appendix B).\nMemory Consumption. Memory consumption is primarily influenced by two aspects: the weight parameters and the intermediate activations. Table 2 also presents the memory usage of BitPipe, where \\(M_e\\) represents the memory consumption of the weights in one device for one model replica, and \\(M_a\\) is the memory consumption of the activations in one device for one micro-batch (as the light green colors shown"}, {"title": "Communication Optimization", "content": "BitPipe applies P2P communication to transfer the intermediate activations and gradients between pipeline stages (except consecutive stages in the same device using local copying). As BitPipe combines bidirectional pipelines together, collective communication (i.e., allreduce) is requisite to synchronize gradients (detailed in Appendix C). This communication can be costly, especially for models with large hidden dimensions and computing clusters with poor interconnection. Under such conditions, maximizing the overlap between computation and communication is a key to achieving higher throughput.\nWe employ eager gradient synchronization (Li and Hoefler 2021) to overlap the all-reduce overhead with computation. As shown in Figure 5(a), an intuitive way for gradient synchronization is to implement the synchronizing step for each stage maintained by the devices after the completion of all the local computations. It is noted that the gradient synchronization for the middle stages (i.e., \\(S_6 S_7\\) and \\(S_2 S_3\\) in Figure 5) is partially overlapped by the computation on the initial and terminal stages (i.e., micro-batch 2 of P1 and micro-batch 4 of P4). To achieve a more profound communication overlap, we initiatively launch allreduce by making use of the bubbles in the pipeline. As shown in Figure 5(b), in the case of BitPipe with four pipeline devices, the gradients synchronization of \\(stage5\\) and \\(stage8\\) is advanced and overlapped by the bubbles and the following computation.\nTo improve communication efficiency, we also explore"}, {"title": "Scale to More Micro-Batches", "content": "For a large mini-batch, the number of micro-batches in an iteration for each device may be more than D (i.e., \\(N>D\\)), especially when the compute resources are limited. Scaling to a large mini-batch, we first use the schedule of D micro-batches in BitPipe as a basic scheduling unit and scale it by concatenating K (\\(K=N/D\\)) basic units together. Figure 7 shows an example with 2D micro-batches per device in a training iteration (i.e., \\(N=2D\\)), which has two basic units (i.e., \\(K=2\\)). The bubbles at the end of the first basic unit can be occupied by the first two forward passes of the second basic unit. The intermediate bubbles can be eliminated by scheduling more forward passes in advance, but at the cost of higher memory usage (detailed discussed in Appendix B).\nExperiments\nExperimental Setup\nHardware. We conduct experiments on a cluster with up to 32 NVIDIA A800 80GB GPUs, where servers are connected by NVIDIA Mellanox 200Gbps HDR Infiniband HCAs and GPUs in a sever are interconnected via NVLink.\nBaselines and Implementation. We compare BitPipe with four synchronous approaches: (a) DAPPLE of the 1F1B"}, {"title": "Main Results", "content": "Pipeline Parallelism Performance. To evaluate the performance of pipeline parallelism separately, the data parallelism size W and pipeline parallelism size D are set to 1 and 8, respectively. To maximize GPU memory usage, the micro-batch size B is set to 4 for BERT-64 and 1 for GPT-96, respectively. The number of micro-batches N in a mini-batch scales from D to 2D and 4D, i.e., the mini-batch size B scales from 32 to 64 and 128 for BERT-64, or from 8 to 16 and 32 for GPT-96.\nMemory Footprint. Figure 8(a) presents the memory footprint distribution (including both activations and weights) of training the two models on 8 GPUs. We observe that: (1) 1F1B-Int and DAPPLE display the most imbalanced memory footprint, as they inject different numbers of micro-batches into each device at the beginning of the pipeline, which results in the highest activations memory consumption on the device responsible for the first pipeline stage. (2) Although having higher average memory consumption due to the stashing of two versions of weights and up to D micro-batches' activations, BitPipe exhibits a nar-"}, {"title": "Ablation Study", "content": "To validate the effectiveness of the V-shaped interleaved schedule and eager gradient synchronization, we compare BitPipe with the following variants:\nBitPipe w/o V: This variant removes the V-shaped interleaved schedule, using the looping schedule of 1F1B-Int.\nBitPipe w/o E: This variant removes the eager gradient synchronization, using default synchronization after the completion of all the local computation.\nTo negate the influence of cross-node communication, the experiments are conducted on a server node with 8 A800 GPUs fully connected with NVLink. Table 5 shows the results of variants on BERT-64, and we observe that: (1) BitPipe outperforms the two variants, suggesting that the V-shaped interleaved schedule and eager gradient synchronization are effective. (2) BitPipe w/o V outperforms BitPipe"}, {"title": "Hyperparameter Study", "content": "To investigate the impact of pipeline parallelism size D and micro-batch size B on BitPipe, we conduct a hyperparameter study on BERT-64 and 32 GPUs. The mini-batch size B is set to 128. Figure 11 presents the results and we observe that: (1) The pipeline parallelism size D is significant to BitPipe, as it controls the communication architecture. Too large or too small D could destroy the mechanism of BitPipe, i.e., using the high-speed NVLink for heavy gradients synchronization and the slow Infiniband for activations communication, resulting in a significant decrease in throughput. (2) BitPipe is also sensitive to the micro-batch size B, and the training throughput increases with the increase of B. This indicates that when memory and communication are not bottlenecks, larger micro-batch size B should be used to achieve higher throughput."}, {"title": "Conclusions", "content": "In this paper, we propose BitPipe, a bidirectional interleaved pipeline parallelism for accelerating large models training. Specifically, a hybrid scheme of fusing interleaved pipelines with bidirectional pipelines is proposed to reduce the computational time of each single micro-batch and multiply the number of devices executing simultaneously. A V-shaped schedule with eager gradient synchronization is introduced to reduce and overlap the communication between devices. Empirical results of training large language models on up to 32 GPU nodes show that BitPipe significantly improves training throughput and memory balance over the state-of-the-art approaches."}, {"title": "Appendix A. Generalize to More Stages", "content": "Although BitPipe can be generalized to incorporate more than two pipelines, which further diminishes the bubbles and balances the activations memory consumption, we do not implement that on account of the expense of weights memory consumption and higher communication overhead. Instead, we choose to generalize the stage number of each pipeline. Theoretically, if each device has v stages (or model chunks), the forward and backward time of a micro-batch for each stage or chunk will be decreased by a factor of v. The size of the pipeline bubble is proportional to this time. As depicted in Figure 12, the pipeline flushing for the same mini-batch size occurs earlier in BitPipe with more stages per pipeline (Figure 12(b))."}, {"title": "Appendix B. Scale to More Micro-Batches with Early Forwarding", "content": "We introduce an early forward scheduling to balance the workload of forward and backward passes, which removes the intermediate bubbles of direct concatenation, as shown in Figure 13(e). By scheduling the first backward pass in each device as soon as possible, the peak activations memory can be maintained at \\(((3D-3)/2)M_a\\), which is lower than that of the scaling methods of Chimera and MixPipe (i.e., \\(2D\\times M_a\\) for Chimera's forward doubling and \\(((3D-2)/2)M_a\\) for MixPipe's K maximizing, respectively), and thus has better device utilization.\nIt can be observed that BitPipe incurs (D-2)/2 bubbles (i.e., \\((D-2)/4\\) bubbles in the forward passes and \\((D-2)/4\\) bub-"}, {"title": "Bubble Ratio", "content": "bles in the backward passes). The total amount of time spent in the pipeline bubble \\(t_{pb}\\) and the ideal processing time for the mini-batch \\(t_{id}\\) can be calculated as follows:\n\\[t_{pb} = \\frac{D-2}{4} (t_f+t_b)\\tag{1}\\]\n\\[t_{id} = N\\cdot (t_f + t_b)\\]\nwhere \\(t_f\\) and \\(t_b\\) are the time to execute a single micro-batch's forward and backward pass, respectively. Given the assumption that \\(t_b\\) is two times of \\(t_f\\), the bubble ratio of BitPipe is:\n\\[bubble\\_ratio = \\frac{t_{pb}}{t_{id} + t_{pb}} = \\frac{D-2}{4N + D-2}\\tag{2}\\]\nwhich is lower than that of BitPipe with direct concatenation (i.e., \\((D-2)/(3N+D-2)\\)).\nTherefore, BitPipe with early forwarding not only has the least bubbles but also exhibits a more balanced and lower peak memory footprint."}, {"title": "Appendix C. Communication Overhead", "content": "The communication overhead of pipeline parallelism in one iteration can be obtained by multiplying the time of a single communication by the number of communications. Table 6 presents the communication overhead of the four pipeline parallelism approaches. For DAPPLE and 1F1B-Int, the communication overhead is primarily the P2P communication to transfer the intermediate activations and gradients between pipeline stages. 1F1B-Int doubles the num-"}]}