{"title": "DIS-CO: Discovering Copyrighted Content in VLMs Training Data", "authors": ["Andr\u00e9 V. Duarte", "Xuandong Zhao", "Arlindo L. Oliveira", "Lei Li"], "abstract": "How can we verify whether copyrighted content was used to train a large vision-language model (VLM) without direct access to its training data? Motivated by the hypothesis that a VLM is able to recognize images from its training corpus, we propose DIS-CO, a novel approach to infer the inclusion of copyrighted content during the model's development. By repeatedly querying a VLM with specific frames from targeted copyrighted material, DIS-CO extracts the content's identity through free-form text completions. To assess its effectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames paired with detailed captions, drawn from films released both before and after a model's training cutoff. Our results show that DIS-CO significantly improves detection performance, nearly doubling the average AUC of the best prior method on models with logits available. Our findings also highlight a broader concern: all tested models appear to have been exposed to some extent to copyrighted content. Our code and data are available at https://github.com/avduarte333/DIS-CO", "sections": [{"title": "1. Introduction", "content": "The rapid evolution of large-scale models has driven a paradigm shift toward multimodality, with recent large vision-language models (VLMs) gaining prominence for their ability to process both visual and textual information (Alayrac et al., 2022; Liu et al., 2023; OpenAI, 2023; Wang et al., 2024). While these models showcase remarkable performance across a variety of tasks, their reliance on vast, diverse datasets introduces challenges in ensuring compliance with ethical and legal standards. Without strict safeguards during the data collection step, proprietary content could be incorporated into the models' knowledge, opening the door to intellectual property infringements and potential legal conflicts (Carlini et al., 2022a; Nasr et al., 2023; Duan et al., 2024). In fact, in the United States alone, more than 24 copyright lawsuits were filed against the AI industry since 2023 (Knibbs, 2024), reflecting growing concerns about the use of protected material in training (Kadrey, 2023; Daily News, 2024).\nDiscovering training data is, therefore, essential for effectively addressing the ethical and legal challenges of model training. However, the lack of transparency in data collection (often justified by competitive concerns) makes it particularly difficult to determine whether specific materials have been used.\nTo tackle these challenges, Membership Inference Attacks (MIAs) serve as a tool to identify whether specific data samples were part of a model's training set. While MIA techniques are well-studied for text-based models, their adaptation to multimodal settings, particularly VLMs, remains less explored - a gap that our work aims to address."}, {"title": "2. Preliminary and Related Work", "content": "Membership Inference Attacks (MIAs) are designed to determine whether a specific data instance was included in the training set of a machine learning model (Shokri et al., 2017; Hu et al., 2022). This area of research has seen growing interest with the increasing use of LLMs, which are known to memorize and occasionally reproduce training data (Nasr et al., 2023; Carlini et al., 2022b; Hans et al., 2024).\nClassical MIAs are typically divided into two main approaches: reference-based and reference-free. Reference-based methods involve training a set of \u201cshadow models\u201d to replicate the target model's behavior (Carlini et al., 2022a; Long et al., 2018; Mireshghallah et al., 2022; Watson et al., 2022). In contrast, reference-free methods rely on calculating specific metrics, such as the perplexity of a sentence, to identify patterns indicative of training set membership (Yeom et al., 2018; Salem et al., 2018; Carlini et al., 2020; Song & Mittal, 2021). Among these, the Min-K%-Prob method stands out as a more refined approach. It hypothesizes that the average log-likelihood of the top-k% least probable tokens in an example is higher if the example was part of the training data compared to if it was not (Shi et al., 2023). Building on this foundation, recent extensions such as Min-K%++ (Zhang et al., 2024b) and DC-PDD (Zhang et al., 2024c) have introduced further improvements. However, a key limitation of most reference-free methods is their dependence on access to token probability distributions, which restricts their interoperability with black-box models such as Gemini (Reid et al., 2024).\nWith recent research shifting focus from text-only models to multi-modal architectures, the task of detecting training data and evaluating model memorization has begun to emerge"}, {"title": "3. Benchmark: MovieTection", "content": "Our proposed benchmark, MovieTection, distinguishes member and non-member data based on a clearly defined temporal constraint. Movies released in 2024 or later are considered non-member data, as they fall outside the knowledge cutoff dates of all tested models. Movies from January to September 2023 are excluded due to uncertainty regarding models' exposure to content from that period. For instance, Qwen2-VL (Wang et al., 2024) reports a knowledge cutoff in June 2023. Movies released on or before 2022 are treated as potential member data, as they are more likely to have been included in the training datasets of such models.\nMovieTection currently comprises frames from 100 movies,"}, {"title": "4. DIS-CO", "content": "Our proposed method, DIS-CO, determines whether examples are memorized by evaluating the model's performance on a question-answering task with free-form text responses. We argue that eliciting free-form completions is preferable to a multiple-choice question-answering (MCQA) format because it significantly reduces the influence of \"luck\" associated with guessing. In multiple-choice settings, models can achieve high accuracy even when responding randomly, as the limited set of predefined options increases the likelihood of selecting the correct answer. In contrast, free-form responses require the model to generate answers without such constraints, making it far less likely for correct responses to arise by chance, even if this comes at the expense of a slightly lower accuracy.\nThe task we propose involves models performing accurate identification of the content's identity which, in the case of copyrighted movie identification, corresponds to correctly identifying the movie title. We operate under the premise that models map a frame to the appropriate title far more reliably when that movie is included in their training data compared to when it is not. The specific prompts used for evaluating models on the MovieTection benchmark are provided in Appendix C.\nThe overall pipeline of DIS-CO is illustrated in Figure 2. After constructing the MovieTection dataset, we first query the models with clean data to establish a baseline for its expected performance on this set. While one might expect the models to fail completely on all these examples, given that these movies were unreleased at the time of the training cutoff, this is not always the case. Some of the movies in this set, though unreleased, were already announced and acknowledged by the models, leading to correct predictions for certain examples (See Table 3 - Section 6.2 and Appendix D). Capturing this baseline performance is crucial to avoid incorrectly classifying a movie as part of the training data simply because some frames were accurately identified.\nAnother important factor to consider is the time effect. In general terms, the older a movie is, the greater the likelihood that a model has residual knowledge about it. This knowledge can come from publicly available online content, such as movie posters, trailers, forum discussions, or datasets like OpenSubtitles (Lison & Tiedemann, 2016), which typically do not raise copyright infringement concerns. To estimate this baseline knowledge accumulated over time, we query the models using the detailed captions from the older movies, as making correct predictions based solely on textual descriptions is unlikely to be problematic.\nFinally, to determine whether a specific movie was likely included in the model's training data, we query the model separately with image frames and then with their corresponding caption information from the suspect movie. After both queries are completed, we compare the predictions from these two inputs. If there is an overlap of correct predictions between the frame-based and caption-based queries, we disregard those results, as they suggest the model did not had to rely on the image content to make accurate predictions. By examining the remaining correct predictions, which rely solely on image content, we infer whether the model is utilizing memorized visual information. Ideally, after removing the intersection, the performance of the suspect movie should fall within this range defined by the baseline performance on the recent movies and the clean baseline performance on older movies with accumulated knowledge over time. However, if performance remains significantly higher than this range, even after removing the intersection, it suggests that the model relied on memorized visual information specific to the movie frames, indicating the movie was likely included in its training data."}, {"title": "4.1. Upper-Bound Estimation of Memorization", "content": "While our proposed approach of removing the intersection between frame-based and caption-based correct predictions provides a more precise set of potentially mem-orized movies, we cannot rule out that those frames are"}, {"title": "4.2. Mitigating the Disclosure of Memorization", "content": "While training on copyrighted data may occasionally be unavoidable, the associated risks can be mitigated by ensuring that the model does not disclose memorization. For a movie that is identified as likely included in the training data, we propose fine-tuning the model on a subset of its frames while replacing the movie label with a neutral designation such as 'Copyrighted Content'. More details on the fine-tuning in Appendix E."}, {"title": "5. Experiments", "content": "We assess the effectiveness of DIS-CO through a range of different experiments, which are guided by the following questions:\n\u2022 Does a longer context reveal more memorization? As LLMs often perform better with more context in their queries, we hypothesize that VLMs behave similarly. Using the MovieTection dataset, we examine the effect of varying the number of frames in the prompt (N \u2208 [1,4]).\n\u2022 Are factors like movie popularity or quality good proxies for memorization? To test whether popularity (e.g., box-office revenue) or quality (e.g., IMDb ratings) are proxies for memorization, we collect movies where one factor varies while the other is controlled. For instance, in the box-office experiment, movies with similar IMDb ratings are chosen to isolate the impact of popularity.\n\u2022 How susceptible are models to memorization when exposed to new data? We investigate the model's ability to memorize new content by fine-tuning it on a movie guaranteed to be outside the training data.\n\u2022 How to prevent a model to disclose memorization? Similarly to the previous experiment, we fine-tune the model (this time on a suspect movie), with a modified labeling objective. This experiment investigates whether this defense mechanism can mitigate memorization disclosure for the suspect movie and whether its effects generalize to other movies.\n\u2022 To what extent does generalization influence the model's performance? Humans are capable of generalizing from partial information, often identifying movies they haven't fully seen by relying on related content such as posters or trailers. To assess how closely models align with humans on this movie detection task, we compare the performance of the models with that of 10 human par-ticipants who were selected to identify 200 images from MovieTection."}, {"title": "5.1. Experiment Setup", "content": "To evaluate DIS-CO, we follow the procedure outlined as follows. Let the \"Suspect\" group be represented as S = {s\u2081,s\u2082,...,sNs} and the \u201cClean\" group as C = {c\u2081, c\u2082,..., cNc}, where Ns and Nc denote the number of movies in each group, respectively. For each movie, we calculate its accuracy: A(si) for si \u2208 S and A(cj) for cj \u2208 C. The accuracy is calculated as the proportion of pre-dictions aligning with the expected outcomes. By default, a weighted average is then applied to account for the unequal proportions of main and neutral frames and the total value is reported. Nonetheless, some results for main and neutral frames are reported individually to provide further insights on the performance across frame types.\nWe then perform a random sampling process with replacement, repeated 10 times. In each iteration, M elements are sampled from each group, where M corresponds to Ns or Nc, depending on the group being sampled. For each iteration, a threshold \u03b8 is optimized to achieve maximum separation between the two groups, and the Area Under the Curve (AUC) is computed.\nTo complete the analysis, we calculate the mean and standard deviation of the AUC or the average accuracy for the \"Suspect\" and \"Clean\" groups over these iterations. Detection is consistently conducted at the movie level, rather than on individual frames."}, {"title": "5.1.1. BENCHMARKS AND BASELINES", "content": "We begin by evaluating DIS-CO using the proof-of-concept dataset VL-MIA/Flickr introduced by Li et al. (2024b). This dataset consists of 600 images, evenly divided into training and non-training based on a temporal split. Member images are sourced from a subset of COCO (Lin et al., 2014), while non-member images are obtained from Flickr, restricted to content published after January 1, 2024. This temporal separation aligns with the knowledge cutoff dates of the models used in our evaluation.\nFor the fine-tuning experiments, we use two movies: IF (2024) and Moana (2016), which have nearly identical durations (1h48min and 1h47min, respectively), allowing us to sample frames at an equal rate, resulting in 6000 frames per movie. The remaining experiments utilize the MovieTection dataset, as detailed in Section 3."}, {"title": "6. Results", "content": "Initially, we evaluate DIS-CO and [DIS-CO\u300d in comparison to baseline methods, focusing on their performance in distinguishing between training and non-training data. For instance, for neutral frames, DIS-CO achieves an average AUC of 0.930, with [DIS-CO\u300d closely following at 0.928, indicating that removing predictions overlapping with captions has minimal impact on detection performance. This finding underscores the robustness of both DIS-CO variants, with [DIS-CO\u300d offering an added advantage by reducing potential biases. Notably, both variants surpass other baselines across AUC metrics, with the R\u00e9nyi method underperforming significantly, yielding an average AUC closer to 0.5.\nNext, we assess the performance of DIS-CO and [DIS-CO] in terms of accuracy. While captions achieve relatively strong AUC values (e.g., 0.858 for neutral frames), their overall accuracy on suspect movies is less compelling. As presented in Table 3, DIS-CO and [DIS-CO\u300d achieve consistently higher average accuracy scores for suspect movies, effectively identifying memorized content with greater reliability. Although MCQA achieves the highest accuracy for suspect movies, it also incorrectly classifies much of the clean data as suspect. This behavior inflates its accuracy which consequently results in a large number of false positives, ultimately lowering its AUC performance,"}, {"title": "6.3. Longer Context", "content": "We evaluate the effect of increasing the number of frames in the prompt on DIS-CO's detection performance. As shown in Figure 4, there is a positive correlation between the number of frames and performance, with the trend closely approximating a linear pattern. Moreover, GPT-40 demonstrates a clear performance advantage, consistently outper-forming Gemini and the two other white-box models. Further results and analysis can be found in Appendix I."}, {"title": "6.4. Popularity and Quality", "content": "We investigate the relationship between memorization and two key factors: movie popularity (box office revenue) and quality (IMDb ratings). As shown in Figures 5 and 6, both factors exhibit a positive correlation with detection performance, albeit with slightly different patterns. Higher box office revenue leads to a consistent improvement across models, with GPT-40 showing the strongest gains. For IMDb ratings, performance generally improves as ratings increase, with a minor U-shaped trend observed at the lower end for GPT-40 and Gemini-1.5 Pro. From a rating of 6 onward, the positive trend becomes more pronounced and consistent across models. These results suggest that both popularity and quality serve as useful proxies for memorization, with each exhibiting unique dynamics that may vary depending on the specific range of the factor being analyzed."}, {"title": "6.5. Preventing Disclosure of Memorization", "content": "The results in Figure 7 validate our premise that fine-tuning a model with an alternate target label can effectively prevent it from revealing its knowledge of a suspect movie.\nThe results from this experiment align closely with those presented in Section 6.1.2. The key insight, however, is that the model learns the task significantly faster, requiring only 500 frames compared to the 1500 frames needed for learning a new movie a 3x reduction in the number of frames needed.\nTo evaluate the generalization capabilities of our approach, we analyze the model's performance on a subset of the MovieTection subset, focusing on the neutral frames."}, {"title": "6.6. Human Experiment", "content": "In this final experiment, our goal is to assess whether some of the performance displayed by DIS-CO could be attributed to generalization capabilities rather than memorization."}, {"title": "7. Conclusions", "content": "In this study, we introduce DIS-CO to analyze the potential inclusion of copyrighted content in VLMs training data, by testing whether models can map movie frames to their titles"}, {"title": "Impact Statement", "content": "This research advances the field of Machine Learning by introducing a method for detecting data used in training vision-language models. Our work primarily serves as an academic reference, contributing to a broader understanding of the presence of copyrighted materials within training datasets, for which our findings may help inform discussions on compliance, attribution, and compensation for content owners. However, while our approach offers a new perspective, we emphasize that its real-world applications should be considered with caution, given the academic nature and limitations of our methodology.\nWe also recognize that the release of the MovieTection dataset may raise ethical considerations related to copyright. However, we argue that the dataset falls within the scope of fair use due to the following reasons:\nFirst, we limit our dataset to 140 frames per title, a small fraction of any full-length film, ensuring minimal redistribution of copyrighted content.\nSecond, the purpose and scope of MovieTection is strictly academic. The dataset is intended solely for research and serves no commercial purpose that could conflict with the interests of copyright holders.\nFinally, we believe that our dataset does not impact the market value of the original films. Since the dataset consists of a sparse collection of individual frames, it does not substitute for watching the films, nor does it reduce demand for legitimate viewings."}, {"title": "H. Additional Main Results", "content": "The additional accuracy results in Table 11 and Table 12 reinforce the trends observed in Tables 2 and 3 from the main text. While GPT-40 consistently achieves the highest performance, the relative ranking of methods remains stable across all models.\n(i) MCQA, once again, demonstrates relatively high accuracy for suspect movies across all models; however, this comes at the cost of a high false positive rate on clean movies. This tradeoff undermines its overall reliability, as it leads to misclassify non-memorized content as suspect.\n(ii) Captions, despite occasionally achieving moderate AUC scores (Table 2), exhibit poor accuracy performance, even in detecting suspect movies. This limitation is most pronounced in models like Qwen2-VL 72B, where caption-based classification of neutral frames results in an accuracy below 10%. Such results suggest that captions alone are insufficient indicators of memorization.\nBy contrast, DIS-CO and [DIS-CO\u300d continue to outperform alternative baselines, demonstrating stronger detection capabilities for suspect movies while maintaining low false positive rates for clean movies. Their consistent superiority across models further underscores their robustness and reliability in identifying memorized content."}, {"title": "I. Long Context - Additional Results", "content": "In the main text, we observed in Section 6.3 a general trend where increasing the number of frames in the prompt led to improved detection performance. Here, we extend this analysis by separately evaluating the impact of the two frame types along the multiple models.\nLarge-Scale Models: From Figure 21 and Figure 22 we observe that, regardless of the frame type, the trend remains: more frames in the prompt consistently lead to better performance. The only key distinction between the two types is that the neutral frames yield lower absolute accuracies. Nonetheless, this is expected given the increased difficulty of detection when using frames that are less informative.\nInterestingly, despite Meta's official recommendation that LLaMA performs best with a single image during inference\u00b3, our results suggest that while the model may not have been explicitly optimized for multi-image inputs, it can still benefit from the extended context in this setting.\nSmaller-Scale Models: These models follow the same pattern observed in Figures 21 and 22. However, their overall accuracy remains lower, which is expected given their smaller size and capacity. Only LLaVA appears to be an exception, as it does not seem to effectively leverage multiple-image inputs, showing limited improvement compared to the other models."}, {"title": "J. Popularity - Additional Results", "content": "In the main text, we observed a general trend where higher box-office revenue correlates with improved detection performance across models (Figure 5). Here, we extend this analysis by separately evaluating the impact of the two frame types along the multiple models.\nLarge-Scale Models: Figures 25 and 26 show that higher box-office revenue consistently improves detection performance, remaining agnostic to the frame type used. Both main and neutral frames follow similar patterns, with the key distinction being that neutral frames yield slightly lower absolute accuracies due to their inherent difficulty. This consistency across frame types confirms that Figure 5 accurately captures the overall trend of the models, despite presenting results based on the grouping of both frame types.\nSmall-Scale Models: Figures 27 and 28 show a much more inconsistent relationship between box-office revenue and detection accuracy compared to larger models. While LLaMA-3.2 11B, shows a noticeable improvement with higher-grossing films, other models, like LLaVA, display erratic fluctuations with less clear trends."}, {"title": "K. Quality - Additional Results", "content": "In the main text, we observed that higher IMDb ratings generally led to improved detection performance across models (Figure 6). Here, we extend this analysis by separating the main and neutral frame types and evaluating performance across both large-scale and smaller models.\nLarge-Scale Models: Figures 29 and 30 reveal an overall upward trend in detection performance as IMDb ratings increase. However, an interesting U-shaped pattern is noticeable, particularly in main frames, where detection accuracy initially drops for lower-rated movies (around Rating\u2208[4,5]) before rising sharply from Rating=6 onward. In contrast, neutral frames display a more gradual improvement without the same dip at low ratings. Only Gemini-1.5 Pro, unexpectedly, shows a sharp drop at Rating=9, deviating from the otherwise consistent trend.\nSmall-Scale Models: Figures 31 and 32, on the other hand, show that overall performance remains weak across most rating levels, with a notable exception in Rating=8, where most models exhibit a sudden increase in accuracy, though the reason for this improvement is unclear."}, {"title": "L. Time Effect on MovieTection", "content": "The proposed temporal split of MovieTection was well suited for the tested models, but as new models emerge, the current suspect/clean split assumption may no longer hold. To explore this, we tested a newer model (Gemini-2.0 Flash) on the clean MovieTection data to assess whether it has started acquiring knowledge of these movies.\nFrom Figure 33, we see that while Gemini-1.5 Pro struggles with identifying clean movies, achieving an accuracy of only 0.01, Gemini-2.0 Flash shows a nearly 10x increase, reaching 0.078. Although these values remain low and do not suggest that most movies in the split were seen by the new model, individual inspection of the results indicates that some titles might raise suspicion. In fact, with Gemini-1.5 Pro, Bob Marley: One Love scores 0.1, but with Gemini-2.0 Flash, the same movie reaches 0.69."}]}