{"title": "Accelerating AI Performance using\nAnderson Extrapolation on GPUs", "authors": ["Saleem Abdul Fattah Ahmed Al Dajani", "David E. Keyes"], "abstract": "We present a novel approach for accelerating AI performance by leveraging An-\nderson extrapolation, a vector-to-vector mapping technique based on a window of\nhistorical iterations. By identifying the crossover point where a mixing penalty is in-\ncurred, the method focuses on reducing iterations to convergence, with fewer more\ncompute-intensive but generally cacheable iterations, balancing speed and memory\nusage with accuracy and algorithmic stability, respectively. We demonstrate signif-\nicant improvements, in both training and inference, motivated by scalability and\nefficiency extensions to the realm of high-performance computing (HPC).", "sections": [{"title": "1 Introduction", "content": "Anderson extrapolation [1, 2, 16, 27, 33] has recently been applied to deep equilibrium models\n(DEQs) [7-10, 17, 24]. Kolter et al. [34] found the gains not substantial due to early termination with\na loose convergence tolerance. They focused on Anderson extrapolation during training. Here, we\nshow significant acceleration of AI performance with Anderson on GPUs for both the forward pass\n(running inferences faster) and training (generating models faster). We demonstrate acceleration of\nthe forward pass with standard Anderson as a baseline for future work with stochastic variants [30]\nand accelerating the backward pass with Jacobian-free methods like Jacobian-Free Backpropagation\n(JFB) and Neumann series gradient approximations [16].\nAs AI demand grows, as shown in Fig. 2 [3, 15, 25, 28], high-performance computing (HPC)\nis becoming critical due to economic pressures from the growth of data and AI infrastructure [29].\nLow-memory acceleration techniques, like Anderson extrapolation, will be key to increasing HPC-\nbased AI computational efficiency. This study investigates matrix-free Anderson extrapolation on\nGPUs, emphasizing gains from advanced computing architectures compared to CPUs. Our goal is\nto maximize computational efficiency while reducing iterations to convergence by reusing previous"}, {"title": "1.1 Leveraging extrapolation for AI and HPC advances", "content": "Anderson extrapolation, a windowing technique for accelerating nonlinear fixed point iterations,\nis widely applied in fields like density functional theory, kinetic theory, and climate spin-up. It\nis well-suited for distributed memory parallelization and GPU implementation. It is a staple of\nmajor open-source large-scale solver libraries, including PETSc [11, 12], SUNDIALS [23], Trilinos\n[19-22], and deal.II [4\u20136, 13]. It can be applied to machine learning training, smoothing out standard\nforward iterations and achieving superior accuracy in training and testing error. Benchmarking results\non CIFAR10 show expected robustness benefits and allow characterization of the temporal advantages\nor disadvantages from the higher cost per iteration, where a small residual minimization step is\napplied at each new function evaluation.\n$z_{k+1}=f(z_k, x), z_k \\in R, k = 0, 1, 2, ...$\n$r_k(x) = z_k - z(x_k), w(k) = \\frac {r_i{k-ri_{k-1}}} {||r_i{k} - V_{ik-1}||^2}$\n$z_{k+1} = f(Z_kx) + \\sum_{4}^{min(k,m)}w(f(x)-f(z_{k-1,x})) $"}, {"title": "1.2 Balancing memory and stability", "content": "Fundamental tradeoffs exist between memory capacity, memory bandwidth, communication cost,\nand algorithmic characteristics of stability and convergence rate. The tradeoffs are generally resolved"}, {"title": "1.3 Deep equilibrium neural network models", "content": "Deep equilibrium models (DEQs) are the continuum limit of explicit neural networks as the number\nof layers approaches infinity [26], approximating many explicit layers with a single, implicit layer\nwith exponentially fewer parameters using a backward pass including the output. This reduces\nthe inverse problem in parameter space to a fixed-point iteration problem, enabling the usage of\nnonlinear, vector-to-vector mapping techniques to compute the fixed-point iterations that converge\nto the deep equilibrium state parameters by minimizing the loss function. With gains in memory\nand acceleration, DEQs are fit for large-scale computer vision and natural language processing tasks\nand benefit more from matrix-vector operation-optimized computing architectures like GPUs and\nCPU-GPU superchips.\nThe standard approach using forward iteration for fixed-point iteration problems often does\nnot efficiently converge to the fixed point and suffers from initially slow error reduction and local\nminimum trapping in nonlinear problems like deep neural networks. Vector-to-vector mapping tech-\nniques like Anderson extrapolation outperform standard forward iteration by combining information\nfrom previous iterations to span a searchable subspace to extrapolate the next iteration, enhancing\nconvergence rates at the expense of memory usage in each iteration.\nDEQs represent any neural network at arbitrary depths and connectivities with a single implicit\nlayer consuming vastly fewer parameters with faster forward passes for accelerated training and\ninferences. The implicit function theorem shows how gradients can be computed in the DEQ\nframework, facilitating backpropagation through the equilibrium state [8, 34].\nDEQs provide a framework for accelerating deep learning, extending the capacity of deep\nnetworks within a single-layer architecture through fixed-point computations and advanced root-\nfinding algorithms. Their amenability to convergence acceleration with techniques like Anderson\npositions DEQs as a robust method to reduce computation needed to build state-of-the-art models\nand scale up beyond current computational limitations."}, {"title": "2 Methods", "content": "This work demonstrates Anderson extrapolation to accelerate AI performance algorithmically without\nincreasing processors. Since it is a vector-to-vector mapping technique, it benefits from hardware\noptimized for uniform operations, like GPUs. We benchmark Anderson acceleration against standard\nforward iteration on GPUs and CPUs."}, {"title": "2.1 Mathematical formulation", "content": "Fixed-point acceleration starts with the fixed point iteration formula $z^* = f(z^*, x)$. Forward iteration,\n$zk+1 = f(zk,x)$, moves step-wise towards this fixed point.\nAnderson acceleration uses a linear combination of prior iterates, $z_{k+1} = \\sum_{i=1}^m \\alpha_i f(z_{k-i+1},x)$,\noptimizing $a_i$ to minimize the residual norm, $\\frac {|| f(z_k,x)-zk||^2} {||f(z_k,x)||^2+1}$, leading to faster convergence. The\ncoefficients must sum to unity:\n$\\text{minimize}_\\alpha ||G\\alpha||^2, \\text{subject to } 1^T\\alpha = 1$  (1)\nThe matrix G is defined as:\n$G = [f(z_k,x) \u2013 z_k,..., f(z_{k-m+1},x) \u2013 z_{k-m+1}]$  (2)\nThe Lagrangian incorporating the equality constraint is:\n$L(\\alpha, \\nu) = ||G\\alpha||^2 \u2013 \\nu(1^T\\alpha \u2212 1)$  (3)"}, {"title": "2.2 Dataset description, compute environment, and training details", "content": "The CIFAR10 dataset, with 60,000 32x32 labeled images in 10 classes, is used for supervised learning\nand image classification tasks. Accuracy is the ratio of correctly predicted labels to the total images,\nusing cross-entropy loss.\nHigh-dimensional tensors in standard PyTorch format are used. The compute environment\nincludes Google Colab Pro with NVIDIA Tesla V100 GPUs and Intel Xeon CPUs. Training uses\ndefault hyperparameters from Kolter et al. [34] for comparison with prior results [7\u201310, 17, 24], with\nAnderson parameters m = 5 and \u03b2 = 1."}, {"title": "2.3 Deep neural networks, deep equilibrium models, and fixed Point equations", "content": "Traditional neural networks use layer-wise transformations:\n$z_1 = X$\n$z_{i+1} = \\sigma(W_iz_i + b_i), i = 1, . . ., k \u2212 1$\n$h(x) = W_kz_k + b_k$\nDEQs model a network as an infinitely deep system, finding a fixed point $z^*$ that satisfies:\n$z^* = \\sigma(Wz^* + Ux + b)$  (6)\nHere, W, U, and b are shared across all layers, and o is the activation function. Solving for $z^*$\navoids computing individual layers, reducing computational cost."}, {"title": "2.4 GPU Optimization and Parallelization", "content": "GPUs, suited for uniform tasks with high throughput, map well with Anderson acceleration. This\nwork combines suitable algorithms with appropriate architecture to enhance performance without\nupgrading hardware or using more processors."}, {"title": "3 Results", "content": "This work demonstrates that Anderson extrapolation has a higher cost per iteration, measured\nin function evaluations or epochs. The main benefit is that Anderson extrapolation exhibits less\nfluctuation in accuracy, as seen in the test accuracy, whereas forward iteration shows more significant\nups and downs in both training and testing accuracy, potentially indicating overfitting. Anderson\nacceleration reaches a higher accuracy plateau for both training and test datasets, suggesting better\ngeneralization capability."}, {"title": "4 Discussion", "content": "These results show that Anderson extrapolation can train DEQ networks to higher accuracy than\nforward iterations and reach a given high accuracy in less time. Anderson extrapolation is also\nefficiently implementable in GPU programming environments, utilizing memory austerity and\noperational uniformity attributes similar to the forward algorithm. For large-scale neural network\ntraining problems requiring distributed memory, this study motivates porting and testing on state-of-\nthe-art GPU architectures, CPU-GPU superchips, and emerging computing hardware.\nGPUs have been shown to accelerate Anderson extrapolation beyond what could be achieved with\nstandard forward iterations or with Anderson on CPUs. This is notable before reaching the 'crossover\npoint,' the trade-off between computation speed and accuracy, illustrated in Fig. 1 and Fig. 6. The\n'mixing penalty' due to the additional computational cost associated with Anderson acceleration is\noffset by the parallel processing capabilities of GPUs, enabling faster convergence than with CPUs or\nstandard forward iterations alone.\nThe increase in time per iteration with Anderson arises from the residual minimization process\nduring each acceleration step. The higher plateau for accuracy with Anderson compared to forward\niteration suggests more robust learning when taking previous iterations into account. Monitoring the\nslowing of Anderson acceleration and switching to approximate forms of Newton's method (e.g.,\nquasi-Newton, modified Newton, or inexact Newton) can be beneficial.\nThe unstable behavior with forward iteration necessitates lower learning rates and more epochs\nfor training, increasing the time needed to reach the same accuracies achieved with Anderson by up"}, {"title": "5 Conclusion", "content": "The integration of Anderson acceleration within deep learning workflows presents substantial im-\nprovements in computational efficiency, accuracy, and generalizability of implicit neural networks.\nPorting and parallelizing vector-to-vector mapping techniques onto emerging CPU-GPU hybrid\narchitectures holds promise. The accuracy and speed of deep equilibrium neural network training and\ninferences could be improved further, making them more viable for real-world applications beyond\nthe classification task demonstrated herein. Based on investigations of explicit and implicit memory\nrequirements [26], optimizations based on an Anderson-accelerated, fixed-point iteration implicit\nmemory approach [34] are effective in memory-intensive computer vision processing, reducing\nmemory and bandwidth consumption without compromising performance [26].\nThese methods applied to implicit neural networks, particularly DEQs, reveal new directions for\nAl research, such as exploring further acceleration gains from stochastic variants of Anderson extrap-\nolation [30]. Exploiting the continuum limit of infinite explicit layers in implicit networks reduces\nmemory usage and achieves favorable performance trade-offs [8], where gradient approximations,\nsuch as truncated backward gradient for backpropagation [16, 24], can be applied for even more\nacceleration."}, {"title": "6 NeurIPS Limitation and Broader Impact Statements", "content": "These results do not comprehensively search the Anderson hyperparameter space, nor do they\nestablish the multiprocessor scalability at which they are aimed. Saving training and inference time\nand energy is the broader impact envisioned for this work. Being algorithmic in nature, it has the\nsame potential for applied use and misuse as neural networks in general."}]}