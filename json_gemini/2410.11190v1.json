{"title": "Mini-Omni2: Towards Open-source GPT-4o Model\nwith Vision, Speech and Duplex", "authors": ["Zhifei Xie", "Changqiao Wu"], "abstract": "GPT40, an all-encompassing model, represents a milestone in the development of\nmulti-modal large models. It can understand visual, auditory, and textual modali-\nties, directly output audio, and support flexible duplex interaction. However, its\ntechnical framework is not open-sourced. Models from the open-source community\noften achieve some functionalities of GPT4o, such as visual understanding and\nvoice dialogue. Nevertheless, training a unified model that incorporates all modal-\nities is challenging due to the complexities of multi-modal data, intricate model\narchitectures, and training processes. In this paper, we introduce Mini-Omni2, a\nvisual-audio assistant capable of providing real-time, end-to-end voice responses\nto user video and voice queries, while also incorporating auditory capabilities. By\nintegrating pretrained visual and auditory encoders, Mini-Omni2 maintains strong\nperformance in individual modalities. We propose a three-stage training process to\nalign modalities, allowing the language model to handle multi-modal inputs and\noutputs after training on a limited dataset. For interaction, we introduce a semantic-\nbased interruption mechanism, enabling more flexible dialogues with users. All\nmodeling approaches and data construction methods will be open-sourced. To the\nbest of our knowledge, Mini-Omni2 is one of the models closest to GPT40 in\nfunctionality, and we hope it can offer valuable insights for subsequent research.", "sections": [{"title": "1 Introduction", "content": "GPT-40 represents a milestone in the development of multi-modal large models,\nparticularly evident in three aspects: (1) its powerful capabilities in multi-modal question answering;\n(2) its ability to transcend traditional text-based input and output, enabling the understanding and\ngeneration of multi-modal content; and (3) its flexible interaction mode with interruption mechanisms,\nwhich facilitates a more natural and fluid human-computer interaction. However, the GPT-40 model\nis not open-sourced to the public, and its technical specifications remain undisclosed. To date,\nmainstream methods predominantly involve employing various pre-trained encoders to obtain textual\noutputs for specific modalities, such as visual and audio inputs, and utilizing model cascading tech-\nniques to integrate a text-to-speech (TTS) module that replicates GPT-40's speech output capabilities,\nthereby simulating its multi-modal functionalities. Achieving end-to-end multi-modal understanding\nand output remains a challenging task.\nRecently, as the capabilities of large models such as Llama3.1 continue to expand,\nresearchers have begun to explore multi-modal approaches to achieve the performance of GPT-\n40. However, these research outcomes often focus only on specific functionalities of GPT-40,\nsuch as vision-text understanding (LLava, Flamingo), audio\ncomprehension (Qwen2-audio), multi-modal understanding (Vita), and direct streaming dialogue output in audio (Mini-Omni, Llama-Omni, Moshi).However, currently, integrating text, vision, and speech\nmodalities remains challenging.\nIn our view, the current challenges in achieving interaction across three modalities involve the fol-\nlowing aspects: (1) Model capability GPT-40 requires a unified model that comprehensively\nunderstands all modalities while maintaining robust performance across a range of tasks; (2) direct\ninference output capabilities in multi-modal contexts our recent work has addressed how to\nenhance the model's streaming output abilities in audio, laying the groundwork for Mini-Omni2's\nvoice interaction capabilities; (3) substantial data requirements - training for GPT-40 necessitates\nthe integration of data across visual, audio, and textual modalities, with quantities increasing expo-\nnentially compared to previous efforts; (4) the design of flexible interaction methods GPT-40's\nfull-duplex capability is also a notable feature.\nIn this paper, we introduce Mini-Omni2 as a continuation of Mini-Omni, employing a single model\nto end-to-end simulate the visual, speech, and textual capabilities of GPT-4o, enhanced by a unique\nsemantic interruption mechanism. Consistent with Mini-Omni, we retain Qwen2 as\nthe foundational model, leveraging this compact architecture to achieve comprehensive multi-modal\nunderstanding and real-time streaming speech inference across the three modalities. Furthermore, we\nenable the model to receive external audio inputs in real time, simulating its \"auditory\" perception\nand controlling the speech output stream based on content semantics. The model architecture of\nMini-Omni2 is illustrated in Figure 1. As an end-to-end model, we enhance data utilization efficiency\nand demonstrate the generalizability of the Mini-Omni2 algorithm by directly employing the classic\npre-trained visual encoder CLIP and the encoder component of the speech\nrecognition model Whisper as feature extractors for visual and audio inputs. The\nfeatures from the pre-trained encoders and the text embeddings are concatenated to form the model's\ninput. Due to challenges related to understanding capabilities, we did not adopt a token-in-token-out\nparadigm. Moreover, the model is capable of reasoning in both speech and text modalities, utilizing a\ndelayed parallel output approach for text and audio, achieving performance consistent with GPT-40.\nIn Mini-Omni2, we propose an efficient training approach based on a limited amount of data, aiming\nto enable the model's training methods to assist other multi-modal models in modality expansion.\nThus, we avoided blindly expanding the dataset exponentially and instead sought to develop a multi-\nmodal extension method using minimal new data. We employed a three-phase training process\nfor modality expansion, alignment, and joint training. Initially, the Mini-Omni2 model underwent\nadapter training using speech recognition and image description datasets without inference logic,\nthereby broadening the scope of multi-modal understanding. Next, Mini-Omni2 was trained for\ntext output in question-answering tasks across modalities, allowing the adapter-based output features\nto align with text embeddings for effective question answering. In the third phase, we focused on\nmulti-modal output expansion by incorporating audio output and training for auditory capabilities\nlike interuption."}, {"title": "2 Mini-Omni2", "content": "With respect to the model's capabilities in voice interaction, Mini-Omni2 continues to utilize the\nSNAC tokenizer, trained with reconstruction loss at a music synthesis level, to\nensure high-quality speech output. However, based on our observations, we believe that the current\nfull-duplex training is still not sufficiently stable. Additionally, we find that the model's ability to\ninterrupt human interactions is quite limited. Therefore, we contend that interruptions based on input\nsemantic information are essential for achieving stable and flexible human-machine interaction. We\nenable the model to perform real-time encoding of its received \"auditory\" waveforms using SNAC,\ngenerating tokens that allow it to control its own output during each generation. As a demonstration,\nwe construct data using the phrase \"stop omni,\" employing frame-level irq and n-irq special tokens\nto control the model while adopting a streaming approach for data construction. The training dataset\nand generation scripts will be open-sourced in the community.\nTo evaluate the multi-modal interaction capabilities of Mini-Omni2, we first tested its performance\non traditional visual and auditory tasks, verifying that the model maintains consistency with the\noriginal model in basic tasks such as image and speech recognition. Next, we will compare the\nmodel's text capabilities with the qwen2 model to validate our approach's effectiveness in preserving\nthe model's original abilities. Finally, we conducted a series of additional experiments to test the\nmodel's response speed, compare multitasking effects, and perform some case studies.\nIn summary, we make the following contributions:\n\u2022 We introduce Mini-Omni2, the first open-source large multi-modal model with capabilities in\nvision, speech, text, and an auditory interruption mechanism. To the best of our knowledge, it is\nthe most functionally similar end-to-end model to the current GPT-40.Figure 2 shows the demo of\nthe model as a video voice assistant.\n\u2022 To address the exponential increase in data volume associated with cross-modal input-output,\nwe propose a novel training pipeline based on the modal expansion method from the previous\nMini-Omni. This pipeline encompasses three training phases, allowing the text model to first\nexpand and align responses to multi-modal inputs, and ultimately extend outputs to the speech\nmodality in the final phase, employing a delayed parallel generation algorithm for real-time speech\noutput.\n\u2022 We explored a semantic-based interruption method, utilizing streaming tokens as input and con-\nstructing training data to enable the model to control its audio output stream based on external\nsemantic cues.\n\u2022 The datasets used by Mini-Omni2 are all publicly available, and all generated data, along with the\ngenerated scripts (semantic interruption data), will be open-sourced."}, {"title": "3 Mini-Omni2", "content": "The model architecture of Mini-Omni2 is illustrated in Figure 1. In addition to the text embedding\nmodule, Mini-Omni2 employs the visual component of CLIP and Whisperv3-Small as encoders\nfor visual and auditory modalities, resulting in highly efficient data utilization during training and\nminimizing extensive pre-training efforts. Additionally, Mini-Omni2 features real-time encoding of\nauditory task generation control flows, providing greater flexibility in model interactions. This section\nincludes 3.1, which discusses the model architecture; 3.2, which presents the modeling methods for\ninput and output streams; and sections 3.3 and 3.4, which detail training methods and interuption\nmanner, respectively."}, {"title": "3.1 Architecture", "content": "Visual Encoder - We utilize the visual component of CLIP, specifically the ViT-B/32 model, as the\nvisual encoder, which converts incoming images into 224 x 224 pixel format, generating a feature\nsequence of length 49 and a consolidated feature sequence of length 1. Mini-Omni2 concatenates\nthese to form a raw feature sequence of length 50, employing a single-layer LlamaMLP as the model adapter. The model extracts key frames from the video stream at a fixed\nfrequency, providing these as visual streaming input, which subsequently produces outputs from the\nspeech model.\nAudio Encoder - In the encoder section, we continue our previous work by using the Whisperv3-\nSmall model as the audio input encoder. We opted not to adopt a token-in-token-out modeling\napproach for audio input and output for two reasons. (i) Strong semantic alignment in speech\nrecognition. The Whisper model, proposed by OpenAI, is trained on thousands of hours of datasets,"}, {"title": "3.2 Multimodal Languague Modeling", "content": "Multimodal Modeling - Consider $Y = (y_i \\in V_{txt} | i = 1,..., t_{txt})$ as a text utterance from a\nvocabulary $V_{txt}$ with length $t_{txt}$. The probability of Y can be expressed as $p(Y) = \\Pi_{i=1}^{t_{txt}} P(y_i |\nY_1,..., Y_{i-1})$. Now, when dealing with a continuous speech signal, we can convert it into discrete\nspeech tokens (dst), represented as $D = (d_i \\in V_{dst} i = 1,..., t_{dst})$ using a tokenizer. In this\ncontext $V_{dst}$ is the vocabulary of discrete speech tokens. These discrete speech tokens can be treated\nas spoken language within $V_{dst}$ and modeled in a manner similar to text. We combine text and\nspeech in a new vocabulary $V_{voxt}$ by $V_{voxt} = V_{txt} \\cup V_{dst}$. Additionally, we introduce video features\n$V = (V_i \\in F_{vis} | i = 1,...,t_{vis})$, where $F_{vis}$ represents the continuous features extracted from\nthe video. Therefore, we can model the probability of both speech, text, and video features as\nZ, where Z = $(z_i \\in V|i = 1,\u2026,t)$. This probability is expressed as $p(Z) = \\Pi_{i=1}^{t_{vis}}P(z_i |\nZ_1,\u00b7\u00b7\u00b7, Z_{i\u22121}, U_1, ..., U_{t_{vis}})$, where Z represents discrete speech tokens D($V = V_{dst}$), text tokens\nY($V = V_{txt}$), and continuous video features V($F = F_{vis}$), or various combinations of Y, D, and\nV. For the audio and text tokens generated simultaneously, the negative log-likelihood loss can be\nformulated as in Equation (1).\n$L(T, A, VIC) = \\Sigma_{j=1}^m \\Sigma_{i=1}^{n_j} log P(T_{i,j}, A_{i,j}|T_{<i,j}, A_{<i,j}, V"}, {"title": "3.3 Training Strategies", "content": "In this section, we will introduce the training phase of the Mini-Omni2 model. The overall training\nprocess of Mini-Omni2 is illustrated in Figure 5. The training process is divided into three stages,\nwith multitask training employed in each stage. In the figure, except for Stage 1, a foundational\ntext-to-text task is additionally incorporated but not explicitly depicted. We categorize the entire\ntraining process into three stages:\n\u2022 Multimodal Encoder Adaptation In the first stage, we employ a rapid, small-scale training focused\nsolely on the weights of the linear layer connecting the language model and the encoder. The"}, {"title": "3.4 Semantic Interruption", "content": "We believe that a real-time conversation model needs to be interruptible by humans in order to enable\nmore flexible interactions. However, this interruption mechanism should not be a simple VAD (Voice\nActivity Detection)-based one, but rather a system that can determine whether the user intends to\ninterrupt the model. Additionally, the model's training in this aspect should be highly robust, capable\nof handling various external situations (e.g., noise, other conversations, and unrelated sounds).\nBackground Noise Selection: (1) We randomly utilized a variety of speech recognition samples\nfrom the Libri-tts dataset as the original human noise data samples. (2) We employed samples from\nthe MUSAN dataset, which includes music, human voices, white noise, and urban noise.\nSemantic Interruption Construction: We synthesized \"Stop Omni\" phrases with random voice\ntimbres, which were subsequently mixed with noise. The specific data construction methods are\nintroduced in the next section.\nCombining the aforementioned data, the model will receive long sequences of data containing \"Stop\nOmni\" phrases amidst various noises. The model will generate two types of state tokens in real time:\nirq and n-irq, representing the intention of the user to interrupt and not to interrupt, respectively. Each\ntime the model generates an audio token, it will predict a new task; when the irq token is output,\nit will interrupt the audio stream writing process. For this task, we use tokens as input to enhance\nthe model's real-time processing capabilities. We observed the interruption mechanism in Vita and\nproposed a method for integrating new tasks in batch form, allowing a unified model to control itself\nin real time, thereby reducing memory overhead by half compared to a dual-model approach."}, {"title": "4 Data and Evaluation", "content": "In this section, we introduce the data used for training Mini-Omni2 and present some initial evaluation\nresults. We will provide a more detailed explanation of the composition and construction process of\nthe data for each modality. In the experimental results section, we only showcase a few application\ncases and basic capability assessments. More comprehensive experiments will be included in an\nupcoming technical report, which will be released shortly."}, {"title": "4.1 Datasets", "content": "The training data for the Mini-Omni2 model is primarily sourced from five components, as shown\nin Table 1. (1) Textual Question-Answering Data: Throughout all training stages, whenever the\nlanguage model weights were unfrozen for training, textual question-answering data was included to\nmaintain the model's reasoning ability. We used the first 1.5 million question-answer pairs from the\nOpen-Orca dataset. (2) Speech Recognition Data: Speech recognition data was used to continuously\nmaintain the model's semantic understanding of external spoken input. We primarily utilized the\nLibriTTS, VCTK, and LibriSpeech datasets. (3) Spoken Question-Answering Data: We did not\nuse a standalone spoken dataset; instead, synthetic data was employed for training. The spoken\nquestion-answering data was derived from the Moss-002-sft dataset. (4) Image Question-Answering"}, {"title": "4.2 Training Parameters", "content": "The Mini-Omni2 model completed all training steps on eight A100 GPUs. During the adapter\ntraining stage, learning rates ranged from 2e-5 to le-3, while training the language model used\nlearning rates between 2e-6 and 2e-4. The final fine-tuning was conducted with learning rates ranging\nfrom 2e-6 to 2e-5. A cosine scheduler was employed, with 1,500 warm-up steps and a global batch\nsize of 192. Each stage was trained for one epoch using the full dataset. The scale of the vision and\naudio encoders was described earlier, and the language model used was the Qwen2-0.5B instruct\nversion, without a TTS adapter. All model adapters used Llama-MLP with an intermediate size of\n4,864."}, {"title": "4.3 Data Construction", "content": "Spoken Dialogue Data: We used our speech recognition dataset as a random voice timbre library.\nTo ensure robust training, a random sample from this dataset was selected as a voice prompt for the\ninput of all spoken dialogue data, and Cosy Voice was employed for zero-shot speech\nsynthesis. For the output of all question-answering data, the same voice timbre was used.\nInterruption Data: First, the noise data is stream-encoded and decoded to simulate real-time\nstreaming input to the model. Then, a random segment of the noise data is extracted. At the end of\nthis segment, a \"Stop Omni\" phrase is inserted, generated with a random voice timbre in the same\nmanner as the dialogue data. Finally, an additional \"tail\" of 0-10 seconds is appended to the end of\nthis segment. In terms of labeling, all data before the tail is labeled as \"n-irq,\" while the tail segment\nis labeled as \"irq,\" indicating that the model should be interrupted by a human.\nIdentity Data: We replaced all instances of \"Moss\" in the Moss-002-sft dataset with \"Omni\" and\nadded 1,000 identity-related response training samples.\n(Visual Assistant Data - Coming soon): We found that the currently available visual question\nanswering datasets tend to have very lengthy answers and a very formal tone, which are not suitable\nfor the tone of an assistant. We are building and open-sourcing a new visual QA dataset with a more\nrelaxed and pleasant tone."}, {"title": "4.4 Experimental Results", "content": "In the experimental section, we are comprehensively evaluating the basic capabilities of Mini-Omni2\nin speech understanding, image understanding, and text question answering. At the same time, we\nwill test the model's latency and synthesized speech quality, etc. This is a series of experiments and\nstill requires some time. We will release it in a very quick second version. Currently, we provide the"}, {"title": "4.5 Case Study", "content": "Here we present some real response examples from Mini-Omni2."}, {"title": "5 Limitations", "content": "For the current Mini-Omni2 model, we believe the following aspects are worth exploring and\nimproving: 1. The continued expansion of model size and data scale. Mini-Omni2 aims to train\nsmall models with limited resources, and we believe that scaling laws can greatly enhance its\ncapabilities. 2. More powerful encoders and base models. 3. Using multi-modal token-in-token-out,\nabandoning the pre-trained encoder to enable extensive multimodal pre-training. 4. Control over TTS\nfor voice output (emotion, naturalness, and timbre). 5. A richer mechanism for semantic interruptions."}, {"title": "6 Conclusion", "content": "In this paper, we present Mini-Omni2, a unified multi-modal model with capabilities in text, speech,\nvision, end-to-end streaming audio output, and duplex interaction. Our goal is to reproduce an\nopen-source GPT-40 model, and to our knowledge, our work is also one of the closest in terms of\nfunctionality. We use multiple pretrained encoders as the vision and speech encoders and align them\nwith the language model to extend the modalities.Furthermore, we propose a three-phase modality\nalignment and expansion training process to achieve the desired capabilities of the model while\navoiding an exponential increase in data volume. We also explore a robust method for semantic\ninterruption modeling and introduce our data construction and interruption logic. Finally, we will\ncontinue to improve the model's performance and create multi-modal data resembling vision-speech\nassistants. All models and datasets will be open-sourced, and we hope Mini-Omni2 can serve as a\nreference for future research."}]}