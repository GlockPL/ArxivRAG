{"title": "MIMICKING HUMAN INTUITION: COGNITIVE BELIEF-DRIVEN Q-LEARNING", "authors": ["Xingrui Gu", "Guanren Qiao", "Chuyi Jiang", "Tianqing Xia", "Hangyu Mao"], "abstract": "Reinforcement learning encounters challenges in various environments related to robustness and explainability. Traditional Q-learning algorithms cannot effectively make decisions and utilize the historical learning experience. To overcome these limitations, we propose Cognitive Belief-Driven Q-Learning (CBDQ), which integrates subjective belief modeling into the Q-learning framework, enhancing decision-making accuracy by endowing agents with human-like learning and reasoning capabilities. Drawing inspiration from cognitive science, our method maintains a subjective belief distribution over the expectation of actions, leveraging a cluster-based subjective belief model that enables agents to reason about the potential probability associated with each decision. CBDQ effectively mitigates overestimated phenomena and optimizes decision-making policies by integrating historical experiences with current contextual information, mimicking the dynamics of human decision-making. We evaluate the proposed method on discrete control benchmark tasks in various complicate environments. The results demonstrate that CBDQ exhibits stronger adaptability, robustness, and human-like characteristics in handling these environments, outperforming other baselines. We hope this work will give researchers a fresh perspective on understanding and explaining Q-learning.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement learning (RL) algorithms aim to learn optimally rewarding behaviors by modeling how an agent acquires optimal strategies through a trial-and-error process within an environment (Sutton & Barto, 2018; Sutton et al., 1999). Although reinforcement learning has achieved significant success in areas like gaming, autonomous driving, and robotics, current algorithms continue to encounter challenges in addressing decision-making issues within complex, dynamic, and uncertain environments (Wu et al., 2024; McAleer et al., 2024; Xu et al., 2020; Watkins & Dayan, 1992; Silver et al., 2016; Mnih et al., 2015; Zhang et al., 2024; Mao et al., 2020; 2022; Guss et al., 2021).\nQ-learning, a cornerstone of model-free reinforcement learning (Watkins & Dayan, 1992; Watkins, 1989; Barto et al., 1989), along with its variants like Double Q Learning, has sought to improve learning by minimizing the mean squared Bellman error (MSBE). However, these methods often encounter challenges such as pessimistic value estimates and theoretical limitations (Ren et al., 2021; Hasselt, 2010; Hui et al., 2024), and they frequently fail to address the fundamental reliance on maximal value estimates (Fujimoto et al., 2018).\nTo overcome these limits, we attempt to solve the problem using a novel approach: Cognitive Science, often seen as a manifestation of human intuition. In this domain, humans typically construct and adjust mental models' subjective beliefs when confronted with uncertainty to predict future events and make corresponding decisions (Peterson & Beach, 1967; Hastie & Dawes, 2009; Gigerenzer et al., 1991). These mental models, grounded in the cognition and experience of the"}, {"title": "2 RELATED WORKS", "content": "The development of RL can be broadly categorized into two main directions: mathematical optimization and learning process simulation. Both approaches stem from the concept of learning from delayed rewards, originally proposed by (Watkins, 1989)."}, {"title": "2.1 ADVANCEMENTS MATHEMATICAL OPTIMIZATION IN Q-LEARNING", "content": "Despite efforts to address overestimation bias, Double Q-Learning (Hasselt, 2010) only partially reduces maximization bias and may still cause underestimation in noisy environments, potentially leading to convergence to near-optimal rather than optimal solutions (Weng et al., 2020; Ren et al., 2021). Wang et al. (2021) proposed ensemble Q-learning as an alternative, using multiple Q-function"}, {"title": "2.2 LEARNING PROCESS INSIGHT ALGORITHMS IN REINFORCEMENT LEARNING", "content": "Ongoing development in human-like science and RL have increasingly focused on integrating human-like reasoning and beliefs, key components of learning process-oriented algorithms. These models aim to emulate human decision-making by adapting beliefs and strategies based on experience. Complementing these efforts, Barber (2012) discusses Bayesian reasoning frameworks that incorporate prior knowledge to manage uncertainty effectively. Building on this, Carroll et al. (2019) explored collaboration by integrating learned human policies into Q-learning. More recently, Zhang et al. (2021) introduced Solipsistic Reinforcement Learning, extracting human-perspective state representations, while Hu et al. (2021) developed Off-Belief Learning (OBL), allowing agents to reason about others' actions with dynamic beliefs. Additionally, O'Donoghue (2021) proposed Variational Bayesian Reinforcement Learning, which offers a novel approach to balancing exploration and exploitation using a risk-seeking utility function. This method introduces a new Bellman operator with associated fixed points, termed 'knowledge values,' which compress both expected future rewards and epistemic uncertainty into a single value. These approaches enhance AI adaptability and align reinforcement learning with human cognition."}, {"title": "3 PROBLEM FORMULATION", "content": "Markov Decision Processes (MDP) To solve a RL problem, the agent optimizes the control policy under an MDP M, which can be defined by a tuple (S, A, pt, r, \u03bc\u03bf, \u03b3, \u03a4) where: 1) S and A denote the space of states and actions. 2) pr(st+1|st, at) and r(st, at) define the transition probability and reward function. 3) \u03bc\u03bf defines the initial state distribution. 4) \u03b3 \u2208 (0, 1) is the discount factor and T defines the planning horizon. The goal of the RL policy \u03c0(as) is to maximize expected discounted rewards:\narg max \u0395\u03c0,\u03c1\u03c4, [$\\sum_{t=0}^{T} \\gamma^{t}r(s_t,a_t)$] \nWe define the action value function given a policy \u03c0:\nQ(s, a) = \u0395\u03c0,\u03c1\u03c4,\u03bco [$\\sum_{t=0}^{T} \\gamma^{t}r(s_t, a_t) | s_0 = s, a_0 = a$] \nand the optimal Q function is:\nQ*(St, at) = \u0395\u03c0,\u03c1\u03c4,\u03bco [r(st, at) + \u03b3Q*(st+1,a)]\nOne of our goals is that Q is guaranteed to converge to Q* (s, a) as t \u2192 \u221e:\nlim Q(st, at) = Q*(St, at)"}, {"title": "Overestimation Error", "content": "Letting Q(st, at; $i) be the action-value function of Q-learning (Watkins &\nDayan, 1992) at iteration i, we follow terminology from (Anschel et al., 2016). We denote ya is\nthe Q-learning target estimation, and ya is the true target:\n= \u0415\u0432 [r(st, at) + y max Q(st+1, \u03b1; \u03a6i\u22121)|St, At]\ns,a = Es [r(st, at) + y max(y+1)|st, at \nwhere B is a replay buffer. We denote Z ot,at the target approximation error (TAE), and River is the\noverestimation error, namely\nZ ot,at = Q(st, at; $i) \u2013 Y\nRat Ystat Yst,at\n(Thrun & Schwartz, 2014) considered the TAE Zia, as a random variable uniformly distributed\nin the interval [\u22126, 6]. Due to the max operator in the target estimation \u01774,a4, the expected overes-\ntimation errors Ez [Riverr] are upper bounded by ye. K is the number of actions. We attempt\nto overcome this overestimation issue with a unique approach and enhance the capabilities of Q-\nlearning methods."}, {"title": "4 MODELLING SUBJECTIVE BELIEF DISTRIBUTION IN Q-LEARNING FRAMEWORK", "content": "In this work, we address a fundamental question: How does integrating subjective beliefs refine decision-making within a Q-learning framework? We propose a novel method, Cognitive Belief-Driven Q-Learning (CBDQ) to incorporate human-like subjective belief components into RL. By leveraging Subjective Expected Utility Theory (SEUT), we dynamically update an agent's belief distribution over time, reflecting evolving perceptions of rewards, actions, and states."}, {"title": "4.1 EXPECTED UTILITY THEORY AND Q-LEARNING: A COGNITIVE PERSPECTIVE", "content": "To closely mirror human cognitive processes, we consider integrating SEUT into RL. SEUT offers a structured framework for decision-making under uncertainty by individual's belief preference, promoting actions that maximize the weighted sum of outcome utilities. This framework aligns seamlessly with MDPs, where the value function represents a specific form of expected utility derived from discounted returns.\nProposition 4.1 Consider a decision-making scenario in a MDP, where the complete set of possible outcomes is represented by X. Let bt(\u00b7 | St+1) represent the agent's belief distribution over possible actions in the next state st+1, and ut(s, x) be the utility of outcome x in state s. Then the expected utility U+(s, x) at time t is given by:\nUt(s,x) = $\\sum_{x \\in X} b_t(\\cdot | S_{t+1}) \\cdot u_t(s,x)$\nProposition 4.1 elucidates how individuals evaluate the utility of various actions within a MDP. It not only reflects the core tenets of SEUT but also provides a foundation for understanding learning processes. SEUT simulates how decision-makers assess potential outcomes through a weighted sum of utilities, which directly corresponds to the term bt(\u00b7 | St+1) \u00b7 ut(s, x) in our formulation. The subjective belief component b\u2081(\u00b7 | St+1) represents an individual's belief, providing flexibility and robustness for modeling beliefs under uncertainty, aligning our model more closely with human cognitive processes. This characteristic aligns with the closely related cognitive processes proposed by (Tversky & Kahneman, 1992). Concurrently, research by (Hogarth & Einhorn, 1992) demonstrates that individuals revise their beliefs based on new information and experience."}, {"title": "4.2 EVOLVING BELIEFS IN Q-LEARNING", "content": "As outlined in proposition 4.1, the expected utility Ut(s, a) in a MDP is computed from transition probabilities, rewards, etc. The CBDQ algorithm extends this by replacing the maximum Q-value"}, {"title": "4.3 BELIEF INTERACTION AND UPDATE", "content": "Because of the limitations of fixed belief frameworks, we explore the application of dynamic beliefs from the perspective of learning processes. Figure 1 illustrates animals' subjective belief-based decision-making process in various contexts. This process reflects how agents simplify decision-making through state-space clustering, utilizing a strategy that groups states based on shared features (Liu et al., 2024).\nTo model belief interaction and update, we introduce Belief-Preference Decision Framework (BPDF), which offers a structured approach to decision-making by integrating human prior knowledge with immediate belief updates. This framework enhances the efficiency and interpretability of decisions in complex environments. The model utilizes human expert knowledge to identify and select informative state features for representation learning. Additionally, clustering algorithms are applied to partition the state space S into N semantically meaningful and internally consistent clusters {Cn}=1, Figure 3 presents an example within the Box2D environment, adhering to the following formal criteria:\nN\nS = \u222aCn, Cin C j = 0, \u2200i \u2260 j\nn=1\nHuman cognition and belief formation are gradual processes, where early decisions rely on immediate rewards. Cognitive science research suggests that in uncertain environments, humans initially depend on short-term feedback, progressively incorporating long-term preferences as experience accumulates (Doya, 2007; Gershman et al., 2015). This shift from reward-driven choices to informed decisions underpins"}, {"title": "Algorithm 1 Cognitive Belief-Driven Q-Learning Algorithm", "content": "Input: Q function Q(s, a; 4), target Q functionQ(s, a; $\u00af), learning rate a, discount factor \u03b3, running steps T, episodes E, replay buffer B and exploration probability e\nOutput: QCBDQ(s, a; \u0444\u0442)\n1: Initialize Q(s, a; 4) with random weights 40;\n2: Initialize replay buffer B with a fixed length;\n3: Initialize Belief-Preference Decision Framework (BPDF) {Cn}n=1;\nN\n4: Initialize a e-greedy exploration procedure: Explore(\u00b7)\n5: for i=0; i < E ; i++ do\n6:\nGet initial state so from the environment\n7:\nfor t = 0; t <T;t++do\n8:\nChoose action at using e-greedy: at ~ U(0,1)\n9:\nExecute at to get reward r(st, at), next state st+1\n10:\nStore (st, at, r(st, at), St+1) into B\n11:\nFind the cognitive cluster Ci of st, update the count of at in Ci\n12:\nSample N tuples from B to update Q function:\n13:\nAt,at = E\u00df [r(st, at) + y \u2211a bt(a | st+1)Q(st+1, a; \u00a2\u00ae)|st, at]\n14:\nUpdate Q\nThe computation of bt (a | St+1) in Equation 14 dynamically integrates immediate\nrewards and subjective beliefs, enabling continuous adaptation based on evolving information.\n15:\nLoss = EB [(Yat - Q(St, at; $))2];\n16:\n;\n17:\nend for\n18: end for"}, {"title": "5 EXPERIMENT", "content": "Running Setting. For a comprehensive comparison, we employ Feasible Cumulative Rewards metric, which calculates the total rewards accumulated by the agent across all environments (higher is better). We run experiments with three different seeds (123, 321, and 666) and present the mean + std results for each algorithm. To ensure a fair comparison, we maintain the same settings and parameters for all baselines. Our code is implemented based on the XuanCe benchmark (Liu et al., 2023). Appendix E.4 reports the detailed parameters."}, {"title": "Comparison Methods.", "content": "We consider CBDQ (Algorithm 1) alongside the following baselines: (1)\nDQN (Mnih et al., 2013) approximates the action-value function using a deep neural network, with\nexperience replay and target networks for stabilization. (2) DDQN improves on this by separat-\ning action selection from value estimation, reducing overestimation bias. (3) DuelDQN further\nenhances learning efficiency through a dual-stream architecture that individually estimates state val-\nues and action advantages. (4) PPO uses a clipped objective function for stable policy updates,\nbalancing exploration and exploitation while maintaining a trust region for policy improvements."}, {"title": "5.1 EMPIRICAL EVALUATIONS IN PHYSICAL SIMULATION ENVIRONMENTS", "content": "The environments shown in Figure 4 and Appendix F highlight the performance of various RL algorithms across three distinct Classic Control and Box2D tasks (Towers et al., 2024; Parberry, 2017). The leftmost column displays the Cartpole environment, where agents are tasked with balancing a pole on a moving cart. Next is the Acrobot environment, where the goal is to swing a two-link arm to reach a specific height. The third column showcases the CarRacing task, a more complex scenario where agents must control a car to drive smoothly along a racetrack. Finally, the rightmost column presents the LunarLander environment, where agents must carefully land a spaceship on the moon's surface. Each environment progressively tests different control and decision-making skills, from balancing and swinging dynamics to managing more complex trajectories and landings.\nFigure 4 illustrates CBDQ significantly significant improvements with faster convergence by leveraging subjective belief modeling and cognitive clustering. It outperforms all other approaches, generating stable, high-reward trajectories that closely resemble optimal policies. In contrast, without the BPDF, traditional Q algorithms struggle with slower convergence and lower final rewards. While PPO shows moderate improvements, it still suffers from inefficiencies in these environments."}, {"title": "5.2 EMPIRICAL EVALUATIONS IN COMPLEX TRAFFIC SCENARIOS", "content": "To evaluate the human-like decision-making and path-planning capabilities of our algorithm, we employ four complex environments within MetaDrive, each designed to mimic real-world driving scenarios that require human-like adaptability (Li et al., 2022). Different letter combinations represent various types of road combinations. More detail of map design is in the Appendix.\nFigure 5 and Appendix F present the obvious advantages of CBDQ, particularly in emulating human-like learning and decision-making. Compared to other algorithms, CBDQ demonstrates faster learn-"}, {"title": "6 FUTURE INSIGHT", "content": "Expanding to Continuous Control Domains. Building on our success in discrete environments, we are exploring ways to adapt our framework to continuous control scenarios. This involves integrating cognitive science principles with advanced reinforcement learning techniques, aiming for more flexible and robust decision-making in complex, continuous action spaces.\nHuman-like Learning Processes in Reinforcement Learning. CBDQ provides new insights for future reinforcement learning, particularly in emulating human learning processes. Future algorithms are expected to increasingly simulate human concept formation and abstract reasoning, with cognitive clustering evolving into autonomously formed conceptual hierarchies. Additionally, dynamic belief updating mechanisms point toward adaptive learning rates and exploration strategies, where algorithms adjust based on task complexity and learning progress. CBDQ's strengths in uncertainty management and long-term planning suggest that human decision psychology will play a greater role in future reinforcement learning."}, {"title": "7 CONCLUSION", "content": "This study introduces the Cognitive Belief-Driven Q-learning (CBDQ) algorithm, integrating cognitive science principles with reinforcement learning to enhance efficiency and interpretability in complex environments. CBDQ incorporates subjective belief probabilistic reasoning and cognitive clustering for state space representation, demonstrating superior performance over traditional Q-learning and advanced algorithms like PPO. This research has broad implications for AI, potentially catalyzing interdisciplinary innovations toward more intelligent, interpretable, and adaptable systems capable of interesting environments."}, {"title": "Convergence Proof", "content": "We outline a proof that builds upon the following result for a formal statement) and follows the framework provided in Theorem 1 The random process {\u2206t} taking value in R and defined as\n\u2206t+1(x) = (1 - at(x))\u2206t(x) + at(x)Ft(x)\nconverges to 0 with probability 1 under the following assumptions:\n\u2022 0 \u2264 a \u2264 1, \u2211tat(x) = \u221e, \u2211\u03c4\u03b1\u00b2(x) < \u221e;\n\u2022 E[||Ft(x)||w] \u2264 \u043a||\u2206t||W + Ct, \u043a\u2208 [0,1) and ct \u2192 0 with probability 1;\n\u2022 var(Ft(x)) \u2264 C'(1 + ||\u2206t||w)\u00b2, C > 0\nwhere ||\u2206t||w denotes a weighted max norm.\nWe are interested in the convergence of Qt towards the optimal value Q and therefore define\n\u2206t = Qt(St, at) - Q*(St, at)\nIt is convenient to write the smoothed update as\nQt+1(St, at) = Qt(St, at) + at(St, at) (rt + y <(Q(st+1, a))a \u2013 Qt(st, at))\nwhere <f(x))x means the expectation of the function f(x) with respect to the distribution of x.\nUsing the smoothed update, we can write\nAt+1(St, at) = Qt+1(St, at) - Q*(St, at)\n= (1 - at)\u2206t + at (rt + y<(Q(st+1, a))a - Q*(St, at))\nIn terms of Theorem 1, we therefore define\nFt = rt + + > bt(a|St+1)Qt(St+1, a) \u2013 Q*(St, at)\na\nProof D.1 For convergence, we need to bound the norm of the expected value of Ft. We can write\n1\nE[Fe] = Ep7 [Gt]"}]}