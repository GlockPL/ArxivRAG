{"title": "Revisiting the Performance of Deep Learning-Based Vulnerability Detection on Realistic Datasets", "authors": ["Partha Chakraborty", "Krishna Kanth Arumugam", "Mahmoud Alfadel", "Meiyappan Nagappan", "Shane McIntosh"], "abstract": "The impact of software vulnerabilities on everyday software systems is concerning. Although deep learning-based models have been proposed for vulnerability detection, their reliability remains a significant concern. While prior evaluation of such models reports impressive recall/F1 scores of up to 99%, we find that these models underperform in practical scenarios, particularly when evaluated on the entire codebases rather than only the fixing commit. In this paper, we introduce a comprehensive dataset (Real-Vul) designed to accurately represent real-world scenarios for evaluating vulnerability detection models. We evaluate DeepWukong, LineVul, Reveal, and IVDetect vulnerability detection approaches and observe a surprisingly significant drop in performance, with precision declining by up to 95 percentage points and F1 scores dropping by up to 91 percentage points. A closer inspection reveals a substantial overlap in the embeddings generated by the models for vulnerable and uncertain samples (non-vulnerable or vulnerability not reported yet), which likely explains why we observe such a large increase in the quantity and rate of false positives. Additionally, we observe fluctuations in model performance based on vulnerability characteristics (e.g., vulnerability types and severity). For example, the studied models achieve 26 percentage points better F1 scores when vulnerabilities are related to information leaks or code injection rather than when vulnerabilities are related to path resolution or predictable return values. Our results highlight the substantial performance gap that still needs to be bridged before deep learning-based vulnerability detection is ready for deployment in practical settings. We dive deeper into why models underperform in realistic settings and our investigation revealed overfitting as a key issue. We address this by introducing an augmentation technique, potentially improving performance by up to 30%. We contribute (a) an approach to creating a dataset that future research can use to improve the practicality of model evaluation; (b) Real-Vul-a comprehensive dataset that adheres to this approach; and (c) empirical evidence that the deep learning-based models struggle to perform in a real-world setting.", "sections": [{"title": "INTRODUCTION", "content": "Software vulnerabilities have a large negative impact on the software systems. Identifying and addressing vulner-abilities in complex systems with multiple interconnected components has only exacerbated the problem, making a comprehensive and systematic approach necessary. Machine learning models that are trained using Deep Neural Net-works (DNNs) have shown promise in identifying software vulnerabilities [1, 2].\nHowever, the reliability of these models in detecting vulnerabilities in real-world scenarios depends on the evalu-ation methodology and dataset. Biases that can affect model performance can arise from various sources, such as the manner in which the dataset is generated and labeled. The generalizability of a model may suffer if the dataset on which it is trained is biased. For instance, synthetic datasets, such as SARD [3], are artificially created using fuzz techniques and evolutionary algorithms and are extensively used to evaluate the effectiveness of deep learning models\nin detecting software vulnerabilities [4, 5, 6]. It is unclear the extent to which the vulnerabilities in these synthetic datasets capture the full range of complexities and variations that are found in real-world vulnerabilities.\nMoreover, these studies classified explicitly identified vulnerable functions as \"vulnerable\" and all remaining functions as \"non-vulnerable\". However, this binary classi-fication overlooks the possibility that some functions might be vulnerable yet undetected. Therefore, for the purposes of our study, we will describe these functions with a more nuanced term, \"uncertain,\" to acknowledge the inherent ambiguity in their vulnerability status.\nOn the other hand, there do exist real-world datasets that contain vulnerabilities from real software systems (e.g., Big-Vul [7], ReVeal [1]); however, these datasets tend to (i) only include a sample of code from a substantially larger codebase, i.e., the vulnerable and uncertain (non-vulnerable or vulnerability not reported yet) samples are extracted from vulnerability-related commits, and hence, the dataset does not reflect a realistic setting where vulnerability detection models are deployed to scan an entire project; and (ii) suffer from label inconsistency wherein the same sample is marked as both vulnerable and uncertain (see Section 3 for detailed explanation). Consequently, we conjecture that"}, {"title": "LIMITATIONS OF EXISTING DATASETS", "content": "As software systems become more complex and larger, the potential for security vulnerabilities also increases. There-fore, it is crucial to have tools to discover these vulnerabil-ities. Machine learning models have proven to be effective in understanding code and detecting vulnerabilities. How-ever, obtaining and evaluating large datasets in a realistic setting [1, 7, 11] remains challenging. Existing studies on vulnerability detection [7, 11] rely on datasets that are based on various criteria and may not reflect reality accurately. These datasets can be classified into three categories: syn-thetic, oracle-based, and real-world datasets. In this section, we describe each category and their limitations.\nSynthetic dataset (e.g., SARD [3]). The Software Assurance Reference Dataset (SARD) is an example of a synthetic dataset. It contains a vast number of artificially produced C/C++ programs containing numerous security vulnerabil-ities. SARD has been created through automated methods like fuzzing or genetic algorithms. The SARD dataset's limitation lies in containing only artificially generated vul-nerabilities, which may not accurately represent real-world software vulnerabilities. This dataset does not reflect the complexities and variations found in actual code written by developers, potentially limiting researchers from evaluating their models in a realistic setting.\nOracle-based dataset (e.g., D2A [12]). D2A is an exam-ple of an Oracle-based dataset. Unlike synthetic datasets, oracle-based datasets rely on third-party sources such as static analysis tools to provide labels for collected data samples. Although such datasets offer more complexity than synthetic datasets, they may not fully represent real-world vulnerabilities due to oversimplification. The inaccuracy of the labeling heavily affects the dataset's reliability which threatens the usability of the vulnerability detection models"}, {"title": "Real-Vul DATASET", "content": "In this section, we introduce our proposed dataset, named Real-Vul, which addresses the limitations of existing vulner-ability detection datasets (Section 2). Project selection. Real-Vul includes separate training and testing datasets designed for training and evaluat-ing vulnerability detection models. To ensure high-quality vulnerable samples, we carefully select projects from the Big-Vul dataset [7] based on the number of vulnerabilities in each project. Big-Vul dataset comprises real-world vul-nerable C/C++ functions. It also provides rich metadata for vulnerabilities, including line numbers, vulnerability-fixing commit hashes, CVE IDs, severity rankings, and summaries from public Common Vulnerabilities and Expo-sures (CVE) databases and related source code repositories. Consequently, all samples within the Real-Vul dataset are real and written in C/C++ as well. Our focus on projects with a higher number of vulnerabilities signifies the sub-stantial effort invested in identifying vulnerabilities in these projects [14]. Our selection consists of the top 10 projects with the most vulnerabilities, creating a dataset that is at least two times larger than those used in previous stud-ies [1, 11]. Remarkably, these ten projects encompass 73% of the vulnerable samples from the entire Big-Vul dataset, making Real-Vul a representative subset without its limita-tions.\nCreation of vulnerable samples. To align with real-world scenarios, we adopt a time-based strategy for sample cre-ation in the training and testing datasets. This approach simulates the process of training models on historical data and then identifying vulnerabilities over time. The time-based strategy is described below:\nFor generating vulnerable samples, we start extracting the dates on which the vulnerable functions were fixed using the vulnerability-fixing commit hashes available in the Big-Vul dataset.\nTo organize vulnerable samples, we order the functions based on their vulnerability-fixing dates (i.e., the dates of"}, {"title": "STUDY DESIGN", "content": "In this section, we present the models we evaluate using Real-Vul dataset. Then, we describe the research questions driving our investigation, and the evaluation metrics."}, {"title": "Employed Models", "content": "We choose four state-of-the-art deep learning-based mod-els, namely LineVul, DeepWukong, ReVeal, and IVDetect to conduct our experiments. Next, we briefly describe the architectural details of these models.\nLineVul. LineVul is a deep learning-based model built using CodeBERT . We opt to include LineVul in our analysis as it is the state-of-the-art deep learning-based vulnerability detection model that utilizes a sequence-based approach for vulnerability detection. LineVul takes in a chunk as input and classifies it as a vulnerable or uncertain chunk utilizing the CodeBERT model. A chunk is a sequence of code tokens generated from source code programs. The LineVul has been trained on the Big-Vul dataset which has been curated using vulnerability-fixing commits from open-source projects.\nDeepWukong. DeepWukong is a leading vulnerabil-ity detection model utilizing a Graph Neural Network (GNN) for in-depth code analysis. It converts code into Program Dependence Graphs (PDGs), then into XFGs (sub-graphs), highlighting the data and control flow dependen-cies within. DeepWukong evaluates these XFGs to identify vulnerable code segments, employing the SARD dataset for training. This model has been included in the study due to its advanced graph-based approach, which sets a new benchmark for accurately detecting code vulnerabilities.\nReVeal. ReVeal is a model that finds vulnerable code using graph neural networks. It has been chosen because it gathers data similarly to Real-Vul, focusing on unchanged functions from specific commits as non-vulnerable exam-ples. ReVeal combines control flow, data flow, syntax trees, and dependency graphs into a Code Property Graph (CPG) for comprehensive code analysis. It trains on a dataset cu-rated from the Linux Debian Kernel and Chromium project.\nIVDetect. IVDetect is a tool that aims to provide precise interpretations of detected vulnerabilities. IVDetect incor-porates representation learning and a graph-based interpre-tation model. It processes code by analyzing control flow, data flow, abstract syntax trees, and program dependency"}, {"title": "Evaluation Metrics.", "content": "The evaluation metric must accurately measure the models' performance on the task at hand. In this study, we use accuracy, precision, recall, F1, and AUC to measure the models' performance.\nAccuracy quantifies the overall correctness of a vulner-ability detection model, representing the proportion of true results (both true positives and true negatives) among the total number of samples that were evaluated. For example, if a model correctly identifies 80 out of 100 functions (vul-nerable or not), its accuracy is 80%. High accuracy indicates effective identification of both vulnerable and uncertain samples with minimal errors.\nPrecision represents the fraction of vulnerabilities de-tected by the model that are truly vulnerabilities. For ex-ample, in the case of the DeepWukong model, it measures how many XFGs predicted as vulnerable are genuinely vulnerable XFGs. A low precision score indicates that the model is incorrectly classifying many uncertain samples as vulnerable, leading to a high number of false positives. Conversely, a high precision score implies that when the model detects a vulnerability, it is likely a real vulnerability.\nRecall, on the other hand, denotes the fraction of actual vulnerabilities in the system that the model successfully detects. For the DeepWukong model, it measures how many true vulnerable XFGs are correctly identified as vulnerable by the model. A high recall score indicates that the model can detect most vulnerabilities correctly, resulting in a low number of false negatives.\nThe F1 score combines precision and recall, providing an overall assessment of how well these two measurements are balanced. By considering both precision and recall, the F1-score offers a more complete understanding of the model's effectiveness in vulnerability detection.\nArea Under the Curve (AUC) evaluates a model's ca-pability to distinguish between classes, like differentiating vulnerable from uncertain XFGs in the DeepWukong model. A high AUC score indicates that the model is proficient in accurately distinguishing between vulnerable and uncertain XFGs. Notably, in a balanced dataset, a random model would achieve an AUC of 0.50."}, {"title": "Research Questions", "content": "We introduce two research questions (RQs) and explain the motivation behind each one.\nRQ1: How do the DeepWukong, LineVul, ReVeal, and IVDetect models perform in a realistic evaluation setting compared to the evaluation setting used in the original studies?\nDeepWukong, LineVul, ReVeal, and IVDetect are four state-of-the-art vulnerability detection models that have demon-strated promising results in identifying security vulnerabili-ties. However, we contend that the datasets used to evaluate these models do not accurately represent real-world usage, where the models would scan entire project source code files for a comprehensive overview of dataset limitations). For instance, the DeepWukong model heavily relies on the SARD dataset for evaluation, which contains artificially generated samples lacking the complexities of real-world vulnerabilities. Similarly, the LineVul model used the Big-Vul dataset [7], which suffers from a scarcity of uncertain samples. Additionally, the ReVeal dataset also suf-fers from limited uncertain samples and significant \"label-inconsistency.\" In this RQ, we evaluate the DeepWukong, LineVul, ReVeal, and IVDetect models using our Real-Vul test dataset, which provides a more realistic representation of vulnerability detection model performance in practical scenarios.\nRQ2: How do the DeepWukong, LineVul, ReVeal, and IVDetect models perform in a realistic evaluation setting when trained using a similar realistic training dataset?\nIn RQ1, we utilize the Real-Vul dataset for model evaluation. However, we recognize that the training dataset employed to train these models might not accurately represent the same distribution as the evaluation dataset (i.e., Real-Vul). Doing so can lead to poor model performance as the training and testing datasets are from different distributions. To achieve optimal results, it is essential for both datasets to share the same distribution as the data encountered in practical usage. Consequently, in RQ2, we investigate the impact of data distribution on model performance by train-ing and testing the models using our Real-Vul dataset. This exploration will enable us to ascertain whether employing a training dataset representative of the same distribution as the evaluation dataset leads to enhanced model perfor-mance."}, {"title": "PRELIMINARY ANALYSIS", "content": "Our study aims to evaluate the performance of the Deep-Wukong, LineVul, and ReVeal models in a more realis-tic setting. As a first step towards our goal, we run the DeepWukong, LineVul, ReVeal, and IVDetect models on the SARD, the Big-Vul, the ReVeal, and the IVDetect datasets, respectively, and verify the findings reported in the original papers. By doing so, we aim to ensure that the results previ-ously reported are reliable and can be replicated. Addition-ally, we use the results obtained from these experiments as our baseline models, which we compare to the performance of the models in our introduced realistic settings.\nApproach. To evaluate the DeepWukong, LineVul, ReVeal, and IVDetect models on the dataset used in their studies, we first download the SARD dataset, the Big-Vul dataset, the ReVeal dataset, used in DeepWukong, LineVul, ReVeal, and IVDetect studies, respectively. We create the DeepWukong model inputs (XFGs), the LineVul model inputs (chunks), the ReVeal model inputs (CPG), and the IVDetect models input PDG using the SARD dataset, the Big-Vul dataset, the ReVeal dataset, and the IVDetect dataset samples, respectively. The generated XFG, chunk, CPG, and PDG datasets are split into training and testing datasets where 80% of the samples belong to the training dataset and the remaining 20% samples belong to the testing dataset."}, {"title": "RESULTS", "content": "Following the authors' original experiments, we train the DeepWukong model for 50 epochs and the LineVul model for ten epochs. For the ReVeal and IVDetect model, we follow the original implementation of ReVeal and IVDetect. The maximum epoch is set to 100 and 50 for ReVeal and IVDetect, respectively. We stop the training procedure if the F1-score does not increase for five consecutive epochs. Finally, We evaluate the models using the testing datasets.\nResults shows the replication results of the Deep-Wukong, LineVul, ReVeal and IVDetect models. From the figure, we observe that the DeepWukong model achieves 98%, 87%, 98%, 93%, and 88% for accuracy, precision, recall, F1-score, and AUC, respectively. The accuracy and F1-score for the DeepWukong model differ by +1 and -2 percentage points, respectively, from the results reported by the Deep-Wukong paper.\nOur results for the LineVul model follow a similar trend. The accuracy, precision, recall, F1-score, and AUC for the LineVul model are 96%, 96%, 84%, 90%, and 85% respec-tively. Comparing with the results reported in the LineVul paper, we find that the precision, recall, and F1-score differ by -1, -2, and -1 percentage points, respectively.\nThe ReVeal model demonstrates an accuracy of 81%, with precision, recall, F1-score, and AUC values of 29%, 59%, 38%, and 78%, respectively. Upon comparing these results with the findings reported in the ReVeal paper, we note a difference of -6, -3, and -7 percentage points in precision, recall, and F1-score, respectively.\nThe IVDetect model demonstrates an accuracy of 85%, with precision, recall, F1-score, and AUC values of 39%, 63%, 24%, and 83% respectively. Upon comparing these results with the findings reported in the IVDetect paper, we note a difference of -2, -2, and -24 percentage points in precision, recall, and F1-score, respectively.\nOverall, we observe that the differences in the metrics are negligible. The little difference in the metrics can be because of the presence of different samples in the training/testing datasets. When a dataset is split into a training set and a testing set, the samples in the two sets are chosen randomly. This means that the samples in the training set and the test-ing set will be different each time we split the dataset. As a result, model performance on the testing set may vary from one split to another. The model performance on the testing set may depend on the specific samples that are included in the testing set. For example, if the testing set contains a particularly difficult or easy sample, this can affect the overall performance. These results serve as baseline models for comparison with the performance of the models tested\nunder realistic settings in the RQs.\nThe effectiveness of these models in vulnerability detec-tion is influenced by the degree to which the embeddings of the vulnerable and uncertain classes are distinct and separa-ble. That is, the greater the distinction and separability of the embedding, the more straightforward it is for the model to differentiate between the two classes. To explore this aspect, we conduct an experiment that involves visualizing the model's capability to distinctly segregate samples belonging to the vulnerable and uncertain classes.\nWe employ t-distributed Stochastic Neighbor Embed-ding (t-SNE)  to visualize the embeddings produced by the DeepWukong, LineVul, ReVeal, and IVDetect models. T-SNE is a machine learning algorithm for visualizing high-dimensional data in a low-dimensional (typically two or three) space. It calculates similarities between data points, converting them into probabilities and minimizing their Kullback-Leibler divergence  to preserve local struc-tures. This process effectively clusters similar data, which makes t-SNE valuable for identifying patterns and relation-ships in multi-dimensional data.\nWe extract the embedding from the DeepWukong, Line-Vul, ReVeal, and IVDetect models used in this RQ. The [CLS] embedding vectors generated by the CodeBERT model rep-resent the embedding for the LineVul model, while the final fixed vector produced by the Graph Neural Network and the representation-learning model represents the embed-ding for the DeepWukong, ReVeal, IVDetect model, respec-tively. It is worth noting that the embeddings are generated for the testing datasets.\nWe create four scatter plots in total, each displaying the reduced embedding along with their corresponding labels (vulnerable or uncertain). The scatter plots are presented in Figure 2. We observe that all the models we created, ex-cept ReVeal and IVDetect, exhibit clear separation between vulnerable and uncertain samples. The inability to clear sep-aration may explain the comparatively lower performance of the ReVeal and IVDetect model."}, {"title": "RESULTS", "content": "In this section, we present our experimental results with respect to each RQ.\nRQ1: How do the DeepWukong, LineVul, ReVeal, and IVDetect models perform in a realistic evaluation setting compared to the evaluation setting used in the original studies?\nApproach. In this RQ, we evaluate the DeepWukong, the LineVul, the ReVeal, and the IVDetect models using the Real-Vul testing dataset described in Section 3. We use the Line-Vul, ReVeal, and the IVDetect model trained in Section 5 for the evaluation, i.e., we train using the same method as the original LineVul, ReVeal, and IVDetect studies, respectively, while using Real-Vul testing dataset for evaluation. Note that LineVul, ReVeal, and IVDetect models are trained using real-world vulnerability datasets. Since the DeepWukong model in Section 5 is trained using SARD (an artificially created vulnerability dataset), we train a new DeepWukong model using the Big-Vul dataset for evaluation to simulate a fair comparison with the other two models. First, we generate\nthe DeepWukong model inputs (XFGs) using the Big-Vul dataset. We obtain a total of 28,294 vulnerable XFGs and 639,047 uncertain XFGs. Similar to preliminary analysis, we train the DeepWukong model for 50 epochs using the generated XFGs.\nTo evaluate the trained models (DeepWukong, LineVul, ReVeal, and IVDetect) using the Real-Vul testing dataset, we first generate the model inputs (XFG, chunk, CPG, and PDG) for the Real-Vul testing dataset. These samples are then evaluated using the respective trained models.\nResults shows the results of the performance of the DeepWukong, LineVul, ReVeal, and IVDetect models eval-uated using the Real-Vul testing dataset. The DeepWukong model obtains an accuracy, precision, recall, F1-score, and AUC of 91%, 1%, 87%, 2%, and 62%, respectively. When comparing these results to those obtained in Section 5 (i.e., compared to the results of the DeepWukong model trained and tested using the SARD dataset), we observe a decrease of 7, 86, 11, and 91 percentage points in accuracy, precision, recall, and F1-score, respectively. The LineVul model ob-tains an accuracy, precision, recall, F1-score, and AUC of 89%, 1%, 90%, 2%, and 58%, respectively. In comparison to the results obtained in Section 5 (i.e., compared to the results of the LineVul model trained and tested using the Big-Vul dataset), we observe a decrease of 7, 95, 88, and 27 percentage points in accuracy, precision, F1-score, and AUC, respectively. However, we also observe an increase of 6 percentage points in the recall.\nThe ReVeal model demonstrates an 89% accuracy rate, with precision, recall, and F1-score values of 10%, 80%, and 17%, respectively. When comparing these results to the\nresults from Section 5 where the ReVeal model was trained and tested using the ReVeal dataset, we observe a decrease of 19, 21, and 17 percentage points in precision, F1-score, and AUC, respectively, but a 21 percentage points improvement in recall.\nThe IVDetect model exhibits an accuracy of 85%, along with precision, recall, and F1-score metrics of 2%, 84%, and 2% respectively. In comparison to the outcomes from Section 5, where the IVDetect model underwent training and testing using the IVDetect dataset, a notable reduction of 37 percentage points in precision, 22 percentage points in the F1-score, and 24 percentage points in AUC can be observed; however, an enhancement of 21 percentage points in recall is also observed.\nOverall, our findings reveal a substantial decrease in pre-cision for all the models. This suggests that the predictions made by these models contain a considerable number of false positives, which can have a huge impact on the ef-fectiveness of vulnerability detection. Specifically, the Deep-Wukong model generated 439,494 false positives, while the LineVul, ReVeal, and IVDetect models produced 114,629, 320,128, and 512,284 false positives, respectively. Such a large number of false positives may have a large impact on the usability of these models in practice.\nTo understand the models' performance in terms of class separation, we follow the same approach used in Section 5, and plot embeddings of the models in two-dimensional space. It is worth noting that the embeddings are generated for the test split of Real-Vul dataset. Figure 3 presents the scatter plots of model embeddings. For all the models, we observe a substantial overlap between vulnerable and un-certain samples of the Real-Vul dataset, which indicates that the models do not distinguish between the classes clearly. This lack of class separation explains the high number of false positives produced by the DeepWukong, LineVul, ReVeal, and IVDetect models.\nOverall, our findings underscore the importance of eval-uating vulnerability detection models using datasets that reflect realistic settings. In this regard, our results demon-strate the critical role played by the Real-Vul dataset in understanding the true capabilities of such models. By uti-lizing this dataset, we gain a more accurate understanding of the performance of vulnerability detection models and can identify areas that require improvement."}, {"title": "ANSWER", "content": "Existing deep learning-based vulnerability detection models, such as DeepWukong, LineVul, ReVeal, and IVDetect produce a high number of false positives when tested using datasets that represent more accurate real-world testing settings.\nRQ2: How do the DeepWukong, LineVul, ReVeal, and IVDetect models perform in a realistic evaluation setting when trained using a similar realistic training dataset?\nApproach. In RQ1, we use the Real-Vul dataset for evalua-tion purposes, i.e., we evaluate the effectiveness of our mod-els using Real-Vul. However, in this RQ, we use the Real-Vul dataset for both model training and evaluation. The training dataset is imbalanced (higher uncertain samples). Hence, we also investigate the impact of the imbalanced dataset on model performance by training additional models on a balanced dataset. To create a balanced dataset for training, we randomly select uncertain samples equal to the number of vulnerable samples.\nOverall, we train a total of eight models (one imbalanced-trained and one balanced-trained model for DeepWukong, LineVul, ReVeal, and IVDetect techniques) using the same training parameters used in Section 5. We evaluate the trained DeepWukong, LineVul, ReVeal, and IVDetect models using the same XFG, chunk, CPG, and PDG test dataset used in RQ1 (i.e., Real-Vul test dataset).\nResults shows the results of the DeepWukong, LineVul, ReVeal, and IVDetect models that are trained us-ing imbalanced datasets. The DeepWukong and LineVul models both exhibit an accuracy of 99%, with precision, recall, F1-scores at 0%, and AUC at 50%. In comparison, the ReVeal model exhibits 95% accuracy, 2% precision, 10% recall, 3% F1-score, and 50% AUC. Similarly, the IVDetect model demonstrates an 81% accuracy, with precision, recall, F1-score, and AUC recorded at 2%, 1%, 0%, and 49.6%, respectively. The accuracy is 99% for both the DeepWukong and LineVul models, while the precision, recall, F1-scores, and AUC are 0% and 50%, respectively. For the ReVeal model, the accuracy, precision, recall, F1-score, and AUC are 95%, 2%, 10%, 3%, and 50%, respectively. A common trend across the models is that the precision, recall, and F1-scores are substantially reduced with respect to the results"}, {"title": "DISCUSSION", "content": "In this section, we delve into the false positive predictions and further examine the performance of the models under review by evaluating their effectiveness across different types of vulnerabilities and levels of severity.\nAnalysis of false positives. To understand why the studied models produce false positives, we have conducted new analyses from three perspectives.\n1) Size of Project. The motivation for considering project size as a critical factor arises from the notion that larger projects offer a more extensive dataset for model learning. This allows models to better comprehend and identify complex vulnerability patterns, potentially leading to a reduction in false positive rates. we illustrate the influence of project size on false positive rates where we observe a downward trend, i.e., as the size of the project increases, the false positive rate correspondingly decreases. Comparing the smallest project, Jasper, consist-ing of 183 training and 132 testing samples, with larger projects such as Chrome and Linux, which encompassed 44,303 and 71,706 training samples and 81,351 and 47,181 testing samples, respectively. The findings supported our hypothesis: larger projects exhibited substantially lower false positive rates (85%-87% for Chrome and 79%-85% for Linux).\n2) Code complexity analysis. Machine learning (ML) models often struggle to comprehend highly complex code [20], which can affect the performance of vulner-ability detection tools. To investigate this, we measured"}, {"title": "RELATED WORK", "content": "In this section, we discuss the related works and reflect on how they compare with ours.\nVulnerability detection datasets. Grahn et al. [33] found that some of the vulnerability detection datasets [7, 34] were not very useful for training models. To address this, they proposed a new dataset called Wild C, comprising 10.3 million C/C++ files from multiple open-source projects. However, the drawback of Wild C is that it lacks labels for each file, making it unsuitable for building vulnerability detection classification models. In contrast, the Real-Vul dataset offers the complete source code of projects along with labeled samples as vulnerable or uncertain, making it more suitable for training vulnerability detection models.\nThe Big-Vul dataset [7] is a collection of C/C++ functions from 348 open-source GitHub projects, used in vulnerabil-ity detection studies. However, it lacks representation of the entire codebase since it only includes functions from vulnerability-fixing commits. To address this, we introduce the Real-Vul dataset, which includes all source code from the top ten real-world projects by vulnerability counts in Big-Vul. This provides a more comprehensive and realistic dataset for training and evaluating vulnerability detection models.\nStudies on vulnerability detection techniques. Several studies explored the effectiveness of traditional machine learning techniques . Neuhaus et al. [35] inves-tigated the prevalence of software vulnerabilities in Red Hat packages using Support Vector Machines (SVM) [37]. The study analyzed the defect data from over 3,241 Red Hat packages and evaluated the effectiveness of SVM in identifying vulnerabilities in software packages. Zheng et al. [5] examined the effectiveness of different machine learn-ing techniques, like Decision Tree [38], Random Forests [39], k-nearest neighbors (KNN) [40], and SVM, in detecting software vulnerabilities. The results of the study provided insights into the strengths and weaknesses of different ma-chine learning techniques for vulnerability detection. Yan et al. [36] employed a combination of static analysis, machine learning, and typestate modeling techniques for static de-tection of use-after-free vulnerabilities in software. Lomio et al. [41] investigated whether machine learning algorithms like SVM, KNN, Decision Tree, and Boosting algorithms  could improve the performance of Just-in-Time software vulnerability detection utilizing various software metrics (process metrics, product metrics, and text metrics).\nOther studies focused on exploring the potential of vari-ous deep learning methods to detect vulnerabilities . For example, Convolutional Neural Networks (CNN) [48] have been used to forecast software defects and locate defective source code [45]. Li et al. [47] made multiple kinds of deep neural networks such as CNN, LSTM [49], and GRU [50] to detect vulnerabilities. Vuldeepecker [13] detects resource management issues and buffer overflows by training an LSTM model with code embedding and data-flow information of a program. VGDetector [46] uses a control flow graph and a graph convolutional network [51] to detect control-flow vulnerabilities. Zhou et al. [11] pinpointed bugs at the method level using Graph Neural"}, {"title": "IMPLICATION", "content": "Below, we distill the implication of our findings for the development of research communities.\nThe Real-Vul dataset can be used to develop tools to assist developers in mitigating vulnerabilities. For example,\n\u2022 Facilitating research studies for proposing risk assess-ment and prioritization techniques. The dataset contains severity information for each vulnerable sample. This data can be used to train a model to predict the presence of vulnerabilities and assess the potential risk or severity. This aids in prioritizing security efforts and resource allocation.\n\u2022 Supporting research on automated patch generation. The dataset contains before-fix and after-fix versions of vulnerable functions. This feature can inform the devel-opment of automated patch-generation tools.\n\u2022 Characterizing the evolution of software with respect to its vulnerabilities. The dataset contains vulnerabil-ity data of 10 common and popular projects that span ten years of history. It contains 270,919 samples, of which 5,528 are vulnerable. Future research could explore whether severe vulnerabilities have become more or less prevalent over time or how coding practices have evolved to mitigate vulnerabilities in these projects."}, {"title": "CONCLUSION", "content": "In this paper, we study the performance of deep learning-based vulnerability detection models in realistic vulnerabil-ity detection settings. First, we create a new comprehensive, realistic vulnerability detection dataset, called Real-Vul. Real-Vul contains complete source code samples of ten diverse real-world open-source projects. Then, we evaluate four state-of-the-art models, LineVul, DeepWukong, ReVeal, and IVDetect on the Real-Vul dataset. Our evaluation indicates a considerable decrease in model performance, as evidenced by a drop in precision and F1 scores of up to 95 and 91 per-centage points, respectively. Our investigation also reveals that the embeddings generated by these models depict a substantial overlap between vulnerable and uncertain sam-ples. This suggests that such models struggle to differentiate between vulnerable and uncertain samples in the Real-Vul dataset, resulting in a high number of false positives. Finally, we observe fluctuations in model performance based on vul-nerability characteristics (e.g., vulnerability types and sever-ity). Our study argues that when it comes to identifying"}]}