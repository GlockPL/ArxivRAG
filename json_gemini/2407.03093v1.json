{"title": "Revisiting the Performance of Deep Learning-Based Vulnerability Detection on Realistic Datasets", "authors": ["Partha Chakraborty", "Krishna Kanth Arumugam", "Mahmoud Alfadel", "Meiyappan Nagappan", "Shane McIntosh"], "abstract": "The impact of software vulnerabilities on everyday software systems is concerning. Although deep learning-based models have been proposed for vulnerability detection, their reliability remains a significant concern. While prior evaluation of such models reports impressive recall/F1 scores of up to 99%, we find that these models underperform in practical scenarios, particularly when evaluated on the entire codebases rather than only the fixing commit. In this paper, we introduce a comprehensive dataset (Real-Vul) designed to accurately represent real-world scenarios for evaluating vulnerability detection models. We evaluate DeepWukong, LineVul, Reveal, and IVDetect vulnerability detection approaches and observe a surprisingly significant drop in performance, with precision declining by up to 95 percentage points and F1 scores dropping by up to 91 percentage points. A closer inspection reveals a substantial overlap in the embeddings generated by the models for vulnerable and uncertain samples (non-vulnerable or vulnerability not reported yet), which likely explains why we observe such a large increase in the quantity and rate of false positives. Additionally, we observe fluctuations in model performance based on vulnerability characteristics (e.g., vulnerability types and severity). For example, the studied models achieve 26 percentage points better F1 scores when vulnerabilities are related to information leaks or code injection rather than when vulnerabilities are related to path resolution or predictable return values. Our results highlight the substantial performance gap that still needs to be bridged before deep learning-based vulnerability detection is ready for deployment in practical settings. We dive deeper into why models underperform in realistic settings and our investigation revealed overfitting as a key issue. We address this by introducing an augmentation technique, potentially improving performance by up to 30%. We contribute (a) an approach to creating a dataset that future research can use to improve the practicality of model evaluation; (b) Real-Vul-a comprehensive dataset that adheres to this approach; and (c) empirical evidence that the deep learning-based models struggle to perform in a real-world setting.", "sections": [{"title": "1 INTRODUCTION", "content": "Software vulnerabilities have a large negative impact on the software systems. Identifying and addressing vulner-abilities in complex systems with multiple interconnected components has only exacerbated the problem, making a comprehensive and systematic approach necessary. Machine learning models that are trained using Deep Neural Net-works (DNNs) have shown promise in identifying software vulnerabilities [1, 2].\nHowever, the reliability of these models in detecting vulnerabilities in real-world scenarios depends on the evalu-ation methodology and dataset. Biases that can affect model performance can arise from various sources, such as the manner in which the dataset is generated and labeled.\nThe generalizability of a model may suffer if the dataset on which it is trained is biased. For instance, synthetic datasets, such as SARD [3], are artificially created using fuzz techniques and evolutionary algorithms and are extensively used to evaluate the effectiveness of deep learning models in detecting software vulnerabilities [4, 5, 6]. It is unclear the extent to which the vulnerabilities in these synthetic datasets capture the full range of complexities and variations that are found in real-world vulnerabilities.\nMoreover, these studies classified explicitly identified vulnerable functions as \"vulnerable\" and all remaining functions as \"non-vulnerable\". However, this binary classi-fication overlooks the possibility that some functions might be vulnerable yet undetected. Therefore, for the purposes of our study, we will describe these functions with a more nuanced term, \"uncertain,\" to acknowledge the inherent ambiguity in their vulnerability status.\nOn the other hand, there do exist real-world datasets that contain vulnerabilities from real software systems (e.g., Big-Vul [7], ReVeal [1]); however, these datasets tend to (i) only include a sample of code from a substantially larger codebase, i.e., the vulnerable and uncertain (non-vulnerable or vulnerability not reported yet) samples are extracted from vulnerability-related commits, and hence, the dataset does not reflect a realistic setting where vulnerability detection models are deployed to scan an entire project; and (ii) suffer from label inconsistency wherein the same sample is marked as both vulnerable and uncertain (see Section 3 for detailed explanation). Consequently, we conjecture that models that are trained and evaluated using such datasets may not perform well when applied in real-world settings.\nTherefore, in this paper, we introduce Real-Vul, a new vulnerability detection dataset designed to accurately rep-resent the realistic settings in which these models would be deployed. By \"realistic settings,\" we refer to evaluation environments that closely mirror real-world conditions, in-cluding the distribution of vulnerable and uncertain sam-ples. This approach ensures that the models are tested under conditions that reflect their actual usage scenarios.\nReal-Vul differs from prior existing datasets in that it includes entire codebases in which vulnerable code exists, representing a more realistic setting in which vulnerability detection models would be applied. Real-Vul also accounts for the label inconsistency problem by comparing the hash of vulnerable and uncertain code segments.\nTo re-evaluate the performance of deep learning-based models in our more realistic setting, we apply four state-of-the-art models (i.e., DeepWukong [4], LineVul [8], ReVeal [1], and IVDetect [9]) to Real-Vul dataset. First, we train and evaluate these models on their original datasets, i.e., the SARD [3], Big-Vul [7], ReVeal [7], and IVDetect [9] datasets, respectively. The DeepWukong and LineVul models achieve notably high precision (87% - 96%) and F1 scores (90% - 93%), while ReVeal and IVDetect achieves precision and F1 scores of 29% - 39% and 78% - 83%, respectively. However, when we evaluate these models on the Real-Vul dataset, we observe a substantial decrease in performance, e.g., preci-sion and F1 score decreases of up to 95 and 91 percentage points, respectively.\nTo understand why these models produce a large num-ber of false positives, we visualize their embeddings for vulnerable and uncertain samples in a two-dimensional space using t-SNE [10]. We find that these models struggle to establish a clear distinction between the two classes, resulting in inaccurate identification of vulnerabilities. For instance, models like LineVul heavily rely on lexical rela-tionships between tokens and as such, encounter difficulties in accurately capturing the complex nature of vulnerabilities in real-world software.\nTo pinpoint why models underperform in realistic settings, we conducted an in-depth manual analysis of false positives. We found that overfitting, where models predict based on specific tokens, was a significant issue. To counteract this, we implemented an augmentation technique that has the potential to enhance performance by up to 30%. Furthermore, to understand the capability of the studied models in identifying different types of vulnerabilities, we stratify our model evaluation by vulnerability type. Our findings indicate that the models achieve 26 percentage points higher F1 scores in detecting vulnerability from specific types, such as information leaks and code injection, and struggle to detect vulnerabilities from other types, like path resolution and predictable return values. Additionally, we find that the models perform poorly in identifying high-severity vulnerabilities, showing further challenges for these models in identifying important vulnerabilities.\nContributions. This paper makes the following contribu-tions:\n\u2022\n\u2022 We introduce Real-Vul, a new vulnerability detection dataset that aims to address the practical limitations of prior datasets.\nWe re-evaluate the performance of existing state-of-the-art vulnerability detection approaches on the Real-Vul dataset. Our evaluation provides a more accurate repre-sentation of the performance of these models when used in realistic settings.\nWe characterize the limitations of the studied approaches in terms of:\n* their embeddings, which show distinct overlaps across vulnerable and uncertain samples, suggesting models' inability to clearly distinguish vulnerable codes from uncertain codes.\n* tendencies to be unable to detect vulnerabilities belong-ing to particular types and severities. Stratifying model performance by vulnerability type and severity allows targeted improvements and efficient resource allocation in addressing distinct security risks.\nFostering open science: To foster future advances to mod-eling approaches for vulnerability detection, we release our Real-Vul dataset, and we also make the scripts used for our experiments publicly available.\u00b9"}, {"title": "2 LIMITATIONS OF EXISTING DATASETS", "content": "As software systems become more complex and larger, the potential for security vulnerabilities also increases. There-fore, it is crucial to have tools to discover these vulnerabil-ities. Machine learning models have proven to be effective in understanding code and detecting vulnerabilities. How-ever, obtaining and evaluating large datasets in a realistic setting [1, 7, 11] remains challenging. Existing studies on vulnerability detection [7, 11] rely on datasets that are based on various criteria and may not reflect reality accurately. These datasets can be classified into three categories: syn-thetic, oracle-based, and real-world datasets. In this section, we describe each category and their limitations.\nSynthetic dataset (e.g., SARD [3]). The Software Assurance Reference Dataset (SARD) is an example of a synthetic dataset. It contains a vast number of artificially produced C/C++ programs containing numerous security vulnerabil-ities. SARD has been created through automated methods like fuzzing or genetic algorithms. The SARD dataset's limitation lies in containing only artificially generated vul-nerabilities, which may not accurately represent real-world software vulnerabilities. This dataset does not reflect the complexities and variations found in actual code written by developers, potentially limiting researchers from evaluating their models in a realistic setting.\nOracle-based dataset (e.g., D2A [12]). D2A is an exam-ple of an Oracle-based dataset. Unlike synthetic datasets, oracle-based datasets rely on third-party sources such as static analysis tools to provide labels for collected data samples. Although such datasets offer more complexity than synthetic datasets, they may not fully represent real-world vulnerabilities due to oversimplification. The inaccuracy of the labeling heavily affects the dataset's reliability which threatens the usability of the vulnerability detection models trained using this type of dataset.\nReal-world dataset (e.g., Big-Vul [7], Devign [11], Re-Veal [1]). Big-Vul and ReVeal datasets are examples of real-world datasets for vulnerability detection. They are more diverse than synthetic and oracle-based datasets; real-world datasets are generated using data available in issue-tracking systems and source code repositories.\nBig-Vul dataset [7] is composed of C/C++ functions collected from 348 open-source GitHub projects spanning from 2002 to 2019. The Devign dataset has been cu-rated from Linux Kernel, QEMU, Wireshark, and FFmpeg projects, whereas the ReVeal dataset [1] is collected from the Chromium and Debian project. The above-mentioned datasets still have limitations. First, such datasets fail to entirely capture vulnerability detection in a realistic setting where a comprehensive scan of the entire source code of a project would be performed. In realistic setting, the vulnerable functions are rare compared to uncertain ones, leading to an imbalance in their occurrence during scans. Conversely, prior datasets typically categorize altered or post-fix functions as uncertain and unchanged or pre-fix functions as vulnerable, resulting in an unrealistic nearly equal ratio of uncertain to vulnerable functions. Moreover, since vulnerable functions are rare in the real world, these datasets tend to be smaller and less diverse. Training on such a limited dataset will result in a biased model. Addi-tionally, since the test dataset will also be biased, it will be challenging to detect these biases in the model.\nSecond, these datasets may suffer from label inconsistency. Figure 1 elaborates further on the problem of label inconsis-tency. In the figure, consider two vulnerability-fixing com-mits, X1 and X2, with X\u2081 occurring before X2. In X1, the vulnerable function is Function B (note that after fixing it, it became Function B'). Following the data collection policy of previous studies (e.g., ReVeal [1]), Function B would be labeled as vulnerable, while the other unchanged functions (Function A and C) would be labeled as uncertain. However, as we progress further in the timeline, we encounter commit X2, which fixes the vulnerability in \"Function A\". Now, as per the aforementioned policy, \"Function A\" would be labeled as vulnerable, while Function B'and C would be labeled as uncertain. Consequently, the dataset will contain two entries for Function A, where in one case, it is marked as vulnerable and in the other as uncertain. This discrepancy in labeling creates the label inconsistency issue. In fact, we verified the presence of label inconsistency in existing datasets and found that 15% vulnerable samples in the ReVeal dataset have been listed as uncertain samples.\nDue to the aforementioned limitations, current deep learning models for vulnerability detection (e.g., [1, 4, 8, 11, 13]) could result in unrealistic evaluations compared to real-world usage. Additionally, the reported model performance might be either exaggerated or understated. Therefore, in this study, we build Real-Vul, a new dataset that takes into consideration (1) a realistic setting where a comprehensive scan of the entire source code of a project would be per-formed; and (2) label consistency. The next section presents Real-Vul and the process we follow to create it."}, {"title": "3 Real-Vul DATASET", "content": "In this section, we introduce our proposed dataset, named Real-Vul, which addresses the limitations of existing vulner-ability detection datasets (Section 2). Table 1 provides an overview of the projects that are incorporated in Real-Vul.\nProject selection. Real-Vul includes separate training and testing datasets designed for training and evaluat-ing vulnerability detection models. To ensure high-quality vulnerable samples, we carefully select projects from the Big-Vul dataset [7] based on the number of vulnerabilities in each project. Big-Vul dataset comprises real-world vul-nerable C/C++ functions. It also provides rich metadata for vulnerabilities, including line numbers, vulnerability-fixing commit hashes, CVE IDs, severity rankings, and summaries from public Common Vulnerabilities and Expo-sures (CVE) databases and related source code repositories. Consequently, all samples within the Real-Vul dataset are real and written in C/C++ as well. Our focus on projects with a higher number of vulnerabilities signifies the sub-stantial effort invested in identifying vulnerabilities in these projects [14]. Our selection consists of the top 10 projects with the most vulnerabilities, creating a dataset that is at least two times larger than those used in previous stud-ies [1, 11]. Remarkably, these ten projects encompass 73% of the vulnerable samples from the entire Big-Vul dataset, making Real-Vul a representative subset without its limita-tions.\nCreation of vulnerable samples. To align with real-world scenarios, we adopt a time-based strategy for sample cre-ation in the training and testing datasets. This approach simulates the process of training models on historical data and then identifying vulnerabilities over time. The time-based strategy is described below:\nFor generating vulnerable samples, we start extracting the dates on which the vulnerable functions were fixed using the vulnerability-fixing commit hashes available in the Big-Vul dataset.\nTo organize vulnerable samples, we order the functions based on their vulnerability-fixing dates (i.e., the dates of"}, {"title": "4 STUDY DESIGN", "content": "In this section, we present the models we evaluate using Real-Vul dataset (Section 4.1). Then, we describe the research questions driving our investigation (Section 4.3), and the evaluation metrics (Section 4.2).\nWe choose four state-of-the-art deep learning-based mod-els, namely LineVul [8], DeepWukong [4], ReVeal [1], and IVDetect [9] to conduct our experiments. Next, we briefly describe the architectural details of these models.\nLineVul [8] is a deep learning-based model built using CodeBERT [15]. We opt to include LineVul in our analysis as it is the state-of-the-art deep learning-based vulnerability detection model that utilizes a sequence-based approach for vulnerability detection. LineVul takes in a chunk as input and classifies it as a vulnerable or uncertain chunk utilizing the CodeBERT model. A chunk is a sequence of code tokens generated from source code programs. The LineVul has been trained on the Big-Vul dataset which has been curated using vulnerability-fixing commits from open-source projects.\nDeepWukong [4] is a leading vulnerabil-ity detection model utilizing a Graph Neural Network (GNN) [16] for in-depth code analysis. It converts code into Program Dependence Graphs (PDGs), then into XFGs (sub-graphs), highlighting the data and control flow dependen-cies within. DeepWukong evaluates these XFGs to identify vulnerable code segments, employing the SARD dataset for training. This model has been included in the study due to its advanced graph-based approach, which sets a new benchmark for accurately detecting code vulnerabilities.\nReVeal [1] is a model that finds vulnerable code using graph neural networks. It has been chosen because it gathers data similarly to Real-Vul, focusing on unchanged functions from specific commits as non-vulnerable exam-ples. ReVeal combines control flow, data flow, syntax trees, and dependency graphs into a Code Property Graph (CPG) for comprehensive code analysis. It trains on a dataset cu-rated from the Linux Debian Kernel and Chromium project.\nIVDetect [9] is a tool that aims to provide precise interpretations of detected vulnerabilities. IVDetect incor-porates representation learning and a graph-based interpre-tation model. It processes code by analyzing control flow, data flow, abstract syntax trees, and program dependency graphs, creating a unified structure for efficient vulnera-bility detection. IVDetect uses datasets created by prior studies [1, 7] comprising projects such as FFmpeg, Qemu, and Chromium."}, {"title": "4.2 Evaluation Metrics.", "content": "The evaluation metric must accurately measure the models' performance on the task at hand. In this study, we use accuracy, precision, recall, F1, and AUC to measure the models' performance.\nAccuracy quantifies the overall correctness of a vulner-ability detection model, representing the proportion of true results (both true positives and true negatives) among the total number of samples that were evaluated. For example, if a model correctly identifies 80 out of 100 functions (vul-nerable or not), its accuracy is 80%. High accuracy indicates effective identification of both vulnerable and uncertain samples with minimal errors.\nPrecision represents the fraction of vulnerabilities de-tected by the model that are truly vulnerabilities. For ex-ample, in the case of the DeepWukong model, it measures how many XFGs predicted as vulnerable are genuinely vulnerable XFGs. A low precision score indicates that the model is incorrectly classifying many uncertain samples as vulnerable, leading to a high number of false positives. Conversely, a high precision score implies that when the model detects a vulnerability, it is likely a real vulnerability.\nRecall, on the other hand, denotes the fraction of actual vulnerabilities in the system that the model successfully detects. For the DeepWukong model, it measures how many true vulnerable XFGs are correctly identified as vulnerable by the model. A high recall score indicates that the model can detect most vulnerabilities correctly, resulting in a low number of false negatives.\nThe F1 score combines precision and recall, providing an overall assessment of how well these two measurements are balanced. By considering both precision and recall, the F1-score offers a more complete understanding of the model's effectiveness in vulnerability detection.\nArea Under the Curve (AUC) evaluates a model's ca-pability to distinguish between classes, like differentiating vulnerable from uncertain XFGs in the DeepWukong model. A high AUC score indicates that the model is proficient in accurately distinguishing between vulnerable and uncertain XFGs. Notably, in a balanced dataset, a random model would achieve an AUC of 0.50."}, {"title": "4.3 Research Questions", "content": "We introduce two research questions (RQs) and explain the motivation behind each one.\nRQ1: How do the DeepWukong, LineVul, ReVeal, and IVDetect models perform in a realistic evaluation setting compared to the evaluation setting used in the original studies?\nDeepWukong, LineVul, ReVeal, and IVDetect are four state-of-the-art vulnerability detection models that have demon-strated promising results in identifying security vulnerabili-ties. However, we contend that the datasets used to evaluate these models do not accurately represent real-world usage, where the models would scan entire project source code files (see Section 2 for a comprehensive overview of dataset limitations). For instance, the DeepWukong model heavily relies on the SARD dataset for evaluation, which contains artificially generated samples lacking the complexities of real-world vulnerabilities. Similarly, the LineVul model used the Big-Vul dataset [7], which suffers from a scarcity of uncertain samples. Additionally, the ReVeal dataset also suf-fers from limited uncertain samples and significant \"label-inconsistency.\" In this RQ, we evaluate the DeepWukong, LineVul, ReVeal, and IVDetect models using our Real-Vul test dataset, which provides a more realistic representation of vulnerability detection model performance in practical scenarios.\nRQ2: How do the DeepWukong, LineVul, ReVeal, and IVDetect models perform in a realistic evaluation setting when trained using a similar realistic training dataset?\nIn RQ1, we utilize the Real-Vul dataset for model evaluation. However, we recognize that the training dataset employed to train these models might not accurately represent the same distribution as the evaluation dataset (i.e., Real-Vul). Doing so can lead to poor model performance as the training and testing datasets are from different distributions. To achieve optimal results, it is essential for both datasets to share the same distribution as the data encountered in practical usage. Consequently, in RQ2, we investigate the impact of data distribution on model performance by train-ing and testing the models using our Real-Vul dataset. This exploration will enable us to ascertain whether employing a training dataset representative of the same distribution as the evaluation dataset leads to enhanced model perfor-mance."}, {"title": "5 PRELIMINARY ANALYSIS", "content": "Our study aims to evaluate the performance of the Deep-Wukong, LineVul, and ReVeal models in a more realis-tic setting. As a first step towards our goal, we run the DeepWukong, LineVul, ReVeal, and IVDetect models on the SARD, the Big-Vul, the ReVeal, and the IVDetect datasets, respectively, and verify the findings reported in the original papers. By doing so, we aim to ensure that the results previ-ously reported are reliable and can be replicated. Addition-ally, we use the results obtained from these experiments as our baseline models, which we compare to the performance of the models in our introduced realistic settings.\nApproach. To evaluate the DeepWukong, LineVul, ReVeal, and IVDetect models on the dataset used in their studies, we first download the SARD dataset,\u00b2 the Big-Vul dataset,\u00b3 the ReVeal dataset,\u2074 used in DeepWukong [4], LineVul [8], ReVeal [1], and IVDetect [9] studies, respectively. We create the DeepWukong model inputs (XFGs), the LineVul model inputs (chunks), the ReVeal model inputs (CPG), and the IVDetect models input PDG using the SARD dataset, the Big-Vul dataset, the ReVeal dataset, and the IVDetect dataset samples, respectively. The generated XFG, chunk, CPG, and PDG datasets are split into training and testing datasets where 80% of the samples belong to the training dataset and the remaining 20% samples belong to the testing dataset."}, {"title": "6 RESULTS", "content": "In this section, we present our experimental results with respect to each RQ.\nRQ1: How do the DeepWukong, LineVul, ReVeal, and IVDetect models perform in a realistic evaluation setting compared to the evaluation setting used in the original studies?\nApproach. In this RQ, we evaluate the DeepWukong, the LineVul, the ReVeal, and the IVDetect models using the Real-Vul testing dataset described in Section 3. We use the Line-Vul, ReVeal, and the IVDetect model trained in Section 5 for the evaluation, i.e., we train using the same method as the original LineVul, ReVeal, and IVDetect studies, respectively, while using Real-Vul testing dataset for evaluation. Note that LineVul, ReVeal, and IVDetect models are trained using real-world vulnerability datasets. Since the DeepWukong model in Section 5 is trained using SARD (an artificially created vulnerability dataset), we train a new DeepWukong model using the Big-Vul dataset for evaluation to simulate a fair comparison with the other two models. First, we generate the DeepWukong model inputs (XFGs) using the Big-Vul dataset. We obtain a total of 28,294 vulnerable XFGs and 639,047 uncertain XFGs. Similar to preliminary analysis, we train the DeepWukong model for 50 epochs using the generated XFGs.\nTo evaluate the trained models (DeepWukong, LineVul, ReVeal, and IVDetect) using the Real-Vul testing dataset, we first generate the model inputs (XFG, chunk, CPG, and PDG) for the Real-Vul testing dataset. These samples are then evaluated using the respective trained models.\nResults. Table 3 shows the results of the performance of the DeepWukong, LineVul, ReVeal, and IVDetect models eval-uated using the Real-Vul testing dataset. The DeepWukong model obtains an accuracy, precision, recall, F1-score, and AUC of 91%, 1%, 87%, 2%, and 62%, respectively. When comparing these results to those obtained in Section 5 (i.e., compared to the results of the DeepWukong model trained and tested using the SARD dataset), we observe a decrease of 7, 86, 11, and 91 percentage points in accuracy, precision, recall, and F1-score, respectively. The LineVul model ob-tains an accuracy, precision, recall, F1-score, and AUC of 89%, 1%, 90%, 2%, and 58%, respectively. In comparison to the results obtained in Section 5 (i.e., compared to the results of the LineVul model trained and tested using the Big-Vul dataset), we observe a decrease of 7, 95, 88, and 27 percentage points in accuracy, precision, F1-score, and AUC, respectively. However, we also observe an increase of 6 percentage points in the recall.\nThe ReVeal model demonstrates an 89% accuracy rate, with precision, recall, and F1-score values of 10%, 80%, and 17%, respectively. When comparing these results to the results from Section 5 where the ReVeal model was trained and tested using the ReVeal dataset, we observe a decrease of 19, 21, and 17 percentage points in precision, F1-score, and AUC, respectively, but a 21 percentage points improvement in recall.\nThe IVDetect model exhibits an accuracy of 85%, along with precision, recall, and F1-score metrics of 2%, 84%, and 2% respectively. In comparison to the outcomes from Section 5, where the IVDetect model underwent training and testing using the IVDetect dataset, a notable reduction of 37 percentage points in precision, 22 percentage points in the F1-score, and 24 percentage points in AUC can be observed; however, an enhancement of 21 percentage points in recall is also observed.\nOverall, our findings reveal a substantial decrease in pre-cision for all the models. This suggests that the predictions made by these models contain a considerable number of false positives, which can have a huge impact on the ef-fectiveness of vulnerability detection. Specifically, the Deep-Wukong model generated 439,494 false positives, while the LineVul, ReVeal, and IVDetect models produced 114,629, 320,128, and 512,284 false positives, respectively. Such a large number of false positives may have a large impact on the usability of these models in practice.\nTo understand the models' performance in terms of class separation, we follow the same approach used in Section 5, and plot embeddings of the models in two-dimensional space. It is worth noting that the embeddings are generated for the test split of Real-Vul dataset. Figure 3 presents the scatter plots of model embeddings. For all the models, we observe a substantial overlap between vulnerable and un-certain samples of the Real-Vul dataset, which indicates that the models do not distinguish between the classes clearly. This lack of class separation explains the high number of false positives produced by the DeepWukong, LineVul, ReVeal, and IVDetect models.\nOverall, our findings underscore the importance of eval-uating vulnerability detection models using datasets that reflect realistic settings. In this regard, our results demon-strate the critical role played by the Real-Vul dataset in understanding the true capabilities of such models. By uti-lizing this dataset, we gain a more accurate understanding of the performance of vulnerability detection models and can identify areas that require improvement."}, {"title": "ANSWER:", "content": "Existing deep learning-based vulnerability detection models", "settings.\nRQ2": "How do the DeepWukong, LineVul, ReVeal, and IVDetect models perform in a realistic evaluation setting when trained using a similar realistic training dataset?\nApproach. In RQ1, we use the Real-Vul dataset for evalua-tion purposes, i.e., we evaluate the effectiveness of our mod-els using Real-Vul. However, in this RQ, we use the Real-Vul dataset for both model training and evaluation. The training dataset is imbalanced (higher uncertain samples). Hence, we also investigate the impact of the imbalanced dataset on model performance by training additional models on a balanced dataset. To create a balanced dataset for training, we randomly select uncertain samples equal to the number of vulnerable samples.\nOverall, we train a total of eight models (one imbalanced-trained and one balanced-trained model for DeepWukong, LineVul, ReVeal, and IVDetect techniques) using the same training parameters used in Section 5. We evaluate the trained DeepWukong, LineVul, ReVeal, and IVDetect models using the same XFG, chunk, CPG, and PDG test dataset used in RQ1 (i.e., Real-Vul test dataset).\nResults. Table 4 shows the results of the DeepWukong, LineVul, ReVeal, and IVDetect models that are trained us-ing imbalanced datasets. The DeepWukong and LineVul models both exhibit an accuracy of 99%, with precision, recall, F1-scores at 0%, and AUC at 50%. In comparison, the ReVeal model exhibits 95% accuracy, 2% precision, 10% recall, 3% F1-score, and 50% AUC. Similarly, the IVDetect model demonstrates an 81% accuracy, with precision, recall, F1-score, and AUC recorded at 2%, 1%, 0%, and 49.6%, respectively. The accuracy is 99% for both the DeepWukong and LineVul models, while the precision, recall, F1-scores, and AUC are 0% and 50%, respectively. For the ReVeal model, the accuracy, precision, recall, F1-score, and AUC are 95%, 2%, 10%, 3%, and 50%, respectively. A common trend across the models is that the precision, recall, and F1-scores are substantially reduced with respect to the results in Section 5 and RQ1.\nThe high accuracy scores of the models are due to the imbalanced nature of the datasets, which have a larger number of uncertain samples. This imbalance leads models to often predict samples as uncertain, boosting accuracy but reducing recall, precision, and F1-scores. The models have difficulty learning from the fewer vulnerable samples because they focus on minimizing a loss function, which discourages wrong predictions. Consequently, they tend to classify samples as uncertain to reduce errors despite inaccuracies. However, the ReVeal model performs better as it uses the SMOTE [18", "19": "shows that models using graphs usually perform a bit better than those using text, which helps us understand why ReVeal (using graphs) performed better than LineVul (using text).\nWe assess the models when trained on Real-Vul and tested on their corresponding original datasets (such as SARD, ReVeal, BigVul, and IVDetect) to cross-check the im-pact of realistic settings. Our experiments indicate a substan-tial improvement in performance, with AUC improvements ranging from 8 to 32 percentage points compared to the model trained and tested solely on the Real-Vul dataset. Specifically, on the original test dataset, the DeepWukong model achieved an accuracy of 88%, precision of 81%, recall of 89%, F1-score of 84%, and an AUC of 81%, observing im-provements over its Real-Vul dataset performance in every metric except accuracy, which decreased by one percentage point. The LineVul model achieved an accuracy of 80%, precision of 84%, recall of 69%, F1-score of 75%, and AUC of 79.9%, showing declines in accuracy and recall by 19 and 30 percentage points, respectively, but gains in precision, F1-score, and AUC. The ReVeal model experienced a 5 percent-age point decrease in accuracy while seeing improvements of 4, 6, 5, and 8 percentage points in precision, recall, F1-score, and AUC, respectively. The IVDetect model achieved an accuracy of 78%, precision of 28%, recall of 48%, F1-score of 35%, and AUC of 78.3%, with a decrease in accuracy by five percentage points but increases in all other metrics"}]}