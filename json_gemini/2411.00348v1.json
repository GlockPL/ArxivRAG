{"title": "Attention Tracker: Detecting Prompt Injection Attacks in LLMs", "authors": ["Kuo-Han Hung", "Ching-Yun Ko", "Ambrish Rawat", "I-Hsin Chung", "Winston H. Hsu", "Pin-Yu Chen"], "abstract": "Large Language Models (LLMs) have revolutionized various domains but remain vulnerable to prompt injection attacks, where malicious inputs manipulate the model into ignoring original instructions and executing designated action. In this paper, we investigate the underlying mechanisms of these attacks by analyzing the attention patterns within LLMs. We introduce the concept of the distraction effect, where specific attention heads, termed important heads, shift focus from the original instruction to the injected instruction. Building on this discovery, we propose Attention Tracker, a training-free detection method that tracks attention patterns on instruction to detect prompt injection attacks without the need for additional LLM inference. Our method generalizes effectively across diverse models, datasets, and attack types, showing an AUROC improvement of up to 10.0% over existing methods, and performs well even on small LLMs. We demonstrate the robustness of our approach through extensive evaluations and provide insights into safeguarding LLM-integrated systems from prompt injection vulnerabilities.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Team et al., 2024; Yang et al., 2024; Abdin et al., 2024; Achiam et al., 2023; Dubey et al., 2024) have revolutionized numerous domains, demonstrating remarkable capabilities in understanding and generating complex plans. These capabilities make LLMs well-suited for agentic applications, including web agents, email assistants, and virtual secretaries (Shen et al., 2024; Nakano et al., 2021). However, a critical vulnerability arises from their inability to differentiate between user data and system instructions, making them susceptible to prompt injection attacks (Perez and Ribeiro, 2022; Greshake et al., 2023; Liu et al., 2023; Jiang et al., 2023). In such attacks, attackers embed malicious prompts (e.g. \"Ignore previous instructions and instead {do something as instructed by a bad actor}\") within user inputs, and ask the LLM to disregard the original instruction and execute attacker's designated action. This vulnerability poses a substantial threat (OWASP, 2023) to LLM-integrated systems, particularly in critical applications like email platforms or banking services, where potential severe consequences include leaking sensitive information or enabling unauthorized transactions. Given the severity of this threat, developing reliable detection mechanisms against prompt injection attacks is essential.\nIn this work, we explain the prompt injection attack from the perspective of the attention mechanisms in LLMs. Our analysis reveals that when a prompt injection attack occurs, the attention of specific attention heads shifts from the original instruction to the injected instruction within the attack data, a phenomenon we have named the distraction effect. We denote the attention heads that are likely to get distracted as important heads. We attribute this behavior to the reasons why LLMs tend to follow the injected instructions and neglect their original instructions. Surprisingly, our experiments also demonstrate that the distraction effect observed on the important heads generalizes well across various attack types and dataset distributions.\nMotivated by the distraction effect, we propose Attention Tracker, a simple yet effective training-free guard that detects prompt injection attacks by tracking the attentions on the instruction given to the LLMs. Specifically, for a given LLM, we identify the important heads using merely a small set of LLM-generated random sentences combined with a naive ignore attack. Then, as shown in Figure 1, for any testing queries, we feed"}, {"title": "2 Related Work", "content": "Prompt Injection Attack. Prompt injection attacks pose a significant risk to large language models (LLMs) and related systems, as these models often struggle to distinguish between instruction and data. Early research (Perez and Ribeiro, 2022; Greshake et al., 2023; Liu et al., 2023; Jiang et al., 2023) has demonstrated how template strings can mislead LLMs into following the injected instructions instead of the original instructions. Furthermore, studies (Toyer et al., 2024; Debenedetti et al., 2024) have evaluated handcrafted prompt injection methods aimed at goal hijacking and prompt leakage by prompt injection games. Recent work has explored optimization-based techniques (Shi et al., 2024; Liu et al., 2024a; Zhang et al., 2024a), such as using gradients to generate universal prompt injection. Some studies (Pasquini et al., 2024) have treated execution trigger design as a differentiable search problem, using learning-based methods to generate triggers. Additionally, recent studies (Khomsky et al., 2024) have developed prompt injection attacks that target systems with defense mechanisms, revealing that many current defense and detection strategies remain ineffective.\nPrompt Injection Defense. Recently, researchers have proposed various defenses to mitigate prompt injection attacks. One line of research focuses on enabling LLMs to distinguish between instructions and data. Early studies (Jain et al., 2023; Hines et al., 2024; lea, 2023) employed prompting-based methods, such as adding delimiters to the data portion, to separate it from the prompt. More recent work (Piet et al., 2024; Suo, 2024; Chen et al., 2024; Wallace et al., 2024; Zverev et al., 2024) has fine-tuned or trained LLMs to learn the hierarchical relationship between instructions and data. Another line of research focuses on developing detectors to identify attack prompts. In Liu et al. (2024b), prompt injection attacks are detected using various techniques, such as querying the LLM itself (Stuart Armstrong, 2022), the Known-answer method (Yohei, 2022), and PPL detection (Alon and Kamfonas, 2023). Moreover, several companies such as ProtectAI and Meta (ProtectAI.com, 2024a; Meta, 2024; ProtectAI.com, 2024b) have also trained detectors to identify malicious prompts. However, existing detectors demand considerable computational resources and often produce inaccurate results. This work proposes an efficient and accurate"}, {"title": "3 Distraction Effect", "content": "3.1 Problem Statement\nFollowing Liu et al. (2024b), we define a prompt injection attack as follows:\nDefinition 1. In an LLM-Integrated Application, given an instruction $I_t$ and data $D$ for a target task $t$, a prompt injection attack inserts or modifies the data $D$ sequentially with the separator $S$ and the injected instruction $I_j$ for the injected task $j$, causing the LLM-Integrated Application to accomplish task $j$ instead of $t$.\nAs illustrated in Figure 1, an exemplary instruction $I_t$ can be \u201cAnalyze the attitude of the following sentence\". Typically, the user should provide data"}, {"title": "3.2 Background on Attention Score", "content": "Given a transformer with L layers, each containing H heads, the model processes two types of inputs: an instruction I with N tokens, followed by data D with M tokens, to generate the output. At the first output token, we define:\n$Attn_{l,h}(I) = \\sum_{i\\in I} a_i^{l,h}, a_i^{l,h} = \\frac{1}{H} \\sum_{h=1}^{H} a_i^{l,h}$ \nwhere $a_i^{l,h}$ represents the softmax attention weights assigned from the last token of the input prompt to token i in head h of layer l."}, {"title": "3.3 A Motivating Observation", "content": "In this section, we analyze the reasons behind the success of prompt injection attacks on LLMs. Specifically, we aim to understand what mechanism within LLMs causes them to \u201cignore\u201d the original instruction and follow the injected instruction instead. To explore this, we examine the attention patterns of the last token in the input prompts, as it has the most direct influence on the LLMs' output.\nWe visualize $Attn_{l,h}(I)$ and a values for normal and attack data using the Llama3-8B (Dubey et al., 2024) on the Open-Prompt-Injection dataset (Liu et al., 2024b) in Figure 2(a) and Figure 2(b), respectively. In Figure 2(a), we observe that the attention maps for normal data are much darker than those for attacked data, particularly in the middle and earlier layers of the LLM. This indicates that the last token's attention to the instruction is significantly higher for normal data than for attack data in specific attention heads. When inputting attacked data, the attention shifts away from the original instruction towards the attack data, which we refer to as the distraction effect. Additionally, in Figure 2(b), we find that the attention focus shifts from the original instruction to the injected instruction in the attack data. This suggests that the separator string helps the attacker shift attention to the injected instruction, causing the LLM to perform the injected task instead of the target task.\nTo further understand how various prompt injection attacks distract attentions, we also visualize their effect separately in Figure 3. In the figure, we plot the distribution of the aggregated $Attn_{l,h}(I)$ across all attention heads (i.e."}, {"title": "4 Prompt Injection Detection using Attention", "content": "In this section, we introduce Attention Tracker, a prompt injection detection method leveraging the distraction effect introduced in Section 3.3."}, {"title": "4.1 Finding Important Heads", "content": "As shown in Figure 2, it is evident that the distraction effect does not apply to every head in the LLMs. Therefore, to utilize this effect for prompt injection detection, the first step is to identify the specific heads that exhibit the distraction effect, which we refer to as important heads.\nGiven a dataset consisting of a set of normal data $D_N$ and a set of attack data $D_A$, we collect the $Attn_{l,h}(I)$ across all samples in $D_N$, denoted as $S_N^{l,h}$, and the $Attn_{l,h} (I)$ across all samples in $D_A$, denoted as $S_A^{l,h}$. Formally, we define:\n$S_N^{l,h} = \\{Attn_{l,h}(I)\\}_{I\\in D_N}, S_A^{l,h} = \\{Attn_{l,h}(I)\\}_{I\\in D_A}$.\nUsing $S_N^{l,h}$ and $S_A^{l,h}$, we calculate the candidate score $score_{cand}(D_N, D_A)$ for a specific attention head (h,l) and use this score to find the set of important heads $H_i$ as follows:\n$score_{cand}(D_N, D_A) = \\frac{\\mu_{S_N^{l,h}} - k\\cdot\\sigma_{S_N^{l,h}} - (\\mu_{S_A^{l,h}} + k\\cdot\\sigma_{S_A^{l,h}})}{\\sigma_{S_N^{l,h}} + \\sigma_{S_A^{l,h}}}$\n$H_i = \\{(l, h) | score_{cand}(D_N, D_A) > 0\\}$\nwhere k is a hyperparameter controlling the shifts of normal/attack candidate scores, and \u00b5 and \u03c3 are"}, {"title": "4.2 Prompt Injection Detection with Important Heads", "content": "With the distraction effect and the important heads discussed in Section 3.3 and 4.1, we now formally propose Attention Tracker. Given the instruction and user query ($I_{test}$, $U_{test}$), we test them by inputting them into the target LLM and calculate the focus score defined as:\n$FS = \\frac{1}{|H_i|}\\sum_{(l,h)\\in H_i} Attn_{l,h}(I)$.\nUsing the focus score FS, which measures the LLM's attention to the instruction, we can determine whether an input contains a prompt injection. Our detection method is summarized in Algorithm 1. The notation \u2295 means text concatenation. Notably, since the important heads are pre-identified,"}, {"title": "5 Experiments", "content": "5.1 Experiment Setup\nAttack benchmarks. To evaluate the effectiveness of Attention Tracker, we compare it against other prompt injection detection baselines using data from the Open-Prompt-Injection benchmark (Liu et al., 2024b), and the test set of deepset prompt injection dataset (deepset, 2023). Both datasets include normal and attack data for evaluation. Detailed settings for each dataset can be found in Appendix A.2.\nModels. We evaluate different methods on four open-sourced LLMs, with model sizes ranging from 1.5 billion to 9 billion parameters: (a) Qwen2-1.5B-Instruct (Yang et al., 2024), (b) Phi-3-mini-4k-instruct (Abdin et al., 2024), (c) Meta-Llama-3-8B-Instruct (Dubey et al., 2024), and (d) Gemma-2-9b-it (Team et al., 2024). For models (a), (b), and (c), which support the chat template for both system and user prompts, we place the instruction in the system prompt and the data in the user prompt. In"}, {"title": "5.2 Performance Evaluation and Comparison with Existing Methods", "content": "As shown in Table 1, Attention Tracker consistently outperforms existing baselines, achieving an AUROC improvement of up to 3.1% on the Open-Prompt-Injection benchmark (Liu et al., 2024b) and up to 10.0% on the deepset prompt injection dataset (deepset, 2023). Among training-free methods, Attention Tracker demonstrates even more significant gains, achieving an average AUROC improvement of 31.3% across all models on the Open-Prompt-Injection benchmark and 20.9% on the deepset prompt injection dataset."}, {"title": "5.3 Qualitative Analysis", "content": "In this section, we visualize the distribution of attention aggregation for important heads in both normal and attack data. Using a grammar correction task and an ignore attack as examples, Figure 4 illustrates that the attack data significantly reduces attention on the instruction and shifts focus to the injected instruction. For further qualitative analysis, please refer to Appendix A.5."}, {"title": "5.4 Discussion and Ablation Studies", "content": "Generalization Analysis. To demonstrate the generalization of important heads (i.e., specific heads consistently showing distraction effect across different prompt injection attacks and datasets), we visualized the mean difference in $Attn_{l,h}(I)$ scores on Qwen-2 model (Yang et al., 2024) between normal and attack data from three datasets: the deepset prompt injection dataset (deepset, 2023), the Open-Prompt-Injection benchmark (Liu et al., 2024b), and a set of LLM-generated data used for head selection in Section 4.1. As shown in Figure 5, although the magnitude of differences in $Attn_{l,h}(I)$ varies across datasets, the relative differences across attention heads remain consistent. In other words, the attention heads with the most distinct difference are consistent across different datasets, indicating that the distraction effect generalizes well across various data and attacks. For the LLM-generated data, we merely use a basic prompt injection attack (e.g., ignore previous instruction and ...), demonstrating that important heads remain consistent even with different attack methods. This further validates the effectiveness of identifying"}, {"title": "6 Conclusion", "content": "In this paper, we conducted a comprehensive analysis of prompt injection attacks on LLMs, uncovering the distraction effect and its impact on attention mechanisms. Our proposed detection method, Attention Tracker, significantly outperforms existing baselines, demonstrating high effectiveness even when utilizing small LLMs. The discovery"}, {"title": "Limitation", "content": "A limitation of our approach is its reliance on internal information from LLMs, such as attention scores, during inference for attack detection. For closed-source LLMs, only model developers typically have access to this internal information, unless aggregated statistics, such as focus scores, are made available to users."}, {"title": "Ethics Statement", "content": "With the growing use of LLMs across various domains, reducing the risks of prompt injection is crucial for ensuring the safety of LLM-integrated applications. We do not anticipate any negative social impact from this work."}, {"title": "A Appendix", "content": "A.1 Introduction of Different Attacks in Figure 3\nIn this section, following Liu et al. (2024b), we will introduce the strategy of ignore, escape, fake complete and combine attack.\n\u2022 Naive Attack: This attack does not use a separator; it simply concatenates the injected instruction directly with the data.\n\u2022 Escape Attack: This attack utilizes special characters, like \u201c\\n,\u201d to trick the LLM into perceiving a context change.\n\u2022 Ignore Attack: This approach uses phrases such as \"Ignore my previous instructions\" to explicitly instruct the LLM to disregard the original task.\n\u2022 Fake Complete Attack: This method presents a fake response to the original task, misleading the LLM into thinking the task is completed, thereby prompting it to execute the injected task instead.\n\u2022 Combined Attack: This attack combines separators from the Escape Attack, Ignore Attack, and Fake Complete Attack, using them together to maximize the effect."}, {"title": "A.2 Dataset Settings", "content": "For Open-Prompt-Injection benchmark (Liu et al., 2024b), We use five tasks (natural language inference, grammar correction, sentiment analysis, spam detection, and duplicate sentence detection), each with 50 different target-inject data, and four type of attacks (naive, ignore, escape and combine), totaling 2,000 testing queries. For deepset prompt injection dataset (deepset, 2023), since there is no instruction in the dataset, we directly use \"Say xxxxxx\" as the instruction for every queries."}, {"title": "A.3 Baseline Settings", "content": "In this section, we provide a more detailed explanation of the settings for our baselines: Prompt-Guard, Protect AI detector, LLM-based Detection and Known-Answer Detection.\nLLM-based Detection. Following Liu et al. (2024b), the prompt for using LLM-based detection is:"}, {"title": "A.7 Position of Important Heads.", "content": "In addition to the number of heads that we should select for the detector, we are also interested in the positions of the attention heads that exhibit more pronounced distraction effect. As shown in Figure 8, we visualize the $Attn_{l,h}(I)$ of each attention heads. Interestingly, the visualization reveals a similar pattern across models: most important heads are located in the first few layers or the middle layers. This shows that attention heads in the first few layers or the middle layers may have a larger influence on the instruction-following behavior of LLMs."}, {"title": "A.8 Impact of Itest Selection", "content": "In this section, we experimented with different selections of $I_{test}$ to evaluate their impact on the final results. As shown in Table 4, we report the AUROC scores on the Deepset dataset (deepset, 2023) for the Qwen-2-1.8B model (Yang et al., 2024). In the table, we randomly generated various sentences as $I_{test}$. The results indicate that the AUROC score remains consistently high regardless of the instruction used. However, when $I_{test}$ consists of specific instructions such as \"Say xxxxx\u201d or \u201cOutput xxxxx,\u201d which explicitly direct the LLM's output, the score tends to be higher."}]}