{"title": "Solving Stochastic Orienteering Problems with Chance Constraints\nUsing a GNN Powered Monte Carlo Tree Search", "authors": ["Marcos Abel Zuzu\u00e1rregui", "Stefano Carpin"], "abstract": "Leveraging the power of a graph neural network\n(GNN) with message passing, we present a Monte Carlo Tree\nSearch (MCTS) method to solve stochastic orienteering prob-\nlems with chance constraints. While adhering to an assigned\ntravel budget the algorithm seeks to maximize collected reward\nwhile incurring stochastic travel costs. In this context, the\nacceptable probability of exceeding the assigned budget is ex-\npressed as a chance constraint. Our MCTS solution is an online\nand anytime algorithm alternating planning and execution that\ndetermines the next vertex to visit by continuously monitoring\nthe remaining travel budget. The novelty of our work is that the\nrollout phase in the MCTS framework is implemented using a\nmessage passing GNN, predicting both the utility and failure\nprobability of each available action. This allows to enormously\nexpedite the search process. Our experimental evaluation shows\nthat with the proposed method and architecture we manage\nto efficiently solve complex problem instances while incurring\nin moderate losses in terms of collected reward. Moreover,\nwe demonstrate how the approach is capable of generaliz-\ning beyond the characteristics of the training dataset. The\npaper's website, open-source code, and supplementary docu-\nmentation can be found at ucmercedrobotics.github.\nio/gnn-sop.", "sections": [{"title": "I. INTRODUCTION", "content": "Orienteering is an APX-hard optimization problem defined\non a weighted graph, G, where each vertex has a reward\nand each edge has a non-negative cost [7]. The goal is to\nplan a path between designated start and end vertices to\nmaximize the total reward collected from visited vertices\nwhile staying within a budget, B, that limits the total path\nlength. This budget is often considered in terms of time,\npower, or distance that can be traveled (see Figure 1). Unlike\nthe traveling salesman problem (TSP), a typical solution to\nan orienteering problem instance does not visit all nodes.\nInstances of the orienteering problem can model various\nreal-world scenarios we encounter daily, such as logistics\n[16], surveillance [14], [25], ridesharing [12], and precision\nagriculture [21], among others. Our interest in this problem\nis driven by its applications in precision agriculture [2],\n[18]\u2013[20], though its range of uses is broad and continually\nexpanding. Most research on orienteering has concentrated\non the deterministic version, where both vertex rewards and\nedge costs are known in advance. However, this is not often"}, {"title": "III. PROBLEM STATEMENT AND BACKGROUND", "content": "In this section, we formally introduce the stochastic ori-\nenteering problem with chance constraints (SOPCC). We\nthen outline our previous solution utilizing MCTS, which\nmotivates the method proposed in this manuscript."}, {"title": "A. Stochastic Orienteering Problem with Chance Con-\nstraints", "content": "The deterministic orienteering problem is defined as fol-\nlows. Let G = (V, E) be a weighted graph with n vertices,\nwhere V is the set of vertices and E is the set of edges.\nWithout loss of generality, we assume G is a complete graph.\nDefine r : V \u2192 R as the reward function assigning a reward\nto each vertex, and c : E\u2192 R+ as the cost function\nassigning a positive cost to each edge. Let vs, ug \u2208 V be\nthe designated start and end vertices, respectively, and let\nB > 0 be a fixed budget. Note that we allow vs = vg for\ncases where the start and end vertices are the same. For a\npath P in G, define R(P) as the sum of the rewards of the\nvertices along P, and C(P) as the sum of the costs of the\nedges in P. The deterministic orienteering problem aims to\nfind\nP* = arg max R(P) s.t. C(P*) \u2264 B\n\u03a1\u0395\u03a0\nwhere II is the set of simple paths in G starting at vs and\nending at vg. A simple path is defined as a path that does not\nrevisit any vertex. Given the connectivity assumption of G,\nrestricting II to simple paths is not limiting. In the stochastic\nversion of the problem, the cost associated with each edge\nis not fixed but rather sampled from a continuous random\nvariable with a known probability density function that has\nstrictly positive support. Specifically, for each edge e \u2208 E we\nassume c(e) is sampled from d(e) where d(e) represents the\nrandom variable modeling the cost of traversing edge e. In\ngeneral, different edges are associated with different random\nvariables. In the stochastic case then, for a given path P the\npath cost C(P) is also a random variable. The constraint\non the path cost C(P) must therefore be expressed using a\nchance constraint, defined formally as follows."}, {"title": "Stochastic Orienteering Problem with Chance\nConstraints (SOPCC)", "content": "With the notation intro-\nduced above, let 0 < Pf < 1 be an assigned failure\nbound. The SOPCC seeks to solve the following\noptimization problem:\nP* = arg max R(P)\n s.t. Pr[C(P*) > B] < Pf.\nRemark: since the random variables associated with the\nedges are assumed to be known, generating random samples\nfor the cost C(P) of a path P is straightforward and does\nnot necessitate computing the probability density function of\nC(P). This is consistent with the assumptions usually made\nwhen using MCTS to solve planning problems [9].\nThe problem definition models a decision maker aiming\nat maximizing the reward collected along a path while\nremaining below the allotted budget. Because the cost of"}, {"title": "B. Monte Carlo Tree Search for SOPCC", "content": "In [2] we introduced a new approach based on MCTS\nto solve the SOPCC. Key to our new solution was the\nintroduction of a novel criterion for tree search dubbed UCTF\nUpper Confidence Bound for Tree with Failures. In this\nsubsection we shortly review this solving method. We refer\nthe reader to [2] for a deeper discussion of our method and\nto [4] for a general introduction to the MCTS methodology.\nNote that in [2] when deciding which vertex to add to the\nroute, MCTS only considers the K-nearest neighbors to the\ncurrent vertex. In the version implemented in this paper to\ntrain the GNN described in the next section, we remove this\nfeature and instead consider all vertices. The reason is that\nin [2] we reduced the number of possibilities to expedite\nthe online search, while here training is done offline. We\ncan therefore consider a larger search space. As pointed\nout in [17] (chapter 8) a solution based on MCTS relies\non the definition of four steps: selection, expansion, rollout\nand backup. In the following, we sketch how these can be\ncustomized for solving SOPCC.\n1) Selection: Selection, also known as tree policy, guides\nthe search from the root to a leaf node based on the value\nassociated with the nodes in the tree. At every level, each\nchild of the current node is evaluated using a metric, and\nthe search then proceeds to the node with the highest value.\nUniversal Confidence bound for Trees (UCT) is commonly\nused to attribute values to nodes [10] and defines the tree\npolicy. UCT, however, aims at maximizing reward only and\nis therefore not suitable for use in problems with chance\nconstraints where high reward could lead to violations of\nthe constraint. To overcome this limit, in [2] we introduced\nUCTF, defined as follows\nUCTF(vj) = Q[vj](1 \u2212 F[j]) + 2\u221alog(N[up])\nVN[j]\n(1)\nwhere the term (1 \u2013 F[vj]) is new. In Eq. (1), Q[vi] is the\nestimate of how much reward will be collected by adding\nvj to the route, F[vj] is a failure probability estimate for a\npath going through vj, N[vj] is the number of times vj has\nbeen explored and N[up] is the number of times its parent\nhas been explored. Critically, in our original approach both\nQ and Fare estimated via rollout, while in this paper we\nwill train a GNN to quickly predict these values. The term\n(1-F[j]) penalizes nodes with high estimates for the failure\nvalue. As in the classic MCTS method, the term z balance\nexploration with exploitation.\n2) Expansion: Expansion is the process of adding a node\nto the tree. In our implementation, this is implicitly obtained\nby assigning a UCTF value of infinity to vertices that are still\nunexplored (i.e., for which N[vj] is 0). This way, we ensure\nall children nodes of parent node are explored at least once\nbefore one of the siblings is selected again."}, {"title": "IV. METHODOLOGY", "content": "The main novelty of the method we propose here is a\nsingle value/failure network architecture that predicts both\nthe expected value, Q, and the failure probability, F, of\nan action, vj, to be used in the tree policy UCTF formula\nfrom Eq. (1). Therefore, after appropriate training, the MCTS\nrollout phase with multiple simulations to estimate both Q\nand F is replaced by a single forward pass on each network,\nthus dramatically cutting the computation time. As a result,\nas explained later, when a node is considered for expansion\nall descendants are generated and evaluated at once. This\nis different from the classic MCTS approach where node\ndescendants are added and evaluated one by one. This feature\ngreatly extends the set of actions explored while not requiring\nmore computational time."}, {"title": "B. Message Passing Neural Network", "content": "Given that orienteering problem instances are defined\nover graphs, a message passing neural network (MPNN)\n[26], [27] is the natural choice to implement the value\nand failure networks. MPNNs are graph neural networks in\nwhich, at every iteration of learning, nodes of the graph\nshare information with neighboring nodes to generate an\nD-dimensional embedding for each node or for the entire\ngraph. In a typical message passing framework, the message\nupdate function encompasses node attributes as well as edge\nattributes, both of which are critical in the SOPCC problem"}, {"title": "C. Attributes", "content": "For each node in the graph, we introduce an eight di-\nmensional vector of attributes that will be used to define hv\nEq. (2) for t = 0. Each node has the following attributes:\n\u2022 a binary value indicating if the vertex has been visited;\n\u2022 its x and y coordinates in the plane;\n\u2022 its reward;\n\u2022 the remaining budget;\n\u2022 a binary value indicating if the node is the start vertex;\n\u2022 a binary value indicating if the node is the end vertex;\n\u2022 a binary value indicating if the node is the vertex where\nthe robot is currently positioned at.\nOur choice for these attributes starts from [24] where a\nsimilar approach was used to solve the traveling salesman\nproblem (TSP). Accordingly, the first three attributes give the\nmodel the spatial awareness that proved to work well in TSP.\nHowever, since SOPCC has more requirements that TSP,\nwe added additional attributes such as reward and residual\nbudget owing to the fact that the nodes are not all uniformly\nimportant and in general the agent will not be able to visit\neach one. Intuition says that more spatial and temporal infor-"}, {"content": "mation will aid the model in learning the greedy heuristic we\ndescribed in Section III-B. Indeed, as we will show in Section\nV-B through an ablation study, these additions were critical\nin performing comparably to previous methods. As for edge\nattributes evw, we set the single attribute to the Euclidean\ndistance between nodes v and w. Note that the edge attributes\ndo not include any information about the length variability\nd(ev,w). This choice was made after preliminary tests showed\nno conclusive performance improvement and a reduction in\nmodel parameters, allowing for a smaller training dataset\nand, ultimately, reducing training time.\nIn selecting our model, we started from the literature\ndiscussed in Section II as well as a test case comparison.\nWe trained both a MPNN [6] and a graph attention network\n(GAT) [23]. Our starting hypothesis was that an attention\nnetwork might be able to identify more valuable nodes\nthrough the attention mechanism. However, it turned out that\nthe performance was a bit worse when training the GAT.\nTo be specific, the average reward computed was roughly\n10% worse in our largest model, with only 50ms gains in\ncomputational time through the entire MCTS pipeline, infer-\nence included. This trade off was, therefore, not substantial\nenough for selecting the GAT. Consequently, we selected\nthe neural message passing model from [6], [13] for its\nperformance and stability. Notably, the pre-built model from\n[13] includes a skip level connection from the initial node\nattributes to post-message passing layers. We removed this\nskip layer to be consistent with [6] and also because we\nobserved it did not yield better results."}, {"title": "D. Training", "content": "With the goal of replacing the rollout phase in MCTS with\na trained model that can estimate both the utility, Q, and\nfailure probabilities, F, without simulation, a necessary pre-\nliminary step is generating data to train the MPNN. To this"}, {"title": "V. RESULTS", "content": "We experimentally assessed how the architecture we de-\nsigned predicts the Q and F values to efficiently solve\ninstances of SOPCC. We compare our GNN-MCTS based\nmethod with an exact method based on mixed integer linear\nprogram [22] (referred to as MILP in the following), as\nwell as the method we formerly presented in [2], referred to\nas MCTS-SOPCC. For completeness we also compare the\nGNN-MCTS against the method described in section III-B\nthat was used for training (referred to as MCTS recall\nthat while it is based on MCTS-SOPCC it has some slight\nimplementation differences as formerly discussed). Our im-\nplementation is written in Python 3.11.5 and we used [13] to\nimplement the network based on pre-existing GNN layers.\nThe test machine has an Intel Core i7-10700F at 2.90GHz,\n64GB of RAM, and an Nvidia 1660 SUPER GPU. For the\nMILP based solution, we used the commercial solver Gurobi.\nThe goal is assessing a value network's ability to learn\nheuristics and generalize while solving SOPCC instances\nof varying complexity. Compared to existing solvers, the\nstrength of the GNN-MCTS solver is expected to be speed."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we have presented a new solution to the\nSOPCC combining MCTS with a message passing graph\nneural network. This is accomplished by training a neural\nnetwork that can replace the time consuming rollout phase\nin MCTS. Key to the idea of solving SOPCC using MCTS\nintroduced in [2], the trained model simultaneously predicts\nboth the anticipated collected reward, Q, as well as the pre-"}, {"content": "dicted failure probability, F a feature not found in previous\nworks using value networks with reinforcement learning.\nAnother advantage of the methodology we presented is that\nduring the expansion phase, a single forward pass of the\nnetwork makes predictions for all children at once, instead\nof considering one child at a time. This allows the algorithm\nto efficiently explore a larger fraction of the search space.\nOur method has been trained using SOPCC solutions\ncomputed by a slightly modified version of the algorithm\npresented in [2]. Extensive simulations compared this new\nsolution with the training method, our former MCTS al-\ngorithm presented in [2], and an exact solution based on\nMILP. Our findings show that the proposed method is orders\nof magnitude faster while meeting the failure constraint and\nretaining a significant fraction of the collected reward, though\nin some cases this requires tuning some hyperparameters.\nOur ablation study also corroborates that the vertices at-\ntributes we selected for the embedding are critical for the\nsuccess of the method."}, {"title": "A. Experiments", "content": "end, we started by solving randomly generated instances of\nthe SOPCC problem using the MCTS algorithm described in\nSection III-B. These training instances are complete graphs\nwith rewards and (x, y) coordinates uniformly and randomly\ndistributed between zero and one (see Figure 2) with edge\nlengths equal to the Euclidean distance. For training, we\ngenerated graphs with 20, 30 and 40 vertices."}, {"title": "B. Ablation Study", "content": "Previous literature showed that including spatial informa-\ntion into the vertices attributes is essential when assessing\nthe ability of GNN based methods to solve problems like\nTSP [5]. However, since solutions of SOPCC instances\nare also influenced by rewards and remaining budget as\nwell as start and goal locations as pointed out in Section\nIV-C in our implementation, we correspondingly extended\nthe set of attributes. Therefore, this subsection, through an\nablation study, we assess the impact of the temporal and\nspatial attribute additions, novel to this paper, of the one-hot\nencoding of the currently visited vertex and the start/goal\nvertices."}, {"title": "C. Generalization", "content": "As with every learning based approach, the quality of the\nsolution is influenced by the nature of the dataset used for\ntraining. In our work, with the explicit goal of assessing\nthe ability to learn and generalize, during training we kept\nPf = 0.1 and B = 2, and used the same underlying graph\ntopology, i.e., complete graphs with vertices uniformly scat-\ntered in the unit square. With these premises, it is essential\nto assess how the proposed architecture can generalize to\ninstances characterized by new parameters. We experimented\ngeneralization from two different angles: generalization of\ngraph sizes and unseen budgets. The ability to generalize\nwith respect to these two parameters is not only important\nfor robustness, but also to expedite training, especially when\nit comes to graph size, as larger instances require more time\nto generate solutions with the MCTS algorithm used for\ntraining. As pointed out in Section IV-D, during training we\nonly considered graphs with 20, 30, and 40 vertices.\nFor generalizing with respect to size, we saw in Table I,\nwithin a certain size range, equally competitive performance.\nFor example, we pushed our model to the high end of what\nwe have seen in SOPCC literature (e.g., up to 70 vertices)\nand found that the generalization is quite effective in terms of\nreward performance. However, due to the sigmoid problem\npreviously described and well known in literature, for larger\nproblem instances the model requires a tuned Pf value. This\nmeans that, as displayed in Table I, if one feeds the model\nwith a desired Pf value, for unseen problem instances we get\nsolutions with failure probabilities significantly exceeding\nthe limit. This problem can however be remedied by feeding\nthe model with a Pf value smaller than the desired one. Note\nthat this hyperparametrization process is necessary also when\nusing the exact method [22] and is not a limitation unique to\nthe method we present. Table III shows how by adjusting the\nPf value in unseen problem instances GNN-MCTS manages\nto successfully solve the problem matching the originally\ndesired failure rate.\nIn the case of budget generalization, we can see in Table\nI that our method was able to maintain similar time and"}]}