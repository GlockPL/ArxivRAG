{"title": "Fine Tuning Swimming Locomotion Learned from Mosquito Larvae", "authors": ["Pranav Rajbhandari", "Karthick Dhileep", "Sridhar Ravi", "Donald Sofge"], "abstract": "In prior research, we analyzed the backwards swimming motion of mosquito larvae, parameterized it, and replicated it in a Computational Fluid Dynamics (CFD) model. Since the parameterized swimming motion is copied from observed larvae, it is not necessarily the most efficient locomotion for the model of the swimmer. In this project, we further optimize this copied solution for the swimmer model. We utilize Reinforcement Learning to guide local parameter updates. Since the majority of the computation cost arises from the CFD model, we additionally train a deep learning model to replicate the forces acting on the swimmer model. We find that this method is effective at performing local search to improve the parameterized swimming locomotion.", "sections": [{"title": "I. INTRODUCTION/RELATED WORK", "content": "In previous research, we parameterize the swimming motion of mosquito larvae and successfully replicate it inside a computational fluid dynamics simulator [1]. We model the swimmer as a 2D boundary and use the immersed boundary lattice Boltzmann method (IB-LBM) [4] to calculate forces and resulting trajectory of a swimming locomotion.\nFor the parametrization, we discretize the swimmer into line segments and estimate the angle 0 between adjacent segments. This angle varies with time $t \\in R$ as well as location on the swimmer $s \\in R$. We found that $\\theta(s,t)$ was well approximated by Equation 1, using an amplitude function $\\theta_0(s)$, a frequency $\\omega$, and phase shift function $\\phi(s)$.\n$\\theta(s,t) = \\theta_0(s) \\cdot sin(\\omega t + \\phi(s))$ (1)\nWe approximate $\\theta_0$ and $\\phi$ as polynomials with respect to $s$ of degrees 5 and 4 respectively. In addition to $\\omega$, this results in a 12 dimensional parameter space.\nExplicitly, we may rewrite Equation 1 using a parameter vector $p \\in R^{12}$:\n$\\theta_p(s,t) = \\sum_{i=0}^{5} p_{i+1} \\cdot s^i \\cdot sin(\\omega t + \\sum_{i=0}^{4} p_{i+7} \\cdot s^i)$ (2)\nWe obtain our initial parameters in [1] by estimating the motion of live mosquito larvae."}, {"title": "B. Local Search/Hill Climbing", "content": "The hill climbing algorithm is a well-known local search method that repeatedly updates a solution to an improvement found by testing a local neighborhood [7]. In continuous search spaces, this can be approximated by fixing a step size $\\delta$ and searching around a solution by taking a $\\delta$ step in every dimension. This approximates a gradient of the objective with respect to the parameter space, and this approximation can be done in O(d) for d the number of dimensions.\nWe may apply this to optimizing the parameters of an initial swimming locomotion. We set our objective to displacement in some set time, and evaluate a solution through a simulation. With this method, a single update (assuming we take a full gradient estimation) will require O(d) simulations.\nThis is a reasonable approach if we utilize our simulation only for evaluating a potential solution. However, by making small adjustments to the swimming policy mid-episode, we can better estimate which updates increase our objective. To make these adjustments, we utilize a Reinforcement Learning (RL) algorithm."}, {"title": "C. Baseline Guided Policy Search", "content": "Hu and Dear explore a similar problem of training an articulated robotic swimmer through RL [3]. They introduce Baseline Guided Policy Search (BGPS), an augmented RL algorithm which starts at an approximated policy and allows an agent to add small adjustments. In their research, they utilize this method to optimize swimming motion in robotic swimmers composed of three segments.\nWe utilize this technique to make adjustments mid-simulation to a swimming locomotion. We will then learn parameters that best approximate this adjusted policy, updating the baseline."}, {"title": "II. METHODS", "content": "In previous research, we create a simulated mosquito larvae inside a Computational Fluid Dynamics (CFD) simulator. The setup is able to replicate the dynamics of real mosquito larvae given the correct swimming motion. We utilize this CFD model to fine tune the locomotion of the same simulated swimmer."}, {"title": "B. RL-Guided Parameter Update", "content": "We implement a RL environment utilizing a CFD simulation.\nWe use this environment to optimize a set of parameters as in Figure 2. We first repeatedly run the BGPS algorithm, searching for an augmentation that outperforms the baseline policy. Once this policy is found, we approximate parameters that match the augmented policy. Finally, if these updated parameters are truly an improvement, we update the baseline and continue with our loop.\nTo approximate parameters, we inspect the augmented policy and record the angles $\\theta^*(s,t)$ between adjacent line segments. We sample values of s that constitute each joint of the swimmer, and values of t within one period of the swimming motion. We then must choose parameters p such that $\\theta_p(s,t)$ from Equation 2 approximates $\\theta^*(s,t)$. Since $\\theta_p$ is differentiable with respect to p, we do this through gradient descent, minimizing the Mean Squared Error loss (Equation 3). We initialize our search with the baseline parameters, since $\\theta^*(s, t)$ results from small adjustments to this.\n$C(p) = E_{s,t} [(\\theta^*(s,t) \u2013 \\theta_p(s,t))^2]$ (3)"}, {"title": "C. CFD Clone", "content": "We utilize deep learning to create a model that predicts the forces acting on a simulated swimmer based on its outline. We experiment with both sequential models and a normal feed forward network.\nWe use CFD on a parameter sweep of parameterized swimming motions to create the training data. To define the model loss, we use the sum of mean squared error loss and cosine similarity loss to further ensure the forces are in the correct direction."}, {"title": "1) Network Input:", "content": "We allow the network to observe the COM-centered outline of the swimmer at each timestep. This is a set of 400 sampled points on the swimmer surface. We use this as our network input since it is the same input to the CFD model. For our feed-forward network, we additionally allow the network to observe the past 3 timesteps to get kinematic information about the swimmer."}, {"title": "2) Network Output:", "content": "The network output is the surface forces on each of the 400 sampled points. We use this as our network output since it is output of the CFD, and it is sufficient to calculate the movement of the swimmer."}, {"title": "3) CFD calculation:", "content": "We use the trained model to create a CFD clone by calculating surface forces at every timestep and applying kinematic equations, similar to the calculations in Zhu et al. [9]."}, {"title": "III. EXPERIMENTS", "content": "We experimented with the seq-to-seq Recursive Neural Network (RNN) model [6] and the Long Short-Term Memory (LSTM) model [2]. We also included a residual network for comparison with non-sequential methods.\nWe hypothesize that sequential models are better suited to handle the estimation of forces on our swimmer. Our reasoning is that a non-sequential approach would suffer from noise in the training data, as an estimate must be made from information in just a few time steps. In contrast, a sequential model can obtain information from the full history of the swimmer, allowing it to be more robust to the noise."}, {"title": "1) Network Architecture:", "content": "In addition to varying the model used, we also evaluate different network sizes in their ability to reduce the objective function. We take our best performing model and vary the depth of the architecture from one layer to eight layers. In our final CFD clone, we use the simplest network that performs comparably well."}, {"title": "B. Baseline Guided Policy Search", "content": "We implement BGPS to make adjustments to a baseline swimming policy. As in Figure 2, we alternate between using BGPS to improve the baseline and fitting parameters to the augmented policy. We use the stable_baselines3 [5] implementation of Proximal Policy Optimization (PPO), a standard on-policy RL algorithm [8]."}, {"title": "1) Observation Space:", "content": "Guided by Hu and Dear's work [3], we allow the agent to observe the current time (encoded periodically by applying sine and cosine at various frequencies), the angles of a few points on its midline, its heading angle, and its position and velocity."}, {"title": "2) Action Space:", "content": "The action space available to the agent is a list of angles corresponding to joints on its midline. The angles output from the RL agent are added onto a baseline policy."}, {"title": "3) Rewards:", "content": "We observe the normalized total displacement of the baseline policy's movement. At each timestep, we give the RL agent rewards equivalent to the displacement in the direction of the baseline displacement vector. We do this to ensure that the sum of rewards in an episode is the swimmer's overall displacement in the same direction as the baseline policy."}, {"title": "IV. RESULTS", "content": null}, {"title": "A. CFD Clone", "content": null}, {"title": "B. BGPS", "content": "We use the resulting CFD clone to optimize our swimming locomotion from the initial choice of parameters. In each episode of training, we record the absolute value of the total displacement.\nFrom Figure 5, we find that the BGPS algorithm is successful in gradually optimizing the movement of the simulated swimmer.\nHowever, the scale of the improvement is small in comparison to the size of the displacement. This could be a result of the scale we allow BGPS to augment the policy."}, {"title": "V. CONCLUSION", "content": "In this study, we fine tune a learned parameterized swimming locomotion for a specific platform. We use a local search to gradually update the parameters towards more optimal neighbors. To increase efficiency, we use RL to learn kinematic information about the swimming locomotion, guiding the local search.\nWe additionally approximate the learning environment with a CFD clone, learned through a deep neural network. We utilize this CFD clone to efficiently conduct model-based RL to improve the baseline policy. Overall, we take advantage of kinematic nature of our optimization problem to improve the speed of local search.\nWe find that these methods are successful in improving the parameterized swimming locomotion through local search."}]}