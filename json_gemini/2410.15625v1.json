{"title": "IMPROVING PARALLEL PROGRAM PERFORMANCE THROUGH DSL-DRIVEN CODE GENERATION WITH LLM OPTIMIZERS", "authors": ["Anjiang Wei", "Allen Nie", "Thiago S. F. X. Teixeira", "Rohan Yadav", "Wonchan Lee", "Ke Wang", "Alex Aiken"], "abstract": "Mapping computations to processors and assigning data to memory are critical for maximizing performance in parallel programming. These mapping decisions are managed through the development of specialized low-level system code, called mappers, crafted by performance engineers. Each mapper is tailored to a specific application and optimized for the underlying machine architecture, a process that requires days of refinement and tuning from an expert. Despite advances in system research, automating mapper generation remains a challenge due to the complex- ity of making millions of decisions to find the optimal solution and generate the solution as code. We introduce an approach that leverages recent advances in LLM-based optimizers for mapper design. In under ten minutes, our method au- tomatically discovers mappers that surpass human expert designs in scientific ap- plications by up to 1.34\u00d7 speedup. For parallel matrix multiplication algorithms, our mapper achieves up to 1.31\u00d7 of the expert-designed solution. To achieve this, we simplify the complexity of low-level code generation by introducing a domain- specific language (DSL) that abstracts the low-level system programming details and defines a structured search space for LLMs to explore. To maximize the ap- plication performance, we use an LLM optimizer to improve an agentic system that generates the mapper code. As a result, this approach significantly reduces the workload for performance engineers while achieving substantial performance gains across diverse applications. Finally, our results demonstrate the effective- ness of LLM-based optimization in system design and suggest its potential for addressing other complex system challenges.", "sections": [{"title": "1 INTRODUCTION", "content": "Task-based programming (Slaughter et al., 2015; Bauer et al., 2012; Augonnet et al., 2009; Chamberlain et al., 2007; Kaiser et al., 2014; Heller et al., 2017; Chandra et al., 2001; Duran et al., 2011; Moritz et al., 2018; Barham et al., 2022) has recently emerged as a prominent paradigm in paral- lel programming. The core idea is to decompose computations into self-contained functions called tasks that do not communicate with other tasks except through their arguments (typically tensors or multi-dimensional arrays). A performance-critical aspect of executing task-based applications is mapping, which involves assigning tasks to processors, mapping data to physical memories, and managing other low-level hardware resources. A concrete mapping policy, implemented as code, is referred to as a mapper. Mappers focus solely on improving performance without altering the correctness of the application's output. The performance impact of a good versus a poor mapper can be substantial, often resulting in differences of an order of magnitude (Abdel-Gawad et al., 2014; Galvez et al., 2017).\nCurrently, writing mappers is a manual, labor-intensive, and time-consuming process that requires deep knowledge of the application, the machine, and the low-level system's C++ mapping APIs."}, {"title": "2 RELATED WORK", "content": "Mapping in Parallel Programming Many parallel programming systems allow users to make their own mapping decisions, such as Legion (Bauer et al., 2012), StarPU (Augonnet et al., 2009; 2010), Chapel (Chamberlain et al., 2007), HPX (Kaiser et al., 2014; Heller et al., 2017), Sequoia (Fatahalian et al., 2006), Ray (Moritz et al., 2018), and Pathways (Barham et al., 2022). Several tech- niques have been proposed to automate mapping, including machine learning models (O'Boyle et al., 2013; Wang & O'Boyle, 2009), static analysis (Poesia et al., 2017; Ren et al., 2008), and auto-tuning (SFX Teixeira et al., 2023). Unlike previous work, we use an agent-based approach with LLMs, exploring a larger search space for mappers than traditional methods.\nAutomated Code Generation With the rise of LLMs, various models (Chen et al., 2021; Nijkamp et al., 2022; Li et al., 2023; Ouyang et al., 2022; Wei et al., 2023) have been trained on code, en- abling them to assist with programming tasks and automate software development (Shypula et al., 2023; Yang et al., 2024b; Jain et al., 2024). While LLMs have shown success in generating iso- lated function-level Python code (Chen et al., 2021; Austin et al., 2021), they face challenges when generating system-level C++ code in large repositories due to complex APIs and broader contex- tual dependencies (Du et al., 2023; Wang et al., 2024). Thus, adopting LLMs to generate code in real-world software remains difficult (Jimenez et al., 2023). Our work addresses this by develop- ing a domain-specific language (DSL) that provides a high-level abstraction to help LLMs generate mapper code more effectively.\nAutomated System Design Optimization The use of machine learning for optimizing system design has garnered significant attention in recent years. Learned cost models have proven effec- tive in optimizing deep learning operators on GPUs (Zheng et al., 2020a;b; 2022b). Methods in reinforcement learning and multi-armed bandit have been applied to tasks such as chip floorplan- ning (Mirhoseini et al., 2021) and program autotuning (Ansel et al., 2014), automatic vectorization of programs (Haj-Ali et al., 2020a), and optimizing compiler phase ordering (Haj-Ali et al., 2020b). Previous work primarily focuses on hyperparameter configurations, while our work aims to improve system performance by directly generating mapper code.\nOptimization with LLMs Recently, LLMs have been increasingly applied to optimization prob- lems traditionally found in numerical domains (Boyd & Vandenberghe, 2004). AhmadiTeshnizi et al. (2023) addressed structured optimization challenges, such as mixed-integer linear program- ming (LP), while Yang et al. (2024a) applied LLMs to unstructured problems, treating them as black-box optimizations. (Nie et al., 2024) highlighted the role of feedback in efficiently finding global minima for black-box functions. Cheng et al. (2024) formally defined optimization problems solvable by LLMs and proposed an optimizer based on execution graphs. More recently, Yuksekgonul et al. (2024) introduced an LLM optimizer inspired by gradient descent. These advancements demonstrate the potential of LLMs to optimize beyond task completion, improving performance metrics across various domains."}, {"title": "3 MAPPER GENERATION TASK", "content": "Task Definition The problem we address is the automated generation of high-performance map- pers for the Legion parallel programming framework (Bauer et al., 2012). Mappers determine how tasks are assigned to processors and how data is placed in memory to optimize performance. A well- designed mapper can achieve up to 10\u00d7 speedup compared to random mapping strategies. However, mappers are highly context-specific and must be carefully tailored to an application's input and the machine's architecture. Finding an optimal mapper is akin to solving a combinatorial optimization problem-a process that is time-consuming, labor-intensive, and traditionally performed by experi- enced performance engineers. The search space for discovering the best mapping strategy is vast, growing to $2^{14}$ even for the simplest scientific applications (as shown in prior work with a smaller search space than ours (SFX Teixeira et al., 2023)), making it challenging to efficiently explore all possible solutions. Moreover, even with a clear mapping strategy, writing the corresponding mapper requires experts to produce hundreds of lines of low-level C++ code, a task that can take several hours. As a result, the entire performance tuning process can take several days to complete.\nMapping Decisions Now we elaborate on the decisions that a mapper has to make. The first critical decision is the processor selection for each task, determining whether a task is better suited for GPUs, CPUs, or OpenMP runtime. This choice depends on factors such as task size, GPU memory, and kernel launch latency. For example, tiny tasks that require very little computation may prefer to run on CPUs due to the GPU kernel launch overhead, whereas tasks with large memory footprints might prefer to be assigned to OpenMP or CPU when GPU memory is insufficient.\nNext, the memory placement of data across different memory spaces is crucial to performance. A mapper must decide where to place data - in the GPU's FrameBuffer for faster access, in ZeroCopy memory for shared access between CPU and GPU, or in CPU system memory for large data. Each choice introduces a trade-off between memory access speed, memory usage, and transfer overhead.\nThe memory layout option determines the optimal memory arrangement for data structures. De- pending on the memory access patterns and the underlying hardware, selecting between a Struct of Arrays (SOA) or an Array of Structures (AOS) layout, along with constraints on array order- ing (Fortran-order or C-order) and memory alignment, can significantly impact performance due to cache efficiency and data locality. Such choice is task-dependent and processor-dependent.\nFinally, mappers need to decide how to perform index mapping. As shown in Figure 2, index map- ping controls how the data (or more accurately, parallel for loops of task launches) is partitioned and mapped to the processors. The data (or tensors) can be multi-dimensional, and the distributed"}, {"title": "4 A NEW APPROACH: GENERATION AND OPTIMIZATION IN A DSL", "content": "4.1 DESIGN A DOMAIN-SPECIFIC LANGUAGE\nWe designed a domain-specific language for mapper generation. We distill the key performance- critical aspects of mapping into language constructs that are easily expressed in the DSL. By pro- viding a higher-level abstraction than C++ APIs, the DSL simplifies interfacing with LLMs. Also, such a modular design enables LLMs to efficiently address the complex optimization challenges of generating high-performance mappers. To implement our DSL, we develop a compiler that can translate the mapper written in DSL into low-level C++ mapping APIs.\nFigure 3a presents an example DSL mapper that illustrates the key features of our domain-specific mapping language. By contrast, Figure 3b shows the (already simplified) code snippet extracted from a C++ mapper. Notably, the C++ snippet covers only part of the functionality that the IndexMap statement (which we discuss in detail below) in the DSL provides.\nWhile a typical C++ mapper requires approximately 400 lines of code, the corresponding DSL mapper can achieve the same functionality in just about 30 lines on average - reflecting a 14\u00d7 reduction, as detailed in Table 1 (based on the 9 applications used in Section 5.2 and Section 5.3). This substantial reduction in code complexity makes the DSL a more accessible target for LLM- based code generation, as it abstracts away many low-level implementation details.\nNext, we explain the DSL's design, highlighting the immense search space it opens for optimization, which makes generating high-performance mappers a challenging task. The grammar of the DSL is shown in Appendix A.1.\nThe Task statement (Line 2) performs processor selection for tasks. Although it may seem straight- forward to always prefer GPUs, this decision is complicated by other factors such as limited GPU memory and kernel execution time as explained in Section 3. This is a per-task decision.\nThe Region statement (Line 5) performs memory placement for data. Possible choices include GPU's FrameBuffer or ZeroCopy memory, CPU's System memory or RDMA memory. Such de- cisions need to be made for each argument of each task, which opens up a huge trade-off space between memory usage, task execution time, and data transfer costs as explained in Section 3."}, {"title": "4.2 LEARNING TO GENERATE VIA INTERACTIVE FEEDBACK", "content": "We conceptualize the mapper generation problem as an online optimization problem. Given a triplet $(\\Theta, \\omega, T)$, where $\\Theta$ is the overall set of possible code that can be produced to generate a mapper; $\\omega$ defines the optimization objective, which is to create a mapper that maximizes the throughput; and $T: \\Theta \\rightarrow \\mathcal{F}$, $\\mathcal{G}$ is the task that returns $f, g = T(\\theta)$, where $f$ is the feedback from executing the generated mapper (the performance metric measured after using the generated mapper to serve the application code), and $g$ is the process graph of how the mapper is generated. In our particular setup, the performance of the mapper (throughput) is deterministic because system researchers have carefully controlled all possible randomness in the environment. If the parameter space is numerical,\nthis online optimization problem can be solved through bandit algorithms (Lattimore & Szepesv\u00e1ri, 2020), reinforcement learning (Sutton & Barto, 2018), or Bayesian optimization (Snoek et al., 2012), but these methods are less effective when the parameter is text (mapper code).\nIn this online optimization problem, we leverage the DSL to bring structure to the parameter space and make the online optimization more efficient. $\\Theta$ is the program code, $\\omega$, and $f$ are expressed as text. We leverage an LLM as an optimizer to solve this optimization problem. LLM has been shown to have some ability to \"optimize\u201d by iteratively proposing better solutions when an explicit objective is described in the text. This phenomenon has been observed and leveraged by Yang et al. (2024a); Cheng et al. (2024); Yuksekgonul et al. (2024); Patel et al. (2024) to solve various"}, {"title": "5 EVALUATION", "content": "Experiments are conducted on a GPU cluster where each node has two Intel 10-core E5-2640 v4 CPUs, 256G main memory, and four NVIDIA Tesla P100 GPUs. Regarding the LLM, we use gpt-40-2024-08-06.\n5.1 EFFECTIVENESS OF THE DSL\nTo evaluate the necessity and effectiveness of our DSL in mapper code generation, we compared the generation of mappers in C++ with DSL. We devised 10 mapping strategies, each described in natural language, to serve as test cases for code generation\u00b9. A complete list of these strategies is provided in Appendix A.9. It is important to note that the objective is not to optimize the perfor- mance of any specific application, but rather to assess whether the LLM can accurately generate a C++/DSL mapper based on a given strategy.\nWe provided the same types of materials in the prompt to ensure a fair comparison. We provided documentation\u00b2, example mapper programs, and starting code for both DSL and C++ interfaces. We measured the success rate of generating mappers that can pass compilation and pass test cases for each strategy. For C++ code generation, we enhanced the process by incorporating compiler feedback for additional attempts. The LLM received compiler error messages and iteratively refined the code up to 10 times. We use DSPy (Khattab et al., 2023) to build our generation pipeline."}, {"title": "5.2 ACCELERATING SCIENTIFIC APPLICATIONS", "content": "We use the following three scientific applications as our benchmarks. The circuit simulation bench- mark (Bauer et al., 2012) models the behavior of an electrical circuit by simulating currents and voltages across interconnected nodes and wires. The stencil computation benchmark (Van der Wi- jngaart & Mattson, 2014) simulates a 2D grid where each point's value is updated based on its neighbors using a stencil pattern. The Pennant benchmark (Ferenbaugh, 2015) models unstructured mesh, Lagrangian staggered-grid hydrodynamics, used for simulating compressible flow.\nIn this experiment, we compared the performance of different mappers. All mappers are imple- mented using our DSL. Our goal is to determine whether LLMs can effectively explore the search space of mappers by generating high-performance DSL mappers.\nExpert-written mappers were manually developed and optimized by domain experts as part of the application development process. We re-implemented these expert-written C++ mappers using our DSL to establish a ground truth for comparison with our approach. Validation confirmed that the DSL-based mappers achieve performance equivalent to the original C++ mappers, providing a fair basis for evaluating the performance of our generated mappers.\nRandomly generated mappers are produced by our MapperAgent with 10 different random seeds and serve as a baseline for this experiment. We ran 10 randomly generated mappers and reported the average performance.\nLLM-generated mappers are implemented using Trace (Cheng et al., 2024), which employs LLMs as optimizers. We tested both the Trace and OPRO (Yang et al.) search algorithms, running 10"}, {"title": "5.3 ACCELERATING MATRIX MULTIPLICATIONS", "content": "We follow the same experimental setup as described in Section 5.2, with the key difference being the focus on matrix multiplication algorithms in this subsection. Different from the applications in"}, {"title": "5.4 ABLATION STUDY OF FEEDBACK", "content": "Within the Trace framework, feedback is provided to the LLM optimizer to inform subsequent im- provement on mappers. The quality of the feedback directly affects the success of the optimization process. We evaluate three types of feedback to assess how different feedback influences the perfor- mance of LLM-based optimizers during mapper search (described in Table 2): (1) system feedback (compile error, execution error, or performance metrics), (2) error explanations, and (3) suggestions for mapper adjustments.\nThe first feedback message includes only system feedback (labeled System in Figure 8). Next, we evaluate an enhanced feedback message that includes both system feedback and error explana- tions (labeled System+Explain). Finally, we provide the full feedback message, including all three"}, {"title": "6 CONCLUSION", "content": "In this paper, we addressed the challenges of automating mapper generation in task-based program- ming through the use of LLMs and a Domain-Specific Language (DSL). By designing a high-level DSL, we effectively simplified the complex task of generating low-level C++ mappers, enabling LLMs to handle mapper generation without deep knowledge of system intricacies. Additionally, we formulated the mapper generation task as a discrete optimization problem, leveraging LLM optimiz- ers to explore a structured and constrained search space defined by the DSL. Our experimental re- sults demonstrate the efficacy of this approach, with LLM-generated mappers achieving up to 1.34\u00d7 speedup over expert-written mappers across nine benchmarks. For matrix multiplication algorithms, we observed a performance boost of up to 1.31\u00d7. These findings show that our LLM-enhanced DSL significantly reduces development time from days to minutes, benefiting both human developers and the performance of parallel applications."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We provide comprehensive details regarding the setup of our benchmark, ensuring full reproducibil- ity based on the provided information. We are planning to open-source the framework, as well as the code to reproduce all experiment results. We provide detailed documentation of the evaluation process, along with all the prompts that we use. We use gpt-40-2024-08-06 with a max token of 16383 and default temperature. All software used in our evaluation (e.g., Legion) is open source. We acknowledge the potential difference in performance measurement across different platforms."}, {"title": "A.1 DSL GRAMMAR", "content": "We show the DSL grammar in Figure A1."}, {"title": "A.2 PROCESSOR SPACE TRANSFORMATION FOR INDEX MAPPING", "content": "A processor space m can be initialized via m = Machine(GPU); as a 2D tuple where the first di- mension represents the node index, and the second dimension represents the index of GPU within the node. To better express mapping between index spaces, we define a set of transformation prim- itives. A transformation primitive is a function of the processor space m that returns a transformed processor space m', where m and m' are related through the mapping shown in the right-hand side of Figure A2.\nOur transformations are inspired by widely used operations for changing the dimensionality of arrays in libraries such as NumPy, but the application to mapping is quite different. We now explain each transformation in detail.\nThe split transformation takes two arguments: an integer i that indicates the dimension to be split, and the splitting factor d. Suppose m is a processor space of size (8, 8), then after executing m' =\nm.split(0, 2), m' will be a processor space of size (2, 4, 8). An important property of split and all DSL transformations is that they are invertible. Thus, mappers can work with the transformed space m' but DSL can translate such uses back into the original processor space m to identify which concrete processors to use. In this example, m' [jo, j1, j2] = m[jo + j1 \u00d7 2, j2].\nThe merge transformation takes two dimensions of the original processor to be fused as its in- put. Suppose m' is a processor space of size (2,4,8). After applying m\" =\nm'.merge(0, 1),\""}, {"title": "A.3 COMMON INDEX MAPPING FUNCTIONS", "content": "We show the common index mapping functions in Figure A3."}, {"title": "A.4 PARALLEL MATRIX MULTIPLICATION ALGORITHMS", "content": "2D Algorithms Cannon's (Cannon, 1969) introduced a systolic communication pattern with tiled data partitioning for distributed matrix multiplication. PUMMA (Choi et al., 1994) and SUMMA (Van De Geijn & Watts, 1997) extended this approach by supporting non-square ma-"}, {"title": "A.5 INDEX MAPPING FUNCTIONS USED BY MATRIX MULTIPLICATION ALGORITHMS", "content": "We show some index mapping functions used by matrix multiplication algorithms in Figure A4"}, {"title": "A.6 EXPLANATION OF INDEX MAPPING FOR SOLOMONIK'S ALGORITHM", "content": "Figure A5 shows a mapper for the Solomonik's algorithm on a 2-node machine with 4 GPUs per node. The result distribution for the 3D iteration space is that each node will get half of the whole iteration space by partitioning along the x-axis, and the 4 GPUs per node will perform a 2D block distribution over the y-z plane.\nThere is a dimension mismatch between the iteration space (3D) and the initial processor space (2D). To conduct the desired mapping required by the algorithm, we first apply the split transformation primitive four times (colored as red in the code). We apply the first (resp. last) two split transfor- mations to make the node dimension (resp. GPU dimension) align with the 3D iteration space. We visualize the result 6D processor space as two 3D spaces. The first 3D space (representing the node dimension) is of size (2, 1, 1) and the second 3D space (representing the GPU dimension) is of size (1, 2, 2)."}, {"title": "A.7 EXAMPLES OF FEEDBACK CONFIGURATIONS", "content": "We give examples for the system feedback and enhanced feedback in Table A1. The enhanced feedback includes explanations of errors and suggestions for mapper modifications."}, {"title": "A.8 TRACE AGENT CODE", "content": "Trace (Cheng et al., 2024) uses Python decorators like @bundle to annotate Python programs. It allows us to design an LLM code generation agent as if we were writing a Python program ourselves. We first set up an end-to-end runnable Python program that can generate a valid mapper program by randomly making decisions over the search space. We show the high-level structure of our Trace Mapper in Figure A6. At each optimization step, Trace will execute DSLMapperGenerator and collect the corresponding execution flow to build up a graph. Then it will make a call to an LLM to perform an update to any function that is decorated with @bundle(trainable=True). The"}, {"title": "A.9 MAPPING STRATEGIES", "content": "Strategy 1: Map the tasks of calculate_new_currents, distribute_charge, update_voltages onto GPUs in this way: linearize the 2D GPU processor space into 1D, then perform 1D block mapping from launch domain to the linearized 1D processor space.\nStrategy 2: Place ghost/shared regions (rp_shared and rp_ghost) onto GPU zero-copy memory\nStrategy 3: Use Array Of Struct (AOS) data layout for all data instead of the default SOA\nStrategy 4: Use Fortran ordering of data layout for all data instead of the default C order\nStrategy 5: Align all the regions to 64 bytes while using the Fortran ordering of data\nStrategy 6 Place the task calculate_new_currents onto CPU\nStrategy 7: Collect all the memory used by task calculate_new_currents\nStrategy 8: Ensure that at most 4 tasks of calculate_new_currents can be run at the same time\nStrategy 9: Map the second region argument of task distribute_charge onto GPU's Zero-Copy mem- ory\nStrategy 10: Map the tasks of calculate_new_currents,distribute_charge,update_voltages onto GPUs in a 1D cyclic manner: perform a cyclic distribution over both the node and processor di- mensions."}, {"title": "A.10 GENERATED MAPPER EXAMPLES", "content": "Here we provide examples of generated mappers for a subset of problems. The mappers, written in DSL, are produced by the mapper agent. While the LLM is responsible for creating and refining the mapper agent, the agent itself is implemented in Python, and it generates mappers as DSL programs.\nFor the Circuit Simulation benchmark, the optimized mapper (Figure A8) is more concise than the initial version (Figure A7), with an additional constraint for byte alignment in the data layout. In contrast, for Solomonik's algorithm, the initial mapper is relatively simple (Figure A9), whereas the final optimized mapper adopts a more complex and detailed index mapping strategy (Figure A10)."}]}