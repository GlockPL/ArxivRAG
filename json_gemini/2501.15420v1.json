{"title": "Visual Generation Without Guidance", "authors": ["Huayu Chen", "Kai Jiang", "Kaiwen Zheng", "Jianfei Chen", "Hang Su", "Jun Zhu"], "abstract": "Classifier-Free Guidance (CFG) has been a default technique in various visual generative models, yet it requires inference from both conditional and unconditional models during sampling. We propose to build visual models that are free from guided sampling. The resulting algorithm, Guidance-Free Training (GFT), matches the performance of CFG while reducing sampling to a single model, halving the computational cost. Unlike previous distillation-based approaches that rely on pretrained CFG networks, GFT enables training directly from scratch. GFT is simple to implement. It retains the same maximum likelihood objective as CFG and differs mainly in the parameterization of conditional models. Implementing GFT requires only minimal modifications to existing codebases, as most design choices and hyperparameters are directly inherited from CFG. Our extensive experiments across five distinct visual models demonstrate the effectiveness and versatility of GFT. Across domains of diffusion, autoregressive, and masked-prediction modeling, GFT consistently achieves comparable or even lower FID scores, with similar diversity-fidelity trade-offs compared with CFG baselines, all while being guidance-free. Code will be available at https://github.com/thu-ml/GFT.", "sections": [{"title": "1. Introduction", "content": "Low-temperature sampling is a critical technique for enhancing generation quality by focusing only on the model's high-likelihood areas. Visual models mainly achieved this through Classifier-Free Guidance (CFG) (Ho & Salimans, 2022). CFG jointly optimizes both conditional and unconditional models during training and combines them to define the sampling process. By altering the guidance scale, it can flexibly trade off image fidelity and diversity at inference time, while significantly improving sample quality. Due to its effectiveness, CFG has been a default technique for a wide spectrum of visual generative models, including diffusion (Ho et al., 2020), autoregressive (AR) (Chen et al., 2020; Tian et al., 2024), and masked-prediction models (Chang et al., 2022; Li et al., 2023).\nDespite the wide application, CFG requires inferencing both conditional and unconditional models to achieve the sampling distribution. This not only doubles the sampling cost but also complicates the post-training of visual models: When distilling pretrained diffusion models (Meng et al., 2023; Luo et al., 2023; Yin et al., 2024b) for fast inference or applying RLHF techniques (Black et al., 2023; Chen et al., 2024b), CFG requires the extra unconditional model to be additionally considered in the algorithm design.\nIt's also worth noting that for language models (LMs), a single model is sufficient to represent sampling distributions across various temperatures. However, similarly following LMs' approach to divide model output by a constant temperature value has largely been found ineffective in visual sampling (Dhariwal & Nichol, 2021), even for visual AR models with similar architecture to LMs (Sun et al., 2024). This leaves us wondering, how can we control the sampling temperature for visual models using only one model?\nExisting attempts mainly include distillation methods for diffusion models (Meng et al., 2023; Luo et al., 2023; Yin et al., 2024b) and alignment methods for AR models (Chen et al., 2024b). These methods rely heavily on pretrained CFG networks for loss definition and do not support training guidance-free models from scratch. Their two-stage optimization pipeline may also lead to performance loss compared with CFG, even after extensive tuning. Generalizability is also a concern. Current methods are tailored for either continuous diffusion models or discrete AR models, lacking the versatility to cover all domains.\nWe propose Guidance-Free Training (GFT) as a foundational algorithm for building guidance-free visual generative models. GFT matches CFG in performance while requiring only a single model for temperature-controlled sampling (Figure 1), effectively halving sampling costs. It offers stable and efficient training with the same convergence rate as CFG, almost no extra memory usage, and only 10-20% additional computation per training update. Notably, GFT is highly versatile, applicable in all visual domains within"}, {"title": "2. Background", "content": "2.1. Visual Generative Modeling\nContinuous diffusion models. Diffusion models (Ho et al., 2020) define a forward process that gradually injects noises into clean images from data distribution p(x):\n$x_t = \\sqrt{\\alpha_t} x + \\sqrt{\\sigma_t} \\epsilon,$\nwhere t \u2208 [0, 1], and \\epsilon is standard Gaussian noise. $\\alpha_t, \\sigma_t$ defines the denoising schedule. We have\n$p_t(x_t) = \\int N(x_t| \\sqrt{\\alpha_t}x, \\sigma_t^2I) p(x)dx,$\nwhere $p_0(x) = p(x)$ and $p_1 \\approx N(0,I)$.\nGiven dataset p(x, c), we can train conditional diffusion models by predicting the Gaussian noise added to $x_t$.\n$\\min_{\\theta} E_{p(x, c), t, \\epsilon} [||\\epsilon_\\theta (x_t | c) - \\epsilon||^2].$"}, {"title": "2.2. Classifier-Free Guidance", "content": "In diffusion modeling, vanilla temperature sampling (dividing model output by a constant value) is generally found ineffective in improving generation quality (Dhariwal & Nichol, 2021). Current methods typically employ CFG (Ho & Salimans, 2022), which redefines the sampling denoising function $\\epsilon_\\theta (x_t | c)$ using two denoising models:\n$\\epsilon_s(x_t | c) := \\epsilon_\\theta (x_t | c) + s[\\epsilon_\\theta (x_t | c) - \\epsilon_\\theta (x_t)],$\nwhere $\\epsilon_\\theta$ and $\\epsilon_\\theta^u$ respectively model the conditional data distribution p(x | c) and the unconditional data distribution p(x). In practice, $\\epsilon_\\theta^u$ can be jointly trained with $\\epsilon_\\theta$, by randomly masking the conditioning data c in Eq. 1 with some fixed probability.\nAccording to Eq. 2, CFG's sampling distribution $p_s(x | c)$ has shifted from standard conditional distribution p(x | c) to\n$p_s(x | c) \\propto p(x | c) [\\frac{p(x | c)}{p(x)}]^s.$"}, {"title": "3. Method", "content": "Despite its effectiveness, CFG requires inferencing an extra unconditional model to guide the sampling process, directly doubling the computation cost.\nMoreover, CFG complicates the post-training of visual generative models because the unconditional model needs to be additionally considered in algorithm design (Meng et al., 2023; Black et al., 2023).\nWe propose Guidance-Free Training (GFT) as an alternative method of CFG for improving sample quality in visual generation without guided sampling. GFT matches CFG in performance but only leverages a single model $\\epsilon_\\theta (x | c, s)$ to represent CFG's sampling distribution $p_s(x | c)$.\nWe derive GFT's training objective for diffusion models in Sec. 3.1, discuss its practical implementation in Sec. 3.2, and explain how it can be extended to discrete AR and masked models in Sec. 3.3."}, {"title": "3.1. Algorithm Derivation", "content": "The key challenge in directly learning sampling model $\\epsilon_\\theta^s$ is the absence of a dataset that aligns with distribution $p_s(x | c)$ in Eq. 5. It is impractical to optimize maximum-likelihood-training objectives like\n$\\min_{\\theta} E_{p_s(x, c), t, \\epsilon} [||\\epsilon_\\theta^s (x_t | c) - \\epsilon||^2].$\nbecause we cannot draw samples from $p_s$.\nIn contrast, training $\\epsilon_\\theta (x_t | c)$ and $\\epsilon_\\theta (x_t)$ separately as in CFG is feasible because their corresponding datasets, {(x, c) ~ p(x, c)} and {x ~ p(x)}, can be easily obtained.\nTo address this, we reformulate Eq. 4 by simple algebra:\n$\\epsilon_s(x_t | c) = \\frac{1}{1 + s} \\epsilon_\\theta (x_t | c) + \\frac{s}{1 + s} \\epsilon_\\theta^u (x_t).$\nAlthough learning $\\epsilon_\\theta^s$ directly is difficult, we note it can be combined with an unconditional model $\\epsilon_\\theta^u$ to represent the standard conditional $\\epsilon_\\theta$, which is learnable. Thus, we can leverage the same conditional loss in Eq. 1 to train $\\epsilon_\\theta^s$:\n$\\min_{\\theta} E_{p(x, c), t, \\epsilon} [||\\frac{1}{1 + s} \\epsilon_\\theta^s (x_t | c) + \\frac{s}{1 + s} \\epsilon_\\theta^u (x_t) - \\epsilon||^2],$\nwhere $\\epsilon$ is standard Gaussian noise, $x_t = \\sqrt{\\alpha_t} x + \\sqrt{\\sigma_t} \\epsilon$ are diffused images. $\\alpha_t$ and $\\sigma_t$ define the forward process.\nTo this end, we have a practical algorithm for directly learning guidance-free models $\\epsilon_\\theta^s$. However, unlike CFG which allows controlling sampling temperature by adjusting guidance scale s to trade off fidelity and diversity, our method"}, {"title": "3.2. Practical Implementation", "content": "Eq. 9 presents a practical loss function of GFT. The implementation is in Algorithm 1.\n$L_{diff} (x, c_{\\O}, t, \\epsilon, \\beta) = || \\beta \\epsilon_{\\theta} (x_t | c_{\\O}, \\beta) + (1 - \\beta) sg[\\epsilon_{\\theta} (x_t | \\O, 1)] - \\epsilon ||^2.$\nStopping the unconditional gradient. The main difference between Eq. 9 and Eq. 8 is that $\\epsilon_\\theta^u$ is computed in evaluation mode, with model gradients stopped by the sg[.] operation. To train the model unconditionally, we randomly mask conditions c with $\\O$ when computing $\\epsilon_\\theta^u$. We show this design does not affect the training convergence point:\nTheorem 1 (GFT Optimal Solution). Given unlimited model capacity and training data, the optimal $\\epsilon_\\theta^*$ for optimizing Eq."}, {"title": "3.3. GFT for AR and Masked Models", "content": "CFG is also a standard decoding method for discrete AR or masked visual models, critical to improving their sample quality (Chang et al., 2022; Li et al., 2023; Tian et al., 2024). Different from diffusion models which apply CFG in the score field, AR and masked models perform guided sampling on model logits:\n$l_\\theta (x_n | x_{<n}, c) = l_\\theta (x_n | x_{<n}, c) + s[l_\\theta (x_n | x_{<n}, c) - l_\\theta^u (x_n | x_{<n})].$\nHere $x_n$ represents the i-th token of a tokenized image x. l are unnormalized model logits.\nSimilar to Sec. 3.1, we can derive the GFT objective for AR and masked models as standard cross-entropy loss:\n$L_{AR} (x, c_{\\O}, \\beta) = - \\sum_i log p_{\\theta} (x_n | x_{<n}, c_{\\O}, \\beta)$\n$= - \\sum_i \\frac{e^{l_{\\theta} (x_n | x_{<n}, c_{\\O}, \\beta)}}{\\sum_{w \\in V} e^{l_{\\theta} (w | x_{<n}, c_{\\O}, \\beta)}}$,\nwhere w is a token in the vocabulary V, and\n$l_{\\theta} (w | x_{<n}, c_{\\O}, \\beta) := \\beta l_{\\theta} (w | x_{<n}, c_{\\O}, \\beta) + (1 - \\beta) sg[l_{\\theta}^u (w | x_{<n})].$\nIn Sec. 5, we apply GFT to a wide spectrum of visual generative models, including diffusion, AR, and masked models, demonstrating its versatility."}, {"title": "4. Connection with Other Guidance-Free Methods", "content": "Previous attempts to remove guided sampling from visual generation mainly include distillation methods for diffusion models and alignment methods for AR models. Alongside GFT, these methods all transform the sampling distribution ps into simpler, learnable forms, differing mainly in how they decompose the sampling distribution and set up modeling targets (Table 1).\nGuidance Distillation (Meng et al., 2023) is quite straightforward, it simply learns a single model to match the output of pretrained CFG targets using L2 loss:\n$L_{GD} = ||\\epsilon_\\theta (x_t | c, s) - [(1 + s) \\epsilon_\\theta (x_t | c) - s \\epsilon_\\theta^u (x_t)]||^2,$\nwhere $\\epsilon_\\theta$ and $\\epsilon_\\theta^u$ are pretrained models. $L_{GD}$ breaks down the sampling model into a linear combination of conditional and unconditional models, which can be separately learned.\nDespite being effective, Guidance distillation relies on pretrained CFG models as teacher models, and cannot be leveraged for from-scratch training. This results in an indirect, two-stage pipeline for learning guidance-free models. In comparison, our method unifies guidance-free training in one singular loss, allowing learning in an end-to-end style. Besides, GFT no longer requires learning an explicit conditional model $\\epsilon_\\theta^u$. This saves training computation and VRAM usage. A detailed comparison is in Table 1.\nCondition Contrastive Alignment (Chen et al., 2024b) constructs a preference pair for each image x in the dataset and applies similar preference alignment techniques for language models (Rafailov et al., 2023; Chen et al., 2024a) to fine-tune visual AR models:\n$L_{CCA} = - log \\sigma [r_{\\theta} (x, c_p)] - log \\sigma [- r_{\\theta} (x, c_n)],$"}, {"title": "5. Experiments", "content": "Our experiments aim to investigate:\n1. GFT's effectiveness and efficiency in fine-tuning CFG"}, {"title": "5.1. Experimental Setups", "content": "Tasks & Models. We evaluate GFT in both class-to-image (C2I) and text-to-image (T2I) tasks. For C2I, we experiment with diverse architectures: DiT (Peebles & Xie, 2023) (transformer-based latent diffusion model), MAR (Li et al., 2024) (masked-token prediction model with diffusion heads), and autoregressive models: VAR (Tian et al., 2024) and LlamaGen (Sun et al., 2024). For T2I, we use Stable Diffusion 1.5 (Rombach et al., 2022), a text-to-image model based on the U-Net architecture (Ronneberger et al., 2015), to provide a comprehensive evaluation of GFT's performance across various conditioning modalities. All these models rely on guided sampling as a critical component.\nTraining & Evaluation. We train C2I models on ImageNet-256x256 (Deng et al., 2009). For T2I models, we use a subset of the LAION-Aesthetic 5+ (Schuhmann et al., 2022), consisting of 12.8 million image-text pairs. Our codebases are directly modified from the official CFG implementation of each respective baseline, keeping most hyperparameters consistent with CFG training. We use official OPENAI evaluation scripts to evaluate our C2I models. For T2I models, we evaluate our model on zero-shot COCO 2014 (Lin et al., 2014). The training and evaluation details for each model can be found in Appendix D."}, {"title": "5.2. Make CFG Models Guidance-Free", "content": "Method Effectiveness. In Table 2 and 3, we apply GFT to fine-tune a wide spectrum of visual generative models. With less than 5% pretraining computation, the fine-tuned models achieve comparable FID scores to CFG while being 2x faster in sampling. Figure 4 visually demonstrates this quality improvement."}, {"title": "5.3. Building Guidance-Free Models from Scratch", "content": "Training Guidance-Free Models from scratch is more tempting than the two-stage pipeline adopted by Sec 5.2. However, this is also more challenging due to higher requirements for the algorithm's stability and convergence speed. We investigate this by comparing from-scratch GFT training with classic supervised training using CFG across various architectures, maintaining consistent training epochs. We mainly focus on smaller models due to computational constraints.\nPerformance. Table 4 shows that GFT models trained from scratch outperform CFG baselines across DiT-B/2, MAR-B, and LlamaGen-L models, while reducing evaluation costs by 50%. Notably, these from-scratch models outperform their fine-tuned counterparts, demonstrating the advantages of direct guidance-free training.\nTraining stability. An informative indicator of an algorithm's stability and scalability is its loss convergence speed. With consistent hyperparameters, we find GFT convergences at least as fast as CFG for both diffusion and autoregressive modeling (Figure 6). Direct loss comparison is valid as both methods optimize the same objective: the conditional modeling loss for the dataset distribution. The only difference is that the conditional model for CFG is a single end-to-end model, while for GFT it is constructed as a linear interpolation of two model outputs.\nBased on the above observations, we believe that GFT is at least as stable and reliable as CFG algorithms, providing a new training paradigm and a viable alternative for visual generative models."}, {"title": "5.4. Sampling Temperature for Visual Generation", "content": "A key advantage of CFG is its flexible sampling temperature for diversity-fidelity trade-offs. Our results demonstrate that GFT models share this capability.\nWe evaluate diversity-fidelity trade-offs across various models, with FID-IS trade-off for c2i models and FID-CLIP trade-off for t2i models. Results for DiT-XL/2 (fine-tuning),"}, {"title": "6. Conclusion", "content": "In this work, we proposed Guidance-Free Training (GFT) as an alternative to guided sampling in visual generative models, achieving comparable performance to Classifier-Free Guidance (CFG). GFT reduces sampling computational costs by 50%. The method is simple to implement, requiring minimal modifications to existing codebases. Unlike previous distillation-based methods, GFT enables direct training from scratch.\nOur extensive evaluation across multiple types of visual models demonstrates GFT's effectiveness. The approach maintains high sample quality while offering flexible control over the diversity-fidelity trade-off through temperature adjustment. GFT represents an advancement in making high-quality visual generation more efficient and accessible."}, {"title": "A. Related Work", "content": "Visual generation model with guidance. Visual generative modeling has witnessed significant advancements in recent years. Recent explicit-likelihood approaches can be broadly categorized into diffusion-based models (Ho et al., 2020; Song et al., 2020b; Dhariwal & Nichol, 2021; Kingma et al., 2021; Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Karras et al., 2022; Bao et al., 2023; Peebles & Xie, 2023; Esser et al., 2024; Xie et al., 2024), auto-regressive models (Chen et al., 2020; Esser et al., 2021; Ramesh et al., 2021; Yu et al., 2021; Tian et al., 2024; Team, 2024; Sun et al., 2024; Ma et al., 2024; Zhang et al., 2024; Tang et al., 2024), and masked-prediction models (Chang et al., 2022; Yu et al., 2023a; Chang et al., 2023; Li et al., 2024; Yu et al., 2024; Fan et al., 2024). The introduction of guidance techniques has substantially improved the capabilities of these models. These include classifier guidance (Dhariwal & Nichol, 2021), classifier-free guidance (Ho & Salimans, 2022), energy guidance (Chung et al., 2022; Zhao et al., 2022; Lu et al., 2023; Song et al., 2023), and various advanced guidance methods (Kynk\u00e4\u00e4nniemi et al., 2024; Karras et al., 2024; Chung et al., 2024; Koulischer et al.; Shenoy et al., 2024).\nGuidance distillation. To address the computational overhead introduced by classifier-free guidance (CFG), One widely used approach to remove CFG is guidance distillation (Meng et al., 2023), where a student model is trained to directly learn the output of a pre-trained teacher model that incorporates guidance. This idea of guidance distillation has been widely adopted in methods aimed at accelerating diffusion models (Luo et al., 2023; Yin et al., 2024b;a; Zhou et al., 2024). By integrating the teacher model's guided outputs into the training process, these approaches achieve efficient few-step generation without guidance.\nCondition Contrastive Alignment. Beyond guidance distillation, Condition Contrastive Alignment (CCA) (Chen et al., 2024b) could also significantly enhance the guidance-free performance of visual AR models through alignment (Rafailov et al., 2023; Chen et al., 2024a) in a self-contrastive manner."}, {"title": "B. Proof of Theorem 1", "content": "We first copy the training objective in Eq. 9 and Eq. 8.\n$L^{GFT}_{train} = E_{p(x,c),t,\\epsilon,\\beta} [||\\beta \\epsilon_{\\theta} (x_t | c, \\beta) + (1 - \\beta) \\epsilon_{\\theta}^u (x_t) - \\epsilon||^2].$\n$L^{GFT}_{practical} = E_{p(x,c\\O),t,\\epsilon,\\beta} [||\\beta \\epsilon_{\\theta} (x_t | c_{\\O}, \\beta) + (1 - \\beta) sg[\\epsilon_{\\theta} (x_t | \\O, 1)] - \\epsilon||^2.$\nTheorem 1 (GFT Optimal Solution). Given unlimited model capacity and training data, the optimal $\\epsilon_{\\theta}^*$ for optimizing Eq. 15 and Eq. 16 are the same. Both satisfy\n$\\epsilon_{\\theta}^* (x_t | c, \\beta) = -\\sigma_t (\\frac{1}{\\beta} - 1) \\nabla_{x_t} log p_t (x_t).$\nProof. The proof is quite straightforward.\nFirst consider the unconditional part of the model. Let $\\beta = 1$ in $L^{GFT}_{train}$, we have\n$L^{GFT}_{train} = E_{p(x,c),t,\\epsilon,\\beta=1} [|| \\epsilon_{\\theta} (x_t) - \\epsilon ||^2]$,\nwhich is standard unconditional diffusion loss. According to Eq. 2 we have\n$\\epsilon_{\\theta}^* (x_t) = -\\sigma_t \\nabla_{x_t} log p_t (x_t)$\nThen we prove stopping the unconditional gradient does not change this optimal solution. Taking derivatives of $L^{GFT}_{practical}$ we have:\n$\\nabla_{\\theta} L^{GFT}_{practical} (c_{\\O} = \\O) = E_{p(x),t,\\epsilon,\\beta} \\nabla_{\\theta} ||\\beta \\epsilon_{\\theta} (x_t | \\O, 1) + (1 - \\beta) sg[\\epsilon_{\\theta} (x_t | \\O, 1)] - \\epsilon||$\n$= E_{p(x),t,\\epsilon,\\beta} 2 \\beta [\\nabla_{\\theta} \\epsilon_{\\theta} (x_t | \\O, 1)] || \\beta \\epsilon_{\\theta} (x_t | \\O, 1) + (1 - \\beta) \\epsilon_{\\theta}^u (x_t | \\O, 1) - \\epsilon ||^2$\n$= E_{p(x),t,\\epsilon,\\beta} 2 \\beta [\\nabla_{\\theta} \\epsilon_{\\theta} (x_t | \\O, 1)] ||\\epsilon_{\\theta} (x_t | \\O, 1) - \\epsilon ||^2$\n$= [2 E_{\\beta}] \\nabla_{\\theta} E_{p(x,c),t,\\epsilon, \\beta=1} [|| \\epsilon_{\\theta} (x_t) - \\epsilon ||^2]$\n$= [2 E_{\\beta}] \\nabla_{\\theta} L^{GFT}_{train} (\\beta = 1)$\nSince [2$E_{\\beta}$] is a constant, this does not change the convergence point of $L^{GFT}_{train}$. The optimal unconditional solution for $L^{GFT}_{practical}$ remains the same.\nFor the conditional part of the model, since both $L^{GFT}_{train}$ and $L^{GFT}_{practical}$ are standard conditional diffusion loss, we have\n$\\beta \\epsilon_{\\theta}^* (x_t | c, \\beta) + (1 - \\beta) \\epsilon_{\\theta}^u (x_t) = - \\sigma_t \\nabla_{x_t} log p_t (x_t | c)$\nCombining Eq. 17, we have\n$\\epsilon_{\\theta}^* (x_t | c, \\beta) = - \\sigma_t (\\frac{1}{\\beta} - 1) \\nabla_{x_t} log p_t (x_t) - (\\frac{1}{\\beta} - 1) \\nabla_{x_t} log p_t (x_t).$"}, {"title": "D. Implementation Details", "content": "For all models, we keep training hyperparameters and other design choices consistent with their official codebases if not otherwise stated. We employ a mix of H100, A100 and A800 GPU cards for experimentation.\nDiT. We mainly apply GFT to fine-tune DiT-XL/2 (28 epochs, 2% of pretraining epochs) and train DiT-B/2 from scratch (80 epochs, following the original DiT paper's settings (Peebles & Xie, 2023)). Since the DiT-B/2 pretraining checkpoint is not publicly available, we reproduce its pretraining experiment. For all experiments, we use a batch size of 256 and a learning rate of 1e \u2013 4. For DiT-XL/2 fine-tuning experiments, we employ a cosine-decay learning rate scheduler.\nFor comparison, we also fine-tune DiT-XL/2 using guidance distillation, with a scale range from 1 to 5, while keeping all other hyperparameters aligned with GFT.\nThe original DiT uses the old-fashioned DDPM (Ho et al., 2020) which learns both the mean and variance, while GFT is only concerned about the mean. We therefore abandon the variance output channels and related losses during training and switch to the Dpm-solver++ (Lu et al., 2022) sampler with 50 steps at inference. For reference, our baseline, DiT-XL/2 with CFG, achieves an FID of 2.11 using DPM-solver++, compared with 2.27 reported in the original paper. All results are evaluated with EMA models. The EMA decay rate is set to 0.9999.\nVAR. We mainly apply GFT to fine-tune VAR-d30 models (15 epochs) or train VAR-d16 models from scratch (200 epochs). Batch size is 768. The initial learning rate is 1e - 5 in fine-tuning experiments and 1e - 4 in pretraining experiments. Following VAR (Tian et al., 2024), we employ a learning rate scheduler including a warmup and a linear decay process (minimal is 1% of the initial).\nVAR by default adopts a pyramid CFG technique on predicted logits. The guidance scale 0 decreases linearly during the decoding process. Specifically, let n be the current decoding step index, and N be the total steps. The n-step guidance scale Sn is\n$S_n = S_0.$\nWe find pyramid CFG is crucial to an ideal performance of VAR, and thus design a similar pyramid $\\beta$ schedule during training:\n$\\beta_n = .$\nwhere $\\beta_n$ represents the token-specific $\\beta$ value applied in the GFT AR loss (Eq. 12). a \u2265 0 is a hyperparameter to be tuned.\nWhen a = 0, we have $\\beta_n = \\beta_0$, standing for standard GFT. When a = 1.0, we have, corresponding to the default pyramid CFG technique applied by VAR. In practice, we set a = 1.5 in GFT training and find this slightly outperforms a = 1.0.\nLlamaGen. We mainly apply GFT to fine-tune LlamaGen-3B models (15 epochs) or train LlamaGen-L models from scratch (300 epochs). For fine-tuning, the batch size is 256, and the learning rate is 2e - 4. For pretraining, the batch size is 768, and the learning rate is 1e - 4. We adopt a cosine-decay learning rate scheduler in all experiments.\nMAR. We apply GFT to MAR-B, including both fine-tuning (10 epochs) and training from scratch (800 epochs). We find the batch size crucial for MAR and use 2048 following the original paper. For fine-tuning, we employ a learning rate scheduler including a 5-epoch linear warmup to 8e \u2013 4 and a cosine decay process to 1e \u2013 4. For training from scratch, we employ a 100-epoch linear Ir warmup to 8e \u2013 4, followed by a constant Ir schedule, which is the same configuration as the original MAR pretraining.\nThe original MAR follows the old-fashioned DDPM (Ho et al., 2020) which learns both the mean and variance, while GFT is only concerned about the mean. We therefore abandon the variance output channels and related losses during training and switch to the DDIM (Song et al., 2020a) sampler with 100 steps at inference. As the $\\beta$ condition may not precisely capture the effects of the guidance scale after training, we tune the inference $\\beta$ schedule to maximize the performance. Specifically, we adopt a power-cosine schedule\n$P_n = .$"}, {"title": "E. Prompts for Figure 12", "content": "We use the following prompts for Figure 12.\n\u2022 A vintage camera in a park, autumn leaves scattered around it.\n\u2022 Pristine snow globe showing a winter village scene, sitting on a frost-covered pine windowsill at dawn.\n\u2022 Vibrant yellow rain boots standing by a cottage door, fresh raindrops dripping from blooming hydrangeas.\n\u2022 Rain-soaked Parisian streets at twilight."}]}