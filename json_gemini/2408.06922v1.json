{"title": "Temporal Variability and Multi-Viewed Self-Supervised Representations to Tackle the ASVspoof5 Deepfake Challenge", "authors": ["Yuankun Xie", "Xiaopeng Wang", "Zhiyong Wang", "Ruibo Fu", "Zhengqi Wen", "Haonan Cheng", "Long Ye"], "abstract": "ASVspoof5, the fifth edition of the ASVspoof series, is one of the largest global audio security challenges. It aims to advance the development of countermeasure (CM) to discriminate bonafide and spoofed speech utterances. In this paper, we focus on addressing the problem of open-domain audio deep-fake detection, which corresponds directly to the ASVspoof5 Track1 open condition. At first, we comprehensively investigate various CM on ASVspoof5, including data expansion, data augmentation, and self-supervised learning (SSL) features. Due to the high-frequency gaps characteristic of the ASV spoof5 dataset, we introduce Frequency Mask, a data augmentation method that masks specific frequency bands to improve CM robustness. Combining various scale of temporal information with multiple SSL features, our experiments achieved a minDCF of 0.0158 and an EER of 0.55% on the ASVspoof 5 Track 1 evaluation progress set.", "sections": [{"title": "1. Introduction", "content": "With the rapid advancement of text-to-speech (TTS) and voice conversion (VC) technologies, deepfake speech has proliferated significantly, making it increasingly difficult for humans to discern. The latest generation models [1], benefiting from audio language models, have even achieved human parity in zero-shot TTS synthesis for the first time. This poses significant threats, including fraud, misleading public opinion, and privacy violations. Therefore, the urgent development of effective countermeasure (CM) technologies is crucial.\n The automatic speaker verification spoofing and counter-measures (ASVspoof) challenge has been at the forefront of developing speech spoofing detection tasks. The common goal is to foster progress in the development of CMs, which are capable of discriminating between real and spoofed speech utterances. ASVspoof has successfully held four editions since 2015 [2-5] and has garnered significant attention. Especially around ASVspoof2019 and ASVspoof2021, many outstanding works have been proposed to address the problem of audio deepfake detection (ADD) [6-9]. Recently, ASVspoof5 was focuses on deepfake detection, dividing into two tracks: the deepfake detection track, which is independent of ASV tasks, and the SASV task, which is related to ASV tasks.\nTo develop CMs for ADD, we focus on Track 1, a stand-alone speech deepfake (bona fide vs spoof) detection task. For Track 1 of ASVspoof5, there are two conditions: closed and open. The closed condition requires not using any data outside of the ASVspoof5 dataset or any pre-trained models. The open condition allows the use of additional datasets and pre-trained features, but these data resource cannot overlap with the source domains of ASVspoof5, meaning they cannot include data from the Multilingual Librispeech (MLS), Libri-Light, or MUSAN speech subset. We believe the design of the closed and open condition is excellent, as it prohibits overlap between training and test data, including pre-trained features, backbone networks, and data. This is a novel concept in the field of domain generalization known as test data information leakage [10]. Avoiding leakage is crucial to effectively measure the generalization capability of the CM.\nIn this paper, we conduct a series of studies on track 1 under open condition. Specifically, we investigate ASVspoof5 from various aspects including data expansion, data augmentation (DA) and self-supervised learning (SSL) feature extraction. For data expansion, we first explored the effectiveness of additional data, including the classic ASVspoof previous series, the latest audio deepfake detection (ADD) datasets such as MLAAD [11], and Codecfake [12]. As data augmentation, we investigated the effectiveness of traditional front-end DA methods, including low-pass filtering, high-pass filtering, time and pitch duration changes, MUSAN [13] and RIR [14] noise augmentation. Considering the high-frequency bands gap characteristic of the ASVspoof5, we propose a novel augmentation method called Frequency mask (Freqmask), which randomly masks frequency bands to enhance the robustness of CM. For SSL feature extraction, we exhaustively investigated the performance of the available pre-trained self-supervised features on ASVspoof5 and explored their performance under different fixed and variable lengths. After conducting the aforementioned studies, we integrated seven CM methods at different scales through log-its score fusion, considering both temporal information and diverse SSL categorical perspectives. Ultimately, we achieved a minimum detection cost function (minDCF) of 0.0158 and an equal error rate (EER) of 0.55% on the ASVspoof5 evaluation progress set."}, {"title": "2. Countermeasure", "content": "In this section, we will sequentially introduce the approaches we considered, covering data expansion, data augmentation, SSL feature selection, and backbone network, from data pre-processing to training."}, {"title": "2.1. Data expansion", "content": "In the open condition, expanding the training data is considered permissible, making additional data expansion a key consideration to enhance the generalization of the CM. We consider to select several representative datasets to co-train with the ASVspoof5 training set. Our selection criteria ensure that the datasets do not overlap with the ASVspoof5 source domain (matching the organizers' rules), are relatively clean (comparable to the ASVspoof5 test set), and have representative spoofing techniques (to enhance CM generalization).\nASVspoof2019LA (19LA): The 19LA training and development sets include six spoofing methods (A01-A06), while the test set includes thirteen spoofing methods (A07-A19). To ensure diversity in the spoofing techniques of the training data, we selected both the training and test sets of 19LA as the augmentation datasets, comprising a total of 96,617 speech samples with 19 spoofing methods.\nMLAAD: The MLAAD dataset currently includes the most spoofing techniques, utilizing 54 TTS models and comprising 21 different architectures, with a total of 76,000 speech samples.\nCodecfake: The codecfake dataset is designed for Audio Language Model (ALM) based audio detection, containing 1,058,216 audio samples synthesized using seven different neural codecs. Given its significantly larger size compared to the ASVspoof5 training set, we selected a subset of 79,369 audio samples from the test set (C1-C6) for the co-training experiment."}, {"title": "2.2. Data augmentation", "content": ""}, {"title": "2.2.1. Freqmask data augmentation method", "content": "As shown in Figure 1, a random selection of samples from the ASVspoof5 evaluation set reveals high frequency bands missing. We refer to this phenomenon as high-frequency band gaps. This frequency band gap phenomenon aggressively removes values in the higher frequency regions, which may be due to the codec reconstruction or resample operation applied to the original audio. While this removal does not significantly impact human perception, it has a substantial influence on CM performance. Since current deepfake speech typically produces artifacts in the high-frequency regions [15-17], which are crucial for CM detection.\nTo effectively address the high-frequency gap phenomenon, the first strategy considered is to use a low-pass filter to remove the high-frequency parts for data augmentation. This low-pass filtering strategy can reduce the disturbance caused by channel effects and codec variabilities, as has been demonstrated to be effective in ASVspoof2021 [18]. In practice, we use the low-pass filter from the audiomentations for low-pass filtering. However, its reconstructed spectrogram, as shown in Figure 2(d), still exhibits some energy above the cutoff frequency, even when the minimum cutoff frequency is set to 0. Thus, we propose the Freqmask data augmentation method to simulate the high-frequency band gap phenomenon in evaluation set as shown in algorithm 1.\nTo be specific, given an input $x$, we use the short-time Fourier transform (STFT) to convert it into a frequency domain matrix $S$. Then, a high-frequency cutoff is randomly selected, and values exceeding this cutoff are masked. Afterward, the inverse short-time Fourier transform (ISTFT) is applied to convert it back to the time domain as $\\hat{x}$, preparing it for subsequent enhancement. In Figure 2, we employ the Freqmask method to enhance one training speech utterance. Specifically, the cutoff frequency is 4 kHz in Figure 2(b) and 7 kHz in Figure 2(d). From the spectrograms, it is evident that this enhancement technique closely resembles the original spectrogram shown in Figure 1."}, {"title": "2.2.2. Other data augmentation method", "content": "The ASVspoof 5 dataset, utilizing MLS data as its source domain, closely resembles real-world scenarios. To better fit the real distribution, we apply DA techniques to expand the source domain data. We utilize MUSAN and RIR for noise data augmentation. Specifically, we employ the noise and music subsets of MUSAN, along with the entire RIR dataset, for augmentation. Additionally, we explored the performance of traditional DA techniques, including high-pass filtering, pitch shifting, and"}, {"title": "2.3. Self-Supervised learning feature", "content": ""}, {"title": "2.3.1. SSL feature selection", "content": "In the open condition, another key aspect is the SSL pre-trained features. The training domain for SSL features consists of a large corpus of real audio, and significant differences between real and fake speech become evident through the SSL layer. Due to the extensive size of the overall training set and limited time, we only experimented with pre-trained features by freezing specific layers rather than finetuning, as detailed below:\nWavLM [19]: WavLM is built based on the HuBERT [20] framework, with an emphasis on both spoken content modeling and speaker identity preservation. We utilized the wavlm-base version from Hugging Face\u00b2, which was pre-trained on 960 hours of the LibriSpeech dataset.\nWav2vec2-large [21]: Wav2vec2 represents a breakthrough in learning robust representations solely from speech audio. Wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. We used the wav2vec2-large version from Hugging Face\u00b3, which was pre-trained and fine-tuned on 960 hours of 16kHz sampled speech audio from the LibriSpeech dataset.\nUniSpeech [22]: UniSpeech aims to enhance the existing SSL framework for speaker representation learning. Specifically, it integrates multi-task learning and mixing strategies into the HuBERT framework, resulting in the proposed speaker-aware pre-training method, UniSpeech. We used the UniSpeech-SAT-Base version from Hugging Face\u2074, which was pre-trained on 960 hours of LibriSpeech dataset. By extracting the hidden states of the pre-trained features, we obtained the SSL features for each audio sample, which typically have dimensions of (batch, length, dimension)."}, {"title": "2.3.2. Reconsider the time duration length", "content": "By extracting the hidden states of the pre-trained features, we obtained the SSL features for each audio sample, which typically have dimensions of (B,L,D). B represents the batch size, L represents the audio length dimension, and D is determined by the transformer parameter of the SSL features. For the three SSL features mentioned above, the dimension of D is 768. However, the choice of audio length, L, needs to be reconsidered. As shown in Figure 4, It is evident that the time-frequency distribution in this case differs from the shorter durations in 19LA, with the training set having an average duration of 11.92 seconds. Additionally, as observed from the spectrogram in Figure 1, the silent segments in the test set are very brief. Most previous studies based on 19LA focused on the first 4 seconds for segment-level judgment, primarily benefiting from detecting the silent segments at the beginning of the audio [23]. However, with increased audio duration in ASVspoof5, we should reconsider the selection of training and testing durations. Therefore, in our experiments, we explored the impact of duration on the CM, including segmenting or padding to the first 4 seconds, 6 seconds, 8 seconds, 10 seconds, 12 seconds, 14 seconds and 16 seconds. Additionally, we experimented with a variable-length training method, during training, each batch sequence was zero-padded to the maximum length within the batch. During inference, single-sample inference was conducted based on the actual length of each sample."}, {"title": "2.4. Backbone", "content": "In previous SSL studies in the field of ADD, an appropriate feature selection is often much more important than the choice of the backbone network. For instance, in the 19LA experiments, using the correct SSL features, such as Wav2vec2-xls-r [24], yields good performance even when other backbones [25] or just FC are used [6]. Therefore, in ASVspoof5 experiment, we only employed the currently the well performing backbone network in the filed of ADD, AASIST [26]. Specifically, we used the SSL-adapted version of AASIST [27] and adjusted the front-end dimension of the fully connected (FC) layer from the original 1024 to 768 to match our SSL dimensions. Hereto, we have comprehensively considered aspects from data expansion to the backbone. The overall pipeline is illustrated in the Figure 3."}, {"title": "3. Experiments", "content": ""}, {"title": "3.1. Details of ASVspoof5 dataset", "content": "Before conducting the experiments, we first analyze the ASVspoof5 dataset as shown in Table 1. The ASVspoof5 training set includes eight spoofing methods (A01-A08), with 18,797 real speech samples and 163,560 fake samples, totaling 182,357 audio samples, where fake samples are in the majority. The development set also includes eight methods (A09-A16), with 23,645 real samples and 82,588 fake samples, totaling 106233 fake samples. For the evaluation set, the organizers initially released the evaluation progress set, which contains a total of 40,765 audio samples. ASVspoof5 participants can submit results up to four times daily via the Codalab platform 5. The Evaluation full set contains a total of 680,774 audio samples, with only one submission opportunity. The experiments and conclusion in this paper are primarily based on the results from the progress set."}, {"title": "4. Results and Discussion", "content": ""}, {"title": "4.1. Initial results for data expansion and feature selection", "content": "In the experimental setup, we initially used some CMs to determine which datasets and which SSL features to use for training. It is important to note that this experiments did not employ any DA strategies. The preliminary experiment in this section aims to validate which datasets and features can effectively enhance CM performance, paving the way for more detailed experiments later.\nFor the initial experiments for data expansion, we used wavlm-base-5, which is the fifth hidden layer representation of WavLM, as the feature. For SSL feature extraction, we use the first 4 seconds of audio, truncating any audio longer than 4 seconds and repeating padding to 4 seconds for audio shorter than 4 seconds. All other parameters remained consistent across the datasets. The experiment results are shown in Table 2. First, we tested using the ASVspoof5 training set (5train) as the baseline, which yielded a minDCF of 0.2720 and an EER of 9.50%. For data expansion, before adding additional data, we first considered co-training with the validation set. Since the spoofing methods in the evaluation set are likely to be different from those in the training and validation sets (A01-A16), expanding the spoofing methods around the source domain data has been shown to be effective in previous research [29]. Experiment results also proved this perspective. Co-training with 5train and ASVspoof5 development set (5dev) showed better performance on the test set compared to training with 5train alone and validating with 5dev, with minDCF decreasing by 0.0932 and EER decreasing by 2.62%. After completing the above experiments, we decided to collectively refer to the 5train and 5dev datasets as 5trndev and to adopt a training approach without a validation set. Building on the training with 5trndev, we further explored the effects of adding 19LA, Codecfake, and MLAAD. The experimental results showed that minDCF was around 0.17 and EER was around 6%, which did not achieve the significant improvement. On the other hand, this indicates that learning from the source domain data in 5trndev is crucial. Therefore, we decided to use the 5trndev (5train + 5dev) as the training set for the subsequent experiments.\nFor the initial experiments on SSL feature selection, we tested the SSL features mentioned in Section 2.3. We first used wavlm-base to investigate the impact of layer selection on performance. In previous experiments, the fifth layer was found to perform well in frozen feature extraction [6]. Therefore, we compared the performance of the last layer and the fifth layer, as shown in the first two rows of Table 3. The results indicate that the fifth layer significantly outperformed the last layer, with minDCF decreasing by 0.075 and EER decreasing by 2.4%. After determining the use of the fifth layer, we experimented with wavlm-base-5, wav2vec2-large-5, and unispeech-base-5. The final results showed that unispeech-base-5 significantly outperformed the other two features, achieving the lowest minDCF of 0.1189 and the lowest EER of 4.10%. Therefore, we chose to use unispeech-base-5 as the primary feature for subsequent experiments."}, {"title": "4.2. Results for data augmentation", "content": "In the experiments of this section, the training dataset is 5trndev, and the SSL feature is unispeech-base-5, which are the best settings identified in the previous subsection. The experiment results are shown in Table 4. It is evident that RIR significantly enhances CM performance. With RIR, the CM achieves a minDCF of 0.0748 and an EER of 2.86%. Next, we explored common data augmentation (DA) configurations in the field of speaker verification, combining MUSAN and RIR. This included using both the MUSAN noise subset alone and the full MUSAN set (excluding the speech subset). The experimental results show that MUSAN also provides a performance improvement for the CM, with the full MUSAN dataset further enhancing performance compared to using only the noise subset.\nAfter validating the effectiveness of MUSAN and RIR, we proceeded to experiment with other DA methods. The comparison results show that using Freqmask performs better than low-pass filter (LP). Specifically, RIR + MUSAN + LP achieved a minDCF of 0.0406 and an EER of 1.46%, while RIR + MUSAN + Freqmask(0.3) achieved the best minDCF of 0.0344 and an EER of 1.21%. It is important to note that Freqmask(0.3) and Freqmask(0.5) represent 30% and 50% probabilities, respectively, for applying Freqmask to the original audio. This suggests that excessively high probabilities for applying the mask may hinder the learning of full-frequency information from the source domain. In addition to the LP, we also experimented with using high-pass filter (HP) in combination with LP, as well as time stretch and pitch shift together. However, these DA combinations did not result in further improvements in CM performance."}, {"title": "4.3. Results for data expansion", "content": "Although we initially conducted preliminary data expansion experiments in section 4.1, the selected feature was not the best, and no DA method was applied. In this section, we consider whether applying DA to the additional expanded data can further improve CM performance. The results can be seen in Table 5, where RMF represents the best DA configuration of RIR + MUSAN + Freqmask(0.3), and Dataset2 refers to the expanded dataset. We can draw two conclusions: first, as shown in rows 1, 3, and 5 of the Table 5, expanding extra data did not significantly improve CM performance. Second, applying DA to the external data decreased CM performance. Certainly, this only indicates that the domain information of 19LA, Codecfake, and MLAAD does not match the ASV5trndev evaluation progress set. Therefore, in subsequent experiments, we will use only the 5trndev as the training set."}, {"title": "4.4. Results for different feature length", "content": "In previous experiments, we followed the 4 seconds audio extraction scheme consistent with the 19LA series. However, this approach does not align with the duration distribution of the ASVspoof5. Therefore, we need to reevaluate the optimal duration for feature extraction based on experimental results. Specifically, we conducted experiments using fixed-length features ranging from 4 to 16 seconds. For fixed-length processing, we used the following strategy: for both training and evaluation audio, any audio longer than the specified duration was truncated, while audio shorter than the specified duration was padded by repetition to achieve the fixed length. In Table 6, it is evident that as the duration of training audio increases, CM performance improves progressively. The best performance was observed with a feature duration of 10 seconds, achieving a minDCF of 0.0254 and an EER of 0.93%. After extending the duration beyond 10 seconds, we observed no significant improvement in CM performance. This could be related to the duration distribution of the evaluation progress set, where most audio samples exceed 10 seconds as shown in Figure 4(c). Additionally, it may be related to the parameters of the AASIST model. To address this, improvements should be made to handle longer duration, such as incorporating deeper ResNet convolution layers in the front-end or increasing the number of temporal graph nodes. We also explored a variable-length architecture, where max length zero-padding was applied at the batch level during training, and single-sample inference was conducted based on the actual length during evaluation. However, this strategy did not outperform the fixed 10-second scheme."}, {"title": "4.5. Temporal variability and multi-viewed SSL fusion for final result", "content": "Since the fixed-length training and testing CM may disregards temporal information beyond the specified duration. For example, using 4 seconds of audio for training a CM may result in the loss of temporal information beyond 4 seconds, while using 16 seconds for training may reduce precision in evaluating the first 4 seconds. Therefore, we used CMs with different durations for score fusion, providing a multi-scaled temporal perspective for a comprehensive evaluation of audio authenticity. Specifically, we combined four different time scales as shown in the Table 7: \u2460 4s, \u2461 10s, \u2462 16s, and \u2463 various durations. Each CM was assigned a fusion weight of 0.25. The fusion results, shown in the first row of Table 8, achieved a minDCF of 0.0193 and an EER of 0.69%.\nIn addition to temporal fusion, we also considered a multi-viewed SSL feature-level fusion. Specifically, we combined four different features: \u2463 unispeech-base-5, \u2464 unispeech-base-last, \u2465 wavlm-base-5, and \u2466 wav2vec2-large, as shown in Table 7. We selected the optimal fixed duration 10s to extract these SSL feature, and each CM was also assigned a fusion weight of 0.25. The fusion outcomes, as presented in the second row of Table 8, yielded a minDCF of 0.0206 and an EER of 0.72%.\nTo simultaneously integrate temporal and feature information across different CM, we fused seven types of CM, as shown in the last row of Table 8. The fusion weights for optimal performance are as follows: the best performance of 10-second duration CM \u2461 is assigned a weight of 0.3, the second best performance CM with various duration \u2462 is assigned a weight of 0.2, and the remaining five CMs are each assigned a weight of 0.1. Our fusion system achieved a minDCF of 0.0158 and an EER of 0.55% on the ASVspoof5 evaluation progress set. This system was also used for our final submission on the ASVspoof5 evaluation full set. However, the performance declined on the ASVspoof5 evaluation full set, with a minDCF of 0.224 and an EER of 7.72%."}, {"title": "5. Conclusion", "content": "In this paper, we focus on the ASVspoof 5 Track 1 open condition. We initially investigate ASVspoof5 from the perspectives of data expansion, data augmentation, and SSL feature selection. Then, based on the high-frequency band gaps characteristic of ASVspoof5, we propose a DA method called Frequency Mask. Finally, by integrating multiple CM from both temporal and feature-type perspectives, we achieve a minDCF of 0.0158 and an EER of 0.55% on the ASVspoof 5 evaluation progress set.\nAlthough our results are promising on the progress set, we observed a significant drop in performance on the ASVspoof5 evaluation full set. This discrepancy may be due to unknown deepfake methods or codec reconstruction methods present in the full set that were not appeared in the progress set. As a result, some conclusions drawn from the progress set, such as the effectiveness of additional data, optimal feature duration, and best features, may differ. In future work, we will focus on optimizing the CM performance on the evaluation full set."}, {"title": "6. Acknowledgements", "content": "This work is supported by the National Natural Science Foundation of China (NSFC) (No.62101553)."}]}