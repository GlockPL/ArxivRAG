{"title": "WHEN ATTENTION SINK EMERGES IN LANGUAGE MODELS: AN EMPIRICAL VIEW", "authors": ["Xiangming Gu", "Tianyu Pang", "Chao Du", "Qian Liu", "Fengzhuo Zhang", "Cunxiao Du", "Ye Wang", "Min Lin"], "abstract": "Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as attention sink. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others. Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how optimization, data distribution, loss function, and model architecture in LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most importantly, we find that attention sink acts more like key biases, storing extra attention scores, which could be non-informative and not contribute to the value computation. We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization. After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters.", "sections": [{"title": "INTRODUCTION", "content": "Xiao et al. (2023) showed that Large Language models (LLMs) allocate significant attention to the initial tokens, irrespective of their semantic relevance. This interesting phenomenon is termed as attention sink and has widespread applications, including streaming/long context generation (Xiao et al., 2023; Han et al., 2024; Yang et al., 2024), KV cache optimization (Ge et al., 2023; Wan et al., 2024; Wu & Tu, 2024), efficient inference (Zhang et al., 2024; Chen et al., 2024), model quantization (Liu et al., 2024b; Huang et al., 2024), and others.\nA seminal of works attempted to understand attention sink. Among them, Cancedda (2024) clarified that attention sink primarily appears only on the first token. They found that early FFNs in LLaMA2 blast off the large norm of hidden states of the first token, thus leading to the attention sink in later layers. This is referred to as massive activations (very few activations exhibit extremely large values compared to others) in Sun et al. (2024). Besides, Sun et al. (2024); Yu et al. (2024) observed that attention sink may also appear in several word tokens carrying limited semantic information and having no fixed position. Despite the above research efforts, a deep understanding of attention sink is still absent. Therefore, we conduct a comprehensive study to investigate when attention sink emerges.\nBased on open-sourced LLMs, we show that the first token acts as biases: the angles between the first key and queries of other tokens are typically small, leading to attention sink. Then we find that"}, {"title": "PRELIMINARIES ON LMS AND ATTENTION SINK", "content": "Let $f_\\theta$ be an auto-regressive LM with $L$ transformer decoder blocks and $X \\in \\mathbb{R}^{T\\times|V|}$ := {$X_1,X_2,...,X_T$} are the input tokens, where each token $x_t$ is a one-hot encoding and $V$ is the vocabulary size of tokenizer $V$. The LM output is also a sequence $Y \\in \\mathbb{R}^{T\\times|V|}$ := {$Y_1, Y_2, ..., Y_T$} = $f_\\theta(X)$, where $y_t$ represents the predicted logits of $p(x_{t+1}|X_{<t})$.\nTransformer blocks. In the forward pass, $X$ is first embedded as $H^0 \\in \\mathbb{R}^{T\\times d}$ := $XW_E + P$, where $W_E \\in \\mathbb{R}^{|V|\\times d}$ is the learnable word embedding, $P \\in \\mathbb{R}^{T\\times d}$ is the positional embedding, and $d$ is the hidden dimension. We denote $H^l \\in \\mathbb{R}^{T\\times d}$ := {$h_1, h_2, ..., h_T$}, $1 \\leq l \\leq L$ to be the output of the $l$-th block. Each block comprises a multi-head self-attention (MHSA) operation and a feed-forward network (FFN). The block has either a pre-norm or post-norm structure according to the location of layer normalization (LN) (Ba et al., 2016; Zhang & Sennrich, 2019). Most of LLMs consider a pre-norm block, as also shown in Figure 1(Left):\n$H^l = FFN(LN(O^l + H^{l-1})) + O^l + H^{l-1}, O^l = MHSA(LN(H^{l-1})),$ (2)\nwhile the post-norm transformer block is\n$H^l = LN \\Big(FFN(LN(O^l + H^{l-1})) + LN(O^l + H^{l-1})\\Big), O = MHSA(H^{l-1}).$ (3)\nMHSA layers. In the MHSA layer, the input $H^{l-1}$ are first transformed into keys, queries, and values: $K^{l,h} = H^{l-1}W_k^{l,h}$, $Q^{l,h} = H^{l-1}W_q^{l,h}$, $V^{l,h} = H^{l-1}W_v^{l,h}$ for each head $1 \\leq h \\leq H$ (we omit the notation of LN when considering pre-norm design for simplicity). Here $W_k^{l,h}$, $W_q^{l,h}$, $W_v^{l,h} \\in \\mathbb{R}^{d \\times d_h}, d_h = d/H$. Then the attention output is computed as\n$A^{l,h} = Softmax(\\frac{Q^{l,h} K^{l,h^T}}{\\sqrt{d_h}} + M), O^l = Concat_h(A^{l,h}V^{l,h}) W_o^l,$ (4)\nwhere $M \\in \\mathbb{R}^{T \\times T}$ is an attention mask. For vanilla causal attention, $M_{ij} = -\\infty$ for $i < j$ and $M_{ij} = 0$ for $i \\geq j$. Finally, the output of final transformer block $H^L$ is fed into an unembedding layer for prediction: $Y = LN(H^L)W_{cls}$, where $W_{cls} \\in \\mathbb{R}^{d \\times |V|}$.\nPositional embedding. NoPE (Kazemnejad et al., 2024) considered no explicit positional embedding (PE) in LMs, where $P = 0$. When using absolute PE (Vaswani et al., 2017), $P$ is a periodic function of token positions. Devlin et al. (2019); Brown et al. (2020) adopted a learnable PE, which means $P$ is a learnable embedding of token positions. The dot product between each query and key meets $<q_i, k_j> = q_i k_j$ when using the above three PEs. While for relative PE (Raffel et al., 2020), ALiBi (Press et al., 2021), Rotary (Su et al., 2024), they have $P = 0$. Instead, they modify the dot product"}, {"title": "PROPERTIES OF ATTENTION SINK", "content": "3.1 THE FIRST TOKEN ACTS AS BIASES\nUniqueness of the first token. It is noted that the calculation of hidden states for the first token has no involvement of self-attention $h_1 = FFN(LN(o_1 + h_1^{l-1})) + o_1 + h_1^{l-1}$, where $o_1 = LN(h_1^{l-1}) [W_k^{l,1} W_q^{l,2} ... W_v^{l,H}] W_o^l$. Therefore, $h_1$, and corresponding queries/keys/values $k_1^h = LN(h_1^{l-1})W_k^{l,h}$, $q_1^h = LN(h_1^{l-1})W_q^{l,h}$, $v_1^h = LN(h_1^{l-1})W_v^{l,h}$ could be considered as the MLP output of input word embedding $x_1W_E$. Cancedda (2024); Sun et al. (2024) showed that $h_1$ has large norm or massive activations, thus behaving as a bias to absorb the attention. Using LLaMA3-8B Base model (Dubey et al., 2024), we show that from certain transformer block, e.g., $l = 2$, the $l_2$-norm of $h_1$ is significantly larger than that of other tokens $h_{t \\neq 1}$ in Figure 2(Top). Despite the large $l_2$-norm of hidden states, we observe that the $l_2$-norm of keys and queries of the first token is significantly smaller than that of other tokens in the same figure. This motivates us to further explore the functionality of massive activations in hidden states.\nQK angles contribute to attention sink. In the $l$-th transformer block, we consider the keys and queries after adding PE (Rotary in LLaMA3-8B Base): $k_t^h = LN(h^{l-1})W_k^{l,h}R_{\\Theta, t}$, $q_t^h = LN(h^{l-1})W_q^{l,h}R_{\\Theta, -t}$, where LN is RMSNorm (Zhang & Sennrich, 2019): $LN(h) = RMS(h) \\odot h$"}, {"title": "MEASURING ATTENTION SINK", "content": "Threshold-based metrics. Xiao et al. (2023) showcased the appearance of attention sink by visualizing attention logits/scores in different heads/blocks. This leads to the intractability of measuring attention sink quantitatively due to the large number of attention heads and blocks. Therefore, we first explore the metrics to measure the attention sink. Within each head, we compute the importance scores for the k-th token $a_k^{l,h} = \\frac{\\sum_{i \\neq k+1} A_{k,i}^{l,h}}{\\sum_{i} A_{k,i}^{l,h}}$. We mainly focus on the first token $a_1^{l,h}$. It is noted that $0 \\leq a_k^{l,h} \\leq 1$ since $\\sum_{i} A_{k,i}^{l,h} = 1$ and $0 \\leq A_{k,1}^{l,h} \\leq 1$. Then we adopt a threshold-based metric, we consider a head has attention sink in the first token if $a_1^{l,h} > \\epsilon$. Considering that the whole"}, {"title": "ATTENTION SINK UNDER DIFFERENT INPUTS", "content": "Different data domains. We first explore the effects of input domains on attention sinks. The pile dataset (Gao et al., 2020), a regular dataset for LM pretraining, has 17 available data domains. As shown in Appendix B.2, input domains have negligible effects on our attention sink metric Sink.\nBeyond natural languages. We also consider two ideal scenarios: (i) randomly sample T tokens from the tokenizer vocabulary V to construct a sequence and (ii) randomly sample 1 token from the tokenizer V and repeat it T times. We exclude the BOS token. As present in Table 1(Left), compared to natural language, random tokens have insignificant effects on the existence of attention sink. However, with repeated tokens, attention sink in Mistral (Jiang et al., 2023) and LLaMA models disappears. In Appendix B.1, we prove that for LMs with NoPE/relative PE/ALiBi/Rotary, if the first T tokens are the same, their corresponding hidden states are the same. They all have massive activations, thus dispersing the attention sink. We also provide the closed form/upper bound for attention scores in these LMs through Propositions 1-4."}, {"title": "ATTENTION SINK UNDER DIFFERENT LMS", "content": "Base vs. chat model. Compared with base models, chat models are typically continually trained through instruction tuning (Ouyang et al., 2022). From Table 1(Right), instruction tuning has an insignificant impact on attention sink, which motivates us to focus on the LM pre-training.\nModel scale. We evaluate the metric Sink of LLaMA2 Base (Touvron et al., 2023), LLaMA3 Base (Dubey et al., 2024), Pythia (Biderman et al., 2023), GPT2 (Radford et al., 2019), OPT (Zhang et al., 2022) families. As visualized in Figure 4(Left), attention sink emerges in small LMs, even in Pythia-14M. Only in Pythia family, larger-sized LMs tend to have more obvious attention sink.\nBlock-wise and head-wise property. We visualize distributions of attention sink across different blocks and heads in LLaMA2/LLaMA3/GPT2/Mistral/Pythia/OPT family in Appendix B.2. We"}, {"title": "EFFECTS OF OPTIMIZATION ON ATTENTION SINK.", "content": "We pre-train a series of LLaMA models to conduct our experiments. Due to the intractability of replicating LLaMA pre-training, we design small-sized models. Following Liu et al. (2024a), we set hidden dimension $d = 768$, block number $L = 10$, head number $H = 8$, intermediate size of FFN as 1536, resulting in approximately 60M parameters except for word embeddings and unembeddings. We keep the other design the same as LLaMA2 models, including Rotary (Su et al., 2024), pre-norm structure, RMSNorm (Zhang & Sennrich, 2019) as LN, SwiGLU activation (Shazeer, 2020) in FFN, etc.\nFor data distribution, we sample 5B tokens from the Pile dataset (Gao et al., 2020). We set the context length to 2048 tokens, the batch size to 1M tokens, and the training step to 20k (including 100 steps for warm-up). We adopt a learning rate of 4e-4 with cosine scheduling. The optimizer is AdamW (Loshchilov & Hutter, 2017) with a weight decay ratio of 0.1. We use the Pile-CC validation loss (Gao et al., 2020; Liu et al., 2024a) to measure the model performance and sample 100 sequences with $T = 64$ (no BOS token) out of training data to measure the metric Sink with $\\epsilon = 0.3$.\nOptimization steps. As visualized in Figure 4(Middle), under our default setup, attention sink emerges after certain optimization steps, e.g., between 1k and 2k steps. With the progression of pre-training, attention sink becomes more obvious.\nLearning rate. With a smaller learning rate, it takes longer training steps to lower training loss, as present in Figure 4(Right). Meanwhile, the emergence of attention sink is also delayed. Besides, we also find that a smaller learning rate (1e-4) results in LMs with less obvious attention sink. But further decreasing learning rate significantly affects the optimization and model performance, thus affecting the emergence of attention sink.\nBatch size. In Table 8(Left), we find that only modifying batch size has no effects on attention sink."}, {"title": "EFFECTS OF DATA DISTRIBUTION PDATA ON ATTENTION SINK", "content": "Training data amount. In the default setup, we consider 5B tokens. We wonder whether the attention sink emerges if we further constrain the data within a fixed compute budget. Therefore, we constrain the training data to 5B, 2.5B, 500M, 200M, 100M, and 50M. Meanwhile, we fix the batch size and optimization steps. As visualized in Figure 5(Right), with less training data, the trained LLMs tend to overfit. Meanwhile, attention sink also disappears."}, {"title": "EFFECTS OF LOSS FUNCTION LON ATTENTION SINK", "content": "Weight decay. The loss function becomes $L = \\sum_{t=2}^C log p_\\theta(x_t | x_{<t}) + \\gamma ||\\theta||_2^2$ when introducing weight decay ratio $\\gamma$. As indicated in Table 2, even $\\gamma = 0$ in the loss function, attention sink still emerges in LMs. Then a larger $\\gamma$ encourages more heads to have attention sink. But further increasing weight decay hurts the optimization, leading to less obvious or even no attention sink.\nPrefix language modeling. Since the first token is not predicted in the auto-regressive loss function, it could be considered as the prefix token. Then the original auto-regressive loss can be generalized into the formula $L = \\sum_{t=p+1}^C log p_\\theta(x_t | x_{p+1:t-1}, x_{1:p})$, with the prefix length $p = 1$. Motivated by Wang et al. (2022), we consider $p > 1$ and the casual mask visualized in Figure 5(Left). Although this design does not affect the emergence of attention sink, it shifts the sink position. In Figure 5(Middle), the attention sink only appears on one token. But it appears among these prefix tokens instead of on the first token only. Massive activations also appear on the corresponding sink token.\nShifted window attention. Motivated by the shifted window attention adopted in Mistral-7B, we further explore the effects of window size on attention sink. With shifted window attention, the loss function becomes $L = \\sum_{t=2}^C log p_\\theta(x_t | x_{t-w:t-1})$, where $w$ refers to the window size. As shown in Figure 6(Left) and (Middle), with shifted window attention, we find that if $t \\leq w$, the t-th token can still \"look at\" the first token, and LMs still have attention sink on the first token. When $t > \\omega$, the t-th token can only attend up to the $t - w + 1$-th token. Although this token is the \"first token\" for the t-th token, typically it has no attention sink. We have similar observations in Mistral-7B. Additionally, from Figure 6(Right), smaller window size prevents the emergence of attention sink."}, {"title": "EFFECTS OF MODEL ARCHITECTURE po ON ATTENTION SINK", "content": "In this section, we mainly explore the effects of positional embedding, pre-norm or post-norm structure, and attention design on the emergence of attention sink. In Appendix C, we also show that varying activation functions in the FFN, multi-head design (including concatenation operation and number of heads) do not affect the emergence of attention sink.\nPositional Embedding\nAttention sink always appears on the first token, which motivates us to explore where such a position property is brought by positional embedding (PE). Therefore, we attempt to replace the original Rotary with other PEs, as shown in Table 3. We differentiate these PEs through the calculations of the hidden states $H^0$ before the first transformer block and the dot product between queries and keys $(q_i, k_j)$. The detailed formulations are delayed to Appendix A. From the same Table, we observe that only the model with relative PE is difficult to train while other models have comparable performance under our setup. Then we note that all these LMs, even the one without explicit PE (NoPE), have attention sink.\nPRE-NORM AND POST-NORM STRUCTURE\nLayer normalization (LN) (Ba et al., 2016; Zhang & Sennrich, 2019) regularizes the hidden states in LMs by re-centering and re-scaling, which may affect the massive activations. This motivates us to explore the effects of LN location on attention sink. In the pre-norm structure, as stated in Equation 2, hidden states from the earlier blocks could be retained by the later blocks through residual connections (He et al., 2016). Therefore, if massive activations appear in a specific block, they will likely be retained in the subsequent blocks. Within a post-norm structure, the hidden states will be normalized before being fed into the following blocks, as present in Figure 1(Left).\nWhen replacing the pre-norm structure with the post-norm structure, Sink becomes 13.54%. This indicates that the attention sink still exists in post-norm LMs. After further investigations, as visualized in Figure 7(Left), massive activations exist in the hidden states before the post LN instead of $h_1^l$.\nATTENTION BIASES\nImplicit biases in attention. In Section 3.1, we have shown that the first token acts as a bias: its key $k_1^h$ is distributed in a different manifold and its value $v_1^h$ has small $l_2$ norm. Xiao et al. (2023) considered a learnable sink token in each chunk before the input tokens during LM pre-training. As this token is fixed in the first token, this could be considered as implicitly introducing biases $k^{*l,h}, v^{*l,h}, q^{*l,h}$ in attention, as shown in the second row in Table 4. These biases are the MLP output of $x^*W_E$. As a result, attention sink appears on this learnable token $x^*$ instead of the actual first token $x_1$."}, {"title": "ATTENTION OPERATION", "content": "General formulation of attention. In the last section, we realize that LMs with softmax attention need key biases to save extra attention. This motivates us to explore whether such a property is related to the dependence among attention scores due to the softmax operation. First, the attention output for the i-th token can be generalized into the following formulation: $v_i = Z_i^{-1} \\sum_{j=1}^C sim(\\varphi(q_i), \\varphi(k_j))v_j =  \\sum_{j=1}^C \\frac{sim(\\varphi(q_i), \\varphi(k_j))}{Z_i}v_j$, where we omit the positional embedding, and block/head indexes for simplicity. Zi is a normalization term and $\\varphi(\\cdot)$ is a kernel function. Normally, $Z_i =  \\sum_{j'=1}^C sim(\\varphi(q_i), \\varphi(k_{j'}))$. For softmax attention, the kernel $\\varphi(\\cdot)$ is an identity kernel and the similarity function is $exp(\\frac{q_i k_j}{\\sqrt{d_h}})$.\nNormalization. In Appendix C, we first show that only scaling the normalization term $Z_i \\rightarrow Z_i^\\alpha$ with $\\alpha < 1$ results in less obvious attention sink but does not stop its emergence. So we consider removing the normalization. Since the exponential function in softmax tends to explode without normalization, we replace it with sigmoid or elu plus one. When evaluating the attention sink, we compute the proxy attention scores by using the term $Z_i =  \\sum_{j'=1}^C sim(\\varphi(q_i), \\varphi(k_{j'}))$ for attention sink metric Sink. As shown in Table 6, without normalization, LMs still have comparable validation loss but no attention sink. With normalization, attention sink also emerges in LMs with sigmoid attention.\nKernel functions. Motivated by linear attention (Katharopoulos et al., 2020), we consider different kernel functions $\\varphi(\\cdot)$, including elu plus one, identity, and MLP. It is noted that $sim(\\varphi(q_i), \\varphi(k_{j'}))$ could be minus for identity and MLP kernels. This brings intrinsic difficulty for normalization during the training and calculation of Sink. For normalization in the training, we consider $Z_i = max(\\sum_{j'=1}^C sim(\\varphi(q_i), \\varphi(k_{j'})), 1)$. When computing Sink, we consider the proxy"}]}