{"title": "ENACT-Heart \u2013 ENsemble-based Assessment Using\n-\nCNN and Transformer on Heart Sounds", "authors": ["Jiho Han", "Adnan Shaout"], "abstract": "This study explores the application of Vision\nTransformer (ViT) principles in audio analysis, specifically fo-\ncusing on heart sounds. This paper introduces ENACT-Heart\n- a novel ensemble approach that leverages the complementary\nstrengths of Convolutional Neural Networks (CNN) and ViT\nthrough a Mixture of Experts (MoE) framework, achieving a\nremarkable classification accuracy of 97.52%. This outperforms\nthe individual contributions of ViT (93.88%) and CNN (95.45%),\ndemonstrating the potential for enhanced diagnostic accuracy\nin cardiovascular health monitoring. These results demonstrate\nthe potential of ensemble methods in enhancing classification\nperformance for cardiovascular health monitoring and diagnosis.", "sections": [{"title": "I. INTRODUCTION", "content": "Cardiac diagnostics have undergone remarkable advance-\nments over the centuries, yet the analysis of heart sounds\nremains a fundamental aspect of assessing cardiovascular\nhealth. These sounds, primarily associated with the closure\nof heart valves, offer crucial insights into cardiac function.\nThe first heart sound (S1), produced by the closure of the\natrioventricular valves, and the second heart sound (S2), asso-\nciated with the closure of the semilunar valves, are commonly\nrecognized as the characteristic 'lub' and 'dub.' In a healthy\nheart, these sounds provide clear indications of proper valve\nfunction. Over time, the practice of analyzing these sounds\nhas become an indispensable non-invasive tool in medical\ndiagnostics, offering a reliable means to detect abnormalities\nand evaluate cardiac performance."}, {"title": "A. Anomalies in the Heart Sounds", "content": "Heart murmurs, extra heart sounds, and extrasystoles are\ncommon anomalies detected during cardiac auscultation. Each\ntype of anomaly provides valuable information about potential\nunderlying cardiac conditions.\nHeart murmurs are among the most common anomalies\ndetected through auscultation. Produced by turbulent blood\nflow strong enough to generate audible noise, these \"whoosh- ing\" sounds can be heard in various scenarios, including some\nhealthy individuals. While innocent murmurs, also known as\nfunctional or benign murmurs, are typically harmless and\nnot associated with structural heart abnormalities, pathologic\nmurmurs indicate underlying conditions such as valve defects,\ncongenital heart defects, or abnormal blood flow patterns.\nExtra Heart Sounds refer to additional heart sounds be-\nyond the normal \"lub-dub\" pattern. These may manifest as\n\"lub-lub dub\" or \"lub dub-dub\" sequences. Extra heart sounds\ncan sometimes indicate underlying conditions, although they\nmay also occur in healthy individuals. Detection of these\nsounds is crucial as they may not be easily identified through\nother diagnostic tools like ultrasound.\nExtrasystole involves irregular heart rhythms, typically\npresenting as extra or skipped heartbeats. These can be heard\nas sequences such as \"lub-lub dub\" or \"lub dub-dub.\" While\nextrasystoles may be benign, they can also signify underlying\nheart disease, making early detection important for effective\ntreatment."}, {"title": "B. Organization of the Paper", "content": "This paper is organized as follows: Section II covers the\nbackground of the study, providing the necessary theoretical\nfoundation. III provides an in-depth review of related work,\ndiscussing the advancements and methodologies in cardiac\nsound analysis and the integration of machine learning models\nin cardiovascular diagnostics. Section IV details the proposed\napproach, including the data preprocessing techniques, the\ngeneration of audiovisual data, and the model architectures.\nSection V describes the experimental setup, including the\ndataset used, training procedures, and evaluation metrics. The\nresults of the experiments are presented in VI, highlighting the\nperformance of individual models and the ensemble method.\nFinally, Section VII concludes the paper with a summary\nof findings, potential implications for clinical practice, and\ndirections for future research."}, {"title": "C. Contributions", "content": "The primary impacts of the proposed experiment are the\nfollowing:\n\u2022\nWe developed ENACT-Heart (See Fig. 1) a novel\ntransformer-based ensemble method specifically designed\nfor diagnosing medical audio data through advanced visu-\nalization techniques. ENACT-Heart demonstrates state-of-\nthe-art performance, surpassing other existing ensemble\nmethods in the field."}, {"title": "II. HISTORICAL BACKGROUND", "content": "The diagnosis of heart conditions dates back to the early\ndays of medicine, where physicians relied on palpation and\npulse assessment to detect abnormalities. A significant break-\nthrough occurred in the 1700s when Jean Baptiste de Senac,\nphysician to King Louis XV of France, established the connec-\ntion between atrial fibrillation and mitral valve disease. Senac's\nwork laid the foundation for cardiology as a distinct field of\nstudy [2].\nThe invention of the stethoscope by Ren\u00e9 Laennec in 1816\nmarked a pivotal moment in cardiac diagnostics. Laennec\nintroduced the technique of \"mediate auscultation\" using his\nnewly created paper acoustic device, allowing for more ac-\ncurate detection of heart sounds and abnormalities [3]. This\ninnovation remains a cornerstone in the history of cardiology.\nThese early diagnostic methods, based on manual interpre-\ntation of heart sounds, evolved significantly over the centuries.\nWith technological advancements, traditional auscultation has\nbeen augmented by electrocardiograms (ECGs) and other\nimaging modalities. In recent years, the integration of artificial\nintelligence (AI) and machine learning has opened new av-\nenues for the analysis of heart sounds, leading to more precise\nand efficient diagnostic tools [4], [5]."}, {"title": "III. RELATED WORKS", "content": "Thematically, the proposed method lies in the field of\ncomputer-assisted diagnosis (CAD) systems for heart disease.\nCAD systems for heart diseases utilize computation techniques\nsuch as machine learning, pattern recognition, and AI to\nanalyze cardiac data and provide decision-support tools for\nhealthcare providers.\nThere have been numerous attempts to apply CAD systems\nfor heart diseases in diverse modalities, including but not\nlimited to ECG, cardiac CT/MRI, etc. These systems analyze\ncardiac data to identify abnormalities and patterns - especially\nindicators of certain heart diseases. However, the direction of\nthe majority of these researches are pointed mainly toward\ncomputer vision over audio AI, mainly due to the advanced\ndeep learning models available.\nFor some researches that emphasized sound classification,\nits methodologies have varied slightly from the approach\nproposed in this study. For instance, Jumphoo et al. utilized\na CNN for feature extraction and Data-efficient Image Trans-\nformer (DieT), a variant of the ViT model, for classification\ntasks through stacking [6]. Another heart sound classification\nmodel, proposed by Liu et al., also uses ViT for classification\nbut employs a different image modality called bispectral\npatterns and relies solely on ViT without integrating other\nmodels [7]. While these studies highlight the effectiveness of\nViT individually, they do not explore the potential benefits of\nusing an ensemble approach.\nOverall, the integration of multiple distinct AI models and\nmodalities from the same sound inputs, as proposed in the\nENACT-Heart using an MoE approach, has not been attempted\nyet. This novel methodology leverages the strengths of both\nViT and CNN models, potentially offering a more robust and\naccurate solution for heart sound classification."}, {"title": "B. Methodologically Related Works", "content": "The use of computer vision as a tool for machine hearing\nis an emerging approach. There have been attempts to use\ncomputer vision techniques as a method of machine hearing -\nanalyzing audio signals by treating them as visual data.\nHsu et al. [8] presented a deep learning-based music clas-\nsification through mel-spectrogram and Fourier tempogram\nfeatures. Although the concept of using multiple different\naudiovisual modalities and models from singular sound data\nis there, the paper employed the short-chunk CNN + ResNet\nas the backbone architecture of their models."}, {"title": "IV. PROPOSED APPROACH", "content": "The choice of heart sound analysis in this study is driven by\nits unique diagnostic value, which complements other modal-\nities such as ECG. Despite the advent of modern diagnostic\ntechniques and sophisticated imaging modalities, cardiac aus-\ncultation and heart sounds remain invaluable diagnostic tools.\nWhile ECG is widely regarded as the gold standard for diag-\nnosing cardiac rhythm disorders and ischemic heart disease, it\nmay not capture certain aspects of cardiac function that heart\nsound analysis can, such as detecting murmurs, rubs, and other\nabnormal heart sounds indicative of structural abnormalities\nlike valvular heart diseases or ventricular hypertrophy. There-\nfore, heart sound analysis provides additional, complementary\ninformation that can enhance diagnostic accuracy.\nResearches has demonstrated that it is possible to process\nspectrograms from audio data as images and apply computer\nvision algorithms such as CNN [9]\u2013[11]. The core problem of\nthe current approaches in using regular CNN-based computer\nvision methods on audio spectrogram representation lies in\nthe distinctiveness of the spectrogram in comparison to other\nimage data.\nVisual transformers leverage attention mechanisms to cap-\nture dependencies between different parts of the input data.\nThis allows them to model long-range dependencies more\neffectively than traditional CNNs, whose feature extraction is\nlimited to local receptive fields. By aggregating information\nfrom across the entire spectrogram, transformers can show a\nglobal contextual understanding of the audio signal, enabling\nthem to capture non-local dependencies and extract meaningful\nfeatures from spectrograms."}, {"title": "A. Spectral Data Visualization & Analysis", "content": "In spectral visualization and analysis, researchers employ\nvarious techniques to gain insights into the frequency content\nof signals. These methodologies enable the examination of\nhow frequencies evolve over time, providing valuable infor-\nmation for tasks such as audio processing, speech recognition,\nand biomedical signal analysis.\nSpectrogram. Spectrograms stand as one of the primary\ntools in spectral visualization. They offer detailed represen-\ntations of frequency spectra over time, revealing how the\nfrequency composition of a signal changes temporally. By\nplotting frequency on the vertical axis, time on the horizontal\naxis, and intensity or magnitude using color or brightness,\nspectrograms provide a comprehensive view of signal dy-\nnamics. This detailed visualization allows analysts to identify\nspecific features, patterns, and transient events within the\nsignal, making spectrograms invaluable for tasks requiring\nfine-grained temporal frequency analysis.\nSpectral Centroid. In contrast to the detailed temporal- frequency mapping provided by spectrograms, spectral cen- troids offer a simplified summary of a signal's frequency content. The spectral centroid indicates the \"center of mass\"\nor average frequency of a signal within each time frame. This single-value representation reduces the complexity of the data while still providing a concise summary of the signal's frequency characteristics. Spectral centroids are particularly useful for enhancing computational efficiency and maintaining robustness against noise and variations in the signal. However, they lack the detailed temporal information that spectrograms provide.\nThe synergy in using spectrograms and spectral centroids with different models lies in their ability to capture distinct and complementary features of audio signals. Spectrograms pro- vide a comprehensive visualization of the frequency content"}, {"title": "B. MOE", "content": "MoE is a powerful ensemble learning methodology used in\nmachine learning and statistical modeling. Within ensemble\nmethods, multiple models are combined to improve predictive\nperformance compared to any individual model. MoE takes\nthis concept a step further by combining various models and\nadjusting the weight of their contributions adaptively per the\ninput data.\nIn MoE, the \"experts\" are individual models or learners,\neach specializing in a particular region of the input space\nor addressing specific patterns in the data. These experts\nmake predictions independently based on their specialized\nknowledge. The key innovation of MoE lies in the gating\nnetwork, which dynamically selects the most relevant expert\nor combination of experts for each input instance.\nThe gating network, often implemented as a neural network,\nlearns to assign weights to the experts based on the input data.\nThese weights determine the contribution of each expert to the\nfinal prediction. By adaptively combining the predictions of\nmultiple experts, MoE can capture complex relationships in the\ndata and achieve superior predictive performance compared to\ntraditional ensemble methods. The flowchart of the proposed\nexperiment, depicted in Figure 2, illustrates the entire process,\nfrom input data processing to the final output generated by the\nMoE."}, {"title": "V. EXPERIMENTS", "content": "In this section, we detail the experimental setup used to\nevaluate the performance of the ENACT-Heart. The overall\nworkflow of this process is summarized in Fig. 2, which\noutlines the key steps from data preparation to the final\nensemble prediction."}, {"title": "A. Dataset Used", "content": "The heart sound dataset provided in the PASCAL Classify- ing Heart Sounds Challenge was used for training and testing the models [12]. The audio files in the dataset vary in length, ranging from 1 second to 30 seconds."}, {"title": "B. Data Preprocessing", "content": "Creating Dataframe. The PASCAL dataset consists of two main folders, each containing labeled audio files. These folders were combined into a single dataframe, which includes the file paths and corresponding labels. An exploratory data analysis (EDA) was performed to understand the distribution of classes and identify any potential issues such as missing or mislabeled data.\nNormalizing. To address the inconsistent length of audio recordings, a preprocessing step was implemented. Each audio file was segmented into 5-second clips. If an audio file was shorter than 5 seconds, it was zero-padded to meet the required input length.\nData Augmentation. To increase the dataset size and introduce variability, Gaussian noise was added to the audio files, generating 9 additional noisy versions for each original file. Gaussian noise with a mean of 0 and a standard deviation of 0.1 was added to the audio files to generate augmented data. Each audio file was segmented into 5-second clips to standardize the input length for model training. This resulted in a total of 10 versions per audio file (1 original + 9 augmented). The augmented audio data was included in the dataframe, and corresponding labels were updated to reflect the augmentation process."}, {"title": "C. Audiovisual Image Data Generation", "content": "Two types of visual representations were generated for each audio file: spectrograms and centroid graphs.\nSpectrogram. Spectrograms were generated to visualize the frequency content of the audio files over time. Given the presence of various background noises in real-world condi- tions, a low-pass filter set at 195 Hz was applied. This filter helps to emphasize the cardiac sounds, which predominantly occur in the lower frequency range, while reducing noise from"}, {"title": "VI. RESULTS", "content": "In this study, we evaluated the performance of two state- of-the-art models, ViT and CNN, on a dataset of heart sound recordings. The goal was to classify the recordings into five categories: artifact, extrahls, extrastole, murmur, and normal.\nViT. Although the ViT model demonstrated strong per- formance across most classes, it was outperformed by the CNN in several key areas. This discrepancy is primarily due to the repetitive nature of heart sounds, which consist of recurring local patterns that CNNs are particularly adept at capturing and analyzing. Consequently, CNN's ability to effectively recognize these local patterns contributed to its superior performance in this context. This, however, doesn't mean the usage of ViT model is futile, as ViT might have identified characteristics that is not evident through the CNN model.\nCNN. The CNN model, on the other hand, achieved higher overall precision and demonstrated more balanced perfor- mance across all classes. This proves the point mentioned earlier: CNN on centroids can provide a more robust model in comparison to ViT."}, {"title": "B. Ensemble Model Performance", "content": "To leverage the strengths of both models, we implemented an ensemble method by combining the predictions of the ViT and CNN models using a Mixture of Experts approach. The ensemble was created using an additive weighted approach, where different weights were tested to find the optimal com- bination.\nENACT-Heart achieved the highest accuracy of 97.52%, significantly outperforming both individual models. The im- provement in accuracy demonstrates the effectiveness of the ensemble approach, particularly in enhancing the model's ro- bustness and generalization capabilities. The ensemble method effectively combined the strengths of both ViT and CNN. The effectiveness of the ENACT-Heart compared to the individual ViT and CNN models"}, {"title": "C. Comparison with State-of-the-Art", "content": "The proposed ENACT-Heart demonstrates competitive per- formance when compared to other state-of-the-art models in heart sound classification. Utilizing a MoE ensemble approach that integrates ViT and CNN, the ENACT-Heart achieves an impressive accuracy of 97.52%. This surpasses the accuracy of individual ViT (93.88%) and CNN (95.45%) models. Ad- ditionally, the ENACT-Heart maintains high precision (0.98), recall (0.97), and F1-score (0.98), indicating a balanced per- formance across various metrics.\nIn comparison, other notable studies in the field exhibit slightly different accuracies. For instance, the work by Liu et al. [7] utilizing bispectrum features and ViT reported an accuracy of 91%, while Yang et al. [13] achieved an accuracy"}, {"title": "VII. CONCLUSION AND DISCUSSION", "content": "In conclusion, the combination of ViT and CNN models using an ensemble method improved the classification perfor- mance in every aspect in general. This study highlights the importance of evaluating individual models to identify their strengths and the potential benefits of using ensemble methods to achieve superior results.\nThe data augmentation techniques employed also played a key role in enhancing model robustness and performance. These findings can inform future research and development of advanced classification systems in the medical field.\nThe proposed ENACT-Heart model demonstrates significant promise in the field of heart sound classification, particularly when considered alongside the advancements in smart wear- able devices. With the proliferation of wearable technology, the collection and analysis of audio data have become more accessible and widespread. This is especially pertinent in the medical field, where heart sound data can be continuously monitored and analyzed in real-time, offering invaluable in- sights into a patient's cardiovascular health.\nMoreover, from a practical point of view, the advancement of smart wearable devices presents a significant opportunity for improving healthcare accessibility, especially in low-resource settings. In many developing countries, access to advanced medical diagnostics is limited due to the lack of infrastruc- ture and trained healthcare professionals. Wearable devices equipped with advanced models like ENACT-Heart can bridge this gap by enabling non-invasive, continuous monitoring of heart health, thus providing timely and accurate diagnostics without the need for expensive and bulky equipment.\nThis technology can revolutionize the practice of medicine in poorer regions, making high-quality healthcare more ap- proachable and affordable. The ability to monitor and ana- lyze heart sounds continuously can lead to early detection of cardiovascular issues, prompt intervention, and ultimately, better health outcomes. As wearable devices become more affordable and their usage more prevalent, the integration of sophisticated models like ENACT-Heart can play a crucial role in democratizing access to advanced medical diagnostics globally.\nIn summary, the synergy between the ENACT-Heart model and smart wearable technology holds great potential for en- hancing healthcare delivery, particularly in underserved re- gions. By providing a reliable and efficient means of heart sound classification, this approach not only advances the field of medical diagnostics but also contributes to the broader goal of equitable healthcare access."}]}