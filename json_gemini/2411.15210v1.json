{"title": "Towards Million-Scale Adversarial Robustness Evaluation With Stronger Individual Attacks", "authors": ["Yong Xie", "Weijie Zheng", "Hanxun Huang", "Guangnan Ye", "Xingjun Ma"], "abstract": "As deep learning models are increasingly deployed in safety- critical applications, evaluating their vulnerabilities to adversarial perturbations is essential for ensuring their reliability and trustworthiness. Over the past decade, a large number of white-box adversarial robustness evaluation methods (i.e., attacks) have been proposed, ranging from single- step to multi-step methods and from individual to ensem- ble methods. Despite these advances, challenges remain in conducting meaningful and comprehensive robustness eval- uations, particularly when it comes to large-scale testing and ensuring evaluations reflect real-world adversarial risks. In this work, we focus on image classification models and propose a novel individual attack method, Probability Mar- gin Attack (PMA), which defines the adversarial margin in the probability space rather than the logits space. We analyze the relationship between PMA and existing cross-entropy or logits-margin-based attacks, and show that PMA can outperform the current state-of-the-art individual meth- ods. Building on PMA, we propose two types of ensemble attacks that balance effectiveness and efficiency. Further- more, we create a million-scale dataset, CC1M, derived from the existing CC3M dataset, and use it to conduct the first million-scale white-box adversarial robustness evalua- tion of adversarially-trained ImageNet models. Our findings provide valuable insights into the robustness gaps between individual versus ensemble attacks and small-scale versus million-scale evaluations.", "sections": [{"title": "1. Introduction", "content": "Despite their success in various applications, deep neural net- works (DNNs) exhibit significant sensitivity to imperceptible perturbations specifically designed to maximize prediction error, a phenomenon known as \"adversarial vulnerability\" [1]. This vulnerability presents a critical safety risk, as ad- versaries can exploit it to launch stealthy attacks that evade human detection, potentially undermining the reliability of real-world systems. Consequently, evaluating the adversarial robustness of DNNs has become essential.\nWhite-box adversarial attack methods evaluate the worst- case performance of a DNN model by crafting attacks di- rectly using adversarial gradients. These attacks, commonly tested in image classification tasks, aim to maximize the model's prediction error while constraining perturbations to be \"small\" through an \\(L_p\\) norm [2, 3]. The Projected Gradient Descent (PGD) [4] and AutoAttack (AA) [5] are two commonly used for such evaluations. PGD is a strong first-order attack, while AA is an ensemble attack with four different methods. However, PGD struggles with obfuscated gradients [6], and AA is computationally expensive [7], high- lighting the trade-off between effectiveness and efficiency in white-box robustness evaluation.\nAs large-scale DNNs are increasingly deployed across various applications, large-scale adversarial robustness eval- uations have become essential. To conduct such evaluations, highly efficient attack methods are necessary. Although AA can be accelerated through adaptive initialization and dis- carding strategies [8], individual attacks hold a significant efficiency advantage in large-scale evaluation. Improving the standard PGD attack is a promising direction for obtaining stronger individual attacks. Existing improvements to PGD include better initialization [9], multiple adversarial targets [10], intermediate feature layers [11], or loss alternation [12], as well as more robust loss functions, such as margin loss [7], the difference of logits ratio (DLR) [5], and Minimize the Impact of Floating-Point Errors (MIFPE) loss [13]. A recent work proposed a Margin Decomposition (MD) attack to cir- cumvent obfuscated/imbalanced gradients using a two-stage margin decomposition strategy.\nIn this work, we build on the attack pipeline of the Margin Decomposition (MD) attack but introduce a novel loss func- tion, probability margin loss, to develop a stronger individual attack. Specifically, the probability margin loss defeines the margin in the probability space, rather than the logits space."}, {"title": "2. Related Work", "content": "White-box Adversarial Attacks A number of white-box adversarial attacks have been proposed to assess the robust- ness of deep neural networks (DNNs). Early white-box attacks include L-BFGS [1], Fast Gradient Sign Method (FGSM) [2], and Basic Iterative Method (BIM) [15]. These attacks utilize second-order optimization or single- and multi-step gradient sign updates to generate \\(L_\\infty\\)-bounded adver- sarial examples. Subsequently, optimization-based attacks, such as the Carlini-Wagner (C&W) attack [3], were intro- duced to jointly optimize both the misclassification objective and the perturbation constraint. However, both L-BFGS and C&W attacks are computationally expensive, while FGSM and BIM, being more efficient, are less effective in generat- ing strong adversarial examples. To strike a balance between attack strength and computational efficiency, the Projected Gradient Descent (PGD) attack [4] was introduced within the adversarial training framework [2, 4, 16]. PGD allows the perturbation to exceed the e-ball and uses a clipping op- eration to ensure that the perturbation remains within the constraint when necessary. Additionally, PGD employs a random initialization strategy to help escape local minima, thereby enhancing its attack strength.\nReliable Adversarial Robustness Evaluation The accu- racy of robustness evaluations relies heavily on the strength of the attack. White-box attacks used for robustness evalua- tion must be resilient to issues such as obfuscated gradients [6] and imbalanced gradients [7]. As shown in [6], stan- dard PGD can overestimate robustness in the presence of obfuscated gradients, leading to the development of methods aimed at improving PGD's effectiveness. These improve- ments can be broadly classified into two categories: 1) attack strategies and 2) loss functions. Improved attack strategies include enhanced initialization [9], adaptive boundaries [17], multiple target optimization [10], and random restarts with step size scheduling [5]. Further strategies such as utiliz- ing intermediate logits [11], alternating objectives [12], and margin decomposition [7] also contribute to increased ro- bustness. In terms of loss functions, the field has progressed from using cross-entropy to margin loss [3, 7], boundary distance loss [17], DLR [5], and MIFPE loss [13]. Among these, margin loss has shown notable resilience to gradient- related issues, as demonstrated by the Margin Decomposi- tion (MD) attack [7]. The AutoAttack (AA) framework [5] integrates four advanced attack methods-two Auto-PGD (APGD) attacks, a FAB attack [10], and a black-box Square attack [18]-to provide a strong, ensemble-based approach for robust evaluation.\nHowever, the extensive runtime of AA often exceeds model training times, making robustness evaluation imprac- tical in many cases. This has led to the development of new attack methods aimed at faster and more efficient eval- uations. For instance, the LAFEAT attack [11] enhances PGD by leveraging intermediate feature layers, while Adap- tive AA (A3) [8] accelerates AA with an adaptive direction initialization strategy. Additionally, the Auto Conjugate Gradient (ACG) attack [19] applies the Conjugate Gradient (CG) method to improve adversarial optimization. Recently, the Margin Decomposition (MD) attack [7] introduced a two-phase approach that integrates margin loss to address gradient issues and strengthen attacks. In this work, we build upon the MD attack by proposing a simpler but more effec- tive loss function that surpasses existing individual attacks."}, {"title": "3. Proposed Attack", "content": "Preliminary In this work, we focus on adversarial attacks on image classification models. Given a target model \\(f\\), the goal of an adversarial attack is to find an adversarial pertur- bation \\(\\delta^*\\) constrained within a small \\(\\epsilon\\)-ball to maximize the model's classification error. Formally, it is:\n\\[\\delta^* = \\arg \\max_{\\|\\delta\\| \\le \\epsilon} L(f(x + \\delta), y),\\]\nwhere \\(L\\) denotes the classification loss, \\(y\\) is the correct label of \\(x\\), and \\(||\\cdot||\\) is the \\(L_\\infty\\) norm. The generated adversarial example is denoted by \\(x_{adv} = x + \\delta^*\\)."}, {"title": "3.1. Probability Margin Loss", "content": "Let \\(z\\) be the logit output of \\(f(x)\\) and \\(z_i\\) be the logits output of the \\(i\\)-th class, and \\(p_i = \\frac{e^{z_i}}{\\sum_{j=1}^N e^{z_j}}\\) be the probability output of the \\(i\\)-th class for a total number of \\(N\\) classes. Sorting the values of \\(z_i\\) in ascending order, \\(z_{\\pi_i}\\) represents the \\(i\\)-th largest logit value (except \\(z_y\\)). Our proposed probability margin loss is defined as:\n\\[L_{pm} (z, y) = P_{max} - P_y = \\frac{e^{z_{max}}}{\\sum_{i=1}^N e^{z_i}} - \\frac{e^{z_y}}{\\sum_{i=1}^N e^{z_i}},\\]\nwhere \\(p_{max} = \\max_{i \\neq y} p_i\\). Compared to the classic margin loss defined on logits (see Table 1), the logit values \\(z_{max/y}\\) are substituted by the probability \\(P_{max/y}\\) in \\(L_{pm}\\). Intuitively, probability-based loss functions take all logit values into consideration at their denominator, which opens up more attack possibilities toward different logit directions.\nRelationship to Existing Adversarial Losses We summa- rize the formulas and adversarial gradients of five adversarial loss functions: 1) untargeted cross-entropy (CE), 2) targeted cross-entropy, 3) DLR [5], 4) margin loss [3], and our 5) probability margin loss. The two targeted and untargeted CE use probabilities to compute the loss, while margin loss only considers the logits of two classes, and DLR exploits the logits of three classes. The DLR loss is more closely related to the margin loss but explores different targets than only the \\(z_{max}\\). Our PM loss is also defined on probabilities which is similar to the two CE losses in this sense, but only on two classes which is similar to the margin loss. By incorporating different logits, our PM loss is also similar to DLR but in the exponential space. Another interesting observation is that the margin loss is equivalent to the sum of the targeted CE loss and the untargeted CE loss. This interesting ob- servation implies that the effective margin loss verified in recent work [7] can be obtained by combining the CE losses. Next, we will show from the gradient perspective that our PM loss is a weighted combination of the targeted and untargeted CE losses and thus is even more effective than the margin loss.\nFrom the gradient formulas, it can be inferred that during the optimization process of untargeted CE, the value of \\(z_y\\) decreases while the values of \\(z_{i \\neq y}\\) increase, with weights depending on the probability of the corresponding class. This provides multiple weighted adversarial directions for exploration but may have a problem in finding the optimal direction, as it is hard to concentrate on one particular direc- tion. In contrast, targeted CE concentrates all exploration toward \\(z_{max}\\) while suppressing all other possibilities via its first term. This resolves the previous issue of untargeted CE but hinders the exploration of alternative adversarial di- rections. Margin loss increases \\(z_{max}\\) while decreasing \\(z_y\\), following the maximum adversarial direction, which is sim- ilar to targeted CE but without the probabilistic weighting."}, {"title": "3.2. Probability Margin Attack (PMA)", "content": "Our proposed PMA attack adopts the PM loss as the adver- sarial objective and follows the two-stage attack pipeline of the MD attack. The reason why we closely follow the MD pipeline as it already combines previous tricks such as multi-targeted attack, objective alternation, multi-stage exploration, random restart, and margin decomposition, into an integrated pipeline. By simply replacing the adversarial objective, we can easily improve the attack effectiveness with no additional costs.\nThe complete attack process of PMA involves alternating the attack objective between the two margin terms of the PM loss. The extract loss term used in each stage of PMA is defined as follows:\n\\[x_{k+1} = \\Pi_{\\epsilon} (x_k + \\alpha \\cdot \\text{sign}(\\nabla_x L_k(x_k, y))\\]\n\\[L_k (x_k, y) = \\begin{cases}P_{max} & \\text{if } k < K^1 \\text{ and } r \\% 2 = 0\\\\-P_y & \\text{if } k < K^1 \\text{ and } r \\% 2 = 1\\\\P_{max} - P_y & \\text{if } K^1 < k < K.\\end{cases}\\]\nIn the above equations, \\([\\Pi\\) denotes the projection operation, ensuring that the perturbations are within the \\(\\epsilon\\)-ball centered at \\(x\\). The attack step is represented by \\(k \\in \\{1, ..., K\\}\\), while \\(k < K^1\\) denotes the attack stage 1 (\\(K^1 \\in [1, K]\\)). \\(r \\in \\{1,..., n\\}\\) represents the \\(r\\)-th restart, which is used to calculate which loss term to use via the modulo operation (\\(%\\)). The loss function \\(L^r\\) switches from the individual"}, {"title": "4. Experiments", "content": "In this section, we first describe our experiment setup and then present the results for individual evaluation, ensemble evaluation, and large-scale evaluation, respectively."}, {"title": "4.1. Experimental Setup", "content": "Datasets and Models Following previous works, we use CIFAR-10, CIFAR-100, and ImageNet-1k as the evaluation datasets. We choose adversarially trained models from the RobustBench leaderboard [20] as our target models, which are the most robust models to date. Specifically, we se- lect the top 10 models from the CIFAR-10 leaderboard, the top 8 models from the CIFAR-100 leaderboard, and the top 11 models from the ImageNet leaderboard. These total of 29 models cover a variety of architectures, includ- ing ResNet[21], WideResNet[22], ViT[23], XCiT[24], Swin Transformer[25], and ConvNeXt[26].\nAttack Setting and Baselines Following previous works [5, 20], we focus on \\(L_\\infty\\) norm adversarial attack and set the perturbation budget to \\(\\epsilon = 8/255\\) for CIFAR-10 and CIFAR-100, \\(\\epsilon = 4/255\\) for ImageNet-1k. For untargeted attacks, the number of attack steps is set to 100, while for targeted (or multi-targeted) attacks, the step for each target is set to 100. For untargeted attacks, we compare our PM loss to several existing loss functions, including cross-entropy (CE), DLR [5], margin loss [3], mixed loss function [12], and MIFPE[13], combined with three attack strategies: the classic PGD strategy [4], the APGD strategy[5], and the Margin Decomposition strategy[7]. Notably, for the Margin Decomposition strategy, we set the hyperparameter K' to 25 and the number of restarts to 1. The effects of varying K' values and the number of restarts n are further analyzed in our ablation study, which is detailed in Appendix B. FAB [17] is also evaluated. For targeted attacks, we evaluate the DLR, margin loss, and our probability margin loss with APGDT[5] attack strategy. This evaluation includes a total of 9 targets, with each target subjected to 100 attack steps."}, {"title": "4.2. Main Results", "content": "Effectiveness of the PM Loss We first fix the attack strat- egy to that of PGD and APGD, and then compare our pro- posed PM loss with other existing loss functions. As shown in Table 2, our PM loss outperforms CE, DLR, and margin losses across all evaluated defense models. The robustness evaluated using the PM loss is about 0.03% to 1.56% lower than that evaluated using the existing loss functions. The ad- vantage of the PM loss is more pronounced on datasets with more classes. Particularly, on ImageNet-1k, our PM loss achieves an average robustness reduction of 1.22%, demon- strating its potential for large-scale evaluation. Given the PM loss's composition of \\(p_{max}\\) and \\(p_y\\) with variable weighting, we assessed its sensitivity to these weights via an ablation study, with results detailed in Appendix E.\nEffectiveness of PMA We then compare the effectiveness of our PMA with other existing attack methods. As shown in Table 3, the robustness obtained by our PMA is the lowest. Particularly, PMA reduces the robustness of different models evaluated by other attacks by up to 0.22%.\nCompared with the recent MD attack which has the same attack pipeline as our PMA, the model robustness tested by PMA is reduced by up to 0.55%. These results confirm that our PMA is the strongest individual attack in the current literature. In the last column of Table 3, we list the robustness evaluated by an ensemble attack: Auto Attack(AA). It can be observed that the results of PMA are very close to that of AA, which requires 4900 attack steps to achieve this strength. Our attack even surpasses the AA on three models"}, {"title": "4.3. PMA Ensemble", "content": "Here, we explore ensemble attacks with PMA. Specifically, we explore two ensemble strategies: 1) PMA+1 ensemble that combines our PMA with one more other attack, and 2) cascade ensemble that incorporates the remaining best attack in a sequence order until all popular individual attacks are included. As shown in Table 4, the PMA+1 ensem- ble always leads to better attacks that further reduce model robustness. Among all the combinations, PMA + APGDT (multi-targeted APGD attack) produces the best performance, which reduces model robustness by 0.52% in the best case. This implies that PMA and AGPDT may explore the vulner- abilities of a model in the most distinct directions.\nThe results of the cascade ensemble are reported in Table 5. As can be seen, when more attacks are appended to the en- semble, it keeps reducing the models' robustness to a lower level than that measured by AA, leading to more and more accurate robustness evaluations. The final \"diff\" column indicates the best result one can obtain with current attacks. The maximum robustness difference compared to AA was observed for the top-3 defense model [29] which is 0.99%. Arguably, there exists a trade-off between effectiveness and efficiency. Our results indicate that the robustness difference is within 1% for the top-ranked CIFRA-10 defense models when applying an ensemble of 17 attacks vs. 1 single attack to evaluate the robustness. As such, we recommend using the PMA+1 ensemble, PMA+APGDT (denoted as PMA+) to be more specific, to evaluate future defenses. A performance"}, {"title": "4.4. Million-Scale Robustness Evaluation", "content": "Here, we aim to scale adversarial robustness evaluation up to the million-scale for adversarially trained models on Ima- geNet. To this end, we first construct a large-scale evaluation dataset named CC1M with 1 million images selected from the CC3M [14] dataset, which is an (image, caption) paired dataset comprising diverse objects, scenes, and visual con- cepts. We first remove the \"unavailable\" images, i.e., images showing 'this image is unavailable', due to a large number of expired image URLs. Next, we remove the noisy images that do not contain any semantic content, such as random icons. Finally, we apply the Local Intrinsic Dimensional- ity (LID) [40] to remove anomaly images. The LID is a representation-space metric that has been shown to be able to detect adversarial images [41], backdoor images [42], or low-quality images that are detrimental to self-supervised contrastive learning [43]. Images with deviated LID scores from the average are often viewed as outliers that are geo-"}, {"title": "5. Limitation", "content": "While PMA outperforms all individual attacks and the PMA+ ensemble achieves state-of-the-art effectiveness, it still has some limitations. Although PMA+ is significantly more efficient than AutoAttack (AA), it still requires considerable time for large-scale evaluations. Additionally, our approach is currently limited to white-box settings, posing challenges for extending its applicability to black-box attack scenarios."}, {"title": "6. Conclusion", "content": "In this paper, we studied the problem of white-box robust- ness evaluation and introduced a novel individual attack, Probability Margin Attack (PMA), which is guided by the newly proposed Probability Margin (PM) loss. We analyzed the relationship between PM loss and several widely used loss functions, including cross-entropy loss (both targeted and untargeted), difference of logits ratio (DLR), and margin loss. Through empirical evaluation, we demonstrated the su- perior performance of the PM loss and the PMA attack, high- lighting their effectiveness. Additionally, we explored the potential for developing an ensemble attack that outperforms AutoAttack (AA) in both effectiveness and speed. Finally, we conducted a million-scale white-box adversarial robust- ness evaluation on adversarially trained ImageNet models, revealing a significant robustness gap between small-scale and large-scale evaluations."}]}