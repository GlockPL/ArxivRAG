{"title": "Coarse-to-Fine Structure-Aware Artistic Style Transfer", "authors": ["Kunxiao Liu", "Guowu Yuan", "Hao Wu", "Wenhua Qian"], "abstract": "Artistic style transfer aims to use a style image and a content image to synthesize a target image that retains the same artistic expression as the style image while preserving the basic content of the content image. Many recently proposed style transfer methods have a common problem; that is, they simply transfer the texture and color of the style image to the global structure of the content image. As a result, the content image has a local structure that is not similar to the local structure of the style image. In this paper, we present an effective method that can be used to transfer style patterns while fusing the local style structure into the local content structure. In our method, different levels of coarse stylized features are first reconstructed at low resolution using a Coarse Network, in which style color distribution is roughly transferred, and the content structure is combined with the style structure. Then, the reconstructed features and the content features are adopted to synthesize high-quality structure-aware stylized images with high resolution using a Fine Network with three structural selective fusion (SSF) modules. The effectiveness of our method is demonstrated through the generation of appealing high-quality stylization results and a comparison with some state-of-the-art style transfer methods.", "sections": [{"title": "1. Introduction", "content": "Artistic style transfer is an attractive image processing technique that is used to generate a new image that pre- serves the structure of a content image but carries the pattern of a style image. Recently, the seminal im- age-optimization method proposed by Gatys et al. [1] was used to achieve style transfer by adopting the correlation of features extracted from a pretrained deep neural network and the iterative optimization process. Like the method presented by Gatys et al. [1], style transfer by relaxed optimal transport and self-similarity (STROTSS) [2] is also an image-optimization style transfer method; this method has achieved superior stylization results by adopting the re- laxed Earth mover's distance (rEMD) loss in a multiscale optimization process. However, the expensive computational cost of these image-optimization methods restricts their use in practice applications in industry. To speed up the op- timization procedure, Johnson et al. [3] and Ulyanov et al. [4] proposed model-optimization style transfer methods. They train a feed-forward neural network that can be used to synthesize images with a single given style image in real time. Both adaptive instance normalization (AdaIN) [5] and whitening and coloring transforms (WCT) [6] are mod- el-optimization methods but are also arbitrary style transfer methods, in which style patterns of arbitrary style images are transferred by adopting some feature transforms. After reviewing these methods, we have found that although local style texture and content structures can generally be combined, some key structures of the style image are not accurately learned. For example, the color blocks and brushstrokes that constitute the main objects in style images are not transferred very well. Meanwhile, in some cases, these methods produce distorted objects and incongruous artistic effects in stylized images. Therefore, our main task is to transfer the local structure of the style image to the content image and adopt a coarse-to-fine strategy to enhance the artistic details of the stylization results.\nWe propose a novel artistic style transfer network for fusing an essential style structure into a content structure and synthesizing a structure-aware stylized image. In our model, a Coarse Network is designed to obtain recon- structed coarse stylized features in the first stage. Because the Coarse Network only works at a low resolution, the coarse stylized features can discard trivial structure details of the content image and combine the global content structure with the style patterns. Then, the task of a Fine Network is to adopt these reconstructed coarse stylized fea- tures obtained at a low resolution and the original content image with a high resolution to synthesize the final high-resolution stylized image in the second stage. By adopting some SSF modules to fuse the coarse stylized features"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Style Transfer", "content": "The goal of style transfer is to combine the texture of a style image with the structure of a content image. Gatys et al. [1] proposed a seminal iterative method based on a pretrained visual geometry group (VGG) network [7]. In this method, the content structure and the style texture can be used to synthesize a new image, but it is expensive and a stylized image is only generated after the training process has been completed. Inspired by Gatys et al. [1], Johnson et al. [3] proposed a feed-forward method, which can be used to synthesize arbitrary images with a fixed style by an en- coder-decoder architecture; the time and computation costs are reduced when using this method. Numerous methods have been developed to speed up the style transfer process [4, 8] and improve the visual quality [9, 10, 11]. Sanakoyeu et al. [12] also improved the stylization quality by proposing a style-aware loss, but they trained a network with a set of style images instead of a style image. This approach aimed to combine many style images created by one artist to synthesize a stylized image with the overall style of this artist. Dual style generative adversarial network (DualStyle- GAN) [13] is proposed to characterize the content and style of a portrait by retaining an intrinsic style path to control the style of the original domain and an extrinsic path to model the style of the target extended domain. Peking Opera face makeup (POFMakeup) [14] also is a portrait style transfer method that can transfer the style of a portrait with a Peking Opera face to a target portrait. Lin et al. [15] combined a universal style transfer method with image fusion and color enhancement methods to solve the problems of the color scheme, the strength of style strokes and the adjustment of image contrast.\nTo simultaneously handle multiple styles, [16] proposed a flexible conditional instance normalization approach embedded in style transfer networks to learn multiple styles, and [17] achieved multistyle generation in a generative network architecture with a learnable inspiration layer. Ye et al. [18] adopt mechanism and instance segmentation to achieve a regional multistyle style transfer model which can solve the problem of unnatural connections between re- gions. Alexandru et al. [19] combined various existing style transfer frameworks to propose a novel framework that can generate intriguing artistic stylization results by performing geometric deformation and using different styles from multiple artists.\nIn AdaIN [5], adaptive instance normalization is implemented to train a network with various styles, providing the ability to transfer arbitrary styles after the training process. In WCT [6], the whitening and coloring transforms are adopted to synthesize arbitrary styles with a pretrained VGG network and a series of pretrained image restructuring decoders. Based on WCT, Wang et al. [20] achieved the diversity of style transfer by adopting a deep feature pertur- bation (DFP) operation while preserving the quality of stylization results, and Wang et al. [21] synthesized ul- tra-resolution stylized images and reduced the convolutional filters by a knowledge distillation method. Style-attentional network (SANet) [22] is also an arbitrary style transfer method that can be used to efficiently generate stylized images by injecting local style patterns into content features based on the style attention mechanism."}, {"title": "2.2. Style Transfer Based on Multiscale Learning", "content": "Recently some style transfer methods have been used to transfer style patterns based on multiscale learning. Multiscale holistic style transfer is achieved in Avatar-Net [23] based on the use of an hourglass with multiple skip connections and a style decorator. STROTSS [2] is an image-optimization method that adopts multiscale learning to update the content image and generate high-quality stylized images. Yang et al. [24] proposed a novel video style transfer framework that can render high-quality artistic portraits based on the multiscale content features and preserve the frame details. A Laplacian pyramid style network (LapStyle) [25] also exhibits high visual quality and is based on a Drafting Network and a Revision Network. First, the former transfers the global style patterns, and then, the latter enhances local style details. However, too many content structure details are preserved in these methods. Key local style structures are not fused into stylized images in any of these methods. In contrast, our method transfers global style patterns at low resolution using a Coarse Network, which only needs to be trained once to reconstruct coarse stylized features. Our Fine Network enhances local style details with multiscale features from the Coarse Network and the high-resolution content image. As a result, our method can discard trivial local content structures and synthesize high-quality structure-aware stylized images by a coarse-to-fine process."}, {"title": "3. Proposed Method", "content": null}, {"title": "3.1. Framework Overview", "content": "Inspired by the painting process of artists, in which the coarse structure and color distribution are first con- structed and then fine details are added, our framework employs a Coarse Network and Fine Network to simulate the coarse-to-fine process. As shown in Figure 1, given a content image $x \\in R^{3 \\times h \\times w}$ and a style image $x_s \\in R^{3 \\times h \\times w}$, our model eventually generates a stylized image $x_{cs} \\in R^{3 \\times h \\times w}$. In the first stage, the Coarse Network takes $\\overline{x_c}$ and $\\overline{x_s}$ as inputs, where $\\overline{x_c}$ and $\\overline{x_s}$ are the results of downsampling $x_c$ and $x_s$ by 2, respectively. Then three different re- structured coarse stylized features $F_{cs}^{(i)} \\in R^{c_i \\times h_i \\times w_i}$ ($i = 1, 2, 3$) are generated by the Coarse Network, where $c_i$, $h_i$, and $w_i$ are the number of channels, height, and width of the $i$ th restructured feature, respectively. In the second stage, the Fine Network takes $x_c$ and $F_{cs}^{(i)}$ as inputs and then generates the final stylized image $x_{cs}$ by adopting SSF modules for feature fusion."}, {"title": "3.2 Coarse Network", "content": "One problem with recent style transfer methods is that too many structural details of the content image are re- tained during the transfer of style patterns. In the stylized image, there are some small structures from the content"}, {"title": "3.2.1 WCT Module", "content": "Inspired by WCT [6], our Coarse Network adopts whitening and coloring transforms to transfer coarse style pat- terns at low resolution. The whitening transform can remove inessential information related to style while preserving the global structure of the content. Then, the coloring transform can capture the salient visual style and fuse some style structures into content structures. WCT is a multilevel stylization process that uses different rectified linear unit (ReLU) layers of VGG features $ReLU\\_X\\_1$ ($X = 1, 2, ..., 5$) and transfers style patterns in a coarse-to-fine pipeline. The higher layer features are adopted to capture complex local structures, while lower layer features carry low-level color and texture information. The difference between our Coarse Network and WCT is that we only use a single-level whitening and coloring transform for stylization. Moreover, we do not directly reconstruct the stylized features to generate an image; however, we utilize the reconstructed features at different layers during reconstruction. As a result, our Coarse Network, which has the ability to capture the multilevel information by reconstructing the coarse stylized features at different levels, can save computing resources."}, {"title": "3.2.2 Architecture of Coarse Network", "content": "The architecture of Coarse Network, which is shown in Figure 1, includes an encoder, a WCT module, and a de- coder. (1) The encoder is a pretrained VGG-19 network, which is fixed during training. Given $X$ and $X_s$, the VGG encoder extracts the content feature $f_c$ and the style feature $f_s$ at $ReLU\\_4\\_1$. (2) Then, we apply a WCT module for whitening and coloring transformation. As shown in Figure 4(a), the whitening transform is adopted to linearly transform $f_c$ to obtain $f_c^\\prime$. Next, the coloring transform is carried out to obtain $f_{cs}$ by using $f_c^\\prime$ and $f_s$. (3) Finally, we adopt a reconstruction decoder to reconstruct the coarse stylized feature $f_{cs}$. The decoder is designed to be sym- metrical to the VGG-19 network, with the nearest neighbor upsampling layer used for enlarging the feature map. We take $f_{cs}$ as input for reconstruction and then generate these restructured stylized features $F_{cs}^{(i)}$ as outputs. In this reconstruction decoder, these outputs are output before the second upsampling layer, the third upsampling layer, and after the last convolution layer. These $F_{cs}^{(i)}$ will become a part of the input of the Fine Network."}, {"title": "3.3 Fine Network", "content": "The Fine Network aims to synthesize high-resolution stylized images by fusing the reconstructed coarse stylized features into the reconstructed content features. The reconstructed content features are from the high-resolution con- tent image, and contains the global semantic information and local detail information. Contrary to the reconstructed content features at high resolution, the reconstructed stylized features generated from the Coarse Network preserve only the main content structure while blending some local structural style information. By fusing multiscale infor- mation, the Fine Network can pay more attention to the holistic structure of the content and ignore some trivial details based on our SSF modules. Then, some significant details can be added to the structure and appealing artistic effects in the stylized image can be enhanced. In addition, fusing the reconstructed coarse stylized information can greatly reduce the time cost of the training process of the Fine Network, and the desired stylization results can be achieved at an earlier point in time."}, {"title": "3.3.1 SSF Module", "content": "The structural selective fusion (SSF) module is designed to fuse the reconstructed coarse stylized features from the Coarse Network into the reconstructed content features in the decoder of the Fine Network. Inspired by the atten- tion mechanism [27, 28], we employ a weight matrix to select key structural information of the reconstructed content features, which is learned by adopting the merged features. The merged features are obtained by concatenating re- constructed coarse stylized features and the reconstructed content features. The matrix can help the SSF module ob- tain the selective features that focus on meaningful structural information, and the selective feature is one part of the output of the SSF module. Another part of the output is the refined merged features that include different scale in- formation such as some crucial local textures or global structures.\nThe architecture of the SSF module is shown in Figure 4(b). First, we concatenate the reconstructed coarse styl- ized features $f_{cs}^{(i)}$ and the reconstructed content features $f_c$ as input $f_{ssf} \\in R^{(c_i+c)\\times h \\times w}$. The reconstructed content fea- tures $f_c$ are the output of the convolution layer in the decoder of the Fine Network (except that the first SSF module uses the content features $f_c$ from the encoder of the Fine Network as $f_c$). We adopt an average-pooling operation to aggregate the spatial information of $f_{ssf}$ to generate the input of the multilayer perceptron, which is adopted to pro- duce an attention map $M \\in R^{c_i \\times 1 \\times 1}$ as the weight matrix. In summary, the attention map is calculated as:\n$$M(f_{ssf}) = \\sigma(MLP(AvgPool(f_{ssf})))$$\nwhere $\\sigma$ denotes the sigmoid function. Then the selective feature $f_{csr}^{(i)}$ is calculated as:\n$$f_{csr}^{(i)} = M(f_{ssf}) \\cdot f_{cs}^{(i)}$$"}, {"title": "3.3.2 Architecture of Fine Network", "content": "where $\\cdot$ denotes elementwise multiplication. Meanwhile, $f_{csr}^{(i)}$ is fed into a convolutional layer to produce a refined merged feature $f_m \\in R^{c \\times h \\times w}$. Eventually, the SSF module generates the final output $F_{ssf}^{(i)} \\in R^{(c_i+c) \\times h \\times w}$ as the fused fea- ture by directly concatenating $f_{csr}^{(i)}$ and $f_m$.\nAs shown in Figure 1, Fine Network is designed as a flexible encoder-decoder architecture, with an encoder, a se- ries of residual blocks, and a decoder. The encoder contains a convolutional layer with a stride of 1 and three convolu- tional layers with strides of 2, followed by several residual blocks. The decoder contains three upsampling layers, three convolutional layers with strides of 1, and three SSF modules. We use an SSF module before each upsampling layer. Given the content image $x_c$ as the input of Fine Network, the encoder and several residual blocks generate the content feature $f_c$. Then, SSF modules generate the fused features $F_{ssf}^{(i)}$ by taking $f_c$ and $f_{csr}^{(i)}$ as inputs, where $f_{csr}^{(i)}$ is the output of these convolution layers in the decoder (except the first SSF module, which takes $f_{csr}^{(i)}$ as $f_c$). These fused features $F_{ssf}^{(i)}$ are fed into an upsampling layer and a convolution layer. Finally, the decoder generates the final stylized image $x_{cs}$ after the last convolution layer."}, {"title": "3.4 Loss Function", "content": "Our Coarse Network only needs to train once, and it is fixed during the training of the Fine Network. Compared to WCT [6], we only train one reconstruction decoder network to reconstruct the coarse stylized feature. Our Coarse Network can reconstruct the stylized features at three different levels or directly generate a coarse stylized image by taking advantage of the reconstruction decoder. Following WCT, we adopt pixel reconstruction and perceptual loss [3] to train our decoder for image reconstruction,\n$$l_1 = ||I_c - I_{cs}|| + \\lambda ||\\Phi(I_c) - \\Phi(I_{cs})||$$\nwhere $I_c$ and $I_{cs}$ are the input image and output image, respectively, and $\\Phi$ is the VGG encoder that extracts fea- tures at ReLU_X_1 (X = 1, 2, 3, 4). In addition, $\\lambda$ is the weight to balance the two losses.\nThe Fine Network is optimized with content and style loss during training. As shown in Figure 5, we keep a sin- gle $x_c$ and a set of $x_s$ from a content dataset, then $x_{cs}$ is a stylized image generated by the Fine Network. For $x_c$, $x_s$ and $x_{cs}$, we can use a pretrained VGG-19 encoder to extract their features $F_c^{(t)} \\in R^{c \\times h \\times w}$, $F_s^{(t)} \\in R^{c \\times h \\times w}$, and $F_{cs}^{(t)} \\in R^{c \\times h \\times w}$, where $t$ denotes the features extracted at ReLU_t (t = 1_1, 1_2, 2_1, 2_2, 3_1, 3_3, 4_1)."}, {"title": "For content loss, we adopt the commonly used perceptual loss between $F_c^{(t)}$ and $F_{cs}^{(t)}$ proposed in [3]. The perceptual loss can measure high-level perceptual and semantic differences between images and is defined as:", "content": "$$l_c = \\frac{1}{c \\times h \\times w}||F_c^{(t)} - F_{cs}^{(t)}||_2^2$$\nFor style loss, we adopt three different style losses. The first and most significant style loss is the relaxed Earth mover's distance (rEMD) loss [2], which helps the Fine Network generate visual effects with minimum distortion to the layout of the content image. This loss plays a key role in migrating the structural forms of the style image to the target image. The rEMD loss between $F_s^{(t)}$ and $F_{cs}^{(t)}$ can be calculated as:\n$$l_s = max_{\\phi} \\left\\{ min_{k=1}^{hw} \\sum_{i=1}^{hw} C_{i,\\phi(i)} , min_{k=1}^{hw} \\sum_{j=1}^{hw} C_{\\phi(j),j} \\right\\}$$\nwhere $C$ is the cost matrix, which can be calculated as the cosine distance between $F_s^{(t)}$ and $F_{cs}^{(t)}$:\n$$C_{cs,j} = D_{cos}(F_s^{(t)},F_{cs}^{(t)}) = 1 - \\frac{F_s^{(t)} \\cdot F_{cs}^{(t)}}{||F_s^{(t)}|| \\cdot ||F_{cs}^{(t)}||}$$\nThe second style loss is the commonly used style reconstruction loss proposed by Gatys et al [1], which is the differ- ence between the Gram matrices of $F_s^{(t)}$ and $F_{cs}^{(t)}$:\n$$l_{gram} = ||G(F_s^{(t)}), G(F_{cs}^{(t)})||$$\nwhere $G$ denotes the calculation of the Gram matrix of the feature vectors. Finally, we use the mean-variance loss as the third style loss, which is similar to the style reconstruction loss. We can use this type of loss to reduce unnecessary visual effects in the stylized image and keep the magnitude of the stylized feature the same as that of the style feature:\n$$l_m = ||\\mu(F_{cs}^{(t)}) - \\mu(F_s^{(t)})|| + ||\\sigma(F_{cs}^{(t)}) - \\sigma(F_s^{(t)})||$$\nwhere $\\mu$ and $\\sigma$ denote the mean and covariance of the feature vectors, respectively.\nThe overall optimization objective is defined as:\n$$L = \\alpha l_c + \\lambda_1 l_s + \\lambda_2 l_{gram} + \\lambda_3 l_m$$"}, {"title": "where $\\alpha$, $\\lambda_1$, $\\lambda_2$ and $\\lambda_3$ are weight terms. By adjusting $\\alpha$, we can control the degree of stylization. Specifically, $l_c$, $l_s$ and $l_{gram}$ both work on ReLU_1_1, ReLU_2_1, ReLU_3_1 and ReLU_4_1; then, $l_m$ works on ReLU_2_1, ReLU_3_1 and ReLU_4_1. Following Johnson et al. [3], $l_{gram}$ works on ReLU_1_2, ReLU_2_2 and ReLU_3_3.", "content": null}, {"title": "4. Experimental Results and Analysis", "content": null}, {"title": "4.1. Experimental Dataset and Implementation Details", "content": "During training, we use the MS-COCO [29] dataset as the set of content images and select some famous art paintings as style images. To show the experimental results of our method, we also select some copyright-free images as content images from Pexels.com.\nIn our experiment, the Coarse Network is trained on the MS-COCO dataset only once for image reconstruction, and the weight $\\lambda$ in Equation (1) is set as 1. In the experiments, we use the content images and the style image with a resolution of 512 \u00d7 512. Then these images are downsampled by 2. The images input into the Coarse Network has a resolution of 256 \u00d7 256. During the training of the Fine Network, we use the Adam [30] optimizer with a learning rate of 1e-4, and the batch size is set as 1 because of the limitation of the graphics processing unit (GPU) memory. To train a style, a training process consists of 15,000 iterations. The loss weight terms, $\\alpha$, $\\lambda_1$, $\\lambda_2$ and $\\lambda_3$ are set to 1, 20, 1,000 and 5, respectively."}, {"title": "4.2. Qualitative Comparisons with Prior Works", "content": "Inspired by the recent WCT [6] and STROTSS [2] methods, our method adopts the whiting and coloring trans- formation proposed in WCT and the rEMD loss proposed in STROTSS. In Figure 6, we compare our method with WCT and STROTSS. WCT can transfer the color distribution and simple texture of arbitrary style images; however, some context local structure is discarded, resulting in messy and disordered stylized images (e.g., rows 1, 2 and 3). STROTSS is an image-optimization style transfer method that transfers the visual attributes from the style image to the content image with minimum semantic distortion. Nevertheless, too many structural details are preserved, and the overall palette of the style image is not accurately transferred (e.g., rows 2, 3). In contrast to these two methods, our method can transfer the main structure and discard some trivial details of the content image. Moreover, some notable local structures of the style image, such as brushstrokes, can be fused into the global structure of the content image, and the overall palette of the stylized image remains the same as that of the style image. For example, in the second and fourth rows, the color blocks of mountains and the brushstrokes of vegetation in our stylized images are explicitly similar to those in style images. Our model can learn some key style structures while ignoring some unimportant content details."}, {"title": "4.3. Quantitative Comparisons with Prior Works", "content": "In the experiment of quantitative comparisons, we use the learned perceptual image patch similarity (LPIPS) proposed in [31] and the structural similarity index measurement (SSIM) proposed in [32] to compute the difference in style structure between the stylized image and style image. In each method, 1,500 pairs of stylized and style images that include ten styles are used to compute the averaged distance. As shown in Table 3, lower values indicate the higher similarity of human perceptual judgments when we use LPIPS as the metric, and higher values indicate the higher structural similarity when we use SSIM as the metric. For both evaluation metrics, our proposed method achieves the highest similarity in style structure. The experimental results show that our method can synthesize structure-aware stylized images that have a higher structural similarity to the style images."}, {"title": "4.4. Comparisons of Time Efficiency with Prior Works", "content": "We further compare the time efficiency of our proposed method with other state-of-art methods. In each method, we synthesize 100 stylized images with a resolution of 512 \u00d7 512. All experiments are conducted on the same envi- ronment configuration. As shown in Table 4, Johnson et al. [3] achieve the highest time efficiency because they only use a simple encoder-decoder architecture to generate stylized images. Like [3], AdaIN [5] and SANet [22] also use the simple encoder-decoder network to generate stylized images. But they apply some feature transform modules in their networks to integrate content features and style features. As a result, their time efficiencies are lower than [3] but are still satisfactory. Different from these three methods that work at the same image scale, our model includes two dif- ferent networks and works in two stages. Although our model can capture richer multiscale information and synthe- size higher-quality stylized images, the time efficiency of our method is only slightly lower than that of AdaIN and SANet. We traded a small increase in time cost for a promising improvement in the quality of stylized images. WCT [6] has a low time efficiency because it uses five encoders and decoders to generate a stylized image. The time efficiencies of STROTSS [2] and Gatys et al. [1] are far lower than other methods because they are image-optimization methods that only generate one stylized image after a training process."}, {"title": "4.5. User Study", "content": "The user study is conducted on social media, and all participants are anonymous and voluntary. We choose 10 content images and 10 style images to synthesize 10 stylized images in each method and then ask subjects to select their favorite one. By the end of this user study, we had collected 341 votes from these anonymous participants. As shown in Figure 8, we show the percentage of votes for each method. The result shows that the stylization results ob- tained by our method are more appealing than those of other methods."}, {"title": "4.6. Ablation Study on Loss Function", "content": "We conduct ablation experiments to verify the effectiveness of each loss term used for training our model, and the results are shown in Figure 9. (1) Without perceptual loss $l_c$, too many structures of the content image are discarded; for example, the basic structure of the dog disappears in the stylized image. (2) Without gram matrix loss $l_{gram}$, the styl- ization result is acceptable because mean-variance loss $l_m$ has a similar effect to $l_{gram}$, but the color distribution of the stylized image is slightly different from that of the style image. Moreover, the textures of the dog in the stylized image are increasingly denser and smaller. (3) Without rEMD loss $l_s$, the texture distribution is chaotic, and some visual artifacts occur in the stylized image. (4) Without mean-variance loss $l_m$, the global color distribution of the stylized image is not exactly the same as that of the style image; for example, the dark color of the dog in the stylized image is more similar to that in the content image. This dark black color is completely absent in the style image."}, {"title": "4.7. Effectiveness of Coarse Network", "content": "During training, we compare our full model with the model without the Coarse Network. As shown in Figure 10, our full model is trained faster than the model without the Coarse Network. The preliminary stylization result can be obtained with fewer iterations. Moreover, the stylized images of the comparison during the training phase are shown in Figure 11. At 3,000 iterations, our full model can generate a stylized image with a basic structure, while the model without the Coarse Network generates a completely unstructured image. At 10,000 iterations, the stylization result of our full model is substantially acceptable. However, the stylized result of the model without the Coarse Network is less than satisfactory as the main structure has not been generated. At 30,000 iterations, the model without the Coarse Network finally synthesizes the final stylized image, but some messy textures and unnatural structures appear in the stylized image. Compared to this compromised stylized result, our full model can generate an enhanced promising stylized result with more refined details, such as the brushstrokes of the cat's fur and eyes at 30,000 iterations, which are more delicate and finer than those at 10,000 iterations."}, {"title": "4.8. Effectiveness of Fine Network", "content": "As shown in Figure 12, we demonstrate the effectiveness of the Fine Network. Without the Fine Network, the Coarse Network can transfer the color and texture of style images, but the local details and global structure are worse than when our full model is utilized. The stylized image directly generated by the Coarse Network resembles an un- finished work in progress."}, {"title": "4.9. Effectiveness of the SSF Modules.", "content": "We compare two different feature fusion methods through some experiments. In the first method, the recon- structed coarse stylized features from the Coarse Network are fused into the reconstructed content features in the Fine Network based on our SSF modules. In the second method, we directly concatenate these two features for feature fu- sion. As Figure 13 shows, the stylization results based on the second method are transferred to the wrong color dis-"}, {"title": "4.10. Additional Experiments", "content": "In Figure 14, we zoom in on some details in style images, content images, and stylized images. The local structures of these style images are transferred to the content image, and the object of the stylized images looks like a reasonable combination that is composed of the style structures rather than a simple mixture of the content structure and the style texture."}, {"title": "As shown in Figure 15, we can control the stylization degree by adjusting the weight term $\\alpha$ in the training phase. It is demonstrated through these experiments that the main content structure can be preserved even though the stylization degree is large. Some local style structures, such as lines or color blocks, can be fused into the global content structure.", "content": null}, {"title": "Following Gatys et al. [26], we incorporate color control and spatial control into our method. In Figure 16(b), the color distribution and the local structure of the stylized image are consistent with those of the style image. Then we use color control to make the stylized image preserve the global color of the content image. In Figure 16(c), although the color is similar to the content image, the local structure and texture are the same as those of the style image. In Figure 17, we use spatial control to transfer different regions of the content image to different styles. The stylization result is appealing as the local style structures and color distribution are maintained greatly. Both experiments demonstrate that our model can synthesize high-quality structure-aware stylized images by fusing key local structures from the style image into the main content structure while discarding some trivial details from the content image.", "content": null}, {"title": "5 Conclusion", "content": "The conclusions are summarized as follows:\n1.\tWe propose a novel feed-forward style transfer algorithm that fuses the local style structure into the global con- tent structure. Different from most style transfer methods that work at the same scale, our model can integrate richer information of features from different scales and then synthesize high-quality structure-aware stylized images.\n2.\tWe first propose a Coarse Network to generate reconstructed coarse stylized features at low resolution, which can capture the main structure of the content image and transfer the holistic color distribution of the style image. Then, we propose a Fine Network to enhance local style patterns and three SSF modules to selectively fuse the reconstructed stylized features into reconstructed content features at different levels.\n3.\tThrough comparative experiments, it is demonstrated that our method is effective in synthesizing appealing high-quality stylized images, and these stylization results outperform the results generated by current state-of-the-art style transfer methods. The experimental results also demonstrate the effectiveness of the Coarse Network, the Fine Network, and the SSF module.\nAlthough the high-quality stylization results can be synthesized by our method, our model only generates the stylized images with a single style after a training process. In future studies, we will achieve a novel arbitrary style transfer framework based on our full model in this paper. Appealing high-quality structure-aware stylized images with arbitrary style can be generated by this framework after a training process. In addition, we will try to use more feature transform methods to replace the whitening and coloring transforms for achieving higher running time effi- ciency."}]}