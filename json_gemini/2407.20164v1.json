{"title": "Language-Conditioned Offline RL for Multi-Robot Navigation", "authors": ["Steven Morad", "Ajay Shankar", "Jan Blumenkamp", "Amanda Prorok"], "abstract": "We present a method for developing navigation policies for multi-robot teams that interpret and follow natural language instructions. We condition these policies on embeddings from pretrained Large Language Models (LLMs), and train them via offline reinforcement learning with as little as 20 minutes of randomly-collected data. Experiments on a team of five real robots show that these policies generalize well to unseen commands, indicating an understanding of the LLM latent space. Our method requires no simulators or environment models, and produces low-latency control policies that can be deployed directly to real robots without finetuning. We provide videos of our experiments at https://sites.google.com/view/llm-marl.", "sections": [{"title": "1 Introduction", "content": "Natural language provides a rich and intuitive interface to describe robot tasks. For instance, commands such as \"navigate to the left corner\" or \"pick up the can\" lend themselves as more powerful and flexible alternatives to specifying (x, y) coordinates or joint configurations. Using language descriptions to specify outcomes, particularly when interfacing with a team of robots, is thus a more natural choice, and one that does not require specially-trained operators.\nRecent work on commanding robots with natural language tend to utilize large pretrained transformers [1] known as LLMs [2, 3, 4] or Large Multimodal Models (LMMs) [5] for both language processing and control. Often, the transformer receives a task and observation, and produces either an action or a sequence of actions to complete the task [6, 7, 8, 9]. The latter case reduces to open-loop control, which cannot adapt to uncertainty, while the former is limited by the high latency of these models, typically measured in seconds or hundreds of milliseconds, precluding them from dynamic scenarios. These drawbacks are especially restricting in multi-agent scenarios, where agents need closed-loop control to react and replan quickly based on the observed behavior of other robots [10, 11, 12]. Developing multi-agent policies that can capitalize on the power of LLMs while retaining fast, reactive, and emergent team behavior is therefore a hard problem.\nIn this work, we present a method that maps high-level natural language commands directly to control outputs for a multi-robot system. First, we project natural language instructions into a latent space using a pretrained LLM, and then condition control policies on these latent embeddings. By"}, {"title": "2 Related Work", "content": "LMMs such as GPT [2] and LLMs such as LLaMa or Mistral [3, 4] demonstrate strong emergent reasoning capabilities, mapping a series of input tokens to output tokens using a transformer architecture [1]. Given their similarities, we refer to both LLMs and LMMs as LLMs moving forward. LLMs often output tokens in the form of text, however, recent work has integrated them into robotic applications due to their powerful reasoning capabilities. For instance, PaLM-E [6] ingests image and text tokens and outputs text describing a list of steps to follow. Prior work demonstrates zero-shot navigation to visual targets by outputting text tokens that map to physical actions [7]. In [9], the authors chain an LLM to a visual foundation model for visual navigation. Going one step further, the method in [8] outputs tokens that correspond to actions, rather than text tokens. Certain works learn policies using imitation learning [13] or value functions [14] for textual action primitives output by an LLM. LLMs are often slow to execute, and these works are often not explicit about the latency between perception and action. For example, the smallest LLaMa-v2 model has roughly 100x fewer parameters than PalM-E model used in [6]. After 2-bit quantization and other efficiency tricks, LLaMa-v2 produces less than two tokens per second on an NVidia Jetson AGX Orin [15]. Given that outputs often consist of multiple tokens, today's hardware precludes them from low-latency and reactive control tasks necessary in multi-robot systems.\nThe use of LLMs in multi-robot systems is still in its infancy, and similar to single-robot settings, relies on mapping text-tokens to plans or actions. Thus, they inherit the latency bottleneck of LLMs, compounded by generating tokens for an increased number of robots. Most prior work operates in simulation, with few papers demonstrating real-world implementations [16]. In [17, 18, 19], individual agents converse in simulation to form a multi-agent system, with the agents eventually"}, {"title": "3 Background", "content": "Sentence Similarity LLMs We are interested in a class of LLMs known as sentence similarity or feature extraction LLMs [21, 22, 23]. An LLM maps n input tokens map to n latent representations at each transformer layer. The final layer predicts a token given n latent representations. Rather than mapping to a token index, feature extraction LLMs pool over the final latent representation to return an embedding that summarizes the input text. These models are trained to have meaningful latent representations, such as semantic similarity corresponding to cosine similarity or Euclidean distance in the latent space. For example, the latent representation of \u201cquick brown fox\u201d would be closer to \"fast maroon badger\" than \u201cmachine learning\".\nReinforcement Learning We refer the reader to [24] for a proper treatment of reinforcement learning. Reinforcement learning aims to learn a policy $\\pi$ to maximize cumulative rewards in a Markov Decision Process (MDP) $\\langle S, A, R, T, \\gamma \\rangle$. The agent selects actions $a \\sim \\pi(s)$ to transition to new states $T(s'|s, a)$ and receive rewards $r = R(s,a, s')$, with the goal of maximizing $E_{\\pi} [\\Sigma_{t=0}^{100}\\gamma^tR(s, a, s')]$. For value-based policies, we will often attempt to learn the value of states $s$ using either a value function $V(s_0) = E_{\\pi} [\\sum_{t=0}^{100}\\gamma^tR(s, a, s')]$, or a Q function $Q(s_0, a_0) = R(s_0, a_0, s_1) + E_{\\pi} [\\sum_{t=1}^{100}\\gamma^tR_{s, a, s'}]$. We then follow a policy that maximizes V or Q, such as the greedy policy $\\pi(s) = argmax_{a\\in a} Q(s, a)$.\nMulti-Agent RL We direct the reader to [25] for a review of Multi-Agent Reinforcement Learning (MARL). It has a number of formulations, depending on the structure of the reward functions. We extend the MDP above to a Decentralized MDP with factorized rewards, defined as $(S, A, T, R, O, \\Omega, \\gamma)$. Each agent i has its own action space, reward function, and observation function: $A = {A_i}_{i=1}^{n}, R = {R_i}_{i=1}^{n}, O = {O_i}_{i=1}^{n}$. The state $s \\in S$ is a function of local observations $s = f(O_1,..., O_n)$, where each observation $o_i \\in \\Omega$ is found via $o_i \\sim O_i(s)$. The state transition function is $T : S \\times A \\rightarrow S$, and the reward function $R_i(s, a, s')$ is defined on the states. There are a variety of MARL algorithms that focus on multi-agent credit assignment, such as MADDPG [26], QMix [27], and COMA [28]. Given our factorized reward function, we choose to learn independent policies using algorithms like Independent Proximal Policy Optimization (IPPO) [29] or Independent Q-Learning (IQL) [30]. Additionally, [31] proposes a softmax-regularized form of the Q-learning objective that provides better results in MARL.\nOffline RL Offline RL learns a policy from existing data, which is especially useful in robotics, where simulators can be inaccurate and data collection can be costly. Unlike imitation learning which requires expert data, offline RL can learn policies from completely random data. In offline RL, we no longer have direct access to the MDP or DecMDP. Instead, a behavior policy $\\pi_\\beta$ collects transition tuples $(s, a, r, s')$ (or $(o_i, a_i, r_i, o'_i)$ in the multi-agent case). With fixed datasets, standard value functions tend to overextrapolate \u2013 incorrectly estimating values for out-of-distribution transitions, which propagate via bootstrapping. Solutions like Conservative Q-Learning (CQL) regularize value estimates to prevent positive overextrapolation [32, 33, 34], while others constrain the learned policy to be close to the data collection policy $\\pi_\\beta$ [35, 36]. There is little work combining MARL with offline RL, likely because both offline RL and MARL are difficult to solve individually. Prior work proposes counterfactual offline multi-agent Q-learning based on CQL [34, 32] and Implicit Constraint Q-Learning (ICQ), which uses soft Q-learning to prevent overextrapolation [37].\nTask-Conditioned Policies There are a number of names for task-conditioned reinforcement learning. It is sometimes called goal-conditioned RL, multi-task RL, or universal value function approximation [38, 39, 40]. Although formulated in different ways, the key difference"}, {"title": "4 Approach", "content": "Our objective is to train multi-agent policies to follow natural language navigation tasks. Our approach consists of two distinct parts, creating the dataset and training the model. To generate the dataset, we first collect real-world data using a single robot. Then, we generate a large quantity of natural language tasks. We sample real-world data and tasks independently, combining them to form a large multi-agent dataset. Finally, we train a policy on the multi-agent dataset using offline RL.\n4.1 Dataset Creation and Augmentation\nWe use the DJI RoboMaster, a four-wheel mecanum-drive holonomic robot, for our experiments [41]. We collect a dataset D by recording (s, a, s') tuples from a single robot as it uniformly samples actions from the action space ($a \\sim U(A)$). We record one tuple per second, resulting in a total of 5400 tuples spanning 90 minutes of data collection. The state space S consists of two-dimensional position and velocity vectors, while the action space A consists of 9 discrete actions, corresponding to eight direction vectors (at 0.3 m/s each), plus an action that corresponds to zero velocity.\nGenerating Task Embeddings Each task $g \\in G$ is a natural language task, such as Agent, the south west corner is your target. We encode each task into a latent embedding $z = \\phi(g)$ using a feature extraction LLM. Each agent in the multi-agent system independently receives tasks, and the tasks differ between agents. For example, one robot might receive the task Agent, the south west corner is your target, while another might receive Agent, navigate to the left edge. We split our tasks and embeddings into a set of train tasks and test tasks. We use 396 tasks for the train set and eight tasks for the test set (Appendix D). Each item in our test set is a never before seen command, enabling us to gauge our model's generalization capabilities.\nCombinatorial Multi-Agent Augmentation Rather than collect a multi-agent dataset, we choose to collect a single agent dataset because it can be exploited to create a large multi-agent dataset in certain scenarios. To turn our single agent dataset into a multi-agent one, we combine single agent transitions into a multi-agent transition\n$S = (S_i, S_j, S_k,...), a = (a_i, a_j, a_k,...), s' = (S'_i, S'_j, S'_k, ...), i, j, k \\in U(|D|).$ (1)\nThis process creates a large amount of multi-agent data from a small amount of single agent data. For example, from our 5400 single agent transitions, we can create a three agent dataset with $Permute(5400, 5) \\approx 10^{18}$ transitions. Given our 396 task embeddings and five agents, we can generate $Permute(396,5) \\approx 10^{12}$ multi-agent task configurations. The product of these two sets results in $10^{30}$ possible task-transition configurations. Given our sample rate, this would equate to collecting data for a duration $10^{12}$ times the age of the universe \u2013 for this reason, we never physically materialize the multi-agent dataset. Rather, we construct these multi-agent transitions on the fly by uniformly sampling transitions with replacement $(s_i, a_i, s'_i) \\sim U(D)$ and tasks without replacement $g_j \\sim U(G)$.\nReward and Termination Functions For each agent i, we create a task-conditioned reward function $R_i$ and episode termination function $D_i$. We compute the reward and termination conditions across various tasks $g_j$ and corresponding goal coordinates $r_j$ as\n$R_i(s, a, s', g_j) = (||(p'_i - r_j)||^2 - || (p_i - r_j)||^2)(1 + v_i + v'_i) - c(p_i, s)$ (2)\n$D_i(s, a, s', g_j) = c(p_i, s),$ (3)"}, {"title": "4.2 Training Objectives", "content": "As stated in Section 3, there are a number of offline RL algorithms, however, the majority are designed for single agent setups. Furthermore, most focus on continuous action spaces and thus utilize KL divergence, causing a divide-by-zero error when considering the greedy policy. Instead, we reformulate Q-learning through the lens of Expected SARSA, a relatively understudied [43] form of value approximation from the Sutton and Barto textbook [24]. In the following subsection, we show how Expected SARSA subsumes approaches like Max Q-learning, softmax-regularized Q-learning [31], and Mean Q-learning [44]. Furthermore, we can implement Expected SARSA on top of Q-learning by changing a single line of code (Appendix A).\nExtending Expected SARSA to Offline Datasets Expected SARSA results in lower variance updates than standard Q-learning [24, 6.6], which is useful in offline learning scenarios where a single bad update can trigger instabilities that result in runaway overextrapolation of Q values [24, 35, 6.6]. Expected SARSA generally results in safer, albeit less optimal policies than Q-learning, as demonstrated in the famous cliffwalker experiment [24, 43, 6.5] (Appendix A). This is especially useful in robotics, where safer policies may be preferred over optimal policies. More formally, Expected SARSA learns a policy-weighted value approximation\n$Q_{\\pi}(s, a) = R(s, a, s') + \\gamma \\sum_{\\alpha' \\in A} \\pi(\\alpha'|s')Q(s', \\alpha').$ (4)\nHowever, Expected SARSA is not designed for offline RL. Normally, a stochastic exploratory policy is slowly annealed into a optimal deterministic Max Q policy. In offline RL, we cannot access the MDP and thus cannot explore. Therefore, it is unclear what to use for $\\pi$ in Eq. (4). If we plug in the standard greedy policy, Expected SARSA decomposes into the standard Max Q-learning objective\n$Q_{*}(s,a) = R(s, a, s') + \\gamma \\sum_{\\alpha' \\in A} \\delta_{argmax_{a'\\in A} Q_*(s', a')}Q_*(s', a')$ (5)\n$= R(s, a, s') + \\gamma max_{\\alpha' \\in A} Q_*(s', a')$ (6)\nwhere $\\delta$ is the Kronecker delta (indicator) function. As discussed in Section 3, Max Q-learning struggles in purely offline scenarios. On the other hand, using the collection policy $\\pi = \\pi_\\beta$ in the objective could prevent overextrapolation at the expense of less optimal behavior. Given that our"}, {"title": "5 Experiments and Discussion", "content": "Our experiments aim to answer four key questions: (1) Is it possible to generalize to an LLM latent space? (2) Which loss function should we use to train our policy? (3) How much data must we collect to train a suitable policy? (4) How does our policy perform on real robots?\nLLM Latent Space Analysis In our first experiment, we quantify how likely a policy is to generalize to the LLM latent space. Recall that we map a natural language goal to a latent embedding using an LLM $z = \\phi(g)$. Now, we attempt to learn a decoder $\\psi$ that maps the latent goal embedding to an alternative representation of the goal $\\overline{g} = \\psi(z)$. If we can correctly predict $\\overline{g}$ for a never-before-seen g, we say that the decoder has generalized to the latent space. To this end, we train an MLP decoder to regress a 2D goal coordinate. For example, the embeddings of 'navigate to the left edge' and 'the west edge is your target' should both decode to $\\overline{g} = (-k, 0)$ for some given k. If our learned decoder $\\psi$, never having seen the phrases 'go to the' or 'north east corner', can correctly decode 'go to the north east corner' to $\\overline{g} = (k, k)$, then we say that our decoder has generalized to the LLM latent space.\nWe compare several LLMs and plot the mean position test error in Fig. 3, utilizing the train and test split from Section 4.1. We find that certain LLMs are not suitable for our purposes (e.g., MiniLM has an error of nearly 1 m). The GTE-Base LLM [22] is a good tradeoff between latent size and performance, with a mean error of 7 cm across never-before-seen tasks. Therefore, we use the GTE-Base LLM in all further experiments."}, {"title": "5.1 Simulation Experiments", "content": "Although our approach does not require a simulator to learn a policy, they can be helpful for analysis. To this end, we learn a single-agent state transition model from our dataset $s_{t+1} = T_i(s_i, a_i)$, where $T_i$ is approximated using a deep neural network. We create a multi-robot simulator via $T(s, a) = {T_i(S_i, a_i)}_{i=1}^{n}$. This model is not perfect \u2013 for example, we can detect collisions between agents but cannot model them. That said, it still allows us to probe various objective functions.\nObjective Function Analysis In Section 4.2, we discuss a number of optimization objectives. Using our simulator, we compare the resulting policies trained using each of these objectives. We report metrics on the test tasks which were not seen during training (Section 4.1). In Appendix B.1, we explore various CQL regularization strengths and Soft Q temperatures for an Expected SARSA Soft Q objective. We compare the best performing CQL and Soft Q variants to the Max Q and Mean Q weightings in Fig. 4, along with perception to action latency. We report the mean distance to the goal over an episode, as well as the number of collisions.\nMax Q begins to overextrapolate around 15k epochs, similar to findings in prior work [35, 54]. CQL's regularization improves upon the Max Q performance. Both Mean Q and Soft Q perform comparably, outperforming both CQL and Max Q with respect to distance and number of collisions. The strong performance of Mean Q suggests that strong regularization towards the dataset collection policy $\\pi_\\beta$ is necessary, even though the regularization reduces optimality. This echos findings from [44], where maximizing the Mean Q Monte Carlo return can outperform CQL. Both Mean Q and Soft Q objectives have nonzero weight over all state-action values. For every state, they always consider the in-distribution action collected during training, which can \u201cground\" the value estimate and further prevent overextrapolation error.\nData Efficiency We investigate how the policy degrades as we reduce the amount of training data available. We subsample our dataset into smaller datasets, and report the results in Fig. 4 right). We find the performance remains nearly the same with as little as 22 minutes of data. Our results also show that we can learn an imperfect yet reasonable policy from just four minutes of real world data."}, {"title": "5.2 Real World Experiments", "content": "Combining offline RL with real-world data should enable us to deploy our policies directly to real robots. In this experiment, we validate that our policies can succeed in the real world without any finetuning. We select the best performing hyperparameters for each loss function in Appendix C.3, and deploy the resulting policies onto either three or five robots. We evaluate each policy twice: once on train tasks, and once on test tasks (Fig. 5, Fig. 1). We consider a task \"solved\u201d if the mean distance to the goal region is less than 0.25 m.\nIn Fig. 5, we find that the policies trained using soft and mean objectives solve most of the train tasks and generalize to the never-before-seen tasks in the test set. Additionally, we observe zero collisions in our 40 minutes of 3-agent tests, and two minor scrapes in our 20 minutes of 5-agent tests. Upon further investigation, the scrapes appear related to issues in the motion capture system, as explained in Appendix B.3. Policies trained with CQL and Max Q move the agents to the edge of the boundaries and keep them there, regardless of the objective. These policies likely learn this behavior to avoid collisions between agents, but are unable to learn to navigate to the given goals. Meanwhile, policies trained using the Mean Q and Soft Q objectives reliably navigate agents to their goals."}, {"title": "6 Limitations and Conclusion", "content": "Given the complexity of fusing offline RL, LLMs, and MARL for real-world robots, we scoped this work to navigation tasks. Future work could extend our methodology towards larger and more general task-spaces. We were able to use a simple IQL learning algorithm because our navigation task has easily factorable rewards. This factorization also enabled us to utilize single-agent data. For more complex tasks with global rewards, future work could collect multi-agent datasets and train using MADDPG. This would also unlock continuous action spaces, but this would require that both the policy and Q function learn to generalize to the LLM latent space. Our random action selection might not scale to larger state spaces, however Expected SARSA Soft Q should work with any dataset collection policy.\nIn summary, we proposed a method that maps natural language tasks to multi-robot control outputs, by combining LLMs with offline RL. We generated multi-agent datasets from single agent data, using it to train policies offline. Then, we investigated different learning objectives, and found that"}, {"title": "C Experiment Details", "content": "In this section, we provide further details for the LLM latent space experiment and the RL experiments.\nC.1 LLM Latent Space Details\nFor the LLM decoder experiment Fig. 3, we utilize a $\\kappa = 1.1$ m and train using mean squared error. Our neural network consists of three blocks B (linear layer, layernorm, and leaky ReLU, see Section 4.2) followed by a final linear layer.\nC.2 Real-World Experimental Setup and Framework\nHere we provide additional details on the robots as well as the framework we use for all our experiments. The robotic platforms we use are DJI Robomaster S1 platforms, which are customized similar to prior work [41]. The robots have an omnidirectional mecanum-wheel drive, and can track fixed-frame position and velocity state references using an onboard control stack (Freyja [55]). The robots have a radius of approximately 25 cm. We operate within a 3.8 m \u00d7 3.8 m subset of a larger 6m x 4m motion-capture arena. We use the default North-East-Down frame convention (with the Down component always zero for ground robots), which is why some of the text prompts contain cardinal directions ('north edge') in them.\nThe actions generated by our trained policies are fixed-frame velocity targets for the robot, commanded at 1 Hz, and are tracked by the onboard controller at 50 Hz. Note that since our policies take < 2 ms to generate actions, we can potentially run them at substantially higher rates if necessary. However, with discretized actions, we find that an action frequency of 1 Hz is sufficient for the navigation tasks we consider. Our control stack, motion-capture setup and the policy execution are all wrapped in a ROS2 ecosystem, and the exact framework is also utilized for collecting real-world datasets.\nC.3 RL Training Details\nWe list the hyperparameters used for both the simulated and real-world RL experiments in Table 2. We provide an arena to the agents, outside of which is considered a collision. During training, we approximate the collision radius of each agent as 0.4 m. During deployment, our actions come from a Boltzmann policy with a temperature of $\\tau = 0.01$\n$\\pi(a_i|s_i) = \\frac{e^{Q(s, a)/\\tau}}{\\sum_{a \\in A} e^{Q(s, a)/\\tau}}$ (20)\n$a_i \\sim \\pi(a_i|s_i).$ (21)\nEach agent i shares the same parameters for $\\pi$. We update the Q function using a Polyak soft update [56]."}, {"title": "A SARSA and Expected SARSA", "content": "SARSA [24] is an on-policy method that learns the following Q function\n$Q_\\pi(s,a) = R(s, a, s') + \\gamma Q_n(s', a'),$ (11)\nwhere $a'$ is the recorded action that taken in the environment. Notice the difference compared to standard Max Q learning, which utilizes a max operator and potentially out-of-distribution action a'\n$Q_{*}(s,a) = R(s, a, s') + \\gamma max_{\\alpha' \\in A} Q_*(s', a').$ (12)\nWhile Max Q learns the Q function for an optimal policy, SARSA learns a Q function for the policy that generated $a'$. Given these considerations, SARSA is not expected to converge to an optimal policy like Max Q is. Only as $\\pi$ approaches $max_{a'\\in A} Q_*(s, a)$ does expected SARSA approach the optimal policy. Although counter-intuitive, [24] shows how this suboptimal policy can be beneficial in the cliff walking scenario. In this scenario, the Max Q policy learns an optimal, but dangerous policy walking near the edge of a cliff to more quickly arrive at the goal. On the other hand, SARSA learns a safer policy that moves away from the cliff at the expense of a slightly reduced return. Even though Max Q should perform better in theory, in practice the Q function is imperfect and Max Q performs worse than SARSA [24].\nWhile the SARSA update utilizes actions $a'$ from the dataset, Expected SARSA uses the policy action distribution instead\n$Q(s,a) = R(s, a, s') + \\gamma \\sum_{\\alpha' \\in A} \\pi(\\alpha'|s)Q(s', a').$ (13)"}, {"title": "D Tasks", "content": "We provide a list of commands and locations that we use to generate tasks. All tasks are in the form Agent, <command to location>. We generate train commands by combining a location with a command string. The {} denote where the location goes in the command string.\nwest edge\neast edge\nsouth edge\nnorth edge\nleft edge\nright edge\nbottom edge\ntop edge\nlower edge\nupper edge\nsouth west corner\nwest south corner\nsouth east corner\neast south corner\nnorth west corner\nwest north corner\nnorth east corner\nSW corner\nSE corner\nNW corner\nNE corner\nbottom left corner\nleft bottom corner\nbottom right corner\nright bottom corner\ntop left corner\nleft top corner\ntop right corner\nright top corner\nlower left corner\nleft lower corner\nlower right corner\nright lower corner\nupper left corner\nleft upper corner\nupper right corner\nright upper corner"}]}