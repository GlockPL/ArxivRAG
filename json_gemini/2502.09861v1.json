{"title": "A Scoresheet for Explainable Al", "authors": ["Michael Winikoff", "John Thangarajah", "Sebastian Rodriguez"], "abstract": "Explainability is important for the transparency of autonomous and intelligent systems and for helping to support the development of appropriate levels of trust. There has been considerable work on developing approaches for explaining systems and there are standards that specify requirements for transparency. However, there is a gap: the standards are too high-level and do not adequately specify requirements for explainability. This paper develops a scoresheet that can be used to specify explainability requirements or to assess the explainability aspects provided for particular applications. The scoresheet is developed by considering the requirements of a range of stakeholders and is applicable to Multiagent Systems as well as other Al technologies. We also provide guidance for how to use the scoresheet and illustrate its generality and usefulness by applying it to a range of applications.", "sections": [{"title": "INTRODUCTION", "content": "It is important for autonomous and intelligent systems\u00b9 to be ex- plainable for a range of reasons. Providing explanations can be required by legislation either directly (e.g. GDPR\u00b2) or indirectly as a consequence of legislation [42]. Providing explanations can also play a crucial role in helping to make autonomous and intelligent systems socially acceptable [11], transparent [1, 39], understand- able [38], accountable [10], and to help establish an appropriate level of trust [11, 22, 32, 34, 35, 40].\nTerminology: Since we consider both autonomous systems and other systems that use a range of Artificial Intelligence techniques, we use \"autonomous and intelligent systems\", sometimes compressed to just \"intelligent systems\". We also avoid the term \"model\" (unless we are specifically talking about machine learning) in favour of \"mod- ule\". Finally, we use \"behaviour\" as shorthand for \"behaviour or outcome\" which encompasses the system taking action or providing some output (e.g. a classification or recommendation).\nhttps://data.consilium.europa.eu/doc/document/ST-5419-2016-INIT/en/pdf\nThe importance of explainability has also been recognised by various standards. For instance, the Ethics Guidelines for Trustwor- thy Al\u00b3 and subsequent Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment\u2074 consider explainability as one of a range of factors (e.g. human agency and oversight, ac- countability, societal & environmental well-being). IEEE P7001 [20] also considers explainability as part of transparency, and defines a number of requirements relating to explainability (e.g. that infor- mation is provided on how a system works in general, or that the system provides the ability to answer \"why?\" questions).\nHowever, this work does not provide adequate guidance for the development and evaluation of the explainability of systems. The Ethics Guidelines for Trustworthy AI only poses questions that ask whether the decisions and outcomes can be understood and whether an explanation is provided, and the Assessment List has just two questions: \"Did you explain the decision(s) of the Al system to the users?\" and \"Do you continuously survey the users if they understand the decision(s) of the AI system?\". Similarly, IEEE P7001 only provides a few explainability requirements (\"why?\u201d and \u201cwhat if?\u201d questions, as well as global explanation - see \u00a72), and Hoffman et al. [16] assign each system only a single number (1-7).\nFollowing IEEE P7001, we propose to provide this guidance in the form of a scoresheet. The P7001 scoresheet focuses on trans- parency, and is complementary to our scoresheet: our scoresheet is specifically for explainability, and provides details, whereas P7001 has considerably less detail on explainability (see \u00a73.1).\nThe scoresheet can be used in various ways with the most obvi- ous being to evaluate the explainability of candidate systems. Used this way, the responses affect which system is chosen because the scoresheet captures that a crucial explainability aspect is lacking, or that another system provides it better.\nExplanations are used by different people for different purposes [4, 7, 13, 21, 29], and therefore we develop our scoresheet by consider- ing the explainability needs of different stakeholders (\u00a72).\nThis paper makes a number of contributions. Firstly, we develop (and justify) a scoresheet\u2076 for explainability (\u00a73). Secondly, we provide additional detailed guidance on how to complete the score- sheet (\u00a74), including an additional checklist for global explanations. Thirdly, we demonstrate that the scoresheet is applicable to a range of systems (\u00a75), showing that the scoresheet is usable and generic, as well as that it is useful (i.e. that it provides a useful summary)."}, {"title": "STAKEHOLDER EXPLAINABILITY NEEDS", "content": "IEEE P7001 [20, 39] defines five stakeholder groups: end users, wider public & bystanders, safety certifiers, incident/accident investiga- tors, and lawyers & expert witnesses. They consider the range of forms of transparency that each requires. For instance, that end users might want to be able to get natural language answers to \"why did you do that?\" and \"what would you do if ...?\" questions. Or that safety certifiers need information on what steps were taken to verify and validate a system. They go on to propose a simple transparency scale for each of the five stakeholder groups. For ex- ample, for an end user, the levels can be summarised as: 0: \"no transparency\"; 1: information provided on how the system works in general (including, if relevant, on data used); 2: same as 1, but interactive; 3: ability to answer\"why?\" questions for specific cases; 4: ability to answer hypothetical \"what if?\" questions; and 5: pro- vision of \"continuous explanation that adapts ... based on the user's information needs and context\".\nArya et al. [3] argue that different stakeholders require different sorts of explanations. They propose a taxonomy (and associated toolkit) that allows stakeholders to select an explanation method that suits their needs. Their context is narrower than ours (machine learning systems that learn from data). Their taxonomy consid- ers factors such as the following. Are explanations (of data) given as particular features (e.g. income or level of debt), examples, or distributions? Do explanations explain individual cases or overall behaviour (local vs. global)? Is the explanation derived directly from the model used to make decisions, or from another (surrogate) model? Elements of their taxonomy are relevant to our scoresheet, and are incorporated in Section 3. These are: explanation of data (where relevant, using examples, distributions, features) vs. expla- nation of the model/module; the distinction between global and local explanations (which is also raised by other literature); and the distinction between an explanation being derived from the module itself, or from a surrogates.\nLiao et al. [23] interviewed 20 UX and design practitioners from IBM to \"identify gaps between the current XAI [eXplainable AI] algorithmic work and practices to create explainable AI products\". Their focus is narrower than ours (explanations of machine learn- ing for end users). One useful contribution of their work is their interview framework: they developed a bank of questions to ensure that the interviews covered a range of important aspects. In order to develop this, they identified a range of question types that can be addressed by current XAI methods, including both widely used questions (How, Why, Why not, What if) and less widely-used questions (how to be that, how to still be this; explained in Sec- tion 3). Their XAI question bank covered six topics: input (i.e. data used), outputs produced, performance (e.g. accuracy, precision, lim- itations), how (global), why & why-not (one topic), and a topic covering hypothetical questions (what if, how to be that, how to still be this).\nThe most directly relevant work to establishing stakeholder needs for explainability is the recent paper by Hoffman et al. [18]\nwhich seeks to establish what various stakeholders need by inter- viewing a range of stakeholders. One key point that they identify in their interviews is that the assumption that there are distinct, clearly distinguishable, stakeholders does not necessarily hold. Rather, they found that people had different roles, but that they adopted the viewpoints of different roles at different times, including roles other than their own. They highlighted the need for both global explana- tions (that are not too high-level, including holistic performance aspects such as biases, assumptions, bounding conditions and lim- itations) and local ones, and noted that it can be desirable to link them by having global explanations that refer to particular cases. They flagged the particular importance of edge cases in understand- ing how the system operates, and what are its limitations. More broadly, they identified the benefit of having access to the system development team and to (trusted) domain practitioners, and of having information about the system's context (e.g. what does it integrate with, how does it support users' goals) and the role of the company making the software, and trust in it, in a broader accountability and responsibility context."}, {"title": "\u0391\u039d \u03a7\u0391\u0399 SCORESHEET", "content": "In this section we present the XAI scoresheet, focusing on what is included, and why it is included. Section 4 provides guidance on how to use the scoresheet.\nThe XAI scoresheet (Figure 1) has a number of sections that each collect different information. An initial section collects some basic information. Then there is a section that focuses on veracity, then global explanations, and finally a section focusing on a range of information relating to local explanations: features of explanations, the concepts used, the explanation types supported, and the level of automation.\nBasic information: There are two pieces of basic information that the scoresheet collects. Firstly, whether the system's source code and (if relevant) training data is available. This is useful to know because access to code (and data) can help in understanding explanations, and in assessing the system's veracity (see below). However, this is of more use if there is access to the developers of the system, who can help to navigate the code (and data), and to (trusted) domain experts who can help to explain the context of use. Hoffman et al. [18] found that access to the system's developers and to trusted domain experts can be important to help understand the system's operation. In the case where the organisation assessing or using the system is also the one that is developing the system, then both these criteria would normally be met.\nVeracity: An important basic requirement of explanations is that they actually correspond to the system's reasoning. An explanation system that invents explanations that do not reflect the actual reasons is clearly not useful, and could in fact mislead, and therefore be worse than not having an explanation at all. We therefore include in the scoresheet a high-level question to indicate the reliability\u2079 of explanations (Low/High\u00b9\u2070).\nOne approach to providing explanations with high reliability is to generate explanations directly from either the actual module used to"}, {"title": "Comparing with IEEE P7001", "content": "Having explained what we have included in our XAI scoresheet and why, we now briefly compare it to the IEEE P7001 transparency scoresheet [20]. Like us, IEEE P7001 proposed a scoresheet in order to help bridge the gap between high-level statements about desir- able properties of systems and actionable metrics. However, there are a number of significant differences. The most significant differ- ence is that P7001 is broader in scope, focusing on transparency, whereas we focus specifically on explainability. For example, P7001 includes requirements about warning bystanders that sensors are collecting information, and providing certification agencies with information about verification and validation activities that were done. Focusing on explainability aspects, P7001 is fairly limited, making our scoresheet useful and complementary. For instance, we also include information on veracity, on how well the system works, and consider factors such as the level of confidence, scope of generalisation, concepts used, level of automation, and additional"}, {"title": "OPERATIONALISING THE SCORESHEET", "content": "In this section we consider the question of how to use the scoresheet, in other words, when filling it out, how does one work out what the answers should be? We also note what other information is useful to capture (apart from what is in the scoresheet).\nHowever, before starting to fill out the scoresheet for a given system that is being considered, we first need to identify who the relevant stakeholders are, and then what are their goals. We also need to identify for the application domain what are the risks that exist, and what level of risk is considered acceptable. This is re- quired because to assess, for instance, whether there is adequate explanation of (globally) how the system operates, we are really answering the question of whether the provided information al- lows the stakeholders to gain an understanding of the system's functioning that is adequate for their goals. In other words, we need to know the stakeholders and their goals to assess this. For example, an elderly person using a domestic robot to support their independent living would need less information on how the robot functions and its limitations (e.g. tasks it cannot do well) than an agency responsible for certifying these robots for domestic use. Similarly, in order to assess the system's reliability, we need to know what the needs are: what can go wrong, and what are the potential consequences?\nBasic information: this covers a few questions, that can be an- swered by asking the developer. However, although these questions appear to be answered by a simple \"yes\" or \"no\", they are actually an example of where there is additional information that is not in the scoresheet itself that is useful to capture. For example, when indicating that there is access to the developers of the system, there"}, {"title": "Multi-Agent Reinforcement Learning", "content": "This work extends multi-agent reinforcement learning with expla- nation features [5], building on earlier work on single agent rein- forcement learning explanation [14]. They apply their approach to three domains: a multi-robot search and rescue scenario, a multi-robot cooperative delivery task, and a grid-based game where agents cooperate and compete to collect food.\nIn essence, they provide two things: an algorithm to create a summary of a policy, and an algorithm to provide explanations for given queries (they extend this in a subsequent paper to temporal logic queries [6]).\nThe first contribution, a summary of a policy, is a global expla- nation (\"policy summarization provides a global view of the agent behavior under a MARL policy\" [5, \u00a74]). However, while the query-based explanations provide what look like typical local explanations, in fact the explanations are in terms of likely paths, rather than in terms of a particular execution of the system.\nRegardless of this though, it is interesting to observe that the three question types they support do not match in an obvious way to the question types that we have included in our scoresheet. Specifi- cally, the first question type (\"When do [agents] do [actions]?\") is used \"for identifying conditions for action(s) of a single or multiple agent(s)\" [5, \u00a74]. This can be seen, in intent, if not phrasing, as being related to \"how to be?\": it is identifying conditions that allow particular actions (i.e. behaviours) to occur. The second question type (\"Why don't [agents] do [actions] in [states]?\") is clearer, cor- responding to our \u201cWhy not?\u201d. Finally, the third question type that they support (\"What do [agents] do in [predicates]?\u201d) is used \"for revealing agent behavior under specific conditions\" (ibid) and can be seen as a form of \"what if?\": given particular conditions, what would happen?\nThe scoresheet clearly captures that this system provides local explanations of various types, and that the explanation generation is done directly from the behaviour-generating module, and hence the explanations can be relied upon."}, {"title": "Taxi planning using learning & planning", "content": "This work [8] proposes an architecture that combines planning and learning, and demonstrates it in a taxi planning domain. The architecture has three levels: a top-level that uses reinforcement learning to identify what are the best goals to select, a middle level that uses an off-the-shelf planner to develop plans to achieve these goals, and a low-level module that uses deep reinforcement learning to perform low-level actions within the plans.\nIn terms of using the scoresheet to assess the explainability aspects of this system a key challenge is that it has three modules, each of which has different explainability features. The planning module (similar to \u00a75.3) captures information that can be used to (manually) generate (highly reliable) explanations. However, the deep reinforcement learning module does not provide any form of explainability.\nThere are two ways in which this can be captured using the scoresheet. The first (which is preferred) is to use a single scoresheet for the whole system, but annotate it to indicate when answers apply to only parts of the system. For example, for veracity we might indicate that it is \"Not Applicable\" for the RL part of the system"}, {"title": "DISCUSSION & CONCLUSION", "content": "We have presented a scoresheet for explainability, along with de- tailed guidance for how to use it. The scoresheet was then applied to a broad range of systems, demonstrating its usability and generality. Looking at the results of applying the scoresheet (Figure 4) we can see that important explainability features of the different systems are captured. For example, for ChatGPT it is clear that explanations may not be reliable, but that the system provides a range of expla- nation types. On the other hand, for PET image generation, the scoresheet captures clearly that only global explanations are avail- able. For the mobile service robot the scoresheet clearly indicates that a range of (local) explanations are available, and that they can be relied upon (because they are generated directly from the plan- ner), but that the construction of explanations from the information is a manual process. The search and rescue (using SARL) and Multi- agent reinforcement learning are similar in providing a range of (reliable) explanations, and do not require manual construction of these explanations. Finally, the taxi planning application scoresheet captures clearly that there are multiple modules in the system, and that these have different explainability characteristics."}, {"title": "Limitations & Future Work", "content": "One limitation is that the scoresheet has only been used by the authors. Therefore, future work includes further use and evaluation of the scoresheet. This could include having a range of people (e.g. various roles, covering the stakeholder types discussed in \u00a72, as well as a range of experience levels and diverse demographics) use it to assess systems. It could also include assessing how well the scoresheet can be used for other use cases (e.g. specifying the explainability requirements of an application, rather than assessing a given system). This would be done by indicating what XAI features are required of a system that is to be used in a certain context, e.g. if a bank was looking to develop a system for making loan decisions it could use the scoresheet to specify what XAI features would be required for the system-to-be. Indeed, it might be possible to use a scoresheet to specify the explainability requirements for a whole sector or domain (e.g. transport, policing), or even to specify regulatory requirements relating to explainability.\nFinally, we highlight some broader research challenges for the XAI research community. There is a need to move beyond explain- ing particular decisions or actions (local explanations) to be able to provide useful information on how the system works, using local explanations to illustrate (i.e. \u201cglobal-local\u201d explanations), includ- ing highlighting edge cases [18]. There is also a need to be able to identify and include information in particular about behaviours that are surprising [17]."}, {"title": "ACKNOWLEDGMENTS", "content": "This research is partially supported by the C2IMPRESS project funded by the EU."}]}