{"title": "The Meta-Representation Hypothesis", "authors": ["Zhengpeng Xie", "Jiahang Cao", "Qiang Zhang", "Jianxiong Zhang", "Changwei Wang", "Renjing Xu"], "abstract": "Humans rely on high-level meta-representations to engage in abstract reasoning. In complex cognitive tasks, these meta-representations help individuals abstract general rules from experience. However, constructing such meta-representations from high-dimensional observations remains a longstanding challenge for reinforcement learning agents. For instance, a well-trained agent often fails to generalize to even minor variations of the same task, such as changes in background color, while humans can easily handle. In this paper, we build a bridge between meta-representation and generalization, showing that generalization performance benefits from meta-representation learning. We also hypothesize that deep mutual learning (DML) among agents can help them converge to meta-representations. Empirical results provide support for our theory and hypothesis. Overall, this work provides a new perspective on the generalization of deep reinforcement learning.", "sections": [{"title": "1. Introduction", "content": "The meta-representation is a higher-order representation about things (Wilson, 2012; Redshaw, 2014). In other words, a meta-representation is a representation of a representation (Scott, 2001). As the saying goes, \"There are a thousand Hamlets in a thousand people's eyes.\" The book Hamlet is a textual representation presented in everyone's mind, while each reader's understanding of the book is a kind of meta-representation.\nHumans process and integrate vast amounts of information from the real world through meta-representations, which are a type of underlying structured information about things we see. Through meta-representation, humans can easily generalize to different tasks with similar underlying semantics. Take video games for example. Once humans have learned how to play a video game, they do not lose the the skills they have already acquired even when the visual presentation of the game changes. This suggests that the ability to perform the learned tasks is not heavily dependent on the specific visual elements of the game, but rather on the underlying cognitive processes that abstract away from such changes. This implies that our brains perform further processing on the images, allowing us to focus only on the core elements of the game itself while ignoring irrelevant features. The emergence of such abstract thinking has been shown to be related to the development of the prefrontal cortex in the human brain (Bengtsson et al., 2009; Dumontheil, 2014). There is also a type of inhibitory neuron that gets activated under specific behavioral conditions, helping the brain process information more effectively (Pi et al., 2013).\nHowever, this poses a challenge for visual reinforcement learning (VRL). Although well-trained agents can solve complex tasks, they often struggle to transfer their experience to new environments, even when the only difference between these environments is a subtle alteration, such as the change of colors in the scene (Cobbe et al., 2019; 2020). Another example in supervised learning is training a neural network with images of black cats and white dogs, and then testing it with images of white cats and black dogs. It has been observed that the network tends to learn spurious features, such as fur color, rather than the true distinguishing characteristics between cats and dogs. This highlights that the network's optimization process is primarily focused on minimizing classification loss, rather than capturing the underlying features that are crucial for robust generalization.\nWhat makes it difficult for reinforcement learning agents to generalize? How can these agents develop the ability to construct meta-representations just like humans?\nOur central theory, presented in Algorithm 1, assumes the existence of several Markov Decision Processes (MDPs) sharing an underlying MDP. Imagine the scenario of building a reinforcement learning benchmark to test the generalization performance of algorithms. We would first implement the core code for the underlying MDP $\\mathcal{M}$, which reflects the intrinsic properties of the task. Then we randomly initialize a rendering function $f$, which obfuscates the underlying state $s_t$ into the agent's observation $o_t = f(s_t)$, much like how different schools of painters may depict the same scene in various styles. In order to achieve good generalization, the agent must learn to filter out the interference from $f$. We refer to the agent's representation of $o_t$ as a meta-representation if it is only a function of $s_t$. Learning such meta-representation indicates that the agent has seen through the noisy observation of to understand the underlying reality, which is much more challenging than simply achieving a high score during training.\nAn analogy of meta-representation is Newton's insight into the law of universal gravitation (Newton et al., 1999) from empirical observations of the world. The empirical observations of the world correspond to $o_t$, while the law of universal gravitation reveals the meta-representation of macroscopic physical reality. This also involves some interesting psychological phenomena, such as the Clever Hans effect (Hediger, 1981; Samhita & Gross, 2013). Hans was a horse that could answer mathematical questions, but he could only give the correct answers when he could see his owner or the questioner. Thus, Hans did not truly understand the mathematical problems $s_t$, but rather reacted to people's involuntary cues $o_t$, learning a certain correspondence between the correct answers and the rendering function $f$.\nIn this paper, we aim to develop a generalization theory for reinforcement learning based on the framework above. We also highlight the difference between this framework and the Partially Observable Markov Decision Process (POMDP) (Murphy, 2000), which primarily emphasizes the partial observability of the true state. Moreover, our meta-representation hypothesis suggests that deep mutual learning (DML) (Zhang et al., 2018b) between agents will help them learn meta-representations from noisy observations, thus improving generalization performance. The main contributions of this paper are summarized as follows:\n\u2022 We theoretically prove that improving the policy robustness to irrelevant features enhances generalization performance. To the best of our knowledge, we are the first to provide a rigorous proof of this intuition.\n\u2022 We propose a hypothesis that deep mutual learning (DML) will help agents learn meta-representations, we also provide intuition to support our hypothesis.\n\u2022 Strong empirical results support our theory and hypothesis, showing that DML technique leads to consistent improvements in generalization performance."}, {"title": "2. Preliminaries", "content": "In this section, we introduce the generalization of reinforcement learning, as well as the DML technique."}, {"title": "2.1. Markov Decision Process and Generalization", "content": "Markov Decision Process (MDP) is a mathematical framework for sequential decision-making, which is defined by a tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, r, P, \\rho, \\gamma)$, where $\\mathcal{S}$ and $\\mathcal{A}$ represent the state space and action space, $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function, $P : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$ is the dynamics, $\\rho : \\mathcal{S} \\rightarrow [0, 1]$ is the initial state distribution, and $\\gamma \\in (0, 1)$ is the discount factor.\nDefine a policy $\\mu : \\mathcal{S} \\times \\mathcal{A} \\leftrightarrow [0, 1]$, the action-value function and value function are defined as\n$$\nQ^{\\mathcal{M}}(s_t, a_t) = \\mathbb{E} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k r(s_{t+k}, a_{t+k}) \\right], \n$$\n$$\nV(s_t) = \\mathbb{E}_{a_t \\sim \\mu(\\cdot | s_t)} [Q^{\\mathcal{M}}(s_t, a_t)].\n$$\nGiven $Q^{\\mu}$ and $V^{\\mu}$, the advantage function can be expressed as $A^{\\mu} (s_t, a_t) = Q^{\\mu}(s_t, a_t) - V^{\\mu}(s_t)$."}, {"title": "2.2. Deep Mutual Learning", "content": "Deep mutual learning (DML) (Zhang et al., 2018b) is a mutual distillation technique in supervised learning. Unlike the traditional teacher-student distillation strategy, DML aligns the probability distributions of multiple student networks by minimizing the KL divergence loss during training, allowing them to learn from each other. Specifically,\n$$\n\\mathcal{L}_{DML} = \\mathcal{L}_{SL} + \\alpha \\mathcal{L}_{KL},\n$$\nwhere $\\mathcal{L}_{SL}$ and $\\mathcal{L}_{KL}$ represent the supervised learning loss and the KL divergence loss, respectively, $\\alpha$ is the weight.\nUsing DML, the student cohort effectively pools their collective estimate of the next most likely classes. Finding out and matching the other most likely classes for each training instance according to their peers increases each student's posterior entropy, which helps them converge to a more robust representation, leading to better generalization."}, {"title": "3. Theoretical Results", "content": "In this section, we present the main results of this paper, demonstrating that enhancing the agent's robustness to irrelevant features will improve its generalization performance.\nA key issue is that we do not exactly know the probability distribution $P_{train}$. Note that $F_{train}$ is a subset of $F$, we can naturally assume that the probability distribution $P_{train}$ can be derived from the normalized probability distribution $p$, which is a mild assumption.\nAssumption 3.1. For any $f \\in F$, assume that\n$$\nP_{train}(f) = \\frac{p(f) \\cdot \\mathbb{I}(f \\in F_{train})}{Z}, P_{eval}(f) = \\frac{p(f) \\cdot \\mathbb{I}(f \\in F_{eval})}{1 - Z},\n$$\nwhere $Z = \\int_{F_{train}} p(f) df$ and $1-Z$ is the partition function, $F_{eval} = F - F_{train}$, $\\mathbb{I}(\\cdot)$ denotes the indicator function.\nAn interesting fact is that, for a specific policy $\\pi$, if we only consider its interaction with $\\mathcal{M}_f$, we can establish a bijection between this policy and a certain underlying policy that directly interacts with $\\mathcal{M}$. We now denote it as $\\mu_f(\\cdot | s_t) = \\pi (\\cdot | f(s_t))$. By further defining the normalized discounted visitation distribution $d^{\\mu}(s) = (1-\\gamma) \\sum_{t=0}^{\\infty} \\gamma^t P(s_t = s | \\mu)$, we can use this underlying policy $\\mu_f$ to replace the training and generalization performance of the policy $\\pi$. Specifically,\n$$\n\\eta(\\pi) = \\frac{1}{1-\\gamma} \\mathbb{E}_{f \\sim P_{train}(\\cdot), s \\sim d^{\\mu_f(\\cdot)}, a \\sim \\mu_f(s)} [r(s, a)],\n$$\n$$\n\\zeta(\\pi) = \\frac{1}{1-\\gamma} \\mathbb{E}_{f \\sim p(\\cdot), s \\sim d^{\\mu_f(\\cdot)}, a \\sim \\mu_f(s)} [r(s, a)].\n$$\nThen, define $\\mathcal{L}_{\\pi}$ as the first-order approximation of $\\eta$ (Schulman, 2015), we can derive the following lower bounds.\nTheorem 3.2 (Training performance lower bound). Given any two policies, $\\tilde{\\pi}$ and $\\pi$, the following bound holds:\n$$\n\\eta(\\tilde{\\pi}) \\geq \\mathcal{L}_{\\pi}(\\tilde{\\pi}) - \\frac{2 \\gamma \\epsilon_{train}}{(1 - \\gamma)^2} \\mathbb{E}_{f \\sim P_{train}(\\cdot), s \\sim d^{\\mu_{\\tilde{f}}(\\cdot)}} [D_{TV} (\\mu_f || \\mu_{\\tilde{f}})[s]],\n$$\nwhere $\\epsilon_{train} = \\max_{f \\in F_{train}} {\\max_s |\\mathbb{E}_{a \\sim \\mu_{\\tilde{f}}(\\cdot | s)} [A^{\\mu_f}(s, a)]|}$."}, {"title": "4. Central Hypothesis", "content": "Despite the theoretical progress, in typical generalization scenarios, both the underlying MDP and the rendering function are unknown to us. In this section, we hypothesize that deep mutual learning (DML) (Zhang et al., 2018b) can be used to enhance the policy robustness against irrelevant features from high-dimensional observations, thus improving generalization performance, as can be seen from Figure 2.\nThe figure illustrates two randomly initialized policies independently trained using reinforcement learning algorithms. In this case, since the training samples only include a portion of the MDPs, the policies are likely to overfit to irrelevant features and fail to converge to a robust hypothesis space.\nIf DML loss is introduced into the training process of these two policies (referred to as policy A and policy B) to facilitate mutual learning, the overfitting to irrelevant features may be mitigated. Due to the random initialization of policies A and B, they will generate different training samples. The DML loss encourages them to make consistent decisions on the same observations, meaning that any irrelevant features learned by policy A are likely to result in suboptimal performance for policy B (see Appendix B for more details), and vice versa. As training progresses, DML will force both policies to learn truly useful representations. Therefore, ideally, we hypothesize that they will eventually converge to meta-representations of high-dimensional observations.\nAn interesting analogy for our hypothesis is the process of truth emergence. Typically, each scholar can present their unique perspective, but this requires widespread acceptance by peers in the same field or even the entire academic community. We can liken DML to the peer review process: if a particular viewpoint is accepted by the majority, it is more likely to be an objective fact (of course, this does not rule out the possibility that everyone could be wrong, as in the era when geocentrism was widely supported). Going deeper, our hypothesis also aligns with the philosophical concept of convergent realism (Laudan, 1981; Kelly & Glymour, 1989; Huh et al., 2024) (i.e., that science is converging on truth)."}, {"title": "5. Implementation Details", "content": "We use Procgen (Cobbe et al., 2019; 2020) as the experimental benchmark for testing generalization performance. Procgen is a suite of 16 procedurally generated game-like environments designed to benchmark both sample efficiency and generalization in reinforcement learning, and it has been widely used to test the generalization performance of various reinforcement learning algorithms (Wang et al., 2020; Raileanu & Fergus, 2021; Raileanu et al., 2021; Lyle et al., 2022; Rahman & Xue, 2023; Jesson & Jiang, 2024).\nWe employ the Proximal Policy Optimization (PPO) (Schulman et al., 2017; Cobbe et al., 2020) algorithm as our baseline, as PPO is one of the most widely used model-free reinforcement learning algorithms. Specifically, given a parameterized policy $\\pi_{\\theta}$ ($\\theta$ represents the parameters), the objective of $\\pi_{\\theta}$ is to maximize\n$$\n\\mathbb{E}_{(o_t, a_t) \\sim \\pi_{\\theta_{old}}} \\left{ \\min \\left[ r_t(\\theta) \\cdot \\hat{A}(o_t, a_t), \\tilde{r_t}(\\theta) \\cdot \\hat{A}(o_t, a_t) \\right] \\right},\n$$\nwhere $\\hat{A}$ is the advantage estimate, and\n$$\nr_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | o_t)}{\\pi_{\\theta_{old}}(a_t | o_t)}, \\tilde{r_t}(\\theta) = \\text{clip} (r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon),\n$$\nwith $\\pi_{\\theta_{old}}$ and $\\pi_{\\theta}$ being the old policy and the current policy.\nWe randomly initialize two agents to interact with the environment and collect data separately. Similar to the DML loss (5) used in supervised learning, we also introduce an additional KL divergence loss term, which is\n$$\n\\mathcal{L}_{DML} = \\mathcal{L}_{RL} + \\alpha \\mathcal{L}_{KL},\n$$\nwhere $\\mathcal{L}_{RL}$ is the reinforcement learning loss and $\\mathcal{L}_{KL}$ is the KL divergence loss, $\\alpha$ is the weight. And then we optimize the total loss of both agents, which is the average of their DML losses, as shown in Algorithm 2.\nFinally, we do not claim to achieve state-of-the-art (SOTA) performance, but rather to verify that the DML technique indeed helps agents learn more robust representations from high-dimensional observations and leads to consistent improvements in generalization performance, providing empirical support for our central theory and hypothesis."}, {"title": "5.2. Empirical Results", "content": "We compare the generalization performance of our approach against the PPO baseline on the Procgen benchmark using the hard level, which is much more challenging than the easy level. The results are illustrated in Figure 3. It can be observed that DML technique indeed leads to consistent improvements in generalization performance across all environments. Notably, for the bigfish, dodgeball, and fruitbot environments, we have observed significant improvements. Moreover, the experimental results for all 16 environments in Procgen benchmark, including training performance and generalization performance, can be found in Appendix C.\nA natural concern arises: how can we determine whether DML improves generalization performance by enhancing the policy robustness against irrelevant features, or simply due to the additional information sharing between these two agents during training (each agent receives additional information than it would from training alone)?"}, {"title": "5.3. Robustness Testing", "content": "To further verify that our method has indeed learned more robust policies, we design a novel approach to test policy robustness against irrelevant features, as shown in Figure 4. For each current frame, we input it into multiple convolutional layers randomly initialized with a standard Gaussian distribution, and then compute the average KL divergence of the policy before and after the perturbation by these random convolutional layers. This allows us to effectively test the robustness of the trained policies without changing the underlying semantics. The results can be seen from Table 1.\nWe can see that the average changes in KL divergence of our method is lower than the PPO baseline in almost all environments, with a smaller standard deviation. This indicates that DML technique indeed helps agents learn meta-representations from high-dimensional observations, providing strong empirical support for our central hypothesis."}, {"title": "5.4. Ablation Study", "content": "To verify that the generalization performance of the agent benefits from more robust policies, we designed additional ablation experiments. Specifically, we used the frozen encoders obtained from the PPO baseline and our method to retrain the policies, the results are shown in Figure 5.\nSince the policy obtained from our method is more robust to irrelevant features (as demonstrated in Section 5.3), the encoder learns a better representation of the high-dimensional observations. Therefore, based on our theoretical results, retraining policies using the frozen encoders obtained from our method should have better generalization performance. We can see that the generalization performance in Figure 5 strongly supports our theoretical results.\nIn summary, Section 5.2 validates the effectiveness of DML technique for generalization, Section 5.3 verifies our central hypothesis, and Section 5.4 confirms our theoretical results."}, {"title": "6. Related Work", "content": "The generalization of deep reinforcement learning has been widely studied, and previous work has pointed out the overfitting problem in deep reinforcement learning (Rajeswaran et al., 2017; Zhang et al., 2018a; Justesen et al., 2018; Packer et al., 2018; Song et al., 2019; Cobbe et al., 2019; Grigsby & Qi, 2020; Cobbe et al., 2020; Yuan et al., 2024).\nA natural approach to avoid overfitting problem in deep reinforcement learning is to apply regularization techniques originally developed for supervised learning such as dropout (Srivastava et al., 2014; Farebrother et al., 2018; Igl et al., 2019), data augmentation (Laskin et al., 2020; Kostrikov et al., 2020; Zhang & Guo, 2021; Raileanu et al., 2021; Ma et al., 2022), domain randomization (Tobin et al., 2017; Yue et al., 2019; Slaoui et al., 2019; Mehta et al., 2020), or network randomization technique (Lee et al., 2019).\nOn the other hand, in order to improve sample efficiency, previous studies encouraged the policy network and value network to share parameters (Schulman et al., 2017; Huang et al., 2022). However, recent works have explored the idea of decoupling the two and proposed additional distillation strategies (Cobbe et al., 2021; Raileanu & Fergus, 2021; Moon et al., 2022). In particular, Raileanu & Fergus (2021) demonstrated that more information is needed to accurately estimate the value function, which can lead to overfitting."}, {"title": "7. Discussion", "content": "In this paper, we provide a novel theoretical framework to explain generalization problem in deep reinforcement learning, we also hypothesize that DML technique facilitates meta-representation learning. Strong empirical results support our central theory and hypothesis.\nAlthough we emphasized the distinction between our framework and POMDP in the introduction, mathematically, they are identical, which suggests that our theoretical results are also applicable to the analysis of POMDP.\nExtracting patterns from empirical observations is considered a powerful abstraction ability unique to humans. We have taken a step in that direction. If human perception of the world is entirely based on electrical and chemical signals in the brain, then what might the true nature of the world be? How can we suppose to infer the true nature of the world from our empirical observations in our mind?\nThis work provides an answer, which is cognitive alignment (Falandays & Smaldino, 2022). Our method encourages agents to make consistent decisions based on the same observations, which is also a form of cognitive alignment. Cognitive alignment has been widely used in the development of human society. For instance, in voting, the principle of majority rule is followed because people tend to believe that decisions supported by the majority are more reliable.\nThrough cognitive alignment, the subjective consciousness of others provides each individual with new perspectives. Over time, the cognition of each individual tends to converge toward the truth, ultimately freeing us from the limited electrical and chemical signals in our brains to understand the true reality. Just like the prisoners in Plato's Allegory of the Cave, who eventually break free from the confines of the \"cave\" to understand the real world."}, {"title": "B. A More Detailed Explanation of Our Hypothesis", "content": "In Section 4, we claimed that \u201cThe DML loss encourages them to make consistent decisions on the same observations, meaning that any irrelevant features learned by policy A are likely to result in suboptimal performance for policy B, and vice versa.\u201d Here, we aim to provide a more detailed explanation to help readers better understand this point.\nLet's consider a simple environment where the agent is in a rectangular space and attempts to pick up coins to earn rewards (see Figure 7). The agent's observations are the current pixels.\nIt is clear that the agent's true objective is to pick up the coins, and the background color is a spurious feature. However, upon observing the training data for policy A, we can see that in the red background, the coins are always on the right side of the agent, while in the cyan background, the coins are always on the left side. As a result, when training policy A using reinforcement learning algorithms, it is likely to exhibit overfitting behavior, such as moving to the right in a red background and to the left in a cyan background.\nHowever, the overfitting of policy A to the background color will fail in the training data of policy B, because in policy B's training data, regardless of whether the background color is red or cyan, the coin can appear either on the left or right side of the agent. Therefore, through DML, policy A is regularized by the behavior of policy B during the training process, effectively preventing policy A from overfitting to the background color. In other words, any irrelevant features learned by policy A could lead to suboptimal performance of policy B, and vice versa. Thus, we hypothesize that this process will force both policy A and policy B to learn the true underlying semantics, ultimately converging to meta-representations."}, {"title": "D. More Implementation Details", "content": ""}, {"title": "D.1. Proximal Policy Optimization", "content": "In our experiments, we employ Proximal Policy Optimization (PPO) as our baseline algorithm. Specifically, given the policy network $\\pi_{\\theta}$, the value network $V_{\\phi}$, and any observation-action pair $(o_t, a_t)$, the value loss is\n$$\n\\mathcal{L}_v = \\frac{1}{2} [V_{\\phi}(o_t) - R_t]^2,\n$$\nwhere $R_t$ is the estimated discounted return at step $t$ using the Generalized Advantage Estimation (GAE) (Schulman et al., 2015) technique. And the policy loss is\n$$\n\\mathcal{L}_p = - \\min \\left{ \\frac{\\pi_{\\theta}(a_t | o_t)}{\\pi_{\\theta_{old}}(a_t | o_t)} \\hat{A}(o_t, a_t), \\text{clip} \\left( \\frac{\\pi_{\\theta}(a_t | o_t)}{\\pi_{\\theta_{old}}(a_t | o_t)}, 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}(o_t, a_t) \\right}, \\mathcal{L}_e = H(\\pi_{\\theta}(\\cdot | o_t)),\n$$\nwhere $H(\\cdot)$ represents the entropy of the output action distribution. The pseudo-code for PPO is provided in Algorithm 3."}, {"title": "D.2. PPO with DML", "content": "Our approach introduces an additional KL divergence loss to encourage mutual learning between the two agents, which is\n$$\n\\mathcal{L}_{DML} = \\mathcal{L}_p + c_1 \\mathcal{L}_v - c_2 \\mathcal{L}_e + \\alpha \\mathcal{L}_{KL},\n$$\nwhere $\\mathcal{L}_p + c_1 \\mathcal{L}_v - c_2 \\mathcal{L}_e$ is the reinforcement learning loss, and\n$$\n\\mathcal{L}_{KL} = D_{KL}(\\pi_{\\theta} || \\pi_{\\hat{\\theta}})\n$$\nis the KL divergence between the current policy and the other agent's policy, $\\alpha$ is the weight, and $\\pi_{\\hat{\\theta}}$ denotes the other agent's policy. Thus, this additional KL loss encourages the two agents to make consistent decisions for the same observations."}]}