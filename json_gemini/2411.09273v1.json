{"title": "Cross-Modal Consistency in Multimodal Large Language Models", "authors": ["Xiang Zhang", "Senyu Li", "Ning Shi", "Bradley Hauer", "Zijun Wu", "Grzegorz Kondrak", "Muhammad Abdul-Mageed", "Laks V.S. Lakshmanan"], "abstract": "Recent developments in multimodal methodologies have marked the beginning of an exciting era for models adept at processing diverse data types, encompassing text, audio, and visual content. Models like GPT-4V, which merge computer vision with advanced language processing, exhibit extraordinary proficiency in handling intricate tasks that require a simultaneous understanding of both textual and visual information. Prior research efforts have meticulously evaluated the efficacy of these Vision Large Language Models (VLLMs) in various domains, including object detection, image captioning, and other related fields. However, existing analyses have often suffered from limitations, primarily centering on the isolated evaluation of each modality's performance while neglecting to explore their intricate cross-modal interactions. Specifically, the question of whether these models achieve the same level of accuracy when confronted with identical task instances across different modalities remains unanswered. In this study, we take the initiative to delve into the interaction and comparison among these modalities of interest by introducing a novel concept termed cross-modal consistency. Furthermore, we propose a quantitative evaluation framework founded on this concept. Our experimental findings, drawn from a curated collection of parallel vision-language datasets developed by us, unveil a pronounced inconsistency between the vision and language modalities within GPT-4V, despite its portrayal as a unified multimodal model. Our research yields insights into the appropriate utilization of such models and hints at potential avenues for enhancing their design.", "sections": [{"title": "1 Introduction", "content": "Recent large multimodal models have showcased remarkable capabilities in tasks that require the integration of multiple modalities and sources of information (Huang et al., 2023). Among these, the performance of Vision Large Language Models (VLLMs) (Zhang et al., 2023a; Yang et al., 2023) stands out, thanks to the vast amounts of image and text data available for training and the rapid progress in both computer vision and language modelling. However, due to the distinct training methodologies employed by these models, such as contrastive learning (Radford et al., 2021; Jin et al., 2024) and embodied image-language modelling (Driess et al., 2023), and the varying quality of training data for each modality (Yin et al., 2023), these networks often exhibit performance disparities across different modalities.\nPrevious research has extensively evaluated the performance of individual modalities in multi-modal systems. For instance, Yang et al. (2023) conducted a thorough assessment of GPT-4V's vision understanding capabilities, and Chen et al. (2023) analyzed model's decision-making abilities. However, assessing a model's performance on each individual modality in isolation does not fully evaluate its true multimodal abilities. It is possible, for example, for a model to excel in numerous vision tasks but still lag significantly behind in language understanding. Moreover, simply testing performance on individual tasks provides no insight into whether and how each modality of the model influences the others. Unfortunately, the cross-modality relationship is frequently overlooked in the aforementioned research.\nIn this study, we go beyond the traditional approach of simply evaluating multimodal systems through separate downstream tasks and reporting their scores. Our focus is primarily on measuring the inherent differences in capabilities between various modalities, with special attention to vision and language, given their prominence among other modalities. To enable a comprehensive analysis, we introduce the concept of cross-modal consistency, complete with a formal definition and an evaluation framework. We consider cross-modal consistency to be an essential element in the design of complex multimodal systems with neural components, as it guarantees coherence and reliability in the system's performance. This is crucial for both interpretability and for fostering user trust.\nWe subsequently construct a comprehensive vision-language parallel dataset encompassing seven tasks, each designed to highlight different facets of vision and language capabilities. This dataset serves as a tool for evaluating the vision-language consistency of VLLMs. Our experiments with the GPT-4V model on the dataset reveal significant inconsistencies between its vision and language capabilities. The results indicate that its performance varies considerably depending on whether the same task instance is prompted in one modality versus the other.\nOur contributions are: (1) We introduce the novel concept of cross-modal consistency, along with a comprehensive evaluation framework. This approach transcends traditional assessment methods for multimodal models, which typically evaluate each modality in isolation. (2) We develop and release seven diverse datasets, carefully designed for vision-language consistency evaluation, opening up opportunities to exploit these datasets in future research. (3) Our experiments on GPT-4V reveal a significant disparity between vision and language abilities within such a system, prompting the introduction of the Vision-Depicting-Prompting (VDP) method as a potential remedy. Our findings offer valuable guidance for more effective future use of such multimodal models."}, {"title": "2 Related Work", "content": "A substantial amount of effort has been dedicated to meticulous evaluation of large multimodal models such as GPT-4V. To assess the capabilities of these models across all their modalities, a wide array of tasks has been tested. E.g., researchers have scrutinized GPT-4V's aptitude in solving problems within specialized domains, including biomedicine (Liu et al., 2023b), medical applications (Wu et al., 2023), and autonomous driving (Wen et al., 2023), employing intricate image inputs. Beyond these domain-specific evaluations, more general skills like chart image understanding (Liu et al., 2023a) and optical character recognition (Shi et al., 2023) have also been analyzed. However, these evaluations often focus solely on performance metrics for each test dataset, with little or no exploration of the relative capability gaps between vision and language. In this study, our primary emphasis lies in uncovering the relative disparities in the abilities of multimodal models across their various modalities, rather than merely assessing absolute performance within specific tasks.\nDespite the lack of cross-modal analysis for multimodal models, previous research has delved into examining cross-lingual abilities in Large Language Models (LLMs). For example, by translating task instances into different languages and analyzing the pairwise results, Zhang et al. (2023b) demonstrated that models like GPT-3.5, primarily trained on English text corpora, exhibit disparities in their performance across various tasks when prompted with different languages. Specifically, these LLMs display a bias toward English. Also, Chou et al. (2024) analyze the consistency concept within each modality for multi-modal models. We extend our research to encompass consistency analysis across various modalities, recognizing that different languages can be regarded as distinct modalities as well. Our generalized framework sheds light on the underlying principles governing the consistency of multimodal models when confronted with tasks in diverse modalities, thereby contributing to a deeper understanding of their capabilities and limitations."}, {"title": "3 Preliminaries and Key Concepts", "content": "As \"consistency\" can carry different interpretations within the specific context we are addressing, a formal definition of the concept of cross-modal consistency for multimodal models is warranted. To that end, we establish an instance of task t, represented as the paired value $(d_a, q)$. Here, $d_a$ represents a data element from the input space $D_a$ corresponding to modality $a$, while $q \\in Q$ represents the abstract query, often presented in the form of a question pertinent to the task at hand. A task set within modality a is then constituted by combining certain data elements from modality a with the queries q, which can be denoted as $S_{t,a} = \\{(d_a^{(1)}, q), (d_a^{(2)}, q), (d_a^{(3)}, q), ...\\}$. When the queries q are held constant, and elements $d_y \\in D_y$ in another modality b are gathered, we obtain the corresponding task set in another modality, denoted as $S_{q,b}$. In essence, the task t embodies the task-specific queries, encompassing, e.g., activities such as solving equations, translation, question answering, etc. Meanwhile, the data elements $d_m$ may take the form of equation instances or question descriptions within modality m, which can involve the modalities of image, text, or speech.\nWe introduce the concept of a 'converter,' a function $K_{a,b}: D_a \\rightarrow D_b$ which maps data elements from modality a to b. While there exist various methods for converting data between modalities (e.g., from language to vision through taking a picture), we are specifically interested in converters that preserve information necessary for solving a given task with query q, denoted as $K_q$. Information-preserving converters are distinctive, as the correct answer for a given task instance $(d, q)$ depends solely on the information within d rather than its modality. Therefore, both $(d_a, q)$ and $(K_{a,b}^q(d_a), q)$ are guaranteed to share the same gold label. In this work, we assume the existence of $K_q$ for every $q \\in Q$, but finding such a converter is beyond the scope of this paper. Inter-modality conversion may be challenging for certain modalities. Some tasks may involve aspects of information, such as emotions in speech or nuanced visual perception in images, that cannot be easily preserved during conversion. We design our experiments with tasks where a $K^q$ clearly exists.\nA multimodal model can be conceptualized as a function, denoted $M : D \\times Q \\rightarrow y$, mapping data elements and queries to an answer. Here, $D$ represents the collective space encompassing all the modalities of interest, formally $D = \\bigcup_m D_m$, where m spans over all relevant modalities. On the other hand, the answer space Y refers to a unified and structured representation, which, in the case of GPT-4V, assumes the form of text. A model M is said to exhibit consistency between modalities a and b provided:\n$M(d_a, q) = M(K_{a,b}^q(d_a), q), \\forall d_a \\in D_a, q \\in Q$\nIn other words, M is consistent if its output is invariant under any modality transformation $K_q$ which preserves all essential information necessary for solving the task associated with query q. E.g., consider solving mathematical equations. A model which solves this task is consistent across the text and image modalities if neither transcribing the equation from image to text, nor imaging an equation presented as text, changes the model's output. In short, a consistent model should remain agnostic to the modality of the task instance and yield identical results as long as an equivalent amount of information is provided, reflecting its capacity to handle multimodal data seamlessly."}, {"title": "4 Method", "content": "In this section, we describe our method for testing cross-modal consistency. We establish a quantitative evaluation framework, with a focus on the vision-language cross-modality. We provide a description of our methodology and the specific metrics we propose for evaluation."}, {"title": "4.1 WorkFlow", "content": "For an instance set of a given task t in modality a, denoted as $S_{t,a} = \\{(d_a^{(1)}, q), (d_a^{(2)}, q), (d_a^{(3)}, q), \\ldots \\}$, our first step involves constructing a parallel instance set $S_{t, b}$ in modality b using an information-preserving converter $K_{a, b}^q$. We do so by applying $K_{a, b}^q$ to each data object $d_a^{(i)}$ to get the object $d_b^{(i)} := K_{a, b}^q(d_a^{(i)})$ in modality b. By doing so, each paired instance $(d_a^{(i)}, q)$ and $(d_b^{(i)}, q)$ shares the same gold label since the information in $d_a^{(i)}$ is preserved for the task with query q. In the context of analyzing the vision and language modalities, our converter is comprised of an optical character recognition (OCR) system combined with human verification for converting images to text, and screenshot software for converting text into images. We carefully select tasks where the information required for solving the task can be fully retained through the utilization of this converter, as exemplified by mathematical equation solving.\nNext, we independently apply the model M to each pair of instances $(d_a^{(i)}, q)$ and $(d_b^{(i)}, q)$ to obtain pairwise results $M(d_a^{(i)}, q)$ and $M(d_b^{(i)}, q)$."}, {"title": "4.2 Metrics", "content": "We introduce our task consistency score $C_t$ based on these pairwise instances:\n$C_t = \\frac{1}{n} \\sum_{i=1}^n c_i$                                   (1)\nwhere\n$c_i = \begin{cases}\n1, & \\text{if } M(d_a^{(i)}, q) = M(d_b^{(i)}, q) \\\\\n0, & \\text{otherwise}\n\\end{cases}$          (2)\nIn essence, $C_t$ is the proportion of instances for which model M has consistent performance on the given task, between modalities a and b."}, {"title": "5 Experiments", "content": "Since there is currently no existing parallel vision-language task dataset, we create our own datasets for both our experiments and also to facilitate future research endeavors. Following the approach outlined in Section 4.1, we meticulously selected seven tasks that gauge various facets of Vision-Large Language models. For each of these tasks, we ensure that data instances can be transformed between image and text formats while preserving all task-related information, utilizing a straightforward converter (e.g., OCR). Recognizing that a flawless converter does not exist in practice, we undertake the manual verification of each converted data instance to prevent any potential errors during the conversion process. We will make our dataset available for use by the research community in the final version of our paper."}, {"title": "5.1.1 Task Description.", "content": "Mathematical reasoning stands as a cornerstone of multi-modal models' capabilities. Mathematical problems typically involve equations presented in a visual format, offering a clear depiction of intricate symbols and notations. Given that formulas can be seamlessly converted to text formats like LaTeX without losing any essential information for solving these equations, constructing a parallel dataset for such tasks is a natural fit for analyzing cross-modal consistency. For our dataset, we source math questions with equations from two distinct origins, each representing varying levels of difficulty. For low difficulty levels, we extract 901 high school-level mathematical questions in LaTeX (text) format from MATH dataset (Hendrycks et al., 2021b), rendering each question using a LaTeX compiler to generate corresponding image data. To introduce a greater level of complexity, we gathered 50 college-level calculus questions, along with their corresponding answers, using the same procedure. Consequently, we paired all the image-based math questions with their corresponding text representations to create our comprehensive equation-solving dataset, encompassing both easy and challenging questions. An illustrative example of this dataset can be found in Figure 3, and detailed data samples are available in Appendix A and Appendix B .\nTo assess the vision-language consistency in logical reasoning abilities for the VLLMs, we employ two distinct datasets: GSM8K (Cobbe et al., 2021) and LogicQA (Liu et al., 2020). GSM8K comprises 8,500 question instances in text format, with each instance representing a problem description in English text paired with a labeled answer. We transform the text into images by capturing screenshots of the rendered text with an appropriate font size and layout. Similarly, LogicQA consists of 8,678 more challenging questions presented in text format and each is converted into an image by us. Subsequently, we pair these resulting images with the original text files, creating a parallel dataset that enables the exploration of this task in both image and text modalities."}, {"title": "5.2 Experiment Details.", "content": "We apply our framework and constructed datasets to evaluate the cross-modal consistency of the OpenAI GPT-4V model, known for its proficiency in both vision and language modalities. Given the limited daily access to prompt this model, our experiments were conducted on a randomly selected subset of 50 samples from each dataset. We select the GPT-4V classical mode, which does not include additional plug-ins and employs a relatively low decoding temperature to minimize variance in its output. To ensure a fair comparison of capabilities between the two modalities, we embedded the query questions into the image and exclusively used images for prompting. This avoids the involvement of any text input when testing the vision modality. Additionally, to prevent the model from performing reasoning steps in text and introducing unintended modality conversions, we explicitly instructed the model to output answers without any reasoning steps. Our results are manually collected for pairwise data instances, and we calculate the consistency scores based on the methodology outlined in Section 4.1."}, {"title": "5.3 Main Results", "content": "The main outcomes of our assessments across seven distinct datasets are outlined in Table 1. Notably, even though the input contains an equivalent amount of information necessary for task completion, substantial disparities emerge between image and text input formats. This phenomenon occurs even in tasks where images are conventionally considered to offer a more vivid and intuitive representation from a human perspective.\nWe note that consistency, being based on response agreement between modalities, can be high or low regardless of per modality accuracy. The highest consistency (0.92) is observed for math reasoning even though both modalities have a relatively low accuracy (\u2264 0.40). By contrast, the consistency drops to 0.64 on logical reasoning (LogicQA) on which the individual modalities have higher accuracy (\u2265 0.44).\nFor tasks that involve intricate reasoning steps, including equation solving, math/logical reasoning, and state machine reasoning, we observe relatively low accuracy even when the input is presented in pure text format. These tasks align with areas where the model generally struggles. When the input modality shifts to using images, the proficiency in solving such tasks deteriorates further, resulting in a noticeable drop in performance, despite the fact that the images contain an equal amount of information. This emphasizes the substantial inconsistency in task-solving across modalities and highlights the model's superior ability in one modality (Language) compared to the other (Vision).\nOn the other hand, for tasks primarily focused on extracting information from provided content and comprehending that information, such as Language Understanding and Table Understanding, we witness near-perfect performance when the model is prompted with text input. However, a more significant drop in accuracy (up to 90%) is observed in such tasks when the input modality shifts to images. This indicates that the change in modality significantly impacts the model's processing capabilities, providing strong evidence of the inconsistency of the model.\nIn conclusion, in multimodal systems like GPT-4V, the language modality demonstrates a dominant advantage over vision modality, when tasks are tackled in text format, despite the presence of the same information in image format. This strongly suggests a non-consistent cross-modal behavior within the network. While each modality exhibits varying levels of task-solving and reasoning capabilities, the inconsistency across modalities is observed across tasks regardless of the accuracy level of each modality for the task in hand."}, {"title": "5.4 Ablation Study on Content Extraction from Images", "content": "As solving tasks in image format inevitably requires accessing essential information from the images, we conducted additional experiments to investigate whether the performance gap is attributable to the model's inability to access information. To address this, we conducted one-step Optical Character Recognition (OCR) using the model's own network on all instances of tasks that exhibited a significant performance gap between image and text. Specifically, for each image input (indicated by the red arrow in Table 1), we prompt the model with the instruction 'extract the exact content in the image' and compare the results with the original input to determine if they match. This approach allows us to eliminate the possibility that the performance issues in image format are due to the model's inability to correctly recognize the input.\nAs shown in Table 2, OCR accuracy approaches nearly 100% for all instances of LogicQA, MMLU, and Table Understanding tasks. This suggests that the model faces no difficulties in accurately extracting information, such as numbers from each row and column in table images. The substantial gap (up to 90%) in accuracy (Table 1) between images and text can be attributed solely to the model's internal reasoning processes for each modality. This underscores the inconsistent internal reasoning employed by the model when presented with the same content in different modalities.\nIn contrast, we observe lower OCR accuracy for Math equation-solving inputs, as complex math equations pose challenges for accurate recognition and extraction. To isolate and distinguish the source of inconsistency \u2013 inaccurate recognition of image data or poor actual internal reasoning, we report conditional consistency scores for image instances given correct versus incorrect OCR results. From Table 3, it becomes evident that there is no direct correlation between consistency scores and direct OCR accuracy. This further bolsters our claim that such models simply exhibit distinct (and inconsistent!) internal behaviors under different modalities."}, {"title": "6 Vision-Depicting-Prompting (VDP)", "content": "As shown in Section 5.3, for the same task, VLLMs such as GPT-4V perform much better when questions are presented in text format, even when the information can be completely extracted from the image instances. Inspired by these findings, we propose a novel method of Vision-depicting-prompting (VDP) for improving model's reasoning ability through image context. We now explain VDP."}, {"title": "6.1 Prompting Details", "content": "In the case of a task instance presented in image format, VDP diverges from directly soliciting an answer solely based on the image input, as illustrated in Figure 4. Instead, we adopt a two-step process: we first prompt the model to extract and articulate the description of the image task using textual language. This aims to maximize the transformation of the image signal into a text signal, recognizing the inherently stronger reasoning abilities associated with text information, as demonstrated earlier. Subsequently, we prompt the model to provide an answer, taking into account both the text description of the task and the original image input, as depicted in Figure 4.\nUnlike previous research that sought to enhance the reasoning abilities of multimodal models by augmenting input images with supplementary text (Lin et al., 2022; Hu et al., 2023; Zhang et al., 2023c), VDP does not focus on information augmentation. Particularly in the task instances designed for our study, images already contain all the necessary information required to complete the task. Therefore, converting these images into text format does not provide any additional information that aids in solving the task. Instead, VDP is rooted in the observation that textual signals can significantly stimulate a model's reasoning capability as model has a bias towards language modality. Instead, VDP is based on the observation that textual signals can significantly stimulate a model's reasoning capability, given the model's inherent bias toward the language modality. VDP achieves this by explicitly extracting textual information from the images, thus directly leveraging the model's language processing capabilities more effectively."}, {"title": "6.2 Experiment Results for VDP", "content": "We apply VDP to five of the tasks previously examined in Section 5, where these tasks demonstrate notable performance disparities between image and text inputs. We therefore investigate whether VDP can effectively bridge the performance gap between modalities on such tasks. The outcomes are detailed in Table 4.\nRemarkably, we observe a substantial improvement in accuracy exceeding 12% when solving problems within the realm of vision modalities using VDP, as compared to naive prompting. In tasks requiring reasoning abilities, we note an average accuracy enhancement of 19%. However, the overall performance still lags behind that of text-based prompting. This discrepancy can likely be attributed to the challenges in accurately depicting and extracting information from objects within images during VDP. In contrast, an impressive average increase of 57% in accuracy is observed in tasks centered around understanding (TU and MMLU). Particularly, in the case of table understanding, we witness a remarkable 90% boost in accuracy, particularly when the table's content is extracted before any necessary calculations are applied. For these tasks, we find that performance eventually reaches parity with text-based prompting, underscoring the effectiveness of VDP, particularly in tasks that involve a deeper understanding of the information within the input instances.\nFurthermore, there is a substantial increase in the consistency score with VDP compared to prompting with plain images (naive prompting), e.g., from 0.64 to 0.80 on LogicQA and from 0.10 to 0.90 on TU. These results reinforce our hypothesis that models such as GPT-4V exhibit varied and often inconsistent reasoning capabilities across different modalities and underscore the effectiveness of our VDP approach for enhancing consistency. Properly addressing such disparities between modalities as done by our VDP approach can also help to improve the performance in solving the tasks."}, {"title": "7 Conclusion", "content": "In this study, we performed a systematic analysis of the consistency across modalities in multimodal systems. Our results demonstrate that models such as GPT-4V maintain a relatively independent internal representation of reasoning between visual and textual signals, as evidenced by results we obtained on our datasets which we specially designed for the tasks. Notably, GPT-4V exhibits superior performance in language modeling compared to reasoning within a visual context. These findings offer valuable insights into the potential applications of such multimodal systems and highlight the need for more integrated system designs. Furthermore, we introduce a Vision-depicting-Prompting solution to effectively address this inconsistency."}, {"title": "Limitations", "content": "While our method is straightforward and effective in revealing inconsistency across modalities, it does encounter challenges when applied to certain existing tasks. Obtaining an information-preserving converter from one modality to another can prove difficult for specific tasks, such as detecting emotions from speech. Consequently, we may not always be able to readily convert the modality of every given dataset and evaluate the cross-modal consistency of these tasks. However, it is important to note that this limitation should not undermine the value of our approach. Our method provides a general framework for assessing cross-modal behavior, and there exist numerous tasks that can be easily converted across modalities without any loss of information, as demonstrated in our constructed datasets. By testing on such tasks, we can gain a comprehensive understanding of a model's cross-modal behavior."}, {"title": "Ethical Consideration", "content": "Our exploration of modality consistency serves as a valuable means to enhance the transparency of multimodal models and gain a profound comprehension of their behavior. By delving into the alignment of model responses across diverse modalities, we uncover intricate insights into the decision-making processes and rationale behind their actions. This comprehensive understanding not only instills confidence in the outcomes generated by these models but also significantly enhances their overall interpretability. Transparency in this context becomes essential not only for establishing trust when these models are integral to pivotal decision-making processes but also for addressing ethical and societal implications. As we unravel the intricacies of multimodal reasoning, it underscores the necessity for continuous ethical contemplation and the implementation of proactive measures to address potential challenges arising from advanced multimodal models."}]}