{"title": "SPIRE: Synergistic Planning, Imitation, and Reinforcement for Long-Horizon Manipulation", "authors": ["Zihan Zhou", "Animesh Garg", "Dieter Fox", "Caelan Garrett", "Ajay Mandlekar"], "abstract": "Robot learning has proven to be a general and effective approach for programming manipulators. Imitation learning is able to teach robots solely from human demonstrations but is bottlenecked by the capabilities of the demonstra-tions. Reinforcement learning uses exploration to discover better behaviors; how-ever, the space of possible improvements can be too large to start from scratch. And for both approaches, the learning difficulty increases exponentially to the length of the manipulation task. Accounting for this, we propose SPIRE, a sys-tem that first uses Task and Motion Planning (TAMP) to decompose tasks into smaller learning subproblems and second combines imitation and reinforcement learning to maximize their strengths. We develop novel strategies to train learning agents when deployed in the context of a planning system. We evaluate SPIRE on a suite of long-horizon and contact-rich robot manipulation problems. We find that SPIRE outperforms prior approaches that integrate imitation learning, rein-forcement learning, and planning by 35% to 50% in average task performance, is 6 times more data efficient in the number of human demonstrations needed to train proficient agents, and learns to complete tasks nearly twice as efficiently. View https://sites.google.com/view/spire-corl-2024 for more details.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) is a powerful tool that has been widely deployed to solve robot ma-nipulation tasks [1, 2, 3, 4]. The RL trial-and-error process allows an agent to automatically discover solutions to a task and improve its behavior over time. However, in practice, it often relies on care-ful reward engineering to guide the exploration process [5, 6]. The exploration burden and reward engineering problem is even more challenging to overcome for long-horizon tasks, where an agent must complete several subtasks in sequence in order to solve the task [7]. Imitation Learning (IL) from human demonstrations [8, 9] is a popular alternative to reinforcement learning. Here, humans teleoperate robot arms to collect task demonstrations. Then, policies are trained using the data. This alleviates the burden of reward engineering, since correct behaviors are directly specified through demonstrations. This paradigm has recently been scaled up by collecting large datasets with teams of human operators and robots and shown to be effective for different real-world manipulation tasks [10, 11, 12]. While these agents can be effective, they typically are imperfect, with respect to both success rates and control cost, and not robust to different deployment conditions, especially when it comes to long-horizon tasks [13]. One way to integrate the benefits of both IL and RL is to first train an agent with IL and then finetune it with RL. This can help improve the IL agent and make it robust through trial-and-error, while also alleviating the need for reward engineering due to the presence of the demonstrations. Several works have used this paradigm successfully, but long-horizon manipulation still remains challenging due to the burden of exploration and long-term credit assignment [7]."}, {"title": "2 Related Work", "content": "Hierarchical approaches for long-horizon tasks. Hierarchical approaches decompose the chal-lenging long-horizon tasks into easier-to-solve subtasks. RL based methods explore the division of sub-tasks with reusable skills [17, 18, 19, 20, 21]. [22, 23, 24, 25, 26] build hierarchical RL solutions with subpolicies and metacontrollers. Our work instead leverages a planner that provides guidance on which policies to learn as well as initial and terminal state distributions of tasks, compared to bottom-up HiRL methods, which tend to be data inefficient. Notably this top-down breakdown may also be achieved with a Language Model which can provide a plan composed of steps and sub-goal targets [27, 28, 29, 30, 31, 15]. Robot manipulation with demonstrations. Behavior cloning (BC) [32] learns a policy by directly mapping observations to actions and is typically trained end-to-end using pre-collected pairs of ob-servation and behavior data. While this is seemingly a supervised learning problem, the context of robotics adds challenges. BC datasets tend to contain data sampled from multimodal distributions, due to intra-expert variations. Recent work address this problem with implicit models including those derived from energy-based models [33, 34], diffusion models [35, 36, 37, 38, 39], and trans-former models [40, 41, 42]. Another challenge is the correlation in sequential data, which can lead to policies which are susceptible to temporally correlated confounders [43]. Recently several works have set out to handle this by predicting action chunks. For example, the Action Chunking Transformer (ACT) line of work [44, 45] and diffusion policy [35]. While BC-based methods com-bined with high-capacity models enable complex robotics tasks from demonstrations, problems in robustness and long-horizon generalization remain. Experts and their demonstrations can be used in combination with RL in multiple ways, including acting as task specifications, exploration guidance, and data augmentation. Inverse RL [46, 47, 48] learns a reward model for RL from demonstrations; [49, 50, 51] explore using demonstrations to bootstrap the reinforcement learning process; [52] uses state matching for reward computation in RL. [53] shares a similar setup with ours, where they also warmstart RL with a BC policy and use a masked BC loss the constrain the RL policy from deviating. [54, 55] propose to fine-tune a semi-expert initial policy by training a residual policy on top of it with RL. Different from the mentioned works, we focus on multi-stage robotic manipulation tasks with high-dimensional input spaces. Learning for Task and Motion Planning. Task and Motion Planning (TAMP) is a search and planning-based approach for sequential manipulation tasks [16, 56, 57, 58]. However, pure TAMP-based methods suffer from reliance on accurate environment modeling and prior knowledge of the skills, making those methods less suitable for tasks involving complex contact-rich skills."}, {"title": "3 Method", "content": "Our approach Synergistic Planning Imitation and REinforcement (SPIRE) learns and deploys closed-loop visuomotor skills within a TAMP system (see Fig. 1). First we frame our problem as a policy learning problem across a sequence of Markov Decision Processes (Sec. 3.1). Next, we describe our approach for incorporating both classical and learned robot skills into TAMP (Sec. 3.2) to enable TAMP-gated learning. Next, we describe how we train an initial agent with TAMP-gated Behavioral Cloning (BC) (Sec. 3.3). We then propose an RL-based finetuning algorithm to improve the BC agent with RL (Sec. 3.4). Finally, we introduce a parallelized training scheduler that is able to intelligently manage dependencies among stages when conducting RL in our setting (Sec. 3.5)."}, {"title": "3.1 Problem Formulation", "content": "In our setup, each robot manipulation task can be decomposed into a series of alternating TAMP sections and handoff sections, where TAMP delegates control to a trained agent \u03c0. These sections are TAMP-gated [14], as they are chosen at the discretion of the TAMP system, and typically involve skills that are difficult to automate with model-based planning. We wish to train an agent to complete these handoff sections efficiently and reliably. We model our TAMP-gated policy learning problem as a series of Markov Decision Processes (MDPs), $M := (S, A, T, \\{r^i\\}, \\{p_0^i\\}, \\gamma)_{i=1}^N$, where N is the number of MDPs (each corresponds to a TAMP handoff section), S and A are the state and action space, T is the transition dynamics, $r^i(s)$ and $p_0^i$ are the i-th reward function and initial state distribution, and \u03b3 is the discount factor. The start and end of each handoff section is chosen by TAMP, consequently, TAMP determines the initial state distribution $p_0^i$ for each handoff section, and the reward function $r^i(s)$, which is a sparse 0-1 success reward based on the successful completion of the section. Our goal is to train a stochastic policy $\u03c0 : S \u2192 A$ that maximizes the expected return $J(\u03c0) := E_{(s_0, a_0)\u223c\u03c0}[\\sum_{t=0}^H \\gamma^t r_t]$. We next describe the TAMP system."}, {"title": "3.2 TAMP with Learned Skills", "content": "Task and Motion Planning (TAMP) [16] is a model-based approach for synthesizing long-horizon robot behavior. TAMP integrates discrete (symbolic) planning with continuous (motion) planning to plan hybrid discrete-continuous manipulation actions. Essential to TAMP is a model of the ac-tions that a planner can apply and how these actions modify the current state. From such a model, TAMP solvers can search over the space of plans to find a sequence of actions and their associated parameters that satisfies the task. In SPIRE, we seek to implicitly learn a select set of TAMP actions that are impractical to manually model and then combine them with traditional actions through planning. In essence, our strategy is to learn policies that control the system from TAMP precondition states to postcondition states (typically described by effects). We adopt the modeling strategy introduced by Mandlekar et al. [14] and deploy PDDLStream [58] to solve each TAMP problem. See Appendix G for a summary of our planning model. Fig. 2 visualizes an interleaved execution of TAMP trajectory control and RL-learned policy control in the Tool Hang domain. Here, the planning model explicitly models the pick as well as intermediate move actions but defers the insert and hang actions to the RL agent. Algorithm 1 describes the SPIRE policy at test time. It begins by observing the state. If the state sat-isfies the task's goal conditions, SPIRE terminates successfully. Otherwise, SPIRE invokes TAMP to plan a sequence of traditional and learned actions. SPIRE executes the trajectory associated with each traditional action until it reaches the first learned action. At that time, SPIRE executes the closed-loop policy associated with the learned action until it achieves its subgoal condition. To account for the stochastic outcome of the policy, SPIRE replans and repeats this process."}, {"title": "3.3 TAMP-Gated Imitation Learning", "content": "TAMP-Gated Data Collection. We collect an initial dataset of human demonstrations through TAMP-gated human tele-operation, where the human operator collects demonstrations for handoff sections when prompted by TAMP, to form the demonstration dataset $D = \\{(s_t, a_t)_{t=1}^{H_i}, g_i\\}$, where $s_t \\in S$, $a_t \\in A$ and $H_i$ is the horizon, and $g_i$ is the handoff section of the i-th trajectory. To improve the data collection efficiency, we replicate the task queuing system from [14]. TAMP-Gated Behavioral Cloning. Given the dataset D, we train a Behavioral Cloning (BC) policy parame-terized by \u03c6 to minimize the negative log-likelihood loss over the demonstration dataset: $\\phi^*= argmin_{\\phi} E_{(s,a)\u223cD}[\u2212log \u03c0_\u03c6(a|s)]$. The trained BC agent $\u03c0_\u03c6$ may have substantial room for improvement, depending on the complexity of the task, and the number of demonstrations available for training. We next describe our RL-based finetuning procedure (Sec. 3.4) that allows this agent to be improved through reinforcement learning."}, {"title": "3.4 RL Finetuning", "content": "Given a trained BC agent $\u03c0_\u03c6$, we wish to train an RL agent $\u03c0_\u03b8$ to improve performance further. To avoid reward engineering, we only assume access to sparse 0-1 completion rewards for each hand-off section provided by TAMP (Sec. 3.1). However, exploration in sparse-reward settings has been shown to be challenging [59, 60, 61, 62], especially in continuous state and action spaces. Fortu-nately, we can use the BC policy trained in the previous section as a reference point for exploration we want to restrict the behavior of the RL policy to be in a neighborhood of the BC policy. This is achieved by a) warmstarting the RL policy optimization using the BC policy, and b) enforcing a constraint on the deviation between the RL policy and the BC policy."}, {"title": "Warmstarting RL optimization with BC.", "content": "We tested two ways to warmstart the RL agent. Initialization. One method is to initialize the weights of the RL agent with those of the trained BC agent, $\u03b8 \u2190 \u03c6^*$, where $\u03c6^* = arg min_\u03c6 L_{BC}(\u03c6)$, and subsequently finetune the weights with online RL objectives. Despite being easy to implement, this can be less flexible since it requires the agent structure of the RL and BC policies to match. Furthermore, researchers have found that retraining neural networks with different objectives can cause the network to lose plasticity [63], which can make the policy harder to optimize because of the objective shift from BC to RL. Residual Policy. An alternative way is to fix the BC policy as a reference policy and train a residual policy on top of it. Let the residual policy be $\u03c0_\u2020(s)$. The residual policy shares the same action space as the normal policy but is initialized to close to zero. The final action is defined as a summation of the reference action $a \u223c \u03c0_{\u03c6^*}(s)$ and the residual action $a+ \u223c \u03c0_\u2020(s)$. In practice, we only add the mean of the reference policy to the residual action instead of sampling the reference action."}, {"title": "Constraining Deviation between BC and RL agents.", "content": "The sparsity of reward signals produces high-variance optimization objectives, which can lead the RL policy to quickly drift away from BC and lose the exploration bonus from warmstarting. Therefore, it is critical to constrain the policy output to be close to the BC agent throughout the training process. We achieve this by imposing a KL-divergence penalty. We conclude our RL optimization objective as follows: $J_{FT}(\u03b8) := J(\u03c0_\u03b8) \u2212 \u03b1D_{KL}(\u03c0_\u03b8 || \u03c0_{\u03c6^*})$, where $D_{KL}(p||q) := E_{(s,a)\u223cp} [log(\\frac{p(s,a)}{q(s,a)})]$ and \u03b1 is the weight for the penalty term."}, {"title": "3.5 Multi-Worker Scheduling Framework", "content": "Making our TAMP-gated framework compatible with modern reinforcement learning procedures requires addressing several challenges. First, TAMP can take dozens of seconds for a single rollout, which severely lowers the throughput of RL exploration. Second, the TAMP pipeline executes each section sequentially, which means that later handoff segments can only be sampled when previous handoff segments are completed successfully. This leads to an imbalance of episodes for the differ-ent handoff segments and is potentially problematic for the RL agent. We propose a multi-worker TAMP scheduling framework to integrate TAMP into RL fine-tuning. The framework consists of three components a group of TAMP workers that run planning in parallel, a status pool that stores the progress of the workers, and a scheduler that distributes tasks to the workers and balances the initial states. We further describe how the framework allows for curriculum learning, and how the framework accelerates learning efficiency for RL training. See Appendix H for more details. TAMP workers. Each TAMP worker has an environment instance and repeatedly runs a TAMP planner. Upon reset, the TAMP worker initiates TAMP until a handoff sec-tion has been reached. It then sends a pair (#worker, #section) representing its ID and which handoff section it has entered to a FIFO status queue, indicating that it is ready to take RL agent actions. The worker then enters an idle state until it receives a command from the scheduler."}, {"title": "Scheduler.", "content": "(Algorithm 2) The scheduler is a centralized component that manages the TAMP work-ers. It also provides an environment abstraction to the single-threaded RL process. The scheduler is configured with a sampling strategy. Upon initialization, it first pops an item from the status queue. According to the sampling strategy, the scheduler either rejects this section, in which case it sends a resetting signal to the corresponding worker; or starts a new episode and interacts with the worker."}, {"title": "Curriculum Learning.", "content": "The behavior of the scheduler depends on a sampling strategy, allowing it to function as a curriculum for the RL agent. We consider two strategies: permissive is the default strategy that allows all sections through, while sequential only accepts a section when the success rate of passing all the previous sections reaches a threshold. sequential allows controlling the initial state distribution during the early stages of training, to ensure the RL agent achieves proficience in each section sequentially before continuing onto the next section."}, {"title": "Remarks on Efficiency.", "content": "Suppose a TAMP planning process takes at most T seconds over the episode; each environment interaction step, counting communication latency, takes at least t seconds; and each handoff section is at least H steps. If the number of TAMP workers $n \u2265 \\frac{T+tH}{T}$, the proposed multi-worker TAMP scheduling framework reaches a throughput of at least 1/t frames per second. In comparison, the single-worker counterpart has a worst-case throughput of $\\frac{H}{T+tH}$ frames per second. Suppose that the planning process is slower than the handoff sections by a factor k (e.g. T = ktH), then our framework is faster than the single-worker alternative by a factor of k + 1."}, {"title": "4 Experiments", "content": "Tasks. For evaluation, we follow [14] and choose a set of long-horizon manipulation tasks, namely Square, Coffee, Three Piece, and Tool Hang. We also include the broad variants of those tasks, where we use a broad object initialization region, and Coffee Preparation, which has the longest horizon with four handoff sections. See Appendix E for more details. Environment Details. Observation space. For most tasks, we use a single 84 \u00d7 84 RGB image from the wrist-view camera. For Tool Hang, we use the front-view camera instead since the wrist-view is mostly occluded. For Tool Hang Broad and Coffee Preparation, we use both wrist-view and front-view cameras, as well as proprioception state (end-effector pose and gripper finger width). Action space. Actions are 7-dimensional (3-dim delta end-effector position, 3-dim delta end-effector rotation, 1-dim gripper actuation). Horizon. Each handoff section is limited to 100 steps (5 seconds with 20Hz control frequency) for all tasks, except for Tool Hang Broad, where the limit is 200 steps. Baselines. We compare our method with two baselines: HITL-TAMP-BC (BC), which is adapted from [14] to match our network structure; and TAMP-gated Plan-Seq-Learn (RL), which is adapted"}, {"title": "4.1 Results", "content": "SPIRE outperforms both TAMP-gated BC and RL. We compare our method with the TAMP-gated BC [14] and RL [15] baselines across all 9 tasks (see Fig. 3). SPIRE reaches 80% success rate in 8 out of 9 tasks, while BC and RL only reach 80% in 3 tasks each. In Tool Hang, our method reaches 94% success rate despite the BC counterpart only having 10%, which is over 9-times improvement. Remarkably, this low-performing BC agent is enough to help address the exploration burden (unlike RL, 0% success) and train a near-perfect agent. Across all 9 tasks, SPIRE averages a 87.8% success rate, while BC and RL only average 52.9% and 37.6% respectively. SPIRE produces more efficient agents than BC through RL fine-tuning. SPIRE agents have lower average completion times than their BC and RL counterparts (Fig. 3, right). Even in tasks such as Square, Square Broad, Coffee, Three Piece, where BC policies already have high success rates, our method improves the efficiency by only using an average of 59% completion time. SPIRE's use of the BC agent helps address the RL exploration burden on challenging long-horizon tasks. Exploration in RL with sparse rewards is extremely challenging, especially for robot manipulation tasks for their continuous and high-dimensional observation and action space. Our method solves the initial exploration problem by anchoring policy learning around the BC agent. As shown in Figure 3, RL policies without utilizing BC only reach nonzero success rates in Square, Coffee and their Broad variants, all of which have only one handoff section and relatively shorter horizons. Even in Coffee Broad, RL encounters exploration difficulties due to the broader object distribution, resulting in only partially solving the task. Qualitatively, SPIRE can improve agent behavior without introducing undesirable behavior, unlike RL. Safety awareness has always been a critical matter in robotics learning. Safety con-straints can be hard to define with numerical values, which adds to the challenges of realizing safety in RL. We notice that in Coffee, RL policy has a much shorter completion time than our method. This is at the cost of ignoring safety concerns. We compare two rollouts of RL and our method in Figure 4. The RL-trained policy attempts to close the lid by knocking the coffee machine with the arm, which can potentially damage the robot and the coffee machine and even cause danger to humans; while our method preserves safety awareness by following the demonstration's practice of closing the lid with its fingers. SPIRE can train proficient agents using just a handful of human demonstrations. BC methods can require several human demonstrations to train proficient agents, which can be a major drawback due to the cost of collecting this data [9]. We reduce the number of human demonstrations used by SPIRE to 10 and 50 (instead of 200 as in Fig. 3), and we plot the minimum of demonstrations needed to reach at least 80% success rate in Fig. 5. As the plot shows, SPIRE can successfully fine-tune a BC policy trained with as few as 10 demos in all evaluated tasks except for Tool Hang"}, {"title": "4.2 Ablation Study", "content": "We conduct two ablative studies to investigate (1) the value of the KL-divergence penalty and (2) the value of curriculum learning, governed by the two scheduler sampling strategies permissive and sequential. In this section, we compare the performance distribution of the 5 runs instead of only the top-1 run for a more comprehensive evaluation. Value of divergence penalty. We ablate the divergence penalty on two representative tasks, Three Piece & Tool Hang (Table 6) and observe a drastic performance drop (84% to 17.6%, 74% to 0%). The sparsity in rewards leads to high-variance opti-mization objectives for RL. As a result, even when warmstarted with BC, the RL policy can quickly de-viate from it, especially when the chance of reach-ing the reward signal is low. Therefore, constraining the policy close to BC throughout the training is crit-ical. We select two representative tasks, Three Piece and Tool Hang for this ablation. The result is shown in Figure 6. Without the divergence penalty, the RL policies deviated immediately and never returned. Value of curriculum learning. We compare the two sampling strategies in Tool Hang task. The re-sult is shown in Figure 5. sequential strategy shows a much smaller variance, while permissive produces the better top-1 seed performance. The main difference between the two strategies is how the second section states emerge during training. For permissive, the second section states emerge gradually as the success rate of passing the first section gets higher, resulting in a more gentle dis-tributional shift that leads to a higher overall success rate; for sequential, the shift is more abrupt, but it has fewer distraction states in the early stage, resulting in a more stable training process."}, {"title": "5 Conclusion", "content": "We presented SPIRE, an integrated approach for deploying RL, BC, and planning harmoniously. We showed how BC can be used to not only warm-start RL but also guide the RL process via focused exploration. We introduced a scheduling mechanism to improve RL data throughput and in-crease learning efficiency. Finally, we evaluated SPIRE in simulation against recent hybrid learning-planning baselines and found that SPIRE results in more successful and efficient policies. Limitations. We focus on tasks that center around object-centric manipulation of rigid objects in table-top environments. We assume that a human teleoperator can demonstrate the learned skills to warmstart RL. The TAMP component assumes that the state is observable and comprised of rigid objects, possibly connected with articulation. To simplify RL training, we only considered Markovian policies; however, using neural network architectures with history, such as RNNs, may boost performance [9]. Finetuning BC policy with RL requires the simulation to be efficient."}, {"title": "A Overview", "content": "The Appendix contains the following content. \u2022 Policy Learning Details (Appendix B)): details on hyperparameters used \u2022 Ablation: SPIRE without TAMP (Appendix C): ablation study on the effect of removing TAMP-gating and directly running BC and RL fine-tuning \u2022 Comparison to Additional Methods (Appendix D): comparison to other RL methods that leverage demonstrations \u2022 Tasks (Appendix E): details on tasks used to evaluate SPIRE \u2022 Variance Across Seeds (Appendix F): discussion on the variance of results across different seeds and how results are presented \u2022 TAMP Formulation(Appendix G): details on the TAMP planner \u2022 Bridging TAMP Planner and RL (Appendix H): details on how we integrate the TAMP planner with RL \u2022 Ablation: SPIRE without Multi-Worker (Appendix I): ablation study on the effect of using multiple parallelized TAMP workers \u2022 Additional Experiment Results (Appendix J): additional experiment results, including RL learning curves"}, {"title": "B Policy Learning Details", "content": "Hyperparameters. The base RL algorithm for all our experiments is DrQ-v2 [64]. The specific hyperparameters are in Table 1. Observation. For most tasks, we use one 84 \u00d7 84 RGB image from the wrist camera as the only observation. For Tool Hang, we use a front-view camera instead since the wrist-view is heavily occluded. For Tool Hang Broad and Coffee Preparation, we use both camera views plus proprio-ception state (end-effector pose and gripper finger width). We use the default CNN structure from DrQ-v2 to encode the image observations. For tasks with multiple observations, we first encode the image observations each with an independent CNN network, then concatenate the CNN outputs alongside the low-dimensional observations such as proprioception states to form the feature vector. Action. All of our tasks share a 7-dimensional continuous action space. It is models 6-DOF delta movement of the end-effector along with 1 dimension for finger control. The action is modeled as a normal distribution with a scheduled standard deviation."}, {"title": "C Ablation: SPIRE without TAMP", "content": "We provide an additional ablation study on the high-level planner, TAMP. To do so, we treat the whole task as one handoff section. The agent only receives a reward of one if it completes the whole task. We collect 200 full demonstrations in Square, train a BC policy, and apply SPIRE to fine-tune the BC policy. Since the trajectory becomes longer and the robot now needs to handle object transportation, a single local wrist-view becomes insufficient. We thus include both the wrist view and the global front view, as well as the robot proprioception states in the observation for the w/o TAMP variant. The result is shown in Table 2. Even though the w/o TAMP variant has more information from observations, the BC and RL policies are significantly worse than the w/ TAMP counterpart. The increased horizon makes the BC policy easier to drift away to regions less frequently visited in demonstrations and makes RL exploration much harder. In Square, despite the low starting quality, SPIRE still fine-tunes BC to reach a 94% success rate, demonstrating the effectiveness of RL fine-tuning. However, when the initialization range increases in Square Broad, even SPIRE fails to find an acceptable policy. In conclusion, TAMP (1) confines the agent-controlled section to a small local area, reducing the need for global information, and (2) decreases the horizon (11.6 w/ TAMP, 101.7 w/o TAMP in Square) for the learned agent, reducing compounding errors and exploration difficulty."}, {"title": "D Comparison to Additional Methods", "content": "In each handoff section from TAMP, SPIRE utilizes the demonstrations by training a behavior cloning agent and using RL to fine-tune it. There are alternative methods to combine expert demon-strations and RL, which can be readily plugged in as replacements to SPIRE. In this section, we make connections from our method to GAIL [46]. The discriminator-based IRL reward in GAIL serves the same purpose as our KL penalty term - preventing the current policy from deviating from the expert policy. We draw further connection by showing that our KL penalty is the same as the IRL reward function in GAIL with an alternative discriminator objective and a different reward form. Let $\u03c0_E$ be the expert policy. The IRL reward function in GAIL is \u2013 log(1 \u2013 D(s, a)), where D : S \u00d7 A \u2192 [0, 1] is the discriminator that maximizes $J(D) := E_{\u03c4\u223c\u03c0_E}[log(1 \u2013 D(s, a))] + E_{\u03c4\u223c\u03c0}[log(D(s,a))]$ If we use an alternative objective: $\\widehat{J}(D) := E_{s\u223c\u03c0_E, a\u223cUnif}[-D(s, a)] + E_{\u03c4\u223c\u03c0}[log(D(s, a))]$ The alternative objective discriminates $\u03c0_E$ from a fixed policy rather than the current learned policy \u03c0. Assume $\u03c0_E$ has full support, then maximizing $\\widehat{J}(D)$ is equivalent to maximize for every s \u2208 S: $\\widehat{J}_s(D) :=E_{a\u223cUnif}[-D(s, a)] + E_{a\u223c\u03c0_E(\u00b7|s)} [log(D(s, a))]$  = - (\\int D(s, a) da) + (\\int \u03c0_E(a | s)log(D(s, a)) da) (1) = - (\\int D(s, a) da) + (\\int \u03c0_E(a | s)log \u03c0_E(a | s) da) + (\\int \u03c0_E(a | s) log da) (2) = - (\\int D(s, a) da ) + H(\u03c0_E(\u00b7 | s)) + (\\int \u03c0_E(a | s) log da) (3)  < - (\\int D(s, a) da ) + H(\u03c0_E(\u00b7 | s)) + (\\int \u03c0_E(a | s) - 1) da) (4)  = - (\\int D(s, a) da ) + H(\u03c0_E(\u00b7 | s)) + (\\int D(s, a) da) - (\\int \u03c0_E(a | s) da) (5) = H(\u03c0_E(\u00b7 | s)) \u2013 1. (7) where H is the entropy. (5) holds since log x \u2264 x \u2212 1 for all x > 0, and only equates when x = 1, i.e., D(s, a) = $\u03c0_E(a | s)$. Since (7) is a constant, the maximum of $\\widehat{J}(D)$ can be taken when (5) equates, which means the optimal solution of $\\widehat{D}$ is D(s, a) = $\u03c0_E(a | s)$. Our KL penalty then is equivalent to using an IRL reward of log(D(s, a)) = log $\u03c0_E(a | s)$."}, {"title": "E Tasks", "content": "Square and Square Broad. The robot must pick up a nut and place it onto a peg. This task has 1 handoff section, where the learned agent places the nut. The Broad version increases the initialization range of both the nut and the peg. Three Piece and Three Piece Broad. The robot must insert one piece into a base and place another piece on top of the first. This task has 2 handoff sections, where the learned agent places the two pieces. The Broad version increases the initialization range of all three pieces including the base. Tool Hang and Tool Hang Broad. The robot must first insert a L-shaped piece into a base to assemble a frame, then hang a wrench off of the frame. This task has 2 handoff sections, where the learned agent inserts the L-shaped piece and hangs the wrench. The Broad version increases the initialization range of all three pieces (base, L-shaped hook, and wrench)."}, {"title": "Coffee and Coffee Broad.", "content": "The robot must pick up a coffee pod, insert it into a coffee machine, and close the lid. This task has 1 handoff section where the learned agent inserts the pod and closes the lid. The Broad version increases the initialization range of the pod and the coffee machine."}, {"title": "Coffee Preparation.", "content": "This is an extended version of Coffee. The robot must place a mug onto the coffee machine, open the lid, open the drawer with the coffee pod, pick up the pod, insert the pod into the coffee machine, and close the lid. This task has 3 handoff sections where the learned agent (1) places the mug and opens the lid, (2) opens the drawer, and (3) inserts the pod and closes the lid."}, {"title": "F Variance Across Seeds", "content": "In Figure 3, we show the best run out of 5 seeds. Here we provide the mean and standard deviation of the success rates in Table 3. We observe that although SPIRE still outperforms BC in terms of mean success rate in most of the tasks, our method exhibits unusually high variances in some of the tasks, for example, Coffee, Three Piece, and Tool Hang. In those tasks, one or more runs result in a performance significantly lower than the rest. Specifically, \u2022 In Coffee, one run has 40% success rate, while the rest are all 100%; \u2022 In Three Piece, one run has 22% success rate, while the rest are at least 98%; \u2022 In Tool Hang, one run has 0% success rate and one has 6%, while the rest are at least 82%. Reinforcement learning methods are known to have high variances, especially in sparse reward settings. SPIRE partially alleviates this problem by enforcing the KL penalty for deviating from an anchor policy. However, in practice, such deviation can still happen. Figure 8 compares the training curve of a successful run (with 88% final success rate) and a failed run (with 0% final success rate). The policy in the failed run drastically deviated from the BC policy early on in the training. This is likely related to the unusually large policy gradient loss, which the KL penalty term was unable to match and failed to constrain the policy. In our experiments, such an abrupt decrease in policy gradient loss happens frequently, with varying scales and timing, causing the training results to have high variance. Using an adaptive weight of the KL penalty might be a potential solution, which we wish to investigate in future work. We do not believe 5 seeds are enough to quantitatively reflect the chance of such sudden deviation happening. An alternative solution would be to compare only the results where such deviation did not happen, which is why we chose to report the top-1 performing seed in our main paper."}, {"title": "G TAMP Formulation", "content": "We specify TAMP formulations in"}]}