{"title": "Fine-Grained Sentiment Analysis of Electric Vehicle User Reviews: A Bidirectional LSTM Approach to Capturing Emotional Intensity in Chinese Text", "authors": ["Shuhao Chen", "Chengyi Tu"], "abstract": "As the electric vehicle (EV) industry and its associated charging infrastructure continue to expand rapidly, user reviews have become a crucial resource for evaluating product performance, user experience, and the effectiveness of charging solutions\u00b9. However, traditional sentiment analysis models typically rely on simplistic binary classifications (e.g., \"positive\" or \"negative\") or multi-class categorizations (e.g., \"positive,\" \"neutral,\" and \"negative\")\u00b2. While these methods may be adequate in straightforward contexts, they often fail to capture the complexity of emotional expression and the varying degrees of emotional intensity found in user reviews\u00b3.\nFurthermore, user reviews frequently reflect multi-dimensional emotional responses, with users expressing different levels of intensity for various aspects of the same product. For example, a reviewer may express strong satisfaction with a vehicle's design while simultaneously highlighting dissatisfaction with its range or charging speed4. Traditional sentiment classification approaches tend to oversimplify these nuanced emotions into rigid categories, such as \"positive\" or \"negative,\" thus failing to accurately represent the full spectrum of emotional intensity and complexity. In contrast, sentiment scoring models offer a more refined approach by assigning quantitative scores to user sentiment, enabling a more detailed understanding of user experiences.\nBy adopting sentiment scoring models, automotive and charging infrastructure providers can conduct more granular analyses of user feedback, allowing them to pinpoint specific strengths and weaknesses of their products and implement more targeted improvements in both product design and service offerings. As a result, sentiment scoring models are emerging as a more advanced and effective method for sentiment analysis.\nThis study proposes the development of a sentiment scoring model based on Bidirectional Long Short-Term Memory (Bi-LSTM) networks to analyze the emotional intensity of user reviews related to EV charging infrastructure\u00b3. Unlike traditional sentiment classification models, the proposed model assigns a sentiment score ranging from 0 to 5 to each review, thus providing a more precise quantification of emotional expression. This approach aims to capture the subtleties of user sentiment, offering a more accurate and meaningful analysis of user experiences.", "sections": [{"title": "Introduction", "content": "As the electric vehicle (EV) industry and its associated charging infrastructure continue to expand rapidly, user reviews\nhave become a crucial resource for evaluating product performance, user experience, and the effectiveness of charging\nsolutions\u00b9. However, traditional sentiment analysis models typically rely on simplistic binary classifications (e.g., \"positive\"\nor \"negative\") or multi-class categorizations (e.g., \"positive,\" \"neutral,\" and \"negative\")2. While these methods may be\nadequate in straightforward contexts, they often fail to capture the complexity of emotional expression and the varying\ndegrees of emotional intensity found in user reviews\u00b3.\nFurthermore, user reviews frequently reflect multi-dimensional emotional responses, with users expressing different\nlevels of intensity for various aspects of the same product. For example, a reviewer may express strong satisfaction with a\nvehicle's design while simultaneously highlighting dissatisfaction with its range or charging speed4. Traditional sentiment\nclassification approaches tend to oversimplify these nuanced emotions into rigid categories, such as \"positive\" or \"negative,\"\nthus failing to accurately represent the full spectrum of emotional intensity and complexity. In contrast, sentiment scoring\nmodels offer a more refined approach by assigning quantitative scores to user sentiment, enabling a more detailed\nunderstanding of user experiences.\nBy adopting sentiment scoring models, automotive and charging infrastructure providers can conduct more granular\nanalyses of user feedback, allowing them to pinpoint specific strengths and weaknesses of their products and implement\nmore targeted improvements in both product design and service offerings. As a result, sentiment scoring models are\nemerging as a more advanced and effective method for sentiment analysis.\nThis study proposes the development of a sentiment scoring model based on Bidirectional Long Short-Term Memory\n(Bi-LSTM) networks to analyze the emotional intensity of user reviews related to EV charging infrastructure\u00b3. Unlike\ntraditional sentiment classification models, the proposed model assigns a sentiment score ranging from 0 to 5 to each review,\nthus providing a more precise quantification of emotional expression. This approach aims to capture the subtleties of user\nsentiment, offering a more accurate and meaningful analysis of user experiences."}, {"title": "Method", "content": "Data preparation\nData processing plays a critical role in enhancing model performance and ensuring high training quality. This study\nutilizes user review data from PC Auto, a prominent Chinese platform dedicated to evaluating charging pile performance. A\ncomprehensive suite of processing steps, including data cleansing, tokenization, stop word removal, and serialization, is\nemployed to structure and normalize the input data, thereby aligning it with the requirements of deep learning models.\nData Source\nThe dataset comprises user-generated reviews on PC Auto, which evaluate various aspects. These reviews form a\nfoundational basis for sentiment analysis and are supplemented by user-provided ratings (on a scale of 0 to 5), serving as\nlabels for supervised learning. A total of 43,678 valid reviews are utilized as training and validation samples for the model,\nensuring a robust dataset for analysis.\nData Cleaning\nBefore processing, the raw data undergoes meticulous cleaning to eliminate noise that could negatively impact model\ntraining. The primary objective of these cleaning processes is to ensure the cleanliness and relevance of the input data,\nthereby minimizing noise and enhancing the model's accuracy and generalization capabilities7. The specific steps involved in\ndata cleaning include:\n1. Removing HTML Tags: Reviews often contain embedded HTML code, which is systematically removed to preserve\nonly the relevant textual content.\n2. Eliminating Special Characters: Reviews may include extraneous elements such as emojis, punctuation marks, and\nother non-alphanumeric characters. These elements are excluded, as they do not contribute meaningfully to sentiment\nanalysis.\n3. Deduplication: To ensure data integrity, duplicate reviews submitted by the same user are identified and removed\nfrom the dataset.\nData Preprocessing\nThe primary aim of data preprocessing is to transform the cleaned text data into a structured format that is suitable for\nmodel input. Key steps in this phase include tokenization, stop word removal, serialization, and padding/truncation."}, {"title": null, "content": "1. Tokenization: Unlike English, Chinese text lacks natural delimiters such as spaces, necessitating the use of\ntokenization. The widely adopted Jieba tool is employed to segment sentences into word sequences, as this word-based\napproach better captures the semantics of the text in natural language processing. Tokenization allows the model to conduct\nsentiment analysis based on word sequences rather than individual characters, thus enhancing semantic comprehension.\n2. Stop Word Removal\u00ba: In natural language processing, stop words\u2014high-frequency words such as \u201c\u7684\u201d\u201c\u662f,\u201d and\n\u201c\u5728\u201d\u4e00contribute little semantic value. While these words may aid in sentence structure, they do not facilitate sentiment\nanalysis and can introduce unnecessary noise. By removing stop words, redundancy in the data is reduced, enabling the\nmodel to focus on sentiment-relevant vocabulary. The Baidu stop word list is utilized for this purpose.\n3. Serialization 10: The tokenized text data requires conversion into a numerical format that is suitable for model\nprocessing. A tokenizer is employed to map each word to a unique integer value based on its frequency, with more common\nwords receiving smaller IDs and rarer words larger IDs. This transformation enables the representation of text data as integer\nsequences for model input. The chosen tokenizer is particularly advantageous for Chinese due to its flexibility in\naccommodating the language's unique characteristics. It dynamically adjusts vocabulary based on word frequency, controls\nvocabulary size to mitigate the impact of rare words, and preserves the sequential information of words to effectively capture\ncontextual semantics.\n4. Padding and Truncation: Long Short-Term Memory (LSTM) models necessitate input sequences of consistent lengths,\nwhile the lengths of actual reviews may vary. To achieve uniformity, reviews shorter than 100 words are padded with zeros,\nand those exceeding this length are truncated11. This approach ensures consistent input lengths, facilitating effective\nprocessing by the LSTM.\n5. Label Normalization: Given that model outputs typically range between 0 and 1, while sentiment ratings span from\n0 to 5, it is essential to normalize the original rating labels prior to training12. This is accomplished by dividing the ratings by\n5, aligning the label range with the model outputs.\nVocabulary Coverage Analysis\nVocabulary coverage analysis is conducted to determine an appropriate vocabulary size that maximizes coverage while\nminimizing the loss of textual information. By calculating coverage rates based on word frequency, we assess the proportion\nof review texts encompassed by varying vocabulary sizes 13. The results reveal that 7,119 words achieve 95% coverage, while\n16,403 words attain 98% coverage (see Fig. 1). Balancing computational efficiency and model complexity, a vocabulary size\nof 7,119 words is ultimately selected to ensure substantial retention of critical information while mitigating the adverse\neffects associated with low-frequency words on model performance."}, {"title": "Model Construction", "content": "The BILSTM model is an extensively adopted neural network architecture in the domain of sentiment analysis. Distinct\nfrom conventional unidirectional LSTMs, BiLSTMs are adept at capturing contextual information in both forward and\nbackward directions simultaneously 14. This capability is particularly vital in sentiment analysis, where emotional expression\nis often heavily influenced by surrounding context. In natural language processing, the sentiment conveyed in a sentence\nfrequently relies not only on the preceding words but also on those that follow. For example, in evaluations of charging piles,\none might encounter a statement like \"The charging pile is very convenient,\" which could conclude with a contrasting remark\nsuch as \"but the charging speed is average.\" If the analysis were to depend solely on forward information, the model could\npotentially overestimate the positive sentiment. By employing a BiLSTM, the model benefits from a comprehensive analysis\nof the sentence's context, enabling it to yield more precise sentiment predictions15.\nModel Architecture Design\nIn the development of a sentiment analysis model leveraging BiLSTM networks15, a hierarchical neural network\narchitecture is meticulously designed to process user-generated reviews and predict corresponding sentiment scores. This\narchitecture encompasses following integral components, each fulfilling a distinct function at various stages of data\nprocessing:"}, {"title": null, "content": "1. Embedding Layer: The model's input consists of integer sequences derived from a tokenizer, where each integer\nsymbolizes a specific word within the review. This layer outputs a fixed-dimensional vector representation (word embedding)\nfor each word, utilizing 128-dimensional embeddings to encapsulate semantic relationships, whereby semantically\nanalogous words are positioned closer together in the vector space. By converting textual data into these semantic vectors,\nthe model is equipped to learn inter-word semantic relationships, thereby enhancing the accuracy of sentiment analysis.\n2. BILSTM Layer: The input to this layer is the sequence of word embeddings. The BiLSTM processes the text in both\nforward and backward directions, thereby extracting emotional features embedded within the reviews. This dual-directional\nprocessing captures contextual dependencies more effectively, with 52 LSTM units configured to extract high-level features\nfrom the reviews. Such a comprehensive approach is particularly advantageous for handling complex and lengthy Chinese\nreviews, facilitating a more precise identification of sentiment orientation.\n3. Dropout Layer: To mitigate the risk of overfitting, especially pertinent in scenarios involving limited datasets, a\nDropout layer is strategically incorporated following the LSTM layer. This mechanism randomly omits connections between\nneurons, thus reducing the model's dependency on specific neurons and enhancing its generalization capabilities. Bayesian\noptimization has determined an optimal Dropout rate of 0.007038, introducing randomness that bolsters the model's\nrobustness when confronted with previously unseen review data.\n4. Dense Layer: The Dense layer's output translates the features extracted by the LSTM layer into a sentiment score.\nUtilizing a single neuron with a linear activation function, this layer produces a real-valued score that reflects the model's\npredicted sentiment rating within a range of 0 to 5. By aggregating the features, this layer ensures that the output\naccommodates continuous values, rendering it suitable for regression tasks.\n5. Loss Function and Optimizer: The Mean Squared Error (MSE) is employed as the loss function, an optimal choice for\nregression tasks as it effectively quantifies the discrepancy between predicted and actual scores, thereby directing the\nmodel's optimization efforts toward enhanced accuracy. The Adam optimizer is utilized, combining momentum with\nadaptive learning rates to facilitate rapid convergence while minimizing the potential for entrapment in local optima.\nIn summary, the proposed model architecture is carefully designed to address the specific demands of sentiment\nanalysis in Chinese text. The Embedding layer provides rich semantic representations, while the BILSTM layer effectively\ncaptures contextual dependencies within the data. The inclusion of the Dropout layer mitigates the risk of overfitting,\nenhancing the model's generalization. The Dense layer generates sentiment scores, with the Mean Squared Error (MSE)\nserving as the loss function and the Adam optimizer refining the training process16. Together, these components demonstrate\nstrong learning and generalization capacities, with each layer playing a critical role in accurately extracting sentiment scores\nfrom the input textual data."}, {"title": "Hyperparameter Optimization", "content": "The selection of hyperparameters in deep learning models is critical for achieving optimal performance outcomes. In\ncontrast to learnable parameters, such as weights that are adjusted during training, hyperparameters must be specified prior\nto the training phase, necessitating careful consideration17. The choice of hyperparameters can substantially affect both the\nefficiency of training and the model's generalization capability to unseen data. Commonly tuned hyperparameters include\nthe learning rate, the number of hidden units, batch size, and various regularization terms. Accurate selection of these\nhyperparameters is essential to ensure effective learning and to mitigate the risks of overfitting and underfitting, both of\nwhich can degrade model performance. To further optimize performance and reduce computational time, hyperparameter\noptimization techniques are employed to identify the most effective configurations. Arbitrary selection or reliance on default\nhyperparameters may result in suboptimal performance or even failure in the training process. Therefore, a systematic\napproach to hyperparameter optimization is necessary to significantly improve the model's efficacy and robustness.\nIn this study, we employ Bayesian Optimization 18, a global optimization technique, to determine optimal\nhyperparameters for the model, specifically focusing on the learning rate, number of LSTM units, and dropout rate. This\napproach is particularly well-suited for optimizing complex, computationally intensive functions, as frequently encountered\nin deep learning applications. Unlike traditional grid or random search methods, Bayesian Optimization utilizes information\nfrom prior evaluations to inform the selection of hyperparameters, providing several distinct advantages. Therefore, this\napproach not only improves model performance but also enhances the efficiency of the optimization process.\n1. Learning Rate 19: The learning rate is a crucial hyperparameter that controls the magnitude of weight updates during\ntraining. A learning rate that is too small may result in slow convergence, while a rate that is too large may cause instability\nand divergence. For this study, we define the optimization range for the learning rate between $10^{-4}$ and $10^{-2}$,\nallowing a thorough exploration of potential values that could lead to optimal training outcomes.\n2. Number of LSTM Units 20: The number of units in each LSTM layer corresponds to the capacity of the model to capture\ntemporal features in the data. While increasing the number of LSTM units can enhance the model's ability to represent\ncomplex temporal dependencies, it also increases computational demand and the risk of overfitting. We set the optimization\nrange for the number of LSTM units between 32 and 128, enabling a balance between model complexity and computational\nefficiency.\n3. Dropout Rate21: Dropout is a regularization technique used to mitigate overfitting by randomly deactivating a subset\nof neurons during training, thereby promoting better generalization. For this study, we explore dropout rates within the\nrange of 0.2 to 0.6, allowing for a comprehensive evaluation of its impact on model performance."}, {"title": "Result", "content": "Experimental Procedure\nThe experimental process begins with an exploratory hyperparameter search, followed by refinement through Bayesian\nOptimization based on the initial outcomes. The overall procedure was systematically divided into the following phases:\n1. Initial Exploration: A broad, random search is conducted to sample various hyperparameter configurations. The\nmodel is trained with these configurations, and performance is evaluated on the validation set. This exploratory phase\nprovides essential prior information to inform the subsequent Bayesian Optimization process.\n2. Bayesian Optimization: Using the results from the initial phase, Bayesian Optimization is implemented to iteratively\nrefine the hyperparameter configurations. The optimization is guided by the objective function-specifically, the MAE on\nthe validation set-leading to a progressive convergence toward optimal hyperparameters. Each iteration produces\nsignificant reductions in the validation error, underscoring the effectiveness of Bayesian Optimization in improving model\nperformance.\n3. Experimental Results: After nine optimization iterations, the validation error reaches a stable point, resulting in the\nfollowing optimized hyperparameter values: Learning Rate: 0.005358, LSTM Units: 52, Dropout Rate: 0.007038. Following\noptimization, the model exhibits a notable reduction in MAE on the validation set, reflecting enhanced robustness.\nAdditionally, the optimized hyperparameters contribute to improved model stability and accelerated convergence during\nthe training process."}, {"title": null, "content": "reduction in MAE, particularly during the initial stages of the process. A notable, albeit temporary, increase in MAE occurs\nat the twelfth iteration, which can be attributed to the inherent exploratory nature of Bayesian Optimization. This\nexploratory phase often involves evaluating hyperparameter configurations that may not immediately yield performance\nimprovements, thereby preventing premature convergence to suboptimal local minima22. This transient rise in MAE\nunderscores the importance of exploration in ensuring a thorough search across the hyperparameter space, which ultimately\nenables the identification of optimal configurations. Following this brief fluctuation, a marked decline in MAE, followed by\nstabilization, is observed, signaling the successful discovery of an optimal hyperparameter set. Overall, this analysis validates\nthe efficacy of the Bayesian Optimization process, highlighting its ability to navigate complex, high-dimensional\nhyperparameter landscapes. Despite occasional performance variations in specific iterations, the optimization process\nreliably converges toward a robust and effective solution.\nModel Training\nAfter determining the optimal hyperparameter configuration, the final training phase of the model is initiated. The\nhyperparameters-learning rate, number of LSTM units, and dropout rate-are precisely tuned using Bayesian optimization.\nThe model is trained over 100 iterations, with performance evaluations conducted on a validation set after each iteration.\nThis iterative evaluation enables continuous monitoring of validation error fluctuations, minimizing the risk of overfitting.\nThe training configuration is detailed as follows:\n1. Optimizer: Optimizer: The Adam optimizer is utilized, selected for its ability to combine momentum with adaptive\nlearning rates. This optimizer enhances convergence speed, particularly in complex neural network architectures, by\nadjusting learning rates during training.\n2. Loss Function23: Mean Squared Error (MSE) is employed as the loss function due to its effectiveness in quantifying\nthe difference between predicted and actual sentiment scores. MSE's sensitivity to larger errors, as it sums squared\ndeviations, renders it well-suited for tasks requiring precision in error measurement.\n3. Training and Validation Set Partitioning: To improve the model's generalization capability, the dataset is split, with 80%\ndesignated for training and 20% reserved for validation24. Performance is evaluated on the validation set after each epoch,\nproviding continuous feedback on the model's generalization and overall efficacy.\nThroughout the training process, both training and validation errors are rigorously monitored. This ensures that the\nmodel not only performs well on the training data but also demonstrates robust generalization to unseen data, thereby\nconfirming its predictive accuracy and reliability."}, {"title": "Evaluation Metrics", "content": "Upon completing the training phase, the model's performance is rigorously evaluated using two primary metrics:\n1. Mean Squared Error (MSE)25: Serving as the model's principal loss function, MSE measures the squared differences\nbetween predicted and actual sentiment scores. Its sensitivity to larger errors makes it particularly effective in identifying\nsignificant deviations, thus providing crucial insights into the model's predictive precision, especially in instances of extreme\nvariance between predictions and true values.\n2. Mean Absolute Error (MAE): Unlike MSE, MAE offers a more interpretable evaluation by averaging the absolute\ndifferences between predicted and actual values 26. This metric provides a straightforward assessment of the model's\npractical accuracy. Given that sentiment scores are bounded within a 0-5 range, a lower MAE signifies a closer alignment\nbetween predictions and real-world outcomes, thereby underscoring the model's applicability and reliability in practical\nscenarios.\nTraining Results and Analysis\nThe model's performance across both training and validation datasets is systematically summarized as follows:\n1. Training Error: A detailed analysis of the loss curve throughout the training process indicates a consistent reduction\nin training error, corresponding with an increasing number of iterations. The error ultimately stabilizes after several iterations,\ndemonstrating the model's ability to effectively capture and internalize the underlying patterns present in the training data.\n2. Validation Error: The validation error curve exhibits a sharp initial decline during the early stages of training, followed\nby a stabilization phase characterized by minor fluctuations. This trend suggests a lack of significant overfitting. After 100\niterations, the MAE for the validation set reached a plateau at a low level, indicating the model's strong predictive\nperformance on previously unseen data.\nA comparative assessment of the training and validation errors yields several key insights. The model demonstrates a\nrobust generalization capability, as evidenced by the minimal disparity between the training and validation errors, indicating\na high resistance to overfitting. Furthermore, the model continuously integrates new features with each iteration, as\nreflected by the steady decrease in both training and validation errors, supporting the conclusion that the model converges\neffectively."}, {"title": "Discussion", "content": "A detailed quantitative comparison between BiLSTM networks and the SnowNLP framework is performed, focusing on\ntheir performance across various natural language processing tasks, particularly sentiment analysis 28. Tab. 1 presents a\ncomprehensive comparison of these models across multiple evaluation metrics."}]}