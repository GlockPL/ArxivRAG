{"title": "Deep Learning Innovations for Underwater Waste Detection: An In-Depth Analysis", "authors": ["Jaskaran Singh Walia", "Pavithra L K"], "abstract": "Addressing the issue of submerged underwater trash is crucial for safeguarding aquatic ecosystems and preserving marine life. While identifying debris present on the surface of water bodies is straightforward, assessing the underwater submerged waste is a challenge due to the image distortions caused by factors such as light refraction, absorption, suspended particles, color shifts, and occlusion. This paper conducts a comprehensive review of state-of-the-art architectures and on the existing datasets to establish a baseline for submerged waste and trash detection. The primary goal remains to establish the benchmark of the object localization techniques to be leveraged by advanced underwater sensors and autonomous underwater vehicles. The ultimate objective is to explore the underwater environment, to identify, and remove underwater debris. The absence of benchmarks (dataset or algorithm) in many researches emphasizes the need for a more robust algorithmic solution. Through this research, we aim to give performance comparative analysis of various underwater trash detection algorithms.", "sections": [{"title": "1. Introduction", "content": "In recent years, there has been a significant rise in underwater debris as a result of poor waste management methods, littering, and the expansion of global industries. This waste has caused multiple environmental problems, including impairment of aquatic life and water pollution [1, 2]. When trash is dumped into the water, it not only scatters on the surface but also makes its way down to the epipelagic layer and remains there for years [3], polluting the water [4] and harming aquatic animals. Extracting debris from below the surface of the"}, {"title": "2. Review of existing literature", "content": "2.1. Challenges in underwater trash detection\nThe detection and management of marine debris are critical to preserving marine ecosystems. The primary challenge lies in the debris' small size relative to the vast ocean, its submerged nature, and the potential for concealment on the sea bottom [6]. A comprehensive review has been done on the various existing literature in this domain explaining the problems faced in harsh marine environments, shown in Figure 1\n2.2. Advancements in Detection Technologies\n2.2.1. Deep Learning and Object Detection\nAdvancements in deep learning have propelled the development of object detection algorithms, significantly enhancing marine debris detection capabilities.Figure 1 [7] shows a taxonomy of deep learning-based object identification, includ-ing detection components, learning algorithms, and applications and benchmarks.This comprehensive approach underpins the ongoing research into autonomous robotic systems[8-10] for environmental monitoring at both surface and un-derwater levels to innovative detection[11-13] and documentation[14] methods post-natural disasters using ultrasonic [15-17] and sonar [18] sensors in various aquatic environments[19, 20].\n2.2.2. Technological Innovations in Debris Detection\nTechnological innovations extend beyond deep learning approaches that utilize neural networks[21-25] and image processing[26-28], incorporating LIDAR [17], sonar [18], and light scattering techniques [19, 20] for underwater image enhancement. The operational use of multispectral images by unmanned aerial vehicles for macro-litter mapping and categorization represents another significant leap forward [10].\n2.3. Deep Learning's Pivotal Role in Underwater Debris Detection\nDeep learning not only facilitates the detection of marine debris but also enhances the quality of underwater imagery. Notable studies employing deep learning for debris detection include the works of ocean pollution detection[21], identification of floating plastic[22], deep sea debris detection[23, 24], submerged debris detection[25], haze removal[26], localization of trash[27], and custom net-works for detection [28]. These efforts demonstrate the efficacy of deep neural networks in identifying underwater debris, with innovations such as the 'YOLO-TrashCan' network emerging as a potent tool for marine debris identification [27, 28].\n2.4. Datasets and Environmental Impact Assessments\nThe development of specialized datasets [12, 29-32] like UIDEF [29], along with studies on the distribution of deep-sea litter [30] and beach litter seg-mentation [31], provides critical resources for training and evaluating detection"}, {"title": "2.5. Integrating Environmental Conservation Efforts", "content": "algorithms. The contributions of research institutions, exemplified by the Mon-terey Bay Aquarium Research Institute's long-term data collection [32] and JAMSTEC's dataset compilation [12], are invaluable to the field. In addition, the TrashCan [33] Dataset has recently emerged as a valuable resource, further enriching the available datasets for training and evaluating detection algorithms.\nTechnological innovations are pivotal for environmental conservation, partic-ularly in addressing marine debris and ecosystem degradation. Clear Blue Sea's Floating Robot for Eliminating Debris (FRED) initiative [2] deploys autonomous robots equipped with advanced sensors and navigation systems to detect and collect debris in coastal waters. FRED, designed to handle diverse marine en-vironments, is engineered to navigate autonomously, allowing it to efficiently capture floating debris before it reaches the open ocean. The robot's design is modular, enabling it to be adapted for different debris sizes and ensuring versatile functionality. Similarly, The Rozalia Project employs underwater rovers, notably the ROV (Remotely Operated Vehicle) Hector [3], which is specifically designed for oceanic cleanup operations. These rovers are equipped with specialized tools that enable the collection of debris from sensitive marine environments like coral reefs, minimizing disturbances to marine life. The Rozalia Project's multifaceted approach includes public education and scientific research, emphasizing the im-portance of maintaining marine ecosystems while exploring innovative solutions for debris removal.\nBoth initiatives illustrate the potential for robotics and technology to address marine pollution on a global scale, demonstrating the effectiveness of targeted interventions in preserving marine biodiversity and reducing ecosystem degra-dation. These projects highlight the transformative potential of technology in preserving marine ecosystems [17, 32, 34, 35], emphasizing the importance of technological innovation in combating marine debris and safeguarding the environment."}, {"title": "2.6. Exploring Alternative Solutions and Innovations", "content": "The exploration of alternative solutions, such as the NOAA's monitoring initiatives [34], which utilize satellite imagery and advanced data analytics to track and predict debris accumulation patterns, and Howell's investigation into marine detritus [32], which leverages field research and oceanographic modeling to understand the sources and distribution of debris, signifies a multifaceted approach in understanding and mitigating marine debris. These efforts aim to develop effective strategies for debris management and prevention by providing crucial data on debris hotspots and movement, thus guiding targeted cleanup operations and informing policy decisions.\nThese initiatives are instrumental in enriching datasets for underwater object detection algorithms by providing valuable real-world observations. NOAA's monitoring efforts offer comprehensive data on marine environments, aiding in algorithm refinement by enhancing the accuracy of detection systems. Howell's"}, {"title": "2.7. Broadening the Scope of Marine Debris Detection Models", "content": "research delves into the composition and distribution of marine debris, offer-ing crucial insights that contribute to algorithm optimization and the overall effectiveness of debris detection methods.\nThe broad scope of marine debris detection models is evident in the recent contributions to the field, with studies [36-61] emphasizing diverse approaches. Guo et al. [36, 37] explore unsupervised underwater image enhancement us-ing transformers, significantly improving image clarity for better detection. A comprehensive review of image analysis methods for microorganism counting, including deep learning approaches, which are essential for understanding the impact of marine debris on microscopic life was performed by Li et al. [38]. Another comparative study on different learning algorithms to classify mangrove species using UAV multispectral images was performed by [39], aiding in the identification of marine debris in coastal ecosystems. Marin et al. [40] inves-tigate deep-feature-based approaches to marine debris classification, revealing the potential of machine learning in accurately categorizing various debris types. Other researchers highlight the importance of accurate classification through a review and case study of marine microdebris[41] and also discuss the use of both remote and in situ devices for assessing marine contaminants[42], emphasizing the role of advanced technology in environmental monitoring.\nUAV imagery and deep learning are also utilized to assess anthropogenic marine debris in the Maldives [43], demonstrating the effectiveness of aerial surveys. Vashisht et al. [44] delves into object detection methods and image classifi-cation for marine debris, providing essential insights into identifying debris patterns[45].Salgado-Hernanz et al. [46] underscore recent approaches in remote sensing for marine litter assessment, pointing toward future goals in debris detec-tion. Mittal et al. in his research [47] focussed on underwater image classification through deep learning techniques.\nAdditionally, the classification of marine microdebris [41] and the use of remote sensing for marine debris detection [42, 43] further expand the methodologies available for marine conservation efforts."}, {"title": "2.8. Contributions to Underwater Image Classification and Debris Detection", "content": "Contributions to underwater image classification and debris detection are continually evolving, with significant overviews and studies provided by Vashisht [44, 45], along with research in remote sensing [46], and survey based study of deep learning architectures[47]. These contributions, along with the development of convolutional architectures for sonar images [48] and the Adaptive Lighting Enhancement algorithm [49], underscore the ongoing advancements in the field."}, {"title": "2.9. Concluding Remarks on Environmental Implications and Future Directions", "content": "The detection and management of marine debris necessitate a comprehensive and multifaceted approach that incorporates deep learning, technological innova-tions, and concerted environmental conservation efforts. The body of research,"}, {"title": "3. Dataset", "content": "including the operationalization of new detection models and the development of enriched datasets, lays a solid foundation for future advancements. Nonetheless, as the field progresses, it is crucial to consider the environmental implications of these technologies and strive for sustainable and ecologically responsible solutions.\n3.1. Publically available datasets\nNumerous open source underwater image datasets exist on the internet, but a common shortcoming is their high specificity and adaptation to the collection environment, introducing potential bias in the model. Some examples of these existing datasets include TrashCAN 1.0 [33] (both instance and material versions), Trash_ICRA19[62], JAMSTEC[63] and [64]. Current studies towards automatic waste detection using these datasets are hardly comparable due to the lack of benchmarks and widely accepted standards regarding the used metrics and data [65]. In our previous study[64], we introduced a custom dataset designed to address these challenges and enhance the efficiency of trash localization.\n3.2. Dataset Comparison\nA comprehensive comparison of all the properties like size, classes, collection location, water-type was performed on existing datasets [66] in order to identify each dataset's limitations and therefore utilizing this information to make the model trained on these datasets more robust.\nThe diverse range of data prompted an analysis of these datasets which is available in the paper's source repository."}, {"title": "4. Methodology", "content": "With recent advancements in computer vision technologies and algorithms, this section introduces cutting-edge identification and categorization models, accompanied by established standards for the utilized dataset. The training metrics are then statistically evaluated for each model providing insights into their efficiencies and potential for real-world application.\n4.1. Object Localization\nThe object detection architectures were carefully selected from the latest, highly coherent, and promising set of networks currently available. Each archi-tecture has its own unique strengths and weaknesses, offering varying degrees of reliability, execution pace, and other metrics which we identify in this literature. In this paper we also leverage several neural network architectures considered to be state-of-the-art models, including YOLOv9, YOLOv8 nano, YOLO v7, YOLO v6s, YOLO v5s, and YOLO v4 Darknet, utilizing their respective source repositories [68]. In addition, custom training was performed on region-based Mask-RCNN and FasterRCNN architectures with Inception v2 [69].\n4.2. Object Segmentation\nBuilding on the advancements in computer vision, this section focuses on cutting-edge object segmentation methodologies. Real-time instance segmenta-tion, exemplified by YOLACT and Mask R-CNN models trained on underwater litter data, offers promising prospects for litter detection, with Mask R-CNN achieving a mean average precision (mAP) of 0.377 and YOLACT providing faster detection speeds[70]. AquaSAM[71] enhances underwater segmentation, particularly in tasks like coral reef segmentation, showcasing a notable 7.13% improvement in Dice Similarity Coefficient. A novel deep learning-based method for trash trap defect detection[72] demonstrates significant enhancements in reliability, with error rates below 15% and accuracy rates exceeding 85%. Fur-thermore, leveraging Cycle Consistent Generative Adversarial Networks (GANs), enhancing unpaired underwater images exhibits superior performance over tra-ditional techniques[73], showing potential for various marine applications such as trash identification and coral reef inspection.State-of-the-art segmentation models are considered, aligning with the latest advancements in technology. The chosen models adhere to predefined standards for the dataset, ensuring a rigorous evaluation of the segmentation performance. Each segmentation architecture brings its distinctive features, strengths, and weaknesses, contributing to a comprehensive understanding of their efficacy."}, {"title": "4.3. Graphics Processing Unit", "content": "In this study, we utilized a Tesla T4 GPU with a total memory capacity of 16GB, comprising 40 Streaming Multiprocessors (SMs) and a shared 6MB L2 cache across all SMs. Released in 2018, this GPU is equipped with 2560 CUDA cores."}, {"title": "4.4. Architectures", "content": "This section provides an overview of latest state-of-the-art models employed in the research and presents the outcomes achieved during experimentation. Presently, object detection frameworks rooted in deep learning can primarily be categorized into two main groups: (i) two-stage detectors, exemplified by Region-based CNN (R-CNN) [74] and its variations [75-77], and (ii) one-stage detectors, such as YOLO [78] and its variations [79, 80]. Two-stage detectors initially utilize a proposal generator to produce a sparse set of proposals and then extract features from each proposal. This is followed by region classifiers that predict the category of the proposed region. In contrast, one-stage detectors directly predict the category of objects at each location on the feature maps without the additional region classification step. While two-stage detectors commonly achieve superior detection performance and report state-of-the-art results on public benchmarks, one-stage detectors are notably more time-efficient and exhibit greater suitability for real-time object detection[7].\n4.4.1. You Only Look Once (YOLO)\nYOLO, an acronym for You Only Look Once, stands out as a prominent and sought-after object identification method. It excels in the real-time identification of multiple items within a coexisting image or video. Unlike traditional methods, YOLO employs a single neural network to predict bounding boxes and class probabilities directly from the entire image in a single analysis. This approach enhances efficiency and precision, making YOLO a superior choice compared to other existing object detection and identification systems. A comparative analysis in Table 2 underscores YOLO's prowess, showcasing its superior performance in terms of model size, mean average precision (mAP), speed, and computational parameters across different variants like YOLOv8n, YOLOv8x, and YOLOv5s.\n\u2022\nYOLOv8x [81] (XLarge): Building upon the innovations of its pre-decessors, YOLOv8x represents an advanced variant of YOLOv8, with a primary focus on achieving heightened accuracy, particularly for small and intricate objects. This improvement is realized through an augmen-tation in model size, achieved by enhancing both depth and width. The increased dimensions enable the model to capture more intricate visual pat-terns, resulting in superior detection performance, especially in challenging scenarios. YOLOv8x outperforms YOLOv8n, making it particularly well-suited for applications demanding exceptional precision. Its larger model size enhances object understanding and localization, thereby significantly improving overall object detection capabilities.\n\u2022\nYOLOv8n [81] (Nano): The YOLOv8n model presents a swifter and more precise unified framework for training models, with its nano version standing out as the fastest and most compact among the available models. However, the heightened speed of the model comes at the cost of detailed prediction accuracy. Despite this trade-off, the nano version proves to be the most suitable for real-time deployment owing to its rapid detection rates and lower memory requirements. Notably, it introduces anchor-free detection, negating the necessity for anchor box adjustments and streamlining the detection process.\nMoreover, YOLOv8n incorporates novel convolutions by substituting the initial 6x6 convolution in the stem with a more efficient 3x3 convolution. Further modifications are made to the concatenation of features in the\n\u2022\nneck. An advanced training routine is employed, encompassing mosaic augmentation to introduce variations during the training process.\nYOLOv7 [82] (tiny/W6): The YOLOv7 algorithm has markedly outper-formed its earlier versions in terms of both speed and precision. Notably, it requires less sophisticated hardware compared to standard conventional architectures. YOLOv7 demonstrates an impressive ability to be trained effectively on smaller datasets without pre-learned weights within a shorter timeframe. However, it is important to highlight that, despite these strengths, YOLOv7 falls short of achieving superior accuracy and inference speeds when compared to the latest YOLOv8 models.\nYOLOv5s [83] and YOLOv6s [84] (Small): They exhibit relatively consistent performances and results, making them suitable for benchmark-ing. However, they fall short when compared to some of the most widely used stable models for object identification and segmentation"}, {"title": "4.4.2. R-CNN", "content": "R-CNN[74], introduced by Girshick et al. in 2014, is a groundbreaking two-stage object detection system. Compared to previous state-of-the-art methods like SegDPM [85], which achieved a mean Average Precision (mAP) of 40.4% on Pascal VOC2010, R-CNN showed a significant improvement with 53.7% mAP.The R-CNN pipeline is divided into three main components: proposal generation, feature extraction, and the region classification.\nR-CNN generates a sparse collection of proposals (about 2,000 for each image) using Selective Search [86]. This technique is intended to filter away regions that can be easily identified as background. A deep convolutional neural network then crops and resizes each proposal into a fixed-size region before encoding it into a feature vector (for example, 4,096 dimensions). Following that, a one-vs-all SVM classifier is used for region classification. Finally, bounding box regressors are trained with the extracted characteristics as input, with the goal of fine-tuning the original suggestions to better capture the objects.\nInspired by spatial pyramid matching (SPM) [87], He et al. proposed SPP-net [88] to boost R-CNN's speed and learn more distinct features. Unlike the"}, {"title": "4.4.3. Fast R-CNN", "content": "traditional method of clipping proposed regions and passing them separately into a CNN model, SPP-net creates the feature map from the full image using a deep convolutional network. It then extracts fixed-length feature vectors from the feature map using a Spatial Pyramid Pooling (SPP) layer.\nFast R-CNN [75] is a multi-task learning detector that outperforms SPP-net while resolving its shortcomings. Similar to SPP-Net, Fast R-CNN generates a feature map for the full picture and extracts fixed-length region features from it. However, unlike SPP-net, Fast R-CNN uses a ROI (Region of Interest) Pooling layer to retrieve region features."}, {"title": "4.4.4. Faster R-CNN", "content": "Despite significant advancements in learning detectors, the proposal genera-tion step remained reliant on conventional approaches such as Selective Search [86] or Edge Boxes [89], which were grounded in low-level visual cues and lacked the capacity to learn in a data-driven manner. To tackle this limitation, faster R-CNN, an improvement over R-CNN[74], which utilized a Region Proposal Network (RPN) [90\u201392] was introduced, enhancing the creation of an end-to-end trainable object detection network [93]. The RPN generates region proposals using the last convolutional feature map, followed by fully connected layers for final detection. Although many implementations typically employ VGG-16 as a standard for feature extraction, we chose to use Inception v2, which has demonstrated superior object identification performance in conventional datasets [94, 95]. However, it may misidentify patches in the background as objects due to its limited ability to consider large contexts.\nWhen comparing generalization errors, YOLO algorithms surpass R-CNN by producing lower generalization errors. In comparison to the YOLO algorithms, Faster R-CNN exhibits slightly fewer localization errors but at the expense of higher computational needs and slower speed."}, {"title": "4.4.5. Mask RCNN", "content": "In order to enhance the overall flexibility of the process, He et al. [96] introduced Mask R-CNN, a model capable of predicting bounding boxes and segmentation masks at the same time, yielding optimal results. Huang et al. [97] developed Mask Scoring R-CNN, a system that prioritizes mask quality awareness. This framework learns the quality of predicted masks and uses calibration approaches to correct the misalignment between mask quality and mask confidence score. This has also emerged as a faster variant by combining ResNet with a Feature Pyramid Network (FPN) the combination of which creates a feature extraction network capable of generating a multi-layer convolutional feature map from input images [84]. The Region Proposal Network (RPN) is then employed to generate object region proposals, while the detection head and mask head are utilized for object detection and instance segmentation. The algorithm is structured into two main components: detection and instance segmentation. [96, 97]"}, {"title": "4.5. Evaluation metrics", "content": "Despite its effectiveness, Mask R-CNN can face challenges in complex maritime environments. External variables such as distracting objects and low image resolution may impact the algorithm's performance by making it difficult to extract object features, leading to reduced attention toward objects during detection.\nAfter model training, we utilize distinct evaluation and validation datasets, separate from the training dataset, to assess the accuracy of the architecture. The model accurately classifies trash by creating bounding boxes with a confidence score of 0.50 or higher. The amount of true positive bounding boxes containing marine plastic debris and the accuracy with which true negatives are detected are used to evaluate the model's performance.\nThese performance metrics were used to evaluate each model's performance, pros and cons:\n\u2022\nTrue positive and True negative values: These values are calculated by counting the number of bounding boxes correctly predicted as trash (true positives) and the number of bounding boxes correctly predicted as not trash (true negatives).\n\u2022\nPrecision and Recall: Evaluates if the model foreseen trash located in the input image in which precision reflects the model's ability to correctly predict plastic waste, while recall reflects the model's ability to identify all instances of plastic waste in the input image. Precision is calculated as the ratio of true positives to the sum of true positives and false positives, while recall is calculated as the ratio of true positives to the sum of true positives and false negatives[98].\n$\\text{Recall} = \\frac{TP}{TP + FN}$ (1)\n$\\text{Precision} = \\frac{TP}{TP + FP}$ (2)\n\u2022\nMean Average Precision: This metric measures the accuracy of the model in identifying underwater trash waste across a set of input images. The mAP is calculated by building a precision-recall curve using the Intersection over Union (IoU) formula to compare the predicted and ground truth bounding boxes [99]. The precision-recall curve is then integrated to obtain the mAP. After collecting the accurate and inaccurate data, we utilize the \"Intersection over Union (IoU)\" formula to give rise to a precision-recall curve:"}, {"title": "", "content": "$\\text{IOU} = \\frac{BBOX_{\\text{pred}} \\cap BBox_{\\text{GroundTruth}}}{BBOX_{\\text{pred}} \\cup BBox_{\\text{GroundTruth}}}$ (3)\nWhere $BBox_{Predicted}$ and $BBox_{Groundtruth}$ represent the expected areas under the curve for ground truth and predicted bounding boxes, respectively. In order to maximize accuracy, it is necessary to set a high threshold for confidence and IoU, ensuring that correct predictions exceed this threshold. Subsequently, The average precision (mAP) can be calculated by integrating the precision-recall curve. [100]:\n$\\text{mAP} = \\int_{0}^{1} p(x) dx$ (4)\n\u2022\nProcessing speeds: This metric denotes the average time taken by the model to perform pre-processing, inference and post-process for the detec-tion and identification of underwater trash in an input image, measured in milliseconds per image.\n1. YOLOv8n gave us an average speed of 1.9ms for pre-process, 73.5ms for inference and 0.95ms post-process per image at shape (1, 3, 416, 416)\n2. YOLOv8x gave us an average speed of 0.3ms for pre-process, 16.6ms for inference and 1.4ms post-process per image at shape (1, 3, 416, 416)\n3. YOLOv5s gave us an average speed of 0.5ms for pre-process, 12.0ms for inference with 1.2ms post-process per image at shape (1, 3, 640, 640)"}, {"title": "5. Results", "content": "5.1. Evaluating results\nA comprehensive analysis of individual components for the best perform-ing frameworks was conducted, involving thorough examinations of trash data publicly available in various scenarios, including clean water, natural and man-made water bodies such as lakes and ponds, surroundings, and ocean beds was performed. The utilized architectures also demonstrate a relatively high F1 score and a high mean-average precision (mAP) score relative to their inference speeds. Table 3 shows the results of an in-depth evaluation comparing multiple architectural networks. The observed trade-offs indicate that the outcomes derived in this research provide a more accurate representation of performance over time across a broader spectrum of water conditions, allowing for an in-depth analysis of the various model's object localization performance in this domain. From table 3 we conclude that both YOLOv8-nano and YOLOv8-XLarge success-fully achieve real-time localization of epipelagic detritus, demonstrating robust metrics. Furthermore, they exhibit a notably higher F1 score compared to the other tested algorithms."}, {"title": "6. Conclusion", "content": "In conclusion, this research evaluated cutting-edge computer vision archi-tectures for the localization and quantification of underwater submerged trash across various water layers, including the epipelagic and mesopelagic zones, and extending to other local water bodies. Leveraging a carefully curated dataset and employing state-of-the-art architectures such as YOLO, our primary objective of reviewing the feasibility of localizing submerged trash underwater, in real-time inference speeds for trash eradication was assessed.\nThe rapid inference speeds observed in this study attest to the viability of maintaining an impressive level of performance which underscores the potential for utilizing Autonomous Underwater Vehicles (AUVs) for the automatic catego-rization of diverse underwater objects, as well as the sorting and collection of trash in locations that are challenging for humans to access due to factors such as high pressure as well as other environmental conditions.\nThese findings hold promise for the automation of trash collection and erad-ication in harsh underwater aquatic environments, showcasing the potential of advanced deep-learning methods in addressing the challenges of underwater debris management. Moreover, this research serves as a crucial foundation and standard for future endeavors that seek to identify and categorize underwater"}]}