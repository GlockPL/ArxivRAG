{"title": "Self-supervised ASR Models and Features For Dysarthric and Elderly Speech Recognition", "authors": ["Shujie Hu", "Xurong Xie", "Mengzhe Geng", "Zengrui Jin", "Jiajun Deng", "Guinan Li", "Yi Wang", "Mingyu Cui", "Tianzi Wang", "Helen Meng", "Xunying Liu"], "abstract": "Self-supervised learning (SSL) based speech foundation models have been applied to a wide range of ASR tasks. However, their application to dysarthric and elderly speech via data-intensive parameter fine-tuning is confronted by in-domain data scarcity and mismatch. To this end, this paper explores a series of approaches to integrate domain fine-tuned SSL pre-trained models and their features into TDNN and Conformer ASR systems for dysarthric and elderly speech recognition. These include: a) input feature fusion between standard acoustic frontends and domain fine-tuned SSL speech representations; b) frame-level joint decoding between TDNN systems separately trained using standard acoustic features alone and those with additional domain fine-tuned SSL features; and c) multi-pass decoding involving the TDNN/Conformer system outputs to be rescored using domain fine-tuned pre-trained ASR models. In addition, fine-tuned SSL speech features are used in acoustic-to-articulatory (A2A) inversion to construct multi-modal ASR systems. Experiments are conducted on four tasks: the English UASpeech and TORGO dysarthric speech corpora; and the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets. The TDNN systems constructed by integrating domain-adapted HuBERT, wav2vec2-conformer or multi-lingual XLSR models and their features consistently outperform the standalone fine-tuned SSL pre-trained models. These systems produced statistically significant WER or CER reductions of 6.53%, 1.90%, 2.04% and 7.97% absolute (24.10%, 23.84%, 10.14% and 31.39% relative) on the four tasks respectively. Consistent improvements in Alzheimer's Disease detection accuracy are also obtained using the DementiaBank Pitt elderly speech recognition outputs.", "sections": [{"title": "I. INTRODUCTION", "content": "DESPITE the rapid progress of automatic speech recognition (ASR) technologies targeting normal speech in recent decades [1], [2], accurate recognition of dysarthric and elderly speech remains highly challenging tasks to date [3]\u2013[14]. Dysarthria is a common type of speech disorder caused by a wide spectrum of motor control conditions including cerebral palsy, amyotrophic lateral sclerosis, stroke and brain injuries. In addition, neurocognitive disorders, such as Alzheimer's disease (AD), are often found among the elderly with speech and language impairments [15], [16]."}, {"title": "A. Dysarthric and Elderly Speech Recognition Prior to SSL Pre-trained Speech Models", "content": "Dysarthric and elderly speech bring considerable challenges to current ASR technologies primarily targeting normal speech recorded from healthy, non-aged users. These include: a) large mismatch against normal speech due to motor control conditions and aging, e.g., articulation imprecision, decreased speech volume and clarity, and increased disfluency; b) data scarcity due to the difficulty in data collection from these speakers with physical disabilities and mobility issues; and c) large speaker-level diversity. A set of publicly available dysarthric and elderly speech corpora are shown in Table I.\nIn order to address the data scarcity issue, one major area of prior researches focused on data augmentation techniques. Speed and temporal perturbation were employed in [9], [11], [25], [47], [62]\u2013[64]. In addition to speaker-independent perturbation of limited in-domain dysarthric or elderly speech only using fixed perturbation factors such as {0.9, 1, 1.1}, more powerful speaker-dependent data augmentation approaches were also developed [9], [11], [25], [42], [47], [65], [66]. Adversarial learning [38], [42], [65], [67]\u2013[69], voice conversion [70], [71] and text-to-speech synthesis [72] based data augmentation approaches designed for dysarthric or elderly speech were also developed. Another major area of prior researches addressing both the issues of data scarcity and mismatch against normal speech focused on domain adapting general purpose ASR systems that are trained using large quantities of out-of-domain healthy and non-aged speech data [6], [9], [27], [31]\u2013[34], [48], [73]. To model the diversity of dysarthric or elderly speakers, a range of speaker adaptation techniques were studied. These include, but are not limited to,"}, {"title": "B. Dysarthric and Elderly Speech Recognition using SSL Pre-trained Speech Models", "content": "The recent emergence of self-supervised learning (SSL) speech foundation models [80]\u2013[84] provides a new paradigm to address the above data scarcity and domain mismatch problems. These speech foundation models are SSL pre-trained on large quantities of unlabelled data. They have been successfully applied to normal speech processing tasks including ASR [80]\u2013[83], [85], speech emotion recognition [86], speaker recognition [87], voice conversion [88] and speech synthesis [89]. In addition, SSL speech representations are robust to domain mismatch [39], [90].\nIn contrast, more limited prior researches on applying SSL pre-trained models to dysarthric and elderly speech have been conducted. Wav2vec2.0 [80] and WavLM [81] models were applied to Japanese electrolaryngeal and English dysarthric speech in [37], where an overall word error rate (WER) of 51.8% was reported on the benchmark UASpeech [58] task. Speaker adaptation of the Wav2vec2.0 model using fMLLR and x-vectors during fine-tuning for dysarthric speech recognition was investigated in [91]. Cross-lingual SSL-based dysarthric speech recognition was studied in [39], where SSL speech representations were extracted from fine-tuned Wav2vec2.0 [80], cross-lingual XLSR [85] and HuBERT [82] models before being fed into the Conformer based ASR systems. Incorporating Wav2vec2.0 models and features into hybrid TDNN and end-to-end Conformer systems was proposed in [14], including input feature fusion, frame-level joint decoding and cross-system multi-pass rescoring. Speech impairment severity was incorporated by adding cross-entropy based severity prediction error in Wav2vec2.0 fine-tuning [12]. Wav2vec2.0 embedding features were used to learn dysarthric speech characteristics in the VAE-GAN based personalized disordered speech augmentation approaches [42]. The authors in [92] proposed to use the AV-HuBERT model to fuse audio and visual modalities to improve the performance of dysarthric speech recognition. A WER of 63.98% on the very low intelligibility group of the benchmark UASpeech dataset was reported. In addition to English, the Wav2vec2.0 model and cross-lingual XLSR model were also evaluated on Dutch dysarthric home-automation data in [93]. Speaker-dependent fine-tuning of SSL pre-trained ASR models for Dutch dysarthric speech was studied in [94]."}, {"title": "C. Key Research Problems and Methodology Design", "content": "1) Pre-trained ASR performance disparity and fairness: Data-intensive fine-tuning a large number of pre-trained ASR model parameters on limited impaired or elderly speech data rapidly leads to poor generalization. This issue is further exacerbated when limited training data provides insufficient coverage. For example, approximately 39% of words in the benchmark UASpeech test set do not occur in the training data. Under such conditions, current mainstream end-to-end ASR systems including SSL pre-trained models have been found to produce large performance disparity on two fronts: a) between seen and unseen words in the often very limited dysarthric speech [12], [25]; and b) between impaired speakers with high and very low speech intelligibility [12], [31].\n2) Use of SSL speech foundation models: Their application to dysarthric and elderly speech needs to account for the underlying issues over data scarcity and domain mismatch. To this end, alternative approaches that can effectively exploit SSL pre-trained ASR models and feature representations, while exhibiting less performance fragility over insufficient data coverage [12], [25] than using the fine-tuned ASR models alone, need to be investigated.\n3) Articulatory features generation and SSL models: Articulatory features are inherently robust to be acoustic signal perturbation. They have been successfully applied to normal and pathological speech [95]\u2013[98] recognition. However, such scarce and specialist data has been traditionally collected mainly from healthy, non-aged English speakers [59], [99]\u2013[103]. This hinders their application to dysarthric and elderly speech across multiple languages. To this end, acoustic-to-articulatory (A2A) inversion techniques further empowered by domain and language invariant SSL pre-trained speech representations need to be developed.\n4) Multi-faceted user applications and evaluation metrics: Advancement of dysarthric and elderly speech recognition technologies in recent decades have broadened their scope of application beyond ASR-based assistive technology to aid communication and improve quality of life for such users [3]\u2013[8], [10]\u2013[14], [25], [42], [47], [65], [104]. Aging presents enormous challenges to health care worldwide. Neurocognitive disorders, such as Alzheimer's disease (AD), are often found among older adults and manifest themselves in speech and language impairments [15], [16]. Dysarthric and elderly speech recognition forms the core technology to facilitate fully automated, large-scale, less intrusive and low-cost neurocognitive disorders screening among the aging population [9], [14], [17]\u2013[19], [45], [46], [48], [51], [53], [56], [105], [106]. To this end, dysarthric and elderly speech recognition technologies tailored for such a wider range of medical domain applications need to be evaluated not only using ASR word error rate, but also additional metrics that are relevant to the specific task such as AD detection.\nIn order to address the above issues, this paper explores a series of techniques to integrate state-of-the-art mono-lingual and multi-lingual SSL pre-trained speech foundation models and their features into hybrid TDNN [1] and Conformer [2] ASR systems for dysarthric and elderly speech recognition. 1) We aim to exploit the diversity and complementarity among them, and to improve the generalization performance on unseen and poorly covered words as well as on the most challenging dysarthric speech data of very low intelligibility. 2) These include: a) input feature fusion between standard acoustic frontends and fine-tuned SSL speech representations;"}, {"title": "II. SSL PRE-TRAINED ASR MODELS", "content": "This section gives an overview of SSL pre-trained ASR models, including Wav2vec2.0 [80], HuBERT [82], WavLM [81] and Data2vec [83] models."}, {"title": "A. Pre-trained Wav2vec2.0 Model", "content": "Wav2vec2.0 is a pre-trained model that jointly learns latent contextualized speech representations and an inventory of discretized latent speech units serving as the pseudo-labels. Contrastive learning based SSL pre-training is performed by distinguishing the target from distractor pseudo-labels.\nModel Architecture: Wav2vec2.0 consists of three components, including 1) a multi-layer CNN-based feature encoder which encodes raw speech audio input X into continuous speech representations $z_t \\in Z$ with a stride of 20 ms and a receptive field of 25 ms; 2) a L-layers transformer-based context network producing contextual representations $c_t \\in C$ over a sequence of randomly masked feature encoder outputs; and 3) a quantization module generating discrete speech units $q_t \\in Q$ as pseudo-labels for SSL based pre-training.\nSSL Pseudo-labels: After mapping the feature encoder output $z_t$ to the logits $[l_1, l_2, ..., l_G] \\in R^{G \\times V}$, the best code is chosen among all the V entries of each codebook by Gumbel-softmax re-parameterization. The discrete quantized unit $q_t$ is obtained by applying a linear transformation to the concatenated G codes, before serving as the pre-training pseudo-labels."}, {"title": "SSL Criterion", "content": "Wav2vec2.0 is pre-trained via an interpolation between a contrastive task $L_c$ and a diversity task $L_d$.\n$L_{w2v,t} = -log \\frac{exp(sim(c_t, q_t)/\\kappa)}{\\sum_{\\tilde{q} \\in Q} exp(sim(c_t, \\tilde{q})/\\kappa)} + \\alpha \\sum_{g=1}^{G_V} \\sum_{v=1}^{G_V} \\tilde{I}_{g,v} log \\tilde{I}_{g,v}$  (1)\nwhere $sim(c_t, q_t)$ is the cosine similarity between the masked contextual representations produced before and after quantization. $\\tilde{q}$ is randomly sampled from $Q_t$ which consists of $q_t$ and \"Distractor\" labels from other masked time steps within the same speech utterance. $\\kappa$ is the non-negative temperature parameter. The entropy-based diversity loss in the second term ensures the pre-training pseudo-labels cover all codebook entries equally. $\\alpha$ is a tuned weight. $\\tilde{I}_{g,v}$ is the average logit of the v-th entry in the g-th codebook within a mini-batch.\nFine-tuning: Wav2vec2.0 is fine-tuned in a supervised mode using the Connectionist Temporal Classification (CTC) [110] loss. A randomly initialized linear layer is added on top of the context network to project the contextual representations into vocabulary tokens."}, {"title": "B. Pre-trained HuBERT Model", "content": "The HuBERT [82] model pre-training alternates between two steps: 1) a clustering step to create pseudo-labels; and 2) a prediction step to produce labels for masked positions.\nModel Architecture: The model architecture of HuBERT is similar to Wav2vec2.0, including a feature encoder, a k-means quantization module and a transformer-based context network followed by a projection layer.\nSSL Pseudo-labels: The HuBERT model derives discrete speech units, denoted as $q_t$, to serve as the pseudo-labels during pre-training. These pseudo-labels were produced using a total of G distinct k-means clusters of varying codebook sizes. The clustering process is initially performed on MFCC features. A latent speech representation $\\hat{s}_t$ is quantized into G discrete units $q_t^1, q_t^2, ..., q_t^G$. During the iterative refinement of pseudo-labels, the latent speech representation $\\hat{s}_t$ obtained from the context network at the t-th time step are then further k-means clustered to update the pre-training labels.\nSSL Criterion: BERT style prediction of masked discrete speech units based on the following cross-entropy loss function is used in HuBERT pre-training,\n$L_{mask}(Z, {Q^{(g)}}_{g=1}^G, M) = \\sum_{t \\in M} E_{g=1}^G logp(q_t^{(g)} | Z^{(mask)})$ (2)\nwhere M represents the position indices in masked pseudo-label prediction, $Z^{(mask)}$ denotes the partially masked version of the continuous speech representation Z at time step t. $p(q_t^{(g)} | Z^{(mask)})$ is the probability of the discrete speech units of the t-th frame assigned by the g-th k-means model.\nFine-tuning: During supervised fine-tuning using the CTC loss, the projection layer is replaced by a randomly initialized linear layer before the model parameters are updated."}, {"title": "C. Pre-trained WavLM Model", "content": "WavLM [81] is also a masked prediction based SSL pre-trained model that shares the same model architecture, discrete speech representation based pre-training and fine-tuning procedures as HuBERT, except for the following two differences: 1) the incorporation of gated relative position bias in the transformer self-attention module; and 2) the use of cocktail party style mixture speech inputs simulated with multiple overlapping speakers and various background noises to produce more acoustic perturbation invariant speech representations."}, {"title": "D. Pre-trained Data2vec Model", "content": "Data2vec [83] is an SSL pre-trained model that shares the same architecture and fine-tuning procedure of Wav2vec2.0. It learns to predict latent speech representations of the complete input audio sequence given a partial view of such input.\nSSL Criterion: The pre-training of Data2vec is performed using an exponential moving average in a teacher-student mode. The teacher parameterization is as [111]:\n$\\theta_{TM, i} = \\begin{cases} \\theta_{SM, 0}, & i=0 \\\\ \\gamma \\theta_{SM, i} + (1 - \\gamma)\\theta_{TM, i-1}, & i > 0 \\end{cases}$ (3)\nwhere $\\theta_{TM, i}$ and $\\theta_{SM, i}$ denote the parameters of the teacher and student models at training step i respectively. $\\gamma$ is the exponential moving average decay rate. For those masked time steps to be predicted by the student model, the training targets $y_t$ are obtained using the average output of the top K transformer blocks of the teacher model's context network. Let $\\tilde{c}_{t}^{M,l}$ denotes the normalized output of the l-th transformer block of the teacher model's context network at frame t, the student model's training targets are computed as $y_t = \\frac{1}{K}\\sum_{l=L-K+1}^{L} \\tilde{c}_t^{M,l}$. The regression loss between the targets and outputs of the Data2vec model is given by\n$L_{Data2vec, t} = \\begin{cases} (y_t - c_t^M)^2 / \\beta, & |y_t - c_t^M| \\leq \\beta \\\\ |y_t - c_t^M| - \\beta, & otherwise \\end{cases}$ (4)\nwhere $c_t^M$ is the student model's predicted output at the t-th time step, and the threshold $\\beta$ (e.g., 0.25 [83]), controls the transition from a squared error based loss to an L1 loss."}, {"title": "III. SSL FEATURES BASED A2A INVERSION", "content": "Human speech production involves the coordinated movements of various articulators such as the lips, teeth, tongue and palate. Articulatory movement representations are inherently invariant to extrinsic acoustic distortion. They have been successfully applied to normal and pathological speech [95]\u2013[98] recognition. In practice, recording detailed articulatory movements and vocal tract shape normally requires the use of intrusive techniques such as electromagnetic articulography (EMA) or magnetic resonance imaging (MRI). Compared to EMA and MRI, ultrasound tongue imaging (UTI) [112]\u2013[114] is more portable, non-invasive, and less costly. UTI utilizes B-mode diagnostic ultrasound to capture the tongue surface movements during speech production at a high frame rate [101]. However, there are very few publicly available UTI corpora [99]\u2013[101], all of which are exclusively in English and limited in size. By far the largest Tongue and Lips (TaL) corpus [101] contains 24 hours of parallel ultrasound, video, and audio data collected from 81 native English speakers.\nThe scarcity of such specialist data hinders the practical and wider use of UTI-based articulatory features in ASR systems for normal, atypical speech task domains and across languages. An alternative approach to obtaining articulatory movement"}, {"title": "IV. PRE-TRAINED ASR MODEL INTEGRATION", "content": "Fundamental modeling differences exist between hybrid and end-to-end (E2E) ASR systems including current SSL pre-"}, {"title": "V. ABLATION EXPERIMENTS ON IMPLEMENTATION DETAILS", "content": "In this section, several key implementation issues affecting the performance of TDNN and Conformer ASR systems integrating a range of pre-trained models and their speech representations that are domain fine-tuned to dysarthric and elderly speech are discussed. These include: a) the appropriate fine-tuning strategy and the choice of the most competitive baseline standalone SSL pre-trained models for dysarthric and elderly speech recognition; b) the detailed model structural configurations, and training costs used in these baseline pre-trained models; and c) the architectural modifications of these baseline pre-trained models that are required to extract suitable speech representations, and their fusion with standard acoustic"}, {"title": "VI. MAIN RESULTS", "content": "The performance of the proposed methods to incorporate domain fine-tuned pre-trained speech models and their features into hybrid TDNN and Conformer ASR systems is examined in this section. Experiments are conducted on four datasets, the English UASpeech [58] and TORGO [59] dysarthric speech corpora, as well as the English DementiaBank Pitt [60] and Cantonese JCCOCC MoCA [61] elderly speech datasets. All pre-trained models use multi-stage fine-tuning and incorporate an additional decoder as described previously in Sec. V-B and V-C. SSL speech representation extraction follows the bottleneck module design details of Sec. V-D. 144-dimensional UTI-based articulatory features are extracted following our previous research [47]. Model-based speaker adaptation using learning hidden unit contributions (LHUC) [122] is further applied to in-domain data trained TDNN systems.\nSec. VI-A and VI-B provide details of the experiments conducted on two dysarthric speech corpora and two elderly speech datasets respectively. Results are measured using WER for English corpora and CER for the Cantonese JCCOCC MoCA dataset. A matched pairs sentence-segment word error (MAPSSWE) based statistical significance test [123] at a significance level of $\\alpha = 0.05$ is performed. In Sec. VI-C, speech recognition-based Alzheimer's disease (AD) detection is performed on the DementiaBank Pitt evaluation set (ADReSS2020 [109]) using ASR system outputs."}, {"title": "A. Experiments on Dysarthric Speech", "content": "1) Baseline ASR System Description: For the UASpeech dataset, hybrid LF-MMI factored time delay neural network (TDNN) systems [1] containing 7 context-slicing layers are trained following the Kaldi [124] chain system setup, except that i-Vector features are not incorporated. The E2E Conformer systems are implemented using the ESPNet toolkit13 [125] to directly model grapheme (letter) sequence outputs. 80-dimensional FBK features are utilized in the Conformer systems, while 40-dimensional FBK features and a 3-frame context window are used in the hybrid TDNN system. Following the configurations given in [3]\u2013[6], a uniform language model (LM) with a word grammar network is used in decoding."}, {"title": "B. Experiments on Elderly Speech", "content": "1) Baseline ASR System Description: Following the Kaldi chain system setup, the hybrid TDNN system contains 14 context-slicing layers with a 3-frame context. For all systems, 40-dimensional Mel-scale FBK features are utilized as input. On the English DementiaBank Pitt dataset, for both the hybrid TDNN and E2E graphemic Conformer systems17, a word-level 4-gram language model (LM) with Kneser-Ney smoothing is trained using the SRILM toolkit [126]. A 3.8k word recognition vocabulary covering all words in the DementiaBank Pitt corpus is used during recognition. On the Cantonese JCCOCC MoCA data, the Conformer model training uses Cantonese characters as the output targets. A word-level 4-gram language model with Kneser-Ney smoothing is trained on the transcription (610k words). A 5.2k recognition vocabulary covering all the words in the JCCOCC MoCA corpus is employed.\nDuring the multi-stage fine-tuning of wav2vec2-conformer as presented in Sec. V-B, the encoder is initially fine-tuned on the out-of-domain 960-hr LibriSpeech and then in-domain DementiaBank Pitt corpus for 30 epochs in turn. For the JCCOCC MOCA dataset, the 0.3B version of XLSR-128 [84] is one-stage fine-tuned on in-domain elderly JCCOCC MOCA speech data only for 30 epochs. For both datasets, the added decoder is further fine-tuned for 10 epochs on the in-domain elderly speech data, while the encoder is frozen."}, {"title": "C. Experiments on AD Detection", "content": "In this subsection, the performance of speech recognition based Alzheimer's disease (AD) detection on DementiaBank Pitt evaluation set (ADRESS2020 [109]) is evaluated using the speech transcripts obtained using various pre-trained ASR systems and features. Following our previous researches [105], [106], 768-dimensional outputs from the last hidden layer of BERT [127], or Roberta [128] model, serve as a vector embedding to represent the speech content from each participant's AD assessment interview. BERT and Roberta text encoders are fine-tuned on the 11591-word manual transcripts of the ADRESS training set with the Masked Language Modelling"}, {"title": "VII. DISCUSSION AND CONCLUSIONS", "content": "Our work has revealed the performance fragility of current SSL pre-trained speech foundation models when being applied to dysarthric and elderly speech data that are highly scarce, mismatched and diverse. Their performance disparity is found in several practical scenarios: a) between seen and unseen words in the often very limited dysarthric speech; b)"}]}