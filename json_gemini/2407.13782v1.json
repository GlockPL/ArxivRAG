{"title": "Self-supervised ASR Models and Features For Dysarthric and Elderly Speech Recognition", "authors": ["Shujie Hu", "Xurong Xie", "Mengzhe Geng", "Zengrui Jin", "Jiajun Deng", "Guinan Li", "Yi Wang", "Mingyu Cui", "Tianzi Wang", "Helen Meng", "Xunying Liu"], "abstract": "Self-supervised learning (SSL) based speech foundation models have been applied to a wide range of ASR tasks. However, their application to dysarthric and elderly speech via data-intensive parameter fine-tuning is confronted by in-domain data scarcity and mismatch. To this end, this paper explores a series of approaches to integrate domain fine-tuned SSL pre-trained models and their features into TDNN and Conformer ASR systems for dysarthric and elderly speech recognition. These include: a) input feature fusion between standard acoustic frontends and domain fine-tuned SSL speech representations; b) frame-level joint decoding between TDNN systems separately trained using standard acoustic features alone and those with additional domain fine-tuned SSL features; and c) multi-pass decoding involving the TDNN/Conformer system outputs to be rescored using domain fine-tuned pre-trained ASR models. In addition, fine-tuned SSL speech features are used in acoustic-to-articulatory (A2A) inversion to construct multi-modal ASR systems. Experiments are conducted on four tasks: the English UASpeech and TORGO dysarthric speech corpora; and the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets. The TDNN systems constructed by integrating domain-adapted HuBERT, wav2vec2-conformer or multi-lingual XLSR models and their features consistently outperform the standalone fine-tuned SSL pre-trained models. These systems produced statistically significant WER or CER reductions of 6.53%, 1.90%, 2.04% and 7.97% absolute (24.10%, 23.84%, 10.14% and 31.39% relative) on the four tasks respectively. Consistent improvements in Alzheimer's Disease detection accuracy are also obtained using the DementiaBank Pitt elderly speech recognition outputs.", "sections": [{"title": "I. INTRODUCTION", "content": "DESPITE the rapid progress of automatic speech recognition (ASR) technologies targeting normal speech in recent decades [1], [2], accurate recognition of dysarthric and elderly speech remains highly challenging tasks to date [3]\u2013[14]. Dysarthria is a common type of speech disorder caused by a wide spectrum of motor control conditions including cerebral palsy, amyotrophic lateral sclerosis, stroke and brain injuries. In addition, neurocognitive disorders, such as Alzheimer's disease (AD), are often found among the elderly with speech and language impairments [15], [16].\nASR technologies tailored to dysarthric and elderly users' needs not only improve their quality of life but also enable large-scale automatic early diagnosis of neurocognitive impairment, such as AD [17]\u2013[19]. To this end, recently there has been increasing interest in developing ASR technologies for dysarthric [4], [13], [20]\u2013[43] and elderly users [9], [44]\u2013[57]."}, {"title": "A. Dysarthric and Elderly Speech Recognition Prior to SSL Pre-trained Speech Models", "content": "Dysarthric and elderly speech bring considerable challenges to current ASR technologies primarily targeting normal speech recorded from healthy, non-aged users. These include: a) large mismatch against normal speech due to motor control conditions and aging, e.g., articulation imprecision, decreased speech volume and clarity, and increased disfluency; b) data scarcity due to the difficulty in data collection from these speakers with physical disabilities and mobility issues; and c) large speaker-level diversity."}, {"title": "B. Dysarthric and Elderly Speech Recognition using SSL Pre-trained Speech Models", "content": "The recent emergence of self-supervised learning (SSL) speech foundation models [80]\u2013[84] provides a new paradigm to address the above data scarcity and domain mismatch problems. These speech foundation models are SSL pre-trained on large quantities of unlabelled data. They have been successfully applied to normal speech processing tasks including ASR [80]\u2013[83], [85], speech emotion recognition [86], speaker recognition [87], voice conversion [88] and speech synthesis [89]. In addition, SSL speech representations are robust to domain mismatch [39], [90].\nIn contrast, more limited prior researches on applying SSL pre-trained models to dysarthric and elderly speech have been conducted. Wav2vec2.0 [80] and WavLM [81] models were applied to Japanese electrolaryngeal and English dysarthric speech in [37], where an overall word error rate (WER) of 51.8% was reported on the benchmark UASpeech [58] task. Speaker adaptation of the Wav2vec2.0 model using fMLLR and x-vectors during fine-tuning for dysarthric speech recognition was investigated in [91]. Cross-lingual SSL-based dysarthric speech recognition was studied in [39], where SSL speech representations were extracted from fine-tuned Wav2vec2.0 [80], cross-lingual XLSR [85] and HuBERT [82] models before being fed into the Conformer based ASR systems. Incorporating Wav2vec2.0 models and features into hybrid TDNN and end-to-end Conformer systems was proposed in [14], including input feature fusion, frame-level joint decoding and cross-system multi-pass rescoring. Speech impairment severity was incorporated by adding cross-entropy based severity prediction error in Wav2vec2.0 fine-tuning [12]. Wav2vec2.0 embedding features were used to learn dysarthric speech characteristics in the VAE-GAN based personalized disordered speech augmentation approaches [42]. The authors in [92] proposed to use the AV-HuBERT model to fuse audio and visual modalities to improve the performance of dysarthric speech recognition. A WER of 63.98% on the very low intelligibility group of the benchmark UASpeech dataset was reported. In addition to English, the Wav2vec2.0 model and cross-lingual XLSR model were also evaluated on Dutch dysarthric home-automation data in [93]. Speaker-dependent fine-tuning of SSL pre-trained ASR models for Dutch dysarthric speech was studied in [94]."}, {"title": "C. Key Research Problems and Methodology Design", "content": "1) Pre-trained ASR performance disparity and fairness: Data-intensive fine-tuning a large number of pre-trained ASR model parameters on limited impaired or elderly speech data rapidly leads to poor generalization. This issue is further exacerbated when limited training data provides insufficient coverage. For example, approximately 39% of words in the benchmark UASpeech test set do not occur in the training data. Under such conditions, current mainstream end-to-end ASR systems including SSL pre-trained models have been found to produce large performance disparity on two fronts: a) between seen and unseen words in the often very limited dysarthric speech [12], [25]; and b) between impaired speakers with high and very low speech intelligibility [12], [31].\n2) Use of SSL speech foundation models: Their application to dysarthric and elderly speech needs to account for the underlying issues over data scarcity and domain mismatch. To this end, alternative approaches that can effectively exploit SSL pre-trained ASR models and feature representations, while exhibiting less performance fragility over insufficient data coverage [12], [25] than using the fine-tuned ASR models alone, need to be investigated.\n3) Articulatory features generation and SSL models: Articulatory features are inherently robust to be acoustic signal perturbation. They have been successfully applied to normal and pathological speech [95]\u2013[98] recognition. However, such scarce and specialist data has been traditionally collected mainly from healthy, non-aged English speakers [59], [99]\u2013[103]. This hinders their application to dysarthric and elderly speech across multiple languages. To this end, acoustic-to-articulatory (A2A) inversion techniques further empowered by domain and language invariant SSL pre-trained speech representations need to be developed.\n4) Multi-faceted user applications and evaluation metrics: Advancement of dysarthric and elderly speech recognition technologies in recent decades have broadened their scope of application beyond ASR-based assistive technology to aid communication and improve quality of life for such users [3]\u2013[8], [10]\u2013[14], [25], [42], [47], [65], [104]. Aging presents enormous challenges to health care worldwide. Neurocognitive disorders, such as Alzheimer's disease (AD), are often found among older adults and manifest themselves in speech and language impairments [15], [16]. Dysarthric and elderly speech recognition forms the core technology to facilitate fully automated, large-scale, less intrusive and low-cost neurocognitive disorders screening among the aging population [9], [14], [17]\u2013[19], [45], [46], [48], [51], [53], [56], [105], [106]. To this end, dysarthric and elderly speech recognition technologies tailored for such a wider range of medical domain applications need to be evaluated not only using ASR word error rate, but also additional metrics that are relevant to the specific task such as AD detection.\nIn order to address the above issues, this paper explores a series of techniques to integrate state-of-the-art mono-lingual and multi-lingual SSL pre-trained speech foundation models and their features into hybrid TDNN [1] and Conformer [2] ASR systems for dysarthric and elderly speech recognition.\n1) We aim to exploit the diversity and complementarity among them, and to improve the generalization performance on unseen and poorly covered words as well as on the most challenging dysarthric speech data of very low intelligibility.\n2) These include: a) input feature fusion between standard acoustic frontends and fine-tuned SSL speech representations;"}, {"title": "II. SSL PRE-TRAINED ASR MODELS", "content": "This section gives an overview of SSL pre-trained ASR models, including Wav2vec2.0 [80], HuBERT [82], WavLM [81] and Data2vec [83] models."}, {"title": "A. Pre-trained Wav2vec2.0 Model", "content": "Wav2vec2.0 is a pre-trained model that jointly learns latent contextualized speech representations and an inventory of discretized latent speech units serving as the pseudo-labels. Contrastive learning based SSL pre-training is performed by distinguishing the target from distractor pseudo-labels.\nModel Architecture: Wav2vec2.0 consists of three components, including 1) a multi-layer CNN-based feature encoder which encodes raw speech audio input X into continuous speech representations $z_t \\in Z$ with a stride of 20 ms and a receptive field of 25 ms; 2) a L-layers transformer-based context network producing contextual representations $c_t \\in C$ over a sequence of randomly masked feature encoder outputs; and 3) a quantization module generating discrete speech units $q_t \\in Q$ as pseudo-labels for SSL based pre-training.\nSSL Pseudo-labels: After mapping the feature encoder output $z_t$ to the logits $[l_1, l_2, ..., l_G] \\in R^{G \\times V}$, the best code is chosen among all the V entries of each codebook by Gumbel-softmax re-parameterization. The discrete quantized unit $q_t$ is obtained by applying a linear transformation to the concatenated G codes, before serving as the pre-training pseudo-labels."}, {"title": "SSL Criterion:", "content": "Wav2vec2.0 is pre-trained via an interpolation between a contrastive task $L_c$ and a diversity task $L_d$.\n$L_{w2v,t} = \\underset{E_{Q_t}}{} \\frac{\\alpha}{exp (sim (c_t, q_t)/\\kappa)} log \\frac{exp (sim (c_t, q_t)/\\kappa)}{\\sum_{ \\tilde{q} \\in Q_t} exp (sim (c_t, \\tilde{q})/\\kappa)} + (1-\\alpha) \\sum_{g=1}^{G_V} \\sum_{v=1}^{G_V} \\bar{l_{g,v}} log {1_{g,v}}$\nwhere $sim(c_t, q_t)$ is the cosine similarity between the masked contextual representations produced before and after quantization. $\\bar{q}$ is randomly sampled from $Q_t$ which consists of $q_t$ and \"Distractor\" labels from other masked time steps within the same speech utterance. $\\kappa$ is the non-negative temperature parameter. The entropy-based diversity loss in the second term ensures the pre-training pseudo-labels cover all codebook entries equally. $\\alpha$ is a tuned weight. $\\bar{l}_{g,v}$ is the average logit of the v-th entry in the g-th codebook within a mini-batch.\nFine-tuning: Wav2vec2.0 is fine-tuned in a supervised mode using the Connectionist Temporal Classification (CTC) [110] loss. A randomly initialized linear layer is added on top of the context network to project the contextual representations into vocabulary tokens."}, {"title": "B. Pre-trained HuBERT Model", "content": "The HuBERT [82] model pre-training alternates between two steps: 1) a clustering step to create pseudo-labels; and 2) a prediction step to produce labels for masked positions.\nModel Architecture: The model architecture of HuBERT is similar to Wav2vec2.0, including a feature encoder, a k-means quantization module and a transformer-based context network followed by a projection layer.\nSSL Pseudo-labels: The HuBERT model derives discrete speech units, denoted as $q_t$, to serve as the pseudo-labels during pre-training. These pseudo-labels were produced using a total of G distinct k-means clusters of varying codebook sizes. The clustering process is initially performed on MFCC features. A latent speech representation $\\bar{s}_t$ is quantized into G discrete units $q^1_t, q^2_t, \\dots , q^G_t$. During the iterative refinement of pseudo-labels, the latent speech representation $\\bar{s}_t$ obtained from the context network at the t-th time step are then further k-means clustered to update the pre-training labels.\nSSL Criterion: BERT style prediction of masked discrete speech units based on the following cross-entropy loss function is used in HuBERT pre-training,\n$L_{mask}(Z, {Q^{(g)}}_{g=1}^{G}, M) = \\sum_{t \\in M} E_{g=1}logp (q^{(g)}_t | Z^{(mask)}_t)$"}, {"content": "where M represents the position indices in masked pseudo-label prediction, $Z^{(mask)}_t$ denotes the partially masked version of the continuous speech representation Z at time step t. $p(q^{(g)}_t | Z^{(mask)}_t)$ is the probability of the discrete speech units of the t-th frame assigned by the g-th k-means model.\nFine-tuning: During supervised fine-tuning using the CTC loss, the projection layer is replaced by a randomly initialized linear layer before the model parameters are updated."}, {"title": "C. Pre-trained WavLM Model", "content": "WavLM [81] is also a masked prediction based SSL pre-trained model that shares the same model architecture, discrete speech representation based pre-training and fine-tuning procedures as HuBERT, except for the following two differences: 1) the incorporation of gated relative position bias in the transformer self-attention module; and 2) the use of cocktail party style mixture speech inputs simulated with multiple overlapping speakers and various background noises to produce more acoustic perturbation invariant speech representations."}, {"title": "D. Pre-trained Data2vec Model", "content": "Data2vec [83] is an SSL pre-trained model that shares the same architecture and fine-tuning procedure of Wav2vec2.0. It learns to predict latent speech representations of the complete input audio sequence given a partial view of such input.\nSSL Criterion: The pre-training of Data2vec is performed using an exponential moving average in a teacher-student mode. The teacher parameterization is as [111]:\n$\\Theta^{TM,i} = \\begin{cases}  \\Theta^{SM,0}  , i=0 \\\\  \\gamma \\Theta^{SM,i} + (1 - \\gamma)\\Theta^{TM,i-1}, i>0\\end{cases}$"}, {"content": "where $\\Theta^{TM,i}$ and $\\Theta^{SM,i}$ denote the parameters of the teacher and student models at training step i respectively. $\\gamma$ is the exponential moving average decay rate. For those masked time steps to be predicted by the student model, the training targets $y_t$ are obtained using the average output of the top K transformer blocks of the teacher model's context network. Let $\\hat{c}^m_t$ denotes the normalized output of the l-th transformer block of the teacher model's context network at frame t, the student model's training targets are computed as $\\bar{c}^m_t = \\frac{1}{K} \\sum_{l=L-K+1}^{L} \\hat{c}^m_t$. The regression loss between the targets and outputs of the Data2vec model is given by\n$L_{Data2vec,t} = \\begin{cases} (y_t - c^m_t)^2 /\\beta,  |y_t - c^m_t| < \\beta   \\\\ |y_t - c^m_t| - \\beta ,  otherwise  \\end{cases}$"}, {"content": "where $c^m_t$ is the student model's predicted output at the t-th time step, and the threshold $\\beta$ (e.g., 0.25 [83]), controls the transition from a squared error based loss to an L1 loss."}, {"title": "III. SSL FEATURES BASED A2A INVERSION", "content": "Human speech production involves the coordinated movements of various articulators such as the lips, teeth, tongue and palate. Articulatory movement representations are inherently invariant to extrinsic acoustic distortion. They have been successfully applied to normal and pathological speech [95]\u2013[98] recognition. In practice, recording detailed articulatory movements and vocal tract shape normally requires the use of intrusive techniques such as electromagnetic articulography (EMA) or magnetic resonance imaging (MRI). Compared to EMA and MRI, ultrasound tongue imaging (UTI) [112]\u2013[114] is more portable, non-invasive, and less costly. UTI utilizes B-mode diagnostic ultrasound to capture the tongue surface movements during speech production at a high frame rate [101]. However, there are very few publicly available UTI corpora [99]\u2013[101], all of which are exclusively in English and limited in size. By far the largest Tongue and Lips (TaL) corpus [101] contains 24 hours of parallel ultrasound, video, and audio data collected from 81 native English speakers.\nThe scarcity of such specialist data hinders the practical and wider use of UTI-based articulatory features in ASR systems for normal, atypical speech task domains and across languages. An alternative approach to obtaining articulatory movement"}, {"title": "IV. PRE-TRAINED ASR MODEL INTEGRATION", "content": "Fundamental modeling differences exist between hybrid and end-to-end (E2E) ASR systems including current SSL pre-"}, {"title": "V. ABLATION EXPERIMENTS ON IMPLEMENTATION DETAILS", "content": "In this section, several key implementation issues affecting the performance of TDNN and Conformer ASR systems integrating a range of pre-trained models and their speech representations that are domain fine-tuned to dysarthric and elderly speech are discussed. These include: a) the appropriate fine-tuning strategy and the choice of the most competitive baseline standalone SSL pre-trained models for dysarthric and elderly speech recognition; b) the detailed model structural configurations, and training costs used in these baseline pre-trained models; and c) the architectural modifications of these baseline pre-trained models that are required to extract suitable speech representations, and their fusion with standard acoustic features. A series of ablation studies are conducted on the UASpeech dysarthric speech corpus [58] and the Dementiabank Pitt elderly speech corpus [60]."}, {"title": "A. Task Description", "content": "1) The English UASpeech Corpus: is the largest publicly available and widely used dysarthric speech dataset [58]. It is an isolated word recognition task with 148912 utterances and a vocabulary size of 455, with approximately 103 hours of speech from 29 speakers, among whom 16 are dysarthric speakers and 13 are control healthy speakers. It is further split into 3 blocks, Block 1 (B1), Block 2 (B2), and Block 3 (B3) per speaker, each containing the same set of 155 common words and a different set of 100 uncommon words. After removing excessive silence at both the beginning and end of the speech audio segments using an HTK [118] trained GMM-HMM system, a total of 30.6 hours of audio data from B1 and B3 (99195 utterances) are used as the training set, while 9 hours of dysarthric speech from B2 (26520 utterances) are used for performance evaluation. Standard speaker-independent speed perturbation was used to expand the limited dysarthric speech (or elderly speech) training data using fixed perturbation factors {0.9, 1, 1.1}. Speaker-dependent speed perturbation was also used to modify the healthy, control speech data to that resembling the voice of each target dysarthric (or elderly) talker. For each dysarthric speaker, a perturbation factor is computed based on phonetic alignment analysis as described in [66]. Such data augmentation produces a 130.1 hours augmented training set (399110 utterances).\n2) The English TORGO Corpus [59]: is a dysarthric speech dataset containing 8 dysarthric and 7 control healthy speakers with 13.5 hours of speech (16433 utterances). Similar to the setting of UASpeech, a speaker-level data partition is conducted by combining all the 7 control healthy speakers' data and two-thirds of the 8 dysarthric speakers' data into the training set (11.7 hours). The remaining one-third of the dysarthric speech is used for performance evaluation (1.8 hours). After the removal of excessive silence and data augmentation [22], [119], the augmented training and test sets respectively contain 34.1 hours (61813 utterances) and 1 hour (1892 utterances) of speech. The entire TORGO dataset contains 1573 distinct words.\n3) The English DementiaBank Pitt Corpus [60]: contains roughly 33 hours of audio recorded from 292 AD assessment interviews between elderly participants and clinical investigators. It is further divided into a 27.2h training set, a 4.8h development and a 1.1h evaluation set. The evaluation set contains the same 48 speakers' Cookie Theft (picture description) task recordings as those in the ADRESS [109] test set, while the development set covers the recordings of these speakers in other tasks, if available. The training set includes 688 speakers (244 elderly participants and 444 investigators), while the development and evaluation sets contain 119 speakers (43 elderly participants and 76 investigators) and 95 speakers (48 elderly participants and 47 investigators) respectively. There is no overlapping between the elderly speakers in the training, development and evaluation sets. After silence stripping [9] and data augmentation via both speaker-independent and elderly speaker-dependent speed perturbation [9], the augmented training set contains 58.9 hours of audio (112830 utterances) while the development and evaluation sets contain 2.5 hours (5103 utterances) and 0.6 hours (928 utterances) of audio, respectively.\n4) The Cantonese JCCOCC MoCA Corpus: contains conversations from 256 cognitive impairment assessment interviews between elderly participants and clinical investigators [61]. The training set includes 369 speakers (158 elderly participants and 211 investigators) with a duration of 32.4 hours. The development and evaluation sets each contain speech from two different sets of 49 elderly speakers not covered by the training set. After silence stripping and data augmentation similar to that used for the DementiaBank Pitt dataset, the augmented training set contains 156.9 hours of audio (389409 utterances) while the development and evaluation sets contain 3.5 hours (13675 utterances) and 3.4 hours (13414 utterances) of audio, respectively."}, {"title": "B. Baseline Pre-trained ASR Model Fine-tuning", "content": "Two aspects of the baseline standalone pre-trained models are examined in the subsection. First, the optimal strategy of supervised fine-tuning on limited dysarthric and elderly speech data based on either: a) a single-stage fine-tuning only on sufficient quantities of out-of-domain normal speech, for example, the 960-hour LibriSpeech data; b) a single-stage fine-tuning only on limited dysarthric or elderly speech; or c) a two-stage fine-tuning performed in turn on normal speech and in-domain dysarthric or elderly speech data. Second, the precise SSL pre-trained model to choose among the following popular forms: Wav2vec2.02, Conformer based Wav2vec2.0 (wav2vec-conformer) with relative position embeddings\u00b3, WavLM\u2074, Hu-BERT\u2075, Data2vec audio model and cross-lingual XLSR-537.\nSeveral trends can be found in the results of Table II. 1) among all six pre-trained models, the above two-stage fine-tuning on the 960-hour LibriSpeech or Common Voice data first, followed by the in-domain UASpeech or DementiaBank Pitt data consistently produced the best performance across both types of data (Sys. 3, 6, 9, 12, 15, 18 vs. the remaining systems in Table II); 2) for the UASpeech dysarthric speech data, the two-stage fine-tuned HuBERT model (Sys. 9, col. 10), which produced the lowest average WER of 27.71%, is adopted in the main experiments of the following Sec. VI for dysarthric speech; 3) for the DementiaBank Pitt elderly speech data, the two-stage fine-tuned wav2vec-conformer model produced the lowest average WER of 21.60% (Sys. 6, last col.), and is selected for the following experiments of Sec. VI for the same task8.\nA comparable set of ablation studies are conducted on the Cantonese JCCOCC MoCA elderly speech dataset using the multi-lingual XLSR-53 model as presented in Table III (Sys. 1-3)10. Due to a potential mismatch between the elderly JCCOCC MOCA corpus and Common Voice dataset, a single-"}, {"title": "C. Improved Model Architecture and Fine-tuning Criteria", "content": "In most previous researches, the ASR fine-tuning loss function of SSL pre-trained models is predominantly based on CTC [110]. In order to improve the performance of pre-trained models during domain fine-tuning, the multi-task combination between CTC and attention based ASR costs [120], [121], which are widely used in encoder-decoder based ASR systems [2], are studied in this subsection. A number of further implementation issues appertaining to adjusting the best model complexity versus performance trade-off during pre-trained model fine-tuning are also investigated as follows: a) the number of transformer blocks in the decoder; b) the choice between either updating the decoder alone use the attention loss while freezing the pre-trained encoder, or jointly updating both the encoder and decoder using interpolated CTC and attention loss11. The encoder parameters of the attention encoder-decoder (AED) model are initialized using the domain-adapted pre-trained models in Sec. V-B (i.e. HuBERT for UASpeech data shown as Sys. 9 and wav2vec2-conformer for Dementiabank Pitt corpus shown as Sys. 6 in Table II, respectively). The added decoder module is built using stacked transformer blocks with feedforward layer dimensionality set to 2048, each of which has 8 attention heads and an input dimensionality of 128.\nSeveral trends can be found in the results of Table IV: 1) the fine-tuned pre-trained models with an added decoder module and its attention training cost outperformed those without such (Sys. 2-4 vs. Sys. 1); 2) using a decoder module containing 6 transformer layers, instead of 3 or 12, produced overall more balanced and competitive performance improvements over the comparable baseline fine-tuned HuBERT or wav2vec2-conformer models across both the UASpeech and Dementiabank Pitt tasks (Sys. 3 vs. Sys. 2, 4), when separately fine-tuning the encoder and decoder in turn; 3) further joint fine-tuning both the encoder and decoder on the in-domain data produced no consistent performance improvement (Sys. 5 vs. Sys. 3). Based on these trends, the encoders of the HuBERT, wav2vec2-conformer models are two-stage fine-tuned12 using the CTC loss on the out-of-domain normal data first, followed by in-domain dysarthric or elderly speech data in turn as set out in Sec. V-B. The added decoder of 6 transformer layers is then fine-tuned using the attention loss on the in-domain data. Such settings are adopted in the main experiments of Sec. VI."}, {"title": "D. SSL Speech Representation Extraction and Fusion", "content": "The SSL pre-trained speech model can be used as a standalone speech recognition system after task fine-tuning. Alternatively, their speech representations can be incorporated into various back-end target domain data trained ASR systems via feature fusion based on, e.g., TDNN or Conformer, as considered in this paper. To this end, a \u201cBottleneck Module\" (the upper part of Fig. 2, in the orange box) is introduced into the SSL pre-trained model to produce more compact speech representations. The \u201cBottleneck module\" contains a stack of four interleaving convolutional and feed-forward layers: the first 1D transposed de-convolution CNN layer is used to change the stride length from 20 ms to 10 ms to allow a frame rate synchronization with that of the back-end ASR systems; a fully connected (FC) block, which consists of a linear layer, rectified linear unit (ReLU) activation and dropout module, is used to change the dimensionality of extracted features. This is followed by a CNN layer and a final FC block to revert the stride back to 20 ms and restore the dimensionality to 1024. The final SSL speech representations are extracted from the first FC layer.\nIn this part of the ablation study, the effect of 1) the form of feature fusion between SSL speech representations and front-end acoustic filter-bank (FBK) features; 2) the dimensionality of extracted features (128, 256 and 512); and 3) the position of the bottleneck module on the final system performance are analyzed. As shown in the left upper part of Fig. 2, three different positions of the bottleneck module (orange box in Fig. 2) are investigated: (e) after the CNN feature encoder and before the contextual transformer network; (f) immediately after the 12th transformer block inside the context network that contains a total of 24 transformer blocks; and (g) after the last, i.e. 24th transformer block of the context network."}, {"title": "VI. MAIN RESULTS", "content": "The performance of the proposed methods to incorporate domain fine-tuned pre-trained speech models and their features into hybrid TDNN and Conformer ASR systems is examined in this section. Experiments are conducted on four datasets, the English UASpeech [58] and TORGO [59] dysarthric speech corpora, as well as the English DementiaBank Pitt [60] and Cantonese JCCOCC MoCA [61] elderly speech datasets. All pre-trained models use multi-stage fine-tuning and incorporate an additional decoder as described previously in Sec. V-B and V-C. SSL speech representation extraction follows the bottleneck module design details of Sec. V-D. 144-dimensional UTI-based articulatory features are extracted following our previous research [47]. Model-based speaker adaptation using learning hidden unit contributions (LHUC) [122] is further applied to in-domain data trained TDNN systems.\nSec. VI-A and VI-B provide details of the experiments conducted on two dysarthric speech corpora and two elderly speech datasets respectively. Results are measured using WER for English corpora and CER for the Cantonese JCCOCC MoCA dataset. A matched pairs sentence-segment word error (MAPSSWE) based statistical significance test [123] at a significance level of $\\alpha$ = 0.05 is performed. In Sec. VI-C, speech recognition-based Alzheimer's disease (AD) detection is performed on the DementiaBank Pitt evaluation set (ADReSS2020 [109]) using ASR system outputs."}, {"title": "A. Experiments on Dysarthric Speech", "content": "1) Baseline ASR System Description: For the UASpeech dataset, hybrid LF-MMI factored time delay neural network (TDNN) systems [1] containing 7 context-slicing layers are trained following the Kaldi [124] chain system setup, except that i-Vector features are not incorporated. The E2E Conformer systems are implemented using the ESPNet toolkit13 [125] to directly model grapheme (letter) sequence outputs. 80-dimensional FBK features are utilized in the Conformer systems, while 40-dimensional FBK features and a 3-frame context window are used in the hybrid TDNN system. Following the configurations given in [3]\u2013[6], a uniform language model (LM) with a word grammar network is used in decoding."}, {"title": "B. Experiments on Elderly Speech", "content": "1) Baseline ASR System Description: Following the Kaldi chain system setup"}, {"content": "OCA dataset, the 0.3B version of XLSR-128 [84] is one-stage fine-tuned on in-domain elderly JCCOCC MOCA speech data only for 30 epochs. For both datasets, the added decoder is further fine-tuned for 10 epochs on the in-domain elderly speech data, while the encoder is frozen.\n2) Performance Analysis on Elderly Speech: Several trends are found among the DementiaBank Pitt results of Table VIII: i) Compared with the TDNN system using 40-dimensional FBK features, the standalone domain fine-tuned wav2vec2-conformer incorporating the in-domain data constructed 4-gram LM produced a large overall WER reduction of 13.69% absolute (40.50% relative, Sys. 5 vs. Sys. 1).\nii) The TDNN systems constructed by fusing the FBK and fine-tuned wav2vec2-conformer features, as well as the cross-domain inverted UTI-based articulatory features, produced a statistically significant overall WER reduction of 1.03% absolute (5.12% relative) over the standalone fine-tuned wav2vec2-conformer baseline model (Sys. 7 vs. Sys. 5).\niii) 3-way frame-level joint decoding among the LHUC speaker adapted TDNN system trained on FBK features, and that constructed using FBK plus domain-adapted wav2vec2-conformer features, as well as that with additional inverted UTI articulatory features (Sys. 3+6+7, shown as Sys. 10)18 produced a statistically significant overall WER reduction of 1.42% absolute (7.06% relative) over the standalone domain-adapted wav2vec2-conformer model (Sys. 10 vs. Sys. 5).\niv) Cross-system multi-pass rescoring the 3-way joint decoding's N-best (N=30) outputs using the standalone fine-tuned wav2vec2-conformer gave a statistically significant overall"}, {"title": "C. Experiments on AD Detection", "content": "In this subsection, the performance of speech recognition based Alzheimer's disease (AD) detection on DementiaBank Pitt evaluation set (ADRESS2020 [109]) is evaluated using the speech transcripts obtained using various pre-trained ASR systems and features. Following our previous researches [105], [106], 768-dimensional outputs from the last hidden layer of BERT [127], or Roberta [128] model, serve as a vector embedding to represent the speech content from each participant's AD assessment interview. BERT and Roberta text encoders are fine-tuned on the 11591-word manual transcripts of the ADRESS training set with the Masked Language Modelling (MLM) and Next Sentence Prediction tasks using 15 different random seeds. Since the fine-tuning objective is based on the MLM cost instead of AD classification error, the detection accuracy scores measured at consecutive BERT or Roberta fine-tuning epochs in practice fluctuate. Models obtained at the final three update epochs during the 30-epoch fine-tuning were used to produce separate text embedding features for the back-end SVM-based AD classifiers. Hence, BERT and Roberta's representations were used to construct separate AD classifiers and evaluated independently. Their respective AD detection outputs are combined by majority voting to reduce the risk of over-fitting and smooth the unstable performance. Mean, standard deviation and best AD detection scores are calculated.\nWe then investigate the correlation between ASR performance and AD detection accuracy, and in particular, whether the elderly speech recognition performance improvements obtained in the experiments of Sec. VI-B will translate to higher AD detection accuracy. To this end, a diverse set of ASR systems are selected to generate the elderly adults' speech transcripts for downstream text-based AD detection. These include: A) the wav2vec2-conformer model without decoder one-stage fine-tuned on out-of-domain 960 hours of LibriSpeech (Sys. 4 in Table II); B) the LHUC speaker adapted TDNN system trained using the FBK features of the in-domain DementiaBank Pitt data (Sys. 3 in Table VIII); C) the standalone domain fine-tuned wav2vec2-conformer model with an external in-domain data constructed 4-gram language model (Sys. 5 in Table VIII); D) N-best outputs produced by the 3-way TDNN joint decoding, which are further cross-system multi-pass rescored using the fine-tuned wav2vec2-conformer model (Sys. 11 in Table VIII); E) the standalone domain fine-tuned XLSR-53 model with an external in-domain data constructed 4-gram language model; and F) N-best outputs produced by the 3-way TDNN joint decoding, which are further cross-system multi-pass rescored using the fine-tuned"}, {"title": "VII. DISCUSSION AND CONCLUSIONS", "content": "Our work has revealed the performance fragility of current SSL pre-trained speech foundation models when being applied to dysarthric and elderly speech data that are highly scarce, mismatched and diverse. Their performance disparity is found in several practical scenarios: a) between seen and unseen words in the often very limited dysarthric speech; b) between impaired speakers with high and very low speech intelligibility; and c) between non-aged clinical investigators and elderly adults, for example, in the DementiaBank Pitt data. To mitigate the above performance disparity issues, this paper explores a series of approaches to integrating cross-domain adapted SSL pre-trained foundation models and their features into TDNN and Conformer ASR systems for dysarthric and elderly speech recognition. Domain-adapted SSL speech representations are further utilized in acoustic-to-articulatory (A2A) inversion to construct multi-modal dysarthric and elderly ASR systems. Experiments conducted on four dysarthric or elderly speech datasets across two languages suggest that the proposed SSL pre-trained model and feature integration approaches can effectively improve the final ASR system's generalization performance on the highly scarce and mismatched dysarthric and elderly speech data. The TDNN systems constructed by integrating domain-adapted HuBERT, wav2vec2-conformer or multi-lingual XLSR models and their features consistently outperform the standalone fine-tuned SSL models by statistically significant WER or CER reductions of 6.53%, 1.90%, 2.04% and 7.97% absolute (24.10%, 23.84%, 10.14% and 31.39% relative) on the four tasks respectively. The lowest published WERs of 20.56% (50.70% on very low intelligibility, 34.28% on unseen words) and 18.07% are obtained on the benchmark UASpeech test set of 16 dysarthric speakers and the DementiaBank Pitt evaluation set of 48 elderly subjects respectively. Future research will focus on the rapid personalization of pre-trained ASR models for diverse dysarthric and elderly speakers, and effective pre-trained model compression approaches to improve their efficiency for practical deployment."}]}