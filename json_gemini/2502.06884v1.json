{"title": "Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models", "authors": ["Sina Tayebati", "Divake Kumar", "Nastaran Darabi", "Dinithi Jayasuriya", "Ranganath Krishnan", "Amit Ranjan Trivedi"], "abstract": "Large Language and Vision-Language Models (LLMs/VLMs) are increasingly used in safety-critical applications, yet their opaque decision-making complicates risk assessment and reliability. Uncertainty quantification (UQ) helps assess prediction confidence and enables abstention when uncertainty is high. Conformal prediction (CP), a leading UQ method, provides statistical guarantees but relies on static thresholds, which fail to adapt to task complexity and evolving data distributions, leading to suboptimal trade-offs in accuracy, coverage, and informative-ness. To address this, we propose learnable conformal abstention, integrating reinforcement learning (RL) with CP to optimize abstention thresholds dynamically. By treating CP thresholds as adaptive actions, our approach balances multiple objectives, minimizing prediction set size while maintaining reliable coverage. Extensive evaluations across diverse LLM/VLM benchmarks show our method outperforms Least Ambiguous Classifiers (LAC) and Adaptive Prediction Sets (APS), improving accuracy by up to 3.2%, boosting AUROC for hallucination detection by 22.19%, enhancing uncertainty-guided selective generation (AUARC) by 21.17%, and reducing calibration error by 70%-85%. These improvements hold across multiple models and datasets while consistently meeting the 90% coverage target, establishing our approach as a more effective and flexible solution for reliable decision-making in safety-critical applications. The code is available at https://github.com/sinatayebati/vlm-uncertainty.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language and Vision-Language Models (LLMS/VLMS) are rapidly becoming indispensable in safety-critical applications, from autonomous systems [1] to healthcare diagnostics [2]. Their ability to process and interpret information across visual and textual modalities presents unprecedented opportunities for complex decision-making. However, their internal workings remain opaque, making it challenging to identify biases, vulnerabilities, and unintended consequences, which hinders effective risk assessment and mitigation. Traditional risk management frameworks, designed for static systems with well-defined rules, struggle to keep pace with the evolving capabilities and emergent behaviors of these models [3]. As decision support systems increasingly rely on LLM/VLM, equipping them with robust mechanisms to identify and manage their prediction risks has become crucial.\nUncertainty quantification (UQ) of LLM/VLM has therefore gained significant attention for assessing prediction reliability and enabling abstention-allowing models to defer decisions when uncertainty is high. However, state-of-the-art UQ methods like conformal prediction (CP), while providing statistical guarantees, rely on static thresholds that fail to adapt to varying task complexities or evolving data distributions. Abstention strategies built on these methods therefore remain inflexible, treating abstention as a binary choice-predict or abstain [4]-without adapting to context. Consequently, state-of-the-art methods such as Least Ambiguous Classifiers (LAC) [5] tend to produce overly narrow prediction sets, sacrificing coverage, while Adaptive Prediction Sets (APS) [6] generate excessively large sets.\nTo address these limitations, we propose a framework for learnable conformal abstention, where models dynamically adjust abstention decisions based on task complexity and evolving data distributions. By integrating reinforcement learn-ing with conformal prediction, our approach enables adap-tive thresholding that surpasses static methods in accuracy, coverage, and reliability. Extensive evaluations across diverse benchmarks demonstrate its effectiveness in improving risk management, selective abstention, and overall decision-making in safety-critical LLM/VLM applications. In particular, our learned policy boosts hallucination detection by up to 22%, improves uncertainty-guided selective generation by more than 20% in certain scenarios, and reduces expected calibration error by 70%-85% compared to standard conformal baselines. Notably, it also sustains at least 90% coverage while reducing the average prediction set size."}, {"title": "II. BACKGROUND", "content": "Uncertainty Quantification (UQ) of Prediction Models:\nSeveral approaches have been explored to capture and manage uncertainty in machine learning models. Conformal prediction [7]\u2013[9] provides a distribution-free, model-agnostic frame-work for generating prediction sets with statistical guarantees, with advances such as inductive, split, and cross-conformal prediction addressing different calibration strategies. Extensions include handling distribution shifts, sequential data, and active learning. Evidential learning captures uncertainty by modeling distributions over parameters or predictions, with"}, {"title": "III. LEARNING CONFORMAL ABSTENTION POLICIES", "content": "We propose a novel framework for learning an absten-tion policy that leverages conformal prediction to generate uncertainty-aware prediction sets with statistical guarantees. The proposed framework, conformalized abstention policy (CAP), allows three possible outcomes per query: a single prediction, a set of plausible predictions, or abstention, balanc-ing informativeness and risk based on prediction confidence.\nWe formulate this as a reinforcement learning (RL) problem, optimizing the CP hyperparameters ($\\alpha$ and $\\beta$) as actions using the REINFORCE [17].\nFirst, to quantify the uncertainty of LLM/VLM responses, we use a nonconformity measure based on softmax proba-bilities. Given an input x, the model produces logits l = $[l_1, l_2, ..., l_k]$ for K classes, which are converted to prob-abilities $p_i(x)$ using softmax. Using a calibration dataset $\\mathcal{D}_{cal} = \\{(X_i, Y_i)\\}_{i=1}^{n}$, where $Y_i$ is the ground-truth label, we compute the nonconformity score for each sample as $score(x_i) = 1 - p_{y_i}(x_i)$. This score quantifies nonconformity, with higher values indicating greater uncertainty. Traditional conformal prediction defines a single threshold as the (1-$\\alpha$)-quantile of the calibration scores:\n$\\hat{q} = Quantile_{1-\\alpha}(\\{s_i\\}_{i=1}^{n})$.\nWe extend this by introducing two thresholds, $\\hat{q}_{predict}$ and $\\hat{q}_{abstain}$, computed as:\n$\\begin{aligned}\\hat{q}_{predict} &= Quantile(\\{s_1, ..., s_n\\}, \\lfloor (n+1)(1-\\alpha) \\rfloor) \\\\\\hat{q}_{abstain} &= Quantile(\\{s_1, ..., s_n\\}, \\lfloor (n+1)(1-\\beta) \\rfloor)\\end{aligned}$\nThese thresholds partition the nonconformity score of each test sample into three regimes:\n1) $score(x) < \\hat{q}_{predict} \\Rightarrow$ Single best prediction.\n2) $\\hat{q}_{predict} < score(x) < \\hat{q}_{abstain} \\Rightarrow$ Set prediction.\n3) $score(x) \\geq \\hat{q}_{abstain} \\Rightarrow$ Abstain.\nAction Probabilities and Stochastic Decisions: We extend the deterministic three-regime decision with a stochastic policy that maps the nonconformity score to action probabilities. Let: $s(x) = score(x) = 1 - max_i p_i(x)$. Using thresholds $\\hat{q}_{predict}$ and $\\hat{q}_{abstain}$, the action probabilities are defined as:\n$\\begin{aligned}p_{single}(s(x)) &= \\sigma(-c[s(x) - \\hat{q}_{predict}]) \\\\p_{abstain}(s(x)) &= \\sigma(c[s(x) - \\hat{q}_{abstain}])\\end{aligned}$\nwhere $\\sigma(z) = 1/(1+e^{-z})$ is the sigmoid function, and $c > 0$ is a scaling constant. The probability of a set prediction is: $p_{set}(s(x)) = 1 - p_{single}(s(x)) - p_{abstain}(s(x))$. For each test point, we stochastically select from \\{single, set, abstain\\} based on \\{psingle, pset, pabstain\\}, capturing model uncertainty.\nReinforcement Learning and Abstention Policy: To dynamically adjust the confidence levels ($\\alpha$, $\\beta$) for optimal performance, we employ a policy-based reinforcement learn-ing approach using the REINFORCE algorithm. The policy network $\\pi_{\\theta}(\\alpha, \\beta)$ learns a distribution over these parameters. We treat $\\alpha$ and $\\beta$ as actions sampled from a multivariate Gaussian distribution defined by the policy network. Let: $(\\mu_{\\theta}, \\sigma_{\\theta}) = f_{\\theta}(s)$, where s is the current state, and $f_{\\theta}$ is a neural network mapping s to the mean and standard deviation vectors $(\\mu_{\\theta}, \\sigma_{\\theta})$. Specifically,\n$\\begin{aligned}\\alpha &\\sim N((\\mu_{\\theta}^{(\\alpha)}), (\\sigma_{\\theta}^{(\\alpha)})^2), \\\\\\beta &\\sim N((\\mu_{\\theta}^{(\\beta)}), (\\sigma_{\\theta}^{(\\beta)})^2).\\end{aligned}$\nAt each iteration, the process involves: (1) sampling $\\alpha$, $\\beta$ from the learned distribution, (2) computing the thresholds"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "We conducted a thorough empirical evaluation to benchmark our proposed CAP framework against the comparative least ambiguous set-valued classifier (LAC) and adaptive predic-tion sets (APS). The experiments focus on multiple-choice question answering (MCQA) tasks, assessing six key metrics: confidence ranking for hallucination detection, uncertainty-guided selective generation, coverage, set size, calibration, and accuracy. This evaluation systematically measures the effec-tiveness of the proposed abstention policy and the reliability of uncertainty estimates."}, {"title": "A. Experimental Settings", "content": "Datasets: We used a diverse collection of ten benchmark LLM/VLM datasets. These datasets are designed for multiple-choice question-answering (MCQA) across various reasoning tasks and uncertainty scenarios. For VLMs, we employ five datasets: (i) MMBench [18], a multi-modal benchmark with 4,000 questions spanning perception and reasoning tasks, standardized to four options; (ii) OODCV-VQA [19], focusing on out-of-distribution instance counting via its \u201cDigits\u201d subset, expanded to four options; (iii) ScienceQA [20], containing 3,952 image-based questions across natural and social sci-ences; (iv) SEEDBench [21], evaluating visual understanding (e.g., object localization) with 14,233 questions; and (v) AI2D [22], featuring 15,000 diagram-based science questions extended to six options. All datasets are reformatted to multiple-choice questions (MCQA) with four or six options to assess uncertainty handling.\nFor LLMs, we evaluate on five tasks: (i) MMLU [23], a question-answering benchmark spanning 57 academic sub-jects; (ii) CosmosQA [24], focusing on reading comprehen-sion requiring contextual inference; (iii) HellaSwag [25], as-sessing commonsense inference for event followup prediction; (iv) HaluDial [26], evaluating dialogue response selection from knowledge-grounded conversations; and (v) HaluSum [26], testing document summarization on news articles. Each dataset is standardized to six options (including \"I don't know\" and \"None of the above\") to align with uncertainty-aware evaluation protocols. This selection ensures diverse assessment of LLM capabilities in knowledge recall, reasoning, and ab-stention under ambiguity.\nModels: We evaluated on a diverse set of LLM/VLM models with parameter scales ranging from 2.7B to 34B. For VLMs, the main body of the paper includes results for the LLaVA-v1.6 series (34B, 13B, and 7B parameters) [27]. Ad-ditional state-of-the-art VLMs\u2014such as the lightweight MoE-LLaVA-Phi2 2.7B [28], Monkey-Chat 7B [29], InternLM-XComposer2-VL 7B [30], Yi-VL 6B [31], CogAgent-VQA 7B [32], MobileVLMV2 Appendix C.\nFor LLMs, the main body presents results for the Yi 34B model [31] and the Qwen series (7B and 14B parameters) [33]. Results for the Llama-2 foundation model series (7B and 13B parameters) are included in Appendix C.\nEvaluation Metrics: CAP is evaluated using the following metrics that assess both prediction quality and UQ, capturing its ability to produce single predictions, set predictions, or ab-stentions. The same metrics are applied to baseline conformal methods, including APS and LAC, following [16], [34]:\nAccuracy: For a test input $X_t$ with true label $Y_t$, let $C(X_t)$ denote the generated prediction set. If a single prediction $Y_t$ is produced (e.g., in confident scenarios under ATCP), accuracy is binary: 1 if $Y_t = Y_i$, and 0 otherwise. For set predictions, accuracy is computed fractionally, inversely proportional to the size of $C(X_t)$ when $Y_t \\in C(X_t)$.\nCoverage: Coverage measures the fraction of instances where the correct label is included in the model's out-put\u2014either as a single prediction or within a prediction set. In"}, {"title": "V. CONCLUSION", "content": "In this work, we propose a reinforcement learning-based approach to adaptively configure conformal prediction thresh-olds for selective abstention in large language and vision-language models. By dynamically adjusting the decision boundary between single-label, set-valued predictions, and abstentions, our method overcomes the limitations of static conformal approaches, such as rigid coverage-uncertainty trade-offs and suboptimal confidence calibration. Extensive evaluations across diverse tasks-from multiple-choice QA to image-based reasoning\u2014demonstrate that our learned con-formal abstention policy (CAP) outperforms APS and LAC, achieving higher accuracy, maintaining coverage guarantees, shrinking prediction sets, and reducing calibration error. Notably, CAP enhances hallucination detection and uncertainty-guided selective generation, highlighting the potential of cou-pling conformal prediction with adaptive policies for robust risk management in foundation models."}, {"title": "APPENDIX A", "content": "FORMAL PROOF OF CONFORMAL COVERAGE GUARANTEE\nWe provide here a classic proof of the coverage property for standard (single-threshold) conformal prediction under i.i.d. assumptions. In the main text, this lays the foundation for our two-threshold extension (see Section III), where an additional threshold is introduced to distinguish between single-label predictions, set-valued predictions, and abstentions. Despite that extension, the core argument below underpins the claimed coverage guarantee at level 1 \u2013 $\\alpha$.\nTheorem 1 (Conformal Coverage Guarantee). Let $\\{(X_i, Y_i)\\}_{i=1}^{n+1}$ be i.i.d. samples from an unknown distribution, partitioned into:\n*   A calibration set of size n: $\\{(X_i, Y_i)\\}_{i=1}^{n}$\n*   A test point $(X_{n+1}, Y_{n+1})$.\nSuppose a nonconformity score function $s(\\cdot, \\cdot)$ assigns a real-valued score $s(X_i, Y_i)$ to each calibration sample, capturing how \"atypical\u201d or \u201cnonconforming\u201d the pair $(X_i, Y_i)$ appears relative to a prediction model. Denoting\n$s_i = s(X_i, Y_i), i = 1,...,n,$\nlet $\\hat{q}$ be the $(1 - \\alpha)$-quantile of these calibration scores:\n$\\hat{q} = Quantile(\\{s_1, ..., s_n\\}, 1 - \\alpha)$.\nThen we define the conformal prediction set for the test point $(X_{n+1,})$ as\n$C(X_{n+1}) = \\{y:s(X_{n+1},y) \\leq \\hat{q}\\}.$\nUnder the i.i.d. assumption, this set satisfies\n$Pr(Y_{n+1} \\in C(X_{n+1})) \\geq 1-\\alpha$.\nProof. Because the samples $\\{(X_i, Y_i)\\}_{i=1}^{n+1}$ are assumed ex-changeable (i.i.d.), any permutation of the $n + 1$ points is equally likely. Consider a random permutation $\\pi$ of the indices $\\{1,...,n+1\\}$, and let\n$(X_i, Y_i) = (X_{\\pi(i)}, Y_{\\pi(i)})$\nbe the permuted data. We then treat the first n permuted samples as a calibration set, computing their nonconformity scores,\n$\\hat{s_i} = s(X_i, Y_i), i = 1,...,n,$\nand defining\n$\\hat{q} = Quantile(\\{\\hat{s_1}, ..., \\hat{s_n}\\}, 1 - \\alpha)$.\nThe point $(X_{n+1}, Y_{n+1})$ is then the \u201ctest\u201d sample in this permuted view, for which the conformal set is\n$C(X_{n+1}) = \\{y: s(X_{n+1},y) \\leq \\hat{q}\\}.$\nWe must show that $Pr(Y_{n+1} \\in C(X_{n+1})) \\geq 1 - \\alpha$ with respect to the randomness of both the original samples and the random permutation. Note that $Y_{n+1}$ belongs to $C(X_{n+1})$ precisely if its nonconformity score\n$\\hat{s}_{n+1} = s(X_{n+1}, Y_{n+1})$\ndoes not exceed the $(1 - \\alpha)$-quantile $\\hat{q}$. Equivalently, $\\hat{s}_{n+1}$ is at most the $\\lfloor (n+1)(1-\\alpha) \\rfloor$-th largest among $\\{\\hat{s_1},..., \\hat{s_{n+1}}\\}$. By symmetry, $\\hat{s}_{n+1}$ is equally likely to appear in any rank among the n + 1 scores $\\hat{s_1},..., \\hat{s_{n+1}}$. Hence, the probability that $\\hat{s}_{n+1}$ falls above that critical rank is at most $\\alpha$. Therefore,\n$Pr(\\hat{Y}_{n+1} \\notin C(\\hat{X}_{n+1})) \\leq \\alpha,$\nand so\n$Pr(\\hat{Y}_{n+1} \\in C(\\hat{X}_{n+1})) \\geq 1-\\alpha.$\nReversing the permutation $\\pi$ simply reverts the data to its original indexing. Because all permutations are equally likely, we conclude that, for the original test point $(X_{n+1}, Y_{n+1})$,\n$Pr(Y_{n+1} \\in C(X_{n+1})) \\geq 1-\\alpha$.\nInterpretation in the Context of Two-Threshold Policies.\nAlthough Theorem 1 is stated for a single threshold $\\hat{q}$, the rank-based argument holds equally under mild modifications when additional thresholds are introduced. In the main text, we exploit two thresholds to partition nonconformity scores into regimes that yield single-label predictions, set-valued predic-tions, or abstentions. The coverage requirement is preserved provided that the relevant thresholds are computed against $\\{s_1,..., s_n\\}$ (the calibration scores) and remain within the same unified conformal scoring framework. As a result, the final coverage probability for the true label $Y_{n+1}$ remains at least $1 - \\alpha$, up to the statistical deviations governed by the i.i.d. assumption on $\\{(X_i, Y_i)\\}$.\nIn our method (see Section III in the main paper), we further optimize these thresholds via reinforcement learning to improve accuracy, set size, and abstention outcomes. Nonethe-less, the conformal criterion ensures that the proportion of samples for which the correct label lies outside the conformal set remains bounded by $\\alpha$."}, {"title": "APPENDIX B", "content": "TRAINING VIA REINFORCEMENT LEARNING\nalgorithm 1 summarizes the training of our proposed adap-tive conformal environment and abstention policy."}, {"title": "APPENDIX C", "content": "EXPERIMENTAL DETAILS\nA. Datasets\nThis section provides details about the datasets used in our evaluation. We focus on two groups of datasets: one for Vision-Language Models (VLMs) on multiple-choice visual question answering (MCQA) tasks, and another for Language Models (LLMs) across multiple tasks. Below, we describe the VLM datasets in detail.\nDatasets for VLMs\nFor the evaluation of Vision-Language Models, we focus on multiple-choice visual question"}]}