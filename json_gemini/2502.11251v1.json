{"title": "Explaining Necessary Truths", "authors": ["G\u00fclce Karde\u015f", "Simon DeDeo"], "abstract": "Knowing the truth is rarely enough we also seek out reasons why the fact is true. While much is known about how we explain contingent truths, we understand less about how we explain facts, such as those in mathematics, that are true as a matter of logical necessity. We present a framework, based in computational complexity, where explanations for deductive truths co-emerge with discoveries of simplifying steps during the search process. When such structures are missing, we revert, in turn, to error-based reasons, where a (corrected) mistake can serve as fictitious, but explanatory, contingency-cause: not making the mistake serves as a reason why the truth takes the form it does. We simulate human subjects, using GPT-40, presented with SAT puzzles of varying complexity and reasonableness, validating our theory and showing how its predictions can be tested in future human studies.", "sections": [{"title": "Introduction", "content": "Explanations\u2014the \"reasons why\" something is true-often refer to the contingencies of life. What we identify, for example, as the reason why we passed an exam, will often refer to things that could have gone differently: we passed the exam, for example, because we studied hard (but might not have), because the class was easy (but might have been hard), or because we got enough sleep the night before (but might have stayed up). When explanations are hard to come by, conversely, we often look for hidden contingencies so that \"why\" questions lead us to seek out new information. A growing literature, centered around Bayesian, probabilistic accounts of how contingencies compose together, tells us, in turn, how to select between different models on the basis of both their raw likelihood, and value judgements such as simplicity, unification, or consilience (Wojtowicz & DeDeo, 2020).\nNot all explanations, however, refer to things that could have gone otherwise. In many cases, the challenge is not in pointing to a particular contingency, but in showing how (as a deductive matter, given the models to hand) the different contingencies interact with each other to produce the thing to be explained. There is a computational challenge implicit in explanatory work, in addition to the challenge of gathering information; as noted by Roth (1996), Bayesian reasoning involves counting up the different ways contingencies interact, a problem whose general solution is in the computational complexity class #P (\"sharp P\", even more resource-demanding than NP) and involves, potentially, mentally simulating an exponentially-branching number of possibilities.\nIt is natural, in other words, to think that explanations refer not only to the contingencies at the base of different models, but also to the logical form of the models themselves, where the challenge, for computationally-limited beings, is not in the search for new information, but in the task of thinking things through. A good explanation can, for example, reveal a hidden property in a model, that the recipient already pos-sesses, which makes its consequences easier to see. While it may be easy to see how studying leads to good exam performance, it might be much harder to see how different theories about a patient's medical conditions make different diseases more or less likely.\nThe role of explanations as computational aids is most ob-vious in the case of purely deductive reasoning, such as oc-curs in mathematical proofs. If we accept that it makes sense to ask \"why\" a theorem is true, we are faced with the prob-lem that there are no contingencies at all: once we believe the axioms (Maddy, 1988), there is nothing, on pain of logical inconsistency, that could have gone the other way.\nIn simple cases, simply repeating the axioms may be enough. Mathematical deductions, however, may begin with a small number of axioms, but quickly ramify: as shown by Viteri and DeDeo (2022), a formal statement of a mathematical theorem involves tens of thousands of logical steps, and even human-readable proofs stretch to hundreds of pages-while mathematicians talk freely about the reasons why a theorem or conjecture is true (Gowers, 2023) that go well beyond a simple reassertion of the necessary definitions. Necessary truths emerge, just as much, in explaining the output of complex computations that occur in neural networks\u2014as does the need to provide explanations for them, an active area for current research in computer science (Barcel\u00f3, Kozachinskiy, Orth, Subercaseaux, & Verschae, 2025).\nTo study the explanation problem, we draw on recent work that connects basic questions in cognitive science to compu-tational complexity theory (CCT). CCT is already the basis of recent exciting empirical work in error-making and difficulty (Franco, Yadav, Bossaerts, & Murawski, 2021; Franco& Murawski, 2023; Hong & Stauffer, 2023; Bossaerts, Yadav, & Murawski, 2019; Heming Str\u00f8mholt Bremnes & Baggio, 2023), in theory (van Rooij, 2008; van Rooij, Blokpoel, Kwisthout, & Wareham, 2019) (the tractable cognition the-sis), and for questions in AI (van Rooij et al., 2023), economics (Camara, 2022), and evolution (Rich, Blokpoel, de Haan, & van Rooij, 2020; Rich et al., 2021)."}, {"title": "Kinds of Reasons Why", "content": "We focus in particular on explanations for what are called SAT or \"satisfiabilityproblems\". In a SAT problem, the subject is presented with a logical formula, and asked to find an assignment to the variables that makes the formula true; for example, if the two variables are x1 and x2, then the following formula,\n$\\left(x_{1}\\right) \\wedge\\left(x_{2} \\vee \\neg x_{1}\\right)$,\n(1)\nor, in words, \"x\u2081 is true, and either x2 is true, or x\u2081 is false\", has only one solution: x1 and x2 both set to true. Eq. 1 takes the \"canonical\" form for a SAT problem, known as conjunctive normal form (CNF), where a set of clauses containing only \"or\"s is composed together with \u201cand\u201ds; CNF is a particularly useful form for these problems because by varying simple properties (e.g., the length of each clause, or the number of times the same variable appears in different clauses), problems can be made increasingly hard. When formulas contain clauses with three or more variables (known as k-SAT, with 3-SAT being a special case), the difficulty of finding solutions can vary dramatically based on the structure of the formula. Formulas where variables appear repeatedly across different clauses in certain patterns can be particularly challenging in practice (Achlioptas & Moore, 2006). For these instances, even our most advanced algorithms typically require exponential time to determine an assignment of true/false values that satisfies all clauses.\nAlthough CCT results often talk about how hardness emerges in the infinite limit (Moore & Mertens, 2011), the cognitive challenges emerge early. Eq. 1 is trivial (it is, in fact, a rephrasing of x1 and x1 \u2192 x2), but a few more clauses suffice to make things far more opaque; consider, for example,\n$\\left(\\neg x_{1} \\vee \\neg x_{2} \\vee \\neg x_{3} \\vee \\neg x_{4}\\right) \\wedge \\left(\\neg x_{1} \\vee \\neg x_{2} \\vee x_{4}\\right) \\wedge\\left(x_{1} \\vee \\neg x_{3}\\right) \\wedge\\left(x_{2} \\vee \\neg x_{3} \\vee \\neg x_{4}\\right) \\wedge \\left(\\neg x_{3} \\vee x_{4}\\right) \\wedge\\left(x_{3} \\vee x_{4}\\right)$ (2)\nEq. 2 has, like Eq. 1, only one solution (x\u2081 and x3 true; x2 and x4 false-TFTF); like Eq. 1, each clause is necessary for that solution to be the only one; but, unlike Eq. 1, it is not immediately clear how the solution emerges from the constraints.\nSAT instances like Eq. 2 form the test-bed of this paper's study of how explanation works in the absence of contingency; given the axioms of logic, all six clauses, and all four variables, contribute equally. And yet, at the same time, we can see how particular reasons why might emerge: looking at the final pair of clauses in Eq. 2, for example, we can see that x3 has to be true\u2014which (clause 3) implies that x1 has to be true, which simplifies clauses one and two into (\u00acx1V\u00acx2Vx3VX4)\u2227(\u00acx1 V\u00acx2 Vx4), which implies, by a similar pattern, that x2 has to be false, and finally (clause 4) that x4 must be false.\nOur deduction, in other words, proceeded x3 \u2192 X1 \u2192 \u00abX2 \u2192 \u00abx4; informally, a cognitively-limited agent would prefer to say that the truth of x3, rather than the falsehood of x4, is a better reason why the full solution is TFTF-even though a deductive proof could proceed with any variable at all; beginning (for example) with the falsehood of x4 to deduce the truth of x3, and so forth. In this case, the initial simplification that enabled us to derive x3 is known as a \u201cresolution\"; another obvious heuristic is to work with clauses that contain only one variable (so-called \"unit clauses\")\u2014these serve as simple axioms. Beginning with \u00acx4 in Eq. 2 would appear to require a lucky, or perhaps \u201cintuitive\", guess.\nWhile heuristics like resolution can simplify some prob-lems, they can not, in general, solve all of them. In computer science, the earliest SAT solvers such as the Davis-Putnam procedure work step by step, choosing a variable at random, assigning an arbitrary truth value and propagating forward the consequences until either a solution, or a contradiction, is reached; in the latter case, the procedure \"backtracks\" to a previous step and tries the opposite assignment, repeatedly going forward and backward until a solution is found. Later variants uses heuristics to improve efficiency, focusing on (for example) variables that appear most frequently in the formula, as these often have the largest impact on satisfiability (Marques-Silva, 1999); this captures a second style of reason why, that focuses on the influence a variable has on differ-ent parts of the problem: a variable that appears more often provides more reasons for the others to take the values they do.\nModulo the possession of a special faculty of insight un-available to machines (Parsons, 1995; Penrose, 2016) the same limitations of these industrial SAT solvers ought to apply to humans, and SAT problems should be full of \"unrea-sonable\" truths (DeDeo, 2024b), where efficient simplifica-tions are hard, or impossible, to find. In these cases, depth-first search with backtracking, in the Davis-Putnam style, with or without heuristics, is a natural tactic.\nIn the absence of simplifications, backtracking can provide a novel source of reasons why distinct from simplification: rather than explaining an answer by reference to a simplifying structural feature of the problem, one can do so by reference to a mistake that one ought not to make. If you begin trying to solve Eq. 2 by picking x4 and seeing what happens if it is set to \"true\", you will eventually reach a contradiction that will require you to return to x4 and set it to \"false\" instead. In this case, one can explain the final answer in terms of the better choice-a form of fictitious cause, grounded not in logic, but the contingent sequence of choices made by the solver."}, {"title": "Computational Experiments", "content": "The SAT instances that probe the kind of explanation-making we care about are challenging and potentially wearying to solve, and there is a lot we do not understand about how to best construct a psychological experiment to test these predictions in actual human subjects. Large-language models such as GPT-40 show a great deal of promise as a tool to approximate human behavior at a level sufficient to serve as proof of concept for more expensive, smaller-scale studies of human behavior. In general, LLMs appear to replicate the kinds of reasons ordinary subject might give; the reasons given by LLMs are also of intrinsic interest, of course, whether or not they deviate from those that might be provided by more or less distracted humans. We emphasise that these experiments do not count as evidence in favor of actual human performance, but rather as proof of concept that our frameworks above are, indeed, both testable and falsifiable.\nOur prompt for our computational experiments, fed via OpenAI's API to GPT-40, is\nHere's a SAT formula.\n[formula]\nTalk through the finding a solution for this SAT formula.\nOnce you think you have a solution, double check it to make sure that it's correct. If not, keep reasoning to get the answer, and if you get a new one, double check it as well, and keep double-checking carefully until you think you have the answer. Keep track of any assumptions that you make that later turn out to be false.\nThen, at the end of your talking, tell me the main reason why this is the solution, focusing on a single variable. Do not use any python code or outside tools.\nReturn, at the end of your response, a JSON object, with four fields. The first field, SOLUTION, should be a string with only T and F providing the satisfying assignment in order. The second field, REASON, should be an integer from 1 to 4, giving the name of the variable that is the main reason why this is the solution. The third field, EXPLANATION, should be a string that contains your explanation why this is a solution. If you made an assumption that later turned out to be false, the fourth field, ERROR, should contain the integer name of the variable you made the incorrect assumption for, and -1 otherwise.\nAs test cases, we consider SAT problems with four variables and between four and six clauses. We require that each problem have a single unique solution, and that every clause and every variable matter (i.e., if any clause is deleted, the number of solutions increases). To study the impact of simplifying heuristics on given reasons, we consider a range of problem complexity, ranging from problems containing a single unit clause (i.e., a clause that fixes one variable as an axiom) to problems containing a simple resolution pair and, finally, to problems with neither unit clauses nor simple resolution pairs. We consider 400 distinct problems of each type, and provide the system with 20 version of each problem with the order of clauses and variables shuffled. The total cost for these 32,000 runs was approximately 1000 USD."}, {"title": "Results: Identifying Reasons", "content": "On the basis of the discussion above, we propose, and test, three main hypotheses for how simulated reasoners identify the reasons behind a logical truth. We track how often a reason is identified when it is possible to do so (i.e., when the simplification exists, does the subject identify the hinge variable of that simplification as the reason why, or when backtracking has been done, does the subject identify the backtracked assumption variable as the reason why). We also track the role that \u201cinfluence\"-the repeated appearance of a variable across multiple clauses-plays in shifting the location of the identified reason; while influence, alone, does not provide a direct mechanism for either simplification or error-correction, a focus on these high-degree variables can provide cognitively-efficient ways to understand how multiple constraints interact. A simple multiple-variable logistic regression helps us determine what might affect the deployment of a reason depending on the presence of competing reasons, including these high-influence variables that connect multiple parts of the problem space. We have three main hypotheses.\nH1. In the presence of a clear simplification, the implicated variable is more likely to be cited as a reason why. We find this to be the case for both the unit clause case (the \"axiomatic\" variable is cited as a reason 52% of the time, and 68% of the time when no competing reason exists; $p \\ll 10^{-3}$), and the resolution case (when a necessary truth exists in a resolution pair, the relevant variable is identified as a reason 34% of the time, and 50% of the time when no competing reason exists; $p < 10^{-3}$). Indeed, the two reasons interfere; the presence of a unit clause reduces the likelihood of a resolution explanation, and vice versa ($p < 10^{-3}$). \nH2. Backtracking, when it exists, can serve as a reason for deductive truth through the fictitious contingency effect. We find this to be the case; when the solution was found in a process that included backtracking, the backtracked variable was cited as the reason 36% of the time, and 38% when only backtracking was present; $p < 10^{-3}$. Backtracking also directs attention away from simplifications: as shown in Table 1, when a deduction leads one down a blind alley, the \"blame\" for the bad branch competes with the simplification as a reason why.\nH3. Influence reasons lend support to both simplifying and fictitious causes; when a simplifying or a backtracked variable also happens to have maximum degree, it is more likely to be identified as a cause. Again, as shown in Table 1, this influence effect is a strong driver of the identification."}, {"title": "Results: Describing Reasons", "content": "The previous section considered the ways in which a particular variable was identified as a reason why, and how this was predicted by the roles it can play in the underlying search and reasoning process. In addition to asking simulated subjects for raw identification, we also had them provide natural-language accounts of why the variable they identified served as a reason why, and we then analyzed that language with simple word count heuristics."}, {"title": "Discussion", "content": "Our main goal in this paper is to draw attention to a puzzling phenomenon: even in cases where all the facts are networked together in logical (rather than causal) patterns, we still seek, and talk about, reasons why these facts hold. This happens even in cases where, as a matter of logic, the rules of the system mean that any truth can be considered primary: if one is given A and A V B, and discovers (somehow) that this requires that both A and B are true, one can just as easily say that A is true because B is true (modus tollens) as that B is true because A is true (modus ponens).\nWe have presented, in turn, an account of necessary explanation that hinges on two key cognitive phenomena. First, the drive for sense-making and simplification (Chater & Loewenstein, 2016), well-known to act in causal-contingent explanatory practices (Lombrozo, 2007), ought to lead subjects to consider moves that simplify the search process-unit clauses corresponding to axioms, or resolution-style reasons that hinge on A being a consequence of both B and \u00acB\u2014as reasons for the final truth of the matter.\nSecond, we show how reasons for why can emerge from the contingent failures that arise when solving unfamiliar problems with limited cognitive resources. When agents fail to find powerful simplifying reasons\u2014an inevitable occurrence given that logical satisfiability is NP-complete-they must backtrack, revising their initial assumptions to avoid logical contradictions. This backtracking process leads to what we call \"fictitious contingency\" explanations: variables whose values needed adjustment during search are more likely to be cited as reasons for the final solution. These backtrack-induced reasons take a contrapositive form: \u201cX must be true because setting it false leads to a chain of deductions that end in contradiction.\" The prominence of X in the explanation stems not from any inherent logical priority, but merely from the path our search process happened to take. As we have seen (Table 1) these fictitious contingencies can emerge even when better reasons are available.\nWhile it is well known that simplicity exercises a fascination for the human mind, the role of error-making in explanation is less familiar. In contrast to the errors we make in predicting contingent facts, the errors we make in logic ought to take us nowhere-by the principle of explosion, at the very least, if we assume a contradiction, we can derive everything and nothing (Munroe, 2010), and the emergence of these trivialities itself can serve as a heuristic detector of error (DeDeo, 2024a; Kosheleva & Kreinovich, 2024).\nAccounts by practicing mathematicians, however, argue otherwise. The mathematician Alexander Grothendieck, for example, refers to the \"repeated knocking and probing\" that mathematicians undertake (Grothendieck, 2023), that leads to \"the detection of a false idea from the first \"break-offs\" observed between the image obtained and certain obvious facts\" and which makes the discovery of error \"one of the crucial moments, a creative moment among all, in all works of discovery\". If errors are creative processes that both create and refine the reasoner's cognitive model, our framework provides an account of how such errors enable a novel form of reasoning based on the contingencies of exploration.\nOur results also point to further sources of logical explanation and new places of contact between the cognitive and computer sciences. Modern SAT solvers, for example, employ conflict-driven clause learning (CDCL; Marques-Silva, Lynce, and Malik (2009)), which analyzes failed assignment attempts to derive new clauses that prevent similar failures --a potential analogy for the kind of error-based reasoning described by Grothendieck. However, even these sophisticated approaches can struggle with the hardest instances, which typically lack unit clauses and simple resolutions, and exhibit, instead, the kind of high uniformity-where variables appear together with similar frequencies across clauses, with no single variable having outsized impact that frustrates influence-based reasons as well.\nCDCL algorithms struggle with problems that possess high amounts of symmetry where permuting variables preserves the formula's overall structure. Symmetry makes problems particularly challenging because different variable assignments can be structurally equivalent and lead to redundantly unproductive, but hard to spot, search paths where the solver cannot leverage learned information from one branch to prune others. This balanced structure leaves solvers without clear signals about which assignments to try first. We expect similar effects to obtain for human reasoners, with knock-on effects for both the reasons they give to explain the solution, and the extent to which they consider the facts, lost as they are in a sea of uniformity, explainable at all.\nThis challenge of navigating symmetries and contradictions opens up broader questions about how discoveries in logio-deductive systems, such as the discovery of proofs, emerge: encountering a contradiction after several deductive steps-the essence of our proposed backtracking-reason mechanism-can often reveal a critical branch point in the structure, which is pointed to by a prior immediately prior decision point. These bifurcation points likely play different roles in expert versus novice proof construction and problem-solving, suggesting a key dimension along which mathematical maturity develops: more sophisticated reasoners may be able to both project forward further, identifying branch points more efficiently, and to recall the prior decision-history more easily, making backtracking more efficient.\nOur work with SAT shows how these contradictions serve as reasons-why that can complement those found in simplification discoveries, but the relationship between expertise and contradiction-handling remains unexplored. Future studies could investigate whether experts show systematic differences in how they leverage contradictions, moving beyond local navigation via backtracking to extract broader structural insights about the proof space.\nEmpirically validating the reasoning patterns and explanatory strategies discussed in this paper is challenging, because they are expected to emerge only when subjects engage, over an extended period, with a difficult problem, and master the relevant notation, heuristics, and (potentially) cover stories. To help refine our ideas, we conducted experiments with simulated subjects; these experiments lend credence to the idea that these systematic explanation biases should be detectable in real-world experiments. These pilot results not only give us insight into the reasoning methods of automated systems but also now enable us to plan a series of experiments at our home institutions involving undergraduates in mathematics, computer science, and logic.\nFor many researchers, a better understanding of the purely deductive explanation is valuable in its own right: sophisticated mathematical-deductive cognition may be evolutionarily novel, but it is culturally ancient, appearing at approximately the same time as philosophy, tragedy and the theological innovations of the Axial Age (Bellah, 2011).\nThe question of deductive explanation, however, reaches well beyond the provinces of mathematical proof. As noted in the introduction to this piece, even when we reason about contingencies, we are usually faced with models of sufficient complexity as to pose calculational challenges equal to, if not exceeding, those of information-gathering. Using Bayes' rule requires us to assess prior probabilities, and to estimate the conditional probabilities of observations given a theory to hand-but it also can require us, in computing the denominator, to solve challenging problems that are no less deductive than SAT problem like those of Eq. 2. Explaining why we believe this or that theory is the better one to believe may often, in turn, focus less on the evidence in the world, and much more on the kinds of simplicity and backtracking-induced reasons that a SAT-style investigation lays bare."}]}