[{"title": "IMPACT GNN: IMPOSING INVARIANCE WITH MESSAGE PASSING IN CHRONOLOGICAL SPLIT TEMPORAL GRAPHS", "authors": ["Sejun Park", "Hyunwoo Park", "Joo Young Park"], "abstract": "This paper addresses domain adaptation challenges in graph data resulting from chronological splits. In a transductive graph learning setting, where each node is associated with a timestamp, we focus on the task of Semi-Supervised Node Classification (SSNC), aiming to classify recent nodes using labels of past nodes. Temporal dependencies in node connections create domain shifts, causing significant performance degradation when applying models trained on historical data into recent data. Given the practical relevance of this scenario, addressing domain adaptation in chronological split data is crucial, yet underexplored. We propose Imposing invariance with Message Passing in Chronological split Temporal Graphs (IMPACT), a method that imposes invariant properties based on realistic assumptions derived from temporal graph structures. Unlike traditional domain adaptation approaches which rely on unverifiable assumptions, IMPACT explicitly accounts for the characteristics of chronological splits. The IMPACT is further supported by rigorous mathematical analysis, including a derivation of an upper bound of the generalization error. Experimentally, IMPACT achieves a 3.8% performance improvement over current SOTA method on the ogbn-mag graph dataset. Additionally, we introduce the Temporal Stochastic Block Model (TSBM), which replicates temporal graphs under varying conditions, demonstrating the applicability of our methods to general spatial GNNs.", "sections": [{"title": "1 INTRODUCTION", "content": "The task of Semi-supervised Node Classification (SSNC) on graph often involves nodes with temporal information. For instance, in academic citation network, each paper node may contain information regarding the year of its publication. The focus of this study lies within such graph data, particularly on datasets where the train and test splits are arranged in chronological order. In other words, the separation between nodes available for training and those targeted for inference occurs temporally, requiring the classification of the labels of nodes with the most recent timestamp based on the labels of nodes with historical timestamp. While leveraging GNNs trained on historical data to classify newly added nodes is a common scenario in industrial and research settings (Liu et al., 2016; Bai et al., 2020; Pareja et al., 2020), systematic research on effectively utilizing temporal information within chronological split graphs remains scarce.\nFailure to appropriately utilize temporal information can lead to significant performance degradation when the model attempts to classify labels of recent data. We conducted a toy experiment on the ogbn-mag dataset, an academic graph dataset having features with chronological split, to confirm"}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 GRAPH NEURAL NETWORKS", "content": "Graph Neural Networks (GNNs) have gained significant attention across various domains, including recommender systems (Ying et al., 2018; Gao et al., 2022), biology (Barabasi & Oltvai, 2004), and chemistry (Wu et al., 2018). Spatial GNNs, such as GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), GAT (Velickovic et al., 2017) and HGNN (Feng et al., 2019), derive topological information by aggregating information from neighboring nodes through message passing.\n$M_{v}^{(k+1)} \\leftarrow A G G\\left(\\left\\{X_{u}^{(k)}, \\forall u \\in \\mathcal{N}_{v}\\right\\}\\right)$\n$X_{v}^{(k+1)} \\leftarrow \\operatorname{COMBINE}\\left(\\left\\{X_{v}^{(k)}, M_{v}^{(k+1)}\\right\\}\\right), \\forall v \\in \\mathcal{V}, k<K$\nHere, $K$ is the number of GNN layers, $X_{v}^{(0)}=X_{v}$ is initial feature vector of each node, and the final representation $X_{v}^{(K)}=Z_{v}$ serves as the input to the node-wise classifier. The AGG function performs topological aggregation by collecting information from neighboring nodes, while the COMBINE function performs semantic aggregation through processing the collected message for each node. Scalability is a crucial issue when applying GNNs to massive graphs. Ego graph, which defines the scope of information influencing the classification of a single node, exponentially increases with the number of GNN layers. Therefore, to ensure scalability, algorithms must be meticulously designed to efficiently utilize computation and memory resources (Hamilton et al., 2017; Shi et al., 2022; Zeng et al., 2019). Decoupled GNNs, whose process of collecting topological information occurs solely during preprocessing and is parameter-free, such as SGC (Wu et al., 2019), SIGN (Rossi et al., 2020), and GAMLP (Zhang et al., 2022), have demonstrated outstanding performance and scalability on many real-world datasets. Furthermore, SeHGNN (Yang et al., 2023) and RpHGNN (Hu et al., 2023) propose decoupled GNNs that efficiently apply to heterogeneous graphs by constructing separate embedding spaces for each metapath based on HGNN (Feng et al., 2019)."}, {"title": "2.2 DOMAIN ADAPTATION", "content": "A machine will learn from a train domain in order to perform on a test domain. Domain adaptation is needed due to the discrepancy between train and test domains. That is, we can not guarantee that a model which performed well on the train domain, will perform well on the test domain. The performance on the test domain is known to depend on the performance of the train domain and the similarity between two domains (Ben-David et al., 2006; 2010; Germain et al., 2013; 2016).\nFor feature space $\\mathcal{X}$ and label space $\\mathcal{Y}$, the goal is to train a predictor function $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ to minimize the risk $R_{t r}(f)=\\mathbb{E}_{(X, Y) \\sim P_{t r}}[\\mathcal{L}(f(X), Y)]$ where $P_{t r}$ is the distribution of the train feature-label pairs, and $\\mathcal{L}$ is a loss function $\\mathcal{L}: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$. We are interested in minimizing $R_{t e}(f)=\\mathbb{E}_{(X, Y) \\sim P_{t e}}[\\mathcal{L}(f(X), Y)]$ where $P_{t e}$ is the distribution of the test feature-label pairs.\nThe domain adaptation bound, or upper bound of generalization error was firstly proposed for a binary classification task by defining the set of all trainable functions $\\mathcal{F}$, symmetric hypothesis class $\\mathcal{F}_{A F}$ (Ben-David et al., 2006), and a metric for distributions, namely $d_{\\mathcal{F} \\triangle \\mathcal{F}}$. $d_{\\mathcal{F} \\triangle \\mathcal{F}}$ is the factor which represents the similarity between two distributions. Recently, theories and applications as setting up the metric between two distributions as the Wasserstein-1 distance, $W_{1}$ instead of $d_{\\mathcal{F} \\triangle \\mathcal{F}}$ have been developed (Lee et al., 2019; Shen et al., 2018; Arjovsky et al., 2017; Redko et al., 2017). For brevity, we omit the assumptions introduced in (Redko et al., 2017) and simply state the theoretical domain adaptation bound below.\n$R_{t e}(f) \\leq R_{t r}(f)+W_{1}\\left(D_{t r}, D_{t e}\\right)$\nwhere $D_{t r}$ and $D_{t e}$ are marginal distributions on $\\mathcal{X}$ of $P_{t r}$ and $P_{t e}$, respectively."}, {"title": "2.3 PRIOR STUDIES", "content": "Despite its significance, studies on domain adaptation in GNNs are relatively scarce. Notably, to our knowledge, no studies propose invariant learning applicable to large graphs. For example, EERM (Wu et al., 2022) defines a graph editor that modifies the graph to obtain invariant features through reinforcement learning, which cannot be applied to decoupled GNNs. SR-GNN (Zhu et al., 2021) adjusts the distribution distance of representations using a regularizer, with computational complexity proportional to the square of the number of nodes, making it challenging to apply to large graphs. This scarcity is attributed by several factors: data from different environments may have interdependencies, and the extrapolating nature of environments complicates the problem."}, {"title": "3 METHOD EXPLANATION", "content": null}, {"title": "3.1 MOTIVATION OF OUR METHOD: IMPOSING INVARIANCE WITH MESSAGE PASSING", "content": "The distribution of node connections depends on both timestamps and labels. As a result, even if features from previous layers are invariant, features after the message passing layers belong to different distributions: $D_{t r}$ for training and $D_{t e}$ for testing. Imposing invariance here means aligning the mean and variance of $D_{t r}$ and $D_{t e}$. While it may seem straightforward to compute and align the mean and variance for each label, this is impractical in real settings since test labels are unknown during prediction. To overcome this, we analyzed real-world temporal graphs and identified practical assumptions about connection distributions. Based on this, we propose a message passing method that corrects the discrepancy between $D_{t r}$ and $D_{t e}$, ensuring feature invariance at each layer."}, {"title": "3.2 PROBLEM SETTING", "content": "Denote the possible temporal information as $\\mathcal{T}=\\{..., t_{\\max }-1, t_{\\max }\\}$, $\\mathcal{Y}$ as the set of labels, and $P_{t r}$ and $P_{t e}$ as the joint probability distribution of feature-label pairs in train data and test data. The training data will be historical labels, that is, nodes with timestamp smaller than $t_{\\max }$. The test data will be recent labels, that is, nodes with timestamp $t_{\\max }$. Therefore, labels of nodes with time $t_{\\max }$ are unknown. We say that a variable is invariant if and only if it does not depend on $t$.\nHere are the 3 assumptions introduced in this study.\n$P_{t e}(Y)=P_{t r}(Y)$\n$P_{t e}(X \\mid Y)=P_{t r}(X \\mid Y)$\n$\\operatorname{Assumption~3}: P_{y t}(\\tilde{y}, \\tilde{t})=f(y, t) g(y, \\tilde{y},\\left|\\tilde{t}-t\\right|), \\forall y, \\tilde{y} \\in \\mathcal{Y}, \\forall t, \\tilde{t} \\in \\mathcal{T}$"}, {"title": "3.3 OUTLINE OF IMPACT METHODS", "content": "In the analysis of IMPaCT methods, we will later define and use the first and second moment of distributions, which are simply the approximations of mean and variance. Occasionally, first moment and second moment are written as approximate expectation and approximate variance, respectively.\nIn Section 4, we will introduce the 1st moment alignment methods, MMP and PMP. These methods impose the invariance of the 1st moment in the initial layer by modifying the original graph data, and pre-serves the invariance among layers during the message passing process. Formally, MMP and PMP ensures the aggregated message $M_{v}^{(k+1)}$ to approximately satisfy $P_{t r}\\left(M_{v}^{(k+1)} \\mid Y\\right)=P_{t e}\\left(M_{v}^{(k+1)} \\mid Y\\right)$ when the represen-tations $X_{v}^{(k)}$ at the k-th layer satisfies $P_{t r}\\left(X_{v}^{(k)} \\mid Y\\right)=P_{t e}\\left(X_{v}^{(k)} \\mid Y\\right), v \\in \\mathcal{V}$.\nFurthermore, in Section 5, we will introduce the 2nd moment alignment methods, PNY and JJNORM, which imposes the invariance of the 2nd moment. These methods are not graph modi-fying methods, and should be applied over 1st moment alignment methods. Specifically, JJNORM algebraically alters the distribution of the final layer in order to impose 2nd moment invariance, without changing the 1st moment invariance property."}, {"title": "4 FIRST MOMENT ALIGNMENT METHODS", "content": "Message passing refers to the process of aggregating representations from neighboring nodes in the previous layer. Here, we assume the commonly used averaging message passing procedure. For any arbitrary target node $v \\in \\mathcal{V}$ with label $y$ and time $t$,\n$M_{v}^{(k+1)}=\\frac{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tilde{t} \\in \\mathcal{T}} \\sum_{w \\in \\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})} X_{w}}{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tilde{t} \\in \\mathcal{T}}\\left|\\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})\\right|}, \\quad X_{w} \\stackrel{I I D}{\\sim} X_{\\tilde{y} \\tilde{t}}^{(k)} \\text { for } w \\in \\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})$\nwhere $\\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})=\\{w \\in \\mathcal{V} \\mid w$ is a neighbor of $v$ with $\\tilde{y}$ and time $\\tilde{t}\\}$. $M_{v}^{(k+1)}$ is the aggregated message at node $v$ in the $k+1$-th layer, and $X_{\\tilde{y} \\tilde{t}}^{(k)}$ is the distribution of representations from the previous layer. For simplification, the term \"$X_{w} \\stackrel{I I D}{\\sim} X_{\\tilde{y} \\tilde{t}}^{(k)}$ for $\\forall w \\in \\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})$\" will be omitted in definitions in the subsequent discussions."}, {"title": "4.1 PERSISTENT MESSAGE PASSING: PMP", "content": "We propose Persistent Message Passing (PMP) as one approach to achieve 1st moment invariance. For the target node $v$ with time $t$, consider the time $\\tilde{t}$ of some neighboring node. For $\\Delta=\\left|\\tilde{t}-t\\right|$ where $0<\\Delta<t_{\\max }-t$, both $t+\\Delta$ and $t-\\Delta$ neighbor nodes can exist. However, nodes with $\\Delta>\\left|t_{\\max }-t\\right|$ or $\\Delta=0$ are only possible when $\\tilde{t}=t-\\Delta$. Let $\\mathcal{T}^{\\text {double }}=\\{\\tilde{t} \\in \\mathcal{T} \\mid 0<\\left|\\tilde{t}-t\\right| \\leq\\left|t_{\\max }-t\\right|\\}$ and $\\mathcal{T}^{\\text {single }}=\\{\\tilde{t} \\in \\mathcal{T} \\mid\\left|\\tilde{t}-t\\right|>t_{\\max }-t$ or $\\tilde{t}=t\\}$. As discussed, the target node receives twice the weight from $\\tilde{t} \\in \\mathcal{T}^{\\text {double }}$ against $\\tilde{t} \\in \\mathcal{T}^{\\text {single }}$. Motivation behind PMP is to correct this by double weighting the neighbor nodes with time in $\\mathcal{T}^{\\text {single }}$, as depicted in Figure 3.\nDefinition 4.1. The PMP from the $k$-th layer to the $k+1$-th layer of target node $v$ is defined as:\n$M_{v}^{\\operatorname{pmp}(k+1)}=\\frac{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tilde{t} \\in \\mathcal{T}^{\\text {single }}} \\sum_{w \\in \\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})} 2 X_{w}+\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tilde{t} \\in \\mathcal{T}^{\\text {double }}} \\sum_{w \\in \\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})} X_{w}}{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tilde{t} \\in \\mathcal{T}^{\\text {single }}} 2\\left|\\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})\\right|+\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tilde{t} \\in \\mathcal{T}^{\\text {double }}}\\left|\\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})\\right|}$\nAs noted, PMP is a graph modifying method. Neighbor nodes in $\\mathcal{T}^{\\text {single }}$ are duplicated in order to contribute equally with nodes in $\\mathcal{T}^{\\text {double }}$. Then, the definition above can be derived.\nTheorem 4.1. The 1st moment of aggregated message obtained by PMP layer is invariant, if the 1st moment of previous representation is invariant.\nSketch of proof Let $\\mathbb{E}\\left[X_{\\tilde{y} \\tilde{t}}^{(k)}\\right]=\\mu_{X}^{(k)}(\\tilde{y})$ as a function invariant with $t$. Then we can get\n$\\mathbb{E}\\left[M_{v}^{\\operatorname{pmp}(k+1)}\\right]=\\frac{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tau \\geq 0} g(y, \\tilde{y}, \\tau) \\mu_{X}^{(k)}(\\tilde{y})}{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tau>0} g(y, \\tilde{y}, \\tau)}$\nwhich is invariant with $t$. See Appendix A.5 for details and implementation."}, {"title": "4.2 MONO-DIRECTIONAL MESSAGE PASSING: MMP", "content": "Besides PMP, there are numerous ways to adjust the 1st moment of train and test distributions to be invariant. We introduce Mono-directional Message Passing (MMP) as one such approach. MMP aggregates information only from neighboring nodes with time less or equal than the target node.\nDefinition 4.2. The MMP from the $k$-th layer to the $k+1$-th layer of target node $v$ is defined as:\n$M_{v}^{\\operatorname{mmp}(k+1)}=\\frac{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tilde{t}<t} \\sum_{w \\in \\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})} X_{w}}{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tilde{t}<t}\\left|\\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})\\right|}$\nTheorem 4.2. The 1st moment of aggregated message obtained by MMP layer is invariant, if the 1st moment of previous representation is invariant.\nSketch of proof Let $\\mathbb{E}\\left[X_{\\tilde{y} \\tilde{t}}^{(k)}\\right]=\\mu_{X}^{(k)}(\\tilde{y})$ as a function invariant with $t$. Then we can calculate\n$\\mathbb{E}\\left[M_{v}^{\\operatorname{mmp}(k+1)}\\right]=\\frac{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tau \\geq 0} g(y, \\tilde{y}, \\tau) \\mu_{X}^{(k)}(\\tilde{y})}{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\tau \\geq 0} g(y, \\tilde{y}, \\tau)}$\nwhich is also invariant with $t$. See Appendix A.6 for details and implementation.\nComparison between PMP and MMP. Both PMP and MMP adjust the weights of messages collected from neighboring nodes that meet certain conditions, either doubling or ignoring their impact. They can be implemented easily by reconstructing the graph according to the conditions without altering the existing code. However, MMP collects less information since it only gathers information only from the past, resulting a smaller ego-graph. Therefore, PMP will be used as the 1st moment alignment method in the subsequent discussions. Furthermore, from Theorem 4.1, we will denote $\\mathbb{E}\\left[M_{v}^{\\operatorname{pmp}(k+1)}\\right]$ as $\\mu_{\\operatorname{pmp}}^{(k+1)}(y)$ for target node $v$ with label $y$ in the following discussions."}, {"title": "4.3 THEORETICAL ANALYSIS OF PMP WHEN APPLIED IN MULTI-LAYER GNNS.", "content": "We will assume the messages and representations to be scalar in this discussion. Now suppose that (i) $\\left|M_{v}^{(k)}\\right| \\leq C$ almost surely for $\\forall v \\in \\mathcal{V}, M_{v}^{(k)} \\sim \\mu_{M v t}^{(k)}$, and (ii) $\\operatorname{var}\\left(M_{v}^{(k)}\\right)<\\mathcal{V}$ for $\\forall v \\in \\mathcal{V}$. Since we are considering 1st moment alignment method PMP, we may assume $\\mathbb{E}\\left[M_{v}^{(k)}\\right]=\\hat{\\mathbb{E}}\\left[M_{v}^{(k)}\\right]=\\mu_{M}^{(k)}(y)$ for $M_{v}^{(k)} \\sim \\mu_{M v t}^{(k)}, \\forall y \\in \\mathcal{Y}, \\forall t \\in \\mathcal{T}$. Here, $W_{1}$ is the Wasserstein-1 metric of probability measures. We also assume G-Lipschitz condition for semantic aggregation functions, $f^{(k)}, \\forall k \\in\\{1,2, ..., K\\}$. Detailed modelling of PMP with probability measures are in Appendix A.7, and proofs of the following theorems are in Appendix A.8. For now on, we will omit the details and only state the theorems and provide interpretations of the theoretical results.\n$W_{1}\\left(\\mu_{M v t}^{(k)}, \\mu_{M v t}^{(k)}\\right) \\leq O\\left(C^{1 / 3} \\mathcal{V}^{1 / 3}\\right)$\nTheorem 4.4. If $\\mu_{M v t}^{(k)}$ is sub-Gaussian with constant $\\tau$, then $W_{1}\\left(\\mu_{M v t}^{(k)}, \\mu_{M v t}^{(k)}\\right) \\leq O(\\tau \\sqrt{\\log C})$.\nWithout PMP, we can only guarantee $W_{1}\\left(\\mu_{M v t}^{(k)}, \\mu_{M v t}^{(k)}\\right) \\leq 2 C$, or $O(C)$. However, PMP gives a tighter upper bound $O\\left(C^{1 / 3} \\mathcal{V}^{1 / 3}\\right)$. Furthermore, with additional assumption of sub-Gaussians, PMP gives a more significant upper bound $O(\\tau \\sqrt{\\log C})$.\nTheorem 4.5. If $\\forall y \\in \\mathcal{Y}, \\forall t, t^{\\prime} \\in \\mathcal{T}, W_{1}\\left(\\mu_{M y t}^{(k)}, \\mu_{M y t^{\\prime}}^{(k)}\\right) \\leq W$, then for $\\forall y \\in \\mathcal{Y}, \\forall t, t^{\\prime} \\in \\mathcal{T}, W_{1}\\left(\\mu_{M y t}^{(k+1)}, \\mu_{M y t^{\\prime}}^{(k+1)}\\right) \\leq \\frac{W}{G^{(k)}}$ where $G^{(k)}>1$ is a constant only depending on the layer $k$.\nThis theorem involves two steps. First, $W_{1}\\left(\\mu_{M y t}^{(k)}, \\mu_{M y t}^{(k)}\\right)<W$ gives $W_{1}\\left(\\mu_{X y t}^{(k)}, \\mu_{X y t}^{(k)}\\right) \\leq G W$. Second, $W_{1}\\left(\\mu_{X y t}^{(k)}, \\mu_{X y t}^{(k)}\\right) \\leq G W$ gives $W_{1}\\left(\\mu_{M y t}^{(k+1)}, \\mu_{M y t^{\\prime}}^{(k+1)}\\right) \\leq \\frac{W}{G^{(k)}}$ for $G^{(k)}>1$, a constant only depending on the layer $k$. The strength of this inequality is that the denominator $G^{(k)}$ is larger than 1. For example, if we assume 1-Lipschitz property of aggregation functions, the upper bound of $W_{1}$ distance decreases layer by layer. The following corollary formulates this interpretation.\nCorollary 4.5.1. Under previous assumptions, for $\\forall y \\in \\mathcal{Y}, \\forall t \\in \\mathcal{T}, W_{1}\\left(\\mu_{M y t}^{(K)}, \\mu_{M y t}^{(K)}\\right) \\leq \\frac{G_{K-1} \\cdots G^{2} G^{(1)}}{} O\\left(\\min \\left\\{C^{1 / 3} \\mathcal{V}^{1 / 3}, \\tau \\sqrt{\\log C}\\right\\}\\right)$\nTherefore, we ensured that the $W_{1}$ distance between train and test distributions of final representa-tions are bounded when PMP is applied in multi-layer GNNs. In Section 2.2, we have previously introduced that the generalization error can be upper bounded by the $W_{1}$ distance. Hence, we have established a theoretical upper bound of the generalization error when PMP method is applied."}, {"title": "4.4 GENERALIZED PMP: GENPMP", "content": "Here, we note that the first moment is a good approximation for the mean only when the number of nodes with specific time label are similar to each other. Hence, if the dataset has a substantial difference among the number of nodes with specific time label, duplicating the single nodes as PMP will still adjust the 1st moment, but this will not be a good approximation for the mean. Therefore, we propose the Generalized PMP (GenPMP) for such datasets.\nFor the target node $v$ with time $t$, consider the time $\\tilde{t}$ of some neighboring node. Instead of $\\mathcal{T}^{\\text {double }}$ and $\\mathcal{T}^{\\text {single }}$, we define $\\mathcal{T}_{\\Delta}=\\{\\tilde{t} \\in \\mathcal{T} \\mid\\left|\\tilde{t}-t\\right|=\\Delta\\}$ for $0 \\leq \\Delta \\leq\\left|t_{\\max }-t\\right|$. By collecting the nodes, we can get a discrete probability distribution $P_{s}$, where $P_{s}(\\mathcal{T})$ is attained by adding $|\\mathcal{T}|$ for all nodes with time label $s$, and then normalizing so that $\\sum_{\\mathcal{T} \\geq 0} P_{s}(\\mathcal{T})=1$.\nDefinition 4.3. The generalized probabilistic message passing (GenPMP) from the $k$-th layer to the $(k+1)$-th layer of target node $v$ is defined as:\n$M_{v}^{\\operatorname{genpmp}(k+1)}=\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\sum_{\\Delta \\geq 0} \\sum_{\\tilde{t} \\in \\mathcal{T}_{\\Delta}} \\sum_{w \\in \\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})} \\frac{P_{t_{\\max }}(\\Delta)}{P(\\Delta)} X_{w}$\nHere, we are giving a relative weight to nodes $w$ in $\\mathcal{N}_{v}(\\tilde{y}, \\tilde{t})$ by generating nodes with a ratio of $\\frac{P_{t_{\\max }}(\\Delta)}{P(\\Delta)}$. Unlike PMP which distinguishes neighbor nodes into only two classes, this method explicitly counts the nodes and adjusts the shape of distributions among train and test data. However, GenPMP has reduced adaptability. Unlike PMP, which can be implemented by simply adding or removing edges in the graph, GenPMP requires modifying the model to reflect real-valued edge weights during the message passing process. Moreover, when the number of nodes per timestamp is equal, GenPMP behaves similarly to PMP. Theoretically, if the ratio $\\frac{P_{t_{\\max }}(\\Delta)}{P(\\Delta)}$ is too large for a fixed"}, {"title": "5 SECOND MOMENT ALIGNMENT METHODS", "content": "While 1st order alignment methods like PMP and MMP preserve the invariance of the 1st moment of the aggregated message, they do not guarantee such property for the 2nd moment. Let's suppose that the 1st moment of the previous layer's representation X is invariant with node's time $t$, and 2nd moment of the initial feature is invariant. That is, $\\forall \\tilde{y} \\in \\mathcal{Y}, \\forall \\tilde{t} \\in \\mathcal{T}, \\mathbb{E}\\left[X_{\\tilde{y} \\tilde{t}}\\right]=\\mu_{\\operatorname{pmp}}^{(k)}(\\tilde{y})$ for $X \\sim X_{\\tilde{y} \\tilde{t}}^{\\operatorname{pmp}(k)}$ and $\\Sigma_{\\tilde{y} \\tilde{t}}^{\\operatorname{pmp}(0)}(\\tilde{y}, t)=\\mathbb{E}\\left[\\left(X_{\\tilde{y} \\tilde{t}}-\\mu_{\\operatorname{pmp}}^{(k)}(\\tilde{y}, t)\\right)\\left(X_{\\tilde{y} \\tilde{t}}-\\mu_{\\operatorname{pmp}}^{(k)}(\\tilde{y}, t)\\right)\\right]=\\Sigma_{\\tilde{y} \\tilde{t}_{\\_{\\max }}}^{(0)}$ where $\\Sigma_{\\tilde{y} \\tilde{t}}^{\\operatorname{pmp}(k)}(\\tilde{y}, t)=\\operatorname{var}(X_{\\tilde{y} \\tilde{t}}^{\\operatorname{pmp}(k)})$. Given that the invariance of 1st moment is preserved after message passing by PMP or MMP, one naive idea for aligning the 2nd moment is to calculate the covariance matrix of the aggregated message $M_{v}^{\\operatorname{pmp}(k+1)}$ for each time $t$ of node $v$ and adjust for the differences. However, when $t \\neq t_{\\max }$, we cannot directly estimate $\\operatorname{var}\\left(M_{v}^{\\operatorname{pmp}(k+1)}\\right)$ since the labels are unknown for nodes in the test set. We introduce PNY and JJNORM, the methods for adjusting the aggregated message obtained using the PMP to achieve invariant property even for the 2nd moment, when the invariance for 1st moment is preserved.\nThe second moment of aggregated message. The approximate variance of $M_{v}^{\\operatorname{pmp}(k+1)}$ can also be calculated rigorously by using the definition of approximate variance in Appendix A.5, as:\n$\\operatorname{var}\\left(M_{v}^{\\operatorname{pmp}(k+1)}\\right)=\\frac{\\sum_{\\tilde{y} \\in \\mathcal{Y}}\\left(\\sum_{\\tilde{t} \\in \\mathcal{T}^{\\operatorname{single}}} 4 P_{y \\tilde{t}}(\\tilde{y}, \\tilde{t})+\\sum_{\\tilde{t} \\in \\mathcal{T}^{\\operatorname{double}}} P_{y \\tilde{t}}(\\tilde{y}, \\tilde{t})\\right) \\sum_{\\operatorname{pmp}}^{(k)}(\\tilde{y}, \\tilde{t})}{\\left(\\sum_{\\tilde{y} \\in \\mathcal{Y}}\\left(\\sum_{\\tilde{t} \\in \\mathcal{T}^{\\operatorname{single}}} 2 P_{y \\tilde{t}}(\\tilde{y}, \\tilde{t})+\\sum_{\\tilde{t} \\in \\mathcal{T}^{\\operatorname{double}}} P_{y \\tilde{t}}(\\tilde{y}, \\tilde{t})\\right)\\mathcal{N}_{y \\tilde{t}}\\right)^{2}}$\nHence, we can write $\\operatorname{var}\\left(M_{v}^{\\operatorname{pmp}(k+1)}\\right)=\\Sigma_{y t}^{\\operatorname{pmp}(k+1)}(y, t)$. Since $\\Sigma_{y t}^{\\operatorname{pmp}(k+1)}(y, t)$ is a covariance matrix, it is positive semi-definite, orthogonally diagonalized as $\\Sigma_{y t}^{\\operatorname{pmp}(k+1)}(y, t)=U_{y t} A_{y t} U_{y t}^{\\mathrm{T}}$."}, {"title": "5.1 PERSISTENT NUMERICAL YIELD: PNY", "content": "If we can specify $P_{y \\tilde{t"}]}, {"as": "nFor affine transformation matrix $A_{t}=U_{y t_{\\_{\\max }"}, {}]