{"title": "SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training", "authors": ["Gengwei Zhang", "Liyuan Wang", "Guoliang Kang", "Ling Chen", "Yunchao Wei"], "abstract": "In recent years, continual learning with pre-training (CLPT) has received widespread interest, instead of its traditional focus of training from scratch. The use of strong pre-trained models (PTMs) can greatly facilitate knowledge transfer and alleviate catastrophic forgetting, but also suffers from progressive overfitting of pre-trained knowledge into specific downstream tasks. A majority of current efforts often keep the PTMs frozen and incorporate task-specific prompts to instruct representation learning, coupled with a prompt selection process for inference. However, due to the limited capacity of prompt parameters, this strategy demonstrates only sub-optimal performance in continual learning. In comparison, tuning all parameters of PTMs often provides the greatest potential for representation learning, making sequential fine-tuning (Seq FT) a fundamen-tal baseline that has been overlooked in CLPT. To this end, we present an in-depth analysis of the progressive overfitting problem from the lens of Seq FT. Considering that the overly fast representation learning and the biased classification layer constitute this particular problem, we introduce the advanced Slow Learner with Classifier Alignment (SLCA++) framework to unleash the power of Seq FT, serving as a strong baseline approach for CLPT. Our approach involves a Slow Learner (SL) to selectively reduce the learning rate of backbone parameters, and a Classifier Alignment (CA) to align the disjoint classification layers in a post-hoc fashion. We further enhance the efficacy of SL with a symmetric cross-entropy loss (SCE), as well as employ a parameter-efficient strategy to implement Seq FT with SLCA++. Across a variety of continual learning scenarios, including class-incremental learning on general datasets like CIFAR-100 and ImageNet-R, fine-grained datasets like CUB-200 and Cars-196, and domain-incremental learning on DomainNet, our approach provides substantial improvements and outperforms state-of-the-art methods by a large margin. Our code is available at: https://github.com/GengDavid/SLCA.", "sections": [{"title": "I. INTRODUCTION", "content": "The purpose of continual learning (CL) is to learn contents that appear in sequence as if they were observed simulta-neously. Previous efforts are mainly based on the premise of learning from scratch, attempting to mitigate catastrophic forgetting [36] of previously-learned knowledge when ac-quiring new information. However, the success of large-scale pre-training has revolutionized the learning paradigm of deep neural networks. The use of pre-training brings positive"}, {"title": "II. RELATED WORK", "content": "Continual Learning. Traditional works on continual learn-ing mainly focus on sequential training of deep neural net-work(s) from scratch, ensuring effective learning of new tasks without catastrophic forgetting of old tasks. Representative strategies include regularization-based methods [2], [12], [28], [34], [55], [69], which preserve the old model and selectively stabilize changes in parameters or predictions; replay-based methods [5], [40], [51], [54], [56], [66], which approxi-mate and recover the previously-learned data distributions; architecture-based methods [44], [45], [57], [67], which allo-cate dedicated parameter sub-spaces for each task. In addition, current advances in continual learning concentrate on the field of computer vision and gradually extend to the field of natural language processing [58]. Continual Learning with Pre-training. In recent years, the benefits of pre-training for continual learning have been in-creasingly explored. For example, the representations obtained from supervised pre-training have been shown to facilitate not only knowledge transfer but also robustness to catastrophic for-getting in continual learning of specific downstream tasks [37], [41], [74]. Also, learning a large number of base classes in the initial training phase allows the model to learn new classes with only minor adaptations [65]. Inspired by the techniques of knowledge transfer in natural language processing, L2P [62] employs an additional set of learnable parameters called \"prompts\" that dynamically instruct the representation layer for learning incremental tasks. DualPrompt [61] extends this idea by attaching complementary prompts to the representation layer for learning task-invariant and task-specific instructions. Both L2P and DualPrompt require a prompt selection phase"}, {"title": "III. CONTINUAL LEARNING WITH PRE-TRAINING", "content": "In this section, we introduce the problem formulation of continual learning with pre-training (CLPT) and perform an in-depth analysis of its particular challenge. We then present our approach based on the analysis."}, {"title": "A. Problem Formulation", "content": "CLPT Setup. Let's consider a neural network $\\mathcal{M}(\\cdot) = h_{cls}(f_{rep}(\\cdot))$ with parameters $\\theta = {\\theta_{rep}, \\theta_{cls}}$ for classifi-cation tasks, which often consists of a representation layer $f_{rep}(\\cdot)$ that projects input images to feature representations, and a classification layer $h_{cls}(\\cdot)$ that projects feature rep-resentations to output predictions. $\\Theta_{rep}$ is initialized on a pre-training dataset $\\mathcal{D}_{pt}$ in a supervised or self-supervised manner (class labels are not necessary for the latter). Then, $\\mathcal{M}$ needs to learn a sequence of incremental tasks from their training sets $\\mathcal{D}_t, t = 1,..., T$ and tries to perform well on their test sets. Following previous efforts of CLPT in the field of computer vision [61], [62], we mainly focus on"}, {"title": "B. Slow Learner is (Almost) All You Need?", "content": "For continual learning from scratch, sequential fine-tuning (Seq FT) represents the worst-case performance in general. This is because $|\\nabla \\mathcal{M}_{gen}|| = 0$ and $|\\nabla \\mathcal{M}_{stab}||$ is often close to 0, and $\\nabla \\mathcal{M}_{plas}$ dominates the training gradients and results in catastrophic forgetting. When $\\mathcal{M}_{\\theta}$ is pre-trained with $\\mathcal{D}_{pt}$, the large-scale nature of $\\mathcal{D}_{pt}$ implicitly provides $\\mathcal{M}_{\\theta}$ a large $\\nabla \\mathcal{M}_{gen}$, making it success when fine-tuning on a wide range of separate downstream tasks. Therefore, a question is natually raised: whether the pre-trained weights can also implicitly supply us a proper $\\nabla \\mathcal{M}_{stab}$ like it does in providing $\\nabla \\mathcal{M}_{gen}$? In previous efforts on CLPT [61], [62], the answer is possibly \"No\" since Seq FT still performs poorly in their"}, {"title": "C. Slow Learner with Classifier Alignment", "content": "To further improve the classification layer, we propose to save statistics of each class during the continual learning progress and then align all classifiers in a post-hoc fashion (see Fig. 6 and Algorithm 1), called Classifier Alignment (CA). Specifically, after learning each task, we collect feature representations $\\mathcal{F} = {r_{c,1},..., r_{c,N_{c}}}$ for each class $c \\in C_{t}$ within the task, where $r_{c,n} = f_{rep}(x_{c,n})$ and $N_{c}$ denotes its amount. Instead of saving the extracted features $\\mathcal{F}_{c}$ of training samples, CA preserves their mean $\\mu_{c} \\in \\mathbb{R}^{d}$ and covariance $\\Sigma_{c} \\in \\mathbb{R}^{d \\times d}$ for each class $c$ ($d$ denotes the feature dimension) for memory efficiency. Whenever the model needs to be evaluated, the classification layers are further aligned as follows. Given the preserved mean $\\mu_{c}$ and covariance $\\Sigma_{c}$ for each classes, we model the feature distribution as a Gaussian $\\mathcal{N}(\\mu_{c}, \\Sigma_{c})$, since the use of pre-training provides well-distributed representations and each class tends to be single-peaked. Considering the possible semantic drift [68] problem, where the feature distribution after learning old tasks may not reflect the feature distribution at inference time, we introduce a slight modification to the feature mean $\\mu_{c}$ in CA. Specifically, according to the research in open-set recognition [6], [11], optimizing softmax cross-entropy loss within a finite category space would make un-known samples to have lower feature magnitude. In continual learning, training on classes of current task can lead to decrease of feature magnitude of old classes. Therefore, we scale down each feature mean $\\hat{\\mu}_{c} = \\lambda_{t} \\mu_{c}$ based on the learning progress with scaling factor $\\Lambda_t = 1 + \\eta * (T - t)$, where $\\eta$ controls the degree of scaling magnitude, which is set to 0.02 in all experiments, thus $\\Lambda_t$ is dynamically determined by the incremental progress, and $t$ is the the task identity that class $c$ belongs to. Next, we sample generated features $\\mathcal{F}_{c} = {f_{c,1},..., f_{c,S_{c}}}$ from the distribution $\\mathcal{N}(\\hat{\\mu}_{c}, \\Sigma_{c})$ of each class $c \\in C_{1:T}$, where $S_{c}$ is the amount of generated features for each class ($S_{c}$ is set to 256 in all experiments), and the number of tasks ever seen $T$"}, {"title": "D. SLCA with Parameter-efficient Fine-tuning (PEFT)", "content": "Our work reveals that tuning all parameters is clearly more advantageous than tuning a few inserted short sequence. Be-sides, with SLCA, a task-shared manner is effective enough to resolve the progressive overfitting problem. However, directly tuning all parameters is considered to be expensive for a large model. Recently, low-rank adaptation (LoRA) [25] provides"}, {"title": "E. Learning Objective", "content": "Typically, we use standard softmax cross-entropy loss $\\mathcal{L}_{CE} = -q \\log p$ on categories of the current task for optimize the network, where $q$ is the one-hot label distribution, $p = \\sigma(\\hat{l})$ is the post-softmax logits and $\\sigma(\\cdot)$ is the softmax function. However, training with cross-entropy loss is known to be dominant by gradients of low confidences samples [38]. In joint fine-tuning or a single task training, it is beneficial as it accelerate the convergence of the network with hard samples. In contrast, since the network is sequentially trained in contin-ual learning, the dominant gradients from the low confidences samples exacerbates the problem of progressive overfitting problem by suppressing $\\nabla \\mathcal{M}_{stab}$ and enlarging $\\nabla \\mathcal{M}_{plas}$, and thus deteriorates the final continual learning performance. Similar to the intuition of slow learner, a strategy for balancing the training speed (i.e., the magnitude of gradients here) is needed. Inspired by symmetric cross-entropy (SCE) [60], originally introduced for solving noisy label problem, we add a reverse cross-entropy (RCE) loss along with the CE loss as: $\\mathcal{L}_{SCE} = \\alpha \\mathcal{L}_{CE} + \\beta \\mathcal{L}_{RCE},$ where the second term $\\mathcal{L}_{RCE} = -p \\log q$ is the \"reverse\" version of $\\mathcal{L}_{CE}$ and two balancing factors $\\alpha$ and $\\beta$ are in-cluded. According to the previous analysis [13], [60], SCE loss can balance the gradients between high and low confidence samples, thus benefit the optimization of continual learning."}, {"title": "IV. EXPERIMENT", "content": "In this section, we first briefly describe the experimental setups, and then present the experimental results."}, {"title": "A. Experimental Setups", "content": "Datasets. Following L2P [62] and DualPrompt [61], we adopt pre-training from ImageNet-21K dataset [43], also known as the full ImageNet [10] consisting of 14,197,122 images with 21,841 classes. We also consider pre-training from ImageNet-1K dataset [31], a subset of ImageNet-21K introduced for the ILSVRC2012 visual recognition challenge, consisting of 1000-class images. To evaluate the performance of downstream continual learn-ing, for class-incremental setting, we consider four repre-sentative benchmark datasets and randomly split each of them into 10 disjoint tasks: The first two follow previous efforts [61], [62] and are relatively coarse-grained in terms of classification, while the last two are relatively fine-grained. Specifically, CIFAR-100 dataset [32] consists of 100-class natural images with 500 training samples per class. ImageNet-R dataset [22] contains 200-class images, split into 24,000 and 6,000 images for training and testing (similar ratio for each class), respectively. Note that although the image categories of ImageNet-R are overlapped with ImageNet-21K, all images are out-of-distribution samples for the pre-train dataset, i.e., hard examples from ImageNet or newly collected data of different styles. It requires considerable adaptations of the pre-trained model, therefore serving as a challenging benchmark for continual learning. CUB-200 dataset [50] includes 200-class bird images with around 60 images per class, 30 of which are used for training and the rest for testing. Cars-196 dataset [30] includes 196 types of car images, split into 8,144 and 8,040 images for training and testing (similar ratio for each class), respectively. We also consider the domain-incremental setting following S-Prompts [59] and evaluate our approach on DomainNet [39], a dataset with 345 categories from 6 different domains, counting a total of more than 600,000 images. Evaluation Metrics. We present the average accuracy of all classes after learning the last task, denoted as Last-Acc (equivalent to \"Avg. Acc\" in [61], [62]). We also compute the average accuracy of the classes ever seen after learning each incremental task and then present their average after learning the last task, denoted as Inc-Acc. Implementations. Following previous efforts [61], [62], we adopt a pre-trained ViT-B/16 backbone for all baselines."}, {"title": "B. Overall Performance", "content": "All continual learning methods based on Seq FT (i.e., the classical baselines) in Table I and II are equipped with our Slow Learner (SL) for fair comparison. For continual learning on relatively coarse-grained classification benchmarks, such as"}, {"title": "C. Ablation Study", "content": "Effect of Slow Learner (SL). We start from two basic baselines, including sequential fine-tuning with a uniform learning rate of 0.005 for both representation and classification layers, and continually learning classifier with a fixed repre-sentation layer, denoted as Seq FT and (Seq FT) w/ Fixed $\\Theta_{rep}$ in Table IV, respectively. Although (Seq FT) w/ Fixed $\\Theta_{rep}$ performs better than Seq FT, it is significantly inferior to Seq FT w/ SL, indicating the necessity of updating the representation layer while using a properly reduced learning rate to mitigate the progressive overfitting problem. Besides, we can see that while the benefits of SL are significant, the improvement is slightly limited in fine-grained datasets. Specifically, for ImageNet-21K supervised pre-training, the improvements of SL are 47.09%, 44.85% on Split CIFAR-100 and Split ImageNet-R but 28.05% and 22.17% for Split CUB-200 and Split Cars-196. As indicated by Fig. 5, a classifier alignment strategy is required. Effect of Classifier Alignment (CA). We first apply CA (as well as LN) on the Seq FT w/ Fixed $\\Theta_{rep}$ baseline. Surpris-ingly, it brings substantial improvements especially on fine-grained datasets (60.44% and 28.92% on Split CUB-200 and Split Cars-196 with IN21K-Sup pre-training), demonstrating it strong capacity for solving the above mis-alignment problem. Moreover, CA+LN also works well with our SL, compensat-ing improvement loss on fine-grained datasets. In details, it brings 2.67%, 5.20%, 19.64% and 17.99% on Split CIFAR-100, Split ImageNet-R, Split CUB-200 and Split Cars-196, respectively over the Seq FT baseline with IN21K-Sup pre-training. With IN1K-Self pre-training, our CA and CA+LN exhibit a similar trend of improvement. Note that our CA is operated in a post-hoc fashion rather"}, {"title": "D. Discussion", "content": "Pre-training Paradigm. Constructing strong pre-trained models typically requires large amounts of pre-training data, while the extensive annotation is scarce and expensive, mak-ing self-supervised pre-training a more realistic option than supervised pre-training. However, most of the existing CLPT studies only consider the use of strong supervised pre-training. Our initial investigation into continual learning with self-supervised pre-training indicates that state-of-the-art methods for CLPT tend to face severe challenges with it. Given its practical significance and inherent difficulty, we suggest future CLPT studies to direct more effort into this avenue. On the other hand, we also suggest subsequent work to develop self-supervised pre-training methods that are better suited for downstream continual learning, and potentially use this as a criterion to evaluate the progress of self-supervised learning. Scalability. We further discuss the scalability of our ap-proach. First, the generated features are only used to align the output layer at test time rather than training the entire back-bone, thus the computation is efficient and not accumulated in continual learning (e.g., ranging from only 0.67% to 5% of the"}, {"title": "V. CONCLUSION", "content": "In this work, we present the advanced Slow Learner with Classifier Alignment (SLCA++), a simple but effective ap-proach for the problem of continual learning with pre-training (CLPT). This approach stems from our in-depth analysis of the progressive overfitting issue in CLPT, where using a uniformly large learning rate to update the entire model tends to compromise the benefits of both stability and generaliz-ability from pre-training. In response, the Slow Learner (SL) demonstrates that using a selectively reduced learning rate for sequential fine-tuning can almost address this challenging issue in the representation layer, regardless of different pre-training paradigms and downstream granularity. Also, the Classifier Alignment (CA) helps to resolve the problem of sub-optimal classification layer by employing feature statistics. The combination of SL and CA unleash the hidden power of sequential fine-tuning for CLPT. We have further improved the efficacy and efficiency of our approach with Symmetric Cross-Entropy (SCE) and a parameter-efficient version called Hybrid Slow Learner (Hybrid SL), respectively. With all these designs, our approach achieves state-of-the-art results across a variety of CLPT benchmarks. Possible future work includes exploring better self-supervised pre-training for downstream continual learning as well as exploring SLCA++ in more scenarios, such as continual learning of multi-modal and embodied tasks."}, {"title": "APPENDIX A", "content": "IMPLEMENTATION DETAILS For class-incremental setting, all baselines follow an im-plementation similar to the one described in [61], [62]. Specifically, a pre-trained ViT-B/16 backbone is adopt for all methods. Adam optimizer is used for prompting-based methods [46], [61], [62] as well as for LAE [15]. An SGD optimizer is utilized for other baselines and ours, with the same batch size of 128. The original implementation of [61], [62] adopts a constant learning rate of 0.005 for all baselines, while our slow learner using 0.0001 for our full version and 0.001 for hybrid version to update representation layer, and 0.01 for the classification layer. In practice, we observe that supervised pre-training usually converges faster than self-supervised pre-training in downstream continual learning. Therefore, for supervised pre-training, we train all baselines for 20 epochs on Split CIFAR-100 and 50 epochs on other benchmarks. For self-supervised pre-training, we train all baselines for 90 epochs on all benchmarks."}, {"title": "APPENDIX B", "content": "EXTENDED ANALYSIS In this section, we provide extended ananlysis to support the main claims in our paper. First, In Tab. VI, we provide an analysis of the impact of learning rates on CLPT using the ImageNet-21K supervised pre-training. We report Last-Acc for varying learning rates of the representation layer ranging from 0 to 0.005. A higher learning rate (0.005) for the representation layer results in even poorer performance than a fixed representation due to progressive overfitting problem. On the other hand, a too small learning rate for drep can not improve learning on challenging datasets like Split Cars-196,"}, {"title": "SLCA++ with Prompt", "content": "We also explore the application of our framework with the prompt tuning. We append pre-trained prompt (pre-trained on ImageNet-1K dataset) with prompt length 10 in a same way as Visual Prompt Tuning [27]. We adopt the hybrid way of updating parameter-efficient part within the transformer. According to [15], prompts typically receive significantly small gradient compared to network parameters, which naturally satisfy the property of \"slow learner\". Therefore, we use a relatively large learning rate (0.02) for prompts. Compared with CODA-Prompt [46], our SLCA++ with prompt show a better performance, especially on fine-grained datasets, while requires much less learnable parameters."}, {"title": "APPENDIX C", "content": "EXTENDED ABLATIONS Combine with other methods. In the main text, the efficacy of SL has been widely validated by combining it with all baseline methods. We have further validated the efficacy of CA, presenting representative non-replay and replay methods on IN21K-Sup as shown in Table VII. Influence of Feature Statistic. In our implementation of CA, we save the covariance matrix Ec of each class to estimate the per class feature distribution. As discussed in the main text, the memory efficiency can be improved by using an global \u03a3 for all classes by momentum updating. As shown in Tab. VIII, only a slight decrease of performance is observed with this modification."}]}