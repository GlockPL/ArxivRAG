{"title": "SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training", "authors": ["Gengwei Zhang", "Liyuan Wang", "Guoliang Kang", "Ling Chen", "Yunchao Wei"], "abstract": "In recent years, continual learning with pre-training (CLPT) has received widespread interest, instead of its traditional focus of training from scratch. The use of strong pre-trained models (PTMs) can greatly facilitate knowledge transfer and alleviate catastrophic forgetting, but also suffers from progressive overfitting of pre-trained knowledge into specific downstream tasks. A majority of current efforts often keep the PTMs frozen and incorporate task-specific prompts to instruct representation learning, coupled with a prompt selection process for inference. However, due to the limited capacity of prompt parameters, this strategy demonstrates only sub-optimal performance in continual learning. In comparison, tuning all parameters of PTMs often provides the greatest potential for representation learning, making sequential fine-tuning (Seq FT) a fundamental baseline that has been overlooked in CLPT. To this end, we present an in-depth analysis of the progressive overfitting problem from the lens of Seq FT. Considering that the overly fast representation learning and the biased classification layer constitute this particular problem, we introduce the advanced Slow Learner with Classifier Alignment (SLCA++) framework to unleash the power of Seq FT, serving as a strong baseline approach for CLPT. Our approach involves a Slow Learner (SL) to selectively reduce the learning rate of backbone parameters, and a Classifier Alignment (CA) to align the disjoint classification layers in a post-hoc fashion. We further enhance the efficacy of SL with a symmetric cross-entropy loss (SCE), as well as employ a parameter-efficient strategy to implement Seq FT with SLCA++. Across a variety of continual learning scenarios, including class-incremental learning on general datasets like CIFAR-100 and ImageNet-R, fine-grained datasets like CUB-200 and Cars-196, and domain-incremental learning on DomainNet, our approach provides substantial improvements and outperforms state-of-the-art methods by a large margin. Our code is available at: https://github.com/GengDavid/SLCA.", "sections": [{"title": "I. INTRODUCTION", "content": "The purpose of continual learning (CL) is to learn contents that appear in sequence as if they were observed simultaneously. Previous efforts are mainly based on the premise of learning from scratch, attempting to mitigate catastrophic forgetting [36] of previously-learned knowledge when acquiring new information. However, the success of large-scale pre-training has revolutionized the learning paradigm of deep neural networks. The use of pre-training brings positive knowledge transfer and robustness to catastrophic forgetting for continual learning of specific downstream tasks [58], which tend to be more significant as the scale of pre-training increases [37], [41]. Therefore, continual learning with pre-training (CLPT) turns out to be an emerging direction and receives growing attention.\nCLPT poses a particular challenge that the pre-trained knowledge should be adapted to each incremental task while maintaining generalizability for future tasks. In this regard, recent prompt-based methods [46], [61], [62] propose to freeze the pre-trained backbone (i.e., the pre-trained knowledge carried therein) and introduce a few prompt parameters to instruct representation learning (see Fig 1), which often involve construction and selection of appropriate prompt parameters for specific tasks. Additionally, a concurrent work [15] devises a unified CLPT framework of parameter-efficient fine-tuning (PEFT) techniques, such as prompt [33], LoRA [25], and adapter [23], which employs the exponential moving average (EMA) of additional parameters to stabilize their updates. Despite some promising results, these methods remain clearly sub-optimal in CL of specific tasks, mainly due to the limited capacity of the additional parameters [52], [53]. In fact, PTMs tend to have stronger adaptability as more parameters can be adjusted, where tuning all parameters often provides the largest potential for representation learning. As a result, sequential fine-tuning (Seq FT) may serve as a fundamental baseline overlooked in CLPT, although it typically represents"}, {"title": "", "content": "the worst case of continual learning from scratch. Accordingly, we retreat to a simple yet important question: What is the sufficient inductive bias to make Seq FT robust in CLPT?\nTo this end, we perform an in-depth analysis of CLPT in terms of Seq FT. First, for a range of representative continual learning methods based on Seq FT, using a maximum stable learning rate [47] for all parameters leads to extremely inferior performance. This problem is largely due to the progressive overfitting of pre-trained knowledge into specific downstream tasks, thus gradually losing its generalizability and stability. We further validate this insight from the perspective of gradients and arouse our solution. We observe that this problem within the representation layer (i.e., the backbone) can be almost avoided by selectively reducing the learning rate, which is sufficient to balance pre-trained knowledge and task-specific knowledge. On the basis of a desirable representation layer, we identify that the classifier (i.e., the output layer in general) suffers from remarkable bias between tasks or classes, which require further alignment after continual learning. In this regard, we propose Slow Learner with Classifier Alignment (SLCA++), a strong baseline approach for CLPT. The former refers to the selectively reduced learning rate, while the latter employs class-wise distributions to rectify the classifier in a post-hoc fashion. To improve both efficacy and efficiency, we incorporates a SCE loss and a hybrid parameter-efficient fine-tuning strategy called Hybrid Slow Learner (Hybrid-SL). SCE furthur enhances the effect of slow learner while Hybrid-SL utilizes slow learner to tune all parameters by incorporating the idea of low-rank adaptation.\nAcross a variety of continual learning benchmarks under supervised and self-supervised pre-training, our approach provides substantial improvements in CLPT, and significantly fills the gap of current progress from the joint training performance. Specifically, our approach consistently improves the regular Seq FT by more than 45% on Split CIFAR-100, Split ImageNet-R, Split CUB-200, and Split Cars-196, thus outperforming the SOTA methods by a large margin. On Split CIFAR-100 and Split ImageNet-R, the performance gap is shorten to less than 2% for supervised pre-training and less than 4% for self-supervised pre-training. In particular, we find that other competitors suffer from remarkable performance drop under the more realistic self-supervised pre-training and more challenging fine-grained datasets, which makes our approach more advantageous and also points the way to subsequent work on CLPT.\nNote that this work is built on a preliminary version presented at ICCV 2023 [70], which introduced the idea of Slow Learner with Classifier Alignment (SLCA) and revealed the potential of Seq FT for all parameters in CLPT. The current version improves upon the previous one with more in-depth analysis, methodological enhancements and more extensive experiments. First, we provide a more in-depth analysis for CLPT, and explain its particular challenge from a gradient perspective. This explanation further reveals the intrinsic mechanisms of SLCA and helps to understand its implications for CLPT. On the methodology aspect, we devise a parameter-efficient fine-tuning technique to implement SLCA, named as Hybrid Slow Learner, and the SCE loss to improve the"}, {"title": "", "content": "performance, collectively referred to as SLCA++. As a result, it reduces the learnable parameters from 85.80M to 0.64M, with comparable or even better performance than before (e.g., improving the performance on Split Cars-196 from 67.73% to 73.97%). In the experimental section, we include extensive explorations of parameter-efficient fine-tuning for SLCA, evaluate domain-incremental learning, as well as compare our method with more recent baselines, so as to justify the generality of SLCA++.\nOur contributions include four aspects: (1) We present an in-depth analysis of CLPT, and demonstrate that the progressive overfitting problem is the key challenge for representative continual learning methods based on Seq FT. A principle explanation from the gradient perspective based on the progressive overfitting problem is included. (2) We propose a simple but effective approach named SLCA++ to unleash the power of sequential fine-tuning in CLPT, which clearly outperforms state-of-the-art competitors, serving as a strong baseline to re-evaluate the current progress and technical route. (3) We devise two strategies to further improve the efficacy and efficiency of this strong baseline, making it more applicable to continual learning scenarios. (4) Our results further identify critical factors and promising directions for CLPT, such as pre-training paradigm and downstream granularity, so as to facilitate subsequent research."}, {"title": "II. RELATED WORK", "content": "Continual Learning. Traditional works on continual learning mainly focus on sequential training of deep neural network(s) from scratch, ensuring effective learning of new tasks without catastrophic forgetting of old tasks. Representative strategies include regularization-based methods [2], [12], [28], [34], [55], [69], which preserve the old model and selectively stabilize changes in parameters or predictions; replay-based methods [5], [40], [51], [54], [56], [66], which approximate and recover the previously-learned data distributions; architecture-based methods [44], [45], [57], [67], which allocate dedicated parameter sub-spaces for each task. In addition, current advances in continual learning concentrate on the field of computer vision and gradually extend to the field of natural language processing [58].\nContinual Learning with Pre-training. In recent years, the benefits of pre-training for continual learning have been increasingly explored. For example, the representations obtained from supervised pre-training have been shown to facilitate not only knowledge transfer but also robustness to catastrophic forgetting in continual learning of specific downstream tasks [37], [41], [74]. Also, learning a large number of base classes in the initial training phase allows the model to learn new classes with only minor adaptations [65]. Inspired by the techniques of knowledge transfer in natural language processing, L2P [62] employs an additional set of learnable parameters called \"prompts\" that dynamically instruct the representation layer for learning incremental tasks. DualPrompt [61] extends this idea by attaching complementary prompts to the representation layer for learning task-invariant and task-specific instructions. Both L2P and DualPrompt require a prompt selection phase"}, {"title": "", "content": "before adaptation. CODA-Prompt [46] improves the utilization of prompts by an attention operation instead of the hard selection. Recently, LAE [15] introduces a unified framework that includes different forms of parameter sets such as LoRA [25], adapter [23] besides prompt. These methods are reported to be far superior to representative continual learning methods based on sequential training (refer to as Seq FT in the context of continaul learning with pre-training), which potentially challenges the current paradigm of using pre-trained knowledge in computer vision.\nParameter-efficient Fine-tuning. The pre-training then fine-tuning is the most adopted paradigm in transfer learning, where full fine-tuning guarantees a powerful performance. As the growth of pre-trained model size, parameter-efficient fine-tuning is introduced for improving efficiency. Partial tuning [20] is a straightforward way that freezes most backbone parts and only updates a small portion of parameters. Another idea is to keep the whole backbone fixed and introduces extra parameters like side-tuning [71], adapter [23], [42] and low-rank adaptation [25]. Recently, prompt-tuning [33] is introduced in language processing for fast adaptation and also introduces to computer vision [27]. However, visual prompt tuning [27] is find to struggle with adapting self-supervised pre-training.\nSelf-supervised Pre-training. Since the large amount of training samples required to construct strong PTMs are typically unlabeled and may also arrive incrementally, self-supervised pre-training emerges as a more preferable choice than supervised pre-training. Several recent studies discover that continual learning in a self-supervised manner suffers from less catastrophic forgetting [14], [24], [35]. Indeed, self-supervised paradigms have been shown to be better adapted to upstream continual learning, i.e., continual learning of generalized representations [8]. However, the effectiveness of self-supervised pre-training for downstream continual learning, i.e., continual learning based on a self-supervised pre-trained model, remains to be investigated."}, {"title": "III. CONTINUAL LEARNING WITH PRE-TRAINING", "content": "In this section, we introduce the problem formulation of continual learning with pre-training (CLPT) and perform an in-depth analysis of its particular challenge. We then present our approach based on the analysis."}, {"title": "A. Problem Formulation", "content": "CLPT Setup. Let's consider a neural network $\\mathcal{M}(\\cdot) = h_{\\theta_{cls}}(f_{\\theta_{rps}}(\\cdot))$ with parameters $\\theta = {\\theta_{rps}, \\theta_{cls}}$ for classification tasks, which often consists of a representation layer $f_{\\theta_{rps}}(\\cdot)$ that projects input images to feature representations, and a classification layer $h_{\\theta_{cls}}(\\cdot)$ that projects feature representations to output predictions. $\\theta_{rps}$ is initialized on a pre-training dataset $\\mathcal{D}_{pt}$ in a supervised or self-supervised manner (class labels are not necessary for the latter). Then, $\\mathcal{M}$ needs to learn a sequence of incremental tasks from their training sets $\\mathcal{D}_t, t = 1, ..., T$ and tries to perform well on their test sets. Following previous efforts of CLPT in the field of computer vision [61], [62], we mainly focus on"}, {"title": "", "content": "the class-incremental setting of continual learning [49]. In details, $\\mathcal{D}_t = \\cup_{c \\in C_t} {\\{ (x_{c,n}, y_{c,n}) \\}_{n=1}^{N_c} \\}$ introduces a set of new classes $C_t$, where $N_c$ denotes the number of training samples $(x_{c,n}, y_{c,n})$ for class $c$, and all the classes ever seen are evaluated without task labels. Besides, in the experiments, we also evaluate our approach under the domain-incremental setting [59], where $\\mathcal{D}_t = {\\{ (x_n, y_n) \\}}_{n=1}^{N_t}$ and $N_t$ denotes the number of training samples belonging to domain $t$. Each $\\mathcal{D}_t$ introduce training samples from new domains, while the set of classes $C$ is determined at the first task and then keep fixed for all domains.\nProgressive Overfitting. To achieve the objective of CLPT, the network needs to (1) effectively transfer the pre-trained knowledge to each incremental task while maintaining its generalizability for future tasks, and (2) properly balance the learning plasticity of new tasks with memory stability of old tasks, also knwon as overcoming the catastrophic forgetting problem [36]. The most straightforward baseline for CLPT is to train the model $\\mathcal{M}$ sequentially on each $\\mathcal{D}_t$, with $\\theta_{rps}$ and $\\theta_{cls}$ updated at a similar speed (i.e., using a larger learning rate for fast convergence). However, due to the lack of $\\mathcal{D}_{pt}$ and $\\mathcal{D}_{1:t-1}$, the performance is severely restricted by a progressive overfitting problem in both aspects. Specifically, the knowledge of $\\mathcal{D}_{pt}$ is largely interfered by $\\mathcal{D}_t$, as $\\theta_{rps}$ is continually updated to accommodate incremental tasks while its generalizability obtained from the pre-training stage is progressively lost. Besides, the knowledge of $\\mathcal{D}_{1:t-1}$ is interfered by $\\mathcal{D}_t$, as $\\theta_{cls}$ and $\\theta_{rps}$ catastrophically forget the old tasks when learning new tasks.\nTo make a clear illustration, we follow the analysis of stability gap in literature [9] and express the progressive overfitting problem from a gradient perspective:\n$\\nabla \\mathcal{M} = \\nabla \\mathcal{M}_{plas} + \\nabla \\mathcal{M}_{stab} + \\nabla \\mathcal{M}_{gen},$ (1)\nwhere $\\nabla \\mathcal{M}_{plas}$, $\\nabla \\mathcal{M}_{stab}$ and $\\nabla \\mathcal{M}_{gen}$ represent three gradient components for updating the model $\\mathcal{M}$ in continual learning. Specifically, $\\nabla \\mathcal{M}_{plas}$ denotes the plasticity gradient, which aims to minimize the classification error on the current task $t$. $\\nabla \\mathcal{M}_{stab}$ and $\\nabla \\mathcal{M}_{gen}$ represent the stability and generalizability gradients for maintaining stability of old tasks and generalizability of pre-trained knowledge, respectively. Below we will discuss their respective impacts on the challenges of continual learning."}, {"title": "B. Slow Learner is (Almost) All You Need?", "content": "For continual learning from scratch, sequential fine-tuning (Seq FT) represents the worst-case performance in general. This is because $\\| \\nabla \\mathcal{M}_{gen} \\| = 0$ and $\\| \\nabla \\mathcal{M}_{stab} \\|$ is often close to 0, and $\\nabla \\mathcal{M}_{plas}$ dominates the training gradients and results in catastrophic forgetting. When $\\mathcal{M}$ is pre-trained with $\\mathcal{D}_{pt}$, the large-scale nature of $\\mathcal{D}_{pt}$ implicitly provides $\\mathcal{M}$ a large $\\nabla \\mathcal{M}_{gen}$, making it success when fine-tuning on a wide range of separate downstream tasks. Therefore, a question is natually raised: whether the pre-trained weights can also implicitly supply us a proper $\\nabla \\mathcal{M}_{stab}$ like it does in providing $\\nabla \\mathcal{M}_{gen}$? In previous efforts on CLPT [61], [62], the answer is possibly \"No\" since Seq FT still performs poorly in their"}, {"title": "", "content": "implementation. By careful dissecting, we find that they use a relatively large learning rate (e.g., 0.005, which can make joint-training converges well on all datasets) for both $\\theta_{rps}$ and $\\theta_{cls}$. We conjecture that $\\nabla \\mathcal{M}_{stab}$ is obscured in this case by a large $\\| \\nabla \\mathcal{M}_{plas} \\|$, which is independent to $\\nabla \\mathcal{M}_{stab}$ and $\\nabla \\mathcal{M}_{gen}$ and can be explicitly modified by adjusting the learning rate. Surprisingly, when using a much smaller learning rate (0.0001) for $\\theta_{rps}$ and a slightly larger learning rate (0.01) for $\\theta_{cls}$, the sequential fine-tuning (Seq FT) baseline is greatly enhanced. As shown in Fig. 21, Seq FT is improved by more than 40% for challenging continual learning benchmarks such as Split CIFAR-100 and Split ImageNet-R, respectively. Besides, we find that such a simple change also makes Seq FT clearly outperform the recent prompt-based methods such as L2P [62] and DualPrompt [61]. These prompt-based methods [46], [61], [62] fixes the representation layer and employes an additional set of learnable parameters $\\theta_{add}$ to instruct the pre-trained model. From the gradient optimization perspective, although it makes $\\| \\nabla \\mathcal{M}_{gen} \\| \\approx \\infty$ for the fixed representation layer, the newly added parameters $\\theta_{add}$ would suffer from even severe progressive overfitting problem with random initialization.\nWe call the simple but remarkably effective strategy \"Slow Learner (SL)\", corresponding to slowing down the updating speed, i.e., reducing $\\| \\nabla \\mathcal{M}_{plas} \\|$ of the representation layer."}, {"title": "C. Slow Learner with Classifier Alignment", "content": "To further improve the classification layer, we propose to save statistics of each class during the continual learning progress and then align all classifiers in a post-hoc fashion (see Fig. 6 and Algorithm 1), called Classifier Alignment (CA). Specifically, after learning each task, we collect feature representations $\\mathcal{F} = {\\{ r_{c,1}, ..., r_{c,N_c} \\}\\}$ for each class $c \\in C_t$ within the task, where $r_{c,n} = f_{\\theta_{rps}} (x_{c,n})$ and $N_c$ denotes its amount. Instead of saving the extracted features $\\mathcal{F}_c$ of training samples, CA preserves their mean $\\mu_c \\in \\mathbb{R}^d$ and covariance $\\Sigma_c \\in \\mathbb{R}^{d \\times d}$ for each class $c$ ($d$ denotes the feature dimension) for memory efficiency.\nWhenever the model needs to be evaluated, the classification layers are further aligned as follows. Given the preserved mean $\\mu_c$ and covariance $\\Sigma_c$ for each classes, we model the feature distribution as a Gaussian $\\mathcal{N}(\\mu_c, \\Sigma_c)$, since the use of pre-training provides well-distributed representations and each class tends to be single-peaked. Considering the possible semantic drift [68] problem, where the feature distribution after learning old tasks may not reflect the feature distribution at inference time, we introduce a slight modification to the feature mean $\\mu_c$ in CA. Specifically, according to the research in open-set recognition [6], [11], optimizing softmax cross-entropy loss within a finite category space would make unknown samples to have lower feature magnitude. In continual learning, training on classes of current task can lead to decrease of feature magnitude of old classes. Therefore, we scale down each feature mean $\\hat{\\mu}_c = \\lambda_t \\mu_c$ based on the learning progress with scaling factor $\\lambda_t = \\frac{1}{1 + \\eta * (T - t)}$, where $\\eta$ controls the degree of scaling magnitude, which is set to 0.02 in all experiments, thus $\\lambda_t$ is dynamically determined by the incremental progress, and $t$ is the the task identity that class $c$ belongs to.\nNext, we sample generated features $\\mathcal{F}_c = {\\{ f_{c,1}, ..., f_{c,S_c} \\}\\}$ from the distribution $\\mathcal{N}(\\mu_c, \\Sigma_c)$ of each class $c \\in C_{1:T}$, where $S_c$ is the amount of generated features for each class ($S_c$ is set to 256 in all experiments), and the number of tasks ever seen $T$"}, {"title": "", "content": "can be any positive integer without being known in advance. The generated features $\\mathcal{F}_{1:T} = {\\{ \\mathcal{F}_1, ..., \\mathcal{F}_{C_{1:T}} \\}\\}$ ($C_{1:T}$ is the total number of classes in $C_{1:T}$) is feed to the classification layer $h_{\\theta_{cls}}$ as input, and a widely-used cross-entropy loss is adopted to furthur optimze the classification layer.\nHowever, a prolonged training of the classification layer can lead to an overconfidence issue, which potentially impairs generalizability to the test set(s). To overcome this issue, we draw inspirations from out-of-distribution (OOD) detection [63] and normalize the magnitude of network outputs when computing the cross-entropy. Let $\\hat{l} = h_{\\theta_{cls}} (r)$ denote the logit (i.e., pre-softmax output) of a generated feature $r$ and $\\hat{l} \\in \\mathbb{R}^{C_{1:T}}$, which can be re-written as the product of two components: $\\hat{l} = \\| \\hat{l} \\| \\cdot \\hat{\\iota}$, where $\\| \\cdot \\|$ denotes L2-norm. Accordingly, $\\| \\hat{l} \\| = \\sqrt{\\sum_{c \\in C_{1:T}} \\| l_c \\|^2}$ represents the magnitude of $\\hat{l}$, and $\\hat{\\iota}$ represents its direction. Then we adopt a modified cross-entropy loss with logit normalization to perform CA:\n$\\mathcal{L}(\\theta_{cls}; r) = - \\log \\frac{e^{l_y / (\\tau \\| \\hat{l} \\|)}}{\\sum_{c \\in C_{1:T}} e^{l_c / (\\tau \\| \\hat{l} \\|)}}$, (2)\nwhere $l_y$ denotes the $y$-th element of $\\hat{l}$ corresponding to the ground-truth label $y$. $\\tau$ is a temperature hyperparameter. The intuition behind it is that normalizing $l_c$ with an input-dependent constant $\\tau \\| \\hat{l} \\|$ will not change the result of prediction $\\underset{c \\in C_{1:T}}{\\mathrm{argmax}}(l_c)$, while forcing the magnitude $\\| \\hat{l} \\|$ before softmax becomes $\\tau$, which can make the criterion only adjust the direction $\\hat{\\iota}$ [63]. Therefore, the normalization in Eqn. 2"}, {"title": "D. SLCA with Parameter-efficient Fine-tuning (PEFT)", "content": "Our work reveals that tuning all parameters is clearly more advantageous than tuning a few inserted short sequence. Besides, with SLCA, a task-shared manner is effective enough to resolve the progressive overfitting problem. However, directly tuning all parameters is considered to be expensive for a large model. Recently, low-rank adaptation (LoRA) [25] provides"}, {"title": "E. Learning Objective", "content": "Typically, we use standard softmax cross-entropy loss $\\mathcal{L}_{CE} = -q \\log p$ on categories of the current task for optimize the network, where $q$ is the one-hot label distribution, $p = \\sigma(l)$ is the post-softmax logits and $\\sigma(\\cdot)$ is the softmax function. However, training with cross-entropy loss is known to be dominant by gradients of low confidences samples [38]. In joint fine-tuning or a single task training, it is beneficial as it accelerate the convergence of the network with hard samples. In contrast, since the network is sequentially trained in continual learning, the dominant gradients from the low confidences samples exacerbates the problem of progressive overfitting problem by suppressing $\\nabla \\mathcal{M}_{stab}$ and enlarging $\\nabla \\mathcal{M}_{plas}$, and thus deteriorates the final continual learning performance. Similar to the intuition of slow learner, a strategy for balancing the training speed (i.e., the magnitude of gradients here)"}, {"title": "APPENDIX A IMPLEMENTATION DETAILS", "content": "For class-incremental setting, all baselines follow an implementation similar to the one described in [61], [62]. Specifically, a pre-trained ViT-B/16 backbone is adopt for all methods. Adam optimizer is used for prompting-based methods [46], [61], [62] as well as for LAE [15]. An SGD optimizer is utilized for other baselines and ours, with the same batch size of 128. The original implementation of [61], [62] adopts a constant learning rate of 0.005 for all baselines, while our slow learner using 0.0001 for our full version and 0.001 for hybrid version to update representation layer, and 0.01 for the classification layer. In practice, we observe that supervised pre-training usually converges faster than self-supervised pre-training in downstream continual learning. Therefore, for supervised pre-training, we train all baselines for 20 epochs on Split CIFAR-100 and 50 epochs on other benchmarks. For self-supervised pre-training, we train all baselines for 90 epochs on all benchmarks."}, {"title": "APPENDIX B EXTENDED ANALYSIS", "content": "In this section, we provide extended ananlysis to support the main claims in our paper. First, In Tab. VI, we provide an analysis of the impact of learning rates on CLPT using the ImageNet-21K supervised pre-training. We report Last-Acc for varying learning rates of the representation layer ranging from 0 to 0.005. A higher learning rate (0.005) for the representation layer results in even poorer performance than a fixed representation due to progressive overfitting problem. On the other hand, a too small learning rate for $\\theta_{rep}$ can not improve learning on challenging datasets like Split Cars-196,"}]}