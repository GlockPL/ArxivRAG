{"title": "Bidding Games on Markov Decision Processes with Quantitative Reachability Objectives", "authors": ["Guy Avni", "Martin Kure\u010dka", "Kaushik Mallik", "Petr Novotn\u00fd", "Suman Sadhukhan"], "abstract": "Graph games are fundamental in strategic reasoning of multi-agent systems and their environments. We study a new family of graph games which combine stochastic environmental uncertainties and auction-based interactions among the agents, formalized as bidding games on (finite) Markov decision processes (MDP). Normally, on MDPs, a single decision-maker chooses a sequence of actions, producing a probability distribution over infinite paths. In bidding games on MDPs, two players-called the reachability and safety players-bid for the privilege of choosing the next action at each step. The reachability player's goal is to maximize the probability of reaching a target vertex, whereas the safety player's goal is to minimize it. These games generalize traditional bidding games on graphs, and the existing analysis techniques do not extend. For instance, the central property of traditional bidding games is the existence of a threshold budget, which is a necessary and sufficient budget to guarantee winning for the reachability player. For MDPS, the threshold becomes a relation between the budgets and probabilities of reaching the target. We devise value-iteration algorithms that approximate thresholds and optimal policies for general MDPs, and compute the exact solutions for acyclic MDPs, and show that finding thresholds is at least as hard as solving simple-stochastic games.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph games are fundamental for reasoning about strategic interactions between agents in multi-agent systems, with [29, 31, 40] or without [3] external environments. Environments, when present, are commonly modeled using stochastic processes, like Markov decision processes (MDP) [39] in reinforcement learning (single-agent), and stochastic games in the multi-agent setting [21, 25, 28].\nWe study games which combine stochastic environments with auction-based interactions among players, formalized as bidding games on MDP arenas. An MDP is a graph whose vertices are partitioned into control vertices and random vertices, and the game involves the players moving a token along the edges of the graph. The rules of the game are as follows. The two players are allocated initial budgets, normalized in a way that their sum is 1. When the token reaches a control vertex, an auction is held to determine who chooses where the token goes next. In these auctions, the players simultaneously submit bids from their available budgets, the higher bidder moves the token and pays his bid amount to the lower bidder. When the token reaches a random vertex, it automatically moves to one of the successors according to the transition probabilities of the MDP (without affecting the budgets of the players).\nWe consider the quantitative reachability objectives, where the goal of the first player, called the reachability player, is to maximize the probability that a given target vertex is reached, and the goal of the second player, called the safety player, is to minimize it."}, {"title": "2 PRELIMINARIES OF MARKOV DECISION PROCESSES (MDP)", "content": "Syntax. An MDP is a tuple $(V, V_c, V_r, E, \\delta)$, where $V$ is a finite set of vertices, $V_c$ and $V_r$ are the control and random vertices such that $V_c \\cup V_r = V$ and $V_c \\cap V_r = \\emptyset$, $E: V_c \\rightarrow 2^V$ is the control transition function, and $\\delta: V_r \\rightarrow \\Delta(V_c)$ is the random transition function, where $\\Delta(V_c)$ is the set of all probability distributions over $V_c$. The set of successors of vertex $v$ will be denoted as $Succ(v)$, where $Succ(v) := E(v)$ if $v \\in V_c$ and $Succ(v) := \\{v' \\in V \\mid \\delta(v)(v') > 0\\}$ if $v \\in V_r$. A vertex $v$ is called sink if $Succ(v) = \\{v\\}$.\nSemantics. Semantics of MDPs are summarized below; details can be found in standard textbooks [38]. A path of an MDP starting at a given vertex $v \\in V$ is a sequence $v^0v^1 \\ldots$ with $v^0 = v$ and every $v^{i>0}$ being a successor of $v^{i-1}$. Paths can be either finite or infinite. We write $Paths_{fin}(M)$ and $Paths_{inf}(M)$ to denote, respectively, the set of all finite and infinite paths, and write $Paths(M)$ to denote the set of all finite paths that end in a control vertex. A scheduler is a function $\\theta: Paths(M) \\rightarrow V$ mapping every finite path $\\rho = v \\ldots v_k$ ending at the control vertex $v_k \\in V_c$ to one of its successors; i.e., $\\theta(\\rho) \\in E(v_k)$. Every scheduler $\\theta$ induces a unique probability distribution $P_{M,v}^{\\theta}(\\cdot)$ over the paths of $M$ with initial vertex $v$.\nSpecifications. A specification $\\varphi$ over an MDP $M$ is a set of infinite paths of $M$. We will consider reachability and safety specifications of both bounded and unbounded variants, defined below. Given a set of vertices $T \\subseteq V$ called the target vertices, and an integer $h > 0$, the bounded-horizon reachability specification is the set of paths that visit $T$ in at most $h$ steps, i.e., $Reach_{M,h}(T) :=$"}, {"title": "3 BIDDING GAMES ON MDP-S", "content": "On a given M, we consider a zero-sum \"token game\" between two players, who will be referred to as the reachability and safety players. Initially, the token is placed in a given initial vertex, and the players are allocated budgets (positive real numbers) whose sum is 1. As convention, we will only specify the reachability player's budget as B, and the safety player's budget will be implicit (i.e., 1 \u2013 B).\nThe game is played as follows. When the token is in a control vertex $v$, the players independently and simultaneously propose their bids which may not exceed their current budgets. Whoever bids higher pays his bid amount to the other player and moves the token to one of $v$'s successors. The budgets are updated accordingly: assuming the reachability player's budget at $v$ was $B$, if he wins by bidding $b_r$, his new budget will reduce to $B - b_r$, and if the safety player wins by bidding $b_s$, the reachability player's new budget will increase to $B + b_s$. On the other hand, when the token is in a random vertex $v$, it moves to a successor $w$ with probability $\\delta(v)(w)$, and the budgets of the players remain unaffected.\nThe game continues in this fashion forever, generating an infinite path traversed by the token. Given a set of target vertices $T$, the goal of the reachability player is to maximize the probability that the token eventually reaches $T$ from the initial vertex, while the goal of the safety player is to minimize this probability.\nPolicies and paths. We formalize bidding games on MDPs as follows. A policy of a player is a function of the form $[0, 1] \\times Paths_{fin}(M) \\rightarrow [0, 1] \\times V$, mapping every pair of available budget $B$ and finite path $v^0 \\ldots v_k$ to a pair of a bid value $b \\leq B$ and a successor of $v_k$. We will write $\\sigma$ and $\\tau$ to represent the policy of the reachability and the safety player, respectively.\nSuppose we are given an initial vertex $v$ and an initial budget $B$ of the reachability player (recall that the safety player's initial budget will be $1 - B$). We will call the pair $(v, B)$ the initial configuration. Every pair of policies $(\\sigma, \\tau)$ and the initial configuration $(v, B)$ induce a scheduler $\\theta(\\sigma, \\tau, B)$ as follows: if the current path is $\\rho \\in Paths_{fin}(M)$ and the current budget of the reachability player is $B'$, then, denoting $\\sigma(B', \\rho) = (b_r, u)$ and $\\tau(1 - B', \\rho) = (b_s, w)$, we define the scheduler $\\theta$ as follows:\n*   if $b_r \\geq b_s$, i.e., if the reachability player wins the bidding, then $\\theta(\\sigma, \\tau, B) (\\rho) = u$, and the reachability player's new budget is $B' - b_r$, and\n*   if $b_r < b_s$, i.e., if the safety player wins the bidding, then $\\theta(\\sigma, \\tau, B)(\\rho) = w$, and the reachability player's new budget is $B' + b_s$.\nWe will write $P_{v,B}^{\\theta_{\\sigma, \\tau}}$ instead of $P^{\\theta(\\sigma,\\tau,B)}_v$ to denote the probability distribution over the set of infinite paths starting at vertex $v$.\nWinning conditions. Let $(v, B)$ be an initial configuration, $\\varphi$ be a reachability specification (bounded or unbounded), and $p \\in [0, 1]$ be the required probability for the reachability player to satisfy $\\varphi$. A winning policy of the reachability player is a policy $\\sigma$ such that for every policy $\\tau$ of the safety player, it holds that $P_{v,B}^{\\theta_{\\sigma, \\tau}}(\\varphi) \\geq p$.\nDually, a winning policy of the safety player is a policy $\\tau$ such that for every policy $\\sigma$ of the reachability player, it holds that $P_{v,B}^{\\theta_{\\sigma, \\tau}}(\\varphi) \\leq p$.\nWe will write $\\Pi_R(B, p, v, \\varphi)$ and $\\Pi_S(B, p, v, \\varphi)$ to denote the sets of winning policies for reachability and safety players, respectively. If $\\varphi$ is clear, we will simply write $\\Pi_R(B, p, v)$ and $\\Pi_S(B, p, v)$.\nThresholds. In traditional reachability bidding games on graphs, where probabilities are unnecessary, the threshold of a vertex $v$ is the budget $B$ such that the reachability player wins from $v$ with every budget $B' > B$, and loses with every budget $B' < B$. In bidding games on MDPs, thresholds generalize to relations over budgets and probabilities: The threshold of $v$ is the set of all pairs $(B, p)$ such that the reachability player wins with every budget greater than $B$ and required probability less than $p$, and loses with every budget less than $B$ and required probability larger than $p$.\nDefinition 3.1 (Threshold). For a given vertex $v$, the threshold of $v$, written $Th_v$, is the set of all pairs $(B, p)$ such that $\\Pi_R(B', p', v)$ is nonempty whenever $B' > B$ and $p' < p$, and $\\Pi_S(B', p', v)$ is nonempty whenever $B' < B$ and $p' > p$.\nA central question in traditional bidding games is whether thresholds exist, because then it can be determined which of the players will win based on the budget allocation, as long as the budget is not exactly equal to the threshold. In our case, the existence question of thresholds generalizes to the question of whether the threshold completely separates the winning points of the two players.\nDefinition 3.2 (Completely separating thresholds). The threshold of $v$ is completely separating if for every point $(B, p) \\notin Th_v$,\n*   there exists $(B', p') \\in Th_v$ such that either $B < B'$ and $p > p'$, or $B > B'$ and $p < p'$, and\n*   exactly one of the sets $\\Pi_R(B, p, v)$ and $\\Pi_S(B, p, v)$ is nonempty.\nThe algorithmic question. We define problem instances as tuples of the form $(M, v, T, B, p)$, where $M$ is an MDP, $(v, B)$ is the initial configuration, $T$ is the target, and $p$ is the required probability of satisfying the reachability specification $Reach_M(T)$. The subject of this paper is how to decide who wins in a given problem instance.\nPROBLEM 1 (QUANTITATIVE REACHABILITY). Let $(M, v, T, B, p)$ be a problem instance. For a given $j \\in \\{R, S\\}$, decide if the set $\\Pi_j(B, p, v, Reach_M(T))$ is nonempty.\nIf $\\Pi_R(B, p, v, Reach_M(T)) \\neq \\emptyset$, our decision procedure will produce the witness winning policy for the reachability player as a byproduct; construction of winning policies for the safety player is solved for acyclic MDPs, and remains open for general MDPs. We will assume that $T$ is a set of sinks, which is without loss of any generality since the game ends as soon as $T$ is reached."}, {"title": "4 BOUNDED-HORIZON VALUE ITERATION", "content": "We start with the bounded-horizon variant of Prob. 1 with horizon $h$. In this setting, we propose a 2-dimensional value iteration algorithm for deciding who wins the game.\nFor reachability, for each vertex $v$, our algorithm computes a monotonically increasing (with respect to \u201c$\\subseteq$\u201d) sequence of \u201cvalues\u201d $r$-$val_0, \\ldots, r$-$val_h \\subseteq [0, 1]^2$, where $r$-$val_i$ will be shown to represent the set of all $(B, p)$ such that for every $B' > B$, the reachability player can reach $T$ from the initial configuration $(v, B)$ with probability at least $p$ in at most $i$ steps. Dually, for safety, for each vertex $v$, our algorithm computes a monotonically decreasing sequence of \u201cvalues\u201d $s$-$val_0, \\ldots, s$-$val_h \\subseteq [0, 1]^2$, where $s$-$val_i$ will be shown to represent the set of all $(B, p)$ such that for every $B' < B$, the safety player can avoid $T$ from the initial configuration $(v, B)$ with probability at least $1 - p$ for at least $i$ steps.\nClearly, if $v$ is in $T$, the target $T$ will be \u201creached\u201d in zero steps, no matter what $(B, p)$ is, and therefore every $(B, p)$ belongs to $r$-$val_0$. In contrast, if $v$ is not in $T$, the target $T$ will be reached in zero steps only with probability $p = 0$. The points with $B = 1$ are trivially included to $r$-$val_0$ as well, because $\\{B \\mid B > B = 1\\} = \\emptyset$. By duality, the definition of $s$-$val_0$ is exactly the opposite.\nWe now consider the case of $i > 0$ and $v \\notin T$. We take the perspective of the reachability player; the case of safety is similar. Consider the following two cases. (a) Suppose $v \\in V_r$. Since there is no bidding in $v$, the budgets of the players at $v$ remain unaffected after the transition. For a fixed budget $B$, if $p_w$ is the probability of reaching $T$ in $i - 1$ steps from the successor $w$ (of $v$), then the probability of reaching $T$ in $i$ steps from $v$ becomes $\\Sigma_w p_w \\cdot \\delta(v)(w)$. (b) Now suppose $v \\in V_c$. For a fixed probability $p$, we can ask for the least budget needed to reach $T$ from $v$. If $B_+$ and $B_-$ are the maximum and minimum budgets required from any successor to reach $T$ in $i - 1$ steps with probability $p$, then the budget required at $v$ for $i$-step reachability is $(B_+ + B_-)/2$. This follows from the fact that the bid $(B_+ - B_-)/2$ will either lead to won bidding and budget $B_-$ or lost bidding and budget $B_+$.\nThe value iteration algorithm is now formally presented below.\n$\\begin{aligned} r-val_0 &:= \\begin{cases} [0, 1]^2 & \\text{if } v \\in T, \\\\ [0, 1] \\times \\{0\\} \\cup \\{1\\} \\times [0, 1] & \\text{otherwise}, \\end{cases} \\\\ s-val_0 &:= \\begin{cases} [0, 1] \\times \\{1\\} \\cup \\{0\\} \\times [0, 1] & \\text{if } v \\in T, \\\\ [0, 1]^2 & \\text{otherwise}, \\end{cases} \\end{aligned}$\nand for $i > 0$, $r$-$val_i := T_v (\\{r$-$val_{i-1}^w \\mid w \\in Succ(v)\\}$) and $s$-$val_i = T_v (\\{s$-$val_{i-1}^w \\mid w \\in Succ(v)\\})$, where the operator $T_v$ is defined as follows: If $v \\in V_r$,\n$T_v (\\{val_w \\mid w \\in Succ(v)\\}$) := \\{(B, \\Sigma_{w \\in Succ(v)} \\delta(v)(w) p_w) \\mid \\forall w \\in Succ(v). (B, p_w) \\in val_w \\}."}, {"title": "5 FROM BOUNDED TO UNBOUNDED HORIZON", "content": "5.1 Limiting Behavior of Value Iteration\nIf we continue the bounded-horizon value iteration for increasing horizon, we obtain the following values in the limit:\n$r$-$val^* := cl(\\bigcup_{i=0}^{\\infty} r$-$val_i)\\quad and \\quad s$-$val^* := \\bigcap_{i=0}^{\\infty} s$-$val_0$,\nwhere $cl[S]$ denotes the closure of a set $S$ in the Euclidean metric; note that $s$-$val^*$ is closed by construction. In this section, we establish a connection between $r$-$val^*$, $s$-$val^*$ and the true values that are winning for the respective player in the unbounded horizon setting. In particular, we relate the limit sets to the threshold"}, {"title": "6 BIDDING GAMES ON RESTRICTED MDP-S", "content": "Now we consider the special cases of Prob. 1 for acyclic and tree-shaped MDPs, which are MDPs whose underlying transition graphs are acyclic (with loops on sinks) and rooted tree, respectively.\n6.1 Acyclic MDPs\nIt is easy to see that, contrary to general MDPs (with cycles), the value iteration algorithm (from Sec. 5) converges in at most $|V|$ iterations for acyclic MDPs. This implies that AlgExact will always terminate in at most $|V|$ iterations.\nLemma 6.1. For acyclic MDPs, for every vertex $v$, $r$-$val_v^{|V|} = r$-$val_v^*$ and $s$-$val_v^{|V|} = s$-$val_v^*$.\nLem. 6.1 implies that Prob. 1 is in EXPTIME for acyclic MDPs.\nTheorem 6.2. For acyclic MDPs, AlgExact is a sound and complete algorithm for Prob. 1 and terminates in at most $|V|$ iterations, yielding the space and time complexity $O(\\frac{|V|^{O(|V|)}}{\\delta_{min}})$.\n6.2 Tree-Shaped MDPs\nIn case the MDP is tree-shaped, we can solve Prob. 1 in NP \u2229 co-NP. The main idea of the proof is to find a certificate of the fact that $(B, p)$ belongs to $r$-$val_V$. According to the inductive definition of $r$-$val^v$ by (3) and (4), the presence of $(B, p)$ in $r$-$val^v$ can be witnessed by a point $(B_w, p_w) \\in r$-$val_v^{|V|-1}$ for every $w \\in Succ(v)$. A similar witness can be constructed to show that each of the points $(B_w, p_w)$ belongs to $r$-$val_v^{|V|-1}$, and the process can be repeated recursively up to the base case $(B_u, p_u) \\in r$-$val_v^v$. Therefore, whenever $(B, p)$ belongs to $r$-$val^v$, there exists a certificate of this fact in the form of a finite set of points satisfying the relations in (3) and (4). If $M$ is a tree, the certificate moreover contains a single point $(B_u, p_u)$ for every vertex $u \\in V$. The whole certificate then satisfies the following constraints:\n$\\begin{array}{ll} \\forall u \\in V_0 & B_u \\le B, p_0 \\ge p \\\\ \\forall u \\in V_0 & B_u = \\frac{B_{u^-} + B_{u^+}}{2} \\\\ & P_u \\le \\min_{w \\in Succ(u)} p_w, B_{u^+} \\ge \\max_{w \\in Succ(u)} B_w \\\\ \\forall u \\in V_r & B_u \\ge \\max_{w \\in Succ(u)} B_w, P_u = \\sum_{w \\in Succ(u)} \\delta(u)(w) \\cdot p_w \\\\ \\forall t \\in T: 0 \\le B_t \\le 1, 0 \\le p_t \\le 1 & \\\\ \\forall z \\in Z: B_z = 1 \\text{ or } p_z = 0, & \\end{array}$\nwhere $Z$ is the set of leaves not in $T$. By fixing a choice of $u$ for every control vertex $u$, and a choice of which of the two equalities should hold for each $z \\in Z$, we create a concrete linear program. The point $(B, p)$ belongs to $r$-$val_v$ if and only if there is a choice that makes the linear program feasible. The same idea can be used to prove a point belongs to $s$-$val_v$. The following result follows.\nTHEOREM 6.3. For tree-shaped MDPs, Prob. 1 is in NP \u2229 co-NP."}, {"title": "7 CONCLUSIONS AND FUTURE WORK", "content": "We studied bidding games on MDPs with quantitative reachability and safety specifications. We show that thresholds are binary relations over budgets and probabilities. This makes their computation significantly more challenging than traditional bidding games on graphs, for which thresholds are scalars (budgets). We developed a new value iteration algorithm for approximating the threshold up to arbitrary precision, and showed how it can be used to decide whether a given initial budget B suffices to win w.p. at least p, assuming $(B, p)$ is not on the threshold (Assump. 1). In acyclic and tree-shaped MDPs, Assump. 1 is not required and the decision procedure becomes significantly more efficient.\nA number of questions remain open: Is Prob. 1 decidable without Assump. 1? What are the exact complexities? (There is a big gap between the upper and lower complexity bounds.) Furthermore, several interesting extensions can be considered, namely extensions to richer classes of specifications (like w-regular and mean-payoff) and extensions to different forms of bidding mechanisms (like poorman and taxman, both with and without charging). Another interesting question is the equivalence with stochastic models (recall that bidding games are equivalent to random-turn games). This is still unclear, because even if the threshold budget were simulated by random turn assignments, this randomness would not \"blend\" with the existing randomness (in the random transitions) in the MDP, and we would obtain stochastic games with two sources of probabilities, which have not been studied to the best of our knowledge.\nFinally, the foundation of auction-based scheduling [13] on MDPs is now ready, and it will be interesting to investigate how policy synthesis for multi-objective MDPs can benefit from it."}]}