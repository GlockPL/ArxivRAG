{"title": "Large Language Models for Scholarly Ontology Generation: An Extensive Analysis in the Engineering Field", "authors": ["Tanay Aggarwal", "Angelo Salatino", "Francesco Osborne", "Enrico Motta"], "abstract": "Ontologies of research topics are crucial for structuring scientific knowledge, enabling scientists to navigate vast amounts of research, and forming the backbone of intelligent systems such as search engines and recommendation systems. However, manual creation of these ontologies is expensive, slow, and often results in outdated and overly general representations. As a solution, researchers have been investigating ways to automate or semi-automate the process of generating these ontologies. This paper offers a comprehensive analysis of the ability of large language models (LLMs) to identify semantic relationships between different research topics, which is a critical step in the development of such ontologies. To this end, we developed a gold standard based on the IEEE Thesaurus to evaluate the task of identifying four types of relationships between pairs of topics: broader, narrower, same-as, and other. Our study evaluates the performance of seventeen LLMs, which differ in scale, accessibility (open vs. proprietary), and model type (full vs. quantised), while also assessing four zero-shot reasoning strategies. Several models have achieved outstanding results, including Mixtral-8\u00d77B, Dolphin-Mistral-7B, and Claude 3 Sonnet, with F1-scores of 0.847, 0.920, and 0.967, respectively. Furthermore, our findings demonstrate that smaller, quantised models, when optimised through prompt engineering, can deliver performance comparable to much larger proprietary models, while requiring significantly fewer computational resources.", "sections": [{"title": "1. Introduction", "content": "Ontologies of research topics, along with other knowledge organization systems (KOS) such as taxonomies and thesauri, are essential for organising and retrieving information from digital libraries. Indeed, major publishers such as Elsevier, SpringerNature, ACM, IEEE, and PubMed, utilise a range of organization systems like the All Science Journal Classification (ASJC),"}, {"title": "2. Related Work", "content": "In this section, we review the literature, by beginning with a discussion on the ontologies of research areas (Section 2.1) and then explore the state-of-the-art methodologies for generating these ontologies (Section 2.2). In this context, an ontology is a formal representation of knowledge within a specific domain, defining the types of entities that exist and the relationships between them. An ontology of research areas applies this concept to the scholarly domain, systematically organising the various fields of study, their sub-disciplines, and their interconnections [18]."}, {"title": "2.1. Ontologies of Research Areas", "content": "In the literature we can find several ontologies of research areas, each differing in scope, size, depth, and curation. Notable examples include the ACM Computing Classification System, Medical Subject Headings, IEEE Thesaurus, and UNESCO Thesaurus [2, 3, 19, 15]. One crucial aspect of these ontologies is their coverage. For instance, the Medical Subject Headings (MeSH) is a single-field ontology with over 30K concepts in the field of Medicine [2]."}, {"title": "2.2. Generation of Ontologies of research topics", "content": "Previous decades have seen the emergence of several automated solutions for ontology creation, yet these have primarily been applied outside the scholarly domain, explaining why scholarly ontologies often remain manually curated. This section will first explore successful general approaches to automatic ontology generation, followed by a look at the limited applications within the scholarly domain. Early work focused on semi-automatic methods, combining human expertise with computational techniques. Maedche and Staab [21] introduced a method using linguistic and statistical techniques to extract concepts and relationships from text, but heavily relied on human input. Cimiano et al. [22] advanced this with Text2Onto, integrating techniques from machine learning and natural language processing to automate extraction, though it still required substantial human validation. Fully automatic methods emerged with OntoLearn [23], which used lexical resources"}, {"title": "3. Background", "content": "This section defines the task under investigation (Section 3.1), describes the gold standard developed for this study (Section 3.2), and presents an overview of the LLMs used in the experiments (Section 3.3)."}, {"title": "3.1. Task definition", "content": "This paper addresses the challenge of identifying semantic relationships between pairs of research topics, denoted in the following as ta and t\u00df. We formalise this task as a single-label, multi-class classification problem, where each topic pair is assigned to one of four distinct categories commonly utilised in research topic ontologies [30]: broader, narrower, same-as, or other. We define the four categories as follows:\n\u2022 broader: ta is a parent topic of t\u00df. E.g., machine learning is a broader area than deep neural networks;\n\u2022 narrower: ta is a child topic of t\u00df. E.g., distributed databases is a specific area within databases. This is the inverse relationship of broader;\n\u2022 same-as: ta and t\u00df can be used interchangeably to refer to same concept. E.g., haptic interface and haptic device. This relationship is symmetric;\n\u2022 other: ta and te do not relate according to the above categories. E.g., software algorithms and cyclones. Also this relationship is symmetric."}, {"title": "3.2. Gold Standard", "content": "To evaluate the performance of LLMs on this task, we created a gold standard by selecting a sample of 1,000 semantic relationships from the IEEE Thesaurus. We have named this dataset IEEE-Rel-1K (IEEE Relations). The IEEE thesaurus contains around 11,730 engineering, technical, and scientific terms, including IEEE-specific society terms. It is compiled from IEEE transactions, journal articles, conference papers, and standards, and reflects the vocabulary used within the engineering and scientific fields of IEEE. Specifically, we utilised the IEEE Thesaurus v1.024, which is dated back to July 2023, and is available as a PDF following the ANSI/NISO Z39.4-2021 standard [31].\nTo create IEEE-Rel-1K, we developed a Python script for extracting the hierarchical structure and relationships between terms from the IEEE Thesaurus PDF file. Next, we randomly selected 250 relationships for each of the categories: broader, narrower, and same-as. For same-as, we used a combination of 'use preferred term' and 'used for' relationships as defined in the ANSI/ISO standard. To complete the dataset, we randomly generated 250 pairs of topics, ensuring they were not semantically related within the IEEE Thesaurus, and labelled these as other.\nWe released IEEE-Rel-1K within the GitHub repository: https://github.com/ImTanay/ LLM-Automatic-Ontology-Generation."}, {"title": "3.3. Large Language Models", "content": "Our experiments included a diverse set of seventeen LLMs, all transformer-based and utilising a decoder-only architecture. However, these models exhibit variations in training parameters, quantisation levels, openness, and fine-tuning methodologies. We categorised these models into two primary groups: open (13) and proprietary (4). This distinction was necessary because we lack comprehensive information about proprietary models in terms of their internal structures and any supplementary systems that may support them. As a result, a direct comparison between open and proprietary models would not be entirely fair.\nWe further divided the 13 open models into two groups: 3 full models and 10 quantised models, the latter being quantised to 8 bits. The quantised models were employed to assess the performance of more computationally efficient solutions. Since some quantised models have been fine-tuned on specific datasets, they may outperform the original models from which they were derived in certain tasks.\nDetailed descriptions of each model are provided below."}, {"title": "3.3.1. Open Models", "content": "Mistral-7B (shortened as mistral) is trained on Open-Web data, has 7 billion parameters, and utilises grouped-query attention and sliding window attention with a context window of 32,000 tokens.\nMixtral-8\u00d77B (shortened as mixtral) is a sparse mixture-of-experts network incorporating 46.7 billion parameters. Only 12.9 billion parameters are used per token, whereas the remaining parameters are part of the \u201cexpert\u201d layers within its sparse mixture-of-experts architecture. It has a context window of 32,000 tokens.\nLlama-2-70B (shortened as llama-2) has 70 billion parameters and a context window of 4,096 tokens. It is trained on a corpus that includes 2 trillion tokens of data from publicly available sources."}, {"title": "3.3.2. Quantised Models", "content": "Dolphin-2.1-Mistral-7B (shortened as dolphin-mistral) is a model with 7 billion parameters and a token context capacity of 4,096. It is based on Mistral-7B and fine-tuned with the Dolphin' dataset, which is an open-source implementation of Microsoft's Orca [32], with an addition of Airoboros dataset.\nDolphin-2.6-Mistral-7B-dpo-laser (shortened as dolphin-mistral-dpo\u00ba) is based on Mistal-7B with 7 billion parameters and fine-tuned on top of Dolphin DPO using Layer Selective Rank Reduction (LASER) [33]. It offers a context window of 4,096 tokens.\nDolphin2.1-OpenOrca-7B (shortened as dolphin-openorca10) is a model that blends Dolphin-2.1-Mistral-7B and Mistral-7B-OpenOrca models. These models were merged using the \"ties merge\" [34] technique, keeping the same number of training parameters and token context window size, respectively 7 billion and 4,096.\nOpenChat-3.5-1210 (shortened as openchat\u00b91) is a model fine-tuned on top of Mistral-7B-v0.1. It possesses similar properties and consists of 7 billion parameters with a context window size of 8,192 tokens.\nOpenChat-3.5-0106-Gemma (shortened as Openchat-gemma12) is a model trained on openchat-3.5-010613 data using Conditioned Reinforcement Learning Fine-Tuning (C-RLFT) framework [35, 36]. This model shares the same properties of the openchat-3.5-0106 model, which was fine-tuned14 on top of Mistral-7B. It consists of 7 billion parameters with a context window of 8,192 tokens."}, {"title": "3.3.3. Proprietary Models", "content": "We considered four proprietary models. Due to undisclosed details, we can only provide limited information. For example, their number of parameters is not officially disclosed.\nGPT-3.5-Turbo-Instruct (shortened as gpt-3.5) is trained using RLHF, following similar methods as InstructGPT, but with some differences in the data collection setup. It has a context window size of 4,096 tokens and was trained with data up to September 2021.\nGPT-4-Turbo (shortened as gpt-4) has been pre-trained to predict the next token in a document. It was trained using publicly available data and data licensed from third-party providers up to December 2023. The model was then fine-tuned using RLHF. It has a context window of 128,000 tokens.\nClaude 3 Haiku (shortened as haiku) is a model with a context size of 200,000 tokens, offering a maximum output of 4,096 tokens with training data up to August 2023.\nClaude 3 Sonnet (shortened as sonnet) offers a context size of 200,000 tokens. It has the capability to generate a maximum output of 4,096 tokens and has been trained on data up to August 2023. While haiku is optimised for speed and cost-effectiveness in applications requiring immediate responses, sonnet features a larger model architecture giving it advantage in handling complex tasks that require in-depth understanding of language [43]."}, {"title": "4. Experiments", "content": "We evaluated the 17 models described in Section 3.3 against IEEE-Rel-1K, the gold standard defined in Section 3.2, employing four different experiments that varied based on the prompting strategy utilised. Figure 1 illustrates the flowcharts of these experiments.\nThe experiments were categorised into two primary groups: standard prompting and chain-of-thought (CoT) prompting [44]. For each category, we conducted two distinct experiments. The first approach, labelled one-way, involves the model inferring the relationship between ta and te directly. The second approach, referred to as two-way, builds upon the first by having the model initially infer the relationship between ta and t\u00df, then infer the relationship between te and t\u0104, and finally combine the results using heuristic rules.\nIn all cases, the performance of the model was assessed using standard precision, recall and F1.\nTo summarise, the four experiments can be labelled as follows:\n1. standard prompting, one-way strategy (left branch of Fig. 1a);\n2. standard prompting, two-way strategy (both branches of Fig. 1a);\n3. chain-of-thought prompting, one-way strategy (left branch of Fig. 1b);\n4. chain-of-thought prompting, two-way strategy (both branches of Fig. 1b).\nIn the following, we will describe them in detail."}, {"title": "4.1. Standard prompting", "content": "In the experiments using standard prompting (Fig. 1a), we generated a prompt for each pair of research topics from the gold standard using a predefined template. The prompt template (detailed in Appendix A) describes the task and clearly defines the four categories, providing one generic example for each. Additionally, the template specifies that the answer should be provided in a numeric code format to facilitate parsing and ensure unambiguous responses. The resulting prompts were then submitted to the LLM to obtain an answer for each pair of topics. To ensure a fair comparison, all models received the same prompt."}, {"title": "4.2. Chain-of-thought prompting", "content": "CoT prompting experiments operated similarly to the previous solution but involved a two-phase interaction with the LLM, as illustrated in Fig. 1b. In the first phase, the model is asked to provide a definition of both topics, formulate a sentence that incorporates both of them, and finally discuss their potential semantic relationships. The resulting response is then fed back into the model in the second phase, with the sole task of determining the type of semantic relationship. The two templates used to produce the prompts for the chain-of-thought experiments are available in Appendix \u0412."}, {"title": "4.3. One-way vs. two-way strategies", "content": "The experiments for both standard prompting and CoT prompting are conducted in two different configurations. The one-way strategy simply asks for the relationship between ta and tB. The two-way strategy involves asking the LLM to identify the relationship between topics ta and t\u00df, as well as the relationship between topics te and t\u0104, when the order of the topics reversed. This straightforward approach helps to detect and potentially correct inconsistencies in the model's responses. This is feasible because the broader and narrower are inverse relationships, while same-as and other are symmetric. For example, if the model determines that t\u0104 is broader than t\u00df, but also that t\u00eb is same-as t\u0104, we can infer that at least one of these answers is incorrect.\nTo manage these potential inconsistencies, we designed a set of empirical rules (cyan boxes as referees in both Fig. 1a and 1b). These rules were designed to prioritise the development of the hierarchical structure.\nConsidering f(X) and s(X) as the relationship types respectively obtained via the first and second branches of the two-way strategy; and len(t\u2081) as the length of the topic's surface form, the rules are established as follows:\n1. broader :- f(broader) ^ s(narrower)\n2. narrower :- f(narrower) / s(broader)\n3. broader :- ((f(narrower) ^ s(narrower)) V (f(broader) ^ s(broader))) ^ len(t\u2081) \u2264 len(t)\n4. narrower :- ((f(narrower) ^ s(narrower)) V (f(broader) ^ s(broader))) ^ len(ta) > len(t)\n5. same-as :- f(same-as) \u2227 s(same-as)\n6. broader :- (f(broader) ^ s(other)) V (f(other) ^ s(narrower))\n7. narrower :- (f(narrower) ^ s(other)) V (f(other) ^ s(broader))\n8. :- f(X)"}, {"title": "4.4. Experimental setup", "content": "To interact with the various LLMs we employed two services, e.g., Amazon Bedrock21, and OpenAI API22; as well as the KoboldAI23 library for running LLMs locally.\nSpecifically, through Amazon Bedrock, we interacted with MistralAI family LLMs (e.g., Mistral-7B and Mixtral-8\u00d77B), Anthropic models (e.g., Cluade 3 Haiku, and Claude 3 Sonnet), and Llama-2-70B by Meta. This facility provides a pre-structured generic wrapper, which encapsulates the interaction with the different LLMs offered by Amazon Bedrock.\nWith the OpenAI API we interacted with GPT models, such as GPT3.5-Turbo-Instruct and GPT4-Turbo.\nFinally, we employed KoboldAI to interface with Dolphin-2.1-Mistral-7B, Mistral-7B-OpenOrca, Dolphin-2.6-Mistral-7B-dpo-laser, Dolphin-2.1-OpenOrca-7B, SOLAR-10.7B-Instruct-v1.0, OpenChat-3.5-1210, OpenChat-3.5-0106-Gemma, EURUS-7B-SFT. KoldbolAI is an open-source tool built on llama.cpp and provides model access through an API endpoint. We run it on a Google Colaboratory instance equipped with Nvidia's V100 and L4 GPUs.\nWithin the GitHub repository, we report the various parameters we set when interacting with the models across the three services."}, {"title": "5. Results and Discussion", "content": "This section presents the results of the complete set of experiments described in Section 4. First we will discuss the standard prompting experiments (see Fig. 1a for flowchart) and then we will report the results of the chain-of-thought prompting experiments (see Fig. 1b). Further, we will compare the results across the various strategies."}, {"title": "5.1. Standard prompting, one-way strategy", "content": "Table 1 reports the result of the experiments with standard prompting when applying one-way strategy. Among the full open models (first three rows), 11ama-2 and mixtral demonstrate good performance in terms of average F1 with 0.669 and 0.779, respectively. In particular, mixtral is highly precise with the broader (0.819) and same-as (1), whereas llama-2 obtains high values of"}, {"title": "5.2. Standard prompting, two-way strategy", "content": "Table 2 presents the performance of the models when using standard prompting with the two-way strategy.\nAmong the full open models, mixtral achieves an average F1-score of 0.847, significantly outperforming both 11ama-2 and mistral. It is able to identify the broader, narrower, and same-"}, {"title": "5.3. Chain-of-thought prompting, one-way strategy", "content": "Table 3 presents the results of the chain-of-thought, one-way strategy experiments. All models demonstrate significantly better performance compared to the analogous experiment using standard prompting. This confirms that CoT is a highly effective technique for enhancing model efficacy. A notable phenomenon observed here is that almost all models exhibit high precision on the same-as relationship. However, this often comes at the cost of recall, with the highest recall achieved by openchat, reaching 0.780.\nAmong full open models, mixtral again outperformed the other model with an average F1-score of 0.808.\nRegarding the quantised models, dolphin-mistral achieves the highest average F1-score of 0.869. It is closely followed by dolphin-mistral-dpo with a score of 0.826, solar at 0.825, and openchat at 0.823. This suggests that models fine-tuned on the OpenOrca corpus are particularly effective for this task. In this context, the use of the CoT strategy enables the quantised model dolphin-mistral to even surpass the much larger mixtral model."}, {"title": "5.4. Chain-of-thought prompting, two-way strategy", "content": "The values are generally consistent with those obtained from the chain-of-thought one-way strategy experiments (see Table 3). Specifically, mixtral, dolphin-mistral, and gtp-4 emerge as the top-performing models among the full open models, the quantised models, and the proprietary models, respectively. The average F1 score increases by a few percentage points in 15 out of 17 models, confirming that the two-way strategy remains effective when combined with CoT."}, {"title": "5.5. Comparative Analysis", "content": "Table 5 presents the average F1-scores for all 17 models across the four strategies, including the relative differences between one-way and two-way strategies for both standard and chain-of-thought prompting. The results indicate that shifting from a one-way to a two-way strategy improved performance in 14 models for standard prompting and in 15 models for chain-of-thought prompting.\nGenerally, we observe a gradual increase in the average F1-score from left to right in Table 5, demonstrating the beneficial effects of both CoT and the two-way strategy. Thirteen models showed overall improvement, while four models (11ama-2, dolphin-openorca, gpt-4, and"}, {"title": "6. Error Analysis", "content": "In this section, we examine common error types displayed by the top-performing LLMs across the three categories: a) mixtral (standard prompting, two-way), b) dolphin-mistral (CoT, two-way), and c) sonnet (standard prompting, one-way). Figure 2 reports the relevant confusion"}, {"title": "7. Conclusions", "content": "This paper conducted an in-depth analysis of the capability of LLMs to identify semantic relationships between research topics for the purpose of ontology generation. We evaluated seventeen LLMs, categorised into three groups: fully open models (3), open quantised models (10), and proprietary models (4). We evaluated their performance against a gold standard of 1,000 relationships"}, {"title": "Appendix A. Prompt for Standard Prompting", "content": "For consistency we applied the same prompt across all the models. We engineered this prompt through various refinements, to ensure optimal comprehension of the task and accurate responses. Below is the template of our prompt, customised for each topic pair by substituting [TOPIC-A] for the first topic and [TOPIC-B] for the second.\nClassify the relationship between '[TOPIC-A]' and '[TOPIC-B]' by applying the following relationship definitions:\n1. '[TOPIC-A]' is-broader-than '[TOPIC-B]' if '[TOPIC-A]' is a \u2190 \u2190 \u2190 \u2190 super-category of '[TOPIC-B]', that is '[TOPIC-B]' is a type, a branch, or a specialised aspect of '[TOPIC-A]' or that '[TOPIC-B]' is a tool or a methodology mostly used in the context of '[TOPIC-A]' (e.g., car is-broader-than wheel).\n2. '[TOPIC-A]' is-narrower-than '[TOPIC-B]' if '[TOPIC-A]' is a \u2190 \u2190 \u2190 sub-category of '[TOPIC-B]', that is '[TOPIC-A]' is a type, a branch, or a specialised aspect of '[TOPIC-B]' or that '[TOPIC-A]' is a tool or a methodology mostly used in the context of '[TOPIC-B]' (e.g., wheel is-narrower-than car).\n3. '[TOPIC-A]' is-same-as-than '[TOPIC-B]' if '[TOPIC-A]' and '[TOPIC-B]' are synonymous terms denoting an identical concept (e.g., beautiful is-same-as-than attractive), including when one is the plural form of the other (e.g., cat is-same-as-than cats).\n4. '[TOPIC-A]' is-other-than '[TOPIC-B]' if '[TOPIC-A]' and '[TOPIC-B]' \u2190 \u2190 either have no direct relationship or share a different kind of relationship that does not fit into the other defined relationships.\nGiven the previous definitions, determine which one of the following statements is correct:\n1. '[TOPIC-A]' is-broader-than '[TOPIC-B]'\n2. '[TOPIC-B]' is-narrower-than '[TOPIC-A]'\n3. '[TOPIC-A]' is-narrower-than '[TOPIC-B]'\n4. '[TOPIC-B]' is-broader-than '[TOPIC-A]'\n5. '[TOPIC-A]' is-same-as-than '[TOPIC-B]'\n6. '[TOPIC-A]' is-other-than '[TOPIC-B]'\nAnswer by only stating the correct statement and its number."}, {"title": "Appendix B. Prompt for Chain-of-Thought Prompting", "content": "In the CoT prompting we have a two-phase interaction with the model. In the first interaction the model is sked to provide a definition of both topics, formulate a sentence that incorporates both topics, and finally discuss their potential semantic relationships. The resulting response is then fed back into the model in the second phase, with the sole task of determining the type of semantic relationship. Below are our prompt templates, customised for each topic pair by substituting [TOPIC-A] for the first topic and [TOPIC-B] for the second.\nFirst interaction\nClassify the relationship between '[TOPIC-A]' and '[TOPIC-B]' by applying the following relationship definitions:\n1. '[TOPIC-A]' is-broader-than '[TOPIC-B]' if '[TOPIC-A]' is a \u2190 \u2190 \u2190 \u2190 super-category of '[TOPIC-B]', that is '[TOPIC-B]' is a type, a branch, or a specialised aspect of '[TOPIC-A]' or that '[TOPIC-B]' is a tool or a methodology mostly used in the context of '[TOPIC-A]' (e.g., car is-broader-than wheel).\n2. '[TOPIC-A]' is-narrower-than '[TOPIC-B]' if '[TOPIC-A]' is a \u2190 \u2190 \u2190 sub-category of '[TOPIC-B]', that is '[TOPIC-A]' is a type, a branch, or a specialised aspect of '[TOPIC-B]' or that '[TOPIC-A]' is a tool or a methodology mostly used in the context of '[TOPIC-B]' (e.g., wheel is-narrower-than car).\n3. '[TOPIC-A]' is-same-as-than '[TOPIC-B]' if '[TOPIC-A]' and '[TOPIC-B]' are synonymous terms denoting a very similar concept (e.g., 'beautiful' is-same-as-than 'attractive'), including when one is the plural form of the other (e.g., cat is-same-as-than cats).\n4. '[TOPIC-A]' is-other-than '[TOPIC-B]' if '[TOPIC-A]' and '[TOPIC-B]' \u2190 \u2190 either have no direct relationship or share a different kind of relationship that does not fit into the other defined relationships.\nThink step by step by following these sequential instructions:\n1) Provide a precise definition for '[TOPIC-A]'.\n2) Provide a precise definition for '[TOPIC-B]'.\n3) Formulate a sentence that includes both '[TOPIC-A]' and '[TOPIC-B]'.\n4) Discuss '[TOPIC-A]' and '[TOPIC-B]' usage and relationship \u2190 (is-narrower-than, is-broader-than, is-same-as-than, or is-other-than).\nSecond interaction\n[PREVIOUS-RESPONSE]\nGiven the previous discussion, determine which one of the following statements is correct: \u2190\n1. '[TOPIC-A]' is-broader-than '[TOPIC-B]'\n2. '[TOPIC-B]' is-narrower-than '[TOPIC-A]'\n3. '[TOPIC-A]' is-narrower-than '[TOPIC-B]'\n4. '[TOPIC-B]' is-broader-than '[TOPIC-A]'\n5. '[TOPIC-A]' is-same-as-than '[TOPIC-B]'\n6. '[TOPIC-A]' is-other-than '[TOPIC-B]'\nAnswer by only stating the number of the correct statement."}]}