{"title": "Benchmarking Data Heterogeneity Evaluation Approaches for Personalized Federated Learning", "authors": ["Zhilong Li", "Xiaohu Wu", "Xiaoli Tang", "Tiantian He", "Yew-Soon Ong", "Mengmeng Chen", "Qiqi Liu", "Qicheng Lao", "Xiaoxiao Li", "Han Yu"], "abstract": "There is growing research interest in measuring the statistical heterogeneity of clients' local datasets. Such measurements are used to estimate the suitability for collaborative training of personalized federated learning (PFL) models. Currently, these research endeavors are taking place in silos and there is a lack of a unified benchmark to provide a fair and convenient comparison among various approaches in common settings. We aim to bridge this important gap in this paper. The proposed benchmarking framework currently includes six representative approaches. Extensive experiments have been conducted to compare these approaches under five standard non-IID FL settings, providing much needed insights into which approaches are advantageous under which settings. The proposed framework offers useful guidance on the suitability of various data divergence measures in FL systems. It is beneficial for keeping related research activities on the right track in terms of: (i) designing PFL schemes, (ii) selecting appropriate data heterogeneity evaluation approaches for specific FL application scenarios, and (iii) addressing fairness issues in collaborative model training. The code is available at https://github.com/Xiaoni-61/DH-Benchmark.", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL) is a promising privacy-preserving collaborative machine learning (ML) paradigm [23, 58]. Under FL, multiple FL clients train a shared model with their own datasets and upload their local model updates to a FL server for aggregation and redistribution [41]. One key challenge of FL is that the data distributions across different clients can be heterogeneous (i.e., non-independently and identically distributed (non-IID)) [29, 31, 65]. This has inspired the field of personalized federated learning (PFL) [49] to build more powerful ML models, better realizing the final goal of FL.\n\nThe non-IIDness of clients' local data entails evaluating data complementarity or similarity between clients and building a personalized ML model for each client. A FL system itself is a collaborative network of clients where a client i may be complemented by other clients j with different weights, which measure the data heterogeneity. The basic way that FL works is to aggregate the clients' local model updates according to their weights, e.g., the vanilla Federated Averaging framework [42]. In the context of PFL, we believe that it is still of fundamental importance to obtain a personalized model for each client by aggregating the clients' local model updates according to the weights. Measuring statistical heterogeneity of data is one way to understand the data complementarity and potential collaboration advantages among clients [14]. Recently, there has been growing interest in measuring"}, {"title": "2 Preliminary", "content": "2.1 FL Notation\n\nConsider a FL setting with N clients. Each client $i \\in \\{1,2,...,N\\}$ has a dataset $D_i = \\{(x_k^{(i)}, y_k^{(i)})\\}_{k=1}^{m_i}$ consisting of $m_i$ samples drawn from an underlying data distribution $D_i$, where $x^{(i)}\\in X$ denotes the feature while $y^{(i)}\\in Y$ denotes the label. The total number of samples across all clients is $m = \\sum_{i=1}^N m_i$. Let $\\beta_i = m_i/m$ denote the proportion of data samples held by client i, and $\\beta = [\\beta_1,..., \\beta_N]$ represent the quantity distribution of data samples across clients. Given a machine learning model (hypothesis) h and a risk function l, the local expected risk of client i is defined as $L_i(h) = E_{(x,y)\\in D_i}l(h(x), y)$ and its local empirical risk is given by $L_i(h) = \\frac{1}{m_i} \\sum_{k=1}^{m_i} l(h(x_k^{(i)}), y_k^{(i)})$. The goal of each client i is to find a model h within the hypothesis space H that minimizes its local expected risk, denoted as $h^* = \\arg \\min_{h\\in H} L_i(h)$, based on its finite local dataset $D_i$.\n\nWe denote $w_i^t$ as the model parameters of client i at the beginning of the training round t. Let $\\alpha_i^t = (\\alpha_{i,1}^t,..., \\alpha_{i,N}^t)$ denote the weighted collaboration vector of client i in the training round t, where $\\sum_{j=1}^N \\alpha_{i,j}^t = 1$ and $\\alpha_{i,j}^t \\ge 0$. Except the scheme in [34], the central server performs a weighted aggregation of the model parameters from all clients for each client i. As a result, the model parameters of i become $w_i^{t+1} = \\sum_{j=1}^N \\alpha_{i,j}^t w_j^t$. The weight $\\alpha_{i,j}^t$ can be viewed as a quantification of the collaboration advantage that client j brings to client i in the training round t. Let $A^t$ be a N \u00d7 N matrix whose i-th row is $\\alpha_i^t$. $A^t$ defines a directed benefit graph among the N clients. In the schemes of [55, 59], the value of $\\alpha_{i,j}^t$ changes in each round of training. In the schemes of [3, 11, 12], the value of $\\alpha_{i,j}^t$ is independent of the round t and is precomputed before the FL training process starts; thus, $\\alpha_{i,j}^t$, $\\alpha_i^t$ and $A^t$ are also simply denoted as $\\alpha_{i,j}$, $\\alpha_i$ and A. The way that the scheme in [34] works will be introduced in Section 3.1.3."}, {"title": "2.2 Typical Non-IID Data Settings", "content": "From a probability distribution perspective, the local data distribution $P(x_i, y_i)$ of client i can be represented by a conditional probability $P(x_i|y_i)P(y_i)$ or $P(y_i|x_i)P(x_i)$. There are three typical non-IID data settings in the FL context [23, 29, 49]: (i) label distribution skew, where $P(y_i)$ differs among clients; (ii) feature distribution skew, where $P(x_i)$ differs among clients; and (iii) quantity skew, where $P(x_i, y_i)$ is the same for all clients i, but the amount of data varies across clients."}, {"title": "3 Data Heterogeneity Evaluation Benchmark", "content": "Existing methods for evaluating data heterogeneity can be roughly grouped into two main categories: those based on statistical divergence of data distributions and those based on model performance. In the following sections, we will detail these two categories of methods and their applications in PFL."}, {"title": "3.1 Distribution Statistical Divergence-based Approaches", "content": "Methods in this category are based on the divergence, a kind of statistical distance measure. It is a function that takes two probability distributions as input and returns a numerical value quantifying the difference between them [14, 46]. In FL, the local model updates are uploaded to the central server, which performs a weighted aggregation of these updates to produce either a global model or a personalized models for each client. The divergence of data distributions among clients can guide more informed decisions on the choice of the aggregation weights [3, 12, 36] or clustering clients with similar data distributions [3, 34, 35].\n\nSome works in this category [3, 12] have some features in common. In these methods, the weighted empirical risk $L_{\\alpha_i}(h)$ for client i is defined as $L_{\\alpha_i}(h) = \\sum_{j=1}^N \\alpha_{i,j}L_j(h)$. The model of i can be learned by minimizing $L_{\\alpha_i}(h)$. Let $h_{\\alpha_i} = \\arg \\min_{h\\in H} L_{\\alpha_i}(h)$ be the hypothesis that minimizes the weighted empirical risk $L_{\\alpha_i}(h)$. The following bound is used by both [12] and [3]:\n\n$L_i(h_{\\alpha_i}) - L_i(h^*) \\le Q_{\\gamma,1} \\sum_{j=1}^N \\alpha_{i,j}/m_j + Q_{\\gamma,2} \\sum_{j=1}^N \\alpha_{i,j}D(D_i, D_j)$\n\nwhere $Q_{\\gamma,1}$ and $Q_{\\gamma,2}$ are parameters related to the type of divergence used in [3, 12]. Eq. (1) shows that for client i the gap between $h_{\\alpha_i}$ and $h^*$ depends on the weight $\\alpha_i$, the number of samples $m_i$ and the divergence $D(D_i, D_j)$ between two data distributions. The optimal model for i can be obtained by choosing appropriate decision variables to minimize the right-hand side of Eq. (1). These methods are based on two statistical divergences, Jensen-Shannon Divergence and C-Divergence."}, {"title": "3.1.1 Jensen\u2013Shannon Divergence", "content": "The Jensen-Shannon (JS) divergence between two distributions P and Q is denoted as JSD(P, Q) [7]. It is based on the Kullback-Leibler (KL) divergence, denoted as KLD(P, Q). For two discrete probability distributions on the same space Z, we have $KLD(P,Q) = \\sum_{z\\in Z} P(z) \\log \\frac{P(z)}{Q(z)}$ and"}, {"title": "3.1.2 C-Divergence", "content": "C-divergence is defined as [4, 43, 54]: $D_c(D_i, D_j) = \\max_{h\\in H} |L_i(h) - L_j(h)|$. In the hypothesis space H, $D_c(D_i, D_j)$ indicates the maximum difference between the local expected risks of two data distributions $D_i$ and $D_j$. Suppose l is the 0-1 loss function. The definition of C-divergence guarantees that its divergence value will be as close to one as possible if two distributions are distinctly different. Thus, it can be used to cluster clients into groups with similar data distributions.\n\nApplication. In the FL context, C-divergence has been used to partition all clients into K coalitions $\\{C_1,..., C_K\\}$, each with similar data distributions [3, 35]. The function l(h(x), y) is also denoted as f(x, y), mapping from $X \\times Y$ to \\{0, 1\\}. Bao et al. [3] transform the C-divergence into the following form: $D_{i,j} = \\max_{f\\in F} |Pr_{(x,y)\\in D_i} [f(x,y) = 1] + Pr_{(x,y)\\in D_j} [f(x, y) = 0] - 1|$. A client classifier f \u2208 F can be trained to predict the distribution similarity. If the estimated distance $D_{i,j}$ is approximately 100%, then the two distributions being compared are significantly different. Unlike in [12], $\\alpha_i$ is a dependent variable here, and the weight $\\alpha_{i,j}$ of j to i is predefined as the proportion of the data quantity of j to the coalition's data quantity. Specifically, for any $i \\in C_k$, $\\alpha_{i,j} = \\beta_j/ \\sum_{l\\in C_k} \\beta_l$ if $j \\in C_k$ and $\\alpha_{i,j} = 0$ otherwise. Here, $D_c(D_i, D_j)$ is used to instantiate the divergence term $D(D_i, D_j)$ on the right-hand side of Eq. (1). Bao et al. [3] provides an efficient optimizer to optimize the coalition structure. Once $\\{C_1,\\cdots,C_K\\}$ is determined, $\\{\\alpha_i\\}_{i=1}^N$ will be determined accordingly. The corresponding PFL scheme is called FedCollab."}, {"title": "3.1.3 Distribution Sketch-based Euclidean Distance", "content": "Let $z_i = (z_{i,1},..., z_{i,d})$, where $i \\in \\{1,2\\}$. The Euclidean distance of two vectors is defined as $\\sqrt{\\sum_{l=1}^d (z_{1,l} - z_{2,l})^2}$. On the other hand, efficient computation of statistical divergences of high-dimensional data is challenging. To address this, Liu et al. [34] proposes a one-pass distribution sketch algorithm to represent the client data distributions. Then, the divergence is measured by the Euclidean distance of such distribution sketches. Central to their algorithm is Locality Sensitive Hashing (LSH), which uses hash functions to map data samples from high-dimensional space to low-dimensional buckets, ensuring that samples close to each other have a high probability of having the same hash value. Building on this, the Repeated Array of Count Estimators (RACE) is adopted [9, 10, 33, 37]. Let z = (x, y) denote a data sample. RACE uses R LSH functions to construct a matrix $A \\in R^{R\\times B}$. Each LSH function $g_i$ maps a data vector into B hash bins. For a given dataset $D_i$, any data sample $z \\in D_i$ is mapped to R different hash values, denoted as $\\{g_i(z) | i \\in [1, R]\\}$. The corresponding positions in matrix A are then incremented by 1. In this way, RACE can characterize the data distribution of $D_i$ for each client i using the corresponding A, referred to as the client data sketch $CS_i$.\n\nApplication. Liu et al. [34] show that an average of all client sketches can effectively approximate the sketch $GS$ of the global data distribution across clients. The central server calculates the Euclidean distance between $GS$ and $CS_i$ before the FL training process starts. The inverses of these distances serve as the probabilities for selecting K clients, denoted as $C^t$ for local training in round t. Specifically, clients with $CS_i$ closer to $GS$ are prioritized for collaboration. The local updates from $C^t$ are aggregated by the central server, resulting in a global model $w^{t+1} = \\sum_{j\\in C^t} w_j^t$. Finally, each client builds a personalized model based on the global model and its local data. We refer to the PFL approach in [34] as RACE."}, {"title": "3.2 Model-based Approaches", "content": "3.2.1 Shapley Value\n\nShapley Value (SV) is a classic approach to quantifying individual contributions within a group under the Cooperative Game Theory [48]. Let $N = \\{1,...,N\\}$. In machine learning, the data Shapley value of client i is defined as $\\Phi_i (N, V) = \\sum_{S\\subseteq N-\\{i\\}} \\frac{|S|!(N-|S|-1)!}{N!} (V(S\\cup\\{i\\})-V(S))$, where V(\u00b7) denotes the utility evaluation function [16, 21, 32]. In FL, the utility evaluation function V (S) is based on the model performance achieved by the participation of S. Let $M_S$ denote the global FL model trained with S on a separate test dataset $D_S$. Then, $V(S) = V(M_S) = V (A(M_S^{(0)},D_S))$, where A is the learning algorithm and $M_S^{(0)}$ denotes the initial model [32]."}, {"title": "3.2.2 Cosine Similarity", "content": "Let $z_i = (z_{i,1},..., z_{i,d})$, where $i \\in \\{1,2\\}$. The cosine similarity of two non-zero vectors is defined as $cos(z_1, z_2) = \\frac{z_1 \\cdot z_2}{||z_1|| ||z_2||}$, where $z_1 \\cdot z_2$ is the inner product and $||\\cdot||$ denotes the Euclidean norm. Intuitively, the closer the two vectors are, the smaller the angle between them, and thus the cosine value will be closer to one. Let us consider the personalized models $h_i$ and $h_j$ of clients i and j. Their cosine similarity is $cos(h_i, h_j)$.\n\nApplication. Intuitively, when the data distribution of two clients i and j are more similar, their models are more similar, and their collaboration strength (i.e., $\\alpha_{i,j}$) should be larger [30, 38]. The similarity between model parameters is used to guide the collaboration strength. In each training round t, Ye et al. [59] proposes minimizing the following function in a privacy-preserving manner while adhering to the standard FL training process: $\\sum_{i=1}^N \\beta_i (L_i (w_i^t) - \\lambda \\sum_{j=1}^N \\alpha_{i,j}^t cos(w_i^t, w_j^t))$, where the decision variables are $\\{w_i^t\\}_{i=1}^N$ and $\\alpha^t$, and $\\lambda$ is a hyperparameter to balance the individual utilities and the collaboration necessity."}, {"title": "3.2.3 Hypernetworks", "content": "Each client i has a risk/loss function $l_i: R^N \\rightarrow R_+$. Given a learned hypothesis $h \\in H$, let the loss vector l(h) = $[l_1,..., l_N]$ represent the utility loss of the N clients under the hypothesis h. The hypothesis h is considered a Pareto solution if there is no other hypothesis h' that dominates h, i.e., $\\nexists h' \\in H, s.t. \\forall i : l_i(h') \\le l_i(h)$ and $\\exists j : l_j(h') < l_j(h)$. Let $r = (r_1,...,r_N) \\in R^N$ denote a preference vector where $\\sum_{k=1}^N r_k = 1$ and $r_k \\ge 0, \\forall k \\in \\{1, ..., N\\}$. The hypernetwork HN takes r as input and outputs a Pareto solution h, i.e., $h \\leftarrow HN(\\phi, r)$, where $\\phi$ denotes the parameters of the hypernetwork [44].\n\nApplication. For each client i, linear scalarization can be used. Cui et al. [11] determine an optimal preference vector $r_i$ to generate the hypothesis h that minimizes the loss with the data $D_i$. This is expressed as $h^* = HN(\\phi,r_i)$, where $r_i = \\arg \\min_r \\hat{L}_i(HN(\\phi, r))$. For each client i, the value of $r_i$ is assigned to $\\alpha_i$. In [11], collaboration equilibrium (CE) is sought with additional considerations. We refer to the PFL scheme here as CE due to its origin in [11]."}, {"title": "4 Experimental Studies", "content": "To investigate the effectiveness of existing FL schemes in studying the statistical heterogeneity of data, we conducted extensive experiments on eight public datasets. These included five image datasets (i.e., MNIST [27], CIFAR-10 [26], FMNIST [57], SVHN [45], FEMNIST [6]), and three tabular datasets"}, {"title": "4.1 Analysis of the Accuracy obtained by Collaboration", "content": "The accuracy of the six methods under the five standard non-IID settings and the IID data setting is shown in Table 1. Non-IID data can degrade the FL performance to some extent. The results under the IID setting is used as a baseline to evaluate the performance of the six methods under non-IID settings.\n\nPerformance Comparison. Figure 1 summarizes the results in Table 1, with the blue ovals at the bottom illustrating which schemes are advantageous under each setting. There is no universally best scheme; the choice depends on the specific setting. In the label distribution skew setting, pFedGraph significantly outperforms other schemes on image data, while CE performs best on average for tabular data. For feature distribution skew, RACE usually achieves the highest accuracy. In the quantity skew setting, pFedGraph continues to excel on image data, while pFedJS achieves the highest accuracy on tabular data. Our observation is as follows: as illustrated by the bottom blue ovals of Figure 1, each non-IID setting has a corresponding technique that maximizes the collaboration advantages and personalized model accuracy."}, {"title": "4.2 Efficiency", "content": "Schemes except for [34] compute a matrix a only once before the FL training process starts [3, 11, 12] or a matrix $a^t$ in each round of FL training [55, 59] to quantify the collaboration advantages of"}, {"title": "4.3 Other Experiments", "content": "Scalability. From Figure 2, it can be observed that the accuracy of schemes may vary, and in many cases, their accuracy decreases as the number of clients increases. Among the six schemes, no scheme dominates the other schemes across all cases. The sensitivity of these schemes' performance to the number of clients implies that scalability should be an important aspect to consider when evaluating a FL scheme in the related research.\n\nMixed Types of Data Skew. In reality, different types of skew may coexist. Like [29], we consider two settings: mixed label and feature skew, and mixed feature and quantity skew; please refer to Appendix B for details. The experimental results are presented in Table 3. When facing mixed types of skew, all these schemes encounter significant challenges. For example, in the mixed feature and quantity skew setting, the performance of all schemes on CIFAR-10 is poor, with accuracy not exceeding 32%, much lower than the case when there is only one type of skew, as illustrated in Table 1. It is crucial to find effective solutions for the mixed types of skew case since this case is also common in real-world scenarios.\n\nThe effects of batch size, communication rounds, and local epochs on these PFL schemes have also been evaluated through experiments. Please refer to Appendix C for detailed results and analysis."}, {"title": "5 Concluding Remarks", "content": "In this paper, we summarize the six major techniques (JS divergence, C-divergence, distribution sketch, Shapley Value, Hypernetworks, cosine similarity) to quantify data heterogeneity in FL settings into a first-of-its-kind unified framework to understand their effects in-depth. Extensive experiments over eight popular datasets have been conducted to compare these schemes under five standard non-IID FL settings, providing much-needed insight into which schemes are advantageous under which settings. The unified framework and the experimental results identify the scenarios, under which the current schemes perform relatively poorly, and future research problems for PFL. It is useful for identifying the right techniques for quantifying the collaboration advantages among clients, guiding the related research on collaboration, fairness, and competition in FL settings. The findings suggest that lightweight FL schemes based on techniques such as the distribution sketch are worth studying in the future."}, {"title": "A Experimental Setup Details", "content": "The statistics of the datasets is summarized in Table 4. For image datasets, we employed a CNN with two 5x5 convolutional layers followed by 2x2 max pooling, and two fully connected layers with ReLU activation [18]. For tabular datasets, we used an MLP with three hidden layers for training. We used the SGD optimizer with a learning rate of 0.1 for rcv1 and 0.01 for the other datasets, along with a momentum of 0.9."}, {"title": "B Mixed Types of Skew", "content": "In the mixed label and feature skew setting, the distribution-based label imbalanced partitioning strategy and the noise-based feature imbalance strategy are applied sequentially. Specifically, the whole dataset is first divided into each client using the distribution-based label imbalanced partitioning strategy. Then, noise is added to the data of each client according to the noise-based feature imbalance strategy. As a result, both label distribution skew and feature distribution skew exist among the local data of different clients.\n\nIn the mixed feature and quantity skew setting, the quantity imbalanced partitioning strategy and the noise-based feature imbalance strategy are applied sequentially. Specifically, the whole dataset is allocated into each client by the quantity imbalanced partitioning strategy. Then, noise is added to the data of each client through the noise-based feature imbalance strategy. Consequently, both feature distribution skew and quantity skew exist among the local data of different clients."}, {"title": "C FL Training Process", "content": "C.1 Communication Efficiency\n\nFigures 3-7 show the training curves of MNIST, FMNIST, SVHN, CIFAR-10, and rcv1 under different non-IID settings. Under the non-IID setting $x \\sim Gau(0.1)$, $p \\sim Dir(0.5)$, and $p_k \\sim Dir(0.5)$,"}, {"title": "C.2 Robustness to Local Updates", "content": "Following [29], we set the local epochs to 10, 20, 40, and 80 to test the robustness of the comparison schemes to local updates. As shown in Figures 8-10, the number of local training epochs is crucial because it directly impacts the final accuracy. Contrary to intuition, a larger number of local epochs does not always lead to higher accuracy. When #C = 2, 3, the accuracy of pFedJS and pFedSV on the test dataset initially increases or decreases in one direction, but as the number of local epochs continues to grow, the accuracy changes in the opposite direction. In the label distribution skew setting, FedCollab may experience overfitting as the number of epochs increases. The accuracy of CE occasionally decreases slightly in the quantity skew setting. RACE and pFedGraph perform very stably across various datasets and non-IID settings. Overall, these schemes achieve good performance with 10 local epochs. Increasing the number of local epochs further provides diminishing returns in terms of cost-effectiveness."}, {"title": "C.3 Batch Size", "content": "Figures 11-12 present the training curves for the six schemes with batch sizes ranging from 16 to 256. As expected, both excessively large and small batch sizes result in poor accuracy. Notably, in the pFedJS and RACE schemes of Figure 11, the accuracy is particularly low when the batch size is 256, which may be due to getting stuck in local optima or having weak generalization ability. Additionally, when the batch size is 16, the accuracy drops sharply during training, likely due to the collaborative clients remaining largely unchanged and leading to overfitting while training personalized models. Therefore, selecting an appropriate batch size is crucial when training personalized models."}]}