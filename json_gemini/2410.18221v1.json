{"title": "Data Augmentation for Automated Adaptive Rodent Training", "authors": ["Dibyendu Das", "Alfredo Fontanini", "Joshua F. Kogan", "Haibin Ling", "C.R. Ramakrishnan", "I.V. Ramakrishnan"], "abstract": "Fully optimized automation of behavioral training protocols for lab animals like rodents has long been a coveted goal for researchers. It is an otherwise labor-intensive and time-consuming process that demands close interaction between the animal and the researcher. In this work, we used a data-driven approach to optimize the way rodents are trained in labs. In pursuit of our goal, we looked at data augmentation, a technique that scales well in data-poor environments. Using data augmentation, we built several artificial rodent models, which in turn would be used to build an efficient and automatic trainer. Then we developed a novel similarity metric based on the action probability distribution to measure the behavioral resemblance of our models to that of real rodents.", "sections": [{"title": "I. INTRODUCTION", "content": "Background: In lab experiments, operant conditioning tasks with food and(or) water rewards are commonly used to train and test rodents in a wide variety of sensory, motor and cognitive tasks. Water rewards can be dispensed with temporal and quantitative precision and consumed rapidly with minimal body movement (particularly so in head-fixed rodents), which makes them ideal for automated behav- ioral training, testing, and electro-physiological and optical recording.\nOne of the main barriers for research on complex be- haviors in rodents, lies in training the animals - a process typically done under close supervision of researchers who frequently modify protocols and procedures on an animal-by- animal basis to improve learning rates and performance. This approach is labor-intensive and time-consuming, and makes the interaction between animal and researcher an integral part of the training process, possibly confounding comparisons of experimental outcomes across animals and labs [1].\nIdeally, behavioral training systems should be fully au- tomatic, ready to scale up, blind in design, and flexible in changing paradigms. There has been a long history of designing automatic behavior-training systems consisting of monitoring and feedback controlling. In freely moving mice, automatic measurement has been implemented in charac- terizing visual performance [2], [3], [4], evaluation of pain sensitivity [5], [6], freezing behavior during fear conditioning [7], [8], home-cage phenotyping [9], [10], anxiety [11], and social behavior [12], [13].\nMotivation: Automatic training systems with multiple cog- nitive behaviors requiring memory, attention and decision making have been developed over the last few years in free- moving [14], [15] and head-fixed [16] rodents. However, majority of these works [16], [15], [17] have been confined to automating the existing training protocols using software and/or hardware tools. Tailoring the training protocols to individual animals to shorten training time has not been con- sidered before. This paper takes a step towards generating tailored training protocols using data-driven techniques.\nIn most existing training procedures, the rodents are trained using a (pseudo-)randomly generated sequence of training inputs (gustatory, olfactory etc.) over a period of several days or even weeks. Our long-term goal is to make the training protocol adaptive, i.e. to algorithmically change the sequence of training inputs based on each rodent's current performance, so that they can successfully complete the training as early as possible. This adaptive strategy will be derived based on historical training data of similar ani- mals under similar conditions, by leveraging computational techniques such as Reinforcement Learning.\nApproach: At a high level, we cast the problem of deter- mining an individually-tailored sequence of training inputs as an instance of a sequential decision making problem, as follows. The training inputs are regarded as the sequence of actions taken by a \"training agent\", based on observing the \"environment\": in this case, a trainee rodent's past behavior. The trainer agent gets a \"reward\u201d that depends on the length of input sequence needed to successfully train a rodent. A rodent's behavior is conditioned on a number of variables, many of which (e.g. descriptions of perceptive and cognitive states) cannot be measured; hence a rodent's behavior itself is modeled as a stochastic process. Thus the rodent training problem is posed as one of determining the sequence of actions of the training agent that maximizes the expected reward. Reinforcement Learning (RL) is a widely- used technique for determining optimal action sequences of agents when the environment's state space is not known a priori, but can only be sampled.\nRL has been used successfully in a variety of prob- lems ranging from robotics [18] to operations research and games [19]. However, we face a significant hurdle when considering RL for the rodent training problem: data-driven techniques like RL require copious data either historical data in large data sets, or the ability to generate samples on demand. Historical rodent training data is limited; data on even hundreds of individual rodents is woefully inadequate for RL-based techniques. Insufficient data results in unstable or sub-optimal solutions. The usual method for deploying RL and other machine learning techniques for data-poor prob-"}, {"title": "lems is to perform \"data augmentation\": adding artificially creating data points based on a domain-specific model. For instance, in computer vision tasks (e.g. object recognition from images), transformations such as rotation, translation and cropping are used to generate more data from a given data set, improving the stability and performance of the learned task [20].\nThis paper describes a data augmentation technique to en- large rodent behavioral data. Such an augmented data set en- ables use of RL for the rodent training problem. In particular, we describe an artificial rodent model whose behavior, when subjected to similar training protocols, resembles that of a real rodent. We bring rigor to the notion of \"resemblance\" by quantifying the degree of behavioral similarity of our artificial rodent with respect to a biological one, using a novel similarity metric based on action probability distribution. A well-defined metric for similarity will enable us to revise or refine the artificial rodent model and help ensure that the solutions to the rodent training problem derived using the augmented data will perform well when applied to the training of real rodents.", "content": "II. TRAINING SETUP\n\nThe rodents are kept water restricted in a dark environment and habituated to head-restraint. Training is divided across multiple sessions with each session performed on a different day. Each training session is composed of a number of trials. In initial training sessions, rodents are presented gustatory stimuli (100mM Sucrose, 100mM NaCl). Once animals are trained on this discrimination, mixtures are introduced and rodents must learn to discriminate 55%/45% from 45%/55% (Sucrose%/NaCl%). This training frequently takes longer than 30 sessions(days) to complete. At the start of each trial a stimulus is presented to the rodents via a central spout kept in front of their mouth. The spout retracts after giving them the chance to lick the stimulus from its tip. After 3-second delay two other lateral spouts advance from both left and right and are kept within the reach of the rodents. Water reward is presented at the left or right spout based on the category of the presented stimulus. Rodents licking the correct spout collect the reward and the process continues. Otherwise, for an incorrect decision, they are penalized by a longer wait interval without water, before the next trial resumes.\nThe session ends when the rodents stop interacting with the spout(s). Typically one session lasts for about 150 trials on average. The entire training procedure ends successfully when a rodent performs with an accuracy of 70% or more for three consecutive sessions.\nThe sequence of taste stimuli generated to train a rodent is (pseudo-)random with a constraint that no more than 3 consecutive inputs are identical.\nTo make the training process more efficient, we aim to optimize this long sequence of sessions, by reducing the number of sessions(days)."}, {"title": "III. MODELLING THE BEHAVIOR OF RODENTS USING\nREINFORCEMENT LEARNING", "content": "In our modelling, we used the following symbols for the gustatory stimuli:\nsweet = 100mM Sucrose\nsalt = 100mM NaCl\nsweet_55% = 55%/45% (Sucrose%/NaCl%)\nsalt_55% = 45%/55% (Sucrose%/NaCl%)\nThe training data for a rodent is the temporal sequence\nof n consecutive training sessions S = (S1, S2,..., Sn),\nwhere the j-th session, Sj is a temporal sequence of\nmj consecutive trials, i.e. Sj = (Tj,1, Tj,2,..., Tj,m;)\nand the k-th trial Tj,k is a triple (\u03c3j,k, Pj,k, Aj,k) with\nstimulus \u03c3j,k \u2208 {sweet, sweet_55%, salt_55%, salt},\nresponse Pj,k \u2208 {left, right, none}, and outcome\nAj,k \u2208 {correct, incorrect}. We use superscripts (such\nSa), Ta, etc.) to denote data for individual rodent \u201ca\u201d\nin order to distinguish between data for different rodents.\nThe structure of the training data is shown in Table I.\n\nA. Artificial Rodent Model\nAs the entire training process is structured around the\nepisodic training sequences, we model the artificial rodent\nas an RL agent, more specifically, based on classical Q- Learning[21]. Any RL agent can be modelled using a Markov\nDecision Process (MDP) with three of its components: states\nS+, actions A, and rewards R defined as follows.\n1) States S+: For our model rodent, we've de- fined the state of the environment (observation) as the tuple of last k input stimuli. The set of states,\nS+ = {(\u03c3\u03c4\u2212k+1,0t\u2212k+2,\u2026\u2026,5t) | t > k \u2212 1} where\neach element in the tuple is a stimulus, i.e., one of\n{sweet, sweet_55%, salt_55%, salt}. We've used k = 3 in\nour experiments. As the state space is discrete (with 4k pos- sible states), we can store the action-value function q\u03c0(s, a)\nin a table known as the Q-Table, where q\u03c0(s, a) denotes\nthe expected cumulative reward that the rodent model can\nget from state s by taking action a and following policy \u03c0\nthereafter.\n2) Actions A: The set of all possible actions is A =\n{left, right, none}.\nThe model picks its actions using the e-greedy policy with\nrespect to its learned Q-Table: with probability e it chooses\nan action at random (explore); and with probability 1 - e"}, {"title": "it chooses an action according to the Q-Table (exploit). To mimic the behaviour closely, the model starts off with high values of e and gradually decreases it by a small term after every episode. An episode ends when the session ends.\nIt has been shown[22] that a learning policy always con- verges to an optimal policy if it satisfies the GLIE (Greedy in the Limit with Infinite Exploration) conditions:\n1) If a state is visited infinitely often and each action in that state is chosen infinitely often.\n2) There is a small (but non-zero) exploration probability ej at the start of episode j.\n3) In the limit the learning policy is greedy (with prob- ability 1) with respect to the learned Q-function, i.e. lim ej = 0\nj\u2192\u221e\nIn our implementation, we've used the following equation to set the exploration probability ej at the start of session (episode) j\n\u03f5j = \u03f5s + e\u22120.025j\nwhere \u03f5s = 0.8 is the starting exploration probability.\nDuring every trial of any arbitrary session j, the model explores with probability ej, i.e. it selects any one of the available actions from A with uniform probability. And with probability 1 - ej it exploits action a ~ P(a; s) i.e. samples an action a from the discrete probability distribution\nP(a; s) = \u03c1q\u03c0(s,a)\n\u03a3\u03b1'\u2208\u0391\u03c1\u03b1\u03c0(s,\u03b1')\nwhere s is the state of the environment at the start of the trial.\n3) Reward Model R: For every correct and incorrect action the model is rewarded with +1 and -1 respectively. Just like what happens with the biological rodents, we stop the training when the model reaches a pre-defined accuracy of 70% or more for three consecutive sessions.", "content": "At any particular trial t, let's say that the state of the environment is st, the agent takes an action at, receives the reward rt and thereby changes the state of the environment to st+1 in the next trial t + 1. The action value function is then updated by the following Q-Learning update rule [21]\nq(st, at) = q(st, t, at)+\u03b1. [rt+\u03b3.max{q(st+1, a')}-q(st, at)]\n\u03b1'\u2208A\nThroughout our experiment, we used the learning rate \u03b1 = 0.2 and the discount factor \u03b3 = 1.0 (no discount).\n\nB. Similarity Metrics\n1) Similarity between individual behaviors: Let a and b be two individuals (data from real rodents, or two distinct executions of our above-described model rodent), and let Sa) and S(b) be data from the i-th session for individual a and the j-th session from individual b, respectively. We now define a metric to quantify the similarity between sessions\nS(a) and S(b). This quantity is defined using a distance metric D over discrete probability distributions. At a high level, we consider sliding windows of fixed length (denoted by \u2206) at various points over Sa) (and S(b)), the frequencies\nof correct actions as distributions, and define the similarity metric as the mean distance between such distributions. Formally, the mean distance between windowed distribution of actions, denoted by Ei,j(a,b) is defined as:\nEi,j (a,b) = 1L-\u2206+1 \u2211l=1L-\u2206D(\u03b4(l)i,t \u03b4(l)j,t) (1)\nwhere\n\u03b4(l)i,t = 1\u2206 \u2211\u2206t'=01[l=correct] (2)\nwhere D is the chosen distance function over distributions, L = min (|Sa|, |Sj|), \u2206 is the window length and 1[x=y]"}, {"title": "is the indicator function defined as:\n1{x=y} = {1 if x = y\n0 otherwise\nAny distance metric can be used to measure the distance between two discrete probability distributions. In our results we've used Match Distance (L\u2081 distance) defined as follows:\nD (D(1), D(2)) = \u2211i\u2208 dom |pmf(1)(i) \u2013 pmf(2)(i)|\nwhere the two distributions D(1) and D(2) have the same domain dom, and pmf(1) and pmf(2) are the probability mass functions of the two distributions.\n2) Similarity between group behaviors: The inherently stochastic nature of the behaviors makes comparison of individuals challenging. To identify trends more clearly, we can compare groups as a whole. We define a comparison metric between group behaviors closely following the mean windowed distribution of actions, as follows. Given a group of N individuals {A1, A2,..., An}, we first define a sliding-window mean of their j-th session as:\n\u015cj = (\u03b4j,0, \u03b4j,1,..., \u03b4j,m;\u2212\u2206)\nwhere\n\u03b4j,t = 1N\u2206 \u2211i=1N\u2211\u2206l=01[l=correct] (3)\nWe define the average distance between the group's behavior between sessions i and j as\nEij = 1L-\u2206+1 \u2211L-\u2206t=0 D(\u03b4i,t, \u03b4j,t) (4)\nwhere L = min (|\u015ci|, |\u015cj|).", "content": "IV. EXPERIMENTAL RESULTS\nWe ran our model on the same input sequence that were used to train the real rodents in the lab. Using black dots in the Fig. 1 we show the accuracy across all the trials for our model-rodent in three particular sessions 1, 6, and 12. We've also shown (in colored lines) the sliding-window moving-mean accuracy across all the trials for every real rodent in the same three particular sessions \u2013 1, 6, and 12.\nAlthough, by visual inspection these accuracy curves of artificial rodents look very similar to that of the real ones, we still need to define the degree of similarity formally and compare the same with respect to the real rodents.\nTo identify the best model among them, we designed a novel similarity metric based on action probability distribu- tion and rigorously quantified the degree of similarity in the behavior of our artificial rodents with respect to biological ones.\nWe ran the artificial rodent-model for 12 consecutive sessions, 5 executions in each session with identical initial conditions. Then we compared the similarity between each such execution using the previously defined Equation 1. Fig. 2a shows the correlation matrix of similarity values (higher values indicate lower similarity) for the rodent model, between any two executions from three arbitrarily chosen sessions = session 1, 6, and 12. In Fig. 2b we show the corresponding values for five real rodents computed in a similar way.\nFig. 3 shows group behavior of our model rodents com- pared across multiple sessions. The figure shows the average distance between each pair of sessions using equation 4, where each cell (i,j) represents the average distance in sessions i and j. The figure shows a clear trend in session- wise behavior as we increase the number of samples (we"}, {"title": "V. CONCLUSIONS", "content": "Behavioral training of lab animals has always been an arduous task for researchers. Optimization of these training protocols while minimizing their dependencies on human trainers, has been the major bottle-neck. With the recent advancement of Artificial Intelligence techniques, our data driven approach towards accomplishing this goal will create new paths for researchers to explore. Usage of data augmen- tation to built artificial rodent models will allow us to use these models in future to build an adaptive and intelligent trainer agent that will reduce the human intervention to a great extent."}]}