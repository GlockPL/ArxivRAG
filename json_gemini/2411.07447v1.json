{"title": "THE EFFECT OF SCHEDULING AND PREEMPTION ON THE EFFICIENCY OF LLM INFERENCE SERVING", "authors": ["Kyoungmin Kim", "Kijae Hong", "Caglar Gulcehre", "Anastasia Ailamaki"], "abstract": "The growing usage of Large Language Models (LLMs) highlights the demands and challenges in scalable LLM\ninference systems, affecting deployment and development processes. On the deployment side, there is a lack\nof comprehensive analysis on the conditions under which a particular scheduler performs better or worse, with\nperformance varying substantially across different schedulers, hardware, models, and workloads. Manually testing\neach configuration on GPUs can be prohibitively expensive. On the development side, unpredictable performance\nand unknown upper limits can lead to inconclusive trial-and-error processes, consuming resources on ideas that end\nup ineffective. To address these challenges, we introduce INFERMAX, an analytical framework that uses inference\ncost models to compare various schedulers, including an optimal scheduler formulated as a constraint satisfaction\nproblem (CSP) to establish an upper bound on performance. Our framework offers in-depth analysis and raises\nessential questions, challenging assumptions and exploring opportunities for more efficient scheduling. Notably,\nour findings indicate that preempting requests can reduce GPU costs by 30% compared to avoiding preemptions at\nall. We believe our methods and insights will facilitate the cost-effective deployment and development of scalable,\nefficient inference systems and pave the way for cost-based scheduling.", "sections": [{"title": "1 INTRODUCTION", "content": "The growing demand for large language model (LLM) in-\nference highlights challenges in efficient and scalable LLM\ninference systems. Unlike training, which is a one-time cost,\ninference is a continuous and far more expensive operation\ndue to the high costs of GPU time and energy consumption.\nFor example, ChatGPT approximately receives 600M visits\nper month*, spending $0.7M a day to run GPUs*. The recent\nOpenAI ol model also underscores the increased demands\nof LLM inference, as the model runs multiple inference\npaths to tackle complex reasoning tasks. Therefore, reduc-\ning the LLM inference latency leads to significant economic\nand environmental savings considering CO2 emissions in\nrunning models (Patel et al., 2024b; Luccioni et al., 2023).\nResearchers and developers have focused heavily on creat-\ning more efficient LLM inference systems and techniques\n(Yu et al., 2022; Kwon et al., 2023; Agrawal et al., 2023;\nZhong et al., 2024b; Lee et al., 2024; Zhu et al., 2024; Xu\net al., 2024; Pan et al., 2024; Dao et al., 2022), but they have\nlargely overlooked scheduling inference requests. A recent\nwork (Agrawal et al., 2024b) shows that mis-scheduling\ncan degrade throughput by more than six times. However,\nthe optimal scheduling policy is yet to be known. Instead\nof naively repeating trials and errors in developing more\nefficient schedulers, we need a more systematic and cost-\neffective approach. Figure 1 illustrates this motivation, that\ntaking a glance at the optimal objective would improve the\nproductivity of development and save costs substantially.\nIt is also non-trivial to predict how the optimal scheduler\nwould behave, since it actually chooses counter-intuitive"}, {"title": null, "content": "policies which we find out in this paper. For example, multi-\nple requests face a race condition as they store their inter-\nmediate data (KV, Section 2.1) in the limited GPU memory.\nIf memory runs out, the system should preempt and restart\nsome requests later, so a common belief is avoiding pre-\nemption and recomputation overheads. However, one of our\nfindings reveals that the optimal scheduler makes the choice\nto preempt and restart requests, particularly when the re-\nquests are short and memory is largely utilized. This can\nsave more than 30% of GPU time compared to waiting for\nother requests to release memory to avoid any preemptions.\nPreempting long requests can rather degrade performance\nby 30% due to longer recomputation times. Such a correla-\ntion between request length and preemption is challenging\nto uncover.\nTo tackle the challenges, we propose INFERMAX, an ana-\nlytical framework to compare different schedulers and as-\nsess scheduling policies against the optimal ones. INFER-\nMAX extends VIDUR (Agrawal et al., 2024a), a framework\nthat allows simulation of schedulers on diverse hardware\nand model configurations. This offers cost-effectiveness as\nthe simulation does not require running GPUs once the in-\nference costs are modeled. While VIDUR focuses on the\nsimulation and searching for the optimal hardware-model-\nscheduler configuration for deployment, it lacks a thorough\nanalysis and exploration of further opportunities to enhance\nperformance, leaving a gap for development. We formulate\nthe problem of finding optimal schedules for the first time\nusing the constraint satisfaction problem (CSP) (Schrijver,\n1998) based on the cost models. Here, one can force particu-\nlar scheduling policies in forms of constraints and optimize\nlatency, throughput, or any objective that can be represented\nas a formula.\nOur analysis using INFERMAX measures the effect of\nscheduling on inference performance, revisiting the over-\nlooked scheduling policies and using the CSP solutions as\nthe optimal policies to pursue. Our key findings include the\ncorrelation between request length and preemption. From\nthe analysis, we envision a cost-based scheduling as the\ncost-based query optimization in databases (Selinger et al.,\n1979), which generalizes across a variety of inference sys-\ntems with unique policies (e.g., KV cache offloading (Lee\net al., 2024; Pan et al., 2024) and just-in-time KV compres-\nsion) considering diverse storage hierarchies.\nIn summary, we propose an analytical framework INFER-\nMAX for analyzing schedulers (Section 3), formulate the\nproblem of finding optimal schedules as CSP, offering theo-\nretical upper bound on the performance (Section 3.3), and\nprovide a comprehensive analysis on schedulers that worth\n200 GPU hours, showing that 30% of latency can further be\nsaved by harnessing preemptions (Section 4). Finally, we\ndiscuss our vision (Section 5)."}, {"title": "2 BACKGROUND", "content": "This section explains the processing of LLM requests and\nrelated work."}, {"title": "2.1 Processing a Single LLM Request", "content": "In LLM inference, the input text submitted by a user is\ncalled the prompt. When the system receives a prompt, it\nprocesses its tokens to generate a new token, a phase known\nas prefill. This is followed by decode steps, where each step\ngenerates a new token based on the last generated one. Each\nstep involves (1) embedding the input tokens into vector rep-\nresentations, (2) transferring these vectors to the GPU, and\n(3) executing matrix multiplications between input vectors\nand model weights, along with other GPU operations like\nattentions (Vaswani et al., 2017). The system repeats the\nstep (3) across all model layers, producing output vectors\nthat match the size of the input vectors. It then feeds the\nfinal layer's output to a sampling component to generate the\nnext token.\nMost LLMs rely on the Transformer architecture, which\ncomputes the attention output for each token by using its\nquery (Q) along with the key (K) and value (V) vectors of\nprevious tokens. To reduce recomputation, key-value (KV)\ncaching (Dai et al., 2019) stores previously computed KVs\nin GPU memory, allowing the system to reuse them for\nfuture tokens. This is widely adopted for efficient LLM in-\nference. In this structure, prefill tokens lack previous tokens\nfor KVs, making their processing compute-bound. In con-\ntrast, decode tokens have an expanding set of KVs to read\nas output length grows, causing decode steps to become\nincreasingly memory-bound (Agrawal et al., 2023)."}, {"title": "2.2 Processing Multiple LLM Requests", "content": "LLM inference systems rely on schedulers to batch re-\nquests at each step, prioritizing resource utilization, par-\nticularly the maximum KV cache size, $M$, which corre-\nsponds to the token limit. This is typically calculated as:\n$\\frac{\\text{available-GPU-memory-model-size}}{\\text{KV\\_size\\_per\\_token}}$ with available GPU mem-\nory often capped at a percentage of total capacity, such as\n90% (Kwon et al., 2023). The scheduler also designates $C$,\nthe maximum number of tokens to process per batch.\nTo improve inference efficiency, systems adapt techniques\nfrom databases and operating systems. Two standard meth-\nods include continuous batching (Yu et al., 2022) and paged\nattention (Kwon et al., 2023), each optimizing computa-\ntional and memory efficiency. Continuous batching sched-\nules new requests step-by-step as resources become avail-\nable, avoiding idle waiting times, while paged attention\nenhances memory utilization by managing KV caches in\npage segments rather than reserving large allocations per\nrequest."}, {"title": null, "content": "Other methods, such as hybrid batching and chunked prefill\n(Agrawal et al., 2023; 2024b), offer alternative scheduling\nfor LLM requests. Hybrid batching processes both prefill-\nand decode-phase requests together, though prefills, due to\nlarge prompts, often consume more resources and can delay\ndecode-phase requests in the same batch. Chunked prefill\nmitigates this delay by enabling partial prompt processing,\nwith a parameter P (where P < C) indicating the maximum\nnumber of prefill tokens per batch. A new token can only be\ngenerated once all prompt tokens are processed.\nTo further manage scheduling, we define two metrics: batch\nsize, representing the number of requests in each batch, and\nrunning size, indicating the count of active (running) re-\nquests holding KV caches. When the total number of KVs\nof running requests reaches the cache budget M, the sys-\ntem may need to preempt some requests, clearing their KV\ncaches in a process we refer to as eviction. Once evicted, a\nrequest's generated tokens are appended to the prompt. We\ncall processing these tokens again as the refill phase. No-\ntably, KV data is recomputed, rather than offloaded to other\nstorage and reloaded, as PCIe bandwidth between GPU and\nexternal storage is lower than the GPU's processing capacity\n(Kwon et al., 2023)."}, {"title": "2.3 Related Work", "content": "LLM Inference System. ORCA (Yu et al., 2022) introduces\ncontinuous batching, VLLM (Kwon et al., 2023) proposes\npaged attention, and SARATHI (Agrawal et al., 2023; 2024b)\nimplements hybrid batching and chunked prefill. Instead\nof hybrid batching, DISTSERVE (Zhong et al., 2024b) and\nDEJAVU (Strati et al., 2024) disaggregate prefill and decode\nphases by using different GPUs, sending the KV cache from\nthe prefill- to decode-handling GPU. VTENSOR (Xu et al.,\n2024) decouples the KV cache allocation and attention com-\nputation in VLLM, removing the page address translation\noverhead. INFINIGEN (Lee et al., 2024) offloads the KVs\nto CPU memory to extend the KV cache and reloads the\nKVs from CPU layer-wise. To minimize the data transfer\noverhead, it streams only the KVs of the most important to-\nkens and approximates the full attention result. NANOFLOW\n(Zhu et al., 2024) proposes a finer-grained batching than\nthe continuous batching, where each batch of requests is\nfurther partitioned into nano-batches. INSTINFER (Pan et al.,\n2024) uses flash drives to offload KVs and attention com-\nputations. Our approach to inference cost modeling and\noptimal scheduling could adapt to these advanced systems,\nleveraging diverse storage hierarchies and data movement\nstrategies for enhanced performance.\nLLM Inference Simulation. VIDUR (Agrawal et al.,\n2024a) provides execution time data for GPU operators\nacross various models and GPU configurations, implement-\ning multiple schedulers, including those of VLLM and"}, {"title": null, "content": "SARATHI, and achieving prediction accuracy within a 9%\nrelative error. However, VIDUR primarily emphasizes sim-\nulation and search, lacking an in-depth analysis or a struc-\ntured methodology for designing improved schedulers. In\ncontrast, LLMVIEWER (Yuan et al., 2024) uses theoretical\nhardware limits, such as GPU FLOPS and memory band-\nwidth, to analyze the performance of single batches across\ndifferent hardware setups, without addressing scheduling as-\npects. Our approach builds on VIDUR's results, with the po-\ntential to further integrate LLMVIEWER's hardware bounds\nfor a more comprehensive optimization.\nOutput Size Prediction. Several studies aim to estimate or\nrank the output sizes of requests to improve scheduling ef-\nficiency (Qiu et al., 2024; Zheng et al., 2023b; Fu et al.,\n2024). Our approach complements this work by examining\nthe fundamental question of how much improvement po-\ntential exists in leveraging output sizes for scheduling. As\ndetailed in Section 4, our findings also apply to scenarios\nwhere requests have identical output sizes, and shed light\non input sizes when requests have heterogeneous input and\noutput sizes."}, {"title": "3 INFERMAX", "content": "This section introduces INFERMAX, our analytical frame-\nwork for evaluating LLM inference performance, illustrated\nin Figure 2. Given an initial configuration (1), built-in or\ncustom schedulers generate schedules (2), defining the set\nof requests processed in each batch. These schedules can\nthen be dispatched to inference systems (3) to execute and\nmeasure performance (4). However, this approach requires\nsubstantial development effort to establish a unified inter-\nface, enabling output from various schedulers to be sent\nacross different inference systems, as well as for standardiz-\ning execution results for performance evaluation. Running\nevery schedule on GPUs also incurs high computational\ncosts.\nTo address these challenges, we adopt an alternative ap-\nproach in this study, predicting batch execution times based\non the number of tokens processed and the KV caches ac-\ncessed (5), using results from VIDUR (6). As an alter-\nnative, theoretical hardware bounds, such as those used in\nLLMVIEWER, may also be applied and visualized through\nroofline models (7) (Yuan et al., 2024).\nThe blue boxes and solid arrows in Figure 2 represent our\nprimary focus areas. In Section 3.1, we propose a unified\nalgorithm for representative schedulers and their variants,\nexamining the impact of design choices on performance\nin Section 4. For batch time prediction, we employ simple\nlinear models as cost models in Section 3.2. Lastly, we\ndefine the task of finding optimal schedules as a constraint\nsatisfaction problem (CSP) in Section 3.3."}, {"title": "3.1 Unified Algorithm for Schedulers", "content": "This section details a unified algorithm (Algorithm 1) for\nschedulers across various inference systems, with distinc-\ntions in the implementation of the GETNEXTBATCH func-\ntion, particularly in steps 1-4. Lines 1-7 of the algorithm\nare shared, as they iteratively process each batch of requests.\nThe requests in Rw and Rr are first ordered and grouped\n(Line 11, 1). For each group G of requests and its candidate\nrequest cand, if hybrid batching is disabled and cand is\nin a different phase (prefill or decode) than the requests al-\nready in B, cand is skipped (2). In step 3, CANALLOCATE\nchecks if cand can be added to B based on the following\nconditions:\n\u2022 C: Ensures the token capacity C isn't exceeded by\nadding cand, which can process up to $C - \\sum_{r \\in B} r.c$\ntokens; r.c denotes the number of tokens to process for\na request r.\n\u2022 P: If cand is in the prefill phase, it can process up to\n$P - \\sum_{r \\in B} r.c$ tokens.\n\u2022 M: Checks if the KV cache can store cand's required\nKVs (one for decode, input size for prefill).\n\u2022 Rmax: Checks if the running size hasn't reached Rmax.\nFor chunked prefill, cand.c can be adjusted to a smaller\nvalue, $\\min(cand.c, P - \\sum_{r \\in B} r.c)$, if cand is in prefill\nphase. If allocation fails due to insufficient M, the scheduler\ncan opt to evict other running requests and reassess cand\n(4) or proceed to the next group. If allocation fails for other\nreasons, no eviction occurs. Any evicted request is removed\nfrom R and appended to Rw. If cand is successfully allo-\ncated, it is added to B and removed from its original queue\n(Line 17).\nWe illustrate using two representative schedulers for LLM"}, {"title": null, "content": "inference: VLLM and SARATHI. In VLLM, the requests are\norganized into two groups, Rw and Rr, with priority given\nto prefill requests in Rw (1). Hybrid batching and chunked\nprefill are disabled (2 and 3). If memory M is insufficient\nfor allocation in 4, VLLM skips to the next group for can-\ndidates in Rw. For candidates in Rr, it attempts to evict\nother low-priority running requests; if this fails, it eventu-\nally evicts the candidate itself. SARATHI, on the other hand,\ncreates three groups: Rd, Re, and Rw, where Rod and Re\nrepresent running requests in the decode and prefill phases.\nThis setup prioritizes decode requests (1), and both hybrid\nbatching and chunked prefill are enabled (2 and 3). If M\nis insufficient for Rd candidates in 4, SARATHI attempts\nevictions."}, {"title": "3.2 Cost Models for Batch Times", "content": "This section explains the prediction model used for esti-\nmating batch times. Figure 3 illustrates examples of GPU\nprocessing times for various operators within a model layer.\nFigure 3(a) shows the total time required for non-attention\noperators, such as MLPs and activations, which depend\non the number of tokens processed in a batch. Figure 3(b)\npresents the time for decode-phase attention, determined\nby the number of KVs read from the cache. In contrast,\nprefill-attention time in Figure 3(c) scales with the square"}, {"title": null, "content": "of the number of tokens processed, $\\sum_{r \\in B} r.c^2$. To predict\nbatch time, we sum the costs of non-attention operators and\nthe attention costs, using either prefill- or decode-attention\nbased on the request phase. For hybrid batches, both prefill-\nand decode-attention costs are included."}, {"title": "3.3 Optimal Scheduling as Constraint Satisfaction\nProblem", "content": "This section applies the constraint satisfaction problem\n(CSP) (Schrijver, 1998) to determine optimal schedules.\nRather than seeking a better scheduling algorithm without\nassured performance outcomes, solving the CSP approach\ndirects us toward optimal schedules, allowing for a more\ngoal-oriented development process, as illustrated in Figure\n1. For instance, we can validate whether a better scheduler\nexists that could reduce the latency of current schedulers by\n10% for specific workloads. The optimization target can be\nadjusted to meet objectives such as latency, throughput, or"}, {"title": null, "content": "fairness, if these can be represented in a formula.\nWe assume that the output sizes of requests are known in\nadvance (estimating them is orthogonal to our work, Section\n2.3), and that a single GPU is in use. This approach could\nbe extended to multiple GPUs by incorporating the linear\ncost modeling of data transfer.\nIn our CSP formulation, we first establish key notations. The\nindex i \u2265 1 and j \u2265 1 represent the i-th request ri and j-th\nbatch Bj. We also use j = 0 as a virtual index to denote\nthe initial system state. Each ri has an input size (number\nof prompt tokens) I\u2081 and output size O\u017c. We use I as the\nindicator variable, 1 if condition holds or 0 otherwise. Now\nwe explain the three parts of our CSP: variables, constraints,\nand objectives.\nVariables. The variable Ii,j represents the input size af-\nter processing batch Bj, as input size may increase fol-\nlowing evictions, with I\u00bf,0 = Ii. mi,j denotes ri.m after\nprocessing Bj, with mi,0 = 0. The maximum memory us-\nage of ri can reach $I_i + O_i \u2013 1$, since the last generated\ntoken doesn't need to be cached for the next token gener-\nation. $d_{i,j} = I_{m_{i,j-1}>I_{i,j-1}}$ indicates if ri has processed\nall prompt tokens before Bj, which implies that it is in the\ndecode phase. gi,j and ei,j indicate whether ri generates a\ntoken or is evicted at Bj. Ci,j denotes ri.c at Bj, while czi,j\n= Ici,j>0 = $I_{c_{i,j}>0}$, Mzi,j = Imi,j>0, and dgi,j = $I_{d_{i,j} \\land g_{i,j}}$.\nConstraints.\nThe constraints establish the interactions between variables\nand the conditions necessary for a valid schedule. We omit\nthe binary variable constraints here for brevity and focus on\nthe remaining constraints:\nTermination Constraint: ri must generate O\u2081 tokens.\n$\\forall i: \\sum_j g_{i,j} = O_i$ (1)\nNon-Decreasing Input Size: Ii,j = max(Ii,j\u22121,Mi,j\u22121 +\n1), where +1 represents the last token generated but not yet\nprocessed.\n$\\forall i,j: I_{i,j}\\ = \\begin{cases} m_{i,j-1}+1 & \\text{if } e_{i,j} = d_{i,j} = 1 \\\\ I_{i,j-1} & \\text{otherwise} \\end{cases}$ (2)\nMemory Management: m should be zero if ri is evicted or\nshould increase by Ci,j. If di,j = 0, m must not exceed I to\nprevent overlapping prefill and decode phases.\n$\\forall i, j: m_{i,j}\\begin{cases} 0 & \\text{if } e_{i,j} = 1 \\\\ M_{i,j-1}+ C_{i,j} & \\text{if } e_{i,j} = 0 \\\\ < I_{i,j-1} & \\text{if } e_{i,j} = d_{i,j} = 0 \\end{cases}$ (3)\nControl of c in Evict/Decode Phases.\n$\\forall i, j: c_{i,j}=\\begin{cases} 0 & \\text{if } e_{i,j} = 1 \\\\ <1 & \\text{if } d_{i,j} = 1 \\end{cases}$ (4)"}, {"title": null, "content": "Batch Constraints: Ensures global constraints per batch.\n$\\forall j: \\sum_i C_{i,j} \\leq C, \\sum_i C_{i,j} \u2013 dg_{i,j} \\leq P,$\n$\\sum_i m_{i,j} \\leq M, \\sum_i m_{zi, j} < R_{max}$ (5)\nIn this configuration, hybrid batching and chunked prefill\nare both enabled, as no constraints enforce separation of\nprefill and decode requests or require a one-time increase of\nmi,j by the prompt size. To disable these features, specific\nconstraints can be added.\nFor implementing conditional constraints based on variables,\nwe use the Big-M method (Pistikopoulos, 1998) to linearize\nthem, since linear programs are more efficient than non-\nlinear ones (Bertsimas & Tsitsiklis, 1997). For example, the\nupper part of (2) can be linearized as\n$\\forall i,j: I_{i,j} \\leq m_{i,j-1} + 1 + M(1 \u2013 e_{i,j}) + M(1 \u2013 d_{i,j})$\n$I_{i,j} \\geq M_{i,j-1} + 1 \u2013 M(1 \u2013 e_{i,j}) \u2013 M(1 \u2013 d_{i,j})$ (6)\nwhere M is a sufficiently large constraint. We implement\nour CSP using GUROBI.\nObjective. The CSP objective can be set to minimize to-\ntal latency, utilizing our batch time prediction model from\nSection 3.2. For example, $\\sum_i C_{i,j}$ represents the tokens pro-\ncessed Bj.\nOnline Setting. Supporting an online setting, where each\nrequest ri has an arrival time Ti, is straightforward. We\nadd variable Accj to track accumulated batch times and set\nCi,j = mi,j = 0 if Accj < Ti.\nAlternative Objectives. Besides minimizing latency, we\ncan optimize request-level metrics, such as Time-to-First-\nToken (TTFT) and Time-Per-Output-Token (TPOT), which\nare widely used. TTFT measures the time until the first\noutput token, while TPOT measures the time between con-\nsecutive tokens. For instance, minimizing TTFT for request\nri could be achieved by using $\\min_j (Acc_j \\cdot g_{i,j} \u2013 T_i)$ as\nthe objective. Constraints can also set specific goals, such\nas verifying if a better schedule exists by running another\nscheduler with latency L and ensuring the new latency is\nunder 0.9L.\nChallenge. A primary challenge in CSP is its limited scal-\nability, as CSPs are generally NP-complete (Russell &\nNorvig, 2020). The complexity grows with the number of re-\nquests and batches, reaching millions of variables for 1,000\nrequests and 1,000 batches. Consequently, we primarily use\nCSP to validate findings in controlled environments (Section\n4) and initialize CSP with outputs from other schedulers as\nseeds. Optimizing CSP for larger scales remains an area for\nfuture work."}, {"title": "4 ANALYSIS", "content": "This section evaluates the performance of different sched-\nulers and addresses key questions, some of which have\nreceived limited attention in previous studies."}, {"title": "4.1 Setup", "content": "Model and Hardware. We follow the default configuration\nof (Agrawal et al., 2024a), using the Llama-2-7B model on\nan A100 GPU, and set C = 4096 and M = 100K as default.\nSchedulers. Using (Agrawal et al., 2024a) as a reference,\nwe employ the VLLM (Kwon et al., 2023) and SARATHI\n(Agrawal et al., 2024b) schedulers as baselines. To better\nanalyze scheduler performance, we vary Rmax from 128 to\nunlimited, aiming to assess the effect of key scheduler fea-\ntures. Additionally, we determine the optimal Rmax value in\nSection 4.6. Table 1 lists the schedulers we compare, exclud-\ning certain variants that either performed similarly or worse\nin our tests. We also implement eviction-free versions of\nthe schedulers by reserving the maximum possible memory\nusage per candidate in the KV cache (in CANALLOCATE in\nAlgorithm 1).\nWorkloads. For clarity, we begin with fixed input and out-\nput sizes I and O for all requests, ranging from 1 to 1024.\nWe select request counts of B = 32 and B = 1024 to\nrepresent low and high contention scenarios. To simplify\nanalysis, we use an offline setting where all requests are\navailable before scheduling begins, which acts as a snapshot\nof an online environment. Analysis of the online setting is\ndeferred to future work.\nMetrics. We measure system performance using total la-\ntency and tokens-per-second (TPS), calculated as the num-\nber of generated tokens divided by latency. Average TTFT\nand TPOT are used to evaluate request-level performance."}, {"title": "4.2 Under High Contention", "content": "We focus on the high-contention scenario with B = 1024\nas low contention with B = 32 does not trigger evictions,\nand its results are covered in subsequent sections (see Ap-\npendix A.1 for low contention details). In this analysis, we\ncategorize the schedulers in Table 1 by performance into"}, {"title": null, "content": "groups: {SARATHI }, {SARATHIP=C, SARATHInocp }, and\n{VLLM, VLLMhy }. We exclude SARATHInohy due to its\nsignificantly higher latency and TTFT (Appendix A.1). Fig-\nure 4 displays the results for SARATHI, SARATHIP=C, and\nVLLM as representatives.\nTPS. TPS peaks as B grows, allowing more requests to pro-\ncess concurrently, generating more tokens per batch. How-\never, TPS declines as I increases due to higher prefill costs.\nBeyond a certain point of O, TPS decreases because small\nO values make prefill dominant, while large O values make\nsubsequent decode batches heavier, as decode costs scale\nlinearly with the number of tokens or KVs read (Figure 5).\nTTFT and TPOT. TTFT and TPOT reveal a trade-off,\nwhere TTFT reflects prefill time and TPOT reflects de-\ncode time. VLLM and SARATHIP_C achieve lower TTFT\nbut higher TPOT than SARATHI. Schedulers other than\nSARATHI can process up to C prefill tokens per batch, start-\ning prefill early, which minimizes TTFT but increases TPOT\ndue to larger batch sizes and more KVs read.\nAn interesting point is that TPOT decreases beyond a certain\nvalue of I. For large O (e.g., 1024), this is due to reduced\nevictions, as frequent evictions cause refills to dominate\nruntimes. In contrast, for small O, refills have a lesser impact,\nso TPOT more directly reflects batch size and the number\nof KVs, as previously noted.\nEvictions and batch size. The parameter M limits the max-\nimum running requests in prefill or decode phases, while\nthe number of new running requests per batch is limited by\nprefill tokens P. As maximum P/I requests can be newly\nrun, high I decrease both batch size and running requests,\nleading to fewer evictions as I rises in Figure 4. However,\nI and O also determine memory reserved per request, so\nsmall values for I and O result in less frequent evictions.\nThe key distinction between I and O in terms of their impact\non evictions is that I represents the immediate memory re-\nserved, whereas O determines the peak memory usage after\napproximately \u03a9(\u039f) batches have been processed. Conse-\nquently, schedulers that only consider I and disregard O\nrisk overloading the system by batching requests without\nfully understanding their long-term memory demands. As O\ngrows, this can lead to a significant increase in the number\nof evictions.\nMemory (KV Cache) Usage. For O = 1, only SARATHI\nshows a positive KV count at I = 1024, as it partitions I"}, {"title": "4.3 How Good Is It to Avoid Evictions?", "content": "With a basic understanding of the factors influencing per-\nformance, we can explore some key questions. Figure 6\nprovides a high-level roadmap for the upcoming sections.\nWe begin by comparing the original schedulers in Table 1\nto their eviction-free versions. We use O = B = 1024,\na scenario with frequent evictions, since in other cases,\neviction-free schedulers perform similarly to their original\ncounterparts."}, {"title": null, "content": "As illustrated in Figure 7, eviction-free schedulers generally\nachieve better system performance, with TPS improvements\nover their original versions reaching up to 6.9%, 1.7%, and\n3.1% for VLLM, SARATHI, and SARATHIP=C. However,\neviction-free schedulers exhibit higher TTFT due to the\nneed to wait for running requests to complete and release\ntheir KVs. This TTFT increase is substantial \u2013 up to 1800x\nfor VLLM and 91.7% for SARATHI \u2013 but is offset by lower\nTPOT, with reductions of up to 13x for VLLM and 6.5x for\nSARATHI. Thus, the general TTFT-TPOT trade-off remains\nconsistent even for eviction-free schedulers."}, {"title": "4.4 Is Increasing M a Silver Bullet?", "content": "To avoid evictions and maximize the effective running size,\none might wonder if simply increasing M to a sufficiently\nlarge value could solve all issues, especially with state-of-\nthe-art GPUs. To explore this, we vary M from 100 to 1M,\ntesting under different memory contention levels and model-\nhardware configurations to simulate lower memory budgets.\nFigure 8 shows results for O = 32 and B = 1024.\nInterestingly, when"}]}