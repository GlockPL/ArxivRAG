{"title": "MMFactory: A Universal Solution Search Engine for Vision-Language Tasks", "authors": ["Wan-Cyuan Fan", "Tanzila Rahman", "Leonid Sigal"], "abstract": "With advances in foundational and vision-language (VLM) models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these models, no single model is able to handle all tasks and/or applications that may be envisioned by potential users. Recent approaches, such as visual programming and multimodal LLMs with integrated tools aim to tackle complex visual tasks, by way of program synthesis. However, such approaches overlook user constraints (e.g., performance / computational needs), produce test-time sample-specific solutions that are difficult to deploy, and, sometimes, require low-level instructions (e.g., code snippets for similar problems) that maybe beyond the abilities of a naive user. To address these limitations, we introduce MMFactory, a universal framework that includes model and metrics routing components, acting like a solution search engine across various available models. Based on a task description and few sample input-output pairs and (optionally) resource and/or performance constraints, MMFactory can suggest a diverse pool of programmatic solutions by instantiating and combining visio-lingual tools (e.g., detection, segmentation, VLMs) from its model repository. In addition to synthesizing these solutions, MMFactory also proposes metrics and benchmarks performance / resource characteristics, allowing users to pick a solution that meets their unique design constraints. From the technical perspective, we also introduced a committee-based solution proposer that leverages multi-agent LLM conversation to generate executable, diverse, universal, and robust solutions for the user. Experimental results show that MMFactory outperforms existing methods by delivering state-of-the-art solutions tailored to user problem specifications.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs), such as GPT [2] and Gemini [51, 52], have demonstrated powerful capabilities across various domains, significantly transforming how people approach their tasks and even their daily lives. Building on these models, a wide range of vision-language (VLM) or multimodal LLMS (MLLMs) [1, 5, 7, 35, 62] have been developed by integrating modality adapters and encoders into their frameworks. This advancement has resulted in state-of-the-art models capable of solving complex visual tasks.\nDespite the push for building AGI-like agents, that are all capable, even models like GPT-40 tend to be inferior, or lacking, on specific tasks [19, 31]. At the same time, with the development of fine-tuning techniques, customized or expert models tailored to specific tasks have become easier to develop. With different training data, fine-tuning approaches, and frameworks, models with varying specialties and characteristics are being introduced daily. One can imagine that in near future such models will be ubiquitous,"}, {"title": null, "content": "creating a marketplace of agents with an overwhelming design choices for users to pick from and build on. In this scenario, routing approaches are needed that can take user-defined tasks, needs, and constraints, acting as a search engine among all types of models, to provide suggested solutions for the user.\nPrevious works in visual programming [22, 49] and multimodal language models (MLLMs) with tool integration [26, 38, 45] have explored using LLMs as planners to utilize external tools or APIs for solving complex visual tasks or to decompose tasks into sub-tasks. While these approaches have shown promise, there are several limitations to consider. First, existing methods assume a single specialized tool for a given sub-task (e.g., detection [36], segmentation [30], depth estimation [59]). This is overly simplistic, as a variety of tools exist for any one sub-task, inculcating within a particular family of models, that differ by backbone, number of parameters and overall performance. Second, these works generally overlook the user's specific computation needs and constraints when generating solutions, resulting in inability to tailor solutions to particular hardware or deployment cost (e.g., a user maybe willing to forgo 1% better performance if inference cost is reduced by 50%). Third, the proposed solutions are often tailored per specific example or scenario, which limits their generalization and applicability to other examples in the task, as shown in Fig. 1. Deployment of such solutions is problematic (e.g., no constant code path exist that maybe distilled to a small model executable on an edge device). Addressing these limitations is essential for creating more versatile and user-centric framework for routing the solutions among different kinds of models in order to create custom agents capable of solving specific user problems in accordance to their specification.\nTo address these challenges, in this work, we introduce MMFactory \u2013 a universal framework for automatic and programmatic development of task-specific agents. MMFactory (Fig. 1a) includes a model and metric routing components; that, in combination, act as a solution search engine for non-expert users. Based on a task description (e.g., comparison of depth of points in an image), a few sample input-output pairs (e.g., set of images with labeled points and which point is closest to camera in each), and (optionally) resource and/or performance constraints (e.g., compute limit), MMFactory can suggest a diverse pool of programmatic solutions by instantiating and combining visual, LLM and VLM tools from its repository. In addition to synthesizing these solutions, MMFactory also proposes metrics and benchmarks performance / resource characteristics, allowing users to pick a solution that meets their unique design constraints. From the technical perspective, we also introduced a committee-based solution proposer that leverages multi-agent LLM conversation to generate executable,"}, {"title": null, "content": "diverse, universal, and robust solutions for the user.\nNotably, unpublished and concurrent work of [41] also explores the idea of routing, but mainly for choosing a single (most accurate) among the K possible LLM / VLM models (see Fig. 1c). MMFactory framework is considerably more general and provides user with family of solutions and their performance characterization. In addition, our solutions, similar to visual programming [22, 49], are drawn from an exponential set of tools that can work in tandem with one another. Further, the fact that our framework proposes solutions that contain a single executable code path, makes them much easier to deploy.\nOur contributions are multiple fold. First, to the best of our knowledge, this work is the first to explore routing across vision, language, and vision-language models. Second, our propose framework can provide multiple solutions in a solution pool for user-defined tasks and constraints. Third, we introduce a novel approach that combines routing and a multi-agent solution proposer to deliver robust results. Fourth, unlike existing approaches, our proposed framework solves all instances of a user-defined task collectively, rather than generating separate solutions for each instance. Fifth, experiments on two benchmarks demonstrate that our framework outperforms the state-of-the-art."}, {"title": "2. Related works", "content": "Multimodal Large Language Models. Building on the recent success of large language models (LLMs) [4, 20, 42, 53], research trends have shifted toward enhancing these LLMs with multi-modal capabilities. Some of these MLLMs [34, 37, 39, 62] are created for general purpose, while others are designed for specific tasks, including coding [21, 27, 46], video understanding [13, 61], 3D [11, 24, 64], audio or speech [8, 15, 18], math [6, 54], scientific chart [17, 23, 40], and robotics [9, 60], which has demonstrated promising results. However, language-based models alone can't handle complex tasks very well. Multi-modal models, which combine text and images, also face challenges, like misinterpreting context when information is split across text and visuals. They might connect unrelated details or miss important clues, leading to errors. Therefore, researchers are exploring tools and interactive systems to improve their understanding of multimodal information.\nVisual programming, LLMs with tools and Routing. As humans, when we face complex tasks, we decompose them into subtasks to understand them better or use tools to make them simpler. These concepts have been extended to neural networks, where previous works [3, 28] suggest that complex vision tasks are fundamentally compositional and can be divided into atomic perceptual units. Following this concept, visual programming [22, 49] and LLMs with tool [35, 44, 45] become prominent research"}, {"title": "3. Methodology", "content": "3.1. Overview of MMFactory\nWe introduce MMFactory, an universal framework designed not only to propose programmatic solutions based on user-defined task examples but also to provide estimated performance and time costs for each solution, allowing users to make informed choices. This framework functions like a solution search engine and interface across various models, enabling access to models for task-solving without requiring extensive background knowledge. MMFactory has several unique features. In addition to proposing multiple solutions with estimated performance and cost plots, the solutions generated are general and can be applied across all examples within the specified task. Specifically, MMFactory consists of two key components: the Solution Router and Metric Router. The former can generate multiple general solutions for solving the task, while"}, {"title": null, "content": "the later evaluate the solutions to estimate their performance and computation cost. The framework is illustrated in Fig. 2.1 Furthermore, we leverage advanced multimodal LLMs (e.g., GPT) as the solution and metric routers. For better understanding, we first introduce the necessary notations, followed by a detailed explanation of these two modules in the following sections.\nProblem Formulation and Notations. As shown in Fig. 2, given a user-specified task with N instances, we represent these instances as a set \\(O = \\{o_1, o_2,..., o_n, o_{n+1},..., o_N\\}\\), where \\(o_i = (z_i, q_i, a_i)\\) and \\(O_i = (z_i, q_i)\\), with \\(z_i\\), \\(q_i\\), and \\(a_i\\) denoting the image set, task request prompt, and ground-truth answer for that instance, respectively. Note that only n instances have ground-truth answers, referred to as example instances \\(O_{ex} = \\{o_1, o_2,..., o_n\\}\\), where \\(n \\ll N\\). The goal of the Solution Router, \\(R_s\\), is to propose programmatic solutions for the task based on the example instances so that the answers for all instances can be inferred by leveraging the proposed solutions. In practice, together with the example instances and predefined task-agnostic prompts (e.g., model definitions), we construct an input prompt P for \\(R_s\\) to generate a solution pool \\(S = \\{s_1,s_2,...,s_l\\}\\). Note that we set \\(n \\ll N\\) to enable model routing to perform reasoning to obtain the answer rather than simply memorizing the ground truth answers. Once solutions are obtained, the Metric Router, \\(R_M\\), samples a subset with m instances from O to evaluate the performance of each solution in S. This evaluation yields a set \\(E = \\{(p_1, c_1), (p_2, c_2), ..., (p_m, c_m)\\}\\), where \\(p_i\\) and \\(c_i\\) denote the performance and computation cost of the i-th solution in S. Optionally, other metrics can also be logged."}, {"title": "3.2. Inputs Structure for Solution Router", "content": "As mentioned in the previous section, our framework is designed to propose multiple solutions that leverage models in the model pool to solve the task. The challenging aspects of this task is that the router must not only understand the task but also comprehend the details of each model in the pool to ensure correct use in the solution. For such complex task, in addition to the initial task prompt, we have to provide extra details for the router, including definitions of the models in the model pool, a requirements list, in-context examples, and the solution pool. For each task, the input prompt P structure is detailed below (examples can also be found in the supplementary.) consisting of task-agnostic information:\n\u2022 Model definitions Pd: Describes the details of each model in the model pool, including functionality, input arguments, return arguments, and example use cases.\n\u2022 Requirements Pr: A predefined list of requirements for the router to consider when generating solutions.\n\u2022 In-context examples Pe: Following previous work [26], we provide four different output examples as references. Note that the in-context examples are not sampled from the user task O.\n\u2022 Solution pool Ps: Showcases all previously generated solutions (Python code only). If no solution exists, \"EMPTY\" will be displayed.\nand user-specified task-specific instructions:\n\u2022 User-specification Pu: Contains the task definition, example instances Oex sampled from the target task, and (optional) user constraints. Note that the task instances' input includes images. User input is illustrated in Fig. 3."}, {"title": "3.3. Multi-agent solution router", "content": "Taking all the aforementioned information as input, the goal of solution router is to propose novel solutions to solve the task at hand. To achieve that, inspired by multi-agent conversation works [16], we deploy multi-agent system for this complex problem. A conversation is instantiated between two teams: the solution proposer team and the committee team. The proposer team generates ideas and solutions, while the committee team checks for correctness, redundancy, and alignment with requirements, providing feedback. Each team consists of members and a leader. After gathering responses from their members, the leaders of two team exchange responses and collect feedback. By iteratively refining the solution based on this feedback, we achieve robust results. An illustration is provided in Fig. 4. We now detail each component and the conversation process within the multi-agent system. Please refer the supplement for example responses from all the agents in the solution Router.\nSolution Proposing Team. The solution proposing process"}, {"title": null, "content": "involves three key components: (i) analyzing existing solutions and committee feedbacks, (ii) outlining step-by-step high-level instructions, and (iii) developing Python code implementation. This process integrates analysis, creative problem-solving, and rigorous coding. We employ two agents for this purpose: the solution proposer Asp and the solution engineer Ase (see Fig. 4). The solution proposer Asp begins by reviewing existing solutions and generating a novel approach with clear, high-level instructions, resulting in the ANALYSIS and THOUGHT sections of the output. Following this, the solution engineer Ase builds on the instructions provided to produce executable Python code, documented in the ACTION section. Together, the ANALYSIS, THOUGHT, and ACTION sections form a comprehensive solution for further review. Please refer to the supplementary materials for output examples.\nSolution Committee Team. The Solution Committee oversees the quality and robustness of the generated solutions. Its main objectives are to verify that each solution meets predefined requirements, ensure code correctness and functionality, and check for redundancy with existing solutions. A significant challenge is validating code logic beyond mere error-free execution. Therefore, we introduce a code debugger that analyzes intermediate results. Additionally, with the code executor, we can provide the committee with intermediate outputs, enabling a detailed, step-by-step review of the logic. As shown in Fig. 4, we introduce two additional agents with specific roles: a requirement checker and a code checker. The requirement checker evaluates whether the solution aligns with the specified requirements. Meanwhile, the code checker assesses both intermediate and final execution results to verify the accuracy and logical soundness of the code. In the final stage, the repetition checker"}, {"title": null, "content": "ensures that the proposed solution doesn't duplicate any existing solutions in the current solution pool. If the logic of proposed solution matches an existing one, it rejects the solution to avoid redundancy in the solution pool. Please refer to the supplemental for output examples.\nConversation between solution proposer and committee. The interaction between the Solution Proposing and Solution Committee Teams refines solutions iteratively, as depicted in Fig. 4. As mentioned in the prior work [16], a multi-agent conversation framework enhances reasoning and improves solution accuracy. However, excessive iterations can lead to error propagation. To address this, we require each committee member to deliver a decision at every iteration, either accepting or rejecting the solution with feedback. If all committee members accept the solution, the iteration concludes. Recognizing that convergence is sometimes challenging, we enforce a maximum number of iterations. At the end of the conversation, if the final solution is not redundant (as confirmed by the repetition checker), the most recent iteration's solution is preserved."}, {"title": "3.4. Metric Router", "content": "After model routing, we are able to collect a pool of diverse solutions, \\(S = \\{s_1,s_2,...,s_m\\}\\) (see Fig. 2). The evaluation router further assesses these solutions, resulting in a set \\(E = \\{(p_1, c_1), (p_2, c_2), . . ., (p_m, c_m)\\}\\), where \\(p_i\\) and \\(c_i\\) represent the performance and computation cost of the i-th solution in S. We introduce an evaluation router, similar to the solution router, which uses the multimodal LLM's reasoning to select the right metric based on the user's task and the format of ground truth and predictions. Once the metric is chosen, we can proceed with performance testing and evaluation, estimating both the performance and cost of each solution. The user can also supply a custom metric rendering evaluation router unnecessary; however, the choice of the metric may not itself be trivial for a naive user.\nInput Structure. We again use MLLM (i.e., GPT-4) as the router to select metrics for evaluation. Below, we detail the input prompt for the router, comprising of task-agnostic:\n\u2022 Metric Definitions: Provides details for each metric in the metric pool, including use cases, input arguments, return arguments, and examples.\nand user-derived task-specific instructions:\n\u2022 Task Instances: Similar to the solution router, this includes task instructions and n example instances sampled from the target task, along with ground truth answers and predictions from the solutions.\nPerformance and Computation Cost Curve. For each proposed solution, we apply the aforementioned metric routing. Once a metric is selected, we first choose larger test cases from the user-provided task. As shown in Fig. 2, we"}, {"title": "4. Experiments", "content": "Datasets and Evaluation To verify the effectiveness of MMFactory, we conduct experiments on two benchmarks: BLINK [19] and Seedbench [31], and compare our model against previous works. These benchmarks contain various tasks covering visual perception and spatial understanding. BLINK includes 14 visual perception tasks with a total of 3,807 multiple-choice questions, while SeedBench covers 9 classical spatial understanding tasks with a total of 14k image-QA pairs, including scene understanding, instance interaction, and visual reasoning. There are some overlapping tasks between the two benchmarks; however, the main difference is that BLINK focuses on evaluating visual perception, where tasks are designed to be solvable by humans at a glance while hard to answer correctly for MLLMs. In contrast, SeedBench emphasizes models' visual spatial understanding, involving complex tasks with small objects or intricate descriptive prompts. For evaluation, since the tasks in these datasets are single-choice questions, we follow their protocol by using GPT to map the open-form predictions from MLLMs to the fixed set of choices and perform string matching to report accuracy for each task."}, {"title": "4.1. Quantitative Analysis", "content": "In this subsection, we evaluate the effectiveness and performance of our MMFactory. Note that, to ensure a fair comparison with previous SoTA models, we fix the multimodal LLMs to the same ones used in the compared methods for quantitative evaluation. For vision models, we use exactly the same models as those employed in the prior work on Visual Sketchpad [26].\nCan MMFactory propose effective solutions? To verify this point, we conducted experiments on BLINK and SeedBench, reporting performance using three different multimodal LLMs (i.e., LLaVA-7B, LLaVA-13B, and GPT-40) as fixed MLLMs. The results are shown in Tables 1 and 2. Our method demonstrates its ability to propose useful solutions with either comparable or improved performance relative to its own base model. Notably, with the routing approach, very significant performance boosts are observed in certain tasks, such as function correspondence (+15% over GPT-40) and jigsaw solving (+20% over GPT40), spatial understanding (+17% over LLaVA-7B), and jigsaw again (+6% over LLaVA-13B). Consistent performance improve-"}, {"title": "4.2. Qualitative Analysis", "content": "Fig. 5 shows qualitative examples of our proposed MMFactory. It samples a few examples from a given task, defined by the user's constraints and task details (e.g., image and prompt), and passes them to MMFactory. The \"solution proposer\" then generates a pool of robust solutions for the task. Simultaneously, the \u201cmetric router\u201d generates a performance curve showing the trade-off between time cost and accuracy based on selected metrics (e.g. GPTScore). Unlike existing methods, our approach generates a solution pool from which users can choose the best option based on their constraints. Additionally, our framework provides solutions tailored to the entire task, rather than to individual samples. Additional examples are provided in supplement."}, {"title": "4.3. Model Analysis", "content": "Ablation studies of the multi-agent corporation. In the solution router, we leverage multi-agent conversation to im-"}, {"title": null, "content": "prove the quality and robustness of the generated solutions. We conduct ablation studies on the multi-agent component of the proposer to verify this, with the results shown in Tab. 3. Specifically, we run the solution router on the first five tasks (listed in Tab. 1) in the BLINK dataset, with three runs per task, each allowing a max of six conversation iterations. Without the code debugger, the code checker cannot access the intermediate results of the solution, resulting in a significantly performance accuracy drop of 10%. Without the code checker, there is no feedback on execution results, which not only reduces the performance but also substantially increases the error rate during solution execution. Furthermore, after ablating the requirement checker, we observe both performance and solution correctness degrade compared to the full model. Lastly, without the repetition checker, the average number of proposed solutions decreases significantly by 33%, verifying the effectiveness of the repetition checker in enhancing solution diversity.\nRouting time and API calling cost. In our solution router, agents iteratively converse to generate the final solutions. As the number of existing solutions in the pool grows, the router may take more time to propose a novel solution. Therefore, we further investigate the routing time cost with varying numbers of solutions in the pool. The average time cost per solution and per iteration is reported in Fig. 7 (top). We observe that the time cost per solution increases as the number of existing solutions grows. We assume this is due to the increasing complexity of the task, requiring the router to utilize the maximum number of iterations to derive the final solution. On average, it takes approximately 8 minutes to generate a solution. Notably, since the generated solutions are applicable to all samples within a task, we only need to perform solution routing once per task, rather than for each sample. We compare execution and routing costs with Visual Sketchpad in Fig. 7 (bottom). Execution cost refers to the time from input prompt to final answer, while routing cost is the time spent coordinating tools (execution time minus tool-calling time). One can find that with the pre-planned solutions, our execution cost is lower. Addi-"}, {"title": null, "content": "tionally, as the proposed solutions are reusable across all task instances, routing cost per sample is nearly zero, significantly less than the on-line routing in previous work. Furthermore, as we use a GPT model for the solution router, we report the average API cost and compare it with previous work, Visual Sketchpad [26] (see Tab. 4). A key benefit of our approach is that we perform routing only for a few runs, with the produced solution applicable to all samples, significantly reducing the cost. In contrast, Sketchpad requires an API call for every sample, resulting in almost five times the cost of our approach on the BLINK dataset.\nBest answer happen in which run progressive performance analysis. In the solution router, we set a maximum number of conversation iterations for the multi-agent cooperation. As mentioned in previous studies [16], multi-agent conversation or debate can lead to error propagation, reducing performance after multiple iterations. To investigate this, we conducted experiments to analyze performance as the number of iterations increased, with results shown in Fig. 6. Specifically, we randomly selected 10 tasks from the BLINK dataset and ran our solution router to generate solutions, setting the maximum number of iterations to six. As shown in the figure, we observe that solutions with the best performance occur around 2-4 iterations."}, {"title": "5. Conclusion", "content": "Selecting the right multimodal LLM for a task can be difficult, especially without domain-specific knowledge or clear user requirements. In this paper, we present a framework to help users select the most suitable solution from a solution pool for a given tasks based on their specific constraints. Our approach uses a multi-agent debate mechanism to generate robust and well-reasoned solution. Unlike"}]}