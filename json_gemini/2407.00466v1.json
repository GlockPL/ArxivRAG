{"title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science", "authors": ["Xinna Lin", "Siqi Ma", "Junjie Shan", "Xiaojing Zhang", "Shell Xu Hu", "Tiannan Guo", "Stan Z. Li", "Kaicheng Yu"], "abstract": "Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models (LLMs). However, to evaluate such systems, people either rely on direct Question-Answering (QA) to the LLM itself, or in a biomedical experimental manner. How to precisely benchmark biomedical agents from an AI Scientist perspective remains largely unexplored. To this end, we draw inspiration from one most important abilities of scientists, understanding the literature, and introduce BioKGBench. In contrast to traditional evaluation benchmark that only focuses on factual QA, where the LLMs are known to have hallucination issues, we first disentangle \u201cUnderstanding Literature\u201d into two atomic abilities, i) \u201cUnderstanding\u201d the unstructured text from research papers by performing scientific claim verification, and ii) Ability to interact with structured Knowledge-Graph Question-Answering (KGQA) as a form of \"Literature\" grounding. We then formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based Retrieval-Augmented Generation (RAG) to identify the factual errors of existing large-scale knowledge graph databases. We collect over two thousand data for two atomic tasks and 225 high-quality annotated data for the agent task. Surprisingly, we discover that state-of-the-art agents, both daily scenarios and biomedical ones, have either failed or inferior performance on our benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent. On the widely used popular knowledge graph, we discover over 90 factual errors which provide scenarios for agents to make discoveries and demonstrate the effectiveness of our approach. The code and data are available at https://github.com/westlake-autolab/BioKGBench.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are so powerful that they facilitate nearly every aspect of daily life and work right now, even research [1, 2, 3, 4]. Observing their marvelous successes in text generation [5, 6], text summarization [7, 8], and other tasks [9, 10], along with their consistent failures such as hallucinations [11, 12], one can conclude that LLMs are powerful in certain tasks involving large-scale unstructured data like daily text or images, but relatively powerless when dealing with data-hungry scenarios. As such, researchers then construct AI agents [13, 14] assisting LLMs with external tools to extend the capabilities of LLMs. These attempts are fruitful in many fields, including autonomous computer [15], shopping web-agent [16], code developing [17], society simulation [18, 19], etc. A natural subsequent attempt is to develop AI agents to simulate scientists, aiding or even taking over the process of scientific discovery [2].\nPreprint. Under review."}, {"title": "2 Related Work", "content": "Science Agent. The swift progression of large language models (LLMs) has catalyzed the widespread deployment of intelligent agents across diverse fields, notably within the science domain. Notable examples include ChemCrow [34] and Coscientist [35] in the field of chemistry, DoInstruct [21] in ocean science, and GeneGPT [36], Almanac [37], MedAgents [38] in biomedical domain, etc. Among them, biomedical agents, in particular, have garnered significant attention due to its critical importance. Biomedical agents [39] impact areas ranging from hybrid cell simulation [40], the design of cellular circuits [41] to the development of new therapies [42] and so on. We posit that biomedical agents will emerge as a focal point of research. However, the current benchmark in this field remains inadequate. For instance, MedAgents is evaluated in MedQA [20], MedMCQA [43], PubMedQA [44], relying heavily on inherent knowledge of LLMs, which leads to hallucinations easily. Our proposed BioKGBench is a dynamic benchmark that evaluates the capabilities of agents in utilizing external tools and knowledge retrieval, thereby addressing this gap.\nAgent Benchmark. As agents are progressively applied across various domains, the urgency to construct corresponding benchmarks is escalating. Currently, the majority of benchmarks for evaluating agents adopt the approach of evaluating LLM-as-Agent [45], linking LLMs to external frameworks to assess their performance on specific tasks. For instance, AgentBench [45] is a general benchmark for evaluating an agent's reasoning and decision-making capabilities, SWE-bench [46] assesses an agent's proficiency in software engineering, and AgentClinic [47] examines an agent's performance in a simulated clinical environment. However, a benchmark in AI Scientist perspective remains largely unexplored. Our benchmark originates from this perspective, taking the processing and understanding of large-scale data scenarios as the entry point, representing an initial attempt in this direction.\nAgent Integrating LLMs and KGs. The collaborative use of LLM and KG has become one of the leading methodologies in contemporary agent design, aimed at alleviating uncertainties stemming from the intrinsic mechanisms of LLMs [48, 49, 50]. This paradigm not only capitalizes on the generalization ability of LLMs but also employs KGs as an external, trustworthy, and structured data source, thereby achieving reasoning proficiency that strikingly emulates human intellect[48]. For instance, StructGPT [51] boosts an LLM's performance on general questions by tapping into the information from a supplied KG. Similarly, KG-Agent [52] leverages knowledge from KGs, synthesizing instruction data for fine-tuning an open-sourced LLM, thereby achieving competitive performance on general question-answering tasks. However, to our knowledge, while this paradigm has been widely applied to the general question-answering area, its potential remains untapped in the biomedical field. BKGAgent, hence, is poised to fill this gap."}, {"title": "3 BioKGBench", "content": "Here, we present our benchmark in detail. As aforementioned, one key ability of \"AI Scientists\" is to understand domain knowledge. However, current LLM-driven agent systems inevitably suffer from hallucinations as a consequence of the statistical nature of LLMs along with the lack of scientific training data compared to data from daily scenarios. We notice that a recent trend in research is to use AI agents to leverage external tools to address these limitations [34, 21]."}, {"title": "3.1 Atomic Ability", "content": ""}, {"title": "3.1.1 Knowledge Graph Question Answering", "content": "This atomic task in the benchmark is to evaluate the agents' ability to interact with structured Knowledge Graph Question Answering as a grounding of academic literature. Without loss of generality, we choose Clinical Knowledge Graph (CKG) [53] as the source of our data, which is one of the most popular large-scale knowledge graph databases in the biomedical domain. CKG is a knowledge graph database with data imported from diverse biomedical databases, aimed at streamlining automated knowledge discovery through the graph's extensive information.\nAs the original database is unnecessarily large, we focus on a sub-graph to mitigate the challenge while preserving all relevant information.\nStarting from the origin of CKG\u2014protein, we select the sub-graph to contain exactly 12 categories of biological entities, as indicated in Figure 2. Thus, the sub-graph consists of 484,955 entities (nodes) across 12 categories (Biologically defined) and 18,959,943 relationships (edges) of 18 types, with each type consisting of relationships between a unique pair of entity categories.\nAfter the sub-graph is ready, we construct the question set for the Question Answering (QA) database in two steps. We first handcraft question templates by selecting biomedical fields and pinpointing entities and relations in the CKG. Natural language questions were constructed in various formats, ensuring their accuracy through peer reviews and expert consultations. We then expand our dataset with autogenerated questions by matching CKG data to constructed QA templates, resulting in the generation of 698 questions across three reasoning types and 16 question categories (refer to Table 2).\nIn this task, we outfit LLMs with a set of atomic KG-querying tools and ask them to answer biomedical questions by querying the provided KG. The responses will be compared with the gold answers and"}, {"title": "3.1.2 Scientific Claim Verification", "content": "This task is designed to evaluate LLMs' understanding of unstructured text from research papers in a retrieval-augmented generation manner. Following the definition in [54], the task is to identify evidence related to the claim from the research literature and give a verdict of \"Support\u201d, \u201cRefute\", or \u201cNEI\u201d (Not Enough Information) based on it. We reconstruct two high-quality biomedical datasets, PubMedQA [44] and SciFact [54], into one dataset for SCV, yielding a corpus constituted of abstracts derived from 5,664 scholarly articles, alongside a dataset comprising 1,385 biomedical claims, as shown in Table 3."}, {"title": "3.2 Agent Task", "content": "Building upon the atomic abilities, we propose a novel and comprehensive task, KGCheck. This task necessitates the initial application of the tool-query functionality to extract information from the KG. Subsequently, it employs the RAG approach or database access to procure evidence pertaining to the queried information, facilitating a determination of either \u201cSupport\u201d or \u201cRefute\u201d. This methodology enables agents to scrutinize the knowledge encapsulated within a large-scale KG, a venture of particular importance considering the prevalence of inaccuracies within numerous datasets, including prominent ones such as ImageNet [55]."}, {"title": "3.3 BKGAgent: A Simple Baseline", "content": "We propose a biomedical knowledge-graph agent (BKGAgent), as shown in Figure 3. It's a multi-agent framework based on langgraph [56], capable of retrieving information from knowledge graph and cross-validating its correctness with multiple information sources. Our framework is comprised of three agents: the team leader for the progress control, the KG agent for information retrieval from KG, and the validation agent for checking the correctness of the information from KG. This setup simulates the workflow of a human research team, where a leader supervises the assistants' work and makes the final decision based on their feedback. Additionally, the tool executor is solely responsible for executing functions, and is not based on LLMs."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Main Results and Analysis: Atomic Abilities", "content": "The detailed experimental results of atomic abilities evaluation on LLMs are shown in Table 5, and we summarize our key findings as follows:\n\u2022 Disparity between open-source and commercial API models. Commercial API models like GPT-4 and GLM-4 generally outperform open-source models in several key metrics. GPT-4, for example, consistently achieves higher scores in both KGQA and SCV tasks, highlighting the advantage of proprietary training techniques and larger computational resources.\n\u2022 Strong performance of open-source large models. Some large OSS models, such as Llama-3-70B-Instruct, perform competitively, sometimes surpassing API models in specific metrics. Llama-3-70B-Instruct, in particular, excels in KGQA executability, suggesting that optimized training can enable open-source models to rival or exceed commercial counterparts.\n\u2022 Model parameters do not always correlate with better performance. In the OSS (Medium) and OSS (Small) categories, smaller models like Llama-3-8B-Instruct sometimes outperform larger models like Qwen1.5-32B-Chat in SCV tasks, indicating that model architecture, training data quality, and fine-tuning strategies significantly impact performance. Notably, Qwen1.5-14B-Chat outperforms Qwen1.5-32B-Chat in KGQA, suggesting the latter's pre-training may be insufficient.\n\u2022 Domain-specific models lack transferability. DeepSeek-LLM-67B-Chat excells in mathematical problems [61], but underperforms in biomedical-related tasks, highlighting its lack of cross-domain transferability. This suggests that specialization in one area may compromise generalizability.\n\u2022 Inconsistent performance of MoE models. While Mixtral-8x7B-Instruct-v0.1 performs well in both KGQA and SCV tasks, other MoE models like Starling-LM-alpha-8x7B-MoE-GPTQ and Qwen1.5-MoE-A2.7B-Chat show significantly lower scores. This inconsistency suggests that the effectiveness of MoE models heavily depends on the implementation and integration of the expert models. Additionally, Mixtral-8x7B-Instruct-v0.1, though strong in main metrics, struggles with controlling response format, indicating that individual expert models still require improvement.\n\u2022 Biomedical knowledge embedded in model parameters. The new metric \"right quote\" for SCV assesses the alignment of retrieved quotes with ground truth evidence. Some models, such as GLM-4, Qwen1.5-72B-Chat, and Qwen1.5-7B-Chat, exhibit higher accuracy metrics than \"right quote\" metrics. This suggests these models can accurately assess input claims even without sufficient literature evidence, indicating they possess specialized biomedical knowledge.\nWe also conduct an ablation experiment on three scopes of RAG, as shown in Figure 4, where 'all' refers to the abstract of 5,664 articles, 'partial' denotes the 1,888 abstracts containing ground truth evidence of claims, and 'match' corresponds to the abstracts of the ground truth evidence for the claims. Interestingly, we observe an unexpected phenomenon where the model's performance in the 'match' setting only increases in terms of the right quotes metric, while the accuracy metric actually decreases. In the \u2018all' setting, we initially thought that irrelevant literature would introduce interference, but the accuracy metric, on the contrary, increases. This suggests that there is a potential connection"}, {"title": "4.2 Main Results and Analysis: BKGAgent", "content": "The experiment results including both process and final answer are shown in Table 6, where the executability metric of the assistant agent means whether it is correctly activated. The exact match score of the final result agrees with what we observed in the atomic abilities experiments: GPT-4 surpasses Llama-3-70B-Instruct in the final result, which shows the best performance in the atomic abilities experiments. We discovered some interesting phenomena based on the analysis of BKGAgent chat history.\n\u2022 Leader agent performance significantly influences the team behavior. While the behavior of the assistant agents like KG agent can be modified by the leader's instruction, the leader itself lacks action-related feedback from others, meaning that a bad decision made by the leader may lead to a catastrophe. We found four common error cases induced by the leader, as shown in Figure 5. Among these cases, the leader either fails to give effective instructions to team members, becomes trapped in repeated self-talks, or attempts to perform the tasks that are meant for the assistants.\n\u2022 Assistant agents can always select the right tool. As shown in the results, assistant agents's executability is almost the same as its right tool selection rate, which means once the agent is correctly woken up, it has a good chance of selecting the right tool.\n\u2022 Good process results do not necessarily mean a good final result. Surprisingly, the agent based on Llama surpasses the one based on GPT in terms of assistant agents' tool selection. However, the final result is far from satisfactory. We watch for the chat history of the Llama-based agent and find out that while the right answer can be found in the history of the agent in many cases, the team starts chatter instead of sending the result to the user thus leading to a final wrong answer.\n\u2022 Balanced abilities of BKGAgent. We classify our tasks into two categories by the way the agent checks the information from KG, namely the web database (UniProt [65] and STRING [66]) check task and the publication database check task. The performance of them is separately evaluated. Considering the exact match score of the final result, both the GPT-based agent and Llama-based agent show similar performance in both types of tasks, indicating BKGAgent does not show any preference for specific tasks."}, {"title": "4.3 Capability Analysis", "content": "Biomedical agents are not widely employed, as most tasks rely solely on models (refer to Appendix F). Additionally, most agents are primarily focused on QA tasks. For example, MedAgents[67] employs a Multi-disciplinary Collaboration (MC) framework and was tested on nine QA datasets. General agents such as the multi-agent framework AutoGen and the single-agent framework AutoGPT are capable of performing web searches and database retrieval enhancements through tool calls. However, since accessing knowledge graphs and biomedical information is beyond daily usage, and the information directly gained from the web being unreliable, it is challenging to achieve the same level of biomedical information retrieval and verification effectiveness using existing general agent frameworks.\nAs presented in Table 7, our framework encompasses the basic abilities, especially for retrieval in the biomedical domain. The range of our information for KG result validation includes the UniProt"}, {"title": "5 Conclusion", "content": "We present BioKGBench, an interactive benchmark that encompasses the KGCheck task with two atomic capabilities for assessment: KGQA and SCV. KGCheck offers agents a valuable scenario for detecting knowledge hallucination within large-scale data, akin to the experience of researchers making discoveries amidst voluminous literature in the real world. We conduct evaluations of these two atomic capabilities across 13 LLMs and select the top-performing open-source model, Llama-3-70B-Instruct, and API-based model, GPT-4, to construct BKGAgent\u2014a multi-agent system serving as the baseline. Comparisons with existing general and biomedical agents revealed their poor performance due to the absence of certain process capabilities, thereby demonstrating the challenging nature of our benchmark. We expect BioKGBench to serve as a valuable endeavor towards paving the path for biomedical agents to become AI scientists."}, {"title": "Limitations and Future Work", "content": "In KGCheck, we guide agents to identify knowledge-based errors within the KG by providing them with specific instructions. This process involves atomic-level inspections from single nodes to triples, which agents could potentially implement autonomously. Future work will explore how agents can autonomously conduct real-time error detection in large datasets by leveraging logic rules and prior knowledge. Additionally, our case studies reveal that despite agents' current underperformance in KGCheck tasks (as shown in Figure 5), their occasional"}]}