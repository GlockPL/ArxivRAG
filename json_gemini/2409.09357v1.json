{"title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration With Improved Intelligibility", "authors": ["Xiaoyu Liu", "Xu Li", "Joan Serr\u00e0", "Santiago Pascual"], "abstract": "Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.\nIndex Terms-Speech restoration, knowledge distillation.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech restoration (SR) aims at restoring full-band speech with high quality and intelligibility from a corrupted signal [1]\u2013[4]. Compared with conventional speech enhancement (SE) that typically employs discriminative modeling based on regression to remove noise [5] and, at most, reverb [6], [7], SR addresses a diverse set of tasks including those that are generative in nature, such as bandwidth extension, packet loss concealment, etc. For both SR and SE, a common finding is that the improved perceptual quality after processing may not translate to improved intelligibility, typically mea- sured by the word error rate (WER) of automatic speech recognition (ASR) systems, since removing distortions could alter the phonetic content [4], [8]\u2013[10]. Actually, the processed speech may even have a higher WER than that of the corrupted speech [4], [8], [10]. One solution is to condition the model on the text transcription of the corrupted speech, which, however, may not be available during both training and inference [4], [8], [11]. Another approach optimizes the model with an additional ASR-related loss [9], [10], but pre-training the ASR model requires large transcribed datasets.\nIn this paper, we propose MaskSR2 (Fig. 1), which significantly reduces the WER without relying on transcribed data. MaskSR2 is based on our previous work MaskSR, a full-band (44.1 kHz) SR system that holistically performs denoising, dereverberation, declipping, and bandwidth extension under a masked token modeling paradigm [12]. MaskSR2 improves upon MaskSR by introducing se- mantic knowledge distillation (KD) in the speech encoder component. During training, given the STFT of a corrupted speech signal, the speech encoder predicts semantic representations of the target speech, extracted using a pre-trained HuBERT model [13]. This teacher model encodes semantic (phonetic) patterns learned through self- supervised learning (SSL), which removes the need for transcribed audio. The KD is imposed by a loss function upon the speech encoder, jointly optimized with the rest of the system. Meanwhile, the generative model is conditioned on the learned semantic features from the hidden layer of the encoder to predict randomly-masked acoustic tokens of the target speech. During inference, the HuBERT teacher is discarded, thus reducing the compute cost of MaskSR2 to the one of MaskSR (same model capacity and inference time). Iterative sampling is performed on the output distribution to generate the target acoustic tokens, which are then converted to a waveform by a pre-trained audio (de)tokenizer.\nWe get inspiration from previous text-guided speech/audio synthe- sis research, which shows the importance of semantic modeling [14]\u2013 [17]. These systems usually consist of two stages, text-to-semantic and semantic-to-acoustic synthesis, each requiring separate training and iterative inference. For SR, since the corrupted speech provides a stronger condition than a text prompt, we are able to jointly train the speech encoder and the generative model, and then run iterative sampling only on the latter. Another SR work [2] also fuses a speech encoder with a generative model, but the encoder is optimized only on spectral targets, such as the STFT and features derived from it. We show that the semantic KD is an effective choice to improve intelligibility. SELM [18] trains a language model to translate discrete noisy SSL tokens to clean ones, thus performing denoising with lower WER. But this framework requires storing and running a (typically large) SSL model during inference, which is not needed by MaskSR2. Also, the generated speech quality is sensitive to the codebook size of the discrete SSL tokens [18]. We avoid this issue by exploring continuous HuBERT features as the teacher for the speech encoder. Overall, MaskSR2 reduces the WER between 19% and 38% relative to MaskSR, and achieves a competitive WER when compared to strong regression models trained to optimize waveform or spectrum alignment (thus tending to produce lower WER than generative models). MaskSR2 also outperforms various models in terms of quality. Samples are available on our demo page\u00b9."}, {"title": "II. MASKSR2", "content": "MaskSR2 improves the speech intelligibility on top of MaskSR by introducing semantic KD to the speech encoder. Given a speech signal x sampled at 44.1 kHz, distorted by the studied artifacts (including noise, reverb, low bandwidth, and clipping), the speech encoder first computes the power-law compressed magnitude STFT, X0.3, using a window and hop size of 2048 and 512 samples, respectively. Next, a 1025-channel 1-D batch normalization layer normalizes the magnitude of each frequency bin. Then, a fully connected (FC) layer projects the normalized features to d dimensions, and a stack of self-attention transformer blocks learn a sequence of latent embeddings. The output from the last transformer block is used to condition the\n\u00b9https://masksr.github.io/MaskSR2/"}, {"title": "A. Speech Encoder", "content": "semantic KD, the speech encoder also learns robust features from the clean HuBERT targets. To show that such robustness is not the key to the improved intelligibility, we compare semantic KD with regression to the STFT-44.1k and STFT-16k spectral targets. Both are power- law compressed magnitude STFTs Y0.3 for a target speech signal y sampled at 44.1 kHz and 16kHz, respectively. Since the HuBERT targets are extracted from 16 kHz speech, the STFT-16k version forms a more fair comparison. We compute 2048-point STFT for a 44.1 kHz target signal, resulting in 1025 frequency bins, and use the first 372 bins to obtain the 16 kHz target. The same MSE loss and feature normalization are used."}, {"title": "B. Audio Tokenizer and Masked Acoustic Modeling", "content": "These components remain the same as in MaskSR. We use the publicly available pre-trained Descript Audio Codec (DAC) [24] as the audio tokenizer. In the training stage, DAC tokenizes a 44.1 kHz target speech signal to a 9 \u00d7 T codegram using 9 residual vector quantizers, each with a codebook size 1024. The frame spacing of the codegram is consistent with that of the speech encoder representation of the corrupted speech. At inference time, DAC converts codegrams to waveforms. DAC is pre-trained to accurately reconstruct the unquantized waveform. Thus, the discrete DAC tokens encode acoustic details of the target speech.\nThe generative model leverages the MaskGIT paradigm [25]. During training, a random subset of the tokens in the target codegram are masked by a special token, and 9 embedding tables embed the 9 codebook sequences, respectively, with the resulting embeddings summed to a Tx d tensor. This tensor is also summed with the learned semantic features from the last transformer block of the speech encoder. We then process the aggregated sequence with a stack of self-attention transformer blocks. An output layer with 9 1024- channel softmax classifiers predicts the logit scores of the tokens. The speech encoder and the MaskGIT are jointly optimized by the unweighted sum of the semantic KD loss (depending on the variants in Sec. II-A) and the cross-entropy loss on the masked positions.\nDuring inference, starting from a fully masked codegram, the target codegram is iteratively synthesized. In each iteration, given the tokens generated from previous iterations, we sample from the output distributions in all the masked positions to unmask these tokens, and we re-mask the sampled tokens with low logit scores. The percentage of re-masking is controlled by a cosine schedule. Gaussian noise is added to the logit scores before ranking them to prevent"}, {"title": "III. EXPERIMENTAL SETUP", "content": "Training set - About 800 hours of clean speech is used, including the 'read speech' subset provided by the 2022 DNS Challenge [28], VCTK [29], and AISHELL-1 [30]. We use 181 hours of noise and 60k room impulse responses (RIRs), also provided by [28]. All data are recorded with 48 kHz or 44.1 kHz sampling rates, and we downsample the 48 kHz data to 44.1 kHz. The corrupted speech considering noise, reverb, low bandwidth, and clipping is created on the fly using the open-source data pipeline provided by Voice- Fixer [1], resulting in distorted samples with an SNR in [\u22125, 20] dB, a bandwidth between [1, 22.05] kHz, and clipped between [0.1,0.5]. We pre-extract from the target speech the DAC codegram and the HuBERT targets before the online data creation for efficient training.\nFull-band test set To create a 44.1 kHz test set with transcripts, we simulate 1000 samples considering all 4 studied artifacts by distorting speech with a similar pipeline. We use 8 VCTK speakers, noise samples from the VoiceBank-DEMAND dataset [31], and simulated RIRs created by the WHAMR! scripts and room settings [32]. All the test speech, noise, and RIRs are unseen during training.\nWide-band test sets To evaluate WER on more challenging sen- tences with a larger vocabulary and longer duration, we simulate 800"}, {"title": "A. Datasets", "content": "samples using 40 speakers in the LibriSpeech test-clean dataset [19]. We also consider the public 2020 DNS Challenge test set [33]. All the wide-band test sets are sampled at 16kHz, and to fairly compare with denoising-only models, we consider the test sets with mainly additive noise (without noticeable reverb). We downsample the full- band output to 16kHz to compare with other models."}, {"title": "B. Implementation Details", "content": "We first train small MaskSR-S and MaskSR2-S. There are 6 and 8 transformer blocks in the speech encoder and the generative model, respectively, each with dimension d = 512 split across 16 attention heads, an MLP with a hidden dimension of 4d, and pre-norm. Sinusoidal positional encoding is added to both the speech encoder and the generative model inputs. Then, we train large MaskSR-L and MaskSR2-L with d = 1024 and increase the number of transformer blocks in the generative model to 12. All models are trained on 4 sec speech segments for 800 k steps on 8 V100 GPUs with a learning rate of 0.0001 using the Adam optimizer. We use a batch size 128 and 256 for the small and large models, respectively. During inference, we decode each non-overlapping 4 sec window with 20 iterations."}, {"title": "C. Baseline Models", "content": "Full-band models We consider 2 full-band models: VoiceFixer [1] and DeepFilterNet3 (DFNet3) [34]. VoiceFixer is a representative GAN-based SR model targeting at the same 4 studied distortions. DFNet3 is a strong regression model trained to remove noise and (a small amount of) reverb and clipping.\nWide-band models We also consider various models specialized in denoising on the 16 kHz test sets, which contain mainly additive noise. These models include regression-based DEMUCS [6] and FRCRN [5], diffusion models SGMSE [35] and StoRM [36], and language model SELM [18].\nUnprocessed, Target, Target-DAC - These refer to the corrupted speech, target speech, and DAC-processed target speech, respectively. Target-DAC is an upper bound for MaskSR and MaskSR2.\nMost of the models (except for VoiceFixer) are trained on datasets comparable to ours, with the majority of the data based on DNS Challenge and VCTK. Thus, we either use their reported results or run"}, {"title": "D. Evaluation Metrics", "content": "Quality Standard metrics such as PESQ and SI-SNR may not properly capture the quality of generative models due to a lack of waveform alignment [37]. Instead, we use the public DNSMOS [38] and our in-house SESQA [39] as reference-free perceptual quality estimators, capable of evaluating generative models in previous works targeting at removing similar distortions [2], [12], [18]. We also adopt the open-source log-spectral distance (LSD) [1] as a common metric to measure bandwidth extension. SESQA and LSD work with full-band speech and DNSMOS works with wide-band speech. We resample the processed speech from various models if necessary.\nIntelligibility We rely on the WER obtained by a publicly available ASR model\u2074 to measure the speech intelligibility.\nSpeaker Similarity We also measure the speaker cosine similarity (Spk Sim) between the target and the processed speech. We use the public WeSpeaker [40] to extract the speaker embeddings.\nSubjective Listening 13 expert listeners rate the quality of the generated speech from anonymized full-band systems on a 1-5 scale considering the 4 studied distortions (without assessing intelligibility due to the commonly used WER). We report the mean opinion scores (MOS) based on 40 samples from the VCTK test set.\nOn the full-band VCTK test set consisting of all the studied distortions, we use all the metrics to evaluate the speech quality and intelligibility. On the wide-band test sets that contain mainly additive noise, we remove SESQA and LSD since we do not target at bandwidth extension (measured by both) and declipping (measured by SESQA). On the DNS test sets we also remove WER or additionally, speaker similarity, depending on the available ground truth."}, {"title": "IV. RESULTS", "content": "Table I shows that DFNet3, VoiceFixer, and MaskSR-S (without guidance) obtain worse WER than that of the corrupted speech\n\u2074https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge"}, {"title": "A. Full-band Speech Restoration", "content": "despite of the improved quality, illustrating the challenge to im- prove both aspects. The classifier-free guidance generally improves the MaskSR-S results, especially the WER and speaker similarity. Semantic KD substantially improves WER further. We also see that KD with the HuBERT targets is superior to STFT targets, showing the key contribution of the semantic KD relative to learning robust spectral features. The Avg-feature provides the best performance (default option hereafter), showing the benefits of using the complete information from all HuBERT layers. The resulting MaskSR2-S reduces the WER by 37.9% relative to MaskSR-S. Finally, scaling MaskSR-S to MaskSR-L greatly reduces WER, and MaskSR2-L further achieves 32.9% WER reduction relative to MaskSR-L. No- ticeably, the semantic KD also generally improves other aspects of the generated speech compared with MaskSR.\nTable II reports the subjective listening scores based on the 40 VCTK test samples with the 4 blended distortions. MaskSR2-L outperforms MaskSR-L, nearly achieving the target quality. This shows the effectiveness of the semantic KD on the quality aspect of the generated speech. It also significantly outperforms other models."}, {"title": "B. Wide-band Denoising", "content": "On the LibriSpeech test set that contains longer sentences built upon a much larger vocabulary than that of the VCTK test set (Table III), MaskSR2-S obtains a lower WER by 18.9% relative to MaskSR-S, showing the importance of the semantic KD. MaskSR2-L further achieves a WER lower than DEMUCS, thus yielding compet- itive performance compared with strong regression models trained to optimize the waveform or spectrum alignment. For speech quality, on both the LibriSpeech and the DNS test sets which contain mainly additive noise (Tables III and IV, respectively), MaskSR2 still provides superior quality. This is despite the fact that MaskSR2 is capable of removing a diverse set of distortions and thus being less specialized."}, {"title": "V. CONCLUSION", "content": "In this work, we proposed MaskSR2, an improved generative framework based on MaskSR for full-band speech restoration with improved intelligibility and quality. MaskSR2 combines semantic KD and acoustic language modeling. Compared with MaskSR, the text-free semantic KD based on SSL greatly reduces the WER of the generated speech without increasing the model size and infer- ence time. MaskSR2 also achieves competitive WER and superior quality compared to several other models. To further improve the performance, we will explore more powerful SSL models [21] and a multitask speech encoder [2] that learns from various teacher models."}, {"title": "A. Embedding Table Initialization", "content": "DAC consists of 9 residual vector quantization blocks, each formed by an input FC layer that projects a 1024-dim feature vector to 8 chan- nels, an embedding table with 1024 8-dim entries that quantizes the projected features, and an output FC layer that projects the quantized centroids back to 1024 channels. Accordingly, each embedding block in the language model also consists of an embedding table and an output FC layer with the same structure as the corresponding DAC vector quantization block (without the input projection). We find that initializing the embedding tables and the output projection weights and biases from the corresponding DAC pre-trained components improves the convergence speed to a lower loss when training MaskSR and MaskSR2. The masked token embedding (the 1025th entry) is initialized from scratch, and we take the first d channels after the output projection if d < 1024."}, {"title": "B. Span Masking", "content": "To further reduce the WER, we also explored the span masking mechanism, which is widely used in masking-based SSL models [13], [41]. By masking a contiguous span of tokens (rather than single tokens) in a sequence, the model has to learn the long term depen- dency between the masked and unmasked tokens (typically apart by a couple of phonemes) in order to predict the masked phonetic content. Therefore, we hypothesize that span masking could be helpful to further reduce the WER of the generated speech.\nInspired by MAGNET [42] which shows the effectiveness of span masking in music synthesis, we extend its masking strategy to our use case. For each training sample, MAGNET randomly selects a codebook sequence k \u2208 {1...9} (if DAC is used), and puts spans of masks in this codebook sequence to achieve a given target mask ratio 0 <rk < 1. Meanwhile, it does not mask the lower level codebooks, and masks all the higher level codebook sequences. The model is trained to predict the masked tokens only in codebook k.\nDifferent than the hierarchical codebook modeling in MAGNET, we aim to achieve a global mask ratio rg by masking all codebooks with spans. Therefore, given a 9 \u00d7 T codegram, we first create a 9 \u00d7 T token level binary tensor that randomly masks 9.Trg positions in the entire codegram (but without applying this mask at this point). Then, we obtain the codebook level mask ratio rk by counting the number of masked positions in each codebook sequence. Finally, given rk, T, and the pre-defined span length l, we adopt the method in MAGNET to compute the number of spans in each codebook sequence to achieve rk, and mask the spans of tokens. The rest of"}, {"title": "APPENDIX", "content": "the training pipeline is identical to that described in Sec. II-B. The inference is also based on spans. In each iteration, after obtaining the token level logit scores predicted by the language model, we compute the span level scores by taking the maximum token score in each non-overlapping span, then add Gaussian noise and rank the span level scores to re-mask the spans with low confidence.\nTable V reports the results on the full-band VCTK test set. We do not observe consistent WER reduction using span masking. For the L9-K500 setting, span masking with a span length of 5 frames brings 10.1% WER reduction relative to the token level masking equivalent to using a span length of 1 frame. But for the Avg-feature case, we do not obtain WER improvements after trying various span lengths. We also observe that span masking provides comparable speech quality to that of the token level masking. We will further explore improvements to the masking strategy in our future work."}]}