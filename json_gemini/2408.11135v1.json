{"title": "MS\u00b3D: A RG Flow-Based Regularization for GAN Training with Limited Data", "authors": ["Jian Wang", "Xin Lan", "Yuxin Tian", "Jiancheng Lv"], "abstract": "Generative adversarial networks (GANs) have made impressive advances in image generation, but they often require large-scale training data to avoid degradation caused by discriminator overfitting. To tackle this issue, we investigate the challenge of training GANs with limited data, and propose a novel regularization method based on the idea of renormalization group (RG) in physics.We observe that in the limited data setting, the gradient pattern that the generator obtains from the discriminator becomes more aggregated over time. In RG context, this aggregated pattern exhibits a high discrepancy from its coarse-grained versions, which implies a high-capacity and sensitive system, prone to overfitting and collapse. To address this problem, we introduce a multi-scale structural self-dissimilarity (MS3D) regularization, which constrains the gradient field to have a consistent pattern across different scales, thereby fostering a more redundant and robust system. We show that our method can effectively enhance the performance and stability of GANs under limited data scenarios, and even allow them to generate high-quality images with very few data.", "sections": [{"title": "1. Introduction", "content": "The challenge of training GANs with limited data has garnered increasing attention within the research community. Numerous studies (Zhao et al., 2020a; Karras et al., 2020a; Jiang et al., 2021; Tseng et al., 2021; Fang et al., 2022; Cui et al., 2023) have reached a consensus: insufficient data often leads to overfitting in the discriminator. This overfitting results in a lack of meaningful dynamic guidance for the generator, ultimately causing its performance to degrade.\nData augmentation, a standard solution to prevent overfitting in deep learning (Shorten & Khoshgoftaar, 2019), has been widely applied to GAN training in limited data scenarios. Traditional (Zhao et al., 2020b; Tran et al., 2021) and differentiable (Zhao et al., 2020a) data augmentation techniques for both real and generated images, as well as various adaptive augmentation strategies (Karras et al., 2020a), have shown good performance on several standard benchmarks. However, the effectiveness of data augmentation heavily depends on specific handcrafted types or a costly search process, which limits its generality (Zhao et al., 2020b; Karras et al., 2020a). Additionally, data augmentation addresses data deficiency by enhancing the quantity and diversity of samples without providing a deeper understanding of the internal dynamics of GANs.\nIn this paper, we address the problem of GAN deterioration from a novel perspective and introduce a model regularization method. Unlike most existing regularization techniques (Tseng et al., 2021; Cui et al., 2022; 2023) that focus on improving the discriminator's generalization across different data distributions to avoid overfitting, our approach explores the intrinsic properties of neural networks to uncover potential clues. Specifically, we observe that in settings with limited data, the gradients provided by the discriminator, i.e., $\u2207_x f(x; )$, gradually exhibit an aggregation pattern. This pattern indicates that the discriminator concentrates its attention on a small portion of the input image, rather than capturing comprehensive details. This phenomenon, which we term the perceptual narrowing phenomenon, varies in degree across different GANs and tasks and accompanies overfitting.\nTo understand this phenomenon, here, we draw inspiration from the renormalization group (RG) concept in physics (Kadanoff, 1966; Wilson, 1971), which progressively separates coarse-grained statistics from fine-grained statistics through local transformations at different scales. We apply this idea to analyze the gradient field $\u2207_x f(x; )$ at different scales. Unlike scatter patterns, the aggregated pattern shows significant divergence from its coarse-grained versions, as depicted in Fig. 1. This self-dissimilarity (SD)\u00b9 can be seen as the system's unique \"signature\" (Wolpert & Macready, 2018; 2007), revealing how information processing changes across different scales in the system (Jacobs & Jacobs, 1992; Wolpert & Macready, 2007). A high SD indicates that the system is efficient, encoding substantial processing into its dynamics, but also sensitive or unstable, where a small change can cause large dynamic shifts. Conversely, a low SD is associated with robustness, where a system can maintain functionality despite disturbances, being less efficient but more reliable due to built-in redundancies. The latter is more desirable for GAN training.\nBased on this analysis, we introduce a multi-scale structural self-dissimilarity (MS3D) regularization based on RG flow. By repeatedly applying RG transformations, we generate a series of coarse-grained versions at different scales. Along the RG flow, we compute SD at each scale and combine them to obtain MS3D. This regularization enforces the gradient field to maintain a similar pattern or structure across scales, promoting a more redundant and robust feedback system for the generator. These properties help avoid overfitting and collapse in GAN training with limited data. Crucially, the proposed MS3D computation method is differentiable and can be readily implemented using popular deep learning frameworks like PyTorch (Paszke et al., 2019) and TensorFlow (Abadi et al., 2016). We verify the effectiveness of our method on various GANs and datasets. The results demonstrate that it improves GAN performance under limited data conditions in terms of generalization and stability. Notably, our method is orthogonal to data augmentation methods and other model constraint techniques, and can be integrated with them to further enhance GAN performance with very small datasets. Our work provides a new perspective on GAN training with limited data and offers a novel regularization method to improve GAN performance in this challenging scenario."}, {"title": "2. Related Work", "content": "Generative adversarial networks. Generative adversarial networks (GANs) (Goodfellow et al., 2014) have made significant advancements in generating high-quality and diverse images. This progress is attributed to the development of more robust objective functions (Arjovsky et al., 2017; Gulrajani et al., 2017; Zhao et al., 2017; Mao et al., 2017; Song & Ermon, 2020), advanced architectures (Miyato et al., 2018; Miyato & Koyama, 2018; Zhang et al., 2019), and effective training strategies (Denton et al., 2015; Zhang et al., 2017; Karras et al., 2018; Liu et al., 2020). Notable examples include BigGAN (Brock et al., 2019) and StyleGAN (Karras et al., 2019; 2020b), which are capable of generating high-resolution images with rich details and diverse styles. This paper focuses on the challenges and solutions for training GANs with limited data.\nTraining GANs under limited data setting. Training GANs with limited data presents significant challenges, as the discriminator may overfit, leading to degraded generated samples (Webster et al., 2019; Gulrajani et al., 2019). One common approach to mitigate this issue is data augmentation (Karras et al., 2020a; Tran et al., 2020; Zhang et al., 2020; Zhao et al., 2020a; 2021; 2020b), which enriches the data distribution by applying transformations to the original samples. However, data augmentation is not straightforward for GANs, as it can alter the target distribution or introduce artifacts (augmentation leaking). Recent methods have been designed to address these challenges, such as differentiable augmentation (Zhao et al., 2020a), adaptive augmentation (Karras et al., 2020a), and generative augmentation (Zhao et al., 2020b).\nAnother approach is model regularization, which prevents the discriminator from overfitting by imposing constraints or penalties on its parameters or outputs. While model regularization is commonly used to stabilize GAN training and prevent mode collapse, it is particularly effective in data-limited settings where overfitting is more severe. Techniques include adding noise to the discriminator's inputs or outputs (Arjovsky & Bottou, 2017; S\u00f8nderby et al., 2017; Jenni & Favaro, 2019), applying gradient penalties (Gulrajani et al., 2017; Mescheder et al., 2018), using spectral normalization (Miyato et al., 2018; Miyato & Koyama, 2018), and adding consistency loss (Zhang et al., 2020). Recent innovations, such as the LC regularization term (Tseng et al., 2021), which modulates the discriminator's evaluations using two exponential moving average variables and connects to LeCam divergence (Cam, 1986), have shown significant benefits. DigGAN (Fang et al., 2022) addresses gradient discrepancies between real and generated images, improving GAN performance. Additionally, leveraging external knowledge by using pre-trained models as additional discriminators (Kumari et al., 2022), aligning discriminator features"}, {"title": "3. Methodology", "content": "3.1. Generative Adversarial Networks\nGenerative adversarial networks (GANs) (Goodfellow et al., 2014) are a class of generative models designed to synthesize realistic data samples from a latent noise vector $z$. GANs consist of two neural networks: a generator $g(\u00b7; \u03b8)$ that transforms the noise vector into a data sample, and a discriminator $f(\u00b7; \u03d5)$ that distinguishes between real and generated samples. These networks are trained adversarially; the generator aims to produce samples that mimic the real data distribution, while the discriminator aims to accurately classify samples as real or fake. The training objective of GANs is formulated as a minimax game:\n$min_{\u03b8} max_{\u03a6} E_{x} [log f(x; \u03d5)]+E_{z}[log(1-f(g(z; \u03b8); \u03d5))].$ (1)\nThe optimal solution is a Nash equilibrium where the generator produces samples indistinguishable from real data, and the discriminator assigns a probability of 0.5 to all samples."}, {"title": "3.2. Perceptual Narrowing Phenomenon", "content": "Discriminator overfitting is a critical issue in GAN training with limited data, leading to a degradation in the quality of generated images (Zhao et al., 2020a; Karras et al., 2020a; Jiang et al., 2021; Tseng et al., 2021). This phenomenon is demonstrated in our experiments with StyleGAN2 (Karras et al., 2020b) on OxfordDog dataset (Parkhi et al., 2012), as shown in Fig. 2(a). The discriminator becomes increasingly confident about the real images from the training set while becoming less confident about real images from the validation set, leading to a deterioration in Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017) over time.\nAdditionally, we observe that the gradient field $\u2207_x f(x; \u03d5)$ of the discriminator with respect to the input becomes more aggregated over time, as shown in Fig. 3(a). Intuitively, this aggregation may provide fragmented guidance to the generator. We refer to this as the perceptual narrowing phenomenon. To quantify this, we devise a metric that counts the connected regions $N_{agg}$ by assigning 1 to gradient values above a threshold and 0 to the rest, followed by a connected component analysis. The ratio of connected regions to the total number of pixels, i.e., $R_{agg} = \\frac{N_{agg}}{HxW}$, indicates the degree of gradient aggregation. Visual representations of connected regions are shown in Fig. 3(a, right).\nExtensive experiments on four small-scale datasets using various divergence measures and GAN architectures (detailed in Section 4) consistently showed that the number of connected regions initially increases but then decreases as training progresses, indicating increasing gradient aggregation (Fig. 2b, left). In contrast, with data augmentation or increased data volume to mitigate overfitting, the number of connected regions remains stable (Fig. 2b, middle and right). These findings suggest a link between overfitting and gradient aggregation in limited data GAN training.\nOur analysis introduces a fresh perspective through the renormalization group (RG) (Kadanoff, 1966; Wilson, 1971), a technique devised to gradually extract the coarser statistical features of a physical system through local transformation. We observe that, in contrast to the dispersed pattern, the aggregated pattern demonstrates a significant divergence from its coarse-grained counterpart, as illustrated in Fig. 1. This self-dissimilarity (SD) reveals that the system processes information distinctively across different scales (Jacobs & Jacobs, 1992; Wolpert & Macready, 2007), implying that the system is capable of encoding extensive information processing into its dynamics. This suggests that the system is both efficient and possesses a high degree of \"plasticity\" (learnability) from a neurological perspective (Hensch, 2004). Nonetheless, it also indicates that the system is sensitive and susceptible to minor disturbances that could lead to notable alterations in its dynamics (Achille et al., 2019). These properties hint at the system's tendency toward overfitting and instability, aligning with our empirical findings. Conversely, a minimal SD points to the system's ability to generalize and maintain stability, which is favorable for GAN training.\nTo verify this point, we employ the Fisher information to examine the properties of deep neural networks during the training process. Although the connection form of weights in neural networks is fixed during training, not all weight connections contribute equally to the final output. We can view $\u03a8(x; \u03c6) \u2266 \u2207_x f(x; \u03c6)$ as a mapping function parameterized by $\u03a6$, encoding the posterior probability $p(y|x; \u03c6)$. The input is the generated or real image, and the output is the gradient of the discriminator with respect to the input, $y = \u2207_x f(x; \u03c6)$. By perturbing the weights and measuring the change in the output distribution using Kullback-Leibler (KL) divergence, we estimate the dependency of the final output on the weights. The second-order Taylor expansion of the KL divergence is:\n$D_{KL} [P(y|x; \u03d5)||p(y|x; \u03c6 + d\u03c6)]$\n$= \u222b p(y|x; \u03c6) log \\frac{[p(y|x; \u03c6 + d\u03c6)]}{p(y|x; \u03c6)} dx$\n$= \\frac{1}{2} d\u03c6^T G d\u03c6.$ (2)\nLemma 3.1. (Amari, 2016) Any standard f-divergence gives the same Riemannian metric G, which is the Fisher information matrix (FIM)\n$G = E_{x,y} [\u2207_\u03c6logp(y|x; \u03c6)\u2207_\u03c6logp(y|x; \u03c6)].$ (3)\nThe FIM serves as a local measure to assess the effect of weight perturbations on outputs, reflecting the stability of the system $\u03a8(x; \u03c6)$. It also represents the strength of effective connectivity in neural networks and indicates their learnability (Kirkpatrick et al., 2016).\nFollowing Achille et al. (2019), we use the diagonal FIM to reduce the computational cost:\n$tr(G) = E_{x,y} [||\u2207_\u03c6log p(y|x; \u03c6)||^2] .$ (4)\nAs illustrated in Fig. 4(a), Fisher information remains high and increases during the later stages of training, implying decreasing system stability and increasing learnability, hence a higher risk of overfitting and collapse. When the aggregation phenomenon is mitigated through augmentation or increased data volume, Fisher information declines during later training stages, as shown in Fig. 4(b). This aligns with our SD analysis and the CR-GAN approach (Zhang et al., 2020), which uses semantically-preserving augmentation to enhance discriminator robustness by penalizing sensitivity to augmented images."}, {"title": "3.3. Multi-scale Structural Self-dissimilarity", "content": "Building upon the previous analysis, we introduce a novel regularization method, termed multi-scale structural self-dissimilarity (MS3D), designed to enhance the performance of GANs when trained with limited data. This method addresses the perceptual narrowing phenomenon by ensuring that the gradient field $\u2207_x(f(x; \u03c6))$ of the discriminator $f(x; \u03c6)$ maintains a consistent pattern across different scales, promoting a more redundant and robust feedback mechanism for the generator.\nFormulation. We quantitatively define MS\u00b3D using the renormalization group (RG) flow concept. Consider the gradient $\u2207_x (f(x; \u03c6))$ of the discriminator with respect to the input $x$, denoted as $\u03a8(x; \u03c6) \u2266 \u2207_x(f(x; \u03c6))$, where $\u03a8(x; \u03c6)$ is parameterized by $\u03a6$ and belongs to the space F of real-valued functions defined on the Euclidean space X. An RG transformation $\u0393$ can be naturally defined for $\u03a8(x; \u03c6)$. In our discrete pixel setting X, $\u0393$ can be implemented using transformations like the Kadanoff block-spin transformation (Kadanoff, 1966) or Gaussian filtering. Applying the RG transformation repeatedly results in a coarse-grained function $\u03a8(x; \u03c6)$, generating the evolution of RG, that is, the RG flow.\nIn the following text, for clarity, let's denote the original system as $\u0393_{o\u2192o}(\u03a8(x; \u03c6)) \u2261 \u03a8(x; \u03c6)$ and the coarse-grained system at scale $s$ as $\u0393_{o\u2192s} (\u03a8(x; \u03c6))$. The RG transformation chain is constructed as $\u0393_{o\u2192s} = \u0393_{s-ds\u2192s} \u25cb \u0393_{o\u2192ds} \u25cb\u00b7\u00b7\u00b7\u25cb \u0393_{o\u2192o}$, where $ds$ represents the scale step, and $\u25cb$ denotes the composition operator. In other words, $\u0393_{o\u2192ds} \u25cb \u0393_{oo}(\u00b7) = \u0393_{o\u2192ds} (\u0393_{oo}(\u00b7))$. For simplicity, we'll use $\u0393_o\u2261 \u0393_{o\u2192o}$, $\u0393_s\u2261 \u0393_{0\u2192s}$, and so on. The coarser-grained version of $\u0393_s(\u03a8(x; \u03c6))$ can be denoted as $\u0393_{s\u2192s+ds} \u25cb \u0393_s(\u03a8(x; \u03c6))$ or simply as $\u0393_{s+ds} (\u03a8(x; \u03c6))$. If there is a difference between the coarse-grained system $\u0393_s(\u03a8(x; \u03c6))$ and its coarser version $\u0393_{s+ds} (\u0393_s(\u03a8(x; \u03c6)))$, we can say that the scale $s$ contributes some information processing to the system. To measure this dissimilarity, we introduce self-dissimilarity (SD) as:\n$D_{\u0393_{s}\u2192\u0393_{s+ds}}$\n$= (\u0393_{s}|\u0393_{s+ds}) - \\frac{1}{2}((\u0393_{s}|\u0393_{s}) + (\u0393_{s+ds}|\u0393_{s+ds}))$\n$= \\frac{1}{2} \u222b_x (\u0393_{s+ds} (\u03a8(x; \u03c6)) \u2013 \u0393_{s}(\u03a8(x; \u03c6)))^2 dx,$ (5)\nwhere $(\u0393_0|\u0393_1) = \u222b_x \u0393_0\u0393_1dx$ represents the overlap between two scales. In the context of Kadanoff decimation, SD is calculated by $|((\u0393_s|\u0393_s) \u2013 (\u0393_{s+ds}|\u0393_{s+ds}))|$ (See the proof in Appendix A). Along the RG flow, the multi-scale structural self-dissimilarity (MSD) is defined as the integration of SD across scales:\n$D_{\u0393_{0\u2192s}} = \u2211_{i=0}^{s/ds} Dr_{i\u2192ids}.$ (6)\nImplementation. We calculate the gradient of the discriminator's logits, denoted as $f(x; \u03c6)$, with respect to the input image $x$ using the equation $\u03a8(x; \u03c6) = \\frac{df(x; \u03c6)}{dx}$. Here, $x$ is an image in the real-valued space $R^{h\u00d7w\u00d7c}$, where $h$, $w$, and $c$ represent the height, width, and number of channels, respectively. The gradient field $\u03a8(x; \u03c6) : R^{h\u00d7w\u00d7c} \u2192 R^{h\u00d7w\u00d7c}$ is then transformed into a square matrix $\u03a8(x; \u03c6) : R^{L\u00d7L} \u2192 R^{L\u00d7L}$, where $L = \u221a{h \u00d7 w \u00d7 c}$. To ensure that the matrix is square, we apply zero-padding to $\u03a8(x; \u03c6)$, resulting in a square matrix $\u03a8(x; \u03c6)$ of size $L \u00d7 L$.\nFor the real function $\u03a8(\u00b7; \u03c6)$ and the input image $x_0$, we normalize the output matrix $\u03a8(x_0; \u03c6)$ to the range [0, 1]:\n$\u03a8(x_0; \u03c6) \u2190 \\frac{\u03a8(x_0; \u03c6)}{max(|\u03a8(x_0; \u03c6)|)}.$ (7)\nWe then use the Kadanoff block-spin transformation to coarse-grain the system, as illustrated in Fig. 5. Although the Kadanoff decimation is the simplest renormalization group (RG) transformation, it yields meaningful and robust outcomes, as verified in our ablation study (detailed in Section 4). During the transformation $\u0393_{s\u2192s+ds}$, the matrix $\u0393_s(\u03a8(x_0; \u03c6))$, with spatial dimensions $L_s \u00d7 L_s$, is tiled by blocks of size $\u03be_s \u00d7 \u03be_s$, where $\u03be_s$ is the coarse-graining factor. In this paper, we set $\u03be_0 = \u03be_1 = \u00b7\u00b7\u00b7 = \u03be_s = 2$. The coarse-grained matrix $\u0393_{s+ds}(\u03a8(x_0; \u03c6))$ is obtained by replacing each block with its average value. This matrix retains the original spatial dimensions $L_{s+ds} = L_s$, but the number of elements is reduced by a factor of $(\u03be_s)^2$. Mathematically, this process is represented as follows:\n${\u0393_{s+ds}(\u03a8(x_0; \u03c6))}_{(i, j)}$\n$= \\frac{1}{(\u03be_s)^2} \u2211_{m=0}^{\u03be_s-1}\u2211_{n=0}^{\u03be_s-1} {\u0393_{s}(\u03a8(x_0; \u03c6))}_{(\\lfloor\\frac{i}{\u03be_s}\\rfloor\u00b7\u03be_s+m, \\lfloor\\frac{j}{\u03be_s}\\rfloor\u00b7\u03be_s+n)}.$ (8)\nwhere $\u230a\u00b7\u230b$ is the floor function, and ${\u0393(\u00b7)}_{(i, j)}$ indicates the element at the i-th row and j-th column of $\u0393(\u00b7)$. Subsequently, the matrix $\u0393_{s+ds}(\u03a8(x_0; \u03c6))$ is tiled by blocks of size $\u03be_{s+ds} \u00d7 \u03be_{s+ds}$, and its coarser-grained version $\u0393_{s+2ds} (\u03a8(x_0; \u03c6))$ is obtained by replacing each block with its average value. This process is repeated until $(\u03be_s)^{t+1} > L$, i.e., the coarse-grained transformation cannot be applied anymore, where t is the number of iterations and $1 < t < log_{\u03be_s} L$. For our purposes, we set $\u03be_s = 2$. To integrate the MS3D regularization within widely-used computational frameworks like PyTorch (Paszke et al., 2019) and TensorFlow (Abadi et al., 2016), we utilize a convolution operation with an average filter of size 2 \u00d7 2 and a stride of 2 to perform the Kadanoff decimation. The coarse-grained output is then scaled back to the original dimensions by a factor of 2. In Appendix B, we present the PyTorch-like pseudo-code of MS3D calculation. The dissimilarity between the two versions is computed by the overlap of the current version and the coarser-grained version (Eq. (5)). We simply implement SD as the mean squared error (MSE). Hence, the MS3D is the cumulative sum of SDs across all scales, formulated as:\n$D_{\u0393_{0\u2192t}} = \u2211_{i=0}^t ||\u0393_i(\u03a8(x_0; \u03c6)) \u2013 \u0393_{i\u2192i+1}(\u03a8(x_0; \u03c6))||^2.$ (9)\nThe loss function for the discriminator is:\n$L_{dis} = E_{x} log f (x; \u03c6) \u2013 E_{z} log(1 \u2212 f(g(z)); \u03c6) + \u03bbE_{Dr_{o\u2192t}}(\u03a8(x; \u03c6))),$ (10)\nwhere $\u03bb$ is the weight of the MS3D regularization term."}]}