{"title": "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time", "authors": ["Yingyu Liang", "Zhizhou Sha", "Zhenmei Shi", "Zhao Song", "Yufa Zhou"], "abstract": "The quadratic computational complexity in the self-attention mechanism of popular transformer architectures poses significant challenges for training and inference, particularly in terms of efficiency and memory requirements. Towards addressing these challenges, this paper introduces a novel fast computation method for gradient calculation in multi-layer transformer models. Our approach enables the computation of gradients for the entire multi-layer transformer model in almost linear time $n^{1+o(1)}$, where $n$ is the input sequence length. This breakthrough significantly reduces the computational bottleneck associated with the traditional quadratic time complexity. Our theory holds for any loss function and maintains a bounded approximation error across the entire model. Furthermore, our analysis can hold when the multi-layer transformer model contains many practical sub-modules, such as residual connection, casual mask, and multi-head attention. By improving the efficiency of gradient computation in large language models, we hope that our work will facilitate the more effective training and deployment of long-context language models based on our theoretical results.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as ChatGPT [SZK+22], GPT-4 [AAA+23], Claude [Ant24], Llama [TLI+23, LT24], and others, have demonstrated immense potential to enhance various aspects of our daily lives, e.g., conversation AI [LCT+24], AI agent [XCG+23, CYL+24], search AI [Ope24], AI assistant [MWY+23, ZKAW23, KHC+24, FJL+24] and many so on. One of the most emergent abilities of LLMs is dealing with long-context information, a format that is crucial for recording material like academic papers, official reports, legal documents, and so on. LLMs have proven adept at tackling long-context tasks, including Retrieval Augmented Generation (RAG) [LPP+20, GXG+23], zero-shot summarization [LSH+23, ZLD+24, CAM24, ZJV+24], and maintaining very long-term conversations [XSW21, XGW+22, MLT+24], and so on. This proficiency has necessitated the development of long-context modeling capabilities within LLMs.\nLLMs are mainly based on Transformer architecture, the key module of which is the self-attention mechanism. In attention computation, we will compute the attention score between each pair of tokens, which is the complexity bottleneck during long context training and inference. In detail, we need to spend $O(n^2d)$ running time for each self-attention block, which is quadratic in $n$, where $n$ is the length of the context token and $d$ is the hidden feature dimension of the model. For example, LLaMA 3.1 405B [LT24], one of the cutting-edge LLMs, supports n =128k and d = 4096 taking 30.84M GPU training hours, which underscores the need for more efficient training processes for such extensive context models. Given the extensive context lengths of LLMs, this quadratic scaling results in several critical challenges: (1) a marked decrease in training efficiency [HLZ+23, LYL+23, HTH+24]; (2) substantial memory requirements to accommodate the large quadratic gradient matrices [LWXZ22, LYG+23]; and (3) significant energy usage, which in turn contributes to higher carbon dioxide emissions [SZM+23, SCZ+24].\nOne seminar work [AS23] showed that the self-attention inference can be approximated in almost linear time. However, this result is for the inference time but does not address the main challenge, which is the expensive computation in the training time. In this work, we address this main challenge, by proving that the quadratic computational complexity in the back-propagation of self-attention can be approximated in almost linear time. This suggests we may be able to save the substantial resources required for training Large Language Models."}, {"title": "1.1 Key background", "content": "We first introduce a basic background, starting with defining the softmax function.\nDefinition 1.1 (Softmax). Let $z \\in \\mathbb{R}^n$. We define Softmax : $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ satisfying\n$$\n\\text{Softmax}(z) := \\exp(z)/\\langle \\exp(z), 1_n \\rangle.\n$$\nHere we apply exp to a vector entry-wise.\nThen, we introduce the definition of a single self-attention layer.\nDefinition 1.2 (Single layer self-attention). Let $n$ be the number of tokens and $d$ be the hidden dimension size. Let $X \\in \\mathbb{R}^{n \\times d}$ denote the input data matrix. Let $Q := XW_Q, K := XW_K, V := XW_V \\in \\mathbb{R}^{n \\times d}$. Let $W := W_QW_K \\in \\mathbb{R}^{d \\times d}$ be the key-query weight matrix, and let $W_V \\in \\mathbb{R}^{d \\times d}$ be the value weight matrix. The self-attention function Attn(Q, K, V) is:\n$$\n\\text{Attn}(Q, K, V) = \\text{Softmax}(Q K^T /\\sqrt{d})V.\n$$\nWe remark that we apply Softmax to each row of the $n \\times n$ matrix."}, {"title": "1.2 Our contributions", "content": "We now state our main contributions as follows:\nTheorem 1.4 (Main result, informal version of Theorem 4.2). Let $n$ be the number of tokens, and $d$ be the hidden dimension size. We assume $d = O(\\log n)$ and each number in matrices can be written using $O(\\log n)$ bits. There exists an algorithm (Algorithm 1) that can compute the gradient of multi-layer self-attention (see also Definition 1.3) in almost linear time $n^{1+o(1)}d = n^{1+o(1)}$, where the approximation error of entire model can be bounded in $1/poly(n)$.\nOur assumption is mild when the context length $n$ is large, as we usually see feature dimension $d$ as a constant, which is also used in [AS23, AS24a]. Our results indicate that large language models (LLMs) can be trained in almost linear time $n^{1+o(1)}$ and maintain a robust approximation guarantee, while the traditional way takes $O(n^2)$ time. This advancement is realized through the application of polynomial kernel approximation [AS23, AS24a]. To be more specific, by leveraging the inherent sparsity within the dense attention matrix, we are able to perform efficient low-rank approximation, thereby significantly accelerating the computation of the dense matrices. Our framework is applicable to any loss function, making it universally applicable. Furthermore, our"}, {"title": "2 Related Work", "content": "Attention mechanism. Attention mechanisms, including self-attention and cross-attention, are pivotal techniques employed in state-of-the-art neural networks. Since it was introduced in [VSP+17], it has gained widespread adoption across various domains. In particular, it is integral to decoder-only LLMS [RWC+19] and the Vision Transformer (ViT) architecture [DBK+20]. The former has been instrumental in the remarkable success of LLMs, while the latter has significantly advanced the field of computer vision, encompassing applications such as image generation [RBL+22, WSD+23, WXZ+24], detection [LMGH22, ZLC+21], segmentation [ZTT+22, TDC+23], and layout generation [GLA+21, CZY23, WCZ+23]. Moreover, attention mechanism can be integrated into multi-modal models [XGH+21, ZHJL24, LSSZ24b, WMS+24], math reasoning [LLS+24a], diffusion models [SSDK+20, LSSZ24c], differential privacy [BEPP22, SSC+22, WCY+23, LSSZ24a, SAMB24] and many other techniques.\nLong-context modeling in LLMs. As LLMs grow in size and capability, in-context learning (ICL) [MLH+22, SWXL24, XSL24] has become a preferred method for directing these models to perform a variety of tasks, as opposed to the resource-intensive process of fine-tuning. Nonetheless,"}, {"title": "3 Preliminary", "content": "We first introduce some basic notation used in this paper in Section 3.1. In Section 3.2, we define the loss function. Then, to better understand our proofs, we provide a closed-form analysis of the three gradient components in a single self-attention transformer layer in Section 3.3."}, {"title": "3.1 Notations", "content": "For any positive integer $n$, we use $[n]$ to denote set $\\{1, 2, \\dots, n\\}$. For two vectors $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^n$, we use $\\langle x, y\\rangle$ to denote the inner product between $x, y$. Namely, $\\langle x,y\\rangle = \\sum_{i=1}^n x_i y_i$. We use $e_i$ to denote a vector where only $i$-th coordinate is 1, and other entries are 0. For each $a, b \\in \\mathbb{R}^n$, we use $a \\odot b \\in \\mathbb{R}^n$ to denote the Hardamard product, i.e. the $i$-th entry of $(a \\odot b)$ is $a_i b_i$ for all $i \\in [n]$. We use $1_n$ to denote a length-$n$ vector where all the entries are ones. We use $||A||_\\infty$ to denote the $l_\\infty$ norm of a matrix $A \\in \\mathbb{R}^{n \\times d}$, i.e. $||A||_\\infty := \\max_{i \\in [n], j \\in [d]} |A_{i,j}|$. We use poly($n$) to denote polynomial time complexity with respective to $n$."}, {"title": "3.2 Loss function", "content": "The loss function is the optimization objective in the training of LLMs, and we define it as follows.\nDefinition 3.1 (Loss function L(X)). For some input matrix $X \\in \\mathbb{R}^{n \\times d}$, we define the one-unit loss function $l(X)_{j,k} : \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}$, for any $j \\in [n], k \\in [d]$. Furthermore, we define the overall loss"}, {"title": "3.3 Close forms of gradient components", "content": "In training large language models (LLMs), updating the model necessitates computing the gradient of weights for every layer. Consequently, it becomes essential to derive the closed-form expressions for all corresponding gradient components with respect to the weights of the query, key, and value matrices in the transformer model. We first define some intermediate variables before detailing these gradient components in each self-attention transformer layer.\nDefinition 3.3 (Intermediate variables $T_i$). Let $m$ denote the number of transformer layers in the model. Let $m$-layer self-attention transformer be defined as Definition 1.3. Let $d$ denote the hidden dimension. Let $n$ denote the sequence length. Let $X \\in \\mathbb{R}^{n \\times d}$ be the input sentence. Let $g_i$ denote components other than self-attention in the $i$-th transformer layer. Let Attn; denote the self-attention module in the $i$-th transformer layer (see also Definition 1.2).\nFor $i \\in \\{0, 1, 2, \\dots, m\\}$, we define $T_i(X) \\in \\mathbb{R}^{n \\times d}$ be the intermediate variable (hidden states) output by $i$-th layer self-attention transformer. Namely, we have\n$$\nT_i(X) = \\begin{cases}\n            g_0(X), & i=0;\\\\\n            (g_i \\circ \\text{Attn}_i)(T_{i-1}(X)), & i \\in [m].\n        \\end{cases}\n$$\nHere, we use $\\circ$ to denote function composition.\nThen, we are ready to introduce the close forms of the three gradient components in a single self-attention transformer layer. Notably, according to the chain rule, the gradient of the $k$-th transformer layer in LLMs depends on the gradient components from the $(k + 1)$-th transformer layer. The gradient can be calculated for every transformer layer by combining the upstream and local gradients. The close forms of the gradients for each layer in multi-layer transformers are formalized in the following lemma (Lemma 3.4)."}, {"title": "4 Main Result", "content": "Now, we present our main findings. We will work through this section in the following order: In Section 4.1, we delineate the computational efficiency of our gradient calculation methods in each single layer. In Section 4.2, we introduce our main theorem (Theorem 4.2) for multi-layer transformer by integrating the preceding results and provide our main algorithm (Algorithm 1). Section 4.3 discusses how we transcend the previous works."}, {"title": "4.1 Fast computing for single layer", "content": "In the case of single-layer attention, we provide our theorem that state the three gradient components can be calculated in almost linear time with negligible error.\nTheorem 4.1 (Single-layer gradient approximation). We assume $d = O(\\log n)$ and each number in matrices can be written using $O(\\log n)$ bits. Let $L(X)$ be defined as Definition 3.1. Suppose we have a single-layer self-attention transformer model ($m = 1$ in Definition 1.3). We can approximate one-layer self-attention for three gradient components, i.e. $\\frac{dL(X)}{dX}$, $\\frac{dL(X)}{dW}$ and $\\frac{dL(X)}{dW_V}$, in $n^{1+o(1)}$ time with $1/poly(n)$ error."}, {"title": "4.2 Fast computing for multi-layer transformers", "content": "Based on the results demonstrated in previous sections, we are ready to introduce our main result: the whole transformer model can be approximated in almost linear time.\nTheorem 4.2 (Main result, formal version of Theorem 1.4). Let $m$ denote the number of transformer layers. We assume $d = O(\\log n)$ and each number in matrices can be written using $O(\\log n)$"}, {"title": "4.3 Beyond the previous works", "content": "Our algorithm exhibits significant advancements over two brilliant related prior studies, namely [AS23] and [AS24a]. In [AS23], the authors proposed an almost linear time algorithm for computing the forward process of the attention mechanism. In contrast, [AS24a] introduced an almost linear time algorithm for the backward of attention mechanism. However, [AS24a] has the following limitations: (1) only computing gradients for a single layer of the attention mechanism, which cannot extend to multiple layers; (2) calculating gradients with respect to a specific loss, namely the $l_2$ loss; (3) computing gradients only for the weight matrix $W_i$ (as defined in Definition 1.2), but ignore other crucial components such as the MLP layer following attention computation and the activation function.\nIn our work, we have the following improvements beyond [AS24a]: (1) we enable almost linear time gradient computation across an entire transformer layer, incorporating both the MLP layer and the activation function; (2) our algorithm supports gradient calculation for any loss function $L(X)$ (see Definition 3.1); (3) we extend the gradient calculation to include not only $W_i$ but also $T_i(X)$ and $W_V$. These advancements collectively demonstrate a substantial leap forward from the methodologies in [AS23] and [AS24a]."}, {"title": "5 Technical Overview", "content": "This section provides a brief overview of the proof techniques used throughout this paper. Section 5.1 delves into intuition under the low-rank approximation applied to the attention matrix $f(X)$. Section 5.2 elaborates on calculating gradients with respect to the intermediate variable (hidden states) $T_i$ in almost linear time. In Section 5.3, we detail the process for computing gradients for the weight matrices $W$ and $W_V$ within transformer layers in almost linear time. Finally, in Section 5.4, we demonstrate how to generalize our findings from single-layer transformers to multi-layer transformers, including an analysis of the runtime and approximation error across the entire model."}, {"title": "5.1 Low-rank approximation for attention matrix", "content": "In this section, we delve into the crucial technique behind our work: the low-rank approximation of the attention matrix, which is achieved through the polynomial method [ACSS20, AA22]. Drawing inspiration from [AS23], the intuition of this approximation lies in the fact that the attention matrix $f(X) \\in \\mathbb{R}^{n \\times n}$ (as defined in Definition 1.2), also referred to as the similarity matrix in attention mechanism, can be effectively approximated by low-rank matrices $U_1, V_1 \\in \\mathbb{R}^{n \\times k_1}$, where $k_1 = n^{o(1)}$. The naive method for calculating the attention matrix $f(X)$ has a time complexity of $O(n^2)$, whereas the input data $X \\in \\mathbb{R}^{n \\times d}$ contains only $d \\cdot n = n^{1+o(1)}$ entries. This discrepancy suggests the potential of using low-rank representations of $f(X)$ to design a fast algorithm.\nAn example of how to use the low-rank representations is the attention forward (Attn(X) := $f(X)V$ in Definition 1.2) as in [AS23]: approximating $f(X)$ along does not lead to fast algorithm, since $U_1V_1^T$ still requires $n \\times n$ entries. But by using the structure of the whole, we can do it faster. By expressing $f(X)$ as $U_1V_1^T$, the attention forward becomes $U_1 V_1^T V$. It is well known that different multiplication sequences can lead to dramatically different numbers of operations required, so the order of matrix multiplications matters. We first perform $V_1^T V \\in \\mathbb{R}^{k_1 \\times d}$ and this cost $O(k_1nd) = n^{1+o(1)}$ time. Then we perform $U_1V_1^TV$ costing $O(nk_1d) = n^{1+o(1)}$ time.\nThis method significantly reduces the computation time of the attention forward from $O(n^2)$ to almost linear time, $n^{1+o(1)}$. Driven by this technique and analyzing the close forms of the gradients, we can extend the acceleration to the gradient of the entire model."}, {"title": "5.2 Accelerating gradient computation of \\(T_i(X)\\)", "content": "Based on the low-rank approximation method mentioned in Section 5.1, we can compute the gradient of $L(X)$ with respect to the intermediate variable $T_i(X)$, which denotes the output of the $i$-th transformer layer. This computation is critical as it enables us to calculate gradients for other gradient components because of the chain rule.\nExtending to any kind of loss function. According to the findings in [DSXY23], the gradient $\\frac{dL(X)}{dT_i(X)}$ can be decomposed into five components, namely $C_6(X), C_7(X), C_8(X), C_2(X), C_4(X)$, as detailed in Lemma D.1. However, the gradient result presented in [DSXY23] is tailored to a specific loss function, the $l_2$ loss, limiting its applicability to a narrow range of scenarios. In this study, we extend the scope of their findings by extending them to apply to any loss function $L(X)$, as defined in Definition 3.1. By incorporating $G_i = \\frac{d \\text{Attn}_i(T_{i-1}(X))}{dT_{i-1}(X)}$, we derive a closed-form expression for the gradient of $L(X)$ with respect to $T_i(X)$, which is detailed in Section D.2.\nAccelerating the gradient computation. To accelerate the gradient computation for $T_i(X)$, we need the matrix form of the gradients, as discussed in Section D. This approach is essential for understanding the underlying mechanisms when applying the low-rank approximation technique in gradient calculations. Subsequently, using that technique, we can accelerate the gradient computation for $\\frac{dL(X)}{dT_i(X)}$ (Lemma 5.1). By individually applying this technique to each of the five terms, we demonstrate that each term can be computed in almost linear time, $n^{1+o(1)}$, as shown in Sections E.1, E.2, E.3, E.4, and E.5.\nThe next step is to aggregate these terms, as described in Section E.6. Since all five terms are $n \\times d$ matrices, the summation of these terms remains almost linear in complexity. Conclusively, we are safe to argue that for any single-layer transformer, the gradient computation with respect to the input tensor can be performed in almost linear time $n^{1+o(1)}$, as stated in Lemma 5.1."}, {"title": "5.3 Accelerating gradient computation of \\(W_i\\) and \\(W_{V_i}\\)", "content": "In Section 5.2, we detailed the fast computation of gradients for intermediate variables $T_i(X)$. Given that $W_i$ is defined as the product $W_Q W_K$ (see Definition 1.2), with $W_Q$ and $W_K$ representing the query and key weight matrices, respectively, the gradients of $W_i$ and $W_{V_i}$ represent all trainable weight matrices in a transformer layer. Consequently, by determining the gradients for $W_i$ and $W_{V_i}$ across each layer, we achieve almost linear time gradient back-propagation throughout multi-layer transformer models.\nFast gradient computation. The prior study in [AS24a] demonstrated that the gradient of $W_i$ can be computed in almost linear time. We extend their findings by adapting their approach to accommodate any loss function $L(X)$ (as defined in Definition 3.1) and further generalize their results to include the gradient computation for both $W_i$ and $W_{V_i}$ in each transformer layer (Lemma 5.2 and 5.3)."}, {"title": "5.4 Accelerating gradient computation for multi-Layer transformers", "content": "In this section, our focus turns to extending the single-layer transformer result from the previous section to a multi-layer transformer."}, {"title": "6 Extension", "content": "Here, we explore potential enhancements to our algorithm. In Section 6.1, we show the potential extension of multi-head attention and residual connections. In Section 6.2, we offer possibilities to incorporate causal masked attention in almost linear time. In Section 6.3, we show our algorithm can accelerate the prompt tuning process. In Section 6.4, we give the possible integration with system-level attention acceleration techniques."}, {"title": "6.1 Multi-head attention and residual connections", "content": "Multi-head attention and residual connections are important components in attention mechanisms. While these components were not involved in our initial analysis for simplicity, incorporating them"}, {"title": "6.2 Causal attention mask", "content": "The causal attention mask is critical to prevent transformers from \"cheating\" during training by ensuring future information is not used. The full-rank characteristic of the causal attention mask poses challenges for low-rank approximations. Nevertheless, we have identified a method to accelerate the computation of causal masked attention by exploiting its inherent properties, as demonstrated in [LLS+24b], remaining almost linear time complexity. A comprehensive explanation is provided in Section B.3."}, {"title": "6.3 Prompt tuning", "content": "Prompt tuning (or prefix learning) is a prevalent approach in parameter-efficient fine-tuning (PEFT), which requires the calculation of gradients on input data $X$. Given our algorithm's ability to compute gradients for intermediate variables $T_i$ in approximately linear time, we can similarly accelerate the gradient computation for input data $X$, thus enhancing the efficiency of the prompt tuning process. Additional details are provided in Section B.5."}, {"title": "6.4 Synergy with system-level attention acceleration", "content": "Many contemporary works focus on system-level acceleration of attention mechanisms, often by leveraging caching and mitigating I/O bottlenecks. Our algorithm has the potential to integrate with such advancements. By combining our theoretical improvements in computation time (from $O(n^2)$ to $n^{1+o(1)}$) with system-level optimizations, the overall efficiency of attention mechanism computation may increase dramatically. We leave the implementation of our method on GPU as future work since there are several coding challenges. More details can be found in Section B.4."}, {"title": "7 Conclusion", "content": "The attention mechanism in transformer models inherently has quadratic time complexity with respect to the input token length. In this work, we propose a novel Algorithm 1, which can approximately train a multi-layer transformer model in almost linear time, introducing only a small error. Moreover, our algorithm is compatible with any loss function, practical sub-modules (residual connection, casual mask, multi-head attention), and gradient-based algorithm and can be seamlessly integrated with other system-level acceleration techniques. While we lack enterprise-scale computational resources for training large language models, our theoretical findings suggest that we can accelerate the training of LLMs in practice."}, {"title": "A More Related Work", "content": "Attention theory. [BCB14] introduced attention mechanisms in NLP, enhancing encoder-decoder architecture with variable-length vectors to improve machine translation. Building on this, [LPM15] developed local and global attention variants, further refining NLP tasks. Attention mechanisms found diverse applications: [XBK+15] applied them to image captioning, [VSP+17]'s Transformer model revolutionized NLP by capturing word relationships, and [VCC+17] incorporated attention into graph neural networks. Recent Large Language Model research has focused extensively on attention computation [DMS23, AS23, ZHDK23, CLP+20, LSZ23, BSZ23, KKL20]. Studies by [ZHDK23, CLP+20, KKL20] use Locality Sensitive Hashing for attention approximation, with [ZHDK23] offering efficient dot-product attention. [BSZ23] and [AS23] explore static and dynamic attention calculations, while [LSZ23] investigates hyperbolic regression regularization. [DMS23] proposes algorithms for reducing attention matrix dimensionality in LLMs. Attention has also been examined from optimization and convergence perspectives [LLR23, GMS23, SZKS21, ZKV+20], investigating word co-occurrence learning [LLR23], regression problems with exponential activation functions [GMS23], attention mechanism evolution during training [SZKS21], and the impact of heavy-tailed noise on stochastic gradient descent [ZKV+20]. Theoretical explorations of attention variants include quantum attention [GSYZ23], tensor attention [AS24b, LSSZ24b], and differentially private attention [LSSZ24a, GSY23, LSSS24].\nMore methods for model acceleration. Various techniques have been developed for model acceleration. One approach involves modifying model architectures to enable faster inference, such as Mamba [GD23, DG24], Linearizing Transformers [ZBKR24, MVK+24], PolySketchFormer [KMZ23], and the Hopfield Model [HCW+24, HCL+24, WHHL24, XHH+24, HLSL24, WHL+24, HYW+23] and so on. Another line of work is to prune the weights in a neural network to reduce running time and memory consumption [HPTD15, FC18, LAT19, WZG19, BGOFG20, BMBE20, LZ20, TKYG20, CJD+21, HABN+21, HCI+21, JCR+22, FA22, FA23, SLBK24, LLSS24]. In addition, specific techniques have been developed to accelerate LLM generation, including KV-Cache compression [GZL+23, LDLG23, XTC+23, ZSZ+24, LWD+23, DYZ+24, XJD+24] and speculative decoding [LCH+24, SCY+24, ESL+24]."}, {"title": "B Discussion and Extension Details", "content": "In Section B.1, we argue that our framework can easily adapt to the multi-head attention mechanism. In Section B.2, we introduce how to integrate residual connection to our framework. In Section B.3, we detail the integration of the causal attention mask into our algorithm. In Section B.4, we discuss the possibility of the synergy between our theoretical side attention acceleration and the existing system-level attention acceleration mechanism. In Section B.5, we show how to expedite prompt tuning using our results."}, {"title": "B.1 Multi-head attention", "content": "The multi-head attention mechanism was first introduced by [VSP+17]. This innovation allows a token to simultaneously attend to multiple positions within the same layer, thereby enriching the model's capacity for capturing various dependencies. However, this enhanced capability comes with an increase in the size of the attention matrix $f(X)$ from $1 \\times n \\times n$ to $h \\times n \\times n$, where $h$ is the number of attention heads. To mitigate the computational burden, each head's vector is derived by splitting the original vector, reducing the dimensionality of each head to $d_h := d/h$. To summarize, the key distinctions between multi-head and single-head attention are (1) an enlarged attention matrix $f(X)$ and (2) a reduced dimensionality $d_h$ within each attention head.\nEnlarged attention matrix. As previously discussed, the attention matrix's dimensionality increases with the number of heads, $h$. Despite this expansion, the application of the low-rank approximation technique, as outlined in Section 5.1, ensures that the computation time for the attention matrix remains almost linear. Specifically, for a constant number of heads $h$ in the multi-head mechanism, the time complexity for computing $f(X) \\in \\mathbb{R}^{h \\times n \\times n}$ is $h \\cdot n^{1+o(1)} = n^{1+o(1)}$.\nReduced dimensionality. Another differentiating factor of multi-head attention is the lower dimensionality processed by each head, i.e. $d_h := d/h$, compared the full $d$ in single-head attention. This reduction ensures that the gradient computation time does not increase with the introduction of multiple attention heads.\nWe provide comprehensive analysis of the synergy of our algorithm with multi-head attention in Section K. We first prove in Lemma K.2, with the addition of multi-head attention, the gradient over the attention mechanism can be computed in almost linear time. Then, we further prove that for any multi-layer transformer, with multi-head attention, the gradient can be computed in almost linear time as well."}, {"title": "B.2 Residual connection", "content": "Residual connection is a pivotal technique in deep neural network architectures, effectively addressing issues such as vanishing and exploding gradients during training process, and facilitating faster convergence of the model. Residual connection is also integrated into the standard attention mechanism. Formally, given the intermediate variable $T_i(X)$ output by the $i$-th transformer layer as defined in Definition 3.3, we provide the formal definition of residual connection in Definition J.1 and J.2. Since the residual connection only brings an additional add operation to each component and with $T(X)$ belonging to the space $\\mathbb{R}^{n \\times d}$, the residual connection introduces only a marginal computational overhead of $O(nd)$ per layer. Consequently, the total computational cost for each layer is $O(n \\cdot d) + n^{1+o(1)} = n^{1+o(1)}$. Hence, by intuition, the inclusion of residual connections does not compromise the overall complexity of our method."}, {"title": "B.3 Causal attention mask", "content": "In transformer training, attention mask is a crucial component, designed to prevent a given token from attending to future tokens in the sequence. Causal attention mask is a widely used attention mask, which is configured as a lower triangular matrix, where elements on or below the main diagonal are ones, with all other entries being zeros.\nNow we describe how to incorporate this into our algorithm. Let $M \\in \\{0, 1\\}^{n \\times n}$ represent the causal attention mask (see Definition I.2). Let $f(X) := D^{-1}(M \\odot A)$ where $A = \\exp(XW X^T / d)$ and $D := \\text{diag}((M A) \\cdot 1_n)$. Lemma I.1 reveals that A has a low-rank representation given by $UoVo$. Using Lemma I.3, we know $(M (UV)) \\cdot v$ for any vector $v \\in \\mathbb{R}^n$ can be computed in almost linear time.\nTo integrate the causal mask into the gradient computation within each transformer layer, we first find all instances that have the structure of $f(X). H$ or $(f(X) \\odot (UV^T)) \\cdot H$, where $H, U, V$ are low rank matrices. Then, we replace $f(X)$ with $f(X)$ in these instances. More detailed analysis of causal attention can be found in Section I. To be more specific, we group the gradient components for $T_i, W_i, W_{V_i}$ into two categories, one for dot product (Lemma I.7), another for Hadamard product (Lemma I.8). After showing each component can be calculated in almost linear time, the overall gradient computation remains $n^{1+o(1)}$ time. Thus, our framework can seamlessly accommodate causal attention masks."}, {"title": "B.4 System-level attention acceleration", "content": "The attention computing acceleration involves a two-pronged strategy that leverages both system-level improvements (e.g. Flash Attention [DFE+22, Dao23, SBZ+24"}]}