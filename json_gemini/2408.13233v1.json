{"title": "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time", "authors": ["Yingyu Liang", "Zhizhou Sha", "Zhenmei Shi", "Zhao Song", "Yufa Zhou"], "abstract": "The quadratic computational complexity in the self-attention mechanism of popular transformer architectures poses significant challenges for training and inference, particularly in terms of efficiency and memory requirements. Towards addressing these challenges, this paper introduces a novel fast computation method for gradient calculation in multi-layer transformer models. Our approach enables the computation of gradients for the entire multi-layer transformer model in almost linear time $n^{1+o(1)}$, where n is the input sequence length. This breakthrough significantly reduces the computational bottleneck associated with the traditional quadratic time complexity. Our theory holds for any loss function and maintains a bounded approximation error across the entire model. Furthermore, our analysis can hold when the multi-layer transformer model contains many practical sub-modules, such as residual connection, casual mask, and multi-head attention. By improving the efficiency of gradient computation in large language models, we hope that our work will facilitate the more effective training and deployment of long-context language models based on our theoretical results.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs), such as ChatGPT [SZK+22], GPT-4 [AAA+23], Claude [Ant24], Llama [TLI+23, LT24], and others, have demonstrated immense potential to enhance various aspects of our daily lives, e.g., conversation AI [LCT+24], AI agent [XCG+23, CYL+24], search AI [Ope24], AI assistant [MWY+23, ZKAW23, KHC+24, FJL+24] and many so on. One of the most emergent abilities of LLMs is dealing with long-context information, a format that is crucial for recording material like academic papers, official reports, legal documents, and so on. LLMs have proven adept at tackling long-context tasks, including Retrieval Augmented Generation (RAG) [LPP+20, GXG+23], zero-shot summarization [LSH+23, ZLD+24, CAM24, ZJV+24], and maintaining very long-term conversations [XSW21, XGW+22, MLT+24], and so on. This proficiency has necessitated the development of long-context modeling capabilities within LLMs.\nLLMs are mainly based on Transformer architecture, the key module of which is the self-attention mechanism. In attention computation, we will compute the attention score between each pair of tokens, which is the complexity bottleneck during long context training and inference. In detail, we need to spend $O(n^2d)$ running time for each self-attention block, which is quadratic in n, where n is the length of the context token and d is the hidden feature dimension of the model. For example, LLaMA 3.1 405B [LT24], one of the cutting-edge LLMs, supports n =128k and d = 4096 taking 30.84M GPU training hours, which underscores the need for more efficient training processes for such extensive context models. Given the extensive context lengths of LLMs, this quadratic scaling results in several critical challenges: (1) a marked decrease in training efficiency [HLZ+23, LYL+23, HTH+24]; (2) substantial memory requirements to accommodate the large quadratic gradient matrices [LWXZ22, LYG+23]; and (3) significant energy usage, which in turn contributes to higher carbon dioxide emissions [SZM+23, SCZ+24].\nOne seminar work [AS23] showed that the self-attention inference can be approximated in almost linear time. However, this result is for the inference time but does not address the main challenge, which is the expensive computation in the training time. In this work, we address this main challenge, by proving that the quadratic computational complexity in the back-propagation of self-attention can be approximated in almost linear time. This suggests we may be able to save the substantial resources required for training Large Language Models."}, {"title": "Key background", "content": "We first introduce a basic background, starting with defining the softmax function.\nDefinition 1.1 (Softmax). Let $z \\in R^n$. We define Softmax : $R^n \\rightarrow R^n$ satisfying\n$\\text{Softmax}(z) := \\exp(z) / <\\exp(z), 1_n>$.\nHere we apply exp to a vector entry-wise.\nThen, we introduce the definition of a single self-attention layer.\nDefinition 1.2 (Single layer self-attention). Let n be the number of tokens and d be the hidden dimension size. Let $X \\in R^{n \\times d}$ denote the input data matrix. Let $Q := XW_Q, K := XW_K, V := XW_V \\in R^{n \\times d}$. Let $W := W_QW_K \\in R^{d \\times d}$ be the key-query weight matrix, and let $W_V \\in R^{d \\times d}$ be the value weight matrix. The self-attention function Attn(Q, K, V) is:\n$\\text{Attn}(Q, K, V) = \\text{Softmax}(QK^T / \\sqrt{d})V$.\nWe remark that we apply Softmax to each row of the $n \\times n$ matrix."}, {"title": "Our contributions", "content": "We now state our main contributions as follows:\nTheorem 1.4 (Main result, informal version of Theorem 4.2). Let n be the number of tokens, and d be the hidden dimension size. We assume d = O(logn) and each number in matrices can be written using O(logn) bits. There exists an algorithm (Algorithm 1) that can compute the gradient of multi-layer self-attention (see also Definition 1.3) in almost linear time $n^{1+o(1)}d = n^{1+o(1)}$, where the approximation error of entire model can be bounded in 1/poly(n).\nOur assumption is mild when the context length n is large, as we usually see feature dimension d as a constant, which is also used in [AS23, AS24a]. Our results indicate that large language models (LLMs) can be trained in almost linear time $n^{1+o(1)}$ and maintain a robust approximation guarantee, while the traditional way takes $O(n^2)$ time. This advancement is realized through the application of polynomial kernel approximation [AS23, AS24a]. To be more specific, by leveraging the inherent sparsity within the dense attention matrix, we are able to perform efficient low-rank approximation, thereby significantly accelerating the computation of the dense matrices. Our framework is applicable to any loss function, making it universally applicable. Furthermore, our analysis can hold when the multi-layer transformer model contains many practical sub-modules, such as residual connection, casual mask, and multi-head attention (Section 6).\nOur contributions are as follows:\n\u2022 We introduce a fast computation method that allows the gradient of each self-attention layer to be approximated in almost linear time $n^{1+o(1)}$ with 1/ poly(n) error, breaking the quadratic time complexity bottleneck (Theorem 4.1).\n\u2022 We extend our single-layer results to module-wise gradient computation so that our Algorithm 1 can provide approximated gradient computation in m\u00b7$n^{1+o(1)}$ time for m-layer transformer. Furthermore, the approximation of the gradient diverges from the exact gradient by an error of 1/poly(n) when considered across the entire model (Theorem 4.2).\n\u2022 Additionally, our analysis can hold when the multi-layer transformer model contains residual connection, casual mask, and multi-head attention. Our results can be applied to any gradient-based algorithm, e.g., training, full fine-tuning, prompt-tuning, and so on (Section 6).\nRoadmap. Our paper organized as follows. Section 2 offers the related work. Section 3 provides essential conceptions and key definitions across the whole paper. Section 4 presents our primary findings, where we articulate our novel algorithm that is capable of calculating gradients across the entire model in almost linear time. In Section 5, we explain the techniques we employ, including low-rank approximation, some tricks for accelerating computation of gradients, and an analysis of the approximation error. Section 6 provides various extensions of our algorithm. Lastly, we conclude this paper in Section 7."}, {"title": "Related Work", "content": "Attention mechanism. Attention mechanisms, including self-attention and cross-attention, are pivotal techniques employed in state-of-the-art neural networks. Since it was introduced in [VSP+17], it has gained widespread adoption across various domains. In particular, it is integral to decoder-only LLMS [RWC+19] and the Vision Transformer (ViT) architecture [DBK+20]. The former has been instrumental in the remarkable success of LLMs, while the latter has significantly advanced the field of computer vision, encompassing applications such as image generation [RBL+22, WSD+23, WXZ+24], detection [LMGH22, ZLC+21], segmentation [ZTT+22, TDC+23], and layout generation [GLA+21, CZY23, WCZ+23]. Moreover, attention mechanism can be integrated into multi-modal models [XGH+21, ZHJL24, LSSZ24b, WMS+24], math reasoning [LLS+24a], diffusion models [SSDK+20, LSSZ24c], differential privacy [BEPP22, SSC+22, WCY+23, LSSZ24a, SAMB24] and many other techniques.\nLong-context modeling in LLMs. As LLMs grow in size and capability, in-context learning (ICL) [MLH+22, SWXL24, XSL24] has become a preferred method for directing these models to perform a variety of tasks, as opposed to the resource-intensive process of fine-tuning. Nonetheless, research has indicated that longer prompts can impair LLMs performance due to the limitation on maximum sequence length during pre-training [LZD+24]. Consequently, extending the maximum sequence length during pre-training and fine-tuning stages is imperative. Enhancing training efficiency is crucial given the prevalent use of the Transformer architecture in LLMs, which incurs a quadratic computational cost relative to sequence length. Addressing this challenge, some studies have explored continued fine-tuning of LLMs with extended context lengths [TSP+24], while others have experimented with the interpolation and extrapolation capabilities of positional embedding [CWCT23, PAA+23, SAL+24]. However, these approaches have not fundamentally addressed the core issue: the quadratic computational cost associated with sequence length in the attention mechanism [KWH23, FCA23]. In this study, we delve into accelerating the attention mechanism, thereby addressing the long-context modeling issue at its essence.\nAttention acceleration. Attention mechanism has faced criticism due to its quadratic time complexity with respect to context length, a concern exacerbated by the increasing length in modern large language models (LLMs) such as GPT-4 [AAA+23], Claude [Ant24], Llama [TLI+23, LT24], etc. Nevertheless, this limitation can be circumvented by employing polynomial kernel approximation techniques [AA22], which enable the derivation of a low-rank representation of the attention matrix. This innovation significantly accelerates both the training and inference processes of a single attention layer, achieving almost linear time complexity [AS23, AS24a], while our work supports both training and inference for any multi-layer transformer. Furthermore, this approach can be extended to higher-order attention mechanisms, i.e., tensor attention, maintaining almost linear time complexity during both training and inference [AS24b, LSSZ24b]. Moreover, there are other theoretical approaches. For instance, [LLS+24b] introduces the conv-basis method to accelerate attention computation. [HJK+24] proposes a near-linear time algorithm under the assumptions of uniform softmax column norms and sparsity."}, {"title": "Preliminary", "content": "We first introduce some basic notation used in this paper in Section 3.1. In Section 3.2, we define the loss function. Then, to better understand our proofs, we provide a closed-form analysis of the three gradient components in a single self-attention transformer layer in Section 3.3."}, {"title": "Notations", "content": "For any positive integer n, we use [n] to denote set {1,2,\u2026,n}. For two vectors $x \\in R^n$ and $y \\in R^n$, we use <x, y> to denote the inner product between x, y. Namely, <x,y> = $\\sum_{i=1}^n x_iy_i$. We use $e_i$ to denote a vector where only i-th coordinate is 1, and other entries are 0. For each a, b \u2208 $R^n$, we use a\u2299 b \u2208 $R^n$ to denote the Hardamard product, i.e. the i-th entry of (a\u2299 b) is $a_ib_i$ for all i \u2208 [n]. We use $1_n$ to denote a length-n vector where all the entries are ones. We use $||A||_\\infty$ to denote the $l_\\infty$ norm of a matrix $A \\in R^{n \\times d}$, i.e. $||A||_\\infty := \\max_{i \\in [n], j \\in [d]} |A_{i,j}|$. We use poly(n) to denote polynomial time complexity with respective to n."}, {"title": "Loss function", "content": "The loss function is the optimization objective in the training of LLMs, and we define it as follows.\nDefinition 3.1 (Loss function L(X)). For some input matrix $X \\in R^{n \\times d}$, we define the one-unit loss function $l(X)_{j,k}$ : $R^{n \\times d} \\rightarrow R$, for any $j \\in [n], k \\in [d]$. Furthermore, we define the overall loss function L(X), such that\n$L(X) = \\sum_{j=1}^n \\sum_{k=1}^d l(X)_{j,k}$\nRemark 3.2. Typically, in Definition 3.1, the most widely used loss function in the LLM training procedure is the cross-entropy loss function, which can also be viewed as a summation of one unit loss function.\nThe output matrix of the multi-layer transformer needs to pass an additional linear layer to map the hidden dimension d to the vocabulary size $d_{voc}$. Assuming $d_{voc}$ is a constant, the weight matrix dimensions for this additional MLP layer are d \u00d7 $d_{voc}$. The probability tensor $Y_{pred}$ \u2208 $R^{n \\times d_{voc}}$ is the final output. We denote the ground truth as $Y_{gt}$ \u2208 $R^{n \\times d_{voc}}$ corresponding to $Y_{pred}$. According to the cross-entropy loss definition, the formula is expressed as\n$L_{cross-entropy}(X) = - \\sum_{j=1}^n \\sum_{k=1}^{d_{voc}} (Y_{gt})_{j,k} \\log((Y_{pred})_{j,k})$\nwhere the summation iterates over all elements, and the ground truth $(Y_{gt})_{j,k} = 1$ for the correct class and 0 otherwise."}, {"title": "Close forms of gradient components", "content": "In training large language models (LLMs), updating the model necessitates computing the gradient of weights for every layer. Consequently, it becomes essential to derive the closed-form expressions for all corresponding gradient components with respect to the weights of the query, key, and value matrices in the transformer model. We first define some intermediate variables before detailing these gradient components in each self-attention transformer layer.\nDefinition 3.3 (Intermediate variables $T_i$). Let m denote the number of transformer layers in the model. Let m-layer self-attention transformer be defined as Definition 1.3. Let d denote the hidden dimension. Let n denote the sequence length. Let $X \\in R^{n \\times d}$ be the input sentence. Let $g_i$ denote components other than self-attention in the i-th transformer layer. Let Attn denote the self-attention module in the i-th transformer layer (see also Definition 1.2).\nFor i \u2208 {0,1,2,\u2026\u2026,m}, we define $T_i(X) \\in R^{n \\times d}$ be the intermediate variable (hidden states) output by i-th layer self-attention transformer. Namely, we have\n$T_i(X) =\n\\begin{cases}\ng_0(X), & i = 0;\\\\\n(g_i \\circ \\text{Attn}_i)(T_{i-1}(X)), & i \\in [m].\n\\end{cases}$\nHere, we use \u25e6 to denote function composition.\nThen, we are ready to introduce the close forms of the three gradient components in a single self-attention transformer layer. Notably, according to the chain rule, the gradient of the k-th transformer layer in LLMs depends on the gradient components from the (k + 1)-th transformer layer. The gradient can be calculated for every transformer layer by combining the upstream and local gradients. The close forms of the gradients for each layer in multi-layer transformers are formalized in the following lemma (Lemma 3.4)."}, {"title": "Main Result", "content": "Now, we present our main findings. We will work through this section in the following order: In Section 4.1, we delineate the computational efficiency of our gradient calculation methods in each single layer. In Section 4.2, we introduce our main theorem (Theorem 4.2) for multi-layer transformer by integrating the preceding results and provide our main algorithm (Algorithm 1). Section 4.3 discusses how we transcend the previous works."}, {"title": "Fast computing for single layer", "content": "In the case of single-layer attention, we provide our theorem that state the three gradient components can be calculated in almost linear time with negligible error.\nTheorem 4.1 (Single-layer gradient approximation). We assume d = O(logn) and each number in matrices can be written using O(logn) bits. Let L(X) be defined as Definition 3.1. Suppose we have a single-layer self-attention transformer model (m = 1 in Definition 1.3). We can approximate one-layer self-attention for three gradient components, i.e. $\\frac{dL(X)}{dX}, \\frac{dL(X)}{dW}$ and $\\frac{dL(X)}{dW_v}$, in $n^{1+o(1)}$ time with 1/poly(n) error.\nProof. We finish the proof by Lemma 5.1, 5.2 and 5.3."}, {"title": "Fast computing for multi-layer transformers", "content": "Based on the results demonstrated in previous sections, we are ready to introduce our main result: the whole transformer model can be approximated in almost linear time.\nTheorem 4.2 (Main result, formal version of Theorem 1.4). Let m denote the number of transformer layers. We assume d = O(log n) and each number in matrices can be written using O(log n) bits. We can show that, for any i \u2208 [m], all the gradient components (see also Lemma 3.4) of the i-th layer can be computed by Algorithm 1 in almost linear time $n^{1+o(1)}$, and the approximation error of entire m layer transformer model can be bounded by 1/poly(n).\nProof. The theorem can be proved by directly combining Theorem 4.1 and Lemma 5.5.\nTheorem 4.2 demonstrates that, during the training of a multi-layer transformer model, at each training iteration, the gradient computation for the weight matrices of each layer can be performed in almost linear time $n^{1+o(1)}$. This result supports the feasibility of fast training for any transformer-based large language models (LLMs). In Algorithm 1, we illustrate the process of back-propagating gradients from the m-th transformer layer back to the first layer. This algorithm highlights the significance of the gradient with respect to the intermediate variables $T_i(X)$. Due to the application of the chain rule in gradient computation, the gradient of $T_i(X)$ is indispensable for determining the gradients of the weight matrices $W_i$ and $W_{v_i}$ at the i-th layer. Consequently, by iteratively computing the gradient for $T_i(X)$, we systematically propagate the gradient through to the initial transformer layer.\nAdditionally, our algorithm is capable of computing the gradient with respect to the input data X. Therefore, our algorithm also supports fast prompt tuning. For a more in-depth discussion on this topic, please refer to Section 6."}, {"title": "Beyond the previous works", "content": "Our algorithm exhibits significant advancements over two brilliant related prior studies, namely [AS23] and [AS24a]. In [AS23], the authors proposed an almost linear time algorithm for computing the forward process of the attention mechanism. In contrast, [AS24a] introduced an almost linear time algorithm for the backward of attention mechanism. However, [AS24a] has the following limitations: (1) only computing gradients for a single layer of the attention mechanism, which cannot extend to multiple layers; (2) calculating gradients with respect to a specific loss, namely the $l_2$ loss; (3) computing gradients only for the weight matrix $W_i$ (as defined in Definition 1.2), but ignore other crucial components such as the MLP layer following attention computation and the activation function.\nIn our work, we have the following improvements beyond [AS24a]: (1) we enable almost linear time gradient computation across an entire transformer layer, incorporating both the MLP layer and the activation function; (2) our algorithm supports gradient calculation for any loss function L(X) (see Definition 3.1); (3) we extend the gradient calculation to include not only $W_i$ but also $T_i(X)$ and $W_v$. These advancements collectively demonstrate a substantial leap forward from the methodologies in [AS23] and [AS24a]."}, {"title": "Technical Overview", "content": "This section provides a brief overview of the proof techniques used throughout this paper. Section 5.1 delves into intuition under the low-rank approximation applied to the attention matrix f(X). Section 5.2 elaborates on calculating gradients with respect to the intermediate variable (hidden states) $T_i$ in almost linear time. In Section 5.3, we detail the process for computing gradients for the weight matrices W and $W_{v_i}$ within transformer layers in almost linear time. Finally, in Section 5.4, we demonstrate how to generalize our findings from single-layer transformers to multi-layer transformers, including an analysis of the runtime and approximation error across the entire model."}, {"title": "Low-rank approximation for attention matrix", "content": "In this section, we delve into the crucial technique behind our work: the low-rank approximation of the attention matrix, which is achieved through the polynomial method [ACSS20, AA22]. Drawing inspiration from [AS23], the intuition of this approximation lies in the fact that the attention matrix $f(X) \\in R^{n \\times n}$ (as defined in Definition 1.2), also referred to as the similarity matrix in attention mechanism, can be effectively approximated by low-rank matrices $U_1, V_1 \\in R^{n \\times k_1}$, where $k_1 = n^{o(1)}$. The naive method for calculating the attention matrix f(X) has a time complexity of O(n\u00b2), whereas the input data $X \\in R^{n \\times d}$ contains only d\u00b7 n = $n^{1+o(1)}$ entries. This discrepancy suggests the potential of using low-rank representations of f(X) to design a fast algorithm.\nAn example of how to use the low-rank representations is the attention forward (Attn(X) := $f(X)V$ in Definition 1.2) as in [AS23]: approximating f(X) along does not lead to fast algorithm, since $U_1V_1^T$ still requires n \u00d7 n entries. But by using the structure of the whole, we can do it faster. By expressing f(X) as $U_1V_1^T$, the attention forward becomes $U_1 \\underset{\\text{nxk}_1}{V_1^T V}$. It is well known that different multiplication sequences can lead to dramatically different numbers of operations required, so the order of matrix multiplications matters. We first perform $V_1^TV \\in R^{k_1 \\times d}$ and this cost O($k_1nd$) = $n^{1+o(1)}$ time. Then we perform $U_1V_1^TV$ costing O($nk_1d$) = $n^{1+o(1)}$ time.\nThis method significantly reduces the computation time of the attention forward from $O(n^2)$ to almost linear time, $n^{1+o(1)}$. Driven by this technique and analyzing the close forms of the gradients, we can extend the acceleration to the gradient of the entire model."}, {"title": "Accelerating gradient computation of T(X)", "content": "Based on the low-rank approximation method mentioned in Section 5.1, we can compute the gradient of L(X) with respect to the intermediate variable T(X), which denotes the output of the i-th transformer layer. This computation is critical as it enables us to calculate gradients for other gradient components because of the chain rule.\nExtending to any kind of loss function. According to the findings in [DSXY23], the gradient $\\frac{dL(X)}{dT(X)}$ can be decomposed into five components, namely $C_6(X), C_7(X), C_8(X), C_2(X), C_4(X)$, as detailed in Lemma D.1. However, the gradient result presented in [DSXY23] is tailored to a specific loss function, the $l_2$ loss, limiting its applicability to a narrow range of scenarios. In this study, we extend the scope of their findings by extending them to apply to any loss function L(X), as defined in Definition 3.1. By incorporating $G_i = \\frac{d\\text{Attn}_i(T_{i-1}(X))}{dT_{i-1}(X)}$, we derive a closed-form expression for the gradient of L(X) with respect to $T_i(X)$, which is detailed in Section D.2.\nAccelerating the gradient computation. To accelerate the gradient computation for $\\frac{dL(X)}{dT_i(X)}$, we need the matrix form of the gradients, as discussed in Section D. This approach is essential for understanding the underlying mechanisms when applying the low-rank approximation technique in gradient calculations. Subsequently, using that technique, we can accelerate the gradient computation for $\\frac{dL(X)}{dT(X)}$ (Lemma 5.1). By individually applying this technique to each of the five terms, we demonstrate that each term can be computed in almost linear time, $n^{1+o(1)}$, as shown in Sections E.1, E.2, E.3, E.4, and E.5.\nThe next step is to aggregate these terms, as described in Section E.6. Since all five terms are n\u00d7d matrices, the summation of these terms remains almost linear in complexity. Conclusively, we are safe to argue that for any single-layer transformer, the gradient computation with respect to the input tensor can be performed in almost linear time $n^{1+o(1)}$, as stated in Lemma 5.1."}, {"title": "Accelerating gradient computation of Wi and Wvi", "content": "In Section 5.2, we detailed the fast computation of gradients for intermediate variables $T_i(X)$. Given that $W_i$ is defined as the product $W_QW_K$ (see Definition 1.2), with $W_{Q_i}$ and $W_{K_i}$ representing the query and key weight matrices, respectively, the gradients of $W_i$ and $W_{v_i}$ represent all trainable weight matrices in a transformer layer. Consequently, by determining the gradients for $W_i$ and $W_{v_i}$ across each layer, we achieve almost linear time gradient back-propagation throughout multi-layer transformer models.\nFast gradient computation. The prior study in [AS24a] demonstrated that the gradient of $W_i$ can be computed in almost linear time. We extend their findings by adapting their approach to accommodate any loss function L(X) (as defined in Definition 3.1) and further generalize their results to include the gradient computation for both $W_i$ and $W_{v_i}$ in each transformer layer (Lemma 5.2 and 5.3)."}, {"title": "Accelerating gradient computation for multi-Layer transformers", "content": "In this section, our focus turns to extending the single-layer transformer result from the previous section to a multi-layer transformer. The statement made for a single transformer layer can be readily generalized to any layer within an m-layer transformer model. For instance, consider the intermediate variables $T_i(X)$ and $T_{i-1}(X)$ (as defined in Definition 3.3), where $T_i(X) = (g_i \\circ \\text{Attn}_i)(T_{i-1}(X))$. Given the gradient $\\frac{dL(X)}{dT_i(X)}$ as established in the previous paragraph, we can compute the gradient with respect to $T_{i-1}(X)$, namely $\\frac{dL(X)}{dT_{i-1}(X)}$, in almost linear time $n^{1+o(1)}$. For a multi-layer transformer model, the above process can be conducted recursively. Thus, we can compute the gradient of the loss function L(X) on any $T_i(X)$ in almost linear time $n^{1+o(1)}$."}, {"title": "Extension", "content": "Here, we explore potential enhancements to our algorithm. In Section 6.1, we show the potential extension of multi-head attention and residual connections. In Section 6.2, we offer possibilities to incorporate causal masked attention in almost linear time. In Section 6.3, we show our algorithm can accelerate the prompt tuning process. In Section 6.4, we give the possible integration with system-level attention acceleration techniques."}, {"title": "Multi-head attention and residual connections", "content": "Multi-head attention and residual connections are important components in attention mechanisms. While these components were not involved in our initial analysis for simplicity, incorporating them into our algorithm is straightforward, as detailed in Sections B.1 and B.2. Our algorithm maintains the capability to compute gradients for multi-layer transformers with multi-head attention and residual connection in almost linear time, suggesting that it can be readily adapted to more practical transformer models.\nThe detailed analysis of incorporating residual connection with our framework can be found in Section J and Lemma J.3. For the synergy with multi-head attention, we provide comprehensive analysis in Section K and Lemma K.2."}, {"title": "Causal attention mask", "content": "The causal attention mask is critical to prevent transformers from \"cheating\" during training by ensuring future information is not used. The full-rank characteristic of the causal attention mask poses challenges for low-rank approximations. Nevertheless, we have identified a method to accelerate the computation of causal masked attention by exploiting its inherent properties, as demonstrated in [LLS+24b], remaining almost linear time complexity. A comprehensive explanation is provided in Section B.3.\nMore detailed analysis of causal attention can be found in Section I and Lemma 1.7 and 1.8."}, {"title": "Prompt tuning", "content": "Prompt tuning (or prefix learning) is a prevalent approach in parameter-efficient fine-tuning (PEFT), which requires the calculation of gradients on input data X. Given our algorithm's ability to compute gradients for intermediate variables $T_i$ in approximately linear time, we can similarly accelerate the gradient computation for input data X, thus enhancing the efficiency of the prompt tuning process. Additional details are provided in Section B.5."}, {"title": "Synergy with system-level attention acceleration", "content": "Many contemporary works focus on system-level acceleration of attention mechanisms, often by leveraging caching and mitigating I/O bottlenecks. Our algorithm has the potential to integrate with such advancements. By combining our theoretical improvements in computation time (from $O(n^2)$ to $n^{1+o(1)}$) with system-level optimizations, the overall efficiency of attention mechanism computation may increase dramatically. We leave the implementation of our method on GPU as future work since there are several coding challenges. More details can be found in Section B.4."}, {"title": "Conclusion", "content": "The attention mechanism in transformer models inherently has quadratic time complexity with respect to the input token length. In this work, we propose a novel Algorithm 1, which can approximately train a multi-layer transformer model in almost linear time, introducing only a small error. Moreover, our algorithm is compatible with any loss function, practical sub-modules (residual connection, casual mask, multi-head attention), and gradient-based algorithm and can be seamlessly integrated with other system-level acceleration techniques. While we lack enterprise-scale computational resources for training large language models, our theoretical findings suggest that we can accelerate the training of LLMs in practice."}, {"title": "More Related Work", "content": "Attention theory. [BCB14", "LPM15": "developed local and global attention variants", "applications": ["XBK+15"], "VSP+17": "s Transformer model revolutionized NLP by capturing word relationships, and [VCC+17", "KKL20": "."}, {"KKL20": "use Locality Sensitive Hashing for attention approximation, with [ZHDK23", "BSZ23": "and [AS23", "LSZ23": "investigates hyperbolic regression regularization. [DMS23", "ZKV+20": "investigating word co-occurrence learning [LLR23", "GMS23": "attention mechanism evolution during training [SZKS21"}, {"ZKV+20": ".", "GSYZ23": "tensor attention [AS24b, LSSZ24b", "LSSS24": ".", "DG24": "Linearizing Transformers [ZBKR24, MVK+24", "KMZ23": "and the Hopfield Model [HC"}]}