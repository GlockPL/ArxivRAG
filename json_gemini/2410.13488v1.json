{"title": "Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes", "authors": ["Dibyanayan Bandyopadhyay", "Mohammed Hasanuzzaman", "Asif Ekbal"], "abstract": "Detecting offensive memes is crucial, yet standard deep neural network systems often remain opaque. Various input attribution-based methods attempt to interpret their behavior, but they face challenges with implicitly offensive memes and non-causal attributions. To address these issues, we propose a framework based on a Structural Causal Model (SCM). In this framework, VisualBERT is trained to predict the class of an input meme based on both meme input and causal concepts, allowing for transparent interpretation. Our qualitative evaluation demonstrates the framework's effectiveness in understanding model behavior, particularly in determining whether the model was right due to the right reason, and in identifying reasons behind misclassification. Additionally, quantitative analysis assesses the significance of proposed modelling choices, such as de-confounding, adversarial learning, and dynamic routing, and compares them with input attribution methods. Surprisingly, we find that input attribution methods do not guarantee causality within our framework, raising questions about their reliability in safety-critical applications.", "sections": [{"title": "Introduction", "content": "Memes have evolved from spreading humor to being used for disseminating offensive content, necessitating the development of neural multimodal systems to detect such content (Kiela et al., 2021). However, these systems often lack transparency, undermining public trust in real-world applications and highlighting the need for interpretability and trustworthiness.\nWhile Large Language Models (LLMs) and Vision Language Models (VLMs) could predict offensive memes and provide self-explanations, these explanations are not always faithful to model behavior (Madsen et al., 2024; Agarwal et al., 2024). Our self-consistency checks (\u00a7Appendix B) confirm this issue for offensive meme detection, prompting us to focus on enhancing the reliability of existing multimodal classifiers. Current interpretability techniques, such as input attributions (e.g., Integrated Gradient (Sundararajan et al., 2017)), struggle with implicit content and causality. For implicitly offensive memes, attribution methods fail to capture underlying concepts like 'racism' (refer to Figure 1). Also, they only detect influential features without considering their causal impact (Chattopadhyay et al., 2019).\nCausality-based techniques like CausaLM (Feder et al., 2021) and Amnesic Probing (Elazar et al., 2021) offer solutions but face scalability issues and focus on global rather than local explanations like: 'Is adjective important for sentiment analysis' or 'Is part-of-speech information crucial for word prediction?' We address these limitations with a novel causal framework integrating VisualBERT (Li et al., 2019) with causal concepts. Our approach extracts implicit context from the meme as a set of causal concepts and uses dynamic routing and adversarial learning to predict meme offensiveness based on both meme content and contribution from causal concepts.\nQuantitative analysis (Section 6) shows that traditional interpretability techniques, which rely on correlation, do not always align with causality, emphasizing that correlation does not imply causation. Through ablation studies, we demonstrate that our framework bases its predictions on relevant causal concepts, enhancing trustworthiness. Qualitative analysis (Section 7) indicates whether the model's decisions are justifiable and explains its error cases.\nOur proposed framework is novel, model-agnostic, and acts as a proof-of-concept which demonstrates the potential of using causal analysis to elucidate the decision-making process of multimodal classifiers."}, {"title": "Related Work", "content": "Causal Interpretability: Causal interpretability aims to understand how counterfactuals cause model outputs to change, thus estimating the causal effect of inputs (Feder et al., 2022). A subfield, causal mediation analysis, explores the mechanisms behind these effects (Geiger et al., 2021; Vig et al., 2020; Meng et al., 2023). Generating exact counterfactuals is challenging (Abraham et al., 2022; Calderon et al., 2022), so recent work focuses on approximations (Geiger et al., 2021) or counterfactual representation (Feder et al., 2021; Elazar et al., 2021; Ravfogel et al., 2021). Our current research concentrates on counterfactual representation. Most of the existing works target single modality (e.g. text or vision) (Feder et al., 2021; Goyal et al., 2020) and answer global questions about feature importance (Elazar et al., 2021). We propose a method for answering local questions about specific concepts (e.g., 'Is the meme offensive due to the presence of Holocaust as a concept?') while addressing scalability issues of prior methods (Feder et al., 2022). Our framework incorporates concept annotations and integrates with VisualBERT for trustworthy local causal interpretability.\nMultimodal Interpretability. Recently, there has been a surge in multimodal models for various tasks (Ding et al., 2021; Du et al., 2022; Li et al., 2023; Liu et al., 2023; Zhu et al., 2023), yet research on generating explanations for their predictions remains limited. Researchers primarily rely on interpretability techniques like LIME (Ribeiro et al., 2016) and SHAP (Lundberg and Lee, 2017a) and various input attribution methods (Sundararajan et al., 2017; Lundberg and Lee, 2017b; Shrikumar et al., 2017). However, recently, there has been a shift towards generating natural language explanations, bridging the gap between text-only and multimodal systems. Methods like NLX-GPT (Sammani et al., 2022) and Semantify (Bandyopadhyay et al., 2024) offer solutions but fail to fully capture implicit causal meanings or the causal impact of input features (Chattopadhyay et al., 2019). This gap motivated us to develop a framework that enables causal interpretations of implicit inputs."}, {"title": "Causal Process", "content": "Our framework is based on a Structural Causal Model (SCM) that integrates both the causal explanation process and multimodal classification objectives, drawing inspiration from Geiger et al. (2021). We assume an exogenous variable $E_1$ that generates causal concepts $C_1$, to $C_n$. Another exogenous variable $E_2$ controls meme text $t$ and image $v$ representation. The collection of the concepts ${C_i}_{i=1}^n$ controls the latent representation $L$. $(t, v)$ along with $L$ controls the intermediate representation $I$, which further controls the output $y$ of the model. Figure 2 represents this in details.\nInspired by\nCounterfactual Representation.\nCausaLM (Feder et al., 2021), we want to intervene $I$ such that the generated counterfactual representation $I_{CF_i}$ is insensitive to concept $c_i$ and similar to $I$ for all the other concepts except $c_i$.\nTo achieve this, we observe the SCM, depicted in Figure 2, where $I = f(L, (t, v))$ and $L$ represents the latent, a weighted sum of concept representations $c_i$, given by $L = \\sum_{i=1}^n W_i. C_i$. Creating a counterfactual latent $(\\bar{L}^i)$ unaffected by a specific concept $c_i$ is straightforward to achieve by setting $w_i = 0$. This explicit modelling offers simple implementation and ensures $I_{CF_i}$ represents a counterfactual unaffected by $c_i$, while still influenced by other concepts.\nNeed for De-confounding. In this formulation"}, {"title": "Causal Diagram", "content": "Our framework is based on a Structural Causal\nModel (SCM) that integrates both the causal expla-\nnation process and multimodal classification objec-\ntives, drawing inspiration from Geiger et al. (2021).\nWe assume an exogenous variable $E_1$ that gener-\nates causal concepts $C_1$, to $C_n$. Another exoge-\nnous variable $E_2$ controls meme text $t$ and image\n$v$ representation. The collection of the concepts\n${C_i}^n_{i=1}$ controls the latent representation $L$. $(t, v)$\nalong with $L$ controls the intermediate representa-\ntion $I$, which further controls the output $y$ of the\nmodel. Figure 2 represents this in details."}, {"title": "Causal Effect of Concept $c_i$.", "content": "Drawing inspi-\nration from existing literature (Feder et al., 2021),\nwe formulate the causal effect of concept $c_i$ (for a\nspecific input) as:\n$RITE_i =< \\phi(I_{CF_i}) - \\phi(I_{DC}) > =< \\phi(f((t, v), L_{CF_i})) - \\phi(f((t, v), L_{DC})) >$         (1)\nHere, $RITE_i$ represents the \"Representation-\nbased Individual Treatment Effect\", for $c_i$, with\n$DC$ as subscript representing the de-confounding\nobjective. The function $f$, modelled as Visual-\nBERT in this paper, takes the input meme as text\nand visual representation $((t, v))$ and latent $L_{DC}$,\noutputting a representation $I_{DC}$. This intermediate\nrepresentation is then passed through a classifier $\\phi$.\nEssentially, $RITE_i$ denotes the absolute change\nin predicted class probability due to the absence of\nconcept $c_i$ and could be used to measure its causal\neffect on the model."}, {"title": "Concept Annotation", "content": "Figure 2 illustrates the causal process, which relies\non the integration of concepts alongside meme in-\nputs to facilitate model predictions. The selection\nof these concepts (which form the \u2018concept set\u2019)\nis pivotal, aiming to i) encapsulate the breadth of\nthemes present within the training dataset while ii)\nminimizing redundancy.\nScalability. To make the annotation process effi-\ncient and scalable, we use the following approach.\nStarting empty, the concept set expands as new\nconcepts are introduced. For example, if the first\nmeme includes 'terrorism' and 'holocaust', these\nare added to the set. If the second meme includes\n'terrorism' and 'racism', the set becomes 'terror-\nism', 'holocaust', 'racism'. Therefore, we only\nappend new concepts to the set if they are absent,\nto minimize redundancy and counter overlap be-\ntween similar concepts.\nAnnotation Process. We enlisted three anno-\ntators, all postgraduate students aged 25-27 with\nexpertise in multimodal machine learning and prior\nexperience curating datasets published in reputable\nvenues, to annotate the concept set. We ensured\nethics and took active steps to ensure their well-being, as detailed in Appendix Section G. Let us\ndenote the three annotated concept sets as ${c_1}$,\n${C_2}$, and ${C_3}$. We then calculate concept rep-\nresentation $r_t(c_i)$, where $c_i$ denotes $i^{th}$ concept\nfrom set ${1}$. Similarly, let us denote meme rep-\nresentation as $r_t(T_i) \\odot r_v(V_i)$, where $T_i, V_i$ are\nthe text and image of the $j^{th}$ meme from the test\nset, and $\\odot$ illustrates element-wise multiplication.\nHere, $r_t(T_i) \\in R^{1\\times768}$ and $r_v(V_i) \\in R^{1\\times768}$ rep-\nresent CLIP (Radford et al., 2021) text and vision\nencodings respectively for the $j^{th}$ meme. We then\ncalculate the total similarity of the set ${c_1}$ to the\nmemes in the training set as $\\sum^N_{j=0} \\sum^{a=n}_{i=0}r_t(c_i)^T .$\n$(r_t(T_i) r_v(V_i))$, where $N$ is the number of\nmemes in the training set and $n$ is the number of\nconcepts in the set ${c_1}$. Similarly, this total sim-\nilarity is calculated for sets ${c_2}$, and ${3}$. We\nobserve that the total similarity of set ${c_2}$ is the\nhighest which leads us to choose this as the final\nset. The concepts in ${c_2}$ are shown in Table 1.\nAmbiguity Resolution. Annotators were in-\nstructed to maintain precision and leverage existing\nannotated concepts when annotating new memes."}, {"title": "Methodology", "content": "Our method relies on a three-step process, namely i) Dynamic Routing, ii) Adversarial learning and iii) De-confounding.\nModel Inputs. A meme consists of text T and image I. We extract image features using FasterRCNN (Ren et al., 2016), yielding dimensions $v \\in R^{B \\times N \\times 768}$. Text tokens pass through the model embedding layer to generate text features with dimensions $t \\in R^{B \\times M \\times 768}$. Concatenating these text and image features (t, v) results in input dimensions of $R^{B \\times (M+N) \\times 768}$ for VisualBERT. Additionally, we introduce a latent representation L, with dimensions $R^{B \\times 1 \\times 768}$, obtained by weighted summation of concept features $c_i$. Formally, $L = \\sum^n_{i=0} (w_i^d + w_i^s)c_i$, where $w_i^d$ is a dynamic weight, $w_i^s = p(c_i, T)$, dependent on the $i^{th}$ concept $c_i$ and the meme text T. This functional relationship is learned through dynamic routing. Similarly, $w_i^s$ are static weights, $w_i^s = r(c_i)$, dependent only on the concept $c_i$."}, {"title": "Dynamic Routing.", "content": "Need for Dynamic Routing. The key idea behind dynamic routing is to learn dynamic weights that determine the importance of each concept for a prediction based on both the meme input and the concept itself. These weights are functions of the meme's text, and concept, serving as learnable parameters that control each concept's influence on the prediction. Without dynamic routing, static weights are used, treating all concepts equally, regardless of the specific meme input.\nHow is it achieved?\nTo model the interaction between the $M$ text features ${t_j}^M_{j=0}$ produced by VisualBERT for a given input T and the concept feature $c_i$, we learn a weight W, which modifies $t_j$ as $t_{ij} = W_{ij} .t_j$. Further, the interaction between $c_i$ and $t_j$ can be modelled by taking a dot product between them. $P_{ij} = t_{ij}^T. c_i$ demonstrates this operation, where $P_{ij}$ is a scalar. To normalize $p_{ij}$ between 0 and 1, we use the softmax function: $b_{ij} = \\frac{exp(p_{ij})}{\\sum^M_{k=0} exp(P_{ik})}$, where n is the number of concepts. To measure the effect of all text inputs on concept i, we calculate the weighted mean: $s_i = \\frac{1}{M} \\sum^M_{j=0} b_{ij} t_{ij}$, where M is the number of text tokens input to the model. Here, $s_i$ shows the cumulative effect of all the text inputs on concept $c_i$. To model the interaction between $c_i$ and T, we want the length of $s_i$ to approximate their interaction. Specifically, longer vectors should have a unit length, whereas shorter vectors should have a length close to zero. This is achieved using the following squashing function (Sabour et al., 2017):\n$v_i = squash(s_i) = \\frac{||s_i||^2}{\\frac{s_i}{1+||s_i||^2}} \\frac{||s_i||}{||s_i||}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (2)$\nThe length of $v_i$ acts as the dynamic weight between $c_i$ and $T_i$, such that $w_i^d = p(c_i, T) = ||v_i||$."}, {"title": "Adversarial Learning", "content": "Notation. Let $m_i$ be a $R^{1\\times768}$ dimensional vector output corresponding to the \u2018[CLS]\u2019 token when VisualBERT processes the text T and image I. Similarly, let $m_i^'$ be the output vector when VisualBERT processes the text T, image I, and latent L (by concatenating with image representation). For classifying an input meme, we utilize a feed-forward neural network (FFN) on the representation $m_i$.\nNeed for Adversarial Learning. Typically, input text and images contain sufficient information to classify a meme into offensive or non-offensive classes, rendering the latent representation less effective compared to text and image inputs alone. To enhance the effectiveness of the latent representation to match that of text and image inputs, we employ adversarial learning. The objective is to make both $m_i$ and L invariant to the output class, while their combined representation $m_i^'$ should retain discriminatory information for classifying memes. The aim is for L to be as effective as the combined text and image inputs.\nHow is it achieved? To achieve this, we utilize a Gradient Reversal Layer (Ganin and Lempitsky, 2015) before passing L and $m_i$ to two separate classifiers for the offensiveness detection task. These classifiers help learn class-invariant L and $m_i$, while the classifier utilizing $m_i^'$ learns class-dependent representation."}, {"title": "De-confounding", "content": "The premise of de-confounding and its necessity is described in Section 3.1. Before de-confounding, we assume, that there exists some W, which can project each L-i to ci. More formally, $WLCF = C$, where $LCF = [-L^{-1}, -L^{-2}, ..., -L^{-n}] \\in R^{768 \\times n}$, and $C = [-C_1, -C_2, ..., -C_n] \\in R^{768 \\times n}$, where $L^{-i}, c_i \\in R^{1 \\times 768}$ and there are n concepts.\nFor De-confounding, we aim to learn a projection matrix P (a nullspace projection matrix of W) such that projecting each ci through it yields counterfactual latent $LCF^'$ incapable of reconstructing C. The following theorem illustrates this.\nTheorem. If P is a nullspace projection matrix of W and $C' = P . C$ for all i, then $W . LCF' = 0$ for all i. For Proof, refer to Appendix Section \u00a7C. After de-confounding, we denote the latent L with a subscript, i.e., $L_{DC}$."}, {"title": "Experimental setups", "content": "The experimental setups and dataset details are elaborated in the Appendix Section F due to space constraints. Here we introduce various metrics and baselines."}, {"title": "Simulating Model Outcome", "content": "Definition. Simulatability, as defined in Hase et al. (2020), refers to how well explanations from model M help an observer (e.g. another simpler model, called a simulator model) predict outputs of M. Intuitively, better simulatability would reflect faithful explanation as there is a pattern between explanation as input and model prediction as output for the simulator to learn. In our proposed framework, explanation would mean causally sorted concepts in descending order. Sorting is necessary to preserve the causal importance order between the concepts."}, {"title": "Quantifying Trustworthiness", "content": "To measure the trustworthiness of a model, understanding whether its predictions originate from relevant concepts within the input is crucial, akin to assessing if the model is right for the right reasons.' To ascertain this, we annotate offensive memes from the test set with relevant concepts from a predefined set of 18 concepts. Subsequently, we employ averaged Precision@5 (P@5), Recall@5 (R@5), and Mean Average Precision (MAP@5) to assess the relevance of the top five concepts (w.r.t. the annotation) from both the $X_{cau}$ and the attribution set $X_{attr}$ obtained through various input attribution methods. Technical specifics are detailed in the Appendix Section E. Better scores in all these metrics reflect that the top attributed concepts align with human judgement, thus essentially making the model more trustworthy in return."}, {"title": "Baselines", "content": "We employ several standard input attribution methods to calculate attribution scores, dividing them into two groups based on their underlying mechanisms:\nPath-integral based Gradient Attribution:\nIntegrated Gradients (IG) (Sundararajan et al., 2017): IG attributes the importance of features by integrating gradients along the path from a baseline input to the actual input. It ensures that attribution is distributed across all input features in a manner that satisfies the completeness property (\u00a7Appendix A).\nDeepLIFT (Shrikumar et al., 2019): This method compares the activation of each neuron to a reference, and assigns importance to the neuron based on the comparison score.\nDeepLIFTSHAP (Lundberg and Lee, 2017a): A variant of DeepLIFT that aligns with SHAP values, combining both methods to compute attribution based on a cooperative game-theory approach.\nGradientSHAP (Ancona et al., 2018): GradientSHAP samples a point between an input-baseline pair and computes the mean gradients with respect to an output class across all such pairs.\nSimple Gradient Attribution:\nSaliency (Simonyan et al., 2014): It Identifies key input features by computing output gradients, highlighting features most influential to the model's prediction.\nInput \u00d7 Gradient (Shrikumar et al., 2017): This method computes the element-wise product of the input and its gradient to measure each feature's contribution to the prediction.\nWe use the Captum library (https://captum.ai/) to calculate attributions for each method and apply these attributions to three matrices: i) the concept matrix (C \u2208 R18\u00d7768), ii) the textual embedding (t \u2208 RB\u00d7M\u00d7768), and iii) the image embedding (v \u2208 RB\u00d7N\u00d7768).\nComparison with Our Methodology. Traditional input attribution methods outlined above rank features (or concepts) by assigning scores based on their importance to the model's output. In contrast, our approach adopts a causal perspective, using the RITE score to measure how much each concept causally influences the prediction.\nWhile our causal framework differs from standard attribution techniques, we explore an intriguing link between the completeness property of path-integral methods and causality, which we discuss in more detail in Appendix A."}, {"title": "Quantitative Findings", "content": "1. Classifier performance. Model performance on the test set is 70.36% as measured by the F1 score. This reflects the VisualBERT in our framework works well and even exceeds some of the benchmark models evaluated as a part of the FB Hateful Meme Dataset (Kiela et al., 2021)."}, {"title": "Analysing the model through Causal Lens", "content": "Is the model always right due to the right reason?\nNo, it is not always the case. We gain insight into\nthis by examining the model causally. In Figure\n6, we present two examples of memes from the\ntest set along with the model's predictions, both\ncorrectly classified as offensive.\nIn the first example, the meme's offensiveness\nis linked to the Holocaust and antisemitic ideas,\naccurately reflected in the Top-5 causal concepts.\nNotably, concepts like 'Violence', 'Holocaust', and\n'Nazism' from the gold standard set directly relate\nto the meme's context. With two out of the Top-5\ncausal keywords aligning with the meme's context,\nwe conclude that the model's prediction was correct\nfor the right reason.\nIn the second example, despite the meme being\nantisemitic, the identified causal concepts are irrel-\nevant to antisemitism, although highly offensive.\nHowever, certain keywords (such as 'Holocaust'\nand 'Genocide') identified by GradientSHAP are\nattributed. Three of these (\u2018Racism\u2019, 'Holocaust\u2019,\nand 'Genocide\u2019) directly relate to the meme's con-\ntext. Initially, it may seem the model was right for\nthe right reason. However, the absence of overlap\nbetween the set of causal concepts and the gold\nstandard concept set indicates that the model classi-\nfied the meme correctly but with erroneous causal\nattribution. This discrepancy suggests that although\naccurate input attributions exist, the model may\nbase its decision on different causal concepts. Such\nanalyses can help a user to trust the model predic-\ntions."}, {"title": "Error Analysis", "content": "In this section, we analyze the model's performance using causal concepts. For example, in the first row of Table 4 (meme index 32), the model identifies 'anti-muslim' among the top 5 causal keywords, even though they are irrelevant. The presence of 'Mohammed' alone leads the model to classify the meme as offensive, indicating a dataset-wide bias towards words like 'Mohammed'. Similarly for the second meme, the mere presence of the concept of 'blackness' was sufficient for classifying the meme as offensive despite having nothing related to racism in it. This also shows dataset bias where lots of racists (~ 47.1%) and religious memes (~ 39.3%) are present.\nSimilarly, in the third meme, the word 'jew' prompts the model to associate the meme with concepts like 'violence', and 'genocide' possibly due to insufficient visual context and association of the word jew in antisemitic offensive memes.\nIn the fourth meme, although offensive, the model finds it humorous due to a lack of background knowledge, especially regarding the word-play on SpongeBob, and the smiling SpongeBob face may further contribute to misclassification.\nThe fifth meme's misclassification can be attributed to a humorous background image unrelated to the meme text, creating a modality conflict. Note that the fourth and fifth memes which are classified as non offensive have the 'funny' keyword as a part of the Top 5 causal keywords.\nThe sixth meme shows a sexually offensive remark but due to conflicting visual modality (showing violence), this meme got misclassified as non offensive. The keywords generated can also be attributed to dataset bias as shown in the first two memes.\nBy examining causal keywords alongside model inputs, this type of error analysis offers insights into why the model made mistakes."}, {"title": "Conclusion", "content": "In this paper, we introduce a multimodal causal framework aimed at transparently analyzing Visual-BERT predictions. Guided by an SCM, the framework compels VisualBERT to base its decisions on interpretable human-understandable concepts.\nEvaluation on the Facebook hateful meme dataset reveals key insights: i) Input attribution methods may lack causal underpinning, ii) Modelling choices significantly influence relevant causal attributions, enhancing model trustworthiness. The qualitative analysis delves into whether the model is 'right for the right' reasons and uncovers causal factors behind misclassifications.\nThe simplicity and versatility of our framework (i.e. the underlying Structural Causal Model and its translation to modelling choices) allow its application across various tasks and multimodal models. Although we show the importance of our architecture on meme offensive detection tasks as a testbed, its application may be important in medicine where the need for trustworthy systems is paramount."}, {"title": "Limitations", "content": "While our approach demonstrates promising results, there are some limitations to consider. Firstly, the reliance on a specific dataset, such as the Facebook Hateful Meme dataset, and a specific model, like VisualBERT, may limit the generalizability of our findings to other datasets and models.\nSecondly, the concept annotation process introduces challenges as it relies on human annotators to define and refine the concept set. This process may introduce subjectivity and biases. To address this challenge, employing more robust annotation guidelines, inter-annotator agreement assessments, and sensitivity analyses can enhance the reliability of the concept annotation process.\nAt the outset, the concept of the paper acts as a seed or proof of concept, further generalizability of which is to be explored through a chain of related future studies. Specifically, exploring potential applications of the framework in other domains beyond meme classification would be valuable. The framework could be applied in areas such as content moderation, sentiment analysis, and trend analysis in social media, news media, marketing, and public opinion research and medicine."}, {"title": "Ethical Declaration", "content": "We acknowledge the potential for misuse of our annotated concepts, which could be employed to filter memes based on racial prejudices. To mitigate this risk, human moderation and intervention are crucial. The purpose of annotating concepts is to facilitate research into the analysis and un-dertstanding of offensive memes on the internet. When used appropriately, we believe it serves as a valuable resource.\nFurther the involvement of annotators to annotate potentially triggering meme may seem prob-lematic. On the other hand, we completely ensure annotators' well-being by making voluntary free session with institutional counsellors available at any time. Also, participation in this process was purely at their own wish and they have been warned on exposing themselves to various offensive and trigerring contents which were marked as a disclaimer. We followed four broad ethical principles during the annotation process: i) Annotators were fully briefed on the nature of the task and provided informed consent to participate. ii) Annotators had access to psychological support via our institutional counseling system. iii) Annotators were compensated fairly in line with institute regulations. iv) The privacy and confidentiality of student participants were strictly protected throughout the study.\nBy adhering to these protocols, we ensured that the ethical concerns associated with using students to label offensive memes were adequately addressed, prioritizing their well-being and ethical treatment. Our study underwent evaluation and approval by our Institutional Review Board (IRB) before proceeding for either annotating offensive memes in the first place or using students to annotate these memes."}]}