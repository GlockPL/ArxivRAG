{"title": "M2I2: Learning Efficient Multi-Agent Communication via Masked State Modeling and Intention Inference", "authors": ["Chuxiong Sun", "Peng He", "Qirui Ji", "Zehua Zang", "Jiangmeng Li", "Rui Wang", "Wei Wang"], "abstract": "Communication is essential in coordinating the behaviors of multiple agents. However, existing methods primarily emphasize content, timing, and partners for information sharing, often neglecting the critical aspect of integrating shared information. This gap can significantly impact agents' ability to understand and respond to complex, uncertain interactions, thus affecting overall communication efficiency. To address this issue, we introduce M2I2, a novel framework designed to enhance the agents' capabilities to assimilate and utilize received information effectively. M2I2 equips agents with advanced capabilities for masked state modeling and joint-action prediction, enriching their perception of environmental uncertainties and facilitating the anticipation of teammates' intentions. This approach ensures that agents are furnished with both comprehensive and relevant information, bolstering more informed and synergistic behaviors. Moreover, we propose a Dimensional Rational Network, innovatively trained via a meta-learning paradigm, to identify the importance of dimensional pieces of information, evaluating their contributions to decision-making and auxiliary tasks. Then, we implement an importance-based heuristic for selective information masking and sharing. This strategy optimizes the efficiency of masked state modeling and the rationale behind information sharing. We evaluate M212 across diverse multi-agent tasks, the results demonstrate its superior performance, efficiency, and generalization capabilities, over existing state-of-the-art methods in various complex scenarios.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has achieved significant milestones in various complex real-world applications, from Game AI (Osband et al. 2016; Silver et al. 2017, 2018; Vinyals et al. 2019) and Robotics (Andrychowicz et al. 2020) to Autonomous Driving (Leurent 2018). However, the landscape shifts markedly when applied to Multi-Agent Reinforcement Learning (MARL) (Lowe et al. 2017; Rashid et al. 2018; Yu et al. 2022), where unique challenges emerge. A principal challenge is the issue of partial observability, where agents must make decisions based on limited local observations, lacking a comprehensive view of the entire environment. In addressing this challenge, multi-agent communication emerges as a potent solution. By enabling agents to share and integrate information, this strategy facilitates a deeper collective understanding of their environment, stabilizing the learning process and promoting synchronized actions among agents.\nDespite advancements, existing methods in multi-agent communication primarily focus on sending policies, such as creating meaningful messages (Zhang, Zhang, and Lin 2019, 2020; Yuan et al. 2022), optimizing the timing (Singh, Jain, and Sukhbaatar 2018; Kim et al. 2019) and selecting appropriate partners (Ding, Huang, and Lu 2020; Niu, Paleja, and Gombolay 2021; Xue et al. 2022) for information exchange. However, these methods exhibit a significant gap in effectively integrating received information to enhance decision-making at receiving end. Typically, a large volume of received messages, processed by basic mechanisms like concatenation (Sukhbaatar, Szlam, and Fergus 2016) is fed directly into policy networks. This approach treats the information integration task as a black box, presupposing that neural networks can autonomously discern the most decision-important information, overlooking the intricacies of cognition and collaborative decision-making. In contrast, human cognitive processes (Etel and Slaughter 2019) demonstrate a superior ability for utilizing received information to perceive the environment and reduce the uncertainty of decision-making. This level of decision-making complexity, inherent in human cognition, is something that current multi-agent communication methods fail to capture.\nInspired by human cognitive processes and recent advancements in representation learning, as illustrated by models like BERT (Devlin et al. 2019) and MAE (He et al. 2022), we redefine the challenge of information integration in multi-agent communication as one of representation learning task. In this context, agents are tasked with developing representations for cooperative decision-making from a limited set of messages. These messages, constrained by partial observability and limited communication resources, only reflect a subset of the environmental states, often proving inadequate for a comprehensive understanding of environmental dynamics. Furthermore, not all received mes-"}, {"title": null, "content": "sages are beneficial for decision-making; some may introduce noise that disrupts the process. Consequently, we argue that an ideal representation in this context must be both sufficient-offering a comprehensive breadth of information for a deep understanding of the environment, and informative-sharply focused on data crucial for facilitating cooperative decision-making.\nFollowing this principle, we introduce M2I2, a novel approach incorporating two self-supervised auxiliary tasks to enhance efficiency of information integration. To meet the standard of \"sufficient\", M2I2 utilizes masked modeling techniques to reconstruct global states from received messages, furnishing agents with comprehensive information for informed decision-making. Essentially, M2I2 introduces a state-level Masked Auto-Encoder (MAE) designed for multi-agent communication. A distinctive aspect of this model is its unique masking mechanism, where the masks are dynamically determined by the communication strategies of the sending agents. Our empirical studies highlight that traditional random mask generating techniques (Devlin et al. 2019; He et al. 2022; Liu et al. 2022) fall short in addressing the complexities encountered in MARL. To this end, we develop the Dimensional Rational Network (DRN) to dynamically adjust the importance of each dimension of observed information. DRN is trained via a meta-learning paradigm, which takes into account the impacts on both decision-making and auxiliary tasks. After exploring the rationale of dimensional observations, we further propose an importance-based heuristic to discern which dimensions of observations should be masked at both training and execution stages, thereby enhancing the efficiency of masked state modeling and communication rationality.\nRegarding the \u201cinformative\u201d aspect, M2I2 integrates an inverse model to predict joint actions from sequential state representations, enabling agents to focus on information pivotal to their decisions. Furthermore, the inverse model enables agents to infer their teammates' intentions during decentralized decision-making processes. This capability is essential for facilitating team communication that goes beyond mere information exchange, enabling a deeper understanding of teammates' intentions and insights that can impact collective strategies. By introducing the self-supervised objective, M2I2 facilitates a deeper integration of received information, allowing agents to align closely with each other's intentions and leading to more efficient and informed decision-making across various scenarios.\nTo validate the effectiveness of M212, we conduct comprehensive evaluations across a range of multi-agent tasks with differing complexities, from Hallway and MPE to SMAC. Compared to state-of-the-art communication methods (Das et al. 2019; Yuan et al. 2022; Xue et al. 2022; Guan et al. 2022), M2I2 demonstrates superior performance, enhanced efficiency, and remarkable generalization capabilities. Our main contributions are summarized in three-fold:\n\u2022 To the best of our knowledge, M2I2 represents the first instance of incorporating self-supervised objectives, i.e., reconstructing global states and predicting joint actions, into the process of information integration under the condition of partial observability and restricted communica-"}, {"title": null, "content": "tion resources.\n\u2022 We integrate a meta-learning paradigm to model the contribution of each dimensional piece of information towards both decision-making and self-supervised objectives, therefore directing agents to transmit and focus on only the most relevant and important information.\n\u2022 Empirically, our proposed method not only facilitates efficient message integration, but also significantly improves communication efficiency, effectively bridging a vital research gap in MARL."}, {"title": "2 Related Works", "content": "Multi-agent communication has emerged as an indispensable component in MARL. Research in this domain has primarily concentrated on three fundamental questions:\nDetermining the optimal content of communication (what to communicate). CommNet (Sukhbaatar, Szlam, and Fergus 2016), as a pioneering work in this area, facilitated agents in learning continuous messages. Following CommNet, several methods have been developed to further refine the message learning process. VBC (Zhang, Zhang, and Lin 2019) aims to filter out noisy parts while retaining valuable content by limiting the variance of messages. TMC (Zhang, Zhang, and Lin 2020) introduces regularizers to reduce temporally redundant messages. NDQ (Wang et al. 2020) employs information-theoretic regularizers to develop expressive and succinct messages. MAIC (Yuan et al. 2022) enabled agents to customize communications for specific recipients, advancing tailored message learning.\nDeciding appropriate timing and partners for information exchange (when and whom to communicate). To enhance communication efficiency, approaches such as IC3Net (Singh, Jain, and Sukhbaatar 2018) and ATOC (Kim et al. 2019) have introduced gating networks to eliminate superfluous communication links. Similarly, SchedNet (Kim et al. 2019) and IMMAC (Sun et al. 2021) have modeled the significance of observations, using heuristic mechanisms to gate non-essential communication. Further, methods such as MAGIC (Niu, Paleja, and Gombolay 2021), I2C (Ding, Huang, and Lu 2020) and SMS (Xue et al. 2022) have been developed to identify the most suitable recipients. These approaches focus on modeling the contribution of shared information to the decision-making processes of the recipients, aiming to direct communication where it most influences decision-making.\nIntegrating incoming messages and making decisions (how to utilize received information). TarMAC (Das et al. 2019) has explored how agents can effectively assimilate crucial information from an abundance of raw messages. MASIA (Guan et al. 2022) take a different approach, employing an Auto-Encoder and a forward model for information integration and becoming the first to introduce self-supervised learning into multi-agent communication. However, MASIA is under the strong assumption that agents have access to all observations from their peers. In this work, we challenge and relax this assumption by introducing the masked state modeling technique, extending the approach"}, {"title": null, "content": "to more realistic environments where communication resources are constrained.\nFurthermore, our work is also related to the mask modeling techniques (He et al. 2022; Devlin et al. 2019; Liu et al. 2022). However, M2I2 is the first to apply this approach within the multi-agent communication domain, utilizing this technique to effectively tackle the challenge of imperfect information that arises from constrained communication resources. A key distinction of M2I2 lies in its innovative masking mechanism, which differs significantly from those used in prior methods. This unique approach will be elaborated upon in Section 4.4."}, {"title": "3 Preliminary", "content": "In this work, we focus on fully cooperative multi-agent tasks, characterized by partial observability and necessity for inter-agent communication. These tasks are modeled as Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) (Oliehoek, Amato et al. 2016), represented by the tuple G = (N, S, O, A, \\Omega, P, R, \\gamma, M). In this formulation, N = \\{1, ..., n\\} denotes the set of agents, S represents the global states, O describes the observations available to each agent, A signifies the set of available actions, \\Omega is the observation function mapping states to observations, P is the transition function illustrating the dynamics of the environment, R is the reward function dependent on the global states and joint actions of the agents, \\gamma is the discount factor, and M specifies the set of messages that can be communicated among the agents. At each time-step t, each agent i \\in N has access to its own observation o_t^i \\in O determined by the observation function \\Omega(o|s_t). Additionally, each agent can receive messages c_t^i = \\sum_{j\\neq i} m_t^j; from teammates j \\in N. Utilizing both the observed and received information, agents then make local decisions. As each agent selects an action, the joint action a_t results in a shared reward r_t = R(s_t, a_t) and transitions the system to the next state s_{t+1} according to the transition function P(s_{t+1}|s_t, a_t). The objective for all agents is to collaboratively develop a joint policy \\pi to maximize the discounted cumulative return \\sum_t \\gamma^t r_t.\nCentralized Training with Decentralized Execution (CTDE) (Lowe et al. 2017) stands as a promising paradigm in cooperative multi-agent tasks. Within this paradigm, individual agents make decisions based on local information, while their policies are trained through a centralized manager with access to global information. This work aligns with the prevailing value-based approaches within the CTDE framework. During the training phase, the joint action-value function Q_{tot}(s_t, a_t; \\theta) will be trained to minimize the expected Temporal Difference (TD) error:\n$L_{RL}(\\theta) = E_{s_t, a_t, r_t, s_{t+1}\\in D} [y_t \u2013 Q_{tot}(s_t, a_t;\\theta)]^2$,\nwhere D is the replay buffer, $y_t = r_t + \\gamma max_{a_{t+1}}Q_{tot}(s_{t+1}, a_{t+1};\\theta')$ is the TD target. $\\theta'$ denotes the parameters of the target network, which is periodically updated by $\\theta$."}, {"title": "4 Methodology", "content": "The framework of M2I2 are shown in Figure 1, with its core components highlighted for effective multi-agent communication. A key component of M212 includes a message encoder and a state decoder, functioning collectively as an extendable module to reconstruct the environmental states in an auto-encoding manner. This design allows for the reconstruction of global states from received messages (i.e. limited observations), thereby providing agents with sufficient information to make well-informed cooperative decisions. Furthermore, M2I2 integrates an inverse model capable of predicting joint actions based on consecutive state representations. This model is pivotal in equipping agents with the ability to infer the intentions of their teammates while making decisions. Another standout feature of M2I2 is DRN, which is adept at evaluating the importance of various observed information based on their gradient contributions to both auxiliary and RL tasks. The DRN is continually refined through a meta-learning paradigm, which effectively avoid the trivial solution and local optimum issues during training. By identifying and emphasizing important information, DRN enables agents to share and focus on important data, thereby optimizing the communication process for efficiency and effectiveness."}, {"title": "4.2 Communication Process of M212", "content": "The communication process of M2I2 can be summarized as the following four steps.\nSelectively masking unnecessary observations for information sharing: At each time-step, agents utilize the DRN to evaluate the importance of observed information in supporting decision-making and auxiliary tasks. This importance is quantified as $w_i = \\{w_{id}|d \\in [1,D]\\}$, where i represents the ID of the agent and D is the dimensionality of observed information. To optimize communication efficiency while ensuring effective decision-making, a topK mechanism is applied for generating observation masks, which is formulated as:\n$topK(w_i) =  \\begin{cases}\nw_{id}, & \\text{if } d \\text{ in top-k largest dimensions} \\\n0, & \\text{others.}\n\\end{cases}$\nThis process allows each agent to selectively share the most important dimensions of the observations while non-essential dimensions are masked to zero. The resulting shared information is represented as:\nm_t^i = o_t^i \\odot topK(\\omega_t^i),\nwhere $m_t^i$ denotes the messages, i.e. masked and weighted observations, and $\\odot$ is an element-wise Hadamard product function. The DRN, central to this selectively observation mask process, is trained using a meta-learning paradigm (detailed in Section 4.4), which enables it to dynamically adjust its assessments based on both decision-making and auxiliary task performances.\nIntegrating received information: Upon receiving messages, M2I2 integrates a scaled dot-product self-attention"}, {"title": null, "content": "module (Vaswani et al. 2017) to adeptly process incoming messages. Specifically, the received messages are transformed into corresponding queries Q, keys K, and values V. The process of integrating this information is mathematically represented as follows:\nz_t^i = for(softmax(\\frac{QK^T}{\\sqrt{D_k}})V)\nwhere $\\Theta_E$ represents the parameters of Message Encoder and $D_k$ represents the dimension of a single key. This message encoder exhibits two notable benefits. Firstly, the encoder's design makes it adaptable to diverse communication contexts, accommodating varying numbers and arrangements of agents. Secondly, by utilizing a weighted sum mechanism, the self-attention module integrates information without excessively expanding the agents' local policy spaces.\nImplicitly Inferring the global states and teammates' intention: Following this, M2I2 encode the received messages into a compact representation. Unlike traditional methods that rely solely on RL objectives, which often struggle to learn effective representation from the limited and noisy messages, M2I2 incorporates two self-supervised objectives. These objectives are specifically designed to develop a representation that is both \"sufficient\" for a thorough understanding of the environment and \u201cpromising\u201d for aiding cooperative decision-making. Although the involved self-supervised auxiliary tasks are conducted only during training, they can implicitly enhance the message encoder's ability to interpret the environment and predict teammates' intentions during decentralized decision-making process.\nMaking cooperative decisions: The culmination of the M2I2 process is reflected in the agents' ability to make co-"}, {"title": null, "content": "operative decisions. Here, the enriched integrated information, blended with each agent's personal observations, is channeled into the policy network. The process of decision-making is mathematically represented as follows:\n$a_t^i = \\pi_i(o_t^i, z_t^i; \\theta_{\\pi})$\nwhere $\\theta_{\\pi}$ represents the parameters of policy network. This convergence of individual perception and collective insights is crucial, as it empowers agents to make decisions that are not only informed, but also aligned with the overarching goals and strategies of the team."}, {"title": "4.3 Self-Supervised Auxiliary Tasks for Efficient Multi-Agent Communication", "content": "Given the inherent constraints in agents' perceptual capabilities and the limitations of communication bandwidth, the information encoded by the message encoder often captures only a fraction of the environment's state. To ensure that agents have access to sufficient information for effective decision-making, we employ a state decoder. This decoder is tasked with reconstructing the global state of the environment, represented by $\\hat{s}_t = gen(z_t)$. The associated loss function is computed using the mean squared error between the reconstructed and global states:\n$L_{RC}(\\Theta_E, \\Theta_D) = E_{z_t, s_t}||s_t - \\hat{s}_t||^2$.\nBy combining the message encoder with the state decoder, we effectively create an extendable masked state modeling. This masked modeling is characterized by a unique masking process, generated both by the environment and the agents themselves. This approach enables the integrated representation $z_t$ to effectively represent the global states of the en-"}, {"title": null, "content": "vironment, thus overcoming the challenges posed by their limited observational scope and communication capacity.\nTo augment the capability of agents in focusing on information promising to their decisions and aligning with the intentions of their counterparts, we introduce an inverse model, denoted as $I_{\\theta_I}: Z \\times Z \\rightarrow A^n$, where Z is the space of state representations. This model is crafted to predict the joint actions that agents take to transition from one state representation to the next. Formally, given a triplet $(z_t, a_t, z_{t+1})$ composed of two consecutive state representations and joint actions taken by agents, we parameterise the conditional likelihood as $p(a_t) = I_{\\theta_I}(z_t, z_{t+1})$, where $I_{\\theta_I}$ embodies a two hidden layers MLP followed by a softmax operation. The parameters of both inverse model $\\theta_I$ and message encoder $\\theta_E$ are optimized via a maximum likelihood approach. The corresponding loss function is formulated as:\n$L_{INV}(\\Theta_E, \\Theta_I) = E_{z_t, a_t} ||a_t - \\hat{a}_t||^2,$\nwhere this loss function measures the discrepancy between the predicted joint actions and the actual joint actions. At first, the objective encourages agents to focus on information that are controllable and expressive pertinent to cooperative decision-making. This focus is crucial for agents to effectively handle elements that they can influence, enhancing their relevance in a coordinated environment. Moreover, the deeper integration of received information facilitated by the model allows agents to implicitly infer the intentions of others during execution. This capability significantly bolsters agents' potential to align their actions with the intentions of their teammates. Such alignment is not only technically beneficial, but also aligns with cognitive research findings (Etel and Slaughter 2019), which underscore the importance of intention understanding in effective social interactions."}, {"title": "4.4 DRN for Importance Modeling", "content": "DRN is designed to discern the importance of different dimensions of observed information, specifically tailoring to the needs of decision-making and auxiliary tasks. The primary challenge here lies in the dynamic nature of the MARL and the variability in communication needs across different stages of a mission. Unlike static scenarios, the importance of information can change dramatically, requiring the DRN to adapt continuously and efficiently. This challenge transcends the realm of simple optimization problems, typically addressed with first-order gradients. To effectively navigate this complexity, we employ a meta-learning approach (Liu, Davison, and Johns 2019), as it is well-suited to circumvent trivial solutions and local optima that often hinder training efficiency. It allows the DRN to dynamically adjust its understanding of information importance in a sophisticated manner, aligning closely with the overarching goals of decision-making and auxiliary tasks.\nIt is important to note that within our training framework, only the parameters of $\\Theta_{DRN}$ are refined using this meta-learning approach. The other parameters of the system are updated using conventional first-order gradient methods. Specifically, in the first regular training step, we focus on training the combined set of parameters $\\theta = (\\Theta_E, \\theta_{\\pi}, \\Theta_D, \\theta_I)$"}, {"title": null, "content": "by jointly minimizing the auxiliary tasks and RL losses, which is formalized by\n$arg min L_{M2I2}(\\theta, \\Theta_{DRN}),$\n$\\theta$\nwhere $L_{M2I2}(\\theta, \\Theta_{DRN}) = L_{RL} + \\beta(L_{RC} + L_{INV})$ and $\\beta$ is a coefficient that controls the balance between RL objective and auxiliary objectives.\nIn the second meta-learning-based step, $\\Theta_{DRN}$ is updated by using the second-derivative technique (Liu, Davison, and Johns 2019; Li et al. 2022). This technique is crucial for adjusting $\\Theta_{DRN}$ to better discern the importance of various information dimensions that significantly impact both RL and auxiliary tasks. The update process involves calculating the gradients of $\\Theta_{DRN}$ in relation to the combined performance metrics from these tasks, encapsulated by $L_{M2I2}$. Formally, we update $\\Theta_{DRN}$ by\n$arg min L_{M2I2}(\\theta^{trial}, \\Theta_{DRN}),$\n$\\Theta_{DRN}$ = E\nD\nwhere $\\theta^{trial} = (\\Theta_E^{trial}, \\theta_{\\pi}^{trial}, \\Theta_D^{trial}, \\theta_I^{trial})$ is the trial weights of the $\\theta$ after one gradient update using the M2I2 loss defined in Equation 8. We formulate the updating of such trial weights as follows:\n$\\theta^{trial} = \\theta - l_o\\nabla_{\\theta} L_{M2I2},$\nwhere $l_o$ is the learning rate. Note that the calculation of trial weights excludes the step of gradient back-propagation. Thus, $\\Theta_{DRN}$ is updated through the second-derivative gradient of $\\theta$. By doing so, we ensure that $\\Theta_{DRN}$ is continuously fine-tuned by the gradient contributions of $L_{M2I2}$, allowing DRN to dynamically evaluate the importance of each observed dimension. Our visualization study in Appendix D further validates this approach: it reveals that the DRN not only distinguishes varying levels of importance across different agent types and observation categories but also dynamically adjusts these importance weights over time, adapting to the evolving demands of the task."}, {"title": "5 Experiment", "content": "In this section, our experimental design is meticulously structured to address three fundamental questions:\n\u2022 RQ1. How does M212's performance and efficiency compare to leading communication methods?\n\u2022 RQ2. What specific components within M2I2 are instrumental to its performance?\n\u2022 RQ3. Is M2I2 versatile enough to be applied across a range of tasks, and can it be seamlessly integrated with multiple existing baselines?"}, {"title": "5.1 Setup", "content": "Benchmarks. In order to demonstrate the effectiveness and generality of M2I2, we conducted extensive experiments across four popular multi-agent communication benchmarks: Hallway (Wang et al. 2020), Predator-Prey (PP) (Lowe et al. 2017), SMAC (Samvelyan et al. 2019) and SMAC-Communication (Wang et al. 2020). Each of these benchmarks provides a substantial testbed for evaluating"}, {"title": null, "content": "multi-agent communication strategies. Baselines. For comparative analysis, we select a diverse set of baselines. This includes QMIX (Rashid et al. 2018), a well-established MARL algorithm that operates without a communication mechanism. To assess our method's performance in the context of communication-enhanced MARL, we also include contemporary state-of-the-art communication methods: TarMAC (Das et al. 2019), MAIC (Yuan et al. 2022), SMS (Xue et al. 2022), and MASIA (Guan et al. 2022). Each of these baselines represents a significant stride in the development of communication strategies within the MARL framework, providing a robust backdrop for evaluating the efficacy and innovation of our proposed approach.\nHyperparameters. To ensure reproducibility, the intricate details of our method's architecture, and our hyperparameter choices are extensively detailed in the Appendix C."}, {"title": "5.2 Performance (RQ1)", "content": "Our evaluation begins with a comparative analysis of the learning curves of M212 against a range of baseline methods across diverse environments. This comparison is aimed at assessing the comprehensive performance of M2I2. As depicted in Figure 2, M2I2 demonstrates a notable performance advantage, consistently outperforming all baselines by a significant margin in each tested environment, indicating M2I2's strong applicability across scenarios of varying complexity. Specifically, in Hallway, where the reward signals of environment is sparse, many methods exhibit poor performance or fail to learn effectively. In contrast, M2I2 rapidly achieves a 100% win rate. This success can be attributed to our proposed auxiliary tasks, which appear to significantly aid agents in understanding the locations and intentions of their teammates. In PP, where the communication-free method QMIX struggles, most communication methods demonstrate effectiveness. Notably, M2I2"}, {"title": "5.3 Efficiency (RQ1)", "content": "Efficiency is a long-standing issue in multi-agent communication, as many real-world applications operate under limited communication resources. Therefore, it is crucial to achieve promising performance while maintaining a low communication resource cost. Notably, the performance of M2I2, as reported in Figure 2, was achieved with a 60% communication frequency, where 60% is a hyper parameter defined by our proposed top-k mechanism in Section 4.1. To further understand the communication efficiency of M2I2, we adopted a mechanism inspired by MAGIC (Niu, Paleja,"}, {"title": "5.4 Ablation (RQ2)", "content": "In order to understand the contribution of each module within M2I2, we conduct an ablation study across three SMAC-Communication scenarios. The evaluated configurations are as follows: M2I2 is our comprehensive method as proposed in the study. QMIX acts as a baseline, representing the fundamental functionality devoid of M2I2's enhancements. M212 w/o DRN is a variant of M2I2 operates without DRN and top-k filter mechanism. Instead, the observation level masking (i.e. when to communicate) is generated randomly during both the training and sampling processes. M212 w/o DRN & INV is a further simplified version of M2I2, excluding both the DRN and inverse loss, retaining only the mask state modeling. The results, as depicted in Figure 3(a), highlight the consistent performance improvements attributable to each component across the three scenarios. This underscores the effectiveness of our proposed auxiliary tasks and the DRN in enhancing the overall functionality of M2I2. The clear distinction in performance between these configurations serves to validate the integral role each module plays in the efficacy of the M2I2 framework.\nFurthermore, to gain insight into how varying communication frequencies impact M2I2's performance, we executed an ablation study with communication rates set at 0.8, 0.6, 0.4. The findings, depicted in Figure 3(b), consis-"}, {"title": "5.5 Generation (RQ3)", "content": "Our previous experiments have conclusively shown M2I2's robust performance in a variety of environments, encompassing scenarios with diverse complexities and scales. Building on this, we extend our evaluation of M2I2 to assess its generality across various MARL baselines, including QMIX, VDN, QPLEX, MAPPO and MADDPG. To provide a clear representation, we present the test win rates for the scenario 1o_2r_vs_4r in Figure 4(a). Remarkably, M2I2 demonstrates consistently superior performance across all these baselines, often achieving a significant margin of improvement. This observation shows that M2I2 is effective not only with off-policy algorithms but also with on-policy approaches, and not only with Q-learning methods but also with policy gradient methods, demonstrating M212's broad applicability and effectiveness within the MARL domain. Additionally, we extend M2I2's application to scenarios featuring varying sight ranges. The results, as depicted in Figure 4(b), confirm that M2I2 not only adapts well but also maintains consistent performance improvements across different sight ranges, further underscoring its versatility and efficacy in enhancing MARL strategies."}, {"title": "6 Conclusion", "content": "In this work, we delve into the complexities of multi-agent information integration. We introduce M2I2, an approach that incorporates two auxiliary tasks to enhance communication efficiency. We specifically design a MAE and an inverse model. These elements play a crucial role in guiding the processes of information filtering and integration, thereby significantly enhancing the agents' ability to navigate uncertain environments and dynamically adapt to their teammates. To substantiate our claims, we conduct exhaustive experiments across a multitude of benchmarks. The results from these tests not only validate the effectiveness of M2I2 but also its efficiency and adaptability in various multi-agent scenarios."}, {"title": "A Notations", "content": "For clarity and precise symbol definitions, we provide a comprehensive list of notations used in this work in Table. 2."}, {"title": "B Implementation Details", "content": "To explain the communication process and training paradigm, we provide the pseudo-code for M2I2 in Algorithm 1."}, {"title": "C Experimental Setup", "content": "All the Result are reported by averaging the results of 5 random seeds. The four test environments of the experiment are shown in Figure 5."}, {"title": "C.1 Environment", "content": "Hallway. This task revolves around multiple Markov chains where n are initially distributed randomly across n chains with varying lengths. The goal is for all agents to simultaneously reach the goal state, despite the constraint of partial observability. For increased challenge, n is set to 4, and each chain has a unique length specified as (4, 6, 8, 10). At each time step, an agent has a limited observation of its current position, and it can choose from three actions: move left, move right, or remain stationary. An episode concludes when any agent reaches state g. The agents collectively succeed and receive a shared reward of 1 only if they all reach state g concurrently. Otherwise, the reward is 0.\nHallwaygroup. This is a variant of Hallway which agents are divided into different groups and different groups have to arrive at different times. To intensify the challenge, we escalated the complexity by increasing both the number of agents and the lengths of the Markov chains. Specifically, in the Hallway benchmark, we set the number of agents to 4, with Markov chain"}, {"title": null, "content": "lengths varying as (4,6,8,10). In the Hallwaygroup variant, we increased the number of agents to 7, dividing them into two groups. The lengths of the Markov chains for these two groups were set to (3,5,7) and (4,6,8,10), respectively.\nPredator Prey (PP). In this task, the objective for predators is to capture randomly moving preys. Each predator has the ability to navigate in four distinct directions, but their perspectives are limited to local views. The game dynamics involve multiple predators attempting to capture the same prey simultaneously, resulting in a team reward of 1. However, if only a single predator successfully captures a prey, they incur a penalty with a score of -2. The game concludes when all preys have been captured. To introduce varying difficulty levels, different grid sizes and numbers of agents are employed for the Predator-Prey (PP) scenarios. The specific configurations for two PP scenarios are detailed in Table. 3.\nStarCraft Multi-Agent Challenge (SMAC). This task revolves around a series of complex scenarios inspired by StarCraftII, a real-time strategy game. Decentralized agents engage in combat against the built-in AI, each having a limited field of vision restricted to adjacent units. Observations include relative positions, distances, unit types, and health statuses. Agents struggle to perceive the status of entities beyond their immediate vicinity, creating uncertainty. The action space varies across scenarios, often including move, attack, stop, and no-option. During training, global states with coordinates and features of all agents are"}, {"title": null, "content": "accessible. Rewards are based on factors like damage infliction, eliminating units, or victory.\nSMAC-Communication. To emphasize the role of communication, we select three super hard maps and further adopt the configuration used in (Wang et al. 2020). The specifics of the chosen scenarios are delineated as follows.\n5z_vs_lul. A team of 5 Zealots faces a formidable Ultralisk. Victory requires mastering a complex micro-management technique involving positioning and attack timing.\n10_10b_vs_1r. In a cliff-dominated terrain, an Overseer spots a Roach. 10 Banelings aim to eliminate the Roach for victory. Banelings can choose silence, relying on the Overseer to communicate its location"}]}