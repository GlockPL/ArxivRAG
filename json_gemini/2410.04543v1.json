{"title": "PULLBACK FLOW MATCHING ON DATA MANIFOLDS", "authors": ["Friso de Kruiff", "Erik Bekkers", "Ozan \u00d6ktem", "Carola-Bibiane Sch\u00f6nlieb", "Willem Diepeveen"], "abstract": "We propose Pullback Flow Matching (PFM), a novel framework for generative modeling on data manifolds. Unlike existing methods that assume or learn restrictive closed-form manifold mappings for training Riemannian Flow Matching (RFM) models, PFM leverages pullback geometry and isometric learning to preserve the underlying manifold's geometry while enabling efficient generation and precise interpolation in latent space. This approach not only facilitates closed-form mappings on the data manifold but also allows for designable latent spaces, using assumed metrics on both data and latent manifolds. By enhancing isometric learning through Neural ODEs and proposing a scalable training objective, we achieve a latent space more suitable for interpolation, leading to improved manifold learning and generative performance. We demonstrate PFM's effectiveness through applications in synthetic data, protein dynamics and protein sequence data, generating novel proteins with specific properties. This method shows strong potential for drug discovery and materials science, where generating novel samples with specific properties is of great interest.", "sections": [{"title": "1 INTRODUCTION", "content": "Since the rise of machine learning in the scientific domain, researchers have focused on the derivation of bigger models trained on increasingly large datasets, e.g., to solve weather forecasting (Bodnar et al., 2024) or protein structure and function generation (Hayes et al., 2024). Relying on such scaling laws is not feasible in many areas of science where data is limited and precise modeling of physical phenomena is crucial. In research where data collection is challenging, accurate interpolation between data points is essential for making reliable predictions and generating realistic representations of complex systems. However, current methods lack the mathematical foundations to accurately interpolate in the latent space (Arvanitidis et al., 2017) and do not capture the geometric structure of the data (Wessels et al., 2024). Our goal is to develop mappings that enable precise interpolation in the latent space and facilitate efficient and accurate generation on data manifolds, ultimately advancing the ability to model complex physical phenomena with limited data.\nWe consider modeling the data under the manifold hypothesis, which states that high-dimensional data lies on a lower dimensional manifold. This has been successfully applied to several downstream tasks in various fields across the scientific domain (Vanderplas & Connolly, 2009; Dsilva et al., 2016; No\u00e9 & Clementi, 2017). Modeling the data in its intrinsic dimension allows for efficient analysis Diepeveen et al. (2024) and generation Rombach et al. (2022). Furthermore, accurately capturing the"}, {"title": "2 NOTATION", "content": "We give a brief summary of the notation used in the paper, and give a more extensive background on Riemannian and pullback geometry in Appendix A.\nA manifold $M$ is a topological space that locally resembles Euclidean space. A d-dimensional manifold $M$ around a point $p \\in M$ is described by a chart $\\psi : U \\rightarrow \\mathbb{R}^d$, where $U \\subset M$ is a neighborhood of $p$. The chart provides a local coordinate system for the manifold. The tangent space at a point $p \\in M$, denoted $T_pM$, is the vector space of all tangent vectors at that point.\nA smooth manifold $M$ equipped with a Riemannian metric is called a Riemannian manifold and is denoted by $(M, \\langle \\cdot, \\cdot \\rangle_M)$. The Riemannian metric $\\langle \\cdot, \\cdot \\rangle_M$ is a smoothly varying inner product defined on the tangent spaces $T_pM$ for all points $p \\in M$, and it defines lengths and angles on the manifold. A geodesic, $\\gamma_{p,q}(t)$ is the shortest path between two points $p, q \\in M$, generalizing the notion of a straight line in Euclidean space.\nThe exponential map $\\exp_p : T_pM \\rightarrow M$ maps a tangent vector $\\Xi_p$ to a point on the manifold by following the geodesic in the direction of $\\Xi_p$ starting from $p$. The inverse of the exponential map is the logarithmic map, denoted by $\\log_p : M \\rightarrow T_pM$, which returns the tangent vector corresponding to a given point on the manifold.\nIn this work, we consider a $d$-dimensional Riemannian manifold $(M, \\langle \\cdot, \\cdot \\rangle_M)$, and a smooth diffeomorphism $\\varphi : \\mathbb{R}^d \\rightarrow M$, such that $\\varphi(\\mathbb{R}^d) \\subseteq M$ is geodesically convex, meaning that any pair of points within this subset are connected by a unique geodesic. This mapping allows us to pullback the geometric structure of $M$ to $\\mathbb{R}^d$ by defining the pullback metric on $\\mathbb{R}^d$. Specifically, for tangent vectors $\\Xi, \\Phi \\in T\\mathbb{R}^d$, the pullback metric is defined as\n$\\langle \\Xi, \\Phi \\rangle_{\\varphi}^* := \\langle \\varphi_*[\\Xi], \\varphi_*[\\Phi] \\rangle_M,$\n(1)\nwhere $\\varphi_*$ is the pushforward of tangent vectors under $\\varphi$. Through this construction, various geometric objects in $M$, such as distances and geodesics, can be expressed in terms of their counterparts in $\\mathbb{R}^d$ with respect to the pullback metric. The distance function $d_{\\varphi}^* : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ on $\\mathbb{R}^d$ with the pullback metric is given by,\n$d_{\\mathbb{R}^d}^\\varphi(x_i, x_j) = d_M(\\varphi(x_i), \\varphi(x_j)),$\n(2)\nwhere $d_M$ denotes the Riemannian distance on $M$. The length-minimizing geodesic connecting $x_i$ and $x_j$ in $\\mathbb{R}^d$ with respect to the pullback metric $\\gamma_{x_i, x_j}^\\varphi : [0,1] \\rightarrow \\mathbb{R}^d$ is given by,\n$\\gamma_{x_i, x_j}^\\varphi(t) = \\varphi^{-1}(\\gamma_{\\varphi(x_i), \\varphi(x_j)}^M(t)),$\n(3)\nhere $\\gamma_{\\varphi(x_i), \\varphi(x_j)}^M$ denotes the geodesic in $M$ connecting $\\varphi(x_i)$ and $\\varphi(x_j)$. This enables computation of geodesics and distances in $\\mathbb{R}^d$ using the geometry of $M$, as stated in Prop. 2.1 of Diepeveen (2024)."}, {"title": "3 PULLBACK FLOW MATCHING", "content": "We propose Pullback Flow Matching (PFM), a novel framework for generative modeling on data manifolds using pullback geometry. Our goal is to transform samples from a simple distribution $x_0 \\sim p$ on the data manifold D into a complex target distribution $x_1 \\sim q$, also on D. Ideally, we would perform this transformation using Riemannian Flow Matching (RFM), see Appendix A for a summary, on $(D, \\langle \\cdot, \\cdot \\rangle_D)$ by optimizing the objective from Chen & Lipman (2024),\n$L_{RFM}(\\eta) = \\mathbb{E}_{t, q(x_1), p(x_0)} \\big\\| v_t^{\\mathbb{D}}(\\gamma_{x_1, x_0}^{\\mathbb{D}}(t); \\eta) - \\dot{\\gamma}_{x_1, x_0}^{\\mathbb{D}}(t) \\big\\|_{\\mathbb{D}}^2,$\n(4)\nwhere $\\| \\cdot \\|_{\\mathbb{D}}$ is the norm induced by the metric $\\langle \\cdot, \\cdot \\rangle_{\\mathbb{D}}$ on the manifold, and $\\dot{\\gamma}_{x_1, x_0}^{\\mathbb{D}}(t)$ denotes the time-derivative of the geodesic between $x_0$ and $x_1$ on D. Solving this objective is intractable due to a lack of closed-form manifold mappings. Existing methods address this by learning or assuming restricted manifold mappings for simulation-free RFM training Chen & Lipman (2024); Kapusniak et al. (2024). We overcome this limitation by using pullback geometry to derive expressive closed-form manifold mappings, enabling flexible, simulation-free RFM training.\nFollowing Diepeveen (2024), we define a new metric on the ambient space $\\mathbb{R}^d$ using the pullback metric. We assume a learned isometry $\\varphi_\\theta$ that approximates geodesics $\\gamma_{x_i, x_j}^{\\varphi_\\theta}$ on $(\\mathbb{R}^d, \\langle \\cdot, \\cdot \\rangle_{\\varphi_\\theta}^*)$ to those $\\gamma^{\\mathbb{D}}$ on $(\\mathbb{D}, \\langle \\cdot, \\cdot \\rangle_{\\mathbb{D}})$. Rewriting the RFM objective under the pullback framework yields the objective,\n$L_{PFM}(\\eta) = \\mathbb{E}_{t, q(x_1), p(x_0)} \\big\\| v_t^{\\varphi_\\theta}(\\gamma_{\\varphi_\\theta(x_1), \\varphi_\\theta(x_0)}^{\\varphi_\\theta}(t); \\eta) - \\dot{\\gamma}_{\\varphi_\\theta(x_1), \\varphi_\\theta(x_0)}^{\\varphi_\\theta}(t) \\big\\|_{\\varphi_\\theta}^*,$ (5)\nwhere $\\| \\cdot \\|_{\\varphi_\\theta}^*$ is the norm induced by the pullback metric $\\langle \\cdot, \\cdot \\rangle_{\\varphi_\\theta}^*$. By applying Equation 3, we can reformulate the PFM objective in terms of manifold mappings on M,\n$L_{PFM}(\\eta) = \\mathbb{E}_{t, q(x_1), p(x_0)} \\big\\| v_t^{M}(\\gamma_{\\varphi_\\theta(x_1), \\varphi_\\theta(x_0)}^{M}(t); \\eta) - \\dot{\\gamma}_{\\varphi_\\theta(x_1), \\varphi_\\theta(x_0)}^{M}(t) \\big\\|_{M}^2,$\n(6)\nwhere $\\| \\cdot \\|_{M}$ is the norm induced by the metric on M. Assuming a latent manifold M with closed-form mappings allows for simulation-free training. For efficiency, we model the $d$-dimensional latent manifold as a product manifold, $M = M_{\\mathbb{d}'} \\times \\mathbb{R}^{d-\\mathbb{d}'}$, where $M_{\\mathbb{d}'} \\subset M$. By encoding samples close to the submanifold $M_{\\mathbb{d}'}$, isometric learning ensures geodesics on $M_{\\mathbb{d}'}$ closely match geodesics on M. As a result, we formulate the $\\mathbb{d}'$-PFM objective,\n$L_{\\mathbb{d}'-PFM}(\\eta) = \\mathbb{E}_{t, q(x_1), p(x_0)} \\big\\| v_t^{M_{\\mathbb{d}'}}(\\gamma_{\\varphi_\\theta(x_1), \\varphi_\\theta(x_0)}^{M_{\\mathbb{d}'}}(t); \\eta) - \\dot{\\gamma}_{\\varphi_\\theta(x_1), \\varphi_\\theta(x_0)}^{M_{\\mathbb{d}'}}(t) \\big\\|_{M_{\\mathbb{d}'}}^2,$ (7)\nwhere $\\| \\cdot \\|_{M_{\\mathbb{d}'}}$ denotes the norm on the submanifold $M_{\\mathbb{d}'}$. The $\\mathbb{d}'$-PFM objective offers two key benefits. First, defining the objective on the submanifold $M_{\\mathbb{d}'}$ results in computational speed-ups during training. Second, the known geometry on the submanifold simplifies the training dynamics of the vector field $v_t(\\cdot; \\eta)$, requiring fewer epochs and parameters $\\eta$."}, {"title": "4 LEARNING ISOMETRIES", "content": "The motivation for learning isometries $\\varphi_\\theta$-metric-preserving diffeomorphisms-is to enable a latent (sub)manifold that supports interpolation with closed-form geometric mappings, facilitating simulation-free training of PFM. Building on the framework of Diepeveen (2024), summarized in Appendix A, we propose a more expressive parameterization of learnable diffeomorphisms $\\varphi_\\theta$ through Neural ODEs and enhance the objective for scalable isometric learning on data manifolds."}, {"title": "4.1 PARAMETERIZING DIFFEOMORPHISMS", "content": "We parameterize diffeomorphisms, invertible and differentiable functions between two manifolds, specifically $\\varphi : \\mathbb{R}^d \\rightarrow M$. In practice, we construct the latent manifold as a product manifold, $M = M_{\\mathbb{d}'} \\times \\mathbb{R}^{d-\\mathbb{d}'}$ and the diffeomorphism $\\varphi$ as,\n$\\varphi := [\\psi^{-1}, Id-\\mathbb{d}'] \\circ \\phi_{\\theta} \\circ T_\\mu,$\n(8)\nwhere $\\psi : U \\rightarrow \\mathbb{R}^{\\mathbb{d}'}$ a chart on a (geodesically convex) subset $U \\subset M_{\\mathbb{d}'}$ of the $\\mathbb{d}'$-dimensional latent submanifold $(M_{\\mathbb{d}'}, \\langle \\cdot, \\cdot \\rangle_{M_{\\mathbb{d}'}})$, $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ a diffeomorphism and $T_\\mu(x) = x - \\mu$. We choose this construction following Diepeveen (2024) because the manifold hypothesis translates to assuming the data manifold is homeomorphic to $M_{\\mathbb{d}'}$. In such case, the rest of the latent manifold should be mapped close to zero, e.g. $\\varphi(x_i)$ is close to $M_{\\mathbb{d}'} \\times 0^{d-\\mathbb{d}'}$ in terms of the metric on $M$.\nWe generate the diffeomorphism $\\phi_{\\theta}$ by solving a Neural ODE (Chen et al., 2018). The advantage of this approach is threefold, i) this parameterization of diffeomorphisms is more expressive and efficient to train compared to Invertible Residual Networks (Behrmann et al., 2019) as chosen by Diepeveen (2024), ii) based on some mild technical assumptions a Neural ODE can be proven to generate proper diffeomorphisms, see Appendix B for the proof, and iii) numerically the accuracy and invertibility of the generated flow can be controlled through smaller step-sizes and higher-order solvers.\nTo define the diffeomorphism $\\phi_{\\theta} : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, we start with the Neural ODE governing the flow:\n$\\frac{dz(t)}{dt} = f(z(t); \\theta),$\n(9)\nwhere $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ is a vector field parameterized by a multilayer perceptron (MLP) with Swish activation functions and a sine-cosine time embedding and $\\theta$ denotes the parameters of the MLP. Given an initial condition $z(0) = x$, the solution to this Neural ODE is:\n$\\phi_{\\theta}(x) := x + \\int_0^1 f(z(t); \\theta) dt.$\n(10)\nTo obtain the inverse $\\phi_{\\theta}^{-1}$ one has to integrate the differential equation backwards in time with initial condition $z(1)$. To solve the Neural ODE we implemented a Runge-Kutta solver in JAX, see Appendix E for further architectural and training related details."}, {"title": "4.2 LEARNING OBJECTIVE", "content": "The primary objectives in learning isometries are twofold, i) to map the data manifold $(D, \\langle \\cdot, \\cdot \\rangle_D)$ into a low-dimensional geodesic subspace of $(M, \\langle \\cdot, \\cdot \\rangle_M)$, specifically $M_{\\mathbb{d}'} \\subset M$, and ii) to preserve local isometry around the data, as motivated by Proposition 2.1 and Theorems 3.4, 3.6, and 3.8 from Diepeveen (2024).\nTo achieve these goals, we build upon the training objective proposed by Diepeveen (2024), as summarized in Appendix A. We use the global isometry loss and submanifold loss to map the data manifold D into the lower dimensional geodesic subspace $M_{\\mathbb{d}'}$. We improve the objective by introducing graph matching loss for isometric learning to enforce global isometry between the data and latent (sub)manifolds (Zhu et al., 2014). Intuitively we can explain this loss as enforcing each sample to be equally isometric to all the other samples.\nThe original objective is limited by its reliance on the pullback metric's Riemannian metric tensor $\\langle \\cdot, \\cdot \\rangle_{\\varphi}^*$ to enforce local isometry, which is (computationally) intractable in practice and scales poorly"}, {"title": "5 EXPERIMENTS", "content": "The goal of this paper is to learn interpolatable latent (sub)manifolds for generation on data man-ifolds. We achieve this through isometric learning in the framework of pullback geometry. In this section we validate our methods on synthetic, simulated and experimental datasets, for full descriptions see Appendix D. For details on the training procedure and hyperparameter settings we refer the reader to Appendix E.\nWe begin our experiments with an ablation study of graph matching loss and stability regu-larization, demonstrating the benefits of including both terms for learning isometries. Second, we compare (latent) interpolation methods with interpolation on the latent manifold $M, \\langle \\cdot, \\cdot \\rangle_{M}$-interpolation, and on the latent submanifold $M_{\\mathbb{d}'}, \\langle \\cdot, \\cdot \\rangle_{M_{\\mathbb{d}'}}$-interpolation. We demonstrate that we can accurately interpolate on the data manifold by interpolating on the latent (sub)manifold $\\varphi^{-1}$. Third, we validate PFM as a generative model on data manifolds and discuss how sample generation is im-proved by generating on the submanifold $M_{\\mathbb{d}'}$. Finally, we inspect the designability of the latent manifold through the choice of metric $\\langle \\cdot, \\cdot \\rangle_{\\mathbb{D}}$ in the task of small protein design."}, {"title": "5.1 ABLATION STUDY", "content": "The goal of the ablation study is to evaluate the effectiveness of the reformulated objective function for learning isometries. To this end, we perform an ablation study for both the graph matching loss and stability regularization on a synthetic ARCH dataset ($n = 500, d = 2$) in the spirit of Tong et al. (2020) and a coarse-grained protein dynamics datasets of intestinal fatty acid binding protein (I-FABP) ($n = 500, d = 131 \\times 3$). We report three metrics on the validation set of 20 % of the data, invertibility $\\mathbb{E}_{inv} = \\frac{1}{n} \\sum_{i=1}^n || x_i - \\varphi_{\\theta}^{-1}(\\varphi_{\\theta}(x_i)) ||^2$, low-dimensionality $\\epsilon_{ld} = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n ||d_{i,j} - d_M(\\varphi(x_i), \\varphi(x_j))||^2$ and isometry $\\mathbb{E}_{iso} = \\frac{1}{n} \\sum_{i=1}^n \\sum_{j\\neq i} || (d_{\\mathbb{R}^d}^\\varphi(x_i, x_\\cdot) - d_{\\mathbb{R}^d}^\\varphi(x_j, x_\\cdot)) - (d_{i,\\cdot} - d_{j,\\cdot}) ||^2$.\nResult. Table 1 demonstrates that incorporating both the graph matching loss and stability regu-larization improves the invertibility and isometry metrics across both datasets, with the combined approach yielding both a low $\\mathbb{E}_{inv}$ and $\\mathbb{E}_{iso}$ values, indicating enhanced model performance in pre-serving the geometry of the data in the synthetic dataset as well ass the more noisy and high dimen-sional simulated dataset."}, {"title": "5.2 INTERPOLATION EXPERIMENTS", "content": "The goal of isometric learning is to learn\nan interpolatable latent (sub)manifold of\nthe data manifold with closed-form man-\nifold mappings. To evaluate whether in-\nterpolation on the latent (sub)manifold ac-\ncurately reflects interpolation on the data\nmanifold, we conduct an interpolation ex-\nperiment using the synthetic ARCH dataset,\nas well as the molecular dynamics datasets\nof Adenylate Kinase (AK) ($n = 100,\nd = 214 \\times 3$) and I-FABP. In both cases\nwe choose $M_{\\mathbb{d}'} = \\mathbb{I}R$, see Appendix C\nfor guidance on latent manifold and met-\nric selection. We approximate the met-\nric on the data manifold $\\langle \\cdot, \\cdot \\rangle_{\\mathbb{D}}$ through the\nlength of Isomap's geodesics Tenenbaum\net al. (2000), see Figure 4 for an example.\nWe compare the accuracy of 100 randomly\nselected geodesics between points in the"}, {"title": "5.3 GENERATION EXPERIMENTS", "content": "We evaluate the effectiveness of our proposed method PFM for generation on data manifolds D. We train two PFMs, one using the latent manifold M and one using the lower dimensional latent submanifold M', named PFM and d'-PFM respectively. Additionally, we train a Conditional Flow Matching (CFM) model on the raw data as a comparison. A visual example of the learned generative flows over time for the ARCH dataset can be viewed in Figure 3.\nResult. Figure 3 we see that the learned isometry to the latent manifold M acts as a strong man-ifold prior, capturing the manifold structure at the start of the continous normalizing flows (CNF) trajectory (t = 0.0). Additionally, the learned isometry to the latent submanifold M' captures the noiseless manifold revealing the underlying manifold used to generate the data. Through this strong (noiseless) manifold prior, we see that both PFM and 1-PFM approximate the distribution on the manifold earlier in the trajectory and better.\nTo evaluate the performance of our generative methods on manifolds, we use the 1-nearest neighbour (NN) accuracy metric Lopez-Paz & Oquab (2016). This metric assesses how well the generated point clouds match the reference point clouds. We classify each point cloud by finding its nearest neighbor from the combined set of generated and reference point clouds of the trainingset. The 1-NN accuracy measures the proportion of correct classifications of the testset, indicating how similar the generated point clouds are to the reference set. An accuracy close to 50% suggests that the generated and reference point clouds come from the same distribution, and thus indistinguishable, reflecting successful learning of the target distribution."}, {"title": "5.4 DESIGNABLE LATENT MANIFOLDS FOR NOVEL PROTEIN ENGINEERING", "content": "The goal of these experiments is to design a latent manifold that captures biologically relevant properties of protein sequences, enabling the generation of novel proteins with specific character-istics. By leveraging our method's flexibility in defining the metric on the data manifold $\\langle \\cdot, \\cdot \\rangle_{\\mathbb{D}}$, we structure the latent space such that it captures protein properties, such as sequence similarity, hydrophobicity, hydrophobic moment, charge, and isoelectric point.\nTo achieve this, we use protein sequences of up to 25 amino acids from the giant repository of AMP activities (GRAMPA) dataset (see Appendix D for details). We construct the following custom metric on the data manifold,\n$d_{\\mathbb{D}}(x_i, x_j) = d_{Levenshtein}(x_i, x_j) + d_{hydrophobicity}(x_i, x_j)$\n(11)\n$+ d_{hydrophobic \\ moment}(x_i, x_j) + d_{charge}(x_i, x_j)$\n(12)\n$+ d_{isoelectric \\ point}(x_i, x_j),$\n(13)\nwhere the Levenshtein distance measures the number of single-character edits (insertions, deletions, or substitutions) required to transform one sequence into another."}, {"title": "6 CONCLUSION", "content": "We introduce Pullback Flow Matching (PFM), a novel framework for simulation-free training of generative models on data manifolds. By leveraging pullback geometry and isometric learning, PFM allows for closed-form mappings on data manifolds while enabling precise interpolation and efficient generation. We demonstrated the effectiveness of PFM through applications in synthetic protein dy-namics and small protein generation, showcasing its potential in generating novel, property-specific samples through designable latent spaces. This approach holds significant promise for advancing generative modeling in fields like drug discovery and materials science, where precise and efficient sample generation is critical."}, {"title": "A BACKGROUND", "content": "To achieve an interpolatable latent manifold we take a Riemannian geometric perspective. We start by introducing the notation and key concepts of differential and Riemannian geometry, for a formal description see Lee (2012). Second, we explain prior work on RAEs Diepeveen (2024), a framework for constructing interpolatable latent manifolds. Third, we summarize CFM for generative modeling Lipman et al. (2022), a scalable way to train generative models in a simulation-free manner. Finally, we discuss how RFM Chen & Lipman (2024) generalize CFM to Riemannian manifolds."}, {"title": "A.1 RIEMMANIAN GEOMETRY", "content": "A d-dimensional smooth manifold M is a topological space that locally resembles $\\mathbb{R}^d$, such that for each point p \u2208 M, there exists a neighborhood U of p and a homeomorphism $\\psi : U \\rightarrow \\mathbb{R}^d$, called a chart. Then the tangent space $T_pM$ at a point p \u2208 M is a vector space consisting of the tangent vectors at p representing the space of derivations at p.\nA Riemannian manifold $(M, \\langle \\cdot, \\cdot \\rangle_M)$ is a smooth manifold M equipped with a Riemannian metric $\\langle \\cdot, \\cdot \\rangle_M$, which is a smoothly varying positive-definite inner product on the tangent space $T_pM$ at each point p. The Riemannian metric $\\langle \\cdot, \\cdot \\rangle_M$ defines the length of tangent vectors and the angle between them, thereby inducing a natural notion of distance on M based on the lengths of tangent vectors along curves between two points.\nThe shortest path between two points on M is called a geodesic, which generalizes the concept of straight lines in Euclidean space to curved manifolds. Geodesics on Riemannian manifold are found by minimizing\n$\\mathbb{E}(\\gamma) = \\frac{1}{2} \\int_0^1 \\langle \\dot{\\gamma}(t), \\dot{\\gamma}(t) \\rangle dt,$\n(17)\nwhereas\n$L(\\gamma) = \\int_0^1 \\sqrt{\\langle \\dot{\\gamma}(t), \\dot{\\gamma}(t) \\rangle} dt,$\n(18)\ndefines the distance between two points on the manifold. The exponential map,\n$\\exp_p: T_pM \\rightarrow M,$\n(19)\nat p maps a tangent vector $\\Xi_p \\in T_pM$ to a point on M reached by traveling along the geodesic starting at p in the direction of $\\Xi_p$ for unit time. The logarithmic map,\n$\\log_p: M \\rightarrow T_pM,$\n(20)\nis the inverse of the exponential map, mapping a point q \u2208 M back to the tangent space $T_pM$ at p.\nThese names, 'exponential' and 'logarithmic' map, are geometric extensions of familiar calculus concepts. Just as the exponential function maps a number to a point on a curve, the exponential map on a manifold maps a direction and starting point to a location along a geodesic. Similarly, the logarithm in calculus reverses exponentiation, and the logarithmic map on a manifold reverses the exponential map, returning the original direction and distance needed to reach a specified point along the geodesic.\nAssume $(M, \\langle \\cdot, \\cdot \\rangle_M)$ is a d-dimensional Riemannian manifold and a smooth diffeomorphism $\\varphi : \\mathbb{R}^d \\rightarrow M$, such that $\\varphi(\\mathbb{R}^d) \\subseteq M$ is geodesically convex, i.e., geodesics are uniquely defined on $\\varphi(\\mathbb{R}^d)$. We can then define the pushforward $\\varphi_* : T\\mathbb{R}^d \\rightarrow TM$ which in turn can be used to define the pullback metric as\n$\\langle \\Xi, \\Phi \\rangle_{\\varphi}^* := \\langle \\varphi_*[\\Xi], \\varphi_*[\\Phi] \\rangle,$\n(21)\nfor tangent vectors $\\Xi$ and $\\Phi$. These mappings allow us to define all relevant geometric mappings in $\\mathbb{R}^d$ in terms of manifold mappings on M, see e.g. Proposition 2.1 of Diepeveen (2024):\n1.  Distances $d_{\\mathbb{R}^d}^\\varphi: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ on $(\\mathbb{R}^d, \\langle \\cdot, \\cdot \\rangle_{\\varphi}^*)$ are given by,\n$d_{\\mathbb{R}^d}^\\varphi(x_i, x_j) = d_M(\\varphi(x_i), \\varphi(x_j)),$\n(22)"}, {"title": "A.2 RIEMANNIAN AUTO-ENCODER", "content": "The goal of RAEs is to create a interpolatable latent representation of the data. This is achieved through data-driven (pullback) Riemannian geometry, encoding the data onto a latent manifold with known geometry. The benefit of this, is that interpolation on the data manifold corresponds to interpolation on the latent manifold. Resulting in a more interpretable latent space compared to traditional auto-encoders.\nSimilar as in Diepeveen (2024), we define a RAE as a Riemannian Encoder RE : $\\mathbb{R}^d \\rightarrow \\mathbb{R}^r$ and Riemannian Decoder RD : $\\mathbb{R}^r \\rightarrow \\mathbb{R}^d$,\n$RAE(x) := (RD \\circ RE)(x)$ s.t.,\n(26)\n$RE(x)_k := \\langle \\log_z(\\varphi(x)), v_k \\rangle$ for $k = 1, . . . r,$\n(27)\n$RD(a) := \\exp_z(\\sum_{k=1}^r a_k v_k),$\n(28)\nwhere z denotes a base point and $\\langle \\cdot, \\cdot \\rangle_z^{\\varphi}$ the pullback metric at z. Furthermore,\n$v_k^{\\varphi} := \\sum_{l=1}^d W_{lk} \\Phi_l^{\\varphi},$\n(29)\nrepresents the basis vectors of the latent space in the tangent space $T_z\\mathbb{R}^d$. Let $\\Phi \\in T_z\\mathbb{R}^d$ be an orthonormal basis in the tangent space at z with respect to $\\langle \\cdot, \\cdot \\rangle_z^{\\varphi}$ and define\n$\\Xi_{i,l} = \\langle \\log_z(\\varphi(x^i)), \\Phi_l^{\\varphi} \\rangle_z^{\\varphi}$ for $i = 1, ..., n$ and $l = 1, . . ., d$.\n(30)"}, {"title": "A.3 LEARNING ISOMETRIES WITH RIEMANNIAN AUTO-ENCODERS", "content": "After constructing the diffeomorphism and Riemannian Auto-Encoder, one can learn an isometry by find the parameters $\\theta$ of $\\varphi_{\\theta}$ in Diepeveen (2024) through minimizing the objective,\n$\\mathbb{L}(\\theta) = \\frac{1}{N(N-1)} \\sum_{i \\neq j=1}^N (d_{\\mathbb{R}^d}^\\varphi(x_i, x_j) - d_{i,j})^2$\n(global isometry loss)\n$+ \\lambda_{sub} \\sum_{i=1}^N ||  \\begin{bmatrix}  I_{d'} & 0 \\\\  0 & 0  \\end{bmatrix} (T_z \\varphi_{\\theta} (x_i) - (T_z \\varphi_{\\theta} (x_i))^2 ||^2$\n(submanifold loss)\n$+ \\lambda_{iso} \\frac{1}{N} \\sum_{i=1}^N || \\sum_{j,j'=1}^d ( \\begin{pmatrix}  (e_i^{\\varphi}, e_{i'}^{\\varphi})_z \\end{pmatrix} \\big ) - I_{d} ||^2,$\n(local isometry loss)\nwhere $|| \\cdot ||_F$ is the Frobenius norm and $( \\begin{pmatrix}  (e_i^{\\varphi}, e_{i'}^{\\varphi})_z \\end{pmatrix})_{j,j'=1}^{\\mathbb{d}}$ denotes a $\\mathbb{d}$-dimensional matrix just as $(A_{ij})_{i,j=1}^n$ denotes a matrix."}, {"title": "A.4 CONDITIONAL FLOW MATCHING", "content": "To achieve the goal of accurate generative modeling on data manifolds through isometric learning", "follows": "n$\\mathbb{L"}, {"paths": "n$\\mathbb{L}_{CFM}(\\eta) = \\"}]}