{"title": "Who is Undercover? Guiding LLMs to Explore Multi-Perspective Team Tactic in the Game", "authors": ["Ruiqi Dong", "Zhixuan Liao", "Guangwei Lai", "Yuhan Ma", "Danni Ma", "Chenyou Fan"], "abstract": "Large Language Models (LLMs) are pivotal AI agents in complex tasks but still face challenges in open decision-making problems within complex scenarios. To address this, we use the language logic game \"Who is Undercover?\" (WIU) as an experimental platform to propose the Multi-Perspective Team Tactic (MPTT) framework. MPTT aims to cultivate LLMs' human-like language expression logic, multi-dimensional thinking, and self-perception in complex scenarios. By alternating speaking and voting sessions, integrating techniques like self-perspective, identity-determination, self-reflection, self-summary and multi-round find-teammates, LLM agents make rational decisions through strategic concealment and communication, fostering human-like trust. Preliminary results show that MPTT, combined with WIU, leverages LLMs' cognitive capabilities to create a decision-making framework that can simulate real society. This framework aids minority groups in communication and expression, promoting fairness and diversity in decision-making. Additionally, our Human-in-the-loop experiments demonstrate that LLMs can learn and align with human behaviors through interactive, indicating their potential for active participation in societal decision-making.", "sections": [{"title": "I. INTRODUCTION", "content": "Decision-making in human society is a complex and important activity [1] that involves individuals and organizations making choices in various fields in response to changing situations [2]. Incorporating AI technology in these activities [3], [4] can improve rationality and effectiveness in decision-making. Chain-of-Thought (CoT) [5] improves the performance of LLMs on complex reasoning tasks and self-consistency [6] proposes a new decoding strategy. AI Agent has been emerging to solve automated tasks like Hugging-GPT [7], and efficiently fine-tune models like AgentTuning [8]. To strengthen LLMs' decision-making abilities, using multiple language model agents to debate in multiple rounds [9], Self-Refine [10] improves the initial output through iterative feedback and improvement, ReAct [11] solves general tasks through collaborative reasoning and action, and Reflexion [12] uses task feedback signals as short-term memory to guide subsequent decision-making. Furthermore, when functioning as Al agents, LLMs can decompose complex problems into more manageable sub-tasks [13], [14] and exhibit human-like natural language interaction abilities [15], [16]. However, Al agents often struggle with open decision-making in complex scenarios. Therefore, LLMs need to have better understanding of human societal rules to enhance decision-making rationality [17].\nIn this work, we use the game \"Who is Undercover?\u201d (WIU), a reasoning game testing decision-making skills, as our foundation. Inspired by the Theory of Mind [18] and Social Identity [19] in Social Psychology [20], we aim to simulate human thought processes in LLMs. Therefore, we employ LLMs as AI agent players and design the \"Multi-Perspective Team Tactic\u201d (MPTT) framework. MPTT alternates between speaking and voting sessions, incorporating several multi-perspective techniques. To enhance game realism and complexity, we designed the \u201cHuman-in-the-loop\" to explore human-AI collaboration in social interactions.\nResearch shows that MPTT iteratively optimizes LLM agents' mindsets, fostering strategic behaviors like confrontation and concealment, alongside tendencies like trust, suspicion, and cooperation. Applied to the WIU game, MPTT creates a decision-making mechanism as a reference for human society and helps minorities communicate and express their choices, promoting balanced decision-making across diverse groups. Additionally, LLMs are expected to actively participate in future social decision-making alongside humans."}, {"title": "II. RELATED WORK", "content": "Some studies [21], [22] explored using LLMs to identify deceptive information. Wang et al. [23] proposed the Recursive Contemplation (ReCon) framework on the Avalon game to explore the potential of LLM in deceptive environments. Xu et al. [24] explored the problem of how to use LLMs in communication games Werewolf. Game theory [25] finds diverse applications in economic analysis [26], spanning market competition and trade freedom [27]. WIU is a process of conducting a static game with incomplete information [25]. Kroer et al.[28] devised strategies for playing against a limited prospective player. But through WIU game training, AI agents gain insights into the challenges posed by incomplete information games in human society. WIU emphasizes logical deduction and reasoning, appealing to players who favor strategic thinking over social manipulation. This distinguishes it from the intense social interaction and deception commonly seen in Werewolf [24] and Avalon [23]."}, {"title": "III. MPTT: A FRAMEWORK FOR REASONING GAME", "content": null}, {"title": "A. Game description and overall process", "content": "\"Who is Undercover? \" is a reasoning game where multiple civilian players are mixed with a minority of undercover players. Each player is given a similar but different word without knowing their identity and takes turns describing their word. The opponents are eliminated through description and thinking. When there is only one civilian left but there is still an undercover, the undercover wins, and if there is no undercover, the civilian wins. The MPTT framework divides the game into two phases: speaking and voting, to privately reflect on roles and generate thoughtful responses that balance revealing information with maintaining secrecy, and analyse previous speeches, identify teammates, and make strategic voting decisions based on incomplete information."}, {"title": "B. Phase I: Reflections on the Speaking Session", "content": "In the first phase of our framework, players reflect on their roles privately before delivering their speeches, aiming to enhance adaptability and flexibility in providing diverse, accurate descriptions while concealing private information. This addresses the issues of (a) hidden private words and (b) broader descriptive content shown in Fig. 2.\nSelf-Perspective. This stage prompts the AI agent to describe words in one sentence from its own perspective. Reference from the first level of human indication of intentionality [29] Suppose it is now the turn of player \u03b1 (\u03b1 \u2208 {1, ..., n}) to speak, Player a will think as follows:\n$T_\\alpha = SelfPerspective{H, O_\\alpha}'_\\alpha$\nIdentity-Determination. Player a determines it identity based on the global historical records H :\n$M_\\alpha = IdentityDetermination{H,O_\\alpha,T_\\alpha}_\\alpha$\nSelf-Reflection. Player a needs to reflect on itselves to find common features in the description to avoid exposure.\n$R_\\alpha = SelfReflection{H, O_\\alpha, T_\\alpha, M_\\alpha}_\\alpha$\nAfter these reflections, the AI agents will make a summary of ideas Oa, which mainly includes self-conclusion and the speaking recommendations, update with rounds:\n$O_\\alpha' = SummaryOrder{T_\\alpha, M_\\alpha, R_\\alpha}_{N=\\alpha} O_\\alpha\\leftarrow O_\\alpha'$\n$0\\leftarrow O\\cup{O_\\alpha}=_\\alpha$\n$W_\\alpha$ is the content of player a's final speech in the r round. It will be added to the historical records H to drive the game.\n$W_r = WordSpeak{T_\\alpha, M_\\alpha, R_\\alpha}_{N=\\alpha}$\n$H\\leftarrow H\\cup{W_\\alpha}=_\\alpha$"}, {"title": "C. Phase II: Reflections on the Voting Session", "content": "In phase II, the voting part reflects the incomplete information game problem in Game theory [25]. MPTT helps A\u0399 agents make strategic voting decisions, addressing the issue of (c) more accurate game trust relationships mentioned in Fig. 2.\nFirst-FindTeammate. Players review the history of others' speeches to identify teammates and opponents, comparing and analyzing characteristics in multiple ways. Before each round of voting opens, each player thinks simultaneously:\n$F_\\alpha = FirstFindTeammate{H, O_\\alpha}_{N=\\alpha}$\nSecond-FindTeammate. As the amount of information gradually increases, Players will reassess their identity and update their strategy based on new information:\n$J_\\alpha = SecondFindTeammate{\u0397, \u039f_\\alpha, F_\\alpha}_\\alpha$\nGame-Decision. Finally, Players use cumulative reflection and judgement to build more explicit trust, and update Oa to better adapt to the dynamic situation (refer to Eq. 4 and add Fa, Ja in it, as well as update Eq. 5):\n$S_\\alpha = GameDecision{H, O_\\alpha, F_\\alpha, J_\\alpha}_{N=\\alpha}$\nPlayers are encouraged to find teammates and fostering cooperation, think strategically in their votes, choosing the right player to vote for to ensure that the results favor their team:\n$V_\\alpha = WordVote{F_\\alpha, J_\\alpha, S_\\alpha}_{N=\\alpha}$\nThe results of all players' votes are tallied for each round of the game, the player with the highest number of votes will be out of the game."}, {"title": "IV. EXPERIMENT", "content": null}, {"title": "A. Baselines and our approach", "content": "Setup. We evaluate the capabilities of our proposed Multi-Perspective Team Tactic (MPTT) by having LLM play the full WIU game. Our game is implemented using ChatGPT (gpt-3.5-turbo)[30] for multiple rounds of multi-role-playing, game topics are based on common things in life. In the game phase, we set up 5 LLM agents participating in the game, with 3 civilians and 2 undercovers. The role assignments and speaking order of each game are randomly determined. We also attempted to verify the generalisation ability of MPTT on Claude 3[31], Gemini[32] and Llama-3-8B[33].\nBaseline. The Baseline approach uses only the game's rules as prompts, following Chain-of-Thought (CoT) [5] to guide players step-by-step through the game, using current round speeches as references for voting.\nMultidimensional Self-Reflection. Building on CoT, we add Multidimensional Self-Reflection, inspired by Avalon [23] and Self-Refine [10], allowing players to consider multiple perspectives during both speaking and voting sessions.\nGlobal History. On top of that, we integrate the global history method from Werewolf [24], enabling players to review all previous speeches before voting.\nMPTT (Ours). Our MPTT framework extends these methods by adding self-summary after each reflection phase, continuously updating players' self-identity judgments and survival strategies."}, {"title": "Metrics", "content": "We analyzed the experimental data from five perspectives, focusing on the performance of civilians and undercovers in MPTT and its ablation experiments. These metrics are as follows: Victory Rate (VR) measures the probability of winning the game, Survival Rate in the First Round (SR@1) and Consecutive Two Rounds (SR@2) measure the survival rates after the first and second rounds. Probability of Successfully Trusting Own Team (PST) and Assessing Enemy Team (PSA) measure the team's ability to recognize teammates and identify opponents:\nQuantitative Results. Fig. 3 and TABLE I shows the performance differences between civilians and undercovers in MPTT and its ablation studies. Due to their majority, civilians in the Baseline quickly recognize teammates and maintain a higher VR. As strategies evolve and perspectives diversify, undercovers leverage their minority status to improve consensus, and locate teammates more efficiently, boosting their VR,they also achieve higher PSA accuracy due to smaller size, though they risk less concentrated voting. MPTT effectively addresses these challenges, enhancing undercover performance."}, {"title": "B. Evaluation of game metrics", "content": "We have defined metrics in five dimensions to evaluate the performance of LLMs agents participating in games. And compare MPTT with the Baseline to verify its effectiveness.\nMetrics. We computed independently for each team by using experimental data because of headcount difference. Voting Success Rate (VSR) measures the probability of successfully voting out an enemy player each round, Influence (INF) measures the frequency of borrowing statements from the opposing team, Comprehension Capability (CCAP) measures the probability of correctly trusting a teammate each round, Reversal Rate (REV) assesses the rate of correcting trust errors, Concealment (CONC) indicates the effectiveness of misleading the enemy into voting incorrectly.\nAnalysis of evaluations. TABLE II shows that due to the numerical superiority of the civilian team in Baseline, they are stronger than undercovers in several metrics, but their larger base cause undercovers mislead civilians to vote incorrectly at a higher rate on CONC. In MPTT, the undercover team improves on all indicators, leading to a weakening of the civilian team's advantage. So MPTT is effective in ameliorating the differences caused by team numbers and balance each other."}, {"title": "C. LLM and human collaborative reasoning", "content": "We explore the impact of integrating a human player into LLM-driven reasoning games, focusing on the differences and similarities between humans and AI. The \u201cHuman-in-the-loop\" protocol features one human and four LLM agents in a WIU game. To assess the human's impact, we selected games where both teams frequently failed and placed a human in the failing team. We also define Judgment Capability (JCAP) to measure a player's self-judgment accuracy and Survival Rate (SUR) to compare survival outcomes between humans and AI agents on the same team. All metrics are calculated separately for players of the same type (human or AI agent).\nAnalysis of two comparisons. Both teams use their frequently failed game topics differently, so we compare the diversity between human and AI agents instead of the gap between teams. The first examines human and AI decision-making within the same team in the \u201cHuman-in-the-loop\" game. MethodI in TABLE III shows that humans and AI agents have similar VSR and INF scores, indicating comparable influence. However, humans achieve higher CCAP and JCAP scores, reflecting better judgment in ambiguous situations, while their lower SUR scores suggest vulnerability to being targeted due to language style differences. Overall, humans and AI agents influence each other's thinking and interaction. The second assesses \u201cHuman-in-the-loop\" vs LLM only in the same game with MPTT. MethodII in TABLE III shows that adding a human player significantly increased the SUR and VR for both teams and balanced overall metrics, highlighting the human's impact. Regardless of their role, human players enhance their team's CCAP and VSR, demonstrating superior analysis and inference abilities.\""}, {"title": "D. Advanced tactics and generalization ability", "content": "We explore advanced tactics used by AI agents in the WIU game and their impact on game dynamics. Conscious Guess: Like humans, Al agents attempt to infer others' difference and adjusting strategies. Fig.4 (a) shows it enhances their reasoning and deduction abilities. Vote for the Teammate: AI undercovers may strategically vote against a fellow undercover with more exposure, creating confusion and gaining civilian trust, as illustrated in Fig.4 (b). While this tactic can mislead opponents, it also increases the challenge for the AI team, requiring strong acumen and adaptability.\nTo demonstrate the generalization ability of MPTT, we validated its validity on the latest LLMs, Claude 3[31] and Gemini[32], both of which performed well in WIU. However, Llama-3-8B[33] didn't fully comply with the required response format, despite exhibiting strategic behaviors like concealment and confrontation. This indicates that open-source LLMs still require improvement in command compliance."}, {"title": "V. CONCLUSION", "content": "Using the WIU game, we developed a multidimensional thinking framework that iteratively optimizes LLM agents' decision-making, with applications to human society. This framework enhances adaptability and information mining through multi-dimensional thinking and global history analysis, enabling LLM agents to autonomously develop strategies like confrontation and concealment while promoting fairness for minority groups. Adding human player shows that LLM agents can align with human behavior, with potential applications in public welfare, legal aid, and community governance. Future research will explore advanced strategies, diverse scenarios, and optimized learning mechanisms to enhance AI's role in social decision-making and human-AI collaboration."}]}