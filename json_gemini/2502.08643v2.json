{"title": "A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards", "authors": ["Shivansh Patel", "Xinchen Yin", "Wenlong Huang", "Shubham Garg", "Hooshang Nayyeri", "Li Fei-Fei", "Svetlana Lazebnik", "Yunzhu Li"], "abstract": "Task specification for robotic manipulation in open-world environments is challenging, requiring flexible and adaptive objectives that align with human intentions and can evolve through iterative feedback. We introduce Iterative Keypoint Reward (IKER), a visually grounded, Python-based reward function that serves as a dynamic task specification. Our framework leverages VLMs to generate and refine these reward functions for multi-step manipulation tasks. Given RGB-D observations and free-form language instructions, we sample keypoints in the scene and generate a reward function conditioned on these keypoints. IKER operates on the spatial relationships between keypoints, leveraging commonsense priors about the desired behaviors, and enabling precise SE(3) control. We reconstruct real-world scenes in simulation and use the generated rewards to train reinforcement learning (RL) policies, which are then deployed into the real world-forming a real-to-sim-to-real loop. Our approach demonstrates notable capabilities across diverse scenarios, including both prehensile and non-prehensile tasks, showcasing multi-step task execution, spontaneous error recovery, and on-the-fly strategy adjustments. The results highlight IKER's effectiveness in enabling robots to perform multi-step tasks in dynamic environments through iterative reward shaping.", "sections": [{"title": "I. INTRODUCTION", "content": "Suppose that a robot is tasked with placing a pair of shoes on a rack, but a shoe box is occupying the rack, leaving insufficient space for both shoes (Figure 1, top right). The robot must first push the box aside to create space and then proceed to place the shoes. This example highlights the importance of task specification for robots in unstructured, real-world environments, where tasks can often involve multiple implicit steps. In such cases, rigid predefined instructions fail to capture the complexities of interaction required to accomplish the goal. To be effective, task specifications must incorporate commonsense priors-expectations about how the robot should behave. For instance, rather than attempting to squeeze the shoes in awkwardly, the robot should realize that it must first clear space.\nRecent vision-language models (VLMs) show promise for freeform robotic task specification due to their rapidly advancing ability to encode rich world knowledge by pre-training on vast and diverse datasets [1-8]. VLMs excel in interpreting natural language descriptions and complex instructions. Their broad knowledge bridges human expectations and robot behavior, capturing human-like priors and problem-solving strategies. However, previous works that leverage VLMs in robotics face two major limitations: (1) they lack the capability to specify precise target locations in 3D, and (2) they are often unable to adapt to the environment changes as the task progresses.\nIn this work, we introduce Iterative Keypoint Reward (IKER), a visually grounded reward function for robotic manipulation that addresses these limitations. Inspired by recent work [9], we draw the observation that both object positions and orientations can be encoded using keypoints. Hence, IKER allows for fine-grained manipulation in 3D, facilitating complex tasks that require accurate location and orientation control. Additionally, IKER incorporates an iterative refinement mechanism, where the VLM updates the task specification based on feedback from the robot's interactions with the environment. This mechanism enables dynamically-adjusting strategies and intermediate steps, such as repositioning objects for a better grasp.\nWhile VLMs excel in processing real-world visual data, training policies directly in the real world is often infeasible due to safety, scalability, and efficiency constraints. To address this, we first generate IKER using real-world observations, then transfer the scene and the reward to simulation for training, and finally, deploy the optimized policy back into the real world. Thus, our system operates in a real-to-sim-to-real loop.\nWe demonstrate the efficacy of our real-to-sim-to-real framework with IKER across diverse scenarios involving"}, {"title": "II. RELATED WORK", "content": "VLMs in Robotics. VLMs have become a prominent tool in robotics [10-38]. Existing works utilizing VLMs in robotics primarily focus on two areas: task specification [10-12, 15, 16, 21] and low-level control [11, 13, 14, 39]. Our work aligns with the former, with an emphasis on flexibility and adaptability in complex, real-world environments.\nFor task specification, many works employ VLMs to break down complex tasks into manageable subtasks, demonstrating their utility in bridging high-level instructions and robotic actions. Huang et al. [40] demonstrate the use of LLMS as zero-shot planners, enabling task decomposition into actionable steps. Similarly, Ahn et al. [10] leverage VLMs to parse long-horizon tasks and sequence them into executable steps for robots. Belkhale et al. [41] introduce \"language motions\" that serve as intermediaries between high-level instructions and specific robotic actions, allowing policies to capture reusable, low-level behaviors. Unlike these works, our approach focuses on flexible interpretation of tasks in the context of a dynamically changing environment.\nBeyond task decomposition, VLMs have been used to generate affordances and value maps that guide robotic actions. Huang et al. [12] employs VLMs to generate 3D affordance maps, providing robots with spatial knowledge of which parts of the environment are suitable for interaction. Liu et al. [15] use VLMs to predict point-based affordances, enabling zero-shot manipulation tasks. Zhao et al. [42] incorporate VLMs into model predictive control, where the models predict the outcomes of candidate actions to guide optimal decision-making. These works demonstrate the potential of VLMs to bridge high-level task understanding with spatial and functional knowledge needed for robotic control. Similar to our work, Huang et al. [9] use keypoints and define relations and constraints between them to execute manipulation tasks, but their approach follows an open-loop strategy. In contrast, we employ a closed-loop approach, enabling dynamic plan adjustments. Additionally, our approach also supports non-prehensile manipulations, such as pushing.\nSome works have also explored VLMs for reward function generation [43-46]. However, most of these approaches have limited real-world applicability. Some lack demonstrations on real robots [44], are restricted to a single real-world scenario [46], or focus on highly constrained tasks like a robot dog walking on a ball [45]. In contrast, our work demonstrates the versatility and robustness of VLM-generated rewards on multiple real-world manipulation tasks."}, {"title": "III. METHOD", "content": "In this section we formally define Iterative Keypoint Reward (IKER) and discuss how it is automatically synthesized and refined by VLMs by continuously taking in environmental feedback. Then, we discuss our overall framework, which uses IKER in a real-to-sim-to-real loop. Our method overview is illustrated in Figure 2, with detailed steps provided in Algorithm 1."}, {"title": "A. Iterative Keypoint Reward (IKER)", "content": "Given an RGB-D observation of the environment and an open-vocabulary language instruction $I$ for a multi-step task, our goal is to obtain a sequence of policies, $\\pi = \\{\\pi_i\\}_{i=1}^{N}$, that complete the task. Crucially, the number of policies $N$ is not predefined, allowing for flexibility in how the robot approaches the task. For example, in the scenario of Fig. 3, the first policy, $\\pi_1$, moves the shoe box to create space, while subsequent policies handle the placement of each shoe.\nFor each step $i$, we denote the RGB observation as $O_i$. We assume a set of $K_i$ keypoints $\\{k_j\\}_{j=1}^{K_i}$ is given (discussed later in Sec. III-B), each specifying a 3D position in the task space. Using these keypoints, our objective is to automatically generate a reward function, termed IKER, that maps the keypoint positions to a scalar reward $f^{(i)}: \\mathbb{R}^{K_i \\times 3} \\rightarrow \\mathbb{R}$.\nTo generate the reward function $f^{(i)}$, we use a VLM (GPT-4o [1] in our case), which is provided with the context comprising (1) the human instruction $I$ describing the task, (2) the current RGB observation $O_i$ with keypoints overlaid with numerical markers, and (3) the sequence of previous observations and reward functions up to step $i-1$, i.e. $\\{O_1, f^{(1)}, ..., O_{i-1}, f^{(i-1)}\\}$.\nAdditionally, the VLM is guided by a prompt that instructs it to generate a Python function for the reward $f^{(i)}$. The prompt directs the VLM to break down the task into executable steps, predict which object to interact with, specify the movement of objects by indicating where their keypoints should be placed relative to other keypoints, and perform arithmetic calculations on these keypoints to predict their final locations. We do not explicitly specify which keypoint belongs to which object, allowing the VLM to infer this information. The prompt also instructs the VLM to present all outputs in a prescribed code format and set the flag done = True if the task is completed. By predicting code, the VLM can perform arbitrary and precise calculations using the current keypoint locations, which would not be possible if limited to raw text. Please refer to Appendix C for the complete prompt, and Figure 7 for a step-by-step walkthrough of RGB observations and generated reward functions for the example of Figure 3.\nUpon receiving the final keypoint locations by executing the generated code, we compute a scalar reward to evaluate the policy's performance. The reward function, $f^{(i)}$, facilitates learning by combining the following terms:\n*   Gripper-to-object Distance Reward ($r_{dist}$): Encourages the robot to approach the object of interest by penalizing large distances between them.\n*   Direction Reward ($r_{dir}$): Guides the robot to move the keypoints in the direction of the target locations.\n*   Alignment Reward ($r_{align}$): Drives the robot to position the keypoints close to their target locations.\n*   Success Bonus ($bonus$): Provides an additional reward when the average distance between the keypoints and their target positions remains within a specified threshold for a certain number of timesteps, indicating successful task completion.\n*   Penalty Term ($r_{penalty}$): Applies penalties for undesirable actions such as excessive movements, dropping the object, or applying excessive force.\n$f^{(i)} = \\alpha_{dist} r_{dist} + \\alpha_{dir} r_{dir} + \\alpha_{align} r_{align} + \\alpha_{bonus} bonus + \\alpha_{penalty} r_{penalty}$"}, {"title": "B. Transferring real-world scene to simulation", "content": "To transfer the real-world scene within the workspace boundary to simulation, we first generate 3D meshes of manipulable objects, such as the shoe box and shoes shown in Figure 3, by capturing video footage of each object as it is moved to ensure the camera captures all sides. These videos allow for accurate 3D mesh reconstruction using BundleSDF [63], and multiple objects can be processed in parallel to speed up the scanning phase. Once a mesh is created for an object, it can be reused in different settings, eliminating the need to recreate it for each new scenario.\nWith the meshes prepared, we use FoundationPose [90] to estimate the objects' poses, enabling precise placement of the corresponding meshes in the simulated environment. For"}, {"title": "C. Policy Training in Simulation", "content": "We control the robot in the end-effector space, which has six degrees of freedom: three prismatic joints for movement along the $x$, $y$, and $z$ axes, and three revolute joints for rotation. The gripper fingers remain closed by default, opening only when grasping objects. Refer to Appendix A for a detailed discussion on grasping.\nState Space: The state space for our policy captures the essential information to execute the task. The input is a vector $s_t$ consisting of the gripper's end-effector pose $(p_e, q_e) \\in \\mathbb{R}^7$, the pose of object currently being manipulated $(p_o, q_o) \\in \\mathbb{R}^7$, a set of object keypoints$\\mathcal{K}_o= \\{k_j\\}_{j=1}^{K} \\in \\mathbb{R}^{K\\times 3}$, and their corresponding target positions$\\mathcal{K}_t = \\{k_j\\}_{j=1}^{K} \\in \\mathbb{R}^{K\\times 3}$. $\\mathcal{K}_o$ is calculated by applying rigid body transformations to keypoints defined in the object's local coordinate frame, mapping them to their corresponding positions in the world frame. $\\mathcal{K}_t$ is derived from the reward function $f^{(i)}$ generated by the VLM. Rotations $q_e$ and $q_o$ are represented as quaternions. This state space $S_t = (p_e, q_e, p_o, q_o, \\mathcal{K}_o, \\mathcal{K}_t)$ captures essential information on objects of interest as well as the goal of the policy. Instead of incorporating raw RGBD data directly into the state space, object poses and keypoints are extracted from RGBD inputs using a vision-based pose estimation method, as detailed in Section III-D. This preprocessing step removes the necessity of including raw RGBD data in the policy.\nAction Space: The action space is defined relative to the gripper's current position and orientation. The policy outputs actions $a_t = (\\Delta p_e, \\Delta r_e)$, where $\\Delta p_e \\in \\mathbb{R}^3$ and $\\Delta r_e \\in \\mathbb{R}^3$ specifies the changes in translation and rotation respectively.\nTraining Algorithm & Architecture: We train our policies using IsaacGym [91] simulator with the PPO [92] algorithm. We use an actor-critic architecture [93] with a shared backbone. The network is a multi-layer perceptron (MLP) consisting of hidden layers with 256, 128, and 64 units, each followed by ELU [94] activation. Currently, it takes about 5 minutes to train per task, which can be prohibitive for certain applications. However, this training time can be reduced by increasing the number of parallel environments and utilizing more powerful GPUs.\nDomain Randomization (DR): Recognizing the challenges inherent in transferring policies between the simulation and the real world, we employ DR to bridge the real-to-sim-to-real gaps. DR is applied to object properties like friction, mass, restitution, compliance, and geometry. We further randomize the object position, the gripper location, and the grasp within a range. We found these to be especially crucial for non-prehensile tasks like pushing. The specific parameter ranges are detailed in Appendix B, and the effectiveness of DR is evaluated in Section IV-E."}, {"title": "D. Deployment of Trained Policy", "content": "The trained RL policy $\\pi_i$ is deployed directly in the real world. Since the policy outputs the end-effector pose, we employ inverse kinematics to compute the joint angles at each timestep. The RL policy operates at 10Hz, producing action commands that are then clipped to ensure the end effector remains within the workspace limits. For keypoint tracking, we utilize FoundationPose [90] to estimate the object's pose. These pose estimates are subsequently used to compute the keypoint locations that are defined relative to the objects. When VLM predicts to grasp objects, we use AnyGrasp [95] to detect grasps in the real-world."}, {"title": "IV. EXPERIMENTS AND ANALYSIS", "content": "We aim to investigate whether Iterative Keypoint Reward can effectively represent reward functions for diverse manipulation skills within our IKER for real-to-sim-to-real pipeline. We also want to see whether our pipeline can perform multi-step tasks in dynamic environments by leveraging Iterative Keypoint Reward as feedback for replanning."}, {"title": "V. CONCLUSION AND LIMITATIONS", "content": "In this work, we introduced Iterative Keypoint Reward (IKER), a framework that leverages VLMs to generate visually grounded reward functions for robotic manipulation in open-world environments. By using keypoints from RGB-D observations, our approach enables precise SE(3) control and integrates priors from VLMs without relying on rigid instructions. IKER bridges simulation and real-world execution through a real-to-sim-to-real loop, training policies in simulation and deploying them in physical environments. Experiments across diverse tasks demonstrate the framework's ability to handle complex, long-horizon challenges with adaptive strategies and error recovery. This work represents a step toward more intelligent and flexible robots capable of operating effectively in dynamic, real-world settings.\nDespite these advancements, our approach has certain limitations. We need to capture objects from all views to obtain object meshes. In the future, this may be simplified by using methods [52] that can generate meshes from a single image. Additionally, our real-to-sim transfer does not account for dynamics parameters, which could be modeled more accurately through system identification techniques. Also, while our framework reconstructs multiple objects in the environment, we do not account for tasks involving complicated multi-object interactions, limiting our evaluation primarily to single-object manipulation at each stage."}, {"title": "APPENDIX", "content": ""}, {"title": "A. Grasping Subroutine", "content": "During training, the gripper fingers open only in the grasp mode, where the end-effector approaches the object with open fingers and then closes them to grasp the object. We employ a heuristic-based grasp for faster training. In real-world, the gripper fingers remain closed until the grasp mode is triggered. AnyGrasp predicts an appropriate grasp pose and the fingers close at the predicted position. To address the sim-to-real gap, we add randomization to the heuristic grasp pose during simulation. This allows the policy to generalize more effectively, resulting in more robust and reliable policies in the real-world."}, {"title": "B. Domain Randomization Parameters", "content": "To enhance the robustness of our policies for effective real-to-sim-to-real transfer, we apply domain randomization to various object properties and initial conditions. Table III details the key randomized parameters and their respective ranges. These variations ensure that our learned policies generalize effectively to real-world conditions, mitigating the discrepancies between simulation and real-world."}, {"title": "C. VLM Prompts", "content": "The VLM receives the image overlaid with keypoints 1,..., K, along with the task description as text. These are given to the VLM, along with the prompt. We do not provide any in-context examples with the prompt. Our prompt for single-step tasks is as follows:"}]}