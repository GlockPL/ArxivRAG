{"title": "Shortcut Learning in In-Context Learning: A Survey", "authors": ["Rui Song", "Yingji Li", "Fausto Giunchiglia", "Hao Xu"], "abstract": "Shortcut learning refers to the phenomenon where models employ simple, non-robust decision rules in practical tasks, which hinders their generalization and robustness. With the rapid development of large language models (LLMs) in recent years, an increasing number of studies have shown the impact of shortcut learning on LLMs. This paper provides a novel perspective to review relevant research on shortcut learning in In-Context Learning (ICL). It conducts a detailed exploration of the types of shortcuts in ICL tasks, their causes, available benchmarks, and strategies for mitigating shortcuts. Based on corresponding observations, it summarizes the unresolved issues in existing research and attempts to outline the future research landscape of shortcut learning.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) have emerged as a hotly pursued research direction, with the advent of major language models such as T5 (Raffel et al., 2020), LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2023), GPT-3 (Brown et al., 2020), Qwen2 (Yang et al., 2024a), and GLM (Zeng et al., 2023). LLMs demonstrate the ability of In-Context Learning (ICL), meaning they can learn from several demonstration examples within a given context (Dong et al., 2022). However, an increasing number of studies have shown that LLMs are susceptible to shortcut learning in ICL and perform poorly in many aspects (Du et al., 2024).\nShortcuts refer to decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions. Shortcut learning is the process by which a model exploits these decision rules during execution, leading to robust output results (Geirhos et al., 2020). In the literature, shortcut learning is also referred to as superficial/spurious correlations (Zhou et al., 2024c), the Clever Hans effect (Lapuschkin et al., 2019), and various types of biases 1, such as learning bias (Du et al., 2024), label bias (Fei et al., 2023), selection biases (Wei et al., 2024), co-occurrence bias (Kang and Choi, 2023), and so forth. In practice, if LLMs establish shortcut 'Flowers Position' based on given samples in Figure 1, predictions may fail when confronted with 'Negative' samples. Apart from this mapping from lexicon to specific labels, shortcuts also exist widely in various forms across different NLU tasks. For example, in Natural Language Inference (NLI) (Zhou and Bansal, 2020), Question-Answering (QA) (Sen and Saffari, 2020), and Reading Comprehension (RC) (Lai et al., 2021), there is a preference for overlapping parts of two input branches. These unhealthy shortcuts have been widely proven to be harmful to the robustness, generalization, and fairness, and may lead to hallucinations in LLMs (Li et al., 2023b; Ji et al., 2023).\nGiven the rapid development of LLMs and the impact of shortcut learning on them, our survey aims to systematically summarize, discuss, and"}, {"title": "2 Types of Shortcuts", "content": "Compared to (Du et al., 2024), we extend and summarize various types of shortcuts based on the characteristics of NLU tasks in LLMs. Specifically, we classify shortcuts into instinctive shortcuts and acquired shortcuts, based on whether LLMs rely on prompts to establish superficial correlations with labels or answers. But in reality, we believe that instinctive shortcuts can influence acquired shortcuts to some degree. In other words, instinctive shortcuts may facilitate LLMs in acquiring shortcuts from demonstrations (Pan et al., 2023)."}, {"title": "2.1 Instinctive Shortcuts", "content": "Instinctive shortcuts refer to the inherent preferences of LLMs for texts or patterns outside the examples provided in a given context. These preferences are not influenced by the contextual demonstrations but hinder the effective learning of the input-label relationships presented in those demonstrations. To avoid naming ambiguity, we have adopted the label bias definition proposed by (Fei et al., 2023), which has been widely discussed.\n\u2022 Vanilla-label bias describes the model's uncon-"}, {"title": "2.2 Acquired Shortcuts", "content": "Acquired shortcuts refer to the shortcuts present in demonstrations, which LLMs capture as non-robust patterns through observation of these demonstrations. These patterns are then utilized by LLMs for inference, leading to a decline in performance.\n\u2022 Lexicon. When spurious correlation are established between certain lexical features and specific labels, the corresponding lexicon may then become a shortcut. Arbitrary text can serve as lexicon shortcuts, including letters, signs, numbers, negation words, adverbs of degree, or even recurring sentences (Tang et al., 2023). Among them, negation words have garnered the most attention from researchers, as LLMs often provide contradictory predictions due to these words (Sun et al., 2024; Yuan et al., 2024; Zhou et al., 2024d). But studies have indicated that LLMs are more likely to rely on n-grams and content words rather than others, such as stop words (Si et al., 2023b; Pacchiardi et al., 2024).\n\u2022 Concept. Concept shortcut refers to the phenomenon where the concepts of certain texts in the demonstrations are disproportionately associated with the labels (Zhou et al., 2024c). For example, if sentences containing cities are associated with negative emotions and samples containing countries are also associated with negative emotions, when LLMs encounter new city words in the test sample, they may predict it as having negative emotions (Zhou et al., 2024d).\n\u2022 Overlap. Overlap shortcuts often emerge in tasks that involve two branches, such as NLI and QA. In NLI, overlap between the premise and the"}, {"title": "3 Causes of Shortcut Learning", "content": "In light of the impact of shortcut learning, some methods attempt to provide explanations for the causes of shortcut learning in ICL. This paper divides the causes of shortcut learning in LLMs into three parts: LLMs training, skewed demonstrations, and LLMs size."}, {"title": "3.1 LLMs training", "content": "Instinctive shortcuts reveal the inherent biases of LLMs. The causes of inherent biases are diverse and closely related to the training process of LLMs, involving factors such as data distribution and instruction tuning. This makes it difficult for LLMs to fully overcome the prediction biases acquired from pre-training data and to consider all contextual information equally during the ICL process (Kossen et al., 2024).\nPretraining. Some studies suggest that instinctive shortcuts originate from the data distribution during the pretraining process (Razeghi et al., 2022; McCoy et al., 2023), which leads to a bias in the predictions of LLMs towards high-frequency and frequently co-occurred words (Kang and Choi, 2023; Ju et al., 2024). Rare behavioral patterns in the training data, such as converse relations, can also confuse LLMs, causing them to rely on shortcuts when making judgments (Qi et al., 2023). Furthermore, (Zhou et al., 2024d) utilize SHAP analysis (Lundberg and Lee, 2017) to confirm that high-frequency words that have spurious correlations with a certain label contribute more significantly to the model's predictions for that label. Additionally, if the frequency of a particular LLMs task is low, its prediction results will also be inferior compared to those of high-frequency tasks (McCoy et al., 2023). Based on the observations mentioned above, (Han et al., 2022; Fei et al., 2023) implicitly assume that there is a shift in the label marginal of the ICL predictive distribution, and (Jiang et al., 2023) theoretically proves that the emergence of shortcuts stems from label shifting caused by the contextual label marginal typically deviating from the true label distribution.\nInstruction Tuning. Another perspective holds that the instinctive shortcuts in LLMs originate from the process of instruction tuning, during which the LLMs learn spurious correlations within the instructions (Sun et al., 2024). In complex multitasking scenarios, LLMs can also learn more intricate associations between tasks, features, and labels during the fine-tuning process. As a result, LLMs can exploit these spurious correlations simply by being provided with useful task information (Pan et al., 2023)."}, {"title": "3.2 Skewed demonstrations", "content": "Corresponding to LLMs training, skewed demonstrations are primarily responsible for the generation of acquired shortcuts, as LLMs learn from and reason based on the models presented in the demonstrations. If the demonstrations are skewed, the corresponding reasoning results will also be affected. Many studies have confirmed this by adding specific shortcut triggers to the demonstrations, where the shortcuts in the demonstrations caused a decline in the performance of LLMs (Tang et al., 2023; Si et al., 2023b; Levy et al., 2023)."}, {"title": "3.3 LLMs size", "content": "While larger LLMs exhibit exceptional capabilities in semantic understanding and reasoning, numerous studies have shown that increasing the LLMs' scale does not mitigate shortcut learning (Kang and Choi, 2023; Zhou et al., 2024d; Sclar et al., 2024). Rather, even larger LLMs are more prone to exploiting shortcuts, whether they are instinctive shortcuts or acquired shortcuts (Razeghi et al., 2022; Tang et al., 2023; Balepur et al., 2024; Yuan et al., 2024; Han et al., 2024). Although not directly stated, (Pan et al., 2023) provide some explanation for this phenomenon: larger LLMs can utilize more demonstrations to enhance their ability to learn new input-label mappings, whereas smaller LLMs cannot. Consequently, when there are more shortcuts in the context, larger LLMs are more susceptible to being influenced by these shortcuts."}, {"title": "4 Benchmarks of Shortcut Learning", "content": "ICL can encompass many of NLU tasks, targeting different types of shortcuts. Therefore, we deem it necessary to show existing evaluation benchmarks to support related research. Specifically, we summarize the datasets used in existing research, along with the corresponding quantitative metrics."}, {"title": "4.1 Datasets", "content": "Most studies have not developed shortcut evaluation datasets, but instead utilize existing datasets (Zhao et al., 2021; Fei et al., 2023; Gupta et al., 2023) or make simple modifications to adapt them to their own shortcut evaluation tasks. The most common approach is to add or delete elements from the samples in existing datasets to construct samples that contain shortcuts. For exampple, (Tang et al., 2023; Si et al., 2023b) introduce shortcuts by injecting shortcut trigger words, typically a sequence of characters, into the samples, and used this method to evaluate the impact of these shortcuts on ICL. Alternatively, shuffling or deleting option IDs can be used to evaluate the impact of"}, {"title": "4.2 Metrics", "content": "The most commonly used metrics in shortcut evaluation are general evaluation methods for NLP tasks such as accuracy, AUC score (Chen et al., 2023) as well as F1 score in classification tasks (Tang et al., 2023; Fei et al., 2023), Hits@n in retrieval related tasks (Kang and Choi, 2023; Pezeshkpour and Hruschka, 2024), and exact match score in machine reading comprehension tasks (Zhang et al., 2024a; Wang et al., 2023a). If the degree of shortcut learning in ICL is significant, then the corresponding perfomance will also undergo changes (Zhou et al., 2024c; Han et al., 2024; Zhou et al., 2024d). Similarly, Fluctuation Rate is designed to represent the ratio of the change in prediction results to the total after a sample is perturbed (Wei et al., 2024). In other studies, Conflict Rate is also equivalent to Fluctuation Rate (Zheng et al., 2024). (Taghanaki et al., 2024) define the Shortcut Selection Ratio, which is used to quantify the tendency of LLMs to maintain their original choices after introducing distracting items."}, {"title": "5 Mitigation of Shortcut Learning", "content": "Given the widespread existence and dangers of shortcuts, numerous studies have investigated corresponding methods to mitigate them. Based on the differences in the targets addressed by these mitigation methods, we categorize the existing approaches into three types: data-centric, model-centric, and prompt-centric."}, {"title": "5.1 Data-centric Approach", "content": "As summarized in Section 3, the shortcuts in LLMs are largely related to the distribution of training data. Therefore, an intuitive approach is to sample task-relevant examples to obtain balanced training data. (Kang and Choi, 2023) alleviate the dependence on shortcuts by filtering out samples with high co-occurrence probabilities and guiding the retraining of LLMs. (Zhou et al., 2024c) employ counterfactual samples generated by LLMs for up-sampling and re-trains to mitigate the reliance on shortcuts. (Nakada et al., 2024) proposes a systematic oversampling method that leverages the capabilities of LLMs to generate high-quality synthetic data for minority groups. However, the cost of fine-tuning LLMs is too high, and it can also result in catastrophic forgetting (Li and Lee, 2024). Therefore, there are relatively few corresponding studies."}, {"title": "5.2 Model-centric Approach", "content": "The model-centric approach mitigates the impact of shortcuts on the results by manipulating neurons and the predictive probability distribution. The main methods can be divided into those based on model pruning and those based on calibration.\nModel pruning alleviates the impact of shortcuts on the model itself by removing biased neurons. This operation can be supported by research on the relationship between neurons and knowledge storage, where neurons are the basic units for storing knowledge and patterns in LLMs (Zhao et al., 2024a). In this process, some commonly used post-hoc interpretable methods, such as Integrated Gradients (Sundararajan et al., 2017), are used to locate the neurons activated during shortcut learning. By removing these neurons, LLMs can be prompted to explore alternative and correct reasoning paths (Ju et al., 2024; Yang et al., 2024b). Generally, the weights of neurons with high shortcut scores are set to 0 to achieve the purpose of pruning (Ali et al., 2024). By quantifying the contribution of feedforward neural networks and attention heads to prediction results, (Zhou et al., 2024b) identify biased components of LLMs and alleviates their prompt brittleness by removing the corresponding components. However, the negative impact of such component removal on LLMs is still debatable, as neurons have been observed to be polysemous, meaning that a single neuron can be activated by multiple unrelated terms (Liu et al., 2022; Bills et al., 2023). That means when shortcut components are removed, some corresponding useful knowledge may also be discarded, introducing new noise.\nCalibration corrects biased outputs of LLMs by modifying theirs probability distribution (Guo et al., 2017). Contextual Calibration obtains context-driven biased distributions by replacing the sample to be predicted with meaningless inputs, such as 'N/A' (Zhao et al., 2021). In an ideal unbiased environment, even if the sample to be predicted is set to 'N/A', LLMs (Large Language Models) should yield an output similar to 50% Positive and 50% Negative. If the prediction distribution shifts, then this shift is attributed to the corresponding context. Subsequently, during actual predictions, LLMs' outputs are adjusted based on the biased distribution to mitigate biases in the context.\nAs research progresses gradually, Contextual Calibration has spawned various calibration methods tailored for different shortcut mitigation scenarios. Prototypical Calibration employs a Gaussian mixture distribution to estimate prototypical clusters for all categories and then utilizes the likelihood of these prototype clusters to calibrate LLMs'"}, {"title": "5.3 Prompt-centric Approach", "content": "The prompt-centric approach aims to reduce LLMs' focus on shortcuts by modifying the prompts. Depending on the specific part of the prompt that is modified, it can be classified into three methods: shortcut-based method, instruction format-based method, and prompt search-based method.\nShortcut-based method mitigates the unhealthy focus of LLMs on shortcuts by masking or perturbing shortcut words or concepts. (Zhou et al., 2024c) masks words with high relevance to shortcut concepts, where relevance is measured using Pointwise Mutual Information. (Wang et al., 2023a) replace entities that may rely on shortcuts with placeholders and alleviates the focus on a particular entity by guiding LLMs to understand the placeholders as a set of similar entities.\nInstruction format-based method guides LLMs to produce unbiased responses by modifying"}, {"title": "6 Rethinking of Shortcut learning", "content": "This section compares the shortcut learning in ICL with that in LPLMs, and outlines several potential future research directions."}, {"title": "6.1 Differences from Shortcuts in LPLMs", "content": "Given that previous reviews have primarily focused on research into LPLMs, we deem it necessary to elucidate the similarities and differences between shortcut learning in LLMs and LPLMs.\nThe difference between them primarily arises from the distinctions in research paradigms between LPLMs and LLMs. LPLMs typically require fine-tuning for different tasks, whereas LLMs unify various NLU tasks and only need a few samples to guide their inference. Therefore, ICL encounters a broader range of shortcut types compared to those in LPLMs, even though both include shortcut types such as lexicon (Niven and Kao, 2019), overlap (Lai et al., 2021), position (Ko et al., 2020), and text style (Qi et al., 2021). For instance, since LLMs do not require fine-tuning, the biases inherent in LLMs themselves can strongly influence various tasks, leading to the emergence of the instinctive shortcuts. Furthermore, since LLMs can easily infer answers from few samples, they naturally encounter shortcuts arising from these samples' group dynamics. The shortcuts related to group dynamics may, to some extent, resemble the challenges faced in long-tailed classification, where models tend to learn answers from head classes rather than tail classes (Li et al., 2023a). But besides the skew in categories, LLMs are also influenced by more complex group decision-making process, such as the parity of options (Balepur et al., 2024). This can be attributed to LLMs' stronger comprehension abilities, enabling them to uncover complex correlations that LPLMs are unable to detect.\nAnother notable difference lies in shortcut mitigation strategies. For LPLMs, a widely adopted mitigation approach involves adjusting the model during fine-tuning, which encompasses techniques such as adversarial training (Rashid et al., 2021), regularization (Stacey et al., 2022), Product-of-Experts (Sanh et al., 2021), reweighting (Liu et al., 2021), contrastive learning (Song et al., 2023), and other related operations. For LLMs, fine-tuning to mitigate shortcuts is rare, and the primary methods focus on research based on forward propagation. But we can still find extensions of LPLMs in the related research on LLMs. For instance, Calibration"}, {"title": "6.2 Future Directions", "content": "More Robust Evaluation Benchmark. (Siska et al., 2024) demonstrate that the correlation between the performance exhibited by the model and the prompts is significant, which suggests that the evaluation benchmark itself also exhibits bias. When the model responds to certain similar prompts within the benchmark, it tends to produce similar results. (Pacchiardi et al., 2024) also indicate that LLMs can exploit shortcuts in the benchmark to obtain inflated evaluation scores. In addition, data contamination and leakage can also impact the evaluation of LLMs' shortcuts, as LLMs tend to provide good prediction results directly for benchmarks that have appeared in the training corpus (Zhou et al., 2023). (Balepur et al., 2024) also imply that leakage of the test set might have an impact on multiple-choice questions without the question. The aforementioned studies suggest that caution should be exercised when interpreting the performance results of LLMs, and the impact of shortcuts should be fully considered. Furthermore, efforts should be made to minimize the potential influence of shortcuts on prediction results before constructing benchmarks.\nMore Shortcut Related Tasks. Despite the extensive exploration of shortcut learning in ICL, given the diversity of NLP tasks, it remains crucial to expand the boundaries of research tasks. For instance, in Table QA tasks, only a small portion of the entire table is relevant to deriving the answer to a given question, and irrelevant information may become potential shortcuts that affect the prediction outcomes of LLMs (Patnaik et al., 2024). These expanded tasks are highly dependent on relevant benchmarks, so designing and constructing corresponding datasets and evaluation methods for new tasks also require attention.\nMore Interpretability. Although the studies in Section 3 explore the causes of shortcuts from different aspects, most of them are empirical summaries based on experimental observations, lacking an interpretation of the causes of shortcuts from an explainable perspective. For instance, how ICL influences the role of instinctive shortcuts in different NLU tasks, and at which step of the forward propagation in LLMs do these shortcuts exert their impact, is it during aggregation or distribution (Wang et al., 2023b)? Can self-explanation methods of"}, {"title": "7 Conclusion", "content": "This paper comprehensively reviews the existing literature related to shortcut learning in LLMs, conducts a thorough classification of shortcuts in ICL, discusses their causes, presents existing evaluation benchmarks and mitigation strategies, and identifies key challenges and potential directions for future work. To our knowledge, this is the first review dedicated to the phenomenon of shortcut learning in ICL. We aim to emphasize the current research status of shortcut learning in the context of LLMs and provide insights to guide future work."}]}