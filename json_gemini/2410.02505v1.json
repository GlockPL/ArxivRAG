{"title": "DOG-IQA: STANDARD-GUIDED ZERO-SHOT MLLM\nFOR MIX-GRAINED IMAGE QUALITY ASSESSMENT", "authors": ["Kai Liu", "Ziqing Zhang", "Wenbo Li", "Renjing Pei", "Fenglong Song", "Xiaohong Liu", "Linghe Kong", "Yulun Zhang"], "abstract": "Image quality assessment (IQA) serves as the golden standard for all models'\nperformance in nearly all computer vision fields. However, it still suffers from poor\nout-of-distribution generalization ability and expensive training costs. To address\nthese problems, we propose Dog-IQA, a stanDard-guided zero-shot mix-grained\nIQA method, which is training-free and utilizes the exceptional prior knowledge\nof multimodal large language models (MLLMs). To obtain accurate IQA scores,\nnamely scores consistent with humans, we design an MLLM-based inference\npipeline that imitates human experts. In detail, Dog-IQA applies two techniques.\nFirst, Dog-IQA objectively scores with specific standards that utilize MLLM's\nbehavior pattern and minimize the influence of subjective factors. Second, Dog-\nIQA comprehensively takes local semantic objects and the whole image as input\nand aggregates their scores, leveraging local and global information. Our proposed\nDog-IQA achieves state-of-the-art (SOTA) performance compared with training-\nfree methods, and competitive performance compared with training-based methods\nin cross-dataset scenarios. Our code will be available at https://github.com/Kai-\nLiu001/Dog-IQA.", "sections": [{"title": "1 INTRODUCTION", "content": "Image quality assessment (IQA) aims to provide accurate quality scores that align with human mean opinion scores (MOS). With the booming of digital technology, the explosion of visual content calls for advanced IQA methods in all fields including communication (Zhou & Wang, 2022), entertainment (Wu et al., 2024e), professional use (Chow & Paramesran, 2016; Fang et al., 2020), and recently popular AI-generated content (Kirstain et al., 2023; Li et al., 2023). Over time, significant contributions have been made in this domain, evolving from traditional hand-crafted feature-based approaches (Wang et al., 2004; Mittal et al., 2012b) to deep neural network (DNN)-based methods (Talebi & Milanfar, 2018; Ying et al., 2020; Qin et al., 2023; Saha et al., 2023), bringing steady improvements in accuracy.\nNonetheless, these IQA methods still suffer from poor out-of-distribution (OOD) generalization abil-ity (You et al., 2024) and expensive training costs (Wu et al., 2024a). One potential solution to the OOD issue involves training DNNs on a combination of multiple IQA datasets. Although it sounds promising, this approach fails due to inconsistent standards used during dataset construction, leading to distribution mismatches across datasets. For instance, an image rated high quality in one dataset may receive a low-quality score in another, ultimately degrading model performance. Another approach is to create a larger, more diverse dataset representing a wide range of distortions. However, aside from the increased training costs, the scoring process is labor-intensive and time-consuming, making this approach impractical. As a result, poor OOD performance remains an open problem."}, {"title": "2 RELATED WORKS", "content": "Training-free IQA. Training-free IQA is a critical approach in the field of image processing, allowing for the evaluation of image quality without the need for distortion-specific or human-rated training"}, {"title": "3 METHODOLOGY", "content": "We provide a comprehensive explanation of our proposed Dog-IQA method. The overall pipeline of our proposed Dog-IQA is shown in Figure 3. The image to be assessed is segmented into multiple sub-images with the segmentation process pipeline. Given a detailed standard, the MLLM rates the whole image and sub-images with scores in {1, 2, . . ., 7}. These scores will be finally aggregated to form the final number. Specifically, we first propose the standard-guided scoring mechanism, which effectively leverages its prior knowledge and its behavior pattern. Second, we discuss the mix-grained aggregation mechanism, which consists of the process of obtaining suitable sub-images and the aggregation of scores. The rationale behind using sub-images as inputs is also included."}, {"title": "3.1\nSTANDARD-GUIDED SCORING MECHANISM", "content": "The ultimate goal of image quality assessment (IQA) is to evaluate images in a manner that closely mirrors human judgment. Thanks to their extensive training data and vast prior knowledge, MLLMs are capable of perceiving images in a way that aligns with human perception (Wu et al., 2023), giving them an inherent advantage for IQA tasks. However, expecting an MLLM to output precise quality scores, such as 87.5, is impractical. This is because a score like 87.5 is not represented by a single"}, {"title": "3.2 MIX-GRAINED AGGREGATION MECHANISM", "content": "The mix-grained aggregation mechanism can be divided into two parts. The first part introduces the segmentation pipeline, while the second part presents the aggregation of multiple scores.\nSegmentation Process Pipeline. When humans recognize an image, they start from the global structure and gradually dive into the local parts. (Navon, 1977; F\u00f6rster, 2012; Gerlach & Poirel, 2018) This hierarchical process also applies when assessing image quality. Therefore, under the assumption that MLLMs share a similar perception process, it is essential to deliberately leverage meaningful sub-images. Specifically, 'meaningful' means that these sub-images should not be obtained through random cropping but through instance or semantic segmentation techniques.\nThe segmentation model is an excellent choice as it tends to segment the semantic objects out. The object segmented by the segmentation model is padded with zeros around. While this padding has minimal impact on human perception, as humans can easily recognize the black padding as meaningless and mentally disregard it, this is not the case for MLLMs. The visual encoder within the MLLM processes the padding as part of the actual image, leading the model to misinterpret the black regions as the real background. This misunderstanding can result in distinct errors, such as the"}, {"title": "4 EXPERIMENTS", "content": "Data and Evaluation. We select the following datasets to evaluate our IQA method: KonIQ (Hosu et al., 2020), LIVE Challenge (Ghadiyaram & Bovik, 2015), SPAQ (Fang et al., 2020), KADID (Lin et al., 2019), and AGIQA (Li et al., 2023). KonIQ and SPAQ are large in-the-wild IQA datasets with more than 10k images. LIVE Challenge is a smaller in-the-wild dataset with 1.1k images. KADID-10k is a synthetic dataset, while AGIQA-3k focuses on AI-generated images. Together, these datasets provide a comprehensive range of image types and quality variations for evaluation.\nAs our proposed method is training-free, we compare its performance against two categories of approaches. The first category is training-free methods, including BIQI (Moorthy & Bovik, 2010), and BLINDS-II (Saad et al., 2010), BRISQUE (Mittal et al., 2012a), NIQE (Mittal et al., 2012b), and"}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "SPAQ"}, {"title": "4.2 COMPARISON WITH STATE-OF-THE-ART METHODS", "content": "We conduct extensive experiments to evaluate the performance of our proposed Dog-IQA model. The comparisons with SOTA methods are divided into two categories: training-free methods, shown in Table 2, and training-based methods, as presented in Table 3.\nTraining-free methods can be broadly categorized into two types. The first category includes CLIP-IQA, which leverages the prior knowledge of CLIP and generates scores based on the similarity between text and image embeddings. The second category consists of models such as BIQI, BLINDS-II, BRISQUE, and NIQE, which rely on hand-crafted features. As shown in Table 2, the traditional hand-crafted features often fail to score accurately due to the complex nature of human opinions on image quality. CLIP-IQA benefits from its prior knowledge and demonstrates higher accuracy than hand-crafted feature-based methods. Our Dog-IQA model consistently achieves superior performance across all metrics and datasets, significantly outperforming existing training-free methods.\nTable 3 summarizes the performance of various training-based methods in cross-dataset evaluations. These experiments test the out-of-distribution generalization ability of the models, which is a crucial aspect of IQA. For these comparisons, we select KonIQ and SPAQ as training sets due to their large size and in-the-wild characteristics. Notably, our Dog-IQA method requires no training or fine-tuning on these datasets, making its strong performance even more remarkable.\nTraining-based methods show variability depending on the dataset used for training. For example, SRCC and PLCC scores of Q-Aling on KADID-10k drop significantly when switching the training set from KonIQ to SPAQ, despite both being in-the-wild datasets. In contrast, Dog-IQA demonstrates stable performance without any training, highlighting its advantage in terms of generalization and cost-efficiency. Moreover, scoring AI-generated images has become increasingly critical in the current era of AI advancements. In the KonIQ \u2192 AGIQA-3k scenario, Dog-IQA achieves the highest SRCC (0.823) and PLCC (0.797), clearly outperforming the second-best model, which only achieves 0.735 SRCC. This result underscores the superiority of Dog-IQA in cross-dataset evaluations, especially"}, {"title": "4.3 VISUALIZATOIN", "content": "We visualize the scores predicted by humans and our proposed Dog-IQA on SPAQ and KonIQ datasets in Figure 4. The range of the final score varies between 1 to 7.66 (SPAQ) and 7.64(KonIQ) which are slightly higher than 7. This is because the final score consists of the area-weighted average of scores and the number of masks. As the scores from MLLM are discrete, the final scores are denser around the integer values. The mask number scheme and area average mechanisms help the continuous-like distribution, which further improve Dog-IQA's performance."}, {"title": "4.4 ABLATION STUDY", "content": "The ablation studies provided in Tables 4, 5, and 6 highlight the significance of various components in our proposed Dog-IQA model. By systematically altering key aspects of the model, the experiment evaluates how each component affects performance on two datasets: SPAQ and AGIQA-3k. We examine components including 1) the standard given to MLLM, 2) the selection of the mask and bounding box, 3) the aggregation of local scores, 4) the effectiveness of $s_{seg}$, 5) the influence of global and local quality, 6) the number of words, and 7) MLLM selection. The experiment results are shown in Tables 4, 5, and 6. Next, we will analyze the impact of each component in detail.\nStandard. Standard-guided scoring is a critical aspect of our model. We compare three forms of standards, namely number, word, and sentence. The number-based approach involves asking the MLLM to score image quality directly in the range {1, 2, . . ., K}. The word-based approach adds descriptive adjectives, such as excellent, fair, and bad, to each score. The sentence-based approach assigns a sentence describing quality for each score level, such as 4: Fair! The overall quality of the image is fair. There are certain merits but also some deficiencies.\nAs shown in experiments 1, 2, and 7 in Table 4, the word-based standard yields the best performance as it provides an accurate mapping between number and quality. While sentences offer more detailed context than numbers, they can introduce abstract terms (e.g., some, certain) that may distract the model, resulting in slightly lower performance. Numbers, on the other hand, perform poorly because the MLLM struggles to understand their relationship to image quality without additional context. In conclusion, associating a word with each score effectively enhances the MLLM's scoring accuracy.\nMask and Bounding Box. When scoring sub-images, we test three input formats: masks (semantic object coverings), bounding boxes (enclosing the masks), and the entire image. As shown in experiments 4, 5, and 7 in Table 4, using masks significantly degrades performance. This is mainly because the constant padding applied to masked areas is still interpreted by the MLLM's visual encoder, negatively influencing the score. Conversely, using the entire image as input provides moderate results, though still inferior to bounding boxes. Bounding boxes improve performance"}, {"title": "5 LIMITATIONS AND DISCUSSIONS", "content": "In this section, we will discuss the limitations of our proposed Dog-IQA. First, the impressive performance of Dog-IQA can be attributed not only to our novel design but also to the capabilities of the underlying MLLM. Ultimately, it is the MLLM that generates the quality scores, while our design better exploits its extensive prior knowledge. However, as shown in Table 6, the performance of MLLM increases significantly with version updation which will finally promote the development of IQA. Consequently, the selection of MLLM matters and Dog-IQA's performance may decline when utilizing MLLMs with poor image understanding ability.\nSecond, as an important part of the overall pipeline, the segmentation process can significantly impact the accuracy of Dog-IQA. If we switch to a segmentation model with subpar performance, mix-grained segmentation may fail, resulting in a direct score for the entire image instead. Additionally, if the segmentation model primarily outputs bounding boxes that lack a clear main object-such as only capturing the edges of an object or half of a human face-this can lead to MLLM's misjudgment and inaccurate scores, further degrade our Dog-IQA's performance. Thus, the choice of segmentation model and the segmentation granularity are critical factors influencing Dog-IQA's performance.\nThird, because the MLLM must evaluate the quality of each mask, the inference speed of Dog-IQA is relatively slow compared to models that require only a single inference. On average, Dog-IQA processes 7.22 masks and the entire image, resulting in 7\u00d7 longer inference time. After testing on a single NVIDIA RTX A6000 GPU, our proposed Dog-IQA can segment the whole SPAQ dataset in 50 minutes and score each mask and the total data within 6 hours. This process can be performed with data parallel, which means it takes around 1.5 hours to obtain the final result when running on 4 GPUs. While the text embeddings can be pre-calculated and reused, allowing for the omission of the text encoder, the total inference time remains longer than single forward inference. In conclusion, Dog-IQA may suffer from low processing speed if the segmentation granularity is too fine."}, {"title": "6 CONCLUSION", "content": "In this work, we propose Dog-IQA, a standard-guided zero-shot mix-grained IQA method, which is training-free and utilizes the exceptional prior knowledge of MLLMs. With the combination of SAM2 and mPLUG-Owl3, we propose two key mechanisms to enhance IQA performance. The standard-guided scoring mechanism ensures consistent and objective quality evaluation by aligning scores with predefined standards. The mix-grained aggregation mechanism refines the final quality score by aggregating global and object-centered sub-image quality scores. We conduct extensive experiments across a variety of datasets, benchmarking our proposed Dog-IQA against SOTA methods. The results demonstrate that Dog-IQA outperforms all previous training-free approaches and achieves competitive performance relative to training-based methods, which strongly supports the novelty and robustness of our proposed mechanisms. We also systematically conduct ablation studies, which further confirm the effectiveness of the novel mechanisms. This work highlights the exceptional image understanding capabilities of MLLMs and confirms the feasibility of attaining remarkable outcomes using solely pre-trained models. Future research will aim to reduce the computational costs associated with multiple inferences and enhance pixel-level quality assessments."}]}