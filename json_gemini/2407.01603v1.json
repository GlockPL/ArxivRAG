{"title": "A REVIEW OF LARGE LANGUAGE MODELS AND AUTONOMOUS AGENTS IN CHEMISTRY", "authors": ["Mayk Caldas Ramos", "Christopher J. Collison", "Andrew D. White"], "abstract": "Large language models (LLMs) are emerging as a powerful tool in chemistry across multiple domains. In chemistry, LLMs are able to accurately predict properties, design new molecules, optimize synthesis pathways, and accelerate drug and material discovery. A core emerging idea is combining LLMs with chemistry-specific tools like synthesis planners and databases, leading to so-called \"agents.\" This review covers LLMs' recent history, current capabilities, design, challenges specific to chemistry, and future directions. Particular attention is given to agents and their emergence as a cross-chemistry paradigm. Agents have proven effective in diverse domains of chemistry, but challenges remain. It is unclear if creating domain-specific versus generalist agents and developing autonomous pipelines versus \"co-pilot\" systems will accelerate chemistry. An emerging direction is the development of multi-agent systems using a human-in-the-loop approach. Due to the incredibly fast development of this field, a repository has been built to keep track of the latest studies: https://github.com/ur-whitelab/LLMs-in-science.", "sections": [{"title": "1 Introduction", "content": "The integration of Machine Learning (ML) and Artificial Intelligence (AI) into chemistry has spanned several decades. 1-10 Although applications of computational methods in quantum chemistry and molecular modeling from"}, {"title": "1.1 Challenges in chemistry", "content": "With the potential to accelerate scientific discovery through the use of AI, it makes sense to connect this potential to some of the more pressing opportunities in the chemical sciences as may be categorized under three broad umbrellas: Property Prediction, Property-Directed Molecule Generation, and Synthesis Prediction. The first challenge is to predict a property for a given compound to decide if it should be synthesized for a specific application, such as an indicator, 49 light harvester, 50 or catalyst. 51 To have better models for property prediction, good quality data is crucial. We discuss the caveats and issues with the current datasets in Section 3.1 and illustrate state-of-the-art findings in Section 3.2.\nThe second challenge is to generate novel chemical structures that meet desired chemical profiles or specific properties. 52 Success would accelerate progress in various chemical applications, but reliable reverse engineering (inverse design) 53 is not yet feasible over the vast chemical space. 54 For instance, inverse design coupled with automatic selection of novel structures (de novo molecular design) could develop a drug targeting a specific protein while retaining properties like solubility, toxicity, and blood-brain barrier permeability. 55 The complexity of connecting de novo design with property prediction is high and raises ethical questions that need to be addressed to guide the development of such models. We show how state-of-the-art models currently perform in Section 3.3.\nThe third major challenge is predicting its optimal synthesis using inexpensive, readily available, and non-toxic starting materials. There will always likely be a solution within a large chemical space: an alternative molecule with similar properties that is easier to synthesize. Exploring this space to find a new molecule with the right properties and a high-yield synthesis route unites these challenges. The number of possible stable chemicals is estimated to be up to 10180 56-59 Exploring this vast space requires significant acceleration beyond current methods. 60 Restrepo56 emphasizes the need to catalog data on failed syntheses to build a comprehensive dataset of chemical features. Autonomous chemical resources can accelerate database growth and tackle this challenge. Thus, automation is considered a fourth major challenge in chemistry.61\u201364 The following discussion explores how LLMs and autonomous agents can provide the most value. Relevant papers are discussed in Section 3.4"}, {"title": "2 Large Language Models", "content": "The prior state-of-the-art for seq2seq was considered to be the Recurrent Neural Network (RNN),65 typically as implemented in Hochreiter and Schmidhuber 66. The RNN retains \u201cmemory\u201d of previous steps in a sequence to predict later parts. However, as sequence length increases, gradients can become vanishingly small or explosively large, 67,68 preventing effective use of earlier information in long sequences. RNNs have thus fallen behind Large Language Models (LLMs).\nLLMs are deep neural networks (NN) with billions of parameters. Mostly, LLMs implement a transformers architecture, introduced by Vaswani et al. 69, although other architectures for longer input sequences are being actively explored. 70\u201372 Transformers are well-developed in chemistry and are the dominant paradigm behind nearly all state-of-the-art sequence modeling results and, thus, the focus. More detailed discussion on current LLMs applied to general purposes can be found elsewhere. 73"}, {"title": "2.1 The Transformer", "content": "The transformer was introduced in, \u201cAttention is all you need\u201d by Vaswani et al. 69 in 2017. A careful line-by-line review of the model can be found in \u201cThe Annotated Transformer\u201d.74 The transformer was the first sequence-to-sequence (seq2seq) model based entirely on attention. Here, a seq2seq model processes a text input, such as an English paragraph, and produces a different text sequence, such as a French translation of that paragraph. The concept of \u201cattention\" is a focus applied to certain words of the input, which would convey the most importance, or the context of the passage, and thereby would allow for better decision-making and greater accuracy. However, in a practical sense, \u201cattention\" is implemented simply as the dot-product between token embeddings with some non-linear learned function, and is described further below."}, {"title": "2.2 Model training", "content": "The common lifetime of an LLM consists of being first pretrained using unsupervised or self-supervised techniques, generating what is called a base model. This base model is then fine-tuned for specific applications using supervised techniques. Finally, the supervised fine-tuned model is further tuned with reward models to improve human preference or some other non-differentiable and sparse desired character. 81\nUnsupervised pretraining A significant benefit implied in all the transformer models described in this review is that unsupervised learning takes place with a vast corpus of text. Thus, the algorithm learns patterns from untagged data, which opens up the model to larger datasets that may not have been explicitly annotated by humans. The advantage is to discover underlying structures or distributions without being provided with explicit instructions on what to predict, nor with labels that might indicate the correct answer.\nSupervised fine-tuning After this pretraining, many models described herein are fine-tuned on specific downstream tasks (e.g., text classification, question answering) using supervised learning. In supervised learning, models learn from labeled data, and map inputs to known outputs. Such fine-tuning allows the model to be adjusted with a smaller, task-specific dataset to perform well on that downstream task.\nLLM alignment A key task after model training is aligning its output with what human users prefer. This can be done for improving LLM style and tone. Or, it can be to reduce harmful outputs because the training objectives used during the pretraining and the fine-tuning do not include human values. 82 Reinforcement learning (RL) techniques are among the most common alignment techniques. RL is a type of machine learning where actions are performed and feedback is received through rewards, the goal being to learn a policy that maximizes the cumulative reward. However, human preferences can only be evaluated on complete outputs from models, preventing supervised fine-tuning (which requires token-by-token losses). 83\u201385 To apply RL, sampling tokens from the model is recast as a Markov Decision Process (MDP) - actions are tokens and the reward is zero until the EOS (end of sequence) token, at which point the human preference reward is applied 86 Reinforcement Learning with Human Feedback (RLHF)87 is the most used alignment technique. However, alternative strategies include reinforcement learning with synthetic feedback (RLSF), 88 proximal"}, {"title": "2.3 Model types", "content": "While the Vaswani Transformer69 employed an encoder-decoder structure for sequence-to-sequence tasks, the encoder and decoder were ultimately seen as independent models, leading to \u201cencoder-only\", and \"decoder-only\" models described below."}, {"title": "2.3.1 Encoder-only models", "content": "Beyond Vaswani's transformer, 69 used for sequence-to-sequence tasks, another significant evolutionary step forward came in the guise of the Bidirectional Encoder Representations from Transformers, or \u201cBERT\u201d, described in October 2018 by Devlin et al. 98 BERT utilized only the encoder component, achieving state-of-the-art performance on sentence-level and token-level tasks, outperforming prior task-specific architectures. 98 The key difference was BERT's bidirectional transformer pretraining on unlabeled text, meaning the model looks at the context both to the left and right of the word in question, facilitated by a Masked Language Model (MLM). This allows a richer understanding of the sentence. The encoder-only design meant BERT focused on generating a deeper \"understanding\" of the input sequence, rather than mapping input sequences to output sequences. In pretraining, BERT also uses Next Sentence Prediction (NSP). \"Sentence\" here means an arbitrary span of contiguous text. The MLM task randomly masks tokens and predicts them by looking at preceding and following contexts simultaneously, inspired by Taylor.99 NSP predicts whether one sentence logically follows another, training the model to understand sentence relationships. This bidirectional approach allows BERT to recognize greater nuance and richness in the input data.\nSubsequent evolutions of BERT include, for example, RoBERTa, (Robustly optimized BERT approach), described in 2019 by Facebook AI's Liu et al. 100. RoBERTa was trained on a larger corpus, for more iterations, with larger mini-batches, and longer sequences, improving model understanding and generalization. By removing the NSP task and focusing on the MLM task, performance improved. RoBERTa dynamically changed masked positions during training and used different hyperparameters. Evolutions of BERT also include domain-specific pretraining and creating specialist LLMs for fields like chemistry, as described below (see Section 3)."}, {"title": "2.3.2 Decoder-only models", "content": "In June 2018, Radford et al. 101 from OpenAI proposed the Generative Pretrained Transformer (GPT) in their paper, \"Improving Language Understanding by Generative Pretraining\". GPT used a decoder-only, left-to-right unidirectional language model to predict the next word in a sequence based on previous words, without an encoder. Unlike the Vaswani Transformer's decoder, GPT could predict the next sequence, applying general language understanding to specific tasks with smaller annotated datasets.\nGPT employed positional encodings to maintain word order in its predictions. Unlike the Vaswani Transformer, GPT's self-attention mechanism prevented tokens from attending to future tokens, ensuring each word prediction depended only on preceding words. Hence a decoder-only architecture represents a causal language model, one that generates each item in a sequence based on the previous items; the generation of each subsequent output is causally linked to the history of generated outputs and nothing ahead of the current word affects its generation."}, {"title": "2.3.3 Encoder-decoder models", "content": "Evolving further, BART (Bidirectional and Auto-Regressive Transformers) was introduced by Lewis et al. from Face- book AI in 2019.102 BART combined the context learning strengths of the bidirectional BERT, and the autoregressive capabilities of models like GPT, which excel at generating coherent text. BART was thus a hybrid seq2seq model, consisting of a BERT-like bidirectional encoder and a GPT-like autoregressive decoder. This is nearly the same architecture as Vaswani et al. 69; the differences are in the pretraining. BART was pretrained using a task that corrupted"}, {"title": "2.3.4 Multi-task and multi-modal models", "content": "In previous sections, we discussed LLMs that take natural language text as input and then they output either a learned representation or another text sequence. These models traditionally perform tasks like translation, summarization, and classification. Raffel et al. 103 developed the Text-to-Text Transfer Transformer (T5) to demonstrate that various tasks can be reframed as text-to-text tasks, allowing the same model architecture and training procedure to be used with the same set of weights across different tasks. This approach has inspired the development of multi-task models that can adapt to different tasks at inference time. For example, Flan-T5104 used instruction fine-tuning with chain-of-thought prompts, enabling it to generalize to unseen tasks, such as generating rationales before answering. More complex approaches have been proposed for robust multi-task models. 105-108\nAdditionally, LLMs have been extended to understand different input modalities, despite initially receiving only text. For instance, Fuyu 109 uses linear projection to adapt image representations into the token space of an LLM, enabling a decoder-only model to write figure captions. Scaling up, next-GPT110 was developed as an \u201cany-to-any\u201d model, processing multiple modalities\u2014text, audio, image, and video\u2014through modality-specific encoders. The encoded representation is projected into a decoder-only token space, and the LLM's output is processed by a domain-specific diffusion model to generate each modality's output. Multitask or multimodel methods are further described below as these methods start to connect LLMs with autonomous agents."}, {"title": "3 LLMs for chemistry and biochemistry", "content": "Integrating LLMs into chemistry and biochemistry has revolutionized molecular design, synthesis prediction, and property analysis. These models, trained on vast datasets, interpret chemical languages like SMILES and InChI to predict molecular behavior accurately. This section explores the importance of trustworthy datasets and the necessity of good benchmarks and LLM applications in molecular representations, property prediction, inverse design, and synthesis prediction. Figure 3 illustrates the capabilities of different LLMs available currently, and Figure 4 shows a chronological map of LLMs in chemistry and biology."}, {"title": "3.1 Molecular Representations, Datasets, and Benchmarks", "content": "Given the diversity of data in chemistry-based machine learning, and the many different ways that a compound can be described, from molecular structure to property description, there are multiple ways a molecule can be represented, underlining this heterogeneity. 111\u2013114 Some common forms include molecular graphs, 115\u2013117 3D point clouds, 118\u2013121 and quantitative feature descriptors. 112,122\u2013125 In this review, we will focus on string-based representations of molecules, given the interest in language models. Among the known string representations, we can cite IUPAC names, SMILES, 41 DeepSMILES, 126 SELFIES, 127 and InChI, 128 as recently reviewed by Das et al. 129."}, {"title": "3.2 Property Prediction and Encoder-only mol-LLMs", "content": "Encoder-only transformer architectures solely comprise an encoder. Chemists can leverage the simpler encoder-only architecture for tasks where the primary goal is classification, efficient exploration of chemical space, and property prediction. Since encoder-only architectures are mostly applied to property prediction, we describe here the relative importance of this principal chemical challenge. Sultan et al. 189 also discussed the high importance of this task."}, {"title": "3.2.1 Property Prediction", "content": "The universal value of chemistry lies in identifying and understanding the properties of compounds to optimize their practical applications. In the pharmaceutical industry, therapeutic molecules interact with the body in profound ways. 190\u2013192 Understanding these interactions and modifying molecular structures to enhance those therapeutic benefits can lead to significant medical advancements. 193 Similarly, in polymer science, material properties depend on chemical structure, polymer chain length, and packing,194 and a protein's function similarly depends on its structure and folding. Historically, chemists have identified new molecules from natural products 195 and screened them against potential targets 196 to test their properties for diseases. Once a natural product shows potential, chemists synthesize scaled-up quantities for further testing or derivatization, 197\u2013199 a costly and labor-intensive process. .200,201 Traditionally, chemists have used their expertise to hypothesize the properties of new molecules derived from those natural products, hence aiming for the best investment of synthesis time and labor. To support the chemical industry in more accurate property prediction, computational chemistry has evolved. 202 Quantum theoretical calculations predict certain properties reliably, while force-field-based Molecular Dynamics (MD)203 predict packing and crystal structures of large molecular ensembles, though both require substantial computational resources. 204-208 Property prediction can now be enhanced through machine learning tools, 124,209\u2013211 and more recent advancements in LLMs lead to effective property prediction without the extensive computational demands of quantum mechanics and MD calculations. Combined with human insight, AI can revolutionize material development, enabling the synthesis of new materials with a high likelihood of possessing desired properties for specific applications."}, {"title": "3.2.2 Encoder-only Mol-LLMs", "content": "Encoder-only models are exemplified by the BERT architecture, which is commonly applied in natural language sentiment analysis to extract deeper patterns from prose. .212 The human chemist has been taught to look at a 2D image of a molecular structure and to recognize its chemical properties or classify the compound. Therefore, encoder-only models would ideally convert SMILES strings, empty of inherent chemical essence, into a vector representation, or latent space, reflecting those chemical properties. This vector representation can then be used directly for various downstream tasks.\nSchwaller et al. 213 used a BERT model to more accurately classify complex synthesis reactions by generating reaction fingerprints from raw SMILES strings, without the need to separate reactants from reagents in the input data, thereby simplifying data preparation. The BERT model achieved higher accuracy (98.2%) compared to the encoder-decoder model (95.2%) for classifying reactions. Accurate classification aids in understanding reaction mechanisms, vital for reaction design, optimization, and retrosynthesis. Toniato et al. 214 also used a BERT architecture to classify reaction types for downstream retrosynthesis tasks that enable manufacture of any molecular target. Further examples of BERT use include unsupervised reaction atom-to-atom mapping. 215,216 These chemical classifications would accelerate research and development in organic synthesis, described further below."}, {"title": "3.3 Property Directed Inverse Design and Decoder-only mol-LLMs", "content": null}, {"title": "3.3.1 Property Directed Inverse Design", "content": "Nature has been a significant source of molecules that inhibit disease proliferation, as organisms have evolved chemicals to protect themselves. This has led to most pharmaceuticals being derived from natural products, 236,237 which offer advantages such as cell permeability, target specificity, and a vast chemical diversity. 238 However, despite these benefits, there are high costs and complexities associated with high-throughput screening and synthesis of natural products. 236,238 While building on the chemical diversity and efficacy of natural products, AI also opens new avenues for synthesizing unique compounds efficiently. Advanced in-silico molecular design allows for rapid mutation239 towards valid de-novo molecular structures that are synthesizable,211,240 streamlining the iterative process of drug development. Yet, the true innovation lies in the potential of LLM-driven target-directed molecular design or \u201cInverse Design,\u201d where we can start with a desired property and directly generate molecules that manifest this attribute, bypassing traditional stepwise modifications. 241 This capability dramatically accelerates the pathway from concept to viable therapeutic agents and aligns well with decoder-only LLM architectures."}, {"title": "3.3.2 Decoder-only mol-LLMS", "content": "In 2021, Adilov 235 presented \u201cGenerative pretraining from Molecules,\u201d one of the first applications of decoder-only models to SMILES strings. It pretrained a GPT-2-like causal transformer for self-supervised learning, introducing \u201cadapters\u201d between attention blocks for task-specific fine-tuning.242 This method, requiring minimal architectural changes, offered versatility in molecule generation and property prediction, aiming to surpass ChemBERTa's encoder- only performance with a more scalable and resource-efficient approach. Another early decoder-only SMILES-based model was Bagal et al's MolGPT model,.234 MolGPT, with a mere 6 million parameters, advanced GPT-type LLMs for molecular generation. Its decoder-only architecture with masked self-attention facilitated learning long-range dependencies, enabling chemically valid SMILES representations that met complex structural rules involving valency and ring closures. The paper also used salience measures for interpretability in predicting SMILES tokens. MolGPT outperformed many existing Variational Auto-Encoder (VAE) based approaches 243\u2013250 in predicting novel molecules with specified properties, being trained on datasets like MOSES251 and GuacaMol. 252 It showed good performance with metrics like validity, uniqueness, Frechet ChemNet Distance (FCD), 253 and KL divergence,.252 While MolGPT's computational demands might be higher than traditional VAEs, its ability to generate high-quality, novel molecules justifies this trade-off, but future research could likely optimize model efficiency or explore lighter versions. A brief summary of advancements in transformer-based models for de-novo molecule generation from 2023 and 2024 follows.\nHaroon et al. 254 further developed a GPT-based model with relative attention for de novo drug design, showing improved validity, uniqueness, and novelty. Both Wang et al. 255 and Mao et al. 256 presented work that surpassed MolGPT, with Mao et al.'s T5-type model257 generating novel compounds using IUPAC names directly. 258 Although T5-based, we include this here because of its relevance to de novo drug design. In a similar vein, Zhang et al. 259 proposed including target 3D structural information in molecular generative models, even though their approach is not LLM-based. However, we nonetheless note its value for future structure-based LLM drug design. They demonstrated that integrating additional biological data significantly enhances the relevance and specificity of generated molecules for targeted drug discovery. Wang et al. 260 discussed PETrans, a deep learning method for generating target-specific ligands using protein-specific encoding and transfer learning. This study emphasized the application of transformer models for generating molecules with a high binding affinity to specific protein targets.\nIn 2024, Yoshikai et al. 261 discussed the limitations of transformer architectures in recognizing chirality from SMILES representations, highlighting the challenges in learning overall molecular structures with, in particular, chirality impacting the prediction accuracy of molecular properties. They coupled a transformer with a VAE to address this. Qian et al. 262 introduced CONSMI, motivated by contrastive learning in NLP, to generate new molecules using multiple SMILES representations, improving molecular novelty and validity. Kyro et al. 233 presented ChemSpaceAL, an active learning method for protein-specific molecular generation, efficiently discovering molecules with desired characteristics without prior knowledge of existing inhibitors. Yan et al. 263 proposed the GMIA framework, featuring a graph mutual interaction attention decoder for drug-drug interaction prediction, enhancing prediction accuracy and interpretability. Lastly, Shen et al. 264 reported on AutoMolDesigner, an AI-based open-source software for automated design of small-molecule antibiotics.\nFor a deeper dive into decoder-only transformer architecture in chemistry, we highlight the May 2023 \u201cTaiga\u201d model by Mazuz et al. 85, and cMolGPT by Wang et al. 44. Taiga first learns to map SMILES strings to a vector space, and then refines that space using a smaller dataset of labeled molecules to produce molecules with targeted attributes. It"}, {"title": "3.4 Synthesis Prediction and Encoder-decoder Mol-LLMs", "content": "The encoder-decoder architecture is designed for tasks involving the translation of one sequence into another, making it ideal for predicting chemical reaction outcomes or generating synthesis pathways from given reactants. We begin with a background on optimal synthesis prediction and describe how earlier machine learning has approached this challenge. Following that, we explain how LLMs have enhanced chemical synthesis prediction and optimization."}, {"title": "3.4.1 Synthesis Prediction", "content": "Once a molecule has been identified through property-directed inverse design, the next challenge is to predict its optimal synthesis, including yield. Shenvi 278 describe how the demanding and elegant syntheses of natural products has contributed greatly to organic chemistry. However, in the past 20 years, the focus has shifted away from complex natural product synthesis towards developing reactions applicable for a broader range of compounds, especially in reaction catalysis.278 Yet, complex synthesis is becoming relevant again as it can be digitally encoded, mined by LLMs, and applied to new challenges. Unlike property prediction, reaction prediction is challenging due to the involvement of multiple molecules. Modifying one reactant requires adjusting all others, with different synthesis mechanisms or conditions likely involved. Higher-level challenges exist for catalytic reactions and complex natural product synthesis. Synthesis can be approached in two ways. Forward synthesis involves building complex target molecules from simple, readily available substances, planning the steps progressively. Retrosynthesis, introduced by E.J. Corey in 1988,279 is more common. It involves working backward from the target molecule, breaking it into smaller fragments whose re-connection is most effective. Chemists choose small, inexpensive, and readily available starting materials to achieve the greatest yield and cost-effectiveness. For example, the first total synthesis of discodermolide 280 involved 36 such steps, a 24-step longest linear sequence, and a 3.2% yield. There are many possible combinations for the total synthesis of a target molecule, and the synthetic chemist must choose the most sensible approach based on their expertise and knowledge. However, this approach to total synthesis takes many years. LLMs can now transform synthesis such that structure-activity relationship predictions can be coupled in lock-step with molecule selection based on easier synthetic routes. This third challenge of predicting the optimal synthesis can also lead to the creation of innovative, non-natural compounds, chosen because of such an easier predicted sytnthesis but for which the properties are still predicted to meet the needs of the application. Thus, these three challenges introduced above are interconnected."}, {"title": "3.4.2 Encoder-decoder mol-LLMS", "content": "Before we focus on transformer use, some description is provided on the evolution from RNN and Gated Recurrent Unit (GRU) approaches in concert with the move from template-based to semi-template-based to template-free models. Nam and Kim 281 pioneered forward synthesis prediction using a GRU-based translation model, while Liu et al. 282 reported retro-synthesis prediction with a Long Short-Term Memory (LSTM) based seq2seq model incorporating an attention mechanism, achieving 37.4% accuracy on the USPTO-50K dataset. 283 The reported accuracies of these early models highlighted the challenges of synthesis prediction, particularly retrosynthesis. Schneider et al. 283 further advanced retrosynthesis by assigning reaction roles to reagents and reactants based on the product.\nEvolving from RNNs and GRUs, the field progressed with the introduction of template-based models. Segler and Waller 284 identified that computational \u201crule-based systems\u201d often failed because they ignored the molecular context, resulting in \"reactivity conflicts.\" They prioritized suitable transformation rules describing how atoms and bonds change during reactions, applied in reverse for retrosynthesis. They trained a model on 3.5 million reactions, achieving 95% top-10 accuracy in retrosynthesis and 97% for reaction prediction on a validation set of nearly 1 million reactions from the Reaxys database (1771-2015). Though not transformer-based, their work paved the way for LLMs in synthesis applications. However, template-based models rely on explicit reaction templates from known reactions, which limits their ability to predict novel reactions and they require manual updates for learning new data.\nSemi-template-based models offered a balance between rigid template-based methods and flexible template-free approaches. They used interpolation or extrapolation within template-defined spaces to predict a wider range of reactions and to adjust based on new data. In 2021, Somnath et al. 285 introduced a graph-based approach recognizing that precursor molecule topology is largely unchanged during reactions. Their model broke the product molecule into \u201csynthons\u201d and added relevant leaving groups, making results more interpretable.286 Training on the USPTO-50k dataset, 283 they achieved a top-1 accuracy of 53.7%, outperforming previous methods.\nIt is, however, the template-free approaches that align well with transformer-based learning approaches because they learn retrosynthetic rules from raw training data. This provides significant flexibility and generalizability across various types of chemistry. Template-free models are not constrained by template libraries and so can uncover novel synthetic routes that are undocumented or not obvious from existing reaction templates. To pave the way for transformer use in synthesis, Cadeddu et al. 287 drew an analogy between fragments in a compound and words in a sentence due to their similar rank distributions. Schwaller et al. 288 further advanced this with an LSTM network augmented by an attention-mechanism-based encoder-decoder architecture, using the USPTO dataset. 283 They introduced a commonly used \"regular expression\u201d (or \u201cregex\u201d) for tokenizing molecules, framing synthesis (or retrosynthesis) predictions as translation problems with a data-driven, template-free sequence-to-sequence model. They tracked which starting materials were actual reactants, distinguishing them from other reagents like solvents or catalysts, and used the regex to uniquely tokenize recurring reagents, as their atoms were not mapped to products in the core reaction.\nIn 2019, Schwaller et al. 222 first applied a transformer for synthesis prediction, framing the task as translating reactants and reagents into the final product. Their model inferred correlations between chemical motifs in reactants, reagents, and products in the dataset (USPTO-MIT, 289 USPTO-LEF, 290 USPTO-STEREO 288). It required no handcrafted rules and accurately predicted subtle chemical transformations, outperforming all prior algorithms on a common benchmark dataset. The model handled inputs without a reactant-reagent split, following their previous work, 288 and accounted for stereochemistry, making it valuable for universal application. Then, in 2020, for automated retrosynthesis, Schwaller et al. 291 developed an advanced Molecular Transformer model with a hyper-graph exploration strategy. The model set a standard for predicting reactants and other entities, evaluated with four new metrics: coverage, class diversity, round-trip accuracy, and Jensen-Shannon divergence. Constructed dynamically, the hypergraph allowed for efficient expansion based on Bayesian-like probability scores, showing high performance despite training data limitations. Notably, accuracy was improved after the re-synthesis of the target product from the newly generated molecular precursors was included in the model. This round-trip accuracy concept was also used by Chen and Jung 292 and Westerlund et al. 293 Also in 2020, Zheng et al. 294 developed a \u201ctemplate-free self-corrected retrosynthesis predictor\u201d (SCROP) using transformer networks and a neural network-based syntax corrector, achieving 59.0% accuracy on a benchmark dataset. 283,295 This approach outperformed other deep learning methods by over 2% and template-based methods by over 6%.\nWe now highlight advancements in synthesis prediction using the BART Encoder-Decoder architecture, including Chemformer by Irwin et al. 139. This paper emphasized the computational expense of training transformers on SMILES and the importance of pretraining for efficiency. It showed that models pretrained on task-specific datasets or using only the encoder stack were limited for sequence-to-sequence tasks. After transfer learning, Chemformer achieved state-of-the-art results in both sequence-to-sequence synthesis tasks and discriminative tasks, such as optimizing molecular structures for specific properties. They studied the effects of small changes on molecular properties using pairs of molecules from the ChEMBL database 133 with a single structural modification. Chemformer's performance\""}, {"title": "3.5 Multi-Modal LLMS", "content": "Multitask or multimodal methods applied to chemistry are now described. In 2021, Edwards et al. 299 proposed Text2Mol, which retrieved molecules using natural language descriptions as queries. This task required integrating both molecular and natural language modalities, making it a cross-lingual retrieval problem due to the distinct grammar of molecular representations. The researchers built a paired dataset of molecules with corresponding text descriptions, and developed an aligned semantic embedding space for retrieval. This was enhanced with a cross-modal attention-based model for explainability and reranking. One stated aim was to improve retrieval metrics, which would further advance the ability for machines to learn from chemical literature. In their follow-up work in 2022, Edwards et al. 159 continued using SMILES string representations along with text descriptions. The two key applications were first, to generate captions for molecules from SMILES strings and second, to output molecules based on textual descriptions of desired properties. With this in mind, several important challenges remain. First, molecules can have many possible descriptions based on their diverse properties. For example, aspirin can be described by its pain-relieving effects, its use in preventing heart attacks, its chemical structure (an ester and a carboxylic acid connected to a benzene ring in ortho geometry), or its degradation into salicylic acid and ethanoic acid in moist conditions. 300 Describing such a molecule therefore requires expertise in different chemistry domains, depending on its functions and properties. In contrast, image captioning for common objects like cats and dogs needs much less domain expertise. Thus, obtaining a large, paired dataset of chemical representations and textual descriptions is challenging. Another issue is that standard evaluation metrics, like BLEU, are inadequate for molecule-language tasks. To address this, Edwards et al. 159 developed MolT5, which pretrained a model on a large corpus of unlabeled natural language text and molecule strings using a denoising objective. They fine-tuned the model on gold standard annotations and improved the metric based on their prior Text2Mol work. 299 MolT5 effectively generated both molecule and caption outputs.\nIn other work, Seidl et al. 301 developed CLAMP. Trained on large biochemical datasets, CLAMP adapts to new tasks using separate modules for chemical and language inputs, and predicts biochemical activity for drug discovery. Xu et al. 302 presented BioTranslator, a method for translating user-written text descriptions into non-text biological"}, {"title": "3.6 Textual scientific LLMs", "content": "LLMs are large deep neural network models. Therefore, their superior performance against other smaller models in classical machine learning tasks is somewhat expected. Addressing tasks that seemed impossible to tackle in the past better illustrates LLM's outstanding capabilities. That is, despite accurately predicting properties given well-structured information such as molecular features and descriptors, LLMs can access less structured data and extract information from simple natural language. For instance, instead of developing a restricted representation, Huang et al. 166 used clinical annotations from the MIMIC-III313 dataset to predict patient readmission. ClinicalBERT implements a pretrained BERT architecture using a combination of masked language modeling and next-sentence prediction training schemes followed by supervised fine-tuning on readmission prediction. Or, instead of pretraining, Zhao et al. 150 fine-tuned LLaMA on epilepsy data using an instruction-following approach to create EpilepsyLLM. Similarly, SciBERT162 and ScholarBERT 165 adapted BERT to scientific literature. Beltagy et al. 162 created a tokenizer specific for scientific texts extracted from scientific articles from semantic scholar134 and showed that SciBERT outperforms fine-tuned BERT-base 98 in every task investigated. It shows that vocabulary quality plays an important role in the model's performance. A few years later, Hong et al. 165 pretrained BERT using RoBERTa314 optimizations to improve pretraining performance. ScholarBERT was pretrained on scientific articles from Public.Resource.Org, Inc and further fine-tuned on the tasks used for evaluation. Despite using a much larger dataset, ScholarBERT did not outperform other LLMs trained with narrower domain datasets. Even against SciBERT, another LLM trained on multi-domain articles, ScholarBERT could not perform relatively better in tasks from biomedical, computer science"}]}