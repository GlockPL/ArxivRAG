{"title": "Appformer: A Novel Framework for Mobile App Usage Prediction Leveraging Progressive Multi-Modal Data Fusion and Feature Extraction", "authors": ["Chuike Sun", "Junzhou Chen", "Yue Zhao", "Hao Han", "Ruihai Jing", "Guang Tan", "Di Wu"], "abstract": "This article presents Appformer, a novel mobile application prediction framework inspired by the efficiency of Transformer-like architectures in processing sequential data through self-attention mechanisms. Combining a Multi-Modal Data Progressive Fusion Module with a sophisticated Feature Extraction Module, Appformer leverages the synergies of multi-modal data fusion and data mining techniques while maintaining user privacy. The framework employs Points of Interest (POIs) associated with base stations, optimizing them through comprehensive comparative experiments to identify the most effective clustering method. These refined inputs are seamlessly integrated into the initial phases of cross-modal data fusion, where temporal units are encoded via word embeddings and subsequently merged in later stages. The Feature Extraction Module, employing Transformer-like architectures specialized for time series analysis, adeptly distils comprehensive features. It meticulously fine-tunes the outputs from the fusion module, facilitating the extraction of high-calibre, multi-modal features, thus guaranteeing a robust and efficient extraction process. Extensive experimental validation confirms Appformer's effectiveness, attaining state-of-the-art (SOTA) metrics in mobile app usage prediction, thereby signifying a notable progression in this field.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's digital era, mobile applications have become an integral part of people's lives, influencing a wide array of daily activities[1-8]. By analyzing interactive information, we can learn the behaviors and preferences of different users, aid in understanding user needs, and provide personalized services to improve user experience. Accurately predicting users' app usage has become a prominent area of research[9\u201313], forming the foundation for personalized recommendation systems and significantly impacting mobile app development and user experience enhancement.\nHowever, predicting mobile app usage behavior remains a formidable challenge, as it is influenced by a multitude of factors including time, spatial context, and individual preferences. The associated data often display attributes like brevity, high dimensionality, discreteness, and multi-modality. Traditional prediction methodologies frequently struggle with issues such as data sparsity, abstract feature extraction, and maintaining accuracy in model predictions. Recently, deep learning techniques, particularly those employing attention mechanisms, have emerged as a significant advancement in the realm of mobile application prediction, offering effective solutions to these intricate challenges.\nIn the realm of mobile app prediction, our research endeavors to address three pivotal challenges:\n1) Accurately representing core data: This involves user IDs, spatiotemporal information (time and location of app usage), and app usage history sequences. We meticulously encode these components to capture their nuances.\n2) Efficiently integrating multimodal data: We employ advanced multimodal data fusion techniques to seamlessly integrate the encoded data, fostering a synergistic synthesis of information.\n3) Enhancing feature extraction: We aim to empower prediction models to distill insightful and discriminative features from the unified dataset, crucial for accurate and robust predictions.\nTo address these challenges, our research presents innovative solutions aimed at enhancing the prediction efficiency for mobile application usage. The key contributions are summarized as follows:\n\u2022 Appformer Framework: We introduce the Appformer framework, leveraging attention mechanisms and Transformer architectures to significantly enhance data fusion and feature extraction. It comprises two synergistic components: a Multi-Modal Data Progressive Fusion Module and a Feature Extraction Module. The former adeptly combines encoded data from diverse sources using cross-modal fusion technology, setting a strong base for data synthesis. The latter, with its Encoder-Decoder architecture, efficiently extracts temporal features from time-series data. Adjustments to the Encoder and Decoder inputs have been made to optimize multi-modal feature extraction. Together, these modules boost the framework's ability to deliver accurate and efficient application predictions.\n\u2022 Multi-modal Data Progressive Fusion: At the heart of"}, {"title": "II. RELATED WORK", "content": "The application prediction problem involves using the his- torical information of application usage to forecast the po- tential applications that users may use in the future. Through literature research, we have established a strong correlation between time information, user information, location infor- mation, and application usage. Based on this, we have de- composed the application prediction problem into three parts: mining and expression of core data, fusion of multi-modal data, and feature extraction network. Each of these parts has been elaborated on in detail.\nCorrelation analysis between core data and application prediction: Wang et al. [14] found not only a strong correlation between the locations accessed by users on their mobile devices and the applications they use but also discovered that users tend to use different apps during various time periods. Nadai et al.[15] significant disparities were observed in how different users engage with apps. Moreover, user app preferences were found to evolve over time. Xia et al.[16] analyzed a large-scale real-world dataset of app usage and found that smartphone app usage exhibits spatio-temporal correlation and personalization. Garrido et al.[17] discovered that the physical locations within a user's city can influence their app usage patterns significantly. Tian et al.[18] found that users with different characteristics have different app preferences and that users use different apps during various time periods. These studies demonstrate a strong correlation between spatio-temporal information, user information, and application usage. We need to make rational use of these data for app prediction.\nMining and expression of core data: App prediction involves data encoding, similar to word embedding in Natural Language Processing (NLP). Word embedding is a significant research area, converting words into high-dimensional vectors to enhance the understanding of natural language. One-hot encoding is a simple and easy-to-implement method, but it fails to capture semantic relationships[19]. The Word2Vec model proposed by Mikolov et al.[20] was a major breakthrough, introducing efficient training algorithms. The GloVe model proposed by Pennington et al.[21] further improved word embeddings by combining global and local information. FastText, proposed by Joulin et al.[22], utilizes deep learning methods and can handle both word-level and subword-level information. We can construct data processing methods for relevant data in app prediction based on the word embedding methods in NLP.\nFusion of multi-modal data: In the field of application prediction, the integration of encoded multi-modal data has become essential. Multi-modal data fusion is a critical process that helps capture the correspondence between different types of contextual information data, such as user personalized features, space, time, and historical app sequences, reducing information redundancy and enhancing generalization. It can significantly improve the accuracy and robustness of application prediction models.\nIn recent years, there has been a surge in research and applications related to multi-modal data fusion. Performing multi-modal data fusion requires building models that are adept at handling and associating information from multiple sources [23][24]. Traditional methods for multi-modal data fu- sion include co-training algorithms[25\u201327], co-regularization algorithms[28][29], marginal consistency algorithms[30][31], and multiple kernel learning (MKL)[32].\nFurthermore, more complex techniques have been used for multi-modal data fusion. These include variants of deep Boltz- mann machines for modeling the joint distribution of different modalities[33][34], extensions of classical autoencoders for discovering correlations between hidden representations of two modalities[35][36], nonlinear extensions of canonical corre- lation analysis (CCA) using deep neural networks[37], and architectures based on convolutional neural networks (CNN) that excel at combining information from multiple sources[38- 40].\nIn addition, groundbreaking approaches have emerged, further enriching the field of application prediction. For example, the Multi-modal Transformer (MulT) technique introduced by Tsai et al.[41], the Multi-modal Temporal Graph Attention Network (MTGAT) proposed by Yang et al.[42], spatial fusion encoding developed by Wang et al. [43], the Multi-modal Attention based Feature Fusion (MAT) by Islam et al.[44], and the Modality-Invariant Cross-modal Attention (MICA) proposed by Liang et al.[45].\nFeature extraction network: Feature extraction network is a type of network structure widely used in deep learning techniques in recent years. Here are some examples of related feature extraction networks:\nLee et al.[46] employed a stacked Long-Short Term Memory (LSTM) architecture, a sequence-based deep learning"}, {"title": "III. PROBLEM DESCRIPTION AND DATASET", "content": "As illustrated in Figure 1, our dataset consists of various feature data types. The application sequence is highlighted in a red box (App_seq), user IDs in a purple box (User IDs), the timestamp sequence of application starts in a green box (Time_seq), the base station ID interacting with the mobile phone during application usage in a yellow box (Base station IDs), and the Points of Interest (POIs) data corresponding to the base station's coverage sector in a grey sector box (POIs). The POI data, indicating the presence of different facilities near a base station, plays a vital role in defining the functionality and appeal of specific locations [53].\nOur goal is to develop a predictive model f(x) capable of precisely predicting the next application a user is likely to use. This model is designed to rank the likelihood of potential next apps in a descending order, starting with the most probable choice as Top1, followed by the next most likely options as Top2, Top3, and so forth. The mathematical representation is illustrated by the following equation:\n\\(A(n + 1) = f (X(n \u2212 m + 1 : n))\\) (1)\nwhere X(n-m+1 : n) represents a multi-dimensional feature sequence, incorporating essential historical data required for prediction. This encompasses sequences of app IDs, user IDs, timestamps, base station IDs, and POI vectors, seamlessly integrated into a coherent structure for the predictive model. The configuration of this sequence is defined as follows:\n\\(X(n \u2212 m + 1 : n) =\\) (2)\n\\(\\{(A(i), U(i), T(i), B(i), P(i)) | i = n \u2212 m + 1, ..., n\\}\\)\nThis structured approach allows our model to effectively leverage the rich, interconnected data, enhancing its predictive accuracy for upcoming application usage.\nOur study employs a real-world application usage dataset collected from Shanghai, China, during a week-long period from April 20 to April 26, 2016 [53]. Each dataset entry contains an anonymous user ID, a timestamp, a base station ID, and the ID of the app being used, organized chronologically by timestamp. Table I provides a comprehensive view of the dataset format, featuring five sequential records from a single user for illustrative purposes.\nMoreover, every base station in the dataset is designated by a unique ID and corresponds to a 17-dimensional Points of"}, {"title": "IV. METHOD", "content": "In this section, we begin by exploring the methodologies employed for data selection and preprocessing, focusing on the implementation of word embeddings, the application of clustering techniques to Points of Interest (POIs) data, and the encoding of temporal data. Following this preliminary discussion, we introduce the Appformer framework, offering a comprehensive overview and then proceeding to elaborate on its two primary components: the Multi-Modal Data Progressive Fusion Module and the Feature Extraction Module.\nTo respect and protect user privacy, we have chosen POI information associated with base stations as the input for\n\\(E = nn.Embedding(n, d)\\) (3)\n\\(Vi = E(i)\\) (4)\n\\(d(xi, zj) = \\sum^{m}_{q=1}\u03b4(Xiq, zjq)\\) (5)\n\\(\\{(Xiq, Zjq) =\n0, if Xiq = Zjq\n1, otherwise\\}\\)\n\\(Zjq = mode(\\xiq|xi is assigned to zj\\)\\) (7)\n\\(a = WE(App\\_seq)\\) (8)\n\\(u = WE(User IDs)\\) (9)\n\\(l = Linear(K-Modes(POIs))\\) (10)\n\\(t =Linear(Concat(WE(Month), WE(Day),\nWE(Weekday), WE(Hour), WE(Minute))\\) (11)"}, {"title": "V. EXPERIMENTS", "content": "We use four distinct metrics to evaluate the effectiveness of different methods: Hit@k (Hit rate at k), MRR@k (Mean Reciprocal Rank at k), NDCG@k (Normalized Discounted Cumulative Gain at k) and F1 score with a macro setting."}, {"title": "VI. RESULT", "content": "In this section, we first conducted experiments comparing with baselines. Then, we performed ablation experiments on the different components of the model. Next, we compared the time encoding and fusion methods with traditional approaches. Subsequently, we replaced the components within the feature fusion module in Appformer and conducted comparative experiments. Following that, we replaced the Feature Fusion Module in Appformer and conducted a comparison experiment on the feature extraction module. Finally, we conducted clustering experiments on the POI data, including experiments to determine the number of clustering centers and selecting the best clustering method based on the number of centers.\nThe comparison results of our method with PAULCI[51] are shown in Table V, with the experimental results of other methods in the table also sourced from PAULCI. The comparison results with DUGN[49] are shown in Table VI, and the experimental results of other methods in that table are also derived from DUGN. Across all metrics, our method achieves SOTA performance, illustrating its effectiveness in the field of mobile app prediction.\nThe significance of user information, spatio-temporal information, and historical app sequences in mobile app prediction is self-evident. Previous research has gradually incorporated these types of data, continuously enhancing their representations. Coupled with advancements in feature extraction techniques, this has led to a steady improvement in performance metrics.\nFor instance, the PAULCI method employs graph embeddings to depict spatial context and multi-modal embeddings to represent temporal context, user identifiers, and previously utilized applications. It also uses a deep learning framework, which includes GRU, attention layers, and softmax layers, for feature extraction and prediction, yielding good results. The DUGN method utilizes a dynamic graph structure, allowing it to capture the intricate correlations between various applications and track the evolution of user interests. By"}, {"title": "VII. CONCLUSION", "content": "This study showcases Appformer as a novel approach for predicting mobile app usage, demonstrating its potential through innovative multi-modal data fusion, feature extraction strategies, and the use of Transformer-like architecture. Specifically, we enhanced the preprocessing of location and time data, laying a solid foundation for data fusion and feature extraction. Through rigorous experimental validation, Appformer outperforms existing methods on several predictive performance metrics, such as Hit@1, MRR, and F1 score. This highlights Appformer's robustness and efficiency in predicting mobile app usage, showcasing its potential to improve personalized services and enhance user experience.\nIn our in-depth analysis of the Appformer approach, we identified its limitations as involving challenges in the data fusion process, constraints in feature extraction, and difficulties in model updates and maintenance. Specifically, effectively integrating multimodal data, addressing inconsistencies and conflicts within the data, and avoiding the introduction of bias or loss of important information during data fusion are key challenges. Additionally, the model may overly rely on predefined features and extraction methods, limiting its ability to explore intrinsic patterns in the data, especially when dealing with complex or unstructured data. Moreover, to adapt to the constantly evolving patterns of app usage and the emergence of new applications, the model requires regular updates. However, frequent updates may demand substantial computational resources and cause short-term fluctuations in model performance. Maintaining the model's continuous learning and adaptation to new data patterns while avoiding the disruption of existing knowledge poses a significant challenge. These multifaceted challenges not only deepen our understanding of the limitations of the Appformer method but also point out directions for future research, namely improving the applicability, performance, and adaptability of the model in the dynamic field of app usage prediction. This ensures that it can effectively adapt to the ever-changing digital ecosystem and meet the diverse needs of users.\nWe would like to release our source code."}]}