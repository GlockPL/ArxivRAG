{"title": "A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks", "authors": ["Saptarshi Mandal", "Xiaojun Lin", "R. Srikant"], "abstract": "Knowledge distillation, where a small student model learns from a pre-trained large teacher model, has achieved substantial empirical success since the seminal work of (Hinton et al., 2015). Despite prior theoretical studies exploring the benefits of knowledge distillation, an important question remains unanswered: why does soft-label training from the teacher require significantly fewer neurons than directly training a small neural network with hard labels? To address this, we first present motivating experimental results using simple neural network models on a binary classification problem. These results demonstrate that soft-label training consistently outperforms hard-label training in accuracy, with the performance gap becoming more pronounced as the dataset becomes increasingly difficult to classify. We then substantiate these observations with a theoretical contribution based on two-layer neural network models. Specifically, we show that soft-label training using gradient descent requires only $O(\\frac{1}{\\gamma^2} \\ln(\\frac{1}{\\epsilon}))$ neurons to achieve a classification loss averaged over epochs smaller than some \u2208 > 0, where y is the separation margin of the limiting kernel. In contrast, hard-label training requires $O(\\frac{1}{\\gamma^4})$ neurons, as derived from an adapted version of the gradient descent analysis in (Ji and Telgarsky, 2020). This implies that when y < \u20ac, i.e., when the dataset is challenging to classify, the neuron requirement for soft-label training can be significantly lower than that for hard-label training. Finally, we present experimental results on deep neural networks, further validating these theoretical findings.", "sections": [{"title": "1. Introduction", "content": "Knowledge distillation is a popular technique for training a smaller 'student' machine learning model by transferring knowledge from a large pre-trained 'teacher' model. A lightweight machine learning model is useful in many resource-constrained application scenarios, such as cyber-physical systems, mobile devices, edge computing, AR/VR, etc. due to several reasons such as limitations in memory, inference speed, and training data availability. Knowledge distillation has been proven to be a powerful solution for these challenges through model compression (Hinton et al., 2015; Buciluundefined et al., 2006; Gou et al., 2021b). Among the various methods of knowledge distillation (Gou et al., 2021a), one prevalent approach involves using the teacher model's output logits as soft targets for training the student model. Specifically, smaller models trained with well-designed soft labels demonstrate competitive performance compared to more complex models trained with original (hard) labels. This method and its variants have been shown to be effective in many settings such"}, {"title": "1.1. Related Work", "content": "There are many papers that aim to explain different aspects of knowledge distillation. In this section, we focus on reviewing the works that contributes to the theoretical understanding of knowledge distillation.\nA few prior works focus on linear models using the cross-entropy loss for the distillation objective. (Phuong and Lampert, 2021) assumes that both the student and teacher models are of the form $f(x) = w^T x$, where w is the weight vector of the student network, x is the input vector, and"}, {"title": "2. Preliminary Experimental Observations", "content": "In this section, we present preliminary experimental observations on the training of a two-layer neural network student model using both ground truth hard labels and soft labels generated by a larger teacher network. For the experiments summarized in Table 2, we perform binary classification on the MNIST dataset, where digits greater than 4 are labeled as class 1 and others as class 0.\nTwo configurations of the MNIST data are considered:\n1. Full dataset: Includes images of all digits (0-9).\n2. Reduced dataset: Excludes images corresponding to the digits {1, 7, 4, 9}.\nThe first configuration is more challenging to classify because of the difficulty in distinguishing between visually similar digits such as 1 and 7 or 4 and 9. Our observations indicate that the performance of the student model trained with soft labels remains relatively stable when transitioning from the easier dataset to the harder one. In contrast, the performance of the student model trained"}, {"title": "3. Theoretical Results", "content": "In the previous section, we presented a few experimental observations on student-teacher training using a simple model of two-layer neural network. In this section, we provide theoretical insights into these observations, summarized in Theorem 2 and the corresponding Corollary 4. We begin by introducing the student-teacher model used for training, outlining a few assumptions about the training data, and specifying the choice of distillation loss for the student to be trained using soft labels. This is followed by a description of the Projected Gradient Descent (PGD) method used for training and a theoretical analysis of the training dynamics."}, {"title": "3.1. Model and Assumptions", "content": "We consider the following fully-connected two-layer neural network,with m neurons in its hidden layer and with ReLU activation, as the student model:\n$f(x; W, a) = \\frac{1}{\\sqrt{m}} \\sum_{j=1}^{m} a_j \\sigma(W_j^T x) = \\frac{1}{\\sqrt{m}} \\sum_{j=1}^{m} a_j 1(\\langle W_j, x \\rangle \\geq 0) W_j x,$\nwhere \u03c3(z) = max(0, z) is the ReLU activation function, $a_j \\in \\mathbb{R}$ and $W_j \\in \\mathbb{R}^d$ for $j \\in [1, m]$ are respectively the final layer weight and the hidden layer weight vector corresponding to each neuron j in the hidden layer. Let the vector a and the matrix W denote the collection of $a_j$ and $W_j$ as their jth element and jth row, respectively. Let the neural network output for an input sample $x_i$ be further denoted as $f_i(W)$ for any i \u2208 [n].\nWe initialized the neural network using the symmetric random initialization which was proposed in (Bai and Lee, 2020) and later used in (Cayci et al., 2023). The initial parameters are given as follows: $a_j = \\frac{i.i.d.}{=} Unif\\{-1,1\\}$ and $W(0)_j = W(0)_{j+m} \\frac{i.i.d.}{=} N(0, I_d)$ independent"}, {"title": "3.2. Neuron Requirement for Soft-label Training", "content": "We are now ready to present the first result of this paper. The following Theorem provides a characterization of the number of neurons required for soft-label training to achieve a small empirical risk, $R^{KL}(\\mathcal{W}(t))$, averaged over iterations t < T.\nTheorem 2 Let \u03b2\u2208 (0, 1), \u03b4 \u2208 (0,1) be fixed real numbers. If the number of neurons m satisfies\n$m \\geq \\frac{C_1}{\\beta^2} (\\sqrt{\\frac{2}{\\pi e}} + 3 \\sqrt{e} \\ln(\\frac{2n}{\\delta}))^2,$\nand the PGD algorithm is run with a projection radius B = 1 for T iterations such that $T \\geq \\frac{9}{\u03b7^2}$, using a constant step size \u03b7 satisfying \u03b7 \u2264 $\\frac{\u03b2}{3H}$. Then, the following bound on the averaged empirical risk holds:\n$\\frac{1}{T} \\sum_{t<T} R^{KL}(\\mathcal{W}(t)) \\leq \u03b2,$\nwith probability at least 1 - 3\u03b4 over the random initialization. Here, $C_1 = \\frac{96}{(1+e^2)}$ is an absolute constant."}, {"title": "3.2.1. COMPARISON WITH HARD-LABEL TRAINING", "content": "Now we are ready to compare the neuron requirement based on Corollary 4 with that of the requirement for hard label training as established in (Ji and Telgarsky, 2020). The empirical risk for hard-label training on the dataset $\\mathcal{D} := \\{x_i, y_i\\}_{i=1}^n$ is defined as:\n$R^h(W) := \\frac{1}{n} \\sum_{i=1}^n \\ln(1 + \\exp(-y_i f_i(W)))$."}, {"title": "3.3. Proof Sketch of Theorem 2", "content": "While the detailed proofs of all the results presented in the paper is provided in the Appendix, we present a high-level proof sketch for Theorems 2 here. Before presenting the proof sketch, we first introduce some additional notations and quantities."}, {"title": "4. More Experimental Results", "content": "In this section, we validate our results using deep neural networks, with VGG 8+3 as the teacher and VGG 2+3 as the student; see (Simonyan and Zisserman, 2014) for a detailed description of the VGG architecture. The dataset chosen for our experiments is the CIFAR-10 cat/dog dataset.\nSince our theory suggests that harder-to-classify datasets benefit more from soft-label training, we added Gaussian noise to the CIFAR-10 cat/dog dataset to make it more challenging to classify. We compare the performance of soft-label vs. hard-label training across different datasets: the original CIFAR-10 dataset and noise-added CIFAR-10 datasets. The results are summarized in Figure 4. Consistent with our theoretical predictions, the experiments demonstrate that harder-to-classify datasets benefit significantly more from distillation.\nAll experimental points are averaged over 10 independent runs. For each run, we employed early stopping with a patience of 20 epochs and a maximum of 100 iterations. The dataset was split into training, validation, and test sets with proportions of 80%, 10%, and 10%, respectively. We trained the models using gradient descent with the Adam optimizer and applied L2 regularization."}, {"title": "5. Conclusions", "content": "In this paper, we provide theoretical results which show that soft-label training leads to fewer number of neurons than hard-label training for the same training accuracy. Our proofs provide some intuition for this phenomenon, as stated at the end of the longer version of the paper, which we summarize here. The parameters of a neural network have a dual role: one is to identify good features and the other is to assign weights to these features. With good initialization, one can start the training with good features. In contrast to hard-label training, soft-label training ensures that the network parameters do not deviate too much from initialization, thus approximately maintaining good features throughout the training process, but they deviate just enough to assign the right weights to various features."}, {"title": "Appendix A. Proof of Theorem 2", "content": "In this section, we provide the detailed proof of Theorem 2. A proof sketch is given as follows:\n1. Initial Feature Map and Separability: Recall that the feature map given by the initial weights for a data input $x_i$ is denoted by $\\phi_i$. Lemma 6 shows that this feature map, based on the initial weights, separates the dataset with a margin of order $O(\\gamma)$, provided the number of neurons is of the order O(\\frac{1}{\\gamma^2}). Lemma 6 further implies the existence of a weight matrix U satisfying $||\\U_j \u2013 \\mathcal{W}\\_j(0)||_2 \\leq \\frac{1}{\\sqrt{m}}$ such that $|\\langle\\phi_i, U\\rangle \u2013 z_i|$ is small for all i \u2208 [n], with high probability, when m is sufficiently large. Notice that Lemma 6 is a extended version of Lemma 6 stated in the main part of the paper.\nThis observation suggests that a linear function (linear in the weight W) of the form $\\langle\\phi, W\\rangle$ can approximate the soft label $z_i$ for each i with high probability. This intuition is crucial for the subsequent steps of the proof.\n2. Convergence of Soft Label Surrogate Loss: Next, we provide a convergence result for the soft-label surrogate loss in Lemma 7 under PGD updates over iterations. To describe the statement in Lemma 7, we first introduce some additional notations and quantities.\nDefine $f_i^t(W)$ for each data sample $x_i$ as:\n$f_i^t(W) := \\frac{1}{\\sqrt{m}} \\sum_{j=1}^{m} a_j 1(\\langle W_j(t), x_i \\rangle \\geq 0) W_j x_i,$\nwhere $W_j(t)$ is the weight of the jth neuron at the tth iteration of PGD. Similar to the definition of $\\phi_i$, define the feature map at iteration t, $\\phi\\_i^t$, based on the weight $W(t)$. Specifically, the feature corresponding to the jth neuron is given by $\\phi\\_i(\\mathcal{W}\\_j(t))$. Hence, for all t < T and i \u2208 [n], we can write:\n$f_i^t(W) = \\frac{1}{\\sqrt{m}} \\langle \\phi_i, \\phi_i^t(W)\\rangle.$\nUsing these definitions, we define the following expression for each t < T:\n$\\mathcal{R}^{t,KL}(W) := \\frac{1}{n} \\sum_{i=1}^{n} \\Psi^{KL}(p_i, f_i^t(W))$.\nWe are now ready to state Lemma 7. This lemma shows that if the number of iterations T is sufficiently large and the step size \u03b7 is small, the soft-label risk averaged over all iterations converges to the quantity \\frac{2 \\eta}{\\beta^2} + \\sum_{t<T} \\mathcal{R}^{t,KL}(W) for any W in the feasible set $S_B$.\nThe remainder of the proof is devoted to showing that $\\mathcal{R}^{t,KL}(W)$ is small for all t < T with high probability, for an appropriately chosen W, provided the number of neurons m is sufficiently large. This ensures that the soft-label risk averaged over all iterations converges to a small value if PGD is run for a sufficient number of iterations under suitable conditions on m."}, {"title": "Appendix B. Proof of Corollary 4", "content": "The Corollary follows from the Theorem 2 and the Lemma 3. Let us first give a proof of Lemma 3."}, {"title": "B.1. Proof of Lemma 3", "content": "First, we use the Pinsker's inequality to upper-bound the classification loss $\\mathcal{R}(W)$ in terms of the surrogate loss $\\mathcal{R}^{KL}(W)$ in Lemma 9 for any soft labels $p_i'$ satisfying $y_i \\mu^{-1}(p_i) > 0, \\forall i \\in [1,n]$.\nLemma 9 If we assume 0 < $p_i'$ < 1, and $y_i \\mu^{-1}(p_i) > 0, \\forall i \\in [1, n]\u00b9, we can relate the classification loss $\\mathcal{R}(W)$ to the surrogate loss $\\mathcal{R}^{KL}(W)$ as:\n$\\mathcal{R}(W(t)) \\leq \\frac{1}{2 \\min_i |p_i'- \\frac{1}{2}|^2} \\mathcal{R}^{kl}(W(t))$\nProof We use the Pinsker's inequality :\n$KL(p_i || \\mu(f_i(W(t)))) \\geq 2|p_i - \\mu(f_i(W(t)))|^2$\nThe followings are true when $y_i \\mu^{-1}(p_i) > 0, \\forall i$:"}, {"title": "B.2. Final Steps for Proving Corollary 4", "content": "From Lemma 3, to achieve the target classification loss $\\mathcal{R}(W(t)$ averaged over t < T to be smaller than some \u2208 > 0, the following condition is sufficient :\n$\\frac{1}{T} \\sum_{t<T} \\mathcal{R}^{KL}(W(t)) \\leq \\frac{32 \\epsilon}{\\gamma}$\nNow substituting \u03b2 with $\\frac{32 \\epsilon}{\\gamma}$ in the proof of the Theorem 2, we arrive at the result by Corollary 4."}, {"title": "Appendix C. Sufficient Condition for Hard Label Training with Projected Gradient Descent: Proof of Proposition 5", "content": "We adapted the analysis of the paper (Ji and Telgarsky", "training": "n$\\mathcal{R"}, {"research": "It emphasizes the ability of soft-label training to achieve comparable performance to hard-label training with significantly fewer neurons, a factor not thoroughly examined in previous literature.\n-It theoretically establishes the neuron requirements for both hard-label and soft-label training using gradient descent, demonstrating the superiority of soft-label training, especially in challenging classification scenarios."}]}