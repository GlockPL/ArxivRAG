{"title": "Multi-Resolution Histopathology Patch Graphs for Ovarian Cancer Subtyping", "authors": ["Jack Breen", "Katie Allen", "Kieran Zucker", "Nicolas M. Orsi", "Nishant Ravikumar"], "abstract": "Computer vision models are increasingly capable of classifying ovarian epithelial cancer subtypes, but they differ from pathologists by processing small tissue patches at a single resolution. Multi-resolution graph models leverage the spatial relationships of patches at multiple magnifications, learning the context for each patch. In this study, we conduct the most thorough validation of a graph model for ovarian cancer subtyping to date. Seven models were tuned and trained using five-fold cross-validation on a set of 1864 whole slide images (WSIs) from 434 patients treated at Leeds Teaching Hospitals NHS Trust. The cross-validation models were ensembled and evaluated using a balanced hold-out test set of 100 WSIs from 30 patients, and an external validation set of 80 WSIs from 80 patients in the Transcanadian Study. The best-performing model, a graph model using 10x+20x magnification data, gave balanced accuracies of 73%, 88%, and 99% in cross-validation, hold-out testing, and external validation, respectively. However, this only exceeded the performance of attention-based multiple instance learning in external validation, with a 93% balanced accuracy. Graph models benefited greatly from using the UNI foundation model rather than an ImageNet-pretrained ResNet50 for feature extraction, with this having a much greater effect on performance than changing the subsequent classification approach. The accuracy of the combined foundation model and multi-resolution graph network offers a step towards the clinical applicability of these models, with a new highest-reported performance for this task, though further validations are still required to ensure the robustness and usability of the models.", "sections": [{"title": "1 Introduction", "content": "Ovarian cancer is the eighth most common cancer in women worldwide [4] and is characterised by heterogeneous histological subtypes. The five most common subtypes, which account for 90% of all ovarian cancers, are high-grade serous (HGSC), low-grade serous (LGSC), clear cell (CCC), mucinous (MC), and endometrioid carcinomas (EC). These subtypes are distinct in their genetics, prognoses, and treatment options [17], making their classification an essential component of ovarian cancer diagnosis. However, determining the subtype from standard histopathology samples can be a difficult task with a high level of inter-observer discordance [15]. Thus, it is common for pathologists to request ancillary tests and second opinions to ensure an accurate diagnosis, slowing the diagnostic pathway and increasing costs.\nArtificial intelligence (AI) models have been proposed as potential assistive tools for pathological diagnosis. While some tools are starting to receive regulatory approval [20] and be clinically validated [22], this is not the case for ovarian cancer subtyping models. Previous research in this area has involved model prototyping with relatively small homogeneous datasets [5], though some recent studies have included larger and more diverse datasets [1,6,11]. The highest reported performances for five-class classification were 81% and 93% balanced accuracies [11,6], compared to median pathologist concordance rates of 78-86% from histopathology alone, and 90% with the addition of immunohistochemical information [15].\nOvarian cancer resection whole slide images (WSIs) contain billions of pixels and are stored in files that are typically 1-4GB in size. Traditional computer vision models cannot handle such large images, so information is typically extracted from small patches and aggregated to the WSI level in a process called 'multiple instance learning' (MIL). These models often treat patches as being functionally independent of one another, such as in attention-based multiple instance learning (ABMIL) [14], where a weighted average of patch embeddings is taken to form a WSI embedding.\nIt can instead be beneficial to leverage the spatial or semantic relationships between patches by integrating graphs [24] or transformers [27] into MIL models [2,10,12,25,26]. Only one previous study has applied such a method to ovarian cancer subtyping [21], where it was reported that a novel multi-resolution graph network gave a better balanced accuracy than previous methods, including ABMIL, TransMIL [25], and single-magnification graph models. However, this study used only a single set of model hyperparameters for all models, and a single dataset for evaluation, making it unclear whether the models were properly tuned to the given task and data, and unclear how well the models would generalise.\nIn this paper, we present the most thorough evaluation of a graph model for ovarian cancer subtyping to date, including hyperparameter tuning and both hold-out and external validations. To the best of our knowledge, it is also the first graph-based MIL model to be conducted using features from the vision transformer (ViT)-based histopathology foundation model, UNI [9]."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Data", "content": "The training dataset comprised 1864 ovarian carcinoma resection WSIs from 434 patients at Leeds Teaching Hospitals NHS Trust. These were retrospectively collected by a pathologist, who confirmed the original diagnoses made by a gynaecological subspecialty expert. A class-balanced independent hold-out test dataset was collected following the same procedure as in the training set, comprising 100 WSIs from 30 patients. All WSIs were generated from slides of haematoxylin and eosin (H&E)-stained formalin-fixed and paraffin-embedded (FFPE) adnexal tissue which was digitised at 40x magnification on an Aperio AT2 scanner. The training set slides contained both primary surgery samples and interval debulking surgery samples, while the hold-out test set contained primary samples alone. An external dataset of 80 WSIs from 80 ovarian cancer patients was sourced from the Transcanadian Study [16], with these provided at 20x magnification."}, {"title": "2.2 Modelling", "content": "A multi-stage WSI classification pipeline was employed (shown in Figure 1). First, the tissue region was segmented from plain background and tissue patches were selected. Patch-level features were then extracted using a pre-trained model. Graphs were constructed based on the spatial arrangement of the patches, with message-passing layers used to share information between neighbouring patches."}, {"title": "2.3 Training and Validation Procedures", "content": "Graph model hyperparameters were tuned through an iterative grid search procedure, with up to two hyperparameters adjusted at a time and all others frozen at their previous best, starting from the optimal hyperparameters of a previous study [6] in which ABMIL was tuned for the same task and dataset as used in this study. Each hyperparameter configuration was evaluated using the average balanced cross-entropy loss across the validation sets of five-fold cross-validation. Models were trained using an Adam optimizer, with class-balanced sampling used to account for the imbalance in the training set. At least 100 unique configurations were evaluated during the tuning of each graph-based model, with the ABMIL hyperparameters instead taken from the previous study [6]. Hyperparameter tuning is described further in Supplementary Section A.\nSeven total models were evaluated to compare different feature extractors, magnifications, and classifiers. The baseline multi-resolution graph model combined 5x and 10x magnifications, which have each previously been found to give an accurate and efficient classification of ovarian cancer subtypes [7]. Comparisons to different models and magnifications were conducted using a multi-resolution graph at 10x and 20x magnifications, a single-magnification graph at 10x, and ABMIL at 10x. In another comparison, the UNI vision transformer foundation model features used in the baseline were changed to features from an ImageNet-pretrained ResNet50. Finally, to compare different multi-resolution feature spaces, the baseline approach of using separate magnification-specific features with average initialisation (concat_avg) was compared to the separate features with zero initialisation (concat_zero) and to a magnification-agnostic combined feature space (naive). Paired t-tests were used to statistically compare each model to the baseline model across the five cross-validation folds, with p-values adjusted for multiple testing [3]."}, {"title": "3 Results", "content": "All results are shown in Figure 2, with full breakdowns and p-values in Supplementary Section B. The best-performing model in cross-validation was the multi-resolution graph with a zero-initialised feature space, with a balanced accuracy of 74.2%, AUROC of 0.944 (second-best, behind the 0.945 of ABMIL), and F1 score of 0.759. In hold-out testing, ABMIL performed best, with a balanced accuracy of 88.0%, AUROC of 0.957 (second-best, behind the 0.960 of the naive graph model), and an F1 score of 0.875. In external validation, the 10x+20x magnification graph performed best by all metrics, with a balanced accuracy of 99.0%, AUROC of 1.000, and an F1 score of 0.991.\nNo single model gave the best performance for all metrics across all evaluations, though the 10x+20x magnification model was the most consistent, never more than 0.014 behind the best for any given metric. In cross-validation and hold-out testing, this model was slightly outperformed by ABMIL and the graph model with the naive feature space, but in external validation the 10x+20x graph model was best by a clear margin (3.6% balanced accuracy, 0.001 AUROC, 0.029 F1 score). The model using the ImageNet-pretrained ResNet50 encoder performed worst in every evaluation, indicating the clear benefit of the newer foundation model features. In fact, the selection of an appropriate feature encoder had a much greater effect on model performance than the subsequent MIL modelling approach, with ABMIL being competitive with the best graph models by most metrics.\nThe effects of modelling with different magnifications were not consistent across all evaluations. The 10x graph model had significantly higher balanced accuracy and F1 scores than the 5x+10x baseline in cross-validation and hold-out testing but performed slightly worse in external validation. The 10x+20x model outperformed the 5x+10x model in all evaluations, though this difference was only statistically significant for the hold-out AUROC. The 10x+20x model gave similar performance to the 10x-only model in cross-validation and hold-out testing but performed much better in external validation.\nIt was unclear which multi-resolution feature space was best overall, with each of the three approaches best in some cases. The baseline average-initialised feature space was generally the worst of the three, with the naive combined feature space best in hold-out testing, and the zero-initialised features best in cross-validation and external validation. As such, the optimal 10x+20x model may have benefitted further from initialisation using zeroes rather than averages."}, {"title": "4 Discussion", "content": "Overall, the results indicate that multi-resolution graph models can offer improvements to ovarian carcinoma subtyping. In particular, the 10x+20x magnification model achieved near-perfect performance in external validation, making this the greatest reported performance for this task to date [5]. However, multi-resolution graph models did not offer a clear benefit over ABMIL in internal validations and, considering the relatively small size of the external validation set, it is unclear how great a benefit these models offer overall.\nAs in the previous study using these datasets [6], performance was greater in hold-out testing than cross-validation, and greater still in external testing. While it is not exactly clear why this is the case, several factors may have had an impact. The hold-out and external validations used model ensembles and did not include interval debulking surgery samples that can be more diagnostically challenging. The external validation data also appeared to have given a \"best-case scenario\", with a single slide per patient and a high proportion of carcinoma within the tissue present on the high-quality WSI. Further, the different scanning magnifications may have led to a difference in image quality after downsampling, though such an effect was not visibly apparent.\nIn external validation, all models achieved AUROC scores between 0.983 and 1.000, despite the balanced accuracy and F1 scores varying from 83.7%-99.0% and 0.849-0.991, respectively. There were also several instances in internal testing where models had highly similar AUROC scores but clearly distinct scores by the other metrics. This highlights the limitation of the AUROC for imbalanced multi-class classification, given its reporting similarly high performances for all models despite obvious differences in the balanced accuracy and F1 scores, which are more representative of clinical utility.\nThe five-class balanced accuracies of 88% and 99% in hold-out and external validations may be sufficient for clinical assistance tools, but some limitations remain. These validations used data from only 30 and 80 patients, respectively, so cannot represent the vast array of variability seen in clinical diagnostic cases. The models are also currently incapable of indicating uncertainty, providing thorough explanations of classification decisions, or coping with tissue which does not contain one of the five most common subtypes of ovarian carcinoma (e.g. non-malignant tissue, carcinosarcomas and non-epithelial malignancies). The large vision transformer feature extractors and multi-resolution graph networks also carry a heavy computational burden, which is likely to lead to logistical difficulties in deploying such models in the clinical setting. None of these issues are insurmountable, and when they are overcome, these models could be invaluable as diagnostic assistive tools offering a rapid second opinion to pathologists."}, {"title": "5 Conclusion", "content": "Overall, we have shown that multi-resolution graph models can improve the accuracy of ovarian carcinoma subtyping at the whole-slide level above the previous state-of-the-art, though it was not beneficial in all validations. In an external validation of 80 WSIs, a graph model achieved a near-perfect 99% balanced-accuracy, but in internal testing this was only 88%, which was no greater than ABMIL. The best model combined data at 10x and 20x magnifications, which was better than either using lower magnifications or only using 10x magnification data. While the benefit provided by graph models may offer a clinically useful second opinion to pathologists, more extensive validations are required to understand the reasons underlying performance variability across different datasets and to improve model consistency."}, {"title": "A Supplementary Hyperparameter Tuning", "content": "A total of 13 hyperparameters were tuned for the graph models, which are grouped into those influencing the learning rate, the optimizer, the regularisation, and the model architecture. The learning rate hyperparameters set the initial value, the decay multiplier, and the decay patience (in epochs). The optimizer hyperparameters, B1, B2, and e, controlled the decay of the first and second moments, and the optimization stability, respectively. Three types of regularisation were used during model training, which were weight decay, parameter dropout, and patch dropout (defined by the maximum number of patches randomly selected per slide). Hyperparameters relating to the model architecture controlled the number of message-passing layers per graph block, the number of graph blocks (and hence the number of pooling layers), the graph pooling factor, and the embedding size per magnification.\nThe best hyperparameters from tuning each model are shown in Table 2. The smallest tuned classifiers were the single-resolution (10x graph, 0.5M; 10x ABMIL, 0.8M) and combined feature space methods (naive features, 0.7M), followed closely by the zero-initialised model (1.2M) and the 10x+20x graph (1.2M), with the largest being the baseline model with average initialisation (7.9M) and the ResNet-based classifier (10.5M). In most cases, the classifier was smaller than the respective feature extractor, with the UNI model having 303M parameters and the ResNet50 having 9M."}, {"title": "B Supplementary Results Tables", "content": ""}]}