{"title": "Multi-Resolution Histopathology Patch Graphs for Ovarian Cancer Subtyping", "authors": ["Jack Breen", "Katie Allen", "Kieran Zucker", "Nicolas M. Orsi", "Nishant Ravikumar"], "abstract": "Computer vision models are increasingly capable of classifying ovarian epithelial cancer subtypes, but they differ from pathologists by processing small tissue patches at a single resolution. Multi-resolution graph models leverage the spatial relationships of patches at multiple magnifications, learning the context for each patch. In this study, we conduct the most thorough validation of a graph model for ovarian cancer subtyping to date. Seven models were tuned and trained using five-fold cross-validation on a set of 1864 whole slide images (WSIs) from 434 patients treated at Leeds Teaching Hospitals NHS Trust. The cross-validation models were ensembled and evaluated using a balanced hold-out test set of 100 WSIs from 30 patients, and an external validation set of 80 WSIs from 80 patients in the Transcanadian Study. The best-performing model, a graph model using 10x+20x magnification data, gave balanced accuracies of 73%, 88%, and 99% in cross-validation, hold-out testing, and external validation, respectively. However, this only exceeded the performance of attention-based multiple instance learning in external validation, with a 93% balanced accuracy. Graph models benefited greatly from using the UNI foundation model rather than an ImageNet-pretrained ResNet50 for feature extraction, with this having a much greater effect on performance than changing the subsequent classification approach. The accuracy of the combined foundation model and multi-resolution graph network offers a step towards the clinical applicability of these models, with a new highest-reported performance for this task, though further validations are still required to ensure the robustness and usability of the models.", "sections": [{"title": "1 Introduction", "content": "Ovarian cancer is the eighth most common cancer in women worldwide [4] and is characterised by heterogeneous histological subtypes. The five most common subtypes, which account for 90% of all ovarian cancers, are high-grade serous (HGSC), low-grade serous (LGSC), clear cell (CCC), mucinous (MC), and endometrioid carcinomas (EC). These subtypes are distinct in their genetics, prognoses, and treatment options [17], making their classification an essential component of ovarian cancer diagnosis. However, determining the subtype from standard histopathology samples can be a difficult task with a high level of inter-observer discordance [15]. Thus, it is common for pathologists to request ancillary tests and second opinions to ensure an accurate diagnosis, slowing the diagnostic pathway and increasing costs.\nArtificial intelligence (AI) models have been proposed as potential assistive tools for pathological diagnosis. While some tools are starting to receive regulatory approval [20] and be clinically validated [22], this is not the case for ovarian cancer subtyping models. Previous research in this area has involved model prototyping with relatively small homogeneous datasets [5], though some recent studies have included larger and more diverse datasets [1,6,11]. The highest reported performances for five-class classification were 81% and 93% balanced accuracies [11,6], compared to median pathologist concordance rates of 78-86% from histopathology alone, and 90% with the addition of immunohistochemical information [15].\nOvarian cancer resection whole slide images (WSIs) contain billions of pixels and are stored in files that are typically 1-4GB in size. Traditional computer vision models cannot handle such large images, so information is typically extracted from small patches and aggregated to the WSI level in a process called 'multiple instance learning' (MIL). These models often treat patches as being functionally independent of one another, such as in attention-based multiple instance learning (ABMIL) [14], where a weighted average of patch embeddings is taken to form a WSI embedding.\nIt can instead be beneficial to leverage the spatial or semantic relationships between patches by integrating graphs [24] or transformers [27] into MIL models [2,10,12,25,26]. Only one previous study has applied such a method to ovarian cancer subtyping [21], where it was reported that a novel multi-resolution graph network gave a better balanced accuracy than previous methods, including ABMIL, TransMIL [25], and single-magnification graph models. However, this study used only a single set of model hyperparameters for all models, and a single dataset for evaluation, making it unclear whether the models were properly tuned to the given task and data, and unclear how well the models would generalise.\nIn this paper, we present the most thorough evaluation of a graph model for ovarian cancer subtyping to date, including hyperparameter tuning and both hold-out and external validations. To the best of our knowledge, it is also the first graph-based MIL model to be conducted using features from the vision transformer (ViT)-based histopathology foundation model, UNI [9]."}, {"title": "2 Methods", "content": null}, {"title": "2.1 Data", "content": "The training dataset comprised 1864 ovarian carcinoma resection WSIs from 434 patients at Leeds Teaching Hospitals NHS Trust. These were retrospectively collected by a pathologist, who confirmed the original diagnoses made by a gynaecological subspecialty expert. A class-balanced independent hold-out test dataset was collected following the same procedure as in the training set, comprising 100 WSIs from 30 patients. All WSIs were generated from slides of haematoxylin and eosin (H&E)-stained formalin-fixed and paraffin-embedded (FFPE) adnexal tissue which was digitised at 40x magnification on an Aperio AT2 scanner. The training set slides contained both primary surgery samples and interval debulking surgery samples, while the hold-out test set contained primary samples alone. An external dataset of 80 WSIs from 80 ovarian cancer patients was sourced from the Transcanadian Study [16], with these provided at 20x magnification."}, {"title": "2.2 Modelling", "content": "A multi-stage WSI classification pipeline was employed (shown in Figure 1). First, the tissue region was segmented from plain background and tissue patches were selected. Patch-level features were then extracted using a pre-trained model. Graphs were constructed based on the spatial arrangement of the patches, with message-passing layers used to share information between neighbouring patches."}, {"title": "2.3 Training and Validation Procedures", "content": "Graph model hyperparameters were tuned through an iterative grid search procedure, with up to two hyperparameters adjusted at a time and all others frozen at their previous best, starting from the optimal hyperparameters of a previous study [6] in which ABMIL was tuned for the same task and dataset as used in this study. Each hyperparameter configuration was evaluated using the average balanced cross-entropy loss across the validation sets of five-fold cross-validation. Models were trained using an Adam optimizer, with class-balanced sampling used to account for the imbalance in the training set. At least 100 unique configurations were evaluated during the tuning of each graph-based model, with the ABMIL hyperparameters instead taken from the previous study [6]. Hyperparameter tuning is described further in Supplementary Section A.\nSeven total models were evaluated to compare different feature extractors, magnifications, and classifiers. The baseline multi-resolution graph model com-"}, {"title": "3 Results", "content": null}, {"title": "4 Discussion", "content": "Overall, the results indicate that multi-resolution graph models can offer improvements to ovarian carcinoma subtyping. In particular, the 10x+20x magnification model achieved near-perfect performance in external validation, making this the greatest reported performance for this task to date [5]. However, multi-resolution graph models did not offer a clear benefit over ABMIL in internal validations and, considering the relatively small size of the external validation set, it is unclear how great a benefit these models offer overall.\nAs in the previous study using these datasets [6], performance was greater in hold-out testing than cross-validation, and greater still in external testing."}, {"title": "5 Conclusion", "content": "Overall, we have shown that multi-resolution graph models can improve the accuracy of ovarian carcinoma subtyping at the whole-slide level above the previous state-of-the-art, though it was not beneficial in all validations. In an external validation of 80 WSIs, a graph model achieved a near-perfect 99% balanced-accuracy, but in internal testing this was only 88%, which was no greater than ABMIL. The best model combined data at 10x and 20x magnifications, which was better than either using lower magnifications or only using 10x magnification data. While the benefit provided by graph models may offer a clinically useful second opinion to pathologists, more extensive validations are required to understand the reasons underlying performance variability across different datasets and to improve model consistency."}, {"title": "A Supplementary Hyperparameter Tuning", "content": "A total of 13 hyperparameters were tuned for the graph models, which are grouped into those influencing the learning rate, the optimizer, the regularisation, and the model architecture. The learning rate hyperparameters set the initial value, the decay multiplier, and the decay patience (in epochs). The optimizer hyperparameters, B1, B2, and e, controlled the decay of the first and second moments, and the optimization stability, respectively. Three types of regularisation were used during model training, which were weight decay, parameter dropout, and patch dropout (defined by the maximum number of patches randomly selected per slide). Hyperparameters relating to the model architecture controlled the number of message-passing layers per graph block, the number of graph blocks (and hence the number of pooling layers), the graph pooling factor, and the embedding size per magnification.\nThe best hyperparameters from tuning each model are shown in Table 2. The smallest tuned classifiers were the single-resolution (10x graph, 0.5M; 10x ABMIL, 0.8M) and combined feature space methods (naive features, 0.7M), followed closely by the zero-initialised model (1.2M) and the 10x+20x graph (1.2M), with the largest being the baseline model with average initialisation (7.9M) and the ResNet-based classifier (10.5M). In most cases, the classifier was smaller than the respective feature extractor, with the UNI model having 303M parameters and the ResNet50 having 9M."}, {"title": "B Supplementary Results Tables", "content": null}]}