{"title": "Evaluation of Large Language Models for Summarization Tasks in\nthe Medical Domain: A Narrative Review", "authors": ["Emma Croxford", "Yanjun Gao", "Nicholas Pellegrino", "Karen K. Wong", "Graham Wills", "Elliot First", "Frank J. Liao", "Cherodeep Goswami", "Brian Patterson", "Majid Afshar"], "abstract": "Large Language Models have advanced clinical Natural Language Generation, creating opportunities to man-\nage the volume of medical text. However, the high-stakes nature of medicine requires reliable evaluation,\nwhich remains a challenge. In this narrative review, we assess the current evaluation state for clinical sum-\nmarization tasks and propose future directions to address the resource constraints of expert human evaluation.", "sections": [{"title": "Introduction", "content": "The rapid development of Large Language Models (LLMs) has led to significant advancements in the field\nof Natural Language Generation (NLG). In the medical domain, LLMs have shown promise in reducing\ndocumentation-based cognitive burden for healthcare providers, particularly in NLG tasks such as summa-\nrization and question answering. Summarizing clinical documentation has emerged as a critical NLG task\nas the volume of medical text in Electronic Health Records (EHRs) continues to expand [1].\nRecent advancements, like the introduction of larger context windows in LLMs (e.g., Google's Gemini 1.5\nPro with a 1 million-token capacity [2]), allow for the processing of extensive textual data, making it possible\nto summarize entire patient histories in a single input. However, a major challenge in applying LLMs to high-\nstakes environments like medicine is ensuring the reliable evaluation of their performance. Unlike traditional\napproaches, generative AI (GenAI) offers greater flexibility by generating natural language narratives that\nuse language dynamically to fulfill tasks. Yet, this flexibility introduces added complexity in assessing the\naccuracy, reliability, and quality of the generated output where the desired response is not as static.\nThe evaluation of clinical summarization by LLMs must address the intricacies of complex medical\ntexts and tackle LLM-specific challenges such as relevancy, hallucinations, omissions, and ensuring factual\naccuracy [3]. Healthcare data can further complicate the LLM-specific challenges because they can contain\nconflicting or incorrect information. Current metrics, like n-gram overlap and semantic scores, used in\nsummarization tasks are insufficient for the nuanced needs of the medical domain [4]. While these metrics\nmay perform adequately for simple extractive summarization, they fall short when applied to abstractive\nsummarization [5], where complex reasoning and in-depth medical knowledge are required. They are also\nunable to differentiate in the needs of various users and provide evaluations that account for the relevancy\nof generations.\nIn the era of GenAI, automation bias further complicates the potential risks posed by LLMs, partic-\nularly in clinical settings where the consequences of inaccuracies can be severe. Therefore, efficient and\nautomated evaluation methods are essential. In this review, we examine the current state of LLM evaluation\nin summarization tasks, highlighting both its applications and limitations in the medical domain. We also\npropose a future direction to overcome the labor-intensive process of expert human evaluation, which is\ntime-consuming, costly, and requires specialized training."}, {"title": "Human Evaluations in Electronic Health Record Documenta-\ntion", "content": "The current human evaluation frameworks for human-authored clinical notes are largely based on pre-\nGenAI rubrics that assess clinical documentation quality. These frameworks vary depending on the type of\nevaluators, content, and the analysis required to generate evaluative scores. Such flexibility allows for tailored\nevaluation methods, capturing task-specific aspects that ensure quality generation. Expert evaluators, with\ntheir field-specific knowledge, play a crucial role in maintaining high standards of assessment.\nSome commonly used pre-GenAI rubrics include the SaferDx [6], Physician Documentation Quality\nInstrument (PDQI-9) [7], and Revised-IDEA [8] rubrics. The SaferDx rubric focuses on identifying diag-\nnostic errors and analyzing missed opportunities in EHR documentation through a 12-question retrospective\nsurvey aimed at improving diagnostic decision-making and patient safety. The PDQI-9 evaluates physician\nnote quality across nine criteria questions, ensuring continuous improvement in clinical documentation and\npatient care. The Revised-IDEA tool offers feedback on clinical reasoning documentation through a 4-item\nassessment. All three of these rubrics place emphasis on the omission of relevant diagnoses throughout the\ndifferential diagnosis process and the relevant objective data, processes, and conclusions associated with\nthose diagnoses. They also require clinical documentation to be free of incorrect, inappropriate, or incom-\nplete information emphasizing the importance of the quality of evidence and reasoning that is present in\nclinical documentation. Each rubric includes additional questions based on the origin and usage of specific\nclinical documentation like the PDQI-9's assessment of organization to ensure a reader is able to under-\nstand the clinical course of a patient. Each of the three also uses different assessment styles based on the\ngranularity of the questions and intention behind the assessment. For instance, the Revised-IDEA tool uses\na count style assessment for 3 of the 4-items to guarantee the inclusion of a minimum number of objective\ndata points and inclusion of required features for a high-quality diagnostic reasoning documentation. In\nrecent publications, the SaferDx tool has been used as a retrospective analysis of the use of GenAI in clinical\npractice [9], whereas the PDQI-9 and Revised-IDEA tools have been utilized to compare the quality of\nclinical documentation that is written by clinicians versus GenAI methods [10, 11, 12]. While each of these\nrubrics was not originally designed to evaluate LLM-generated content, they offer valuable insights into the\nessential criteria for evaluating text generated in the medical domain.\nHuman evaluations remain the gold standard for LLM outputs [13]. However, because these rubrics were\ninitially developed for evaluating clinician-generated notes, they may need to be adapted for the specific\npurpose of evaluating LLM-generated output. Several new and modified evaluation rubrics have emerged\nto address the unique challenges posed by LLM-generated content, including evaluating the consistency and\nfactual accuracy (i.e., hallucinations) of the generated text. Common themes in these adapted rubrics include\nsafety [14], modality [15, 16], and correctness [17, 18]."}, {"title": "Criteria for Human Evaluations", "content": "In general, the criteria that are used to make up evaluation rubrics for LLM output fall into seven broad\ncriteria: (1) Hallucination [4, 17, 18, 19, 20, 21, 22], (2) Omission [14, 19], (3) Revision [23], (4)\nFaithfulness/Confidence [15, 16, 23], (5) Bias/Harm [14, 16, 22], (6) Groundedness [14, 15], and\n(7) Fluency [15, 17, 20, 23]. Hallucination encompasses any evaluative questions that intend to capture\nwhen information in a generated text does not follow from the source material. Unsupported claims, non-\nsensical statements, improbable scenarios, and incorrect or contradictory facts would be flagged by questions\nin this criteria. Omission-based questions are used to identify missing information in a generated text.\nMedical facts, important information, and critical diagnostic decisions can all be considered omitted when\nnot included in generated text, if those items would have been included by a medical professional. When\nan evaluator is asked to make revisions or estimate the number of revisions needed for a generated text, the\nevaluative question would fall under Revision. Generated texts are revised until they meet the standards\nset forth by a researcher, hospital system, or larger government body. Faithfulness/Confidence is gen-\nerally characterized by questions that capture whether a generated text has preserved the content of the\nsource text and presented conclusions that reflect the confidence and specificity present in the source text.\nQuestions about Bias/Harm evaluate whether generated text is introducing potential harm to a patient\nor reflecting bias in the response. Information that is inaccurate, inapplicable, or poorly applied would be"}, {"title": "Analysis of Human Evaluations", "content": "The method of analysis for evaluation rubrics can also vary based upon the setting and task. Evaluative\nscores can be calculated using binary/Likert categorizations [14, 15], counts/proportions of pre-specified\ninstances [22], edit distance [23], or penalty/reward schemes similar to those used for medical exams\n[24]. Binary categorizations answer evaluative questions using True/False or Yes/No response schema.\nThis set-up allows complex evaluations to be broken down into simpler and potentially more objective\ndecisions. A binary categorization places more penalization on smaller errors by pushing responses to be\neither acceptable or unacceptable. Likert-scaled categorizations allow for a higher level of specificity in the\nscore by providing an ordinal scale. These scales can consist of as many levels as necessary, and in many\ncases there are between 3 and 9 levels including a neutral option for unclear responses. Scales with a higher\nnumber of levels introduce more problems with meeting assumptions of a normal distribution into an analysis,\nalong with complexity and disagreement amongst reviewers. Count/proportion-based evaluations require\nan evaluator to identify pre-specified instances of correct or incorrect key phrases related to a particular\nevaluative criteria. A precision, recall, f-score, or rate can then be computed from an evaluator's annotations\nto establish a numerical score for a generated text. Edit distance evaluations also require an evaluator to\nmake annotations on the generated text that is being evaluated. In these cases, an evaluator makes edits to\nthe generated text until it is satisfactory or no longer contains critical errors. These edits can be corrections\non factual errors, inclusion of omissions, or removal of irrelevant items. The evaluative score is the distance\nfrom the original generated text and the edited version based upon the number of characters, words, etc. that\nrequired editing. The Levenshtein distance [25] is an example of an algorithm used to calculate the distance\nbetween the generated text and its edited version. This distance is calculated as the minimum number of\nsubstitutions, insertions, and deletions of individual characters required to change the original to the edited\nversion. Finally, one of the more complex ways to compute evaluative scores is to use a Penalty/Reward\nschema. These schema award points for positive outcomes to evaluative questions and penalize negative\noutcomes. This schema is similar to those seen on national exams which account for positive and negative\nscores, using the importance and difficulty associated with different questions. For example, the schema used\nto evaluate LLMs on the Med-HALT dataset is an average of the correct and incorrect answers which are\nassigned +1 and -0.25 points respectively [24]. This evaluation schema provides a high level of specificity\nfor assigning weights representative of the trade-off between false positives and false negatives."}, {"title": "Drawbacks of Human Evaluations", "content": "While human evaluations provide nuanced assessments, they are resource-intensive and heavily reliant on the\nrecruitment of evaluators with clinical domain knowledge. The experience and background of an evaluator\ncan significantly influence how they interpret and evaluate generated text. Additionally, the level of guidance\nand specificity in evaluative instructions determines how much of the assessment is shaped by the evaluators'\npersonal interpretations and beliefs about the task. Although increasing the number of evaluators could\nmitigate some of these biases, resources\u2014both time and financial often limit the scale of human evaluations.\nThese evaluations also require substantial manual effort, and without clear guidelines and training, inter-rater\nagreement may suffer. Ensuring that human evaluators align with the evaluation rubric's intent requires\ntraining, much like annotation guidelines for NLP shared tasks [26, 27, 28]. In the clinical domain, medical\nprofessionals are typically used as expert evaluators, but their time constraints limit their availability for\nlarge-scale evaluations. The difficulty of recruiting more medical professionals, compounded by the time\nneeded for thorough assessments, makes frequent, rapid evaluations impractical.\nAnother concern is the validity of the evaluation rubric itself. A robust human evaluation framework"}, {"title": "Pre-LLM Automated Evaluations", "content": "Automated metrics offer a practical solution to the resource constraints of human evaluations, particularly\nin fields like Natural Language Processing (NLP), where tasks such as question answering, translation, and\nsummarization have long relied on these methods. Automated evaluations employ algorithms, models, or\nheuristic techniques to assess the quality of generated text without the need for continuous human interven-\ntion, making them far more efficient in terms of time and labor. These metrics, however, depend heavily\non the availability of high-quality reference texts, often referred to as \"gold standards.\u201d The generated text\nis compared against these gold standard reference texts to evaluate its accuracy and how well it meets the\ntask's requirements. Despite their efficiency, automated metrics may struggle to capture the nuance and\ncontextual understanding required in more complex domains, such as clinical diagnosis, where subtle differ-\nences in phrasing or reasoning can have significant implications. Therefore, while automated evaluations are\nvaluable for their scalability, their effectiveness is closely tied to the quality and relevance of the reference\ntexts used in the evaluation."}, {"title": "Categories of Automated Evaluation", "content": "Automated evaluations in the clinical domain can be categorized into five primary types (Figure 1), each\ntailored to specific evaluation goals and dependent on the availability of reference and source material for\nthe generated text: (1) Word/Character-based, (2) Embedding-based, (3) Learned metrics, (4)\nProbability-based, (5) and Pre-Defined Knowledge Base.\nWord/Character-based evaluations rely on comparisons between a reference text and the generated\ntext to compute an evaluative score. These evaluations can be based on character, word, or sub-sequence\noverlaps depending on the need of the evaluation and the nuance that may be present in the text. Recall\nOriented Understudy for Gisting Evaluation (ROUGE) [29] is a prime example of a word/character-based\nmetric. The many variants of ROUGE N-gram Co-Occurrence (N), Longest Common Sub-sequence\n(L), Weighted Longest Common Sub-sequence (W), Skip-Bigram Co-Occurrence (S) represent the level\nof comparison between the reference and generated texts. ROUGE-L is the current gold standard for\nautomated evaluation, especially in summarization, and relies on the longest common subsequence between\nthe reference and generated texts. The evaluative score is computed as the fraction of words in the text that\nare in the longest common subsequence. Edit distance metrics [25] would also fall under this category as\nthey are based on the number of words or characters that would need to be changed to match the reference\nand generated texts. Edits can be classified as insertions, deletions, substitutions, or transpositions of the\nwords/characters in the generated text.\nEmbedding-based evaluations create contextualized or static embeddings for the reference and gen-\nerated texts for comparison rather than relying on exact matches between words or characters. These\nembedding-based metrics are able to capture semantic similarities between two texts since the embedding\nfor a word or phrase would be based on the text that surrounds it as well as itself. The BERTScore [30]\nis a commonly used metric that falls under this category. For this metric, a Bidirectional Encoder Repre-\nsentations from Transformers (BERT) model [31] is used to generate the contextualized embeddings before\ncomputing a greedy cosine similarity score based on those embeddings."}, {"title": "Drawbacks of Automated Metrics", "content": "Prior to the advent of LLMs, automated metrics would generate a single score meant to represent the quality\nof a generated text, regardless of its length or complexity. This single-score approach can make it difficult\nto pinpoint specific issues in the text, and in the case of LLMs, it is nearly impossible to understand the"}, {"title": "FUTURE DIRECTIONS: LLMs as Evaluators to Complement\nHuman Expert Evaluators: Prompt Engineering LLMs as\nJudges", "content": "LLMs are versatile tools capable of performing a wide range of tasks, including evaluating the outputs\nof other LLMs. This concept, where an LLM acts as a model of a human expert evaluator, has gained\ntraction with the advent of instruction tuning and reinforcement learning with human feedback (RLHF)\n[81]. These advancements have significantly improved the ability of LLMs to align their outputs with human\npreferences, as seen in the transition from GPT-3 to GPT-4, which marked a paradigm shift in LLM accuracy\nand performance [82].\nAn effective LLM evaluator would be able to respond to evaluative questions with precision and accuracy\ncomparable to that of human experts, following frameworks like those used in human evaluation rubrics.\nLLM-based evaluations could provide many of the same advantages as traditional automated metrics, such as\nspeed and consistency, while potentially overcoming the reliance on high-quality reference texts. Moreover,\nLLMs could evaluate complex tasks by directly engaging with the content, bypassing the need for simplistic\nheuristics and offering more information into factual accuracy, hallucinations, and omissions.\nAlthough the use of LLMs as evaluators is still emerging in research, early studies have demonstrated\ntheir utility as an alternative to human evaluations, offering a scalable solution to the limitations of man-\nual assessment [83]. As the methodology continues to develop, LLM-based evaluations hold promise for\naddressing the shortcomings of both traditional automated metrics and human evaluations, particularly in\ncomplex, context-rich domains such as clinical text generation."}, {"title": "Zero-Shot and In-Context Learning", "content": "One method for designing LLMs to perform evaluations is through the use of manually curated prompts\n(Figure 3). A prompt consists of the task description and instructions provided to an LLM to guide its\nresponses. Two primary prompting strategies are employed in this context: Zero-Shot and Few-Shot [3]. In\nZero-Shot prompting, the LLM is given only the task description without any examples before being asked\nto perform evaluations. Few-Shot prompting provides the task description alongside a few examples to help\nguide the LLM in generating output. The number of examples varies based on the LLM's architecture,\ninput window limitations, and the point at which the model performs optimally. Typically, between one and\nfive few-shot examples are used. Prompt engineering, through both Zero-Shot and Few-Shot (\"in-context\nlearning\") approaches (collectively referred to as \"hard prompting\u201d), enables an LLM to perform tasks that\nit was not explicitly trained to do. However, performance can vary significantly depending on the model's\npre-training and its relevance to the new task."}, {"title": "Parameter Efficient Fine-Tuning", "content": "Even though an LLM may be pre-trained on a vast corpus, it can struggle with tasks requiring domain-\nspecific knowledge or handling nuanced inputs. To address these challenges, Supervised fine-tuning (SFT)\nmethods with Parameter Efficient Fine-Tuning (PEFT) using quantization and low rank adaptors can be\nemployed, where the model is trained on a specialized dataset of prompt/response pairs tailored to the\ntask at hand. Fine-tuning every weight in a LLM can require a large amount of time and computational\nresources. In these instances, quantization and low rank adaptors are added to the fine-tuning process for\nPEFT. Quantization reduces the time and memory costs of training by using lower precision data types,\ngenerally 4-bit and 8-bit, for the LLMs weights [85]. Low rank adaptors (LoRA) freeze the weights of a LLM\nand decompose them into a smaller number of trainable parameters ultimately also reducing the costs of SFT\n[86]. PEFT helps refine an LLM by embedding task-specific knowledge, ensuring the model can respond\naccurately in specialized contexts. The creation of these datasets is critical\u2500performance improvements are\ndirectly tied to the quality and relevance of the prompt/response pairs used for fine-tuning. The goal is\nto adjust the LLM to perform better in specific use cases, such as medical diagnosis or legal reasoning, by\nnarrowing its focus to task-specific behaviors through PEFT."}, {"title": "Parameter Efficient Fine-Tuning with Human-Aware Loss Function", "content": "In certain applications, the focus of fine-tuning is to align the LLM with human values and preferences,\nespecially when the model risks generating biased, incorrect, or harmful content. This alignment, known as\nHuman Alignment training, is driven by high-quality human feedback integrated into the training process. A\nwidely recognized approach in this domain is Reinforcement Learning with Human Feedback (RLHF) [87].\nRLHF is applied to update the LLM, guiding it toward outputs that score higher on the reward scale. In\nthe reward model stage, a dataset annotated with human feedback is used to establish the reward, typically\nscalar in nature, of a particular response. The LLM is then trained to produce responses that will receive\nhigher rewards through a process known as Proximal Policy Optimization (PPO) [88]. This iterative process\nensures the model aligns with human expectations, but it can be resource-intensive, requiring significant\nmemory, time, and computational power.\nTo address these computational challenges, newer paradigms have emerged that streamline Human Align-\nment training by directly optimizing the LLM-based on human preferences, without the need for a reward\nmodel with Direct Preference Optimization (DPO) [89]. DPO reformulates the alignment process into a\nhuman-aware loss function (HALO), optimized on a dataset of human preferences where prompts are paired\nwith preferred and dis-preferred responses (Figure 4). This method is particularly promising for aligning\nLLMs with human preferences and can be applied to ordinal responses, such as the Likert scales commonly\nseen in human evaluation rubrics. While PPO improves LLM performance by aligning outputs with human\npreferences, it is often sample-inefficient and can suffer from reward hacking [90]. DPO, in contrast, directly\noptimizes model outputs based on human preferences without needing an explicit reward model, making it\nmore sample-efficient and better aligned with human values. DPO simplifies the training process by focusing\ndirectly on the desired outcomes, leading to more stable and interpretable alignment. While these methods\nhave been successfully applied in other domains [91, 92, 93], their use in the medical field is under-explored.\nTraining data from the human evaluation rubric on a much smaller scale to overcome labor constraints can\nbe incorporated into a loss function designed for human alignment using DPO.\nIn the last year, many variants of DPO have emerged for alignment training methods that can prevent\nover-fitting and circumvent DPO's modeling assumptions with modifications to the underlying model and\nloss function (Figure 5). Alternative methods such as Joint Preference Optimization (JPO) [94] and Simple\nPreference Optimization (SimPO) [95] were derived from DPO. These methods introduce regularization\nterms and modifications to the loss function to prevent premature convergence and ensure more robust\nalignment over a broader range of inputs. Other alternative methods such as Kahneman-Tversky Optimiza-\ntion (KTO) [96] and Pluralistic Alignment Framework (PAL) [97] use alternatives to the Bradley-Terry\npreferences model that underlies DPO. The alternative modeling assumptions used in these methods can\nprevent the breakdown of DPO's alignment in situations without direct preference data and heterogeneous\nhuman preferences."}, {"title": "Drawbacks of LLMs as Evaluators", "content": "LLMs hold promise for automating evaluation, but as with other automated evaluation methods, there are\nsignificant challenges to consider. One major issue is the rapid pace at which LLMs and their associated\ntraining strategies have evolved. This rapid development often outpaces the ability to thoroughly validate\nLLM-based evaluators before they are used in practice. In some cases, new optimization techniques are\nintroduced before their predecessors have undergone peer review, and these advancements may lack sufficient\nmathematical justification. The speed of LLM evolution can make it difficult to allocate time and resources\nfor proper validation, which can compromise their reliability.\nMoreover, despite their advancements, LLMs remain sensitive to the prompts and inputs they receive.\nAs LLMs continue to update and change their internal knowledge representations and as their prompts also\nchange, the output can be highly variable. The exact LLM, or model version, that is used can also add\nanother layer of variability. The same prompts and inputs can produce different results based on the LLM's\ninternal structure and pre-training schema. LLMs have also been noted for egocentric bias which could\naffect evaluations as more and more LLM generated text appears in source texts [112]. As a result, the\nuse of LLMs as evaluators must be accompanied by stringent testing and safety checks to mitigate risks.\nEnsuring fairness in their responses is also critical, particularly in sensitive domains like healthcare, where\nbiased or stigmatizing language could have serious consequences. These challenges highlight the need for\ncontinuous evaluation, testing, and refinement to make LLM-based evaluators both reliable and safe for\nmedical evaluations."}]}