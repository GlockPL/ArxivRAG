{"title": "Rethinking and Improving Visual Prompt\nSelection for In-Context Learning Segmentation", "authors": ["Wei Suo", "Lanqing Lai", "Mengyang Sun", "Hanwang Zhang", "Peng\nWang", "Yanning Zhang"], "abstract": "As a fundamental and extensively studied task in computer\nvision, image segmentation aims to locate and identify different seman-\ntic concepts at the pixel level. Recently, inspired by In-Context Learning\n(ICL), several generalist segmentation frameworks have been proposed,\nproviding a promising paradigm for segmenting specific objects. However,\nexisting works mostly ignore the value of visual prompts or simply apply\nsimilarity sorting to select contextual examples. In this paper, we focus\non rethinking and improving the example selection strategy. By compre-\nhensive comparisons, we first demonstrate that ICL-based segmentation\nmodels are sensitive to different contexts. Furthermore, empirical evi-\ndence indicates that the diversity of contextual prompts plays a crucial\nrole in guiding segmentation. Based on the above insights, we propose a\nnew stepwise context search method. Different from previous works, we\nconstruct a small yet rich candidate pool and adaptively search the well-\nmatched contexts. More importantly, this method effectively reduces the\nannotation cost by compacting the search space. Extensive experiments\nshow that our method is an effective strategy for selecting examples and\nenhancing segmentation performance.", "sections": [{"title": "1 Introduction", "content": "Image segmentation is a fundamental task in computer vision, aiming to lo-\ncate and identify different semantic concepts at the pixel level. It is essen-\ntial in multiple applications such as autonomous driving [43,54], video surveil-\nlance [20,36,39,41] and complex reasoning [24,34,40]. To achieve precise segmen-\ntation, researchers have proposed a variety of segmentation models [5,12,23,29].\nAlthough these methods have made considerable progress, they require training\nthe specialist model for each segmentation task. Recently, inspired by In-Context\nLearning (ICL) [26,30,49], several generalist segmentation frameworks have been\nproposed and achieved impressive performance such as MAE-VQGAN [6] and\nSegGPT [45]. Different from the traditional frameworks, ICL-based segmenta-\ntion methods establish a promising paradigm for localizing vision concepts by\nfeeding one or a few examples as references during inference. Meanwhile, the\nmethodology offers a general solution for various real-world scenarios [44].\nICL learning is a relatively new research line in computer vision, while the\nfundamental ideas can be traced back to the Natural Language Processing (NLP)\nfield [6, 44]. With the development of Large Language Models (LLM), numer-\nous studies have already proved that selecting a suitable context is crucial for\nfollowing human instruction and improving performance [1,14,31]. However, re-\nlated researches about this topic is still limited in our community. As shown in\nFig.1 (a), existing works typically randomly select examples [44] or simply apply\nsimilarity sorting to construct contextual prompts [37,56]. The above insights\nmotivate us to question: 1) Whether different contexts would significantly im-\npact performance? 2) What is the critical factor of visual prompt selection for\nICL-based segmentation models? In this paper, we first try to answer the above\nquestions. By comprehensive comparisons, we find that the ICL-based models\nare sensitive to different examples. With multiple experiments, the performance\ngap between using different contextual examples even exceeds 5 points. Fur-\nthermore, we synthetically investigate the effectiveness of the existing prompt\nselection strategy. Surprisingly, although the similarity measure (i.e., choosing"}, {"title": "2 Related work", "content": "2.1 Visual Segmentation\nAs a fundamental task in computer vision, visual segmentation entails locating\nand identifying different semantic concepts at the pixel level. Numerous works for\nspecific tasks have continued to flourish over the years. For example, Crossover-\nNet [52] learns the vertical and horizontal crossover relation for medical image\nsegmentation. RFNet [35] fuses RGB and depth information which is designed\nfor real-time road-driving segmentation. [16] proposes DGEN, a densely con-\nnected global entropy network in which noise is suppressed by context informa-\ntion losses. MSNANet [25] accurately extracts water bodies from remote sensing\nscenes by utilizing multiscale attention. Recently, inspired by ICL learning, sev-\neral general segmentation methods have been proposed [18,44,45]. Different from\nthese specialist models, the paradigm can locate visual concepts by feeding a few\nexamples, known as the In-Context Learning (ICL) segmentation frameworks.\n2.2 In-Context Learning\nIn-context learning (ICL) is a new learning paradigm that initially emerged from\nGPT-3 [7]. It presents a text fill-in-the-blank problem with examples, eliminating\nthe necessity for fine-tuning model parameters on downstream NLP tasks. In\nthe vision domain, MAE-VQGAN [6] transforms various visual tasks into grid\ninpainting problems and proposes the first ICL-based segmentation framework.\nDifferent from the previous segmentation methods, this new paradigm provides a\npromising manner to segment arbitrary objects with a few examples. Painter [44]\nadopts masked image modeling on continuous pixels, unifying the output form\nacross different visual tasks. As a variant of Painter, SegGPT [45] integrates\nvisual segmentation tasks into in-context coloring problems and demonstrates\npowerful capabilities in target segmentation. Beyond the previous works, we\npresent comprehensive experiments on example selection for visual ICL models.\n2.3 Contextual Example Selection\nWith the development of LLMs, the selection of contextual examples is important\nfor ICL learning [1,30,51]. The emerging studies can be categorized into rule-\nbased unsupervised methods and supervised methods. KATE [22] uses the KNN\nmechanism to enhance a retriever for selecting in-context examples. EPR [30]\nadopts a contrastive learning strategy to train a dense retriever. UDR [19] ex-\nplores the process of retrieval demonstrations for various tasks in a unified man-\nner. However, few studies investigate these effects in the vision domain. Current\nworks typically select examples randomly [44] or simply apply similarity sorting\nto construct contextual demonstrations [37,56]. Different from the previous rule-\nbased methods, we introduce an adaptive search method with a small yet rich\ncandidate pool. This strategy effectively reduces the annotation cost and boosts\nsegmentation performance."}, {"title": "3 Preliminary", "content": "In this section, we would first present a simple overview of the ICL-based seg-\nmentation frameworks. Then, we construct extensive experiments to compre-\nhensively analyze the influence of different examples and the effectiveness of the\nexisting selection strategies. Finally, we experimentally proved that contextual\ndiversity is an important factor for guiding segmentation.\n3.1\nBackground\nThe design of segmentation models has achieved tremendous development in re-\ncent years [20, 41, 52, 57]. However, these solutions require expensive annotation\ncost and face challenges in adapting to new scenarios. In contrast, the ICL-based\nsegmentation frameworks [6,45] can implement various specific tasks by lever-\naging several segmented examples. The convenient and versatile vision models\nprovide a promising perspective for our community."}, {"title": "4 Our Method", "content": "Based on the discussion in Sec. 3, we design a new SCS method to construct a\nsmall yet rich candidate pool and adaptively search the contextual demonstra-\ntions. Compared to previous works, our method has two potential advantages:\n1) Fewer Annotations. The SCS focuses on building a small yet diverse candi-\ndate pool. It only needs to annotate a portion of samples while achieving better\nperformance. 2) Adapting Selection. Compared to the previous rule-based mech-\nanism [37,56], our method can adaptively search the well-matched examples for\ndifferent test instances. Next, we would introduce our method in detail.\n4.1\nConstructing Candidate Pool\nAs mentioned in Sec. 3, it is important to provide examples for the ICL-based\nsegmentation paradigm. However, previous research mostly depend on huge an-\nnotations as contextual candidates, it would inevitably suffers from expensive\nannotation cost. Motivated by the of Sec. 3.5, we aim to build a small yet di-\nverse candidate pool that can be used for subsequent context searching.\nGiven N unlabeled samples, follow in [56], we utilize CLIP to extract image\nfeature into d-dimensional vectors, represented by \\(F = \\{f_i\\}_{i=1}^N\\), \\(f_i \\in \\mathbb{R}^d\\), where\n\\(f_i\\) is the i-th encoded image feature. To construct a candidate pool comprising\na set of representative examples, we employ k-means clustering algorithm [58]\nto generate M clusters \\(C = \\{c_m\\}_{m=1}^M\\), which can be determined by minimizing\nthe sum of squared intra-cluster norms:\n\\(\text{arg min } \\sum_{m=1}^M \\sum_{f_i \\in c_j} || f_i - \\mu_m ||\\),\nwhere \\(\\mu_m\\) is centroid of cluster \\(c_m\\). Initially, this algorithm randomly selects M\ncentroids. Each instance is assigned by computing the distance to all selected\ncentroids. The process is iteratively executed until the clusters are stable.\nTo construct the candidate pool, a simple method is to select members clos-\nest to the centroid, while previous studies have shown that samples closest to\nthe center do not necessarily represent the entire cluster [11,46]. Moreover, re-\nlated works have provided several alternative sampling methods such as density"}, {"title": "4.2 Adaptive Search Module", "content": "In this module, our goal is to select well-matched examples from the constructed\ncandidate pool. As shown in Fig. 4, we propose a new adaptive search module\nthat can further search and assemble the suitable contexts for given queries.\nSpecifically, for a given test instance \\(I_q\\), we first apply the same image en-\ncoder (i.e. CLIP) to extract the corresponding feature vector \\(f_q \\in \\mathbb{R}^d\\). Then,\nwe concatenate the image feature \\(f_q\\) and each example \\(f_c^m\\) from the candidate\npool. These fused features are fed into a search agent that consists of a group of\nlight and simple Multi-Layer Perceptrons (MLP). The search computation can\nbe denoted as follows:\n\\(s_a^m = \\text{MLP}([f_c^m; f_q])),\n\\(\\{a_a^1, a_a^2, ..., a_a^{2M}\\} = \\text{softmax}(\\{s_a^1, s_a^2, ..., s_a^{2M}\\}),\\)\nwhere \\(a_a^m\\) represents the probability of m-th example in the candidate pool.\nTo guide the search agent in learning select examples, we apply the IoU scores\nas rewards for selecting well-matched demonstrations from the candidate pool.\nSpecifically, following the format of the ICL-based segmentation model, we re-\norganize query \\(I_q\\), the original image of context examples \\(I_a\\) and corresponding\nmask \\(G_a\\) into a joint image, as shown in Fig. 4. These images are then fed into\nthe ICL-based segmentation model to obtain the IoU scores \\(U = \\{u_a\\}_{a=1}^{2M}\\). In-\nspired by [4,38], we use the average IoU score \\(u_{avg}\\) as the base score. By applying\nreinforcement learning, the gradient is calculated by:\n\\(\\nabla_{\\theta} L(\\theta) = \\frac{1}{2M} \\sum_{m=1}^{2M} (u_a^m - u_{avg}) \\nabla_{\\theta} \\text{log}(a_a^m)\\),\nwhere \\(a_a^m\\) and \\(\\theta\\) are the probability of m-th example and the parameters of\nthe search agent. Based on the above computation, this gradient would tend to\nincrease the probability of the m-th context when its IoU scores \\(u_a^m\\) are higher\nthan the average IoU score \\(u_{avg}\\).\nDuring inference, we directly input the test samples and candidate pool ex-\namples into the search agent. The contextual examples are then determined by\nsorting them based on probability scores \\(a_a^m\\) as detailed in Eq. 4."}, {"title": "5 Experiment", "content": "We use different prompt selection strategies and various generalist segmentation\nframeworks to demonstrate the superiority of our method. Furthermore, we com-\npare our approach with traditional methods and conduct extensive qualitative\nand quantitative studies on multiple datasets.\n5.1 Experimental Setting\nDataset. We follow the previous works [6, 45, 56] and evaluate our approach\non PASCAL-52 [32], COCO-20 [21] and iSALD-5 [50]. PASCAL-5 includes 20\nsemantic categories and is created from PASCAL VOC 2012 [9] with external\nannotations from SDS [10]. COCO-20 is built upon MSCOCO [21], which is\na more challenging dataset and contains 80 categories. For all categories, both\nCOCO-20 and PASCAL-5 are evenly divided into 4 folds for cross-validation,\nwhere each fold is selected as the test set and the remaining three folds are used\nfor training. The iSALD-5 contains 15 geospatial categories which derived from\niSALD [48] in the remote sensing image semantic segmentation task. Similarly,\nthe iSALD-5 is also partitioned into 3 folds for cross-validation.\nEvaluation Metric. Following [13], mean intersection over union (mIoU) and\nforeground-background IoU (FBIoU) are adopted as evaluation metrics. The\nformer measures the average overall IoU values for all object classes, while the\nlatter reports the average IoU scores for the foreground and background.\n5.2 Quantitative Evaluation\nIn Table 1, we show a performance comparison of two general ICL-based seg-\nmentation models with and without proposed SCS. Following [37,56], all results\nare reported by repeating the random experiment five times. It can be observed\nthat our method can significantly boost performance in all settings including\n1-shot and 5-shot settings. In particular, for the 1-shot setting, the SCS leads\nto MAE-VQGAN with the mIoU gain of 7.0 and 6.5 on the PASCAL-5 and\nCOCO-20 dataset, respectively. Besides, compared to previous rule-base mod-\nels [56] in 1-shot and 5-shot semantic segmentation tasks, our method surpasses\nit by a considerable margin. Moreover, following the few-shot setting and ensur-\ning a fair comparison, we report related results based on the pre-trained weights\nfor SegGPT [3]. It can be observed that our method significantly outperforms\nthese rule-based visual prompt selection methods for SegGPT by 9.7 and 6.8 on\nthe 1-shot and 5-shot settings for COCO-20. More importantly, the above re-\nsults demonstrate that the proposed SCS can be considered as a \"plug-and-play\"\nmodule to enhance the segmentation ability of ICL-based frameworks."}, {"title": "5.3 Ablation Study", "content": "To clarify the effectiveness of our method, we conduct further ablation studies\non the COCO-20 dataset with the 5-shot setting. The SegGPT is used as the\nbasic model due to its outstanding segmentation performance.\nEffects of Cluster. In Table 2, we first build a baseline by randomly selecting\nexamples from the entire training set. The average result is reported across 5\ndifferent seeds. In the second row of Table 2, we follow [56] and utilize cosine\nsimilarity ranking to select the top-5 examples as contexts. In the third row,\nwe investigate the impact of the cluster on in-context example selection, using\nthe closest examples to each cluster centroid to construct the candidate pool.\nCompared to the baseline, the results show that compacting the search space\nnot only reduces annotation cost but also improves model performance.\nEffects of Sort. As shown in row 4 of Table 2, we experimentally demonstrate\nthat integrating our sorting mechanism results in 1.9 mIoU improvement com-\npared to directly using the closest examples to each cluster centroid. The inherent\nreason is that centroid examples describe a limited feature space [33,46], mak-\ning it challenging to support a variety of different contexts for test instances. In\ncontrast, our sorting mechanism introduces the nearest and farthest examples,\nwhich constructs more diverse demonstrations for ICL learning.\nEffects of Adaptive Search Module. Finally, we further explore the effec-\ntiveness of our adaptive search module in the last row of Table 2. Compared\nto using a simple similarity sorting method, our approach shows an improve-"}, {"title": "5.4 Different Model Settings", "content": "In this section, we explore several alternative model settings to further discuss\nthe proposed method. All experiments are conducted on the COCO-20 with\nthe SegGPT model. In the first two rows, we first compare the performance\nusing different metrics. Specifically, we use the rule-based methods to select the\ncontextual examples from the complete training set. which including Entropy\nsorting [11] and Foreground sorting [53]. In particular, \"Entropy\" is executed by\ncalculating the maximum entropy values. The \"Foreground\" refers to a two-stage\napproach [53]. It uses an object detector [42] to extract the foreground boxes\nand the foreground similarity is used as the metric. From the 1-shot and 5-\nshot results, it can be observed that our SCS can dramatically outperform these\nmethods by a large margin. In rows 3-4, we construct the experiments with\ndifferent feature encoders. We replace the CLIP-ViT with ViT [8] and CLIP-\nresnet [27] to obtain image feature vectors, These extracted features would be\nutilized to construct the candidate pool and perform the adaptive searching.\nThe results show that our SCS is robust for different image encoders. Finally, in\nrows 5-6 of Table 3, we replace our selection mechanism with several alternative\nmanners. However, these approaches mostly involve complex computation. For\nexample, the Density-sort [2] requires additional computations on the order of\nO(MN) in each iteration to update density lists, where M is the number of\nclusters and N is the size of the unlabeled data. Instead, our method uses the\nbasic clustering algorithm and obtains a comparable performance.\n5.5 Further Discussion\nMore Segmentation tasks. In this section, we evaluate the effect of our\nmethod on more segmentation tasks and datasets. Because MAE-VQGAN can-\nnot distinguish between individual instances of objects within the same class,\nwe construct more experiments with the SegGPT model. Specifically, we report\ninstance segmentation results on PASCAL-5i in Table 4 and aerial image seman-\ntic segmentation results on iSALD-5i in Table 5. The above results indicate that\nthe SCS can enhance the segmentation ability for various real-world scenarios."}, {"title": "6 Conlusion", "content": "In this paper, we focus on the example selection method for the ICL-based seg-\nmentation frameworks. By building comprehensive comparisons, we empirically\ndemonstrate that diversity of contexts is a key factor in guiding segmentation\nand boosting performance. Beyond the above insights, we propose a new step-\nwise context search method that improves the diversity of the candidate pool and\nsignificantly alleviates the annotation cost. We hope that this work can promote\nfurther research to better understand in-context learning in computer vision."}]}