{"title": "Rethinking and Improving Visual Prompt Selection for In-Context Learning Segmentation", "authors": ["Wei Suo", "Lanqing Lai", "Mengyang Sun", "Hanwang Zhang", "Peng Wang", "Yanning Zhang"], "abstract": "As a fundamental and extensively studied task in computer\nvision, image segmentation aims to locate and identify different seman-\ntic concepts at the pixel level. Recently, inspired by In-Context Learning\n(ICL), several generalist segmentation frameworks have been proposed,\nproviding a promising paradigm for segmenting specific objects. However,\nexisting works mostly ignore the value of visual prompts or simply apply\nsimilarity sorting to select contextual examples. In this paper, we focus\non rethinking and improving the example selection strategy. By compre-\nhensive comparisons, we first demonstrate that ICL-based segmentation\nmodels are sensitive to different contexts. Furthermore, empirical evi-\ndence indicates that the diversity of contextual prompts plays a crucial\nrole in guiding segmentation. Based on the above insights, we propose a\nnew stepwise context search method. Different from previous works, we\nconstruct a small yet rich candidate pool and adaptively search the well-\nmatched contexts. More importantly, this method effectively reduces the\nannotation cost by compacting the search space. Extensive experiments\nshow that our method is an effective strategy for selecting examples and\nenhancing segmentation performance.", "sections": [{"title": "1 Introduction", "content": "Image segmentation is a fundamental task in computer vision, aiming to lo-\ncate and identify different semantic concepts at the pixel level. It is essen-\ntial in multiple applications such as autonomous driving [43,54], video surveil-\nlance [20,36,39,41] and complex reasoning [24,34,40]. To achieve precise segmen-\ntation, researchers have proposed a variety of segmentation models [5,12,23,29].\nAlthough these methods have made considerable progress, they require training"}, {"title": "2 Related work", "content": "As a fundamental task in computer vision, visual segmentation entails locating\nand identifying different semantic concepts at the pixel level. Numerous works for\nspecific tasks have continued to flourish over the years. For example, Crossover-\nNet [52] learns the vertical and horizontal crossover relation for medical image\nsegmentation. RFNet [35] fuses RGB and depth information which is designed\nfor real-time road-driving segmentation. [16] proposes DGEN, a densely con-\nnected global entropy network in which noise is suppressed by context informa-\ntion losses. MSNANet [25] accurately extracts water bodies from remote sensing\nscenes by utilizing multiscale attention. Recently, inspired by ICL learning, sev-\neral general segmentation methods have been proposed [18,44,45]. Different from\nthese specialist models, the paradigm can locate visual concepts by feeding a few\nexamples, known as the In-Context Learning (ICL) segmentation frameworks."}, {"title": "2.2 In-Context Learning", "content": "In-context learning (ICL) is a new learning paradigm that initially emerged from\nGPT-3 [7]. It presents a text fill-in-the-blank problem with examples, eliminating\nthe necessity for fine-tuning model parameters on downstream NLP tasks. In\nthe vision domain, MAE-VQGAN [6] transforms various visual tasks into grid\ninpainting problems and proposes the first ICL-based segmentation framework.\nDifferent from the previous segmentation methods, this new paradigm provides a\npromising manner to segment arbitrary objects with a few examples. Painter [44]\nadopts masked image modeling on continuous pixels, unifying the output form\nacross different visual tasks. As a variant of Painter, SegGPT [45] integrates\nvisual segmentation tasks into in-context coloring problems and demonstrates\npowerful capabilities in target segmentation. Beyond the previous works, we\npresent comprehensive experiments on example selection for visual ICL models."}, {"title": "2.3 Contextual Example Selection", "content": "With the development of LLMs, the selection of contextual examples is important\nfor ICL learning [1,30,51]. The emerging studies can be categorized into rule-\nbased unsupervised methods and supervised methods. KATE [22] uses the KNN\nmechanism to enhance a retriever for selecting in-context examples. EPR [30]\nadopts a contrastive learning strategy to train a dense retriever. UDR [19] ex-\nplores the process of retrieval demonstrations for various tasks in a unified man-\nner. However, few studies investigate these effects in the vision domain. Current\nworks typically select examples randomly [44] or simply apply similarity sorting\nto construct contextual demonstrations [37,56]. Different from the previous rule-\nbased methods, we introduce an adaptive search method with a small yet rich\ncandidate pool. This strategy effectively reduces the annotation cost and boosts\nsegmentation performance."}, {"title": "3 Preliminary", "content": "In this section, we would first present a simple overview of the ICL-based seg-\nmentation frameworks. Then, we construct extensive experiments to compre-\nhensively analyze the influence of different examples and the effectiveness of the\nexisting selection strategies. Finally, we experimentally proved that contextual\ndiversity is an important factor for guiding segmentation."}, {"title": "3.1 Background", "content": "The design of segmentation models has achieved tremendous development in re-\ncent years [20, 41, 52, 57]. However, these solutions require expensive annotation\ncost and face challenges in adapting to new scenarios. In contrast, the ICL-based\nsegmentation frameworks [6,45] can implement various specific tasks by lever-\naging several segmented examples. The convenient and versatile vision models\nprovide a promising perspective for our community."}, {"title": "3.2 Influence of Different Examples", "content": "As mentioned above, research on the influence of different examples in segmen-\ntation remains insufficient. In this paper, we aim to fill this gap by conducting\nexperiments to quantitatively analyze the impact of different demonstrations.\nSpecifically, we randomly select examples from the entire training set with 5 dif-\nferent seeds. As shown in Fig. 2 (a), we report the best, worst, and average results\nacross the 5 experiments. It reveals that allocating different contexts would dra-\nmatically affect the segmentation scores under the 1-shot and 5-shot settings.\nIn particular, the best results can surpass the worst by margins of 5.6 and 4.1\npoints, respectively. Due to the huge performance discrepancy between different\ncontextual prompts, designing an automated method for searching well-matched\nexamples emerges as a critical challenge."}, {"title": "3.3 Example Selection with Similarity", "content": "To the best of our knowledge, the related researches to our work are [37,56],\nwhere they simply rely on cosine similarity to select examples with qualitative\nanalysis. In this paper, we try to rethink the strategy through a quantitative dis-\ncussion across different similarity computation manners. Specifically, we explore\ntwo types of image similarity calculation strategies including appearance-level\nand semantic-level. For the appearance-level comparison, PSNR [47], SSIM [15],\nFSIM [55] and HaarPSI [28] obtain the similarity between the test instance and\nthe entire training set. On the other hand, for semantic-level comparison, we\nfollow [37,56] to utilize the off-the-shelf image encoders (i.e., CLIP [27] and\nViT [8]) for feature extraction. To verify the effectiveness of these similarity-\nbased approaches, we carefully select the examples based on cosine similarity\nsorting.\nAs shown in Fig. 2 (b), each metric is represented by two bars including\nnearest > farthest (yellow bar) and nearest < farthest (green bar). We take\nyellow as an example, it denotes the proportion of instances where using the\nmost similar examples (nearest) as contexts results in a better IoU than using\nthe most dissimilar ones (farthest), and the green bar indicates the opposite.\nSurprisingly, we observed that assembling the most dissimilar contexts leads to\nsuperior performance for numerous test instances. There is a substantial portion\nof the test samples (~40%) that achieve better IoU scores by utilizing the farthest\nexamples. The above results indicate that using similarity measures is not the\nonly way to search for contextual examples."}, {"title": "3.4 Example Selection with Diversity", "content": "Based on the above analyses, it is imperative to explore a more effective way for\nsearching visual prompts. In Fig. 2 (b), we find that applying the most dissimilar"}, {"title": "3.5 Discussion", "content": "In this section, we would try to summarize the above findings with a deeper dis-\ncussion. Based on the insights presented in Sec. 3.2, we find that the ICL-based\nsegmentation framework is sensitive to contextual examples. Given the different\nvisual prompts, the performance gap is even over 5 points on the mIoU. There-\nfore, building an effective prompt selection method is important for boosting the\nsegmentation results and achieving stable performance.\nTo further explore the key factors that influence example selection, we design\nexperiments in Sec. 3.3 and Sec. 3.4. The results demonstrate that combining\nboth similar and dissimilar demonstrations can significantly enhance the IoU\nscores. We argue that the underlying reason is that this manner provides richer\nsemantic information about the segmented objects by expanding the diversity\nof visual prompts. Based on the above insights, a natural idea is to select these"}, {"title": "4 Our Method", "content": "Based on the discussion in Sec. 3, we design a new SCS method to construct a\nsmall yet rich candidate pool and adaptively search the contextual demonstra-\ntions. Compared to previous works, our method has two potential advantages:\n1) Fewer Annotations. The SCS focuses on building a small yet diverse candi-\ndate pool. It only needs to annotate a portion of samples while achieving better\nperformance. 2) Adapting Selection. Compared to the previous rule-based mech-\nanism [37,56], our method can adaptively search the well-matched examples for\ndifferent test instances. Next, we would introduce our method in detail."}, {"title": "4.1 Constructing Candidate Pool", "content": "As mentioned in Sec. 3, it is important to provide examples for the ICL-based\nsegmentation paradigm. However, previous research mostly depend on huge an-\nnotations as contextual candidates, it would inevitably suffers from expensive\nannotation cost. Motivated by the of Sec. 3.5, we aim to build a small yet di-\nverse candidate pool that can be used for subsequent context searching.\nGiven N unlabeled samples, follow in [56], we utilize CLIP to extract image\nfeature into d-dimensional vectors, represented by \\(F = \\{f_i\\}_{i=1}^{N}\\), \\(f_i \\in R^d\\), where\n\\(f_i\\) is the i-th encoded image feature. To construct a candidate pool comprising\na set of representative examples, we employ k-means clustering algorithm [58]\nto generate M clusters \\(C = \\{c_m\\}_{m=1}^{M}\\), which can be determined by minimizing\nthe sum of squared intra-cluster norms:\narg min \\( \\sum_{m=1}^{M} \\sum_{f_i\\in c_j} || f_i - \\mu_m ||\\),\n\nwhere \\(\\mu_m\\) is centroid of cluster \\(c_m\\). Initially, this algorithm randomly selects M\ncentroids. Each instance is assigned by computing the distance to all selected\ncentroids. The process is iteratively executed until the clusters are stable.\nTo construct the candidate pool, a simple method is to select members clos-\nest to the centroid, while previous studies have shown that samples closest to\nthe center do not necessarily represent the entire cluster [11,46]. Moreover, re-\nlated works have provided several alternative sampling methods such as density"}, {"title": "4.2 Adaptive Search Module", "content": "In this module, our goal is to select well-matched examples from the constructed\ncandidate pool. As shown in Fig. 4, we propose a new adaptive search module\nthat can further search and assemble the suitable contexts for given queries.\nSpecifically, for a given test instance \\(I_q\\), we first apply the same image en-\ncoder (i.e. CLIP) to extract the corresponding feature vector \\(f_q \\in R^d\\). Then,\nwe concatenate the image feature \\(f_q\\) and each example \\(f_m^a\\) from the candidate\npool. These fused features are fed into a search agent that consists of a group of\nlight and simple Multi-Layer Perceptrons (MLP). The search computation can\nbe denoted as follows:\n\\(s_m^a = MLP([f_m^a; f_q])),\n\n\\(a_1^a, a_2^a, ..., a_{2M}^a = softmax(s_1^a, s_2^a, ..., s_{2M}^a)\\),"}, {"title": "5 Experiment", "content": "We use different prompt selection strategies and various generalist segmentation\nframeworks to demonstrate the superiority of our method. Furthermore, we com-\npare our approach with traditional methods and conduct extensive qualitative\nand quantitative studies on multiple datasets."}, {"title": "5.1 Experimental Setting", "content": "We follow the previous works [6, 45, 56] and evaluate our approach\non PASCAL-52 [32], COCO-202 [21] and iSALD-5\u00b2 [50]. PASCAL-52 includes 20\nsemantic categories and is created from PASCAL VOC 2012 [9] with external\nannotations from SDS [10]. COCO-20\u00b2 is built upon MSCOCO [21], which is\na more challenging dataset and contains 80 categories. For all categories, both\nCOCO-20 and PASCAL-52 are evenly divided into 4 folds for cross-validation,\nwhere each fold is selected as the test set and the remaining three folds are used\nfor training. The iSALD-52 contains 15 geospatial categories which derived from\niSALD [48] in the remote sensing image semantic segmentation task. Similarly,\nthe iSALD-5i is also partitioned into 3 folds for cross-validation.\nFollowing [13], mean intersection over union (mIoU) and\nforeground-background IoU (FBIoU) are adopted as evaluation metrics. The\nformer measures the average overall IoU values for all object classes, while the\nlatter reports the average IoU scores for the foreground and background."}, {"title": "5.2 Quantitative Evaluation", "content": "In Table 1, we show a performance comparison of two general ICL-based seg-\nmentation models with and without proposed SCS. Following [37,56], all results\nare reported by repeating the random experiment five times. It can be observed\nthat our method can significantly boost performance in all settings including\n1-shot and 5-shot settings. In particular, for the 1-shot setting, the SCS leads\nto MAE-VQGAN with the mIoU gain of 7.0 and 6.5 on the PASCAL-52 and\nCOCO-20\u00b2 dataset, respectively. Besides, compared to previous rule-base mod-\nels [56] in 1-shot and 5-shot semantic segmentation tasks, our method surpasses\nit by a considerable margin. Moreover, following the few-shot setting and ensur-\ning a fair comparison, we report related results based on the pre-trained weights\nfor SegGPT [3]. It can be observed that our method significantly outperforms\nthese rule-based visual prompt selection methods for SegGPT by 9.7 and 6.8 on\nthe 1-shot and 5-shot settings for COCO-20\u00b2. More importantly, the above re-\nsults demonstrate that the proposed SCS can be considered as a \"plug-and-play\"\nmodule to enhance the segmentation ability of ICL-based frameworks."}, {"title": "5.3 Ablation Study", "content": "To clarify the effectiveness of our method, we conduct further ablation studies\non the COCO-20\u00b2 dataset with the 5-shot setting. The SegGPT is used as the\nbasic model due to its outstanding segmentation performance.\nIn Table 2, we first build a baseline by randomly selecting\nexamples from the entire training set. The average result is reported across 5\ndifferent seeds. In the second row of Table 2, we follow [56] and utilize cosine\nsimilarity ranking to select the top-5 examples as contexts. In the third row,\nwe investigate the impact of the cluster on in-context example selection, using\nthe closest examples to each cluster centroid to construct the candidate pool.\nCompared to the baseline, the results show that compacting the search space\nnot only reduces annotation cost but also improves model performance.\nAs shown in row 4 of Table 2, we experimentally demonstrate\nthat integrating our sorting mechanism results in 1.9 mIoU improvement com-\npared to directly using the closest examples to each cluster centroid. The inherent\nreason is that centroid examples describe a limited feature space [33,46], mak-\ning it challenging to support a variety of different contexts for test instances. In\ncontrast, our sorting mechanism introduces the nearest and farthest examples,\nwhich constructs more diverse demonstrations for ICL learning.\nFinally, we further explore the effec-\ntiveness of our adaptive search module in the last row of Table 2. Compared\nto using a simple similarity sorting method, our approach shows an improve-"}, {"title": "5.4 Different Model Settings", "content": "In this section, we explore several alternative model settings to further discuss\nthe proposed method. All experiments are conducted on the COCO-20 with\nthe SegGPT model. In the first two rows, we first compare the performance\nusing different metrics. Specifically, we use the rule-based methods to select the\ncontextual examples from the complete training set. which including Entropy\nsorting [11] and Foreground sorting [53]. In particular, \"Entropy\" is executed by\ncalculating the maximum entropy values. The \"Foreground\" refers to a two-stage\napproach [53]. It uses an object detector [42] to extract the foreground boxes\nand the foreground similarity is used as the metric. From the 1-shot and 5-\nshot results, it can be observed that our SCS can dramatically outperform these\nmethods by a large margin. In rows 3-4, we construct the experiments with\ndifferent feature encoders. We replace the CLIP-ViT with ViT [8] and CLIP-\nresnet [27] to obtain image feature vectors, These extracted features would be\nutilized to construct the candidate pool and perform the adaptive searching.\nThe results show that our SCS is robust for different image encoders. Finally, in\nrows 5-6 of Table 3, we replace our selection mechanism with several alternative\nmanners. However, these approaches mostly involve complex computation. For\nexample, the Density-sort [2] requires additional computations on the order of\nO(MN) in each iteration to update density lists, where M is the number of\nclusters and N is the size of the unlabeled data. Instead, our method uses the\nbasic clustering algorithm and obtains a comparable performance."}, {"title": "5.5 Further Discussion", "content": "In this section, we evaluate the effect of our\nmethod on more segmentation tasks and datasets. Because MAE-VQGAN can-\nnot distinguish between individual instances of objects within the same class,\nwe construct more experiments with the SegGPT model. Specifically, we report\ninstance segmentation results on PASCAL-5i in Table 4 and aerial image seman-\ntic segmentation results on iSALD-5i in Table 5. The above results indicate that\nthe SCS can enhance the segmentation ability for various real-world scenarios."}, {"title": "6 Conlusion", "content": "In this paper, we focus on the example selection method for the ICL-based seg-\nmentation frameworks. By building comprehensive comparisons, we empirically\ndemonstrate that diversity of contexts is a key factor in guiding segmentation\nand boosting performance. Beyond the above insights, we propose a new step-\nwise context search method that improves the diversity of the candidate pool and\nsignificantly alleviates the annotation cost. We hope that this work can promote\nfurther research to better understand in-context learning in computer vision."}]}