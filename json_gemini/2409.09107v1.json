{"title": "PROACTIVE AND REACTIVE CONSTRAINT PROGRAMMING FOR STOCHASTIC PROJECT SCHEDULING WITH MAXIMAL TIME-LAGS", "authors": ["Kim van den Houten", "L\u00e9on Planken", "Esteban Freydell", "David Tax", "Mathijs de Weerdt"], "abstract": "This study investigates scheduling strategies for the stochastic resource-constrained project scheduling problem with maximal time lags (SRCPSP/max)). Recent advances in Constraint Programming (CP) and Temporal Networks have re-invoked interest in evaluating the advantages and drawbacks of various proactive and reactive scheduling methods. First, we present a new, CP-based fully proactive method. Second, we show how a reactive approach can be constructed using an online rescheduling procedure. A third contribution is based on partial order schedules and uses Simple Temporal Networks with Uncertainty (STNUs). Our statistical analysis shows that the STNU-based algorithm performs best in terms of solution quality, while also showing good relative offline and online computation time.", "sections": [{"title": "Introduction", "content": "In real-world scheduling applications, durations of activities are often stochastic, for example, due to the inherent stochastic nature of processes in biomanufacturing. At the same time, hard constraints must be satisfied: e.g. once fermentation starts, a cooling procedure must start at least (minimal time lag) 10 and at most (maximal time lag) 30 minutes later. The combination of maximal time lags and stochastic durations is especially tricky: a delay in duration can cause a violation of a maximal time lag when a resource becomes available later than expected. Such constraints are reflected in the Stochastic Resource-Constrained Project Scheduling Problem with Time Lags (SRCPSP/max). This problem has been an important focus of research due to its practical relevance and the computational challenge it presents, as finding a feasible solution is NP-hard.\nBroadly speaking, there are two main schools of thought regarding solution approaches for stochastic scheduling in the literature: 1) proactive scheduling and 2) reactive scheduling. The main goal of proactive scheduling is to find a robust schedule offline, whereas reactive approaches adapt to uncertainties online. Proactive and reactive approaches can be considered as opposite ends of a spectrum. Both in practice and literature, it is often observed that methods are hybrid, such as earlier work on the SRCPSP/max.\nHybrid approaches appear for example in the form of a partial order schedule (POS), which is a temporally flexible schedule in which resource feasibility is guaranteed. The state-of-the-art POS approach for SRCPSP/max is the algorithm BACCHUS , although the comparison provided by the authors themselves shows that their earlier proactive method SORU-H  performs better. Partial order schedules are often complemented with a temporal network , in which time points (nodes) are modeled together with temporal constraints (edges). Recent advances in temporal networks with uncertainties  pave the way for improvements in POS approaches for SRCPSP/max.\nState-of-the-art methods like BACCHUS and SORU-H use Mixed Integer Programming (MIP), whereas Constraint Programming (CP), especially with interval variables , has become a powerful alternative for scheduling. This is evidenced by a CP model for resource-constrained project scheduling provided by . Additionally, a recent comparison between solvers for CP and MIP demonstrated CP Optimizer's superiority over CPLEX across a broad set of benchmark scheduling problems . These advances, however, have not yet been explored for SRCPSP/max, despite potential applications. CP can be used for finding robust proactive schedules or within a reactive approach with rescheduling during execution, which has been viewed as too computationally heavy."}, {"title": "The Scheduling Problem", "content": "The RCPSP/max problem  is defined as a set of activities J and a set of resources R, where each activity j \u2208 J has a duration dj and requires from each resource r \u2208 R a certain amount indicated by rr,j. A solution to the RCPSP/ max is a start-time assignment sj for each activity, such that the following constraints are satisfied: (i) at any time the number of resources used cannot exceed the max resource capacity cr; and (ii) for some pairs (i, j) of activities, precedence constraints are defined as minimal or maximal time lags between the start time of i and the start time of j. The goal is to minimize the makespan of the schedule.\nWe use the following example from  of a simple RCPSP/max instance. There are five activities: a, b, c, d, and e, with start times sa, sb, sc, sd, and se. The activities have durations of 2, 5, 3, 1, and 2 time units, respectively, and resource requirements of 3, 2, 1, 2, and 2 on a single resource with a capacity of cr = 4. The precedence relations are as follows: activity b starts at least 2 time units after a starts, activity b starts at least 1 time unit before c starts, activity e cannot start later than 6 time units after a starts, and activity d starts exactly 3 time units before e starts. These constraints can be visualized in a project graph (Figure 1). A feasible solution to this problem is {$a = 1, sb = 3, sc = 4, sd = 0, se = 3}, for which the Gannt chart is visualized in Figure 2 (ignore the arrows for now).\nA special property of (S)RCPSP/max is the following:\nProposition 1. Suppose we are given a problem instance 1 with durations d\u00b9, resource requirements r\u00b9, and capac- ity c\u00b9, and let s\u00b9 be a feasible schedule for this instance. Suppose now that we transform instance 1 into instance 2, where all parameters stay equal except that one or more of the activity durations d\u00b2 are shorter than the durations d\u00b9, so \u2200 j \u2208 J : d \u2264 d\u00b2. Then, s\u00b9 is also feasible for instance 2.\nProof. Schedule s\u00b9 is still precedence feasible because the start times did not change and the precedence constraints are defined from start to start (they are deterministic). Schedule s\u00b9 is resource feasible for d\u00b9. Since d\u00b2 is strictly smaller than d\u00b9, the resource usage over time can only be smaller than for instance 1, and thus it will not exceed the capacity, and schedule s\u00b9 is also resource feasible for d\u00b2."}, {"title": "Background and Related Work", "content": "In this section, we discuss existing approaches for SRCPSP/max. Furthermore, we introduce all concepts that are needed to understand our scheduling methods that are presented in Section 4. We introduce proactive techniques based on Sample Average Approximation in Section 3.1. We explain partial order schedules and temporal networks in Section 3.2. Finally, Section 3.3 gives an overview of related work on benchmarking scheduling methods for SRCPSP/ max."}, {"title": "Proactive Scheduling", "content": "Proactive scheduling methods aim to find a robust schedule offline by taking information about the uncertainty into ac- count [HL02]. In this section, we explain a core technique used in proactive methods: Sample Average Approximation (SAA). Furthermore, we discuss the state-of-the-art proactive methods for SRCPSP/max."}, {"title": "Sample Average Approximation", "content": "A common method for handling discrete optimization under uncertainty is the Sample Average Approximation (SAA) approach . Samples are drawn from stochastic distributions and added as scenarios to a stochastic pro- gramming formulation. The solver then seeks a solution feasible for all scenarios while optimizing the average ob- jective. However, adding more samples to the SAA increases the number of constraints and variables, significantly raising the solution time."}, {"title": "SORU and SORU-H", "content": "The most recent proactive method on SRCPSP/max is proposed by  and is recognized as the state of the art. The authors present the algorithm SORU, an SAA approach to the scheduling problem that relies on Mixed Integer Programming (MIP) and aims to minimize the a-robust makespan. It can be summarized as follows: (i) a selection of samples is used to set up the SAA; (ii) the model seeks a start time vector s such that the minimal and maximal time lags of precedence constraints are satisfied; (iii) it allows for a% of the scenarios to be resource infeasible; (iv) it minimizes the sample average makespan.\nSince SORU is computationally expensive, the authors also propose a heuristic version, dubbed SORU-H. Instead of a set of samples, one single summarizing sample is used, which represents a quantile of the distribution. Note that because of Proposition 1, this heuristic approximates the a-robust makespan. At the same time, it is much cheaper to compute because typically the runtime increases for a larger sample size in SAA, as shown by .\nSince nowadays CP is the state of the art for deterministic project scheduling , we re-investigate SAA ap- proaches for SRCPSP/max with CP. We expect that the scalability of SAA is much less of a problem than it was when the SORU methods were presented."}, {"title": "Partial Order Scheduling", "content": "In this section, we discuss partial order scheduling approaches, which can be seen as a reactive-proactive hybrid. Partial order schedules have been used in the majority of the contributions to SRCPSP/max."}, {"title": "Constructing Ordering Constraints", "content": "A partial order schedule (POS) can be seen as a collection of schedules that ensure resource feasibility, but maintain temporal flexibility. A POS is defined as a graph where nodes represent activities, and edges temporal constraints between them. There are several methods to derive a POS. In the original paper by , two approaches are outlined to construct the ordering constraints between activities, either analyzing the resource profile to avoid all possible resource conflicts, based on Minimal Critical Sets (MCS) , or using a fixed resource-feasible schedule together with a chaining procedure to construct resource chains (see Figure 2). They find that using the single-point"}, {"title": "Temporal Networks", "content": "Partial order schedules are often complemented with a temporal model to reason over the temporal constraints. Most POS approaches rely on a Simple Temporal Network (STN), which is a graph consisting of time points (nodes) and temporal difference constraints (edges). The Simple Temporal Network with Uncertainty (STNU) extends the STN by introducing contingent links. The duration of these contingent constraints can only be observed, while for regular constraints it can be determined during execution. The works by  and  on POS are the only ones that we know of using STNUs as their temporal model. They introduce nodes for the start and the end of each activity with duration constraints between them, connecting respective start nodes with edges for the precedence constraints."}, {"title": "Execution Strategies", "content": "An STNU is dynamically controllable (DC) if there is a strategy to determine execution times for all controllable (non-contingent) time points which ensures that all temporal constraints are met, regardless of the outcomes of the contingent links. Some DC-checking algorithms generate new so-called wait edges to make the network DC . The DC STNU with wait edges is referred to as an Extended STNU (ESTNU) and can be given to a Real-Time Execution Algorithm. Such an algorithm is the online component that transforms an STNU into a schedule (i.e. an execution time for each time point), given the observations for the contingent time points. An algorithm specifically tailored to ESTNUs is RTE* .\nAs far as we know, the DC-checking and RTE* algorithms have not been applied to SRCPSP/max, despite their efficiency.  used constraint propagation for DC-checking, but do not use RTE*. Other works on POS for SRCPSP/max that do not use STNUs and DC-checking risk violations of minimal or maximal time-lags during execution . Thus, we conclude that there is a research gap in applying the developments in STNU literature to SRCPSP/max."}, {"title": "Benchmarking Approaches", "content": "Benchmarking procedures for comparing scheduling methods are inconsistent in the literature, with varying problem sets and comparison methods. Some studies used industrial scheduling instances , while the major- ity  relied on PSPlib  instances, which are deterministic and transformed into stochastic versions with noise. Research typically focused on instances with 10, 20, and 30 activities (j10-j30). Different studies assessed varying metrics, such as schedule flexibility and robustness , solver performance , or the a-robust makespan . However, no comprehensive benchmarking paper exists that evaluates both solution quality and computation time while also correctly accounting for infeasibilities. We take inspiration from , who compare different planners that can fail, providing a frame- work for comparing solution quality and speed while also correctly considering correctly considering these failures."}, {"title": "Statistical Tests for Pairwise Comparison", "content": "A strategy for benchmarking is to provide partial orderings of the scheduling methods for the different metrics of interest (i.e. solution quality, runtime offline, runtime online). A partial ordering can be obtained by executing pairwise comparisons of the methods per problem size and per metric, taking inspiration from . \nThe Wilcoxon Matched-Pairs Rank-Sum Test (the version by ) looks at the ranking of absolute differences and gives insight into which of a pair of methods has consistently better performance than another method. Infeasible cases can be handled by assigning infinitely bad time and solution quality to these cases, leading to an absolute difference of or- that will be pushed to the highest and lowest rankings.\nAn alternative test to the Wilcoxon test that is also used by  is the proportion test (see Test 4 in the book by ). This test is weaker than the Wilcoxon, but provides at least information about significance in the proportion of wins when the Wilcoxon test shows no significant difference.\nAn interesting additional test is to test whether there is a significant difference in the magnitude of the different metrics. This can be tested with a pairwise t-test on two related samples of scores (see Test 10 in the book by ). This test can only be performed on double hits, because infinitely bad computation time or solution quality will disturb the test.\nWe provide a detailed explanation of all of the above statistical tests in our Technical Appendix."}, {"title": "Scheduling Methods", "content": "This section outlines the proposed methods. We explain how to use CP for these scheduling problems so far dominated by MIP approaches.\nFirst, we present a deterministic CP model for RCPSP/max in Section 4.1. The new, stochastic methods for SRCPSP/ max are proposed in Section 4.2. We explain the statistical tests for performing pairwise comparisons of these new methods that lead to partial orderings based on solution quality and computation time in Section 4.3."}, {"title": "Constraint Programming for RCPSP/max", "content": "The CP model is:\n$\\begin{aligned}\n&\\text{Min} \\quad \\text{Makespan}  \\\\ &\\text{s.t} \\\\\n&\\qquad \\max\\{end(x_j)\\} < \\text{Makespan} \\quad \\forall j \\in J  \\\\ &\\qquad start(x_i) \\geq min_{j,i} + start(x_j); \\quad \\forall j \\in J \\ \\forall i \\in S_j \\\\\n&\\qquad start(x_i) \\leq max_{j,i} + start(x_j); \\quad \\forall j \\in J \\ \\forall i \\in S_j  \\\\ &\\qquad \\sum_{j \\in J} Pulse(x_j, r_{r,j}) \\leq c_r \\quad \\forall r \\in R \\\\\n&\\qquad x_j: IntervalVar(J, d_j) \\quad \\forall j \\in J\n\\end{aligned}$\nFor the deterministic RCPSP/max, we use the modern interval constraints from the IBM CP optimizer . The CP model is defined in equations 1a-1f, we use IBM's syntax and modify the RCPSP example from  to RCPSP/max. We use the earlier introduced nomenclature together with the minimal time lags minj,i and maximal time lags maxj,i that are the temporal differences between start times of activities j and i if i is a successor of j. We introduce the decision variable xj as the interval variable for activity j\u2208 J. The Pulse function generates a cumulative expression over a given interval xj with a certain value. For a task j, this value is its resource usage rr,j. The aggregated pulse values are constrained so that their sum does not exceed the total available resource capacity Cr."}, {"title": "New Methods for SRCPSP/max", "content": "This section introduces three new methods for SRCPSP/max. The first method is a CP-based version of a proactive model. Then, we present a novel, fully reactive scheduling approach employing the deterministic model for RCPSP/ max. Finally, we propose an STNU-based approach using CP and POS. We refer to these approaches as proactive, reactive, and stnu, respectively."}, {"title": "Proactive Method", "content": "We outline how to use a scenario-based CP model (instead of MIP) for SRCPSP/max which we call proactivesaa.\nFor this SAA method, we can reuse the deterministic RCPSP/max CP model, introduced in Section 4.1, but we introduce scenarios. This model is inspired by the MIP version by . A special variant is the SAA with only one sample, for which a y-quantile can be used which we call proactive0.9. If a feasible schedule can be found for the y-quantile, this schedule will also be feasible for all duration realizations on the left-hand side of the y-quantile because of Proposition 1. We provide the SAA model below. We use the same nomenclature as for the deterministic model, but we introduce the notion of scenarios w \u2208 \u03a9, and find a schedule  that is feasible for all scenarios it has seen if one exists."}, {"title": "Reactive Method", "content": "We now present a CP-based fully reactive approach for SRCPSP/max that we refer to as reactive.\nFully reactive approaches, involving complete rescheduling by solving a deterministic RCPSP/max, have been con- sidered impractical due to high computational demands and low schedule stability . However, advances in CP for scheduling  mitigate these issues. In practice, decision-makers often reschedule their entire future plan when changes occur. Thus, including this reactive approach in our comparison is valuable. To our knowledge, such an approach has not been evaluated before. The outline is:\n\u2022 Start by making an initial schedule with an estimation of the activity durations d. We can see d as a hyperpa- rameter for how conservative the estimation is. For example, using the mean of the distribution could lead to better makespans, but the risk of becoming infeasible for larger duration realizations, while taking the upper bound of the distribution could lead to a very high makespan.\n\u2022 At every decision moment (i.e. when an activity finishes) resolve the deterministic RCPSP/max while fixing all variables until the current time to reschedule with new information, we again use the estimation d for the activities that did not finish yet. Resolving is only needed when the finish time of an activity deviates from the estimated finish time. We warm start the solver with the previous solution."}, {"title": "STNU-based Method", "content": "Finally, we present a partial order schedule approach using CP and STNU algorithms  which we call stnu. This approach is inspired by many earlier works . We use a fixed-solution approach for constructing the ordering constraints. The outline of the approach is:\n1. We make a fixed-point schedule by solving the deterministic RCPSP/max with an estimation of the activity durations d and using the chaining procedure, which was explained in Section 3.2.\n2. We construct the STNU as follows:\n\u2022 For each activity two nodes are created, representing its start and end.\n\u2022 Contingent links are included between the start of the activity and the end of the activity with [LB, UB], where LB and UB are the lower and upper bounds of the duration of that activity.\n\u2022 The minimal and maximal time lags are modeled using edges  and , where A and B are the preceding and succeeding activity, respectively. These edges correspond to the edges in the precedence graph of the instance, see Figure 1.\n\u2022 The resource chains that construct the POS are added as additional edges as  if activity A precedes activity B. Each arrow in Figure 2 would lead to a resource chain edge.\nThe resulting STNU is tested for dynamic controllability (DC), and if the network is DC its extended form (ESTNU) is given to the RTE* algorithm. Since makespan minimization is of interest, we slightly adjust the algorithm from : instead of selecting an arbitrary executable time point and an arbitrary allowed execution time, we always choose the earliest possible time point at the earliest possible execution time."}, {"title": "Statistical Tests for Pairwise Comparison", "content": "A strategy for benchmarking is to provide partial orderings of the scheduling methods for the different metrics of interest (i.e. solution quality, runtime offline, runtime online). A partial ordering can be obtained by executing pairwise comparisons of the methods per problem size and per metric, taking inspiration from . \nThe Wilcoxon Matched-Pairs Rank-Sum Test (the version by ) looks at the ranking of absolute differences and gives insight into which of a pair of methods has consistently better performance than another method. Infeasible cases can be handled by assigning infinitely bad time and solution quality to these cases, leading to an absolute difference of \u221e or \u2212\u221e that will be pushed to the highest and lowest rankings.\nAn alternative test to the Wilcoxon test that is also used by  is the proportion test (see Test 4 in the book by ). This test is weaker than the Wilcoxon, but provides at least information about significance in the proportion of wins when the Wilcoxon test shows no significant difference.\nAn interesting additional test is to test whether there is a significant difference in the magnitude of the different metrics. This can be tested with a pairwise t-test on two related samples of scores (see Test 10 in the book by ). This test can only be performed on double hits, because infinitely bad computation time or solution quality will disturb the test.\nWe provide a detailed explanation of all of the above statistical tests in our Technical Appendix."}, {"title": "Results Statistical Tests", "content": "shows a subset of the pairwise test results for the metrics solution quality, runtime offline, and runtime online. In our Technical Appendix, we provide all test results per instance set / noise level c setting. The outcomes of the test results can be used to make partial orderings of the methods, distinguishing between a strong partial ordering (Wilcoxon test) and a weak ordering (proportion test)."}, {"title": "Partial Ordering Visualization", "content": "In Figure 3\u20135, an arrow A \u2192 B indicates that in the majority of the settings either A is consistently better than B (Wilcoxon) and/or A is better than B a significant number of times (proportion). Due to space constraints and for clarity, we have chosen to display only the most common pattern per metric rather than all partial orderings per instance set and noise level. Consequently, the distinction between strong and weak partial orderings is omitted in these figures. We refer to the Technical Appendix for the partial orderings per setting including the distinction between a strong and weak partial ordering."}, {"title": "Analysis", "content": "Figure 3 shows the visualization of the partial ordering for solution quality (makespan). The results are consistent for the different instance sets and noise levels. The stnu shows to be the outperforming method based on solution quality. The reactive approach outperforms the proactive methods. Furthermore, proactivesaa outperforms in many cases proactive0.9, although for larger instances and a higher noise level a significant difference is not present. In earlier work, proactive approaches were considered state-of-the-art, but in our analysis, we found better makespan results for the STNU-based approach. We found that for each pair for which an arrow is visualized in Figure 3 also a significant magnitude difference was found on the double hits."}, {"title": "Reproducibility", "content": "Together with this article, we provide a Technical Appendix in which we include explanations of the statistical tests, hyperparameter tuning, tables with test results from all pairwise comparisons that led to the partial orderings in Figure 3, 4 and 5. Furthermore, we provide a zip file comprising our code that can be used to rerun all experiments and statistical tests. In the camera-ready version of this paper, this section will refer to a public GitHub repository of our code."}, {"title": "Conclusion and Future Work", "content": "This study proposes new scheduling methods for SRCPSP/max using the latest advances in CP and STNUs. We conducted an extensive, statistical analysis to compare the proposed algorithms for the problem instances and target the existing research gap of a lacking benchmarking paper for this problem.\nUntil now, a MIP-based proactive method was considered the best method for SRCPSP/max, although partial or- der schedules have shown potential in earlier research. We found that proactive methods can be improved with online rescheduling, resulting in better solution quality for the method reactive compared to proactive approaches proactivesaa and proactive0.9. We find that the algorithm stnu that uses partial order schedules outperforms the other methods on solution quality in our evaluation. Although in general, proactive and reactive have better offline com- putation time than stnu, and proactivesaa and proactive0.9 have better online computation time than stnu, the stnu also showed good relative runtime results due to the polynomial time STNU-related algorithms.\nIn future work, the same approach could be used to evaluate other scheduling problems, and we can gain more in- sight into how these methods perform on both well-known problems from the literature and in practical situations. Furthermore, the set of methods could even be broadened by including sequential methods such as those provided by [Hal22] and/or machine learning-based approaches such as the graph neural network approach by  that was developed for stochastic RCPSP."}, {"title": "Tuning", "content": "This section describes the tuning process for the different scheduling methods. First, we investigated the effect of the time limit on solving the deterministic CP. Then, for the proactive approach, we tuned the time limit and the sample selection for the Sample Average Approximation (SAA). For reactive, we tuned the quantile approximation and the time limit for rescheduling."}, {"title": "Time limit for CP on deterministic instances", "content": "To understand how the time limit may affect the results, we first consider the deterministic instances of RCPSP/max. [VFL16] reported on their time budget that SORU was able to obtain solutions within 5 minutes for every one instance in J10 and 2 hrs for the J20 instances. However, for J30, we were unable to get optimal solutions for certain instances in the cut-off limit of 3 hrs. On the other hand, SORU-H was able to generate solutions for J10 instances within half of a second, J20 instances within 10 seconds, and J30 instances within 10 minutes on average.\nIn our research, we extend the instance sets with ubo50 and ubo100, for which the time limit can become more crucial. For each set (j10-ubo100), we fixed the first 50 instances from the PSPlib [KS96] (PSP1 - PSP50). For j10-j30, the IBM CP Optimizer could solve all instances (PSP1-PSP50) within 60 seconds to optimality or prove infeasibility.\nFor the ubo50 instances PSP1 to PSP10 and the ubo100 instances PSP1 to PSP10, the effect of the time limits of 60 seconds, 600 seconds, and 3600 seconds respectively, is presented in Table 3 and 4. We observe that the solver status does not change when increasing the time limit to 600 seconds, and only flips 1 instance from feasible to infeasible when increasing the time limit from 600 seconds to 3600 seconds. The makespan only improves significantly for the ubo100 instances for higher time limits. In the remaining experiments, we fixed the time limit to 60 seconds for solving deterministic CPs."}, {"title": "Proactive Approach", "content": "The sample selection process is expected to influence the performance of the proactive method. We used a subset of the j10 and ubo50 instances (for both sets PSP_1-PSP_20, and used 10 duration samples per instance. We fixed the time limit to 60 seconds and compared the feasibility ratios in Table 5. We found that the higher feasibility ratios were obtained using two settings for the proactive method, being 1 sample with y = 0.9, to which we will refer with proactive0.9, and the setting with the smart samples to which we will refer with proactivesaa in the remaining of our experiments. For the SAA with four samples (smart samples), we investigated the effect of the time limit on the makespan in Table 6. We observed that the makespan would still improve while making the step from 600 seconds to 3600 seconds. We decide to use a time limit of 1800 seconds instead of 3600 seconds in the remaining of the experiments to be able to conduct more experiments."}, {"title": "Reactive Approach", "content": "First, we observed the effect of the duration estimations on the performance of reactive. We used a subset of the j10 and ubo50 instances (for both sets PSP_1-PSP_20, and used 10 duration samples per instance. We fixed the time limit for the initial schedule to 60 seconds and for the rescheduling to 2 seconds.\nRemarkably, we observe similar feasibility ratios to the proactive approach, indicating that for feasibility the initial schedule is quite important. For the final evaluation, we fixed y = 0.9 for reactive, and will analyze how the solution quality improves with the rescheduling procedure compared to a standard proactive approach.\nNext, we observe the effect of the time limit for rescheduling. We used the same subset of the j10 and ubo50 instances (for both sets PSP_1-PSP_20, used 10 duration samples per instance, and runtime limits of 1, 2, 10 and 30 seconds. The results (in Table 7) are almost similar for the different time limits, this might be because the solver finishes already before the time limit, and the increase in time online has mainly to do with the number of solver calls. In the experimental evaluation, we therefore fixed the rescheduling time limit to 2 seconds, which we expected to be sufficient for larger or slightly more complicated problems."}, {"title": "Hyperparameters Selection", "content": "This subsection presents the hyperparameters in Table 9 that are used in the final experiments that are also presented in the main paper."}, {"title": "Wilcoxon Test", "content": "The Wilcoxon test that we use follows [Cur67] and is described below:\nCollect a set of matched pairs (the results from two different methods on one instance sample).\nCompute the difference between the two test results for each pair.\nAn important remark is that because of the discrete objective values (makespan), we can obtain a zero difference when there is a tie, we use Pratts procedure for handling ties [Pra59], which includes zero-differences in the ranking process, but drops the ranks of the zeros afterward.\nOrder the pairs according to the absolute values of the differences.\nAssign ranks to the pairs based on these absolute values.\nSum the positive ranks (Tpos) and the negative (Tneg) ranks separately.\nTake the smaller of the two T = min(Tpos, Tneg).\nIf the two methods have no consistent difference in their relative performances, then the rank-sums should be approximately equal. This is tested with a normal approximation for the Wilcoxon statistic which is outlined by [Cur67]. [Cur67] propose a corrected normal approximation which is needed because of usage of the the Pratt procedure for handling zero differences.\nThe normalized Z-statistic is given by the formula: z = (T d)/se, where d is the continuity correction from [Cur67], and se is the standard error.\nAll of the above can be executed using the Python package SciPy [VGO+20] built-in method scipy.stats.wilcoxon that is called with parameters method=approx, zero_method=pratt, and correction=True."}, {"title": "Z-test for Proportion (Binomial Distribution)", "content": "We use [Kan06] as a reference for the Z-test for Proportion. It is important to mention that this test is approximate as it assumes that the number of observations justifies a normal approximation for the binomial. (In contradiction to the SciPy package and its built-in method scipy.stats.binomtest containing an exact test).\nThe proportion test investigates whether the is a significant difference between the assumed proportion of wins p0 and the observed proportion of wins p. In our analysis, two methods are compared and the number of wins for each method is counted based on one metric.\nThe procedure for the proportion test is as follows:\nCollect a set of matched pairs (the results from two different methods on one atomic instance form a pair).\nFor each pair, determine which method wins, and count the wins for both methods. Exclude all ties.\nCalculate the ratio of wins for one of the two methods.\nTest where this ratio differs significantly from p0 = 0.5 (equal probability of winning).\nThe test statistic is $Z = \\frac{p-p_{0}-\\frac{1}{2n}}{\\{\\frac{p_{0}(1-p_{0})}{n}\\}^{1/2}\\}$\nThe term $\\frac{1}{2n}$ in the numerator is a discontinuity correction.\nFor a two-sided test with a significance level \u03b1 = 0.05 the acceptance region for the null hypothesis is -1.96 < Z < 1.96."}, {"title": "Magnitude Test", "content": "The magnitude test we use is a t-test for two population means, or the method of paired comparisons such as Test 10 in the book by [Kan06]. The test whether there is a significant difference between two population means. The procedure for this paired comparison t-test is as follows:\nCollect a set of matched pairs (so the results from two different methods on one atomic instance forms a pair).\nNormalise the performances for each pair by computing the mean value of the pair and dividing the two items in the pair by the pairs mean such that all normalized observations will be between 0 and 2, and 1 indicates a tie.\nCompute the differences $d_i$ between the two test results for each pair i.\nCompute the variances of the differences with the following formula: $s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(d_i - \\overline{d})^2$\nCompute the means of both methods $\\overline{x}_1$ and $\\overline{x}_2$.\nCompute the t-statistic using the formula: $t = \\frac{\\overline{x}_1-\\overline{x}_2}{s/\\sqrt{n}}$\nTest significance by checking whether t lies within the acceptance region for which the values are given by the Students t-distribution (two-sided) with n\u22121 degrees of freedom.\nAfter the normalization step, it is possible to execute the test with the Scipy package and specifically, its built-in method scipy.stats.ttest_rel."}, {"title": "Results Tables", "content": "Please find the results of the statistical test in this section. This data led to the Figures that are included in our main paper.\nTables 12 for noise level c = 1 and 14 for noise level c = 2: including the results of the Wilcoxon and proportion tests on solution quality.\nTables 13 for noise level c = 1 and 15 for noise level c = 2: including the results of the magnitude test (independent t-test) on solution quality.\nTables 16 for noise level c = 1 and 18 for noise level c = 2: including the results of the Wilcoxon and proportion tests on time offline.\nTables 17 for noise level c = 1 and 19 for noise level c = 2: including the results of the magnitude test (independent t-test) on time offline.\nTables 20 and Tables 22: including the results of the Wilcoxon and proportion tests on time online.\nTables 21"}]}