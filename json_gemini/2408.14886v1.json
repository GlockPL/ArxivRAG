{"title": "The VoxCeleb Speaker Recognition Challenge: A Retrospective", "authors": ["Jaesung Huh", "Joon Son Chung", "Arsha Nagrani", "Andrew Brown", "Jee-weon Jung", "Daniel Garcia-Romero", "Andrew Zisserman"], "abstract": "The VoxCeleb Speaker Recognition Challenges (VoxSRC) were a series of challenges and workshops that ran annually from 2019 to 2023. The challenges primarily evaluated the tasks of speaker recognition and diarisation under various settings including: closed and open training data; as well as supervised, self-supervised, and semi-supervised training for domain adaptation. The challenges also provided publicly available training and evaluation datasets for each task and setting, with new test sets released each year.\nIn this paper, we provide a review of these challenges that covers: what they explored; the methods developed by the challenge participants and how these evolved; and also the current state of the field for speaker verification and diarisation. We chart the progress in performance over the five installments of the challenge on a common evaluation dataset and provide a detailed analysis of how each year's special focus affected participants' performance.\nThis paper is aimed both at researchers who want an overview of the speaker recognition and diarisation field, and also at challenge organisers who want to benefit from the successes and avoid the mistakes of the VoxSRC challenges. We end with a discussion of the current strengths of the field and open challenges. Project page : https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/workshop.html", "sections": [{"title": "I. INTRODUCTION", "content": "The VoxCeleb Speaker Recognition Challenges (VoxSRC) were a series of challenges held annually from 2019 to 2023 [1]\u2013[4]. The goals of the challenges were threefold: (i) to explore and promote novel research in the field of speaker recognition and diarisation, encouraging important directions such as self-supervised learning and domain adaptation; (ii) to measure and calibrate the state of the art through public evaluation tools; and (iii) to provide free and open-source data to the community, accessible to all. The primary tasks were speaker verification (\u201cdo these two speech segments come from the same speaker?\u201d) and speaker diarisation (\u201clabel a multi-speaker segment with who speaks when\u201d). VoxSRC consisted of an annual competition and workshop (co-located with the Interspeech conference), where each year's results and methods were discussed.\nWhen the challenge was first introduced in 2019, there were already a number of noteworthy speaker recognition challenges, such as those organised by the National Institute in Standards of Technology (NIST) [5]\u2013[9], and Speakers In the Wild (SITW) [10] and speaker diarisation challenges, such as DIHARD [11]. While these challenges provide immeasurable value to the community, the goal of VoxSRC was to provide complementary support in the form of open-source contributions \u2013 all the training and validation data was (and will continue to be) free and available to researchers irrespective of whether they enter the challenges or not, and evaluation is performed via a public leaderboard that is visible to all. VoxSRC workshops were also free for participants to attend. The focus of VoxSRC has been on unconstrained speech from the web, with data consisting of noisy, varied and sometimes very short and fleeting speech segments.\nThis paper serves as a retrospective on all five VoxSRC challenges, including their mechanics, methods, results, and discussion. We hope that this paper will be useful for two audiences: (i) speaker recognition researchers aiming to determine what the state of the art in the field has been over the last five years, as measured by performance on the VoxSRC datasets, as well as wishing to understand how best practises and techniques have evolved during this period; (ii) challenge organisers, for whom we hope our learnings serve as a useful guide for the organisation of future challenges. The structure of this report is as follows: we begin by describing the two primary tasks, speaker verification and speaker diarisation and tracks we have hosted over the last five years (Section II). We then describe the datasets (Section III), and challenge mechanics (Section IV), followed by results for each track (Section V) and a detailed analysis of the winners' method (Section VI). We describe how we hosted the workshops (Section VII), and finally we reflect on the trends over the years of the challenge, discuss current challenges in speaker recognition, and conclude with lessons for future challenge organisers (Section VIII)."}, {"title": "II. TASKS AND TRACKS", "content": "This section provides an overview of the tracks hosted over the past five years, covering two primary challenge tasks: speaker verification and speaker diarisation. Within the speaker verification task, there were four different tracks over the last four years depending on the type of data participants are allowed to use. In the following, we first specify the task description, and then describe the dataset con- straints and the motivation behind each of the tracks. Please also refer to Figure 1 which shows the progression of the VoxSRC workshops."}, {"title": "A. Speaker verification (2019\u20132023)", "content": "The speaker verification task is to determine whether two utterances are spoken by the same speaker or not. The evaluation is conducted using a list of utterance pairs (i.e. trials), and each trial is processed independently. VoxSRC participants submitted a real-valued prediction score for each trial pair so that evaluation could be performed using our metrics (See Section IV). Four tracks were defined based on the choice of training data: (i) Speaker verification \u2013 closed, (ii) Speaker verification \u2013 open, (iii) Self-supervised speaker verification, and (iv) Semi-supervised domain adaptation.\n1) Speaker verification \u2013 closed (2019\u20132023): This has been the main challenge track since the first VoxSRC challenge in 2019. It is closed in that for training their systems, participants were only allowed to use the publicly available VoxCeleb2 [12] dev set, which spans 1,092,009 utterances from 5,994 different speakers. The motivation here was to enable comparisons between training algorithms and model architecture approaches while keeping the data fixed. There were two reasons for choosing only VoxCeleb2 dev set and not the rest of VoxCeleb. First, we used the utterances from VoxCeleb1 as the validation set, and we did not want to have any overlap between train and validation set in our setting. Second, the size of VoxCeleb2 dev set is adequate to train large models. The details of VoxCeleb2 are described in Section III.\n2) Speaker verification \u2013 open (2019\u20132023): In this track, par- ticipants were permitted to use any other data in addition to the Vox-Celeb2 dev set for training except the test data. Regardless of whether the training data is public or not, participants here were encouraged to achieve state-of-the-art performance, pushing the limit every year. The test data was the same as that used for the closed track to quantify the effect of external training data on speaker models.\n3) Self-supervised speaker verification (2020\u20132021): In response to the progress of self-supervised learning approaches in diverse domains such as vision [13]\u2013[15] and natural language processing [16], [17], we introduced a self-supervised speaker verification track to investigate methods for training speaker verification models without labels. Participants in this track were only allowed to use the Vox-Celeb2 dev set without labels to train the model. The test data was identical to the previous two tracks, which allowed the performance gap to be studied between methods training with and without labels.\n4) Semi-supervised domain adaptation (2022-2023): This do- main adaptation track aimed to assess how models pretrained on large labelled data in a source domain can adapt to a new target domain, given (i) a large set of unlabelled data from the target domain and (ii) a small set of labelled data from the target domain. This setting was especially relevant to low-resource real-world scenarios, where large-scale labelled data in the source domain, and a small set of labelled data in the target domain are available in addition to large-scale unlabelled data in the target domain. Specifically, we focused on the task of speaker verification from one language which is the source domain (mainly English), to a different language in a target domain (Chinese). Here we used VoxCeleb2 [12] as the source domain and CNCeleb [18] as the target domain. Participants were allowed to use the VoxCeleb2 dev set with speaker labels, a large subset of CNCeleb without speaker labels which contains 454,946 utterances from 1,807 speakers, and a small subset of CNCeleb with speaker labels which consists of 1,000 utterances from 50 speakers."}, {"title": "B. Speaker diarisation (2020-2023)", "content": "The speaker diarisation task is to identify \"who speaks when\u201d given audio containing single or multi-speaker speech segments. Participants were required to (i) identify speaker regions, and (ii) assign a speaker label to each region in the audio file. Participants did not need to match speech segments to specific known speakers; instead, they could simply cluster the segments according to different speakers. Participants were allowed to use any data to train their models except the test data."}, {"title": "III. DATASETS", "content": "This section describes the training, validation and test sets used in the challenges. Each year, we created (i) validation sets for which labels were available to the public, and (ii) test sets constructed from hidden data to make the task challenging and interesting, for which labels were not available publicly. For each track, the validation set featured a data distribution similar to that of the corresponding test set. This section details the datasets used each year, encompassing both publicly available and organiser-created hidden datasets. Additionally, we include a discussion of the annotation methods. We show the training set for each track in Table I. The complete statistics of the validation and test sets each year are given in the Appendix A.\nDuring the VoxSRC challenges, the labels of the test sets were not released. However, following the end of the challenges, they have now been released for the community."}, {"title": "A. Speaker verification (Tracks 1 and 2)", "content": "The challenge datasets were based on VoxCeleb [12], [19], [20], a large-scale speaker recognition dataset comprising utterances from celebrities, sampled from interviews and TV shows on YouTube. The dataset was created by extracting multiple single-speaker utterances from each YouTube video using face tracking [21], face verification [22] and active speaker detection [23]. This dataset consists of two versions. VoxCeleb1 [19] includes over 150,000 utterances from 1,251 speakers in 22,496 unique recordings. The audio dataset covers diverse background environments and recording conditions. VoxCeleb2 [12] expands the initial version to a larger scale, containing over a million utterances from 6,112 speakers in 150,480 recordings, making it five times larger than VoxCeleb1.\n1) Training set: For the closed track (Track 1), participants were only allowed to use the VoxCeleb2 dev set for training their systems. The dev set contains 1,092,009 utterances from 5,994 speakers. For the open track (Track 2), participants could use any data except the challenge test set. The training set on these two tracks remained the same for all five years.\n2) Validation set: The validation sets for all five years were based on VoxCeleb1. In addition, the validation set for 2020 and 2023 challenges made use of audio clips from the VoxMovies dataset, and the 2022 and 2023 challenges utilised audio segments from the VoxConverse dataset.\n3) Test set: The test data was created from YouTube videos in the same way as the training and validation sets, but they came from identities that do not appear in VoxCeleb1 or VoxCeleb2. The test data was checked manually for any errors using the same procedure described in [19]. This was done using a simple web-based tool that shows all video segments for each identity. To monitor the performance improvements over time, the VoxSRC 2019 test trials were always included in the test sets of all subsequent years.\n4) Themes: Each year's challenge was designed to emphasise important research directions. The following paragraphs provide additional information on how the data was collected for each challenge. Speakers at the Movies (2020). In VoxSRC 2020, we introduced a new set of speaker segments from movie material, namely the VoxMovies dataset [24], into the validation and test sets, which served as out-of-domain data for some of the identities in VoxCeleb. VoxMovies is a speaker recognition dataset comprising utterances from movies, collected using a similar pipeline to VoxCeleb. It poses greater challenges than VoxCeleb because actors tend to disguise their voices and put more emotions in their speech in movies compared to in interviews. The utterances vary in emotion, accent, and background noise and therefore come from an entirely different domain to the VoxCeleb utterances, which mostly contain celebrity interviews.\nUtterances trials were constructed from the hidden sets of VoxMovies and VoxCeleb to create more challenging test pairs. Pairs were constructed in two different ways: (i) both utterances originated from either VoxCeleb or VoxMovies; or (ii) one utterance from VoxCeleb and the other from VoxMovies.\nCross-lingual pairs (2021). In VoxSRC 2021, multi-lingual verification pairs were introduced into the speaker verification validation and test data. The goals of this were twofold: first to promote the fairness and accessibility of speaker verification models, so as to allow people from diverse language groups to use these deep learning models; and second, to provide a more challenging test set for the speaker verification tracks.\nDue to the design of the dataset collection pipeline [20], the Vox-Celeb datasets consist of mainly English-speaking speech segments. In the validation and test sets in 2021, a multi-lingual focus was added by sampling more positive and negative pairs that contain non-English speech segments. This required the use of language labels, which do not exist for the VoxCeleb datasets. We obtained the language labels using a three-step pipeline consisting of a combination of automatic and manual annotation. First, we obtained automatic language predictions for each VoxCeleb video using a model trained on VoxLingua107 [25] to make language predictions across 107 languages. We assumed each speaker in a video uses only one language and randomly selected one utterance per video together with its predicted language label. Second, we manually annotated the correctness of the language predictions for the 12 most frequently occurring languages using a customised LISA annotation tool [26]. The 12 languages include English, French, Dutch, Italian, German, Spanish, Hindi, Portuguese, Russian, Chinese, Japanese, and Korean. Third, we used these manual annotations to obtain language-specific classification thresholds for the automatic predictions. These thresholds were then applied to classify each speech segment in VoxCeleb1 as either one or none of these 12 languages.\nHard positive pairs with large age gaps (2022). We curated hard positive validation and test pairs where the age of the speaker differs considerably between the two utterances. These hard positives were found by selecting utterance pairs from the same speaker that have a large age gap (i.e. two audio files for the same identity where the age is very different) via a two-step process in the hidden video data. In this data, for each video segment, we had the face location, the identity and the audio recording. First, the age of the speaker was estimated by predicting the age for a random set of frames, using an open-source face age prediction network [27], and averaging the result. Second, we sampled positive pairs from utterances for the same speaker with large age gaps. We performed this process within VoxCeleb1 to create the validation set, and within the hidden data to create the test set.\nHard negative pairs that share the same background noise (2022). We constructed hard negative validation and test pairs by sampling utterances from different speakers within the same video that therefore share very similar acoustic conditions. Most of the negative pairs in the VoxCeleb training datasets are from different videos, therefore speaker verification systems might be able to use the environmental cues as a shortcut for speaker prediction. Our goal here was to construct harder negative pairs by sampling utterances from different speakers that are sourced from the same video. In this case, the environmental conditions are shared across the two utterances and only the speaker's identity changes. We sampled the hard negative pairs using the VoxConverse [28] speaker diarisation dataset, where each audio file consists of multiple short speech segments from different speakers [29]. To generate these trials, we first cropped short speech segments. We then removed segments that were either too short (<1.5s) or had overlapping speech. Finally, we selected trials using two segments from different speakers within the same audio file.\nCombination of all themes (2023). In the final year, we composed a test set including all scenarios aforementioned, making it the most comprehensive test set. This includes all test pairs from the years 2019, 2021, 2022, and 2023, as well as approximately 200,000 randomly selected pairs from 2020. To avoid redundancy, we ensured that each pair in the VoxSRC 2019 test set was included only once. Note, only 200,000 of the approximately 1.7 million pairs in the VoxSRC2020 test set are included in order to balance the number of test pairs for each year, and also to reduce the computational expense of evaluation."}, {"title": "B. Self-supervised speaker verification (Track 3 in 2020 and 2021)", "content": "For the self-supervised track introduced in 2020, participants were allowed to use VoxCeleb2 dev set without labels. Training, validation and test sets are otherwise identical to Track 1 and 2 in the respective years."}, {"title": "C. Semi-supervised domain adaptation (Track 3 in 2022 and 2023)", "content": "As described in Section II-A4, the domain adaptation that we focused on was from one language in the source domain (mainly English) to a different language in the target domain (Chinese). For this challenge, the VoxCeleb dataset represents the source domain, and CNCeleb [18] represents the target domain. CNCeleb is a large-scale speaker verification dataset, mostly from Chinese speakers, which contains more than 600,000 utterances from 3,000 identities. Among the 11 genres CNCeleb spans, we removed \"singing\", \"play\u201d, \u201cmovie\u201d, \u201cadvertisement\u201d and \u201cdrama\u201d genres to focus on language domain adaptation tasks.\n1) Training set: Participants were allowed to use three types of datasets in this track:\n\u2022 VoxCeleb2 dev set with speaker labels (source domain). This can be used for pretraining.\n\u2022 A large subset of CNCeleb2 without speaker labels (target domain). It consists of 454,946 utterances from 1,807 identities. This can be used for domain adaptation.\n\u2022 A small subset of CNCeleb1 with speaker labels (target domain) comprising 50 speakers with 20 utterances per speaker.\n2) Validation set: We provided a list of trial speech pairs from identities in the target domain. These utterances were sampled from CNCeleb1.\n3) Test set: The test set consists of 56 disjoint identities not present in either CNCeleb1 or CNCeleb2. We used the hidden set of CNCeleb, provided by the authors, and constructed 30,000 and 80,000 pairs in years 2022 and 2023, respectively."}, {"title": "D. Speaker diarisation (Track 4 in 2020 to 2023)", "content": "The VoxConverse [28] dataset is an audio-visual speaker diarisation dataset which includes 448 videos from YouTube. These videos are mostly from debates, talk shows and news segments. It has multi-speaker, variable-length audio segments, with overlaps and challenging acoustic conditions. Inspired by other audio-visual dataset creation pipelines such as VoxCeleb [20] and VGGSound [30], the pipeline leverages an automatic audio-visual speaker diarisation method using active speaker detection [23], audio-visual source separation [31] and speaker verification [32], followed by manual verification. Only audio files were provided for this challenge.\n1) Training set: Participants were allowed to use any public or internal datasets except for the test data to train their systems.\n2) Validation set: We provided the dev set of VoxConverse as the validation set for the 2020 challenge. The participants were allowed to use the entire VoxConverse as the validation set for the remaining years.\n3) Test set: In the year 2020, we used the VoxConverse test set as our challenge test set, which was included in the validation set for subsequent years. The test sets for the following years were curated using the same pipeline as VoxConverse, but we utilised a hidden set of videos that were not public. These videos, sourced from YouTube, span diverse categories including news, documentaries, lectures, and commercials. We curated test sets with 264, 360, and 413 audio files in the years 2021, 2022, and 2023, respectively."}, {"title": "IV. CHALLENGE MECHANICS", "content": "This section outlines the challenge evaluation metrics for all tracks, followed by an overview of how we hosted the challenge including submission rules and formats."}, {"title": "A. Evaluation metrics", "content": "Each year, we released a validation toolkit to allow participants to assess their systems. The code in this toolkit was identical to the one which organisers use for evaluation, preventing possible performance differences stemming from implementation mismatches. All verification tracks shared the same metrics, minimum Detection Cost Function (minDCF) and Equal Error Rate (EER). The diarisation track used Diarisation Error Rate (DER) and Jaccard Error Rate (JER) as evaluation metrics. The evaluation protocols and metrics are adopted, with minor modifications, directly from NIST SRE challenges [7] for the verification tracks, and from DIHARD [11] for the diarisation track.\n1) Speaker verification: minDCF is the calibration insensitive metric to measure speaker verification performance. The DCF is computed as:\n$C_{DET} = C_{miss} \\times P_{miss} \\times P_{tar} + C_{fa} \\times P_{fa} \\times (1 - P_{tar})$\n$P_{miss}$ and $P_{fa}$ are normalised error rates by counting the errors from positive and negative pair trials respectively, and $P_{tar}$ is a prior probability that a target speaker event occurs in the real world, which is provided by the evaluator. minDCF is the minimum value of $C_{DET}$ by varying the threshold. We set $C_{miss} = C_{fa} = 1$ and $P_{tar} = 0.05$ in our cost function. The value of $P_{tar} = 0.05$ is similar to the proportion of positive pairs (4%) in the VoxSRC 2019 test set, and is also used in NIST SRE evaluation. EER corresponds to the value where False Acceptance (FA) and False Rejection (FR) rates are equal. EER is independent of parameters, unlike minDCF which is dependent on a set of predefined parameters such as $C_{miss}, C_{fa}$ and $P_{tar}$.\n2) Speaker diarisation: DER is the standard metric for evaluating diarisation results between prediction (i.e. hypothesis) and ground truth (i.e. reference).\nIt is computed as:\n$DER = \\frac{MISS + FA + CONF}{Reference}$\nwhere $Reference$ is the total length of reference, $MISS$ is the total length of speech that is present in reference but not in hypothesis and $FA$, false alarm, is the total length of speech that is present in hypothesis but not in reference. $CONF$, denoting confusion, refers to the total duration of speech which the system incorrectly identifies the speakers. We applied a forgiveness collar of 0.25 seconds when computing the DER to compensate for small inconsistencies in the annotation. Overlapping speech was considered during the DER calculation.\nJER was newly introduced in the DIHARD II challenge [33] as another diarisation metric. It is adopted from the Jaccard similarity index, which is used to evaluate image segmentation. To compute this, we first construct the mapping between speakers in reference and hypothesis using the Hungarian algorithm. Then for each reference speaker, we compute $JER_{spk}$ using Equation 3 where $Total$ is the duration of the union of speaker segments in reference and hypothesis, $MISS$ is the duration of speaker segments in the reference which are not present in hypothesis, and $FA$ is the duration of speaker segments in the hypothesis which do not exist in reference. The total JER is the average of $JER_{spk}$.\n$JER_{spk} = \\frac{MISS + FA}{Total}$"}, {"title": "B. How we hosted the VoxSRC challenge", "content": "The challenge started approximately two months before the workshop when the validation and test sets for each track were released. The submission format differed between speaker verification and diarisation tracks. For speaker verification tracks, we provided a list of pairs with corresponding audio files and asked the participants to submit the prediction scores for whether each pair was from the same speaker or not. For speaker diarisation tracks, participants were required to submit their results in RTTM format, which contains channel id, start time, duration and speaker id for each speech segment. We also provided a validation set per track with ground truth labels to let participants measure their systems' performance during development.\nThe challenge was hosted via CodaLab [34]. Each year we used the submission server provided by the CodaLab platform except for the last two years when we created our own backend to deal with bugs and errors effectively.\nOur challenge had two phases: the challenge phase and the permanent phase. The challenge phase ended before the workshop and only submissions made within this phase were considered for the prizes and certificates at the following workshop. We also opened a permanent phase after the workshop ended to let people both in industry and academia evaluate their systems with our test set and compare them against competition winners' performance. Participants were allowed to submit only one submission per day. The total number of submissions allowed was 5 in 2019 and 2020, and then was increased to 10 for future challenges reflecting participants' feedback. The limit on submission was to prevent overfitting on the test set. To prevent the same team from making submissions across multiple CodaLab accounts, we only allowed participants who registered with either academic or industry email accounts to participate. We show the participant statistics each year in the Appendix B.\nWe also provided baselines with code and models for all tracks to help new participants get started each year. We mostly used ResNet-34 models adopted from [35] for speaker verification. For diarisation, we adopted the clustering-based approach from [36] using pyannote [37] VAD, speaker embedding extractor from either [32] or [38] and agglomerative hierachical clustering. We released both verification [39] and diarisation baseline [40] in public GitHub repositories."}, {"title": "V. RESULTS", "content": "This section discusses the results of each track over the last five years. We also study the longitudinal progress on verification and diarisation tracks over the years by comparing performance on the VoxSRC 2019 verification and 2021 diarisation test sets."}, {"title": "A. Speaker verification \u2013 Track 1 and 2", "content": "Table II gives an overview of the winners' system for Tracks 1 and 2, comparing them over six aspects. The performance of winners each year is shown in Table III. As the input, speech waveforms could be directly utilised or could be converted into acoustic features. All teams adopted a DNN-based embedding extractor, which maps a variable length speech segment into a single speaker representation. Participants trained the DNN with diverse architectures such as ResNet [41], ECAPA-TDNN [38] or RepVGG [49] and combine several model outputs for final submission. Commonly leveraged data augmentation techniques included additive noise with MUSAN [42], reverberation using public room impulse response (RIR) dataset [56] or spec augmentation [48].\nAngular Margin softmax loss (AM loss) [50] and Additive Angular Margin softmax loss (AAM loss) [43] were the two most commonly used training objectives to learn the embedding extractor. A few winners adopted additional techniques such as large-margin finetuning [57] or Inter-TopK penalty [58] to further improve their model performance.\nThe scoring procedures included probabilistic linear discriminant analysis (PLDA), and most commonly, direct cosine scoring. Both of them optionally followed by score normalisation. From the year 2020, the VoxSRC winners started to use the Quality Measure Function (QMF), which has already been explored in the NIST SREs [59], [60]. They include quality metrics such as speech duration or magnitude of non-normalised embeddings to model various conditions of the trial utterances using logistic regression. In Track 2, several participants utilised external public data [46], [47]. However, it did not show a clear performance gap between Tracks 1 and 2 until VoxSRC 2021. Both the VoxSRC 2022 winner [61] and 2023 winner [62] demonstrated that self-supervised pretrained models generalise well in the new domain with a clear gap between Tracks 1 and 2 submissions (EER 1.49% to 1.21%). They leveraged self-supervised pretrained models [51]\u2013[54] to extract features and train another speaker model using these features. The VoxSRC 2022 and 2023 winner also curated their own datasets, Self-VoxCeleb and VoxTube [55] respectively, using a similar data creation pipeline to that of VoxCeleb.\nPerformance progression. The VoxSRC 2019 test set has been included as a subset in all challenges. We analyse the progress of state of the art by comparing the winning teams' performances on this set. Results are shown in Figure 2. By comparing the Track 1 winning methods over the last four years, we see that state-of-the-art performance each year has steadily improved, except for the last two years. However, when comparing Track 2 submissions, the performance has improved significantly even in the last year due to the use of public self-supervised pretrained models."}, {"title": "B. Self-supervised speaker verification (2020-2021)", "content": "The self-supervised speaker verification track was held for two editions, VoxSRC 2020 and 2021. The winners of this track employed a similar set of stages in both years: they (i) first trained the network using contrastive learning, (ii) then generated pseudo-labels based on the model from the first stage, and (iii) finally trained the network in a supervised way using these pseudo-labels. Table IV displays the overall results.\nThe VoxSRC 2020 winner [57] exploited Momentum Contrast (MoCo) [15] for the first stage, followed by iterative clustering using both efficient mini-batch k-means and Agglomerative Hierarchical Clustering (AHC) to make pseudo-speaker labels. A large ECAPA-TDNN was then trained with these labels using a sub-center AAM-softmax [76] layer.\nThe VoxSRC 2021 winner [74] extended their previous two-stage iterative labelling framework [77], which came second in 2020. They also leveraged visual data on top of the audio data for the first time in the VoxSRC challenges. They devised a unique clustering ensemble technique to fuse pseudo-labels from various modalities, which enhances the robustness of the speaker representations extracted. Since we used the same test set across Tracks 1, 2 and 3, the self-supervised test set also includes the VoxSRC 2019 test set in both years 2020 and 2021. Therefore, we can show the performance progress of the winners' method each year by measuring the performance on the VoxSRC 2019 test set. Table IV shows the result. Compared to 2020, both the first and second place in VoxSRC 2021 show lower EER on VoxSRC 2019 test set, 2.26% to 1.49% and 6.49% to 2.40% respectively. The winner of VoxSRC 2021 demonstrated a performance (EER 1.49%) remarkably similar to the VoxSRC 2019 supervised track winner (EER 1.42% on Track 1). This highlights the significant advancement of self-supervised methods in the field. Note, we only hosted this track until 2021 for two reasons: first, because most participants used similar techniques, adapting self-supervised methods from the computer vision literature; and second, because the scenario was somewhat artificial (assuming that there were no labelled data) and we wished to move to the more practical scenario of semi-supervised domain adaptation, described next."}, {"title": "C. Semi-supervised domain adaptation (2022-2023)", "content": "The semi-supervised domain adaptation track is a new track introduced in VoxSRC 2022. Here, we describe the winners' submissions in the years 2022 and 2023. Each year's winner performance is reported in Table V. Baseline [35] is the same baseline we used for Tracks 1 and 2, which is only trained with VoxCeleb2 dev set.\nThe winner [78] in VoxSRC 2022 used two frameworks, pseudo labelling and self-supervised learning. A novel sub-graph clustering algorithm was used to generate pseudo-labels based on two Gaussian fitting and multi-model voting. The model was trained in two stages, first using the labelled source domain data and the pseudo-labelled target domain data, and secondly fine-tuning the CNCeleb data by fixing the VoxCeleb weights of the classification layer using circle loss. The pseudo-label correction method was then applied and the model was retrained with these new pseudo-labels. They also explored various types of domain adaptation techniques, such as CORAL [82] or CORAL+ [83].\nThe VoxSRC 2023 winner [80] introduced a novel pseudo-labelling method based on triple thresholds. They utilised the well-trained speaker model using the source domain to extract embeddings from the target domain. They conducted initial clustering using the K-Nearest Neighbours (KNN) algorithm, followed by data cleaning, sub-centre purification and class merging to obtain the pseudo labels. At the last stage, they finetuned the model using both unlabelled data with pseudo-labels and labelled data."}, {"title": "D. Speaker diarisation \u2013 Track 4", "content": "The speaker diarisation track has been held since 2020. Table VI compares the winning methods on Track 4. The winning methods in this track all consist of the following steps: Voice Activity Detection (VAD) to detect the voice regions, speaker embedding extraction by using a sliding window approach over the voice region, and a clustering step to determine the speaker labels.\nWinners used speech separation-based VAD or train their own VAD using ResNet [41", "84": "or ECAPA-TDNN [38", "37": "or word-level timestamps from Kaldi [97", "85": "or SimAM-ResNet34 [88", "94": "used pseudo-labels from VoxConverse utterances to mitigate the effect of domain mismatch between VoxCeleb and VoxConverse and VoxSRC 2023 winner [95"}]}