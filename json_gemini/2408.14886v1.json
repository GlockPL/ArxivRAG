{"title": "The VoxCeleb Speaker Recognition Challenge: A Retrospective", "authors": ["Jaesung Huh", "Joon Son Chung", "Arsha Nagrani", "Andrew Brown", "Jee-weon Jung", "Daniel Garcia-Romero", "Andrew Zisserman"], "abstract": "The VoxCeleb Speaker Recognition Challenges (VoxSRC) were a series of challenges and workshops that ran annually from 2019 to 2023. The challenges primarily evaluated the tasks of speaker recognition and diarisation under various settings including: closed and open training data; as well as supervised, self-supervised, and semi-supervised training for domain adaptation. The challenges also provided publicly available training and evaluation datasets for each task and setting, with new test sets released each year.\nIn this paper, we provide a review of these challenges that covers: what they explored; the methods developed by the challenge participants and how these evolved; and also the current state of the field for speaker verification and diarisation. We chart the progress in performance over the five installments of the challenge on a common evaluation dataset and provide a detailed analysis of how each year's special focus affected participants' performance.\nThis paper is aimed both at researchers who want an overview of the speaker recognition and diarisation field, and also at challenge organisers who want to benefit from the successes and avoid the mistakes of the VoxSRC challenges. We end with a discussion of the current strengths of the field and open challenges. Project page : https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/workshop.html\nIndex Terms-Speaker verification, Speaker diarisation", "sections": [{"title": "I. INTRODUCTION", "content": "The VoxCeleb Speaker Recognition Challenges (VoxSRC) were a series of challenges held annually from 2019 to 2023 [1]\u2013[4]. The goals of the challenges were threefold: (i) to explore and promote novel research in the field of speaker recognition and diarisation, encouraging important directions such as self-supervised learning and domain adaptation; (ii) to measure and calibrate the state of the art through public evaluation tools; and (iii) to provide free and open-source data to the community, accessible to all. The primary tasks were speaker verification (\u201cdo these two speech segments come\nfrom the same speaker?\u201d) and speaker diarisation (\u201clabel a multi-speaker segment with who speaks when\u201d). VoxSRC consisted of an annual competition and workshop (co-located with the Interspeech conference), where each year's results and methods were discussed.\nWhen the challenge was first introduced in 2019, there were already a number of noteworthy speaker recognition challenges, such as those organised by the National Institute in Standards of Technology (NIST) [5]\u2013[9], and Speakers In the Wild (SITW) [10] and speaker diarisation challenges, such as DIHARD [11]. While these challenges provide immeasurable value to the community, the goal of VoxSRC was to provide complementary support in the form of open-source contributions \u2013 all the training and validation data was (and will continue to be) free and available to researchers irrespective of whether they enter the challenges or not, and evaluation is performed via a public leaderboard that is visible to all. VoxSRC workshops were also free for participants to attend.\nThe focus of VoxSRC has been on unconstrained speech from the web, with data consisting of noisy, varied and sometimes very short and fleeting speech segments.\nThis paper serves as a retrospective on all five VoxSRC challenges, including their mechanics, methods, results, and discussion. We hope that this paper will be useful for two audiences: (i) speaker recognition researchers aiming to determine what the state of the art in the field has been over the last five years, as measured by performance on the VoxSRC datasets, as well as wishing to understand how best practises and techniques have evolved during this period; (ii) challenge organisers, for whom we hope our learnings serve as a useful guide for the organisation of future challenges. The structure of this report is as follows: we begin by describing the two primary tasks, speaker verification and speaker diarisation and tracks we have hosted over the last five years (Section II). We then describe the datasets (Section III), and challenge mechanics (Section IV), followed by results for each track (Section V) and a detailed analysis of the winners' method (Section VI). We describe how we hosted the workshops (Section VII), and finally we reflect on the trends over the years of the challenge, discuss current challenges in speaker recognition, and conclude with lessons for future challenge organisers (Section VIII)."}, {"title": "II. TASKS AND TRACKS", "content": "This section provides an overview of the tracks hosted over the past five years, covering two primary challenge tasks: speaker verifi-cation and speaker diarisation. Within the speaker verification task, there were four different tracks over the last four years depending on the type of data participants are allowed to use. In the following, we first specify the task description, and then describe the dataset con-"}, {"title": "A. Speaker verification (2019\u20132023)", "content": "The speaker verification task is to determine whether two utterances are spoken by the same speaker or not. The evaluation is conducted using a list of utterance pairs (i.e. trials), and each trial is processed independently. VoxSRC participants submitted a real-valued prediction score for each trial pair so that evaluation could be performed using our metrics (See Section IV). Four tracks were defined based on the choice of training data: (i) Speaker verification closed, (ii) Speaker verification open, (iii) Self-supervised speaker verification, and (iv) Semi-supervised domain adaptation.\n1) Speaker verification \u2013 closed (2019\u20132023): This has been the main challenge track since the first VoxSRC challenge in 2019. It is closed in that for training their systems, participants were only allowed to use the publicly available VoxCeleb2 [12] dev set, which spans 1,092,009 utterances from 5,994 different speakers. The moti-vation here was to enable comparisons between training algorithms and model architecture approaches while keeping the data fixed. There were two reasons for choosing only VoxCeleb2 dev set and not the rest of VoxCeleb. First, we used the utterances from VoxCeleb1 as the validation set, and we did not want to have any overlap between train and validation set in our setting. Second, the size of VoxCeleb2 dev set is adequate to train large models. The details of VoxCeleb2 are described in Section III.\n2) Speaker verification \u2013 open (2019\u20132023): In this track, par-ticipants were permitted to use any other data in addition to the Vox-Celeb2 dev set for training except the test data. Regardless of whether the training data is public or not, participants here were encouraged to achieve state-of-the-art performance, pushing the limit every year. The test data was the same as that used for the closed track to quantify the effect of external training data on speaker models.\n3) Self-supervised speaker verification (2020\u20132021): In response to the progress of self-supervised learning approaches in diverse do-mains such as vision [13]\u2013[15] and natural language processing [16], [17], we introduced a self-supervised speaker verification track to investigate methods for training speaker verification models without labels. Participants in this track were only allowed to use the Vox-Celeb2 dev set without labels to train the model. The test data was identical to the previous two tracks, which allowed the performance gap to be studied between methods training with and without labels.\n4) Semi-supervised domain adaptation (2022-2023): This do-main adaptation track aimed to assess how models pretrained on large labelled data in a source domain can adapt to a new target do-main, given (i) a large set of unlabelled data from the target domain and (ii) a small set of labelled data from the target domain. This setting was especially relevant to low-resource real-world scenarios, where large-scale labelled data in the source domain, and a small set of labelled data in the target domain are available in addition to large-scale unlabelled data in the target domain. Specifically, we focused on the task of speaker verification from one language which is the source domain (mainly English), to a different language in a target domain (Chinese). Here we used VoxCeleb2 [12] as the source domain and CNCeleb [18] as the target domain. Participants were allowed to use the VoxCeleb2 dev set with speaker labels, a large subset of CNCeleb without speaker labels which contains 454,946 utterances from 1,807 speakers, and a small subset of CNCeleb with speaker labels which consists of 1,000 utterances from 50 speakers."}, {"title": "B. Speaker diarisation (2020-2023)", "content": "The speaker diarisation task is to identify \"who speaks when\u201d given audio containing single or multi-speaker speech segments. Participants were required to (i) identify speaker regions, and (ii) assign a speaker label to each region in the audio file. Participants did not need to match speech segments to specific known speakers; instead, they could simply cluster the segments according to different speakers. Participants were allowed to use any data to train their models except the test data."}, {"title": "III. DATASETS", "content": "This section describes the training, validation and test sets used in the challenges. Each year, we created (i) validation sets for which labels were available to the public, and (ii) test sets constructed from hidden data to make the task challenging and interesting, for which labels were not available publicly. For each track, the validation set featured a data distribution similar to that of the corresponding test set. This section details the datasets used each year, encompassing both publicly available and organiser-created hidden datasets. Ad-ditionally, we include a discussion of the annotation methods. We show the training set for each track in Table I. The complete statistics of the validation and test sets each year are given in the Appendix A.\nDuring the VoxSRC challenges, the labels of the test sets were not released. However, following the end of the challenges, they have now been released for the community."}, {"title": "A. Speaker verification (Tracks 1 and 2)", "content": "The challenge datasets were based on VoxCeleb [12], [19], [20], a large-scale speaker recognition dataset comprising utterances from celebrities, sampled from interviews and TV shows on YouTube. The dataset was created by extracting multiple single-speaker utterances from each YouTube video using face tracking [21], face verification [22] and active speaker detection [23]. This dataset consists of two versions. VoxCeleb1 [19] includes over 150,000 utterances from 1,251 speakers in 22,496 unique recordings. The audio dataset covers diverse background environments and recording conditions. VoxCeleb2 [12] expands the initial version to a larger scale, containing over a million utterances from 6,112 speakers in 150,480 recordings, making it five times larger than VoxCeleb1.\n1) Training set: For the closed track (Track 1), participants were only allowed to use the VoxCeleb2 dev set for training their systems. The dev set contains 1,092,009 utterances from 5,994 speakers. For the open track (Track 2), participants could use any data except the challenge test set. The training set on these two tracks remained the same for all five years.\n2) Validation set: The validation sets for all five years were based on VoxCeleb1. In addition, the validation set for 2020 and 2023 challenges made use of audio clips from the VoxMovies dataset, and the 2022 and 2023 challenges utilised audio segments from the VoxConverse dataset.\n3) Test set: The test data was created from YouTube videos in the same way as the training and validation sets, but they came from identities that do not appear in VoxCeleb1 or VoxCeleb2. The test data was checked manually for any errors using the same procedure described in [19]. This was done using a simple web-based tool that shows all video segments for each identity. To monitor the performance improvements over time, the VoxSRC 2019 test trials were always included in the test sets of all subsequent years.\n4) Themes: Each year's challenge was designed to emphasise im-portant research directions. The following paragraphs provide addi-tional information on how the data was collected for each challenge.\nSpeakers at the Movies (2020). In VoxSRC 2020, we introduced a new set of speaker segments from movie material, namely the VoxMovies dataset [24], into the validation and test sets, which served as out-of-domain data for some of the identities in VoxCeleb. VoxMovies is a speaker recognition dataset comprising utterances from movies, collected using a similar pipeline to VoxCeleb. It poses greater challenges than VoxCeleb because actors tend to disguise their voices and put more emotions in their speech in movies compared to in interviews. The utterances vary in emotion, accent, and background noise and therefore come from an entirely different domain to the VoxCeleb utterances, which mostly contain celebrity interviews.\nUtterances trials were constructed from the hidden sets of VoxMovies and VoxCeleb to create more challenging test pairs. Pairs were constructed in two different ways: (i) both utterances originated from either VoxCeleb or VoxMovies; or (ii) one utterance from VoxCeleb and the other from VoxMovies.\nCross-lingual pairs (2021). In VoxSRC 2021, multi-lingual verification pairs were introduced into the speaker verification validation and test data. The goals of this were twofold: first to promote the fairness and accessibility of speaker verification models, so as to allow people from diverse language groups to use these deep learning models; and second, to provide a more challenging test set for the speaker verification tracks.\nDue to the design of the dataset collection pipeline [20], the Vox-Celeb datasets consist of mainly English-speaking speech segments. In the validation and test sets in 2021, a multi-lingual focus was added by sampling more positive and negative pairs that contain non-English speech segments. This required the use of language labels, which do not exist for the VoxCeleb datasets. We obtained the language labels using a three-step pipeline consisting of a combination of automatic and manual annotation. First, we obtained automatic language predictions for each VoxCeleb video using a model trained on VoxLingua107 [25] to make language predictions across 107 languages. We assumed each speaker in a video uses only one language and randomly selected one utterance per video together with its predicted language label. Second, we manually annotated the correctness of the language predictions for the 12 most frequently occurring languages using a customised LISA annotation tool [26]. The 12 languages include English, French, Dutch, Italian, German, Spanish, Hindi, Portuguese, Russian, Chinese, Japanese, and Korean. Third, we used these manual annotations to obtain language-specific classification thresholds for the automatic predictions. These thresholds were then applied to classify each speech segment in VoxCeleb1 as either one or none of these 12 languages.\nHard positive pairs with large age gaps (2022). We curated hard positive validation and test pairs where the age of the speaker differs considerably between the two utterances. These hard positives were found by selecting utterance pairs from the same speaker that have a large age gap (i.e. two audio files for the same identity where the age is very different) via a two-step process in the hidden video data. In this data, for each video segment, we had the face location, the identity and the audio recording. First, the age of the speaker was estimated by predicting the age for a random set of frames, using an open-source face age prediction network [27], and averaging the result. Second, we sampled positive pairs from utterances for the same speaker with large age gaps. We performed this process within VoxCeleb1 to create the validation set, and within the hidden data to create the test set.\nHard negative pairs that share the same background noise (2022). We constructed hard negative validation and test pairs by sampling utterances from different speakers within the same video that therefore share very similar acoustic conditions. Most of the negative pairs in the VoxCeleb training datasets are from different videos, therefore speaker verification systems might be able to use the environmental cues as a shortcut for speaker prediction."}, {"title": "D. Speaker diarisation (Track 4 in 2020 to 2023)", "content": "The VoxConverse [28] dataset is an audio-visual speaker diarisa-tion dataset which includes 448 videos from YouTube. These videos are mostly from debates, talk shows and news segments. It has multi-speaker, variable-length audio segments, with overlaps and chal-lenging acoustic conditions. Inspired by other audio-visual dataset creation pipelines such as VoxCeleb [20] and VGGSound [30], the pipeline leverages an automatic audio-visual speaker diarisation method using active speaker detection [23], audio-visual source separation [31] and speaker verification [32], followed by manual verification. Only audio files were provided for this challenge.\n1) Training set: Participants were allowed to use any public or internal datasets except for the test data to train their systems.\n2) Validation set: We provided the dev set of VoxConverse as the validation set for the 2020 challenge. The participants were allowed to use the entire VoxConverse as the validation set for the remaining years.\n3) Test set: In the year 2020, we used the VoxConverse test set as our challenge test set, which was included in the validation set for subsequent years. The test sets for the following years were curated using the same pipeline as VoxConverse, but we utilised a hidden set of videos that were not public. These videos, sourced from YouTube, span diverse categories including news, documentaries, lectures, and commercials. We curated test sets with 264, 360, and 413 audio files in the years 2021, 2022, and 2023, respectively."}, {"title": "IV. CHALLENGE MECHANICS", "content": "This section outlines the challenge evaluation metrics for all tracks, followed by an overview of how we hosted the challenge including submission rules and formats."}, {"title": "A. Evaluation metrics", "content": "Each year, we released a validation toolkit to allow participants to assess their systems. The code in this toolkit was identical to the one which organisers use for evaluation, preventing possible performance differences stemming from implementation mismatches. All verification tracks shared the same metrics, minimum Detection Cost Function (minDCF) and Equal Error Rate (EER). The diarisation track used Diarisation Error Rate (DER) and Jaccard Error Rate (JER) as evaluation metrics. The evaluation protocols and metrics are adopted, with minor modifications, directly from NIST SRE challenges [7] for the verification tracks, and from DIHARD [11] for the diarisation track.\n1) Speaker verification: minDCF is the calibration insensitive metric to measure speaker verification performance. The DCF is computed as:\n$C_{DET}=C_{miss} \\times P_{miss} \\times P_{tar}+C_{fa} \\times P_{fa} \\times (1-P_{tar})$\n$P_{miss}$ and $P_{fa}$ are normalised error rates by counting the errors from positive and negative pair trials respectively, and $P_{tar}$ is a prior probability that a target speaker event occurs in the real world, which is provided by the evaluator. minDCF is the minimum value of $C_{DET}$ by varying the threshold. We set $C_{miss} =C_{fa}=1$ and $P_{tar} =0.05$ in our cost function. The value of $P_{tar} =0.05$ is similar to the proportion of positive pairs (4%) in the VoxSRC 2019 test set, and is also used in NIST SRE evaluation. EER corresponds to the value where False Acceptance (FA) and False Rejection (FR) rates"}, {"title": "2) Speaker diarisation:", "content": "DER is the standard metric for evaluating diarisation results between prediction (i.e. hypothesis) and ground truth (i.e. reference).\nIt is computed as:\n$DER=\\frac{MISS + FA + CONF}{Reference}$\nwhere $Reference$ is the total length of reference, $MISS$ is the total length of speech that is present in reference but not in hypothesis and FA, false alarm, is the total length of speech that is present in hypothesis but not in reference. CONF, denoting confusion, refers to the total duration of speech which the system incorrectly identifies the speakers. We applied a forgiveness collar of 0.25 seconds when computing the DER to compensate for small inconsistencies in the annotation. Overlapping speech was considered during the DER calculation.\nJER was newly introduced in the DIHARD II challenge [33] as another diarisation metric. It is adopted from the Jaccard similarity index, which is used to evaluate image segmentation. To compute this, we first construct the mapping between speakers in reference and hypothesis using the Hungarian algorithm. Then for each reference speaker, we compute $JER_{spk}$ using Equation 3\n$JER_{spk}=\\frac{MISS + FA}{Total}$\nB. How we hosted the VoxSRC challenge\nThe challenge started approximately two months before the workshop when the validation and test sets for each track were released. The submission format differed between speaker verification and diarisation tracks. For speaker verification tracks, we provided a list of pairs with corresponding audio files and asked the participants to submit the prediction scores for whether each pair was from the same speaker or not. For speaker diarisation tracks, participants were required to submit their results in RTTM format, which contains channel id, start time, duration and speaker id for each speech segment. We also provided a validation set per track with ground truth labels to let participants measure their systems' performance during development.\nThe challenge was hosted via CodaLab [34]. Each year we used the submission server provided by the CodaLab platform except for the last two years when we created our own backend to deal with bugs and errors effectively.\nOur challenge had two phases: the challenge phase and the permanent phase. The challenge phase ended before the workshop and only submissions made within this phase were considered for the prizes and certificates at the following workshop. We also opened a permanent phase after the workshop ended to let people both in industry and academia evaluate their systems with our test set and compare them against competition winners' performance."}, {"title": "V. RESULTS", "content": "This section discusses the results of each track over the last five years. We also study the longitudinal progress on verification and diarisation tracks over the years by comparing performance on the VoxSRC 2019 verification and 2021 diarisation test sets."}, {"title": "A. Speaker verification \u2013 Track 1 and 2", "content": "Table II gives an overview of the winners' system for Tracks 1 and 2, comparing them over six aspects. The performance of winners each year is shown in Table III. As the input, speech waveforms could be directly utilised or could be converted into acoustic features. All teams adopted a DNN-based embedding extractor, which maps a variable length speech segment into a single speaker representation. Participants trained the DNN with diverse architectures such as ResNet [41], ECAPA-TDNN [38] or RepVGG [49] and combine several model outputs for final submission. Commonly leveraged data augmentation techniques included additive noise with MUSAN [42], reverberation using public room impulse response (RIR) dataset [56] or spec augmentation [48].\nAngular Margin softmax loss (AM loss) [50] and Additive Angular Margin softmax loss (AAM loss) [43] were the two most commonly used training objectives to learn the embedding extractor. A few winners adopted additional techniques such as large-margin finetuning [57] or Inter-TopK penalty [58] to further improve their model performance.\nThe scoring procedures included probabilistic linear discriminant analysis (PLDA), and most commonly, direct cosine scoring. Both of them optionally followed by score normalisation. From the year 2020, the VoxSRC winners started to use the Quality Measure Function (QMF), which has already been explored in the NIST SREs [59], [60]. They include quality metrics such as speech duration or magnitude of non-normalised embeddings to model various conditions of the trial utterances using logistic regression.\nIn Track 2, several participants utilised external public data [46], [47]. However, it did not show a clear performance gap between Tracks 1 and 2 until VoxSRC 2021. Both the VoxSRC 2022 winner [61] and 2023 winner [62] demonstrated that self-supervised pretrained models generalise well in the new domain with a clear"}, {"title": "B. Self-supervised speaker verification (2020-2021)", "content": "The self-supervised speaker verification track was held for two editions, VoxSRC 2020 and 2021. The winners of this track employed a similar set of stages in both years: they (i) first trained the network using contrastive learning, (ii) then generated pseudo-labels based on the model from the first stage, and (iii) finally trained the network in a supervised way using these pseudo-labels. Table IV displays the overall results.\nThe VoxSRC 2020 winner [57] exploited Momentum Contrast (MoCo) [15] for the first stage, followed by iterative clustering using both efficient mini-batch k-means and Agglomerative Hierarchical Clustering (AHC) to make pseudo-speaker labels. A large ECAPA-TDNN was then trained with these labels using a sub-center AAM-softmax [76] layer.\nThe VoxSRC 2021 winner [74] extended their previous two-stage iterative labelling framework [77], which came second in 2020. They also leveraged visual data on top of the audio data for the first time in the VoxSRC challenges. They devised a unique clustering ensem-ble technique to fuse pseudo-labels from various modalities, which enhances the robustness of the speaker representations extracted.\nSince we used the same test set across Tracks 1, 2 and 3, the self-supervised test set also includes the VoxSRC 2019 test set in both years 2020 and 2021. Therefore, we can show the performance progress of the winners' method each year by measuring the perfor-mance on the VoxSRC 2019 test set. Table IV shows the result. Com-pared to 2020, both the first and second place in VoxSRC 2021 show lower EER on VoxSRC 2019 test set, 2.26% to 1.49% and 6.49% to 2.40% respectively. The winner of VoxSRC 2021 demonstrated a performance (EER 1.49%) remarkably similar to the VoxSRC 2019 supervised track winner (EER 1.42% on Track 1). This highlights the significant advancement of self-supervised methods in the field.\nNote, we only hosted this track until 2021 for two reasons: first, because most participants used similar techniques, adapting self-supervised methods from the computer vision literature; and second, because the scenario was somewhat artificial (assuming that there were no labelled data) and we wished to move to the more practical scenario of semi-supervised domain adaptation, described next."}, {"title": "C. Semi-supervised domain adaptation (2022-2023)", "content": "The semi-supervised domain adaptation track is a new track intro-duced in VoxSRC 2022. Here, we describe the winners' submissions in the years 2022 and 2023. Each year's winner performance is reported in Table V. Baseline [35] is the same baseline we used for Tracks 1 and 2, which is only trained with VoxCeleb2 dev set."}, {"title": "D. Speaker diarisation \u2013 Track 4", "content": "The speaker diarisation track has been held since 2020. Table VI compares the winning methods on Track 4. The winning methods in this track all consist of the following steps: Voice Activity Detection (VAD) to detect the voice regions, speaker embedding extraction by using a sliding window approach over the voice region, and a clustering step to determine the speaker labels.\nWinners used speech separation-based VAD or train their own VAD using ResNet [41], Conformer [84], or ECAPA-TDNN [38] architecture. External public VAD models, such as the segmentation model from pyannote [37] or word-level timestamps from Kaldi [97] ASR systems were also explored.\nSeveral embedding networks have been used such as variants of Res2Net [85] or SimAM-ResNet34 [88] trained with VoxCeleb. VoxSRC 2022 winner [94] used pseudo-labels from VoxConverse utterances to mitigate the effect of domain mismatch between VoxCeleb and VoxConverse and VoxSRC 2023 winner [95] included additional data during training.\nAll winners used AHC to cluster the embeddings extracted using the sliding window approach. Some winners [92], [94] also adopted spectral clustering [36]. After these three steps, all winning methods adopted Dover-LAP [86] to fuse the results from different models.\nMultiple methods were used to deal with overlapping speech. Winners either trained a separate Overlap Speech Detection (OSD) model on their own or used TS-VAD [89] to detect the overlapping speech regions and integrated the results into their final submission."}, {"title": "VI. DETAILED ANALYSIS", "content": "In this section, we further analyse winners' submissions. We add analysis on (i) the verification pairs that winners get wrong, (ii) multi-lingual pairs in the year 2021, (iii) hard positive and negative pairs in the year 2022, and (iv) semi-supervised domain adaptation submissions with several baseline results in the year 2022."}, {"title": "A. Verification pairs that winners get wrong", "content": "Over the previous five years, the state-of-the-art performance on the VoxSRC 2019 test set has progressively improved, as shown in Figure 2. Models however still predict differently from the ground truth in some trials.\nOf the 232 pairs that all the winners of VoxSRC 2022 and 2023 (first and second places on both closed / open tracks) get wrong, 16 are positive pairs and the rest are negative pairs. We manually verify whether these pairs are correctly labelled or not.\nFor the positive pairs, 12 out of 16 are label errors. Four of these errors are the result of utterances from different individuals with the same name, a confusion caused by the YouTube scraping process based on video titles. The remaining eight pairs are utterances from siblings who are mistakenly assigned the same identity due to their inclusion in YouTube videos titled with the name of a well-known celebrity. These were not caught during the manual verification stage. Nevertheless, the proportion of erroneous labels is minimal and does not significantly impact the system's performance, accounting for only 0.006% of the total 208,008 pairs. The rest of the four pairs without label errors are from the same person but extracted at different ages, which is a challenging setting that we focused on in VoxSRC 2022, and is still an open area of research.\nIn the set of 216 negative pairs, no labelling errors are observed. This can be attributed to our method of random sampling from the speaker pools in the raw data, which significantly reduces the likelihood of labelling errors. All these pairs consist of different individuals of the same gender, and we do not notice any other significant trends."}, {"title": "B. Analysis of performance by utterance length and gender", "content": "In this section we analyse the performance on the VoxSRC2019 test set of the Track 1 [69] and Track 2 [62] VoxSRC2023 winners against utterance length and gender.\nTable VIII shows the performance as the utterance length is varied, where >x sec denotes the subset of VoxSRC2019 test pairs where both utterances of each pair are longer than x seconds. Both models show better performance as the utterance length increases."}, {"title": "C. Multi-lingual focus \u2013 VoxSRC 2021", "content": "The verification tracks in 2021 had a multi-lingual focus, via the inclusion of multi-lingual data in the test set. We provide some analysis on the performance of the winning methods from the supervised tracks (1st and 2nd places on Track 1) and the provided baseline [32] on the multi-lingual data.\nWe measure performance on the identical language subsets for the five most common languages in VoxCeleb. The results are shown in Figure 5. We calculate a performance measure per language by calculating the EER from the same-language pairs. For all five languages, there are at least 1000 same-language pairs (both positive and negative) in the test set. The exact number of same-language pairs per language is: English: 282,039; French: 5,987; German: 1,066; Italian: 1,129; and Spanish: 4,241. The winning methods perform much better than the baseline across all languages, although there is still considerable variation in performance between different languages. Interestingly, although English is the most common language in the training set, none of the models performs best on the English language pairs. However, we note that there is more statistical uncertainty for the languages with a lower number of pairs, e.g. German with 1,066 pairs, compared to those with many pairs, e.g. English with 282,039 pairs. This is due to the error introduced when estimating population statistics using small sample sizes."}, {"title": "D. Hard positive and negative pairs \u2013 VoxSRC 2022", "content": "In the year 2022, we introduced new trial types to make the test set harder \u2013 hard positive pairs with large age gaps, and hard negative pairs that share the same background noise. Here we analyse how these pairs affect the winners' performance. The VoxSRC 2022 test set consists of four types of trials, (i) hard positive pairs taken from the same speaker at different ages (P-Hard), (ii) hard negative pairs taken from the same environment (N-Hard), (iii) positive pairs from VoxSRC 2019 test set (P-Vox19), and (iv) negative pairs from VoxSRC 2019 test set (N-Vox19). We compare the performance of our baseline model [32] and the top 2 winners of Track 1 on these subsets.\nTable X shows the results. The 1st [61] and 2nd place [68] performed better than our baseline model by a large margin. Comparing the performance of E-1 to the others shows that both the hard positives and the hard negatives made the challenge more difficult. For the most challenging set, E-4 with both hard positive and negative pairs, the 1st place method (which achieves an impressive 0.9% EER on the VoxSRC-19 test set) could only achieve 2.07%. Interestingly, the second-place method performed better in E-1, E-2 and E-3 than the 1st place but achieved worse results in E-4. This led to a slightly better EER performance of the second place in 2022 than the first place on the overall VoxSRC2022 test set (1.40% vs 1.49%). While both use similar training strategies and identical training set in Track 1, the first place uses ResNet100 with a base channel of 128, while the second place uses the variants of ResNet34 with a base channel of 64. The larger capacity of the first place might have led to a slight overfitting to the training set. However, as shown in Table X, the difference is not significant."}, {}]}