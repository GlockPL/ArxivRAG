{"title": "Accelerated Preference Optimization for Large Language Model Alignment", "authors": ["Jiafan He", "Huizhuo Yuan", "Quanquan Gu"], "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal tool for aligning large language models (LLMs) with human preferences. Direct Preference Optimization (DPO), one of the most popular approaches, formulates RLHF as a policy optimization problem without explicitly estimating the reward function. It overcomes the stability and efficiency issues of two-step approaches, which typically involve first estimating the reward function and then optimizing the policy via proximal policy optimization (PPO). Since RLHF is essentially an optimization problem, and it is well-known that momentum techniques can accelerate optimization both theoretically and empirically, a natural question arises: Can RLHF be accelerated by momentum? This paper answers this question in the affirmative. In detail, we first show that the iterative preference optimization method can be viewed as a proximal point method. Based on this observation, we propose a general Accelerated Preference Optimization (APO) framework, which unifies many existing preference optimization algorithms and employs Nesterov's momentum technique to speed up the alignment of LLMs. Theoretically, we demonstrate that APO can achieve a faster convergence rate than the standard iterative preference optimization methods, including DPO and Self-Play Preference Optimization (SPPO). Empirically, we show the superiority of APO over DPO, iterative DPO, and other strong baselines for RLHF on the Alpaca Eval 2.0 benchmark.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have emerged as a pivotal technique in the era of artificial general intelligence and recently demonstrated impressive capabilities in tasks such as text generation (Bubeck et al., 2023; Anil et al., 2023; Touvron et al., 2023), coding (Chen et al., 2021; Austin et al., 2021), and problem solving (Cobbe et al., 2021; Wei et al., 2022). A key element contributing to these achievements is the alignment of LLMs with human preference data, utilizing reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019; Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022; Munos et al., 2023).\nThe standard RLHF method (Ouyang et al., 2022) involves three main steps: feedback collection, reward modeling, and policy optimization. Specifically, the LLM receives human-generated prompts"}, {"title": "2 Preliminaries", "content": "In the setting of RLHF, we assume a finite context set \\( \\mathcal{X} \\), and possible response set \\( \\mathcal{Y} \\). For any prompts \\( x \\in \\mathcal{X} \\), a policy \\( \\pi : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{Y}) \\) maps the prompt \\( x \\) to the discrete distributions over the response set \\( y \\). For a given context \\( x \\in \\mathcal{X} \\) collected from distribution \\( \\rho \\), we generate two responses \\( y_1, y_2 \\) with a reference policy \\( \\mu \\) and receive preferences from either humans or more advanced language models between these two responses (\\( y^w \\succ y^l \\)), where \\( y^w \\) and \\( y^l \\) represent the preferred and dispreferred generated responses in \\( \\{y_1, y_2\\} \\). Following Christiano et al. (2017); Ouyang et al. (2022); Rafailov et al. (2023), we assume the existence of a latent reward model \\( r^*(x, y) \\), and the preference distribution satisfies the Bradley-Terry (BT) model (Bradley and Terry, 1952):\n\n\n\\begin{equation}\nP(y_1 \\succ y_2 | x) = \\frac{\\exp (r^*(x, y_1))}{\\exp (r^*(x, y_1)) + \\exp (r^*(x, y_2))}\n\\tag{2.1}\n\\end{equation}\n\nUnder this assumption, the standard RLHF first estimates the reward model by minimizing the following negative log-likelihood of BT model:\n\n\n\\begin{equation}\n\\mathcal{L}(r) = -\\mathbb{E}_{(x,y^w,y^l) \\sim \\mathcal{D}} [\\log \\sigma(r(x, y^w) - r(x, y^l))],\n\\tag{2.2}\n\\end{equation}\n\nwhere \\( x \\) is generated from distribution \\( \\rho \\), \\( \\{y^w, y^l\\} \\) are collected with reference policy \\( \\mu \\) and \\( \\sigma(z) = 1/(1+\\exp(-z)) \\) is the Sigmoid function. After the reward modeling phase, the LLM (i.e., the policy) is fine-tuned with the learned reward \\( r(x, y) \\), which aims to maximize the expected reward with KL-regularization:\n\n\n\\begin{equation}\n\\pi \\leftarrow \\arg \\max_{\\pi \\in \\Pi} \\mathbb{E}_{x \\sim \\rho, y \\sim \\pi(\\cdot | x)} [r(x, y)] - \\beta \\mathbb{E}_{x \\sim \\rho} [KL(\\pi(\\cdot | x) || \\pi_{\\text{ref}}(\\cdot | x))],\n\\tag{2.3}\n\\end{equation}\n\nwhere \\( \\Pi \\) denotes the policy class, \\( \\rho \\) is the distribution of prompts, and the KL regularization with parameter \\( \\beta > 0 \\) is used to control the deviation of the learned policy \\( \\pi \\) from the reference policy \\( \\pi_{\\text{ref}} \\). In detail, the optimization problem is usually solved with the PPO method (Schulman et al., 2017).\nLater, Rafailov et al. (2023) identified the following closed-form solution to the optimization problem in (2.3):\n\n\n\\begin{equation}\n\\pi(x) = \\frac{1}{Z(x)} \\cdot \\pi_{\\text{ref}}(x) \\cdot \\exp \\left(\\frac{r(x,\\cdot)}{\\beta}\\right)\n\\end{equation}"}, {"title": "3 Accelerated Preference Optimization", "content": "In this section, we present a general framework for language model alignment, namely accelerated preference optimization (APO), which is built upon the iterative preference optimization framework."}, {"title": "3.1 Iterative Preference Optimization Framework", "content": "Under the iterative Preference Optimization framework (Xu et al., 2023; Yuan et al., 2024b; Chen et al., 2024; Wu et al., 2024), the algorithm progressively updates the policy \\( \\pi_t \\), aiming to converge to the optimal policy. In detail, for each iteration \\( t \\in [T] \\), it designates the reference policy as the policy generated from the previous iteration, denoted by \\( \\pi_\\tau \\). It estimates the reward model by minimizing the expected loss function \\( l(r, x, y^w, y^l) \\) over the dataset \\( \\mathcal{D}_t \\):\n\n\n\\begin{equation}\n\\begin{aligned}\nr_t(\\cdot, \\cdot) \\leftarrow \\arg \\max_{r(\\cdot, \\cdot)} \\mathbb{E}_{(x,y^w,y^l) \\sim \\mathcal{D}_t} [l(r, x, y^w, y^l, \\pi_t)].\n\\end{aligned}\n\\tag{3.1}\n\\end{equation}\n\nThen, it updates the reference policy by solving the following KL-regularized optimization problem:\n\n\n\\begin{equation}\n\\pi_{t+1} \\leftarrow \\arg \\max_{\\pi \\in \\Pi} \\mathbb{E}_{x \\sim \\rho, y \\sim \\pi(\\cdot | x)} [r_t(x, y)] - \\beta \\mathbb{E}_{x \\sim \\rho} [KL(\\pi(\\cdot | x) || \\pi_t(\\cdot | x))].\n\\tag{3.2}\n\\end{equation}\n\nAccording to Rafailov et al. (2023), for each iteration \\( t \\in [T] \\), the optimization problem (3.2) has the following closed-form solution:\n\n\n\\begin{equation}\n\\pi_{t+1}(y|x) \\propto \\pi_t(y|x) \\cdot \\exp \\left(\\frac{r_t(x, y)}{\\beta}\\right).\n\\end{equation}\n\nThus, we can reparamterize the reward function for each policy as follows:\n\n\n\\begin{equation}\nr_\\pi(x, y) = \\beta \\log \\frac{\\pi(y|x)}{\\pi_t(y|x)}.\n\\end{equation}\n\nWith this reparameterized reward function, the previous two-step optimization process in (3.1) and (3.2) can be integrated into the following one-step direct preference optimization:\n\n\n\\begin{equation}\n\\pi_{t+1} \\leftarrow \\arg \\min_{r_\\pi \\in \\mathcal{R}_t} \\mathbb{E}_{(x,y^w,y^l) \\sim \\mathcal{D}_t} [l(r_\\pi, x, y^w, y^l, \\pi_t)],\n\\tag{3.3}\n\\end{equation}\n\nwhere \\( \\mathcal{D}_t \\) represents the data collected at iteration \\( t \\) using the reference policy \\( \\pi_t \\).\nFor the vanilla iterative preference optimization framework, the updated policy \\( \\pi_{t+1} \\) is directly used as the reference policy in the next iteration, where \\( \\pi_{t+1} = \\pi_{t+1} \\). In this situation, the iterative"}, {"title": "3.2 Accelerated Preference Optimization", "content": "So far, we have demonstrated that the iterative preference optimization framework resembles the proximal point method. For standard optimization problems, it is well known that Nesterov's momentum method (Nesterov, 1983, 2008, 2013) can accelerate the optimization algorithm both theoretically and empirically. In particular, Lin et al. (2018) proposed a framework called Catalyst, which extends Nesterov's momentum method to the proximal point method and has shown that it can accelerate it provably. In the Catalyst method, after solving the proximal operator\n\n\n\\begin{equation}\nx_{t+1} = \\arg \\min_{x} \\{f_t(x) = f(x) + \\kappa D(x, y_t)\\},\n\\end{equation}\n\nwhere \\( f(x) \\) is the target function and \\( D(x, y_t) \\) is the Bregman divergence, an extrapolation step is introduced as follows:\n\n\n\\begin{equation}\ny_{t+1} = x_{t+1} + a_t(x_{t+1} - x_t),\n\\end{equation}"}, {"title": "4 Theoretical Analysis", "content": "In this section, we provide a theoretical analysis of APO in Algorithm 1.\nWe begin with the following theorem, which outlines the optimization dynamics of the policy \\( \\widehat{\\pi}_{t+1} \\) over different iterations \\( t \\in [T] \\).\nTheorem 4.1. Suppose that \\( \\widehat{\\pi}_{t+1}(y|x) \\cdot (\\widehat{\\pi}_{t+1}(y|x)/\\pi_t(y|x))^{\\alpha} \\) belongs to the policy class \\( \\Pi \\) for each iteration \\( t \\in [T] \\). Then, the updated policy \\( \\widehat{\\pi}_{t} \\) in Algorithm 1 satisfies\n\n\n\\begin{equation}\n\\begin{aligned}\n\\widehat{\\pi}_{t+1}(y|x) = \\frac{1}{Z_t(x)} \\cdot \\pi_{\\text{ref}}(y|x) \\cdot \\exp \\left( \\sum_{i=0}^t \\left( \\frac{1}{1-\\alpha} - \\frac{\\alpha^{t+1-i}}{1-\\alpha} \\right) r_i(x, y) \\right),\n\\end{aligned}\n\\end{equation}\n\nwhere \\( r_t(x, y) = \\beta \\log\\widehat{\\pi}_{t+1}(y|x) - \\beta \\log \\pi_t(y|x) \\) represents the reparameterized reward at iteration \\( t \\), and \\( Z_t(x) = \\sum_y \\pi_{\\text{ref}}(y|x) \\exp \\left( \\sum_{i=0}^t (1/(1 - \\alpha) - \\alpha^{t+1-i}/(1 - \\alpha)) \\cdot r_i(x,y)/\\beta \\right) \\) is the partition function.\nTheorem 4.1 illustrates how the policy \\( \\widehat{\\pi}_{t+1} \\) evolves with respect to the reparameterized reward \\( r_t(x, y) \\), which is highly dependent on the choice of the loss function \\( l \\) in Algorithm 1.\nFor the Bradley-Terry (BT) model with the loss function \\( l_{\\text{DPO}} \\) in Example 3.1, Theorem 1 in Rafailov et al. (2023) demonstrates that all reward functions compatible with the BT model can be expressed by the reparameterized reward. In addition, we introduce the following two assumptions, which are required by our analysis.\nAssumption 4.2 (Realizability). For each iteration \\( t \\in [T] \\) and each policy \\( \\pi \\in \\Pi \\), the following updated policy belongs to the policy class \\( \\Pi \\):\n\n\n\\begin{equation}\n\\pi_\\star(\\cdot|x) = \\frac{1}{Z_\\pi(x)} \\cdot \\pi(\\cdot|x) \\cdot \\exp \\left(\\frac{r^*(x,\\cdot)}{\\beta}\\right) \\in \\Pi,\n\\end{equation}\n\nwhere \\( Z_\\pi(x) = \\sum_y \\pi(x) \\cdot \\exp (r^*(x,y)/\\beta) \\) is the partition function.\nAssumption 4.3 (Boundedness). For each iteration \\( t \\in [T] \\) and each policy \\( \\pi, \\pi_t \\in \\Pi \\), we have\n\n\n\\begin{equation}\n\\beta \\log \\frac{\\pi(y|x)}{\\pi_t(y|x)} \\in [-R, R],\n\\end{equation}\n\nfor all \\( x \\in \\mathcal{X}, y \\in \\mathcal{Y} \\).\nSimilar assumptions have been used in Rosset et al. (2024) to provide an analysis of the statistical error for the reparameterized reward. Equipped with these assumptions, we have the following performance guarantee for APO."}, {"title": "5 Experiments", "content": "In this section, we detail the experimental settings used to validate the efficacy of our proposed APO algorithm. We evaluate the model's performance across various benchmark tasks and explore the impact of different momentum schedules."}, {"title": "5.1 Experimental Setup", "content": "Model and Datasets. We use Mistral Al's Mistral-7B-Instruct-v0.2 (Jiang et al., 2023a) as our base model, which is a fine-tuned version based on the pretrained Mistral-7B-v0.2 (Jiang et al., 2023a) on several publicly available datasets for instruction-following. This architecture has demonstrated robust performance improvements over Llama2 13B Chat(Touvron et al., 2023) in tasks such as Chatbot Arena(Chiang et al., 2024), MT-Bench (Zheng et al., 2024b), and other related evaluations. As a result, Mistral-7B-Instruct-v0.2 has become a popular choice for base models in recent reinforcement learning (RL) fine-tuning research (Hoang et al., 2023; Kawin et al., 2023). For training, we employ the UltraFeedback dataset (Cui et al., 2023) with loss function (DPO. Unlike traditional fine-tuning methods that depend on responses and preference labels generated by proprietary models like GPT-4, we utilize only the instruction set from UltraFeedback. All responses are autonomously generated by our model following an online principle, and the preference pairs are labeled using a separate reward model, PairRM (Jiang et al., 2023b). The instruction set used for both training and validation includes a total of 64k instructions that span a diverse range of tasks, such as UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA, and FLAN, as detailed in Cui et al. (2023). Over three training iterations, we divided the instruction set into three folds as in Hoang et al. (2023); Kawin et al. (2023), allocating approximately 2k instructions per iteration, with an additional 1k left out for validation. Overall, our training pipeline is independent of human or GPT inputs. We provide details on hyperparameters in Appendix F.1.\nEvaluation. For evaluating the performance of our model, we utilize three common benchmarking tasks: the AlpacaEval 2.0 (Li et al., 2023b), the MT-Bench (Zheng et al., 2024b), and the Open LLM Leaderboard (Beeching et al., 2023; Gao et al., 2023). Among them, AlpacaEval 2.0 is the most indicative benchmark for our method with the current choice of experimental settings, focusing on general instruction-following capabilities as assessed by GPT-4-Turbo, with outcomes measured by a weighted win-rate against GPT-4-Turbo. Another important benchmark is MT-Bench, which also targets instruction-following but offers less differentiation between models. Additionally, we present results from the Open LLM Leaderboard, which is based on accuracy of multiple-choice questions.\nAlpacaEval 2.0. As our primary evaluation metrics, AlpacaEval 2.0 incorporates an extensive set of 805 prompts. These prompts are simplified versions derived from the AlpacaFarm (Dubois et al., 2023) evaluation set, covering a wide range of topics such as Health, Linguistics, Entertainment, Technology, Coding, Gaming, Arts, Sports, and more (Yuan et al., 2024b). During the evaluation, we consult the help of GPT-4-Turbo to compare the responses generated by our model with those produced by GPT-4-Turbo. The final win rate are weighted based on the uncertainty of the judge."}, {"title": "5.2 Experimental Results", "content": "Having introduced our evaluation metrics and baselines, we now turn to our training pipeline and main results. In Algorithm 1, we begin by setting \\( \\pi_0 = \\widehat{\\pi}_0 = \\pi_{\\text{ref}} \\) as the base model, Mistral-7B-Instruct-v0.2. At each iteration, we sample five pairs of responses under the current policy \\( \\pi_t \\) and rank them using their PairRM score (Jiang et al., 2023a). We designate the top-ranked response as the winner and the bottom-ranked response as the loser. Following the proximal point update described in (3.5), we proceed with a momentum extrapolation. We note that when the policy is a softmax linear function, update (3.6) reduces to a momentum extrapolation in the parameter space. Consequently, we carry out an extrapolation in the parameter space to approximate the corresponding momentum step (3.6) in the probability space."}, {"title": "5.3 Ablation Studies", "content": "Effect of Coefficient a. In Table 3, we explore how different choices of a might affect performance. At iteration 1, performance fluctuates when changing a, peaking at a = 0.3 with a win rate of"}, {"title": "6 Conclusions and Future Work", "content": "In this work, we studied the iterative preference optimization framework for aligning large language models (LLMs) with human preferences and showed that it resembles the proximal point method. Based on this observation, we introduced a general framework, APO, incorporating Nesterov's momentum technique. Theoretically, we show that our method achieves a faster convergence rate than the standard iterative DPO and SPPO methods. Our experimental results demonstrate the superiority of APO over iterative DPO on the AlpacaEval 2.0 benchmark and on the instruction-following tasks of MT-Bench, achieving both accelerated convergence rate, and better final performance.\nLimitation. Due to limited computational resources, we do not evaluate APO with the SPPO (Wu et al., 2024) loss function in the current experiments, and we plan to investigate it in our future work. Additionally, while our model demonstrates consistent improvements on instruction-following tasks, it faces challenges in solving math problems. This limitation is largely due to the choice of dataset and the restriction of not utilizing additional information from GPT-4 or human sources. In the future, we aim to address this by incorporating larger datasets and leveraging GPT-4 supervision."}, {"title": "A Related Work", "content": ""}, {"title": "A.1 Reinforcement Learning from Human Feedback", "content": "Learning from human feedback in reinforcement learning can be traced back to Knox and Stone (2009); Wirth et al. (2017) and was later popularized by Christiano et al. (2017), which incorporated human preferences into deep reinforcement learning. Recently, RLHF has gained popularity in natural language processing and has become a paradigmatic method for fine-tuning large language models (LLMs) (Achiam et al., 2023; Touvron et al., 2023; OpenAI, 2023) to align with human objectives. The standard process for alignment with human feedback (Ouyang et al., 2022) requires both reward modeling and policy optimization. Recently, Rafailov et al. (2023) proposed the Direct Preference Optimization method, which replaces the reward modeling process with a reparameterized reward and directly performs policy optimization using preference data. Similarly, Kawin et al. (2023) proposed the Kahneman-Tversky Optimization method (KTO) with a reparameterized reward, while considering different human-aware loss functions.\nMore recently, iterative variants of policy optimization have garnered increasing attention. Xu et al. (2023) investigated the iterative preference optimization method and proposed the PAIRWISE CRINGE algorithm, which iteratively generates a new preference dataset using the current model and then updates the model with a combination of the original preference data and the newly labeled data. Additionally, they studied the iterative version of Direct Preference Optimization (DPO) and demonstrated that it achieves better performance than standard PPO or DPO. Later, Yuan et al. (2024b) studied Self-Rewarding Language Models, where the algorithm iteratively updates the reference policy in the DPO method and uses LLM-as-a-Judge prompting (Zheng et al., 2024b) to provide its own rewards for the generated new preference data. Despite this work demonstrating the superiority of iterative optimization process, there is a lack of theoretical foundations for these practical frameworks. Recently, Xiong et al. (2023) proposed Gibbs Sampling from Human Feedback, offering theoretical analysis with the aid of linear function approximation and the incorporation of an uncertainty bonus. Compared with previous methods, the reference policy remains fixed, the newly trained model is only used to generate new preference data, and this trained model will not be inherited in the subsequent iteration. On the contrary, our research focuses on studying the iterative DPO model without incorporating an uncertainty bonus, and it continually updates the reference policy throughout the process.\nMost of the works motioned above rely on the assumption that the latent preference distribution \\( P(y_1 \\succ y_2|x) \\) follows the Bradley-Terry (BT) model (Bradley and Terry, 1952)), and there exists a series of works focusing on general preference, where human preference may not strictly be transitive. Under the general preference assumption, iterative optimization (Munos et al., 2023; Swamy et al., 2024; Rosset et al., 2024; Wu et al., 2024) is also employed to find the Nash-equilibrium policy. We leave the extension of our algorithm to general preference model in the future work."}, {"title": "A.2 Accelerated Optimization with Momentum", "content": "The idea of accelerating gradient methods has been extensively explored over the decades. One of the earliest contributions to this field was the Polyak momentum (Polyak, 1964), which achieved faster convergence by leveraging the history of previous iterates. However, this early approach sometimes failed to converge even for strongly convex objectives (Lessard et al., 2016). This was further refined by Nesterov's accelerated gradient (NAG) (Nesterov, 1983), with a guarantee for"}, {"title": "B Accelerated Preference Optimization with General Preferences", "content": "In the previous sections, we assumed the existence of a latent reward function and focused on the Bradley-Terry (BT) model. However, (Tversky, 1969) observed that the preferences across all possible responses may not be monotonous or transitive, which cannot be represented by a latent reward function. In this section, we extend the analysis to environments with general preferences and focus on the win probability between different responses.\nIn detail, for an environment with general preferences, we denote the probability that response y1 is preferred over response y2 given the context x as \\( P(y_1 > Y_2|x) \\), and we assume that the preference model is antisymmetric, such that \\( P(y_2 > y_1|x) = 1 - P(y_1 > y_2|x) \\). Under this assumption, we define the reward \\( r_{\\pi,\\pi'} = \\mathbb{E}_{y_1 \\sim \\pi, y_2 \\sim \\pi'} [P(Y_1 > Y_2|x)] \\) as the win probability for policy \\( \\pi \\) against policy \\( \\pi' \\). Our goal is to identify the von Neumann winner (Dud\u00edk et al., 2015) with the preference model. Specifically, the von Neumann winner corresponds to the Nash equilibrium of the following two-player zero-sum game:\n\n\n\\begin{equation}\n(\\pi^*, \\pi^*) = \\arg \\max_{\\pi} \\min_{\\pi'} \\mathbb{E}_{y_1 \\sim \\pi, y_2 \\sim \\pi'} [P(Y_1 > Y_2|x)]\n\\end{equation}\n\n\n\\begin{equation}\n= \\arg \\max_{\\pi} \\min_{\\pi'} \\Upsilon_{\\pi,\\pi'}.\n\\end{equation}"}, {"title": "B.1 Reduction to SPPO with lsPPO", "content": "For the Self-Play Preference Optimization (SPPO) (Wu et al., 2024) algorithm, if we set the learning rate \\( \\beta \\) in Algorithm 1 to be the inverse of the learning rate \\( \\eta \\) in the SPPO algorithm, the preference optimization process can be expressed as follows:\n\n\n\\begin{equation}\n\\begin{aligned}\n\\pi_{t+1} &= \\arg \\min_{\\pi} \\mathbb{E}_{x \\sim \\rho, y \\sim \\pi(\\cdot | x)} \\left[\\eta \\left( \\log \\frac{\\pi(y|x)}{\\pi_t(y|x)} \\right) - \\eta P(y \\succ \\pi_t|x) - \\log Z_{\\pi_t}(x)\\right]^2\n\\\\\n&= \\arg \\min_{\\pi} \\mathbb{E}_{x \\sim \\rho, y \\sim \\pi(\\cdot | x)} \\left[\\beta \\log \\frac{\\pi(y|x)}{\\pi_t(y|x)} - \\beta P(y \\succ \\pi_t|x) - \\log Z_{\\pi_t}(x)\\right]^2\\\\\n&= \\arg \\min_{\\pi} \\mathbb{E}_{x \\sim \\rho, y \\sim \\pi(\\cdot | x)} \\left[r_\\pi(x, y) - \\beta Ey' \\sim \\pi [P(y \\succ y'|x)] + \\log Z_{\\pi_t}(x)\\right]^2\\\\\n&= \\arg \\min_{\\pi} \\mathbb{E}_{x \\sim \\rho, y, y' \\sim \\pi(\\cdot | x)} \\left[r_\\pi(x, y) - 1(y \\succ y'|x) + \\log Z_{\\pi_t}(x)\\right]^2\\\\\n&+ \\mathbb{E}_{x \\sim \\rho, y, y' \\sim \\pi(\\cdot | x)} \\left[r_\\pi(x, y') - 1(y' \\succ y|x) + \\log Z_{\\pi_t}(x)\\right]^2/2 + C_{\\pi_t},\n\\end{aligned}\n\\end{equation}\n\nwhere \\( Z_{\\pi_t}(x) = \\sum_{y \\in \\mathcal{Y}} \\pi_t(y|x) \\exp (\\eta P(y \\succ \\pi_t|x)) \\) represents the partition function for behavior policy \\( \\pi_t \\), \\( C_{\\pi_t} = \\mathbb{E}_{x \\sim \\rho, y, y' \\sim \\pi_t(\\cdot | x)} [P(y \\succ y'|x) - 1(y \\succ y'|x)]^2 \\) is the variance of behavior policy \\( \\pi_t \\), the second equation holds due to \\( \\beta = \\eta^{-1} \\) and the last equation holds because \\( y, y' \\) collected under the same behavior policy. Therefore, the preference optimization process in SPPO is aligned with our Algorithm 1 using the SPPO loss function:\n\n\n\\begin{equation}\nl_{\\text{SPPO}} (r_\\pi, x, y, Y', \\pi_t) = \\frac{1}{2} (r_\\pi (x, y) - 1 + \\log Z_{\\pi_t}(x))^2 + \\frac{1}{2} (r_\\pi (x, y') + \\log Z_{\\pi_t}(x))^2.\n\\end{equation}"}, {"title": "B.2 Theoretical Analysis with General Preferences", "content": "In Section 4, Theorems 4.4 and 4.8 analyze the performance of Algorithm 1 under the Bradley-Terry (BT) model. For a general preference model, Theorem B.2 provides a convergence rate for Algorithm 1 with the loss function \\( l_{\\text{SPPO}} \\) in Example 3.2, under the assumption of a minimal sub-optimality gap.\nAssumption B.1 (Minimal sub-optimality gap with general preferences). For each prompt \\( x \\in \\mathcal{X} \\), we assume there exist a unique optimal response \\( y^*_x \\in \\mathcal{Y} \\) such that for any other sub-optimal responses \\( y' \\neq y^* \\), we have\n\n\n\\begin{equation}\nP(y^*_x \\succ y' | x) - P(y \\succ y'|x) \\geq \\Delta, \\forall y' \\in \\mathcal{Y}.\n\\end{equation}\n\nTheorem B.2 (APO with lSPPO). For general preference model with loss function lSPPO, under the mild assumptions of realizability (Assumption E.3), boundedness (Assumption E.4) and As- sumption B.1, with probability at least 1 \u03b4, the TV-distance between \ud835\udef1T +1 and the optimal policy \\( \\pi^*(x) = y^*_x \\) is bounded by\n\n\n\\begin{equation}\n\\begin{aligned}\n&\\mathbb{E}_{x \\sim \\rho} [D_{TV} (\\pi_{T+1}(\\cdot|x), \\pi^*(\\cdot|x))] \\\\\n&\\le O\\left(\\frac{(T+1) \\sum_{t=0}^T \\kappa_t \\cdot \\log (T|\\Pi|/\\delta)}{N\\beta^2(1-\\alpha)^2} + \\exp\\left(-\\mathbb{O}\\left(\\frac{t\\Delta}{(1-\\alpha) \\beta}\\right)\\right)\\right).\n\\end{aligned}\n\\end{equation}"}, {"title": "C Proof of Main Results", "content": ""}, {"title": "C.1 Proof of Theorem 4.1", "content": "In this section, we provide the proof of Theorem 4.1, which is crucial for understanding the optimization dynamics of Algorithm 1.\nProof of Theorem 4.1. Based on the definition of the reparameterized reward \\( r_t \\), we have\n\n\n\\begin{equation}\nr_t(x, y) = \\beta \\log \\widehat{\\pi}_{t+1}(y|x) - \\beta \\log \\pi_t(y|x).\n\\tag{C.1}\n\\end{equation}\n\nFurthermore, the extrapolation step (3.6) satisfies\n\n\n\\begin{equation}\n\\pi_{t+1}(y|x) = \\frac{1}{Z_t(x)} \\cdot \\widehat{\\pi}_{t+1}(y|x) \\cdot (\\widehat{\\pi}_{t+1}(y|x)/\\pi_t(y|x))^\\alpha,\n\\end{equation}\n\nwhere \\( Z_t(x) = \\sum_y \\widehat{\\pi}_{t+1}(y|x) \\cdot (\\widehat{\\pi}_{t+1}(y|x)/\\pi_t(y|x))^\\alpha \\) represents the partition function. Taking the logarithm of both sides yields the following equation\n\n\n\\begin{equation}\n\\log \\pi_{t+1}(y|x) = (1 + \\alpha) \\log \\widehat{\\pi}_{t+1}(y|x) - \\alpha \\log \\pi_t(y|x) - \\log Z_t(x).\n\\tag{C.2}\n\\end{equation"}]}