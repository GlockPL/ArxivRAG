{"title": "C2T: A Classifier-Based Tree Construction Method in Speculative Decoding", "authors": ["Feiye Huo", "Jianchao Tan", "Kefeng Zhang", "Xunliang Cai", "Shengli Sun"], "abstract": "The growing scale of Large Language Models (LLMs) has exacerbated inference latency and computational costs. Speculative decoding methods, which aim to mitigate these issues, often face inefficiencies in the construction of token trees and the verification of candidate tokens. Existing strategies, including chain mode, static tree, and dynamic tree approaches, have limitations in accurately preparing candidate token trees for verification. We propose a novel method named C2T that adopts a lightweight classifier to generate and prune token trees dynamically. Our classifier considers additional feature variables beyond the commonly used joint probability to predict the confidence score for each draft token to determine whether it is the candidate token for verification. This method outperforms state-of-the-art (SOTA) methods such as EAGLE-2 on multiple benchmarks, by reducing the total number of candidate tokens by 25%, while maintaining or even improving the acceptance length.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) Achiam et al. (2023); Touvron et al. (2023); Chiang et al. (2023) have shown remarkable abilities in various fields, but face significant bottlenecks in autoregressive token generation due to high memory bandwidth demands and underutilized GPU resources Patterson (2004); Shazeer (2019), as each token requires access to all model parameters Radford et al. (2019); Brown et al. (2020). To address this issue, Speculative Decoding (SD) Chen et al. (2023); Leviathan et al. (2023) has been developed, which quickly generates multiple draft tokens and verifies them all at once using the target model to maximize GPU computational capacity, and it has been applied in the latest influential LLMs (Liu et al., 2024).\nVanilla SD employs a chain structure for the draft tokens, and the verification process follows a topological order Chen et al. (2023); Leviathan et al. (2023). If a token is rejected, all subsequent tokens are also discarded. To overcome inefficiency, tree-structured draft tokens have been proposed Miao et al. (2024); Sun et al. (2024), which integrate multiple chains. Static tree methods, such as EAGLE-1 Li et al. (2024b) and Medusa Cai et al. (2024), use preset tree structures that bias the sampling rate of specific positions Chen et al. (2024), while dynamic methods, such as EAGLE-2 Li et al. (2024a), rely on contextual information.\nFor dynamic tree methods, most are designed to build and prune trees on the basis of confidence scores. The most straightforward approach is to use the joint probability as confidence Li et al. (2024a); Wang et al. (2024); Brown et al. (2024); Qin et al. (2024). However, directly using joint probability as confidence is not enough for complex situations, leading to misjudgments. As shown in Figure 1, which will be explained in detail in 3. From the perspective of tree data structure, whether a token is accepted or not, depends not only on its own property but also other nodes' properties. This means that incorporating more variables together is necessary in the confidence score calculation. Therefore, we propose a tree construction method based on a designed tiny classifier to perform this calculation. Our contributions are summarized below:\n\u2022 We propose a classifier-based method named C2T for the dynamic construction of the token tree. And it can be easily integrated into any confidence-based SD system.\n\u2022 Our classifier demonstrates strong transferability across different datasets and within the same model family, thus being plug-and-play.\n\u2022 Compared to the SOTA method EAGLE-2, C2T reduces the number of candidate tokens by 25% while maintaining or improving the acceptance length on multiple benchmarks."}, {"title": "2 Background", "content": "2.1 Speculative Decoding\nSpeculative decoding (SD) Chen et al. (2023); Leviathan et al. (2023) is an algorithm designed to speed up model reasoning by leveraging the parallel computing capabilities of attention mechanisms. It consists of two main stages. The first stage is the draft phase, where a smaller model, known as the draft model Md, generates draft tokens. The second stage is verification, where the draft tokens are verified all at once by the larger model, known as the target model Mt. At the same time, we call the draft tokens chosen to be verified as candidate tokens.\n2.2 Tree Attention and EAGLE-2\nVanilla SD uses a chain-like structure, where if a token is rejected, all subsequent tokens are also discarded. SpecInfer's Miao et al. (2024) Tree Attention achieves the integration of multiple speculations at a minimal cost and has been widely adopted in other SD methods He et al. (2023); Sun et al. (2024); Cai et al. (2024); Li et al. (2024b); Chen et al. (2024); Svirschevski et al. (2024). In the pioneer SD works, the token tree was static, with a preset tree shape, and the draft model generated tokens layer by layer and filled in the corresponding positions. This static method is undoubtedly rough, so various heuristic dynamic generation methods appeared later Li et al. (2024a); Wang et al. (2024); Brown et al. (2024); Qin et al. (2024); Huang et al. (2024). Several trainable dynamic methods have been also proposed Mamou et al. (2024); Huang et al. (2024); Zhang et al. (2024), but due to design redundancies, they have only been applied to early stopping in chain-like drafting, which significantly limits their applicability.\nThe current SOTA dynamic tree construction method EAGLE-2 Li et al. (2024a) introduces the joint probability of each node as contextual information, and divides the sampling process into two stages: expand and rerank. The former is to build the tree and the latter is for post-pruning. We can denote the token tree after expansion as T\u2081, and the token tree reranked as T2. There are three important parameters in EAGLE-2:\n1. K: During expanding, each token in current layer generates TopK tokens. Then, among all the generated tokens, the TopK tokens are selected to generate the next tree layer.\n2. dmax: The rounds of the drafting in the expansion phase.\n3. N: Use joint probability to rerank all nodes in T\u2081, and then take the TopN nodes as T2. Since the joint probability of the parent node is always greater than that of the child nodes, T2 must be a valid subtree of T\u2081. Therefore, TopN determines the size of T2, which directly determines the number of candidate tokens."}, {"title": "3 Motivation", "content": "As shown in Figure 2, we use the EAGLE-2's LLaMA-2 7B model pair to reason on the MT-bench. We performed a single forward to calculate the entropy and divided it into seven intervals. We then computed the average probability of different ranks within these intervals to create the heatmap in Figure 2(a) which shows that lower entropy corresponds to more concentrated distributions.\nWe also tested the acceptance rate of tokens with different ranks in these intervals and generated the heatmap in Figure 2(b). The similarity between 2(a) and 2(b) confirms the correlation between probability and acceptance rate.\nHowever, Figure 2(b) is less stable than Figure 2(a), indicating that there is a discrepancy. To quantify this, we subtracted the data matrix of Figure 2(a) from that of Figure 2(b) to create Figure 2(c), which shows the bias between probability and acceptance rate.\nAs shown in Figure 2(c), there are two main deviations between probability and acceptance rate:\n\u2022 When the entropy is high, the node with the highest probability in the distribution is underestimated in terms of probability;\n\u2022 When the entropy is low, the node with the highest probability in the distribution is overestimated in terms of probability.\nThese two deviations may lead to misjudgments as shown in Figure 1(a). For the \"all\" and \"great\", although the probabilities of their parent are greater than those of the parent of \"a\" and \"to\", due to their higher entropy in the probability distribution, the normalized generation probabilities are not high. As a result, they are outperformed in joint probability by the \"a\" from a distribution with lower entropy.\nIn addition, due to the nature of the token tree, if a token is rejected, all its subsequent child nodes will also be rejected. Therefore, the acceptance rate is a value that decreases with depth. Nodes at shallower levels should be given greater confidence. If only joint probability is considered, the depth factor will become inactive in some cases. As shown in Figure 1(b), although the \"the\" and the \"has\" are both the smaller ones in their respective distributions and the \"has\" is shallower, the joint probability of the former is still greater than that of the latter because the probability of the former's parent node is too large (our previous experiments can also prove that in distributions with lower entropy, the node with the highest probability is overestimated). Consequently, in subsequent recalls, the \"has\" node is also unlikely to be selected, even though it is on the second level.\nTherefore, directly using joint probability as confidence cannot handle complex situations and will inevitably lead to misjudgments. This necessitates that we consider more features like entropy and depth when designing the confidence function. However, introducing more features makes the function design more challenging, and low-dimensional functions are not robust. Naturally, we thought of introducing a learnable neural network to fit an efficient confidence function for us. To this end, we propose an efficient method using Classifier to build Trees named C2T in speculative decoding. Please refer to Appendix D for how the classifier addresses the two situations mentioned above."}, {"title": "4 Methodology", "content": "Our primary goal is to maintain the accept length with fewer candidate tokens using a lightweight classifier. To enhance its versatility, the training features should be based solely on the token tree's properties, without relying on model or dataset information. This classifier will serve as a predictor for pre-pruning during token tree construction, helping create a concise tree efficiently.\n4.1 Classifier\nThe ultimate goal of our method is to reduce latency, so the classifier must be as lightweight as possible. At the same time, we aim for a plug-and-play solution where a model, trained on one dataset, remains effective when applied to others, avoiding features that are strongly tied to specific datasets or models, such as hidden states. Instead, we should use only the mathematical properties of nodes as features for the classifier. Additionally, these features should be easy to compute and minimal in number.\nWe adopt a two-layer Feed-Forward Network (FFN) as the classifier to achieve our goal, using ReLU as the activation function and sigmoid for normalizing the final logits. This classifier is extremely simple, with minimal parameters and computational load. For features, we start with the joint probability of each node, as methods like EAGLE-2 have shown their effectiveness as a confidence measure. Next, we select the entropy of the probability distribution in which each node resides as the third feature, as lower entropy indicates a higher acceptance rate. Finally, we include the depth of each node, since shallower nodes have a higher likelihood of acceptance. To simplify calculations, we only consider the top 1000 probabilities of each node's child nodes when calculating entropy. This simplification is necessary and almost lossless, and the detailed proof can be found in Appendix C. By selecting the top 1000 based on probability, we can almost certainly include the topK of the confidence scores (K is typically much smaller than 1000).\nThe introduction of entropy and depth is not only because they are related to the acceptance rate, but also because the dimensionality-increased node information is more conducive to solving the problems raised by only using joint probability in the motivation. More details in Appendix D.\nWith these features, a complete classifier structure is obtained. More details of training settings are shown in Appendix B.\n4.2 Tree Construction\nC2T is a pre-pruning approach, which can be divided into two steps: the first pruning based on confidence and the second pruning based on topK."}, {"title": "4.2.1 First Pruning based on Confidence", "content": "As shown in Figure 3, after obtaining the classifier, we can use it to build the token tree. The tree construction based on the classifier is a layer-by-layer construction process. The draft model can obtain the generation probability of each node during each forward pass. Let a node be i, and the generation probability of this node is pi, then the joint probability of this node is denoted as\n$P = \\prod_{j \\in Path(root,i)} P_j$\nWhere the Path(root, ti) represents the set of all nodes on the path from the root node to node i. At this point, we can also determine the entropy of node i's probability distribution.\n$H_i = - \\sum_{j \\in S(i)} p_j \\log p_j$\n$H_i \\approx - \\sum_{j \\in S_{1000}(i)} p_j \\log p_j$\nWhere S(i) represents the probability distribution of i, and S1000(i) represents the set of nodes with top 1000 probabilities.\nWe can also easily record the depth of node di.\nAfter obtaining these three features, we can use the trained classifier to obtain the confidence of node i, denoted as\n$C_i = F(P_i, H_i, d_i)$\nWhere F(*) represents the classifier's output. Since Ci is normalized, we set a threshold \u03b2 between 0 and 1 to determine which tokens participate in generating the next tree layer. This screening-generation process repeats at each tree layer until the current depth di reaches the maximum depth dmax we set, or until no nodes in the current tree layer have confidence greater than B."}, {"title": "4.2.2 Second Pruning based on Topk", "content": "In our experiments, setting \u03b2 appropriately ensures that the classifier does not qualify too many tokens at each tree layer, making the naive generation method sufficient. However, in practical applications, we often need to set TopK to manage tree generation. TopK serves two main purposes:\n1. To reduce the classifier's computational cost, we calculate confidence only for the TopK tokens with the highest generation probability.\n2. To prevent excessive tree expansion, we limit the number of tokens participating in the next tree layer's generation. After identifying the tokens that pass the classifier test in the current tree layer, we select only the TopK tokens with the highest confidence as the final candidates.\nAppendix B.6 shows that when \u03b2 is sufficiently large, the use of TopK does not impact the method's effectiveness. However, when \u03b2 is less strict, TopK helps prevent the number of candidate tokens from becoming too large, though it may also limit the algorithm's maximum performance. Generally, the first strategy is essential for reducing classifier costs proven in Appendix C Proof-2, while the second can be applied as needed, with the size of TopK adjusted based on GPU's capabilities or omitted if unnecessary."}, {"title": "5 Experiments", "content": "Models: We used LLaMA-2-Chat 7B, 13B, 70B Touvron et al. (2023) and Vicuna 7B, 13B, 33B Chiang et al. (2023) as target model Mt, and the corresponding draft model Ma is from EAGLE Li et al. (2024b).\nTasks: To compare with EAGLE-2 Li et al. (2024a), we aligned with it on the dataset. For tasks such as multi-round dialogue, code generation, mathematical reasoning, instruction following, summarization, and Q&A, we selected the MT-bench Zheng et al. (2023), HumanEval Chen et al. (2021), GSM8K Cobbe et al. (2021), Alpaca Taori et al. (2023), CNN/Daily Mail Nallapati et al. (2016), and Natural Questions Kwiatkowski et al. (2019), respectively.\nMetrics: The ultimate goal of this work is to let the Mt verify as few candidate tokens as possible while maintaining the hit rate unchanged or even better, so we mainly focus on the following two device-independent indicators:\n\u2022 The number of candidate tokens y: The total number of tokens verified by Mt.\n\u2022 Accept length 7: The average length accepted by Mt for each generation, and this indicator in this paper does not include the initial token generated by the target model itself, so it needs to be added 1 compared to some papers.\nIn this paper, the experiment with the LLaMA-2 7B model focuses on precision rather than inference time, as the method's main advantage is obtaining a more accurate token tree at minimal cost. For powerful GPUs and lightweight LLMs, if the GPU's parallel computing capability threshold is not reached, this optimization may not improve time performance. So we will demonstrate the efficiency advantage of this method on the LLaMA-2 70B model. When analyzing inference time, there will be the following two indicators:\n\u2022 Draft time: The total time measured from the completion of the last verification to the generation of the next verification input on benchmark data.\n\u2022 Verify time: The total time used for verification on benchmark data.\nComparison: This work will mainly compare with the SOTA dynamic strategy, EAGLE-2 Li et al. (2024a). The N will be our variable to control \u03b3 and T. To make the comparison fairer, we will set EAGLE-2's K = 15 and dmax = 10. This parameter configuration nearly reaches the limit of EAGLE-2's Ma capabilities."}, {"title": "5.1 Feature Ablation", "content": "We conducted ablation studies on three features: joint probability, entropy, and depth. The model trained with all three features served as the baseline, while other combinations were used as control groups. As shown in Table 1, the joint probability is the most critical factor for performance improvement. In contrast, entropy and depth alone lead to significant performance degradation. Entropy and depth primarily serve as corrective factors, while joint probability is essential for confidence scoring. Combining joint probability with either entropy or depth slightly outperforms EAGLE-2. However, using all three features together significantly enhances performance, demonstrating their strong complementarity."}, {"title": "5.2 Dataset Transferability", "content": "EAGLE-2, being a post-pruning method, directly controls y by adjusting TopN. In contrast, our pre-pruning approach controls y by setting a pruning threshold \u03b2 during tree generation. Thus, a direct step-by-step comparison is not feasible. Instead, we compare the precision of both methods using a scatter plot of \u03c4 versus \u03b3. To further validate transferability and data independence, we transferred the classifier's parameters to other datasets without fine-tuning.\nAs shown in Figure 4, C2T consistently achieves lower y for similar 7 compared to EAGLE-2, indicating superior precision. Moreover, C2T's curve is more gradual, showing its increasing advantage ast grows. Cross-validation is in Appendix B.5."}, {"title": "5.3 Model Transferability", "content": "We trained the classifier on LLAMA-2 7B's token tree and tested its transferability to other models (LLaMA-2 13B, 70B, Vicuna 7B, 13B, and 33B) by freezing its parameters. Results in Table 2 show that C2T performs well on LLaMA-2 models that stably utilizes 75% to 80% of y while keeping T unchanged. But the ability has declined on Vicuna models, though still better than EAGLE-2. When the classifier is fine-tuned on Vicuna's token trees, its performance comes back to the level achieved on the LLaMA-2 model family. The cost of this fine-tuning is minimal. For details, please refer to Appendix B.4.\nIn summary, C2T has strong transferability within the same model family to still obtain good performance but may require fine-tuning for optimal performance when transferring to a different model family."}, {"title": "5.4 Speed up in Larger LLMS", "content": "To evaluate the time efficiency of C2T under GPU limits, we tested the LLaMA-2 70B model on 2 * A100 (80G) GPUs. Figure 5(a) shows that with EAGLE-2, as TopN increases, y grows linearly but verify time increases in steps, not linearly.\nWe compared C2T with EAGLE-2 by matching T. Figure 5(b) shows that C2T has a slightly higher draft time, but lower y resulting in a time advantage. When the parallel computing capability of GPUs is pushed to the limit, our method reduces the time by 18% compared to EAGLE-2 under the same \u0442. For detailed latency analysis, see Appendix E.1."}, {"title": "5.5 Benefits in Chain Mode", "content": "Experiments in chain mode are meaningful because current dynamic tree construction does not support batch sizes greater than 1. This is due to the inability to have different attention masks within the same batch. In contrast, a chained token tree is always compatible with multi-batch scenarios.\nIn chain mode, C2T degrades as an early exit strategy and EAGLE-2 is rendered ineffective, essentially reverting to EAGLE-1. We varied the maximum draft length from 5 to 9 tokens for EAGLE-1/2. Additionally, we compared dynamic methods using the maximum probability and joint probability as early stopping criteria. The results, shown in Table 3, demonstrate that C2T retains an advantage in chain mode."}, {"title": "6 Conclusion", "content": "In this paper, we propose C2T to address the limitations of previous tree construction methods that rely solely on joint probability. By training a classifier with additional features, we improved the precision of tree construction. We conducted extensive evaluations on multiple benchmarks and various LLMs to compare with the SOTA method EAGLE-1/2. Our method achieved superior results in all experiments and demonstrated strong transferability and applicability. C2T can construct a more precise tree using 75% to 80% of the tokens while maintaining the acceptance length. When the parallel computing capability of GPUs is pushed to the limit, C2T reduces the time by 18% compared to EAGLE-2 under the same acceptance length."}, {"title": "Limitation", "content": "C2T, similar to other dynamic tree construction approaches, currently does not support batch sizes greater than 1 (bs > 1) due to the use of different tree masks within the same batch, which is not supported by existing engineering implementations. However, C2T supports early stopping in chain mode, which is compatible with bs > 1. In practical industry use, bs > 1 is typically used in conjunction with chain mode Liu et al. (2024). And the preliminary combo experiments of the MTP-style layer in DeepSeek-V3 and C2T are in Appendix F.\nCompared to methods using joint probability directly, C2T adds minimal overhead. This overhead is negligible when verification time is significant. And the quantitative analysis shows that the additional FLOPs are negligible and imply potential for optimization in engineering implementation. Please refer to Appendix E.2 for details."}, {"title": "Ethics Statement", "content": "Our research adheres to the ACL Code of Ethics. We have ensured that our work respects user privacy and does not include any personal information in the datasets used. The datasets are publicly available and were labeled through interactions with English-speaking users. The tools and models used in this study are utilized in compliance with their intended purposes and are accessible under permissive licenses. We are committed to upholding the ethical standards of the ACL and promoting responsible research practices within the NLP community."}, {"title": "A EAGLE-1 and EAGLE-2", "content": "We conducted our experiments on LLaMA-2 7B and MT-bench. According to Table 1 of EAGLE-2 Li et al. (2024a), the accept length of EAGLE-1 Li et al. (2024b) is 3.62, and that of EAGLE-2 is 4.70. We also obtained similar results in our reproduction.\nHowever, during the experiment, we found that this comparison is not fair. Based on Appendix A of EAGLE-1 paper and EAGLE-2 paper, we can obtain the shapes of their respective token trees, where the size of the token tree in EAGLE-1 is 26, while that in EAGLE-2 is 60. Therefore, we further aligned the sizes of the token trees of both models and introduced our method C2T. The experimental results are shown in Figure 4.\nAfter aligning the tree sizes, EAGLE-2's performance falls short of expectations, with only an 11% improvement in accept length. In practice, dynamic methods incur additional costs, leading to worse wall-clock times (on 2 A100 80G GPUs). While dynamic methods generate more accurate token trees than static methods, the extra computational cost means the tree size must be increased to achieve a speedup. Essentially, dynamic methods optimize GPU utilization further, as manually designing larger trees is extremely difficult and impractical for complex scenarios. However, given GPU limitations, increasing tree size also increases the verification burden on the target model. Thus, C2T's ability to generate more compact trees while maintaining the same accept length is particularly valuable."}, {"title": "B Classifier", "content": "B.1 Dataset\nThe classifier used in this paper, if not specifically mentioned, is trained on the token tree generated by LLaMA-2 7B on the MT-bench using the EAGLE-2 strategy. The settings are dmax = 11 (excluding the root generated by Mt), TopK = 10, and the TopN = 1011 (meaning no recall is performed during the rerank stage, and the complete tree is used as the training dataset). Each data entry uses joint probability, depth, and entropy as features, and whether it is accepted as the label. We simply clean the data by dropping entries containing NA values, resulting in 8880 token trees, each containing 1011 nodes, for a total of 8880 * 1011 = 8,977,680 training data entries.\nB.2 Training and Evaluation\nWe split the dataset in a ratio of 0.95:0.05. Since the dataset has sparse positive samples, for example, each token tree has 1011 nodes, however, only 3.5 tokens are accepted by the Mt, so we perform negative sampling on this sparse dataset. During training, we set the batch size to 1024, and during evaluation, to align with the token tree verification configuration, we set the batch size to 1011. We use Adam as the optimizer with a learning rate (lr) of 1 \u00d7 10-3 and train for 10 epochs, and use BCE as the criterion.\nFor evaluation, we focus more on recall, meaning the classifier should try to recall all tokens that are ultimately accepted by Mt. At the same time, we should also pay attention to the positive rate, which is the probability that the classifier predicts a token as positive, denoted as 8. This value corresponds to the ratio of TopN to the size of T\u2081 in EAGLE-2 and is positively correlated with the final y. When selecting the classifier, priority should be given to models with significantly higher recall. Among models with similar recall, choose the one with a smaller 0.\nB.3 Structure\nIn this paper, we also briefly explored the effects of classifiers with different FFN structures. We mainly discussed the performance of FFNs with two layers and different hidden states, setting the hidden state of the classifier to 2, 6, 12, 24, 36, and 48, respectively. We trained various FFNs according to the training configuration in B.2"}, {"title": "B.4 Fine-tuning", "content": "In this paper, we also explored the fine-tuning, using Adam as the optimizer with a lr of 1 \u00d7 10-4 and training for 10 epochs using 10% of the data. In addition to fine-tuning in the Experiment 5.3 testing model transferability, we also attempted fine-tuning between different token trees inferred on different benchmarks. The fine-tuning process is shown in Figure 7. As shown in the figure, although fine-tuning improves the classifier's recall, it also increases the positive rate. When we applied the fine-tuned classifier to inference, we found that the distribution relationship between candidate tokens number y and accept length 7 remained almost unchanged. The only difference is that the fine-tuned classifier requires a larger \u1e9e to achieve the same y and as before. This means that the fine-tuned classifier becomes more confident, but there is no significant improvement in precision. This also indirectly proves the data-free characteristic of our classifier."}, {"title": "B.5 Cross Validation", "content": "Since our previous experiments involved inferring token trees on MT-bench to train the classifier, and then transferring the classifier to other datasets for speculative decoding, we now cross-validate the effectiveness of C2T on MT-bench. To do this, we train the classifier from scratch using token trees generated from other datasets and then apply it to inference on MT-bench. This experiment was conducted on LLaMA-27B. The results are shown in Figure 8. Classifiers trained on other datasets and those trained directly on MT-bench show nearly identical distributions in the scatter plots when used for inference with C2T. This cross-validates the feasibility of C2T on MT-bench."}, {"title": "B.6 Use TopK for Second Pruning", "content": "In the methodology section, we discussed the improvement of further constraining the tree shape using TopK in pre-pruning. In the methodology, we mentioned that using TopK to simplify the computation process is necessary, but the latter TopK pruning after obtaining the confidence scores is optional. In this experiment, we will conduct a variable analysis for the latter case. This approach results in a smaller and more stable token tree, and by varying TopK, we can generate multiple scatter plots. As shown in 4, it is observed that after introducing TopK for second pruning, the method maintains a similar distribution on the graph as the original method, and in some threshold values, it even yields slightly better results. Regardless of fluctuations, it consistently outperforms EAGLE-2. As shown in Table 5, under a high \u1e9e, secondary pruning can get gains in y with minimal loss of T, but it also limits the maximum capability under a low \u03b2. In our other experiments, we default to using TopK=15 for two complete rounds of constrained pruning."}, {"title": "C Proof of Simplified Calculation", "content": "The method proposed in this paper, when calculating entropy, requires first obtaining the topM probabilities and then calculating the entropy of these M probabilities. Let the vocabulary size be V, and it is known that the implementation of torch.topk is based on the quickselect algorithm.\nIf we consider only the calculation of entropy, obtaining the topM probabilities first and then calculating the entropy is more complex than directly calculating the entropy.\nProof-1. The FLOPs for directly calculating the entropy is F\u2081 = 2 * V. In contrast, obtaining the topM probabilities involves F2 = C1 * V, after calculating the entropy of M probabilities involves F3F2+2* M C1 * V + 2* M, where C1 > 2 in most cases. So 2 * V < C1 * V + 2 * M, which means F1 < F3. In summary, selecting first and then calculating the entropy is more complex than directly calculating the entropy in most cases.\nHowever, we need to take into account the impact of the joint probability calculation step. Both C2T and EAGLE-2 only need to calculate the joint probabilities of the TopK when computing joint probabilities. We first argue the necessity of this step.\nProof-2. If we were to fully calculate the joint probabilities and then select, since each tree layer has at most K nodes, considering the parallel computing capability of GPUs, the overall complexity for calculating the joint probabilities is O(V). There would be K * V probabilities in total, and selecting the TopK from them would involve O(K * V). Therefore, the total complexity would be O(V + K * V). In contrast, by only taking the TopK for each node, due to the parallel computing nature of GPUs, the total complexity for selection is O(V), resulting in K\u00b2 probabilities. The complexity for selecting the TopK from these probabilities is O(K\u00b2), so the overall complexity is O(V + K\u00b2). Since K\u00b2 << V, therefore O(V+K*V) > O(V+K\u00b2). In summary, it is necessary to first select and then calculate the joint probabilities.\nTherefore, what we are actually comparing are the complexities of the following two scenarios:\n\u2022 Directly calculating the entropy and then selecting the TopK probabilities.\n\u2022 First selecting the topM probabilities, then calculating the entropy of these M probabilities, and finally selecting the TopK probabilities from these M probabilities.\nProof-3. From Proof-1, we know that for the first scenario, the FLOPs before taking the TopK is F\u2081 = 2 * V, and the FLOPs for taking the TopK is F4 = C2 * V. Therefore, the total FLOPs is F5 = (2 + C2) * V. For the second scenario, the FLOPs before taking the TopK is F3 = C1*V+2*M, and the FLOPs for taking the TopK from the M probabilities is F5 = C3 * M. Therefore, the total FLOPs is F6 = C1 * V + (2 + C3) * M. Since for the quickselect algorithm, the final number of computations is independent of the number of elements to be selected, C\u2081 \u2248 C2. Also, since V >> M, we have F4 - F6 = (C2 - C1) * V + 2 * V - (2 + C3) * M \u2248 2 * V - (2 + C3) * M > 0, which means F4 > F6. In summary, considering the computation of joint probabilities, the FLOPs of the first scenario are more than the second scenario."}, {"title": "D Confidence", "content": "Since the features of our classifier are a triplet and the output is a scalar, it is highly suitable for statistical analysis. First, we created a 3D heatmap for the overall input-output by randomly generating 1000 data points, where the joint probability is a float in [0,1", "0,10": "and depth is an integer in [0,10"}]}