{"title": "Blockchain for Large Language Model Security and Safety: A Holistic Survey", "authors": ["Caleb Geren", "Amanda Board", "Gaby G. Dagher", "Tim Andersen", "Jun Zhuang"], "abstract": "With the advent of accessible interfaces for interacting with large language models, there has been an associated explosion in both their commercial and academic interest. Consequently, there has also been an sudden burst of novel attacks associated with large language models, jeopardizing user data on a massive scale. Situated at a comparable crossroads in its development, and equally prolific to LLMs in its rampant growth, blockchain has emerged in recent years as a disruptive technology with the potential to redefine how we approach data handling. In particular, and due to its strong guarantees about data immutability and irrefutability as well as inherent data provenance assurances, blockchain has attracted significant attention as a means to better defend against the array of attacks affecting LLMs and further improve the quality of their responses. In this survey, we holistically evaluate current research on how blockchains are being used to help protect against LLM vulnerabilities, as well as analyze how they may further be used in novel applications. To better serve these ends, we introduce a taxonomy of blockchain for large language models (BC4LLM) and also develop various definitions to precisely capture the nature of different bodies of research in these areas. Moreover, throughout the paper, we present frameworks to contextualize broader research efforts, and in order to motivate the field further, we identify future research goals as well as challenges present in the blockchain for large language model (BC4LLM) space.", "sections": [{"title": "1. INTRODUCTION", "content": "From disparate areas such as software development, the solicitation of political advice, and assistance in creative writing tasks, the introduction of large language models into everyday life has occurred at unprecedented pace and scale [106]. Consequently, many of the vulnerabilities that large language models contend with are well known and understood in the current literature, such as prompt injection [74][98], hallucinations [52] [130] [83] [12], and data poisoning [37][54]. Despite this, relatively little in terms of mitigation has been introduced to combat such weak points [124][2]. The ramifications of this fact are far-reaching in regards to user experience and mounting data integrity concerns surrounding AI systems at large [118][40][68].\nFor example, and representative of the variety of threats that LLMs face, is the concerning fact that LLMs are easily coaxed into revealing personally identifiable information (PII), as in the case when an LLM was persuaded into divulging undisclosed names, physical addresses, emails, phone numbers, and twitter handles associated with specific individuals to an unauthorized user [20]. Similarly, attacks such as prompt injection can be compounded with a model's tendency to divulge information, resulting in massive data leakage [118]. Typically, defensive responses to these threats manifest themselves in the application of established machine learning techniques, such as differential privacy strategies applied to entire corpus' to improve privacy guarantees [1][122]. While this is an important application, it is often the case that DP guarantees are not enough to fully ensure data privacy in a large language model [68] due to DP's ability to protect primarily \"by whom\" data is contributed, rather than \"about whom\" the data is focused on. Additionally, another common attempt to tackle the data privacy problem in LLMs is the use of federated learning (FL) in the training process. This technique distributes training across multiple nodes to create a decentralized environment for model data to exist within [75]. Naturally, this lends itself to further obscuring sensitive information in a model's corpus. However, it has been shown that by taking model weights or gradients, original data from the model can still be reconstructed [63]. Additionally, federated learning solutions are susceptible to many of the same types of attacks as large language models, such as single-point-of-failure attacks or man-in-the-middle attacks [86]. This trend of typical machine learning solutions failing to exhaustively defend against the range of attacks now affecting LLMs continues across multiple traditional threat/defense models [96, 132, 122].\nIt is clear that to counter new threats arising from large language models there is a simultaneous need for new technologies to be introduced into the space. Situated in a similar position to LLMs in their rapid emergence and adoption are blockchain systems. They have the ability to ensure data integrity via various tamper-evident mechanisms, introduce a high degree of confidentiality into otherwise centralized systems, and allow for data provenance guarantees through traceable and auditable data [27][19][77][4]. These qualities of blockchain systems place themselves at an ideal juncture to be used for bolstering the robustness of large language models. This integration creates the necessary conditions to allow for stronger privacy preservation, enhanced inference checks, anti-adversarial attack technologies, and similar defenses to be incorporated into the design of large language models.\nThis unification of technologies has become a hot topic of research within both blockchain and large language model oriented communities over the last several years. As a field in its infancy, blockchain for large language models (BC4LLM) demands rigorous analysis so that it may progress unhindered from any limitations that may arise from the combination of two newly introduced fields, especially given their relative complexity and various nuances. Towards this end of progress in the fields of large language models and blockchain systems, we present this survey paper as a vehicle for researchers to better grasp the state of the two fields and especially understand the combination of the two technologies in the vein of how blockchain can better serve large language model systems. To clarify our goals with this paper, we introduce four research questions that motivate the entirety of the paper and generalize our aim in writing this survey. They are as follows:\nRQ1. What are the pressing LLM-related security concerns that may be addressed with blockchain technology?\nRQ2. How can we meaningfully differentiate between security and safety in the context of BC4LLMs?\nRQ3. In what ways can blockchain technology be used to enhance the safety of LLMs?\nRQ4. What are prominent gaps within the BC4LLMs area, how can these gaps influence research directions, and what resources can we provide to enable potential new directions?\nOf note is our focus specifically on how blockchain systems may impact large language model security and safety. We limit the scope of this survey to these terms in order to provide finer-grained analysis and categorization of seemingly disparate works and to more clearly spur research advances in specific directions. Indeed, we typically see attacks on LLMs occur via malicious third parties which exploit system vulnerabilities (security) [5][121][132][44][66] or as passive issues embedded in the structure of a LLM which places users at risk with no malicious outside influence (safety) [34][109][92][125]. It is because of this encompassing distinction that we offer our analysis of the blockchain for LLM (BC4LLM) space in the context of security and safety. Moreover, we further bolster the significance of this distinction with definitions of security and safety in the context of large language models. To the best of our knowledge, we are the first paper to rigorously introduce such definitions as we aim to lay a foundation upon which future BC4LLM works can build. It is also worth pointing out our contributions towards further defining privacy measures in the form of active privacy and passive privacy efforts, modeled after Yan et al's survey on bolstering data privacy [122].\nTo separate ourselves, and our analysis of BC4LLM through the lens of safety and security, from other similar works, we look at several other closely related reviews. He et al. [42] examine the relationship between large language models and blockchain in analyzing how large language models can further enhance blockchain systems in the LLM4BC space. Mboma et al. [74] provide an exploratory review of general integrations between blockchain and large language models, which is similar to Heston's analysis of integrating the two technologies in the sphere of telemedicine [30]. In the case of Salah, et al. [91], Bhumichai et al. [13], and Dinh et al. [28], we see the exposition of potential and existing technologies between blockchain and artificial intelligence in general. In summary, existing reviews that contend specifically with the blockchain and large language model space are not focused on the direction in which they apply the technologies. On the other hand, reviews that concern blockchain and AI as a whole lose the benefits of tighter granularity and focus. We present an overview of related works in table 1, which juxtaposes the above papers' contents with our specific focuses, stressing the differentiating factors that we introduce into the space."}, {"title": "1.1 Contribution", "content": "We outline the exact contributions of this survey paper, and highlight their impact in answering our research questions. They are as follows:\n1. We present several key contributions in the form of various frameworks, definitions, and compiled resources. Most prominently, we introduce a taxonomy of blockchain for large language Models in Figure 3. This taxonomy aims to succinctly explain the relevant interactions between various blockchain components and corresponding large language model vulnerabilities [RQ1][RQ3]. To further motivate and contextualize this taxonomy as well as our general discussion of existing literature, we introduce two definitions of safety and security as they apply to large language models [RQ2]. Moreover, we also provide multiple datasets relevant to BC4LLM in order to provide future researchers in the area with the tools to expand upon connections highlighted by the taxonomy and motivated by our definitions [RQ4].\n2. We also highlight several other important components of our paper, intended to support our main contributions but still relevant in their own regard as pertinent artifacts of the BC4LLMs space. One such artifact is our definitions of specific areas that are found within our definition of safety, further expanded upon in Table 3 [RQ2][R3]. These definitions add further weight to our definition of safety for large language models. We go on to bolster our definitions of both safety and security by purposefully focusing on the issue of privacy in the context of security, consequently reaffirming two terms as introduced by Yan et al. [122]: passive and active privacy [RQ1] [RQ3]. Also relevant in their own right but primarily proposed as supporting contributions are our contextualization of LLMs as a sub-field of various AI areas [RQ1], and our succinct taxonomic overview of blockchain components [RQ1][RQ3]."}, {"title": "2. BACKGROUND", "content": "In this section, we present an overview of blockchain as a distributed ledger technology and relate the abilities of large language models to their capacities as agents with respect to their nature as both AI models and their tendency to interact with vast quantities of data."}, {"title": "2.1 Blockchain", "content": "Since the introduction of Bitcoin as a decentralized currency by Satoshi Nakamoto in 2008 [79] there has been a subsequent explosion of academic and commercial interest in its underlying blockchain technology. Additionally, and as highlighted by the introduction of Vitalik Buterin's Ethereum blockchain in 2014 [17], there has been a particular focus on blockchain's potential applications in fields entirely disparate from digital currencies. The interest in blockchain, or distributed ledger technologies, stems from its guarantees about data sovereignty, transparency, and relative permanence. Concisely, these properties are often referred to as immutability and irrefutability. Ranging from many diverse fields such as health care record management, digital identity management, or tax auditing, these properties are widely applicable and highly desirable, even though the mechanisms through which we achieve them can be somewhat complex and opaque. In light of the oftentimes convoluted nature of blockchain systems, we introduce blockchain to the reader in a piecemeal fashion in order to emphasize the modular, yet interconnected nature of such systems. Figure 1 represents an overview of our characterization of blockchain systems in general. We purposefully exclude certain components such as the incentive mechanism, or wallets, as they are beyond the scope of our analysis of blockchain as a means to serve large language models."}, {"title": "2.1.1 Blockchain Components", "content": "Consensus Protocol. Of particular interest to the BC4LLM space, and arguably the most fundamental component within a blockchain, the consensus protocol is the governing system that controls how data is added to a blockchain's ledger. At its core is the consensus mechanism, which both ensures the validity of proposed data and fosters an environment of accountability, so that nodes submitting invalid information may be penalized accordingly. For example, the Proof of Work (PoW) consensus mechanism [79] is by far the most widely known. In it, nodes must solve a complex mathematical equation in order to gain rights to propose data for the blockchain. When such a node submits new data, it is scrutinized by every other node in the system. If the data is malicious, or untruthful, the proposal is rejected and the corresponding processing power performed by the malicious node has effectively been wasted, as that node will not receive the incentive, a Bitcoin reward. The underlying ideas of accountability, certain nodes being selected as 'block proposers', and the 'proof' of the ability to submit information to the chain are central ideas in consensus protocols across blockchains with different consensus protocols [81].\nVerifiable Ledger. At a blockchain's core sits the verifiable ledger, a repository of data bolstered by a secure way of maintaining the integrity of that data. Of note is the particular technique through which data itself is verified on the ledger: the Merkle tree [76], or a variation thereof. Typically implemented as a ground-up binary tree, data is stored in leaf nodes, with hashed pointers of that data cascading up the tree. This structure results in a comprehensive 'Merkle root', a hash pointer consisting of all the other hash pointers in lower levels of the tree, which is ultimately based on the data stored in the leaf nodes. This technique ensures the integrity of information in the leaf nodes, as any alteration to the data is instantly reflected in the Merkle root. Likewise, new additions to the Merkle tree can be checked against previous states of the tree via a recalculation of the Merkle root accounting for the new transactions. This technique, complementing the verifiable ledger, is often the key to LLM data provenance and traceability solutions that rely on blockchain technology.\nDecentralized Network. Critically, blockchain's are decentralized networks. That is, no central server or group of servers may assume control of the network in a way that would compromise the network's state of trustlessness. This is achieved through multiple avenues, such as the aforementioned consensus protocol, the distribution of the verifiable ledger among a large number of independent nodes, and the accessibility of a given blockchain's network. [27] In this way, no users in the network are required to trust any other user. This fundamental aspect of blockchain is responsible for already realized and potential advancements with LLMs concerning areas such as RAG, the training process, and even supply chain issues.\nLayer 2 Technologies. Apart from the fundamental components found within all blockchains themselves, there exist several external architectures that interface with blockchains and further enhance their applicability. Typically, these are referred to as layer 2 technologies, as they sit a 'layer' above the 'layer 1' blockchain. Increasingly relevant as blockchain's influence grows, layer 2 solutions are a burgeoning area with numerous novel research directions. Most prominent among these is the space concerning smart contracts, scripts that rely on a blockchain's security guarantees in order to facilitate off-chain transactions [140]. Also of note in the layer 2 field, and sometimes combined with the efficacy of smart contracts, are zero-knowledge rollups. Often used to strengthen scalability, zero-knowledge rollups batch unproposed transactions together, and instead of submitting the transactions themselves, submit proof that the transactions are indeed valid [103]. This allows for transactions to be added on-chain without the need for every full node to redo the calculations found within those transactions. This area of layer 2 technologies is pivotal as it relates to BC4LLM layer 2 has the necessary dynamism to react quickly to new and emerging LLM vulnerabilities."}, {"title": "2.2 Large Language Models", "content": "In recent years, large language models (LLMs) have grown in popularity as a driving force in artificial intelligence (AI), being used across various fields, such as trustworthiness [51, 25], scholarly document processing [142], signal processing [89], quantum computing [58], climate production [60, 59], software engineering [139], and healthcare [42] among multiple other learning environments. Zhao et al. [135] and Yang et al. [123] define LLMs and pre-trained language models (PLMs) from the perspectives of model size and training approach. Generally speaking, PLMs refer to language models that are pre-trained on large amounts of general text data and then fine-tuned for specific tasks. LLMs are a kind of PLM. The key distinction is that LLMs are generally larger in scale with more parameters. These large language models have demonstrated the ability to learn universal representations of language, used in various natural language processing (NLP) tasks [47], bolstering their applicability.\nLLMs are a sub-field of AI. Given this connection, discussions of blockchain for AI (BC4AI) research can also be applicable to LLMs. Figure 2 demonstrates the connections and following developments between AI, machine learning (ML), deep learning (DL), and LLMs. Al is largely discussed, yet we still see improvements in ML, such as FL that is better correlated to LLMs than general AI research. We then note DL as a subset of ML, showing advancements with neural network structures that are similar to how LLMS learn and operate. To define ML, we refer to Liu et al. [62] who state that ML is an automated learning process using algorithms and statistical models to efficiently perform specific tasks without the use of explicit instructions. DL is a sub-field of ML that utilizes multiple layers of artificial neurons to learn the latent space representation of the most basic form of data such as images, text, and speech signals. As described by Shafay et al. [93] deep learning has been able to achieve human-like accuracy, or even better accuracy than humans, on a variety of difficult tasks. Below, we elaborate upon the process of how models are trained and fine-tuned to become a usable, safe, and secure system."}, {"title": "2.2.1 Model Training", "content": "During the pre-training phase, the LLM is trained on a diverse, large dataset of textual data from various sources to learn the statistical properties of language. The LLM is equipped with a myriad of adjustable parameters, commonly reaching more than ten billion [47]. Due to the huge model size and the vast amount of data used to train it, it is computationally challenging to successfully train a capable LLM, requiring distributed training algorithms for learning the model parameters [135]. Another crucial factor for LLM training is the data itself. Data that models are trained on come from a wide variety of sources, but the data itself may not be up to date [101]. To mitigate this shortcoming, recent advancements have introduced Retrieval-Augmented Generation (RAG), which is designed to augment and rectify the information returned by LLMs by consulting up-to-date online sources. The data that the LLM was trained on also has other deficiencies, like knowledge gaps in healthcare fields where data is private and restricted [50]. Due to these knowledge gaps, the LLM may conjure up hallucinations where the model generates false information during prompting [72, 3] because of a lack of relevant information. However, hallucinations may also occur with a plethora of data available as they are inherent problems in LLMs. Methods of preventing these hallucinations are elaborated on in 4.2.2. RAG can help rectify hallucinations, and fill in the gaps of data the LLM is missing, by using up-to-date and validated information from trustworthy online resources. This method of data retrieval introduces novel vulnerabilities since the information gathered by the retriever is largely un-audited and may contain poisoned data or data that can lead to unsafe responses from the LLMs."}, {"title": "2.2.2 Model Tuning and Utilization", "content": "After pre-training, the parameters of LLMs can be further updated by training on domain-specific datasets in downstream tasks. This process is known as fine-tuning (FT) [16]. A kind of fine-tuning method called supervised fine-tuning (SFT), aims to improve LLMs' responsiveness to instructions, ensuring more desirable reactions involving three major components of instructions, inputs, and outputs. Inputs relate to prompting and the inputs depend on the instructions, similar to applications of open-ended generation in ChatGPT. By providing both inputs and outputs they form an instance, and multiple instances can exist for a single instruction [42]. Among fine-tuning, there are a multitude of other training techniques within prompting the model such as instruction tuning and alignment tuning. By FT from a mixture of multi-task datasets formatted via natural language descriptions with the use of instruction tuning, LLMs are enabled to follow task instructions for new tasks without needing explicit examples, highlighting the ability of generalization for instruction following [135]. However, LLMs can demonstrate versatility, even without FT where they produce a phenomenon known as zero-shot learning, exhibiting the ability to perform tasks for which the model was never explicitly trained [16].\nAlignment tuning, a form of reinforcement learning, is used to promote the LLM to be a safe interactive machine. Since LLMs are trained to capture the data characteristics of uncurated pre-training corpora involving both high-quality and low-quality data, the LLM can generate toxic, biased, or harmful content for humans. To mitigate this problem, a FT process based on reinforcement learning from human feedback (RLHF) is used to align the LLM with safety values in order to make a trustworthy model [135]. The RLHF process ranks LLM outputs, with rewards scaled to positive and negative values. The LLM is then trained to produce highly-ranked responses and avoid low-ranked responses. In healthcare, RLHF provides advantages to the model such as improved accuracy and reliability through continuous feedback from medical professionals and customizes the interactions based on real clinical settings and patient needs [41]. These advanced training techniques improve LLM's ability to generalize across tasks and improve their overall utility in various domains."}, {"title": "3. RESEARCH METHODOLOGY", "content": "The discussion of blockchain technology's incorporation into large language models necessitates a corresponding exploration into the implications of various terms and definitions found at that intersection. For example, due to the rapid emergence of LLMs, there exists an absence of consensus in describing common phenomena concerning LLM safety and security. To ameliorate this effect, we take care to stress opposing, but related, definitions of safety found within many different works in Table 2. In light of these distinctions, we offer two formal definitions of security and safety in order to contextualize these differing but similar areas of research. These definitions will also serve to highlight where particular blockchain technologies could be applied in their respective domains, and focus research efforts."}, {"title": "3.1 Research Approach and Limitations", "content": "Literature surveys often are limited in their depth and scope by unconscious factors that impact the authors' ability to fairly select papers for review. To be transparent, and to aid researchers conducting similar or future reviews, we outline our research approach and its associated limitations. While conducting our research, we used the search engine Google Scholar, and several databases including ACM Computing Surveys, IEEE Xplore, SpringerLink, and arXiv. We chose these databases as they either produce quality research and contribute to the growth of interest in novel areas, or in the case of arXiv have the most up-to-date papers available. With Google Scholar, we used keyword searches such as \"blockchain for LLMs\" and \"blockchain-based LLMs\" as starting points for relevant, intriguing research papers.\nTo solve problems concerning scope and interrelated domains of disparate areas, we gathered various applications of blockchain for AI, blockchain-enabled machine learning, federated learning, and deep learning tactics to apply them to LLMs. Lastly, of note is the fact that we were largely aided in this further research effort by a waterfall approach to finding research papers. That is, we found several foundational papers in the BC4LLM field, explored citations in those papers, and subsequently explored citations in those secondary papers. We continued investigating relevant citations in this waterfall fashion until we reasonably exhausted all relevant articles. Admittedly, this method of finding prominent research articles is limited in its natural tendency to develop blind spots to less well-known research articles or venues."}, {"title": "3.2 Model of Threat Categorization", "content": "There exist a wide variety of threats which affect LLMs. Oftentimes, many of these threats originate from the nature of LLMs acting as AI systems. In Figure 3 we refer to these vulnerabilities similarly to Yao et al [124] who categorized the most extensively discussed LLM vulnerabilities and Al-inherent vulnerabilities together, yet also included external threats under non-AI inherent vulnerabilities. We contribute further by applying these vulnerabilities to each respective process within developing a large language model and tie these to respective applications of blockchain. Beginning with the training process, LLMs are prone to threats such as data poisoning and backdoor attacks. As defined in Yao et al. [124] data poisoning is where attackers influence the training process by injecting malicious data into the training set, introducing vulnerabilities within the security and effectiveness of the model. Following the trend of poisoned data, there can be backdoor attacks implemented on the training data, as defined in Li et al. [56] who categorize backdoor attacks into attacks on training data and attacks on local models. The backdoor attacks on training data are further divided into attacks based on label flipping and attacks based on planting triggers. Attacks based on label flipping focus on manipulating the labels, whereas attacks on planting triggers modify the input data and labels, effectively constructing an adversarial sample. Then, attacks on local models are further divided into attacks based on modifications to the training process and attacks based on manipulating the trained model [56]. The backdoor attacks can be applicable to both the training and prompting phases of LLMs when using this distinction.\nRAG attacks have a variety of issues, including privacy issues [127][5] and knowledge poisoning attacks [143]. For RAG specifically, Xue et al. [121] propose BadRAG to identify security vulnerabilities, exposing direct attacks on the retrieval phase from semantic triggers, and uncovering indirect attacks on the generative phase of LLMs that were caused by a contaminated corpus. These RAG-specific attacks and defenses are elaborated on in section 4.1.2. When interacting with a LLM, we see Al's inherent vulnerabilities become relevant, as in Yao et al. [124] note that LLMs are fundamentally AI models themselves. We focus on the prevalent adversarial attacks that malicious users may use to tamper with the LLM, attempts to find out sensitive information, or try to ruin the system entirely. We recognize jailbreaking and prompt injection as two separate but similar types of adversarial attacks that are initiated within prompting. For instance, jailbreaking prompts are designed to bypass the restrictions set by service providers during model alignment or other containment approaches [96]. Whereas prompt injection attacks aim to override an LLM's original prompt and directs it to follow a set of malicious instructions, leading to erroneous advice or unauthorized data leakage [66]. In our Sections 4.2.1, 4.2.2 we discuss instances of misinformation and passive privacy leakage addressed as safety concerns. Note that from above, we included backdoor attacks based on modifications to the trained model in prompting, since these backdoor attacks can still happen after the model has been trained [56].\nOther attacks relating to LLMs and AI are membership inference attacks (MIA), relating to a type of privacy attack where some malicious user, given access to the model, is able to determine whether a given point was used to train that model with high accuracy [80]. However, Neel and Chang [80] state this attack is more related to information about the training point data leaking through the model, and that malicious users must have access to a candidate point in order to run the attack. Therefore this attack is more prevalent with passive privacy, highlighting the need for preventing data leakage. Similar attacks are user inference attacks that seek to gain knowledge or insights about the model or data's characteristics, often by observing the model's responses or behavior [124].\nLastly, we explore denial of service (DoS) attacks and supply chain vulnerabilities. From Yao et al. [124] a DoS attack is a type of cyber attack that aims to exhaust computational resources, resulting in latency or making the technology resources unavailable. We focus on distributed denial of service attacks (DDoS) which is a type of DoS attack where requests flood the system, attacking simultaneously from multiple sources on the network [29]. Yao et al. [124] also defined LLM supply chain vulnerabilities, referring to the risks in the lifecycle of LLM applications that may occur from using vulnerable components or services, including third party plugins that may be used to steal chat histories, access private information, and or execute code on a user's machine [124]. All of these security vulnerabilities are substantial threats to LLMs that need to be mitigated or prevented. Possible methods of defense are discussed within Section 4.1, using current blockchain frameworks and experiments for these security problems, as listed by each developmental phase of the LLM, AI inherent threats, and supply chain issues."}, {"title": "4. EXISTING LITERATURE ON BC4LLM", "content": "Independently, the fields of both LLMs and blockchain research have grown substantially over the past several years. It is no surprise that the literature surrounding these topics has begun to morph and relate to each other. In previous research, we have seen LLMs for Blockchain Security [42] as well as an introduction to the term BC4LLM in Luo et al [69] where they provide a comprehensive survey of blockchain for LLMs. However, they do not acknowledge the multitude of safety and security solutions that blockchain provides for certain LLM vulnerabilities. Effectively, Luo et al. [69] aims to introduce BC4LLM for trusted AI, enabling reliable learning corpora, secure training processes, and identifiable generated content. In juxtaposition, our survey aims to analyze possible BC4LLM solutions closely related to our definitions of safety (2) and security (1) when looking at inherent system vulnerabilities in LLMs. To begin our analysis, we define these security problems based on previous work and highlight areas of research that are applicable to areas of BC4LLM safety and security."}, {"title": "4.1 Blockchain for Large Language Model Security", "content": "Few papers and experiments analyze how the integration of these two technological powerhouses interact with one another. We have seen benefits of this integration that apply to our definition of security (1). Balija et al. [10] introduced a peer-to-peer (P2P) federated LLM, namely PageRank, which works with a blockchain. This system operates in a fully decentralized capacity. Demonstrably, the blockchain implementation led to more efficient accuracy and latency results. With that being stated, Balija et al. [10] provide a developing direction in the field of BC4LLM to enhance system security. Below, we address several current vulnerabilities in LLMs and analyze them individually. In order to better understand these security problems, we categorize these vulnerabilities to their respective LLM training stages, highlight blockchain for AI works, and provide well-researched blockchain applications as a solution.\nVulnerabilities are present at each step in the process of developing a LLM. In early methods of model training, we encounter adversarial attacks such as data poisoning and backdoor attacks within the corpus [96, 122]. Progressing into model fine-tuning and general use, the LLM can fall victim to prompt-based attacks, [2][134] [122][66], inference attacks [44] [54], and RAG related attacks [121] [22][5][26][143] [127]. These attacks are common vulnerabilities in both LLMs and AI since LLMs and AI are closely related as seen in Figure 2. Some of the threats against LLMs can be addressed by implementations from blockchain for AI (BC4AI) research, as elaborated below in Section 4.1.3. Considering the volume of potential attacks against LLMs, we make a further distinction of solutions that are specifically related to BC4LLM research and other blockchain-based solutions from BC4AI research. With this, we are able to highlight shared vulnerabilities for LLMs and AI. We provide an analysis of how blockchain can help defend against and mitigate these vulnerabilities, starting with threats during each phase of LLM training and utilization, continuing onto different blockchain solutions for AI inherent threats, and lastly noteworthy technology inherent attacks such as denial of service (DDoS) attacks and issues with supply chain logistics."}, {"title": "4.1.1 Blockchain for Threats within LLM Training Process", "content": "Situated at a critical juncture in the process of model development is the issue of data selection. An area of paramount concern for this data is exactly how we can verify that training data is authentic and safe as well as tamper-proof from data poisoning attacks. A new approach may be the LLM's ability to unlearn this poisoned data, or data that is unsafe according to our definition of safety 2. From Zuo et al. [145] they establish federated TrustChain as a method of enhancing LLM training and unlearning through a blockchain-based federated learning framework. By integrating blockchain using Hyperledger, their findings present that the framework is efficient in unlearning capabilities, showcasing an accuracy score initially of 99.15% and after unlearning, the accuracy drops to 0.70% [145]. This highlights the potential for BC4LLM to improve security and privacy guarantees, where the LLM can selectively forget specified data points while simultaneously preserving their performance with the leverage of Low-Rank Adaptation (LoRA) and tuning hyper-parameters. This method of a blockchain-enabled federated unlearning process is further detailed as a future research possibility that has been thoroughly explored by few, as emphasized later in Section 7.1.\nAnother significant problem in the area of adversarial threats against LLMs is data poisoning. Gong et al. [37] proposed a possible blockchain solution, introducing dynamic large language models (DLLM) on blockchains. Instead of using the traditional centralized data sets that LLMs are provided with, developing LLMs on blockchains enables the creation of decentralized datasets. These datasets are less likely to be tampered with and can be easily audited for accuracy. Gong et al. presents DLLM to evolve after the training process. This was implemented through neural network parameters that offer the LLM the ability to learn during its usage.\nAdditionally, blockchain-based systems can help assess where data poisoning may occur and as shown in [54] blockchain can protect datasets and detect potential inference attacks through a two-level privacy preserving module. This research included a framework based on blockchain and deep learning, including two levels of privacy mechanisms as shown in Figure 4. For the first level, Keshk et al. [54] used SHA512 to generate secure hashes and then implemented an enhanced-proof-of-work (ePoW) technique for authenticating data records and preventing data poisoning attacks. The second level consisted of a VAE technique for converting original data into an encoded format for mitigating inference attacks that could be learned from system-based machine learning. In their testing, these mechanisms were effective in preventing data poisoning and inference attacks from manipulated smart power network datasets. BC4LLMs could benefit from this similar type of implementation, working with secure methods of hashing and blockchain-based deep learning privacy preservation techniques. By integrating a two-level privacy-preserving module, BC4LLMs can ensure data integrity and confidentiality while effectively detecting and mitigating both data poisoning and inference attacks.\nPoisoned data has been an interest with RAG in particular. For example, Xue et al. [121] developed a way to identify security vulnerabilities from a poisoned corpus, but they do not use blockchain as a solution. We address the absence of research on blockchain and RAG, especially when using blockchain to help prevent RAG security issues. We discuss this as a possible future research direction as there remains a current gap in research of blockchain-based RAG systems and elaborate on this topic in 7.2. Poisoned data overall is a major concern within LLMs and we offer blockchain as a potential source of ground truth to aid in mitigating this threat during the pre-training stages of LLMs and potentially mitigate RAG security concerns. In addition to data poisoning, LLMs are susceptible to backdoor attacks hidden in the training data during the LLM pre-training phase. Notable research by Zhao et al. [134] introduced ProAttack which improves the stealth of backdoor attacks by accurately labeling poisoned data samples. As these attacks improve and become more sophisticated, it is crucial to explore robust defense mechanisms for LLMs. Few defense mechanisms using blockchain have been made, but we see in Li et al. [56] a proposal for a blockchain-based federated-learning framework (DBFL) that withstands backdoor attacks in a blockchain environment through incorporating an RLR aggregation strategy into the aggregation algorithm of a user and the addition of gradient noise to limit the effectiveness of backdoor attacks. The robustness of FL against backdoor attacks is enhanced by using various blockchain functions, including digital signature verification and simulation of chain resynchronization [56]."}, {"title": "4.1.2 Blockchain for Threats within LLM Prompting and Utilization", "content": "LLMs undergo continuous training through instruction tuning, alignment tuning, and fine-tuning, which warrants a list of vulnerabilities such as prompt injection [74"}]}