{"title": "REVISITING CONVOLUTION ARCHITECTURE IN THE REALM OF DNA FOUNDATION MODELS", "authors": ["Yu Bo", "Weian Mao", "Yanjun Shao", "Weiqiang Bai", "Peng Ye", "Xinzhu Ma", "Junbo Zhao", "Hao Chen", "Chunhua Shen"], "abstract": "In recent years, a variety of methods based on Transformer and state space model (SSM) architectures have been proposed, advancing foundational DNA language models. However, there is a lack of comparison between these recent approaches and the classical architecture-convolutional networks (CNNs)\u2014on foundation model benchmarks. This raises the question: Are CNNs truly being surpassed by these recent approaches based on transformer and SSM architectures? In this paper, we develop a simple but well-designed CNN-based method, termed ConvNova. ConvNova identifies and proposes three effective designs: 1) dilated convolutions, 2) gated convolutions, and 3) a dual-branch framework for gating mechanisms. Through extensive empirical experiments, we demonstrate that ConvNova significantly outperforms recent methods on more than half of the tasks across several foundation model benchmarks. For example, in histone-related tasks, ConvNova exceeds the second-best method by an average of 5.8%, while generally utilizing fewer parameters and allowing faster computation. In addition, the experiments observed findings that may be related to biological characteristics. This indicates that CNNs are still a strong competitor compared to Transformers and SSMs. We anticipate that this work will spark renewed interest in CNN-based methods for DNA foundation models. Code is available at:\nhttps://github.com/aim-uofa/ConvNova", "sections": [{"title": "INTRODUCTION", "content": "Consistently at the forefront of scientific research, DNA has catalyzed significant advancements across a multitude of fields, including aging research, synthetic biology, and disease treatment. In contrast, unlike NLP, comprehending DNA poses a formidable challenge, owing to the myriad undiscovered biological principles underlying its structure and function. Fortunately, the rapid progress of genome language modeling has showcased its dominance in numerous subsequent applications. These include the prediction of promoters, gene expression, DNA methylation, analysis of chromatin state, prediction of promoter-enhancer interactions, TF-DNA binding prediction, variant effect prediction, and gene network prediction.\nVersatile sequence models have been established as foundational models for genetics, typically falling into two categories: Transformer-based methods (e.g., DNABERT, DNABERT-2, NucleotideTransformer) and SSM-inspired methods (e.g., HyenaDNA, Caduceus). These approaches have made significant progress in genomic language modeling tasks and are now widely adopted. Prior to these developments, however, convolutional neural networks (CNNs) were the dominant technique for DNA modeling in bioinformatics, giving rise to notable models such as Basset, Bassenji, and LegNet. Despite their success, CNN architectures have not been systematically compared to the newer DNA foundation models in recent research. Comparisons between CNNs and modern approaches have been limited to specific domains, such as NTv2 and Caduceus. In these cases, only a few domain-specific CNN models, like Spliceator and some benchmark baselines, have been evaluated.\nIn this landscape, we rethink and re-evaluate whether CNN methods are indeed less effective than the current leading paradigms, including Transformer-based methods and SSM-inspired methods, which have been the primary focus of recent research in DNA foundation models. The motivation behind this is straightforward, as CNNs still hold advantages over existing methods: 1) The input lengths for downstream tasks on DNA vary greatly (ranging from tens to thousands). Transformers are not robust to sequence length variations, and their performance can be affected. 2) Transformers have a computational complexity of O(n\u00b2) , higher than CNN methods. Additionally, to avoid overly long sequences, transformers also require tokenizers, which affect translational invariance. 3) Mamba is better suited for tasks involving long sequences with autoregressive properties, while DNA sequences typically do not exhibit autoregressive characteristics. 4) CNNs possess a local inductive bias, which can provide better data utilization efficiency and are more friendly towards tasks with small training data volumes.\nBased on this motivation, we propose a simple yet well-designed CNN method, named ConvNova. Extensive empirical experiments demonstrate that ConvNova outperforms recent Transformer and SSM-inspired methods on more than half of the evaluated tasks, indicating that CNNs continue to demonstrate superiority. Through an analysis of the design of ConvNova, we identify and propose three key designs that enable CNNs to achieve superior performance: 1) dilated convolution, 2) gated convolution, and 3) a dual-branch framework for the gated convolution. Specifically, we found that for DNA tasks, increasing the receptive field of the CNN through downsampling (U-Net style) can severely degrade CNN performance. This stands in contrast to observations in other fields, such as computer vision. However, dilated convolution can circumvent this issue by enlarging the receptive field without the need for downsampling. Additionally, gated convolution can significantly enhance CNN performance. We hypothesize that this is because DNA may contain a substantial amount of irrelevant segments, and the gating mechanism can suppress this information. Lastly, we discovered that splitting the network pathways into two branches, with one branch exclusively providing gating signals to the other, can improve network performance. For detailed designs, please refer to \u00a73 and \u00a74.\nComprehensive experiments demonstrate the superiority of ConvNova. On the NT benchmarks, ConvNova achieves SoTA performance on 12 out of 18 datasets. Notably, in the H3K4me3 task, it outperforms the second-best method by 10.5%, and in the histone-related tasks, it surpasses the"}, {"title": "PRELIMINARIES AND RELATED WORK", "content": "Transformers have become a dominant force across various fields in the deep learning community, and recent research has begun to explore their potential within the realm of genomics. DNABERT was the first to establish the transformer as a foundational model for DNA, utilizing k-mer tokenization. Nucleotide Transformer (NT) scaled up the transformer model (with sizes of 500M and 2.5B) and investigated the impact of different pretraining datasets (HG38\u00b9 and Multispecies\u00b2) on 18 downstream tasks. Their experiments demonstrate that multispecies data and larger models generally yield superior performance. DNABERT-2, also pretrained on multispecies data, tackled several problems encountered with DNABERT. They further explored the Byte Pair Encoding (BPE) tokenization and ALiBi positional embedding as effective strategies for modeling DNA with transformer models. Additionally, they utilized FlashAttention to accelerate the processing. Meanwhile, BigBird, despite not being specifically designed, had also been applied in genomics.\nWhile adept at capturing long-range dependencies, transformer-based models in genomics face challenges compared to CNNs. CNNs possess an inductive bias towards local, translation-invariant features, aligning well with many genomic patterns. In contrast, transformers lack this inherent bias for local structure modeling. Additionally, the attention layer's O(n\u00b2) complexity poses computational challenges, especially for long DNA sequences."}, {"title": "SSM-INSPIRED DNA MODELS", "content": "Another approach that has gained traction involves the use of State Space Models (SSMs). HyenaDNA is a decoder-only, sequence-to-sequence model that incorporates a modified convolution operator (Hyena). This innovative approach enables the expansion of input sequences of the model to 1M-size nucleotides and significantly outperforms transformer-based models in a variety of tasks with a model size that is 300 times smaller. Additionally, Mamba"}, {"title": "CNN-BASED DNA MODELS", "content": "Convolutional Neural Network (CNN)\u2013based DNA models are powerful deep learning tools designed to analyze and interpret DNA sequences by automatically extracting significant motifs and patterns. Early models like DeepBind predicted the sequence specificities of DNA and RNA-binding proteins, while DeepSEA used CNNs to predict the effects of noncoding variants. Basset advanced the field by learning the regulatory code of the accessible genome using deep CNNs.\nRecent advancements have further enhanced the capabilities of CNN-based genomic models. Enformer integrates convolutional layers with attention mechanisms to predict gene expression from DNA sequences, effectively capturing short- and long-range regulatory interactions. Borzoi builds upon the Basenji family by incorporating dilated convolutions and skip connections, improving performance on various genomic prediction tasks. LegNet employs a convolutional architecture for predicting gene expression and single-nucleotide variant effects in regulatory regions, achieving first place in the DREAM 2022 challenge.\nThese models demonstrate the CNNs' strengths, capturing local sequence features and motifs crucial for understanding genetic information. However, despite their success, they have not been incorporated into the broader consideration of DNA foundation models."}, {"title": "METHOD", "content": "In this section, we first present our overall network architecture (\u00a73.1), followed by an introduction to the core component of this architecture - the Gated Convolution Block (\u00a73.2). Subsequently, we delve into the selection of convolutional network methods, specifically whether to use dilation or down-sampling (\u00a74.4). Finally, we discuss our pretraining approach and downstream usage (\u00a73.3)."}, {"title": "CONVNOVA", "content": "Figure 2 shows the overall illustration of ConvNova. DNA sequences are mapped to the hidden space through a convolution operation, followed by N gated convolution blocks (GCBs), and finally processed through a multilayer perceptron (MLP). Different output heads are used in pretraining and downstream tasks. Each GCB utilizes dilated convolution to increase the receptive field and aggregate the features through a dual-branch structure."}, {"title": "GATED CONVOLUTION BLOCK", "content": "Yu et al. introduce gated convolution as a solution to a problem inherent to standard convolution, which indiscriminately treats all input pixels as valid. This method enhances partial convolution by offering a learnable dynamic feature selection mechanism for each channel at every spatial location throughout all layers. This mechanism allows for more nuanced and effective feature selection, improving the performance of convolutional neural networks.\nWith the motivation to effectively retain and forget information, we propose Gated Convolutional Blocks (GCBs). The dual-branch structure of these blocks is designed to facilitate independent feature extraction, thereby promoting complementary representation learning. Let's define A \u2208 \u211d^(l\u00d7d) as the feature extracted by the left branch and B \u2208 \u211d^(l\u00d7d) as the right branch feature.\nWe first process the left branch feature A through a LayerNorm layer and then pass it through a convolution layer, followed by the GELU activation function to obtain the intermediate features h of the current layer. In the other path, the right branch features B are processed through other"}, {"title": "MLM PRETRAINING AND DOWNSTREAM USAGE", "content": "We use the bidirectional masked language model (MLM) pretraining method and have observed significant performance improvement in downstream tasks.\nIn this work, 10% of the nucleotides in a primary DNA strand are masked, and the model is trained to predict the type of the masked tokens. Therefore no extra labels are required. The objective of this pretraining is to infer the type of nucleotides that have been masked by utilizing the surrounding nucleotides. The optimization objective is to minimize the cross-entropy loss. The pretraining data is HG38, same as HyenaDNA.\nUpon completing the pretraining phase for 400 epochs, we proceed to finetune ConvNova on the downstream tasks, keeping the entire model adjustable. We employ the pooling"}, {"title": "EXPERIMENTS", "content": "Some other studies have demonstrated that adopting multispecies data for pretraining can enhance downstream task performance. However, in this work, all pretraining tasks are performed on the human reference genome\u00b3 to align with the previous work (HyenaDNA and Caduceus). In addition, we employ a tokenization scheme at the character or base pair level, which has been proven to function well in SSM-inspired models. We also show the speed comparison between ConvNova and other models along sequence length (103 to 106). At a sequence length of 106, ConvNova is 1.35 times faster than its fastest counterpart-HyenaDNA, as shown in Figure 1. Please refer to A.1 for additional details on the pretraining dataset and recipes."}, {"title": "SHORT RANGE", "content": "We start the evaluation with the recently proposed Nucleotide Benchmark. These datasets encompass three types of tasks: histone marker prediction, regulatory annotation prediction, and splice site annotation prediction.\nTo assess performance, we follow the methodology outlined in, applying different metrics depending on the task: Matthews Correlation Coefficient (MCC) for histone marker"}, {"title": "GENOMIC BENCHMARK", "content": ""}, {"title": "LONG RANGE", "content": ""}, {"title": "BEND GENE FINDING", "content": "Gene finding is a multiclass classification task aimed at predicting the nucleotide types within DNA sequences (exons, introns, donors, acceptors, noncoding regions). This task"}, {"title": "CHROMATIN PROFILE PREDICTION", "content": "This classification task focuses on predicting chromatin profiles and epigenetic markers from DNA sequences, which is crucial for quantifying the functional effects of non-coding variants. The dataset comprises 919 chromatin features, including transcription factor (TF) binding profiles, DNase I hypersensitive sites (DHS), and histone mark (HM) profiles. For a given DNA sequence of length 1,000 bps, the task involves jointly predicting 919 binary classes corresponding to the chromatin profile of a central region of the sequence. We fine-tune our pretrained ConvNova models for 10 epochs, along with all baseline models, which range from 436K (tiny HyenaDNA) to 500M (NT-v2). Notably, ConvNova outperforms all baseline models, including DeepATT, which is a previously state-of-the-art supervised model for this task, as illustrated in Figure 3.\nMoreover, we provide compelling evidence of ConvNova's scalability when ample data is available. As we augment the model's parameter count from 6.8M to 51.1M, ConvNova shows a discernible improvement in the performance capabilities exhibited. See Appendix A.2.2 for experiment details and comparing different model sizes and training methods, whether pretrained or from scratch. Remarkably, without pretraining, ConvNova outperforms all Hyena-DNA models and DeepATT under the same epoch hyperparameter settings, even with a smaller parameter size."}, {"title": "ABLATION", "content": ""}, {"title": "DILATION OR DOWNSAMPLING", "content": "In image processing, the U-Net architecture has been proven highly effective through its use of downsampling and upsampling. Downsampling reduces image size while increasing the receptive field, enabling the capture of broader contextual information. Additionally, dilation, another method to increase the receptive field, expands the convolutional operation's receptive field by introducing gaps within the convolution kernel.\nIn DNA sequence modeling tasks, both the upsampling-downsampling structure and dilation convolution represent potentially viable approaches. For the dilation mechanism, we use the design shown in Figure 2. For the downsampling architecture, we implement a variant of ConvNova with U-Net style, maintaining the gating mechanism. We retain the same pretraining settings and parameter size (1.7M) and conduct comparative experiments on randomly selected tasks within the NT benchmark.\nDue to the significant performance gap observed as shown in Table 4, we opt not to conduct multiple experiments using ten different random seeds. Instead, we only utilize a single random seed. It is evident that while the downsampling (U-Net style) architecture can approximate the performance achieved using dilation in the 'promoter all' task, it significantly lags behind in other tasks, even decreasing by 50 points in 'splice sites all'."}, {"title": "KEY DESIGNS", "content": "We conduct ablation experiments on the gate mechanism and the double-branch design in ConvNova. We implement an ordinary gate convolution model with the same parameter count (1.7M) named \u201cSingle Branch\u201d to compare with the double-branch structure, and an additive aggregation model (1.7M) named \"w/o Gate\" in Table 5 to assess the role of the gate mechanism. We select homologous protein tasks from the NT benchmark and maintain the same method of selecting ten random seeds as described in \u00a74.1.1 for comparison. For the details, refer to \u00a7A.3.1.\nThe results indicate that both the feature aggregation method and the gate mechanism in ConvNova are crucial design components, effectively supporting the overall model capability.\nFor detailed ablation on kernel size, dilation rate, please refer to \u00a7A.3.2."}, {"title": "DISCUSSION", "content": "We utilize dilation to control the receptive field in histone tasks from the NT-benchmark. H3K4me2, H3K4me3, and H3K14ac demonstrate significantly improved classification accuracy when the receptive field covers approximately 15% of the input length, compared to a global receptive field.\nThe enhancement effects of H3K14ac (Figure 6A) and H3K4me3 (Figure 6C) align with previous biological research"}, {"title": "CONCLUSION", "content": "In this study, we revisit the convolutional paradigm for DNA modeling. Through extensive analysis and experimentation, we show that convolutional neural networks maintain a competitive edge in DNA modeling tasks. This reevaluation prompts a reconsideration of CNNs, a relatively early method, for such functions within the community. We have introduced ConvNova, featuring three key design elements: dilated convolution, gated convolution, and a dual-branch structure. With these innovations, ConvNova achieves state-of-the-art performance on multiple benchmarks with small model sizes. Additionally, we have investigated the varying demands for receptive field sizes across different tasks. While some tasks exhibit expected behaviors, others may suggest the presence of undiscovered biological phenomena.\nOne limitation is that this work does not consider the multi-species pre-training dataset. Furthermore, we have not considered the specific genomic regions used in pretraining; it is possible that training exclusively in known functional regions could lead to improved performance. Furthermore, the current foundation model benchmarks are primarily limited to classification tasks, which results in a lack of diversity in the types of tasks evaluated. The positive social impact is that this work can accelerate DNA research. No negative social impact is perceived."}, {"title": "APPENDIX", "content": "All the experiments are conducted with 4 RTX-4090 GPUs."}, {"title": "PRETRAINING", "content": "We adhere to most of the pretraining hyperparameters and settings used for HyenaDNA, including the dataset selection. However, we implemented some small differences. We pretrained on sequences with a maximum length of 16K, while other pretraining sequence lengths were 1K or 2K, aligning with HyenaDNA's recommendation to use sequences 2-4 times longer than those required for downstream tasks. We set the global batch size to 512 and trained for 400 epochs with a learning rate of 1e-3. For MLM pretraining, selected nucleotides were \"masked\" by replacing them with \"N\" to predict the original nucleotide type. Table 7 and Table 8 illustrate the parameter settings for models of various sizes and the time needed to pretrain. The pretraining dataset is HG384."}, {"title": "DOWNSTREAM TASKS", "content": "All hyperparameters for the ConvNova model on downstream tasks can be found in Table 9."}, {"title": "NUCLEOTIDE TRANSFORMER BENCHMARK", "content": "Objective: Sequence classification."}, {"title": "CHROMATIN PROFILE PREDICTION", "content": "Objective: Perform 919 binary classification tasks on DNA fragments using 4.4 million training data points. Even small improvements (<1%) are significant for this challenging task.\nModels and Setup: The baseline models include HyenaDNA (0.4M, 3.3M, 6.5M), NT-v2 (50M, 100M, 250M, 500M), DNABERT-2, and DeepATT (previous state-of-the-art CNN for this task), trained for 10 epochs. All models are tested using 32-bit floating point precision.\nExperiment Configuration: The train/test split follows the DeepSEA paper methodology. The optimizer used is AdamW with settings from Table 7, and a learning rate of 1e-3 for ConvNova, with official learning rates used for the baselines.\nWe provide more detailed comparisons of different models in Table 10. Besides the pretrained ConvNova model, we also train ConvNova from scratch. Remarkably, even without pretraining, the model outperformed all versions of HyenaDNA at similar parameter sizes, further demonstrating ConvNova's superior ability to encode long-sequence information. Hyperparameters can be found in Table 9."}, {"title": "GENEFINDING", "content": "Objective: Classify each nucleotide based on its gene structure context, predicting exon-intron boundaries and capturing long-range dependencies for accurate annotation.\nModels and Setup: The baseline models include NT-H, DNABERT2, HyenaDNA, and GENA_LM. ConvNova is tuned for 10 epochs, while the results of the other baselines are taken from BEND.\nExperiment Configuration: The optimizer used is AdamW with settings from Table 7, and a learning rate of 1e-3 for ConvNova.\nDilation and Layer impact on Gene Finding We find that the gene-finding task requires long-range dependency. Both decreasing dilation rate and the number of layers can result in performance decline in Figure 4. Models are trained from scratch in all experiments. For Dilation impact, the model dimension is 256 and contains 5 GCBs. For GCB counts impact, the model dimension is set to 256, and the dilation rate is set to 4. For additional hyperparameter settings in this task, readers can refer to Table 9."}, {"title": "GENOMIC BENCHMARK", "content": "Objective: Sequence classification task."}, {"title": "GATING MECHANISM AND DUAL BRANCH", "content": "In this section, we provide more details about the ablation experiments. There are two variants of the ConvNova model: \"Single Gate\" and \"Dual Branch\", and one variant without any gating mechanism.\nSingle Gate Our \"Single Gate\" design follows a sequential process as shown in Eq.2. First, a single Conv1D layer processes the input feature A \u2208 \u211d^(l\u00d7d) with weight matrix Wa \u2208 \u211d^(k\u00d7d\u00d7d) and bias term ba \u2208 \u211d^d to produce z \u2208 \u211d^(l\u00d7d). This output is then processed through two parallel activation paths - GELU activation and sigmoid activation (\u03c3). The final output is obtained through residual addition of element-wise multiplication (O) of activated h and gated g. To maintain parameter count parity with ConvNova, we increase the dimension of the Conv1D layer.\nz = Wd * A + bd\nh = GELU(z)\ng = \u03c3(z)\nA = A +h\u25ccg (2)\nDual Gate Our \"Dual Branch\" design processes input features through two parallel convolutional paths with gating mechanisms, as shown in Eq.3. The inputs A, B \u2208 \u211d^(l\u00d7d) are processed through two separate Conv1D layers with weights W1,d/\u221a2, W2,d/\u221a2 \u2208 \u211d^(k\u00d7d\u00d7d/\u221a2) and biases b1,d/\u221a2, b2,d/\u221a2 \u2208 \u211d^(d/\u221a2) to produce intermediate features Z1, Z2 \u2208 \u211d^(l\u00d7d/\u221a2). The first path applies GELU activation to z1, while the second applies sigmoid activation to z2. The final output A is obtained through residual addition of element-wise multiplication (0) of activated h and gated g, and B is obtained through residual addition of gated g."}, {"title": "DILATION MECHANISM, KERNEL SIZE AND LOCAL DEPENDENCY ANALYSIS", "content": "In this section, we present the results of the ablation experiments on the dilation rate and kernel size. We perform experiments on the NT benchmark, testing kernel sizes of 3, 5, 7, 9, 11 and dilation rates of 1, 2, 3, 4. We report the average results across 18 tasks from the entire benchmark.\nAs shown in Table 11, the performance generally improves with increasing dilation rate and kernel size. However, to balance compatibility with long-range tasks and maintain relatively small model parameters, we ultimately select a kernel size of 9 and a dilation rate of 4.\nFurthermore, we observe that even for short-range tasks (all having a sequence length of 500 bp), the intensity of local dependencies varies significantly. For instance, the tasks H3K4me2, H3K4me3, and H3K14ac, as discussed in \u00a75, exhibit strong local dependencies, with better performance observed when the dilation rate is small. On the other hand, tasks like splice site prediction show a much stronger global dependency, which benefits from a larger dilation rate."}, {"title": "SUPERVISED METHODS AGAINST FOUNDATION MODELS", "content": "We conduct a comparison between previous supervised models and DNA foundation models on both the NT benchmark and genomic benchmark. For our comparison, we select the popular Basenji and the recently established state-of-the-art model, LegNet, which excels in short DNA regulatory regions. Originally, these supervised models are designed for specific tasks. We apply them to the benchmarks used for foundation models. As observed in Table 13 and Table 14, although they show decent performance on specific tasks, they do not achieve the same level of consistency across various datasets as foundation models do. This inconsistency may arise from the enhanced modeling capabilities afforded by pretraining in foundation models."}, {"title": "NEIGHBORHOOD IMPORTANCE IN DNA MODELING", "content": "To support our hypothesis that the inductive bias of CNNs-emphasizing neighboring nucleotides-is beneficial for DNA modeling, we make modifications to the Rotary Position Embedding (ROPE) mechanism. Specifically, we adjust the original \u03b8 values, enabling each attention head to have distinct \u03b8 values.\nAdditionally, we modified the initialization of the bias term in the linear layers of the K and Q projections. The bias was initialized to [0, 0, 0, 0, 0, . . ., 1], while all other parameters retained the initialization strategy of NTv2 (standard deviation \u03c3 = 0.02, mean \u00b5 = 0). This adjustment ensures that the initialization process places greater emphasis on neighboring nucleotides. Refer to Figure A.5 for an illustration of the improved attention map, which places greater emphasis on neighboring tokens.\nTo validate our hypothesis, we train NTv2 from scratch on three tasks where ConvNova with small dilation values performs better (H3K4me2, H3K4me3, and H3K14ac). The results, presented in Table"}]}