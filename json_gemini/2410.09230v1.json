{"title": "IMPROVING SEMANTIC UNDERSTANDING IN SPEECH LANGUAGE MODELS VIA BRAIN-TUNING", "authors": ["Omer Moussa", "Dietrich Klakow", "Mariya Toneva"], "abstract": "Speech language models align with human brain responses to natural language to an impressive degree. However, current models rely heavily on low-level speech features, indicating they lack brain-relevant semantics which limits their utility as model organisms of semantic processing in the brain. In this work, we address this limitation by inducing brain-relevant bias directly into the models via fine-tuning with fMRI recordings of people listening to natural stories-a process we name brain-tuning. After testing it on 3 different pretrained model families, we show that brain-tuning not only improves overall alignment with new brain recordings in semantic language regions, but also reduces the reliance on low-level speech features for this alignment. Excitingly, we further show that brain-tuning leads to 1) consistent improvements in performance on a range of downstream tasks and 2) a representational space with increased semantic preference. Our results provide converging evidence, for the first time, that incorporating brain signals into the training of language models improves the models' semantic understanding.", "sections": [{"title": "1 INTRODUCTION", "content": "It is an exciting time for the cognitive neuroscience of language with the rise of language models which have been shown to align with (e.g. predict) brain activity evoked by natural language to impressive and unprecedented degrees (Wehbe et al., 2014; Jain & Huth, 2018; Toneva & Wehbe, 2019; Schrimpf et al., 2021; Caucheteux & King, 2022; Goldstein et al., 2022; Karamolegkou et al., 2023). Researchers aim to use language models as model organisms (Toneva, 2021) of reading and listening in the brain to learn more about the underlying information processing that leads to brain-like representations of language.\nHowever, recent work has questioned whether current popular speech language models can serve this role fully, as their alignment with semantic brain regions was shown to be mostly due to low- level speech features, indicating that speech language models lack brain-relevant semantics (Oota et al., 2024a). Given that most large public brain recordings datasets are of speech-evoked language (LeBel et al., 2023; Nastase et al., 2021; Deniz et al., 2019; Momenian et al., 2024), having access to speech models with improved brain-relevant semantics is important and will provide better model organisms for auditory language processing. The lack of brain-relevant semantics in speech models (Oota et al., 2024a) may also be related to their incomplete semantic understanding for downstream language tasks (Choi et al., 2024).\nTo bridge the gap between language understanding in speech models and the human brain, we pro- pose to augment pretrained speech model training directly with brain recordings in a process we call brain-tuning (see Fig. 1a for illustration of the training approach). We then evaluate the resulting brain-tuned speech models in three distinct ways (see Fig. 1c for an illustration of the evaluation ap- proach): 1) alignment with new brain recordings in semantic regions of the brain, which we expect to significantly increase if brain-tuning successfully induces brain-relevant semantics, 2) effect of low- level features, such as Tri-Phones and Articulation, on the alignment with these semantic regions, which we expect to significantly decrease if brain-tuning successfully induces brain-relevant seman- tics 3) downstream performance on linguistic tasks that require semantic understanding, which we expect to significantly improve if the brain-relevant semantic understanding induced by the brain- tuning is also useful for downstream tasks."}, {"title": "2 RELATED WORK", "content": "Our work is most closely related to that of Schwartz et al. (2019), who fine-tune one pretrained text- based language model (BERT (Devlin et al., 2019)) using fMRI and MEG recordings of participants reading a chapter of a book. We instead focus on speech models, validate our method across three model families, and conduct comprehensive analyses to reveal that brain-tuning improves semantic understanding in speech language models for the first time. Separately, a growing literature inves- tigates the alignment between human brains and pretrained language models. A number of studies have shown a degree of alignment between language-evoked brain activity with text-based language models (Wehbe et al., 2014; Jain & Huth, 2018; Toneva & Wehbe, 2019; Caucheteux & King, 2022; Jat et al., 2019; Abdou et al., 2021; Schrimpf et al., 2021; Toneva et al., 2022a;b; Antonello et al., 2021; Oota et al., 2022; Merlin & Toneva, 2022; Aw & Toneva, 2023; Oota et al., 2024b; Lamarre et al., 2022; Antonello et al., 2024), and with speech-based language models (Millet et al., 2022; Vaidya et al., 2022; Tuckute et al., 2023; Oota et al., 2023; 2024a; Chen et al., 2024). Our approach of brain-tuning pretrained language models is complementary and can be used in addition to previously proposed techniques for analysis of the alignment between language models and brain activity."}, {"title": "3 METHODS", "content": null}, {"title": "3.1 SPEECH LANGUAGE MODELS", "content": "We build on three popular pretrained transformer-based speech language model families: Wav2vec2.0 (Baevski et al., 2020), HuBERT (Hsu et al., 2021), and Whisper (Radford et al., 2023). We chose versions of these models that have comparable sizes (~90M parameters), the same num- ber of encoder layers (12), and the same embedding size (768). Wav2vec2.0 and HuBERT are self-supervised models that are trained to predict representations of masked portions of the input. They both divide the input into tokens of 20ms and then use a CNN feature extractor. We use the base architectures which are trained on ~960 hours of audio. Whisper, unlike Wav2Vec2.0 and Hu- BERT, is trained in a weakly supervised manner, using 680K hours of paired audio-text data and has an encoder-decoder architecture. Contrary to HuBERT and Wav2Vec2.0, Whisper takes a fixed 30s input and then converts it to log-mel spectrograms. We fine-tune only the Whisper encoder for two reasons: 1) to keep the model of comparable size to the other two models, and 2) since the encoder"}, {"title": "3.2 NATURALISTIC BRAIN DATASET AND DATA PREPROCESSING", "content": "We use the largest public dataset of fMRI recordings (LeBel et al., 2024) for brain-tuning. The dataset contains fMRI recordings for 8 participants listening to 27 short stories from the Moth Radio Hour podcast for a total of 6.4 hours of audio per participant (11,543 fMRI images (TRs) with TR = 2.0045s). To fine-tune a model using fMRI recordings, we need to build a paired dataset of fMRI recordings and the corresponding audio snippets that were presented to the participants. We follow previously proposed approaches for this (Oota et al., 2024a; Vaidya et al., 2022; Antonello et al., 2024; Schwartz et al., 2019). Specifically, we first partition the audio input by utilizing a sliding window of length T seconds with a stride W seconds. This way, at each time t in the audio, a window of length [t \u2013 T, t] seconds is provided as input to the speech model. We use T = 16s and W = 0.1s. We next align the stimulus presentation rate with the slower fMRI acquisition rate by downsampling using a 3-lobed Lanczos filter. Lastly, we account for the slowness of the fMRI hemodynamic response by modeling it as a finite response filter with 10 seconds (5 TRs). These steps result in a audio-fMRI paired dataset that can be used for brain-tuning or evaluation.\nEstimated noise ceiling. Noise in the fMRI data can impair the brain-tuning and evaluation, so it is important to estimate the \"noise ceiling\" of each voxel and participant in the fMRI recordings. We estimate the voxel-wise noise ceiling for all participants' fMRI data based on the preferred method by the original dataset paper (LeBel et al., 2023), which leverages within-participant repetitions of the same story. This noise ceiling value estimates the amount of explainable variance (the maximum variance that can be explained by a given model) and its values range from 0 to 1. We use this estimated noise ceiling to filter noisy voxels and to normalize the brain alignment during evaluation."}, {"title": "3.3 BRAIN-TUNING SPEECH MODELS", "content": "Brain-tuning approach. Given an input audio and its corresponding fMRI response, obtained via the method in Section 3.2, we aim to fine-tune a pretrained speech model with the fMRI responses (i.e., brain-tune the model). Specifically, we fine-tune the model to reconstruct the fMRI responses corresponding to the voxels with high noise ceiling (> 0.4). The approach is illustrated in Figure 1a. To this end, we add a pooling layer and a projection head on top of the output tokens. The projection head predicts the fMRI response from the pooled model tokens. More formally, given the $0_1...0_N$ output tokens, we have a function H, that predicts fMRI targets such that $H(0_{1:ON}) = FC(P(0_{1: ON}))$, where P is an average pooling function and FC is a linear function. The training objective is a reconstruction loss (L2 loss) between the outputs of H and the fMRI voxels. We freeze the feature extractor and backpropagate the loss to fine-tune the projection head as well as the transformer layers.\nTraining details. We used a base learning rates of $5 \\times 10^{-5}$ and $10^{-4}$ respectively for the transformer layers and the linear projection head. Both had a linear decay scheduler for the learning rate with a warmup period for 10% of the epochs. The 27 fMRI stories are split into a training set (24 stories), a validation set (2 stories), and a held-out test set (1 story). The training is stopped when the validation loss saturates or begins to diverge. Since the number of voxels differs for each participant, this fine-tuning process is done separately for each fMRI participant. We apply this approach to the 3 pretrained models: Wav2vec2.0, HuBERT, and the Whisper encoder.\nAdditional comparison models. In addition to comparing the brain-tuned models to the pretrained ones, we further train 2 baselines for comparison: 1) a Random-fMRI baseline to test how addition of any fMRI data impacts model performance. This baseline uses the same fine-tuning process in Fig. 1a but, instead of using the matched fMRI responses for the input stimulus, it uses block-permuted fMRI responses. 2) a Big Spoken-Language Model (BigSLM) baseline to test the importance of having fMRI responses as the reconstruction targets. This baseline replaces the fMRI targets for the input stimuli with representations for the same stimuli obtained from a BigSLM. We use Whisper Medium (800M parameters) as the BigSLM and use a concatenation of its internal representations from all layers of decoder. Both baselines create a proxy for the true fMRI targets, one with ran- dom fMRI signals that don't correspond to the input and the other with rich representations from a large pretrained state-of-the-art speech model. Comparing brain-tuned models with these baselines helps understand changes in performance due to simple regularization and additional data. Any improvements over these baselines will thus be due to the correct fMRI targets."}, {"title": "3.4 EVALUATION", "content": "We evaluate multiple aspects of the brain-tuned models and illustrate our evaluation strategy in Fig. 1c. If brain-tuning successfully improves semantic understanding in speech models, we expect that brain-tuned models will align better with semantic language regions in new brain recordings, have impact of lower low-level features on the alignment with these regions, and have improved downstream performance on semantic tasks."}, {"title": "3.4.1 BRAIN ALIGNMENT", "content": "To compare brain alignment for a model before (i.e., the pretrained version) and after brain-tuning, we compute the normalized brain alignment using standard voxel-wise encoding models and re- port it for language- and speech-related brain regions. For each region, we statistically test whether brain-tuning leads to significantly better alignment.\nNormalized brain alignment. We estimate standard voxel-wise encoding models to evaluate the brain alignment of a model representation (Antonello et al., 2024; Vaidya et al., 2022; Oota et al., 2024a). We carry out this voxel-wise encoding as shown in the original alignment branch in Figure 1b. The audio data is processed as detailed in Section 3.2, then a voxel-wise encoding function h is learned using ridge regression on the training portion of the dataset. The prediction performance of this encoding function is computed over the held-out testing portion of the dataset via Pearson"}, {"title": "3.4.2 IMPACT OF LOW-LEVEL FEATURES ON BRAIN ALIGNMENT", "content": "Previous work showed that the alignment of pretrained speech models with late language regions is mostly due to low-level features (Oota et al., 2024a), which is undesirable. We further set out to test the impact of low-level features on the brain-tuned models' alignment with the brain. To enable comparisons with previous work, we estimate the low-level feature impact on brain alignment using the same approach as in Oota et al. (2024a). Intuitively, the impact of a specific low-level feature is estimated by comparing the brain alignment of a model before and after this low-level feature is computationally removed from the model. If, after removal of the low-level feature, the alignment is significantly lower than the original one, the low-level feature is said to have high impact on the brain alignment. We illustrate this process in Fig. 1b and provide details about this method below.\nLow-level features. We focus on four low-level speech features: Power Spectrum (the time-varying power spectrum across frequency bands), Di-Phones & Tri-Phones (adjacent pairs and triples of phonemes), and Articulation (articulatory characteristics of the phonemes). These features cover different stages of speech and are considered to be non-semantic features. The specifics of obtaining these features from the audio are detailed in (Oota et al., 2024a)\nLow-level feature impact. First, for a given low-level feature of the input audio, a linear function F is learned using this feature as input to predict the representations of the model. Then, the predicted model representations are subtracted from the true model representations, and the brain alignment of this residual is estimated via a standard encoding model, as described in Sec. 3.4.1. We define the low-level impact R as:\n$R = 100 \\cdot \\frac{B_o - B_r}{B_o}$\nwhere $B_o$ and $B_r$ correspond to the original and residual brain alignments. R represents the percent- age drop in alignment due to the removed low-level feature. Large R means that much of the original alignment was due to the low-level feature. To test for significant differences between models, we perform the same statistical tests as described in Sec. 3.4.1."}, {"title": "3.4.3 DoWNSTREAM TASKS", "content": "To test whether improving brain-relevant semantics via brain-tuning also improves semantic under- standing in models, we evaluate our models on a range of downstream tasks at different semantic levels. We also test the semantic vs. phonetic preference of the models' representations.\nDownstream tasks. We choose tasks with several semantic difficulties, namely: automatic speech recognition (ASR), phonetic sentence type prediction, sequence understanding, phonemes predic- tion, word identity prediction, and emotion recognition. We use standard datasets for these tasks: TIMIT (Garofolo, 1993), Crema-D (Cao et al., 2014), Speech Commands (Warden, 2018), and"}, {"title": "4 RESULTS", "content": null}, {"title": "4.1 BRAIN ALIGNMENT WITH HELDOUT DATA", "content": "We estimate the normalized brain alignment described in Section 3.4.1 separately for two important language-related areas of the brain: the late language regions and the primary auditory regions. The late language regions are thought to support semantic language processing, while the primary auditory regions support mostly lower-level processing related to the speech signal (Deniz et al., 2019). For each of the three model families, we evaluate the normalized brain alignment for the pretrained and brain-tuned versions, along with the alignments of the two comparison baselines- BigSLM fine-tuned and Random-fMRI (see Section 3.3).\nIn Fig. 2a and b, we show the normalized brain alignment averaged across voxels, layers, and par- ticipants for all models. We observe that brain-tuning significantly improves alignment with late language regions for the self-supervised models (Wav2vec2.0 and HuBERT), with an increase of 30% over the corresponding pretrained models. This gain in alignment with late language regions can also be seen on the level of individual voxels (Fig. 2c for Wav2vec2.0 and one representative participant; the brain maps for the remaining participants are shown in Appendix D.1). In con- trast, the two comparison models\u2013BigSLM Fine-tuned and Random-fMRI (see Appendix Fig. 6 for Random-fMRI results)\u2013lead to lower brain alignment than the pretrained models. This suggests that the gain from the brain-tuned models is due to incorporating the correct fMRI signal that cor- responds to the audio input. We do not observe significant gains for Whisper in the late language regions or for any of the model families in the primary auditory regions.\nThe result that brain-tuning improves the alignment of two of the pretrained models with semantic late language regions, and not with less semantic regions, such as the primary auditory cortices, sug- gests that brain-tuning may improve the brain-relevant semantics in at least some speech language models. We test this further in the next sections."}, {"title": "4.2 EFFECT OF LOW-LEVEL FEATURES ON BRAIN ALIGNMENT", "content": "We further test the dependence on low-level features of the observed gain in brain alignment due to brain-tuning. Fig. 3a and b present the impact of low-level features on the brain alignment across model families (averaged over voxels, layers, low-level features, and participants)."}, {"title": "4.3 DOWNSTREAM PERFORMANCE", "content": "We next investigate the models' semantic understanding via their performance on downstream tasks (detailed in Section 3.4.3), and their semantic-phonetic preference (detailed in Appendix C).\nPerformance on downstream tasks. Fig. 4 presents the models' performances on popular down- stream tasks that require different levels of semantic understanding. The brain-tuned performance is averaged across layers and participants, with the exception of ASR where the performance is only averaged across participants since evaluations are done only once per model. We observe that brain- tuning boosts the performance of all model families across all downstream tasks with varying seman- tic difficulty, with the exception of emotion prediction, in which the performance improves slightly for one model family and decreases slightly for the other two (Fig. 4f). Specifically, brain-tuning leads to the biggest gains for tasks that are most semantically challenging and require recognition and understanding beyond the local context. Brain-tuning increases the ASR accuracy on English ASR by up to 12% (Fig. 4a), enhances the phonetic sentence type understanding by 20-23% (Fig. 4b), and nearly doubles the performance on sequence understanding for the self-supervised model families (Fig. 4c). Tasks that relate to local word or phoneme understanding see moderate gains"}, {"title": "5 DISCUSSION", "content": "In this work, we present a method to augment speech model training directly with fMRI recordings of people listening to natural stories and show two converging lines of evidence that this leads to improved semantic understanding in the models.\nFirst, two of the three tested model families have improved alignment with new brain recordings in the semantic language regions after brain-tuning (Fig. 2). For all model families, brain-tuning also significantly reduces the impact of low-level speech features on alignment with late language"}, {"title": "6 CONCLUSION", "content": "Our systematic analyses of the utility of brain-tuning on semantic brain alignment and downstream performance reveal a parallel among the gain in brain alignment, its reduced impact of low-level speech features, and increased downstream performance on several tasks with varying semantic difficulty. We further observe an increase in the semantic preference of late layers of the brain-tuned models. These exciting results provide evidence for the first time that incorporating brain signals into the training of language models improves their semantic understanding. Future work can investigate further refinement of the brain-tuning loss and the incorporation of additional participants and brain datasets in the brain-tuning process. We hope that the brain-tuned models we provide will serve as better model organisms for auditory language processing in the brain, and will inspire more work on improving the alignment between language in machines and language in the brain."}, {"title": "A BRAIN ALIGNMENT", "content": null}, {"title": "A.1 ROIS DETAILS", "content": "The human cerebral cortex multi-modal parcellation (Glasser Atlas) has 180 labeled ROIs per hemi- sphere(Glasser et al., 2016). It has language regions that include the following labels: Angular gyrus (AG: PFm, PGs, PGi, TPOJ2, TPOJ3), lateral temporal cortex (LTC: STSda, STSva, STGa, TEla, TE2a, TGv, TGd, A5, STSdp, STSvp, PSL, STV, TPOJ1), inferior frontal gyrus (IFG: 44, 45, IFJa, IFSp) and middle frontal gyrus (MFG: 55b) ((Oota et al., 2024a), Desai et al. (2023)). It also has the primary auditory (A1) and the early auditory (A1, PBelt, MBelt, LBelt, RI, A4) regions."}, {"title": "A.2 BRAIN-TUNING ALIGNMENT COMPARISON WITH BIGGER MODELS", "content": "To get a clearer idea about the improvement in brain alignment we get from Brain-tuning on late language regions, we compare the HuBERT model's base architectures (90M parameters ) both pretrained and brain-tuned to the pretrained HuBERT large architecture (320M parameters) in Fig. 5. The pretrained large architecture has a much larger alignment in late language regions than the pretrained base model, which is in line with the trend of increase in alignment when scaling the model size shown in (Chung et al., 2024); however, the Brain-tuned base model is very close to the pretrained large model's alignment. Moreover, the low-level impact of the Brain-tuned model is noticeably less than the pretrained large model. This is an indicator that we can explain much more in the brain late language semantic regions with smaller models and brain-tuning bigger models might break the plateau we see in (Antonello et al., 2024) when reaching huge model sizes. Breaking this plateau might allow for better and more accurate computational models for speech semantic processing in the brain."}, {"title": "A.3 BRAIN ALIGNMENT WITH RANDOM-FMRI", "content": "We extend Fig. 6 by showing the Random-fMRI baseline alongside the brain-tuned, pretrained, and BigSLM fine-tuned models. Fig. 6b clearly shows that Random-fMRI models have the lowest brain alignment values in late language regions compared to even BigSLM fine-tuned baselines, and in the primary auditory regions, it's also much lower than the pretrained version. This strongly indicates that randomly permuting the fMRI targets while brain-tuning the models with the same stimuli substantially harms the model's alignment. Hence, having the correct fMRI targets is essential for the alignment results we get from the brain-tuned models."}, {"title": "A.4 LoW-LEVEL IMPACT WITH RANDOM-FMRI", "content": "We extend Fig. 3 by showing the Random-fMRI baseline alongside the brain-tuned, pretrained, and BigSLM fine-tuned models. Fig. 7 clearly shows that Random-fMRI models have the highest low- level impact (highest drop) compared to even BigSLM fine-tuned baselines in both late language regions and the primary auditory region. Even though the original normalized alignment values for the Random-fMRI models are very low to begin with, they still undergo a substantial drop after we remove the low-level features. Similar to the results from Fig. 6, these results also indicate that randomly permuting the fMRI targets while brain-tuning the models with the same stimuli substantially harms the model's semantics. Hence, having the correct fMRI targets is essential for reduced low-level impact we get in the brain-tuned models."}, {"title": "B DOWNSTREAM TASKS", "content": "We detail here the description and dataset used for each downstream task mentioned in Section 3.4.3."}, {"title": "Automatic Speech Recognition (ASR).", "content": "We run an ASR fine-tuning pipeline for the self- supervised models (Wav2vec2.0 and HuBERT). For ASR we fine-tune the whole model to get the best possible performance. The rationale is to essentially see if the brain-tuning process will make it harder or require longer training to fine-tune the speech model for tasks like ASR. We use a version of TIMIT (Garofolo, 1993) for ASR to carry out this experiment. We use the CTC loss (Baevski et al., 2020) for both self-supervised models and calculate Word-Error-Rate (WER) as the evaluation metric (on the padded test-set). We then report the ASR accuracy as 1 WER."}, {"title": "Phonetic Sentence Type Prediction.", "content": "The TIMIT dataset Garofolo (1993) has 3 different sentence types: SA (Dialect), SX (Compact), and SI (Diverse). The SA sentences are supposed to expose the dialectal variations of the speakers ( and are designed to span all the phonemes of American English); the SX sentences should provide good coverage of phones (phonetically balanced and cover a wide range of phonetic contexts with a small number of words). The SI sentences are anything else (phonetically diverse and more naturalistic). We add a classification head and we use the F1-score for evaluation."}, {"title": "Sequence Understanding.", "content": "This is very similar to Intent Classification tasks; it tests the ability to understand a sequence. We use the SLURP (Bastianelli et al., 2020) dataset that has audio paired with actions. For example, if the input is \"Wake me up at eight o'clock\", the action should be \"set_alarm\". We have 46 possible actions and we add a linear head to predict the action and evaluate using the F1-score."}, {"title": "Phonemes Prediction.", "content": "Phoneme prediction is formulated as a multi-label classification problem, where the classifier predicts which of the 39 Phonemes were present in the input audio clip. We use the TIMIT dataset Garofolo (1993) for its phonetically rich sentences; we evaluate the test set using the F1-score."}, {"title": "Word Identity Prediction.", "content": "We want to test the model's ability to decode words from input audio. To simplify this task to befit a classification head, we convert it to a classification task on the Speech Commands dataset (Warden, 2018) which has audio clips, each of which has only one word belong- ing to a set of 35 commands. The classifier predicts which of the 35 commands were said, and the evaluation is done using the F1-score."}, {"title": "Emotion Recognition.", "content": "We add a classification head for emotion recognition on the CREMA- D dataset Cao et al. (2014); CREMA-D has 7.4K clips from 91 actors, and six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad). The classifier predicts which of the 6 emotions is present and is evaluated using the F1-score."}, {"title": "C MEASURING SEMANTIC-PHONETIC PREFERENCE", "content": "To test if there are semantic changes to the models' representations at a more fundamental level, we try to quantify how a model prefers phonetic over semantic features and compare our brain- tuned models to the pretrained ones. To be able to quantify this difference, we take inspiration from the work by Choi et al. (2024), which found that speech models have a huge bias towards phonetic features. This was done by constructing phonetically similar pairs and semantically similar pairs and then computing their similarity/ distance in the embedding space. Doing this clearly shows that phonetically similar words are closer than semantically similar ones in the embedding space (i.e., the model has strong phonetic preference ). This behavior persists across layers and across models. One aspect of desirable change in that behavior is to have the differences between phonetically and semantically similar pairs lower in the more semantic layers (both should still be better than random), or at least to have a clear hierarchy of that change. To do a similar but more curated analysis, we build a dataset of 2K words, each of word is paired with several semantically similar (e.g., Synonyms) and phonetically similar words (e.g., Homophones). Then, we compute the representational distance between them, namely semantic distance for the distance of the word to the synonym and phonetic distance for the distance of the word to the homophone. After that, we are able to compute a semantic-phonetic preference d for any given layer or model. We define d as the average difference between semantic and phonetic distances. Essentially, since we know phonetic"}, {"title": "D BRAIN PLOTS FOR DIFFERENT PARTICIPANTS", "content": "Here, we extend the whole-brain plots (flat and lateral views) for different participants for both brain alignment and low-level impact analyses."}, {"title": "D.1 BRAIN ALIGNMENT VOXEL-WISE DIFFERENCES", "content": "We show in Fig. 9 more whole-brain analyses for the voxel-wise differences in brain alignment be- tween brain-tuned and pretrained models for different participants. The trend we detailed in section 4.1 is also consistent with the plots below and is in line with the increase in the values of alignment in late language regions (Fig. 2a) and the insignificant change in alignment in primary auditory regions (Fig. 2b). All shown Brain plots are for the Wav2vec2.0 model family."}, {"title": "D.2 Low-LEVEL IMPACT VOXEL-WISE DIFFERENCES", "content": "In Fig. 10, we show more whole-brain analyses for the voxel-wise differences in low-level impact due to brain-tuning (between brain-tuned and pretrained models) for different participants. The trend we detailed in section 4.2 is also generally consistent with the plots below and is in line with the increase in the values of alignment in late language regions (Fig. 3a) and the insignificant change in alignment in primary auditory regions (Fig. 3b). All shown Brain plots are for the Wav2vec2.0 model family."}]}