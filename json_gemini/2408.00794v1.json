{"title": "CCSRP: Robust Pruning of Spiking Neural Networks through Cooperative Coevolution", "authors": ["Zichen Song", "Jiakang Li", "Songning Lai", "Sitan Huang"], "abstract": "Spiking neural networks (SNNs) have shown promise in various dynamic visual tasks, yet those ready for practical deployment often lack the compactness and robustness essential in resource-limited and safety-critical settings. Prior research has predominantly concentrated on enhancing the compactness or robustness of artificial neural networks through strategies like network pruning and adversarial training, with little exploration into similar methodologies for SNNs. Robust pruning of SNNs aims to reduce computational overhead while preserving both accuracy and robustness. Current robust pruning approaches generally necessitate expert knowledge and iterative experimentation to establish suitable pruning criteria or auxiliary modules, thus constraining their broader application. Concurrently, evolutionary algorithms (EAs) have been employed to automate the pruning of artificial neural networks, delivering remarkable outcomes yet overlooking the aspect of robustness. In this work, we propose CCSRP, an innovative robust pruning method for SNNs, underpinned by cooperative co-evolution. Robust pruning is articulated as a tri-objective optimization challenge, striving to balance accuracy, robustness, and compactness concurrently, resolved through a cooperative co-evolutionary pruning framework that independently prunes filters across layers using EAs. Our experiments on CIFAR-10 and SVHN demonstrate that CCSRP can match or exceed the performance of the latest methodologies.", "sections": [{"title": "Introduction", "content": "In recent years, spiking neural networks (SNNs) have achieved significant success in the field of dynamic vision, such as spiking neural state image classification and object detection. Despite their impressive performance, the high computational cost associated with converting artificial neural networks (ANNs) to SNNs limits their deployment in resource-constrained scenarios.[1] Additionally, SNNs are susceptible to malicious attacks, posing a challenge to their reliability in safety-critical environments. Thus, enhancing both the compactness and robustness of SNNs is crucial in many practical applications, such as event cameras. [2]"}, {"title": "Related work", "content": "The objective of pruning in spiking neural networks (SNNs) is to enhance operational efficiency by eliminating unnecessary components. Current pruning strategies are predominantly categorized into two types: unstructured and structured pruning. [9] Unstructured pruning involves direct adjustments to the weights within the network, theoretically offering significant computational speed-ups. However, the resulting sparse matrices and discontinuous structures are often incompatible with existing software and hardware environments, making practical acceleration challenging to achieve. In contrast, structured pruning targets the systematic removal of components, such as filters in convolutional layers of SNNs, demonstrating superior performance in real-world applications, thereby gaining increased popularity and attention. [10]\nRegarding the identification of redundant components, previous structured pruning approaches can be broadly divided into criteria-based and learning-based methods.[11] Criteria-based methods rely on expert-designed rules to identify and prune non-essential components, whereas learning-based methods utilize auxiliary modules to assess the importance of components for subsequent pruning. Nevertheless, both methods heavily depend on domain expertise, restricting their widespread application and flexibility.[12]\nTo reduce the dependency on expert knowledge, employing evolutionary algorithms (EAs) for the automatic discovery of optimized pruned network architectures emerges as a natural solution. Despite this, the vast search space of SNNs presents a significant challenge to EAs. [13] Recently, an innovative pruning method inspired by cooperative co-evolution, named CCEP, has been introduced. It adopts a divide-and-conquer strategy to tackle the immense search space, demonstrating impressive performance and underscoring the substantial potential of EA-based methods in the domain of neural network pruning. However, previous EA-based pruning approaches have not taken into account the robustness of the network, a factor of critical importance for many application scenarios.[14]"}, {"title": "Robust Spiking Neural Network Pruning", "content": "Recent studies have explored the relationship between robustness and network capacity, uncovering that a sub-network of the original network may exhibit similar or even superior robustness compared to the original network, with considerable variance in robustness among different sub-networks. This discovery has spurred research into robust pruning for spiking neural networks (SNNs), aiming to identify a compact SNN that retains robustness.[15] The existing handful of methods typically employ adversarial training to train a network and proceed with unstructured pruning based on criteria designed by experts. For instance, ADV-LWM prunes weights with small \ud835\udc591-norms and fine-tunes the resulting network through adversarial training to regain robustness. Utilizing the ADMM pruning framework, one approach replaces the original training loss with an adversarial counterpart. HYDRA assigns importance scores to all weights within the network, optimizing the adversarial loss by adjusting these scores while freezing the weights. Subsequently, weights with minimal importance scores are pruned. DNR opts to prune filters corresponding to feature matrices with small Frobenius norms. Moreover, these methods necessitate appropriate pruning ratios for each layer, which often requires extensive expert knowledge and iterative experimentation.[16]"}, {"title": "CCSRP Method", "content": "Consider \ud835\udc41 to be a thoroughly trained neural network composed of \ud835\udc5b convolution layers, designated as \ud835\udc3f1, \ud835\udc3f2, \u2026, \ud835\udc3f\ud835\udc5b, where each layer \ud835\udc3f\ud835\udc56 contains \ud835\udc59\ud835\udc56 filters and \ud835\udc3f\ud835\udc56\ud835\udc57 refers to the \ud835\udc57\ud835\udc61\u210e filter of the \ud835\udc56\ud835\udc61\u210e layer. The process of robust pruning in neural networks is an optimization challenge aimed at isolating a group of filters within \ud835\udc41. The goal is to enhance both the network's accuracy and its robustness against perturbations while concurrently reducing the computational burden. We define a mask vector \ud835\udc40 = {\ud835\udc5a\ud835\udc56\ud835\udc57 | \ud835\udc5a\ud835\udc56\ud835\udc57 \u2208 {0,1}, \ud835\udc56 \u2208 {1,2, ..., \ud835\udc5b}, \ud835\udc57 \u2208 {1,2, ..., \ud835\udc59\ud835\udc56}} with \ud835\udc5a\ud835\udc56\ud835\udc57 = 1 signifying the retention of filter \ud835\udc3f\ud835\udc56\ud835\udc57. Therefore, we can represent a pruned network with the mask \ud835\udc40 as follows:\n$N_M = \\cup_{i=1}^n \\cup_{j=1}^{l_i} m_{ij} L_{ij}$\nThe performance of the pruned network \ud835\udc41\ud835\udc40 is measured by \ud835\udc34\ud835\udc36\ud835\udc36(\ud835\udc41\ud835\udc40), indicating accuracy on standard datasets, and \ud835\udc34\ud835\udc36\ud835\udc36\ud835\udc5f(\ud835\udc41\ud835\udc40), indicating robustness as measured by accuracy against adversarial examples. Additionally, \ud835\udc39\ud835\udc3f\ud835\udc42\ud835\udc43s(\ud835\udc41\ud835\udc40) accounts for the floating-point operations count, which assesses the computational expenses. We pose the robust pruning task as:\n$\\arg \\max_{M} (ACC(N_M), ACC_r(N_M), -FLOPs(N_M))$\nGiven that a Spiking Neural Networks (SNNs) can have a substantial quantity of filters eligible for pruning, identified by \u2211\ud835\udc56=1 \ud835\udc59\ud835\udc56, this scenario poses a substantial optimization conundrum. To address this, we introduce CCSRP, a cutting-edge method for robust pruning. Drawing on the concepts from CCEP and CCRP, we utilize a cooperative coevolution-based pruning framework. This stratagem segments the search"}, {"title": "EA", "content": "Algorithm 1 delineates the procedure of the CCSRP framework. This framework refines a well-trained neural network through iterative pruning, ultimately yielding a suite of pruned networks that maintain robustness for selection. The iterative process is articulated as follows. Initially, the algorithm generates a mask \ud835\udc40 tailored to the network designated for pruning. Subsequently, this mask \ud835\udc40 is partitioned into \ud835\udc5b segments corresponding to the hierarchical layers of the network. Following this, a set of adversarial examples \ud835\udc37\ud835\udc4e is produced for assessing the efficacy of the pruned network. For each segmented group, an evolutionary algorithm (EA) is employed to orchestrate optimization, procuring \ud835\udc5a\ud835\udc56, indicative of the pruning outcome for the \ud835\udc56\ud835\udc61\u210e layer. By amalgamating \ud835\udc5a\ud835\udc56 for all \ud835\udc5b layers and applying them to the baseline network \ud835\udc41\ud835\udc4f, the pruned network \ud835\udc41' is procured. Post-pruning, to recoup the network's accuracy and robust accuracy, the pruned network \ud835\udc41' undergoes fine-tuning via adversarial training. The fine-tuned model is then established as the new baseline network \ud835\udc41\ud835\udc4f for subsequent pruning in the next iteration and is archived in \ud835\udc3b. [18]\nAfter \ud835\udc47 iterations, the CCSRP framework ceases and returns the pruned networks stored in archive \ud835\udc3b. An illustrative depiction of the CCSRP framework is also exhibited in Figure 1. Within each group of the evolutionary algorithm (EA) process, we engage a quintessential evolutionary procedure that commences with the random generation of an initial subpopulation. New individuals are bred by applying reproductive operators, followed by the evaluation of fitness for each, and the selection of the fittest individuals to advance to the succeeding generation. Upon reaching the termination criterion, the EA selects an individual from the ultimate subpopulation, which epitomizes the pruned layer for that group.[19]\nThe EA process within each group is elucidated in detail. Initially, it commences by generating an initial subpopulation \ud835\udc43 consisting of \ud835\udc51 individuals. An individual |\ud835\udc5a|0, with all bits set to 1, is incorporated into \ud835\udc43 to foster conservative pruning strategies. The remaining \ud835\udc51-1 individuals are generated using a modified bitwise mutation operator with a mutation rate \ud835\udc5d1. During each EA generation, \ud835\udc51 new progenies are engendered by randomly choosing \ud835\udc51 individuals from the subpopulation with replacement, followed by the application of a bitwise mutation operator with a mutation rate \ud835\udc5d2. In accordance with CCEP, we have adapted the standard bitwise mutation operator to mitigate overly aggressive pruning. Specifically, a ratio bound \ud835\udc5f constrains the number of filters to be pruned. [20]"}, {"title": "Robustness and Comparison", "content": "Typically, the robustness of spiking neural networks is gauged by their resilience to adversarial attacks. In this study, we define robust accuracy (\ud835\udc34\ud835\udc36\ud835\udc36\ud835\udc5f) as the measure of robustness, determined by the network's performance against crafted adversarial examples.[23] We employ the advanced PGD white-box attack algorithm to generate these examples, noted for its iterative and time-intensive nature. [24] To circumvent the prohibitive computational expense of generating unique adversarial samples for each pruned network, we have devised an efficient generation method. This technique produces a shared adversarial dataset \ud835\udc37\ud835\udc4e during a single iteration of CCSRP by apply-"}, {"title": "Experiments and Results", "content": "Our experimental investigation is conducted in three dimensions. Initially, we compare the performance of CCSRP (Cooperative Coevolutionary Spiking Neural Network Pruning) with the cutting-edge CCRP approach. Subsequently, we expand several prevalent unstructured and structured pruning methods to include robust pruning and juxtapose them with CCSRP. The third dimension involves conducting iterative experiments to evaluate the consistency of CCSRP and the visualization of the pruned network architectures. This evaluation employs two renowned image classification datasets: CIFAR-10 and SVHN, as well as three prototype neural networks adapted into their spiking versions: SVGG, SResNet, and SWRN. [27]\nIn the comparison with unstructured robust pruning techniques, the performance of CCSRP was first compared against leading unstructured robust pruning methods based on accuracy degradation, robust accuracy degradation, and inference speed, as detailed in Table 1.[30] Considering that the decrease in FLOPs for unstructured models does not accurately reflect computational efficiency in practical applications, inference speed was used as a measure of computational cost. The inference speed was tested under a batch size of 128 for 100,000 32\u00d732-pixel images. For CCSRP, results from the 10th iteration were presented in Table 1 for comparison. [31] In most cases, CCSRP achieved lower accuracy and robust accuracy degradation, along with faster inference speed. Despite HYDRA and ADV-LWM surpassing CCSRP in terms of robust accuracy degradation on the SVHN dataset, they experienced greater accuracy degradation and slower inference speeds.[32] In comparison with structured robust pruning techniques, to enable a more comprehensive analysis, two structured pruning methods, L1 and HRank, were extended to robust pruning scenarios through the incorporation of adversarial training in both pre-training and fine-tuning steps. For CCSRP, results from the 16th iteration were selected for comparison. The outcomes in Table 2 indicate that, compared to L1 and HRank, CCSRP consistently achieved better performance on at least two evaluation metrics.[33]"}, {"title": "Conclusion", "content": "This paper introduces a novel approach for automatic robust pruning of spiking neural networks, named CCSRP, aimed at enhancing the stability and performance of spiking neural networks when faced with various disturbances and anomalies. Unlike conventional pruning methods that focus solely on improving network performance, CCSRP treats robust pruning as an optimization problem that integrates accuracy, sparsity, and robustness, and employs an innovative adaptive co-evolution framework to solve this problem. The essence of this method lies not only in pursuing network performance optimization but also in enhancing the network's resistance to external interferences, thereby maintaining good stability and accuracy under various challenging conditions. To our knowledge, this is the first time evolutionary algorithms (EAs) have been applied to the pruning problem of spiking neural networks. This interdisciplinary innovative application not only offers a new perspective and method for optimizing the robustness of spiking neural networks but also opens up new avenues for using evolutionary algorithms to solve complex optimization problems. Comparative experiments demonstrate that CCSRP exhibits comparable or even superior performance in some aspects to existing techniques, further validating the effectiveness and potential of this method."}, {"title": null, "content": "Future work will focus on two main areas: one is to conduct more in-depth theoretical analysis to better understand the behavior and performance of CCSRP under different conditions, and the other is to explore and apply more advanced multi-objective optimization techniques to further enhance the performance of CCSRP. In particular, by introducing the latest multi-objective optimization algorithms and strategies, we aim to more finely balance the relationship between robustness, sparsity, and accuracy, thereby enhancing the overall performance and applicability of spiking neural networks in a broader range of application scenarios."}]}