{"title": "CCSRP: Robust Pruning of Spiking Neural Networks through Cooperative Coevolution", "authors": ["Zichen Song", "Jiakang Li", "Songning Lai", "Sitan Huang"], "abstract": "Spiking neural networks (SNNs) have shown promise in various dynamic visual tasks, yet those ready for practical deployment often lack the compactness and robustness essential in resource-limited and safety-critical settings. Prior research has predominantly concentrated on enhancing the compactness or robustness of artificial neural networks through strategies like network pruning and adversarial training, with little exploration into similar methodologies for SNNs. Robust pruning of SNNs aims to reduce computational overhead while preserving both accuracy and robustness. Current robust pruning approaches generally necessitate expert knowledge and iterative experimentation to establish suitable pruning criteria or auxiliary modules, thus constraining their broader application. Concurrently, evolutionary algorithms (EAs) have been employed to automate the pruning of artificial neural networks, delivering remarkable outcomes yet overlooking the aspect of robustness. In this work, we propose CCSRP, an innovative robust pruning method for SNNs, underpinned by cooperative co-evolution. Robust pruning is articulated as a tri-objective optimization challenge, striving to balance accuracy, robustness, and compactness concurrently, resolved through a cooperative co-evolutionary pruning framework that independently prunes filters across layers using EAs. Our experiments on CIFAR-10 and SVHN demonstrate that CCSRP can match or exceed the performance of the latest methodologies.", "sections": [{"title": "1 Introduction", "content": "In recent years, spiking neural networks (SNNs) have achieved significant success in the field of dynamic vision, such as spiking neural state image classification and object detection. Despite their impressive performance, the high computational cost associated with converting artificial neural networks (ANNs) to SNNs limits their deployment in resource-constrained scenarios.[1] Additionally, SNNs are susceptible to malicious attacks, posing a challenge to their reliability in safety-critical environments. Thus, enhancing both the compactness and robustness of SNNs is crucial in many practical applications, such as event cameras. [2]"}, {"title": "2 Related work", "content": "However, most previous work has focused solely on enhancing either the compactness or robustness of spiking neural networks.[3] On one hand, various model compression techniques have been proposed to reduce the computational cost of SNNs, such as neural network pruning and quantization. Among these, neural network pruning aims to remove redundant parameters in networks while maintaining accuracy, and has achieved considerable success. On the other hand, methods like adversarial training, which aim to minimize training loss on adversarial examples, can significantly enhance the robustness of SNNs.[4]\nRecent efforts have considered network robustness in the context of pruning SNNs. Typically, these approaches use expert-designed criteria to measure the importance of network weights and prune accordingly. However, designing and tuning such criteria require extensive expertise and laborious experimentation, making them difficult to apply in practical scenarios with diverse datasets and SNN architectures. Moreover, these efforts mainly focus on unstructured neural network pruning, which hardly reduces computational costs in real-world applications due to the resulting irregular structures being incompatible with mainstream software and hardware frameworks. [5] Thus, an automated structured robust pruning method is essential for practical applications. Robust pruning of SNNs can naturally be framed as an optimization problem, aiming to find a subnet of the original network that maintains high accuracy and robustness but with lower computational cost. [6] Evolutionary algorithms (EAs), inspired by natural evolution, have been used for automatically pruning SNNs. However, unlike artificial neural networks from the last century, modern SNNs typically comprise dozens of layers and millions of parameters, implying a vast search space. For EAs, finding satisfactory solutions within a limited computational overhead is challenging. Recently, Shang et al. proposed an evolutionary pruning method inspired by cooperative co-evolution, CCEP, which has shown encouraging results for large-scale pruning problems. However, their focus was solely on accuracy without considering robustness. In this paper, we introduce a novel Cooperative Coevolutionary Strategy for Robust Pruning (CCSRP).[7] The robust pruning problem is explicitly formulated as a three-objective optimization problem, aiming to simultaneously optimize accuracy, robustness, and compactness. A cooperative co-evolution framework is employed to tackle the robust pruning problem, dividing the search space by layer and applying an EA to optimize each group independently. Additionally, to address the time-consuming process of generating adversarial examples for each pruned network, we devise an adversarial example generation method to improve the efficiency of robustness evaluation. [8]\nOur contributions are summarized as follows:\n1. We present a novel framework, CCSRP, that considers network robustness during the pruning process and automatically solves the three-objective robust pruning problem through cooperative co-evolution. To our knowledge, this is the first application of EAs to robust pruning of spiking neural networks."}, {"title": "2.1 Spiking Neural Network Pruning", "content": "The objective of pruning in spiking neural networks (SNNs) is to enhance operational efficiency by eliminating unnecessary components. Current pruning strategies are predominantly categorized into two types: unstructured and structured pruning. [9] Unstructured pruning involves direct adjustments to the weights within the network, theoretically offering significant computational speed-ups. However, the resulting sparse matrices and discontinuous structures are often incompatible with existing software and hardware environments, making practical acceleration challenging to achieve. In contrast, structured pruning targets the systematic removal of components, such as filters in convolutional layers of SNNs, demonstrating superior performance in real-world applications, thereby gaining increased popularity and attention. [10]\nRegarding the identification of redundant components, previous structured pruning approaches can be broadly divided into criteria-based and learning-based methods.[11] Criteria-based methods rely on expert-designed rules to identify and prune non-essential components, whereas learning-based methods utilize auxiliary modules to assess the importance of components for subsequent pruning. Nevertheless, both methods heavily depend on domain expertise, restricting their widespread application and flexibility.[12]\nTo reduce the dependency on expert knowledge, employing evolutionary algorithms (EAs) for the automatic discovery of optimized pruned network architectures emerges as a natural solution. Despite this, the vast search space of SNNs presents a significant challenge to EAs. [13] Recently, an innovative pruning method inspired by cooperative co-evolution, named CCEP, has been introduced. It adopts a divide-and-conquer strategy to tackle the immense search space, demonstrating impressive performance and underscoring the substantial potential of EA-based methods in the domain of neural network pruning. However, previous EA-based pruning approaches have not taken into account the robustness of the network, a factor of critical importance for many application scenarios.[14]"}, {"title": "2.2 Robust Spiking Neural Network Pruning", "content": "Recent studies have explored the relationship between robustness and network capacity, uncovering that a sub-network of the original network may exhibit similar or even"}, {"title": "3 CCSRP Method", "content": "Consider $\\mathcal{N}$ to be a thoroughly trained neural network composed of $n$ convolution layers, designated as $L_1, L_2, \\ldots, L_n$, where each layer $L_i$ contains $l_i$ filters and $L_{ij}$ refers to the $j$th filter of the $i$th layer. The process of robust pruning in neural networks is an optimization challenge aimed at isolating a group of filters within $\\mathcal{N}$. The goal is to enhance both the network's accuracy and its robustness against perturbations while concurrently reducing the computational burden. We define a mask vector $\\mathcal{M} = \\{m_{ij}| m_{ij} \\in \\{0,1\\}, i \\in \\{1,2, ..., n\\}, j \\in \\{1,2, ..., l_i\\}\\}$ with $m_{ij} = 1$ signifying the retention of filter $L_{ij}$. Therefore, we can represent a pruned network with the mask $\\mathcal{M}$ as follows:\n$\\mathcal{N}_\\mathcal{M} = \\bigcup_{i=1}^n \\bigcup_{j=1}^{l_i} m_{ij} L_{ij}$\nThe performance of the pruned network $\\mathcal{N}_\\mathcal{M}$ is measured by $ACC(\\mathcal{N}_\\mathcal{M})$, indicating accuracy on standard datasets, and $ACC_r(\\mathcal{N}_\\mathcal{M})$, indicating robustness as measured by accuracy against adversarial examples. Additionally, $FLOPs(\\mathcal{N}_\\mathcal{M})$ accounts for the floating-point operations count, which assesses the computational expenses. We pose the robust pruning task as:\n$\\arg \\text{max}_\\mathcal{M}(ACC(\\mathcal{N}_\\mathcal{M}), ACC_r(\\mathcal{N}_\\mathcal{M}), -FLOPs(\\mathcal{N}_\\mathcal{M}))$\nGiven that a Spiking Neural Networks (SNNs) can have a substantial quantity of filters eligible for pruning, identified by $\\sum_{i=1}^{n} l_i$, this scenario poses a substantial optimization conundrum. To address this, we introduce CCSRP, a cutting-edge method for robust pruning. Drawing on the concepts from CCEP and CCRP, we utilize a cooperative coevolution-based pruning framework. This stratagem segments the search"}, {"title": "3.1 EA", "content": "Algorithm 1 delineates the procedure of the CCSRP framework. This framework refines a well-trained neural network through iterative pruning, ultimately yielding a suite of pruned networks that maintain robustness for selection. The iterative process is articulated as follows. Initially, the algorithm generates a mask $\\mathcal{M}$ tailored to the network designated for pruning. Subsequently, this mask $\\mathcal{M}$ is partitioned into $n$ segments corresponding to the hierarchical layers of the network. Following this, a set of adversarial examples $D_a$ is produced for assessing the efficacy of the pruned network. For each segmented group, an evolutionary algorithm (EA) is employed to orchestrate optimization, procuring $m_i$, indicative of the pruning outcome for the $i$th layer. By amalgamating $m_i$ for all $n$ layers and applying them to the baseline network $\\mathcal{N}_b$, the pruned network $\\mathcal{N}'$ is procured. Post-pruning, to recoup the network's accuracy and robust accuracy, the pruned network $\\mathcal{N}'$ undergoes fine-tuning via adversarial training. The fine-tuned model is then established as the new baseline network $\\mathcal{N}_b$ for subsequent pruning in the next iteration and is archived in $\\mathcal{H}$. [18]\nAfter $T$ iterations, the CCSRP framework ceases and returns the pruned networks stored in archive $\\mathcal{H}$. An illustrative depiction of the CCSRP framework is also exhibited in Figure 1. Within each group of the evolutionary algorithm (EA) process, we engage a quintessential evolutionary procedure that commences with the random generation of an initial subpopulation. New individuals are bred by applying reproductive operators, followed by the evaluation of fitness for each, and the selection of the fittest individuals to advance to the succeeding generation. Upon reaching the termination criterion, the EA selects an individual from the ultimate subpopulation, which epitomizes the pruned layer for that group.[19]\nThe EA process within each group is elucidated in detail. Initially, it commences by generating an initial subpopulation $\\mathcal{P}$ consisting of $d$ individuals. An individual $|m|_0$, with all bits set to 1, is incorporated into $\\mathcal{P}$ to foster conservative pruning strategies. The remaining $d-1$ individuals are generated using a modified bitwise mutation operator with a mutation rate $p_1$. During each EA generation, $d$ new progenies are engendered by randomly choosing $d$ individuals from the subpopulation with replacement, followed by the application of a bitwise mutation operator with a mutation rate $p_2$. In accordance with CCEP, we have adapted the standard bitwise mutation operator to mitigate overly aggressive pruning. Specifically, a ratio bound $r$ constrains the number of filters to be pruned. [20]"}, {"title": "3.2 Robustness and Comparison", "content": "Typically, the robustness of spiking neural networks is gauged by their resilience to adversarial attacks. In this study, we define robust accuracy ($ACC_r$) as the measure of robustness, determined by the network's performance against crafted adversarial examples.[23] We employ the advanced PGD white-box attack algorithm to generate these examples, noted for its iterative and time-intensive nature. [24] To circumvent the prohibitive computational expense of generating unique adversarial samples for each pruned network, we have devised an efficient generation method. This technique produces a shared adversarial dataset $D_a$ during a single iteration of CCSRP by apply-"}, {"title": "5 Conclusion", "content": "This paper introduces a novel approach for automatic robust pruning of spiking neural networks, named CCSRP, aimed at enhancing the stability and performance of spiking neural networks when faced with various disturbances and anomalies. Unlike conventional pruning methods that focus solely on improving network performance, CCSRP treats robust pruning as an optimization problem that integrates accuracy, sparsity, and robustness, and employs an innovative adaptive co-evolution framework to solve this problem. The essence of this method lies not only in pursuing network performance optimization but also in enhancing the network's resistance to external interferences, thereby maintaining good stability and accuracy under various challenging conditions. To our knowledge, this is the first time evolutionary algorithms (EAs) have been applied to the pruning problem of spiking neural networks. This interdisciplinary innovative application not only offers a new perspective and method for optimizing the robustness of spiking neural networks but also opens up new avenues for using evolutionary algorithms to solve complex optimization problems. Comparative experiments demonstrate that CCSRP exhibits comparable or even superior performance in some aspects to existing techniques, further validating the effectiveness and potential of this method."}, {"title": "Future work", "content": "Future work will focus on two main areas: one is to conduct more in-depth theoretical analysis to better understand the behavior and performance of CCSRP under different conditions, and the other is to explore and apply more advanced multi-objective optimization techniques to further enhance the performance of CCSRP. In particular, by introducing the latest multi-objective optimization algorithms and strategies, we aim to more finely balance the relationship between robustness, sparsity, and accuracy, thereby enhancing the overall performance and applicability of spiking neural networks in a broader range of application scenarios."}]}