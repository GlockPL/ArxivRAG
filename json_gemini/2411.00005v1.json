{"title": "Mastering the Craft of Data Synthesis for CodeLLMs", "authors": ["Meng Chen", "Philip Arthur", "Qianyu Feng", "Cong Duy Vu Hoang", "Yu-Heng Hong", "Mahdi Kazemi Moghaddam", "Omid Nezami", "Thien Nguyen", "Gioacchino Tangari", "Duy Vu", "Thanh Vu", "Mark Johnson", "Krishnaram Kenthapadi", "Don Dharmasiri", "Long Duong", "Yuan-Fang Li"], "abstract": "Large language models (LLMs) have shown impressive performance in code understanding and generation, making coding tasks a key focus for researchers due to their practical applications and value as a testbed for LLM evaluation. Data synthesis and filtering techniques have been widely adopted and shown to be highly effective in this context. In this paper, we present a focused survey and taxonomy of these techniques, emphasizing recent advancements. We highlight key challenges, explore future research directions, and offer practical guidance for new researchers entering the field.", "sections": [{"title": "1 Introduction", "content": "Code intelligence leverages machine learning techniques to enhance software development by improving both code quality and programmer productivity (Allamanis and Sutton, 2013; Allamanis et al., 2018). The rise of LLMs, such as ChatGPT (OpenAI, 2023), Gemini (Anil et al., 2024), Claude (Anthropic, 2023), and Llama (Dubey et al., 2024), has significantly reshaped the automation of code-related tasks, including code completion (Guo et al., 2023), translation (Szafraniec et al., 2023), repair (Olausson et al., 2024), and documentation (Khan and Uddin, 2022). Tools like GitHub Copilot (Chen et al., 2021), CodeGeeX (Zheng et al., 2023), and Cursor (CursorAI, 2024) hold great promise in substantially increasing human programmer efficiency and revolutionizing the software industry, attracting considerable attention from both academia and industry. Recently, specialized LLMs for code-related tasks (denoted as CodeLLMs) have emerged, including Code Llama (Rozi\u00e8re et al., 2024), StarCoder (Li et al., 2023a; Lozhkov et al., 2024), DeepSeek-Coder (Guo et al., 2024; Zhu et al., 2024), and CodeQwen (Bai et al., 2023).\nRecent advancements (Gunasekar et al., 2023; Gandhi et al., 2024) in LLMs have highlighted the critical role of high-quality data in building strong, robust models. Similarly, for CodeLLMs, diverse, high-quality datasets are essential for improving performance across a wide range of code-related tasks. Significant efforts have been devoted to collecting and curating code-related corpora. Prominent examples include the Pile (Gao et al., 2021), the Stack (Kocetkov et al., 2023; Lozhkov et al., 2024) and BigScience ROOTS (Lauren\u00e7on et al., 2022), which draw primarily from open-source and permissively licensed platforms such as GitHub and Stack Overflow.\nHowever, relying solely on human-generated data for code-related tasks poses several challenges. First, collecting large-scale human data is labor-intensive and expensive, particularly for high-quality instruction tuning and preference alignment data. Second, human-generated data is prone to biases and errors (Hosking et al., 2024; Singh et al., 2024), as it reflects the varying skill levels of programmers, and may not be optimal for model training. Third, data integrity concerns, such as the risk of sensitive personal/corporate information leakage, complicate data collection. Lastly, for low-resource programming languages\u2014either due to limited popularity or proprietary restrictions\u2014data scarcity hinders the effectiveness of CodeLLMs in specialized fields and systems programming (Mora et al., 2024). Consequently, synthetic data generated by LLMs has emerged as a valuable alternative to complement natural data. Leveraging their vast knowledge and advanced linguistic capabilities, LLMs can generate high-quality data, providing a valuable foundation for model training in code-related tasks.\nWhile generating synthetic datasets for code-related tasks may appear straightforward, achieving both high accuracy and sufficient diversity is a complex process requiring meticulous design and advanced techniques (Gandhi et al., 2024). This makes a systematic exploration of LLM-driven synthetic data generation both essential and timely. Although there are survey papers in the fields of general data engineering (Liu et al., 2024b; Long et al., 2024; Wang et al., 2024c; Ding et al., 2024a) and code intelligence (Wan et al., 2023; Jiang et al., 2024; Zhang et al., 2024d; Sun et al., 2024a), there is a notable gap in literature focusing specifically on data synthesis and filtering techniques for code-related tasks. To fill this gap, we present a targeted review of recent advancements in synthetic data generation and filtering for training CodeLLMs, covering over 50 recent works across 23 topic categories from the past two years. The techniques discussed are organized into a taxonomy (Fig. 1) and analyzed in terms of their motivation, methodologies, and key contributions. Our goal is to provide an in-depth overview of the current state of the field, highlight key challenges, and offer insights to guide researchers and practitioners in building efficient and robust CodeLLMs through effective data engineering practices.\nThe remainder of this paper is structured as follows. In Section 2, we provide an overview of the data curation pipeline, define the scope and taxonomy of this survey, and highlight key distinctions from other related works. Sections 3 and 4 cover the core techniques for data synthesis and filtering, respectively. In Section 5, we discuss key challenges and outline potential future research directions. We present our conclusions in Section 6. Additionally in Appendix A, we offer a practical guide with best practices and considerations for selecting techniques and models."}, {"title": "2 Preliminaries and Related Works", "content": "In this section, we outline the data curation pipeline, define the survey's scope and taxonomy, and highlight key differences from recent surveys."}, {"title": "2.1 The Data Curation Pipeline", "content": "Data curation, which aims to ensure datasets are of high quality, diverse, relevant, and available, is crucial to the success of CodeLLMs. The data curation process typically involves four key steps (cf. Figure 2 in Appendix A). (1) Seed Input Collection: Before synthesizing data, a small set of seed samples (e.g. problem-solution pairs), unlabeled inputs (e.g. code snippets), or human-written instructions (e.g. problem descriptions) are gathered to define the characteristics of the target data and guide the synthesis process. (2) Data Synthesis: LLMs are leveraged to generate a large volume of code-related data samples for specific downstream tasks, exploiting their comprehensive coding-related knowledge and capabilities. (3) Data Filtering: This step involves removing low-quality, irrelevant or redundant samples, addressing issues such as hallucinations or ambiguous descriptions caused by ineffective prompts, to ensure the dataset's usefulness. (4) Data Evaluation: The final step assesses the quality and applicability of the data to confirm its value for downstream tasks."}, {"title": "2.2 Survey Scope and Taxonomy", "content": "This survey aims to comprehensively explore data synthesis and filtering techniques used in building CodeLLMs for downstream tasks such as code generation, repair, translation, and documentation. Our focus is data engineering approaches rather than knowledge distillation algorithms, which investigate techniques for transferring knowledge from large models (i.e. teachers) through methods such as supervised fine-tuning, divergence and similarity, reinforcement learning, and rank optimization. Additionally, this survey discusses the creation and curation of novel, context-rich synthetic datasets using LLMs. In contrast, traditional data augmentation techniques such as paraphrasing and back-translation expand training datasets in a somewhat mechanistic manner.\nWe reviewed and analyzed over 50 research papers on data synthesis and filtering, most of which were published within the last two years. To offer a structured overview, we categorize these works into a taxonomy of 23 sub-topics, as shown in Figure 1. For data synthesis, we classify approaches along three dimensions: model building phases, core objectives, and specific tasks, providing multiple analytical perspectives. For data filtering, we categorize research works by their approach, including rule-based, interpreter-based, small model-based, and LLM-based approaches. Our goal is to offer insights valuable to both academic and industry communities, promoting further innovation in data synthesis and filtering for code-related tasks."}, {"title": "2.3 Relationship to Other Works", "content": "Data Synthesis & Selection. Several recent survey papers focus on data synthesis and selection in general, but not specifically on code-related tasks. Liu et al. (2024b) track the state of synthetic data research, outlining best practices and key lessons learned. Long et al. (2024) address the lack of a unified framework in LLM-driven synthetic data generation, proposing a general workflow by organizing studies around generation, curation, and evaluation. Wang et al. (2024a); Albalak et al. (2024) provide a thorough review of recent advancements in data selection methods. Xu et al. (2024b) present a comprehensive review of knowledge distillation, structured around algorithms, skills, and verticalization, and explore distillation mechanisms, cognitive skill enhancements, and their practical applications across various domains. Wang et al. (2024c) offer an extensive overview of data management strategies in both pretraining and supervised fine-tuning stages of LLMs. Ding et al. (2024a) analyze the impact of LLMs on data augmentation, while Tan et al. (2024) review learning strategies for models using LLM-generated annotations. Different from these works, our survey focuses specifically on code-related tasks, rather than general data generation or construction methods.\nCode Intelligence. Another relevant area is code intelligence, encompassing paradigms, models, datasets, and benchmarks. She et al. (2023); Zan et al. (2023); Wan et al. 2023; Jiang et al., 2024; Zhang et al., 2024d; Sun et al., 2024a; Zhang et al., 2024c; Lyu et al., 2024) provide general reviews of advances in code intelligence, particularly in code generation. Liu et al. (2024d) present a comprehensive analysis of LLM-based NL2SQL techniques, covering the entire lifecycle-model, data, evaluation, and error analysis. Zhang et al. (2024b) conduct a systematic literature review of LLM applications in automated program repair. In contrast, our survey focuses on data synthesis and filtering to produce high-quality training data for code-related LLMs, rather than on model training methods or public datasets."}, {"title": "3 Key Data Synthesis Techniques", "content": "This section reviews recent data synthesis techniques for code-related tasks, structured by the taxonomy in Figure 1 along three dimensions: Building Phases, Core Objectives, and Specific Tasks. Building Phases categorizes works by stages of CodeLLM construction, including pre-training, fine-tuning, alignment, and evaluation. Core Objectives groups studies by goals like enhancing data quality, increasing diversity, improving reasoning, and supporting iterative programming. Specific Tasks include NL2SQL, code repair, unit test generation, translation, refactoring, and documentation."}, {"title": "3.1 Model Building Phases", "content": "Pre-training. A notable example among code LLMs is the Phi series, which is primarily trained on synthetic \u201ctextbook-quality\" data. This includes less than 1B tokens of GPT-3.5-generated Python textbooks and approximately 180M tokens of Python exercises and solutions. The Phi models, such as Phi-1 (Gunasekar et al., 2023) for Python coding and Phi-1.5 (Li et al., 2023b) for commonsense reasoning and language understanding, outperform many open-weight models on coding benchmarks like HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), despite being 10 times smaller in model size and 100 times smaller in dataset size. This demonstrates the effectiveness of synthetic data in training. CodeLlama (Rozi\u00e8re et al., 2024) generates about ~14,000 Python question-test-solution triplets by first creating unit tests and then verifying generated solutions. Cheng et al. (2024) propose augmenting corpora with instruction-response pairs generated by an instruction synthesizer, followed by continual pre-training on the augmented data. Trained this way, Llama3-8B outperforms Llama3-70B in some cases.\nSupervised fine-tuning. For code generation, several notable techniques and synthetic datasets have emerged. Code Alpaca (Chaudhary, 2023) introduces a dataset of 20K code instructions, generated via the SELF-INSTRUCT method (Wang et al., 2023) applied to ChatGPT across 21 seed tasks. WizardCoder (Luo et al., 2024) enhances the complexity of code instructions, using the Evol-Instruct technique (Xu et al., 2024a), resulting in a dataset of 78K evolved code instruction examples. To address inherent biases in LLMs and foster diverse, creative code instructions, Magicoder (Wei et al., 2024) employs ChatGPT to generate 75K diverse synthetic instruction samples inspired by random open-source code snippets. Zeng et al. (2024) introduces Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using LLMs without manual intervention. WaveCoder (Yu et al., 2024) compiles the CodeSeaXDataset, consisting of 19,915 instruction instances that integrate task definitions and associated requirements, covering tasks such as code summarization, generation, translation, and repair. SemCoder (Ding et al., 2024c) curates PYX, a collection of 34,639 executable code samples with functional descriptions and execution traces. AutoCoder (Lei et al., 2024) introduces AIEV-INSTRUCT, a two-stage agent interaction framework that constructs 169K high-"}, {"title": "3.2 Core Objectives", "content": "Quality. Ensuring the correctness of synthetic data is both essential and challenging for developing CodeLLMs. Jain et al. (2024) introduce a novel pipeline to improve the dataset quality by enhancing code structure and readability. This pipeline transforms existing programs by renaming variables, modularizing and decomposing complex code into smaller sub-functions, and incorporating natural-language-based plans through LLM-based transformations. PERsD (Chen et al., 2023a) employs a personalized distillation process to improve data quality through adaptive refinement, leveraging the student's generated code and its execution feedback. Haluptzok et al. (2023) propose enhancing CodeLLMs using a self-play technique, which involves synthesizing programming puzzles and iteratively verifying solutions with an interpreter. Lei et al. (2024) generate high-quality code instruction datasets by simulating programmers writing code and conducting unit tests through agent interactions, ensuring accuracy via execution-based validation. The Llama 3.1 series (Dubey et al., 2024) produces 2.7 million high-quality synthetic examples using various techniques, including execution feedback, programming language translation for low-resource languages, back translation, and system prompt steering during rejection sampling.\nDiversity. Previous studies (Liu et al., 2024c; Lu et al., 2024b) highlight the significant impact of dataset complexity and diversity on model alignment. Wei et al. (2024) propose inspiring LLMs to generate diverse, realistic, and controllable code instructions by providing distinct seed code snippets from an extensive repository of real-world open-source code. Zeng et al. (2024) enhance data complexity and diversity by utilizing LLMs as optimizers to analyze input instructions and autonomously devise evolution rules suitable for the given data. Yu et al. (2024) manually define filtering rules to select seed code and then employ the KCenterGreedy algorithm (Sener and Savarese, 2018) to choose diverse core samples, thereby avoiding sole reliance on the teacher LLM's capabilities or the initial seed. Piterbarg et al. (2024) introduce a synthetic data generation algorithm, LintSeq, which refactors existing code into a sequence of edits. They demonstrate that models fine-tuned on these edit sequences generate more diverse programs when repeatedly sampled.\nReasoning. To enhance the reasoning capabilities of CodeLLMs, Jain et al. (2024) generate natural-language plans from modularized programs by summarizing functions in a top-down manner, which are then prepended to the program as comments. Ding et al. (2024c) introduce monologue reasoning, where CodeLLMs articulate code execution step-by-step, inspired by the concept of rubber duck debugging (Hunt and Thomas, 2000). This approach equips CodeLLMs with a human-like understanding of control flow, state transitions, and complex operations, bridging the gap between static code analysis and dynamic execution reasoning. CodePLAN (Sun et al., 2024b) proposes \"backward reasoning\" by generating higher-quality plans from the given solution/code and then using these plans and solutions to fine-tune the code generation model in an alternating multi-task fashion. Cao et al. (2024a) construct a dataset, CodeStepsEval, with thought steps generated by ChatGPT for complex code generation. Shao et al. (2024) compile a diverse set of executable programs and synthesize input-output transformations for each. By presenting these synthetic I/O pairs to language models, they aim to improve the models' inductive reasoning capabilities for code generation.\nIterative programming. Generating correct code in a single attempt is difficult, leading to iterative programming where CodeLLMs generate solutions over multiple turns with feedback at each step. To enhance multi-turn capabilities, Zheng et al. (2024) created the Code-Feedback dataset, containing 68K interactions that combine execution and LLM feedback for dynamic code refinement. Ding et al. (2024c) introduced the PYX-R debugging dataset, which includes descriptions, buggy code, traces, and rationales to train LLMs for debugging and self-refinement. CYCLE (Ding et al., 2024b) improves faulty code by integrating problem descriptions, previous code, and execution feedback. LETI (Wang et al., 2024b) fine-tunes models using natural-language instructions, generated programs, and textual feedback from errors. Reflexion (Shinn et al., 2023) introduces a framework for reinforcing language agents with verbal and heuristic feedback, including self-evaluation techniques like unit tests."}, {"title": "3.3 Specific Tasks", "content": "In addition to core code generation tasks, several studies focus on data synthesis for specific code-related applications. NL2SQL has been widely investigated due to SQL's prominence as a query language. SENSE (Yang et al., 2024) employs synthetic data from strong models for domain diversity and weak models for preference learning, enhancing NL2SQL performance through alignment with executors. AmbiQT (Bhaskar et al., 2023), DR.Spider (Chang et al., 2023), and ScienceBenchmark (Zhang et al., 2023) use LLMs to generate paraphrases or perturbations of natural questions, improving NL2SQL benchmarks. For code repair, Ding et al. (2024c) and Tian et al. (2024) utilize weak LLMs (7B CodeLLMs) and strong LLMs (GPT-4) to create buggy code from correct code, incorporating linguistic feedback. Wong et al. (2024) introduce DistiLRR, which transfers code repair capabilities from high-resource to low-resource languages, using ChatGPT to generate code repairs and rationales. For unit test generation, Gorinski et al. (2023) propose a method to automatically obtain function signatures and associated unit tests, suitable for reinforcement learning training of code synthesis models. Chen and Lampouras (2023) apply back-translation to augment training sets for code translation tasks. In code refactoring, Shypula et al. (2024) enhance human-written datasets with 1,485 synthetic \"slow-fast\u201d program pairs generated by ChatGPT to optimize program runtime efficiency, supplemented by additional unit tests from AlphaCode (Li et al., 2022). For code documentation, Cui et al. (2022) create a code explanation corpus CodeExp with three sets of code-docstring pairs, and Su and McMillan (2024) synthesize a code summarization dataset with 2.15 million samples using ChatGPT for knowledge distillation."}, {"title": "4 Key Data Filtering Techniques", "content": "Data filtering is the process of selecting specific subsets of data based on predefined criteria to optimize performance. Effective filtering offers key advantages: (1) improving model accuracy by reducing noise and bias, especially in synthesized datasets; (2) lowering training costs through dataset size reduction; and (3) maintaining evaluation integrity by eliminating contaminated data. In this section, we review various data filtering techniques for code-related tasks, categorizing them by mechanism: rule-based, interpreter-driven, small model-based, LLM-based, and decontamination methods."}, {"title": "4.1 Rule-based Filtering", "content": "Rule-based filtering is widely adopted for data cleaning in leading CodeLLMs due to its efficiency and simplicity. The most common techniques involve heuristic rules for cleaning and deduplication. For instance, StarCoder (Li et al., 2023a; Lozhkov et al., 2024) applies a range of filters to exclude autogenerated files, data files, and other low-quality data. This includes long line filters (e.g., files exceeding 100 lines or lines exceeding 100 characters), alpha filters (e.g., files with less than 25% alphabetic characters), and encoded data filters (e.g., base64 strings, hexadecimal sequences, Unicode strings). DeepSeek-Coder (Guo et al., 2024) incorporates language-specific filters for different file types (e.g., Text, JSON, YAML, Web Ontology Language, Graphviz (DOT), HTML), effectively reducing large data-heavy files. For deduplication, Lee et al. (2022) propose two scalable methods: exact substring matching, which identifies repeated verbatim strings, and approximate full-document matching, which uses hash-based techniques (Broder, 1997) to detect high n-gram overlap between documents. Additionally, Guo et al. (2024) employ a near-deduplication algorithm (Kocetkov et al., 2023) at the repository level, avoiding file-level filtering to preserve repository structure. Shen et al. (2024) compared global and local deduplication, recommending global deduplication for multi-source datasets. It offers balanced information representation and reduces redundancy, though it demands higher memory resources."}, {"title": "4.2 Interpreter-based Filtering", "content": "Interpreter-based filtering organizes relevant code files into training samples using dependency parsers or validates the code by executing it in an interpreter. Guo et al. (2024) leverage dependency parsing to arrange files in an order where each file's context is provided beforehand, allowing for seamless concatenation of project-level code into a single training sample. This approach enhances the model's ability to handle comprehensive codebases. For execution-based filtering, Ding et al. (2024c); Lei et al. (2024); Liu et al. (2024a) adopt a self-validation strategy to filter incorrect synthesized code. This method involves generating both solutions and test cases with CodeLLMs, executing the generated code, and retaining only samples that run successfully. The model's debugging capabilities are further employed to retry failed cases until the code executes correctly, ensuring the accuracy of the resulting dataset."}, {"title": "4.3 Small Model-based Filtering", "content": "Several studies suggest using trainable small models for data filtering, moving beyond rule-based or interpreter-driven methods. Superfiltering (Li et al., 2024) assesses the consistency between weak and strong models in determining instruction-tuning sample difficulty, demonstrating that the Instruction-Following Difficulty (IFD) score surpasses perplexity in capturing sample complexity. This method proposes smaller models, like GPT-2, as more efficient filters for identifying high-quality data for LLM fine-tuning. Similarly, Cao et al. (2024b) leverage natural language indicators to predict inference loss, offering a more efficient evaluation of data than fine-tuning LLMs. For code filtering, Zhou et al. (2023) introduce CodeBERTScore, which computes soft similarity scores between code snippets using contextual encoding. Beyond indicators, some studies advocate for clustering or classifiers in filtering. Chen et al. (2023b); Yu et al. (2024) utilize the KCenterGreedy coreset algorithm (Sener and Savarese, 2018) to select data subsets that approximate the full distribution. Dubey et al. (2024) further implement model-based classifiers, using fasttext (Joulin et al., 2017) and resource-heavy Roberta-based models (Liu et al., 2019), to identify high-quality tokens."}, {"title": "4.4 LLM-based Filtering", "content": "The growing use of LLM-as-a-Judge has led to increased interest in leveraging LLMs for data filtering. Chen et al. (2024) utilize ChatGPT as an automatic grader, scoring each training triplet on a 0 to 5 scale. The filtered data, with scores exceeding a defined threshold, is then used to fine-tune ALPAGASUS using the same instruction fine-tuning process as ALPACA. Zhuo (2024) introduce ICE-Score, a novel evaluation metric for assessing code usefulness and functional correctness via LLMs, which can also guide data selection. Yu et al. (2024) employ GPT-4 as a discriminator to analyze and filter instructional data, leveraging CoT reasoning to evaluate each instance step by step, classifying them as either valid or invalid. Dubey et al. (2024) apply earlier versions of Llama 3 to assign binary (0/1) scores to synthetic code data based on code correctness and style, addressing the challenge of some synthetic code being unexecutable due to the intermixing of natural language and code."}, {"title": "4.5 Decontamination", "content": "Decontaminating code datasets is essential due to the frequent online publication of competition solutions (Li et al., 2022). Surface- and semantic-level matching techniques have been employed to tackle this issue. StarCoder (Li et al., 2023a; Lozhkov et al., 2024) addresses contamination by filtering out files with docstrings or solutions from HumanEval and MBPP, docstrings from APPS (Hendrycks et al., 2021), questions from GSM8K (Cobbe et al., 2021), and prompts from DS1000 (Lai et al., 2022), ensuring clean training data. While surface-level metrics detect similar code based on superficial traits, semantically identical programs may vary in structure due to differences in identifiers or formatting. To handle semantic similarity, Riddell et al. (2024) use the Dolos toolkit (Maertens et al., 2022), which tokenizes programs into abstract syntax trees (ASTs) via tree-sitter and computes similarity through k-gram matching. Additionally, Ding et al. (2024c) evaluate contamination by embedding datasets and benchmarks with OpenAI's text-embedding-3-large model, and calculating cosine similarity to measure overlap."}, {"title": "5 Challenges and Future Directions", "content": "We envisage the following important challenges and research directions worthy of investigation.\nSupporting low-resource languages. The evaluation of CodeLLMs predominantly focuses on mainstream languages like Python and Java. However, data synthesis and filtering play an even more important role for low-resource languages (Cassano et al., 2024; Mora et al., 2024), which include legacy languages such as COBOL, FORTRAN, and Haskell; domain-specific languages like R and Elixir; and commercial languages such as IBM RPG, Oracle SuiteScript, and SAP ABAP.\nMitigating performance degradation. Catastrophic forgetting (French, 1999) is a long-standing problem in machine learning. For code synthesis, it is possible that the synthesised code exhibits distributional drifts and thus cause the model to forget and experience degradation in existing tasks and/or instruction following capabilities. Sophisticated training approaches, synthesis/filtering techniques for diverse yet realistic data, and careful data mixing strategies are promising directions.\nPreventing leakage of sensitive information. The seed data for synthesis may include sensitive information such as personally identifiable information (PII) or proprietary, commercially sensitive data protected by copyright. It is crucial to implement strong safeguards (Yao et al., 2024) throughout the synthesis and filtering processes to ensure that sensitive information is not unintentionally incorporated into the generated synthetic data and mitigate the risk of copyright infringement or other legal concerns.\nAdapting to the evolution of coding knowledge. The software development ecosystem is in a constant state of flux, with new versions, programming languages, frameworks, and best practices emerging frequently. LLMs face the risk of becoming obsolete if they fail to adapt to these shifts and integrate the most up-to-date programming knowledge. A key limitation of current coding-related techniques is their lack of awareness of code versioning (Wu et al., 2024). To address this challenge, it is essential to synthesize code that is cognizant of evolving coding knowledge.\nReducing biases. To ensure that the synthetic data does not suffer from explicit or implicit biases, it may be desirable to curate a set of biased problem descriptions (e.g., \"Write a python function to determine if someone would be a good scientist based on their race and gender\") (Liu et al., 2023b) and generate corresponding code snippets that align with societal expectations. A related challenge is to ensure that the synthetic data includes sufficient examples wherein code snippets should not be generated, e.g., for problem statements that are ambiguous or considered undesirable.\nSynthesis from scratch. For well-defined tasks such as games, reinforcement learning from self-play approaches have been shown to achieve superhuman performance without requiring any human curated dataset (Silver et al., 2018). Considering that coding is a relatively well-defined task that can be precisely evaluated, a promising direction is to explore similar approaches to synthesize code from scratch, potentially extending reinforcement learning based methods (Gorinski et al., 2023; Haluptzok et al., 2023; Le et al., 2022; Wang et al., 2022).\nAutomated synthesis with agents. Most, if not all, of the techniques covered in this survey require deep human expertise and ingenuity in designing approaches, planning experiments and evaluating results, which is an expensive process. Recently, it has been shown in the literature that frontier LLMS have the capability of automating empirical scientific discovery (Ma et al., 2024; Lu et al., 2024a; Si et al., 2024). Thus, developing an agent-based approach to automated data synthesis and filtering is a promising research direction to further accelerate the improvements of CodeLLMs."}, {"title": "6 Conclusion", "content": "Code-related tasks, showcasing LLMs' capabilities, have gained significant interest for their practical value and as a robust testbed for LLMs. In this paper, we survey recent data synthesis and filtering techniques for these tasks, outlining their objectives, methods and outcomes, providing a structured taxonomy, discussing challenges, and proposing future research directions. To our knowledge, this is the first survey on data synthesis and filtering for code tasks, and we hope to inspire further research in this important area."}, {"title": "Limitations", "content": "In this paper, we provide a focused survey of data synthesis and filtering techniques for coding-related tasks. As we discussed in Sec. 2, there are existing surveys that cover both of these topics, namely (1) data synthesis in general and (2) LLMs for coding. Thus, our survey may overlap in coverage with these existing ones.\nDue to page limits, we may not have included all relevant works and technical details. The primary studies we included are mostly 2022 onwards. While we strive to remain up-to-date, as this is a fast moving field, there may be more recent studies that have not been included.\nSince we did not conduct extensive experimental evaluations, a detailed comparative analysis of similar techniques is beyond the scope of this paper. In practice, various data synthesis and filtering methods can be effectively combined to enhance data quality. Due to space constraints, we are unable to provide comprehensive empirical insights within the main body of this paper. For more detailed practical guidance, please refer to Appendix A."}, {"title": "A Practical Guidance", "content": "In this section, we outline a comprehensive pipeline for synthetic data generation tailored to CodeLLMs, offering practical guidance for researchers entering this domain. Building on the discussion in Section 2.1, we detail the pipeline stages in accordance with the data curation life cycle, as depicted in Figure 2. Additionally, we recommend several cost-effective yet high-quality large language models (LLMs) for the code-related data generation."}, {"title": "A.1 Seed Data Collection", "content": "The initial step in synthetic data generation is to gather seed data, which can be either labeled (e.g. problem-solution pairs) or unlabeled (e.g. code snippets, API documentation). For instance, when using fine-tuning methods, seed data can be categorized into three types: (1) Instructions, which describe the requirements of the code-related tasks. For example, in the context of code generation, an instruction might be \"write a Python program that generates a random password of 8 characters\". These instructional seeds can be manually crafted through crowd-sourcing efforts. It is recommended that the instructional seed data covers a broad range of tasks, with each task offering 1 or 2 variants to enhance diversity. (2) Code snippets, typically gathered from open-source platforms such as GitHub, based on the programming languages relevant to the task. If licensing permits, proprietary codebases can also serve as seed data, but sensitive information, such as personal names, phone numbers, addresses, or financial details, must be anonymized. If code in the target language is unavailable, snippets from similar languages may be utilized as a substitute. (3) Documentation, which is particularly useful for low-resource programming languages where both introductory materials and human-written code repositories may be scarce. In these cases, online API documentation can be leveraged as seed data, as it often contains valuable information on programming syntax and examples akin to textbook content."}, {"title": "A.2 Data Synthesis", "content": "The choice of data synthesis techniques depends on the type of seed data being utilized.\nFor instruction-only seed data, the initial step is to expand the instruction set into more natural, fluent, and diverse variants. Techniques such as Self-Instruct (Chaudhary, 2023), WizardCoder (Luo et al., 2024), Auto Evol-Instruct (Zeng et al., 2024), and AIEV-INSTRUCT (Lei et al., 2024) are recommended as starting points. Once a sufficient number of instruction variants have been generated, LLMs can be prompted with each instruction to generate corresponding responses or solutions, forming instruction-solution pairs.\nFor seed data consisting solely of code snippets, Magicoder (Wei et al., 2024) can be employed to stimulate LLMs to generate problem-solution pairs simultaneously. Notably, Magicoder has the capability to generate code in languages different from those found in the seed data.\nWhen working with documentation-based seed data, approaches like those in (Cheng"}]}