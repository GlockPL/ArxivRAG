{"title": "A Deep Learning Approach to Localizing Multi-level Airway Collapse Based on Snoring Sounds", "authors": ["Ying-Chieh Hsu", "Stanley Yung-Chuan Liu", "Chao-Jung Huang", "Chi-Wei Wu", "Ren-Kai Cheng", "Jane Yung-Jen Hsu", "Shang-Ran Huang", "Yuan-Ren Cheng", "Fu-Shun Hsu"], "abstract": "This study investigates the application of machine/deep learning to classify snoring sounds excited at different levels of the upper airway in patients with obstructive sleep apnea (OSA) using data from drug-induced sleep endoscopy (DISE). The snoring sounds of 39 subjects were analyzed and labeled according to the Velum, Oropharynx, Tongue Base, and Epiglottis (VOTE) classification system. The dataset, comprising 5,173 one-second segments, was used to train and test models, including Support Vector Machine (SVM), Bidirectional Long Short-Term Memory (BiLSTM), and ResNet-50. The ResNet-50, a convolutional neural network (CNN), showed the best overall performance in classifying snoring acoustics, particularly in identifying multi-level obstructions. The study emphasizes the potential of integrating snoring acoustics with deep learning to improve the diagnosis and treatment of OSA. However, challenges such as limited sample size, data imbalance, and differences between pharmacologically induced and natural snoring sounds were noted, suggesting further research to enhance model accuracy and generalizability.", "sections": [{"title": "Introduction", "content": "Obstructive sleep apnea (OSA) manifests as recurrent episodes of apnea or hypopnea during sleep. In the United States, OSA affects approximately 13% of men and 6% of women\u00b9. Patients afflicted with OSA commonly present with symptoms such as snoring (often prompting medical consultation), excessive daytime sleepiness, morning headaches, heightened susceptibility to cardiovascular ailments, and increased risk of vehicular accidents. Polysomnography (PSG) is the gold standard for diagnosing OSA, albeit offering limited insights into the precise anatomical locations of upper airway obstruction. A myriad of treatment modalities, both non-invasive and invasive, exist for managing OSA. Non-invasive interventions encompass continuous positive airway pressure (CPAP), oral appliances, weight reduction strategies, and positional therapy2,3, while invasive approaches comprise nasal surgeries, uvulopalatopharyngoplasty, tongue base reduction surgery, genioglossus advancement, and maxillomandibular advancement4-7. However, the efficacy of these treatments varies, necessitating accurate identification of the site of upper-airway obstruction to guide therapeutic decisions. For instance, OSA patients with epiglottic collapse often exhibit poor adherence to CPAP therapy, while oral appliances are more suitable for individuals with tongue base obstruction.\nSeveral invasive techniques exist for localizing upper airway obstructions, including nasofibroscopy with the Muller maneuver, upper airway pressure measurement, and drug-induced sleep endoscopy (DISE)10-12. However, the awake state of patients during the Muller maneuver limits its ability to accurately reflect upper airway dynamics during sleep, leading to substantial discrepancies in identifying sites of collapse compared to DISE, with a reported mismatch rate of 76%13. Non-invasive methods for identifying upper airway collapse include computed tomography and dynamic magnetic resonance imaging14,15. Nonetheless, computed tomography poses radiation exposure risks, while dynamic sleep MRI is hindered by excessive noise levels and substantial costs. Among these methods, only DISE enables direct and precise assessment of upper airway collapse during pharmacologically induced sleep12,16.\nAcoustic analysis of snoring sounds serves various diagnostic purposes, including OSA diagnosis, localization of upper airway obstruction, and post-operative monitoring17-20. Snoring, the hallmark symptom of OSA, arises from the vibration of collapsed soft tissues during sleep21. Notably, the acoustic characteristics of snoring, such as frequency spectrum, exhibit variations corresponding to the locations of soft tissue collapse. Thus, analysis of snoring acoustics offers a promising avenue for pinpointing the precise sites of obstruction, encompassing both single and multi-level obstructions. Notably, multi-level obstructions could present in up to approximately 65% of OSA patients, with single-level obstructions observed in the remaining 35%22,23. However, prior studies investigating obstruction localization through acoustic analysis have primarily focused on identifying single-level obstructions. Given this gap, the current study aims to develop an AI model capable of accurately localizing obstruction sites, including both single and multi-level obstructions, based on snoring sounds in OSA patients. To this end, snoring signals and obstruction site data were concurrently collected during DISE procedures and utilized for training and validating the AI model."}, {"title": "Material and Methods", "content": "A cohort comprising 39 individuals diagnosed with OSA and aged 20 to 70 years was assembled from outpatient otolaryngology clinics between November 2019 and November 2021. Diagnosis of OSA was corroborated through attended PSG, adhering to the standardized protocols outlined by the American Academy of Sleep Medicine24. Specifically, individuals intolerant to continuous CPAP therapy who actively sought surgical interventions were included in the study. Exclusion criteria encompassed individuals with a history of prior OSA surgery, American Society of Anesthesiologists class 3 or above with a heightened anesthesia risk for a DISE procedure, documented adverse reactions to propofol, or pregnancy status. Before participation, all enrolled subjects provided written informed consent per the guidelines established by the Research Ethics Review Committee of Taipei Tzu-Chi Hospital (permit number: 09-XD-079).\nDISE was conducted within an operating theater under the supervision of a seasoned otolaryngologist for every enrolled subject. Before the procedure, subjects received nasal cavity decongestion and local anesthesia. Sedation was induced through intravenous administration of propofol utilizing a target-controlled infusion system. The subject was kept in a supine position during the procedure. A flexible nasofibroscope was inserted through the subjects' right nostril to evaluate upper airway collapse. The bispectral (BIS) index was maintained between 65 and 70 throughout the DISE procedure to ensure optimal anesthesia depth.\nDuring the DISE procedure, the subjects' breathing sounds were recorded using a digital stethoscope (AccurSound AS-101, Heroic Faith Medical Science Co., Ltd., New Taipei, Taiwan) connected to a smartphone (Mi 9T pro, Xiaomi, Beijing, China). The acoustic patch of the digital stethoscope was securely affixed to the subjects' submental region by tapes. The breathing sounds were sampled at a rate of 4 kHz with a bit depth of 16 bits.\nFollowing the DISE procedure, videos documenting DISE assessments were compiled and subsequently scored using the Velum (V), Oropharynx (O), Tongue Base (T), and Epiglottis (E) categorization system, collectively referred to as the VOTE system. This system delineates anatomical regions, including the soft palate, uvula, lateral pharyngeal wall (V); palatine tonsils, lateral pharyngeal wall (O); tongue base, lingual tonsils (T); and epiglottis (E)25. The endoscopic video snapshots on the left side of Fig 1 a\u2013d display examples of partial upper airway obstruction at the different levels."}, {"title": "VOTE Labeling on Breathing Sounds", "content": "The recorded breathing sounds were subsequently transformed into spectrograms using labeling software26, employing a short-time Fourier transform with a Hanning window size of 256 and a hop length of 64. After synchronizing the endoscopic video tapes with the spectrograms, two board-certified otorhinolaryngologists annotated obstruction periods' start and end times according to the video assessment reference. The graphs on the right side of Fig 1 a-d display the spectrogram of snoring sounds excited by the obstructions at the V, O, T, and E levels, respectively."}, {"title": "AI Models for Obstruction Site Classification", "content": "We first truncated the labeled periods of the breathing sounds into 1-second segments. The 1-s segments underwent preprocessing to extract various features, serving as the primary input for the Al models. In this study, we trained three different models, namely, bi-directional long short-term memory (BiLSTM) networks27, support vector machine (SVM)28, and ResNet-50 networks29, to classify the 1-s segments based on the obstruction sites.\nWe distributed the 1-s segments into training and test data sets based on a ratio close to 9:1. In the training data set, one-tenth of the segments were further assigned to the validation set.\nFor the BiLSTM model, 13 static, 13 delta, and 13 delta-delta Mel-frequency cepstral coefficients (MFCCs)30 were computed, resulting in a total of 39 MFCCs. The MFCCs were calculated with a Hanning window length of 100 and a hop length of 25. An L2 normalizer was used to normalize the MFCCs to a unit form. The normalized MFCCs were fed into the BiLSTM as input.\nFor the SVM model, we further used the Bag of Audio Words (BoAW) model to encode the MFCCs into new feature vectors31,32. The BoAW model's codebook size was 200, and the assignment was 5.\nFor the ResNet-50 networks, the Mel-spectrogram was used as model input. The Mel- spectrogram was calculated with a Hanning window length of 2048 and a hop length of 32.\nAll the feature extraction and model training were completed using Python. The flow chart of feature extraction and model training is displayed in Fig 2."}, {"title": "Statistics", "content": "Precision, recall, and F1-score are crucial metrics used to evaluate the performance of AI models, especially in classification tasks33,34. These metrics are essential for understanding the performance of Al models, as they provide insights into different aspects of the model's predictive capabilities.\nPrecision measures the proportion of true positive predictions among all positive predictions made by the model. It indicates how many of the predicted positive instances are positive. High precision means that the model has a low false positive rate. Mathematically, it is defined as:\nPrecision = $\\frac{TP}{TP + FP}$\nwhere TP is the number of true positives and FP is the number of false positives33.\nRecall, also known as sensitivity or true positive rate, measures the proportion of true positive predictions among all actual positive instances. It indicates how well the model can identify positive instances. 35 High recall means that the model has a low false negative rate. Mathematically, it is defined as:\nRecall = $\\frac{TP}{TP + FN}$\nThe F1-score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall, making it useful when you need to consider both false positives and false negatives. Mathematically, it is defined as:\nF1-score = 2x $\\frac{Precision \u00d7 Recall}{Precision + Recall}$\nThe F1-score is particularly useful in scenarios where the class distribution is imbalanced34.\nIn this study, we used these three performance indexes to evaluate the trained models."}, {"title": "Results", "content": "The average age of the 39 subjects enrolled in this study was 40.05 years. Among them, 33 were male (84%) and six were female (16%). The mean apnea-hypopnea index (AHI) was 39.63 events"}, {"title": "Discussion", "content": "Earlier studies on the correlation between snoring sounds and upper airway collapse commonly differentiate between palatal and non-palatal snoring. For example, Osborne et al. demonstrated a significantly higher peak factor ratio for palatal snoring compared to non-palatal snoring36. Recent advancements in snore sound classification have focused on labeling snore sounds based on DISE videos. Qian et al. utilized enhanced wavelet features combined with the BoAWs technique for snore sound classification, achieving a 69.4% unweighted average recall (UAR)37,38. Demir et al. explored the use of low-level image texture features in snore sound classification, attaining a 72.6% UAR39. These studies suggest that varying levels of upper airway obstruction manifest in distinct snore sounds, which can be identified using artificial intelligence.\nThe most prominent snore sound classifier to date has been developed using the Munich Passau Snore Sound Corpus (MPSSC) 40. Introduced at the INTERSPEECH 2017 Computational Paralinguistic Challenge, the MPSSC dataset encompasses snoring sounds from patients undergoing DISE, collected by three medical centers and classified into four classes. However, this dataset primarily contains single-level obstructions, whereas multi-level obstructions are more prevalent in patients with OSA. Notably, Traxdorf et al.41 observed a combination of velum and/or oropharyngeal obstructions in 40% of severe OSA patients. While many Al studies analyzing snoring sounds in OSA patients utilize the MPSSC dataset, which predominantly features single-level obstructions, clinical settings reveal that most patients exhibit multi-level obstructions. To address this discrepancy, we developed models capable of classifying snores associated with multi-level obstructions, thereby aiding physicians in accurately identifying obstruction locations.\nOur results revealed the performance of the SVM model on multi-class classification. The model achieved an overall accuracy of 0.80, with macro averages for precision, recall, and F1- score of 0.86, 0.60, and 0.66, respectively. The weighted averages for precision, recall, and F1- score were 0.81, 0.80, and 0.78, respectively. The model performed best on label VT with an F1- score of 0.91 and worst on label E with an F1-score of 0.29. Similarly, the BiLSTM model achieved an overall accuracy of 0.77, with macro averages for precision, recall, and F1-score of 0.72, 0.67, and 0.69, respectively. The weighted averages for precision, recall, and F1-score were all 0.77. The model performed best on label T with an F1-score of 0.92 and worst on label E with an F1-score of 0.22. The ResNet-50 model achieved an overall accuracy of 0.78, with macro averages for precision, recall, and F1-score of 0.81, 0.71, and 0.75, respectively. The weighted averages for precision, recall, and F1-score were all 0.78. The model performed best on label O\u0415 with an F1-score of 0.92 and worst on label OT with an F1-score of 0.61. Among the three models, the SVM model demonstrated the highest macro average precision, while the ResNet-50 model had the highest macro average recall and F1-score. The BiLSTM model had the lowest overall performance in terms of accuracy and F1-score. Each model had varying strengths and weaknesses across different labels, indicating that the choice of model may depend on the specific requirements and priorities of the classification task.\nThe performance of the three models\u2014SVM, BiLSTM, and ResNet-50\u2014on multi-class classification tasks highlights distinct strengths and weaknesses. The SVM model demonstrated the highest macro average precision, indicating its ability to identify positive instances across different classes correctly. However, its recall and F1-score were lower, suggesting it might miss some positive instances. The BiLSTM model, while having the lowest overall accuracy and F1- score, showed strong performance on label T, achieving the highest F1-score among the models. This indicates that BiLSTM might be particularly effective for certain classes, but its overall performance is less consistent. The ResNet-50 model achieved the highest macro average recall and F1-score, indicating a balanced performance in identifying positive instances and minimizing false negatives. Its performance on label V was particularly strong, making it a reliable choice for tasks where this label is critical. In conclusion, the choice of model should be guided by the specific requirements of the classification task. If precision is paramount, the SVM model might be preferred. For tasks requiring high recall and balanced performance, the ResNet-50 model is a strong candidate. The BiLSTM model, despite its lower overall performance, could be valuable for specific classes where it excels. Further fine-tuning and hybrid approaches might also be explored to leverage the strengths of each model.\nGiven that snoring originates from the vibration of collapsed soft tissues during sleep, we hypothesized that different obstruction levels correspond to distinct audio features. Consequently, mixed-type obstructions may exhibit characteristics of their constituent classes. For instance, \"VO\" snoring sounds might share audio characteristics of both the velum and oropharynx, potentially leading to misclassification by a deep learning model as \"V\" or \"O\" snores. To address this, we devised a snoring sound classification model incorporating single and multi-level obstructions."}, {"title": "Conclusion", "content": "In this investigation, we have introduced a novel snoring sound classifier, offering a non-invasive approach to healthcare professionals' precise determination of upper-airway collapse conditions, thereby facilitating tailored treatment strategies. To the best of our knowledge, this study represents a pioneering effort in employing machine/deep learning methodologies to classify snoring sounds associated with single and multi-level obstructions. Our proposed model has impressive overall accuracy with F1-scores between 77\u201380%. Our findings suggest that these models hold promise as a non-invasive tool for accurately assessing upper-airway conditions."}]}