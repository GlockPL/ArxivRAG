{"title": "Contextual Representation Anchor Network to Alleviate Selection Bias in Few-Shot Drug Discovery", "authors": ["Ruifeng Li", "Wei Liu", "Xiangxin Zhou", "Mingqian Li", "Yuhua Zhou", "Yuan Yao", "Qiang Zhang", "Hongyang Chen"], "abstract": "In the drug discovery process, the low success rate of drug candidate screening often leads to insufficient labeled data, causing the few-shot learning problem in molecular property prediction. Existing methods for few-shot molecular property prediction overlook the sample selection bias, which arises from non-random sample selection in chemical experiments. This bias in data representativeness leads to suboptimal performance. To overcome this challenge, we present a novel method named contextual representation anchor Network (CRA), where an anchor refers to a cluster center of the representations of molecules and serves as a bridge to transfer enriched contextual knowledge into molecular representations and enhance their expressiveness. CRA introduces a dual-augmentation mechanism that includes context augmentation, which dynamically retrieves analogous unlabeled molecules and captures their task-specific contextual knowledge to enhance the anchors, and anchor augmentation, which leverages the anchors to augment the molecular representations. We evaluate our approach on the MoleculeNet and FS-Mol benchmarks, as well as in domain transfer experiments. The results demonstrate that CRA outperforms the state-of-the-art by 2.60% and 3.28% in AUC and AAUC-PR metrics, respectively, and exhibits superior generalization capabilities.", "sections": [{"title": "I. INTRODUCTION", "content": "Accelerating drug discovery is crucial for effective disease management and public health [1]\u2013[3]. Artificial intelligence (AI) has revolutionized this field by developing AI-driven drug discovery (AIDD) [4]\u2013[6], which uses deep learning to streamline traditional processes. This significantly reduces the time and cost of predicting and designing new drug molecules [7], [8]. As an integral component of AIDD, Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) models establish relationships between molecular structures and their biological activities or properties, enhancing the efficiency of drug discovery [9]\u2013[12]. Despite their success with large datasets [13]\u2013[16], the persistent scarcity of training data-stemming from the costly and time-consuming nature of chemical experiments with inherently low success rates-poses a significant challenge [17], [18]. To overcome this, few-shot learning has emerged as a promising strategy, enabling enhanced molecular property prediction in data-limited scenarios [19]. Few-shot learning enables models to adapt and perform well on new tasks using only a few training examples. Recently, several methods, such as IterRefLSTM [19], Meta-MGNN [20], ADKF-IFT [21], and PAR [22], have been proposed to address this challenge by leveraging metric-based methods or meta-learning strategies to generate molecular representations that can effectively generalize from a limited number of support set samples. These methods heavily rely on the support set to learn task-specific representations and establish decision boundaries between different molecular categories. However, the heavy reliance on small support sets makes these methods susceptible to sample selection bias. The bias arises when the samples in the support set fail to adequately represent the overall distribution of the molecular space. In drug discovery, samples often come from non-random chemical experiments and lack broad representativeness [23], [24]. These samples typically cover only a subset of the entire chemical space, causing the risk of selection bias [25]. Additionally, the significant domain gap between different tasks in drug discovery further exacerbates this challenge. Therefore, addressing sample selection bias is crucial for solving the few-shot learning problem in drug discovery. Inspired by contextual data augmentation strategies [26], we utilize contextual molecules, i.e., abundant molecule samples without labels, to fill the knowledge gaps in the limited labeled samples. Contextual data augmentation effectively alleviates sample selection bias by incorporating diverse unlabeled samples to calibrate class-specific feature deviations, thereby pre-"}, {"title": "II. RELATED WORK", "content": "Few-shot learning methods in drug discovery can be broadly categorized into transductive and inductive approaches. Trans-ductive methods, such as embedding-based nearest neighbor techniques, map molecules into a chemical space to predict properties by analyzing inter-molecular relationships within a specific task. For example, IterRefLSTM [19] employs iterative reasoning and LSTM networks, while PAR [22] generates task-specific molecular representations to construct a relation graph for label propagation. Conversely, inductive methods, including optimization-based or fine-tuning approaches like GNN-MAML [20] and ADKF-IFT [21], leverage the meta-learning framework [30]\u2013[32] to rapidly generalize to new molecules. While both methods have shown effectiveness, they often overlook the critical issue of sample selection bias. MHNfs [28] enhance molecular representations using contextual molecules, but they struggle to effectively differentiate between intra-class and inter-class relationships. Unlike MHNFs, our CRA establishes dynamic connections between unlabeled data and intra-class feature representations through contextual representation anchors, enabling the model to more accurately enhance intra-class features and mitigate sample selection bias."}, {"title": "B. Semi-supervised Few-shot Learning", "content": "Semi-supervised few-shot learning, a method that integrates a small amount of labeled data with a larger pool of unlabeled data, has gained significant attention in the field of machine learning. The commonly used strategy in this method is to train the labeled few-shot data together with the unlabeled data by using pseudo-labels [33]\u2013[35]. For instance, Meta-SSFSL [36] incorporates soft k-means and unlabeled data to refine prototypes, surpassing the performance of ProtoNets [37]. PRWN [38] enhances few-shot learning models by implementing prototypical magnetization and global consistency to improve representation learning. Cluster-FSL [39] adopts Multi-Factor Clustering for generating pseudo-labels and integrates data augmentation techniques to achieve superior results. Semi-supervised learning is not only applicable to computer vision but also suitable for processing molecular graphs in chem-informatics, where the sparsity of molecular graphs requires additional chemical information to optimize predictions. In our method, we leverage the attention mechanism to implement the dual-augmentation mechanism, effectively strengthening the connections between the unlabeled data, anchors, and intra-class molecules, resulting in more precise and robust molecular representations."}, {"title": "C. Sample Selection Bias", "content": "Sample selection bias, which leads to distorted results, occurs when samples fail to accurately represent the entire feature space, causing models that perform poorly on unseen data. This challenge is especially pronounced in few-shot learning due to the limited number of samples, making bias reduction a critical area of extensive research [40], [41]. Bias reduction techniques can be generally divided into normalization-based and calibration-based methods. For example, normalization-based approaches like Z-Score Normalization (ZN) [42], SimpleShot [43], and SEN [44] employ various normalization strategies to mitigate bias. On the other hand, calibration-based methods, such as the Distribution Calibration Method (DCM) [45] and Task-Calibrated Prototypical Representation (TCPR) [27], develop specific strategies tailored to address this issue. Our method uniquely targets sample selection bias within drug discovery, specifically addressing the few-shot molecular property prediction challenge.Unlike these methods, we introduce contextual representation anchors as bridge which dynamically enhance inter-class molecular representations to mitigate sample bias."}, {"title": "III. BACKGROUND", "content": ""}, {"title": "A. Molecular Property Prediction", "content": "Typically, deep learning models the function $f_\\theta$ to predict the molecular properties or activities $\\hat{y}$ based on the molecular features x. The model $f_\\theta$ consists of two main components: an encoder $f_e$ and a classifier g. The encoder $f_e$ is capable of processing various molecular features x, such as Simplified Molecular Input Line Entry System (SMILES) [46], [47], descriptors [48], [49], fingerprints [50], [51], and molecular graphs [52]\u2013[55], then outputs high-dimensional molecular representations. The classifier g usually takes the high-dimensional molecular representations as input to generate the predicted categories $\\hat{y}$."}, {"title": "B. Multi-Head Attention Mechanism", "content": "The multi-head attention mechanism with H heads takes a query $Q\\in \\mathbb{R}^{N_1\\times d_k}$, a key $K\\in \\mathbb{R}^{N_2\\times d_k}$, and a value $V \\in \\mathbb{R}^{N_2\\times d_k}$ as inputs, calculates H attention results, and projects them into the output embedding space:\n$Y = Multi\\text{-}HeadAttention(Q, K, V)$\n$= \\mathop{Concat}\\limits_{i=1}^H(softmax(\\frac{QW_i^K(KW_i^K)^T}{\\sqrt{d_k}})VW_i^VW_i^O)$\nwhere $d_k$ is the dimension of the key, and all $W\\in \\mathbb{R}^{d_k \\times d_k}$ are learnable parameter matrices. Here, we denote the multi-head self-attention with a residual connection as:\nX' = R-MHA(X)\n=X + Multi-HeadAttention(X, X, X)."}, {"title": "IV. METHOD", "content": "In this section, we present a detailed overview of our Contextual Representation Anchor Network (CRA), with its overall architecture illustrated in Figure 3. First, we define the few-shot learning problem in the context of molecular property prediction and introduce the role of the attention mechanism. Then, we describe the architecture of our CRA model, followed by an explanation of the CRA algorithm. Finally, we detail the training and inference phases of our model."}, {"title": "A. Problem Definition", "content": "We consider the problem of few-shot molecular property prediction. A set of molecular property prediction tasks ${T_i}_{i=1}^{N_t}$ are available for model training. Each task $T_i = {(x_{i,s}, y_{i,s})}_{s=1}^{N_s}$, where $x_{i,s} \\in \\mathbb{R}^d$ represents the molecular features and $y_{i,s} \\in \\{-1, 1\\}$ represents the molecular property or activity. Let $N_{i,c}$, denote the number of molecules in class c in $T_i$, where $c \\in \\{-1, 1\\}$. The goal is to learn a generalized model $f_\\theta$ that can quickly adapt to new, previously unseen few-shot molecular property prediction tasks ${T_i}_{i=N_t+1}^{N_{t+N_e}}$ in drug discovery. Specifically, a new few-shot learning task $T_\\tau$ comprises a support set $S_\\tau = {(x_{\\tau,s}, y_{\\tau,s})}_{s=1}^{N_s}$ to be learned with, and a query set $Q_\\tau = {(x_{\\tau,q}, y_{\\tau,q})}_{q=1}^{N_q}$ to be classified. A large unlabeled molecular dataset $B = {x_b}_{b=1}^{N_b}$ can be used as reference since it is easily accessible and has rich structural information."}, {"title": "B. Architecture", "content": "In this work, we propose a simple yet effective method called the Contextual Representation Anchor Network (CRA). This method employs a dual-augmentation mechanism to dynamically enhance the intra-class co-occurrence structures"}, {"title": "1) Context Augmentation Module:", "content": "The context augmentation module (CAM) enhances the representativeness of the initial anchors by utilizing unlabeled reference. In data-limited scenarios, the initial anchors may not sufficiently represent their respective categories. CAM incorporates contextual information from the reference, enriching the class-level contextual representation anchors with task-specific knowledge and more accurately capturing the underlying molecular co-occurrence structures. In Figure 3, given a new task that contains a support set $S_\\tau$ with an initial molecular feature matrix $S_\\tau \\in \\mathbb{R}^{N_s \\times d}$, a query set $Q_\\tau$ with an initial molecular feature matrix $Q_\\tau \\in \\mathbb{R}^{N_q \\times d}$ and a batch of context $B_\\tau$ sampled from unlabeled data set B with an initial molecular feature matrix $B_\\tau \\in \\mathbb{R}^{M \\times d}$, we first employ a shared encoder $f_e : \\mathbb{R}^d \\rightarrow \\mathbb{R}^h$ to obtain embeddings for all molecules as follows:\n$S' = f_e(S_\\tau^T), Q' = f_e(Q_\\tau), B' = f_e(B_\\tau),$\nwhere $S' \\in \\mathbb{R}^{N_s \\times h}, Q' \\in \\mathbb{R}^{N_q \\times h}$, and $B' \\in \\mathbb{R}^{M \\times h}$."}, {"title": "2) Anchor Augmentation Module:", "content": "To enhance the intra-class representativeness of individual molecules for a given task $T_\\tau$, we adopt the contextual representation anchors $P_\\tau^*$ as bridges to evoke co-occurrence structures and semantic information within each class. Specifically, we concatenate each molecular embedding with both anchors from the positive"}, {"title": "3) Matching Module:", "content": "Following previous work [28], [57], we employ a cosine similarity function $sim(\\cdot, \\cdot)$ to measure the similarity between the query molecular embedding $q_j^*$ and the support molecular embedding $s_i^*$, within the task $T_\\tau$. Here, $q_j^*$ represents the j-th embedding of $Q^*$ and $s_i^*$ represents the i-th embedding of $S^*$. The similarity value $sim(q_j^*, S^*)$ serves as the weight coefficient for the corresponding support molecular label $y_{\\tau,i}$. The prediction is then obtained by:\n$p(\\hat{y}_{\\tau,j} = 1) = \\sigma(\\frac{1}{\\sqrt{2h}}\\sum\\limits_{i=1}^{N_s} \\frac{y_{\\tau,i}}{\\mathbb{N}_{\\pm}(y_{\\tau,i})} * sim(q_j^*, s_i^*) ) ,$\nwhere $\\sigma(\\cdot)$ is the sigmoid function, and $\\mathbb{N}_{\\pm}(y_{\\tau,i})$ is the number of molecules with label $y_{\\tau,i}$ in the support set $S_\\tau$. Both $\\mathbb{N}_{\\pm}(y_{\\tau,i})$ and $\\sqrt{2h}$ prevent the logit from being too large, ensuring stable training. Additionally, $\\mathbb{N}_{\\pm}(y_{\\tau,i})$ addresses the class imbalance issue present in the FS-Mol dataset [58]. The probability of $p(\\hat{y}_{\\tau,j} = -1)$ is given by 1 \u2013 $p(\\hat{y}_{\\tau,j} = 1)$."}, {"title": "D. Training and Inference", "content": "We train our model using a set of training tasks ${T_i}_{i=1}^{N_t}$. For each task $T_\\tau$, we sample two labeled sets, $S_\\tau$, $Q_\\tau$, from $T_\\tau$, and randomly sample an unlabeled reference set $B_\\tau$ from B. The model is then trained to predict the labels in $Q_\\tau$ using $S_\\tau$ as the support set. After obtaining the predicted probability $p(\\hat{y}_{\\tau,j} = c)$ for $c \\in \\{-1, 1\\}$ across all query molecules in the query set $Q_\\tau$, we use the Binary Cross Entropy (BCE) loss function to calculate the loss:\n$L=-\\frac{1}{N_q}\\sum\\limits_{j=1}^{N_q}\\sum\\limits_{c\\in\\{-1,1\\}} [I_c(y_{\\tau,j}) log p(\\hat{y}_{\\tau,j} = c)],$\nwhere $y_{\\tau,j}$ is the ground-truth label of the j-th query molecule in $Q_\\tau$, and $I_c(y_{\\tau,j})$ is the indicator function that takes the value of 1 if $y_{\\tau,j} = c$, and 0 otherwise."}, {"title": "V. EXPERIMENTS", "content": "In this section, we evaluate the performance of CRA as introduced in Section IV-B. We first conduct experiments on the MoleculeNet and FS-Mol benchmarks to validate the efficacy of our method. To analyse the impact of various components on the model, we also conduct ablation experiments on the three components, the capacity of reference molecules, and the support set size during testing. Additionally, we visualize"}, {"title": "C. Ablation Study", "content": "To demonstrate that: 1) utilizing references (context) can enhance class-level contextual representation anchors by introducing task-specific contextual knowledge; and 2) these augmented these anchors can serves as bridges to transfer contextual knowledge to molecules, thereby reinforcing their representations and alleviating the selection bias, we conduct ablation experiments on the AM, CAM, and AAM components, based on the baseline (encoder only). The AM (attention module) is part of the AAM for comparison purposes. While the AM employs the same attention mechanism as the AAM, its embedding matrices are not"}, {"title": "D. Visualization", "content": "To validate the efficacy of CRA in enhancing molecular embeddings by introducing prior knowledge with calibrated class prototypes, we visualize the"}, {"title": "E. Domain Shift Experiment", "content": "The Tox21 dataset is a public dataset for molecular toxicity, comprising 12 different toxicological tasks and 12,707 compounds [63], [73]. The data includes a variety of sources, such as drugs and chemicals from industrial and consumer products. There is a significant domain shift from the FS-Mol dataset to the Tox21 dataset. The FS-Mol dataset considers the interactions between molecules and specific target proteins, while the Tox21 dataset is focus on the toxicities of small molecules across multiple proteins. To demonstrate the strong out-of-domain generalization ability of CRA, we train three models\u2014CRA, MHNfs [28], and PRWN [38]-on the FS-Mol dataset and evaluate their performances on the Tox21 dataset."}, {"title": "F. Generalization to Different Context Sets", "content": "To study the effect of different context sets on our model, we also conduct experiments using the GEOM dataset [74] as the context. The GEOM dataset is a standard dataset for studying molecular geometries, containing various molecules and their three-dimensional structural information. It is widely used in computational chemistry and molecular simulation research. We used 100,000 pre-processed molecules as the context. The experimental results are shown in Table IV. The results show that the choice of context dataset has a minimal impact on the performance of CRA, as both context sets yield nearly identical results. This indicates that CRA can effectively generalize across different context sets, with both yielding strong performance and minimal variation in outcomes."}, {"title": "VI. CONCLUSION", "content": "In this work, we proposed the Contextual Representation Anchor Network (CRA) to address the challenge of sample selection bias in few-shot molecular property prediction. By introducing a dual augmentation mechanism-comprising context augmentation and anchor augmentation-CRA dynamically incorporates task-specific contextual knowledge, enhancing the expressiveness of molecular representations. Our extensive evaluation on MoleculeNet and FS-Mol benchmarks, as well as domain transfer experiments on the Tox21 dataset, demonstrates that CRA consistently outperforms state-of-the-art methods in terms of both AUC and AAUC-PR metrics. Furthermore, CRA's ability to generalize across domains highlights its potential for robust application in drug discovery scenarios where labeled data is scarce. In future work, we aim to further explore the application of CRA in broader molecular and biochemical datasets, as well as to investigate the integration of additional contextual information to further improve its performance."}, {"title": "VII. BIOGRAPHY SECTION", "content": "If you include a photo:"}]}