{"title": "Decentralized Learning in General-sum Markov Games", "authors": ["Chinmay Maheshwari", "Manxi Wu", "Shankar Sastry"], "abstract": "The Markov game framework is widely used to model interactions among agents with heterogeneous utilities in dynamic and uncertain societal-scale systems. In these systems, agents typically operate in a decentralized manner due to privacy and scalability concerns, often acting without any information about other agents. The design and analysis of decentralized learning algorithms that provably converge to rational outcomes remain elusive, especially beyond Markov zero-sum games and Markov potential games, which do not adequately capture the nature of many real-world interactions that is neither fully competitive nor fully co-operative. This paper investigates the design of decentralized learning algorithms for general-sum Markov games, aiming to provide provable guarantees of convergence to approximate Nash equilibria in the long run. Our approach builds on constructing a Markov Near-Potential Function (MNPF) to address the intractability of designing algorithms that converge to exact Nash equilibria. We demonstrate that MNPFs play a central role in ensuring the convergence of an actor-critic-based decentralized learning algorithm to approximate Nash equilibria. By leveraging a two-timescale approach, where Q-function estimates are updated faster than policy updates, we show that the system converges to a level set of the MNPF over the set of approximate Nash equilibria. This convergence result is further strengthened if the set of Nash equilibria is assumed to be finite. Our findings provide a new perspective on the analysis and design of decentralized learning algorithms in multi-agent systems.", "sections": [{"title": "1 Introduction", "content": "The Markov game framework is a well-established model for capturing the coupled interactions of agents with heterogeneous utilities in dynamic and uncertain environments. This framework has been widely applied to various large-scale societal applications, including autonomous driving [1], smart grids [2], and e-commerce [3], among others. In such environments, agents must learn to adapt their behavior in the presence of other agents. Due to privacy and scalability concerns, these agents typically operate in a decentralized manner, meaning they act independently without direct communication or coordination with other agents and rely solely on local information. Furthermore, agents might not even be aware of the number or types of other agents in the environment.\nExisting literature has developed decentralized learning algorithms for specific scenarios, including purely competitive zero-sum games (see [4\u20136] and references therein), purely cooperative Markov team games (see [7\u20139] and references therein), and some of their slight generalizations (see [10-12] and references therein). However, these frameworks often fall short in capturing the complex mix of cooperative and competitive interactions that characterize real-world, large-scale multi-agent environments. Such interactions are better modeled as general-sum games. Recent research has explored decentralized learning in general-sum Markov games (e.g., [13\u201315]). Nevertheless, these algorithms typically converge only to weaker equilibrium concepts, such as correlated and coarse-correlated equilibria, which can include non-rationalizable outcomes [16], rather than reaching a Nash equilibrium. A key question still remains:\nHow to design decentralized learning algorithms for general-sum Markov\ngames with provable convergence to (or approximate) Nash equilibrium in\nthe long-run?\nDesigning decentralized learning algorithms with provable convergence to Nash equilibrium is, in general, intractable [17]. We address this intractability by introducing a new framework for studying multi-agent interactions\u2014the Markov near-potential function (MNPF)\u2014where the change in an agent's value function due to a unilateral shift in its policy is \"approximately\" captured by the corresponding change in the MNPF (see Definition 3.1). We show that MNPFs always exist for Markov games with finite state and action sets (Proposition 3.2). We leverage the structure of MNPFs to study the convergence of a recently introduced actor-critic-based decentralized learning algorithm from [10] for any general-sum Markov game. In this algorithm, each agent independently updates their local estimate of long-horizon utility (Q-function) on a faster timescale and their policies on a slower timescale, using only the bandit feedback about the state transitions and one-stage local reward feedback. Due to the two-timescale nature of the algorithm, we leverage results from two-timescale stochastic approximation theory [18] to study its convergence. This theory shows that it is sufficient to demonstrate (i) the convergence of the fast updates (i.e., Q-function estimates) while assuming the slow updates (i.e., policy updates) are static, and (ii) the convergence of a continuous-time dynamical system associated with the slow updates, constructed using the converged values of the fast updates. While (i) follows from standard results on the convergence of temporal difference learning in reinforcement"}, {"title": "2 Setup", "content": "A (general-sum) Markov game G is identified by the tuple $(I, S, (A_i)_{i\\in I}, (u_i)_{i\\in I}, P, \\gamma)$, where\n\u2022 I is a finite set of players;\n\u2022 S is a finite set of states;"}, {"title": "3 Markov Near-potential Function", "content": "We introduce the following notion of Markov near potential game which is crucial for subsequent disposition."}, {"title": "4 Decentralized Learning Algorithm: Design and Anal-\nsis", "content": "In this section, we present a decentralized learning algorithm in which each player makes decisions based solely on the current state information and their local reward feedback. Furthermore, players do not need any knowledge about the other players. Using MNPF, we provide theoretical guarantees on the long-run outcomes of the algorithm.\nFor every $i \\in I, s \\in S, a_i \\in A_i, \\pi\\in \\Pi$, we define Q-function as\n$Q_i(s,a_i; \\pi) := u_i(s, a_i, \\pi_{-i}) + \\gamma\\sum_{s'\\in S} P(s'\\vert s,a_i, \\pi_{-i})V_i(s', \\pi)$,\nwhich is player i's expected long-horizon discounted utility when the game starts in state s and they play action $a_i$ in the first stage and then employs policy $\\pi_i$ from the second stage onwards, and other players always employ policy $\\pi_{-i}$. With slight abuse of notation, we define $Q_i(s;\\pi) = (Q_i(s,a_i;\\pi))_{a_i\\in A_i} \\in \\mathbb{R}^{|A_i|}$. Furthermore, given a Q-function $Q_i$ and a policy $\\pi\\in \\Pi$, we define optimal one-stage deviation from in state $s\\in S$ as"}, {"title": "4.1 Decentralized Learning Algorithm", "content": "We employ the discrete-time decentralized learning algorithm proposed in our previous paper [10, Algorithm 1], which is based on actor-critic algorithms. In each iterate t of the algorithm, every player $i \\in I$ updates the following quantities: (i) the counters $n^t = (n^t(s))_{s\\in S}$ and $\\tilde{n}_i^t = (\\tilde{n}_i^t(s,a_i))_{s\\in S, a_i\\in A_i}$, which keep track of the number of visits to all states and all state-action pairs up to the current iteration; (ii) their estimate of the local Q-functions $q_i^t$, which is updated as linear combination of the previous estimate and a new estimate based on realized one-stage reward and the long-horizon discounted value from the next state as estimated from the q-function estimate and policy from previous iterate (refer (3)); and (iii) their local policies $\\pi_i^t$, which is updated as a linear combination of the policy in the previous iterate, and player i's optimal one-stage deviation (refer (4)). Finally, every player samples an action $a_i^t \\sim \\pi_i^t$ with probability"}, {"title": "4.2 Convergence Guarantees", "content": "Before presenting the convergence guarantees, we introduce assumptions that are central to our analysis.\nAssumption 4.1. The initial state distribution $\\mu(s) > 0$ for all $s \\in S$. Additionally, there exists a joint action vector $(a(s))_{s\\in S}$ such that the induced Markov chain with probability transition $(P(s'\\vert s,a(s)))_{s',s\\in S}$ is irreducible and aperiodic.\nAssumption 4.1 is a common assumption to ensure ergodicity in the state-action trajectories to ensure learning the optimal policies and Q-functions [10]. Next, we require the stepsizes to satisfy the following assumption\u00b9:"}, {"title": "A Auxiliary Results", "content": "We present a technical result used in proof of Lemma 4.1.\nLemma A.1. For any Markov game G and an associated MNPF $\\Phi$ with closeness parameter $\\kappa$, it holds that\n$\\vert \\frac{\\nu_i}{\\nu_i^T} \\frac{\\partial \\Phi(\\mu,\\pi)}{\\partial \\pi_i} - \\frac{\\partial V_i(\\mu,\\pi)}{\\partial \\pi_i} \\vert \\leq \\alpha ||v_i||_2, \\forall i \\in I, v_i \\in \\mathbb{R}^{|S||A_i|}$.\nProof. For any $v_i \\in \\mathbb{R}^{|S||A_i|}$ it holds that\n$\\frac{\\nu_i}{\\nu_i^T} \\frac{\\partial \\Phi(\\mu,\\pi)}{\\partial \\pi_i} = \\lim_{h \\to 0} \\frac{\\Phi(s, \\pi_i + hv_i, \\pi_{-i}) - \\Phi(s, \\pi_i, \\pi_{-i})}{h}$\n$\\frac{\\nu_i}{\\nu_i^T} \\frac{\\partial V_i(\\mu,\\pi)}{\\partial \\pi_i} = \\lim_{h \\to 0} \\frac{V_i(s, \\pi_i + hv_i, \\pi_{-i}) - V_i(s, \\pi_i, \\pi_{-i})}{h}$\nAdditionally, using the definition of MNPF, we obtain\n$V_i(s, \\pi_i + hv_i, \\pi_{-i}) - V_i(s, \\pi_i, \\pi_{-i}) - \\alpha h ||v_i||_2 \\leq \\Phi(s, \\pi_i + hv_i, \\pi_{-i}) - \\Phi(s, \\pi_i, \\pi_{-i}) \\leq V_i(s, \\pi_i + hv_i, \\pi_{-i}) - V_i(s, \\pi_i, \\pi_{-i}) + \\alpha h ||v_i||_2$.\nDividing everything by h in (17) and using (16), we obtain\n$ \\vert \\frac{\\nu_i}{\\nu_i^T} \\frac{\\partial \\Phi(s, \\pi)}{\\partial \\pi_i} - \\frac{\\partial V_i(s, \\pi)}{\\partial \\pi_i} \\vert \\leq \\alpha ||v_i||_2, \\forall i \\in I, v_i$.\nWe now present a technical result used in proof of Theorem 4.2.\nLemma A.2. For any $\\delta \\geq 0$, the function $\\Gamma(\\delta)$ exists, is upper semicontinuous and weakly increasing."}]}