{"title": "Reinforcement Learning Platform for Adversarial Black-box Attacks with Custom\nDistortion Filters", "authors": ["Soumyendu Sarkar", "Ashwin Ramesh Babu", "Sajad Mousavi", "Vineet Gundecha", "Sahand\nGhorbanpour", "Avisek Naug", "Ricardo Luna Guti\u00e9rrez", "Antonio Guillen", "Desik Rengarajan"], "abstract": "We present a Reinforcement Learning Platform for Adver-\nsarial Black-box untargeted and targeted attacks, RLAB, that\nallows users to select from various distortion filters to cre-\nate adversarial examples. The platform uses a Reinforcement\nLearning agent to add minimum distortion to input images\nwhile still causing misclassification by the target model. The\nagent uses a novel dual-action method to explore the input\nimage at each step to identify sensitive regions for adding\ndistortions while removing noises that have less impact on\nthe target model. This dual action leads to faster and more\nefficient convergence of the attack. The platform can also be\nused to measure the robustness of image classification models\nagainst specific distortion types. Also, retraining the model\nwith adversarial samples significantly improved robustness\nwhen evaluated on benchmark datasets. The proposed plat-\nform outperforms state-of-the-art methods in terms of the av-\nerage number of queries required to cause misclassification.\nThis advances trustworthiness with a positive social impact.", "sections": [{"title": "Introduction", "content": "Despite deep learning models demonstrating impressive per-\nformance in various tasks, they remain highly susceptible to\ninput data corruption. This susceptibility is particularly con-\ncerning in safety-critical applications such as self-driving\ncars, facial recognition systems, and image-based authen-\ntication. The possibility of natural and domain-specific dis-\ntortions at deployment poses a significant challenge for these\nmodels. Therefore, measuring the robustness of deep learn-\ning models against distortions becomes essential to uncover\nvulnerabilities and limitations of poorly trained models.\nIn the field of adversarial attacks, white-box attacks have\nbeen widely explored (Szegedy et al. 2013)(Goodfellow,\nShlens, and Szegedy 2014) (Chakraborty et al. 2018) (Su,\nVargas, and Sakurai 2019). These attacks require complete\nknowledge of the target model, including its architecture\nand parameters, to assess its vulnerability against distor-\ntions. However, access to such detailed information about\nthe model is often restricted in real-world scenarios due to\nintellectual property concerns and support issues. This lim-\nitation makes white-box attacks less practical for evaluating\nmodels in many applications.\nIn contrast, black-box attacks do not require complete vis-\nibility into the target model. These attacks operate with lim-\nited information and exploit vulnerabilities by interacting\nwith the model through input queries. However, black-box\nattacks suffer from inefficiency, often necessitating a large\nnumber of queries to create adversarial samples that can\neffectively compromise the evaluated model. Furthermore,\nmany state-of-the-art black-box attack approaches rely on\nspecific unnatural distortions based on hand-crafted heuris-\ntics, limiting their applicability and effectiveness.\nIn this work, we propose a Reinforcement Learning agent\nfor a Platform (RLAB) that can learn a policy to make an ad-\nversarial attack with fewer queries and a high success rate.\nThis RL agent is versatile enough to allow the user to plug\nin one or more of their own distortion filters in the spirit of"}, {"title": "Related Works", "content": "Traditional metrics like accuracy, precision, recall, and F1\nscore often fail to capture vulnerabilities exposed by ad-\nversarial examples. Szegedy et al. (Szegedy et al. 2013)\nfirst demonstrated adversarial attacks, and Goodfellow et\nal. (Goodfellow, Shlens, and Szegedy 2014) introduced the\nwidely-used Fast Gradient Sign Method (FGSM) for white-\nbox attacks.\nBuilding on this work, subsequent studies explored gradient-\nbased distortions to mislead models (Kurakin et al. 2016;\nKurakin, Goodfellow, and Bengio 2016; Dong et al. 2018).\nPapernot et al. (Papernot et al. 2016) introduced a saliency\nmap to identify vulnerable regions of the input for targeted\nattacks. Similarly, Moosavi et al. (Moosavi-Dezfooli, Fawzi,\nand Frossard 2016) proposed DeepFool, a straightforward\nyet effective method for adding perturbations to deceive ma-\nchine learning models.\nBlack-box attacks operate with limited or no visibility\ninto the model. In partially visible cases, information\nlike loss functions, prediction probabilities, or top-K la-\nbels may guide query-based attacks. Comprehensive sur-\nveys by Michel et al. (Michel, Jha, and Ewetz 2022) and\nChakraborty et al. (Chakraborty et al. 2018) highlight trends\nin adversarial attacks, while Ilyas et al. (Ilyas et al. 2018)\ntackled constraints like limited visibility and query access.\nNotable black-box methods, including Square Attack\n(Andriushchenko et al. 2020), SimBA (Guo et al. 2019), and\nLeBA (Yang et al. 2020), operate within fixed L2/L\u221e bud-\ngets and successfully target convolutional networks. Guo et\nal. (Guo et al. 2019) iteratively sampled vectors from an or-\nthonormal basis to modify images, while Andriushchenko et\nal. (Andriushchenko et al. 2020) used square-shaped updates\nat random positions under budget constraints.\nRecent advancements, such as EigenBA (Zhou et al.\n2022), Pixle (Pomponi, Scardapane, and Uncini 2022),\nQueryNet (Chen et al. 2021), AdvFlow (Mohaghegh et al.\n2020), and CG Attack (Feng et al. 2022), achieve state-of-\nthe-art results. Most methods rely on unnatural distortions,\nwhich may not generalize across use cases (Ratner et al.\n2017; Shijie et al. 2017), emphasizing the need for adaptable\nplatforms. Natural perturbations like Gaussian noise, blur, or\nbrightness changes could offer more practical alternatives.\nMost state-of-the-art approaches focus on adding similar\nbut unnatural distortions to the input to generate adversarial\nsamples. There is no guarantee that they would still work if\nit is applied for a different use case (Ratner et al. 2017)(Shi-\njie et al. 2017). This raises the need for a common platform\nthat allows switching the type of distortion used based on\nthe actual needs of individual use cases. Also, adversarial at-\ntacks using naturally occurring perturbations such as Gaus-\nsian noise, blur, changes in brightness, and dead pixels may\nbe more useful."}, {"title": "Reinforcement Learning for Adversarial Attacks", "content": "Reinforcement Learning (RL) has demonstrated success in\nsolving problems where classical machine learning often\nfalls short, with applications spanning robotics, healthcare,\ncontrols, energy, and medical imaging. However, its poten-\ntial in adversarial attacks remains underexplored. Sun et al.\n(Sun et al. 2020) applied RL to target graph neural net-\nworks through node injection, while Yang et al. (Yang et al.\n2020) (Patch Attack) utilized RL to attack CNN models by\noverlaying textured patches on input images, though their\nmethod struggled to minimize distortions.\nIn contrast, our RL agent employs a detailed state rep-\nresentation that captures the model's sensitivity to different\nimage regions and facilitates a patch-based attack process"}, {"title": "RLAB Platform", "content": "A Deep Neural Network (DNN) model under evaluation can\nbe expressed as $y = argmax f (x; \u03b8)$, where x represents\nthe input image, y represents the prediction, and 0 repre-\nsents the model parameters. A non-targeted black-box attack\nwithout access to the e generates a perturbation 6 such that,\ny = f(x + 6; 0). The distance between the original and ad-\nversarial sample, D(x, x + 6) will be any function of the lp\nnorms. The objective is to fool the classifier while keeping\nD to a minimum."}, {"title": "Bring Your Own Filters (BYOF)", "content": "The RLAB platform is highly versatile because the user can\nuse it with any type of distortion of their choice. The RL al-\ngorithm learns a policy to adapt to the filter used such that\nthe adversarial samples are generated with minimum distor-\ntion D. Further, the algorithm can be used with a mixture of\nfilters such that the agent first decides on which filter (Gaus-\nsian Noise, Gaussian Blur, brightness, etc.) to use for every\nstep and further decides on the number of patches to which\nthe filter needs to be added. We experimented with multiple\nfilters and have presented four naturally occurring distortion\nfilters as part of this paper. Figure 2 shows adversarial exam-\nples generated using different filters. Figure 3 shows adver-\nsarial examples generated with a mix of different distortion\nfilters."}, {"title": "How it works", "content": "In RLAB, the input image is divided into square patches of\nsize n\u00d7n and the sensitivity of the ground truth probability\nPGT, to the addition and removal of various types of dis-\ntortions is estimated for the patches. Using the sensitivity\ninformation, the RL agent performs two actions,\n\u2022 Adds distortions to selected patches.\n\u2022 Removes distortions from selected patches.\nThis process is repeated iteratively until the model mis-\nclassifies the image or the budget for the maximum allowed\nsteps is exhausted. Once an adversarial sample is success-\nfully generated (i.e., the model misclassifies), we apply an\niterative image cleanup as a post-processing step to further"}, {"title": "RL for Adversarial Attack", "content": "We designed a state space that balances sufficient visibility\nfor the RL agent with simplicity for efficient training. Sen-\nsitivity analysis is used to identify key patches in the input\nimage.\nSensitivity Analysis: Distortion filters (masks) of size\nn\u00d7n matching the square patches, are generated for sensi-\ntivity analysis. Each filter has fixed hyperparameters, such as\nnoise or brightness levels, throughout the experiment. Dur-\ning training and validation, the mask is applied across all\npatches to measure the change in the ground truth classi-\nfication probability PGT. The hyperparameters associated\nwith the distortion filters (noise intensity, amount of blur-\nring, etc.) are kept minimal to allow finer-grained distortion\nadditions in successive steps, aiding in controlling the Lp\nnorm. The distorted samples are constrained to the values\n[0, 1]d. In a multi-filter setting, where the RL agent has a\nchoice of filters, the hyperparameters for the individual fil-\nters were chosen such that the impact on Lp norm is the\nsame after adding any filter. Additional experiments were\nperformed for different types of patch sizes as represented\nin the table 5 to compare the effect of the patch sizes.\nState Vector: The state vector incorporates the results of\nthe image sensitivity analysis, ordered based on the drift in\nPGT for patches during addition (LISTADD) and removal\n(LISTREMOV E) of distortions. It also includes the classifi-\ncation probabilities for each class at each step (LISTP ROB)\nand the Lp norm, as illustrated in Figure 6.\nInexpensive Query on a GPU: The sensitivity analysis\ntakes just a few GPU cycles on a V100 GPU, as the operation\nis a fully parallelized one-shot filter operation on an image.\nAlso, multiple images can be processed in parallel on a V100\n32GB GPU based on image size. So, processing a query to\nget the states is a fast operation on a GPU. This makes the\nexecution very fast and efficient."}, {"title": "Action", "content": "At each step, the RL agent selects the number of patches\n(NADD DIST) from LISTADD to which distortion will"}, {"title": "Reward", "content": "We define a Probability Dilution (PD) metric to quantify\nthe shift in classification probability from the ground truth\nto other classes (untargetted) or, to a specific class (target-\ntted). The change in PD resulting from an action, denoted\nas (APD), measures the effectiveness of that action. Addi-\ntionally, the cost of an action is defined as the change in\nL2-distance (AL2), which represents the distortion intro-\nduced. The reward is then calculated as the normalized PD,\nas shown in equation 2.\n$PD_{untargeted} = \\frac{1}{log \\frac{1}{pg}} + \\frac{1}{log \\frac{1}{pk_1}} + ...+ \\frac{1}{log \\frac{1}{Fkn}}$\n$Rt = APD_{normalized} = APD/AL2$\nwhere ptgt represents the probability score of the target\nclass. The goal here is to choose the patch to which, when\nnoise is added, there is the highest increase in the target\nclass. Through hyperparameter tuning, we obtained a dis-\ncount factor y = 0.95, where y determines how much the\nRL agent cares about rewards in the distant future relative to\nthose at the current step."}, {"title": "RL Algorithm", "content": "We utilized a Dueling DQN algorithm-based Reinforce-\nment Learning (RL) agent for RLAB as the adversarial\nattack agent (Sewak 2019), which also evaluates the ro-\nbustness of CNN image classification models. The Dueling\nDQN model is well-suited to the discrete action space, ac-\ncommodating the limited possible values of NADD DIST\nand NREM DIST, while maintaining an appropriate level\nof complexity for effective prediction within a reasonably\nbounded training process. The overall training procedure for\nthe proposed approach is detailed in Algorithm 1."}, {"title": "Experimental Details", "content": "In this section, we discuss the effectiveness of our proposed\nmethod with the same experimental setup as our competi-\ntors. We evaluate two popular image classification datasets,\nILSVRC2012 (Russakovsky et al. 2015) and CIFAR-10.\nEighty percent of the original validation set was used to\ntrain the RL algorithm, and 20 percent was used for evalua-\ntion. We performed our attacks on three major Convolution-\nbased Neural Network architectures: ResNet-50, Inception-\nV3, and VGG-16 for both targeted and un-targeted attacks.\nWe used three metrics to evaluate the performance of our\napproach, Lp norm, which is a measure of distortion, the\naverage number of queries (AVG.Q) to make a model miss-\nclassify a correctly classified sample, and the average suc-\ncess rate (ASR).\nWith the values of pixels set to a range between 0 and 1\nand a maximum query budget of 3500, 1000 samples from\nthe ImageNet dataset on ResNet-50 architecture are evalu-ated. A failure case is when the proposed method could not\nfool the victim model within the given budget, and failure\ncases were not included in any of the metrics calculated ex-\ncept for the success rate. All experiments were performed\nfor a patch size of 2 \u00d7 2 and a Gaussian noise-based distor-\ntion filter, as we got the best results for this configuration.\nThe pipeline computation is executed on GPU and\nefficiently parallelized, batched, and scaled on GPUs.\nCaching techniques, such as noise masks, were used for\npre-computed information for improved efficiency. Apollo\nservers with 8 \u00d7 V100 32 GB GPUs were used for train-\ning and validation. We processed 16(images per GPU) x\n8(GPUs) = 128 images in a batch for the complete pipeline."}, {"title": "Results and Discussion", "content": "It is worth mentioning that the proposed robustness mea-\nsure in DeepFool [4] involves minimizing the amount of\ndistortion needed for misclassification, which is defined by\n\u2206(x;k) := min, ||r||2 subject to k (x + r)/= k(x), where\nmin || r|| 2 = min D and A(x; k) is the robustness of classi-\nk for input x. As we can see, this is consistent with our\ngoal, which is minimizing D."}, {"title": "Evaluation on ImageNet", "content": "filter operation on an image, the queries are very fast, inex-\npensive, and efficient on a GPU. In addition, multiple images\ncan be processed in parallel on a V100 32GB GPU based on\nthe image size.\nOur max value for Linf is marginally higher than the com-\npetitor's maximum budget for the ImageNet dataset. This\ncould be because of the way Linf is computed (returns\nthe maximum change from the original image). Perturba-\ntion that can affect the entire image by a very small value\nwill have a smaller Linf value but a higher L2. Our ap-\nproach focuses on exploring and attacking only vulnerable\nregions in the input image as represented in Figure 3 leading\nto marginally higher Linf."}, {"title": "Evaluation on CIFAR-10", "content": "Table 3 shows the performance of the proposed method\nagainst state-of-the-art attacks on the CIFAR-10 dataset. It\ncan be observed that the success rate of our proposed method\nis the same as that of the competitors, which is 100 percent,\nwhile the average queries of the proposed approach outper-\nform every state-of-the-art technique. Except for EigenBA\n(Zhou et al. 2022) and CG-Attack(Feng et al. 2022), which\nare close to our results, our approach beats the competitors\nby a large margin."}, {"title": "Evaluating Different Filters", "content": "Table 4 represents the performance of the proposed approach\nwith different filters. The table shows that the approach gen-\nerates the best results for Gaussian noise, followed by the"}, {"title": "Performance vs. Complexity", "content": "In our proposed work, we generated all our results with the\npatch of size 2 \u00d7 2 and Noise distortion filter for best re-\nsults. Even though the computation for sensitivity analysis\nprimarily depends on the size of the image and can be ac-\ncelerated, scaled, and batched on the GPU, there is a smaller\nvariation based on the number of patches due to the post-\nprocessing overhead, which is typically around 10% of the\ntotal computation for 224 \u00d7 224 images with 2 \u00d7 2 patch\nsizes. Depending on the use case, our approach allows the\nuse of different patch sizes at varying performance levels,\nrepresented in Table 5."}, {"title": "Ablation Study on Filter Parameters", "content": "Table 6 shows the ablation study for hyperparameters and\nthe highlighted values were chosen for the final results. Ex-\nperiments showed that higher noise levels increased the final\nL2 of the adversarial sample, while too little noise impacted\nthe average number of queries. We evaluated the impact of\ndifferent noise levels on the metrics as represented in table\n6. We applied the same noise level for evaluation on both"}, {"title": "Extensions of RLAB", "content": "Enhancing Robustness: Retraining the victim model\nwith adversarial samples from RLAB significantly improved\nrobustness when evaluated on benchmark datasets such as\nCIFAR-10-C with the metrics of adversarial error and mean\ncorruption error (mCE). To study the effectiveness of the re-\ntrained model in successfully defending itself against other\nattacks, we evaluated the retrained model with adversarial\nsamples generated by other attack methods (adversarial er-\nror).\nVisual Explanation: RLAB can also be extended to vi-\nsual explanations for image classification models."}, {"title": "Signals and Video", "content": "Signals and Video: RLAB is effective as an adversarial\nattack tool to address robustness beyond image classifiers\nto models classifying signals, 3D images, and multi-modal\nsatellite images."}, {"title": "Conclusion", "content": "The use of RL strengthens the RLAB platform, which can\noptimize black-box adversarial attacks with different types\nof distortion filters or a mix of them. With the \"Bring Your\nOwn Filters - BYOF\" approach, the RLAB platform sup-\nports any new types of distortion relevant to specific real-\nlife use cases. This also helps assess the robustness against\nthe most pertinent non-malicious adversarial perturbations.\nCompared to the hand-crafted heuristics of most state-of-\nthe-art adversarial attacks, this approach expands the scope\nand applicability to new distortion filters. As part of future\nwork, we are expanding the scope of RLAB for adversarial\nattack-based evaluation of other elements of trustworthiness\nlike bias and fairness of both computer vision applications\nand natural language processing."}]}