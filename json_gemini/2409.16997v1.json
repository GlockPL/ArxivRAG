{"title": "INT-FLASHATTENTION: ENABLING FLASH ATTENTION FOR INT8 QUANTIZATION", "authors": ["Shimao Chen", "Zirui Liu", "Zhiying Wu", "Ce Zheng", "Peizhuang Cong", "Zihan Jiang", "Lei Su", "Tong Yang"], "abstract": "As the foundation of large language models (LLMs), self-attention module faces the challenge of quadratic time and memory complexity with respect to sequence length. FlashAttention accelerates attention computation and reduces its memory usage by leveraging the GPU memory hierarchy. A promising research direction is to integrate FlashAttention with quantization methods. This paper introduces INT-FlashAttention, the first INT8 quantization architecture compatible with the forward workflow of FlashAttention, which significantly improves the inference speed of FlashAttention on Ampere GPUs. We implement our INT-FlashAttention prototype with fully INT8 activations and general matrix-multiplication (GEMM) kernels, making it the first attention operator with fully INT8 input. As a general token-level post-training quantization framework, INT-FlashAttention is also compatible with other data formats like INT4, etc. Experimental results show INT-FlashAttention achieves 72% faster inference speed and 82% smaller quantization error compared to standard FlashAttention with FP16 and FP8 data format. All related codes are open-sourced 1.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs), such as GPT Achiam et al. (2023) and Llama Touvron et al. (2023), have achieved significant breakthroughs across various domains. Self-attention module is the foundation of LLMs, allowing them to capture dependencies between different tokens in a sequence Vaswani et al. (2017). However, the computation of self-attention module entails quadratic time and memory complexity with respect to the sequence length, which hinders the scaling of LLMs to longer contexts. To address this bottleneck, Dao et al. propose excellent FlashAttention Dao et al. (2022), which exploits the GPU memory hierarchy to design a tiling strategy to speed up the attention computation and reduce memory from quadratic to linear w.r.t. sequence length.\nAnother popular method of improving the computational performance and memory usage of LLMs is to reduce the bit size of the floating-point data used in these models, which is a technique known as model quantization. Modern hardware incorporates advanced Tensor Cores to support efficient general matrix-multiplication (GEMM) for FP16, FP8 Micikevicius et al. (2022), and INT8 van Baalen et al. (2023). Quantization methods make full use of these dedicated computing units to improve the spatiotemporal efficiency of LLMs by compressing the parameters and activations into FP8, INT8, and even ternary format. Existing quantization methods can be broadly divided into two categories: quantization during training and post-training quantization. This paper focuses on the latter, utilizing quantization to enhance LLM inference efficiency.\nIntegrating FlashAttention with quantization methods is a promising research direction. Designed for the most advanced NVIDIA Hopper GPUs, the latest FlashAttention-3 Shah et al. (2024) already supports FP8 data format. Unfortunately, NVIDIA Ampere series GPUs, such as A100, do not pro- vide hardware support for FP8. While Hopper architecture significantly enhances both computa- tional power and energy efficiency, existing Ampere architecture still holds a substantial share in the data center GPU market. According to available data, A100 GPUs are still estimated to contribute"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 STANDARD ATTENTION\nGiven input sequence $Q, K, V \\in \\mathbb{R}^{N\\times d}$, where N is the sequence length and d is the head dimension, standard attention computes the attention output $O \\in \\mathbb{R}^{N\\times d}$.\n$S = QK^T \\in \\mathbb{R}^{N \\times N}$, $P = softmax(S) \\in \\mathbb{R}^{N \\times N}$, $O = PV \\in \\mathbb{R}^{N \\times d}$,\nwhere softmax is applied row-wise. Here, we call S attention score matrix and call Pattention weight matrix.\nStandard attention implements the intermediate matrices S and P to HBM, which takes $O(N^2)$ memory. During forward computation, standard attention implementation first loads the entire inputs from GPU high bandwidth memory (HBM), calculates $S = QK^T$ and writes S back to HBM. Then it loads S from HBM, calculates $P = softmax(S)$, and writes P back to HBM. It finally calculates O = PV to get the final results. We can see that in the circumstance above, most of the operations are bounded by the HBM bandwidth, and thus the large number of memory accesses dominates the time taken by attention operations.\n2.2 FLASHATTENTION FAMILIES\nTo speed up attention on hardware accelerators with hierarchical memory, FlashAttention Dao (2023) proposes to use the tiling techniques to reduce memory reads/writes and fuse the attention operations into a single kernel. Specifically, FlashAttention divides the input sequences into smaller blocks. During the forward computation, it first loads blocks of inputs from HBM to SRAM, com- putes attention for each block and then updates the output without writing the large intermediate matrices S and P back to HBM. Since the softmax function in the attention mechanism needs to be applied to the entire row, FlashAttention employs an online softmax method Milakov & Gimelshein (2018); Rabe & Staats (2021) to split the computation into blocks and finally rescale the output"}, {"title": "2.3 MODEL QUANTIZATION", "content": "LLM quantization. Recent years have witnessed a great surge in applying various quantization methods to reduce the memory and energy consumption of LLMs. Quantization methods compress the parameters from standard FP32 to continuous FP16 Dao et al. (2022), FP8 Shah et al. (2024); Lee et al. (2024), discrete INT8 Dettmers et al. (2022), and even ternary formats Chen et al. (2024). Many studies suggest that INT8 consumes significantly less memory for loading model weights and requires less energy than the FP8 and FP16 counterparts Dally (2015); van Baalen et al. (2023). The naive LLM quantization approaches adopt tensor-level quantization Zhou et al. (2024). How- ever, many studies found that the weight distributions vary significantly across different tokens, and the existence of activation outliers makes LLM difficult to quantize at whole-tensor level Tao et al. (2022); Xu et al. (2024). As a result, many works adopted token-level or block-level quan- tization methods to improve model accuracy Zhou et al. (2024); Li et al. (2024). The most recent FlashAttention-3 Shah et al. (2024) adopts a block-level FP8 quantization method. However, to the best of our knowledge, there is no existing work that integrates token-level quantization with FlashAttention, neither is there a version of FlashAttention that supports INT8 data format.\nPost-training quantization. There is a line of post-training quantization (PTQ) strategies for effec- tively reducing model sizes and improving the inference speed of LLMs. Some work design custom quantization blocks to improve quantization accuracy Dettmers et al. (2022); Li et al. (2021). Other works use feature segmentation strategies to protect the outlier features so as to reduce the quanti- zation error Shang et al. (2023); Dettmers et al. (2023). GPTQ Frantar et al. (2022) designs a more accurate one-shot quantization framework based on approximate second-order information Frantar & Alistarh (2022), achieving good accuracy in extreme low-bit quantization regime (2-bit). Subse- quent works propose to identify and select salient weights and preserve their information through scaling transformations Lee et al. (2023) or residual approximation Huang et al. (2024)."}, {"title": "3 THE INT-FLASHATTENTION ARCHITECTURE", "content": "Developed based on FlashAttention Dao et al. (2022), INT-FlashAttention implements the Q, K, and V matrices in fully INT8 format (colored green in Figure 1) with token-level quantization. We use the INT8 general matrix multiplication (GEMM) kernel to replace all matrix multiplications in FlashAttention (originally in FP16/FP8 format). Due to the efficient INT8 compression, INT- FlashAttention can read larger blocks from HBM per iteration compared to basic FlashAttention with FP16 format. Additionally, the INT8 matrix multiplication operators in INT-FlashAttention also outperform their floating-point counterparts. As a result, INT-FlashAttention achieves no- table improvement in inference speed compared to basic FlashAttention. Furthermore, with per- token quantization, INT-FlashAttention offers better inference accuracy than the FP8 version of FlashAttention-3 Shah et al. (2024) with tensor-level quantization.\nIn this section, we first introduce the workflow of the online softmax operation in Section 3.1, which forms the foundation of both FlashAttention and INT-FlashAttention. Then we explain how INT-FlashAttention seamlessly integrates INT8 quantization into the online softmax workflow in Section 3.2."}, {"title": "3.1 PRELIMINARY OF ONLINE SOFTMAX", "content": "We first describe the workflow of the online softmax method Milakov & Gimelshein (2018); Rabe & Staats (2021) in detail, which is the base of FlashAttention Dao et al. (2022) and our INT- FlashAttention. As shown in Figure 1 and Algorithm 1, in the forward workflow of FlashAt- tention (also our INT-FlashAttention), we iterate over blocks of the input Q and K/V matrices. For each iteration in the outer loop (line 4-5 in Algorithm 1), we load a block $Q_i$ from HBM,"}, {"title": "3.2 INT-FLASHATTENTION ATTENTION DESIGN", "content": "INT-FlashAttention implements the Q, K, and V matrices in self-attention module with fully INT8 format. In Figure 1, we use green color to represent the data in INT8 format and the INT8 GEMM operations and use orange color to represent data and operations with FP32 format. The GEMM used in INT-FlashAttention takes two INT8 input matrices and produces an INT32 output matrix. As shown in Figure 1 and Algorithm 1, during inference workflow, all data stored in HBM (Q, K, and V) and all matrix multiplication operations ($QK^T$ and $PV$) are implemented with INT8 format.\nWe maintain two vector scalars $S_Q, S_K \\in \\mathbb{R}^N$, for token-level quantization of the Q and K ma- trices. For the V matrix, we maintain a constant scalar $S_V \\in \\mathbb{R}$ for tensor-level quantization\u00b2. After training, we perform linear symmetric quantization on the Q, K, and V matrices, quantiz- ing them into INT8 format and obtaining scalars $S_Q = \\frac{R}{rowmax(|Q|)}$, $S_K = \\frac{R}{rowmax(|K|)}$, and $S_V = \\frac{R}{max(|V|)}$, where $R = 256$ is the INT8 quantization range."}, {"title": "4 EMPIRICAL EVALUATION", "content": "We evaluate the inference speed and quantization accuracy of INT-FlashAttention, and compare it to FlashAttention with FP16 Dao (2023) and FP8 Shah et al. (2024) data format. We also compare"}, {"title": "4.1 INFERENCE SPEED", "content": "We implement INT-FlashAttention and the other candidate solutions in Triton, where we fix their batch size, number of heads, and dimension size per head to be the same. We evaluate the inference time with different context length. As shown in Figure 2, compared to standard FlashAttention with FP16 data format, INT-FlashAttention achieve 31%, 52%, 66%, 72%, and 73% smaller inference time under the sequence length of 1k, 2k, 4k, 8k, and 16k, respectively. We also notice that the in- ference time gap between INT-FlashAttention and FlashAttention-FP16 becomes larger as sequence length increases. On the other hand, INT-FlashAttention has nearly the same inference speed than FlashAttention with FP8 data format, with the gap narrowing as sequence length increases."}, {"title": "4.2 QUANTIZATION ACCURACY", "content": "We compare the quantization accuracy of INT-FlashAttention with the other candidate solutions. We manually create a one-layer self-attention module with the activations in its Q, K, and V ma- trices following normal distribution N(0, 1) or uniform distribution U(-0.5, 0.5). We evaluate model quantization accuracy by measuring the Mean Relative Error (MRE) between original acti- vations and activations after quantization and subsequent restoration. As shown in Table 1, under normal-distributed activations, INT-FlashAttention achieves up to 1.8\u00d7 smaller MRE compared to FlashAttention with FP8 data format. As shown in Table 2, under uniform-distributed activations, INT-FlashAttention achieves up to 5.6\u00d7 smaller MRE compared to FlashAttention with FP8 data format."}, {"title": "5 CONCLUSION, LIMITATIONS, AND FUTURE WORK", "content": "This paper proposes INT-FlashAttention, the first token-level INT8 post-training quantization ar- chitecture compatible with FlashAttention forward workflow. We implement a INT-FlashAttention prototype with fully INT8 activations and GEMMs, which significantly improves the inference speed of FlashAttention on Ampere GPUs. The limitation of this work is that the V matrix currently is only implemented with tensor-level quantization. It remains a significant challenge to quantize V on a per-token basis. In future work, we will use per-block quantization to optimize the implementation of the V matrix. We also plan to combine our INT-FlashAttention with Hadamard transformations to further accelerate the inference process while maintaining high accuracy of the weight coefficients."}]}