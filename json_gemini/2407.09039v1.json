{"title": "Overcoming Catastrophic Forgetting in Tabular Data Classification: A Pseudorehearsal-based approach", "authors": ["Pablo Garc\u00eda-Santaclara", "Bruno Fern\u00e1ndez-Castro", "Rebeca P. D\u00edaz-Redondo"], "abstract": "Continual learning (CL) poses the important challenge of adapting to evolving data distributions without forgetting previously acquired knowledge while consolidating new knowledge. In this paper, we introduce a new methodology, coined as Tabular-data Rehearsal-based Incremental Lifelong Learning framework (TRIL3), designed to address the phenomenon of catastrophic forgetting in tabular data classification problems. TRIL3 uses the prototype-based incremental generative model XuILVQ to generate synthetic data to preserve old knowledge and the DNDF algorithm, which was modified to run in an incremental way, to learn classification tasks for tabular data, without storing old samples. After different tests to obtain the adequate percentage of synthetic data and to compare TRIL3 with other CL available proposals, we can conclude that the performance of TRIL3 outstands other options in the literature using only 50% of synthetic data.", "sections": [{"title": "I. INTRODUCTION", "content": "Continual learning (CL) [De Lange et al.(2021)], [Wang et al.(2024)], also known as lifelong learning [Parisi et al.(2019)], is an artificial intelligence approach that focuses on the ability of models to adapt and improve over time as they incrementally learn while processing dynamic data-streams. The underlying philosophy is using batches of data, a batch may be even just one sample, taken from a data-stream to train the system: each batch is used only once. This means that it is not possible to access previously processed data and, therefore, this entails a radical change compared to the classical pipeline of training, validating, and testing in ML.\nTherefore, CL is highly recommendable when facing scenarios where the model needs to adapt quickly to new data or when the model needs to be personalized. However, there is an important challenge in CL due to its nature: since models easily adapt to new knowledge, they tend to forget past knowledge. This effect, known as catastrophic forgetting [French(1999)], [Kirkpatrick et al.(2017)], entails models to reduce their performance when acquiring new knowledge, which impacts their usefulness. This is especially severe in class-incremental learning, when it is expected the model is able to differentiate among a set of classes. When new classes appear, the model learns to identify them but this new knowledge alters the decision boundaries of the previous classes, causing the model to deteriorate.\nAlthough there are multiple approaches to mitigate catastrophic forgetting, rehearsal methods [Robins(1995)] are one of the most widely used. They are based on combining new samples with representative samples of past knowledge, which can be obtained by keeping old samples or by employing generative models to map statistical distributions of the training data. Both, (i) the old or generated synthetic samples and (ii) new and unseen data are combined to train CL models. Despite of its advantages, rehearsal methods do not solve all drawbacks of catastrophic forgetting [Verwimp et al.(2021)]. On the one hand, these methods are usually implemented through deep learning networks that reliably reproduce past data distributions. However, they do not work in an incremental way, so they are ineffective when dealing with non-stationary streams or concept-drift [Lu et al.(2018)] scenarios. On the other hand, the majority of the state-of-the-art approaches are designed to deal with image classification problems [Mai et al.(2022)], [Qu et al.(2021)], but there are scarce contributions focused on other types of data, such as tabular or time series.\nOur proposal, coined as TRIL3 (Tabular-data Rehearsal Incremental LifeLong Learning method), precisely deals with these two previously mentioned aspects offering a incremental-based rehearsal method that dynamically adapts to non-stationary tabular data-streams that is highly robust to the catastrophic forgetting phenomenon. Instead of using neural networks to recreate or keep past knowledge, we apply an incremental learning model based on prototypes, XuILVQ [Gonz\u00e1lez Soto et al.(2022)], [Gonz\u00e1lez-Soto et al.(2024)], that can be trained online. XuILVQ generates previously learned examples that are replayed to feed a Deep Neural Decision Forest [Kontschieder et al.(2015)] algorithm, in charge of learning classification tasks from tabular data.\nOur approach represents a step forward in CL applied to fields different from image recognition and it demonstrates strong performance on a variety of real-world datasets, effectively handling both large sample sizes and numerous features. In fact, it outperforms in most datasets other techniques designed to mitigate forgetting and the traditional offline training method."}, {"title": "II. RELATED WORK", "content": "In this section, we first summarize the main state-of-the-art lifelong learning techniques that have addressed one of the main limitations in lifelong learning systems: catastrophic forgetting. Ideally, the main objective in lifelong learning systems is to maintain the old knowledge (capability to perform previously acquired tasks) while new knowledge is learned. To overcome this problem, different philosophies are detailed in Section II-A). After that, in Section II-B), we focus on the most relevant proposals in the literature in the field of rehearsal techniques, since our methodology is precisely aligned with this idea."}, {"title": "A. Dealing with catastrophic forgetting in CL", "content": "Approaches to address the problem of catastrophic forgetting in connectionist models [French(1999)], [Kirkpatrick et al.(2017)], such as neural networks, are usually classified according to the following three strategies: dynamic architectures [Li et al.(2019)], regularization techniques [Chen et al.(2021)] and rehearsal mechanisms [Robins(1995)], [Atkinson et al.(2021)], which are detailed as follows.\n1) Dynamic architecture techniques:\nThese techniques [Li et al.(2019)] propose to expand the neural network by adding new layers in order to have different components or subnetworks for each task that the model needs to learn. These additions can be done according to different approaches: [Lesort et al.(2020)]\n\u2022 Explicit Dynamic Architecture, where any method might add, clone, or store parts of model parameters.\n\u2022 Implicit Architecture Modification, where the model is adapted without explicitly altering its architecture. Thus, some learning units might be deactivated or the forward-pass route might be changed.\n\u2022 Dual Approach, where the neural network is divided into two models: one to learn the current task, easily adaptable, and another to keep memories of past experiences.\n2) Regularization techniques:\nRegularization-based approaches [Chen et al.(2021)] attempt to address catastrophic forgetting by imposing constraints on the updates of neural networks. These approaches are often inspired by theoretical models from neuroscience suggesting that consolidated knowledge can be shielded from forgetting through synapses with a cascade of states producing different levels of plasticity. From a computational perspective, this is generally modeled through additional regularization terms added to the cost function that penalize changes in the mapping of a neural network during training. Regularization approaches offer a means to prevent catastrophic forgetting under certain circumstances. However, they involve additional loss terms to preserve previously acquired knowledge that can reduce the performance of both old and new tasks when neural resources are limited, In general, while they are successful in mitigating catastrophic forgetting, they have not achieved adequate results in complex contexts or datasets [Mai et al.(2022)].\n3) Rehearsal techniques:\nRehearsal techniques [Atkinson et al.(2021)] involve replaying old data alongside the new data to prevent the model from forgetting previously learned knowledge while simultaneously learning new one. Their effectiveness usually diminishes with small buffer sizes, and they are not applicable in scenarios with data privacy concerns. It can be mainly divided into two approaches:\n\u2022 Pure Rehearsal: also known as replay, raw samples are stored as a memory of past tasks at a non-negligible memory cost.\n\u2022 Generative Replay, also known as pseudo-rehearsal, instead of using a memory buffer to store samples for replay, a generative model is trained to artificially generate data from distributions of previous tasks. A major drawback of these approaches is that generative models must be continuously updated when processing non-stationary data."}, {"title": "B. Rehearsal methods in the literature", "content": "Rehearsal techniques typically achieve the best performance in the current state-of-the-art [Masana et al.(2022)] and they can be classified into two categories: (i) replay or buffer-based rehearsal, and (ii) generative or pseudorehearsal.\nBuffer-based rehearsal typically involves storing a subset of old data samples in a memory buffer, which are then interleaved with new data during training [Lopez-Paz and Ranzato(2017)], [Rebuffi et al.(2017)], [Chaudhry et al.(2019)]. By periodically revisiting these old samples, the model can reinforce its understanding of past tasks, thereby reducing the risk of catastrophic forgetting. This method is relatively simple and computationally efficient. However, there are two concerns while using this approach. The first is storage, specially in situations where there is not much storage space, such as in edge computing contexts or IoT settings. In these scenarios, it is sometimes not possible to store a large number of samples that are not known to what extent they will scale. The other is security, since in many cases privacy policies make it impossible to keep a memory of old data.\nPseudo-rehearsal techniques use generative models to produce synthetic data that resemble the distribution of the original training data [Shin et al.(2017)], [Seff et al.(2017)], [Van de Ven et al.(2020)]. Although this method does not require storing large amounts of real data, it relies heavily on the quality of the generative model to accurately capture the underlying data distribution."}, {"title": "III. BACKGROUND", "content": "In this section, we introduce the two algorithms used in our proposal (TRIL3). First, the incremental prototype-based algorithm XuILVQ [Gonz\u00e1lez Soto et al.(2022)] (Section III-A), is used to generate synthetic data to preserve old knowledge. This model generates samples from past experiences while continuously learning to map the probability distribution of unseen data over time. Second, the Deep Neural Decision Forests (DNDF) algorithm [Kontschieder et al.(2015)] (Section III-B), which combines Decision Forests with Deep Neural Networks (DNN) to learn classification tasks from tabular data."}, {"title": "A. XuILVQ", "content": "XuILVQ [Gonz\u00e1lez Soto et al.(2022)], a prototype-based algorithm, is an implementation of the Incremental Learning Vector Quantization (ILVQ) algorithm [Xu et al.(2012)] optimized for Internet of Things (IoT) scenarios.\nPrototypes are explicit representations of observations and the set of prototypes generated by the model should effectively encapsulate the essence of the entire dataset or global knowledge. XuILVQ works by incrementally updating the class prototypes according to the received data. This involves adjusting the weight vectors based on the distance between the input data and the closest prototypes. The prototypes are stored in memory, which allows the algorithm to dynamically adapt to changes in data streams without requiring historical data storage.\nThe algorithm works as follows: each received sample is analyzed to determine the closest prototype and the second closest prototype. If the sample either belongs to a new class or is significantly distant from both closest prototypes, it is added as a new prototype. In other cases, prototypes are updated to be grouped or ungrouped according to the class they belong to. Moreover, XuILVQ incorporates a forgetting mechanism to gradually diminish the impact of outdated data. This mechanism entails removing old isolated samples or points within low-density classes, thereby ensuring the model effectively adapts to the evolving data distribution.\nThis algorithm has been included in the TRIL3 framework with the aim of mapping relevant training samples and generating prototypes as adequate representations of the underlying patterns in the data over time. These prototypes are used in combination with new data during incremental model updates, avoiding catastrophic forgetting.\nThe main advantages of XuILVQ include its adaptive capability to dynamically adjust to changes in data patterns. Unlike other solutions, XuILVQ does not require storing old data, thus eliminating storage management issues and the need for additional resources. It offers an efficient IoT solution suitable for edge devices with limited resources. Additionally, its incremental nature facilitates online training."}, {"title": "B. Deep Neural Decision Forests", "content": "Initially proposed by Kontschieder et al. [Kontschieder et al.(2015)], Deep Neural Decision Forests are the other significant component of the TRIL3 framework. DNDF proposes a solution that combines decision forests with Deep Neural Networks (DNN).\nA neural decision tree is a tree structure consisting of decision and leaf nodes (also known as terminal or prediction nodes). Leaf nodes have an associated probability function which represents the probability distribution of the classes reaching each leaf. These functions are the first set of learnable weights that each tree uses to predict its output.\nDecision nodes are assigned a decision function that is responsible for routing samples along the tree. Every sample in a decision node is routed to the right or left subtree based on the output of the stochastic decision function.\nBoth probability and decision functions are differentiable, so optimal weights are obtained during model training following a Stochastic Gradient Descent (SGD) approach.\nNeural Decision Forests consist of an ensemble of neural decision trees. Predictions for every sample are obtained by averaging the output of each tree within the ensemble.\nTherefore, DNDF are appropriate for solving incremental learning classification tasks due to the following key aspects:\n\u2022 Problem scope: DNDF are suitable for tabular data, which fits the scope of our scenario. Although this technique is particularly effective for tabular data, it should be noted that it can also be replaced by other differentiable models suitable for this type of data.\n\u2022 Performance: DNDF outperforms conventional Decision Forests (DF) by improving their performance. By incorporating stochastic and differentiable decision functions, DNDF improves the capabilities of traditional DF.\n\u2022 Partially updatable: DNDFs are inherently differentiable, which allows them to be partially updated using gradient-based optimization techniques. By taking advantage of gradient updates, DNDFs can refine their decision rules and feature representations iteratively."}, {"title": "IV. TRIL3: METHODOLOGY OVERVIEW", "content": "TRIL3 (Tabular-data Rehearsal Incremental LifeLong Learning) is a novel framework for continual classification of tabular data based on a pseudo-rehearsal strategy. It is mainly composed of two algorithms already detailed in Section III: (i) the XuILVQ model, used to generate prototypes as suitable representations of past knowledge, (ii) a DNDF algorithm that continually learns how to perform classification tasks. Therefore, the proposed TRIL3 framework would effectively adapt to changing data distributions, facilitating continuous learning and being robust against forgetting."}, {"title": "V. VALIDATION AND RESULTS", "content": "In order to validate our proposal and compare it with other state-of-the-art approaches, we have proceeded as follows. First, we have selected representative real-world datasets related to multi-class tabular data classification problems as our data source. Then, we have designed a set of tests organized into three groups: (i) evaluate the TRIL3 performance when increasing the percentage of synthetic data to rehears the model; (ii) compare the TRIL3 performance to a state-of-the-art rehearsal-based algorithm; and (iii) compare TRIL3 performance to performance of the classical offline DNDF algorithm.\nIn this section, we first detail the implementation decisions in Section V-A, and then we describe the selected datasets in Section V-B. After that, the detailed tests and their results are described in Section V-C, and a discussion is included in Section V-D."}, {"title": "A. Implementation details", "content": "As detailed in Section IV, TRIL3 uses two algorithms: DNDF to perform the tabular data classification tasks and XuILVQ to generate samples from past knowledge and rehearse the TRIL3 model. For the former, DNDF, we have used the implementation available at Keras [Keras(2022)] as our base, although we have adapted this code to allow the model incremental retraining with one epoch of stochastic gradient descent on a given input data batch. For the latter, XuILVQ, we have used the code detailed in [Gonz\u00e1lez Soto et al.(2022)]. Additionally, to perform the online standardization, we have used the StandardScaler from the library River [River(2024)]. All the additional code was developed in Python.\nIn order to compare the performance or TRIL3 to conventional machine learning approaches, we have used the DNDF model, a batch learning-based classification model, since the DNDF model is the one we have used to perform classification tasks in TRIL3. Instead of incrementally training the model with partial updates from incoming batches as described in Section IV), the original DNDF is fitted after processing all the training data at once. As in TRIL3, the implementation used was the one available at the Keras website [Keras(2022)], but, for this experiment, no modifications in the available code were required.\nFinally, and with the aim of comparing our framework with other state-of-the-art lifelong learning solutions, we have used the solutions available at the Avalanche library [Carta et al.(2023)]. Avalanche, developed by ContinualAI\u00b9, is a PyTorch-based end-to-end CL library for rapid prototyping, training, and reproducible evaluation of lifelong learning algorithms. Thus, Avalanche provides several pre-built models with different strategies to mitigate catastrophic forgetting. We have selected the Replay method [Bagus and Gepperth(2021)], since it also uses a rehearsal approach. The replay strategy keeps a buffer"}, {"title": "D. Discussion", "content": "As Table IV shows, TRIL3 with 50% synthetic data consistently shows outstanding results in all analyzed datasets. For example, the CICIDS-2017 Friday dataset achieves an almost perfect F1-score (0.98-0.99) both before and during the forgetting phase for both classes. Moreover, in the diabetes dataset, it stands out especially for class 0 before the forgetting phase (0.48) and improves performance during this phase (0.52). In the credit card dataset, TRIL3 with 75% and 50% synthetic data shows the best results for classes 0 and 1 respectively, evidencing its ability to handle different data distributions effectively. Finally, in the wine dataset, TRIL3 with 50% real data shows exceptional performance both before and during the forgetting phase. On the other hand, the model performance notably drops with 100% of real data within the training batch, that is, when the DNDF classification model has no support to retain past knowledge during the forgetting phase.\nCompared to the MLP method with replay implemented using Avalanche, TRIL3 shows several advantages in the analyzed datasets. In the case of the CICIDS-2017 Friday dataset, there was a tie between TRIL3 with 50% synthetic data, replay, and offline training, achieving in all cases a near-perfect F1 score for both classes. On the diabetes and credit card datasets, TRIL3 results are generally better than Replay. Finally, in the wine dataset, the smallest one, MLP with replay performs better. Therefore, in comparison with the MLP with the replay method, TRIL3 obtains equal or better results in all the datasets except for the wine dataset.\nIn terms of performance compared to traditional offline training, TRIL3 also shows promising results. In all studied datasets, TRIL3 manages to approach or improve the performance of batch offline training. On Friday CICIDS-2017 dataset, both achieve equal maximum performance. In diabetes and credit card datasets, where offline performance drops for certain classes, TRIL3 demonstrates better capability except for class 0 of the diabetes dataset. Finally, in the wine dataset, TRIL3 performs worse than offline training.\nOverall, TRIL3 with 50% synthetic data is the best performer in most datasets, matching or even outperforming batch training, followed by MLP with replay. In the case of naive online training, as expected, there is a generalized drop in performance during the forgetting phase."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we introduce a new methodology, coined as TRIL3. This novel continual learning system is designed to address the problem of catastrophic forgetting in the online classification of tabular data. Our approach integrates the prototype-based incremental generative model XuILVQ to generate synthetic data to preserve old knowledge and the DNDF algorithm, which was modified to run in an incremental way, to learn classification tasks for tabular data. This proposal, based on a rehearsal strategy, does not require the storage of old samples and it is able to adapt to non-stationary data streams, being robust against catastrophic forgetting.\nWe have assessed TRIL3 comparing its performance to other solutions in the literature using four representative real-world datasets, available online, for multi-class tabular data classification problems. Using this data, we have compared TRIL3 with another CL approach based also on a rehearsal philosophy (Replay) that is available at the Avalanche library and we have also compared TRIL3 with the classical DNDF model. In both cases, the performance of TRIL3 generally outstands the other options. Besides, and after analyzing which percentage of synthetic data would be the most adequate to rehearse the model, the results indicate that 50% is the best option. Thus, TRIL3 not only offers good results according to the system performance but it also offers a good solution against catastrophic forgetting.\nIn future work, we plan to extend our framework to address classification problems involving multiclass target variables. Although promising results have been obtained using synthetic data sets, it is crucial to identify high-quality real data sets for further validation. In addition, exploring the adaptation of this framework for use with time series data is an interesting possibility. This adaptation could offer advantages for industrial applications."}]}