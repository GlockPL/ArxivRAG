{"title": "GPT-4'S ASSESSMENT OF ITS PERFORMANCE IN A\nUSMLE-BASED CASE STUDY", "authors": ["Uttam Dhakal", "Aniket Kumar Singh", "Suman Devkota", "Yogesh Sapkota", "Bishal Lamichhane", "Suprinsa Paudyal", "Chandra Dhakal"], "abstract": "This study investigates GPT-4's assessment of its performance in healthcare applications. A simple\nprompting technique was used to prompt the LLM with questions taken from the United States\nMedical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence\nscore before posing the question and after asking the question. The questionnaire was categorized into\ntwo groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The\nmodel was asked to provide absolute and relative confidence scores before and after each question.\nThe experimental findings were analyzed using statistical tools to study the variability of confidence\nin WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance\nvariation for the WF and NF groups. Results indicate that feedback influences relative confidence but\ndoesn't consistently increase or decrease it. Understanding the performance of LLM is paramount in\nexploring its utility in sensitive areas like healthcare. This study contributes to the ongoing discourse\non the reliability of AI, particularly of LLMs like GPT-4, within healthcare, offering insights into how\nfeedback mechanisms might be optimized to enhance AI-assisted medical education and decision\nsupport.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive performance in the various sectors[1]. With advanced\ntuning capabilities incorporated into models such as GPT-4 and GPT-3.5, they surpass human capabilities in areas\nlike programming, policy making, and solving complex problems[2]. LLMs contribute significantly to Artificial\nIntelligence (AI) by understanding natural language and producing extensively accurate natural language with minimal\nhuman intervention[3]. They find applications in various AI contexts, including document management[4], chatbots[5],\ntext-based games[6], law[7], human resources[8], knowledge extraction[9], market research[10], and healthcare[11].\nLLMs exhibit considerable strength and versatility for various applications, so a comprehensive understanding of their\noperation becomes important. It is paramount to understand the operation of these models. In critical domains such as\nhealthcare, policy-making, and law enforcement, where minimizing risk is imperative due to the potential for significant\nlosses from erroneous decisions, it is crucial to gain insight into the functionality of these models.\nOur previous efforts explore the potential of conducting such studies and underscore their importance [12]. Especially\nin the health domain, understanding how these models operate and their cognitive abilities is crucial. With the current\nstate-of-the-art LLMs, they can diagnose a disease based on symptoms and patient information [13]. Our study aims to\nexplore and analyze the intricate interactions within the current capabilities of LLM, specifically focusing on GPT-4"}, {"title": "2 Related works", "content": "The growing use of LLMs in the healthcare domain has piqued the interest of researchers worldwide, leading to\nnumerous published studies. A plethora of noteworthy research studies, spanning from AI-generated medical advice\n[14], biomedical text generation [15], language models for radiology [16], and models for health care services [17].\nNumerous studies investigating the integration of ChatGPT and the USMLE have been conducted. The study by Brin\net al. [18] compared the performance of ChatGPT and GPT-4 in the context of USMLE soft skill assessments. The\nresults revealed that GPT-4 exhibited superior performance by correctly answering 90% of USMLE soft skills questions\ncompared to ChatGPT's 62.5%. A similar study by Nori et al. [19] not only compared ChatGPT and GPT-4 but also\ncompared another model called Flan-PaLM and concluded that GPT-4 significantly outperformed other models. The\nstudy's findings conclusively demonstrated that GPT-4 exhibited significant superiority over the other models under\nconsideration. Patel et al. [20] evaluated the impact of different prompt engineering strategies on ChatGPT in addressing\nmedical problems, specifically focusing on medical calculations and clinical scenarios."}, {"title": "2.1 Cognitive Abilities of LLMs", "content": "Studies have shown that LLMs possess capabilities that align with certain cognitive abilities traditionally associated\nwith humans. These include chain-of-thought reasoning and aspects of the theory of mind, allowing them to generate\ncoherent and contextually appropriate text outputs. Their proficiency in tasks requiring abstract knowledge and reasoning\nhas characterized them as \u201cthinking machines\u201d and raised questions about their potential cognitive capacities, which\nmay extend to goal direction, agency, and executive [3][21].\nThe GPT series, as an example of LLMs, are trained on vast text corpora with the primary objective of predicting the\nnext or missing words in sentences. This training has evolved to include additional objectives to enhance their language\nprocessing capabilities [22]. Emerging studies suggest that LLMs may exhibit cognitive abilities, but systematic\nevaluation is necessary to substantiate these claims. This involves multiple tasks, control conditions, iterations, and\nstatistical robustness tests. Moreover, when fine-tuned with data from psychological experiments, LLMs have mirrored\nhuman behavior and, in some cases, outperformed traditional cognitive models, particularly in decision-making domains\n[23]."}, {"title": "2.2 AI Ethics in Healthcare", "content": "The integration of AI into healthcare brings forth significant ethical challenges. Informed consent, safety and transparency,\nalgorithmic fairness and biases, and data privacy are among the primary concerns that require thorough examination.\nAs AI reshapes clinicians' roles and practices, ethical frameworks are crucial to guide the integration of AI in clinical\ndecision-making. Best practices developed by leading institutions aim to address transparency, fairness, and privacy\nconcerns, promoting ethical AI implementation in healthcare [24]. A systematic approach is necessary to identify and\naddress gaps in the ethical application of AI, paving the way for evidence-informed practices. The applications of AI in\nhealthcare underscore the necessity of accurate and ethically sound decision-making [25]. As LLMs are increasingly\nadopted for tasks such as medical diagnosis, the confidence-competence gap, and its implications have become a critical\nfocus of AI safety and reliability. Current AI in healthcare literature suggests a need for rigorous standards and ethical\noversight to ensure that the deployment of AI does not compromise patient safety or care quality.\nSimple prompting techniques are exclusively employed in this study to elicit responses from GPT-4 on clinical knowledge\nsets derived from USMLE questions. The accuracy of responses provided by GPT-4 is assessed, with the dataset\ncomprising instances of both correct and incorrect responses generated by ChatGPT, along with the associated confidence"}, {"title": "3 Methodology", "content": "In this section, the approach of data organization and analyses are detailed. The primary tool for assessment was the\nGPT-4 model, and the questionnaire comprised questions predominantly from the USMLE question bank. The LLM\nmodel's proficiency in healthcare knowledge and ability to assess confidence has been evaluated based on its response\nto the questionnaire. In addition, 16 questions for the questionnaire were taken from high school biology textbooks.\nWhile these questions may not have the rigor of the USMLE test, these questions were interspersed randomly in the\nquestionnaire. One of the areas of interest of this study was to observe how the models assessed and adjusted their\nconfidence level between random questions of varying difficulties when transitioning between questions of varying\ndifficulties. Assessing top-tier models like GPT-4 is critical, especially when considering their implication in sensitive\nand vital areas such as healthcare. Understanding their capabilities and limitations is crucial before considering their\ndeployment in such critical sectors."}, {"title": "3.1 Data Collection", "content": "GPT-4 was asked multiple-choice questions from the USMLE question pool to gather data for this study. These questions\ncovered a wide range of topics with varying levels of difficulty. The difficulty levels of the questions were not disclosed\nto the model. Some questions were challenging, based on the USMLE standards, while others were easier, focusing on\ngeneral health topics. A simple prompting strategy was employed to ensure that the model accurately comprehended the\nquestions."}, {"title": "3.1.1 With Feedback vs. No Feedback", "content": "Two groups, WF and NF, were employed to gather data for assessing the self-evaluation of a language model. The\nfeedback mechanism aimed to streamline confidence and self-assessment to understand the effect of feedback in\nenhancing confidence levels. This process involved providing feedback after posing a question to the model. Specifically,\nthe model was asked to rate its absolute and relative confidence before and after answering the question. Additionally,\nbefore rating absolute and relative confidence, the model was informed about the correctness of its response. The\nobjective was to evaluate the effects of the WF and NF group on the model's self-assessment. For the NF group, the\nmodel was not purposely informed about the accuracy of its response."}, {"title": "3.2 Question Design", "content": "The USMLE exam is one of the toughest tests taken globally[27] and is well-recognized in the medical field. Since\nUSMLE exam questions are the standards in assessing the proficiency of medical students, they choose to assess\nknowledge of the LLM as well in addition to a few questions from the high school level. These questionnaires often\npresent real-life scenarios, including details like a patient's age, personal traits, medical history, habits, and other\nfactors affecting their health. The exam questions demonstrate thoughtful construction to thoroughly test the critical\ncompetencies expected of medical graduates who will enter clinical practice. There is broad coverage of topics across\nall major organ systems, spanning various diseases, patient demographics, and clinical settings. This requires examines\nto have a strong generalist medical knowledge foundation. Questions range from foundational concepts like anatomy and\nphysiology to complex clinical reasoning about diagnosis and management. The vignette-based format presents a short\nclinical scenario with details like patient age, symptoms, medical history, and initial workup findings. This emulates\nreal-world clinical encounters and tests the application of knowledge, not just memorized facts. The vignettes provide\nsufficient patient details, so the information is neither vague nor an unrealistic laundry list. This makes the task clear to\nthe examinee yet still challenging. The questions avoid ambiguous phrasing and indicate what is being asked. The\nanswer options are thoughtful and well-differentiated. The correct answer is clear, concise, and well-supported by the\nvignette. Incorrect options include common mistakes and misconceptions a novice might make rather than completely\nirrelevant answers. This appropriately distinguishes between superficial knowledge and true mastery. The questions\nvary in difficulty level, including some testing core concepts and others requiring deep analysis of clinical findings or\nscientific principles. This discriminates the depth of knowledge across candidates."}, {"title": "3.3 Variable Definitions", "content": "During the data collection process, we employed absolute and relative confidence. Absolute confidence refers to the\nlevel of certainty or confidence the model has in its answer to a question, irrespective of external factors. It represents"}, {"title": "4 Results", "content": "The result section encompasses descriptive statistics, correlation analysis, comparative examination of feedback groups,\nand visualization. Across all instances, the median confidence scores for AC1, AC2, RC1, and RC2 consistently reached\n0.9. Notably, the WF group achieved a response rate of 88 questions answered correctly out of 100 questions, whereas\nthe NF group demonstrated a slightly higher accuracy, with 92 out of 100 questions answered correctly. The model\nshowed varied confidence depending on whether feedback is provided or not. The mean confidence for AC1 before\nasking a question was 0.91, and for RC1, it was 0.9. After posing the question, the mean for AC increased to 0.94, and\nfor RC, it rose to 0.93, indicating a higher confidence after asking the question. The standard deviation was higher for\nAC1 at 0.05 and lower for RC2 at 0.046, suggesting greater variability in AC1. AC1 and AC2 ranged from 0.7 to 1,\nwhereas RC1 and RC2 ranged from 0.75 to 1, showing different confidence levels. The mean values for AC1 and AC2\nclosely matched the overall mean for the WF condition. After the feedback, the standard deviation for AC1 and AC2\ndecreased, whereas it increased for RC1 and RC2. The minimum values for AC1, AC2, RC1, and RC2 were 0.7, 0.78,\n0.76, and 0.81 respectively with the maximum being 1. For the NF group, mean values for AC1 and AC2 were similar.\nThe standard deviation for AC1 and AC2 was higher than the overall standard deviation, while for RC1 and RC2, it\nwas lower. The minimum confidence values for AC1 and AC2 were 0.7, and for RC1 and RC2, they were 0.75. The\nmaximum values for AC1, AC2, RC1, and RC2 were observed to be 0.98, 0.99, and 1 respectively."}, {"title": "4.1 Visualizations", "content": "The pair plot in Figure 2 illustrates the correlation between absolute and relative confidence in the model. The model\nexhibited higher confidence levels of more than 0.8 in most cases when providing accurate responses. Moreover, the"}, {"title": "5 Discussion", "content": "The median confidence scores for AC1, AC2, RC1, and RC2 were consistently high, at around 0.9. This indicates that,\non average, the model exhibits a strong sense of confidence in its ability to answer questions. The WF group correctly\nanswered 88 out of 100 questions, while the NF group had a slightly higher accuracy with 92 correct answers out of\n100. This suggests that feedback has an impact on the performance of the model. However, it is interesting to note that\nthe presence of feedback doesn't necessarily correlate with higher accuracy. The mean confidence score for both AC\nand RC increased after the questions were posed (AC from 0.91 to 0.94 and RC from 0.9 to 0.93). This suggests that\nLLM became more confident in its ability to answer once it understood the context of the questions. With feedback,\nthe standard deviation decreased for AC1 and AC2 but increased for RC1 and RC2. This could mean that feedback\nmakes the model more certain about its capabilities but less certain when comparing itself to others. The system shows\nlow confidence in answering simple questions while demonstrating increased confidence with more complex ones,\nhighlighting the critical role of the feedback.\nThe violin plots show that in the NF group, the model exhibits median AC1 scores below 0.9 for incorrect answers and\nabove 0.9 for correct ones, with AC2 indicating higher confidence in correct answers. While the AC2 is generally higher\nfor correct answers, the median of AC2 for incorrect answers also increased. This suggests that the model initially\nlacked confidence in their responses, but gained confidence when the question appeared easier, often performing better\nthan initially anticipated. Additionally, it appears that the model might possess some awareness of the accuracy of its\nresponses. The model exhibited a consistent pattern when providing incorrect answers. However, for correct answers,\nthe mean confidence level tends to decrease for AC1. In contrast, for AC2, the confidence scores remain on par with\nthose in the NF group. This suggests that the model, when provided with feedback, might be attempting to calibrate its\nresponses. It seems to exhibit caution about giving an incorrect answer before seeing the question, but once the question\nis viewed, it maintains a similar level of confidence, regardless of the presence of feedback. Similarly, for both RC1\nand RC2, high confidence levels are observed in the correct answers in both WF and NF scenarios. However, in the\nabsence of feedback, there is a consistency in confidence levels, whereas in the WF scenario, this consistency is lacking,\nwhich is evident from Figure 4. Specifically, in RC2, confidence decreases when feedback is provided. This indicates\nthat the model may be less certain about its responses when compared to another model. Although the tests indicate\na lack of statistically significant difference in average confidence scores with and without feedback, the distribution\nplots revealed higher confidence scores for the NF group in some instances. In high-stakes domains such as healthcare,\nthese differences can have major implications for the health of a patient. These extreme cases of the confidence score\ndistribution reflected in the non-overlapping regions of the density plot in Figure 5 and Figure 6 must be studied. In\nthese scenarios, it is essential to consider the tails of the distribution alongside the central tendencies.\nThe Sequential Analysis of AC1 Scores, in Figure 7, over 100 problems provides a visual narrative of the model's\nperformance for both the WF and NF group. The NF group starts lower but consistently becomes higher than WF\ncases after one-third of the problems, indicating sensitivity to feedback. Instances of significant drops in AC1 scores\nafter incorrect answers in the WF group suggest the model's high sensitivity to feedback. Analysis of RC1 against the\nsequence of problems suggests the model often displays confidence above 0.8, both with and without feedback, indicating\nit may perceive itself as superior to other models. However, there are instances where the model's confidence decreases\nafter receiving feedback, even with correct answers, which could be due to overestimation errors or adjustments to align\nwith other scores. Feedback influences relative confidence but doesn't consistently increase or decrease it. Examining\nthe correlation between RC1 and the sequence of problems reveals that the model consistently expresses confidence\nlevels above 0.8, irrespective of feedback. Feedback occasionally influences the model's confidence, either decreasing\nafter a correct answer or increasing significantly. The analysis doesn't show a definitive and consistent trend in how\nfeedback affects confidence but emphasizes its influence on the model's relative confidence.\nThe confidence level of LLMs like GPT-4 plays a crucial role in gaining the trust of users for its response. Overconfidence\nin LLM predictions can lead to erroneous answers, potentially leading users to overlook alternative solutions or fail to\nseek necessary human expertise, especially in critical areas like healthcare. Whereas, under-confidence might result in\nthe under-utilization of useful AI insights, hindering decision-making processes or the adoption of AI tools for effective\nsolutions. Balancing confidence levels is essential to maximize the practical applications and trustworthiness of LLM\npredictions."}, {"title": "6 Conclusion", "content": "In conclusion, our research analyzing GPT-4's responses to the USMLE questionnaire, covering both WF (With\nFeedback) and NF (No Feedback) groups, has underscored the complex interaction between confidence calibration and\nits impact on model accuracy. This study brings to light the challenges of implementing AI in the healthcare sector. The\nconfidence level exhibited by a Language Model like GPT-4 is crucial in shaping user trust in its predictions. Notably,\nboth high and low levels of confidence can have significant implications in practical applications. Our findings suggest\nthat feedback influences absolute confidence levels, as indicated by a reduction in the variability of confidence scores.\nHowever, this did not result in enhanced average confidence values. This pattern implies that the model consistently\nmaintains certain confidence levels, regardless of feedback or the correctness of responses. Future research should\nfocus on increasing the study's sample size and conducting comparative analyses with other models, aiming to more\ncomprehensively investigate how feedback influences AI's effectiveness in clinical decision-making."}, {"title": "A.1 Sequential Plots", "content": ""}]}