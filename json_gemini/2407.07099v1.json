{"title": "Nash CoT: Multi-Path Inference with Preference Equilibrium", "authors": ["Ziqi Zhang", "Cunxiang Wang", "Xiong Xiao", "Yue Zhang", "Donglin Wang"], "abstract": "Chain-of-thought (CoT) prompting has\nemerged as a powerful technique for enhancing\nthe reasoning capabilities of Large Language\nModels (LLMs) on complex problems. Among\nCoT-related studies, self-consistency (Multi-\npath inference with answer filtering through\nvoting) involves generating multiple reasoning\npaths using the CoT framework and then\nselecting the most frequently produced outputs\nstanding out as a concise yet competitive\napproach. While self-consistency has indeed\nled to the improvements in LLM inference,\nthe use of multi-path inference also escalates\ndeployment costs. Therefore, maintaining\nthe performance benefits of self-consistency\ninherited from multi-path inference while\nreducing the inference costs holds significant\nvalue. In this research, we conceptualize\nlanguage decoding as a preference consensus\ngame, constructing a bi-player gaming system\nwithin each local path, and introduce Nash\nChain-of-Thought (Nash CoT). Specifically,\nfor a given question, we leverage LLM to\nautonomously select the contextually relevant\ntemplate and generate outputs guided by this\ntemplate, aiming to reach Nash Equilibrium\nalongside normal generation in each path. This\napproach allows us to achieve comparable\nor improved performance compared to\nself-consistency while using fewer inference\npaths on various inference tasks, including\nArabic reasoning, Commonsense Question\nanswering, and Symbolic inference.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have revolution-ized Natural Language Processing (NLP) (Ouyanget al., 2022; etc., 2023; Jiang et al., 2023; Brownet al., 2020b; OpenAI, 2024). In particular,leveraging human-designed instructions as input,LLM demonstrates superior inference performanceacross various types of simple reasoning tasks (Rad-ford et al., 2019; Brown et al., 2020a). But, itsperformance remains variable in complex reason-ing tasks (Rae et al., 2022). To enhance LLM'sinference capabilities on complex issues, we canemploy a step-by-step inference approach, knownas Chain-of-Thought (CoT) prompting (Wei et al.,2023). For instance, starting with a template like\"Let's think step by step\", LLM first gen-erates rationales and then arrives at a final prediction.This approach significantly improves LLM's infer-ence performance across tasks like Arabic, SymbolInference, and CommonsenseQA.\nSubsequently, the impressive performance ofCoT on complex inference tasks has spurred newdevelopments in this direction (Wang et al., 2023;Wei et al., 2023; Jin and Lu, 2023; Zhang et al.,2022; Shi et al., 2022). Among these developments,self-consistency (Wang et al., 2023) emerges asa competitive CoT approach. It leverages uncer-tainty from multiple inference paths from a sin-gle LLM, and ranking generated answers by fre-quency can significantly enhance LLM's inferenceperformance. This approach significantly improves\nthe performance of GPT-3 utilizing zero-shot CoTwithout any options for parameter tuning. Mean-while, experimental results indicate that inferenceperformance improves with the number of gen-erated samples (Wang et al., 2023), suggestingthat the potential of LLM's inference capabili-ties has not yet to be fully exploited. Althoughself-consistency is straightforward to implementand requires no additional turning, it has a signifi-cant drawback: substantially higher inference costscompared to directly utilizing CoT.\nOn the other hand, the performance improve-ments resulting from self-consistency imply thatrelying solely on single-path inference cannot fullyharness the inference capabilities of LLM, andmulti-path inference encompasses the potential cor-rect answers. Therefore, it's necessary to main-"}, {"title": "Related Work", "content": "Chain-of-Thought (CoT). There are three ma-jority kinds of CoT approaches. Zero-shot CoT thatPrompts LLM with simple yet instructions to guideLLM generate step by step (Kojima et al., 2023).Manual CoT that initialized by randomly sampleds several cases from dataset or designed by human,and followed by utilizing these cases as demonstra-tion to guided LLM generate follow the mannerof demonstration (Wei et al., 2023), however, suchmethods can be biased if the demonstration canrepresent real distribution. Automatic (or batch)CoT (Zhang et al., 2022) first sample cases whichhave the most representative sentence embeddingin each clusters, followed by inference with thesame manner as manual CoT. Self-Consistency(Wang et al., 2023) showcases strong performancein vast benchmarks. Apart from its impact on in-ference performance,self-consistency also boastsscalability as a key advantage. It seamlessly inte-grates with different approaches, such as tabularconsistency of transformations (tab-CoT) (Jin andLu, 2023), making it adaptable and versatile forvarious applications. Despite self-consistency hasimproved LLM's performance on Arabic bench-marks. Correspondingly,self-consistency shouldhave to inference multi-times, thus it burdens thedeploy budgets.\nOne way to address this limitation is by initiallysampling multiple inference paths and then fine-tuning using the highest frequency path. Specifi-"}, {"title": "Preference Equilibrium in mini-batch inference", "content": "Self-consistency has shown that the inference ofLLMs under a single path may not represent theirfull capabilities. By simply conducting multiple in-ferences and filtering answers through voting, it ispossible to achieve more accurate results. However,multi-path inference lacks a strong theoretical foun-dation to determine the optimal number of infer-ence paths, potentially leading to much more com-putational resource consumption. To reduce thenumber of paths required for multi-path inference,we utilize the concept of Nash Equilibrium to lo-cally construct a binary game system in multi-pathinference. Specifically, the preference of each validinference path of the LLM needs to achieve Nash\nequilibrium with the preferences of the generation\nguided by the template. This approach increases"}, {"title": "Mini-batch inference with Preference Equilibrium", "content": "Subsequently, based on the concept of PreferenceEquilibrium, we conceptualize a Mini-batch infer-ence (shown in Figure 1) as a bi-player gaming sys-tem. This approach aims to achieve better inferencecompared to direct inference, while still preserv-ing some of the inherent randomness of standardinference methods. To begin with proposing thissystem, we first define $x_t$ as the template of zero-shot CoT, ${x_0, x_1,\\dots,x_n}$ (we provided severalcases in Player Templates) as the candidates tem-plate for template guided generation. Meanwhile,in this system, the template can to be chosen by areward model $r_\\theta$.\nIn terms of the process of mini-batch inference,we firstly inference LLM twice times (we have con-ducted ablations about 'twice' in section ablation)i.e. $[y_0, y_1] \\leftarrow [\\pi(\\cdot|x_t,x), \\pi(\\cdot|x_t, x)]$. Meanwhile,due to the inherent uncertainty of LLM, the gen-eration of $[y_0, y_1]$ can be considered a potentialset of distinct predictions. Subsequently, the tem-plate guided generation can be sampled by query-ing LLM with $x^\\epsilon$ and $x_t$ i.e. $y^* \\leftarrow \\pi(\\cdot|x^\\epsilon, x_t,x)$.Furthermore, we can select an answer from $y_1$ and$y_2$ that is the same as $y^*$, thereby satisfying theNash Equilibrium described in Theorem 3.1. Basedon the mini-batch inference, we further introduceNash CoT in the next chapter. (Notably, the pat-terns in this chapter may not always hold true. Forinstance, $y^*$ may not always in $[y_1, y_2]$. We willaddress this issue in the following chapters.)"}, {"title": "Nash Chain of Thought (Nash CoT)", "content": "Nash CoT can be seen as an extension of Mini-batch inference with Preference Equilibrium, im-plementing multiple Mini-batch inferences to en-hance performance. This approach is influenced byexperimental results from self-consistency, whichsuggest that increasing the number of paths canimprove inference accuracy. Meanwhile, the rea-soning process for each question is divided into twostages: Answer Gathering and Answer Filtering.\nAnswer Gathering. When generating candidateanswers, the process predominantly involves twotypes of loops: Mini-batch Loops ($n_{mini}$): InChapter 3.1, we discussed the implementation ofmini-batch inference with Preference Equilibrium.As shown in Algorithm 1, this process involvessearching for template-guided generations withintwo rounds of generation ([y1, y2]). We refer tothe times of these two predictions as the nmini.Moreover, to mitigate the impact brought from low-frequency predictions, we introduce iterating nminimultiple times. This leads us to another type ofloop: Outer Loops (nouter): This loop resemblesthe concept of multi-path in self-consistency. Af-ter completing loop nouter, we filter the generatedanswers and retain the answer that reaches equilib-rium most frequently (shown in Algorithm 2), asthe preferred answer.\nAnswer Filtering. In terms of answer filtering,as shown in Algorithm 2 we first count the mostfrequent prediction satisfy Preference equilibrium.Specifically, we count all y* satisfy $y^* \\in [y_1, y_2]$and compute their frequency. Subsequently, wereturn the most frequent case. Otherwise, if is nocases satisfy $y^* \\in [y_1, y_2]$, we adopt the strategyof self-consistency by selecting the most frequentprediction among all generated answers."}, {"title": "Experiments", "content": "The goal of our experiment is to 1) demonstrate theperformance advantage and effectiveness of NashCoT. 2) shocase whether Nash CoT help reduce theoverall inference time. In the following sections,we first introduce our experimental setup and thenpresent the experimental results and analysis.\nDatasets. Our majority benchmarks are com-posed of three different kinds of inferencetasks. 1) arithmetic reasoning: SingleEq (Koncel-Kedziorski et al., 2015), AddSub (Hosseiniet al., 2014), MultiArith (Roy and Roth, 2016),GSM8K (Cobbe et al., 2021), AQUA (Ling et al.,2017), and SVAMP (Patel et al., 2021). 2) sym-bolic reasoning: Last Letters, Coin Flip (Weiet al., 2023), and Object Tracking, BigbenchDate. 3) commonsense question answering: Com-monsenseQA (Talmor et al., 2019) and Strate-gyQA (Geva et al., 2021). For more details aboutthe dataset please refer to Appendix A.\nLLMs. To validate that Nash CoT is a generalCoT method, we selected different large models astest models, including Mistral-7B (Instruct) (Jianget al., 2023), GLM4-9B-Chat (Zeng et al., 2022;Du et al., 2022). In particular, all of these selectedLLMs are turned via RL with human feedback(RLHF), and the difference between LLM turnedwith RLHF and the original foundation models\nhave been detailed by Ouyang et al. (2022).\nBaselines. The preliminary baselines we utilizedinclude zero-shot, zero-shot CoT (Wei et al., 2023),andself-consistency (Wang et al., 2023). We test\nthese approach with freezed LLMs.\nSettings. Our evaluation on all selected tasks uti-lizes the same experimental settings bellow:"}, {"title": "Experimental Results", "content": "Evaluated Scores. The majority experimental re-sults are demonstrated in table 1, 2 and 3. NashCoT can improve Mistral-Instruct (7B) on almostall selected inference tasks, while showcasing simi-lar performance to self-consistency with twice in-ference paths on GLM4-chat (9B). In particular,"}, {"title": "Ablation Study", "content": "In order to further validate the effectiveness of NashCoT, we conducted extensive ablations to answerthe following questions: 1) What will happen whenthe number of inference paths for Nash CoT is fur-ther increased? Will Nash CoT eventually surpassself-consistency, and what is the relationship be-tween the number of loops and performance? 2)Does the template really improve the accuracy ofpath predictions, and what impact does it have onexperimental performance?\nAs the number of inference paths increases,Nash CoT can obviously surpass self-consistencywith fewer inference paths. To address question1), we selected Mistral-Instruct (7B) and conductedevaluation on three different reasoning tasks, ad-justing the Nmini and Nouter. As shown in Figure 4,as the number of loops increases, Nash CoT has"}, {"title": "Conclusion", "content": "In this study, we proved the existence of Nashequilibrium in preference model, subsequently, weproposed a new CoT approach Nash CoT, andvalidated its performance on various inferencebenchmarks. Experimental results show that NashCoT can perform equally or even better than self-consistency while only require half inference costs."}, {"title": "Limitations and Future Work", "content": "Despite Nash CoT showcase competitive perfor-mance with only half of inference paths, it requirespre-defined template, thus it's in-convenient to uti-lize Nash CoT in new emerging scenario, in thefuture we will develop a automatic approach tobalance task feedback and template design."}, {"title": "Ethics Claims", "content": "Despite LLM has showcased superiority perfor-mance on vast benchmarks, but pre-train or fine-tune a LLM requires numerous computing re-sources. Therefore, it's crucial to study how toinference a LLM to reach the ceiling of its capacity.CoT is a ideal approach which has been proved thatcan obviously evaluate the performance of LLMs'inference. Among that,self-consistency is one ofthe best CoT approach.\nOur method effectively reduce the inferencetimes of multi-path inference, thereby reducing thedeploy budgets of self-consistency. We believe ourapproach can further elevate the effectiveness ofmulti-path inference, thereby further improving theeffectiveness of LLM."}]}