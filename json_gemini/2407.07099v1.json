{"title": "Nash CoT: Multi-Path Inference with Preference Equilibrium", "authors": ["Ziqi Zhang", "Cunxiang Wang", "Xiong Xiao", "Yue Zhang", "Donglin Wang"], "abstract": "Chain-of-thought (CoT) prompting has emerged as a powerful technique for enhancing the reasoning capabilities of Large Language Models (LLMs) on complex problems. Among CoT-related studies, self-consistency (Multi-path inference with answer filtering through voting) involves generating multiple reasoning paths using the CoT framework and then selecting the most frequently produced outputs standing out as a concise yet competitive approach. While self-consistency has indeed led to the improvements in LLM inference, the use of multi-path inference also escalates deployment costs. Therefore, maintaining the performance benefits of self-consistency inherited from multi-path inference while reducing the inference costs holds significant value. In this research, we conceptualize language decoding as a preference consensus game, constructing a bi-player gaming system within each local path, and introduce Nash Chain-of-Thought (Nash CoT). Specifically, for a given question, we leverage LLM to autonomously select the contextually relevant template and generate outputs guided by this template, aiming to reach Nash Equilibrium alongside normal generation in each path. This approach allows us to achieve comparable or improved performance compared to self-consistency while using fewer inference paths on various inference tasks, including Arabic reasoning, Commonsense Question answering, and Symbolic inference.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolution-ized Natural Language Processing (NLP) (Ouyang et al., 2022; etc., 2023; Jiang et al., 2023; Brown et al., 2020b; OpenAI, 2024). In particular, leveraging human-designed instructions as input, LLM demonstrates superior inference performance across various types of simple reasoning tasks (Radford et al., 2019; Brown et al., 2020a). But, its performance remains variable in complex reasoning tasks (Rae et al., 2022). To enhance LLM's inference capabilities on complex issues, we can employ a step-by-step inference approach, known as Chain-of-Thought (CoT) prompting (Wei et al., 2023). For instance, starting with a template like \"Let's think step by step\", LLM first generates rationales and then arrives at a final prediction. This approach significantly improves LLM's inference performance across tasks like Arabic, Symbol Inference, and CommonsenseQA.\nSubsequently, the impressive performance of CoT on complex inference tasks has spurred new developments in this direction (Wang et al., 2023; Wei et al., 2023; Jin and Lu, 2023; Zhang et al., 2022; Shi et al., 2022). Among these developments, self-consistency (Wang et al., 2023) emerges as a competitive CoT approach. It leverages uncertainty from multiple inference paths from a sin-gle LLM, and ranking generated answers by frequency can significantly enhance LLM's inference performance. This approach significantly improves the performance of GPT-3 utilizing zero-shot CoT without any options for parameter tuning. Meanwhile, experimental results indicate that inference performance improves with the number of generated samples (Wang et al., 2023), suggesting that the potential of LLM's inference capabilities has not yet to be fully exploited. Although self-consistency is straightforward to implement and requires no additional turning, it has a significant drawback: substantially higher inference costs compared to directly utilizing CoT.\nOn the other hand, the performance improvements resulting from self-consistency imply that relying solely on single-path inference cannot fully harness the inference capabilities of LLM, and multi-path inference encompasses the potential correct answers. Therefore, it's necessary to maintain this strategy. Meanwhile, numerous practical applications indicate that when a LLM is given the appropriate templates, it can perform specific tasks more proficiently. Hence, an intuitive strat-egy to reduce the number of inference paths in self-consistency is to use templates to guide the LLM to correctly infer each path.\nTo achieve this goal, we use the contextual information required by the question as a template to guide the inference in each path. This approach in-creases the probability that the LLM can correctly solve the question in each path, thereby potentially reducing the number of inference paths needed for multi-path inference. Meanwhile, to alleviate over-confidence in template-guided generation, we develop a bi-player gaming system that introduces the Nash Equilibrium of preference of strategy (defined as Preference Equilibrium) in multi-path inference. This system samples generations that balance the preferences of both template-guided and normal generation, ensuring the output robustly matches the context of the given question while sustaining some moderate generation pattern. Subsequently, we combine multi-path inference with Preference Equilibrium to propose Nash CoT. We present the comparison between Self-Consistency and Nash CoT in Figure 1.\nWe conduct experiments with two local deployed LLMs-Mistral-Instruct (Jiang et al., 2023) and GLM4 (Zeng et al., 2022; Du et al., 2022) on var-ious inference tasks, including Arabic Reasoning (Koncel-Kedziorski et al., 2015; Hosseini et al., 2014) and Symbolic Reasoning (Wei et al., 2023) and Commonsense Reasoning (Talmor et al., 2019; Geva et al., 2021). As shown in Figure 2, Nash CoT"}, {"title": "2 Related Work", "content": "Chain-of-Thought (CoT). There are three ma-jority kinds of CoT approaches. Zero-shot CoT that Prompts LLM with simple yet instructions to guide LLM generate step by step (Kojima et al., 2023). Manual CoT that initialized by randomly sampled several cases from dataset or designed by human, and followed by utilizing these cases as demonstration to guided LLM generate follow the manner of demonstration (Wei et al., 2023), however, such methods can be biased if the demonstration can represent real distribution. Automatic (or batch) CoT (Zhang et al., 2022) first sample cases which have the most representative sentence embedding in each clusters, followed by inference with the same manner as manual CoT. Self-Consistency (Wang et al., 2023) showcases strong performance in vast benchmarks. Apart from its impact on in-ference performance,self-consistency also boasts scalability as a key advantage. It seamlessly inte-grates with different approaches, such as tabular consistency of transformations (tab-CoT) (Jin and Lu, 2023), making it adaptable and versatile for various applications. Despite self-consistency has improved LLM's performance on Arabic bench-marks. Correspondingly,self-consistency should have to inference multi-times, thus it burdens the deploy budgets.\nOne way to address this limitation is by initially sampling multiple inference paths and then fine-tuning using the highest frequency path. Specifically, Huang et al. (2022) suggest that gathering in-ferences from multiple paths and sampling the most frequent generation to fine-tune a smaller LLM can enhance the LLM's inference performance. However, this approach still requires updating the LLM's parameters, which is inefficient. Therefore, it is necessary to further explore inference methods to maintain the performance of self-consistency while reducing the number of inference paths.\nPreference Optimization. The training policy with Reinforcement Learning (RL) to reflect pref-erence, termed Reinforcement Learning with Hu-man Feedback (RLFH), was initially introduced by (Akrour et al., 2011) and has since undergone consistent improvement and refinement by (Cheng et al., 2011; Busa-Fekete et al., 2013; Wilson et al., 2012). This approach has been widely applied to adjust Large Language Models' (LLMs) parame-ters to align with human preferences (Ouyang et al., 2022; Jiang et al., 2023; etc., 2023). Recently, a new approach called Direct Optimizing from Pref-erence (DPO) has been proposed by (Rafailov et al., 2023), aiming to directly adjust LLMs to reflect hu-man preferences without requiring a reward model and RL algorithm. Additionally, (Munos et al., 2023) proposed combining DPO with Nash equi-librium to ensure convergence of the last iterated policy. Our study also utilizes the concept of equi-librium in preference model, but the main differ-ence is that we utilize preference equilibrium as a standard to pick up the most preferred answer instead optimizing the LLM's parameters."}, {"title": "3 Preference Equilibrium in mini-batch inference", "content": "Self-consistency has shown that the inference of LLMs under a single path may not represent their full capabilities. By simply conducting multiple in-ferences and filtering answers through voting, it is possible to achieve more accurate results. However, multi-path inference lacks a strong theoretical foun-dation to determine the optimal number of infer-ence paths, potentially leading to much more com-putational resource consumption. To reduce the number of paths required for multi-path inference, we utilize the concept of Nash Equilibrium to lo-cally construct a binary game system in multi-path inference. Specifically, the preference of each valid inference path of the LLM needs to achieve Nash equilibrium with the preferences of the generation guided by the template. This approach increases"}, {"title": "Preference Model.", "content": "Given text input x, and the sampled predictions (answers) $y_1, y_2$, we first de-fine $y_1$ prefers $y_2$ as Equation 1:\n$P(y_1 \\succ y_2|x) := sign(re(y_1|x)) - sign(re(y_2|x))$, (1)\nwhere re denotes preference model that reflecting the preference when comes to the pairs. Then we imply the existence of Nash equilibrium in Pref-erence model, specifically, Equation 1 is strictly linear, i.e. $P(y_1 \\prec y_2|x) = 1 - P(y_2 \\prec y_1|x)$."}, {"title": "Player Templates:", "content": "Templates for our preference model that are shown the structure: id(player): description.\nMathematician: You are a mathematician, you excel at analyzing problems from a mathematical logical perspective and arrive at conclusions that align with your values.\nLiterary scholar: You are a literary scholar who has read a vast array of literary works. Please consider the problem from the per-spective of a literary scholar.\nPhilosophical: You are a philosopher, your knowledge base includes a wealth of philo-sophical knowledge. You enjoy approach-ing problems from a philosophical perspec-tive and arriving at conclusions that align with your values.\nGeographer: You are a geographer with a deep understanding of geographical knowl-edge. Please approach the given problem from the perspective of a geographer.\n(other cases have been appended to the Appendix.)\nAdditionally, drawing from the definition from Munos et al., we define one policy as more preferred over another as:\n$P(\\pi_1 \\prec \\pi_2) := E_{\\substack{y_1\\sim\\pi_1(-\\x)}} [P(y_1 \\prec y_2|x)]$ (2)\n$\\substack{y_2\\\\sim \\pi_2(x)}$\nPreference Equilibrium. We aim to select the best template (we provide several cases in above example 3 for the current problem, thus facilitating the large model in problem-solving, correspond-ingly reduces the required num of inference paths. However, this may lead to some issues: 1) If the template is incorrectly chosen, it may cause the agent to generate answers outside the range of cor-rect responses corresponding to the current prob-lem, resulting in erroneous replies. 2) The large model may excessively generate context-dependent responses, affecting its robustness. To address these issues, we build a local bi-player gaming system that the preference of template guided LLM (player 1) over normal status of LLM (player 2) is the pay-off of template guided LLM, vice visa. If player 1 and player 2 reaches Nash Equilib-rium, then the strategy can match the preference of both player 1 and player 2.\nSubsequently, we define the status that player 1 and player 2 reach Nash Equilibrium as Preference Equilibrium (Definition 1). Meanwhile, in Theorem 3.1, we prove the existence of Nash Equi-librium in this system. Specifically, the strategy of player 1 equal to player 2 is one solution that this system has reached Preference Equilibrium.\nTheorem 3.1 (Existence of Preference Equilibrium). Given any two policy (player) $\\pi_1$ and $\\pi_2$ within the gaming system defined in Definition 1, where $\\pi\\in \\Pi$. $\\pi_1 = \\pi_2$ denotes a solution where the gaming system reaches Nash Equilibrium.\nMeanings of the existence of Preference Equi-librium. Theorem 3.1 proves the existence of an Nash Equilibrium between the template guided LLM and the normal status of LLM. When reach-ing Preference Equilibrium, the preference of de-cisions made by the template guided LLM are aligned with those made by the LLM under nor-mal status. Meanwhile, the preference of template guided LLM generation is much more closed to the requirement of quesitons' context, while those of the normal status LLM are predominantly based on its parameters which is much more robust than template guided LLM. Therefore, this equilibrium can balance the requirement of contextual informa-tion and robustness of the model generation during problem-solving. Notabaly, $\\pi_1 = \\pi_2$ means their outputs are also likely to be equal. This insight forms a fundamental basis for piratically imple-menting Nash CoT."}, {"title": "3.1 Mini-batch inference with Preference Equilibrium", "content": "Subsequently, based on the concept of Preference Equilibrium, we conceptualize a Mini-batch infer-ence (shown in Figure 1) as a bi-player gaming sys-tem. This approach aims to achieve better inference compared to direct inference, while still preserv-ing some of the inherent randomness of standard inference methods. To begin with proposing this system, we first define $x_t$ as the template of zero-shot CoT, $\\{x^0, x_1,\\ldots, x_n^\\}$ (we provided several cases in Player Templates) as the candidates tem-plate for template guided generation. Meanwhile, in this system, the template can to be chosen by a reward model $r_\\theta$.\nIn terms of the process of mini-batch inference, we firstly inference LLM twice times (we have con-ducted ablations about 'twice' in section ablation) i.e. $[Y_0, Y_1] \\leftarrow [\\pi(\\cdot|x_t,x^0), \\pi(\\cdot|x_t, x^0)]$. Meanwhile, due to the inherent uncertainty of LLM, the gen-eration of $[y_0, y_1]$ can be considered a potential set of distinct predictions. Subsequently, the tem-plate guided generation can be sampled by query-ing LLM with $x^0$ and $x_t$ i.e. $y^* \\leftarrow \\pi(\\cdot|x^\\, x_t,x^0)$. Furthermore, we can select an answer from y1 and Y2 that is the same as y*, thereby satisfying the Nash Equilibrium described in Theorem 3.1. Based on the mini-batch inference, we further introduce Nash CoT in the next chapter. (Notably, the pat-terns in this chapter may not always hold true. For instance, $y^*$ may not always in $[Y_1, Y_2]$. We will address this issue in the following chapters.)"}, {"title": "4 Nash Chain of Thought (Nash CoT)", "content": "Nash CoT can be seen as an extension of Mini-batch inference with Preference Equilibrium, im-plementing multiple Mini-batch inferences to en-hance performance. This approach is influenced by experimental results from self-consistency, which suggest that increasing the number of paths can improve inference accuracy. Meanwhile, the rea-soning process for each question is divided into two stages: Answer Gathering and Answer Filtering.\nAnswer Gathering. When generating candidate answers, the process predominantly involves two types of loops: Mini-batch Loops (nmini): In Chapter 3.1, we discussed the implementation of mini-batch inference with Preference Equilibrium. As shown in Algorithm 1, this process involves searching for template-guided generations within two rounds of generation ([y1, y2]). We refer to the times of these two predictions as the nmini. Moreover, to mitigate the impact brought from low-frequency predictions, we introduce iterating nmini multiple times. This leads us to another type of loop: Outer Loops (nouter): This loop resembles the concept of multi-path in self-consistency. Af-ter completing loop nouter, we filter the generated answers and retain the answer that reaches equilib-rium most frequently (shown in Algorithm 2), as the preferred answer.\nAnswer Filtering. In terms of answer filtering, as shown in Algorithm 2 we first count the most frequent prediction satisfy Preference equilibrium. Specifically, we count all y* satisfy $y^* \\in [Y_1, Y_2]$ and compute their frequency. Subsequently, we return the most frequent case. Otherwise, if is no cases satisfy $y^* \\in [Y_1, Y_2]$, we adopt the strategy of self-consistency by selecting the most frequent prediction among all generated answers."}, {"title": "5 Experiments", "content": "The goal of our experiment is to 1) demonstrate the performance advantage and effectiveness of Nash CoT. 2) shocase whether Nash CoT help reduce the overall inference time. In the following sections, we first introduce our experimental setup and then present the experimental results and analysis.\nDatasets. Our majority benchmarks are com-posed of three different kinds of inference tasks. 1) arithmetic reasoning: SingleEq (Koncel-Kedziorski et al., 2015), AddSub (Hosseini et al., 2014), MultiArith (Roy and Roth, 2016), GSM8K (Cobbe et al., 2021), AQUA (Ling et al., 2017), and SVAMP (Patel et al., 2021). 2) sym-bolic reasoning: Last Letters, Coin Flip (Wei et al., 2023), and Object Tracking, Bigbench Date. 3) commonsense question answering: Com-monsenseQA (Talmor et al., 2019) and Strate-gyQA (Geva et al., 2021). For more details about the dataset please refer to Appendix A.\nLLMs. To validate that Nash CoT is a general CoT method, we selected different large models as test models, including Mistral-7B (Instruct) (Jiang et al., 2023), GLM4-9B-Chat (Zeng et al., 2022; Du et al., 2022). In particular, all of these selected LLMs are turned via RL with human feedback (RLHF), and the difference between LLM turned with RLHF and the original foundation models have been detailed by Ouyang et al. (2022).\nBaselines. The preliminary baselines we utilized include zero-shot, zero-shot CoT (Wei et al., 2023), andself-consistency (Wang et al., 2023). We test these approach with freezed LLMs.\nSettings. Our evaluation on all selected tasks uti-lizes the same experimental settings bellow:"}, {"title": "5.1 Experimental Results", "content": "Evaluated Scores. The majority experimental re-sults are demonstrated in table 1, 2 and 3. Nash CoT can improve Mistral-Instruct (7B) on almost all selected inference tasks, while showcasing simi-lar performance to self-consistency with twice in-ference paths on GLM4-chat (9B). In particular,"}, {"title": "6 Ablation Study", "content": "In order to further validate the effectiveness of Nash CoT, we conducted extensive ablations to answer the following questions: 1) What will happen when the number of inference paths for Nash CoT is fur-ther increased? Will Nash CoT eventually surpass self-consistency, and what is the relationship be-tween the number of loops and performance? 2) Does the template really improve the accuracy of path predictions, and what impact does it have on experimental performance?\nAs the number of inference paths increases, Nash CoT can obviously surpass self-consistency with fewer inference paths. To address question 1), we selected Mistral-Instruct (7B) and conducted evaluation on three different reasoning tasks, ad-justing the $N_{mini}$ and $N_{outer}$. As shown in Figure 4, as the number of loops increases, Nash CoT has a high probability of significantly outperforming self-consistency with fewer paths. However, differ-ent from self-consistency, the experimental results of Nash CoT do not show a monotonic (linear) rela-tionship with the total number of total paths. This indicates that there is a significant difference be-tween Nash CoT and self-consistency. Unlike Nash CoT, the experimental results of self-consistency show a clear improvement in performance as the number of paths increases.\nThe performance is impacted by the player tem-plate. To illustrate the impact of the template, we removed the mathematical templates from the Player Templates and then evaluated Nash CoT on selected Arabic reasoning. Results are shown in Table 6, showing an approximately 9.2% decrement in GSM8K and 6.2% decrement in SVAMP. Therefore, the performance of Nash CoT is impacted by the Player Template."}, {"title": "7 Conclusion", "content": "In this study, we proved the existence of Nash equilibrium in preference model, subsequently, we proposed a new CoT approach Nash CoT, and validated its performance on various inference benchmarks. Experimental results show that Nash CoT can perform equally or even better than self-consistency while only require half inference costs."}, {"title": "Limitations and Future Work", "content": "Despite Nash CoT showcase competitive perfor-mance with only half of inference paths, it requires pre-defined template, thus it's in-convenient to uti-lize Nash CoT in new emerging scenario, in the future we will develop a automatic approach to balance task feedback and template design."}, {"title": "Ethics Claims", "content": "Despite LLM has showcased superiority perfor-mance on vast benchmarks, but pre-train or fine-tune a LLM requires numerous computing re-sources. Therefore, it's crucial to study how to inference a LLM to reach the ceiling of its capacity. CoT is a ideal approach which has been proved that can obviously evaluate the performance of LLMs' inference. Among that,self-consistency is one of the best CoT approach.\nOur method effectively reduce the inference times of multi-path inference, thereby reducing the deploy budgets of self-consistency. We believe our approach can further elevate the effectiveness of multi-path inference, thereby further improving the effectiveness of LLM."}, {"title": "A Dataset", "content": "Our majority dataset are composed of three different kinds of inference tasks. 1) arithmetic reasoning: SingleEq (Koncel-Kedziorski et al., 2015), AddSub (Hosseini et al., 2014), MultiArith (Roy and Roth, 2016), GSM8K (Cobbe et al., 2021), AQUA (Ling et al., 2017), and SVAMP (Patel et al., 2021). 2) symbolic reasoning: Last Letters, Coin Flip (Wei et al., 2023), and Object Tracking, Bigbench Date. 3) commonsense question answering: CommonsenseQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021). For more details about the dataset please refer to (Wang et al., 2023).\nB Uesage of LLM.\nWe utilize LLM to rectify grammar errors.\nC Computing Resources\nOur experiments were run on a computer cluster with 32GB RAM, 4-Core CPU, and NVIDIA-A100 (80G, 32G)/NVIDIA-V100 (32G) GPU, Linux platform.\nD Source Code.\nWe have provided source code for reference. Additionally, our code are based on https:// github.com/amazon-science/auto-cot and refer to the coding manner from https://github.com/ eureka-research/Eureka.\nE Proof of theorem 3.1.\nSubsequently, we prove the existence of Nash equilibrium in this system. For any two given polices $\\pi_1 \\in \\Pi$ and $\\pi_2 \\in \\Pi$ We first define the pay-off of $\\pi_1$ and $\\pi_2$ as $R(\\pi_1; \\pi_2)$ and $R(\\pi_2; \\pi_1)$:\n$R(\\pi_1; \\pi_2) = P(\\pi_1 \\succ \\pi_2)$\n$R(\\pi_2; \\pi_1) = P(\\pi_1 \\prec \\pi_2)$, (3)\nwe provide the proof of the existence of Nash equilibrium in this system. We define$\\vec{\\pi} = [\\pi_1, \\pi_2]$, $v(\\vec{\\pi}) = [R(\\pi_1; \\pi_2), R(\\pi_1; \\pi_2)]$. According to the Nash equilibrium, it should have to satisfy this relationship:\n$v(\\vec{\\pi^*})(\\vec{\\pi^*} - \\vec{\\pi}) \\leq 0$ (4)\nSubsequently, refer to Munos et al., we can learn that if we want Equation 4 holds true, we just have to guarantee Equation 5 holds true.\n$(v(\\vec{\\pi}) - v(\\vec{\\pi'}))( \\vec{\\pi} - \\vec{\\pi'}) \\leq 0$, (5)\nwhere $\\vec{\\pi}$ and $\\vec{\\pi'}$ are any two given policy set. Subsequently, we can further darrive at the following"}, {"title": "relationships:", "content": "$(v(\\vec{\\pi}) - v(\\vec{\\pi'}))(\\vec{\\pi} - \\vec{\\pi'}) = (\\frac{R(\\pi_1; \\pi_2) - R(\\pi_1'; \\pi_2))}{R(\\pi_2; \\pi_1) - R(\\pi_2; \\pi_1')}) \\cdot (\\frac{\\pi_1 - \\pi_1'}{\\pi_2 - \\pi_2'})$\n= $(R(\\pi_1; \\pi_2) - R(\\pi_1'; \\pi_2)) \\cdot (\\pi_1 -\\pi_1') + (R(\\pi_2; \\pi_1) - (R(\\pi_2; \\pi_1')))\\cdot (\\pi_2 -\\pi_2') $ \n= $ (P(\\pi_1 \\succ \\pi_2) - P(\\pi_1' \\succ \\pi_2)) \\cdot (\\pi_1 - \\pi_1') + \\{2 - (P(\\pi_1 \\succ \\pi_2) + P(\\pi_1' \\succ \\pi_2))\\} \\cdot (\\pi_2 - \\pi_2)$ $  $\\prec P(\\pi_1 \\pi_2')P(\\pi_1'$\\n= $ (P(\\pi_1 \\succ \\pi_2) - P(\\pi_1' \\prec \\pi_2)) \\cdot (\\pi_1 - \\pi_1' - \\pi_2 + \\pi_2') + 2\\cdot (\\pi_2 - \\pi_2)$ $\nP(\\pi_1P(\\pi_1'$$\n= $ (P(\\pi_1 \\succ \\pi_2) - P(\\pi_1' \\prec \\pi_2) - 2) \\cdot (\\pi_2 - \\pi_2)+$\n$P(\\pi_1)$ (6)\nIn particular, we can find that if $\\vec{\\pi} \\equiv \\vec{\\pi'}$ then $(v(\\vec{\\pi}) - v(\\vec{\\pi'}))^T(\\vec{\\pi} - \\vec{\\pi'}) = 0$, thus $\\vec{\\pi} = \\vec{\\pi'}$ is one solution that $\\pi_1$ and $\\pi_2$ has reached equilibrium."}]}