{"title": "THE RELATIONSHIP BETWEEN REASONING AND PERFORMANCE\nIN LARGE LANGUAGE MODELS\u201403 (MINI) THINKS HARDER,\nNOT LONGER.", "authors": ["Marthe Ballon", "Andres Algaba", "Vincent Ginis"], "abstract": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging\nchain-of-thought and test-time compute scaling. However, many open questions remain regarding the\ninterplay between reasoning token usage and accuracy gains. In particular, when comparing models\nacross generations, it is unclear whether improved performance results from longer reasoning chains\nor more efficient reasoning. We systematically analyze chain-of-thought length across 01-mini and\no3-mini variants on the Omni-MATH benchmark, finding that 03-mini (m) achieves superior accuracy\nwithout requiring longer reasoning chains than 01-mini. Moreover, we show that accuracy generally\ndeclines as reasoning chains grow across all models and compute settings, even when controlling\nfor difficulty of the questions. This accuracy drop is significantly smaller in more proficient models,\nsuggesting that new generations of reasoning models use test-time compute more effectively. Finally,\nwe highlight that while 03-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by\nallocating substantially more reasoning tokens across all problems, even the ones that 03-mini (m)\ncan already solve. These findings provide new insights into the relationship between model capability\nand reasoning length, with implications for efficiency, scaling, and evaluation methodologies.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have evolved from handling basic natural language processing tasks to solving complex\nproblems [1, 2, 3, 4]. Scaling model size, data, and compute [5] has enabled larger models to develop richer internal\nrepresentations [6, 7] and emergent capabilities [8]. Recently, a new class of reasoning models has emerged that\ncouples reinforcement learning with test-time compute scaling [9, 10]. These models leverage reasoning tokens to\nguide the chain-of-thought process and maintain coherence throughout complex problem-solving tasks [11, 12, 13]. By\nexplicitly optimizing the chain-of-thought in the reasoning tokens during training [14] and iteratively refining outputs at\ninference, these models achieve superior performance, including on challenging mathematical benchmarks [15, 16].\nMoreover, new test-time scaling laws demonstrate that longer reasoning-i.e. more reasoning tokens-yields log-linear\nperformance gains [9].\nIn this paper, we examine whether more capable models within a single family (o-series of OpenAI) require a longer\nchain-of-thought to achieve higher performance or if they can reason more effectively. By systematically comparing the\nnumber of tokens in the chain-of-thought generated by 01-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH\ndataset [17], we find that more proficient models (01-mini vs. 03-mini (m)) do not generate longer reasoning chains"}, {"title": "Results", "content": "Our data consists of 4428 Olympiad-level math problems, the Omni-MATH benchmark, together with a reference\nanswer and relevant metadata fields Domain and Difficulty (Appendix Figs. A1 and A2). We consider six elementary\nmathematics domains, Algebra, Applied Mathematics, Calculus, Discrete Mathematics, Geometry and Number Theory\n(Appendix Fig. A3) and divide the data into four difficulty tiers, Tier 1, Tier 2, Tier 3 and Tier 4 (Appendix Fig. A4)."}, {"title": "Accuracy and relative token usage across models, disciplines, and difficulties", "content": "Figs. 1 and 2 show the accuracy of OpenAI models gpt-40, 01-mini, o3-mini (m) and 03-mini (h) across disciplines\nand difficulty tiers. The gpt-4o model performs consistently between 20% and 30% for all math disciplines but clearly\nlags behind the three reasoning models. 01-mini significantly improves accuracy in all categories, reaching 40-60%\non all domains. The introduction of 03-mini (m) further enhances performance, achieving 50% in all categories. The\no3-mini (h) model improves with approximately 4% on average compared to 03-mini (m) and surpasses 80% accuracy\nfor Algebra and Calculus. A notable outlier is Discrete Mathematics, where performance deviates from the overall\ntrend for all models. In general, accuracy declines as tier level increases. An exception is observed in gpt-40, which\nperforms better on Tier 4 than on Tiers 2 and 3 (the reasoning models also perform slightly better on Calculus Tier 3\nthan Tier 4). This anomaly suggests that the model might leverage unexpected heuristics or struggle disproportionately\nwith mid-tier complexity.\nBesides indicating accuracy (via the colors of the progress bars), Fig. 2 also shows relative use of reasoning tokens (via\nthe length of the progress bars) across the Omni-MATH dataset for 01-mini, o3-mini (m), and o3-mini (h). The relative\nuse of tokens increases with the level of difficulty for all models, highlighting the need for computational resources for\nmore difficult tasks. Discrete Mathematics stands out as a token-intensive domain, indicating a heavier combinatorial or\nmulti-step reasoning load. Foundational mathematics areas such as Calculus and Algebra tend to consume fewer tokens,\npossibly because they are more procedurally straightforward. Interestingly, we observe that a relatively longer chain of\nreasoning does not generally lead to better performance, as many Tier 4 math problems from token-intensive domains\nremain unsolved. Notable exceptions are the Geometry Tier 3 problems, where all three reasoning models allocate\nmore reasoning compute to Tier 3 than Tier 4 problems, resulting in a higher accuracy for Geometry Tier 3 than Tier 4."}, {"title": "Reasoning token distribution and performance vs. token usage", "content": "Figs. 3 and 4 display the relationship between the number of reasoning tokens and the performance of 01-mini, o3-mini\n(m), and o3-mini (h) on the Omni-MATH dataset (consult Appendix Fig. A5 for gpt-40 analysis with completion\ntokens, which encompass both the tokens leading up to the answer and the answer itself). Fig. 3 shows the proportion\nof the correct (green bars) versus incorrect (red bars) model responses across the reasoning token distribution. The\nred dashed line depicts the conditional error rate, i.e. the probability that the model answers incorrectly given the\namount of used reasoning tokens (see Methods). One first thing to note is that higher performing models have a better\nratio of correct to incorrect answers, even for high token counts. This pattern is also reflected in the conditional error\nrate (red dashed line): the conditional error rate is almost instantly at 50% for 01-mini whereas it takes about 12,000\ntokens for 03-mini (m) and 30, 000 for o3-mini (h) to reach a 50% error rate. A second thing to note is that the token\ndistributions of o1-mini and o3-mini (m) are very similar. Fig. 4b together with the left QQ-plot in Fig. A6 further\ninvestigate this behavior by comparing the distribution of the reasoning tokens only for the questions that the models\nanswered correctly. Indeed, the almost identical token distributions show that o3-mini (m) does not use more reasoning\ntokens to achieve its superior performance to o1-mini on Omni-MATH. This suggests that o3-mini (m) reasons more\neffectively. The token distribution of o3-mini (h) spans a significantly wider range of values, with the model allocating\nover 50, 000 reasoning tokens for some math problems. In addition, the right QQ-plot in Fig. A6 shows that o3-mini (h)\nuses more reasoning tokens to solve all (correctly answered) questions, indicating that the small accuracy gain of 4%\ncompared to 03-mini (m) is accompanied by a large extra computational cost.\nThe panels below the histograms in Fig. 3 display the relative proportion of tier levels in each bin. They reveal a clear\ntransition from a region where the majority of the questions come from the lowest tiers to a region where the majority\nof the questions come from the highest tiers. Note that this pattern is visualized by the purple filled histograms in Fig. 3\n(higher token regions sometimes have insufficient data counts to show this pattern). The gradient confirms that more\ncomplex questions systematically demand greater reasoning depth, which is in line with prior observations. Fig. 4a\nshows that the average accuracy decreases with increasing use of reasoning tokens for all three models, but that this\ntrend is the most pronounced for 01-mini and smaller for o3-mini (m) and 03-mini (h). While this could be attributed to\nhigher-tier questions requiring more tokens, Fig. 4c shows that the trend remains even when stratifying by tier level. In\nFig. A7, we show this also holds when stratifying across domains. This suggests that increased token usage, rather\nthan question complexity alone, is related to accuracy. We use a logistic regression to quantify the effect size of using\nadditional reasoning tokens on the probability of answering a question correctly, controlling for different levels of\ndifficulty and domains (see Methods). We report the average marginal effects in Appendix C. The accuracy decrease per\n1000 reasoning tokens is 3.16% for 01-mini, 1.96% for o3-mini (m), and 0.81% for o3-mini (h). These results indicate\nthat while deeper reasoning is necessary for solving complex problems, there is a diminishing return, where excessive"}, {"title": "Discussion", "content": "By systematically comparing the number of tokens in the chain-of-thought generated by o1-mini, o3-mini (m), and\no3-mini (h) on the Omni-MATH dataset [17], we find two important results. First, more proficient models (o1-mini vs.\no3-mini (m)) do not require longer reasoning to achieve higher accuracy. Second, while accuracy generally declines with\na longer chain-of-thought, this effect is notably smaller in more proficient models, underscoring that \u201cthinking harder\"\nis not the same as \u201cthinking longer\". A possible hypothesis for this accuracy drop is that models tend to reason more on\nproblems they cannot solve. Another possibility is that longer reasoning chains inherently have a higher probability of\nleading to a wrong final solution, highlighting the need for mathematical benchmarks with reference reasoning templates.\nA practical takeaway from our study is that constraining the chain-of-thought (by setting max_completion_tokens)"}, {"title": "Methods", "content": "We describe our experimental setup and provide the data processing details necessary to replicate our analysis. At the\nend of this section, we elaborate on the regression analysis conducted to analyse the effect size of increased reasoning\ntoken usage on accuracy.\nThe Omni-MATH dataset The Omni-MATH benchmark [17] contains Olympiad-level math problems specifically\ndesigned to test the reasoning abilities of Large language models. Each entry in the dataset consists of a problem, an\nexact answer, and a written out solution together with the following metadata fields: Domain, Difficulty, and Source\n(see Fig. A1). Each problem has between one and three domains of the form Mathematics \u2192 Primary domain \u2192\nwith a maximum length of five. In this paper, we only take the primary domains into account, as a more granular\nclassification gives rise to very imbalanced or underpopulated classes. Fig. A3 shows the number of math problems\nper (primary) domain where we follow [17] in double- or triple-counting the multi-domain questions. We made sure\nto delete the duplicate entries, e.g. some data entries had multiple domain trees but the same primary domain. Every\ndomain-specific analysis in the paper follows this convention. Finally, we joined the Calculus and Pre Calculus class\nand deleted the Other class to obtain a more balanced domain distribution. Math problems are also classified according\nto difficulty level as presented in Fig. A4. We divide the data into difficulty tiers based on the quartiles of the difficulty\ndistribution (without separating difficulty levels).\nOpenAI models We evaluate the performance of the several OpenAI models that are affordable for most users:\ngpt-40-06-08-2024, 01-mini-12-09-2024, 03-mini-31-01-2025 medium (default) and 03-mini-31-01-2025 high. The\no3-mini high model, instead of medium, is obtained by setting reasoning_effort to high. We feed each model the\nmath problems using the Batch API with the following vanilla prompt as user message:\nSolve the following problem. Enclose the final answer in a \\boxed{{}} environment. Problem: {problem}\nFurthermore, we set max_completion_tokens limits of 25, 000 for 01-mini and o3-mini medium, and a 100,000\ntoken limit for o3-mini high. Each reasoning model refused to answer a few questions (flagged as invalid prompts),\nwhich were subsequently omitted from the analysis.\nOmni-Judge To correct the responses of the four OpenAI models on the Omni-MATH dataset, we employ another\nLarge language model called Omni-Judge (KbsdJames/Omni-Judge). Omni-Judge is an efficient and low cost open-\nsource math-evaluation model developed by the authors of [17]. The model is trained to assess the correctness of an\nanswer generated by an LLM, given the problem and a reference answer (see Fig. A2). Table 9 in [17] shows that\nOmni-Judge is 91.78% consistent with gpt-4o as a judge (who is 98% consistent with human evaluators) and has almost\na 100% success rate of correctly parsing model generated answers. To judge the models' generated answers, we make\nrequests to the chat completions endpoint of the kbsdjames.omni-judge API by running the model in LM Studio.\nWe use the same few-shot prompt as in [17] and set the max_new_tokens parameter to 300. In the very few cases\nwhere Omni-Judge fails to parse the model output (<1%), we omit that question from the performance evaluation.\nConditional probability The conditional probability appearing in Fig. 3 and Fig. A5 is computed using a full\nBayesian model with uninformative priors (we assume that P(False) = P(True) = 0.5). In particular, we have that\n$$P(False | T > B_i) = \\frac{P(T > B_i | False)}{P(T > B_i | False) + P(T > B_i | True)},$$ (1)\nwhere {T > Bi} is the event that the number of tokens exceeds the right bin threshold and \"False\" indicates that the\nmodel answered incorrectly. Because Bi can only take a finite number of values, we have that\n$$P(False | T > B_i) = \\sum_{k=i+1}^n \\frac{P(T \\in B_k | False)}{P(T \\in B_k | False) + P(T \\in B_k | True)} = \\sum_{k=i+1}^n \\frac{\\text{False} \\in B_k}{\\text{False} \\in B_k | + |\\text{True} \\in B_k|},$$ (2)\nwhich can be easily computed using the stacked histogram data."}, {"title": "Estimating effect sizes", "content": "We use a logistic regression to estimate the effect of additional reasoning tokens on the\nprobability of an accurate response on a question $Y_i$, while controlling for different levels of difficulty and domains.\nThe regression takes the following form:\n$$\\log\\left(\\frac{Pr(Y_i = 1)}{Pr(Y_i = 0)}\\right) = \\beta_0 + \\beta_1\\text{tokens}_i + \\sum_{k=1}^{K-1} \\delta_k \\text{difficulty tier}_{k(i)} + \\sum_{m=1}^{M-1} \\gamma_m \\text{domain}_{m(i)},$$ (3)\nwhere $i$, $k$, and $m$ denote the question-response pair, the difficulty tier, and the domain, respectively. Moreover, $k(i)$\nand $m(i)$ indicate that the difficulty tier $k$ and domain $m$ depend on the question-response pair $i$. The difficulty tier and\ndomain fixed effects can be estimated by including dummy variables, which are equal to one if the difficulty tier or\ndomain is equal to the difficulty tier or domain of the current question-response pair and equal to zero otherwise, with\nthe exclusion of a reference category (i.e., $K - 1$ and $M - 1$). The reference category for difficulty tiers is the lowest\ndifficulty Tier 1 and for the domains it is Algebra. We obtain similar results when using the more fine-grained difficulty\nlevels (0 \u2013 10) instead of difficulty tiers.\nTo facilitate interpretation, we compute the Average Marginal Effect (AME) of additional reasoning tokens on the\nprobability of an accurate response. Unlike the raw logistic regression coefficients, which are expressed in log-odds, the\nAME directly quantifies the effect of an additional token in probability terms. Specifically, it represents the average\nchange in the probability of accuracy for a one-token increase, while holding difficulty tier and domain constant. By\ncomputing AMEs, we ensure that our estimates account for the full distribution of difficulty levels and domains, rather\nthan relying on effects evaluated at a single reference point."}]}