{"title": "NL-EYE: ABDUCTIVE NLI FOR IMAGES", "authors": ["Mor Ventura", "Michael Toker", "Nitay Calderon", "Zorik Gekhman", "Yonatan Bitton", "Roi Reichart"], "abstract": "Will a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-EYE, a benchmark designed to assess VLMs' visual abductive reasoning skills. NL-EYE adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of hypothesis images based on a premise image and explain their decisions. NL-EYE consists of 350 carefully curated triplet examples (1,050 images) spanning diverse reasoning categories: physical, functional, logical, emotional, cultural, and social. The data curation process involved two steps-writing textual descriptions and generating images using text-to-image models, both requiring substantial human involvement to ensure high-quality and challenging scenes. Our experiments show that VLMs struggle significantly on NL-EYE, often performing at random baseline levels, while humans excel in both plausibility prediction and explanation quality. This demonstrates a deficiency in the abductive reasoning capabilities of modern VLMs. NL-EYE represents a crucial step toward developing VLMs capable of robust multimodal reasoning for real-world applications, including accident-prevention bots and generated video verification.", "sections": [{"title": "1 INTRODUCTION", "content": "Abductive reasoning refers to the ability to infer and predict plausible outcomes or causes given a context scene Peirce et al. (1934); Fann (2012); Douven (2021). This reasoning skill is crucial for Visual Language Models (VLMs), as they are likely to become increasingly integrated into our daily lives (Yildirim et al., 2024; Anwar et al., 2024; Chiang et al., 2024; Shah et al., 2023). These models will be required to accurately monitor and interpret daily life scenes and correctly infer plausibility to prevent accidents and provide timely advice. For instance, would a bot warn us from slipping on a wet floor when there is no warning sign? or would it infer a missing pacifier as a cause of a crying baby?\nAlthough this capability is critical, previous work has mainly evaluated VLMs in a single scene setting such as visual entailment or detecting improbable events like a fire in a closed jar or in sequential scenes, such as next-frame prediction Xie et al. (2019); Fu et al. (2022); Hessel et al. (2022); Fu et al. (2024); Ganz et al. (2024); Yarom et al. (2024); Kadiyala et al. (2024). Consequently, it remains unclear to what extent existing VLMs are capable of abductive reasoning.\nTo address this, we introduce NL-EYE, a benchmark designed to evaluate visual abductive reasoning capabilities of VLMs across multiple images. NL-EYE is inspired by the textual abductive NLI task Bhagavatula et al. (2019) and applies it to the visual domain. In NL-EYE, a VLM is presented with a premise image and one or two hypothesis images. It then needs to infer how likely (plausible) a hypothesis image is to result from or lead to the premise image. The plausibility evaluation can be either done individually or in comparison to an alternative hypothesis. For instance, in Figure 1, the VLM needs to infer that, given the broken leg in the context image, it is more likely that the man slipped on the wet floor which lacked a warning sign (i.e., selecting hypothesis image 1)."}, {"title": "2 THE NL-EYE BENCHMARK", "content": "Beyond plausiblity prediction, NL-EYE facilitates the evaluation of the models' capability to provide faithful explanations. This allows us to explore whether they are correct for the right reasons rather than relying on shallow heuristics McCoy et al. (2019). For example, a valid explanation for the broken leg scene would suggest that the presence of a warning sign would have made the man more alert, thereby potentially preventing the accident. In contrast, a shallow explanation might suggest that the man was simply resting on a cozy rainy day.\nEach NL-EYE example features a premise image alongside two hypothesis images, annotated with a gold label indicating the index of the more plausible hypothesis. The examples also include a gold explanation detailing why the chosen hypothesis is more plausible than the alternative. Each example is categorized into one of six reasoning categories - physical, logical, emotional, functional, cultural, and social \u2013 and includes temporal annotations that specify whether the hypotheses occur before, after, or simultaneously with the premise, and whether the time duration between the premise and hypothesis scenes is short or long. This rich annotation aids in diagnosing current VLMs and highlights their strengths and weaknesses.\nTo create NL-EYE, we collected a large pool of high-quality textual scenes created by experienced human annotators. The resulting scenes were then provided to professional designers who utilized Midjourney and DALL-E (Ramesh et al., 2021) to synthesize the corresponding images. The designers are also tasked with categorizing each example and creating the explanation that is used as the gold label. The image generation process was iterative, requiring multiple attempts to ensure consistency between the textual descriptions and the visual scenes, as well as visual coherence among the images within the same triplet. This process resulted in a total of 1,050 generated images, yielding 350 image triplets. Overall, NL-EYE is characterized by carefully curated examples, offering high quality both in terms of the scenarios and the consistency and quality of the images.\nThe first analysis is human evaluation where annotators select the more plausible hypothesis and explain their choice. Our results indicate that humans successfully identify the more plausible hypothesis in 85% of the cases. Furthermore, in our assessment of the quality of the human-generated explanations, we find that in 94% of the cases where the correct hypothesis was selected, the humans also provided a valid explanation. This demonstrates that humans perform reasonably well on the NL-EYE tasks.\nNext, we design a comprehensive study to evaluate the abductive reasoning abilities of modern VLMs. We take multiple measures to ensure the robustness of our evaluation, including addressing sensitivity to the order in which hypotheses are presented and exploring various input strategies, such as feeding the model three separate images or presenting it with a single combined-image that composites all three. Since real-world scenarios may not always provide two alternatives, we also evaluate the model's ability to assign a plausibility score to a single hypothesis, in addition to comparing two candidates. We have also developed a framework that utilizes a text-based baseline that processes textual descriptions of visual scenes. Specifically, we compare the results with gold descriptions and with the captions of the images as generated by the VLMs. Lastly, evaluating model-generated explanations is challenging, as comparing generated text to a single reference (gold) explanation can be limiting and may not capture the variety and validity of possible correct answers. To address this, we adopt the evaluation proposed by Bitton-Guetta et al. (2023): human annotators are presented with an image triplet where the correct hypothesis is already labeled and select valid explanations from a provided set.\nOur results show that while humans perform well on NL-EYE, VLMs struggle, with most models failing to surpass a random baseline in the plausibility prediction task. Even when identifying the plausible hypothesis, VLMs fail to provide accurate explanations in over 50% of cases, revealing a major weakness in their abductive reasoning. Furthermore, our text-based experiments indicate that these models often succeed in textual reasoning even when they fail to reason over images. Interestingly, when we prompt the VLMs to generate image captions, the resulting captions prove ineffective for solving the task. Consequently, we hypothesize that the VLMs reasoning is hindered by inaccurate visual interpretations. We also find that these models are sensitive to the order in which the hypotheses are presented and to the input format (three separate images vs a single combined-image). This sensitivity is concerning, as it raises the possibility that the models may not genuinely understand the underlying concepts, potentially relying on superficial cues to make decisions.\nTo summarize, we introduce NL-EYE a carefully curated benchmark designed to test the abductive reasoning abilities of VLMs across various categories and temporal relations. We then conduct a comprehensive study evaluating modern VLMs on NL-EYE and find notable deficiencies in their abductive reasoning capabilities. We believe NL-EYE represents a crucial step toward enhancing the reasoning abilities of VLMs, moving them closer to truly understanding complex, real-world scenarios and providing more reliable and interpretable outputs."}, {"title": "2.1 TASKS", "content": "Our objective is to explore and benchmark the abductive reasoning capabilities of modern VLMs. Unlike much of the previous work in NLP, our focus is on reasoning solely based on visual inputs: premise and hypothesis images."}, {"title": "2.2 BENCHMARK STRUCTURE AND CATEGORIZATION", "content": "In this subsection, we describe the structure of each example in our benchmark and discuss the taxonomy we proposed for categorizing the examples. In Figure 2, we present the structure of two examples, which contains: (1) the premise image; (2) two hypothesis images; (3) the label, which indicates the more plausible hypothesis and is given by the benchmark designers; (4) the textual descriptions of the three images that were used for generating the images; (5) the gold explanation, which clarifies why the correct hypothesis is more plausible, and is written by the benchmark designers; (6) reference explanations, which were written and validated by crowd-workers; (7) categorization of the example, which indicates the involved reasoning category, temporal direction and duration.\nIn \u00a73, we describe the data creation process and specifically elaborate on components (1)-(5). The crowd-worker annotations (component 6) are detailed in \u00a74.2. We next outline our proposed categorization, which serves two purposes: first, to ensure our benchmark is diverse, balanced, and covers a wide range of domains and reasoning types; and second, to aid in diagnosing areas where VLMs fall short.\nReasoning Categories We identify six different categories: Physical, Logical, Emotional, Functional, Social, and Cultural, ranging from physical reasoning (e.g., predicting the color of a rotten banana) to cultural reasoning (e.g., determining if a habit like wearing house shoes implies another cultural trait, such as owning Matryoshka dolls).\nTemporal Categories The temporal categories are based on direction and duration. Temporal direction refers to the logical relationship between the premise and hypothesis, indicating whether the event depicted in the premise image causes the hypothesis event (forward), is caused by it (backward), or if the events occur simultaneously (parallel). Examples that do not occur at the same time and are not categorized as parallel can also be classified by temporal duration, which is determined by the time gap between the events depicted in the premise and hypothesis images. These include short-term \u2013 when the events occur close in time, possibly in the same physical environment, or with no significant sequence of events separating them, and long-term \u2013 when the events take"}, {"title": "3 DATA CURATION", "content": "Joining recent efforts in evaluating VLMs with an emphasis on the quality of test sets over their sheer size (Thrush et al., 2022; Bitton-Guetta et al., 2024; Bitton et al., 2023; Bitton-Guetta et al., 2023; Padlewski et al., 2024), we carefully curated 350 test set examples. The creation process of NL-EYE required human involvement at every key step enabling the creation of diverse, high-quality examples tailored to the evaluation's specific goal.\nNL-EYE is a multi-image benchmark consisting of daily life scenes. A \u201cscene\u201d refers to a specific setting where objects, people, and actions are arranged in a particular context, which can be represented either textually or visually. The benchmark includes both representations, and the following key steps in its creation process: (1) textual description writing, (2) image generation, and (3) explanation and categorization.\nTextual Descriptions Scenes were manually crafted by a group of 20 annotators who were tasked with creating triplets consisting of a premise scene, and two hypothesis scenes, while one is more plausible than the other. Each annotator had the flexibility to develop hypotheses across diverse reasoning categories, time directions, time durations, and domains. Annotators' creativity and experiences generated unique, everyday scenes that are often undocumented or scattered, making it hard to gather automatically similar data. We manually filtered scenes from the suggested pool based on several key criteria: (1) premise necessity, ensuring the scene is essential for determining the more plausible hypothesis; (2) visual relevance, guaranteeing the scenes can be effectively communicated visually; and (3) uniqueness, verifying that we do not replicate similar existing examples or logical patterns. We also applied preferences for receiving a range of challenges, from easy to difficult, as well as diverse time shifts, including varying directions and durations. After applying these filters, we retained 75% of 450 suggested ideas."}, {"title": "4 EXPERIMENTAL SETUP", "content": "Image Generation The images in NL-EYE were manually curated by two of the authors (noted as \"the designers\"), who have experience with text-to-image models. This careful generation process ensures high-quality images and verifies consistency, alignment between text and images, and overall clarity. The images were generated based on the textual descriptions using mainly Midjourney and DALL-E 3 (Ramesh et al., 2021). During the prompt augmentation phase, The designers had the option to utilize assistance from Gemini and GPT-4 (Achiam et al., 2023) to transform the human-generated concise descriptions from step 1, into more detailed prompts with specific visual elements, enhancing visual consistency. Once the revised prompts were verified to ensure they don't interfere with the essential content  and manual edits were made if necessary, the prompts were ready for image generation.\nTypically, the process begins with generating the premise image. Image generation is an iterative process, involving repeated cycles of manual editing and image-to-image alignment until high quality and consistency are achieved. The image generation phase produces photorealistic images that are visually consistent, meaning that objects, people, and environments appearing in one image of the triplet are the same as in the others. The last guideline arises from the crucial need to exclude style from reasoning considerations in the future evaluation of VLMs on the task. Technically, visual consistency is achieved not only through prompt augmentations but also via inpainting (i.e., editing a specific region of an image using a textual prompt), image-conditioned prompting (generating an image while conditioning on another image), and using the same seed (initial noise distribution number) for all triplet images.\nExplanation For each example, the designers wrote gold-standard explanations. The gold explanation represents the original reasoning behind the scenes at the time of their curation. The gold explanation clarifies why the correct hypothesis is a more plausible outcome or cause of the premise. It often follows a pattern like \u201cUsually, X tends to Y\u201d or \u201cBecause X made Y to...\". Naturally, the explanation written at this stage is not the only possible explanation of the reasoning. Humans can suggest multiple plausible explanations and stories to justify connections between observations, also even for less likely scenarios.\nValidation and Categorization The validation process consists of two key checks: (1) image-text alignment and (2) plausibility validation. First, we ensured that the images were correctly aligned with their corresponding texts. Second, we qualitatively assessed each example's plausibility, evaluating how difficult it is to understand the intended meaning and make any necessary adjustments. This process includes manual verification by the designers, supported by a human baseline with a high human success rate on the plausibility prediction task and strong inter-annotator-agreement confirms the clarity.In addition, the designer classified the examples into relevant categories, as outlined in the previous section.\nDataset Statistics NL-EYE is categorized by reasoning categories, domains and temporal information. The Logical, Social, Physical, Cultural, Functional, and Emotional reasoning categories comprise 28%, 24.6%, 17.8%, 14.3%, 7% and 8%, respectively, of the benchmark examples. 78% of the examples are in the time duration of short-term, divided mainly with the forward direction. The long-term examples are 22% while 27% of them are also associated with the backward direction.\""}, {"title": "4.1 TASKS AND SETUPS", "content": "Recall that the NL-EYE benchmark includes two tasks: Plausibility Prediction and Plausibility Explanation. Both tasks require reasoning about the relationship between the context image, the premise, which is denoted as \\(I_p\\), and a candidate image, a hypothesis, denoted by \\(I_H\\). In addition to the images, the model f receives a textual query (a prompt) that contains instructions describing the task it should perform. We introduce two setups for solving the tasks: the Triplet setup and the Pairs setup.\nThe Triplet Setup which is illustrated in the left box of Figure 5 the model receives the query along with three images: the premise (\\(I_p\\)) and two candidate hypotheses (\\(I_{H1}\\) and \\(I_{H2}\\)). In the prediction task, the model's goal is"}, {"title": "4.2 EVALUATION", "content": "Predictions in the Triplet Setup At first, we evaluated models based on accuracy. However, we found that all models are sensitive to the positioning (in the all-in-one strategy) or the order (in the sequential strategy) of the hypothesis images, that is, whether \\(I_{H1}\\) is placed or fed before or after \\(I_{H2}\\). For example, models may perform differently when given Triplet A versus Triplet B from Figure 5. To address this sensitivity, we provide predictions for both orders of the hypotheses and then aggregate the two predictions. A prediction is considered correct if and only if the model selects the correct hypothesis in both orders. This approach reduces the likelihood of a correct prediction by chance and ensures the model demonstrates consistency. The performance score in the triplet setup is the described consistency accuracy (proportion of correct and consistent predictions; see \u00a7A.1).\nPredictions in the Pairs Setup In the pairs setup, we aim to evaluate the plausibility score predicted by the model. However, as previously mentioned, we do not want to constrain the model (or developers) to produce a specific score or adhere to a specific scoring function. Therefore, we do not support direct evaluation of the score, i.e., we do not provide a gold standard score against which the predicted score is compared. This raises the question: how do we plan to evaluate models in the pairs setup? The only assumption we require from the scoring function is order-faithfulness : if for a given premise \\(I_p\\), the evaluated model m scores"}, {"title": "4.3 MODELS AND BASELINES", "content": "Table 1 outlines the models used in our study, detailing their configuration, reasoning approach, and input strategy (specific versions in Appendix Table 4). Below, we provide more details on the models and baselines.\nVLMs We employ state-of-the-art closed source VLMs, including, Gemini-1.5-pro (Google, 2024), GPT-4-vision and GPT-40 (Achiam et al., 2023), and Claude-Sonnet-3.5 and Claude-Opus-3 (Anthropic, 2024). In addition, we employ open-source VLMs, including LLaVa 1.6 (Liu et al., 2024) and Fuyu (Bavishi et al., 2023).\nNLI models Recall that in the text-only reasoning approach, we provide the model with the gold text descriptions (used to generate the images) and ask it to predict which hypothesis description is more plausible. The predictor models we use in this approach include all the closed-source VLMs mentioned above, as well as fine-tuned NLI models such as DeBERTa-v3 and BART-L. When the predictor is an LLM, we ask it to determine which hypothesis description is more plausible given the textual premise description. For fine-tuned NLI models, we compute two 'entailment' scores between the premise and each hypothesis, and the final prediction is made based on the hypothesis with the higher score.\nRandom baselines We present two simple baselines. The first is the random baseline, which randomly selects a hypothesis in the triplet setup or assigns a random score in the pairs setup. However, it is inconsistent due to its sensitivity to hypothesis order. To improve consistency, we introduce the dumb pixel baseline, which selects a hypothesis or assigns a score based on a predefined rule using the upper-leftmost pixel. For example, the hypothesis with the brighter pixel is deemed more plausible, or the score is calculated from the pixel's value.\nH umans Currently, there are indications that models can perform inference tasks at a level comparable to, or even exceeding, that of humans. Accordingly, we want to investigate whether these VLMs can match human performance on our tasks that appear straightforward for humans and expect them to succeed. To this end, we recruited 15 crowd-workers on the AMT platform. Pre-qualifications for workers were high approval rates and English-speaking countries."}, {"title": "5 RESULTS", "content": "Formally, the consistency-accuracy (triplet accuracy) is:\n\\[\nConsistency Acc.(I_P, I_{H1}, I_{H2}, H_{gold}) = \\begin{cases}\n1, & \\text{if } f(I_P, I_{H1}, I_{H2}) = f(I_P, I_{H2}, I_{H1}) = H_{gold}\\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\nFormally, the order-faithful accuracy (pairs accuracy) is:"}, {"title": "5.1 VLMS FAIL TO PERFORM ABDUCTIVE REASONING WITH IMAGES", "content": "Table 2 presents the performance of humans, VLMs, and baselines on both tasks, prediction and explanation, for different setups and input strategies. For detailed results, refer to Appendix \u00a7B.1 and \u00a7B.2, where we compare human performance to VLM performance and analyzing their alignment.\nVLMs Fail Where Humans Excel The results reveal a large performance gap between humans and VLMs on both tasks. Except GPT-40, which achieves 60% accuracy in the triplet setup for combined-image inputs, all VLMs perform worse than the dumb pixel baseline. The situation is even more concerning for current open-source VLMs, such as LLaVA 1.6 and Fuyu, which score below random baselines. In contrast, human participants achieve 83-85% accuracy in the prediction task and 95% in the explanation task. Notably, the participants are crowd-workers who are not experts or highly skilled. This suggests that the task is neither unsolvable nor particularly difficult. Rather, current VLMs lack the visual abductive reasoning capabilities necessary to solve it effectively. Importantly, the finding that tasks easily handled by humans pose significant challenges for VLMs underscores the relevance of our benchmark and highlights areas where the research community can focus its efforts. In addition, we found that VLMs are better in comparative or relative judgment setups (triplet, selecting which hypothesis is more plausible than the other) than in absolute judgment (pairs, predicting a plausibility score for a single hypothesis). This is unsurprising, as it is a known and well-studied phenomenon of humans which was also observed in LLM-as-a-judge tasks. To the best of our knowledge, we are the first to show it for visual reasoning tasks.\nEven When Correct, VLM Explanations Are Unhelpful To assess the quality of the explanations, we conduct human and automatic evaluations. Since nearly half of the predictions are incorrect, we focused only on explanations for correct predictions, ensuring we can determine whether an explanation is genuinely poor rather than simply a result of the model failing to predict the correct answer. Note that in \u00a76 we provide a qualitative analysis of explanations of wrong predictions to better understand why they fail. As can be seen, humans almost always produce correct explanations, as 95% of the explanations were selected by the annotators. On the other hand, at best, only half of the explanations are selected. This demonstrates that even when the VLMs predict correctly, the explanation is unhelpful. In addition, we used valid human explanations as references for the automatic evaluation, which produces scores similar to those of the human evaluation in most cases. The automatic evaluation suggests that VLMs produce explanations that describe different reasoning than humans."}, {"title": "5.2 REASONS FOR FAILING TO REASON", "content": "This subsection presents experiments and analyses to explore why VLMs fail at visual abductive reasoning.\nVLMs Can Perform Textual Reasoning \u2013 The Failure is in Visual Interpretation Table 3 presents the results of text-based experiments aimed at decoupling the textual reasoning capabilities from the visual ones. In the text-"}, {"title": "6 VLM FAILURE ANALYSIS", "content": "This section presents a qualitative analysis of VLMs' explanations for wrong predictions. We qualitatively analyze 120 explanations \u2013 40 from each VLM: Gemini 1.5, GPT-4 and Claude 3.5.\nWe identify five main factors contributing to the models' failures: (1) style & consistency: When irrelevant visual details influence the decision; (2) time: When the explanation relies on incorrect time direction or duration; (3) ignoring key details: Overlooking important information; (4) missing knowledge: Misinterpreting key details despite recognizing them; (5) failed comparison: Justifying a less plausible hypothesis with logical reasoning.\nAs Figure 7 shows, all models struggle with understanding temporal progression. Notably, Claude often relies on style considerations, with 30% of its errors resulting from this factor, indicating an overemphasis on irrelevant visual details. Both Gemini (32%) and GPT-4 (25%) frequently miss key details, suggesting recognition gaps. GPT-4 has the highest rate of failed comparisons, often making the incorrect decision at the final plausibility stage. To further understand these failures, researchers can try to interpret models' internal thought processes."}, {"title": "7 RELATED WORK", "content": "Recent advances in multimodal learning have enabled models to integrate textual and visual data across diverse tasks. Powerful visual encoders like CLIP and SigLip, coupled with the progress in LLMs , have given rise to sophisticated VLMs such as BLIP2, GPT-4, and Gemini. These VLMs are pushing the boundaries of multimodal capabilities, tackling tasks like visual question answering (VQA)  and visual entailment (VE). Our work builds on and extends research in these areas, with an emphasis on commonsense reasoning.\nFrom Textual to Visual Entailment A cornerstone of our work is the expansion of Natural Language Inference (NLI), traditionally a text-based task , into the visual domain. While previous research has explored NLI in the context of image-text alignment (e.g., SNLI-VE , TIFA , WYSIWYR , Mismatch-Quest), and even video-text entailment , we introduce a novel framework for image-to-image entailment. This framework goes beyond simply selecting plausible alternatives by requiring models to explain their choices, thus offering a deeper evaluation of their abductive reasoning. Furthermore, we introduce a \"pairs\" setup that requires scoring the plausibility of image pairs, aligning our task more closely with the original formulation of textual entailment.\nSynthetic Data for Multi-Image Reasoning Our work uniquely employs synthetic images generated by models like DALLE, allowing greater control over visual complexity and diversity compared to natural image datasets such as Winoground, Sherlock, and VCOPA . By emphasizing multi-image reasoning, we address limitations in existing datasets that focus primarily on single-image tasks, like WHOOPS! and Visual Riddles. Our approach complements research on synthetic image understanding and is better suited for commonsense reasoning in real-world contexts, enhancing datasets like SEED-Bench  and MMTOM-QA , which tackle different aspects of multi-image reasoning. Unlike ScienceQA and NTSEBench, which focus on diagrams and scientific domains, our dataset employs photorealistic scenes from everyday life, making it more appropriate for evaluating commonsense reasoning. This integration of synthetic data, multi-image reasoning, and image-to-image entailment establishes a new benchmark for assessing VLMs' reasoning capabilities."}, {"title": "8 CONCLUSION", "content": "We introduced NL-EYE, a benchmark designed to assess the visual abductive reasoning capabilities of VLMs across multiple images. This skill is essential for real-world applications, such as accident-prevention bots. We paid special attention to detail in order to ensure that NL-EYE consists of high-quality and challenging examples, which required extensive human involvement at every stage of its curation. Our carefully designed study highlights critical challenges faced by modern VLMs in delivering satisfying plausibility predictions. We demonstrate that although humans perform well on these tasks, VLMs struggle significantly. This indicates a significant limitation of current models' ability to integrate visual interpretation with logical reasoning. Furthermore, models not only struggle to make correct predictions but also often fail to consistently provide helpful explanations. In future work, we would like to address these gaps, building on our insights to develop new VLM architectures with higher reasoning skills, mirroring human cognitive processes in complex environments."}, {"title": "ETHICS STATEMENT", "content": "The NL-EYE benchmark includes AI-generated images, with the potential presence of unpleasant or insensitive content. While we strive to minimize harmful biases, the inclusion of reasoning based on common sense knowledge and cultural perspectives may introduce further bias, particularly related to social norms. Additionally, the labels in this benchmark are based on consensus from human annotators, whose judgments may be influenced by their own cultural backgrounds, which could amplify bias. We also recognize the challenges related to text-to-image (TTI) copyrights, where the ownership of AI-generated content remains unclear. Researchers are encouraged to carefully consider these ethical and legal concerns when utilizing the benchmark."}, {"title": "REPRODUCIBILITY", "content": "To ensure the reproducibility of our results and promote further research, we will publicly release the NL-EYE benchmark, along with the code. Detailed technical instructions, as well as documentation on how to use and adapt the benchmark, will be provided in a publicly accessible repository. Additional technical details, including model versions and specific configurations used in the experiments, are available in the Appendix (\u00a7A.1). By sharing these resources, we aim to foster transparency and support the research community in advancing the evaluation of VLMs."}, {"title": "A APPENDIX", "content": "A.1 REPRODUCIBILITY AND RESOURCES"}, {"title": "C NL-EYE DATASET CREATION - COMPLEMENTARY INFORMATION", "content": "C.1 REASONING CATEGORIES DEFINITIONS\nPhysical Reasoning. Involves understanding the physical world and how objects interact within it. The scenes include changes in temperature, phase, shadow's location, color, shape, etc. This reasoning is inspired by the spatial-temporal reasoning definition (Deza et al., 2009).\nFunctional Reasoning. Requires an understanding of objects' functionalities and tools' common usage. This type of reasoning involves not just recognizing objects and tools, but also comprehending their intended purposes and how they interact within various contexts. For instance, a hammer is not merely identified by its shape but also by its function of driving nails into surfaces. Functional reasoning allows a model to infer the appropriate use of an object within a given scenario, such as using a knife for cutting or a broom for sweeping.\nSocial Reasoning. Understanding social norms, relationships, and interactions. Social reasoning allows for the comprehension of social norms and etiquette, such as knowing how to greet someone depends on the context. This includes recognizing familial roles, friendships, professional relationships, and the varying degrees of formality and familiarity in interactions.\nEmotional Reasoning. Understanding and interpreting emotions and emotional responses. It refers to the ability to identify a wide range of emotions, including happiness, sadness, anger, fear, surprise, and disgust, and to understand the context in which these emotions arise.\nCultural Reasoning. Involves acknowledging cultural traits and traditions while correctly associating them with their respective cultures. It includes the ability to recognize and interpret cultural symbols, rituals, languages, and behaviors accurately. For instance, it includes understanding that certain gestures may have different meanings in different cultures or that specific holidays and celebrations are unique to particular cultural or religious groups.\nLogical Reasoning. Requires an understanding of general processes and broad commonsense. It enables the analysis of situations, draw inferences, and make decisions based on logical principles and widely accepted knowledge. Logical reasoning involves the ability to follow a sequence of steps to solve problems, recognize patterns, and identify relationships between different pieces of information."}, {"title": "C.2 IMAGE GENERATION STEP", "content": "Text-to-Image prompt. The Text-To-Image prompt (in Midjourney) is consisted of 3 parts, while the last one is optional:\n\u2022 Text description. The textual scene caption, basic or improved.\n\u2022 Photorealistic style. Adding textual styling of photorealistic images by mentioning it is captured with Nikon D850.\n\u2022 Visual consistency. Making an image consistent with another image by setting the same seed number, and referring to the reference image with the flag cref and its conditioning strength with the flag cw ranging from 0-100.\nAll these parts are aggregated into the following template:\n< prompt caption >, captured with a Nikon D850 and a 24-70mm lens at f/2.8-seed <> -cref <> -cw 80\nPrompt augmentation. Augmenting a text description by prompting Gemini or GPT-4 with the following prompt:\nDescribe visually a specific looks of < interacting component1 >, < interacting component2 > and < environment >. keep it short and concise, and avoid NSFW words. and integrate these details into every reference of them in the following captions smoothly and consistently. do not change the content of the captions besides the visual description integrations. return in a JSON format: 1) <first image caption> 2) <plausible second image caption> 3) <implausible second image caption>\nNote: integrate the environment only if it fits the context of the caption.\nFor example: interacting component1: little child, interacting component2: vaccine, environment: nurse room, first image caption: a child gets a vaccine., plausible second image caption: a child cries after getting a vaccine., implausible second image caption: a child smiles after getting a vaccine. Response: improved first image caption: a short curly-haired child wearing a green t-shirt receives a vaccine with a silver syringe in a nurse's room filled with toys. improved plausible second image caption: the short curly-haired child in a green t-shirt cries after receiving a vaccine in the toy-filled nurse room. improved implausible second image caption: the short curly-haired child in a green t-shirt smiles after receiving a vaccine in the nurse's room."}, {"title": "B.2 FAILURE ANALYSIS", "content": "Are Human and Model Difficulties Aligned? In the plausibility prediction task using the triplet setup of the separate-images input strategy, humans and VLMs agree on 55% of the predictions (full comparison). When humans are incorrect, the model's success rate falls to 30%, below random chance. While the model outperforms humans in only 5% of cases, its explanations in these instances are rarely accurate. Furthermore, only 21% of the models' errors overlap with human errors, indicating that humans and models tend to make different types of mistakes."}]}