{"title": "Entropy Controllable Direct Preference Optimization", "authors": ["Motoki Omura", "Yasuhiro Fujita", "Toshiki Kataoka"], "abstract": "In the post-training of large language models (LLMs), Reinforcement Learning from Human Feedback (RLHF) is an effective approach to achieve generation aligned with human preferences. Direct Preference Optimization (DPO) allows for policy training with a simple binary cross-entropy loss without a reward model. The objective of DPO is regularized by reverse KL divergence that encourages mode-seeking fitting to the reference policy. Nonetheless, we indicate that minimizing reverse KL divergence could fail to capture a mode of the reference distribution, which may hurt the policy's performance. Based on this observation, we propose a simple modification to DPO, H-DPO, which allows for control over the entropy of the resulting policy, enhancing the distribution's sharpness and thereby enabling mode-seeking fitting more effectively. In our experiments, we show that H-DPO outperformed DPO across various tasks, demonstrating superior results in pass@k evaluations for mathematical tasks. Moreover, H-DPO is simple to implement, requiring only minor modifications to the loss calculation of DPO, which makes it highly practical and promising for wide-ranging applications in the training of LLMs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have exhibited remarkable performance across various tasks (OpenAI et al., 2023; Dubey et al., 2024). However, large datasets often include data created for various purposes, and the models trained on these datasets are not always suitable for users' specific needs. Additionally, some datasets include malicious text and code related to cyberattacks, posing risks of misuse by humans or the AI itself (Bender et al., 2021; Bai et al., 2022; Ji et al., 2023; Shevlane et al., 2023).\nReinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022) is an effective approach to make an LLM follow human instructions and suppressing undesired outputs. In RLHF, a reward model is trained based on data evaluated according to human preferences. The LLM then learns to maximize rewards, aligning its outputs with human preferences. To prevent significant deviation from the original model, regularization using reverse KL divergence is added to the reward maximization process, and RL algorithms such as PPO (Schulman et al., 2017) are employed.\nHowever, RLHF has issues such as high computational costs, the reliance on a learned reward model, and the inherent instability and hyperparameter sensitivity of RL algorithms. To address these problems, Direct Policy Optimization (DPO) (Rafailov et al., 2023) has emerged and is now widely used. DPO proposes a loss function that directly optimizes the policy through a change of variables, eliminating the need for the reward model and allowing training with a simple binary cross-entropy loss. While more stable and lightweight than RLHF, DPO can optimize the same objective function as RLHF, which involves reward maximization and regularization with the reverse KL divergence. Other types of divergences have also been proposed to"}, {"title": "2 Related Work", "content": "Alignment Language models trained through next-token prediction have rapidly advanced and show strong performance on many tasks in zero-shot or few-shot settings (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2023). Fine-tuning using human preferences and instructions, known as alignment, has proven effective in improving instruction following and reducing harmful outputs (Christiano et al., 2017;"}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Reinforcement Learning from Human Feedbacks (RLHF)", "content": "In the context of LLM training, RLHF is a process of aligning an LLM to human preferences after pre-training, typically consisting of three steps: supervised fine-tuning (SFT), reward modeling, and RL fine-tuning.\nSupervised Fine-Tuning (SFT) SFT is the process of adapting an already pre-trained LLM to specific tasks by optimizing the model parameters using a task-specific dataset. Using high-quality data related to the task, the model is optimized through supervised learning to obtain SFT.\nReward Modeling Next, a reward model is trained to reflect human preferences in RL. Let $r_{\\phi}(x, y)$ be a reward model parameterized by $\\phi$, where x is a prompt and y is a completion. It is typically assumed that human preference for a pair of completions follows the Bradley-Terry (BT) model (Bradley & Terry, 1952), where the probability of preferring $y_1$ to $y_2$ is represented using a difference of rewards:\n$$P(Y_1 > Y_2 | x) = \\sigma(r(x,y_1) - r(x, y_2)),$$\nwhere $\\sigma(x) = \\frac{1}{1+exp(-x)}$ is a sigmoid function. The larger the value of $r(x, y)$ is, the more preferable a completion y is to a prompt x.\nUsing a labeled dataset $D = \\{x^i, y^w_i, y^l_i\\}_{i=0}^N$ with user preferences, where $y^w$ is preferred to $y^l$ for prompt $x^i$, the loss function for training the reward model is formulated by minimizing the negative log-likelihood:\n$$L(r_{\\phi}) = -E_{(x,y^w,y^l\\sim D)} [log \\sigma(r_{\\phi}(x, y^w) \u2013 r_{\\phi}(x,y^l))].$$\nRL Fine-Tuning Finally, the language model is fine-tuned, using the trained reward model, to maximize the following objective function:\n$$J(\\pi_{\\theta}) = E_{x\\sim D,y\\sim \\pi_{\\theta}} [r_{\\phi}(x, y)] \u2013 \\beta D_{KL}(\\pi_{\\theta}(y | x)||\\pi_{ref}(y | x)),$$\nwhere $\\beta$ is a hyperparameter that controls the deviation from $\\pi_{ref}$. $\\pi_{\\theta}$ is trained to maximize the reward while being regularized by the reverse KL divergence to not deviate too much from $\\pi_{ref}$. Typically, $\\pi_{ref}$ is fixed to $\\pi_{SFT}$ while $\\pi_{\\theta}$ is initialized with $\\pi_{SFT}$."}, {"title": "3.2 Directed Preference Optimization (DPO)", "content": "In RLHF, the need to train the reward model and apply an online RL algorithm such as PPO imposes significant computational and memory costs. DPO suggests a method for directly learning to reflect human preferences in a supervised manner without using the reward model by mapping language model policies and reward functions. The objective function is equivalent to that of RLHF, and the optimal policy that maximizes Equation (3) when the reward model is optimal is derived as follows:\n$$\\pi^*(y | x) = \\frac{1}{Z(x)} e^{\\frac{1}{\\beta} r^*(x,y)},$$\nwhere Z(x) is the partition function. From this equation, the optimal reward can be expressed using the optimal policy:\n$$r^*(x, y) = \\beta log \\frac{\\pi^*(y | x)}{\\pi_{ref}(y|x)} + \\beta log Z(x).$$"}, {"title": "4 Entropy Controllable Directed Preference Optimization", "content": "In DPO, reverse KL divergence is used as a regularizer that controls the deviation from $\\pi_{ref}$. The reverse KL divergence is defined as $D_{KL}(\\pi_{\\theta}||\\pi_{ref}) = \\int \\pi_{\\theta}(y | x) log \\frac{\\pi_{\\theta}(y | x)}{\\pi_{ref}(y|x)} dy$. Here, the integrand is zero for regions where $\\pi_{\\theta}(y | x) = 0$, meaning that only the regions supported by $\\pi_{\\theta}(y | x)$ affect the divergence. Consequently, fitting by minimizing the reverse KL divergence is known to be mode-seeking and generally performs better than other divergences such as forward KL (Ke et al., 2021; Wang et al., 2024a).\nHowever, in this study, we discuss cases where even using reverse KL divergence can fail to achieve mode-seeking fitting with respect to the target distribution. We verify such cases through preliminary experiments and show that controlling the entropy of the distribution enables more effective mode-seeking fitting. To control the entropy of the output probability by language models in DPO, we propose H-DPO, which incorporates such entropy-controllable optimization into DPO."}, {"title": "4.1 Mode-seeking Property", "content": "As a preliminary experiment on the mode-seeking property of reverse KL divergence, we fit a Gaussian distribution to a mixture of two Gaussian components. Specifically, given a Gaussian mixture model $\\pi_{ref}$, we compute the location and scale parameters of a Gaussian distribution that minimize the reverse KL divergence $D_{KL}(\\pi||\\pi_{ref})$. If the fitting is mode-seeking, the estimated Gaussian distribution should capture one of the components of the mixture model. However, as shown in Figure 1, despite the reverse KL minimization, which is supposed to have the mode-seeking property, the fitting may look mode-covering, not mode-seeking. In this case, if $\\pi$ is a language model, it is likely to generate from valleys where $\\pi_{ref}$ has a low probability, possibly leading to degraded performance of $\\pi$.\nThe cause of such mode-covering fitting could be the inherent property of reverse KL divergence minimization, which aims to preserve some variance. If $\\pi$ captures only one component, its variance should be smaller compared to $\\pi_{ref}$ as a whole because it must ignore the other component. As shown on the left side of Figure 1, however, reverse KL minimization does not take this into account, resulting in mode-covering estimation.\nWe consider an objective that can reduce variance or entropy as a remedy. To adjust the entropy of $\\pi$, we note that the reverse KL divergence can be decomposed into entropy and cross-entropy components as follows:\n$$D_{KL}(\\pi||\\pi_{ref}) = \\int (\\pi(x) log \\pi(x) - \\pi(x) log \\pi_{ref}(x))dx = -H(\\pi) + H(\\pi, \\pi_{ref}).$$\nBy attaching a coefficient $\\alpha$ to the entropy $H(\\pi)$, we can derive another objective that can control entropy: $D_a = -\\alpha H(\\pi) + H(\\pi,\\pi_{ref})$. By making $\\alpha$ less than 1, we can reduce the entropy while fitting between"}, {"title": "4.2 H-DPO", "content": "As discussed in the previous section, by decomposing the reverse KL divergence into its entropy and cross-entropy components, we can adjust the entropy with $\\alpha$. The objective function for DPO with entropy adjustment is shown below:\n$$J_{H-DPO} = E_{x\\sim D,y\\sim \\pi} [r(x, y)] \u2013 \\beta D_a(\\pi||\\pi_{ref}) \\\\ = E_{x\\sim D,y\\sim \\pi} [r(x, y)] + \\alpha\\beta H(\\pi) \u2013 \\beta H(\\pi, \\pi_{ref}).$$\nHere, when $\\alpha$ equals 1, it becomes the same objective function as that of standard DPO. By setting $\\alpha$ to be smaller than 1, the learning process aims to reduce the entropy. Similar to Wang et al. (2024a), we consider a constrained optimization. By applying Lagrange multipliers under the constraints that $\\pi$ is a probability distribution, i.e., $\\Sigma_y \\pi(y | x) = 1$ and $\\forall y, \\pi(y | x) \\geq 0$, we obtain the following:\n$$L(\\pi, \\lambda, C) = E_{x\\sim D,y\\sim \\pi}[r(x, y) \u2013 \\alpha\\beta log \\pi(y | x) + \\beta log \\pi_{ref}(y | x)] -\\lambda (\\Sigma \\pi(y | x) - 1)- \\Sigma C(y)\\pi(y | x),$$\nwhere $\\Lambda$ and C are the dual variables. Solving this problem, the optimal policy $\\pi^*$ can be derived as\n$$\\pi^*(y | x) = \\frac{1}{Z(x)} \\pi_{ref}(y | x)^{1/\\alpha} exp(\\frac{r^*(x,y)}{\\alpha\\beta}),$$\nFrom this equation, the reward function can be expressed using the policy as follows:\n$$r^*(x, y) = \\alpha\\beta log \\pi^*(y | x) \u2013 \\beta log \\pi_{ref}(y | x) + \\alpha\\beta log Z(x).$$\nWhen applying this reward function to the BT model and performing the maximum likelihood estimation, the loss function using $\\alpha$ is\n$$L_{H-DPO} = -E_{x, y_w,y_l\\sim D} \\bigg[ log \\sigma\\bigg(\\frac{\\alpha\\beta log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{\\theta} (y_l | x)} - \\beta log \\frac{\\pi_{ref} (y_w | x)}{\\pi_{ref}(y_l|x)}}{\\alpha} \\bigg) \\bigg].$$\nComparing this equation to the DPO loss function in Equation (7), we can see that entropy adjustment using $\\alpha$ can be implemented by simply replacing the coefficient for $\\pi_{\\theta}$ from $\\beta$ to $\\alpha\\beta$."}, {"title": "5 Experiments", "content": "In this section, we evaluate the performance of H-DPO in comparison to standard DPO using widely recognized metrics."}, {"title": "5.1 Experimental Setup", "content": "We conducted DPO training based on Zephyr-7B-Beta (Tunstall et al., 2023; Tunstall et al.). We started from zephyr-7b-sft-full, which is based on Mistral 7B (Jiang et al., 2023) and fine-tuned with UltraChat (Ding et al., 2023). We performed DPO training on it with UltraFeedback (Cui et al., 2023). We evaluated the performance when H-DPO was used instead of standard DPO. The hyperparameters during training were the same as those of Zephyr-7B-beta, except for the variable $\\alpha$. The $\\alpha$ was varied in the range from 0.8 to 1.2.\nThe evaluation tasks included diverse grade school math word problems (GSM8K (Cobbe et al., 2021)), coding task (HumanEval (Chen et al., 2021)), multiple-choice question task (MMLU-Pro (Wang et al., 2024d)) and instruction-following task (IFEval (Zhou et al., 2023)). The training was conducted with three different seeds. Further experimental details are provided in Appendix A.3 and A.4."}, {"title": "5.2 Performance and Diversity", "content": "Table 1 shows the scores for each task when $\\alpha$ was decreased. By reducing $\\alpha$ by 0.05 to 0.1, performance improved on all tasks compared to the conventional DPO ($\\alpha = 1$).\nTable 2 presents diversity metrics when $\\alpha$ was varied in H-DPO. When the temperature was set to 1, smaller $\\alpha$ values resulted in lower diversity, while larger $\\alpha$ values increased diversity. This indicates that diversity can be controlled through $\\alpha$. However, it should be noted that diversity changes with temperature, and the optimal temperature varies depending on the value of $\\alpha$. Hence, even with a smaller $\\alpha$, diversity could be increased if a higher temperature is used.\nFor MMLU-Pro, the scores and entropy with varying temperatures are shown in Figure 2. The left figure illustrates the relationship between temperature and score, highlighting that smaller $\\alpha$ values exhibit less performance degradation and greater robustness to temperature selection. This is because entropy remains low even when a higher temperature is used. The right figure shows the relationship between entropy and score, where the entropy of the samples obtained at each temperature replaces the temperature shown in the left figure. At the same score point, the entropy is larger when $\\alpha$ is smaller. In other words, with a smaller $\\alpha$, it is possible to achieve more diverse generations even with the same performance."}, {"title": "5.3 Coverage Evaluation", "content": "As mentioned in the previous section, a smaller $\\alpha$ enabled more diverse outputs at the same performance level. Wang et al. (2024b) demonstrated that high diversity positively impacts coverage performance, where"}, {"title": "6 Conclusion", "content": "In this study, we proposed H-DPO, a generalization of DPO, and thoroughly examined its effectiveness. H-DPO allows for the adjustment of entropy during training through the hyperparameter $\\alpha$, enabling the control of distribution sharpness and achieving more effective mode-seeking fitting compared to standard DPO. This new method allows trained models to generate more accurate and diverse outputs, better aligning with their intended purposes. In the experiments, we aligned Mistral-7B-based models using the proposed method and compared them with standard DPO. H-DPO demonstrated superior performance compared to DPO across various tasks. In mathematical tasks, it showed excellent performance in pass@k evaluations. These results confirmed that the diversity and quality of the generated outputs improved, establishing H-DPO as a powerful method for improving the training process of LLMs. Moreover, H-DPO is extremely simple to implement, requiring only minor modifications to existing DPO, which adds to its practicality and potential for widespread application. The need to adjust $\\alpha$ is a limitation of this method, and automating the search of appropriate $\\alpha$ values for each task can be a focus of future research."}]}