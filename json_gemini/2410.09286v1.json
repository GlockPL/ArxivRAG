{"title": "Language-Model-Assisted Bi-Level Programming for Reward Learning from Internet Videos", "authors": ["Harsh Mahesheka", "Zhixian Xie", "Zhaoran Wang", "Wanxin Jin"], "abstract": "Learning from Demonstrations, particularly from biological experts like humans and animals, often encounters significant data acquisition challenges. While recent approaches leverage internet videos for learning, they require complex, task-specific pipelines to extract and retarget motion data for the agent. In this work, we introduce a language-model-assisted bi-level programming framework that enables a reinforcement learning agent to directly learn its reward from internet videos, bypassing dedicated data preparation. The framework includes two levels: an upper level where a vision-language model (VLM) provides feedback by comparing the learner's behavior with expert videos, and a lower level where a large language model (LLM) translates this feedback into reward updates. The VLM and LLM collaborate within this bi-level framework, using a \u201cchain rule\" approach to derive a valid search direction for reward learning. We validate the method for reward learning from YouTube videos, and the results have shown that the proposed method enables efficiently reward design from expert videos of biological agents for complex behavior synthesis. For videos showcasing our results and demonstrations used, please visit videos.", "sections": [{"title": "1 Introduction", "content": "Learning from demonstrations (LfD) offers an appealing way to enable robots or learner agents to learn from expert skills without expensive programming from scratch [1, 2, 3]. A learner agent can either learn the expert policies via imitation learning [4, 5] or infer a reward function via inverse reinforcement learning [6] to derive its own policies. While LfD [5, 6] is early motivated from acquiring motion skills from biological agents like humans and animals, collecting ready-to-use expert data is always a challenge. Specialized setups, e.g., marker-based motion capture [7] and dedicated devices [8], are often needed [9, 10, 11]. The complexity of data acquisition significantly limits the scalability of LfD methods, and many works [12, 13] turn to animated motion data.\nMethods for learning from internet videos [14, 15, 16] offer promising solutions to data issue in LfD. However, video inputs adds another dimension of challenge in algorithm design. A dedicated pre-processing like pose estimation [17, 14, 15] is typically needed to extract motion data from videos. Additionally, motion retargeting is sometimes required to address morphological differences of biological experts or the differences of camera perspectives [18, 16, 19].\nTo address the data preparation challenges in learning from in-the-wild videos, two questions need to be answered. Q1: Without data preparation, how can expert videos directly inform the improvement of robot behavior? Q2: How to represent robot behavior to facilitate the informed improvement?\nFor Q1, rather than preprocessing videos, we record robot behavior as video clips and feed to a vision-language model (VLM), such as Google's Gemini [20] and OpenAI's MiniGPT-4 [21]. By harnessing the semantic understanding and cross-modal reasoning capabilities of VLMs, we are able to directly obtain the semantic feedback from VLM on how to improve robot behavior to emulate the expert's. Such feedback, while in textual form, can be used for robot motion improvement.\nFor Q2, we choose reward code as a representation of robot behavior. This is inspired by the recent success of 'language-to-reward' methods [22, 23, 24]. Thus, to update the reward given the feedback from VLM, we use a large language model (LLM) [21] to codify the textual feedback into the updates of executable codes of robot reward. The updated reward will be optimized by the learner robot to derive its policy via reinforcement learning (RL).\nThe above VLM-LLM approach forms a bi-level framework for reward search, which we refer to as language-model-assisted bi-level programming. It enables to a RL agent (robot) learns its reward directly from in-the-wild videos. The upper-level VLM provides textual feedback by the video clips of robot behavior with provided expert videos, and the lower-level LLM updates the robot reward based on the textual feedback. To our best knowledge, this is the first method to enable reward learning from in-the-wild videos without dedicated data preparations. It could pave the way toward a salable method for visual inverse reinforcement learning."}, {"title": "2 Related Works", "content": "VLM/LLM in Reinforcement Learning and Robotics The near-human reasoning capabilities of LLMs make them well-suited for integration into agent decision making. For example, Brohan et al. [25], Lin et al. [26], Xie et al. [27], Singh et al. [28], Huang et al. [29] use LLM as high-level planners. Xu et al. [30], Liu et al. [31], Liang et al. [32], Ma et al. [33] use LLMs as code generator. Other works use LLMs to guide skill acquisition [31], evaluate environments [30]. LLMs have also been used as exploration modules [33] and policy regularizer [34]. VLMs further have the cross-modal understanding and reasoning abilities. They are thus used to enhance semantic observations [35], extract language-conditioned features [33, 36, 37]. Guan et al. [38], Shridhar et al. [39], Huang et al. [29] use VLMs in critiquing undesired behavior during the evaluation stages of RL.\nVLM/LLM for Reward Design Encoded with multi-domain knowledge, VLM/LLM can be used to directly rate the quality of motion trajectories [40, 41], generating reward signals [42, 43, 44] in the RL training. Recently, Eureka [23] uses LLM to refine the reward coding. The method also shows sim-to-real transfer with safety and domain-randomization [45]. Similarly, Xie et al. [24] proposes to use LLMs for few-shot reward code generation [24] with human feedback. Yu et al. [22] use LLMs to generate the cost function code for model predictive control. While our proposed bi-level framework also involves LLM-assisted reward generation module, we focus on the problem of inverse reinforcement learning with a proposed higher-level VLM used to guide the reward search.\nInverse Reinforcement Learning Our method essentially falls to the category of inverse reinforcement learning (IRL) [46, 47, 48, 49, 50]. As noted earlier, a shared challenge of existing IRL methods is expert data acquisition. While recent efforts have explored learning rewards from videos [51, 52], they typically require task-specific video processing modules to extract expert motion. In contrast, our method is preprocessing-free visual IRL. By leveraging the visual reasoning capabilities of foundational models, our approach directly updates the reward by intaking both expert and learner motion videos. This significantly improves the scalability of IRL for learning from the vast array of demonstration videos available on the internet.\nVisual Imitation Learning Complementary to our work are the methods that learn policies from videos of biological agents [53, 54]. A key is to extract and map the biological motion in videos to robot embodiment. The features e.g., keypoints [14, 15, 16, 19] or skeletons [18] are typically extracted from videos. These features will be used for re-target network training for teleoperation [16] or RL [19]. Extracting key features from videos typically involves heavy engineering. Our method provides an alternative, yet direct way of learning rewards from video demonstrations.\nLearning from Biological Agents Works in [9, 55, 15, 14] use imitation learning to transfer animal skills into robots, but these methods are also engineering-heavy because motion capture and"}, {"title": "3 Problem Statement", "content": "We formulate a robot (learner agent) as a Markov Process, defined by $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, T, p_0)$. Here, $\\mathcal{S}$ and $\\mathcal{A}$ denote the robot state and action space, respectively; $T : \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$ is the robot dynamics; $p_0$ denotes the distribution of initial robot state. Given a reward function $R : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$, the robot derives its optimal policy $\\pi_R \\sim \\pi_R(a_t|s_t)$, by maximizing the expected reward\n$\\eta_{\\pi_R} = \\arg \\max_\\pi \\mathbb{E}_{\\pi,\\mathcal{M}}[\\sum_{t=0}^T \\gamma^t R(s_t, a_t)],$ (1)\nwith discount factor $\\gamma \\in (0, 1)$. We define the robot behavior as the rollout trajectory of its optimal policy $\\pi_R$, i.e.,\n$\\xi(\\pi_R) = {s_0, a_0, s_1, ..., s_{T-1}, s_T} \\sim p_0(s_0) \\prod_{t=0}^{T-1} T(s_{t+1}|s_t, a_t) \\pi_R(a_t|s_t).$ (2)\nWe follow [56] to formulate the problem of learning reward from video demonstrations. Consider the robot is given expert demonstration video $D_e = {I_t:T}$, which is in an observation (video) space. We define an observation model $\\mathcal{D} : \\Xi \\rightarrow \\mathcal{D}$ to map the robot behavior $\\xi(\\pi_R) \\in \\Xi$ to the observation (video) space.\n$D' = \\mathcal{D}(\\xi(\\pi_R)),$ (3)\nwhich is a video of robot behavior $\\xi(\\pi_R)$. Then, the problem of learning reward $R$ from expert video $D_e$ is\n$\\min_{\\mathbb{E}_{\\xi(\\pi_R)}} \\mathcal{L}(D_e, \\mathcal{D}(\\xi(\\pi_R)))$ with $\\pi_R$ is from (1). (4)\nHere, $\\mathcal{L}$ is a imitation loss, defined in the observation space, evaluating the mismatch between the expert motion and robot behavior in videos. The expectation is with respect to (2)."}, {"title": "3.1 Classic Gradient-Based Bi-Level Programming", "content": "LfD in (4) is a bi-level optimization, where the upper level is to minimize visual imitation loss of the robot behavior $\\xi(\\pi_R)$ of policy $\\pi_R$, which is obtained from the lower-level through reinforcement learning under the current reward $R$. Classic methods, e.g., [56], solve (4) based on gradient-based methods, with the search direction of the reward given by\n$\\nabla_R \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial \\xi(\\pi_R)} \\frac{\\partial \\xi(\\pi_R)}{\\partial R}.$ (5)\nEach gradient-based update of the reward $R$ requires solving the agent behavior $\\xi(\\pi_R)$ in the lower-level RL (1), and also differentiate through RL solution for gradient $\\frac{\\partial \\xi(\\pi_R)}{\\partial R}$.\nApplying the classic bi-level programming (5) to visual LfD problem in (4) faces one challenge: the visual loss $\\mathcal{L}$ and observation model $\\mathcal{D}$ has to be analytically given. This can be challenging for in-the-wild videos as it requires data-preprocessing and modeling effort. In the sequel, we will show how the capable VLMs and LLMs can be used as a gradient-free method to solve (4)."}, {"title": "4 Language Model-Driven Bi-Level Programming", "content": "4.1 Framework Overview\nA key insight of (5) is that the search direction of the reward $R$ needs to combine two informed feedback via chain rule. (I) The first is visual feedback $\\frac{\\partial \\mathcal{L}}{\\partial \\xi(\\pi_R)}$, from visual imitation loss to the robot behavior. This feedback informs how to improve the robot behavior to minimize visual imitation loss. (II) The second is reward update $\\frac{\\partial \\xi(\\pi_R)}{\\partial R}$, from the robot behavior $d\\xi(\\pi_R)$ to robot reward $\\partial R$. This informs how to update reward in response to the robot behavior improvement."}, {"title": "4.2 Upper-Level: Visual Feedback from VLM", "content": "Given the video clips of expert demonstration and a recording of robot's current behavior, the high-level VLM provides feedback in the form of natural language about where the robot should improve its behavior to match that of expert in the videos. Thus, the VLM plays the role of $\\frac{\\partial \\mathcal{L}}{\\partial \\xi(\\pi_R)}$. Notably, with minimal prompting, we find that VLM feedback is closely aligned with human feedback, indicating its strong ability to analyze and compare motion patterns across videos. Our experiment uses Gemini 1.5-Pro as our VLM as it proved superior in video analysis during our experiments. An example of visual feedback is given in Fig. 2. The complete set of prompts used is provided in the Appendix A.1.1."}, {"title": "4.3 Lower-Level: Reward Feedback from LLM", "content": "The feedback from high-level VLM is then passed to the low-level LLM for reward update. The low-level LLM plays a role of translating the VLM's textual suggestion into the update of the robot reward. To achieve so, similar to [22, 24, 23], we represent the robot reward as code, which the LLM can directly manipulate due to is strong code understanding and writing capabilities. Thus, the update of reward code from LLM is similar to the role of performing gradient $\\frac{\\partial \\xi(\\pi_R)}{\\partial R}$.\nIn our implementation, to generate effective reward update, we additionally pass the robot environment code as a context to the low-level LLM, as in [23]. This context reveals the necessary robot variables and environment parameters for successful code writing. LLMs are capable of understanding large complex code, and thus no changes are needed in environment code. We also pass the values of current reward components at intermediate policy training checkpoints. This helps ground"}, {"title": "5 Experiments", "content": "We evaluate our approach on three robots-Ant, Humanoid, and ANYmal-in Isaac Gym environments [57], learning rewards from video demonstrations of their biological counterparts: spider, human athlete, and dog. The biological motion videos, obtained directly from YouTube, are used to train the robots for various skillful motion tasks, including Spider Walking, Spider Jumping, Human Running, Human Split Landing, Dog Hopping, as shown in Fig. 4.\nAll videos are from YouTube, spanning 10-15 seconds [58, 59, 60, 61, 62]. The selection of videos is flexible, and we only require the motion of a biological agent is clearly presented in the video. To avoid confusion for the VLM, we avoid the videos where multiple agents are present. We train the policies of the three robots, Ant, Humanoid, ANYmal in the Issac Gym environment, using Proximal Policy Optimization (PPO) [63] with default hyperparameters. For each baseline, we run 5 independent runs and report the average performance achieved in these runs. Two example runs demonstrating the effectiveness of our approach are available in A.3. For videos showcasing our results and demonstrations used, please visit Videos."}, {"title": "5.1 Baselines", "content": "Given the limited baselines for reward learning from in-the-wild videos, we select baselines from the domains of LLM-assisted and human-feedback-based reward design. The maximum number of reward iteration for all methods is set to $I = 5$.\nEureka: a state-of-the-art method that searches for RL reward by LLM-assisted evolutionary optimization. It requires a fitness function for evaluation of the agent policy. We follow the Eureka paper [23] and set the fitness function (See Appendix A.2) that reflects the key biological motion features in the video for each learning task. The iteration batch size in Eureka is set $K = 8$.\nEureka-GT: We use a best-tuned score function (by an expert) as the fitness function in Eureka. As described later, this best-tuned score corresponds to the minimal imitation loss for the biological motion in given videos. We set the iteration batch size $K = 4$ here.\nHuman as VLM: This baseline is a modification of our bi-level method by replacing the high-level VLM with a human to provide the visual feedback to the low-level LLM. This is similar to Eureka with human input [23] and text2reward [24], except the goal here is learning from videos."}, {"title": "5.2 Evaluation Metrics", "content": "Normalized Expert Score To evaluate the learned reward R, we choose an expert best-tuned score function $S_{\\text{expert}}$ to evaluate the robot behavior $\\xi(\\pi_R)$ of the reward R. This score function is designed by human expert, through trial and error, such that robot behavior $\\xi(\\pi_{S_{\\text{expert}}})$ generated by directly optimizing this best-tuned score can capture the key biological motion features in the videos. Thus, a higher score value $S_{\\text{expert}}(\\xi(\\pi_R))$ means robot behavior is more similar to the biological motion (this score can be considered as a negative visual imitation loss in (4)). The fitness functions used in [23] are not suitable evaluation metrics because they are rudimentary, leading to agents exploiting the fitness criteria rather than genuinely learning the desired behaviors or motions.\nTo prove the validity of the best-tuned reward $S_{\\text{expert}}$, as will be shown later, we train robot policy $\\pi_{S_{\\text{expert}}}$ with it, and present the corresponding robot behavior $\\xi(\\pi_{S_{\\text{expert}}})$, together with the robot behavior from the other learned rewards, to human ranking evaluation (next metric), and we find that the $\\xi(\\pi_{S_{\\text{expert}}})$ is always ranked at top in the similarity to the biological motion video."}, {"title": "5.3 Results and Analysis", "content": "Fig. 5 shows the performance comparison between our method and three baselines (Eureka, Eureka-GT, Human-as-VLM) under two metrics (normalized expert score and human preference score), for the five LfD tasks (Fig. 4). All prompts to VLM and LLM for the tasks are given in Appendix A.1.\nOur approach outperforms Eurekas: Fig. 5 show that the proposed method consistently outperforms Eureka across all tasks. Comparison between Eureka and Eureka-GT shows the importance of choosing proper fitness function, which is typically difficult in LfD tasks or generally in reward design. Although Eureka-GT has the best possible fitness function, our approach outperforms Eureka-GT, proving the effectiveness of the reward search guided by VLM.\nVLM feedback on par with human feedback: Fig. 5 shows a comparable performance between the proposed method and human as VLM. Recall that the difference between the two methods is that we only replace VLM with human in the high level. The results show VLM achieves near-human-level capability to provide meaningful feedback for robots to mimic biological counterparts.\nOur method continuously improves reward: Fig. 6 shows the normalized expert score after each iteration during the reward update process of the proposed method for two LfD tasks. The continuous improvement of normalized expert score indicates that the search directions provided by the VLM and LLM are valid, and the reward is effectively optimized to minimize the imitation loss.\nThe learned reward captures complex motion skills in videos: The learned reward by the our method is able to capture the fine-grained details of the biological motion in videos. For example, in Spider-Jumping task shown in Fig. 7a, the Ant learns to crouch its body to a energy-storing position before jumping, mimicking that of the spider. In the Human Split Landing in Fig. 7b, the"}, {"title": "5.4 Ablation Study: Bi-Level versus Single-Level", "content": "To demonstrate the benefits of our bi-level design, we perform an ablation study to compare the proposed VLM-LLM bi-level method with single-level VLM that directly takes in expert videos and generates reward update. We perform five runs for the Spider Walking and Human Running tasks, and show comparison in Fig. 8. The results clearly show the advantage of the proposed VLM-LLM design over the single-level VLM design. This advantage can be attributed to the hierarchical architecture, allowing VLM and LLM to specialize in their respective components: the VLM is tasked with high-level planning and the design of reward structures, while LLM is dedicated to environment-specific code generation and refinement. The results are consistent with prior research [64, 65], where decomposing complex tasks into simpler, specialized components leads to better outcomes."}, {"title": "6 Conclusion", "content": "This paper proposed a language-model-assisted bi-level programming framework to learn reward functions from expert demonstration videos. The framework integrates a high-level VLM to analyze expert and learner behavior videos, generating visual feedback for the learner, and a low-level LLM to perform reward updates based on the VLM's feedback. By leveraging the semantic and cross-modal capabilities of the VLM, and the coding abilities of the LLM, the framework enables gradient-free reward search to minimize the visual imitation loss. We envision that this framework can extend existing inverse reinforcement learning methods, facilitating scalable learning from in-the-wild videos without requiring dedicated video processing routines."}, {"title": "A Appendix", "content": "A.1 Full Prompts\nIn this section, we present the complete prompts utilized by the high-level Visual Language Model (VLM) and the low-level Large Language Model (LLM)."}, {"title": "A.1.1 High-Level VLM Prompts", "content": "High Level VLM: System Prompt\nThis shows observation with all available variables and information about the environment that LLM has. At any cost, limit the suggestion to what can be implemented from this data:\n{Task Obs Code String:}\nTask Obs Code String: Raw environmental code provided to expose environmental specifications to LLM.\nHigh Level VLM: Initial Prompt\nYou are an expert in reinforcement learning and robotics. A Large Language Model (LLM) is tasked with writing a reward function to train a reinforcement learning agent to imitate the motion demonstrated by a {Creature Name} in the video.\nThe task is {Task Description}. Since the LLM does not have access to the video, your task is to analyze the video and provide a detailed description of the spider's jumping motion to assist the LLM in writing the reward function. The description should contain the goal of the task along with necessary reward function considerations to help in its development. Do not include anything else.\nThe format of your response must be:\n1. Task: A short description of the task.\n2. Possible Reward Function Considerations (bullet points): List the three most important components needed to make the task possible. Important: The components should be easy to implement as the LLM writing reward has limited access to information about the environment.\nCreature Name: Name of the biological agent you are trying to replicate.\nTask Description: A short one-line description of the task you want the RL agent to pursue while mimicking the motion shown in the video.\nHigh Level VLM: Review Prompt\nI trained an agent and got the following results. Provide the major problems, along with possible improvements in motion. The format of response should be :\n1. Problems: Describe the motion and problems with by carefully studying the video.\n2. Rewrite Component: If some component seems to be wrongly implemented, suggest rewriting it.\n3. Remove Component: If some component is not needed or is doing harm, then it is good to remove it.\n4. New Component: Only if required, suggest new components needed; otherwise, there are none.\nTips for response:\n1. Study the videos, reward function, reward component values and prior performances to provide feedback.\n2. Do not suggest complicated reward components which are hard to implement.\n3. Keep the suggestion limited to what can be changed using the reward function.\n4. The output should be only the 4 points mentioned above.\nDon't include anything else.\n5. The Low-Level LLM writing reward function doesn't have access to video, so it cannot see or analyse motion in the video instead, you should analyse it and provide specific suggestions regarding the motion.\nThis was the reward function:\n{Reward Function}\nYou can also use the values I tracked for the individual components in the reward function every epochfreq} epoch and the maximum, mean, and minimum values encountered:\n{Scalar values of all reward components}\nReward Function: The reward function is generated by low-level LLM.\nScalar values of all reward components: An automatically generated summary of the progress of various reward component values."}, {"title": "A.1.2 Low-Level LLM Prompts", "content": "Low Level LLM: System Prompt\nYou are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effectively as possible. Your goal is to write a reward function for the environment that will help the agent learn the task described in the text. Your reward function should use useful variables from the environment as inputs. As an example, the reward function signature can be: {Task Reward Signature String}\nSince the reward function will be decorated with @torch.jit.script, please make sure that the code is compatible with TorchScript (e.g.,use torch tensor instead of numpy array). Make sure any new tensor or variable you introduce is on the same device as the input tensors. Please provide only the reward function without defining any new functions. Ensure that the code only uses existing PyTorch functions and do not introduce any new functions.\nTask Reward Signature String: A short example of the structure of the reward function required.\nLow Level LLM: Initial Prompt\nThe Python environment is {Task Obs Code String}. Write a reward function for the following task: {Task Description}. The output of the reward function should consist of two items: (1) the total reward, (2) a dictionary of each individual reward component. The code output should be formatted as a Python code string: \"\"\"python\nSome helpful tips for writing the reward function code: (1) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor (2) Try to keep the code and avoid writing overly complicated reward components. (3) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have the prefix self.). (4) Under no circumstance can you introduce new input variables.\nLow Level LLM: Review Prompt\nI trained an agent based on the reward function you provided and have the following suggestions {Suggestions from High-Level VLM} For adjusting the weights you can use the values I tracked for the individual components in the reward function every {epochfreq} epoch and the maximum, mean, and minimum values encountered: {Scalar values of all reward components}\nPlease prioritise addressing these problems and provide a new, improved reward function that can better solve the task\nSuggestions from High-Level VLM: Textual feedback from VLM\nLow Level LLM: Error Resolving Prompt\nExecuting the reward function code above has the following error: {Traceback Msg}. Please fix the bug and provide a new, improved reward function!\nTraceback Msg: Error message generated while executing reward function"}, {"title": "A.2 Baseline Details", "content": "The fitness functions used for running the Eureka baseline for each task are -\n1. Spider Walking:\n$F_{SpiderWalking}$ = Forward_Velocity\n2. Spider Jumping:\n$F_{SpiderJumping}$ = Forward_Velocity + |Vertical_Velocity|\n3. Human Running:\n$F_{HumanRunning}$ = Forward_Velocity\n4. Human Splitting:\n$F_{HumanSplitting}$ = Hip_to_right_leg_angle - $\\frac{\\pi}{2}$ + Hip_to_left_leg-angle-$\\frac{\\pi}{2}$+|Left_knee|+|Right_knee|\n5. Dog Hopping:\n$F_{DogHopping}$ = -(linvel_error + angvel_error)\nThe functions of tasks 1, 3, and 5 are directly derived from the Eureka paper, as these tasks closely align with those presented in the original work. The remaining tasks are extrapolated based on the same underlying principles and logic."}, {"title": "A.3 Example Runs", "content": "This section provides two comprehensive examples that demonstrate the functionality of our approach. Each example includes a detailed task description, the Environment Context provided, suggestions given by the VLM, and the corresponding reward function generated by LLM."}, {"title": "A.3.1 Example 1: Spider Jumping", "content": "Task Description: The task is to make ant move forward by performing high forward moving jumps on a flat simulated environment, imitating the spider's forward jumping style."}, {"title": "A.3.2 Example 2: Human Running", "content": "Task Description: The task is to make a humanoid run forward on a flat surface as fast as possible, replicating the gait shown in the video."}]}