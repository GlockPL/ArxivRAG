{"title": "A new approach for encoding code and assisting\ncode understanding", "authors": ["Mengdan Fan", "Wei Zhang*", "Haiyan Zhao", "Zhi Jin*"], "abstract": "Some companies (e.g., Microsoft Research and\nGoogle DeepMind) have discovered some of the limitations of\nGPTs' autoregressive paradigm next-word prediction, manifested\nin the model's lack of planning, working memory, backtracking,\nand reasoning skills. GPTs rely on a local and greedy process of\ngenerating the next word, without a global understanding of the\ntask or the output. We have confirmed the above limitations\nthrough specialised empirical studies of code comprehension.\nAlthough GPT-4 is good at producing fluent and coherent text,\nit cannot handle complex logic and generate new code that\nhaven't been seen, and it relies too much on the formatting\nof the prompt to generate the correct code. We propose a new\nparadigm for code understanding that goes beyond the next-word\nprediction paradigm, inspired by the successful application of dif-\nfusion techniques to image generation (Dalle2, Sora) and protein\nstructure generation (AlphaFold3), which have no autoregressive\nconstraints. Instead of encoding the code in a form that mimics\nnatural language, we encode the code as a heterogeneous image\nparadigm with a memory of global information that mimics both\nimages and protein structures. We then refer to Sora's CLIP\nupstream text-to-image encoder model to design a text-to-code\nencoder model that can be applied to various downstream code\nunderstanding tasks. The model learns the global understanding\nof code under the new paradigm heterogeneous image, connects\nthe encoding space of text and code, and encodes the input of text\ninto the vector of code most similar to it. Using self-supervised\ncomparative learning on 456,360 text-code pairs, the model\nachieved a zero-shot prediction of new data. This work is the basis\nfor future work on code generation using diffusion techniques\nunder a new paradigm to avoid autoregressive limitations.", "sections": [{"title": "I. INTRODUCTION", "content": "Some studies have found that there are several limitations\nof the GPT's autoregressive next-word prediction paradigm.\nMicrosoft Research notes that GPT-4 lacks planning, working\nmemory, ability to backtrack, and reasoning abilities [1]. For\nexample, although the model has enough knowledge to answer\nthe question, the architecture of the GPT-4 cannot give the\nwhole correct answer at once, but needs to be guided step\nby step to give the correct answer. The autoregressive nature\nof GPT-4 forces it to solve problems sequentially. Google\nDeepmind notes that GPTs are not applicable to most areas\nof mathematics due to the high cost of converting human\nproofs into a machine-verifiable autoregressive format [2]. And\npermuting the premise order can cause a performance drop of\nover 30% [3]. When producing full natural-language proofs\non a set of geometry problems from the IMO competitions,\nGPT-4 has a success rate of only 0%, often making syntactic\nand semantic bugs throughout its output, and showing little\nunderstanding of the geometry knowledge and the problem\nstatements themselves. In summary, GPTs rely on a local and\ngreedy process to generate the next word, without a global\nunderstanding of the task or the output [1].\nWe further do empirical studies in the field of code un-\nderstanding and confirm the limitations of the autoregressive\nparadigm. For example, in terms of code understanding and\ngeneration, GPT-4 cannot understand complex logic with\nmulti-step operations and generates incomplete code. More-\nover, GPT-4 relies excessively on the form of the prompt.\nEven if the input text has the same meaning, but just different\nsyntax, the generated code will be much different and often\ncontain bugs. GPT-4 is also not good at matrix operation.\nIt often introduces new bugs while fixing matrix operation's\nbugs. But after giving the prompt step by step, GPT-4 can\ngenerate a correct answer, which shows that the model is\nactually trained with enough knowledge, but the autoregressive\nparadigm makes it not answer the question well. All of these\nissues confirm the limitations of GPT-4 noted by Microsoft\nand Google. The limitations makes it overly dependent on the\ngiven prompt, unable to understand the meaning of the code\nglobally, and also unable to generate complete and correct code\nincluding multi-step operation/matrix operation at one time.\nDiffusion technology has made significant progress in the\ngeneration of images [4, 5] and life molecules [6] recently.\nIt is a wonder that the model does not have limitations like\nthat of GPTs. The model can learn the global information\nof the picture/life molecule, and generate the picture/life\nmolecule at one time instead of step by step. Diffusion is\na class of latent variable models inspired by considerations\nfrom non-equilibrium thermodynamics, which was originally\nused to generate high-quality images [7]. In particular, the\nCLIP+diffusion model (Dalle2) can create new pictures that\nhave never been seen in the data set according to the input\ntext, such as panda mad scientist mixing sparkling chemicals,\nartstation [4, 5]. The model seems to really understand the\nimage. Where CLIP is a transferable visual models from\nnatural language supervision, which provides text embedding\nvector for diffusion model to generate high-quality images [8].\nThe core idea of CLIP is to embed images and texts into"}, {"title": "II. MOTIVATION", "content": "GPTs' autoregressive next-word prediction paradigm has\nsome limitations, such as lack of planning, working memory,\nability to backtrack, and reasoning abilities, and do not apply\nto most areas of mathematics [1, 2]. Researchers believe that\nGPTs lack a global and deep understanding of the task or\nthe output because they rely on a local and greedy process of\ngenerating the next word [1]. In addition, during the long-term\nuse of GPT-4 in the field of code understanding, we further\nconfirmed the limitations through following cases studies."}, {"title": "A. Cases Studies", "content": "1) Lack of ability to handle complex logic: We asked GPT-\n4 some code problems with complex logic and multi-step\noperations, such as \"My training data is ndarray with shape\n(359, 4), which means 359 arrays, and each array contains 4\nlists (length is 1149, 3000, 3000, 18 respectively.).The label\nof the training data is ndarray with shape (359,). Now, I want\nto train an embedding network whose input is the training\ndata and whose output is an ndarray with shape (359, 4),\nrepresenting 359 arrays, each containing 4 lists of length n.\nThe output is an ndarray with shape (359, 4). How should\nit be implemented? \" The statement may seem convoluted,\nso we have added a summarising \"In a nutshell, we want the\ntraining data to be input into this embedding network, and each\narray contains 4 lists that are embedded to the same length\nn.\u201d Although human beings can understand the meaning of\nthe problem. GPT-4 gives us incomplete code full of logical\nbugs.\nE.g., GPT-4 does not understand\nthe shape of the output we want (batch size, 4), and wrongly\nconcatenates 4 lists of each item. The training process is also\nnot complete, e.g., there is no for loop of epochs. The variables\nindicated by red wavy line are also not explained.\n2) Lack of working memory: If we ask the same question\nagain after some rounds of dialogue because we forgot the\nearly answer, GPT-4 often give a completely different answer.\nThis will result in almost all dialogue rounds being invalid.\nFor example, we repeated the GPT-4 question \"How do I use\na terminal command to output a commit that changes more\nthan n files in a repository?\" twice, with several rounds of\ndialogue between them. GPT 4 answered two different wrong\nanswers."}, {"title": "B. Discussion", "content": "1) Using CLIP+Diffusion [4] in image understand-\ning/generation can avoid some limitations similar to using\nGPTs in code understanding/generation: As shown Fig. 5(a)\nand (b), even if a piece of text with complex logic is input,\nCLIP+Diffusion can globally understand the text and correctly\ngenerate images with complex structures. The lines and colors\nin (a) are complex, precise, and smooth. The complex fur\nand texture in (b) are generated clearly. Even if the form\nof the input text is changed, as long as the meaning is\nthe same, the generated image is still correct. In addition,\nCLIP+Diffusion also has creativity. As shown Fig. 5(c) and\n(d), panda mad scientist and a cat dressed as french emperor\nnapoleon holding a piece of cheese hardly ever appear in real\nlife and the training data. But CLIP+Diffusion technology can\ncreate unprecedented images based on human descriptions.\nTherefore, we believe that if this technology is applied to code\nunderstanding/generation, it will greatly improve performance\nin this area.\n2) In contrast to CLIP+Diffusion for image under-\nstanding/generation, why do GPTs for code understand-\ning/generation have the above limitations: We find that the\nCLIP model would memorize the entire image information\nglobally while training, rather than only memorizing the\nprevious code to infer the next token as GPTs do. For example,\nCLIP learned about the global distribution of fur color and\ntexture, as well as the structure and position information of\nfacial features of many pictures of shiba inu. Thus, the image\nrepresentation corresponding to the textual description given\nby the human output by CLIP already has global information\nabout the image [4]. Then Diffusion further restores a high-\nquality image on this image representation.\nHowever, due to GPTs adhering to the paradigm of predict-\ning the next token, the model is unable to globally remember\nfully functional code information. As shown in Fig. 1, if we\nask GPT-4 to generate a logically complex code, even if GPT-"}, {"title": "III. APPROACH", "content": "The code has a highly standardized structure and is not\nas flexible as the structure of natural language, so we first\nbreak the commonly used autoregressive next-word prediction\nparadigm for code understanding and generation, and propose\na novel single-channel, one-dimensional, heterogeneous image\nparadigm. Both code and image have heterogeneity, meaning\nthat both image and code are composed of different compo-\nnents. Therefore, we refer to images and consider different\ncomponents in the code (e.g., classes, methods, variables,\noperators, numbers, symbols) as different components in the\nimage (e.g., red, yellow, blue, green, etc.). Different entities\nwithin the same component, e.g., class 1 and class 2, etc. in\nclass) are represented by similar numerical values (IDs) to\nsimulate the pixel values of different entities within the same\ncomponent in the image (e.g., dark red, light red, pink, etc. in\nred). To prevent significant differences in the values assigned\nto tokens due to too many different tokens (OOV problem),\nwe take a series of measures: 1) Clean the code. Replace the\nmessy data that will affect the accuracy of code tokenization\nwith placeholders of different categories. For example, the\nmessy string output by the print function is occupied with\n'STR'. This step is optional and needs to be ensured that\nremoving such messy data will not reduce the accuracy of\nthe text-to-code matching results. 2) Divide tokens of entities\nsuch as the definition of classes, methods, and variables into\nbuilt-in tokens in the code library and user-defined tokens,"}, {"title": "A. Heterogeneous image paradigm", "content": "The code has a highly standardized structure and is not\nas flexible as the structure of natural language, so we first\nbreak the commonly used autoregressive next-word prediction\nparadigm for code understanding and generation, and propose\na novel single-channel, one-dimensional, heterogeneous image\nparadigm. Both code and image have heterogeneity, meaning\nthat both image and code are composed of different compo-\nnents. Therefore, we refer to images and consider different\ncomponents in the code (e.g., classes, methods, variables,\noperators, numbers, symbols) as different components in the\nimage (e.g., red, yellow, blue, green, etc.). Different entities\nwithin the same component, e.g., class 1 and class 2, etc. in\nclass) are represented by similar numerical values (IDs) to\nsimulate the pixel values of different entities within the same\ncomponent in the image (e.g., dark red, light red, pink, etc. in\nred). To prevent significant differences in the values assigned\nto tokens due to too many different tokens (OOV problem),\nwe take a series of measures: 1) Clean the code. Replace the\nmessy data that will affect the accuracy of code tokenization\nwith placeholders of different categories. For example, the\nmessy string output by the print function is occupied with\n'STR'. This step is optional and needs to be ensured that\nremoving such messy data will not reduce the accuracy of\nthe text-to-code matching results. 2) Divide tokens of entities\nsuch as the definition of classes, methods, and variables into\nbuilt-in tokens in the code library and user-defined tokens,"}, {"title": "B. Contrastive Language-Code Pre-training", "content": "1) Architecture: We adopt a similar architecture of the Con-\ntrastive Language Image Pre-training (CLIP) model [8], and\npropose the Contrastive Language Code Pre-training (CLCP)\nmodel. During the training phase, we jointly trains an code\nencoder and a text encoder to predict the correct (code, text)\npairs of a batch. As shown in Fig. 7, given a batch of N\n(code, text) pairs, CLCP is trained to predict which of the\n$N \\times N$ possible (code, text) pairings in the batch are the\ncorrect pairs. CLCP learns a multi-modal embedding space by\njointly training an code encoder and a text encoder. In view of\nthe strong advantages of the pre-trained transformer model in\nunderstanding natural language [9-12], we continue to train\ndirectly on the model with our data. However, because we\nhave adopted a new code coding paradigm, we have designed\na new code encoder, which will be introduced later.\nThe model parameters are updated by the loss function,\nwhich maximizes the cosine similarity $C_i T_i (1 \\leq i \\leq N)$\nof the code embeddings $C_i$ and the text embeddings $T_i$ of\nthe correct pairs N in the batch while minimizing the cosine\nsimilarity $C_iT_j(1 \\leq i, j \\leq N, i \\neq j)$ of the code embeddings\n$C_i$ and text embeddings $T_j$ of the incorrect pairings $N^2 \u2013 N$.\nSpecifically, we optimize a symmetric cross entropy loss over\nthese similarity scores [8]. We expect these two encoders to be\nused for many downstream tasks for code understanding in the\nfuture, such as using a text encoder to encode input text into a\nvector in the multi-modal embedding space and then inputting\nthat vector into the diffusion model to restore high-quality\ncode. Alternatively, use code encoders in different languages\nto translate code written in one programming language into\ncode written in another programming language, etc."}, {"title": "2) Code encoder based on one-dimensional convolution", "content": "For the code encoder, we refer to two-dimensional convolution\nfor learning on images [13], and design a one-dimensional\nconvolutional and pooling neural network suitable for one-\ndimensional heterogeneous images. As shown in Fig. 8, the\nsource code is first encoded in one-dimensional heterogeneous\nimages according to the new encoding paradigm. Secondly, we\nuse one-dimensional convolution to convolve one-dimensional\nheterogeneous images with a kernel size of k and a step\nsize of s to learn the semantic and structural information\nof the code. That is, the convolution kernel slides on the\ninput one-dimensional image with a step size of s, and at\neach step the IDs in the one-dimensional image are multiplied\nby the corresponding weights in the convolution kernel and\nsummed to obtain the IDs of the dimensions of the output\nnew vector/feature map. Each value of the new vector will\nbe activated by the Relu function. Third, a one-dimensional\n(1D) max or average pooling is applied to the activated new\nvector, with window size k' and step size s'. The basic idea\nof pooling operation is to divide the input 1D image (also\nknown as a vector) into several sub-vectors and calculate the\nmaximum or average value of each sub-vector as the value\nof each dimension of the output vector. Overall, convolution\noperations are mainly used to learn semantic and structural\ninformation between code tokens, while pooling is mainly used\nto aggregate information from sub-code fragments. Finally,\nthe code encoder consists of M blocks, each containing a\nconvolution layer and a pooling layer."}, {"title": "3) Strategies for optimizing the training process", "content": "We use\nRelu function [14] to activate the output of the convolutional\nlayer and use the He initialization [15] method to initialize the\nparameters of the convolutional layer. Compared with tradi-\ntional Sigmoid and Tanh activation functions, Relu has a linear\nrelationship when the input is greater than 0, which means it\nwill not saturate (i.e. the gradient will not approach 0). This\nhelps to solve the problem of gradient vanishing, especially\nin deep networks [16-18]. However, in deep networks, if the\nweight values are large, the accumulation of gradients during\nbackpropagation can lead to gradient explosion problems\n[15, 18]. So we use He Initialization [15] to avoid the above\nproblem. The core idea of Kaiming initialisation is to adjust\nthe initial values of the weights based on the number of input\nnodes, in order to maintain consistency in the variance of the\nactivation values across all layers. Specifically, if a neuron\nhas n input nodes, its weights are initialised to a Gaussian\nor uniform distribution with a mean of 0 and a variance of\n$2/n$. The advantage of this is that no matter how deep the\nnetwork is, the distribution of activation values will remain\nwithin a reasonable range, avoiding the problem of vanishing\nor exploding gradients and making the network easier to train.\nIn addition, Batch Normalization (BN) is a commonly used\ntechnique that normalizes each batch of data. The BN layer\ncan reduce the data distribution changes caused by parameter\nvariations in the network, thereby avoiding gradient vanishing\nor exploding problems during training and improving the con-\nvergence performance of the model. However, by normalizing\nthe input of each layer, BN may suppress the diversity of\nsome representations [19] because BN forces each layer's\ninput to have similar mean and variance during training, which\nmay result in the model losing sensitivity to some important\nchanges and diversity in the input distribution. This means that\nBN may not be applicable to certain types of neural networks,\nespecially those that require preserving certain characteristics\nof the input distribution. For example, CNN models used for\nimage classification typically have the first few layers respon-\nsible for extracting low-level features of the image, such as\nedges, corners, textures, etc. In these layers, each feature map"}, {"title": "IV. EXPERIMENTS", "content": "The goal of this paper is to propose a new code encoding\nparadigm and design an applicable text-code encoder training\nmodel that can be migrated to different downstream tasks of\ncode understanding so that the model may avoid the limitations\nof the autoregressive model on code understanding. Following\nquestions are set on 4\u00d7NVIDIA Tesla V100 with 32GB RAM:\nRQ1. How effective is CLCP on Zero-Shot Transfer? It\nis difficult for the proposed CLCP to achieve high accuracy\non the zero-shot task with a limited amount of data (456,360\n(code, text) pairs) compared to the dataset (400 million (image,\ntext) pairs) used for the CLIP model. For example, some\nprevious work also used natural language supervision for\nimage representation learning like CLIP [20-23]. However,\ndue to the limited amount of training data, their effects are not\nas amazing as those of the CLIP (e.g., Li et al [20] reach only\n11.5% accuracy on ImageNet in a zero-shot setting), so they\nhave not attracted enough attention. Collecting a large amount\nof high-quality data is still an arduous task in the future.\nIn this paper, to evaluate whether the proposed method has\napplication value, we change the size of training and testing\nsets to magnify the effect of the model on the zero-shot task\nand evaluate whether the model can effectively learn the global\ninformation of the code.\nRQ2. Ablation experiment. In this RQ, we conducted some\nablation experiments to explore training strategies for learning\ncode heterogeneous images, rather than training strategies that\nare applicable to all models. Specifically, pooling operations\nreduce the dimensionality of feature maps by aggregating\ninformation from adjacent regions, which inevitably results\nin the loss of some detailed information. For example, in\nmax/average pooling, only the max/average value within each\nregion is retained, while all other values are discarded. This\ntype of information loss can gradually accumulate in multi-\nlayer networks, leading to poor model performance when fine\nfeatures are required. Therefore, in this RQ, we removed the\npooling layer of CLCP model to verify its effectiveness.\nIn addition, we use He initialization [15] to avoid gradient\nexplosion problems in backpropagation, and do not use BN\nto avoid blurring the distribution characteristics of the input\nimage. In this RQ, we remove He initialization and add BN\nrespectively to verify the effectiveness of the optimization\nstrategy we adopted during the training process.\nRQ3. Will the prompt engineering improve the results?\nPrompt engineering is a common method to improve the\neffect of model based on natural language supervision [8-\n11]. We observed that the description of the code in the\ndataset is complex and redundant. The common categories of\nredundancy is summarized in table III. So we try to clean up\nand reconstruct the text to improve the experimental results."}, {"title": "V. THREAT TO VALIDITY", "content": "Internal validity. Due to hardware and data limitations, we\ndid not use as many datasets as OpenAI used to train CLIP\nmodels to train CLCP models. We simply followed CLIP's\nprevious work of proposing text-to-image (which was only\ninitially trained and tested on limited data) and proposed an\ninitial text-to-code work. This results in the trained CLCP\nbeing not robust and cannot be directly used for downstream\ntasks of code understanding. The main purpose of this paper\nis to provide a new perspective for researchers engaged in\ncode understanding. In future work, we will collect more\ndatasets and apply for more funding to evaluate our model\non better hardware environments. And because we did not\nuse pre-trained embeddings for code encoding, but instead\nproposed a new encoding paradigm and retrained it, we did not\ncompare with other works that used ready-made embeddings.\nExternal validity. The source code of clip model is not open,\nand many training details have not been published. Therefore,\nwe replicated and improved its framework ourselves, which to\nsome extent reduced the performance of proposed model."}, {"title": "CONCLUSION", "content": "Microsoft Research and Google DeepMind have found\nmany limitations in the GPTs' autoregressive paradigm, man-\nifested in the model's lack of planning, working memory,\nbacktracking, and reasoning skills. Microsoft Research found\nthat GPT relies on a local greedy process of generating the next\nword, without a global understanding of the task or output.\nWe further summarized the limitations of the autoregressive\nparadigm on code understanding through empirical research.\nThat is to say, GPTs cannot handle complex logic and generate\nnew code that has not been seen before, and they rely too\nmuch on the format of prompts to generate correct code. To\naddress this issue, we propose a new code encoding paradigm\ninspired by the successful application of diffusion techniques\nin image generation, such as the Dalle2 and Sora models. We\nfirst discovered that the structure of code has both natural\nlanguage characteristics and image/protein molecular chain\ncharacteristics, and encoded the code into a heterogeneous\nimage paradigm with global information memory that mimics\nthe structure of images and proteins. Then, we refer to Sora's\nCLIP upstream text-to-image encoder model and design a text-"}]}