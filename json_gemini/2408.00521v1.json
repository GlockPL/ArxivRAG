{"title": "A new approach for encoding code and assisting code understanding", "authors": ["Mengdan Fan", "Wei Zhang", "Haiyan Zhao", "Zhi Jin"], "abstract": "Some companies (e.g., Microsoft Research and Google DeepMind) have discovered some of the limitations of GPTs' autoregressive paradigm next-word prediction, manifested in the model's lack of planning, working memory, backtracking, and reasoning skills. GPTs rely on a local and greedy process of generating the next word, without a global understanding of the task or the output. We have confirmed the above limitations through specialised empirical studies of code comprehension. Although GPT-4 is good at producing fluent and coherent text, it cannot handle complex logic and generate new code that haven't been seen, and it relies too much on the formatting of the prompt to generate the correct code. We propose a new paradigm for code understanding that goes beyond the next-word prediction paradigm, inspired by the successful application of diffusion techniques to image generation (Dalle2, Sora) and protein structure generation (AlphaFold3), which have no autoregressive constraints. Instead of encoding the code in a form that mimics natural language, we encode the code as a heterogeneous image paradigm with a memory of global information that mimics both images and protein structures. We then refer to Sora's CLIP upstream text-to-image encoder model to design a text-to-code encoder model that can be applied to various downstream code understanding tasks. The model learns the global understanding of code under the new paradigm heterogeneous image, connects the encoding space of text and code, and encodes the input of text into the vector of code most similar to it. Using self-supervised comparative learning on 456,360 text-code pairs, the model achieved a zero-shot prediction of new data. This work is the basis for future work on code generation using diffusion techniques under a new paradigm to avoid autoregressive limitations.", "sections": [{"title": "I. INTRODUCTION", "content": "Some studies have found that there are several limitations of the GPT's autoregressive next-word prediction paradigm. Microsoft Research notes that GPT-4 lacks planning, working memory, ability to backtrack, and reasoning abilities [1]. For example, although the model has enough knowledge to answer the question, the architecture of the GPT-4 cannot give the whole correct answer at once, but needs to be guided step by step to give the correct answer. The autoregressive nature of GPT-4 forces it to solve problems sequentially. Google Deepmind notes that GPTs are not applicable to most areas of mathematics due to the high cost of converting human proofs into a machine-verifiable autoregressive format [2]. And permuting the premise order can cause a performance drop of over 30% [3]. When producing full natural-language proofs on a set of geometry problems from the IMO competitions, GPT-4 has a success rate of only 0%, often making syntactic and semantic bugs throughout its output, and showing little understanding of the geometry knowledge and the problem statements themselves. In summary, GPTs rely on a local and greedy process to generate the next word, without a global understanding of the task or the output [1].\nWe further do empirical studies in the field of code un- derstanding and confirm the limitations of the autoregressive paradigm. For example, in terms of code understanding and generation, GPT-4 cannot understand complex logic with multi-step operations and generates incomplete code. More- over, GPT-4 relies excessively on the form of the prompt. Even if the input text has the same meaning, but just different syntax, the generated code will be much different and often contain bugs. GPT-4 is also not good at matrix operation. It often introduces new bugs while fixing matrix operation's bugs. But after giving the prompt step by step, GPT-4 can generate a correct answer, which shows that the model is actually trained with enough knowledge, but the autoregressive paradigm makes it not answer the question well. All of these issues confirm the limitations of GPT-4 noted by Microsoft and Google. The limitations makes it overly dependent on the given prompt, unable to understand the meaning of the code globally, and also unable to generate complete and correct code including multi-step operation/matrix operation at one time.\nDiffusion technology has made significant progress in the generation of images [4, 5] and life molecules [6] recently. It is a wonder that the model does not have limitations like that of GPTs. The model can learn the global information of the picture/life molecule, and generate the picture/life molecule at one time instead of step by step. Diffusion is a class of latent variable models inspired by considerations from non-equilibrium thermodynamics, which was originally used to generate high-quality images [7]. In particular, the CLIP+diffusion model (Dalle2) can create new pictures that have never been seen in the data set according to the input text, such as panda mad scientist mixing sparkling chemicals, artstation [4, 5]. The model seems to really understand the image. Where CLIP is a transferable visual models from natural language supervision, which provides text embedding vector for diffusion model to generate high-quality images [8]. The core idea of CLIP is to embed images and texts into"}, {"title": "II. MOTIVATION", "content": "GPTs' autoregressive next-word prediction paradigm has some limitations, such as lack of planning, working memory, ability to backtrack, and reasoning abilities, and do not apply to most areas of mathematics [1, 2]. Researchers believe that GPTs lack a global and deep understanding of the task or the output because they rely on a local and greedy process of generating the next word [1]. In addition, during the long-term use of GPT-4 in the field of code understanding, we further confirmed the limitations through following cases studies."}, {"title": "A. Cases Studies", "content": "1) Lack of ability to handle complex logic: We asked GPT- 4 some code problems with complex logic and multi-step operations, such as \"My training data is ndarray with shape (359, 4), which means 359 arrays, and each array contains 4 lists (length is 1149, 3000, 3000, 18 respectively.).The label of the training data is ndarray with shape (359,). Now, I want to train an embedding network whose input is the training data and whose output is an ndarray with shape (359, 4), representing 359 arrays, each containing 4 lists of length n. The output is an ndarray with shape (359, 4). How should it be implemented? \" The statement may seem convoluted, so we have added a summarising \"In a nutshell, we want the training data to be input into this embedding network, and each array contains 4 lists that are embedded to the same length n.\u201d Although human beings can understand the meaning of the problem. GPT-4 gives us incomplete code full of logical bugs as shown in Fig. 1. E.g., GPT-4 does not understand the shape of the output we want (batch size, 4), and wrongly concatenates 4 lists of each item. The training process is also not complete, e.g., there is no for loop of epochs. The variables indicated by red wavy line are also not explained.\n2) Lack of working memory: If we ask the same question again after some rounds of dialogue because we forgot the early answer, GPT-4 often give a completely different answer. This will result in almost all dialogue rounds being invalid. For example, we repeated the GPT-4 question \"How do I use a terminal command to output a commit that changes more than n files in a repository?\" twice, with several rounds of dialogue between them. GPT 4 answered two different wrong answers as shown in Fig. 2 (a) and (b) respectively."}, {"title": "B. Discussion", "content": "1) Using CLIP+Diffusion [4] in image understand- ing/generation can avoid some limitations similar to using GPTs in code understanding/generation: As shown Fig. 5(a) and (b), even if a piece of text with complex logic is input, CLIP+Diffusion can globally understand the text and correctly generate images with complex structures. The lines and colors in (a) are complex, precise, and smooth. The complex fur and texture in (b) are generated clearly. Even if the form of the input text is changed, as long as the meaning is the same, the generated image is still correct. In addition, CLIP+Diffusion also has creativity. As shown Fig. 5(c) and (d), panda mad scientist and a cat dressed as french emperor napoleon holding a piece of cheese hardly ever appear in real life and the training data. But CLIP+Diffusion technology can create unprecedented images based on human descriptions. Therefore, we believe that if this technology is applied to code understanding/generation, it will greatly improve performance in this area.\n2) In contrast to CLIP+Diffusion for image under- standing/generation, why do GPTs for code understand- ing/generation have the above limitations: We find that the CLIP model would memorize the entire image information globally while training, rather than only memorizing the previous code to infer the next token as GPTs do. For example, CLIP learned about the global distribution of fur color and texture, as well as the structure and position information of facial features of many pictures of shiba inu. Thus, the image representation corresponding to the textual description given by the human output by CLIP already has global information about the image [4]. Then Diffusion further restores a high- quality image on this image representation.\nHowever, due to GPTs adhering to the paradigm of predict- ing the next token, the model is unable to globally remember fully functional code information. As shown in Fig. 1, if we ask GPT-4 to generate a logically complex code, even if GPT-"}, {"title": "III. APPROACH", "content": "The code has a highly standardized structure and is not as flexible as the structure of natural language, so we first break the commonly used autoregressive next-word prediction paradigm for code understanding and generation, and propose a novel single-channel, one-dimensional, heterogeneous image paradigm. Both code and image have heterogeneity, meaning that both image and code are composed of different compo- nents. Therefore, we refer to images and consider different components in the code (e.g., classes, methods, variables, operators, numbers, symbols) as different components in the image (e.g., red, yellow, blue, green, etc.). Different entities within the same component, e.g., class 1 and class 2, etc. in class) are represented by similar numerical values (IDs) to simulate the pixel values of different entities within the same component in the image (e.g., dark red, light red, pink, etc. in red). To prevent significant differences in the values assigned to tokens due to too many different tokens (OOV problem), we take a series of measures: 1) Clean the code. Replace the messy data that will affect the accuracy of code tokenization with placeholders of different categories. For example, the messy string output by the print function is occupied with 'STR'. This step is optional and needs to be ensured that removing such messy data will not reduce the accuracy of the text-to-code matching results. 2) Divide tokens of entities such as the definition of classes, methods, and variables into built-in tokens in the code library and user-defined tokens,"}, {"title": "A. Heterogeneous image paradigm", "content": "4 has already learned the complex code, because it can only predict the next token based on the previous code, GPT-4 cannot generate a structurally complete code. Other limitations of GPTs should also be caused by this paradigm. Specifically, after a few rounds of dialogue, when we ask the same question again, due to changes in the previous text, the answers given by GPT-4 are completely different. And, questions with the same meaning but different forms are different previous texts, so GPT-4 will generate different answers. In addition, the reason why GPTs are not good at handling matrix operations and has creativity is because it does not truly understand the code. Understanding code requires learning the entire code information globally during the training process, rather than local context. Overall, we believe that GPTs' autoregressive next-word prediction paradigm is the fundamental reason for the limitations of GPTs.\n3) Why is the autoregressive paradigm adopted by GPTs to understand/generate code: Autoregressive paradigm is used for understanding and generating code by GPTs because it is mainly applicable to understanding and generating natural language. Both code and natural language are tools that people use to communicate. Natural language is used for communication between humans, while code is used for communication between humans and computers. Both natural language and code are token sequences with specific syntax rules. However, natural language is more flexible than code. Natural language can change according to different contexts, e.g. the meaning of words can change according to the context. That is, the same words have ambiguity in different contexts. Therefore, the un- derstanding and generation of natural language require a strict reliance on contextual analysis. This is also the main reason why GPTs use autoregressive paradigm. Although Transformer encoders can globally learn the information of the entire text and classify the predicted words, Transformer decoders still use the autoregressive paradigm in the generation process [9]. This paradigm generates tokens step by step to ensure semantic correctness, relying on the previous text for each step, rather than outputting complete sentences. And GPTs only adopt the decoder of the transformer. We summarize the above analysis in columns 1 and 3 of table I.\n4) What are the challenges and solution in proposing a new paradigm that can avoid the limitations of GPTs: To avoid the limitations of GPTs, we attempt to mimic images and propose a new structured coding paradigm for code, making it easier to apply CLIP+Diffusion technology. But we found that the code not only has features of images but also features of natural language. This has led to the challenge of proposing a new paradigm. As shown in columns 2 and 4 of table I, in terms of imitating images, we find there are similarities between codes and images. Images consist of components/colors (e.g., yellow, blue), and each component contains different entities (e.g., light yellow, dark yellow for yellow). Code consists of components/types of entities (e.g., classes, methods, variables), and each component contains different entities (e.g., method 1, method 2 for method). We call the property an object is composed of different types of components is heterogeneity."}, {"title": "B. Contrastive Language-Code Pre-training", "content": "1) Architecture: We adopt a similar architecture of the Con- trastive Language Image Pre-training (CLIP) model [8], and propose the Contrastive Language Code Pre-training (CLCP) model. During the training phase, we jointly trains an code encoder and a text encoder to predict the correct (code, text) pairs of a batch. As shown in Fig. 7, given a batch of N (code, text) pairs, CLCP is trained to predict which of the $N \\times N$ possible (code, text) pairings in the batch are the correct pairs. CLCP learns a multi-modal embedding space by jointly training an code encoder and a text encoder. In view of the strong advantages of the pre-trained transformer model in understanding natural language [9-12], we continue to train directly on the model with our data. However, because we have adopted a new code coding paradigm, we have designed a new code encoder, which will be introduced later.\nThe model parameters are updated by the loss function, which maximizes the cosine similarity $C_i T_i (1 \\leq i \\leq N)$ of the code embeddings $C_i$ and the text embeddings $T_i$ of the correct pairs N in the batch while minimizing the cosine similarity $C_iT_j(1 \\leq i, j \\leq N, i \\neq j)$ of the code embeddings $C_i$ and text embeddings $T_j$ of the incorrect pairings $N^2 \u2013 N$. Specifically, we optimize a symmetric cross entropy loss over these similarity scores [8]. We expect these two encoders to be used for many downstream tasks for code understanding in the future, such as using a text encoder to encode input text into a vector in the multi-modal embedding space and then inputting that vector into the diffusion model to restore high-quality code. Alternatively, use code encoders in different languages to translate code written in one programming language into code written in another programming language, etc."}, {"title": "2) Code encoder based on one-dimensional convolution:", "content": "For the code encoder, we refer to two-dimensional convolution for learning on images [13], and design a one-dimensional convolutional and pooling neural network suitable for one- dimensional heterogeneous images. As shown in Fig. 8, the source code is first encoded in one-dimensional heterogeneous images according to the new encoding paradigm. Secondly, we use one-dimensional convolution to convolve one-dimensional heterogeneous images with a kernel size of k and a step size of s to learn the semantic and structural information of the code. That is, the convolution kernel slides on the input one-dimensional image with a step size of s, and at each step the IDs in the one-dimensional image are multiplied by the corresponding weights in the convolution kernel and summed to obtain the IDs of the dimensions of the output new vector/feature map. Each value of the new vector will be activated by the Relu function. Third, a one-dimensional (1D) max or average pooling is applied to the activated new vector, with window size k' and step size s'. The basic idea of pooling operation is to divide the input 1D image (also known as a vector) into several sub-vectors and calculate the maximum or average value of each sub-vector as the value of each dimension of the output vector. Overall, convolution operations are mainly used to learn semantic and structural information between code tokens, while pooling is mainly used to aggregate information from sub-code fragments. Finally, the code encoder consists of M blocks, each containing a convolution layer and a pooling layer."}, {"title": "3) Strategies for optimizing the training process:", "content": "We use Relu function [14] to activate the output of the convolutional layer and use the He initialization [15] method to initialize the parameters of the convolutional layer. Compared with tradi- tional Sigmoid and Tanh activation functions, Relu has a linear relationship when the input is greater than 0, which means it will not saturate (i.e. the gradient will not approach 0). This helps to solve the problem of gradient vanishing, especially in deep networks [16-18]. However, in deep networks, if the weight values are large, the accumulation of gradients during backpropagation can lead to gradient explosion problems [15, 18]. So we use He Initialization [15] to avoid the above problem. The core idea of Kaiming initialisation is to adjust the initial values of the weights based on the number of input nodes, in order to maintain consistency in the variance of the activation values across all layers. Specifically, if a neuron has n input nodes, its weights are initialised to a Gaussian or uniform distribution with a mean of 0 and a variance of $2/n$. The advantage of this is that no matter how deep the network is, the distribution of activation values will remain within a reasonable range, avoiding the problem of vanishing or exploding gradients and making the network easier to train.\nIn addition, Batch Normalization (BN) is a commonly used technique that normalizes each batch of data. The BN layer can reduce the data distribution changes caused by parameter variations in the network, thereby avoiding gradient vanishing or exploding problems during training and improving the con- vergence performance of the model. However, by normalizing the input of each layer, BN may suppress the diversity of some representations [19] because BN forces each layer's input to have similar mean and variance during training, which may result in the model losing sensitivity to some important changes and diversity in the input distribution. This means that BN may not be applicable to certain types of neural networks, especially those that require preserving certain characteristics of the input distribution. For example, CNN models used for image classification typically have the first few layers respon- sible for extracting low-level features of the image, such as edges, corners, textures, etc. In these layers, each feature map"}, {"title": "IV. EXPERIMENTS", "content": "The goal of this paper is to propose a new code encoding paradigm and design an applicable text-code encoder training model that can be migrated to different downstream tasks of code understanding so that the model may avoid the limitations of the autoregressive model on code understanding. Following questions are set on 4$\\times$NVIDIA Tesla V100 with 32GB RAM:"}, {"title": "A. Research Questions", "content": "RQ1. How effective is CLCP on Zero-Shot Transfer? It is difficult for the proposed CLCP to achieve high accuracy on the zero-shot task with a limited amount of data (456,360 (code, text) pairs) compared to the dataset (400 million (image, text) pairs) used for the CLIP model. For example, some previous work also used natural language supervision for image representation learning like CLIP [20-23]. However, due to the limited amount of training data, their effects are not as amazing as those of the CLIP (e.g., Li et al [20] reach only 11.5% accuracy on ImageNet in a zero-shot setting), so they have not attracted enough attention. Collecting a large amount of high-quality data is still an arduous task in the future. In this paper, to evaluate whether the proposed method has application value, we change the size of training and testing sets to magnify the effect of the model on the zero-shot task and evaluate whether the model can effectively learn the global information of the code.\nRQ2. Ablation experiment. In this RQ, we conducted some ablation experiments to explore training strategies for learning code heterogeneous images, rather than training strategies that are applicable to all models. Specifically, pooling operations reduce the dimensionality of feature maps by aggregating information from adjacent regions, which inevitably results in the loss of some detailed information. For example, in max/average pooling, only the max/average value within each region is retained, while all other values are discarded. This type of information loss can gradually accumulate in multi- layer networks, leading to poor model performance when fine features are required. Therefore, in this RQ, we removed the pooling layer of CLCP model to verify its effectiveness.\nIn addition, we use He initialization [15] to avoid gradient explosion problems in backpropagation, and do not use BN to avoid blurring the distribution characteristics of the input image. In this RQ, we remove He initialization and add BN respectively to verify the effectiveness of the optimization strategy we adopted during the training process.\nRQ3. Will the prompt engineering improve the results? Prompt engineering is a common method to improve the effect of model based on natural language supervision [8- 11]. We observed that the description of the code in the dataset is complex and redundant. The common categories of redundancy is summarized in table III. So we try to clean up and reconstruct the text to improve the experimental results."}, {"title": "B. Dataset", "content": "Different program languages need different code parsers to make vocabularies. Therefore, the number of tokens and the corresponding IDs are different in different programming languages. To reduce the workload of program language preprocessing, this paper makes an exemplary evaluation in Python. We use the Python data set of the CodeSearchNet corpus, including 456,360 (code, text) pairs in the training set and 22176 pairs in the testing set. The CodeSearchNet corpus contains multiple public code libraries from GitHub, covering a variety of programming languages (such as Java, python, JavaScript, etc.). The public code libraries contain a large num- ber of code snippets from a variety of open-source software repositories, and the document comments corresponding to the code snippets help to understand the function and purpose of the code. This corpus has been initiated and maintained by GitHub to encourage the development of code-understanding technologies. We selected 13,760 samples in the testing set that were completely different from the training set category by identifying different text descriptions and manually verifying them. These samples were used as an initial data set to evaluate the accuracy of the model in zero-shot tasks."}, {"title": "C. Experimental Setup", "content": "Because the amount of data is limited, we imitate the works [20\u201322, 24] referred to in the clip paper to do some preliminary exploration to verify the proposed approach is effective (e.g. the accuracy in [20] is only 11.5%), instead of pursuing the effect of training under 400 million data as in the clip model.\n1) Settings for RQ1: Dataset size: We observe whether the proposed approach is effective on the zero-shot transfer task by gradually increasing the size of the dataset and the depth of the model. As for data set size, each time we randomly sample datasets of different sizes to evaluate the improvement of the model in zero-shot transfer compared to the results of random prediction. Specifically, the size of the training set is 30000,"}, {"title": "D. Experiment Results", "content": "Answer to RQ1: As shown in Fig. 10 (stacked area chart), the legend represents the size of the model/the size of the training set. The different colors represent the growth in the model accuracy in different model sizes and dataset sizes. We can see that as the sizes of the training set and model increase, the performance of all models improves on the testing sets of different size. And all models perform better than the estimated Acc(EA) of random prediction. In particular, the overall effect of variants using global pooling is not as good as that of variants using local pooling. This may be because global pooling compresses the entire feature map into a single value by averaging or maximizing operations, which to some extent loses some important local information. If this information is\nAnswer to RQ2: Fig. 12 shows the decrease in accuracy of models CLCPip and CLCPrn compared to the original model when adding or removing different components. EA represents the estimated Acc of the baseline random prediction. The vertical axis represents adding (+) or deleting (-) components, and the horizontal axis represents decreased average ACC. of CLCPip and CLCPrn with different sizes across all different training/testing dataset sizes. The model size and dataset size are the same as that of RQ1. Legends of different colors represent different original models.\nAs shown in fig. 12, removing the pooling layer did not alleviate underfitting as expected, but resulted in a significant decrease in accuracy. The accuracy of CLCPip-Pool is even lower than that of random prediction. We observed that after 2-3 epochs of training, the validation loss of CLCPip-Pool and CLCPrn-Pool no longer decreased, which means that the training process of the models fell into local optima. There- fore, after removing the pooling layer, the model experienced overfitting. In addition, as expected, after removing He initial- ization, the performance of both CLCPip-Init and CLCPrn- Init decreased compared to the original model. After adding the BN layer, the performance of CLCPip and CLCPrn also decreased. This may because BN independently normalizes each batch of code feature maps, which may disrupt the relative differences between feature maps.\nAnswer to RQ3: As the sizes of the training set and model increase, the performance of CLCP with and without prompt engineering on the testing sets of same and different size are show in Fig. 13 and 14. The overall performance of CLCP without prompt engineering (CLCPlp and CLCPrn) is worse"}, {"title": "V. THREAT TO VALIDITY", "content": "Internal validity. Due to hardware and data limitations, we did not use as many datasets as OpenAI used to train CLIP models to train CLCP models. We simply followed CLIP's previous work of proposing text-to-image (which was only initially trained and tested on limited data) and proposed an initial text-to-code work. This results in the trained CLCP being not robust and cannot be directly used for downstream tasks of code understanding. The main purpose of this paper is to provide a new perspective for researchers engaged in code understanding. In future work, we will collect more datasets and apply for more funding to evaluate our model on better hardware environments. And because we did not use pre-trained embeddings for code encoding, but instead proposed a new encoding paradigm and retrained it, we did not compare with other works that used ready-made embeddings.\nExternal validity. The source code of clip model is not open, and many training details have not been published. Therefore, we replicated and improved its framework ourselves, which to some extent reduced the performance of proposed model."}, {"title": "CONCLUSION", "content": "Microsoft Research and Google DeepMind have found many limitations in the GPTs' autoregressive paradigm, man- ifested in the model's lack of planning, working memory, backtracking, and reasoning skills. Microsoft Research found that GPT relies on a local greedy process of generating the next word, without a global understanding of the task or output. We further summarized the limitations of the autoregressive paradigm on code understanding through empirical research. That is to say, GPTs cannot handle complex logic and generate new code that has not been seen before, and they rely too much on the format of prompts to generate correct code. To address this issue, we propose a new code encoding paradigm inspired by the successful application of diffusion techniques in image generation, such as the Dalle2 and Sora models. We first discovered that the structure of code has both natural language characteristics and image/protein molecular chain characteristics, and encoded the code into a heterogeneous image paradigm with global information memory that mimics the structure of images and proteins. Then, we refer to Sora's CLIP upstream text-to-image encoder model and design a text- to-code encoder model that can be applied to various down- stream code understanding tasks. This model learns the global understanding of code under the new paradigm, connects the encoding space of text and code, and encodes text input into the most similar code vector. By conducting self-supervised comparative learning on 456360 text code pairs, the model achieved zero-shot prediction for new data. This work is the foundation for using diffusion techniques to generate code in a new paradigm to avoid autoregressive limitations in the future."}]}