{"title": "Possible principles for aligned structure learning agents", "authors": ["Lancelot Da Costa", "Tom\u00e1\u0161 Gaven\u010diak", "David Hyland", "Mandana Samiei", "Cristian Dragos-Manta", "Candice Pattisapu", "Adeel Razi", "Karl Friston"], "abstract": "This paper offers a roadmap for the development of scalable aligned artificial intelligence (AI) from first principle descriptions of natural intelligence. In brief, a possible path toward scalable aligned AI rests upon enabling artificial agents to learn a good model of the world that includes a good model of our preferences. For this, the main objective is creating agents that learn to represent the world and other agents' world models; a problem that falls under structure learning (a.k.a. causal representation learning). We expose the structure learning and alignment problems with this goal in mind, as well as principles to guide us forward, synthesizing various ideas across mathematics, statistics, and cognitive science. 1) We discuss the essential role of core knowledge, information geometry and model reduction in structure learning, and suggest core structural modules to learn a wide range of naturalistic worlds. 2) We outline a way toward aligned agents through structure learning and theory of mind. As an illustrative example, we mathematically sketch Asimov's Laws of Robotics, which prescribe agents to act cautiously to minimize the ill-being of other agents. We supplement this example by proposing refined approaches to alignment. These observations may guide the development of artificial intelligence in helping to scale existing or design new-aligned structure learning systems.", "sections": [{"title": "1 Introduction", "content": "This paper examines the challenge of developing scalable aligned AI agents following biomimetic princi-ples. We consider the research questions to be addressed, along with guiding principles; providing a broad perspective that synthesizes ideas across mathematics, physics, statistics, and cognitive science.\nA first principles approach to intelligence: We aim to be inclusive about\u2014and relevant to-naturalistic approaches to artificial intelligence. For this we commit to a 'first principles' approach to modeling intelli-gence known as active inference [1-3]. Active inference is not divorced from other naturalistic approaches to modeling intelligence, but rather aims to accommodate them within a broader framework. Active inference follows a long lineage of ideas, perhaps originating from Helmholtz's motion of perception as unconscious inference, which was reincarnated in the neurosciences at the turn of this century as predictive coding, and generalized as the Bayesian brain hypothesis. Active inference was proposed shortly afterward, in the mid 2000s, extending these Bayesian accounts by postulating that action optimizes the same objective as per-ception and learning [4,5]. This account was suggested to be a potentially unifying brain theory, in the sense of being able to accommodate a wide range of previously existing and partially non-overlapping brain theories as special cases [6]. In light of the descriptive power of these ideas, researchers have sought to justify this account in terms of statistical physics, with increasing mathematical rigor and sophistication. These efforts have birthed a nascent field of non-equilibrium physics, known as 'Bayesian mechanics', that bridges stochastic descriptions of particles with inferential ones. This has been used to derive the active inference framework as we present it here, offering a description of sentient behaviour from first principles [7]. The active inference framework can be used to model a remarkable range of phenomena in cognitive science, rang-ing from human choice behavior [8] to psychopathology [9], to many known features of the brain's anatomy and physiology [10, 11], including the activity of neural populations [12, 13]- -see [1-3,14] for reviews. Active inference has more recently gained traction in machine learning and robotics- -see [15-17] for reviews of opportunities and challenges.\nLearning generative (world) models: The key challenge to unlock the usefulness of this naturalistic approach to AI at scale is to enable agents to learn their model of the world, as current approaches to address this remain limited (but see [18,19]). Note that this problem is not unique to active inference: it is shared across all of model-based reinforcement learning [20]. This structure learning\u00b9 problem forms the"}, {"title": "2 First principles approach to natural intelligence", "content": "We aim for a 'first principles' description of natural intelligence. To this aim, we summarize a physical theory describing the dynamics of things that actively interact with their surrounding world.\nNotation: In the following, and unless stated otherwise, we will denote stochastic processes on a finite time interval by lowercase letters and index these stochastic processes by time to denote their (random) value at some timepoint(s). We denote by P their probability distribution.\nThe setup: We summarize, under minimal assumptions, the various components of a particle that has internal states, such as an organism or agent (see Figure 1 for an illustration): Consider the world x, comprised of an object of study\u2014such as a particle, organism or agent\u2014and its environment. This partition implies a boundary through which states internal to the particle interact with external states. Thus, the world process x partitions into a process external to the agent \u03b7, a process internal to the agent \u03bc, and a boundary process b. Explicitly: x = (\u03b7, b, \u03bc). We further decompose the boundary process b into two processes; those that are not directly influenced or caused by the external and internal processes respectively (which may or may not be empty): we call these the active a and sensory o processes respectively b = (o, a). Here, we can interpret the distribution of the world as a generative model for how external processes affect the agent, i.e. Bayes rule\n$\\begin{equation}P(\\eta | o, a, \\mu) P(o, a, \\mu) = P(o, a, \\mu | \\eta) P(\\eta)\\tag{1}\\end{equation}$\nMaximizing model evidence: A tautology here is that the most likely internal and active dynamics will maximize the evidence for a generative world model (1). Precisely, the more likely a trajectory of active and internal processes (given a sensory trajectory), the higher the model evidence and vice-versa. This is a simple observation that underwrites all that follows: we can frame internal and active dynamics of things as optimizing one single objective: the evidence for a generative model of the world. In what follows, we review characterizations of these self-evidencing dynamics in natural systems. The following characterizations assume some functional form to the dynamics of the world, usually (but not limited to) a stochastic differential equation, as these form the basis of a large part of physics, for instance statistical"}, {"title": "2.1 The active inference framework", "content": "This theory underpins a normative framework for modeling and simulating the internal and active dynamics of things, such as cognition and behavior, known as active inference [1-3,35]. In active inference, internal and active dynamics are taken to maximize the evidence for a generative 'world' model P that specifies the interactions between external, sensory and active processes. This is instantiated by numerically minimizing variational free energy (2) and/or expected free energy (3). In other words, internal and active dynamics are a function of and only of the generative model. The problem for simulating aligned intelligent behavior therefore rests upon choosing the right kind of generative model. This is an open problem which forms the focus of this article.\nWe outline two features of active inference that will be relevant later and which contextualize it with other approaches to behavior:\nModel evidence guides behavior: In active inference, the goal of behavior is to maximize the evidence for a generative model of the world. This means that the agent's generative model of the world describes how things should behave from its perspective and behavior simply fulfills these preferences. For example, if we consider the cost function for active and internal trajectories, which is the expected free energy, this decomposes into risk and ambiguity, where risk is the KL divergence between predictions and preferences, a prediction error which the agent seeks to minimize:\n$\\begin{equation}G = D_{KL} [P(\\eta | \\mu, a, d) | P(\\eta | d)] + E_{P(\\eta|\\mu,a,d)} [-log P(o | \\eta, d)] \\tag{4}\\end{equation}$\nThe dependence of preferences on the data means that these can be inferred\u2500i.e. learned\u2014over time [34,36]. This decomposition into risk and ambiguity has technical implications for AI safety that we will develop in Section 5. There are no native reward or utility functions in active inference, but the expected free energy can be connected to reinforcement learning if we interpret log probabilities as reward functions [37,38]. On this reading, the expected free energy is a conservative bound on expected utility plus expected information gain [32].\nDrawing the boundary around the agent's brain. Another feature of active inference is that the agent's body is usually modeled as part of the external process. That is, when modeling intelligent agents like ourselves, the boundary between internal and external is typically drawn around the agent's brain as opposed to around its body. For example, to simulate an arm movement in active inference, the locations of the arms will be part of the external process, the sensory process will be the brain's sensations about the arm's locations, and the actions would be the ways in which the brain can influence these locations [8]. This contrasts with most reinforcement learning schemes [39]."}, {"title": "3 Bayesian structure learning", "content": "Structure learning, here used synonymously with causal representation learning, is the problem of learning the mechanisms of cause and consequence in the data generating process [21, 23]. This is a fundamental problem in causality, cognitive science and artificial intelligence: Indeed, cognitive development can be seen as a structure learning process [40, 41] and structure learning may be a way toward human-like artificial intelligence by starting from a child's mind and gradually growing it into an adult mind, as already argued by Turing [42].\n3.1 The problem\nThe data generating process is an unknown (causal) Bayesian network \u03b7, with unknown latent variables and causal relationships. The reason for this is fundamental: Bayesian networks are a natural mathematical for-malism for accounting for random variables and their causal relationships [43]: all data generating processes"}, {"title": "3.1.1 Optimizing marginal likelihood...", "content": "We want to obtain a generative model P(d, \u03b7), that maximizes the model evidence P(d) (a.k.a., marginal likelihood) for the data. This formally furnishes a minimal length description for the data [44,45]. The log evidence factorizes into accuracy minus complexity\n$\\begin{equation}log P(d) = E_{P(\\eta|d)} [log P(d | \\eta)] \u2013 D_{KL}[P(\\eta | d) | P(\\eta)]\\tag{6}\\end{equation}$\nwhere accuracy quantifies how much posterior beliefs accurately fit the data and complexity quantifies how far the posterior moves from the prior. Maximizing accuracy entails maximum likelihood inference, while minimizing complexity enforces a constrained maximum entropy (technically minimum relative entropy) that regularizes the posterior. The complexity can also be seen as a proxy for the computational cost of inference, and via Landauer's principle, energetic cost [46]. In short, optimizing the marginal likelihood with respect to some data yields models that are maximally accurate, but also minimally complex, instantiating a form of Occam's razor."}, {"title": "3.1.2 through a variational bound", "content": "Because the marginal likelihood is intractable to compute exactly, we optimize a variational bound: the variational free energy For evidence lower bound. In particular this entails approximate Bayesian inference over the latent Bayesian network \u03b7, by optimizing an approximate posterior distribution Q(\u03b7) over the network structure m, parameters \u03b8, and states s; see Figure 2. From (5):\n$\\begin{equation}\\tag{7}\\end{equation}$\nIn the last line of (7), we exploit the fact that the approximate posterior distribution factorizes as Q(m, \u03b8, s) = Q(s | m,\u03b8)Q(\u03b8 | m)Q(m) so that it is possible to decompose the problem into hierarchical inference about states, parameters and structure.\nRemark 3.1 (Encoding uncertainty about structure). Compare the problems of maximizing the evidence (6) with the problem of finding the structure with the highest marginal likelihood: i.e. arg maxm P(d | m). The latter can be seen as doing maximum a posteriori inference (MAP) about structure\u2014i.e. Q(m) is a point mass in (7) with a uniform prior P(m) over structures. This also corresponds to maximizing the likelihood of the structure given the data (i.e. maximum likelihood). However, our prior knowledge about structures is generally not uniform, making the prior P(m) non-uniform. Furthermore, in the finite and even infinite data regime there may be multiple structures with the same likelihood (i.e. unidentifiability [22,47]), implying that entertaining one single structure is prone to over-fitting. To avoid this, it helps to entertain a richer family of approximate posterior distributions that encode uncertainty about structure in (7).\nMaximizing the marginal likelihood over Bayesian networks by optimizing the variational bound (7) is a very difficult problem to solve at scale [48, 49]. One of the main intrinsic difficulties lies in the fact that the number of possible causal networks increases super-exponentially in the number of latent variables [50], hence the space of models that might explain any given dataset a priori is huge. In the following, we discuss ways to optimize the variational bound with respect to the prior and approximate posterior, with the aim of producing more scalable methodologies."}, {"title": "3.2 The prior: model reduction", "content": "The prior P(\u03b7) should represent the prior state of knowledge about the external world and not overcommit to certain hypotheses a priori when they are not directly supported by prior knowledge. For example, it is common to argue that the prior should be the maximum entropy distribution that is consistent with prior knowledge when this is expressed in terms of constraints on that distribution [51].\nBayesian model reduction [1,52,53] is an extremely effective computational tool for selecting better priors after receiving some data. The idea is to have a collection of prior distributions P\u03bb(\u03b7) \u2261 P(\u03b7 | \u03bb) indexed in some set \u03bb\u2208 \u039b. Then the model evidence (and posterior) become dependent on A even though the likelihood is fixed"}, {"title": "3.3 The prior: information geometry", "content": "The space of models has some structure to it\u2014intuitively, a geometry and this structure should be accounted for in the choice of prior, and in the variational inference problem at hand.\nRegarding the prior, if two models express the exact same information, they should be assigned the same prior probability, and if they express a similar amount of information they should be assigned a similar prior probability see the illustration in Figure 3. Mathematically it seems that there ought to be an information geometry (i.e. a notion of distance) on the space of models that expresses the extent to which two models differ in their information content, and the prior should be continuous in the associated topology (i.e. map similar models to similar probabilities).\nAn information geometry is induced by a distance or divergence [54, 55]; so what is the natural information distance or divergence on the space of models and what might be ways of tractably implementing this ideal in practice? The difficulty with these questions is that the space of models seems to be a stratified space, i.e. a disjoint union of different strata where each strata is a space of probability distributions on the same underlying space; that is, a set of all models with the same joint space of states and parameters."}, {"title": "3.4 The posterior: approximate inference over structures", "content": "It remains to optimize the variational bound (7) with respect to the approximate posterior Q(m): i.e. variational inference. We focus on how to infer structures variationally by optimizing Q(m) to match P(m | d) in (7). This is because states and parameter inference given a structure is a solved problem in the cases we will consider later [57-59]. The space of structures m is inherently discrete and thus, the posterior distribution P(m | d) is a categorical distribution. This means that the approximate posterior Q(m) must also be categorical. We summarize the representative approaches to structural inference based on the parameterization of the approximate posterior (see Figure 4 for an illustration):\n1. Particle approximate posterior . This is when the variational inference method entertains a (typically small) number n \u2265 1 of structures mi, which are optimized to capture the modes of the posterior distribution, and whose respective posterior probabilities \u03bci are optimized accordingly. In this setting, we can optimize the structures being considered by making small or large updates:\n\u2022 Local updates:\n(a) Markov chain Monte-Carlo (MCMC) approaches run a stochastic process on the space of structures to sample the true posterior. Samples are produced sequentially by the pro-cess according to some stochastic rule (e.g. adding nodes in a Bayesian network with some probability). The process is ensured to converge to the target in distribution through some consistency procedure like Metropolis-Hastings [60,61], and may be optimized in various ways to increase the speed of convergence [62-64].\n(b) Constrained continuous optimization approaches embed the space of structures as a con-straint set in a larger continuous space [65\u201367], thereby finessing the complexities of discrete space variational inference by allowing the use of mature toolboxes from continuous particle optimization, e.g. [68], to perform the inference.\n\u2022 Global updates:\n(a) Discrete particle variational inference is a variational inference procedure on discrete spaces where structures are updated through a conjugate free energy descent [69]."}, {"title": "4 Structure learning agents", "content": "We now turn to discussing agents that learn the causal structure of the world. The agentic setup is illustrated in Figure 5 (left panel): the agent is in dynamic exchange with the external process, whereby the current external state \u03b7t yields an observation ot, then the agent takes an action at, which influences the external process etc; and the perception-action cycle repeats. Compared to Section 3, the agent has access to an in-coming stream of (interventional) data t\u2194 d(t) \u2252 d comprised of past sensations and actions d \u2286 {0, a<t}, that is continuously updated at each cycle."}, {"title": "4.1 Model-based planning and multi-scale inference", "content": "Following Section 2, we propose to investigate this problem through the lens of active inference. Practically, this implies a commitment to model-based planning and multi-scale inference.\nModel-based planning: The agent possesses a generative model about the latent states, parameters, and causal structure describing the world (we will see examples later). It uses this model for planning, by optimizing an objective combining explorative and exploitative drives, such as the expected free energy (4).\nMulti-scale inference: The defining feature of Bayesian approaches to behavior is inferring the external process \u03b7 from the data d. This involves approximating posterior beliefs such as P(m,\u03b8,s | d) about (the past, present and future) structure, parameters and states of the world. This may be solved variationally (7) by updating an approximate posterior distribution Q(m, \u03b8, s) to match incoming data. As we have seen in (7), this inference can be hierarchically decomposed by inferring states Q(s | m,\u03b8) (i.e. perception), then parameters Q(\u03b8 | m) (i.e. learning) and then causal structure Q(m) (i.e. structure learning). Additionally, the agent may engage in Bayesian model reduction to simplify its model of the world.\nThese inferential processes may operate at different time-scales: perception faster than learning, which is faster than structure learning, which is faster than model reduction. This is because more data is needed for accurate learning than for perception, and even more so for accurate structure learning and ensuing model reduction. There is empirical evidence that the brain complies with this separation of time-scales: perception by neuronal populations is plausibly encoded in their firing rates which are fast processes-while learning is encoded in the modulation of neural connection strengths (i.e. Hebbian plasticity) that fluctuate much more slowly [12,13,78]. Could it be that causal structure is encoded in the functional connectivity between neural populations, and be updated even more slowly? Model reduction can plausibly be interpreted as pruning connections in or between neuronal populations [79], as is seen to happen during development and throughout life (e.g., during the sleep-wake cycle). In physics, processes operating at different scales are known as multi-scale processes [80].\nTo simulate this multi-scale inferential process in practice, one would set the learning rates in the optimization of Q(m) to be much lower than Q(\u03b8 | m), which are much lower than Q(s | m,\u03b8). For convenience, what is commonly done in practice is inferring states after every new observation, inferring parameters after every small batch of observations [1], and inferring structure after every bigger batch of observations and reducing the model after even larger batches. Specifying the respective batch sizes corresponds to specifying the relative timescales of the different inferential processes. In physics, this corresponds to an adiabatic approximation of a multi-scale process [80]. See the summary in Figure 5 (right panel)."}, {"title": "4.2 Related work", "content": "One very related line of work is theory-based reinforcement learning [81-83]. In a foundational paper [81], an agent maintains beliefs about probabilistic programs, which implicitly encode the causal structure, pa-rameters, and states of the world. The agent then optimizes expected utility plus information gain to select the next action (note the similarity with (4)). The authors deployed this architecture in a suite of simplified Atari games, and found that not only did their agent achieve human learning efficiency across the games (after comparing with data from human participants), but the agents' learning trajectories were also rela-tively similar to that of humans. This work serves as a proof of concept that combining inference about the structure of the world with model-based planning\u2014leveraging both exploration and exploitation can achieve human-level sample efficiency and performance and relatively human-like behavior.\nCurrent active inference schemes engage in multi-scale perception, learning, structure learning and model reduction [1,2,79,84]. Structure learning active inference agents is an area of active research and current schemes do hold beliefs about more than one alternative structure [19, 79, 85-87]."}, {"title": "4.3 Refining the search space of possible structures", "content": "Building agents that scalably learn causal models of the world are a relatively open challenge [21]. Perhaps the main difficulty is the explosion of the search space of possible structures that might explain more and more complex worlds [50]. To illustrate the problem, consider the above theory-based reinforcement learning work [81]. The search space of explanatory hypotheses about the world their agents consider is the whole set of programs (up to a certain length) that can be generated from the code generating the computer program that generates the data. This is an extremely large search space even for the simplified Atari environments their agents face, and a feat of this work is that structural inference is made tractable even so; however, this approach is obviously limited in its scalability: 1) in more complex environments, the space of programs that can be generated from the code grammar generating the environment may be far too large to be searchable, 2) in general the modeler does not know the generative process and cannot form a space of candidate explanations that contains the data generating process. We now investigate ways to address these shortcomings by respectively considering core knowledge priors and universal generative models."}, {"title": "4.3.1 Core knowledge priors", "content": "Core knowledge represents prior knowledge about the external world that would be valid across any world that the agent could be born in. As much as possible, this core knowledge should be reflected in the prior probability for potential model explanations for the world, to reduce the search space of possible explanations. For agents operating in naturalistic worlds, core knowledge may include an intuitive understanding of physics, e.g. statements like \u201cobjects cannot interact at a distance but agents can\", and many others [77,88].\nEvolution has carved this kind of core knowledge into our genome so that humans and animal newborns are born with rich prior knowledge about the world. For example, human infants are endowed with at least seven rich systems of core knowledge about objects, places, agents, numbers, geometry, social groups, and others' mental states [77,88]. These are shared by humans across ages and cultures and sometimes across several animal species [77]. One can think of the process by which evolution has learned this kind of prior knowledge as a process of evidence maximization on an evolutionary timescale [89].\nThis 'common sense' prior knowledge vastly improves the evidence for an agent's model of the world. Core knowledge avoids compromising model accuracy by precluding overly specific assumptions about the natural world and vastly reduces model complexity by restricting the search space of explanations for the world. This knowledge greatly aids structure learning: core knowledge provides a valid carving of the world into distinct classes of things (such as objects or agents) with distinctive properties, rather than leaving this as structure to be learned. Through this, core knowledge dramatically speeds up inference and learning; for instance, if two things seemingly interact at a distance, then it can be inferred with certainty that at least one of them is an agent.\nReverse-engineering human and animal systems core knowledge into priors over models or probabilistic programs is an ambitious and ongoing research effort [82,88]. Follow up work on theory-based reinforcement learning encoded core knowledge into soft constraints on the types of programs that might explain a given (Atari) world, and found that the agents followed more human-like learning trajectories with core knowledge than without such inductive biases [82]. In more complex worlds, we hypothesize that core knowledge priors become absolutely essential to learn with any efficiency.\nCore knowledge therefore constitutes knowledge that is valid across any naturalistic world, which translates into significant constraints on the prior over models. The prior over models as explanations for the world is then constrained by the consistency with the underlying information geometry (i.e. local constraints) and core knowledge constraints (i.e. non-local constraints).\""}, {"title": "4.3.2 Toward universal, interpretable, agentic generative models", "content": "A fundamental question is what might be a 'universal' set of primitives and compositional rules to produce a space of models as potential explanations for the world that is both [91]:\n1. Sufficiently expressive to be able to approximately express any kind of naturalistic, dynamic inter-actions between agent and environment.\n2. Sufficiently coarse so that inference on this space can be made computationally tractable.\nFurthermore, each model in this space should be:\n3. Interpretable so that the agent's understanding and ensuing behavior can be easily understood from the model it entertains.\n4. Supportive of fast action, perception and learning.\nDefinition 1 (Universal generative model). We call a space of models satisfying requirements 1-4 a universal space of models. A generative model based on a universal space of models is therefore apt to causally explain any kind of naturalistic world; we will call this a universal generative model.\nThere is already a tension between requirements 1 and 2 and a significant difficulty lies in balancing these requirements. Asking what a universal space of models might look like, we first consider the existing lit-erature: Spaces of probabilistic programs are easily made extremely expressive, but it is not clear how to do so while keeping them coarse enough for inference to remain tractable. Probabilistic programs are not always easily interpretable, and, barring specific assumptions, do not support efficient perception and learn-ing, as Bayesian inference over states and parameters may require sampling. One example of probabilistic programs that might satisfy these requirements to a first approximation\u2014are hierarchical discrete and con-tinuous state partially observed Markov decision processes (POMDPs) [92,93]. Indeed, it has been shown that dynamic models with continuous random variables interacting across time are capable of performing Turing-complete computation [94]. Moreover, it is striking that nearly all modeling work in active infer-ence, which spans nearly two decades, employed models that are built by hierarchically stacking these two types of layers [1,2,14,15]. This might be a bias, but it nevertheless indicates that this space of models is very expressive in being able to reproduce a wide variety of behavioral simulations and empirical data. Importantly, these networks support fast action, perception and learning where inference about states and parameters is implemented with fast variational inference procedures [1,2, 35, 95, 96], which have a degree of biological plausibility in being able to reproduce a wide range of features from real neural dynamics, e.g. [10,12,13,97]. Barring the use of neural networks for expressing non-linearities in these layers [17], each of the layers furnishes an interpretable model of dynamics."}, {"title": "4.3.3 Expressivity in terms of stochastic processes", "content": "From this, we might envision a set of basic structural modules satisfying requirements 3 and 4 that can be assembled hierarchically to express a wide range of dynamic agent-environment interactions. Here, we pursue this line of thought by describing two building blocks that can be combined to express a large class of stochastic processes on both discrete and continuous states [91].\nDiscrete dynamics: Markov processes are a fairly ubiquitous class of stochastic processes [98]. All Markov processes on discrete states have simple transition dynamics that are given by linear algebra. When these transitions also depend on actions, we obtain a Markov decision process. When states are partially observed and observations depend only on the current latent state, we obtain POMDPs. We can add auxiliary latent states to those POMDPs [99] (i.e. the equivalent of momentum, acceleration etc) to account for the effect of memory in the system, producing semi-Markovian POMDPs. Lastly, we can stack these layers hierarchically to express multi-scale semi-Markovian processes. In summary, extended discrete POMDPs hierarchically compose a very general class of models for agent-environment interactions on discrete states. See Figure 6 for a graphical representation of discrete POMDPs and their various degrees of freedom.\nContinuous dynamics: For expressing continuous dynamics, the situation is somewhat more involved. Repeating the construction from discrete state-spaces seems hardly possible because continuous-space Markov processes are given by linear operators in infinite (as opposed to finite) dimensional spaces [100]. A working alternative is to restrict ourselves to a more manageable but still very expressive class of processes. We can consider continuous POMDPs with latent dynamics given by stochastic differential equations (SDEs),"}, {"title": "4.4 Generative models for structure learning agents", "content": "Now that we have seen a space of models that may be apt to describe the dynamical structure of a wide-range of worlds, we return to the generative models that agents might employ to infer this structure.\nIn the simplest case, the causal structure of the environment is constant over time. In this case the simplest appropriate world model describes the causal network as a static hyperparameter that needs to be inferred, whence the agent only influences the states and parameters of the external process through action. We illustrate this generative model in Figure 8.\nMore generally, the causal relationships in the environment may evolve over time and may or may not be controllable by the agent. This is, for example, the case with games containing levels of increasing difficulty, where each level varies in complexity or curriculum learning environments that gradually introduce more complex concepts as learning progresses [107,108]. The causal network of the environment may be control-lable, for example, when taking a particular action removes (e.g. kills) another object or agent in a game. To represent both of these scenarios agents require more complex generative models: hidden Markov models and POMDPs on the causal network, which lead the agent to optimize beliefs about the (past, present and future) causal network of the world, which may or may not be conditioned on a course of action (i.e. during planning (4)). Please see the illustration in Figure 9."}, {"title": "4.5 Looking forward", "content": "Toward a universal, interpretable, agentic class of models: We have described a class of models that approximates a very large class of stochastic processes on both discrete and continuous states, and which may serve as a generic model class for agent environment interactions. This class of models is very expressive while being sufficiently sparse that it can plausibly be searched [86]. (This is because the causal network is largely determined from the latent representations, so that this finesses the combinatorial explosion of"}, {"title": "5 AI alignment", "content": "We now shift gears and discuss AI alignment as a potential application of structure learning active inference agents. AI alignment refers to the challenge of ensuring that artificial intelligence systems behave in ways that are aligned with human values and intentions. This problem has gained increasing attention as AI systems become more capable and autonomous, with potential far-reaching consequences for humanity. AI safety, a closely related topic, focuses on developing AI systems that are robust, reliable, and safe in their operations. These topics have been extensively explored in seminal works such as Superintelligence by Nick Bostrom [112] and Human Compatible by Stuart Russell [113], which highlight the potential risks and challenges of advanced AI systems.\nIn what follows, we approach AI alignment through the lens of active inference and structure learning, using Asimov's Three Laws of robotics [114] as a simple illustrative example. Our focus is on providing new ways of thinking about the AI alignment problem, rather than recommending a specific solution to be implemented. One conceptual point is that we can frame alignment as taking actions that comply with the other's preferences, and we can learn those preferences through structure learning, which, in terms from psychology, corresponds to a sophisticated form of theory of mind."}, {"title": "5.1 Related work", "content": "The approach presented here shares important connections with other causal modeling approaches to AI alignment. Like us, the work of Everitt and colleagues uses causal models of agent-environment interactions, but with the aim of identifying agent's incentives [115,116]. This provides a complementary perspective on analyzing and designing AI systems with desirable incentive structures, and aligns well with our discussion of theory of mind, including its potential for benevolent as well as adversarial uses. The structure learning approach that we develop may provide a way to dynamically construct these causal networks, and go beyond identifying the mere presence or absence of various properties such as incentives [115], intentions [117], and deception [118], by quantifying these phenomena.\nThe challenge of avoiding unintended consequences of action, e.g. [119], also resonates with the desirability of risk-averse agents. The concept of penalizing actions that lead to significant, irreversible and potentially harmful changes is in line with the risk-averse behavior produced by expected free energy minimization in active inference."}, {"title": "5.2 Well-being, alignment and cautious AI", "content": "For building safe and aligned AI systems", "deliverables": "nDefining well-being and harm: In active inference", "t": "n$\\begin{equation"}, "well\\text{-}being(t) \\approx log P(d) \\approx -harm(t),\\tag{10}\\end{equation}$\nwhere d \u2286 d(t) is the data entertained by the agent at time t; necessarily a subset of past and present states of the agent (boundary and internal states). Note that well-being, when expressed in this way, is quantified in terms of natural units of information (nats). This definition of well-being is fairly established in the active inference literature [120-123"], "124].\nAlignment": "On this view, being aligned with another is just having a high model evidence under the other's model of the world. This means conforming with the other's model of the world, which, in active inference, describes how things should ideally behave from the other's perspective (recall Section 2.1). For instance, an AI assistant that accurately completes tasks as desired and intended would have high model evidence under a human's generative model of helpful behavior. Conversely, an AI system that acts in unexpected or harmful ways would be highly surprising under this model and thus misaligned. This perspective on alignment emphasizes the importance of learning and respecting the preferences and expectations embedded in others' world models, which is a key challenge in developing safe and beneficial AI systems.\\"}