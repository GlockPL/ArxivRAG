{"title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques", "authors": ["Nathaniel Tomczak", "Sanmukh Kuppannagari"], "abstract": "Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve \"true sparsity\u201d are lacking. In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).", "sections": [{"title": "I. INTRODUCTION", "content": "Transformers have emerged as one of the most successful AI/ML techniques for modeling sequence data. They have drastically impacted a variety of scientific and engineering applications such as language modeling [1], molecular design [2], cancer pathology classification [3], genomic sequence modeling [4], and others. These models derive their power from an operation called the attention mechanism. Given a sequence of tokens of length L, the attention mechanism extracts pairwise similarities between the tokens [5].\nHowever, the time and memory complexity of the attention mechanism is quadratic in L as it captures interactions between each pair of the sequence. This has severely limited the context length, i.e., the length of a sequence over which a transformer model can capture these interactions (also know as sequence length in the literature). For applications such as genomics, at least 4-5 orders of magnitude of increase in context length is needed [4].\nA popular approach for reducing the computational com- plexity is to introduce sparsity in the attention mechanism [6]\u2013 [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach.\nHowever, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform dense- dense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have fo- cused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have non- zero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements.\nIn this work, we aim to achieve \u201ctrue sparsity\u201d, meaning that only the necessary computations are performed for any arbitrary attention mask. We accomplish this by posing atten- tion as a graph problem with individual tokens of the sequence perceived as nodes and the interactions, as defined by the attention mask, determining the edges between the nodes. With this perspective, we develop efficient algorithms for masked attention with various mask patterns. The contributions of this paper are:\nWe propose a graph computing perspective of the at- tention operation, enabling us to realize a reduction in computations due to sparse attention mechanisms.\nUsing this view, we develop graph algorithms that are work-optimal over arbitrary attention masks.\nFor local, global, and dilated attention, we further opti- mize our graph algorithms to achieve higher performance through longer context lengths.\nWe conduct thorough experiments to demonstrate the benefits of our graph computing perspective where we obtain a 4.46x and 51.06x speedup relative to FlashAtten- tion for sequence lengths of 2,097,152 and 160 million, respectively. Our implementations are able to achieve these context lengths on a single NVIDIA A100 GPU (80 GB), paving the way for training models that can handle extremely long sequences using distributed training.\nWe create a PyTorch back-end for our algorithms for seamless integration into existing LLMs."}, {"title": "II. BACKGROUND", "content": "Transformer models are encoder-decoder architectures con- sisting of a sequence of \"transformer layers\". A transformer layer takes, as input, a sequence of tokens of length L, where each token is a d dimensional vector. This sequence is projected into a set of $d_k$ dimensional queries packed in a matrix $Q \\in \\mathbb{R}^{L \\times d_k}$, a set of $d_k$ dimensional keys packed in a matrix $K \\in \\mathbb{R}^{L \\times d_k}$, and a set of $d_v$ dimensional values, packed into a matrix $V \\in \\mathbb{R}^{L \\times d_v}$ using learnable weight matrices $W_Q, W_K$ and $W_V$ respectively [5]. This operation can be represented using the following equation:\n$Attention(K, Q, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$         (1)\nIn practice, a multi-headed attention mechanism is utilized where Q, K, and V are sliced into several smaller dimensions, attention is applied in parallel to each projection, and the results are concatenated to obtain the attention output [5]. Following the attention operation, a learnable fully connected network is used to project the output into a L \u00d7 d dimension matrix to be processed by the subsequent transformer layer."}, {"title": "B. Attention Mechanism", "content": "Figure 1 provides a graphical perspective of the attention mechanism. When viewed as matrices, attention performs ma- trix multiplication between the query and key matrices, row- wise softmax on the output, and another matrix multiplication between the softmax result and the value matrix.\nIntuitively, attention performs a soft lookup of values for each query. Specifically, for a query i (a row of the Q matrix in Figure 1), its dot product with columns of the K matrix followed by a softmax on the output produces a probability vector (i.e., all entries are non-negative and sum up to one). A linear combination of value vectors (rows of the matrix V) with the weights of the probability vector produces the final output corresponding to the query i. In other words, instead of outputting a single value vector as performed in a hard lookup, a combination of values, dependent upon similarities between the queries and the keys is used to produce the output vector."}, {"title": "C. Sparsity in Attention", "content": "This soft lookup perspective of attention motivated the introduction of sparsity into the mechanism. Researchers have identified that, for each query, only the most important terms, as determined by the similarity matrix resulting from key- query interactions, need to be considered [11]. Furthermore, to determine similar keys, without computing the key-query dot products, researchers have proposed various connectivity pat- terns between keys and queries to capture their similarity [6]\u2013 [8]. These patterns are represented in L \u00d7 L 0-1 attention masks where the rows denote queries and the columns denote keys. As both queries and keys are projections of the same L sized sequence, the attention mask can be perceived as capturing important pairwise similarities between the tokens of the sequence.\nFigure 2 shows common attention patterns and their com- binations used by two transformers: the left and central masks are utilized by Longformer and the right by BigBird [6], [8]. The white cells correspond to terms that are masked from the calculation. The forms of attention in relation to Figure 2 are described below.\nLocal/Windowed Attention: The black cells represent local attention which gives a token the ability to look n tokens forwards and backwards from itself [6], [8], [12].\n1D Dilated Windowed Attention: Local attention can also be dilated as shown in the central figure. This creates uniform gaps of a set size between each attended token that widens the effective view distance for each token [8]. For an index (i, j), a window size of w, and a dilation factor r, we can determine if a value is masked (returns 0) or not (returns 1) by:\nif ((abs (i - j) < w) && (abs (i - j) % (r + 1) == 0)) {\nreturn 1;\n} else {\nreturn 0;\n}\n2D Dilated Windowed Attention: Dilation can also occur over two dimensions with blocks [7]. Given a context length L, an index (i, j), a block height and width of b, and a dilation factor r, we can determine if a value is masked by:\nif (floor(i/(L/b)) == floor(j/(L/b))) {\ni_b = i % b;\nj_b = j % b;\nif ((i_b % (r + 1) == 0) && (j_b % (r + 1) == 0)) {\nreturn 1;\n} else {\nreturn 0;\n} else {\nreturn 0;\n}\nGlobal Attention: The blue is global attention and is reserved for designated tokens that can attend to all other tokens in the sequence [6], [13].\nRandom Attention: The orange indicates token-token rela- tionships that are chosen from a uniform random distribution [6].\nThese and other forms of attention are often represented by blocks larger than 1 token, called a block sparse format [6], [14], [15]. This representation allows for good utilization of GPU resources, however it restricts the resolution of sparsity that can be achieved. The work in this paper removes resolu- tion restrictions on imposed sparsity without requiring excess masking computations."}, {"title": "D. How Sparse are the Masks?", "content": "A natural question to ask at this point is: What are the typical sparsity levels of these masks? As, depending upon the level, a graph computing view may or may not be justified. We first define a term known as the sparsity factor, Sf, which can be calculated by the equation:\n$S_f = \\frac{NNZ}{TE}$     (2)\nwhere NNZ is the number of non-zero elements in the mask and TE is the total number of elements in the mask. Sf is constant for a mask in the range [0,1] where zero is a fully sparse matrix and one is a fully dense matrix. It operates as a multiplicative constant for the number of dot products required to build the attention matrix: O(SfL2).\nPopular attention masks from Bigbird [6], and Longformer [8] achieve O(L) dot products. In other words, the sparsity factor Sf reduces as O($\\frac{1}{L}$) with context length. However, the hidden constant term is not clear to obtain a concrete sparsity value.\nTo obtain this number, we refer to LongNet [7] which shows that the number of dot products needed is $(\\frac{2 \\alpha}{2\\alpha-1})w_0 L$ where \u03b1 is the geometric series ratio used to compute the segment length and dilation parameters in their algorithm and wo is the first element of the segment length geo- metric series. Setting \u03b1 = 2 and wo = 2048 based on the parameters they use in the experimental results, this number equals to 2730L leading to a sparsity factor of $\\frac{2730}{L}$. The value of sparsity factor for selected sequence lengths of {16k, 32k, 1 million, ..., 160 million, 1 billion} will be {0.17,0.085, 0.0027, ..., 0.000017,2.7e-6}. This implies that, for large sequence lengths, a graph computing view, as proposed in this work, is very well motivated and has the potential to significantly improve upon the existing tensor based implementations."}, {"title": "III. RELATED WORKS", "content": "As the key objective of this work is to increase context length by exploiting sparsity, we discuss relevant works that focus on either achieving efficient sequence parallelism or on achieving efficient implementation of sparse masks.\nSequence Parallelism: DeepSpeed Ulysses [16] achieves se- quence parallelism for the original L2 complexity attention by distributing the partitions of the K, Q, V matrices along the sequence dimensions across computing nodes. Megatron [17] performs sequence parallelism that is tightly coupled with their tensor parallelism [16]. Ring attention achieves sequence parallelism for block sparse attention masks [14]. LongNet achieves sequence parallelism using a dilated attention mask and requires all-gather of K, Q matrices [7]. On a single node, these techniques still rely on dense matrix operations for attention calculations. Our technique, that exploits sparsity, is orthogonal to these methods and has the potential to significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes.\nEfficient Sparse Attention Implementations: While works such as BigBird [6], Longformer [8], and Reformer [11] proposed sparse attention masks, their implementations in libraries such as PyTorch [9] and xFormers [10] still rely on dense operations. Specifically, they employ a variant of the Scaled Dot Product (SDP) attention [5] where they first perform a dense matrix multiplication of Q and K that can incorporate block sparsity for low-resolution masking, set the excess terms corresponding to the zero entries in the attention mask to -\u221e, perform a row-wise softmax (which results in -\u221e getting converted to 0) and finally a sparse- dense matrix multiplication is performed between the resultant matrix (sparse) and the V matrix (dense).\nAs a result, FlashAttention a highly optimized version of SDP - has remained the most efficient attention implementa- tion despite the fact that it performs the full L\u00b2 computations [18]\u2013[20]. Recent works [21], [22] have enabled support for sparsity by partitioning the attention mask and the K, Q, V matrices into blocks and only computing the blocks that have at least one non-zero element using the FlashAttention algorithm.\n[21], [22], while improving upon FlashAttention in exploit- ing sparsity, do not achieve true sparsity. Specifically, they still compute dot products corresponding to the 0 entries in the blocks leading to O(d) unnecessary computations, where d is the embedding dimension. In contrast, our work achieves true sparsity and only performs the computations that are needed."}, {"title": "IV. GRAPH COMPUTING VIEW OF ATTENTION", "content": "Our graph computing view of attention is motivated by its intuitive definition described in Section II-B. Specifically, for each token i, the corresponding outputs are determined by its query, and the keys and values of a subset of tokens to which it is closely related to. Thus, instead of modeling it as tensor operations, as performed by state-of-the-art deep learning frameworks, we describe attention as a graph computation below.\nFor an input $d_k$ dimensional token sequence $X_1, X_2,..., X_L$, we build a graph G with vertex set $V_G = V_1, V_2,..., U_L$. The attributes of the vertices are the projected keys, queries, and values. The edge set $E_G$ is determined by the L XL attention mask (A). Specifically, a directional edge exists between vertex $v_i$ and $v_j$ if the $A_{ij} = 1$."}, {"title": "B. Graph Algorithms for Attention", "content": "We develop two algorithms based on the graph model of attention. Both algorithms are single-batch and single-headed to facilitate focus on the experiments, though it is trivial to scale them to a multi-headed approach. Both algorithms were inspired by FlashAttention and utilize the online softmax operation [18], [23]. The first uses an explicit input mask while the second has an implicit mask that is calculated from input parameters as the kernel executes. The six graph processing algorithms that were implemented are as follows, subset by their mask type:\nExplicit Mask\nCOO (coordinate): The row indices, column in- dices, and values vectors are passed in.\nCSR (compressed sparse row): The row offset, column indices, and values vectors are passed in.\nImplicit Mask\nLocal: A window size, n, is passed in and the window indices are calculated relative to the index token of a row.\n1D Dilation: A window size, w, and a dilation factor, r, are passed in and the mask indices are calculated relative to the index token of a row.\n2D Dilation: A block height and width, b, and dilation factor, r, are passed in and the mask indices are calculated relative to the index token of a row.\nGlobal (non-local): A vector of global token indices and window size, n, are passed in. Attention indices are calculated for both the global and local mask and then the local mask is subtracted from the global.\nThe COO and CSR versions for the first algorithm can handle any arbitrary attention mask and their performance is compared through microbenchmarks in Section V-C. The local, 1D dilation, 2D dilation, and global (for simplicity, we will drop non-local for the rest of the paper) implementations are inflexible attention masks outside of their parameter inputs, we will refer to these collectively as \"ordered sparsity\" patterns within the paper. Both algorithmic approaches can be described by Algorithm 1."}, {"title": "Algorithm 1: Graph Processing Attention", "content": "Input: Attention-specific parameters $P_a$ and Matrices Q, K, V \u2208 $\\mathbb{R}^{L \\times d_k}$ and Graph G = (VG, EG) with vertex attributes VG = {v \u2208 $\\mathbb{R}^{L}$: \u2200 $v_z$ \u2208 v\u2203 $Q_i, K_i, V_i$ \u2208 $\\mathbb{R}^{dk}$ such that $v_i$ = ($Q_i, K_i, V_i$)}\nOutput: O \u2208 $\\mathbb{R}^{L \\times d_k}$\nInitialize: O \u2208 $\\mathbb{R}^{L \\times d_k}$ \u2190 0 \u2200 $0_{i,j}$ \u2208 0,\nl\u2208 $\\mathbb{R}$ \u2190 0 \u2200 $l_i$ \u2208 l,\nm\u2208$\\mathbb{R}$\u2190 -\u221e \u2200 $m_i$ \u2208 m\nfor 1 \u2264 i \u2264 L do in parallel\n$v_i$_Neighbors \u2190 Get_Neighbors(G, i, $P_a$)\nfor j \u2208 $v_i$_Neighbors do\n$K_j$ \u2190 Pull(K)\n$W_{Q_iK_j}$ = $Q_i K_j^T$\n$m_{new}$ = max($m_i$, $W_{Q_iK_j}$)\n$l_{new}$ \u2190 $l_i*exp(m_i-m_{new})+exp(W_{Q_iK_j}-m_{new})$\n$V_j$\u2190 Pull(V)\n$O_i$ \u2190 ($l_{new}$)^{-1} * [$l_i * exp(m_i - $m_{new}$)* $O_i$ +\nexp($W_{Q_iK_j}$ - $m_{new}$)* $V_j$]\n$l_i$\u2190 $l_{new}$, $m_i$\u2190 $m_{new}$\nReturn: O\nAll implementations utilize a shared memory model coupled with online softmax [18], [23] and are parallelized along the L dimension, simultaneously operating on rows of the attention matrix. The primary difference lies in their inputs outside of the Q, K, and V matrices, which all of them receive. The COO and CSR algorithms have no additional attention- specific parameters, $P_a$, but their sparse mask (graph), G, is an explicit input. The local, 1D dilation, 2D dilation, and global approaches have attention-specific parameters, $P_a$, as described above, but no explicit mask is provided. The \u201cGet_Neighbors\u201d function will, depending on the inputs, calculate the mask indices (neighbors) that will be operated on for each row. Then, for a given row, i, the algorithm pulls in neighbor information for K and V to calculate a subset of the attention output, adjusting for changes in the softmax statistics with each new neighbor that is pulled from.\nThe algorithms proposed are truly sparse, meaning that for any arbitrary mask provided, they will conduct the attention operation with the minimum number of calculations required. This means that, as the mask becomes more sparse, the amount of work required decreases.\nWork Optimality: The serial complexity of computing at- tention with a mask is O(Sf \u00d7 L\u00b2 \u00d7 d) assuming d as the same dimension of K, Q, and V. Assuming a CRCW PRAM model [24], state-of-the-art implementations that require a dense-dense matrix multiplication followed by invalidation of zero entries, followed by a sparse matrix multiplication using p processors will have a cost of O(\\frac{L^2d}{p}+SfL^2d) which is not cost optimal (the cost of a parallel algorithm should be equal to the serial complexity for cost/work optimality). In contrast, our algorithm only performs computations for the non-zero elements of the mask and achieves a cost of O(\\frac{Sf \u00d7 L\u00b2 \u00d7 d}{p}), making it work optimal."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "Each of the custom graph processing algorithms discussed in this paper were written in NVIDIA's CUDA programming language which is designed to be run on NVIDIA GPUs. The CUDA kernels were written with and compiled by PyTorch's custom extension system which required the Ninja build tool. This process created a binding that exposed the functions so that they could be called directly from Python and interact with PyTorch tensor objects. These PyTorch back-ends of the algorithms will be open-sourced. The software versions utilized are:\nPython: 3.10.8\nPyTorch: 2.4.0 (CUDA 12.1)\nCUDA: 12.1.1\nNinja: 1.11.1\nIn order to verify correctness, all variants of the algorithms written were tested against PyTorch's scaled_dot_product_attention function that utilized the C++ back-end as it allows for arbitrary binary masks to be passed in. The verification process entailed creating a mask as a PyTorch tensor and converting it into the desired sparse matrix representation, or making sure that the mask matched the implicit one that would be utilized by the ordered sparsity algorithms. The query, key, and value matrices had context lengths of 256 and embedded dimensions of 32; each was created from the uniform random distribution [0,1) and was identical for both functions. Resulting outputs were compared using PyTorch's allclose function with an absolute tolerance of 1 \u00d7 10-08, a relative tolerance of 1 \u00d7 10-05, and NaN values set to equal. The outputs were deemed identical for attention with varied levels of sparsity."}, {"title": "B. System Specifications", "content": "All empirical results were captured using three different HPC systems. Within this paper, each system will be referred to by its associated GPU. The specifications of each cluster are shown below in Table I."}, {"title": "C. Microbenchmarking", "content": "1) Objective: To analyze the comparative runtime perfor- mance of the graph processing algorithms across varied systems and problem dimensions.\n2) Setup and Results\nA series of microbenchmarks were conducted across all three systems shown in Table I. All six graph processing algo- rithms were measured as well as PyTorch's SDP attention that utilized a mask. Each algorithm, except for the COO variant, was run with context lengths (L) of 8,192; 16,384; and 24,576, embedded dimensions ($d_k$) of 64; 128; and 256, and sparsity factors ($S_f$) in the range (0, 1]. The COO implementation was only run with a context length of 8,192, all three embedded dimensions, and sparsity factors in the range (0,0.4] due to its long runtime. A dilation factor of 1 was used for both the 1D dilation and 2D dilation masks. The local, 1D dilation, and 2D dilation masks calculated window/block size to fit the associated sparsity factor. Additionally, the NVIDIA V100 GPU does not have data for a context length of 24,576 due to memory restrictions. Each combination of input parameters were run 10 times for a warm up and then an additional 15 iterations were timed for the benchmark.\n3) Analysis\nWithin Figure 3, there is a noticeable uniformity of the trends between both different GPUs and L. Across all algo- rithms, except for COO, one can easily observe an increase in average runtime as $d_k$ increases in size. This is expected as growth along $d_k$ corresponds directly to the number of multiplications and additions required for each dot product operation. For COO, all the markers are close to one another. Due to COO being a selection of ordered coordinates (grouped rows and sorted columns), the current algorithm must search to find the limits of a row before running the attention process; the search cost grows as the algorithm strays farther from row zero. The incurred cost is unique to COO as none of the other attention methods must search to find bounds before they can run. The ordered sparsity approaches do calculate indices, which is similar, however the operation cost does not increase with later tokens in the sequence.\nThe SDP algorithm demonstrates constant performance with increases to $S_f$. This is due to the fact that the function con- ducts a fully dense matrix multiplication before masking and then conducts another dense matrix multiplication regardless of the sparsity of the attention matrix. SDP outperforms the graph processing algorithms until greater sparsity is achieved ($S_f$ \u2264 0.01). With greater sparsity, fewer operations are conducted by the graph processing algorithms and, outside of the COO form, they are able to complete the operation faster than the PyTorch function. Outside of shared memory utilization, the current forms of the written algorithms are naive and untuned, with future optimizations the performance crossover with SDP should occur at a lower sparsity. Further optimizations are discussed in Section VI-A.\nAmongst the graph processing algorithms, the local, 1D dilation, 2D dilation, and CSR implementations are relatively similar in performance. They demonstrate the best average runtimes across all GPUs, L, $d_k$, and levels of $S_f$. Their trends are also identical due to their similar implementations. The global variant has comparable performance to CSR at high $S_f$, but demonstrates a slower decrease in runtime with increased sparsity. This is due to the form of sparsity present in a global mask: certain rows are entirely masked while others are entirely non-masked (minus the identity diagonal as the smallest local size was chosen for benchmarking, and global token columns). Because parallelization occurs along the L dimension, certain rows will be fully dense (minus one) as long as $S_f$ is not 1. This can create an imbalanced distribution of work amongst CUDA blocks, and the algorithm can only be as fast as its slowest block. This disparity is unlikely to happen in the CSR implementation and will not happen in the local variant for higher levels of sparsity.\n2D dilation had the best average performance across all $d_k$ and L for $S_f$ < 0.001, showing speedups over SDP of 13.37x, 42.12x, and 11.88x for the V100, L40, and A100, respectively. With the same conditions, 1D dilation demonstrated speedups over SDP of 6.74x, 26.40x, and 6.95x for the V100, L40, and A100, respectively. Local observed average speedups over SDP of 7.87x, 27.56x, and 8.07x for the V100, L40, and A100 respectively. The global algorithm, with the same context, saw average speedups over SDP of 1.40x, 2.87x, and 0.87x for the V100, L40, and A100 respectively. CSR showed the best average speedup for explicit masks with speedups over SDP of 9.85x, 31.59x, and 7.81x for the V100, L40, and A100, respectively. With $S_f$ < 0.1, COO saw speedups over SDP of 0.002x, 0.003x, and 0.001x on the V100, L40, and A100, respectively. The search within the kernel was harmful to COO's performance. Moving forward, empirical results will focus on CSR for explicit mask exploration.\nAcross all of the GPUs, the L40 demonstrated the fastest average runtimes, possibly because it was the newest GPU tested. The L40's crossover in performance with the CSR and local algorithms appears to happen at a higher sparsity factor, closer to 0.1, than the other GPUs. Additionally, its SDP runtimes are much closer together than the other algorithms. In the future, we will profile each of the algorithms across all GPUs to gain a better understanding of trade-offs between GPUs."}, {"title": "D. Theoretical Context Length Limits", "content": "1) Objective: To quantitatively understand, using various algorithms, limitations on context length introduced by the amount of memory an accelerator has.\n2) Setup and Results\nThe theoretical context length limits for a given sparsity were calculated by solving inequalities that relate the total GPU memory to the amount of memory occupied by tensors during runtime. The NVIDIA A100 GPU, which has an 80 GB capacity, was considered for the calculations. The results are show in Figure 4 where 4a and 4b showcase theoretical log context length limits for embedded dimensions of 64 and 128, respectively, as the log sparsity factor increases along the x-axis. All plots portray performance of a single-batch, single- headed attention implementation. The left-most graphs relate to 32-bit floating-point (FP32) tensors while the right-most relate to 16-bit floating-point (FP16) tensors. FlashAttention does not operate on FP32 data. Table II tabulates the data presented in Figure 4. It also shows the theoretical limit of all attention mechanisms on one A100 using dimensions from the Llama 3 series 8 billion parameter model: 32 heads and $d_k$ of 4,096 [25].\nEmpirical tests were run on the NVIDIA A100 system from Table I with the software described in Section V-A and node setup from Section V-B. Three algorithms were benchmarked at set context lengths: FlashAttention, local, and CSR. The local implementation had $S_f$ values (except for 160 million) according to the relation from Section II-D, CSR does not for L over 1.6 million due to memory restrictions. Every test utilized the FP16 data type. For each configuration, an algorithm had ten warm up runs and then was timed for 15 benchmark runs with the average runtime reported in Table III. FlashAttention, for a context length of 160 million, was the only exception, there was no warm up and only one benchmark run due to the large amount of time required for a single iteration.\n3) Analysis\nAs demonstrated in Figure 4 and Table II, greater sparsity in the attention matrix allows the CSR and COO implementations to theoretically perform attention with context lengths nearly two orders of magnitude longer than an SDP implementation. The CSR variant possesses a longer context length than COO because of its sparse matrix structure: CSR has one vector with memory complexity of O(L) and two others with O(SfL2) while COO has all three vectors with O(SfL2) memory complexity. FlashAttention, local, 1D dilation, and 2D dilation algorithms can all achieve even greater context lengths independent of the sparsity because they utilize an online softmax and do not require explicit storage of the attention matrix, only two statistics vectors, each with memory complexity of O(L) [18], [23]. Global is similar, except it has one additional input to indicate which tokens require global attention with maximum length L. As shown in Table II, this minutely reduces its maximum context length in relation to the other implicit mask algorithms.\nTable III shows that sparse operations can outperform hardware-optimized algorithms like FlashAttention for ex- tremely long context lengths. For L of 1.6 million, local and CSR were 0.28x and 0.25x as fast as FlashAttention, respectively. However, as L increases and Sf is adjusted according to Section II-D, local saw speedups of 1.49x and 2.99x over FlashAttention for L of 8 million and 16 million, respectively. For L of 160 million, we observed a speedup of 51.06x over FlashAttention. This trend is expected, as less work is being conducted by the GPU. Between sparse implementations, local outperformed CSR for L of 1.6 million because it does not have to read from and write to global memory for the attention matrix."}, {"title": "E. Runtime Trade-Off Given a Context Length", "content": "1) Objective: To observe the runtime effects of sparsity on a graph processing algorithm in comparison to the dense FlashAttention.\n2) Setup and Results\nAll benchmarks were run on the NVIDIA A100 system from Table I. Two algorithms were used: FlashAttention and our local implementation. The FP16 data type was used for compatibility with FlashAttention. The local window size (length a token can see behind or ahead) was held constant in one set of tests for decreasing sparsity with increasing context length and was dynamic in the other to achieve a constant sparsity. FlashAttention does not take sparsity into account as it is a dense operation. For each variable combination, a given algorithm had ten warm up runs and was timed for 15 runs afterwards with the average runtime reported.\n3) Analysis\nIn the left-most plot of Figure 5, one can see that increasing sparsity in the local algorithm does allow it to outperform FlashAttention. However, with a fixed low sparsity, the gap in performance continues to grow with increased context length. This can be seen in Table III and the right plot of Figure 5, where with a low sparsity factor of 0.0001 and L of 65,536 the local variant saw a 1.41x speedup over FlashAttention that increased to a 4.46x improvement for L of 2,097,152."}, {"title": "F. Performance on Popular Attention Masks", "content": "1) Objective: To explore the runtime of our algorithms in relation to PyTorch's SDPA while using standard attention masks.\n2) Setup and Results\nAll benchmarks were collected from the NVIDIA A100 system from Table I. PyTorch's SDP served as a baseline comparison for the graph processing algorithms as FlashAtten- tion does not support arbitrary masking. The sparsity patterns utilized are shown in Figure 2", "ways": "a combination of our local and global algorithm run sequentially and then only our CSR approach. The central mask was run only with our CSR algorithm. The right-most mask was constructed in two forms: a sequential combination of our local, global, and CSR (for random at- tention) algorithms and then only our CSR variant. For all approaches the local size was set to 50 in each direction and three global tokens were used, the dilated local window used a dilation"}]}