{"title": "Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models", "authors": ["Alexander R. Pelletier", "Joseph Ramirez", "Irsyad Adam", "Simha Sankar", "Yu Yan", "Ding Wang", "Dylan Steinecke", "Wei Wang", "Peipei Ping"], "abstract": "The vast amount of biomedical information available today presents a significant challenge for investigators seeking to digest, process, and understand these findings effectively. Large Language Models (LLMs) have emerged as powerful tools to navigate this complex and challenging data landscape. However, LLMs may lead to hallucinatory responses, making Retrieval Augmented Generation (RAG) crucial for achieving accurate information. In this protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease Distinction), a comprehensive workflow designed to support investigators with knowledge integration and hypothesis generation, identifying validated paths forward. Relevant biomedical information from publications and knowledge bases are reviewed, integrated, and extracted via text-mining association analysis and explainable graph prediction models on disease nodes, forecasting potential links among drugs and diseases. These analyses, along with biomedical texts, are integrated into a framework that facilitates user-directed mechanism elucidation as well as hypothesis exploration through RAG-enabled LLMs. A clinical use-case demonstrates RUGGED's ability to evaluate and recommend therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy (DCM), analyzing prescribed drugs for molecular interactions and unexplored uses. The platform minimizes LLM hallucinations, offers actionable insights, and improves the investigation of novel therapeutics.", "sections": [{"title": "INTRODUCTION:", "content": "Hypothesis generation process in biomedical research is essential to uncover novel protein-disease associations to understand pathogenesis and unlock therapeutic potential. This process draws evidence from existing biomedical knowledge, synthesizing findings based on logic leads embedded within peer-reviewed literature, such as the 36 million publications comprising PubMed, and integrating high-confidence curated evidence rooted among the many biomedical knowledge bases. Recent advancements streamline this laborious manual effort by applying text mining on literature corpra1-3 as well as employ graph-based analyses 4-7 to synthesize relevant information and uncover new avenues for investigation. Despite efforts, our current approaches often lack deep contextual understanding of these fragmented data with limited ability to draw inferences and interactively explore new hypotheses.\nLarge Language Models (LLMs) shed new light to these challenges, demonstrating deep contextual understanding by training on vast amounts of information across multiple disciplines 8-10. In the biomedical domain, LLMs have been applied to tasks such as extracting patient information\u00b9\u00b9, general clinical question answering12, 13, domain-specific question answering14, and use in primary clinical care15. These models demonstrate ability to reason and draw inferences from complex datasets, making them well-suited for generating hypotheses in biomedical research. Furthermore, some models feature chat-like interaction which engage users and enable dynamic exploration of topics, surpassing the conventional boundaries of knowledge bases and traditional web search engines 16, 17.\nDespite their potential, LLMs face challenges, such as hallucinating information, displaying unwarranted confidence in potentially inaccurate explanations, lacking interpretability, and being susceptible to biased or inappropriate content18\u201321. When LLMs are applied directly to hypothesis generation or guiding clinical decision-making, the responses and predictions have high stakes, any errors may potentially guide costly laboratory experiments or influence decisions that affect patient health trajectories22, 23. Thus, reliable and trustworthy LLM responses are paramount, as their advice must be firmly rooted in evidence, explicitly laying out their reasoning and substantiating their claims. In these scenarios, interpretability is not a luxury but a necessity for understanding why these models make the predictions they do.\nTo this end, Retrieval-Augmented Generation (RAG) is a system designed to minimize LLM hallucinations. By identifying and incorporating relevant text documents from reliable and trustworthy sources, RAG grounds LLM responses in evidence, enhancing their accuracy and reliability24, 25. For example, integrating an LLM (e.g., ChatGPT) with PubMed allows for the identification of relevant citations to user queries 26, 27. This method leverages Named Entity Recognition (NER) to connect literature but does not yet integrate information from biomedical knowledge bases or from predictive analyses."}, {"title": null, "content": "Knowledge Graphs (KGs) have been applied to LLMs for tasks such as fact-checking, transparent reasoning 28-30, encoding knowledge31, improving question answering32, and completing knowledge graphs 33. By encoding factual information from verified sources, KGs enhance the accuracy, transparency, and reliability of LLM responses. Link prediction techniques within these graphs leverage deep learning to identify previously hidden relationships between proteins and diseases, presenting new opportunities for wet lab investigation3, 34, 35. Recent advancements in explainable Al predictions further enhance transparency and interpretability of these link prediction tasks, offering insights to support biomedical hypotheses as a viable avenue for investigation36-38. This approach, which provides node-level and edge-level insights, would enable identifying key biomedical entities and relevant subgraphs influencing protein-disease predictions. These advancements ensure that LLM-generated decisions are both accurate and evidence-based, significantly boosting their applicability in biomedical research.\nIn this protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease Distinction) as an accessible and efficient workflow for biomedical hypothesis generation. This workflow protocol leverages the vast resources of biomedical literature and knowledge bases for the extraction of relevant information, enabling query-tailored retrieval processes. We employ explainable artificial intelligence predictions to uncover interpretable and actionable insights from the existing biomedical knowledge, thereby enhancing the transparency and utility of predictive models. The completed workflow streamlines the exploration of knowledge graphs and model predictions via RAG-enabled LLMs, facilitating intuitive and informed interactions for researchers, clinicians, and clinical professionals.\nThis section lays the groundwork for the protocol, with steps to implement this approach described in the following section. Next, we present representative results of this approach with an example use-case. Finally, implications and discussion of this protocol are discussed."}, {"title": "PROTOCOL:", "content": "NOTE: This protocol has been developed in Python and implemented as a Docker container in Windows. The commands provided are based on the Unix environment, within the Docker container. The software is available at https://github.com/pinglab-utils/RUGGED.\n1.\nInstall Software\n1.1.\nInstall Docker. Visit the Docker website (https://www.docker.com/), click on \u2018Get started', and choose the appropriate version for your operating system (Windows, macOS, or Linux). Download and run the installer. Verify installation by typing `docker --version` in the terminal; successful installation will report the version of Docker installed.\n1.1.1. Initialize Inter-container Networking. Enable Docker containers to be configured to connect to other services on your device (e.g., other Docker containers). Type into the terminal the following command: `docker network create rugged_network`.\n1.2.\nInstall Git. Visit the Git website (https://www.git-scm.com/), click on 'Downloads', and choose the appropriate version for your operating system. Download and run the installer. Verify"}, {"title": null, "content": "installation by typing `git --version` in the terminal; successful installation will report the version of Git installed.\n1.3.\nEnable GPU acceleration. (Recommended, optional) This step enables GPU acceleration for local LLM and Explainable Al predictive analysis and greatly decreases runtime of the software. If your device has an NVIDIA RTX GPU, install the necessary drivers and the CUDA Toolkit from NVIDIA website (https://developer.nvidia.com/cuda-downloads).\n1.4.\nSet up Large Language Models (LLMs) services. RUGGED supports the OpenAl API for models such as GPT-3.5 and ChatGPT-40, and local models using Ollama (e.g., Llama3). Choose the appropriate service based on your needs.\n1.4.1. Obtain an OpenAl API Key. If using OpenAl services, no software installation is needed. Proceed to OpenAl's website (https://openai.com/blog/openai-api) to create an account and obtain an API key by loading funds into the account. Save this key in a safe place, as it will be needed for software configuration.\nNOTE: OpenAl API is a paid service. At time of publication, OpenAl API costs for GPT4 is $30.00/1M tokens (visit https://openai.com/pricing).\n1.4.2. Install Ollama. If using a local LLM, install Ollama on your device or download the Docker container. To install Ollama, visit the Ollama website (https://ollama.com/download) and follow installation instructions. To install Ollama on Docker, run the following command:\ndocker pull ollama/ollama\nNOTE: At time of publication, there is no stable release for Ollama on Windows OS. Docker installation is recommended for ease of installation.\n1.4.3. Start Ollama Service. Ensure the CUDA toolkit from Step 1.3 is installed for optional GPU acceleration and the Ollama Docker image is successfully downloaded or the Ollama GUI is installed. Start the Ollama service.\nIf using Ollama GUI, run the GUI executable (e.g., ollama.exe)\nIf using Docker, run:\ndocker run -name ollama --net rugged_network d -v ollama:/root/.ollama -p 11434:11434 ollama/ollama\nIf using Docker with GPU acceleration, run:\ndocker run -name ollama --net rugged_network -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 ollama/ollama\n1.4.4. Run Ollama. Determine which Local LLM to use by visiting the full list of Ollama supported models at their website at https://ollama.com/library. Recommended models: llama3, llama2, mistral, mixtral. If using Docker, open the command line and type \"docker exec run"}, {"title": null, "content": "ollama run MODEL\u201d, replacing MODEL with the model name; if using Ollama GUI, type \u201collama run MODEL\u201d.\n1.5.\nSet up the Knowledge Graph Database. RUGGED supports Neo4j, accessible as a Docker container, Neo4j Desktop, or Neo4j AuraDB online server. Choose the appropriate service based on your needs, following the below step for the corresponding service.\n1.5.1. Neo4j Docker. Run the following commands to set up Neo4j in Docker, with PATH_TO_FOLDER as the full file-path for the folder (e.g., /Users/username/RUGGED). For more details on troubleshooting, refer to the Neo4j Docker website (https://hub.docker.com/ /neo4j).\ndocker pull neo4j\ndocker run -name neo4j --net rugged_network --publish=7474:7474 --publish=7687:7687 -d -v PATH_TO_FOLDER\\neo4j\\data:/data neo4j\nNOTE: When using Neo4j in Docker for the first time, initialize the service by setting up the default username and password. Run the neo4j_setup.py script (e.g., python neo4j_setup.py) or log in to the web interface at http://localhost:7474.\n1.5.2. Neo4j Desktop. If using Neo4j Desktop, download and install it from Neo4j website (https://neo4j.com/) and start the application. Create a new project by clicking \u201cNew\u201d and naming it appropriately, then click \u201cAdd\u201d to create a new Database Management System (DBMS). Select \u201cLocal DBMS\u201d, set a password, click \u201cCreate\u201d, then click \u201cStart\u201d and wait for the service to start. A green \u201cACTIVE\u201d text indicates the DBMS is running.\n1.5.3. Neo4j AuraDB. Visit the Neo4j website at (https://neo4j.com/cloud/aura-free/) create an account and log in. Select \u201cNew Instance\" to create an empty instance and save the URI and initial password to access the bolt interface (e.g., bolt://myurl.neo4j.com). Start the instance by clicking the play button if it has not started already, which will display the connection URlin the instance information box.\nNOTE: Neo4j AuraDB offers a free tier up to 200,000 nodes and 400,000 relationships. For larger graphs, visit Neo4j pricing.\n1.6. Set up the RUGGED environment. Run the following commands in the terminal to download the RUGGED Docker image and code repository:\ndocker pull pinglabutils/rugged:latest\ngit clone https://github.com/pinglab-utils/RUGGED\nVerify the downloaded Docker images by typing `docker images`. All Docker images from the previous step should be listed (e.g., Ollama, Neo4j).\n1.6.1. Configure OpenAl Service. If using OpenAl service, ensure your account and associated API key from Step 1.4.1 have sufficient funds. Modify RUGGED configuration files by editing the configuration file at \u2018RUGGED/config/openai_key.txt' and add the API key to the file.\n1.6.2. Configure OpenAl Agents. Determine which LLM agents within RUGGED's system will use OpenAl models. Modify the configuration file at 'RUGGED/config/llm_agents.json' and"}, {"title": null, "content": "update the agent fields to specify the selected OpenAl model version. Determine which OpenAl models to use from OpenAl's model documentation at https://platform.openai.com/docs/models. Recommended models: \"gpt-3.5-turbo\u201d, \u201cgpt-40\".\n1.6.3. Configure Ollama API Endpoint. By default, Ollama is accessible via API at \u2018http://localhost:11434'. If using a different service endpoint, modify the configuration files at 'RUGGED/config/ollama_config.json' and update 'OLLAMA_URI' field accordingly.\n1.6.4. Configure Ollama Agents. Determine which LLM agents within RUGGED's system will use Ollama. Modify the configuration file at \u2018RUGGED/config/llm_agents.json' and update the agent fields to specify 'ollama' as the selected model.\n1.6.5. Configure the Neo4j Endpoint. The default password and username for Neo4j in Docker is initialized in the \u2018RUGGED/config/neo4j_config.json' configuration file. If following steps 1.5.2 or 1.5.3, update the configuration file with the appropriate username, password, and URI by modifying the 'uri', 'username', and 'password' fields.\n1.7.\nStart the RUGGED Service. RUGGED integrates several services, which must be individually tested to ensure the software is working as expected. Start the RUGGED service by running command:\ndocker run -it --net rugged_network -gpus=all -v PATH_TO_FOLDER\\RUGGED\\:/data\nping-lab-utils:RUGGED /bin/bash\nTo verify the services are working as expected, navigate to the RUGGED directory and execute the Steps 1.7.1. through 1.7.4. in this terminal window.\n1.7.1. Verify LLM Service Functionality. Execute test scripts to verify OpenAl and/or Ollama services are functioning as expected. Navigate to the 'test' folder in the RUGGED directory and execute the following commands, as applicable:\ncd ./test\npython test_openai.py\npython test_ollama.py\n1.7.2. Verify Named Entity Recognition Service Functionality. By default, the required packages to execute the Named Entity Recognition of user queries are installed in the RUGGED. Execute a test script to verify it is functioning properly.\npython test_ner.py\n1.7.3. Verify Neo4j Service Functionality. Execute test scripts to verify the Neo4j service is functioning as expected.\npython test_neo4j.py\n1.7.4. (Optional) Verify HTTP Access to Neo4j. Open a web browser and visit the Neo4j user interface. For Neo4j in Docker or Desktop, the default URL is http://localhost:7474. For Neo4j AuraDB, use the link from step 1.5.3."}, {"title": null, "content": "1.8. (Optional) Troubleshooting. Ensure the services supporting RUGGED are verified ahead of time to anticipate issues when executing the rest of the protocol. Troubleshoot any unsuccessful tests from Step 1.7., if they exist. Test scripts will provide error messages describing the issues.\n1.8.1. Verify Docker Containers. Confirm all Docker containers are running by using `docker ps` in the terminal. The following containers should be running:\nRUGGED docker container\nNeo4j docker container (optional)\nOllama docker container (optional)\n1.8.2. Verify Networking Ports. For Docker services, ensure the correct ports are open and check logs with \u201cdocker logs neo4j\u201d or \u201cdocker logs ollama\u201d. By default, Neo4j uses ports 7474 for http, 7687 for its bolt interface; Ollama uses port 11434.\n1.8.3. Verify Service Applications. For applications installed directly on the device (e.g., Ollama and Neo4j Desktop), open the applications to confirm they are running.\n1.8.4. Verify Web Services. For Neo4j AuraDB, log into the website and verify the service is running.\n1.8.5. Verify Firewall Rules. Modify device firewall rules to ensure the firewall is not blocking any external services.\n1.8.6. Restart Device. If issues are not resolved, restart the device and retry from Step 1.8.1.\n1.8.7. Open an Issue. If problems persist, please open an issue on the RUGGED GitHub (https://github.com/pinglab-utils/RUGGED).\n2.\nAccessing Biomedical Knowledge and Information Extraction\nNOTE: These steps outline the process to incorporate biomedical knowledge and literature using results from two different information extraction pipelines: 1) the CaseOLAP LIFT biomedical text mining pipeline\u00b3 and 2) the Know2BIO knowledge graph construction workflow7. To use RUGGED with your own data, proceed to Step 4.\n2.1.\nBiomedical Literature Extraction. CaseOLAP LIFT is a computational protocol designed to investigate sub-cellular proteins and their associations with disease through biomedical literature text mining. This workflow enables the identification of relevant biomedical text and extracting high-level protein-disease relationships, which will inform our RAG workflow, enhancing analyses with targeted insights from biomedical reports.\n2.1.1. Run CaseOLAP LIFT Text Mining Analysis. Visit the CaseOLAP LIFT JoVE Protocol (https://app.jove.com/t/65084) and complete Steps 1 through 3; Steps 4 and 5 are not necessary for this analysis."}, {"title": null, "content": "2.1.2. Move Processed Text Documents. Successful completion of step 3 of this pipeline will result in a large number of biomedical text documents parsed and downloaded (pubmed.json) along with their full text (pmid2full_text_sections.json) found in the data folder of CaseOLAP LIFT. Move these files into the data folder of RUGGED using the below commands:\nmv PATH_TO_FOLDER/caseolap_lift/caseolap_lift_shared_folder/data/pubmed.json PATH_TO_FOLDER/RUGGED/data/text_corpus\nmv PATH_TO_FOLDER/caseolap_lift/caseolap_lift_shared_folder/data/pmid2full_text_sections.json PATH_TO_FOLDER/RUGGED/data/text_corpus\n2.1.3. Move Text Mining Results. Successful execution of the text mining pipeline will result in a knowledge graph formatted list of protein-disease associations (merged_edge_list.tsv) found in the result/kg folder. Number of publications and protein-disease associations will vary depending on settings used in Steps 1-3 of the protocol (see Table 2 for example). Move this file into the data folder of RUGGED using the below commands:\nmv PATH_TO_FOLDER/caseolap_lift/caseolap_lift_shared_folder/result/graph_data/merged_edge_ list.tsv PATH_TO_FOLDER/RUGGED/data/knowledge_graph\n2.2. Biomedical Knowledge Extraction. This step details the knowledge graph construction using Know2BIO, a comprehensive biomedical knowledge graph integrating data from 30 diverse sources. It features continuous updates and multi-modal data integration, which will be used to support our Retrieval-Augmented Generation (RAG) workflow.\n2.2.1. Clone Know2BIO Repository. Clone the repository by typing in the command line, using the below command. Navigate to the Know2BIO repository.\ngit clone https://github.com/Yijia-Xiao/Know2BIO.git\ncd Know2BIO\n2.2.2. Prepare Input Data and Licenses. Navigate to the dataset folder and follow the instructions in the README.md file. Note that some resources require the creation of user accounts (e.g., UMLS thesaurus, DrugBank).\ncd dataset\n2.2.3. Download Knowledge Base Resources. Execute the create_edge_files.py script and monitor progress of the knowledge graph extraction pipeline. Successful execution will result in a large number of biomedical entities represented in a .csv file in the Know2BIO/dataset/output folder.\npython create_edge_files.py"}, {"title": null, "content": "2.2.4. Construct Knowledge Graph. Execute the prepare_kgs.py script to integrate the information extracted in the previous step. This script automatically combines the extracted relationships into a unified knowledge graph, formatting the graph by data source and domain.\npython prepare_kgs.py\n2.2.5. Verify Output. Verify the completed kg files in the Know2BIO/dataset/know2bio_dataset directory. The combined knowledge graph will be in the file \u2018whole_kg.txt' which will be used for the RAG pipeline. The remaining steps in the repository README are not required for this analysis.\n2.2.6. Move Knowledge Graph Results. Successful execution of this pipeline will result in 'whole_kg.txt' with over 6 million edges, see Table 3. Verify the number of lines in the file and move the file into the /data/text_corpus/ of the RUGGED directory using the below command or using the file explorer on your computer.\nmv PATH_TO_FOLDER/Know2BIO/dataset/know2bio/whole_kg.txt PATH_TO_FOLDER/RUGGED/data/knowledge_graph\n2.3.\nConstruct a Combined Knowledge Graph. Steps 2.1 and 2.2 result in a knowledge graph from biomedical resources and high-level protein-disease relationships from text mining, respectively. This step integrates these data within a single unified knowledge graph.\n2.3.1. Verify Results in RUGGED directory. Verify the knowledge graph construction result file (whole_kg.txt) and the text mining relationship results (merged_edge_list.tsv) are in the knowledge_graph directory within the data folder.\n2.3.2. Iterate Results. Execute the provided combine_kg_results.py script to combine the text mining results within the knowledge graph results. This script will merge the extracted relationships and entities from both sources into a cohesive knowledge graph, ensuring the data is formatted consistently and integrated correctly. Below is an example command:\npython ./rugged/knowledge_graph/combine_kg_results.py --output_file ./data/knowledge_graph/rugged_knowledge_graph.txt ./data/knowledge_graph/whole_kg.txt ./knowledge_graph/merged_edge_list.tsv\n2.4. Filter Knowledge Graph. (Optional) This step samples a subset of the knowledge graph which will be used for the predictive analysis. This step is crucial for addressing technical limitations, as it filters out irrelevant data and reduces the graph size to execute the deep learning predictive analysis on limited computational resources.\n2.4.1. Identify Relevant Nodes. Determine which biomedical entities are of interest for the predictive analysis in Step 3. This can be done by reviewing the knowledge graph to pinpoint relevant nodes. For this protocol example, we focus on disease nodes for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy (DCM), represented by MeSH_Disease:D002312 and MeSH_Disease:D002311, respectively."}, {"title": null, "content": "2.4.2. Filter Knowledge Graph. Use the filter.py script to tailor the knowledge graph for predictive analysis by focusing on specific subgraphs or diseases of interest. The script employs k-hop filtering from disease nodes of interest to include relevant entities and relationships: Example command below, filtering the graph reachable within 2 nodes from the selected disease nodes.\npython ./rugged/knowledge_graph/filter.py --k 2 --disease \"MeSH_Disease:D002312,MeSH_Disease:D002311\"\n--input_file ./data/knowledge_graph/rugged_knowledge_graph.txt --output_file ./data/knowledge_graph/filtered_k2_rugged_knowledge_graph.txt\nNOTE: Increasing the k-hop value expands the data scope within the graph for prediction analysis but also demands more computational resources. Adjust the k-hop value based on the available resources and the analysis complexity.\n3.\nPerform Explainable Prediction Analysis. In this step, we will use GNNExplainer38 on a Graph Convolutional Network (GCN) to perform prediction analysis. The process involves splitting the filtered knowledge graph into training, validation, and test sets with an 85:5:10 ratio. This split allows us to evaluate the model's performance accurately. The goal is to predict potential edges (relationships) in the knowledge graph, providing insights into previously unknown associations.\n3.1. Ensure the RUGGED Docker container is running. If the previous terminal window as closed, connect to the Docker container by using the below command:\ndocker exec --it rugged /bin/bash\nOnce connected to the Docker container, navigate to the RUGGED directory.\n3.2. Determine the edge(s) to predict. Provide the edges as pairs of nodes in a .txt file (e.g., edges_to_predict.txt). The edges already existing in the knowledge graph will be filtered out from the predictions.\n3.3. Run the prediction analysis script. Specifying the edges to predict and the input knowledge graph as command line arguments. Example command:\npython ./rugged/predictive_analysis/generate_explainable_prediction.py -p edges_to_predict.txt -i ./data/knowledge_graph/filtered_k2_rugged_knowledge_graph.txt -o output -n 5 -k 10\n-p edges_to_predict.txt: Path to the file containing edges to predict.\n-i filtered_k2_rugged_knowledge_graph.txt: Path to the input knowledge graph.\n-o output: Directory for output files.\n-n 5: Number of top predictions to output.\n-k 10: Number of top important edges to visualize."}, {"title": null, "content": "3.4. Verify results are in the output folder. Examine the model results in prediction_results.csv and examine the top n predictions within the output folder. Review the top n predictions in the output folder. For each prediction, a graph visualization and relative importance scores for the top k edges are provided. The visualizations illustrate the most pertinent edges contributing to each prediction.\n3.5. Move Predictive Analysis Results. Once satisfied with the predictive analysis results, move the results into the /data/predictions/ of the RUGGED directory.\nNOTE: Explainability Al prediction analysis enhances the interpretability of predictions but requires substantial computational resources and time. Steps 4.1 and 4.2 can be performed concurrently with Step 3.\n4.\nHypothesis Generation\n4.1.\nConnect to the RUGGED Docker Container.\n4.1.1. Ensure the RUGGED Docker container is running. If the previous terminal window was closed, connect to the Docker container by using the below command:\ndocker exec --it rugged /bin/bash\n4.1.2. Navigate to the RUGGED directory. The remaining steps will be issued in this command line window and directory.\ncd/workspace/RUGGED\n4.1.3. Verify the supporting services are running. For example, if using Ollama and Neo4j in Docker, ensure the containers are running by typing 'docker ps'. Repeating Step 1.7 to verify services are functioning properly and Step 1.8 to troubleshoot issues if they exist.\n4.2. Prepare RAG Data. This step sets up the knowledge graph and text corpus for retrieval. Knowledge graph and literature may be substituted with user defined data by following the format outlined in our GitHub repository (https://github.com/pinglab-utils/RUGGED/tree/main/data). Place the data into the data/knowledge_graph/ and data/text_corpus/ directories, respectively.\n4.2.1. Ensure the text corpus is in the data/text_corpus/ directory from Step 2.1.6., the knowledge graph with text mining predictions file is located in the data/knowledge_graph/ directory from Step 2.3.3, and the prediction results are in data/predictions/ directory from Step 3.5.\n4.2.2. Populate the Neo4j Graph Database. Execute the provided script to create the necessary nodes, edges, and node features. This will allow RUGGED to access the data:\npython ./neo4j/prepare_neo4j.py"}, {"title": null, "content": "4.2.3. Index the Text Corpus. Execute the provided script to index the text corpus. This script will chunk the text into sections of 1000 tokens and create a vector database using the BERT (cite) model to generate embeddings. This process enables RUGGED to retrieve relevant text documents based on user queries:\npython ./text/prepare_corpus.py\n4.2.4. (Optional) Test Neo4j Database Retrieval. Send a test query to the Neo4j database to ensure it is populated correctly and can return expected results. Verify that the output matches the expected nodes and relationships in the database. Example command:\npython ./neo4j/test_neo4j_retrieval.py --query \"MATCH (n) RETURN n LIMIT 5\"\n4.2.5. (Optional) Test RAG Corpus Retrieval. Send a test query to the RAG text corpus to ensure the text retrieval system is working. Check that the retrieved documents are relevant to the query and that the embeddings are functioning as expected. Example command:\npython ./text/test_rag_retrieval.py --query \"example query text\"\n4.3. Explore Hypotheses with RUGGED. Start RUGGED in the command line interface to interact with the system and explore your hypotheses. Type the following command in your terminal:\npython rugged.py\n4.3.1. Interact with RUGGED. Once the CLI is running, you can start querying the system to retrieve relevant information. Use natural language queries or specific commands to interact with the knowledge graph and text corpus. An example use case is presented in Figures 3, 4, and the full transcript is in the Supplementary Materials.\n4.3.2. Query the Knowledge Graph. To extract specific information from the knowledge graph prepared in Step 2, pose your question in natural language, starting with the keyword \u201cquery\u201d. For example:\nquery \"What example of drug?\"\n4.3.3. Explore Predictions. To explore link prediction analyses from Step 3, ask to search for a specific relationship, leading with the keyword \u201cpredict\". For example:\npredict \"What are some new potential drug targets to treat ACM?\"\n4.3.4. Literature Retrieval. To explore documents related to a specific biomedical topic from Step 2, pose the question in natural language, leading with the keyword \"search\". For example:\nsearch \"What literature supports the possibility that protein X is related to disease Y?\"\n4.3.5. Iterate and Refine. Since RUGGED utilizes the chat-like capabilities of LLM models, user questions can be revised and refined in-context. Based on the initial results, refine your queries and continue exploring different hypotheses. Use the interactive capabilities of RUGGED to iteratively improve your understanding and gather more precise information."}, {"title": null, "content": "4.3.6. Rerun Cypher Commands in Neo4j. (Optional) The knowledge graph query results will provide a Cypher command used to retrieve the information. You can rerun or modify this command by visiting the Neo4j browser interface from Step 1.7.4 (e.g., at http://localhost:7474). Paste and modify the Cypher commands as needed to refine your queries and gather more specific insights from your data.\n4.3.7. Summarize Conversation. Review the retrieved information and summarize the conversation with RUGGED. Type the keyword \"summarize\u201d to output a summary of the interaction to a text file for later analysis. The full text response will be displayed in the terminal.\n4.3.8. Review Chat Logs. For troubleshooting and reproducibility, full text of the interaction is accessible in the /log/ folder in RUGGED, including intermediate commands and conversations between LLM agents within RUGGED.\n4.4.\nShutting Down and Restarting RUGGED.\n4.4.1. Get Docker Container IDs. Use docker ps to list all running containers and obtain the container IDs for RUGGED, Neo4j, and Ollama.\ndocker ps\nFor all following commands, replace <container_id_for_rugged>, <container_id_for_neo4j>, and <container_id_for_ollama> with the actual container IDs.\n4.4.2. Stop Docker Containers. Shut down RUGGED and the associated Docker containers using their container IDs.\ndocker stop <container_id_for_rugged>\ndocker stop <container_id_for_neo4j>\ndocker stop <container_id_for_ollama>\nNOTE: While shutting down your device will stop all Docker containers automatically, manually stopping these containers first is recommended to prevent potential data loss and ensure all processes close properly.\n4.4.3. Restart Docker Containers. To restart the RUGGED system, use the container IDs to start the necessary Docker containers.\ndocker start <container_id_for_rugged>\ndocker start <container_id_for_neo4j>\ndocker start <container_id_for_ollama>\n4.4.4. Reattach to Docker Network. If needed, use these commands to re-attach the containers to the network.\ndocker network connect rugged_network <container_id_for_rugged>\ndocker network connect rugged_network <container_id_for_neo4j>\ndocker network connect rugged_network <container_id_for_ollama>\n4.4.5. Verify Service Functionality. Upon restart, repeat Step 1.7 and 1.8 to ensure the software is working as expected."}, {"title": "REPRESENTATIVE RESULTS:", "content": "These representative results were obtained by following the procedure outlined in this protocol. A text mining association analysis was performed following the CaseOLAP LIFT protocol with default parameters, studying 8 broad categories of cardiovascular disease and their association with mitochondrial proteins (GO:0005739)\u00b3. In total, 635,696 publications through May 2024 were determined as relevant to these diseases; among them, 4,655 high confidence protein-disease associations were identified to inform downstream analyses. A biomedical knowledge graph was constructed using the software code from Know2BIO using default settings in May 20247. The resulting knowledge graph consists of 219,450 nodes and 6,323,257 edges as well as node features for 189,493 nodes, consisting of node descriptions, protein/gene sequences, chemical structure, etc. where available.\nThe RUGGED system was initialized by constructing vector```json\n databases for both the knowledge graph nodes and features as well as the CVD-relevant research publications. All knowledge graph nodes, edges, and node features were chunked using a chunk size of 20 tokens with the BART39 embedding model to prepare for RAG vector search. Similarly, original contribution and review articles were processed using a chunk size of 500 tokens and the BART embedding model to prepare for RAG vector search. For literature retrieval, full text publications greater than 500 tokens were hierarchically summarized based on the publication subsections by the BART embedding model. The ChatGPT 40 model was used for remaining LLM agents in the system.\nAn example use-case of hypothesis exploration pertaining to ACM and DCM diseases is outlined in Figure 3, with the model response in Figure 4. A full transcript of the interaction is in Supplementary Materials. To predict novel avenues for therapeutic treatment, the resulting knowledge graph was filtered based on relevance to, including only nodes and edges within 2-hop from Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy (DCM), represented by MeSH_Disease:D002312 and MeSH_Disease:D002311, respectively. This use-case examines a finer grain investigation of the major cardiovascular disease subtypes, in particular ACM which is a subtype of arrhythmia (ARR). The resulting knowledge graph of 75,621 nodes and 1,376,517 edges was applied to train a graph convolutional neural network prediction model, with evaluation metrics reported in Table 4. The top 10 predictions by the model were examined by a graph explainability module, GNNExplainer38, to identify the top nodes and edges contributing to the prediction."}, {"title": "DISCUSSION:", "content": "The RUGGED protocol leverages modern language models with up-to-date knowledge to empower investigators to dynamically explore the evolving biomedical landscape and uncover new knowledge. This protocol should be executed in the outlined sequence. Sections 2 and 3 are essential for preparing biomedical knowledge, while Section 5 prepares the data for retrieval-augmented generation and user interaction with the LLM system. Some time-intensive steps may run concurrently. For example, creating the Neo4j graph (Section 5a) can begin during prediction analysis (Section 4), and indexing can begin after constructing the knowledge graph (Section 2b) and text-mining (Section 3e). These steps must be re-run to integrate intermediate results. While designed for biomedical information retrieval, this protocol can handle any text and graph data, such as in-house data, clinical notes, or electronic health records. Data formatting details are in Section 4.\nOur platform relies on several interconnected technologies, including language models, graph databases, and vector databases. Test scripts are included in the test folder in the GitHub repository and are useful to verify these services are running properly. External services such as OpenAl's LLM and Neo4j AuraDB incur user costs, with prices subject to change by the vendor. These optional services have no-cost alternatives using locally hosted services, requiring only sufficient computational resources. However, these alternatives may reduce model performance with improved convenience, making these alternatives unsuitable for some use cases.\nWith the rapidly evolving LLM landscape, new landmark models and task-specific models are released regularly. At the time of this publication, the most appropriate models were chosen for the task. Users can choose which LLM to use and update the configuration file accordingly by visiting the Hugging Face repository (see Section 1.iv.2.a) for open sourced models, or by visiting the LLM service provider website of choice (e.g., OpenAl). These models can be selected depending on their relevance to a particular use case. For example, incorporating models focused on ensuring model responses are fair, censored, and free of hate speech, into this workflow is essential for public-facing models. Furthermore, prompt engineering is necessary to achieve optimal behavior from the LLM. While prompt engineering is largely handled within the RUGGED workflow, effective prompts for one LLM may be suboptimal for another, particularly if the models are fine-tuned on different materials. Users can edit prompts used within the RUGGED workflow in the configuration folder, within the prompts.json file.\nWhile RAG systems aim to reduce hallucinations in LLMs by grounding responses in evidence, these models can still produce incorrect information. This often occurs when retrieved information exceeds the model's context window, causing it to 'forget' some details. Choosing a suitable LLM model helps to mitigate this issue. For instance, GPT-40 has a context limit of 128k tokens, significantly more than GPT-3.5 Turbo's 16k token limit, albeit at a higher cost. Furthermore, LLMs fine-tuned with specific domain knowledge can potentially enhance accuracy in biomedical applications. Despite these measures, it is essential to cross-check the information before proceeding with costly wet lab experiments."}, {"title": null, "content": "RUGGED leverages explainable Al within a Retrieval-Augmented Generation (RAG) pipeline to make link predictions, identifying both reliable and previously undiscovered relationships. While traditional RAG systems rely on bulk similarity-based retrieval, our approach connects explainability with a targeted response augmentation. Table 4 highlights the model's strong performance, demonstrating high recall (validation: 0.9748, test: 0.9755) and balanced F1-scores (validation: 0.7964, test: 0.7973), indicating reliability in identifying true positives, albeit with a higher rate of false positives. The model's robustness is further supported by its AUROC (validation: 0.9634, test: 0.9642) and AUPRC (validation: 0.9710, test: 0.9716) values. Precision (validation: 0.6732, test: 0.6741), however, could benefit from threshold tuning, including detailed node features, or improved handling of class imbalance. The model's effectiveness is highly dependent on the input knowledge graph; overfitting is a risk with smaller graphs, while larger graphs demand greater computational resources.\nRUGGED's primary use is hypothesis generation to investigate various hidden relationships, such as disease mechanisms, drug treatments, and speeding up literature triage. To reduce computational burden, many applications can be hosted on a server (e.g., AWS or computational server) and configured to update periodically with the latest information. This workflow can also be adapted to serve specific applications such as serving as a platform to include patient data with local models to uphold security, privacy, and confidentiality. The system can be configured to assist with e-Learning to explain concepts or knowledge at different levels, such as explaining research findings or knowledge base information to the public or students."}]}