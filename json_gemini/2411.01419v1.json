{"title": "PSFORMER: PARAMETER-EFFICIENT TRANSFORMER\nWITH SEGMENT ATTENTION FOR TIME SERIES FORE-\nCASTING", "authors": ["Yanlong Wang", "Jian Xu", "Fei Ma", "Shao-Lun Huang", "Danny Dongning Sun", "Xiao-Ping Zhang"], "abstract": "Time series forecasting remains a critical challenge across various domains, often\ncomplicated by high-dimensional data and long-term dependencies. This paper\npresents a novel transformer architecture for time series forecasting, incorporat-\ning two key innovations: parameter sharing (PS) and Spatial-Temporal Segment\nAttention (SegAtt). We also define the time series segment as the concatena-\ntion of sequence patches from the same positions across different variables. The\nproposed model, PSformer, reduces the number of training parameters through\nthe parameter sharing mechanism, thereby improving model efficiency and scal-\nability. The introduction of SegAtt could enhance the capability of capturing lo-\ncal spatio-temporal dependencies by computing attention over the segments, and\nimprove global representation by integrating information across segments. The\ncombination of parameter sharing and SegAtt significantly improves the forecast-\ning performance. Extensive experiments on benchmark datasets demonstrate that\nPSformer outperforms popular baselines and other transformer-based approaches\nin terms of accuracy and scalability, establishing itself as an accurate and scalable\ntool for time series forecasting.", "sections": [{"title": "INTRODUCTION", "content": "Time series forecasting is an important learning task with significant application values in a wide\nrange of domains, including the weather prediction (Ren et al., 2021; Chen et al., 2023a), traffic\nflow (Tedjopurnomo et al., 2020; Khan et al., 2023), energy consumption (Liu et al., 2020; Nti et al.,\n2020), anomaly detection (Zamanzadeh Darban et al., 2022) and the financial analysis (Nazareth &\nRamana Reddy, 2023), etc. With the advancement of artificial intelligence techniques, significant\nefforts have been devoted to developing innovative models that continue to improve the prediction\nperformance (Liang et al., 2024; Wang et al., 2024). In particular, the transformer-based model\nfamily has recently attracted more attention for its proved success in nature language processing\n(OpenAI et al., 2024) and computer vision (Liu et al., 2021; Dosovitskiy et al., 2021). Moreover,\npre-trained large models based on the transformer architecture have shown advantages in time series\nforecasting(Liu et al., 2024a; Jin et al., 2024; Chang et al., 2023; Woo et al., 2024), demonstrating\nthat increasing the amount of parameters in transformer models and the volume of training data can\neffectively enhance the model capability.\nOn the other side, many simple linear models (Zeng et al., 2023; Li et al., 2023) also shown competi-\ntive performance compared to the more complex designs of transformer-based models. One possible"}, {"title": "RELATED WORK", "content": ""}, {"title": "TEMPORAL MODELING IN TIME SERIES FORECASTING", "content": "In recent years, time series analysis has received widespread attention, with more deep learning\nmethods being applied to time series forecasting. These deep learning methods focus on estab-\nlishing temporal dependencies within time series data to predict future trends. The models can be\nbroadly categorized into RNN-based, CNN-based, MLP-based, and Transformer-based approaches.\nRNNs and their LSTM variants were widely used for time series tasks in the past, with related\nworks such as DeepAR(Salinas et al., 2020). CNN-based methods like TCN (Bai et al., 2018) and\nTimesNet (Wu et al., 2023) have been designed to adapt convolutional structures specifically for\ntemporal modeling. MLP-based approaches, such as N-BEATS (Oreshkin et al., 2020), RLinear\n(Li et al., 2023), and TSMixer (Chen et al., 2023b), have demonstrated that even simple network\nstructures can achieve solid predictive performance. Moreover, Transformer-based models have\nbecome increasingly popular in time series forecasting due to the unique attention mechanism of\nTransformers, which provides strong global modeling capabilities. Many recent works leverage this\nto enhance time series modeling performance, such as Informer (Zhou et al., 2021), Autoformer\n(Wu et al., 2021), Pyraformer (Liu et al., 2022), and Fedformer (Zhou et al., 2022). Additionally,\nPatchTST (Nie et al., 2023) further divides time series data into different patches to enhance the abil-\nity to capture local information. However, the aforementioned models primarily focus on temporal\nmodeling, with less emphasis on modeling the relationships between variables. Although PatchTST\nattempted to incorporate cross-channel designs, it observed degraded performance in their model."}, {"title": "VARIATE MODELING IN TIME SERIES FORECASTING", "content": "In addition to modeling temporal dependencies, recent works have focused on modeling inter-\nvariable dependencies (donghao & wang xue, 2024; Zhang & Yan, 2023; Ilbert et al., 2024; Woo\net al., 2024; Liu et al., 2024b). ModernTCN (donghao & wang xue, 2024) employs different 1-D\nconvolutions to capture the temporal and variable dimensions separately; Crossformer (Zhang &\nYan, 2023) constructs attention mechanisms across both time and spatial dimensions; SAMformer\n(Ilbert et al., 2024) applies attention to the variable dimension to model cross-channel dependen-\ncies; MORAI (Woo et al., 2024) \"flattens\" multivariate time series into univariate sequences to\nmerge information across variables; iTransformer (Liu et al., 2024b) represents multivariate time se-\nries and captures global dependencies. All of these works emphasize the simultaneous modeling of\nboth variable and temporal dependencies as critical directions for improving multivariate time series\nmodeling, which helps establish global spatial-temporal dependencies. However, this may weaken\nthe ability to capture local spatial-temporal dependencies. Additionally, expanding the global recep-\ntive field of spatio-temporal dependencies could increase model complexity, which in turn may lead\nto overfitting due to the larger number of parameters. Our work addresses these issues and proposes\nsolutions to mitigate them."}, {"title": "THE PSFORMER FRAMEWORK", "content": ""}, {"title": "PROBLEM FORMULATION", "content": "We denote the input multivariate time series $X \\in \\mathbb{R}^{M\\times L}$ as with M variables and look-back window\nL: $(X_1, X_2, ..., X_L)$, where $x_t$ represents the M-dimensional vector at time step t. L will be equally\ndivided into N non-overlapping patches of size P. $P(i)$ denote the i-th patch with size P, where"}, {"title": "MODEL STRUCTURE", "content": "The PSformer model we constructed is depicted in Figure 2. The key components of the model\ninclude the PSformer Encoder, segment attention, and PS Block. The PSformer Encoder serves as\nthe backbone of the model and contains both the segment attention and the PS Block. The PS Block\nprovides the parameters for all layers within the Encoder, utilizing the parameter sharing technique.\nForward Process The univariate time series of length L for the i-th variable, starting at time index\n1, is denoted as $x^{(i)} = (x^{(i)}_1, x^{(i)}_2,...,x^{(i)}_L)$, where $i = 1,..., M$. Then the input $(x_1,...,X_L)$ with M\ndimensions is presented as $x_1 \\in \\mathbb{R}^{M\\times L}$, and $x_1$ is used as the input to the transformer network\nstructure. Similar to other time series forecasting methods, we use a RevIN layer Kim et al. (2022),\nwhich is added at both the input and output of the model."}, {"title": "Spatial-Temporal Segment Attention", "content": "We introduce spatial-temporal segment attention (SegAtt), which merges patches from different\nchannels at the same position into a segment and establishes spatial-temporal relationships across\ndifferent segments. Specifically, the input time series $X \\in \\mathbb{R}^{M \\times \\check{Z}}$ is first divided into patches, where\nL = P \u00d7 N, and then transformed into $X \\in \\mathbb{R}^{(M \\times P) \\times N}$. By merging the M and P dimensions,\nthe input becomes $X \\in \\mathbb{R}^{C \\times N}$, where C = M \u00d7 P, facilitating the subsequent cross-channel\ninformation fusion.\nIn this transformed space, identical $Q \\in \\mathbb{R}^{C \\times N}$, $K \\in \\mathbb{R}^{C \\times N}$, and $V \\in \\mathbb{R}^{C \\times N}$ matrices are\ngenerated by applying a shared block's non-linear projection (applied by Parameter Shared Block),\nwith weights $W_Q \\in \\mathbb{R}^{N \\times N}$ and $W_K \\in \\mathbb{R}^{N \\times N}$ used to project the input $X_{in} \\in \\mathbb{R}^{C \\times N}$. The\n$Q$ and $K$ matrices are then multiplied using a dot-product operation to form the attention matrix\n$QK^T \\in \\mathbb{R}^{C \\times C}$, which captures relationships between different spatial-temporal segments (in the\nC dimension) and is used to act on the V matrix. While the computation of Q, K and V involves\nnolinear transformations of the input $X_{in}$ across segments in the N dimension (corresponding to\nthe dmodel dimension of attention in the NLP domain), the scaled dot-product attention primarily\napplies attention across the C dimension, allowing the model to focus on dependencies between\nspatial-temporal segments across channels and time.\nThis mechanism ensures that information from different segments is integrated through the com-\nputation of Q, K, and V. It also captures local spatial-temporal dependencies within individual"}, {"title": "Parameter Shared Block", "content": "In our work, we propose a novel Parameter Shared Block (PS Block),\nwhich consists of three fully connected layers with residual connections, as illustrated in Figure 2.\nSpecifically, we construct three trainable linear mappings $W^{(i)} \\in \\mathbb{R}^{N \\times N}$ with $j \\in \\{1,2,3\\}$. The\noutput of the first two layers is computed as:\n$X_{out} = GeLU(X_{in}W^{(1)})W^{(2)} + X_{in}$,\nwhich follows a similar structure as the feed-forward network (FFN) with residual connections. This\nintermediate output $X_{out}$ is then used as the input for the third transformation, yielding:\n$X_{out} = X_{in} W^{(3)}$.\nTherefore, the PS Block as a whole can be expressed as:\n$X_{out} = (GeLU(X_{in}W^{(1)})W^{(2)} + X_{in})W^{(3)}$,\nand we denote PS Block output as $X_{out} = X_{in}W_S$, where $W_S \\in \\mathbb{R}^{N \\times N}$ and $X_{out} \\in \\mathbb{R}^{C \\times N}$. The\nstructure of the PS Block allows it to perform nonlinear transformations while preserving a linear\ntransformation path. Although the three layers within the PS Block have different parameters, the"}, {"title": "PSformer Encoder", "content": "In the PSformer Encoder, as Figure 2, each layer shares the same parameters\n$W_S$ of PS Block. For the input $X_{in}$, the transformation in the PSformer Encoder can be expressed\nas follows:\nSegAtt stage one is represented as: $Q^{(1)} = X_{in}W_S, K^{(1)} = X_{in}W_S, V^{(1)} = X_{in}W_S$,\nTherefore, $Q^{(1)}, K^{(1)}, V^{(1)} \\in \\mathbb{R}^{C\\times N}$. Dot-product attention operation\n$O^{(1)} = Attention^{(1)} (Q^{(1)}, K^{(1)}, V^{(1)}) = Softmax(\\frac{Q^{(1)} K^{(1)}}{\\sqrt{d_k}})V^{(1)}$.\nthen with ReLU activation:$O^{(1)}_{act} = ReLU(O^{(1)} )$.\nSegAtt stage two is represented as: $Q^{(2)} = O^{(1)}_{act} W_S, K^{(2)} = O^{(1)}_{act} W_S, V^{(2)} = O^{(1)}_{act}W_S$,\nTherefore, $Q^{(2)}, K^{(2)}, V^{(2)} \\in \\mathbb{R}^{C\\times N}$. Dot-product attention operation\n$O^{(2)} = Attention^{(2)} (Q^{(2)}, K^{(2)}, V^{(2)}) = Softmax(\\frac{Q^{(2)} K^{(2)}}{\\sqrt{d_k}})V^{(2)}$.\nThe two-stage SegAtt mechanism can be viewed as analogous to an FFN layer, where the MLP is\nreplaced with attention operations. Additionally, residual connections are introduced between the\ninput and output, and the result is then fed into the final PS Block.\nThe transformation in the final PS Block is represented as $O^{out} = O^{in}W_S$. Therefore, the entire\nencoder can be expressed as\n$X^{out} = (Attention^{(2)} (ReLU(Attention^{(1)}(X^{in}))) + X^{in})W_S,$\nwith $X^{out} \\in \\mathbb{R}^{C\\times N}$. Then we apply a dimensionality transformation to obtain $X^{out} \\in \\mathbb{R}^{M\\times L}$,\nsince C = M \u00d7 P and L = P \u00d7 N.\nAfter passing through n layers of the PSformer Encoder, the final output is $X^{pred} = X^{out} W_F$\nwhere $X^{pred} \\in \\mathbb{R}^{M\\times F}$, and $W_F \\in \\mathbb{R}^{L \\times F}$ is a linear mapping, where F is the prediction length.\nThe $X^{pred}$ is the final output of the PSformer model."}, {"title": "EXPERIMENT", "content": "Datasets In this paper, we focus on the long-term time series forecasting. We follow the time series\nforecasting work in Ilbert et al. (2024) and use 8 mainstream datasets to evaluate the performance of\nour proposed PSformer model. As shown in Table 1, these datasets include 4 ETT datasets (ETTh1,\nETTh2, ETTm1, ETTm2), as well as Weather, Traffic, Electricity, and Exchange. These datasets\nhave been used as benchmark evaluations in many previous time series forecasting studies."}, {"title": "RESULTS AND ANALYSIS", "content": ""}, {"title": "Summary of Experimental Results", "content": "The main experimental results are reported in Table 2. PS-\nformer achieved the best performance on 6 out of 8 major datasets in long-term time series fore-\ncasting tasks, demonstrating its strong predictive capabilities across various time series prediction\nproblems. This success is attributed to its segment attention mechanism, which enhances the ex-\ntraction of spatial-temporal information, and the parameter-sharing structure, which improves the\nmodel's robustness. The complete experimental results are in Appendix B.2."}, {"title": "Comparison with Other SOTA models", "content": "Compared to other Transformer-based models, PSformer\ndemonstrates higher predictive accuracy, reflecting the effectiveness of dividing multivariate time\nseries data into spatial-temporal segments for attention calculation. The neural network's ability\nto extract information from all spatial-temporal segments enhances the prediction performance. In\ncontrast to current large pre-trained models, PSformer not only achieves better accuracy but also\nreduces the amount of parameters through parameter sharing. Although linear models are simpler\nand have fewer parameters with satisfactory performance in some cases, the ability to extract rich\ninformation from complex data is limited. In contrast, PSformer integrates the residual connections,\nthus enabling a linear path while retaining the capability to process complex nonlinear information.\nMoreover, the ConvFFN component in ModernTCN tailored for temporal data also confirms that the\nconvolutional mechanism, which actually embodies the idea of parameter sharing, is also effective\nin the time series domain. With the same spirit, we have successfully applied the parameter sharing\nto the transformer-based models in the time series field and achieved superior performance."}, {"title": "ABLATION STUDIES", "content": ""}, {"title": "The Effect of segments Numbers", "content": "Since PSformer employs a non-overlapping patching to construct\nsegments, the model's performance is affected by the number of segments. Therefore, we tested the\nmodel's performance with different segment numbers on two datasets, ETTh1 and ETTm1. Given\nthat the input sequence length is fixed at 512, the number of segments must be a divisor of the\nsequence length. Consequently, we set the number of segments to 8, 16, 32, and 64, and test the\nmodel on four different forecasting horizons. The test results are shown in Table 3, which indicate\nthat the number of segments impacts the model's prediction accuracy to some extent. Across both\ndatasets, a moderate number of segments (such as 32) tends to perform the best, particularly for\nboth short and long forecasting horizons. For shorter horizons (96, 192), using 16 or 32 segments\ngenerally yields the lowest MSE, while fewer segments (8) often result in poorer performance. As\nthe horizon increases (336, 720), 32 segments consistently lead to optimal results, indicating that\nthis number balances the extraction of both local and global temporal features effectively."}, {"title": "The Effect of PSformer Encoder Numbers", "content": "Since PSformer adopts the segment attention mecha-\nnism, with non-shared PS Block parameters across different Encoders (shared within Encoder), we\ntested the impact of varying the number of encoders on model performance. We conducted tests\non the ETTh1 and ETTm1 datasets, varying the number of encoders from 1 to 4 with four differ-\nent forecasting horizons. The experimental results are shown in Table 4. The results indicate that\nETTm1 performs best with 3 encoders, while ETTh1 achieves better performance with just 1 en-\ncoder. This may suggest that for smaller datasets, fewer encoders result in better performance, as\nreducing the number of encoders decreases the amount of model parameters, thereby mitigating the\nrisk of over-fitting."}, {"title": "Ablation of Parameter Sharing methods", "content": "We investigate the impact of parameter-sharing mecha-\nnism on the model performance. In addition to the default parameter-sharing approach, which shares\nparameters only within encoder (In-Encoder), we also test the following approaches: a. no pa-\nrameter sharing, i.e., None; b. sharing parameters only across encoders (with different parameters"}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In this work, we proposed the PSformer model for multivariate time series forecasting, which lever-\nages segment attention technique to facilitate information transfer among time series variables and\ncapture spatial-temporal dependencies. By employing parameter sharing, the model effectively\nimproves parameter efficiency and reduces the risk of overfitting when the data size is relatively\nlimited. This network structure, which combines parameter sharing with segment attention mecha-\nnism, achieves state-of-the-art performance on long-term multivariate forecasting tasks by enhanc-\ning model parameter efficiency and improving the utilization of channel-wise information. Future\nwork can focus on applying this parameter sharing and segment attention technique to the develop-\nment of pre-trained large models for time series forecasting, to overcome the issue of excessively\nlarge parameter counts in existing pre-trained models, and improve the capability of extracting in-\nformation from multivariate time series. Additionally, exploring model architecture designs with\nparameter sharing to improve prediction performance still holds potential value."}, {"title": "EXPERIMENTAL CONFIGURATION", "content": ""}, {"title": "DETAILS OF BASELINE SETTINGS", "content": "We conducted all of our experiments with look-back window L = 512 and prediction horizons\nH \u2208 {96,192,336,720}. Results of PSformer and ModernTCN reported in Table 11 come from\nour own experiments. The difference between our experiment with ModernTCN and its official\ncode is that we standardized the look-back window to 512 and set drop_last=False for the test set\nin the dataloader to ensure consistency with our experimental settings for a fair comparison. For\nMOMENT and GPT4TS, the results are collected from (Goswami et al., 2024), except Exchange\nrate (which is tested on official code repository). The results of SAMformer, TSMixer, FEDformer\nand Autoformer are obtained from (Ilbert et al., 2024), while the results of iTransformer, PatchTST,\nand RLinear are taken from (Liu et al., 2024b)."}, {"title": "SETTINGS FOR PSFORMER", "content": "We provides an overview of the experimental configurations for the PSFormer model across various\ntasks and datasets in Table 7. The experiments cover multiple time-series datasets, including ETTh1,\nETTh2, ETTm1, ETTm2, Weather, Electricity, Traffic, and Exchange. In all experiments, the input\nsequence length (Input Length T) is set to 512, and each input is equally divided into 32 non-\noverlapping segments (Segments Num. N). The model architecture uses 3 Encoders for tasks,\nwhile for the ETTh1, ETTh2, ETTm2 and Exchange, 1 Encoder are used.\nThe learning rate adjustment strategy (lradj) is set to \"constant\" for all experiments, with a fixed\nlearning rate (LR) of $10^{-4}$. The loss function used in all experiments is Mean Squared Error (MSE).\nThe batch size is set to 16 for most tasks, except for the Traffic dataset, where the batch size is 8.\nEach experiment runs for 300 epochs, with a patience value of 30 for early stopping. A fixed random\nseed of 1 is applied across all experiments to ensure reproducibility."}, {"title": "MODEL SIZE COMPARISON", "content": "Table 8 presents a comparison of the parameter size across different models, including the PS-\nFormer and other baseline models such as SAMformer, TSMixer, ModernTCN, and RLinear.\nThe comparison is conducted on ETTh1, Weather, and Traffic datasets, with prediction horizons\nH \u2208 {96, 192, 336, 720}. PSFormer is evaluated in two configurations: the full model and the en-\ncoder part. The parameters of the encoder part refer to the number of parameters after excluding the\nlinear mapping in the output layer. The table denote that both PSFormer (full) and SAMformer have\nparameter sizes that are close to RLinear, where RLinear's parameters mainly stem from the linear\nmapping between input and output. Notably, the parameter sizes of these three models are relatively\nunaffected by the number of input channels. In contrast, TSMixer and ModernTCN exhibit signifi-\ncantly larger parameter sizes, with the number of input channels playing a major role in the overall\nparameter burden. The relative size comparison shows that TSMixer and ModernTCN have several"}, {"title": "RUNNING TIME COMPARISON", "content": "The average running time per iteration (s/iter) of different models on the ETTh1 and Weather\ndatasets with varying prediction horizons is shown in Table 9. PSformer demonstrates relatively\nstable running times across different horizons. For the ETTh1 dataset, the running time remains\nbetween 0.011 and 0.012 seconds, while for the Weather dataset, it varies slightly between 0.026\nand 0.027 seconds. PSformer also shows comparatively efficient running times across the datasets,\nwith performance that remains competitive even as the prediction horizon increases. This indicates\nthat PSformer manages computational costs effectively, especially when compared to more complex\nmodels."}, {"title": "DISCUSS ABOUT POSITIONAL ENCODING", "content": "The reasons of eliminating positional encoding Similarly, SAMformer does not use positional\nencoding because it applies attention to the channel dimension, where there is no strict sequential\nrelationship between channels. Although SegAtt is also a cross-channel structure, it involves local\ntime series constructed by patches. We consider such local sequences as local representations (or"}, {"title": "SHARPNESS-AWARE MINIMIZATION (SAM)", "content": "Optimization steps SAM optimizer (Foret et al., 2021) modifies the typical gradient descent update\nto seek a flatten optimum. Below is the mathematical formulation:\nLet \u03b8 be the model parameters, L(\u03b8) be the loss function, and \u2208 be a small perturbation applied to\nthe parameters. The SAM optimization process can be described in two steps:\n\u2022 Find the adversarial perturbation that maximizes the loss in a neighborhood of the current\nweights \u03b8:\n$\\hat{\\epsilon} = arg \\underset{\\|\\epsilon\\|<\\rho}{max} L(\\theta + \\epsilon)$\nwhere p is a small constant that controls the size of the neighborhood.\n\u2022 Update the parameters in the direction that minimizes the loss with respect to the perturbed\nparameters:\n$\\theta \\leftarrow \\theta \u2013 \\eta V_{\\theta}L(\\theta + \\hat{\\epsilon})$\nwhere n is the learning rate.\nAs used in SAMformer, we also employ this optimization technique to train our models, which can\ngenerate promising results."}, {"title": "MORE RESULTS AND ANALYSIS", "content": ""}, {"title": "INVESTIGATION OF HYPER-PARAMETER P", "content": "The Effect of p Since we employed SAM to ensure training stability, we also conducted sensitivity\ntests on the hyper-parameter p in SAM. We divided the range of p from 0 to 1 into 10 equal parts\nand tested its effect on model prediction performance across the ETTh1, ETTm1, ETTm2, and\nWeather datasets. The results are shown in Figure 3. It can be observed that as the parameter\np gradually increases in SAM can improve the model's prediction performance to some extent.\nHowever, if p is set too large, it may degrade the model's performance. When selecting p, it's\nimportant to consider the dataset's complexity and noise levels, as well as the model's architecture.\nFor more complex datasets or larger models, a slightly larger p can help smooth the loss landscape\nand improve generalization. Further, p should also be balanced with the learning rate to avoid\ninstability or performance degradation. As a comparison, we also report the p* used by PSformer,\nSAMformer and TSMixer in the Table 10."}, {"title": "FULL RESULTS", "content": "Table 11 presents the detailed experimental results of different models and forecasting horizons,\nproviding a comprehensive evaluation of their performance in long-term time series forecasting\ntasks. The performance is measured using Mean Squared Error (MSE) and Mean Absolute Error\n(MAE). In the table, red values represent the best performance in the respective task and metric,\nwhile blue-lined values indicate the second-best performance.\nThe last row of the table summarizes the number of first-place results for each model across all tasks.\nAs can be seen, PSformer achieved the best MSE performance in 20 out of 32 prediction tasks, and\nranked second in 8 tasks. In terms of MAE, PSformer achieved the best results in 22 tasks and came\nsecond in 5 tasks. This clearly demonstrates the superior performance of PSformer compared to\nother baseline models in long-term time series forecasting tasks.\nThe next best-performing model is ModernTCN, which achieved the best MSE results in 6 tasks and\nthe best MAE results in 3 tasks. While other models such as SAMformer and PatchTST also showed\ncompetitive performance in some tasks, their overall results are not as strong as those of PSformer\nand ModernTCN. In summary, PSformer's strong performance across multiple benchmark tasks\nsuggests its potential effectiveness in long-term forecasting."}, {"title": "TRAINING LOSS", "content": "Figure 4 illustrates the training and validation loss curves of the ETTh1 and ETTm1 datasets with\nprediction horizons H = {96, 192}. In this experiment, we set the number of epochs to 200 and\ndisabled early stopping to observe the complete training process. As shown in the plots, despite the\nmodel reaching convergence early in the training (as indicated by the flattening of the training loss\ncurve), the validation loss remains consistently low throughout the training duration. This indicates\nthe model's stability and its ability to generalize well to unseen data over extended epochs."}, {"title": "VISUALIZATION", "content": "To better understand our method, we present the forecast curves and the corresponding attention\nmaps for several selected samples. The attention maps visualize the attention weights in different\nstages of the model, providing insight into how the model processes the input data at each stage.\nFor the ETTh1 in Figure 5, the input sequence length is 512, and we display the last 100 time steps\nof the input. The prediction length is set to 96. We select five samples from the ETTh1 dataset, and\nfor each sample, we visualize the attention maps for stage 1 and stage 2 of the SegAtt. From the\nattention maps, it is evident that there are significant variations in the attention distributions across\ndifferent samples. Additionally, the attention maps from stage 1 and stage 2 also show noticeable\ndifferences, despite both stages sharing the same PS block parameters. This indicates that while the\ntwo-stage share parameters, they are able to handle and process the information differently, capturing\ndifferent aspects of the input data at each stage of the model.\nFor the Weather in Figure 6, the input length is also 512, and the last 100 time steps are displayed,\nwhile the prediction length is 192. Since the model for this dataset employs a three-layer En-\ncoder structure, we display the attention maps for both stages across each layer. Specifically, the\nnotation\"1-2\" represents the attention map for layer 1, stage 2, and similarly for the other layers.\nThe first two rows of attention maps correspond to the attention distributions from the three En-\ncoder layers. Following that, the prediction curves for nine selected variates are plotted, providing a\ndetailed view of the model's forecast performance across different variates."}]}