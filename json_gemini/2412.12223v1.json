{"title": "Can video generation replace cinematographers? Research on the cinematic language of generated video", "authors": ["Xiaozhe Li", "Kai Wu", "Siyi Yang", "Yizhan Qu", "Guohua Zhang", "Zhiyu Chen", "Jiayao Li", "Jiangchuan Mu", "Xiaobin Hu", "Wen Fang", "Mingliang Xiong", "Hao Deng", "Qingwen Liu", "Gang Li", "Bin He"], "abstract": "Recent advancements in text-to-video (T2V) generation have leveraged diffusion models to enhance the visual coherence of videos generated from textual descriptions. However, most research has primarily focused on object motion, with limited attention given to cinematic language in videos, which is crucial for cinematographers to convey emotion and narrative pacing. To address this limitation, we propose a threefold approach to enhance the ability of T2V models to generate controllable cinematic language. Specifically, we introduce a cinematic language dataset that encompasses shot framing, angle, and camera movement, enabling models to learn diverse cinematic styles. Building on this, to facilitate robust cinematic alignment evaluation, we present CameraCLIP, a model fine-tuned on the proposed dataset that excels in understanding complex cinematic language in generated videos and can further provide valuable guidance in the multi-shot composition process. Finally, we propose CLIPLoRA, a cost-guided dynamic LoRA composition method that facilitates smooth transitions and realistic blending of cinematic language by dynamically fusing multiple pre-trained cinematic LoRAs within a single video. Our experiments demonstrate that CameraCLIP outperforms existing models in assessing the alignment between cinematic language and video, achieving an R@1 score of 0.81. Additionally, CLIPLORA improves the ability for multi-shot composition, potentially bridging the gap between automatically generated videos and those shot by professional cinematographers.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in text-to-video (T2V) generation have leveraged diffusion models to enhance the visual coherence of videos generated from textual descriptions [2, 3, 10, 11, 21, 23, 31]. However, most T2V research has primarily focused on object motion, emphasizing content alignment and visual coherence. Moreover, due to the high labor costs associated with data collection from real-world scenarios, text-controlled cinematic language video generation remains largely unexplored.\nCurrent T2V models cannot replace skilled cinematographers due to their limited ability to generate complex cinematic language in videos. Previous works, such as Animate-Diff [9], introduced motion priors to enhance animation fluidity and used MotionLoRA to efficiently fine-tune new motion patterns. However, these approaches restricted camera motion to basic operations, such as zooming and rotating. MotionCtrl [33] added modules for independently controlling camera and object motion, while Direct-a-Video [36] aimed to simulate real-world video shooting by decoupling object and camera motion, allowing users to control camera translation, zoom, and directional movement. Despite these advancements, both MotionCtrl and Direct-a-Video are limited in generating a full range of cinematic styles and require additional camera parameter inputs, making flexible camera control challenging.\nWhile these advancements have laid the groundwork for camera control in T2V models, significant limitations remain in generating various cinematic language styles. The challenges are threefold: 1) the lack of a high-quality cinematic language dataset, 2) the absence of an evaluation model to assess cinematic language alignment between videos and their descriptions, which also results in a lack of guidance for optimizing generation methods, and 3) the absence of a method for multi-cinematic language compo-"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Image/Video CLIP Models", "content": "CLIP [19] is a multi-modal model based on contrastive learning, widely used to assess image-text similarity. Trained on large-scale text-image pairs, CLIP demonstrates strong zero-shot generalization, making it applicable across tasks like detection [8, 15], segmentation [14, 35], image and video understanding [17, 34], retrieval [29], and image generation [5, 6, 20, 27]. For video analysis, Vi-CLIP [32] incorporates spatio-temporal attention and partial random patch masking. Long-CLIP [37] addresses embedding length limitations through positional embedding interpolation, while VideoCLIP-XL [30] enhances alignment for long-form video descriptions using text-similarity-guided primary component matching.\nWhile these models advance multi-modal learning and video-text alignment, they primarily focus on overall video semantics or handling long-text inputs. None are specifically designed to understand cinematic language, such as shot framing, shot angle, and camera movement, in video content. To bridge this gap, we introduce CameraCLIP, aimed at enhancing the model's understanding and alignment with detailed cinematic language."}, {"title": "2.2. Text-to-Video Generation for Camera Control", "content": "Early research in video generation primarily utilized Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) [24\u201326, 28]. Following the success of diffusion models in high-fidelity image generation [10, 21,"}, {"title": "2.3. Multi-LoRA Composition", "content": "Recently, Low-Rank Adaptation (LoRA) [12] has demonstrated significant potential in text-to-image/video generation, allowing users to bring their imagination to life in high-quality images or videos at an affordable computational cost. Building on this foundation, researchers have begin explored various style composition methods. LORA Merge [22] was introduced to create diverse styles by linearly combining multiple LoRAs into a unified LoRA for diffusion models. Zhong [38] further expanded LoRA's capabilities with LoRA Switch and LoRA Composite, which focus on the diffusion denoising stage. LoRA Switch alternates between different LoRAs at specific denoising steps, while LORA Composite combines multiple LoRAs by averaging their guidance scores, thereby enhancing composition quality and robustness in multi-element scenarios.\nDespite these advancements, current methods remain limited by their static nature, as they lack the ability to dynamically select LoRAs based on intermediate denoising results, especially in complex cinematic language fusion. To address these limitations, we propose a reward-guided LORA composition approach (CLIPLoRA), which employs an evaluator to assess LoRA fidelity and applies a greedy search algorithm to optimize LoRA selection throughout the diffusion process, enhancing the realism of camera motion in video generation."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Cinematic Language Dataset Construction", "content": "We need to construct a cinematic language dataset, as existing datasets are not systematically organized or specifically classified for cinematic language. We used video sources from Pexels\u00b9 and Videvo\u00b2, two websites offering free high-quality video data. As shown in Figure 2, the dataset construction process was divided into three steps.\nStep I: Cinematic Language Classification and Video Data Collection. We categorized cinematic language into three primary types: shot framing, shot angle, and camera movement. To efficiently build a high-quality cinematic language dataset on a large scale, we employed automated tools to extract content descriptions and cinematic language annotations for each video. In total, we gathered approximately 1800 video samples. The detailed data distribution of the cinematic language dataset is shown in Figure 3."}, {"title": "Step II: Manual Proofreading and Supplementation of the Cinematic Language Video Data.", "content": "Since each video's cinematic language includes shot framing, shot angle, and camera movement, automatically generated descriptions from online sources often contain inaccuracies or lack certain shot types. To address these issues, after completing the data collection in Step I, we manually proofread and supplement the data by adding missing shot types, ensuring the accuracy and completeness of the cinematic language annotations. Detailed information on the cinematic language dataset, after verification, is provided in the Supplementary Material."}, {"title": "Step III: Fine-grained camera movement interval annotation.", "content": "Since the collected data comes from real-world scenarios, it may contain noise, such as inconsistent camera movements or transitions for certain shot types like 'rack focus', which typically occur over a few seconds. To enhance the data quality, we manually annotated each video after Step II, precisely marking the start and end intervals of these fine-grained camera movement transitions."}, {"title": "3.2. CameraCLIP", "content": "Our CameraCLIP extends the capabilities of the pre-trained CLIP model [19] to the domain of cinematic language video-text alignment. Figure 4 presents the architecture of"}, {"title": "3.3. Cost-Guided Dynamic LoRA Composition", "content": "Previous multi-LoRA composition methods, such as LoRA Merge, LoRA Switch, and LoRA Composite, rely on static, hand-crafted strategies, which struggle to capture dynamic interactions between multiple LoRAs, particularly in complex cinematic language composition tasks. To address this limitation, we introduce a cost-guided dynamic LoRA composition approach, as shown in Figure 6. This method enhances smooth transitions and realistic blending of cinematic language by dynamically selecting which LoRA to activate during the diffusion denoising process.\nOur goal is to select the optimal LoRA to activate at each diffusion step T during the denoising stage. To achieve the most effective LoRA activation, we employ a genetic algorithm [7], which iteratively refines candidate solutions. Each individual in the population represents a set of LoRA parameters sampled from the search space P, where P denotes the set of LoRAs to be composited.\nIn each generation, the cost score of an individual $X_T$ is evaluated based on its proximity to the target cinematic quality score:\n$Cost(X_T) = |CameraCLIP(X_T) \u2013 target|$\nHere, CameraCLIP is used to evaluate the cinematic quality of the current solution, where target refers to the desired alignment score, representing the ideal Camera-CLIP score value. By computing the cost score, we guide"}, {"title": "4. Experiment", "content": "In this section, we evaluate the performance of Camera-CLIP and CLIPLORA. We begin by introducing the models, dataset, and LoRA composition baselines used in our experiments. First, we assess CameraCLIP's video-text retrieval accuracy on the validation dataset derived from the cinematic language dataset (see Section 4.2). Next, we present the results of multi-camera motion composition using CLIPLora (see Section 4.3). Finally, we conduct an ablation study to analyze our approach to temporal modeling of video features (see Section 4.4)."}, {"title": "4.1. Setup", "content": "Video CLIP Models. We evaluate four representative models in video-text alignment: CLIP4CLIP [17], ViCLIP [32], LongCLIP [37], and VideoCLIP-XL [30]. CLIP4CLIP introduces three similarity matching strategies (meanP, seqLSTM, and seqTransf) to optimize the alignment between video frames and textual descriptions. ViCLIP incorporates spatio-temporal attention within the video encoder and applies partial random patch masking during training to improve robustness. LongCLIP uses primary component matching of CLIP features, enhancing the model's ability to understand lengthy textual descriptions associated with video content. VideoCLIP-XL builds upon LongCLIP by implementing text-similarity-guided primary component matching, encouraging nuanced distribution learning within the feature space and expanding the model's capacity for handling long descriptions.\nDataset. The cinematic language dataset consists of 20 categories based on shot framing, shot angle, and camera movement, totaling 1831 instances of cinematic language data. A detailed description of these categories is provided in Figure 2, and the number of instances in each category after data supplementation is listed in the Supplementary Material. Each entry in the dataset includes a video, a de-"}, {"title": "4.2. CameraCLIP Performance Evaluation", "content": "In this section, we analyze the performance of our proposed CameraCLIP by comparing it with several representative models: CLIP [19], CLIP4CLIP [17], ViCLIP [32], LongCLIP [37], and VideoCLIP-XL [30]. We conduct experiments across different models and resolutions to evaluate the accuracy, with a primary focus on R@1 score, in tasks related to understanding cinematic language in videos.\nRecall at 1 (R@1) represents the proportion of queries for which the correct answer is found within the top 1 returned result. The formula for calculating R@1 is as follows:\n$R@1 = \\frac{1}{N} \\sum_{i=1}^{N} \\delta(rank_i \\leq 1)$\nwhere N denote the total number of queries, and $rank_i$ represent the rank position of the correct answer for the i-th query. The indicator function $\\delta(rank_i \\leq 1)$ equals 1 if the correct answer for the i-th query is ranked within the top 1 position, and 0 otherwise.\nConsequently, we use the R@1 metric to evaluate the model's ability to retrieve the correct text description for a given video input, assessing whether the model accurately aligns the video's cinematic language with the corresponding text descriptions."}, {"title": "4.3. Result on CLIPLORA", "content": "To evaluate the effectiveness of our proposed CLIPLORA method, we compare it against other LoRA composition approaches, including LoRA Merge, LoRA Composite, LoRA Switch, and the direct generation method (origin). CameraCLIP is used to measure the quality of T2V-generated videos by evaluating the consistency between the generated videos and cinematic language. We present the composition results of CLIPLORA in Figure 1 and provide quantitative comparisons with other baselines in Figure 7."}, {"title": "4.4. Ablation Study", "content": "To validate the effectiveness of the mean pooling approach for modeling video frames, we conducted an ablation study comparing various temporal modeling methods, including Transformer, LSTM, MLP, Multi-Head Attention, and Transformer+LSTM. As shown in Table 2, Transformer achieved the second-best R@1 score of 0.43, while mean pooling significantly outperformed it with an accuracy of 0.81. This suggests that, for our task, mean pooling provides a more robust frame representation, especially when the training dataset is small, as complex models tend to overfit and reduce generalization, consistent with prior work's conclusion [17]."}, {"title": "5. Conclusion", "content": "In this work, we focus on enhancing T2V models' ability to generate cinematic language videos. To address this, we propose a threefold solution. We first introduce a comprehensive cinematic language dataset encompassing shot framing, shot angle, and camera movement, enabling T2V models to learn diverse cinematic patterns. Building on this, we further propose CameraCLIP, a model fine-tuned on this dataset, designed to evaluate the alignment between generated video content and cinematic language descriptions, thereby providing valuable guidance for optimizing video generation. Finally, we propose CLIPLORA, a cost-guided dynamic LoRA composition method that facilitates smooth transitions and realistic blending of cinematic styles within a single video.\nOur experiments demonstrate that CameraCLIP achieves an R@1 score of 0.81, outperforming other models in measuring cinematic alignment. Additionally, CLIPLORA effectively combines multiple LoRAs, enabling T2V models to generate videos with complex cinematic compositions. These contributions provide a solid foundation for controllable cinematic language generation in T2V models, potentially bridging the gap between automated video generation and expert cinematographers."}, {"title": "A. Detailed Data Description", "content": "To provide a more comprehensive introduction to the cinematography dataset, we further elaborate on the meanings of the 20 types of cinematic language in Section A.1 and describe the data distribution in Section A.2."}, {"title": "A.1. Meanings of 20 Types of Cinematic Language", "content": "\u2022 Long Shot: Shows the entire subject within its surroundings.\n\u2022 Medium Shot: Captures the subject from the waist up, focusing on body language.\n\u2022 Close Up Shot: Highlights a specific part of the subject, often the face, for detail.\n\u2022 Full Shot: Frames the whole subject, providing context without much background.\n\u2022 Low Angle: Camera positioned below the subject, making it appear powerful or imposing.\n\u2022 High Angle: Camera positioned above the subject, making it seem smaller or vulnerable.\n\u2022 Bird Angle: Overhead view, offering a comprehensive look at the scene from above.\n\u2022 Eye Level: Camera is at the subject's eye height, creating a neutral perspective.\n\u2022 Dutch Angle: Tilted camera angle, used to create tension or unease.\n\u2022 Rack Focus: Shifts focus between subjects at different distances to guide viewer attention.\n\u2022 Panning Left: Horizontal movement of the camera to the left, capturing a scene's width.\n\u2022 Panning Right: Horizontal movement of the camera to the right, capturing a scene's width.\n\u2022 Tilt Up: Vertical movement of the camera upwards, usually to reveal height.\n\u2022 Tilt Down: Vertical movement of the camera downwards, often to show depth.\n\u2022 Dolly In: Camera moves closer to the subject, creating an immersive effect.\n\u2022 Dolly Out: Camera moves away from the subject, broadening the scene's context.\n\u2022 Tracking Shot: Camera follows the subject, maintaining focus while moving.\n\u2022 Zoom In: Camera lens zooms in, bringing the subject closer without physical movement.\n\u2022 Zoom Out: Camera lens zooms out, showing more of the surroundings.\n\u2022 Still: The camera remains fixed, with no movement, offering a steady view."}, {"title": "A.2. Data Distribution", "content": "Table A1 presents the distribution of types across the categories of shot framing, shot angle, and camera movement in our cinematic language dataset, providing a detailed breakdown of the number of different cinematic elements within each category."}, {"title": "B. Experiment Detailed Settings", "content": "We provide the detailed description of the experimental settings in Section B.1 for CameraCLIP and in Section B.2 for CLIPLORA."}, {"title": "B.1. CameraCLIP", "content": "We trained the proposed CameraCLIP on a single NVIDIA A6000 Ada GPU (48GB). The specific training hyperparameters are detailed as follows:\n\u2022 Batch size: We set the batch size to 16.\n\u2022 Number of epochs: The model was trained for 2 epochs."}, {"title": "B.2. CLIPLORA", "content": "This section outlines the experimental setup for training specific cinematic language patterns using LoRA and multi LORA composition, as detailed in B.2.1. Additionally, we describe the 26 text prompts used in this experiment for T2V generation in B.2.2."}, {"title": "B.2.1 Training Details", "content": "The training configuration for training specific cinematic language patterns using LoRA [12] is based on the setting of AnimateDiff [9]. We set both the LoRA rank and alpha to 32, as this configuration provides performance comparable to increasing the rank and alpha to 128 while requiring less training time. Each LoRA requires between 30 minutes and 1 hour of training on a single A100 with a learning rate of le-5 and a batch size of 4. The sample stride and number of frames are set to 4 and 16, respectively. Training is conducted for a maximum of 10 epochs per LoRA, and the best epoch is selected using our proposed CameraCLIP method to prevent overfitting.\nFor the LORA composition search in CLIPLORA, we set the population size to 100 with 15 iterations, a mutation probability of 0.2, and a crossover probability of 0.5. We select the top 10% as elite children. The search is a discrete optimization process, determining which LoRA is used at each of the 50 steps. The entire search takes approximately two days; however, a satisfactory result is often achieved after 12 hours."}, {"title": "B.2.2 Prompts Used in Text-to-Video Generation", "content": "\u2022 Cricket still life with close up of ball and bat lying in the grass in front of stumps.\n\u2022 A man is skiing.\n\u2022 A rabbit is eating a watermelon on the table.\n\u2022 A car is moving on the road.\n\u2022 Bear playing guitar happily, snowing.\n\u2022 Boy walking on the street.\n\u2022 Ball And Bat Lying In Grass In Front Of Stumps.\n\u2022 A black swan swims on the pond.\n\u2022 A girl is riding a horse fast on grassland.\n\u2022 A boy sits on a chair facing the sea.\n\u2022 Two galleons moving in the wind at sunset.\n\u2022 Cinematic photo melting pistachio ice cream dripping down the cone. 35mm photograph, film, bokeh.\n\u2022 Large motion, surrounded by butterflies, a girl walks through a lush garden.\n\u2022 An astronaut is waving his hands on the moon.\n\u2022 A man cruises through the city on a motorcycle, feeling the adrenaline rush.\n\u2022 A monkey eating a pizza in central park, GoPro film style\n\u2022 A cyberpunk city street.\n\u2022 A bird sits on a branch.\n\u2022 A rabbit, forest, haze, halation, bloom, dramatic atmosphere, centered, rule of thirds, 200mm 1.4f macro shot.\n\u2022 B&W photo of a young man in black clothes, bald, face, body, high detailed skin, skin pores, coastline, overcast weather, wind, waves, 8k UHD, DSLR, soft lighting, high quality, film grain, Fujifilm XT3.\n\u2022 Photo of coastline, rocks, storm weather, wind, waves, lightning, 8k UHD, DSLR, soft lighting, high quality, film grain, Fujifilm XT3.\n\u2022 Night, B&W photo of old house, post-apocalypse, forest, storm weather, wind, rocks, 8k UHD, DSLR, soft lighting, high quality, film grain.\n\u2022 Pacific coast, Carmel by the sea ocean and waves.\n\u2022 Robot dancing in Times Square.\n\u2022 The Golden Temple in India.\n\u2022 Flowers in Garden."}, {"title": "C. Multi LoRAs Composition Results", "content": "In this section, we present the results of our proposed CLIPLORA method and other baseline methods, as shown in Figure C1. These results demonstrate the effectiveness of CLIPLORA in comparison to the baselines under various cinematic conditions.\nThe experiment evaluates the performance of various LORA composition methods in generating cinematic videos under complex camera movements, such as panning, tilting, and zooming. In the first row, which depicts a long shot at a high level with a panning right movement, CLIPLORA demonstrates the most consistent and realistic camera control. The second row illustrates a long shot at a high level with panning left and tilting up. In this scenario, the origin and LoRA Merge methods fail to generate combined movements, LORA Switch and LoRA Composite handle only the panning left movement, whereas CLIPLORA successfully integrates both panning left and tilting up movements. The third row introduces a scenario involving camera zoom-a long shot at a high level with zooming in and panning right. In this case, CLIPLORA excels in cohesively fusing multiple cinematic movements, while the baseline methods are limited to generating a single movement and fail to achieve multi-shot integration.\nIn summary, as the number of LoRA composition increases, CLIPLORA consistently outperforms all baselines by maintaining stable cinematic control and enabling precise multi-shot integration. In contrast, methods like LoRA Merge, LoRA Switch, and LoRA Composite exhibit instability in generating complex camera movements, often failing to synthesize multiple movements. These results highlight the superiority of CLIPLORA, demonstrating its potential to bridge the gap between the capabilities of T2V models and the expertise of professional cinematographers."}]}