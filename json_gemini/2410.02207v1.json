{"title": "Adapting Segment Anything Model to Melanoma Segmentation in Microscopy Slide Images", "authors": ["Qingyuan Liu", "Avideh Zakhor"], "abstract": "Melanoma segmentation in Whole Slide Images (WSIs) is\nuseful for prognosis and the measurement of crucial prognostic factors\nsuch as Breslow depth and primary invasive tumor size. In this paper,\nwe present a novel approach that uses the Segment Anything Model\n(SAM) for automatic melanoma segmentation in microscopy slide im-\nages. Our method employs an initial semantic segmentation model to\ngenerate preliminary segmentation masks that are then used to prompt\nSAM. We design a dynamic prompting strategy that uses a combination\nof centroid and grid prompts to achieve optimal coverage of the super\nhigh-resolution slide images while maintaining the quality of generated\nprompts. To optimize for invasive melanoma segmentation, we further\nrefine the prompt generation process by implementing in-situ melanoma\ndetection and low-confidence region filtering. We select Segformer as\nthe initial segmentation model and EfficientSAM as the segment any-\nthing model for parameter-efficient fine-tuning. Our experimental results\ndemonstrate that this approach not only surpasses other state-of-the-art\nmelanoma segmentation methods but also significantly outperforms the\nbaseline Segformer by 9.1% in terms of IoU.", "sections": [{"title": "1 Introduction", "content": "Melanoma, one of the most serious forms of skin cancer, originates in melanocytes,\nthe pigment-producing cells responsible for melanin production [22]. Based on its\nprogression and location within the skin, melanoma can be categorized into two\nmain types: in-situ melanoma and invasive melanoma. While in-situ melanoma\nrepresents cancerous melanocytes that are confined to the epidermis, invasive\nmelanoma penetrates beyond the epidermis into the dermis, posing a significant\nrisk of spreading to other vital organs. As invasive melanoma grows, it may in-\nvade blood vessels and lymphatic vessels, allowing cancer cells to detach from\nthe primary tumor and cause metastatic cancer [1]. In this paper, we primarily\nfocus on segmenting invasive melanoma to assist dermatologists in deciding on\ntreatment options.\nEarly detection and accurate diagnosis of melanoma are crucial for improv-\ning the survival rate [3,6]. The standard diagnosis practice begins with an initial"}, {"title": "2 Related Work", "content": "SAM in Medical Imaging. Given SAM's impressive results on various nat-\nural image segmentation tasks [4, 13, 20], recent works have explored its appli-\ncation to medical image segmentation. However, several studies [8, 12, 19, 32]\nhave shown that SAM underperforms in medical image segmentation. This is\nattributed to the model's lack of domain-specific medical knowledge, the uncer-\ntain and complex object boundaries, intricate structures, and the wide-range of\nscales unique to medical objects [12]. Recent efforts have focused on adapting\nSAM for medical images, primarily through fine-tuning or adapting SAM to la-\nbeled medical dataset. Ma et al. [15] proposed to fine-tune SAM fully on labeled\nmedical data, which is not cost-effective due to the vast number of parameters.\nWu et al. [27] proposed to integrate adapter modules into SAM, allowing effi-\ncient fine-tuning by freezing all modules except for the adapters during training.\nThese studies all focus on utilizing SAM as an interactive model and evaluate\nits performance by providing accurate prompts based on the ground-truth an-\nnotations. For microscopy slide image segmentation, SAM's interactive feature\ncan assist annotators but cannot fully automate the labor-intensive process of\nsegmenting high-resolution whole slide images. Although SAM offers automatic\nmask generation, this feature is designed for segmenting everything on an entire\nimage rather than targeting specific small objects, such as scattered melanoma\ncells in microscopy slide images. Zhang et al. [31] proposed to enhance medical\nimages by adding semantic structures using SAM's automatic mask generation.\nThis approach combines generated masks, features and stability scores to help\ntrain other image segmentation models with enhanced data. The success of this\nmethod depends on SAM's ability to generate useful structural information dur-\ning the automatic mask generation process.\nMelanoma Segmentation in Slide Images. Melanoma semantic segmen-\ntation can be classified into two categories: 1) skin lesion segmentation based\non images captured at a macroscopic scale; 2) microscopy slide image segmen-\ntation that involves WSIs captured at a microscopic scale. While skin lesion\nsegmentation only segments a large rounded melanoma component at a macro-\nscopic level, microscopy slide image segmentation requires segmenting irregular\nmelanoma scattered across high-resolution, detailed views of tissue samples at\nthe cellular level, making the task significantly more challenging. Our work fo-\ncuses exclusively on the latter.\nMost studies [2, 16-18, 21, 24] focus on utilizing CNN-based methods for\nmelanoma segmentation in microscopy slide images. Phillips et al. [18] devised\na multi-stride fully convolutional network (FCN) that can effectively segment\ntumour, epidermis and dermis. Shah et al. [21] developed a two-stage method to\nsegment invasive melanoma by leveraging the difference between in-situ melanoma\nand invasive melanoma. They used HRNet-OCR [30] and HookNet [23] as back-\nbones and trained two models, one for epidermis segmentation and one for tu-\nmor segmentation, to predict two separate segmentation masks for epidermis\nand melanoma. By removing all predicted melanoma from the epidermis mask,\nthey obtained segmentation masks for invasive melanoma."}, {"title": "3 Method", "content": "3.1 Overview of the Proposed Method\nOur proposed method comprises an initial semantic segmentation model and a\nsegment anything model as illustrated in Fig. 1. To segment invasive melanoma\nautomatically, we prompt SAM with prompts generated from the mask pro-\nduced by the initial segmentation model. To optimize for our task of segment-\ning invasive melanoma, we select Segformer as the initial segmentation model\ndue to its superior performance compared to other models [26]. We choose Effi-\ncientSAM [29] as the segment anything model for parameter-efficient fine-tuning.\nThe entire framework is described as follows.\nStep 1. Initial Mask Generation. We run Segformer [28] to generate the\ninitial segmentation mask X. Then we perform in-situ melanoma detection to\nseparate X into the estimated in-situ melanoma regions $X_s$ and the remaining\ninvasive melanoma regions $X_u$. We filter out low-confidence regions from $X_s$ to\nobtain the filtered mask $X_{ic}$ and combine it with $X_u$ to obtain the post-processed\nmask $\\hat{X}^m$. The details are described in Secs. 3.2 and 3.3.\nStep 2. Prompt Generation. We generate single point prompts from the\npost-processed mask $\\hat{X}^m$. We determine the best prompt type for each connected"}, {"title": "3.2 In-situ Melanoma Detection", "content": "We exclude low-confidence invasive melanoma predictions that touch the epi-\ndermis, since these are considered to be in-situ melanoma from a medical per-\nspective. This process is shown in Fig. 2a. The initial mask X produced by\nSegformer contains both the epidermis mask $X_e$, and the invasive melanoma\nmask $X^m$. The invasive melanoma mask can be represented as a union of inva-\nsive melanoma connected components (CCs) $X^m = \\bigcup X_i^m$, where i denotes the\nith invasive melanoma connected component. Similarly, the mask for epidermis\ncan be represented as $X_e = \\bigcup X_j^e$, where j denotes the jth epidermis connected\ncomponent. Since SAM heavily relies on prompts to specify the exact objects to\nsegment, inaccurate and ambiguous prompts can significantly degrade its per-\nformance. This can lead SAM to produce more false positives compared to the\ninitial segmentation mask from which prompts are generated. Therefore, we fil-\nter out low-confidence invasive melanoma components to improve the accuracy\nof prompts so that it is clicked on true positives in most cases.\nTo begin with, we iterate over all connected components $X_i^m$ in the invasive\nmelanoma mask $X^m$. For each connected component, we determine whether it\ntouches any epidermis component $X_j^e$. If such a touch exists, we compute the\nratio between the area of the melanoma component $X_i^m$ and the area of the"}, {"title": "3.3 Low-Confidence Region Filtering", "content": "We further filter out low-confidence estimated in-situ melanoma regions from\n$X_s$. We keep high-confidence regions even if they touch the epidermis, since\nthis could result from false predictions in the epidermis that make them touch\neach other. We determine the confidence level based on the probability map\ngenerated by Segformer. As shown in Fig. 2b, for each component $X_i^s$, we first\nfind its high-confidence sub-component $X_{i,\\beta}^{s}$ that has a probability exceeding the\ndefined threshold $\\beta$:\n$X_{i,\\beta}^{s} = \\{x | P(x) > \\beta,x \\in X_i^s\\}$.\nWe then determine the confidence level by computing the ratio between the\narea of $X_{i,\\beta}^{s}$ and $X_i^s$. If the ratio fails to meet the confidence threshold $\\alpha_c$, we\ndiscard this component and do not generate prompts from it. Careful inspec-\ntions of Segformer's mask show that it generally predicts a larger proportion\nof low-probability areas for in-situ melanoma compared to invasive melanoma.\nTherefore, if we keep only regions with a high proportion of high probability\nareas, we can significantly reduce the number of false positives in $X^m$. Next,\nwe combine the filtered melanoma regions $X_{ic}$ and the rest invasive melanoma\nregions $X_u$ to obtain the post-processed invasive melanoma mask $\\hat{X}^m$."}, {"title": "3.4 Prompt Generation", "content": "Melanoma can be in various forms in microscopy slide images, ranging from\nrounded clusters to jagged streaks or any combination of irregular shapes. To"}, {"title": "3.5 Final Mask Generation", "content": "After deriving the prompts, we run SAM to generate its own mask $\\hat{Y}^m$ as shown\nin Fig. 1. To obtain the final mask, we combine the post-processed Segformer's\nmask $\\hat{X}^m$ and SAM's mask $\\hat{Y}^m$ by performing a union between the two:\n$Y = \\hat{Y}^m \\bigcup \\hat{X}^m$\nWe choose to combine these two masks because single point prompts are sparse\nand may not comprehensively cover all regions. This sparsity can lead to am-\nbiguity and result in missing some high-confidence predictions from Segformer.\nTherefore, we choose to include the post-processed mask $\\hat{X}^m$ to ensure that\nhigh-confidence regions are kept in the final mask Y."}, {"title": "3.6 Training Strategy", "content": "We devise a training strategy involving two stages that optimizes SAM for the\nway we use prompts. Since we only use single point prompts, we first train SAM\non single point prompts for melanoma. For each patch sampled from a microscopy\nslide image, we first find all connected components and for each connected com-\nponent we generate a single point prompt clicked at a random position inside\nit. This enables the model to learn to segment melanoma anywhere in a patch.\nAfter the initial training on random point prompts, we fine-tune our model on\npatches centered at each individual single point prompt. Each patch might con-\ntain multiple components, but we only set the component containing the point\nprompt as the target to reduce ambiguity. This allows the model to optimize for\ninference using our SAM-based method."}, {"title": "4 Experiments", "content": "4.1 Dataset\nOur dataset comprises 101 microscopy slide images, with sizes ranging from\n23700 \u00d7 21199 pixels to 1996 \u00d7 1679 pixels. These images are derived from skin\nbiopsies stained with H&E [10] and captured under a microscope at 40x mag-\nnification. Detailed Annotations at this magnification level are provided by an\nexpert dermatopathologist. Each image in the dataset is carefully selected and\ncropped from the original WSIs by the dermatopathologist. This is because whole\nslide images often contain multiple focal planes of the same tissue, leading to\nredundancy in the data. By selecting the most informative regions at a single\nfocal plane, we reduce the redundancy and ensure that the dataset focuses on\nthe most relevant tissue structures. The annotations for our microscopy images\ninclude seven classes: air, background cells, epidermis, invasive melanoma, in-\nflamed tumor, fibrotic tumor, and uncertain tumor. It is important to note that\ninvasive melanoma is the only type of melanoma precisely annotated. In-situ\nmelanoma is labeled as epidermis in our dataset due to its confinement to the\nepidermis and due to our focus on segmenting invasive melanoma.\nSegformer Dataset Generation. To generate the dataset for Segformer,\nWe follow the approach described by Wang et al [26]. Given the extremely high\nresolution of microscopy slide images, we divide each slide into non-overlapping\npatches of 512\u00d7512 and 1024 \u00d7 1024 pixels. We re-categorize the original annota-\ntions into three distinct classes: invasive melanoma, epidermis, and others. This\nreclassification is essential since the ambiguous boundaries of fibrotic and in-\nflamed tumor make them less suitable for effective segmentation model training.\nWe under-sample the background by discarding patches with 97% of background\ncells and air to address class imbalance issues. The resulting dataset comprises\n14885 patches of around 3.9 billion pixels for 512 \u00d7 512 resolution and 4326\npatches of around 4.5 billion pixels for 1024 \u00d7 1024 resolution. This prepro-\ncessing approach ensures a balanced and representative dataset for training the\nSegformer model.\nSAM Dataset Generation. We generate the SAM dataset corresponding\nto the two-stage training process described in Sec. 3.6. For the first stage, we\ndivide each microscopy image into non-overlapping patches and generate a sin-\ngle point prompt within each connected component randomly. The ground truth\nfor each prompt is the mask corresponding solely to that particular connected\ncomponent, rather than all components in the patch. This aims to minimize\nthe ambiguity of single point prompts as much as possible and facilitates model\nconvergence. For the second stage, we use both the centroid and randomly sam-\npled points as the training prompts. We use patches centered at each prompt\nto optimize SAM specifically for our method. This stage's dataset comprises\n8357 patches of 512 \u00d7 512 pixel resolution and 6170 patches of 1024 \u00d7 1024 pixel\nresolution."}, {"title": "4.2 Implementation Details", "content": "Model Settings. We use Segformer B0 and B1 [28] as the initial segmentation\nmodel and EfficientSAM-S [29] as the segment anything model in our approach.\nWe choose Efficient SAM-S since it is the most efficient variant of SAM that\nreconstructs the image embeddings of ViT-H [9] in the original SAM. To further\nimprove training efficiency, we integrate adapters into the ViT [9] image encoder\nfollowing the approach described in Med-SA [27]. We use adapters with an input\nand output dimensionality of $d_a = 768$ and set the dimensionality of hidden\nlayers to $d_h = 1024$. With adapters, we only fine-tune 6.7M parameters for\nEfficientSAM-S with a total of 29.3M parameters.\nTraining. For Segformer, we use AdamW [14] optimizer with $\\beta_1$ = 0.9 and\n$\\beta_2$ = 0.999, and a weight decay of 0.01. We use an initial learning rate of 5e - 4\nand a polynomial decay scheduler. We use the weights of Segformer pretrained\non ImageNet [7] as our starting point and train the model for 150 epochs. For\nEfficientSAM, we use the Adam optimizer with an initial learning rate of 1e - 4,\nan exponential decay rate of $\\beta_1$ = 0.9 and $\\beta_2$ = 0.999, and a weight decay of\n0.05. We use the initial learning rate for the first 10 epochs and apply a learning\nrate decay factor of 0.5 every 10 epochs. We use a batch size of 8 for patch\nresolution 512 \u00d7 512, and a batch size of 6 for patch resolution 1024 \u00d7 1024. The\nmodel is first trained for 100 epochs on the first-stage dataset and then trained\nfor 150 epochs on the second-stage dataset. The model is trained with a binary\ncross entropy loss function with equal weights for the invasive melanoma and\nthe backgrounds. All experiments are implemented in PyTorch and executed on\n4 NVIDIA Quadro RTX 8000 GPUs.\nInference. To generate the segmentation mask with Segformer, we process each\nmicroscopy slide image into patches. Specifically, we use a sliding window to\ncreate patches by shifting the window both horizontally and vertically with a\nstep size of 128 pixels. In addition, we apply a 2D Gaussian kernel as a weight-\ning mechanism for each pixel within a patch. The kernel has the same size as\nthe patch and a standard deviation of of the patch's side length. To gener-\nate the segmentation mask for EfficientSAM, we run our method with a fixed\nset of hyperparameters. We set the threshold for determining in-situ melanoma\n$\\alpha_m$ = 0.1, the probability threshold for high-confidence regions $\\beta$ = 0.8, and\nthe threshold for excluding low-confidence regions $\\alpha_c$ = 0.4. Additionally, the\nthreshold for the ratio between the sides of a minimum bounding box $a_b$, as\ndescribed in Algorithm 1, is set to 3. The grid prompt employs a vertical and\nhorizontal gap of 64 pixels between neighboring points."}, {"title": "4.3 Results", "content": "To evaluate the effectiveness of our proposed method, we compare it with state-\nof-the-art melanoma segmentation methods on our dataset. As shown in Tab. 1,\nwe compare our method with melanoma segmentation methods including Multi-\nScale FCN [18], HRNet & OCR [21], HIPT [26], and Segformer [28]. The results\nfor Segformer BO and B1 are reproduced following the methods described in"}, {"title": "4.4 Ablation Studies", "content": "We now analyze our method through a series of ablation studies on each com-\nponent of the framework.\nIn-situ Melanoma Detection We investigate the impact of in-situ melanoma\ndetection described in Sec. 3.2 by filtering out all detected estimated in-situ\nmelanoma regions regardless of their confidence levels. Tab. 2 shows that this\nconsistently improves performance, with IoU gains ranging from 0.5% to 11.8%.\nThe improvement varies based on the quality of the initial segmentation mask"}, {"title": "Low-Confidence Region Filtering", "content": "We study the impact of low-confidence\nregion filtering presented in Sec. 3.3. As shown in Tab. 2, our method achieves a\n3.9% IoU gain for Segformer B0 and a 4.6% IoU gain for Segformer B1 when using\na 1024 \u00d7 1024 patch size. However, there is a slight drop of 0.1% in IoU for Seg-\nformer B0 with a 512 \u00d7 512 patch size. This suggests that some high-confidence\ninvasive melanoma regions touch the epidermis, which are not filtered by the\nalgorithm, are actually in-situ melanoma. Overall, this demonstrates that low-\nconfidence region filtering improves the robustness of our segmentation results."}, {"title": "Prompt Types", "content": "We study the effectiveness of different prompt types. We test\nprompts with centroid alone, grid alone and our proposed strategy that uses both\nas presented in Sec. 3.4. Tab. 3 shows that in most cases using both achieves\nthe highest IoU in the final mask. Compared to grid prompts alone, using both\nachieves a gain as high as 3.7% IoU. Compared to centroid prompts alone, using\nboth achieves gains up to 2.1% IoU with one case showing no improvement.\nIt is noticeable that centroid prompts alone outperforms grid prompts alone in\nthe final mask, but performs much worse in pre-merged mask. This shows that\ncentroid prompts allow more accurate segmentation and thus complement the\ninitial mask well, achieving high IoU in the final mask after merging. In contrast,\ngrid prompts alone achieve full coverage over the initial mask, but not all points\nserve as effective prompts for accurate segmentation. This demonstrates that\nour method effectively leverages the strengths of both prompts by using grid\nprompts for large, irregular melanoma components and centroid prompts for\nsmall, regular melanoma components, maximizing the advantages of each prompt\ntype."}, {"title": "Final Mask Generation", "content": "We study the impact of final mask generation. As\nshown in Tab. 4, the final mask consistently achieves the highest IoU in all cases.\nEfficientSAM's mask outperforms the post-prococessed mask in most cases, with\nthe most significant improvement being a 5.5% increase for Segformer BO using\n512x512 patches. Merging two masks into a final mask significantly enhances\naccuracy. The post-processed mask shows gains ranging from 2.2% to 5.5% IoU,\nwhile the EfficientSAM's mask gains improvements between 0.8% and 2.3%\nIoU. This demonstrates that EfficientSAM compliments well the post-processsed\nmask in generating the final mask. In addition, we evaluate the EfficientSAM's\nperformance with prompts generated from the ground truth. Its IoU performance\nis 10.8% higher than the best result of using EfficientSAM alone and 8.9% higher"}, {"title": "5 Conclusion", "content": "We proposed a novel approach to explore the potential of SAM for melanoma seg-\nmentation in microscopy slide images. Our method utilizes Segformer to generate\ninitial segmentation masks and subsequently prompts EfficientSAM using a dy-\nnamic selection of centroid and grid prompts for automatic invasive melanoma\nsegmentation. Our experimental results demonstrate that this approach sur-\npasses other state-of-the-art melanoma segmentation methods by a large margin\nof 9.1% in IoU."}]}