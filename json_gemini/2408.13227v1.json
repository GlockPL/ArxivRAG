{"title": "Enhancing Few-Shot Transfer Learning with Optimized Multi-Task Prompt Tuning through Modular Prompt Composition", "authors": ["Ahmad Pouramini", "Hesham Faili"], "abstract": "In recent years, multi-task prompt tuning has garnered considerable attention for its inherent modularity and potential to enhance parameter-efficient transfer learning across diverse tasks. This paper aims to analyze and improve the perfor- mance of multiple tasks by facilitating the transfer of knowledge between their corresponding prompts in a multi-task setting. Our proposed approach decom- poses the prompt for each target task into a combination of shared prompts (source prompts) and a task-specific prompt (private prompt). During train- ing, the source prompts undergo fine-tuning and are integrated with the private prompt to drive the target prompt for each task. We present and compare mul- tiple methods for combining source prompts to construct the target prompt, analyzing the roles of both source and private prompts within each method. We investigate their contributions to task performance and offer flexible, adjustable configurations based on these insights to optimize performance. Our empirical findings clearly showcase improvements in accuracy and robustness compared to the conventional practice of prompt tuning and related works. Notably, our results substantially outperform other methods in the field in few-shot settings, demon- strating superior performance in various tasks across GLUE benchmark, among other tasks. This achievement is attained with a significantly reduced amount of training data, making our method a promising one for few-shot settings.", "sections": [{"title": "1 Introduction", "content": "Presently, transfer learning has become the dominant paradigm of natural language processing [15]. The prevalent technique involves pretraining a model on an extensive corpus of raw data and subsequently fine-tuning it for specific tasks using a limited dataset. Despite its success, this approach suffers from a series of limitations, such as negative interference [14] and catastrophic forgetting [6].\nAs a remedy to address these challenges, the concept of modular deep learning has gained traction [12]. Within this framework, computation units are designed as self- contained, parameter-efficient modules. These modules are selectively engaged and then combined to generate the ultimate output. For instance, incorporating adapter modules, task-specific layers [5], or task-specific embeddings allow for the model to focus more on certain tasks or to better separate the representations for different tasks [2]. The main advantages of a modular neural architecture are positive transfer, compositionality, and parameter efficiency [12].\nPrompt-Tuning is a parameter efficient method for fine-tuning a pretrained lan- guage model on downstream tasks [7, 9, 10]. It can also serve as a method for modular design to specialize different prompts for different tasks [12]. From this perspective, task-specific prompt tokens, after encoded and optimized by the model's embedding layer, corresponds to modular parameters that elicit the desired behaviour. The knowl- edge distilled within these soft prompts can subsequently be transferred to related tasks or combined to provide the necessary knowledge for tackling new tasks [17]. Yet, the effective utilization of this extensive cross-task knowledge within a multitask learning framework has remained elusive.\nIn this paper, we explore task transferability within the context of multi-task prompt tuning. We introduce a specialized framework called ComPT that combines multiple shared source prompts and task-specific prompts to drive the target prompt for each task within a multi-task learning setting. Our objective is to enhance overall performance by facilitating knowledge exchange between these tasks. We propose sev- eral methods for combining these prompts and evaluate their effectiveness in solving diverse tasks.\nIn the following sections, we outline the specifics of our approach and describe our proposed methods. Subsequently, we present our experimental results and conduct an in-depth analysis of the results and contributing factors affecting performance. Finally, we review related works, compare our method with existing approaches, and conclude the paper."}, {"title": "2 Method", "content": "Figure 1 illustrates the ComPT approach. We train N tasks in multi-task settings. These tasks use M source prompts, denoted as Ps, to formulate a target prompt Pt for each distinct target task t. In addition to source prompts, a private prompt, denoted as Pu is dedicated for each task. The final target prompt is obtained by combining the source prompts and then composing them with the private prompt. This is achieved by activating and combining the shared source prompts through a trainable attention module, which assigns the weights wts from the source prompt s to the target task t. Training is conducted using multi-task learning across all tasks. Learning a shared prompt space in multi-task settings can pose practical challenges, as it necessitates learning commonalities across various source tasks while simultaneously minimizing interference. The source and private prompts are initially randomized. However, in practice, prompts can also be initialized using prompts obtained from a pre-training stage. In this study, we opt to simultaneously learn the prompts and the attention weights, which resulted in high performance, even when compared to approaches involving prompt initialization. In the following sections, we describe the details of our approach."}, {"title": "2.1 Prompt Training", "content": "Before delving into further details of our method, let us first review the process of prompt tuning. Prompt tuning typically entails optimizing the input prompt of a language model LM to enhance its performance on a specific task.\nWe begin with a template $T = {x, p_1,..., p_m}$, comprising the input sequence $x = [x_1,...,x_l]$ and prompt tokens $p_r$. The placement of prompt tokens can occur either before or after the input tokens. In our experiments, we positioned the prompt tokens after the input tokens, yielding slightly better results. Prompt tuning then entails mapping this template to a set of trainable prompt embeddings as follows:\n$\\ {\\text{emb}(x), h_0,...h_m}\\ $\n(1)\nWe denote a soft prompt as $P = [h_1,...,h_m] \\in \\mathbb{R}^{m \\times d}$, where m is the prompt length, and d is the dimensionality of the underlying language model LM. This soft prompt can be considered as a continuous representation of the sequence of prompt tokens that is used to condition the language model."}, {"title": "2.2 Target Prompt Decomposition", "content": "In contrast to vanilla prompt tuning, the prompts in our approach result from com- bining multiple source prompts. In other words, we decompose the target prompt into several source prompts that are shared among tasks. The backpropagation mechanism updates these source prompts and parameters for their combination to obtain an opti- mal target prompt. In the context of a specific target task, the target prompt emerges as a fusion of the shared source prompts along with a trainable task-specific prompt, which we denote as the private prompt.\nPrecisely, let S represent the set of shared source prompts, and let Pu denote the private prompt corresponding to the target task t. In this context, the computation of the target prompt Pt is as follows:\n$\\Pt = P_u \\oplus \\Big(\\bigoplus_{s \\in S} w_{ts}.Ps \\Big)$\n(4)\nThe weights wts are determined by an attention mechanism implemented through an attention module R, which dynamically assigns weights to the shared prompts. This mechanism controls the contribution of each source prompt to a specific task based on its relevance to the target task, while a private prompt captures task-specific nuances. If none of the shared source prompts are useful for a target task, the target prompt primarily relies on the private prompt for its formation.\nThe composition of the source prompts can be achieved using various functions, with $\\oplus$ denoting the composition function. In this paper, we explored summation and concatenation as potential composition functions for the combing the source prompts. The resulting combination is then composed with the private prompt using the $\\circ$ function. For composing the source prompts into the private prompt, we considered both summation and multiplication. This framework is generalizable, allowing for the exploration of various methods. Here, we cover three specific cases as follows, each with their corresponding names listed after the formula:\n$\\Pt = \\begin{cases}\nP_u + (\\sum_{s \\in S} w_{ts} \\cdot Ps), & \\text{if } \\circ = \\text{mul}, \\oplus = \\text{sum} : \\text{SSUM} \\\\\nP_u * (\\sum_{s \\in S} w_{ts} \\cdot Ps), & \\text{if } \\circ = \\text{mul}, \\oplus = \\text{sum}: \\text{MSUM} \\\\\nP_u * (\\text{concat}(\\sum_{s \\in S} w_{ts}.Ps), & \\text{if } \\circ = \\text{mul}, \\oplus = \\text{concat}: \\text{MCAT}\n\\end{cases}$\n(5)\nIn the case of concatenation (MCAT), the length of the target prompt is equal to the sum of the lengths of the source prompts. For the other cases, the source prompts and the target prompt have equal lengths. The multiplication of source prompts to the target prompt of each task can model the projection of the source prompts onto the prompt for each task. Meanwhile, the summation of the target prompt to the combination of source prompts can serve to complement and augment the source prompts. We will delve deeper into the functions and implications of these operations in the Discussion section 5.\nTo ensure that the weights associated with each shared source accurately reflect their relevance to a target task during the training process, we normalize these weights using the softmax function. After calculating the target prompt Pt, the contributing elements are updated by optimizing to the following objective function.\n$\\underset{P_t, R}{max} p_{\\theta}(y | [P_t; x])$\n(6)\nIn multi-task learning, the loss for each private prompt Pu is computed only when the input sample corresponding to the task is fed to the model. In contrast, the shared source prompt Ps is updated at each iteration for all input samples. As a result, the attention weights for the shared prompt are adjusted based on the updated content of these source prompts. This dynamic adjustment can facilitate both positive and negative knowledge transfer among tasks, which will be explored in more detail in the Discussion section 5."}, {"title": "2.3 Source-Target Attentions", "content": "The attention module R governs the influence of each source prompt on the target prompt by computing attention weights $w_{1t}, ..., w_{mt}$ for the shared source prompts. These weights can be learned as free parameters or through a probability distribution. Specifically, we employed the Relaxed Bernoulli distribution to formulate these atten- tion weights, as it demonstrated efficacy and learning stability in our experimental evaluations compared to using free parameters.\nThe Relaxed Bernoulli distribution represents a continuous relaxation of the tradi- tional Bernoulli distribution, facilitating stochastic gradient computation through the reparameterization trick [11].\nThe Relaxed Bernoulli distribution is defined as follows:\n$\\hat{w}_{i,j} = \\sigma(\\log \\frac{u}{\\frac{\\sigma(w_{i,j})}{\\tau}} - \\log \\frac{(1 - u)}{(\\frac{1 - \\sigma(w_{i,j})}{\\tau})})^{1/\\tau}\\text{ where } u \\sim \\text{Uniform}(0, 1)$\n(7)\nHere, a temperature parameter $\\tau\\in (0,\\infty)$ controls the degree of approximation, $w_{ij}$ are the learnable logits, and $\\hat{w}_{ij}$ are the resulting sampled attention weights from the distribution used during training. At inference time, the learned weights $w_{ij}$ are utilized.\nFast and Slow Learning. Learning the stochastic attention module may intro- duce various challenges, including issues like training instability and overfitting [1, 12]. Moreover, balancing the learning of source and private prompts to achieve optimal performance poses another challenge. To address these challenges, we prioritize rapid learning for the attention module by employing a higher learning rate, enabling it to quickly adapt by reusing existing prompts for the current task. Additionally, we prioritize the learning of source prompts over private prompts to facilitate greater information sharing through the source prompts. We will further analyze the impact of these parameters in the Discussion section 5."}, {"title": "2.3.1 Inference", "content": "During inference, we load all shared source prompts and private prompts, as well as the shared attention module only once. The target prompt for a given task is derived by applying Equation 5. Unlike during training, at inference time, the learned attention module, which includes both positive and negative weights (logits of the Relaxed Bernoulli distribution), is directly utilized without the need for additional sampling. By applying softmax over these learned scores, probability distributions are obtained.\nAfter obtaining the target prompt, it is concatenated to the input instance xand fed to the model. The model generates the output y by utilizing its internal attention mechanism to consider both the learned target prompt and the input data.\nIn the following sections, we will present the results obtained by applying each method to various tasks and provide a comprehensive comparison among them."}, {"title": "3 Experiments", "content": "We assess the performance of proposed methods across various tasks, including eight tasks from the GLUE benchmark [18] and six tasks from other datasets. These datasets are publicly available on Hugging Face's model hub.\nOur evaluation involves two distinct experiments. In Experiment 1, the GLUE tasks encompass MNLI, QNLI, RTE (NLI), COLA (grammatical acceptability), SST2 (sentiment analysis), STSB (semantic similarity), MRPC, and QQP (paraphrase detection).\nFor Experiment 2, we include MultiNLI, SciTail (NLI), IMDB, Yelp-Polarity (sen- timent analysis), PAWS, and MRPC (paraphrase detection). We collectively refer to this group of tasks as SET2 tasks."}, {"title": "3.2 Implementation Details", "content": "We utilize the T5-base language model [13] as our base model, which is commonly employed in related works. The prompt length is an effective parameter, and we inves- tigated shorter lengths (20 tokens), which proved more effective compared to longer prompts. Specifically, for MCAT, shorter lengths such as 10 tokens proved to be more stable and efficient.\nIn the quest for an optimal prompt encoder, we experimented with different archi- tectures and activation functions. Initially, we tested an LSTM-based encoder, similar to [10], but found superior performance with an MLP-based encoder. Further explo- ration led us to conclude that the GELU activation function yielded slightly better results compared to RelU and SiLU. Our chosen prompt encoder, a two-layer MLP, generates prompt embeddings of size d corresponding to each token. This MLP com- prises a fully connected layer with a hidden size of d (the language model's embedding size), followed by a GELU activation function, and another fully connected layer responsible for producing the final prompt embedding.\nFor the RelaxedBernoulli distribution generator, we started with an initial temper- ature of 5 and linearny reduced it using annealing to le-3 during entire training. The attention module was trained with a learning rate of 0.1, while the prompt encoders for source prompts were trained with a learning rate of 0.05 and the prompt encoders for the private prompts were trained with a learning rate of 0.02."}, {"title": "4 Results", "content": "In the following sections, we will present the results of the methods, and then provide an analysis of the implications arising from the involving factors in Discussion section."}, {"title": "4.1 Comparison of Proposed Methods", "content": "In order to evaluate the effectiveness of the methods discussed in Section 2, we per- formed a series of experiments in a few-shot learning scenario. For each task, we utilized sample sizes of 8, 16, 32 for few-shot settings. To ensure the reliability of the results, each experiment was repeated three times with different random seeds.\nTable 1 and 2 present average performance of the proposed methods across all tasks in two experiments, offering valuable insights into their effectiveness. In the following, we will conduct a comparative study of these methods."}, {"title": "4.1.1 Few-shot Efficiency", "content": "A consistent finding across all scenarios is that the proposed methods substantially outperform prompt tuning in few-shot settings. This highlights the effectiveness of incorporating source prompts and training them in multi-task settings. The adoption of multi-task settings facilitates the exchange of knowledge among these tasks, further enhancing overall performance. This characteristic proves advantageous in few-shot learning contexts where data availability is constrained.\nAccording to the tables, the major enhancement compared to prompt-tuning is observed for tasks such as QNLI, RTE, and MNLI in GLUE tasks, as well as PAWS, MNLI, and SciTail in SET2 tasks."}, {"title": "4.1.2 More Training data", "content": "We have expanded our experiments to include a larger training dataset. The results of our proposed methods, utilizing 5000 samples for each task, are displayed in Table 3 for GLUE tasks and Table 4 for tasks in SET2.\nWhile our designed methods demonstrate significant improvements in performance in few-shot settings, they also achieve state-of-the-art results with a larger training dataset, outperforming the overall performance compared to the related works, which are detailed in the Related Works section 6."}, {"title": "5 Discussion", "content": "In this section, we explore the pivotal roles played by source and private prompts in our proposed methods, emphasizing the control and balance required to achieve desirable results. We delve into how these prompts contribute to task performance and knowledge transfer, examining strategies to optimize their impact for enhanced model performance."}, {"title": "5.1 Role of Source Prompts", "content": "Figure 2 illustrates the impact of the number of source prompts on the performance of proposed methods. It is evident that fewer source prompts generally lead to greater effectiveness across all methods. However, MSUM and SSUM can perform well even with higher numbers of source prompts. MSUM demonstrates greater robustness to variations in the number of source prompts and training data, while MCAT?s per- formance consistently declines with an increasing number of source prompts. Unlike MSUM and SSUM, which maintain a fixed prompt length, MCAT experiences an increase in target prompt length, potentially adversely affecting its performance.\nAdditionally, in cases of MSUM and SSUM, increasing the number of source prompts to certain values decreases the variance in performance across different train- ing data in few-shot settings, as evidenced by the standard deviation in multiple runs, as shown in Figure 2."}, {"title": "5.1.2 Contribution of Source Prompts", "content": "To assess the impact of each source prompt on the final results, we isolated individual prompts and measured their performance on a test set of 100 samples. Specifically, we retained one specific source prompt and masked the others, then evaluated the test data. In the following sections, we will analyze the results obtained from this technique for each of the proposed methods.\nSSUM\nFigure 3 illustrates the scores associated with each source prompt for the SSUM method, employing one, two, and three source prompts for two experiments. In these figures, each column represents the results when only the corresponding source prompt is retained, while the private prompt and other source prompts are masked. The col- umn labeled all src shows the results when all source prompts are retained, while the private prompt is ignored. The private column displays results when only the private prompt is retained. On the right side of each figure, the results for including all source prompts and the private prompt are shown labeled as total.\nAs shown, certain tasks like SST-2 and COLA mainly rely on the private prompt. These tasks exhibit the least similarity with the rest of the tasks based on the input and output. According to Table 1, these tasks also derive the least benefit from the proposed multi-tasking prompt tuning.\nOn the other hand, tasks such as STS-B, QNLI, and MRPC primarily rely on the source prompts, while tasks like QQP, RTE, and MNLI use a combination of source and private prompts. It is noteworthy that the final score for these tasks is higher when employing both source and private prompts together, compared to using either source or private prompts alone, highlighting the crucial role of source prompts in enhancing performance. Similarly, according to the results from Table 1, these tasks benefit the most from multi-task prompt tuning.\nWe then investigated the predictions made using isolated source or private prompts. Table 5 shows the predictions for MNLI when using a single source prompt. As evident from the table, the source prompt mainly predicts the entailment class, a label shared with the QNLI and RTE tasks, which are the dominant tasks in the source part (See Figure 3-1).\nIn contrast, the private prompt mainly predicts the contradiction class. This illustrates the complementary roles of the source and private prompts. Notably, the combined prompt achieves higher scores than both source and private prompts by pre- dicting all three classes. A similar but opposite trend can be observed for the RTE task (refer to Table 6), where the source part predominantly predicts not-entailment and the private part predicts entailment. This contrasting pattern aids in distinguishing between MNLI and RTE on the shared source part.\nWhile the labels for STS-B, MRPC, and QNLI differ, these tasks show a stronger reliance on source prompts, which may be due to their inherent relationships. Each task involves analyzing two sentences: STS-B assesses sentence similarity, QNLI determines if one sentence entails another, and MRPC identifies paraphrases.\nTo verify this relationship, we conducted a series of experiments where we evaluated the test data of one task using prompts generated for another task. Figure Al shows the results for tasks QNLI and STSB, as well as RTE and STSB. The prompts derived for STSB and QNLI, which mainly rely on the source part, can effectively classify the other task by converting the labels. It's noteworthy that, based on the figure, although QNLI is similar to RTE, its correlation with STSB is lower. Generally, according to the scores, QNLI and STSB are relatively easier tasks to solve.\nWhile QQP is similar to QNLI, it exhibits conflict with QNLI as the main predic- tion for this task is not-entailment on the source part. However, similar to RTE, it uses the private prompt to compensate for the source prompt by predicting mainly the duplicate class. In other words, the not-entailment on the source part acts more like the not-duplicate class. The final score is higher than when employing either the source or private prompt alone, implying an implicit com- bination and conversion to predict the proper classes. This again demonstrates how QQP can benefit from QNLI, despite the tasks having distinct labels."}, {"title": "5.1.3 The Effect of Learned Weights", "content": "The question arises: what would happen if we omit the attention module and assign constant or uniform attention to all source prompts? Figure 5 compares the results obtained from employing constant and variable weights learned by the attention mod- ule. Notably, the number of source prompts was fixed at 3 for MSUM and 2 for MCAT and SSUM, which yielded the best results in experiments. Across all methods, con- stant weights perform better when data is limited. However, as the amount of data increases, the learned weights achieve higher scores. This suggests that in few-shot settings, the available data may not be sufficient for effectively adjusting the weights, but with more data, the learned weights can lead to improved results.\nWe also investigated the role of weights when the number of source prompts increases. We hypothesized that weights could help specialize different source prompts to resolve conflicting tasks. As observed from Figure 5, using learned weights is effec- tive for SSUM but less effective for MCAT as the number of source prompts increases. For MSUM, both learned and constant weights closely track each other across different numbers of source prompts.\nIn MCAT, since source prompts are concatenated, the in-built attention mechanism of the model can adjust attention accordingly, making variable weights less effective. Conversely, in SSUM, the weighted prompts are averaged and combined with a private prompt to shape the target prompt, allowing weights to activate and emphasize each source prompt effectively.\nTo demonstrate the effectiveness of weights, we analyzed the impact of isolated source prompts. Figure 6 compares the results of employing two source prompts across SET2 tasks when the weights are constant and learned.\nAs observed, using constant weights leads to similar contributions from all source prompts. However, using variable weights allows for the separation of conflicting task groups such as MRPC and PAWS (Paraphrasing), and Yelp-Polarity and IMDB (Senti- ment Analysis), resulting in improved performance on these tasks. The learned weights, represented as logits used for the RelaxedBernoulli distribution, were also shown in the figure, demonstrating how this separation occurs."}, {"title": "5.2 Balancing the Impact of Source and Private Prompts", "content": "To achieve better results, we can balance the impact of source and private prompts by employing different learning rates for each part. Figure 7 shows the performance of the proposed methods across different learning rates of the private prompt at 30 and 50 epochs, with the learning rate of the source prompt fixed at 0.05. Additionally, the figure displays the scores of isolated source and private prompts at different learning rates for SSUM and MSUM methods.\nAdjusting the learning rates for the private prompt relative to the source prompt can influence the attention towards or away from the source prompts and impact the final score. Higher learning rates for the source prompt compared to the private prompt typically result in better performance, emphasizing the importance of source prompts. This effect is especially noticeable in SSUM and MCAT methods.\nAs demonstrated earlier, tasks like MNLI, RTE, and QQP derive the most benefit from the complementary roles of source and private prompts. The individual prompt scores highlight that when attention is concentrated on source prompts, the overall performance of these tasks, as indicated by the source prompt and total score, is higher. RTE benefits significantly from the shared source space, particularly when the private learning rate is lower (e.g., plr=0.01). However, in terms of epochs, the source prompts in this scenario require more time to adjust the weights and demonstrate their effectiveness.\nThe performance of the MSUM method tends to improve when either the learning rate of the source prompt or the private prompt is higher. Conversely, it shows lower performance when the learning rates of the source and private prompts are equal. To demonstrate this effect, the bottom heatmap in Figure 7 displays the scores of isolated source prompts combined with the private prompt at 30 epochs with different learning rates. As observed, when the learning rates are equal, the scores are significantly lower. The results improve at 50 epochs (refer to Figure A2). However, even at that point, the score with a lower learning rate of the private prompt, particularly for tasks like RTE and MNLI, remains higher compared to other cases. It's important to note that higher learning rates for private prompts primarily enhance the score of tasks such as SST-2, which rely mainly on the private prompt."}, {"title": "5.3 Modularity", "content": "In our approach, the selection of tasks for multi-task prompt tuning can signifi- cantly impact performance. Additionally, our method allows the initialization of source prompts with pre-trained prompts obtained from a training stage using individual tasks.\nOur method maintains its modular nature, enabling the inclusion or exclusion of additional source tasks to further enhance the performance of the target tasks.\nFor instance, when considering the GLUE tasks, excluding SST-2 from the set of source tasks can be a justifiable decision due to its limited utility or contribution to the other tasks. Conversely, the inclusion of tasks like MNLI or STS-B has proven effective and beneficial for other related tasks.\nAs another example, we added the MNLI task to SET2 and compared the perfor- mance of the tasks with and without MNLI. The results are presented in Table 7. As observed, the inclusion of MNLI boosts the performance in all methods, particularly for tasks such as MultiNLI and SciTail, which are closely related to MNLI."}, {"title": "6 Related Works", "content": "This work lies in the line of parameter-efficient transfer learning for pretrained lan- guage models. These approaches focus on fine-tuning a subset of parameters to adapt pretrained language models to downstream tasks. The fine-tuned parameters can take on various forms, such as lightweight neural adapters positioned between PTM lay- ers [3], soft prompts appended to input examples [7], hidden states [8], bias terms integrated within PTM parameters [20], or low-rank matrices incorporated into atten- tion weights [4]. Our work is closely aligned with the realms of transfer learning and modular design facilitated through the utilization of soft prompts.\nVu et al. [17] introduced SPoT, which delves into the enhancement of prompt tuning performance across multiple tasks via soft prompt transfer. SPOT offers two distinct approaches. In the first approach, it initiates by learning a generic prompt across one or more source tasks using multi-task learning. This learned prompt then serves as the foundation for initializing prompts in target tasks. Alternatively, in the second approach, SPoT individually learns separate prompts for various source tasks. It subsequently identifies an optimal source prompt and employs it to initialize the training of a target prompt, thus enabling efficient knowledge transfer.\nSimilarly, Wang et al. [19] introduced MPT (Multitask Prompt Tuning), which employs a similar approach by learning a single transferable prompt through multi- task learning. This transferable prompt is learned via prompt decomposition and distillation. Subsequently, low-rank updates are incorporated into this shared prompt, facilitating its efficient adaptation to each distinct downstream target task.\nSun et al. [16] explored the concept of modular prompts within the scope of few-shot learning. Their methodology, named Multi-task Pre-trained Modular Prompt (MP2), is comprised of two stages. In the first stage, modular prompts are pre-trained along- side a trainable router with multi-task learning. This router facilitates the selection and activation of source prompts for various target tasks, Subsequently, in the second stage, a subset of prompts is activated and fine-tuned to align with downstream tasks. Notably, the initial phase of their method resembles our methodology. However, there is a distinction: in their approach, the obtained source prompts serve as pre- trained prompts for the subsequent stage. In contrast, in our approach, learning and utilizing the source prompts are merged into a single stage. More importantly, we employ a private prompt for each task, which is crucial for balancing the knowledge from shared source tasks and a specific task knowledge.\nAnother related study to our work is conducted by Asai et al., named ATTEMPT [1]. They first procure source prompts as encodings derived from extensive source tasks. Their methodology involves training an attention module to interpolate between the source prompts and a newly initialized target prompt for each instance within the target task, a concept analogous to the private prompts in our approach. One notable distinction is that their attention module operates on an instance-wise basis, with learning occurring for each input independently.\nAdditionally, they maintain the source prompts in a frozen state during target prompt training, whereas our results emphasize that fine-tuning the source prompts holds greater potential for enhancing performance, especially within few-shot scenar- ios. Notably, in this context, all tasks are equipped to exchange knowledge during training.\nUnlike the methods discussed above, our approach stands out by guiding the cre- ation of the target prompts through the incorporation of multiple source prompts and a task-specific prompt throughout the training process. Additionally, we intro- duced multiple methods to combine these prompts and compared their mechanisms and advantages. We also investigated an approach of concatenating modular source prompts, which is a novel technique.\nFurthermore, unlike the multi-stage approaches used in these methods, our approach operates within a single stage, performing knowledge transfer concurrently with learning the target prompt for each task. While it is possible to initialize the source prompts using pre-trained prompts, we achieve higher results by training all prompts in a single stage, as detailed in the next section."}, {"title": "6.1 Comparison with Related Works", "content": "In Table 8, we conducted a comparison between our method and the reviewed related works, which include vanilla prompt tuning (PT) [7], SPOT, ATTEMPT, and MPT, across 8 GLUE tasks. 'ATTEMPT-m' refers to the multi-tasking version of ATTEMPT. The reported numerical values are directly cited from the respective pub- lished papers. It's worth noting that MPT evaluated their approach on Chinese tasks, which led us to exclude them from this particular comparison.\nFor our comparison, we utilized the test sets of the tasks. However, due to resource constraints, we conducted testing on a subset of 5000 samples for our best-performing configurations. Despite this limitation, our overall performance outperforms these other methods, and our scores are close to the top scores for various tasks. Neverthe- less, we consider the main advantage of our approach to be in few-shot settings, where it boosts the performance of tasks through knowledge sharing among related tasks."}, {"title": "7 Conclusion", "content": "In this study, we explored the realm of task transferability within the context of multi- task prompt tuning, focusing on the potential for knowledge transfer among multiple tasks trained with a mixture of soft prompts.\nOur investigation revealed that the strategic use of source prompts, private prompts, and attention modules plays a pivotal role in driving shared knowledge across tasks and enhancing the performance of related tasks. This enhancement is particu- larly critical in scenarios where data is scarce, as observed in few-shot settings. Our modular approach, including the selection of tasks, determination of the number of source prompts, method for combining source prompts into private prompts, and bal- ancing their contributions through learning rates, amplifies the resilience and flexibility of the approach.\nThese insights serve as valuable stepping stones for future research endeavors, highlighting the value of multi-task prompt tuning as a versatile, modularity-driven strategy for enhancing knowledge transfer and enabling parameter-efficient transfer learning."}, {"title": "Declarations", "content": "\u2022 Competing Interests: The authors declare no competing interests.\n\u2022 Authors' Contributions:\n Hesham Faili: Supervision, Conceptualization, Methodology, Resources, Valida- tion, Writing - Review & Editing.\n Ahmad Pouramini: Conceptualization, Investigation, Data Curation, Methodol- ogy, Writing - Original Draft Preparation.\n\u2022 Ethical and Informed Consent for Data Used: This study did not involve human or animal subjects. All data are publicly available and cited appropriately. Informed consent was not required.\n\u2022 Data Availability and Access: Data are available on a public repository. Code will be publicly available on GitHub: https://github.com/puraminy/ComPT"}]}