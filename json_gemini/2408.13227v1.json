{"title": "Enhancing Few-Shot Transfer Learning with Optimized Multi-Task Prompt Tuning through Modular Prompt Composition", "authors": ["Ahmad Pouramini", "Hesham Faili"], "abstract": "In recent years, multi-task prompt tuning has garnered considerable attention for its inherent modularity and potential to enhance parameter-efficient transfer learning across diverse tasks. This paper aims to analyze and improve the performance of multiple tasks by facilitating the transfer of knowledge between their corresponding prompts in a multi-task setting. Our proposed approach decomposes the prompt for each target task into a combination of shared prompts (source prompts) and a task-specific prompt (private prompt). During training, the source prompts undergo fine-tuning and are integrated with the private prompt to drive the target prompt for each task. We present and compare multiple methods for combining source prompts to construct the target prompt, analyzing the roles of both source and private prompts within each method. We investigate their contributions to task performance and offer flexible, adjustable configurations based on these insights to optimize performance. Our empirical findings clearly showcase improvements in accuracy and robustness compared to the conventional practice of prompt tuning and related works. Notably, our results substantially outperform other methods in the field in few-shot settings, demonstrating superior performance in various tasks across GLUE benchmark, among other tasks. This achievement is attained with a significantly reduced amount of training data, making our method a promising one for few-shot settings.", "sections": [{"title": "1 Introduction", "content": "Presently, transfer learning has become the dominant paradigm of natural language processing [15]. The prevalent technique involves pretraining a model on an extensive corpus of raw data and subsequently fine-tuning it for specific tasks using a limited dataset. Despite its success, this approach suffers from a series of limitations, such as negative interference [14] and catastrophic forgetting [6].\nAs a remedy to address these challenges, the concept of modular deep learning has gained traction [12]. Within this framework, computation units are designed as self-contained, parameter-efficient modules. These modules are selectively engaged and then combined to generate the ultimate output. For instance, incorporating adapter modules, task-specific layers [5], or task-specific embeddings allow for the model to focus more on certain tasks or to better separate the representations for different tasks [2]. The main advantages of a modular neural architecture are positive transfer, compositionality, and parameter efficiency [12].\nPrompt-Tuning is a parameter efficient method for fine-tuning a pretrained lan- guage model on downstream tasks [7, 9, 10]. It can also serve as a method for modular design to specialize different prompts for different tasks [12]. From this perspective, task-specific prompt tokens, after encoded and optimized by the model's embedding layer, corresponds to modular parameters that elicit the desired behaviour. The knowledge distilled within these soft prompts can subsequently be transferred to related tasks or combined to provide the necessary knowledge for tackling new tasks [17]. Yet, the effective utilization of this extensive cross-task knowledge within a multitask learning framework has remained elusive.\nIn this paper, we explore task transferability within the context of multi-task prompt tuning. We introduce a specialized framework called ComPT that combines multiple shared source prompts and task-specific prompts to drive the target prompt for each task within a multi-task learning setting. Our objective is to enhance overall performance by facilitating knowledge exchange between these tasks. We propose sev- eral methods for combining these prompts and evaluate their effectiveness in solving diverse tasks.\nIn the following sections, we outline the specifics of our approach and describe our proposed methods. Subsequently, we present our experimental results and conduct an in-depth analysis of the results and contributing factors affecting performance. Finally, we review related works, compare our method with existing approaches, and conclude the paper."}, {"title": "2 Method", "content": "Pt for each distinct target task t. In addition to source prompts, a private prompt, denoted as Pu is dedicated for each task. The final target prompt is obtained by combining the source prompts and then composing them with the private prompt. This is achieved by activating and combining the shared source prompts through a trainable attention module, which assigns the weights wts from the source prompt s to the target task t. Training is conducted using multi-task learning across all tasks. Learning a shared prompt space in multi-task settings can pose practical challenges, as it necessitates learning commonalities across various source tasks while simultaneously minimizing interference. The source and private prompts are initially randomized. However, in practice, prompts can also be initialized using prompts obtained from a pre-training stage. In this study, we opt to simultaneously learn the prompts and the attention weights, which resulted in high performance, even when compared to approaches involving prompt initialization. In the following sections, we describe the details of our approach."}, {"title": "2.1 Prompt Training", "content": "Before delving into further details of our method, let us first review the process of prompt tuning. Prompt tuning typically entails optimizing the input prompt of a language model LM to enhance its performance on a specific task.\nWe begin with a template \\(T = {x, p_1,..., p_m}\\), comprising the input sequence x = \\([x_1,...,x_l]\\) and prompt tokens \\(p_r\\). The placement of prompt tokens can occur either before or after the input tokens. In our experiments, we positioned the prompt tokens after the input tokens, yielding slightly better results. Prompt tuning then entails mapping this template to a set of trainable prompt embeddings as follows:\n\\(\\{emb(x), h_0,...h_m\\}\\)  (1)\nWe denote a soft prompt as \\(P = [h_1,...,h_m] \u2208 R^{m\u00d7d}\\), where m is the prompt length, and d is the dimensionality of the underlying language model LM. This soft prompt can be considered as a continuous representation of the sequence of prompt tokens that is used to condition the language model."}, {"title": "2.2 Target Prompt Decomposition", "content": "In contrast to vanilla prompt tuning, the prompts in our approach result from com- bining multiple source prompts. In other words, we decompose the target prompt into several source prompts that are shared among tasks. The backpropagation mechanism updates these source prompts and parameters for their combination to obtain an opti- mal target prompt. In the context of a specific target task, the target prompt emerges as a fusion of the shared source prompts along with a trainable task-specific prompt, which we denote as the private prompt.\nPrecisely, let S represent the set of shared source prompts, and let Pu denote the private prompt corresponding to the target task t. In this context, the computation of the target prompt Pt is as follows:\n\\(P_t = P_u  W_{ts}.P_s\\)  (4)\nSES\nThe weights wts are determined by an attention mechanism implemented through an attention module R, which dynamically assigns weights to the shared prompts. This mechanism controls the contribution of each source prompt to a specific task based on its relevance to the target task, while a private prompt captures task-specific nuances. If none of the shared source prompts are useful for a target task, the target prompt primarily relies on the private prompt for its formation.\nThe composition of the source prompts can be achieved using various functions, with \u0970 denoting the composition function. In this paper, we explored summation and"}, {"title": "2.3 Source-Target Attentions", "content": "The attention module R governs the influence of each source prompt on the target prompt by computing attention weights W1t, ..., Wmt for the shared source prompts. These weights can be learned as free parameters or through a probability distribution. Specifically, we employed the Relaxed Bernoulli distribution to formulate these atten- tion weights, as it demonstrated efficacy and learning stability in our experimental evaluations compared to using free parameters."}, {"title": "2.3.1 Inference", "content": "During inference, we load all shared source prompts and private prompts, as well as the shared attention module only once. The target prompt for a given task is derived by applying Equation 5. Unlike during training, at inference time, the learned attention module, which includes both positive and negative weights (logits of the Relaxed Bernoulli distribution), is directly utilized without the need for additional sampling. By applying softmax over these learned scores, probability distributions are obtained.\nAfter obtaining the target prompt, it is concatenated to the input instance xand fed to the model. The model generates the output y by utilizing its internal attention mechanism to consider both the learned target prompt and the input data.\nIn the following sections, we will present the results obtained by applying each method to various tasks and provide a comprehensive comparison among them."}, {"title": "3 Experiments", "content": "We assess the performance of proposed methods across various tasks, including eight tasks from the GLUE benchmark [18] and six tasks from other datasets. These datasets are publicly available on Hugging Face's model hub.\nOur evaluation involves two distinct experiments. In Experiment 1, the GLUE tasks encompass MNLI, QNLI, RTE (NLI), COLA (grammatical acceptability), SST2"}, {"title": "3.2 Implementation Details", "content": "We utilize the T5-base language model [13] as our base model, which is commonly employed in related works. The prompt length is an effective parameter, and we inves- tigated shorter lengths (20 tokens), which proved more effective compared to longer prompts. Specifically, for MCAT, shorter lengths such as 10 tokens proved to be more stable and efficient.\nIn the quest for an optimal prompt encoder, we experimented with different archi- tectures and activation functions. Initially, we tested an LSTM-based encoder, similar to [10], but found superior performance with an MLP-based encoder. Further explo- ration led us to conclude that the GELU activation function yielded slightly better results compared to RelU and SiLU. Our chosen prompt encoder, a two-layer MLP, generates prompt embeddings of size d corresponding to each token. This MLP com- prises a fully connected layer with a hidden size of d (the language model's embedding size), followed by a GELU activation function, and another fully connected layer responsible for producing the final prompt embedding.\nFor the RelaxedBernoulli distribution generator, we started with an initial temper- ature of 5 and linearny reduced it using annealing to le-3 during entire training. The attention module was trained with a learning rate of 0.1, while the prompt encoders for source prompts were trained with a learning rate of 0.05 and the prompt encoders for the private prompts were trained with a learning rate of 0.02."}, {"title": "4 Results", "content": "In the following sections, we will present the results of the methods, and then provide an analysis of the implications arising from the involving factors in Discussion section."}, {"title": "4.1 Comparison of Proposed Methods", "content": "In order to evaluate the effectiveness of the methods discussed in Section 2, we per- formed a series of experiments in a few-shot learning scenario. For each task, we utilized sample sizes of 8, 16, 32 for few-shot settings. To ensure the reliability of the results, each experiment was repeated three times with different random seeds.\nTable 1 and 2 present average performance of the proposed methods across all tasks in two experiments, offering valuable insights into their effectiveness. In the following, we will conduct a comparative study of these methods."}, {"title": "4.1.1 Few-shot Efficiency", "content": "A consistent finding across all scenarios is that the proposed methods substantially outperform prompt tuning in few-shot settings. This highlights the effectiveness of incorporating source prompts and training them in multi-task settings. The adoption of multi-task settings facilitates the exchange of knowledge among these tasks, further enhancing overall performance. This characteristic proves advantageous in few-shot learning contexts where data availability is constrained.\nAccording to the tables, the major enhancement compared to prompt-tuning is observed for tasks such as QNLI, RTE, and MNLI in GLUE tasks, as well as PAWS, MNLI, and SciTail in SET2 tasks."}, {"title": "4.1.2 More Training data", "content": "We have expanded our experiments to include a larger training dataset. The results of our proposed methods, utilizing 5000 samples for each task, are displayed in Table 3 for GLUE tasks and Table 4 for tasks in SET2.\nWhile our designed methods demonstrate significant improvements in performance in few-shot settings, they also achieve state-of-the-art results with a larger training dataset, outperforming the overall performance compared to the related works, which are detailed in the Related Works section 6."}, {"title": "5 Discussion", "content": "In this section, we explore the pivotal roles played by source and private prompts in our proposed methods, emphasizing the control and balance required to achieve desirable results. We delve into how these prompts contribute to task performance and knowledge transfer, examining strategies to optimize their impact for enhanced model performance."}, {"title": "5.1 Role of Source Prompts", "content": ""}, {"title": "5.1.1 Number of Source Prompts", "content": "However, MSUM and SSUM can perform well even with higher numbers of source prompts. MSUM demonstrates greater robustness to variations in the number of source prompts and training data, while MCAT?s per- formance consistently declines with an increasing number of source prompts. Unlike MSUM and SSUM, which maintain a fixed prompt length, MCAT experiences an increase in target prompt length, potentially adversely affecting its performance.\nAdditionally, in cases of MSUM and SSUM, increasing the number of source prompts to certain values decreases the variance in performance across different train- ing data in few-shot settings, as evidenced by the standard deviation in multiple runs,"}, {"title": "5.1.2 Contribution of Source Prompts", "content": "To assess the impact of each source prompt on the final results, we isolated individual prompts and measured their performance on a test set of 100 samples. Specifically, we retained one specific source prompt and masked the others, then evaluated the test data. In the following sections, we will analyze the results obtained from this technique for each of the proposed methods."}, {"title": "5.2 Balancing the Impact of Source and Private Prompts", "content": "To achieve better results, we can balance the impact of source and private prompts by employing different learning rates for each part. "}, {"title": "5.3 Modularity", "content": "In our approach, the selection of tasks for multi-task prompt tuning can signifi- cantly impact performance. Additionally, our method allows the initialization of source prompts with pre-trained prompts obtained from a training stage using individual tasks.\nOur method maintains its modular nature, enabling the inclusion or exclusion of additional source tasks to further enhance the performance of the target tasks.\nFor instance, when considering the GLUE tasks, excluding SST-2 from the set of source tasks can be a justifiable decision due to its limited utility or contribution to the other tasks. Conversely, the inclusion of tasks like MNLI or STS-B has proven effective and beneficial for other related tasks.\nAs another example, we added the MNLI task to SET2 and compared the perfor- mance of the tasks with and without MNLI. The results are presented in Table 7. As observed, the inclusion of MNLI boosts the performance in all methods, particularly for tasks such as MultiNLI and SciTail, which are closely related to MNLI."}, {"title": "6 Related Works", "content": "This work lies in the line of parameter-efficient transfer learning for pretrained lan- guage models. These approaches focus on fine-tuning a subset of parameters to adapt pretrained language models to downstream tasks. The fine-tuned parameters can take on various forms, such as lightweight neural adapters positioned between PTM lay- ers [3], soft prompts appended to input examples [7], hidden states [8], bias terms integrated within PTM parameters [20], or low-rank matrices incorporated into atten- tion weights [4]. Our work is closely aligned with the realms of transfer learning and modular design facilitated through the utilization of soft prompts.\nVu et al. [17] introduced SPoT, which delves into the enhancement of prompt tuning performance across multiple tasks via soft prompt transfer. SPOT offers two distinct approaches. In the first approach, it initiates by learning a generic prompt across one or more source tasks using multi-task learning. This learned prompt then serves as the foundation for initializing prompts in target tasks. Alternatively, in the second approach, SPoT individually learns separate prompts for various source tasks. It subsequently identifies an optimal source prompt and employs it to initialize the training of a target prompt, thus enabling efficient knowledge transfer.\nSimilarly, Wang et al. [19] introduced MPT (Multitask Prompt Tuning), which employs a similar approach by learning a single transferable prompt through multi- task learning. This transferable prompt is learned via prompt decomposition and distillation. Subsequently, low-rank updates are incorporated into this shared prompt, facilitating its efficient adaptation to each distinct downstream target task.\nSun et al. [16] explored the concept of modular prompts within the scope of few-shot learning. Their methodology, named Multi-task Pre-trained Modular Prompt (MP2), is comprised of two stages. In the first stage, modular prompts are pre-trained along- side a trainable router with multi-task learning. This router facilitates the selection and activation of source prompts for various target tasks, Subsequently, in the second stage, a subset of prompts is activated and fine-tuned to align with downstream tasks. Notably, the initial phase of their method resembles our methodology. However, there is a distinction: in their approach, the obtained source prompts serve as pre- trained prompts for the subsequent stage. In contrast, in our approach, learning and utilizing the source prompts are merged into a single stage. More importantly, we employ a private prompt for each task, which is crucial for balancing the knowledge from shared source tasks and a specific task knowledge.\nAnother related study to our work is conducted by Asai et al., named ATTEMPT [1]. They first procure source prompts as encodings derived from extensive source tasks. Their methodology involves training an attention module to interpolate between the source prompts and a newly initialized target prompt for each instance within the target task, a concept analogous to the private prompts in our approach. One notable distinction is that their attention module operates on an instance-wise basis, with learning occurring for each input independently.\nAdditionally, they maintain the source prompts in a frozen state during target prompt training, whereas our results emphasize that fine-tuning the source prompts"}, {"title": "6.1 Comparison with Related Works", "content": "In Table 8, we conducted a comparison between our method and the reviewed related works, which include vanilla prompt tuning (PT) [7], SPOT, ATTEMPT, and MPT, across 8 GLUE tasks. 'ATTEMPT-m' refers to the multi-tasking version of ATTEMPT. The reported numerical values are directly cited from the respective pub- lished papers. It's worth noting that MPT evaluated their approach on Chinese tasks, which led us to exclude them from this particular comparison.\nFor our comparison, we utilized the test sets of the tasks. However, due to resource constraints, we conducted testing on a subset of 5000 samples for our best-performing configurations. Despite this limitation, our overall performance outperforms these other methods, and our scores are close to the top scores for various tasks. Neverthe- less, we consider the main advantage of our approach to be in few-shot settings, where it boosts the performance of tasks through knowledge sharing among related tasks."}, {"title": "7 Conclusion", "content": "In this study, we explored the realm of task transferability within the context of multi- task prompt tuning, focusing on the potential for knowledge transfer among multiple tasks trained with a mixture of soft prompts.\nOur investigation revealed that the strategic use of source prompts, private prompts, and attention modules plays a pivotal role in driving shared knowledge across tasks and enhancing the performance of related tasks. This enhancement is particu- larly critical in scenarios where data is scarce, as observed in few-shot settings. Our modular approach, including the selection of tasks, determination of the number of source prompts, method for combining source prompts into private prompts, and bal- ancing their contributions through learning rates, amplifies the resilience and flexibility of the approach.\nThese insights serve as valuable stepping stones for future research endeavors, highlighting the value of multi-task prompt tuning as a versatile, modularity-driven strategy for enhancing knowledge transfer and enabling parameter-efficient transfer learning."}, {"title": "Declarations", "content": "\u2022 Competing Interests: The authors declare no competing interests.\n\u2022 Authors' Contributions:\nHesham Faili: Supervision, Conceptualization, Methodology, Resources, Valida- tion, Writing - Review & Editing.\nAhmad Pouramini: Conceptualization, Investigation, Data Curation, Methodol- ogy, Writing - Original Draft Preparation.\n\u2022 Ethical and Informed Consent for Data Used: This study did not involve human or animal subjects. All data are publicly available and cited appropriately. Informed consent was not required.\n\u2022 Data Availability and Access: Data are available on a public repository. Code will be publicly available on GitHub: https://github.com/puraminy/ComPT"}]}