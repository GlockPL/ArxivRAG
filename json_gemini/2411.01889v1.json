{"title": "LiDAttack: Robust Black-box Attack on LiDAR-based Object Detection", "authors": ["Jinyin Chen", "Danxin Liao", "Sheng Xiang", "Haibin Zheng"], "abstract": "Abstract-Due to the significantly increasing number of au-\ntonomous driving systems incorporated with LiDAR sensors, the\npoint cloud captured by LiDAR for object detection plays an\nindispensable role. In particular, deep neural network (DNN)\nbased point cloud object detection has achieved dominant\nperformance compared with traditional methods. However, since\nDNN is vulnerable to carefully crafted adversarial examples,\nadversarial attack on LiDAR sensors have been extensively studied.\nIn all, these attacks are challenged from three aspects: (i) effective\n- they're generally effective in white-box scenarios, but their\nperformance will significantly degrade in black-box scenarios;\n(ii) robustness - they are much less effective when exposed to\nthe diverse changes in the real world (i.e., angle and distance\nchanges); (iii) concealing - attacks will be defended since most\nof them overlook the stealthy goal of escaping detection. To\naddress these challenges, we introduce a robust black-box attack\ndubbed LiDAttack. It utilizes a genetic algorithm with a simulated\nannealing strategy to strictly limit the location and number of\nperturbation points, achieving a stealthy and effective attack.\nAnd it simulates scanning deviations, allowing it to adapt to\ndynamic changes in real world scenario variations. Extensive\nexperiments are conducted on 3 datasets (i.e., KITTI, nuScenes,\nand self-constructed data) with 3 dominant object detection\nmodels (i.e., PointRCNN, PointPillar, and PV-RCNN++). The\nresults reveal the efficiency of the LiDAttack when targeting a\nwide range of object detection models, with an attack success\nrate (ASR) up to 90%. To evaluate the robustness of LiDAttack,\nexperiments at different distances and angles are conducted,\nand the results show that LiDAttack can maintain consistent\nperformance over a certain range. The practicality of LiDAttack\nis further validated in the physical world both indoors and\noutdoors. The results show that LiDAttack-generated adversarial\nobjects can still maintain effective attack in the physical world.\nIn order to achieve a concealed attack, we limit the volume of the\ngenerated adversarial object to less than 0.1% of the volume of\nthe target object to achieve ASR up to 90%. The code is available\nat https://github.com/Cinderyl/LiDAttack.git.", "sections": [{"title": "I. INTRODUCTION", "content": "LiDAR sensor is an advanced technology that uses lasers to\nanalyze the surrounding environment for wide-ranging areas,\nsuch as autonomous driving system. It calculates distance\nand 3D effects by measuring the time gap to fire a laser\npulse, and returns it from the target. Among other things,\nthe LiDAR sensor is capable of generating a point cloud for\nobject detection[1], [2], [3], enabling real-time monitoring of\nobstacles, pedestrians, and other vehicles around the target\none. Thus, the autonomous driving system can make accurate\ndecisions to ensure safe driving.\nFrom the perspective of representation learning strategies of\nthe irregular laser point cloud, object detection can be broadly\ncast into three categories, i.e., point-based methods[3], [22],\n[23], [24], voxel-based methods[1], [25], [26], [27], and point-\nvoxel based methods[28], [29], [30], [31]. Most of them rely on\ndeep neural network (DNN) to process point cloud data due to\nthe help of DNN's powerful extraction and learning capabilities.\nNot surprisingly, DNN-based point cloud object detection has\nachieved dominant performance. However, recent research[7],\n[6] has revealed that DNN is susceptible to adversarial attack, in\nwhich imperceptibly perturbed examples will easily manipulate\nthe DNN make incorrect predictions. Due to DNN's wide\napplication in some safety-critical areas such as autonomous\ndriving systems' LiDAR-based object detection[4], [5], the\ncarefully crafted adversarial example of LiDAR point cloud\nwill be a serious security threat.\nFurthermore, the vulnerability of LiDAR sensors towards\nphysical attack will threat the autonomous driving system\nin a more direct manner. These attacks can be categorized\ninto three groups, i.e., laser-based attack, object-based attack,\nand location-based attack. The laser-based attacks [8], [11],\n[10], [9], [12] are conducted by manipulating the signal\ndetection system of a LiDAR sensor. Specifically, an attacker\nutilizes a photodiode to receive laser pulses from the LiDAR,\nand triggers the attack by activating a delay component,\nthereby simulating a real echo pulse. Generally, these attacks\nrequire prior knowledge of the victim's LiDAR laser emission\nfrequency, so as to take the synchronization of the jammer and\nthe attacked LiDAR into account. They fall under the category\nof white-box attack. Laser-based attack exhibits heightened\nsensitivity to the movement of the target object, resulting in\na lack of robustness. For object-based attacks[10], [19], [16],\n[14], [18], [17], objects with adversarial shapes are proposed to\nattack LiDAR sensors. These adversarial objects are large and\nunusually shaped, resulting in inadequate concealment. The\nlocation-based attack[20] aims to find the key confrontation"}, {"title": "II. RELATED WORK", "content": "We first briefly review deep learning for LiDAR-based object\ndetection. Next, point cloud object detection and its potential\nattack and defense are fully explored."}, {"title": "A. 3D Object Detection based on LiDAR Point Cloud", "content": "LiDAR-based object detection can be mainly categorized into\npoint-based methods, voxel-based methods, and point-voxel\nbased methods[21].\nPoint-based 3D object detection. Point-based 3D object\ndetection[3], [22], [23], [24] consists of two main basic com-\nponents, i.e., sampling of the point cloud and feature learning.\nThe point-based target detector first requires preprocessing of\nthe point cloud, and then performs foreground or background\npoint segmentation along with the learned high-dimensional\nfeature points. This segmentation information is then used to\nbootstrap the anchor-free network to output a target candidate\nfor each point.\nVoxel-based 3D object detection. Voxel-based point cloud\nobject detection[1], [25], [26], [27] converts irregular point\ncloud data into a regular form of voxels, each of which can\nbe considered as a 3D pixel. A voxel is labeled as non-empty\nif a point cloud falls into that grid cell. Due to the sparsity of\nthe point cloud data, most voxels are empty. Therefore, only"}, {"title": "B. Point Cloud Adversarial Attack", "content": "Several recent studies have shown that networks that use\npoint cloud data as input may be threatened by adversarial\nattack[14]. These attacks that use point clouds as inputs can be\ncategorized into the following three main types, i.e., point shift-\nbased attack[37], [38], [39], [40], point add-based attack[41],\n[42], [43], and point drop-based attack[44], [45].\nPoint shift-based attack. It misleads the LiDAR-based\nobject detection through small perturbations[37], [38], [39],\n[40], which can be implemented by changing attributes such\nas the coordinates or reflection intensity of a point.\nPoint add-based attack. It is another common digital\ndomain attack designed to deceive the object detection system\nby adding extra points to the original point cloud[41], [42],\n[43]. These extra points can be false obstacles, non-existent\nobjects, or objects that do not match the real scene.\nPoint drop-based attack. It works by selectively removing\na number of points from the original point cloud in order to\nprevent the system from correctly sensing a specific object or\nregion[44], [45]. Zheng et al[44] proposed an efficient method\nto construct a saliency map of a 3D point cloud to measure\nthe contribution (importance) of each point to the model's\nprediction loss. Using the saliency map, they further standard-\nized the point removal process. To explore the robustness of\nobject detection models in adversarial environments, Matthew\nWicker and Marta Kwiatkowska[45] demonstrated that critical\npoint sets induced by latent spatial translations in 3D deep\nlearning, for both point cloud and volume representations,\nexpose weaknesses in adversarial occlusion attacks."}, {"title": "C. Physical Adversarial Attack", "content": "Physical attack on LiDAR-based object detection can be\ncategorized into three types, i.e., laser-based attack[8], [11],\n[10], [9], [12] and object-based attack[10], [19], [16], [14],\n[18], [17] and location-based attack[20].\nLaser-based attack. Through the attack, hacker can interfere\nwith the victim's decision-making by firing lasers at them.\nCao et al.[8] first found that a LiDAR-based perception deep\nlearning model used in autonomous driving systems can cause\nthe perception system to mistakenly detect a fake vehicle (one"}, {"title": "D. Defense for Point Cloud based Object Detection", "content": "Adversarial defenses against point clouds generally fall into\nfour categories, i.e., input transformations[38], [52], [54], [37],\ndata optimization, sensor-level optimization, and other defenses.\nInput transformation. It is a preprocessing step before\nthe deep model processes the point cloud, aiming to reduce\nthe model's susceptibility to malicious attack and increase\nthe difficulty of the attack by introducing transformations[38],\n[52], [54], [37]. Considering the relative prominence of the\nperturbed points of the input, Zhou et al.[52] detected outliers\nand removed outliers by counting the Euclidean distance\nbetween each point and its k nearest neighbours. Xiang et\nal.[37], proposed a simple random sampling (SRS) to randomly\nremove a certain number of points from the input point cloud\nin order to reduce the influence of the perturbed points.\nData optimization. Data optimization refers to the opti-\nmization of training data to improve the robustness of deep\nmodels against adversarial attack[38], [55]. For example, data\naugmentation, adversarial training, etc. Liu et al. [38] first\ndescribed adversarial training of point clouds. sun et al.[55]\napplied self-supervised learning to adversarial training of 3D\npoint clouds.\nSensor-level optimization. It is designed to improve the\ndetection accuracy and robustness of an autonomous driving\nsystem by designing a structure for multi-sensor fusion.\nMany research efforts have been devoted to improving the\nperformance of sensor fusion modules to enhance the safety\nand reliability of autonomous driving systems[56], [57], [58],\n[59].\nOther defenses. Occlusion-aware structural anomaly detec-\ntion (CARLO), and sequence view fusion (SVF) all designate\nthe attack as a black-box emergence attack as proposed in Sun's\nwork[9] and defend against the attack by removing dubious\npoints from the input point cloud or deleting suspicious objects\nfrom the final prediction. Xiao et al.[61] proposed a novel\ndefense module called local objectness predictor (LOP), which\ndetects and removes false obstacles in LiDAR target detectors\nby learning to predict the truthfulness of local parts of an"}, {"title": "III. PRELIMINARIES", "content": "This section introduces the attack scenario for object\ndetection, the object detection model, and the definitions of\nadversarial attack. For convenience, the definitions of some\nimportant symbols are listed in Table I."}, {"title": "A. Object Detection Model", "content": "The principle of LiDAR-based point cloud object detection is\nto process and analyze the point cloud data collected by LiDAR\nto achieve the detection and identification of target objects. In\nthis case, the point cloud data is composed of distance values\nthat are measured by the LiDAR to measure the flight time of\nthe emitted laser beam and calculated. The specific detection\nprinciple is as follows.\nGiven a deep learning model $M$, input data $D$ and cor-\nresponding ground truth labels $y^*$. The goal of the object\ndetection model is to output labels that are the same as the\nground truth labels after inputting data $D$, i.e., $y = M (D)$,\n$y = y^*$"}, {"title": "B. Adversarial Attack", "content": "The goal of an attacker against an autonomous driving system\nis usually to interfere with or disrupt the proper functioning\nof the object detection model in the system, rendering the\nobject detection ineffective, which can lead to unsafe driving\nor navigational behaviors. The specific attack principle is as\nfollows.\nGiven a deep learning model $M$, input data $D$, and\ncorresponding ground truth labels $y^*$, the goal of the adversarial"}, {"title": "IV. METHODOLOGY", "content": "A black-box attack named LiDAttack is proposed, which\naims to achieve an attack on the target detection model by\noptimizing the location of perturbation points to manipulate it\nto make incorrect detection. Specifically, LiDAttack is designed\nbased on the GSA, which combines the global search capability\nof the genetic algorithm and the local search capability of the\nsimulated annealing algorithm, thus it is able to efficiently\ngenerate covert and effective perturbation points.\nFig. 2 illustrates the main framework of LiDAttack. In the\ninitial stage, the system randomly generates perturbation points\nand adds them to the target point cloud. Then, these perturbation\npoints are modeled and scanned by a simulated LiDAR to\nobtain the corresponding simulation results. Afterwards, the co-\nordinates of the perturbed points are encoded as chromosomes\nand a fitness function is defined to evaluate the effectiveness\nof these perturbed points. Subsequently, a new generation of\nperturbations is generated using the operations of selection,\ncrossover, mutation, generation update, and simulated annealing\nin the simulated annealing algorithm. This process continues\nuntil the termination condition is met, at which point the system\noutputs the adversarial example with the approximate optimal\nfitness function value."}, {"title": "A. Framework", "content": "A black-box attack named LiDAttack is proposed, which\naims to achieve an attack on the target detection model by\noptimizing the location of perturbation points to manipulate it\nto make incorrect detection. Specifically, LiDAttack is designed\nbased on the GSA, which combines the global search capability\nof the genetic algorithm and the local search capability of the\nsimulated annealing algorithm, thus it is able to efficiently\ngenerate covert and effective perturbation points.\nFig. 2 illustrates the main framework of LiDAttack. In the\ninitial stage, the system randomly generates perturbation points\nand adds them to the target point cloud. Then, these perturbation\npoints are modeled and scanned by a simulated LiDAR to\nobtain the corresponding simulation results. Afterwards, the co-\nordinates of the perturbed points are encoded as chromosomes\nand a fitness function is defined to evaluate the effectiveness\nof these perturbed points. Subsequently, a new generation of\nperturbations is generated using the operations of selection,\ncrossover, mutation, generation update, and simulated annealing\nin the simulated annealing algorithm. This process continues\nuntil the termination condition is met, at which point the system\noutputs the adversarial example with the approximate optimal\nfitness function value."}, {"title": "B. Technical Designs", "content": "In LiDAttack, global search is mainly realized by genetic\noperations, i.e., searching for optimal solutions by selec-\ntion, crossover, and mutation operations. Adaptive genetic\nalgorithm[49] is used to improve convergence accuracy and\nspeed. The algorithm automatically adjusts the crossover and\nmutation probabilities according to the changes in the fitness\nfunction values. Initially, a larger probability is selected to\nmaintain diversity, and later it is gradually adjusted down to\navoid destroying the optimal solution. The genetic algorithm\nhas a strong global search capability and is able to perform\nextensive searches in the entire solution space, thus avoiding\npremature convergence to a local optimum. While the simulated\nannealing algorithm is good at performing local search, it helps\nto jump out of the local optimal solution by accepting the\nworse solution with a certain probability, which enhances the\nalgorithm's local search ability. The specific process is shown\nas follows.\n1) Initialize: The initial solution is obtained through random\ninitialization, i.e., generating random Gaussian perturbation\npoints within a specified range, Gaussian perturbation can\ngenerate random numbers with different means and standard\ndeviations, which helps introduce diversity into the initial\npopulation. The corresponding initial adversarial example can\nbe expressed as $D_{init} = D + \\delta$, where $\\delta \\sim \\\u039d (\\mu,\\sigma^2)$. \u03a4\u03bf\nimprove the diversity of initial perturbations, different types\nof initial perturbations are generated for different numbers of\nperturbation points in this paper.\n2) Scanning simulation: Considering that the uncertainty\nin the point cloud reconstruction process will weaken the ASR\nin the real environment, the initial adversarial point cloud is"}, {"title": "V. EXPERIMENTAL SETTING", "content": "Platform. Intel XEON6420 2.6GHzX128C (CPU), Tesla\nV100 32GiG (GPU), 16GB DDR4 RECC2666 (memory),\nUbuntu 16.04 (OS), Python 3.7. In this experiment, 3D printing\nwas utilized to construct the necessary adversarial objects. In\nthe physical experiments, space data acquisition relied on the\nMid-40 lidar sensor from the Livox.\nVictim models. For the experiments in the digital world,\nPointRCNN[3], PointPillar[1], and PV-RCNN++[34] are cho-\nsen as the target models. PointRCNN is an advanced point\ncloud object detection framework that improves the detection\naccuracy in complex scenes through a two-stage process of first\nextracting candidate regions, followed by fine classification and\nlocalization. PointPillar is a real-time 3D object detection that\nefficiently handles large-scale point clouds and is suitable for\nrapid dynamic environment analysis by converting point cloud\ndata into columns perpendicular to the ground and applying\na 2D convolutional network. PV-RCNN++ is an efficient\ndetection network developed on the basis of PointRCNN, which\nemploys multi-scale feature fusion and an improved regional\nproposal network to enhance the detection of targets of different\nsizes and improve the model robustness. The performance on\ndifferent datasets can be found in Table II.\nDataset. For point cloud data, the KITTI, nuScenes, and\nself-constructed data are used.\n\u2022\nKITTI[51]. KITTI is widely used to train many state-of-\nthe-art object detection models. In our experiments, from\nthe KITTI dataset, 3000 LiDAR examples were extracted"}, {"title": "VI. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "In this section, the performance of LiDAttack is evaluated in\nterms of attack effectiveness, generalisability, and practicality.\nThe following research questions (RQs) are posed."}, {"title": "A. RQ1. How effective is LiDAttack against object detection model?", "content": "When reporting the results, we focus on the following aspects,\ni.e., different types of objects, the number of perturbation points\nand different recognition difficulty. The evaluation results are\nshown in Table IV and Fig. 6.\nDifferent object. LiDAttack is effective against different\nobjects. Table IV shows the ASR of LiDAttack's attack\non different object detection models, different number of\nperturbation points, different types of objects and different\nrecognition difficulty. Fig. 6(a) shows the ASR of LiDAttack\nattack for different object detection models and different objects\nwhen the number of perturbation points is 10. It can be\nobserved that for \u201cCar\u201d, \u201cPedestrian\" and \"Cyclist\" three types\nof target objects, LiDAttack can effectively attack them all, For\nPointRCNN and PointPillar, the ASR of LiDAttack on different\nobjects is more than 80%, and the ASR of PV-RCNN++ is\nslightly lower, which proves the PV-RCNN++ effectiveness\nand stability from the side. Moreover, the LiDAttack attack is\neffective for \"Cyclist\" type of attack always has the greatest"}, {"title": "B. RQ2. How about LiDAttack's robustness?", "content": "When reporting the results, we focus on the following aspects,\ni.e., distance and angles. The evaluation results are shown in\nFig. 8 and Fig. 9.\nIn real world scenarios, it is not possible to ensure that an\nobject can be placed back to its original position accurately,\nso it is necessary to explore the potential impact of object\nplacement bias on ASR. Specifically, we consider two main\nfactors, i.e., the distance between the object and the LiDAR,\nand the angle at which the object is placed relative to the\nLiDAR.\nRobustness in distance change. LiDAttack attack is robust\nto different object detection models over a range of distances.\nThe distance between the object and the LiDAR can be\nobtained by translating the coordinates of the object. From Fig.\n8(a), LiDAttack shows strong robustness in the perturbation\nrange of -1m to 1m. This shows that our method has better\nrobustness compared to the adversarial objects generated in\npaper [53]. However, the ASR decreases as the perturbation\ndistance increases or decreases. In particular, the ASR against\nPointPillar and PV-RCNN++ declines faster than the ASR\nagainst PointRCNN. This difference may be due to the fact\nthat PointPillar and PV-RCNN++ employ different strategies\nto process point cloud data.PointPillar does this by converting\nthe point cloud into a sparse 3D voxel representation and\nutilizing a convolutional neural network for feature extraction\nand prediction, while PV-RCNN++ divides the point cloud\ninto multiple local regions for feature extraction and prediction"}, {"title": "C. RQ3. How does the physical Practicality of LiDAttack attack?", "content": "When reporting the results, we focus on the following aspects,\ni.e., indoor and outdoor. We did experiments in the physical\nworld and the results are displayed in Fig. 10 to Fig. 14. In\norder to show our experimental process more intuitively, we\nhave placed a demonstration video of our experimental process\nin https://github.com/Cinderyl/self-constructed-data.\nFig. 10 illustrates the LiDAR configuration used for point\ncloud data acquisition. In this experiment, the attacker achieved\na spoofing attack on the LiDAR by placing an adversarial\nscrambling object next to the target object. These interfaces\ninclude a dedicated laser detection rangefinder connector\ninterface, a synchronisation signal interface, a power supply\ninterface and an Ethernet interface for data communication. The\nLiDAR unit communicates with external devices over Ethernet\nusing the UDP protocol. We add a negative sign to indicate\nwhen the object is closer than the original distance or when\nthe object is rotated clockwise relative to the original position.\nIndoor experiment. LiDAttack attack remains effective\nindoors. In the indoor environment, the size of the adversarial\nobjects was limited to 0.2m \u00d7 0.2m \u00d7 0.2m. The flowerpot\nand box were used as controls. The printed adversarial objects\nand normal objects are shown in Fig. 11. The device is built\nas shown in Fig. 12, Table VI shows the misclassification rate\nof the object detection model, the misclassification rate shows\nthat normal objects can not make the object detection model"}, {"title": "D. RQ4. How to defend against LiDAttack and improve model robustness?", "content": "The adversarial examples generated by LiDAttack are used\nto enhance the defense capability of the target detection model\nthrough adversarial training. The core idea of adversarial\ntraining is to introduce adversarial examples during the training\nprocess so that the model gradually adapts and defends against\nadversarial attack during the learning process. Specifically,\nadversarial examples are generated by LiDAttack and the\nproportion of adversarial samples is gradually increased in\nthe training dataset starting from using all original samples\nuntil further increase in the proportion of adversarial samples\ndoes not significantly improve the model performance.\nAs shown in Table. VII, the models PointRCNN and PointPil-\nlar exhibited a notable performance degradation when subjected\nto the LiDAttack, with an increase in the misidentification\nrate of 70% to 80%. This suggests that the LiDAttack has\na significantly adverse impact on these models, effectively\nimpairing their detection capabilities. On the other hand,\nalthough the PV-RCNN++ was also affected by the attack,\nthe increase in its misidentification rate was relatively smaller,\nat 60% to 70%. This indicates that PV-RCNN++ has certain\nstructural advantages or higher robustness in its design, enabling\nit to better resist a certain level of attack. To counter such attack,\nadversarial training was employed as a defensive mechanism.\nAfter adversarial training, when the models were attacked again\nwith LiDAttack, PointRCNN's misidentification rate signifi-\ncantly decreased by 60% to 70%. This indicates that adversarial\ntraining effectively enhanced PointRCNN's resistance to the\nLiDAttack. Similarly, the misidentification rates of PointPillar\nand PV-RCNN++ also declined after adversarial training when\nfaced with LiDAttack, decreasing by 50% to 60% respectively.\nThese results confirm the effectiveness of adversarial training\nas a defensive measure."}, {"title": "VII. CONCLUSION", "content": "A novel black-box attack LiDAttack is proposed, which\naims to attack the target detection model in a LiDAR-based\nsensing system. The method uses a genetic simulated annealing\nalgorithm to optimise the position of the perturbation point to\nachieve stealth and robustness while ensuring the effectiveness\nof the attack. Experimental results show that LiDAttack exhibits\nexcellent performance against mainstream target detection\nmodels such as PointRCNN, PointPillar, and PV-RCNN++\non KITTI, nuScenes, and self-constructed data datasets, and\neffectively improves the robustness of the models through\nadversarial training.\nHowever, LiDAttack has some limitations. Firstly, LiDAt-\ntack's ASR decreases significantly when the angle changes,\nwhich limits its application in complex scenarios. Secondly, the\nadversarial training can only partially improve the robustness\nof the model, and there still exists the possibility of successful\nattack, which requires further research on more comprehensive\ndefense methods.\nIn order to further improve the attack effect and application\nscope of LiDAttack, future research will be conducted in the\nfollowing aspects, i.e., 1) exploring more effective local search\nstrategies, e.g., using methods such as reinforcement learning,\nto improve ASR; 2) researching more comprehensive adversar-\nial training, e.g., using meta-learning or data augmentation, to\nimprove the robustness of the model; 3) applying LiDAttack\nto other scenarios, such as drones, robots, etc., to explore its\napplication potential in more fields."}]}