{"title": "OBI-BENCH: CAN LMMS AID IN STUDY OF ANCIENT\nSCRIPT ON ORACLE BONES?", "authors": ["Zijian Chen", "Tingzhu Chen", "Wenjun Zhang", "Guangtao Zhai"], "abstract": "We introduce OBI-Bench, a holistic benchmark crafted to systematically evaluate\nlarge multi-modal models (LMMs) on whole-process oracle bone inscriptions (OBI)\nprocessing tasks demanding expert-level domain knowledge and deliberate cogni-\ntion. OBI-Bench includes 5,523 meticulously collected diverse-sourced images,\ncovering five key domain problems: recognition, rejoining, classification, retrieval,\nand deciphering. These images span centuries of archaeological findings and years\nof research by front-line scholars, comprising multi-stage font appearances from\nexcavation to synthesis, such as original oracle bone, inked rubbings, oracle bone\nfragments, cropped single character, and handprinted character. Unlike existing\nbenchmarks, OBI-Bench focuses on advanced visual perception and reasoning\nwith OBI-specific knowledge, challenging LMMs to perform tasks akin to those\nfaced by experts. The evaluation of 6 proprietary LMMs as well as 17 open-source\nLMMs highlights the substantial challenges and demands posed by OBI-Bench.\nEven the latest versions of GPT-40, Gemini 1.5 Pro, and Qwen-VL-Max are still\nfar from public-level humans in some fine-grained perception tasks. However, they\nperform at a level comparable to untrained humans in deciphering task, indicating\nremarkable capabilities in offering new interpretative perspectives and generating\ncreative guesses. We hope OBI-Bench can facilitate the community to develop\ndomain-specific multi-modal foundation models towards ancient language research\nand delve deeper to discover and enhance these untapped potentials of LMMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Oracle bone inscriptions (OBI) have been recognized as valuable records of making divination and\npraying to gods by the late Shang people from 1400 B.C. to 1100 B.C., in addition to being the earliest\nevidence of a Chinese writing system (UNESCO, 2017). Their discovery and interpretation provide\na unique window for evidence of the thought, language, society, and history of past civilizations.\nAll along, there have been several challenges from the excavation to the interpretation of OBI. First,\nafter thousands of years buried in the ground, the original OBIs are inherently eroded, fragmented,\nand susceptible to damage. Recognizing oracle bone characters from scattered pieces not only helps\nin initially determining the existence of the characters but also aids in their digital archiving with\nminimal physical handling. Second, the rejoining work of OBI is an indispensable prerequisite for\nfollow-up research, which restores the original appearance of oracle Bones and offers complete\nand accurate information for OBI scholars (Zhang et al., 2022). Current rejoining procedures are\nnon-trivial and involve time-consuming and specialized workflows (Men, 2008), thus posing demands\nto develop automatic or AI-assisted rejoining techniques. Third, OBIs exhibit a wide range of stylistic\nvariations in font and normally suffer from various types of noise in real rubbings (e.g., stroke broken,\nedge-cracked, and spindles), making it arduous to distinguish from distortions and accurately classify\nthem. Fourth, OBI retrieval, as a core component for building large-scale databases, its difficulty lies\nin differentiating the inter-class and intra-class discrepancies of oracle bone characters. Fifth, among\nthe thousands of OBIs that have been excavated, over half of them remain undeciphered (Wang &\nDeng, 2024). The traditional methods for oracle character deciphering typically involve a combination\nof historical linguistics and archaeological context. More recently, lots of efforts (Jiang et al., 2023;\nGao et al., 2024; Qiao et al., 2024; Guan et al., 2024b) have been made to leverage the representation\ncapability of neural networks so as to establish correspondences between OBIs and modern Chinese.\nHowever, many characters have evolved in meaning through cultural and historical changes, and\nnot all oracle bone characters can be translated into modern Chinese with synonymous expressions.\nAnother problem lies in the absence of a comprehensive corpus and the lack of OBI-encoding systems,\nwhich limits the applicability of language models. These five tasks are crucial steps towards placing\nan OBI both in history and within the world of the people who read and study it.\nNowadays, the emergent large multi-modal models (LMMs) have brought opportunities for solving\nmultidisciplinary tasks with powerful visual perception, understanding, and reasoning abilities (Yue\net al., 2024; Caffagni et al., 2024; OpenAI, 2024b). Meanwhile, the ability of LMMs remains unclear\non fine-grained perception and cognition (i.e., high-level perception), which play significant roles in\ninterpreting ancient texts and its associated tasks on image material pre-processing. The perception at\ndifferent granularities is strongly associated with a wide range of OBI tasks, such as distinguishing\nnoise from real oracle bone character recognition (Wang et al., 2022), marginal sealability checking\nin rejoining (Zhang et al., 2021c) and component-level structural decomposition in retrieval (Hu et al.,\n2024). Henceforth, it is worth evaluating the current abilities of LMMs in fine-grained perception\nand cognition based on actual needs to relieve extensive human resources and seek feasible solutions\nfor future OBI research.\nIn this work, we propose the first comprehensive OBI-focused benchmark, OBI-Bench, to evaluate\nthe recent LMMs in whole-process OBI tasks including recognition, rejoining, classification, retrieval,\nand deciphering under different settings. Our benchmark is constructed around a key question:\nCan LMMs aid in study of ancient script on oracle bones?\nSpecifically, we define two general evaluation principles for LMMs in targeting OBI problems:\n\u2022 Task-oriented Perception. As shown in the bottom of Fig. 1, an LMM is expected to answer\naccurately to achieve the objectives of five in-process tasks like an OBI specialist, such as\nbounding the position of each oracle bone character for the recognition task or providing a\nreasonable interpretation of OBIs for the deciphering task.\n\u2022 Spanning from excavation to synthesis. As shown in the middle of Fig. 1, an LMM should\nbe able to handle five different forms of oracle bone inscriptions and show good adaptiveness\nto the appearance or structural variants of OBI within the same task.\nTo systematically evaluate the whole-process perception ability on various visual granularity under\ndiverse OBI tasks, we collect 5,523 OBI images from 11 distinct sources. Due to the lack of publicly\navailable OBI recognition datasets on real oracle bones and OBI rejoining datasets, we propose the"}, {"title": "2 CONSTRUCTING THE OBI-BENCH", "content": "2.1 GENERAL PRINCIPLES\nFocusing on OBI Task-oriented Abilities of LMMs. Different from existing LMM benchmarks (Liu\net al., 2023b; Li et al., 2024b; Liu et al., 2024b; Yue et al., 2024) that either aim at all-round abilities\nor generic scenarios, the evaluation principles in OBI-Bench are OBI task-oriented. Specifically, we\nfocus on five major issues in the field of oracle bone inscription research: 1) Recognition, involves\nthe positioning and identification of OBI characters on different carriers. 2) Rejoining, refers to the\nprocess of stitching broken OBI fragments together to rebuild complete inscriptions. 3) Classification,\nmeans categorizing the recognized OBI characters into groups based on their shapes or meanings. 4)\nRetrieval, is to find a cognate character from large collections of OBIs. 5) Deciphering, includes\ninterpreting the meaning of the oracle bone characters even the contextual semantic information. We\nadhere to the principles in designing the visual and cognitive tasks, making the proposed OBI-Bench\na focused reflection on the OBI processing abilities of LMMs.\nCovering Multi-stage Font Appearances. To cover diverse appearances of the oracle bones since\ntheir excavation, we collect multi-sourced images for each task, as depicted in Tab. 1. In particular,\nconsidering that there are currently no available original oracle bone recognition datasets and rejoining\ndatasets, we proposed the O2BR and OBI-rejoin datasets respectively (See more details in App. B).\nDue to the specific requirement of domain knowledge in OBI research, three domain experts and one\nsenior OBI scholar were involved in the annotation process of O2BR and OBI-rejoin as well as the"}, {"title": "2.2 BENCHMARK FROM VISUAL ABILITY TO COGNITIVE ABILITY", "content": "In the five tasks of OBI-Bench, we evaluate the visual perception and high-level cognitive abilities of\nLMMs to examine whether they can handle OBI problems by using simple natural queries. For this\npurpose, we collect 5,523 images (1) from multiple sources (Tab. 1) with diverse task concerns. Then,\nwe design different question prompts (Q) based on the type of tasks and obtain the correct answer\n(A) from the corresponding dataset for each image. The 5,523 (I, Q, A) tuples compose into the first\nvisual question answering (VQA) testbench (Fig. 2) in the OBI field. Specifically, the questions in\nOBI-Bench cover two quadrants of concerns for evaluation (Sec. 2.2.1) and four question types (Sec.\n2.2.2). The details are elaborated as follows.\n2.2.1 QUADRANTS FOR EVALUATION CONCERNS\nAxis 1: From Coarse-grained Perception to Fine-grained Perception. The primary axis differenti-\nates the visual perception granularity of OBI tasks. First, coarse-grained perception focuses on the\nbasic forms, themes, or overall structures of the oracle bone inscriptions, which distinguishes OBIS\nfrom other symbols or ancient characters (Qiao et al., 2024). Second, fine-grained perception (Xing\net al., 2019; Fujikawa et al., 2023) not only involves identifying subtle characteristics, quantity, and\nspatial positions of OBIs but also reflects the high-level cognitive ability of LMMs (e.g., OBI domain\nexpertise and interdisciplinary reasoning). Several studies (Liu et al., 2023a; He et al., 2024; Tong\net al., 2024) followed this paradigm and built on it to refine pixel-level perception while extending\ninteractions beyond text-specific inputs.\nAxis 2: From Excavation to Synthesis. In the era predating the prevalence of artificial intelligence\n(AI), unearthed oracle bone fragments were normally processed through the manual rubbing and\nimage cropping processes for preservation. After thousands of years of natural weathering, corrosion,\nand man-made destruction, there are various types of noise in oracle bone rubbings. To improve the\nreadability of the original OBIs and expand the data size that could be processed by deep learning\nalgorithms, researcher obtained pseudo-oracle bone characters through handwriting or generative\nmodels (Han et al., 2020; Li et al., 2020; Wang et al., 2024b). Acknowledging these forms of OBIS,\nwe curate 5,523 OBI images from different stages of processing, that require LMMs to grasp the\ncontent or other relevant domain knowledge to answer correctly.\n2.2.2 QUESTION TYPES\nIn OBI-Bench, we design four question types, What, Yes-or-No, How, and Where, to simulate multiple\nhuman queries in various OBI tasks, each tailored to different levels of granularity as follows.\n\u2022 What Question. The What questions are a common type of query in some LMM benchmarks (Wu\net al., 2024; Zhang et al., 2024b). In OBI-Bench, they serve as global coarse-grained perception in"}, {"title": "3 EXPERIMENTS", "content": "In OBI-Bench, we evaluate 17 up-to-date popular and competitive open-source LMMs, together with\n6 proprietary LMMs, under zero-shot settings. More results and analyses are appended in App. C.\n3.1 EVALUATION ON RECOGNITION\nIn this section, we examine the recognition ability of LMMs, comparing them from coarse-grained\nglobal content perception to fine-grained OBI locating.\nSetup. To evaluate how well LMMs perform under various OBI recognition scenarios, we design four\nevaluation schemes across two image sources (i.e., original oracle bone from our self-curated O2BR\ndataset and inked rubbings from the YinQiWenYuandetection (AYNU, 2020) dataset): (1) evaluating\nvia the What query to measure the preciseness of the global perceptual descriptions on OBIs. Given\nthe commonality of contents in sampled test sets, we predefined reference descriptions for each of the"}, {"title": "3.2 EVALUATION ON REJOINING", "content": "In this section, we delve into the rejoining performance of LMMs against various eroded, tiny, and\nirregularly structured fragments of OBI, focusing on the estimation of rejoinable fragments. This task\nis also a major concern in the recovery of paper money, calligraphy, and painting files recovery.\nSetup. Given the absence of publicly available rejoining datasets in OBI domain, we create a dataset\nconsisting of 200 complete OBI pieces across two appearances (i.e., original and inked oracle bone"}, {"title": "3.3 EVALUATION ON CLASSIFICATION", "content": "In this section, we evaluate the performance of LMMs on OBI classification tasks and try to answer:\n(1) How robust are LMMs in classifying oracle bone characters? Since the intra-class and inter-class\nsimilarity of OBIs are sometimes numerically close, we then investigate the capabilities of LMMs in\nfocusing on atypical characteristics and try to answer: (2) How well do LMMs distinguish between\ndifferent categories of OBIs?\nSetup. We sample 500 images across 100 categories from each of the HWOBC, Oracle-50k, and\nOBI125 datasets as test set. One image is kept as the label in each category and the remaining four as\nquery images. To answer the above questions, we construct three evaluation scenarios: (1) evaluation\non oracle bone rubbings and handprinted images, aiming to assess the robustness of LMMs to content\nqualities. Due to the inadequacy of manual rubbing and the limitation of scanning equipment, there\nare inevitably various noises in rubbing images, which could greatly affect the classification accuracy;\n(2) evaluation on different queries (i.e., Yes-or-No question and How question) designed to generate\nabsolute and probability forms of responses, aiming to assess the robustness of LMMs under varied\nquery prompts; (3) evaluation on {10, 20, 30, 50, 100} classes that encompass mixed image sources\nand subsets characterized by similar structures, aiming to study the conditions under which LMMs\nwill fail in classification. We thereby reconstruct a variable-quantity mixed test set by sampling an\nequal number of oracle bone character classes from three datasets. More details are in App. C.3."}, {"title": "3.4 EVALUATION ON RETRIEVAL", "content": "In addition to classification, the retrieval task is also critical for building large-scale OBI image\ndatabases. Considering that LMMs combine powerful visual perception and natural language\nunderstanding capabilities, they can serve as the demand processing core of OBI retrieval systems.\nHere, we evaluate the performance of LMMs on OBI retrieval tasks given different types of queries.\nSetup. We adopt two query formats (i.e., Yes-or-No and How questions) akin to those employed in\nthe classification tasks. A total of 600 images sampled from OBC306 and OBI-IJDH datasets are\nused for evaluation. Among each test set, half of the images are used as distractors to simulate a real\nOBI retrieval scenario. During experiments, OBI images in the same category are retrieved. We take\nthe averaged Recall@k and mean Average Precision (mAP) metrics to quantify the OBI retrieval"}, {"title": "3.5 EVALUATION ON DECIPHERING", "content": "Deciphering the oracle bone inscriptions has long been the ultimate goal of this ancient language\nstudy. Large multi-modal models have strong cross-modal understanding capabilities, enabling the\nmodels to perform multilingual visual-text transformation with linguistic analysis. In this section, we\nexamine the potential OBI deciphering abilities of LMMs by asking: (1) Can LMMs interpret oracle\nbone characters correctly? (2) How do models behave in the face of different character formation\nprinciples (e.g., pictograph, ideogram, and radical-based variants) and character frequencies? (3)\nWhat is the reasoning mechanism of LMMs during the OBI deciphering process?\nSetup. To evaluate the decipherment performance of LMMs, we construct four evaluation scenarios:\n(1) deciphering oracle bone characters with different frequency levels F. Three test sets (i.e., Tier-1:\nF > 500, Tier-2: 100 < F < 500, Tier-3: 10 < F < 100) with a total of 40 characters sampled\nfrom the HUST-OBS dataset according to the taxonomy in (Liu, 2010; Chen, 2010); (2) deciphering\noracle bone characters in pictograph formation. We follow the category definitions in (Chou, 1979;\nDa WEIH, 2021) and choose 28 representative OBI characters from the EVOBC dataset as test set; (3)\ndeciphering oracle bone characters in ideogram formation. Similar to scenario (2), 22 OBI characters\nfrom the EVOBC dataset are selected; (4) deciphering oracle bone characters with structurally similar\nvariants (i.e., radical). We pick out 10 different radicals from the OBI Component 20 dataset (Hu et al.,\n2024), each accompanied by 5 component-level variants. Fig. 9, Fig. 10, and Fig. 11 in the appendix\nvisualize the sampled test sets for the above scenarios. Different from other approaches (Chang et al.,\n2022; Guan et al., 2024b) that take the decipherment of OBI as establishing correspondences with\nmodern Chinese characters, we directly use the LMMs to generate a segment of interpretation, as\nillustrated in Fig. 4. Given the absence of dedicated evaluation criteria for OBI decipherment, we\nproduce text embeddings for all decipherment results and calculate the BERTScore (Zhang et al.,\n2019) with expert-labelled golden descriptions (i.e., text references). More details are in App. B.3.\nResults. For scenario (1), Tab. 6 presents the evaluation results across different character frequencies.\nWe find that the evaluated LMMs achieve on average 0.1905, 0.1898, and 0.1791 on Tier1, Tier2,"}, {"title": "4 CONCLUSION", "content": "We provide comprehensive evaluations of the applicability of LMMs in OBI domain from different\nperspectives. Overall, there is still considerable room for LMMs to improve when addressing tasks\nthat need domain-specific knowledge. However, taking LMM as a unique technique to assist studies\non oracle bone inscriptions is a promising direction, which may significantly reduce learning costs\nfor domain professionals and help them solve OBI-related problems more easily than learning or\ntraining traditional deep learning algorithms. Additionally, there are many properties of inputs that\naffect the performance of OBI processing based on our evaluations, which is worth further exploring.\nWe hope these results can bring insights into the future improvements of LMMs for finer-grained and\nmore robust visual perception. We address the ethical concerns of our research in App. A and discuss\nthe limitations of this work in App. D."}, {"title": "A ETHICAL DISCUSSIONS", "content": "A.1 ETHICAL DISCUSSIONS OF OUR RESEARCH\nOur work holds the potential for significant social impact, both positive and negative. Firstly, the\nmain motivation of this work is that the archaeological discoveries of oracle bone inscriptions (OBI)\nare normally considered as highly serendipitous findings while their interpretation relies heavily\non the large volume of manual efforts (from excavation to digital archiving), yet most of their\nmeanings are still unknown, hindering historians and sociologists to further understand the contents\nof production and life in ancient times. In the early stage of OBI research, the deciphering of oracle\nbone characters is highly limited, either by referring to ancient books or by working backward from\nthe perspective of font evolution, which was in a fairly stagnant state for much time. Recently, with a\nlarge volume of large multi-modal models (LMMs) claiming their powerful cross-modal information\nprocessing capability, it is unknown for both computer vision experts and OBI scholars if LMMs\ncan identify and decipher them. In our work, we find while most LMMs are still not completely\napplicable in some OBI processing tasks that need fine-grained perception and deeper cognition, it\nexhibits a trend that the performance of the model improves year by year and relies on its design for\nspecific tasks. Our findings offer a promising direction that using LMMs to tackle the OBI problems\nhas a lower knowledge cost than the traditional model-task in one-to-one form on certain occasions.\nSecondly, our research on the model applicability in OBI tasks provides a necessary understanding\nof the nature and potential causes of the inferior perception and cognition abilities of LMMs in the\nknowledge-specific domain. The evaluation of model performance and the subsequent discoveries\nwould spark a broader discussion on the rational use of LMMs to facilitate the study of ancient\ncharacters. Our work could serve as a reference point for discussions on developing next-generation\nLMMs and evaluation metrics for the OBI area. Thirdly, we acknowledge that unregulated and\nunlimited deciphering results of LMMs may present potential risks in generating misinformation and\nspreading false cultural values.\nA.2 ETHICAL DISCUSSIONS OF DATA COLLECTION\nIn this section, we detail the ethical concerns that might arise during the experiments. Specifically, we\ninvite five Chinese college students with no background in the ancient Chinese language to represent\npublic-level performance. They are instructed to perform two tasks that are of great interest to OBI\nscholars, i.e., recognition and deciphering. Note that all participants are clearly informed of the"}, {"title": "B MORE INFORMATION ON BENCHMARK DATASETS", "content": "B.1 THE PROPOSED O2BR DATASET\nThe objective of recognizing oracle bone inscription lies in accurately locating each single character on\noriginal oracle bones, rubbings, or fragments, thus assisting downstream classification or deciphering\ntasks (Xing et al., 2019). In the past decade, a variety of datasets (Guo et al., 2015; Huang et al., 2019;\nHan et al., 2020; AYNU, 2020; Zhang et al., 2021a; Yue et al., 2022; Wang et al., 2022) have been\nestablished for oracle bone character recognition. Among them, Oracle-20k (Guo et al., 2015) was\nproposed under a controllable lab environment with a total of 20,039 synthesized OBI images in 291\ncategories. Later, Oracle-50k (Han et al., 2020) was released with more diverse character classes and\nimages. In (Zhang et al., 2021a), a large-scale dataset with character-level annotations, OracleBone-\n8000, was collected, which contains 7,824 OBI rubbing images scanned from (Guo, 1982) and\n128,770 annotated character crops. Similarly, YinQiWenYuan-detection (AYNU, 2020) dataset\ncontributed 9,306 annotated OBI rubbing images with character-level bounding boxes. However, the\nimage content of the above OBI recognition datasets are all handprinted or rubbing form, rather than\noriginal oracle bone. More importantly, the majority of datasets either lack the location information\nfor individual oracle characters or do not release the data publicly. To address these problems, we\npropose the Original Oracle Bone Recognition (O2BR) dataset, which consists of 800 carefully\ncurated original oracle bone images and 4,211 bounding boxes.\nB.1.1 SOURCE CONTENT COLLECTION\nTo curate the O2BR dataset, we began with the initial filtering of available websites using hashtags\ncommonly associated with oracle bone image, such as #oracle bone archaeology, #shell\nand bone script,and #ancient character. This process allowed us to obtain a set of\ncandidate websites on oracle collection supported by civic organizations, museums, and research\ninstitutes around the world. Considering the type, quantity, and usage requirements of the images,\nwe choose to download original oracle bone images from the website\u00b9 of open museum led by the\nInstitute of History & Philology, Academia Sinica. Out of the potentially accessible retrieved results,\naround thirty thousand contained downloadable and visible OBI photos as of July 2024. We utilized\na Python-based crawler to download a total of 21,453 raw images. Note that all images downloaded\nfrom the website are released under an appropriate Creative Commons (CC) license that allows\nfurther editing and redistribution. After that, two graduate students with oracle research backgrounds\nwere involved in removing small fragments that do not contain complete oracle bone characters.\nFinally, 800 original oracle bone images are selected as the annotation source. Fig. 6 presents several\nsamples of this dataset. Tab. 7 provides detailed statistics.\n\u0392.1.2 ANNOTATION PROCESS\nSome studies (Li et al., 2020; Diao et al., 2023) suggested the use of\nautomatic object detection methods to save manual annotation effort.\nHowever, considering the particularity of the image content in this\ndataset and the lack of relevant trainable datasets, we adopted the\ncommonly used toolbox \u201cLabelImg", "Catalog of Oracle Bone Rejoinings\" (Tsai,\n1999), \\\"Zui Gu": "Research on Oracle Bone Rejoinings"}, {"title": "B.2.1 SOURCE CONTENT COLLECTION", "content": "We used a Python-based crawler to scrape metadata from the official website\u00b2 of the Pre-Qin Research\nOffice at the Institute of History, Chinese Academy of Social Sciences. This process covered 217"}, {"title": "B.2.2 ANNOTATION PROCESS", "content": "Here, we refer to the OBI rejoining results as \"map\" to reflect its special role in the construction\nprocess of OBI-rejoin dataset. Three domain experts in OBI research and one senior OBI scholar\nwere involved in the annotation process of OBI-rejoin. Each annotator was equipped with a PC\nwith commercial photo editing software. Fig. 7(b) illustrates the annotation interface. To precisely\nreproduce the authentic conditions of oracle bone fractures, the annotator manually tears \"maps\"\non appropriate borders utilizing their domain expertise to induce central or peripheral fractures. A\ndouble-check procedure was applied after the annotation to ensure quality and rationality. Overall, it\ntook each annotator approximately 3 hours to finish annotation, and the whole construction process\ntook over a week to complete. A lot of effort and discussion were required in order to manually\nretrieve suitable rejoining results from the crawled metadata. To the best of our knowledge, OBI-\nrejoin is the first open-source oracle bone inscription rejoining dataset. We hope that OBI-rejoin\ncan facilitate future development of advancing OBI rejoining algorithms and serve as a benchmark\ndataset to evaluate the performance of rejoining methods.\nB.3 COLLECTING GOLDEN DESCRIPTIONS FOR DECIPHERING TASK\nThe existing OBI deciphering works merely decipher oracle bone characters by linking them to\nmodern Chinese characters (Chang et al., 2022; Guan et al., 2024b), which does not constitute a true\ndecipherment. With the historical and cultural changes, many oracle bone characters can no longer\nfind correspondences from modern Chinese characters. Thus, we turn to the most traditional dictionary\nform and extract golden descriptions from an authoritative Chinese character database (CUHK, 2014)."}, {"title": "C ADDITIONAL EXPERIMENTAL RESULTS", "content": "C.1 BENCHMARK CANDIDATES\nTo ensure the comprehensiveness and timeliness of the results, we select 23 up to date and prevailing\nLMMs for evaluation. The proprietary LMMs include Gemini 1.5 Pro (Reid et al., 2024), Gemini\n1.5 Flash (Reid et al., 2024), GPT-4v (Achiam et al., 2023), GPT-40 (OpenAI, 2024a), Qwen-VL-Max\n(Bai et al., 2023), and GLM-4v (GLM et al., 2024). The open-source LMMs include xGen-MM-\ninstruct-interleave (Xue et al., 2024), mPLUG-Owl3-7B (Ye et al., 2024), MiniCPM-V 2.6-8B (Yao\net al., 2024), Moondream2-1.6B (Moondream.ai, 2024), InternVL2-Llama3-76B (Chen et al., 2024a),\nInternVL2-40B (Chen et al., 2024a), InternVL2-8B (Chen et al., 2024a), GLM-4V-9B (GLM et al.,\n2024), CogVLM2-LLaMA3-Chat-19B (Wang et al., 2023), LLaVA-NeXT-72B (Li et al., 2024a),\nLLaVA-NeXT-8B (Li et al., 2024a), IDEFICS-2-8B Lauren\u00e7on et al. (2024), DeepSeek-VL-7B (Lu\net al., 2024), InternLM-XComposer2-VL-7B (Dong et al., 2024), LLaVA-v1.5-13B (Liu et al.,\n2024a), LLaVA-v1.5-7B (Liu et al., 2024a), and Qwen-VL (Bai et al., 2023). Tab. 9 summarizes and\ncompares the vision and language architectures of open-source LMMs.\nAdditionally, considering the cultural uniqueness of OBI, and to ensure the representativeness of\nhuman performance on the OBI-Bench, we recruited five Chinese college students with no background\nin ancient Chinese language to reflect public cognition. Initially, we instructed all participants to have\na clear and consistent understanding of all question-answer patterns by familiarizing themselves with\nthe tasks through exposure to similar cases. Note that we conduct user studies in the same order of\ntasks as we test LMMs to avoid interference between tasks. Given the inherent variability of LMMs,\nidentical prompts can yield responses. To address the impact of such situations on our evaluation, we\nimplemented a 5-round average strategy.\nC.2 ADDITIONAL DETAILS OF EVALUATION ON RECOGNITION\nC.2.1 EVALUATION CRITERIA\nFor What question, we calculate the cosine similarity between the embeddings of output answers and\nthe golden descriptions. Specifically, we use the Sentence-Transformers python library and\nthe all-MiniLM-L6-v24 model, which maps sentences to a 384 dimensional dense vector space.\nFor How question, we use the relative counting error metric to measure the performance of different\nLMMs:\nMRE = 1/N \u2211 (\\frac{|Ctruth - Cpred|}{Ctruth})\nwhere N is the number of evaluated OBI images. Ctruth and Cpred denote the actual number of\noracle bone characters in the image and the predicted number of oracle bone characters respectively."}, {"title": "C.2.2 EXTENDED RESULTS ON YES-OR-NO QUESTION", "content": "Considering that the test set used in the recognition task is composed solely of authentic oracle\nbone rubbings or fragments", "1": 1.0}]}