{"title": "Temporal Heads: Where Language Models Recall Time-specific Information", "authors": ["Yein Park", "Chanwoong Yoon", "Jungwoo Park", "Minbyul Jeong", "Jaewoo Kang"], "abstract": "While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads primarily responsible for processing temporal knowledge through circuit analysis. We confirm that these heads are present across multiple models, though their specific locations may vary, and their responses differ depending on the type of knowledge and its corresponding years. Disabling these heads degrades the model's ability to recall time-specific knowledge while maintaining its general capabilities without compromising time-invariant and question-answering performances. Moreover, the heads are activated not only numeric conditions (\"In 2004\") but also textual aliases (\u201cIn the year ... \"), indicating that they encode a temporal dimension beyond simple numerical representation. Furthermore, we expand the potential of our findings by demonstrating how temporal knowledge can be edited by adjusting the values of these heads.", "sections": [{"title": "Introduction", "content": "\u201cRemembrance of things past is not necessarily the remembrance of things as they were.\u201d (Proust, 1992)\nThis profound and intricate relationship between memory and truth resonates deeply with one of the central challenges in modern artificial intelligence. While large language models (LLMs) like GPTS (OpenAI, 2022, 2024a,b) and LLaMA familiess (Touvron et al., 2023a,b; Dubey et al., 2024) have demonstrated remarkable capabilities in leveraging factual knowledge, they face a unique challenge that mirrors human memory: the accurate representation of temporal knowledge\u2014facts that transform across different time points.\nUnlike static facts (e.g., \"The capital of France is Paris\"), many real-world facts change over time (e.g., a politician's term in office, a sports player's team membership in a given year). This time-evolving nature necessitates that LLMs accurately capture such change. To do so, they must not only track newly updated facts within a specific timeline, but also retain historical information across different time periods (Jang et al., 2022). This presents a significant challenge, as models must contend with tracking and reasoning over temporal changes in knowledge (Kasai et al., 2023). However, beyond prompting (Mitchell et al., 2022; Park et al., 2025) or retrieval-augmentated generation (Lewis et al., 2020; Gutierrez et al., 2024), the internal mechanisms by which models adapt to temporally evolving facts remain relatively underexplored.\nEmpirical observations suggest that LLMs already possess some level of temporal awareness (Nylund et al., 2023; Mousavi et al., 2025). This raises the question of whether the model is inherently capable of encoding and utilizing temporal knowledge. For instance, when prompted with time-specific queries like \u201cIn 1999, [X] was a member of sports team\u201d, the model may generate"}, {"title": "Preliminaries", "content": "In this section, we provide background on the Circuit Analysis (Olah et al., 2020; Nanda et al., 2023; Conmy et al., 2023), which represents the model's computation through structured subgraph of its components."}, {"title": "Circuit Analysis", "content": "Circuit analysis represents a transformer's computation as a directed acyclic graph (DAG) G = (N, E), where each node in N corresponds to a distinct component in the model: attention heads $A_{l,j}$ (at layer l and head j), MLP modules $M_l$ for each layer, the input node I (embeddings), and the output node O (logits). Thus, we formally define the set of nodes as:\n$N = {I, A_{l,j}, M_l, O}$.\n(1)\nThe edges in E represent residual connections that propagate activations between these nodes:\n$E = {(n_x, n_y) | n_x, n_y \\in N}$.\n(2)\nA circuit is defined as a subgraph $C \\subseteq (N, E)$ selected to explain a specific behavior of interest-for instance, how certain tokens influence the model's output or how factual knowledge is stored and elicited. By examining which nodes and edges are crucial for producing a particular prediction, we can identify the subgraph (the circuit) that governs each behavior."}, {"title": "Knowledge Circuit", "content": "A knowledge circuit (Yao et al., 2024) focuses on how a model treats the subject s, and relation r to generate the object o using a knowledge triplet (s, r, o). By systematically ablating (i.e. zeroing) parts of the model, it identifies the crucial nodes responsible for this generation and constructs a subgraph $KC \\subseteq (N, E)$ whose removal breaks the model's ability to produce the correct object. Concretely, it define a performance metric as:\n$S(e_i) = log(p_G(o | s,r)) - log(P_{G/e_i} (o | s,r))$.\n(3)\nwhere $P_{G/e_i}$ denotes the model's probability of next-token prediction after ablating (i.e. zeroing) the activation of a node or edge $e_i$. If $S(e_i)$ exceeds a threshold \u03c4, $e_i$ is deemed critical and retained in KC; otherwise, $e_i$ is pruned. This yields a minimal"}, {"title": "Knowledge Circuit Deciphers Temporal Head in LLMS", "content": "We now explore how knowledge circuits, extracted via EAP-IG pruning, can reveal specialized Temporal Heads in large language models (LLMs). We extend knowledge circuits in 2.2 to temporal knowledge circuits by analyzing how the same subject-relation pair can produce different objects across multiple time points. Specifically, we seek to identify which edges encode time-dependent specificity, such that an edge $e_i$ is crucial for predicting the time-relevant object $o_k$ at period $T_k$. Given a knowledge circuit score $S(e_i)$ (Eq. 3), we define its temporal variant as follows:\n$S(e_i, T_k) = log P_G(o_k | s, r, T_k) - log P_{G/e_i} (o_k | s,r,T_k) > \\tau$.\n(4)\nwhere $T_k$ indicates a specific time (or period), and $o_k$ is the corresponding object for subject s and relation r at time $T_k$. Thus, $S(e_i, T_k)$ measures the contribution of edge $e_i$ to correctly predicting $o_k$ under time $T_k$. For highlighting importance and simplifying graphs, edges retained in the temporal circuit satisfy $S(e_i, T_k) > \\tau$, ensuring they encode time-dependent knowledge. Here, we decide to attach temporal conditioning in front of subject, following prior insight from causal tracing (\u00a78.2) and details in Appendix 8.3."}, {"title": "Implementations", "content": "We conduct experiments primarily on three LLMs: Llama-2-7b-chat-hf (Touvron et al., 2023b), Qwen1.5-7B-Chat (Bai et al., 2023; Team, 2024), Phi-3-mini-4k-instruct (Abdin et al., 2024). We adopt transformer lens (Nanda and Bloom, 2022) to intercept and ablate model components, enabling EAP-IG-based circuit discovery. We mainly illustrate results on Llama2, though similar trends emerge in the other models. More details are described in Appendix 8.1.1."}, {"title": "Circuit Reproduction Score", "content": "To evaluate how well a pruned circuit reproduces the full model's behavior, we define the Circuit Reproduction Score (CRS), ranging from 0 to 100. Let B be the baseline performance of the full model on time-conditioned prompts, and P be the performance of the pruned circuit. If the pruned circuit maintains or exceeds the baseline performance (P > B when B > 0), we assign it the maximum CRS as follows:\n$CRS(B, P) = 100$.\n(5)\nOtherwise, the score follows an exponential decay:\n$CRS(B, P) = 100 \\times \\sigma exp(-\\frac{d}{|d|})$, (6)\nwhere d = max{B, 0}. The factor $\u03c3\u2208 (0, 1]$ accounts for sign mismatches, adjusting for cases where the pruned circuit's output deviates in direction from the full model. A higher CRS indicates better reproduction of the full model's predictions. Details on hyperparameters and adjustments are deferred to the Appendix 8.4."}, {"title": "Dataset", "content": "Our dataset comprises:\n\u2022 Temporal Knowledge: Various categories of knowledge samples that embed a specific year (e.g., 1999, 2004, and 2009) alongside a factual statement (e.g., which sports team or president is correct in that year) based on Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014).\n\u2022 Time-Invariant Knowledge: Commonsense data from LRE (Hernandez et al., 2024) (e.g., object superclass, fruit inside color), plus newly implemented numerical facts embedded in subject/object (e.g., geometric shape or roman numerals). These tasks assume no explicit time-based shift.\n\u2022 Unstructured QA: We utilize TriviaQA (Joshi et al., 2017) and Math (Wang, 2022) QA in ChroKnowledge (Park et al., 2025) for unstructured, general QA to verify the ablation effect with basic LLM's tasks.\nFor each data point, we run both a clean prompt and a corrupted prompt, following EAP-IG guidelines. We focus on the first token(s) that differ, capturing the key transition that determines correctness. In"}, {"title": "Findings", "content": "We now identify common nodes in all circuits (e.g., [input], [logits], MLP m2, m24, m30, etc.) and a set of temporal-only nodes that appear exclusively in circuits for year-dependent prompts as in Figure 2. Firstly, most MLP nodes were appeared both temporal and time-invariant knowledge, as they are activated for storing knowledge (Geva et al., 2021; Dai et al., 2022; Niu et al., 2024).\nWhat is impressive stood out in the attention heads. Temporal Heads, appearing in almost every temporal knowledge circuits but not time invariants, are shown: a15.h0, a18.h3 in Llama2. Those temporal heads reoccur across multiple year-specific circuits, and it is different for other model's cases like a17.h15 for Qwen 1.5 in Table 2. Visualizing their attention maps in Figure 2 (C) indicates a strong focus on \u201cIn 19xx\u201d and subsequent subject phrases, as key tokens revolve around temporal conditions with queries hooking into the subjects. This pattern corroborates the idea that these heads facilitate year-subject binding-justifying the label \"temporal\", as this kind of task specific attention heads were previously suggested by Wang et al., 2023; Merullo et al., 2024; Chughtai et al., 2024; Wu et al., 2024 and Zheng et al., 2024.\nWhen lowering the ratio of exhibition (e.g., 70-80%), additional heads (e.g., a0.h15, a20.h17, a31.h25) emerge. These Backup Temporal Heads are also exclusive to temporal knowledge circuits, though their emerging varies different among types of knowledge and years. But interestingly, even at high ratio, no heads are exclusive in time-invariant knowledge circuits. This suggests that many \u201cgeneral knowledge\" heads overlap with or are reused by knowledge recalling tasks, whereas certain specialized heads exist only for time-based tasks.\nIn the next (\u00a74), we delve into further ablation experiments to verify that ablating temporal heads indeed degrades year-specific predictions, reinforcing their role as the crucial channel through which the model recall knowledge conditioned on time."}, {"title": "In-Depth Analysis of Temporal Heads", "content": "We conduct a more fine-grained analysis to understand how temporal heads identified in the extracted circuits impact final predictions, especially for temporally changing facts. Drawing inspiration from Borchmann 2024 on log-probability based evaluation, we perform targeted Attention Head Ablation Inference (\u00a74.1) to observe how the model's"}, {"title": "Attention Head Ablation Inference", "content": "Motivation While temporal knowledge circuit construction based on EAP-IG pruning (\u00a73) reveals the structure of temporal knowledge processing, we still need direct evidence that certain \"temporal heads\" genuinely mediate year-based predictions. We adopt a hard-coded approach that sets the selected attention head's output weights to zero, thus preventing it from contributing to the residual stream. We then measure changes in the model's log probability for the correct target object vs. competing objects in different time.\nLog Probability Variation Following Borchmann 2024, we assess temporal knowledge retention by evaluating changes in object probabilities under head ablation. Let O be the set of all candidate objects (e.g., teams, presidents) in the time range, and $p(o | s, r, T)$ the model's probability of selecting object o from subject s, relation r and time T. The model's default choice is labeled Target if it matches the correct temporal fact, otherwise Non-Target. After ablating suspected temporal head(s), we recompute object probabilities:\n$z_o = log p_{ablate}(o | s, r,T)$,\n(7)\n$p_o = \\frac{exp(z_o)}{\\sum_{o' \\in O} exp(z_{o'})}$.\n(8)\nwhere $p_{ablate}$ denotes the log-probability computed by forward pass of model, ablating corresponding heads. This evaluates how the probability distribution over O shifts, rather than just predicting the most likely answer. Details in Appendix 8.6."}, {"title": "Result of Temporal Knowledge", "content": "As shown in Figure 3 (A), ablation significantly reduces log probability for the correct year-specific Target in temporal tasks. When ablating a15.h0 or a18.h3 or both of them, the model frequently chooses Non-Target objects from O (e.g., a president of different year). Not just raising of those percentage, specific attention heads influence each years differently; some are more critical for 1999, while others have a stronger effect in 2004 or 2009. For instance, ablating a18.h3 significantly impacts 2004 but has a lesser effect on 2002.\nFigure 3 (B) illustrates the varying degrees of performance degradation across different years. The red arrows highlight these degradation levels, where darker and thicker arrows indicate a more pronounced effect of ablation. Notably, around object transition periods (e.g., between 2002-2003"}, {"title": "Result of Time Invariant Knowledge", "content": "By contrast, ablating the same heads for invariant knowledge (e.g., fruit inside color) causes minimal performance drop in Table 2 and Figure 4. This indicates that \"temporal heads\" indeed route only temporally conditioned knowledge, and disabling them forces the model to make temporally incorrect rather than incorrect of stable knowledge. Besides, Phi-3-mini affects more sensitively than others as its parameter size is half of other two models, resulting more reactive to small changes in attention alignment. This even causes a slight gain of performance in time-invariant knowledge tasks."}, {"title": "Result of General QA", "content": "As Table 2 and result in Appendix 8.7 shows, ablating temporal heads doesn't harm common knowledge recalling or answering general knowledge questions. Here, we test TriviaQA and Math ChroKnowledge and find out that just ablating temporal heads doesn't effect the performance of basic QA, droping almost less than 0.6 in f1 score."}, {"title": "Alias Test With Textual Conditioning", "content": "In previous findings of Section \u00a73.4, we experimented with cases where numeric values were present either in the prompt (Roman Numerals) or in the answer object (Geometric Shape) under time-invariant conditions (like \u201cTriangle has 3 sides\"). For all scenarios, temporal heads did not emerge, suggesting that their activation is not merely a response to numerical information but rather specific to temporal knowledge processing. We further investigate whether these same heads appear for less direct numeric conditioning. Instead of a literal \u201cIn 2004\u201d prompt, we use \u201cIn the year the Summer Olympics were held in Athens\u201d or \u201cFor his first,\u201d providing an indirect textual condition referencing the relevant time. We again construct knowledge circuits and observe which heads surpass threshold.\nSuch \"alias\" statements yield smaller CRS (e.g., 40.3 in president cases), though, temporal heads still appears. These heads may not always exceed normal threshold (e.g. \u03c4 = 0.1), they still register moderate importance. Coupled with results from the numeric \"In 2004\" prompt, this indicates that those heads do not rely solely on numeric tokens, but also respond albeit less strongly to textual or event-based temporal conditioning. This further validates that they encode a temporal dimension, rather than merely responding to arbitrary numbers. Visualized results are in Figure 14 of Appendix."}, {"title": "Temporal Knowledge Editing", "content": "Lastly, we explore an approach to confirm that injecting or amplifying temporal head's attention value can effectively \"edit\" year-specific knowledge as in Figure 5. Given a source_prompt (where the model is confident about a certain year's fact) and a target_prompt (where it confuses the same year) based on log probability results, we:\n1. Extract the value of attention head asrc from the source_prompt at a chosen layer/head (e.g. a18.h3).\n2. Average over total source prompts (e.g., \"In 2009, the name of president of South Korea was\").\n3. Inject the modified attention value into the target_prompt at the corresponding temporal token position, scaled by a coefficient \u03bb:\nDetails of adding an attention is in Appendix 8.8.\nThis modification is applied dynamically using a forward hook mechanism at inference time, preserving the overall model parameters while selectively influencing time-conditioned factual recall. We test it with model wrong answer in a normal condition, varying the injection coefficient across three cases (X = 1,3, 6), following Turner et al., 2023; Rimsky et al., 2024, which emphasized its impact."}, {"title": "Related Works", "content": "Despite advancements in LLMs, handling temporal knowledge remains a key challenge. While prior works focus on factual consistency (Petroni et al., 2019; Kassner and Sch\u00fctze, 2020) or refining model editing in MLP layer (Mitchell et al.; Meng et al., 2022; Meng et al.), few address how facts evolve over time. Studies on time-aware QA and temporal probing (Chen et al., 2021; Zhang and Choi, 2021; Dhingra et al., 2022; Jang et al., 2022) reveal that LLMs struggle with dynamically shifting facts. Recent approaches attempt explicit temporal alignment (Kim et al., 2024; Zhao et al., 2024; Mousavi et al., 2025; Park et al., 2025), but have focused on external evaluations. Our findings highlight that LLMs encode temporal facts implicitly, relying on manipulable attention heads, underscoring the need for better temporal supervision and disentangled knowledge representations."}, {"title": "Attention Heads in Language Models", "content": "Under mechanistic interpretability (Olah et al., 2020; Vig et al., 2020; Sharkey et al., 2025), researches about attention heads were done by Voita et al., 2019; Wang et al., 2023; McDougall et al., 2024, showing off specific heads that copy key tokens to the output, ensuring consistency in transformers. These Mover Heads are a kind of induction heads (Olsson et al., 2022) moving syntactic information (Ferrando and Voita, 2024). Other works were followed as finding out retreval heads (Wu et al., 2024), heads for semantic information for color (Merullo et al., 2024), or subject and relation (Chughtai et al., 2024). Those various kinds of attention heads attend to critical tokens and directly influence the logits by writing their embeddings into the residual stream (Zheng et al., 2024).\nExperiments show that ablating those heads significantly disrupts tasks like syntactic induction or semantic information understanding, highlighting their specific roles. A special case, Backup Heads, remains inactive under normal conditions but replicates task specific head functionality when primary heads are ablated. This ensures model robustness by maintaining token copying behavior even when key circuit components are disrupted. We treat founded temporal attention heads as a subcategory of semantic heads like subject heads and relation heads (Chughtai et al., 2024) in our experiments."}, {"title": "Conclusion", "content": "We systematically investigate how LLMs can handle temporal knowledge, focusing on time-dependent facts. Through our experiments, we uncovered Temporal Heads that selectively mediate the activation of time-variant knowledge. Ablating these heads leads to temporal mismatches while leaving time-invariant knowledge and general QA performance unaffected. Note that these heads are also activated under textual conditioning, and using their value for editing successfully changes the models' responses with minimal intervention.\nAs a foundational step, our work explores how LLMs can actively manage temporal information rather than merely integrating temporal context. We believe our analysis offers valuable insights into the inner mechanisms of LLMs and can inspire future approaches for time-aware model alignment and precise temporal updates by selectively targeting temporal heads, rather than relying on global retraining."}, {"title": "Limitations", "content": "While our approach demonstrates promising results in identifying and analyzing temporal knowledge circuits, we acknowledge some limitations in our current work. First, analysis of unstructured temporal QAs like General ChroKnowledge (Park et al., 2025) were constrained, as the underlying multiple-choice options in those tasks typically do not exhibit temporal dependencies. So we focused more on our temporal knowledge dataset, abundantly describing the effect of ablation in these cases.\nOn the other side, as EAP-IG didn't support models with Grouped-Query Attention (GQA), which cannot use the split_qkv_input option, our main analysis exclude those models like Llama-3-8B-Instruct (Dubey et al., 2024). Still, we checked their results and found that even their CRS is not quite enough and their circuit construction is not detailed, temporal heads are still could be founded: a18.h15 and a23.h26."}, {"title": "Appendix", "content": null}, {"title": "Effective Attribution Pruning-Integrated Gradients (EAP-IG)", "content": "We perform Effective Attribution Pruning (EAP) by ablating (zeroing) candidate edges and measuring the drop in correct predictions following Hanna et al., 2024. In tandem, we use Integrated Gradients (IG) to capture gradient-based contributions:\n$IG(z, z') = \\int_{0}^{1} \\frac{\\partial}{\\partial \\alpha} L(z+\\alpha(z-z')) d\\alpha$,\n(9)\nwhere L is the loss (e.g., negative log-likelihood), and z' a baseline embedding or activation. Furthermore, not just combining signals to rank each node/edge by its importance, we extend EAP-IG to time-sensitive knowledge. We construct temporal knowledge circuits by analyzing variations across different years $T_k$. For a given (s, r) pair:\n\u2022 Clean input: (s, r, $o_t$) where $o_t$ is correct at $T_t$.\n\u2022 Corrupted inputs: (s, r, $o_{t'}$) where $o_{t'}$ is the correct object for a different time $T_{t'}$ \u2260 $T_t$.\nRather than treating $o_{t'}$ as incorrect, we leverage the contrast between different valid temporal associations to isolate time-dependent components. An edge $e_i$ is retained in the temporal circuit if:\n$S(e_i, T_k) = log p_G(o_k | s,r,T_k) - log P_{G/e_i} (o_k | s,r,T_k) > \\tau$.\n(10)\nThis identifies edges that encode temporal specificity rather than general factual associations. By ablating edges across different $T_k$, we verify if disruptions occur primarily at the corresponding time while preserving outputs for other years. This ensures the extracted circuits genuinely reflect temporal dependencies."}, {"title": "Implementation Details in EAP-IG", "content": "In each model's configuration, we set split_qkv_input to true in transformer lens (Nanda and Bloom, 2022), ensuring attention heads are disentangled enough for targeted pruning."}, {"title": "Causal Tracing", "content": "Causal Tracing (Vig et al., 2020; Meng et al., 2022) aims to reveal which hidden states in an autoregressive Transformer cause correct recall of a fact. Let a fact be (s,r,o) (e.g., (L. Messi, sports_team,Newell's Old Boys)), and time T (e.g., In 1999). We construct a prompt p (e.g., \"In 1999, Lionel Messi was a member of sports team ...\") and measure the model's probability of generating o at output:\n$P_{clean}(o) = G(p)$,\n(11)\nwhere G is the Transformer. Next, we create a corrupted prompt p' (e.g., replacing \u201cLionel Messi\" with a fake name). Denote the model's probability,\n$P_{corr} (o) = G(p')$.\n(12)\nBecause key information is obfuscated, $P_{corr}(o)$ typically drops. Finally, in the corrupted-with-restoration run, we overwrite certain hidden states in the corrupted run with their clean-run counterparts:\n$P_{restored}(o) = G_{restore} (p', \\{h_{clean} \\})$,\n(13)\nwhere $h_{clean}$ are layer-l hidden states from the clean run. If restoring layer l significantly boosts Prestored(o), those states at layer l are causally important for retrieving the fact. Applying this procedure to time-conditioned facts (e.g., specifying \u201cIn 1999,\u201d \u201cIn 2009,\u201d etc.) localizes temporal knowledge within specific tokens and layers."}, {"title": "Where Does Temporal Condition Exert Influence on Knowledge Triplets?", "content": "We next investigate precisely where a temporal cue, such as \"In 1999,\" or \"In 2004,\" exerts its main influence within the triplet (s,r, o). To this end, we adopt a causal-tracing approach (inspired by ROME (Meng et al., 2022)) targeted at isolating temporal effects. Specifically, we compare two prompts:"}, {"title": "Year-Based Causal Tracing of Subject Tokens", "content": "Heatmap Illustrations The top 6 plots in Figures 6 depict example heatmaps for single-layer restoration (left) vs. MLP-interval and Attention-interval restoration (center, right). Each subplot visualizes how restoring a given layer (or set of layers) changes the probability of a target answer (e.g., p(New) or p(Barcelona)). Darker regions indicate larger improvements in the model's correctness after that restoration. We compare:\n\u2022 Top row: Restoration effect on p(New) or p(Barcelona) for different single or grouped layers, showing which layers are most responsible for selecting a new or correct team.\n\u2022 Bottom row: Similar restoration but for alternative completions (e.g., p(2) or p(Lion)), revealing how subject or year tokens can shift the model's internal preference.\nWe observe that certain mid-range layers, especially around 10\u201320, exhibit strong spikes: when we restore those layers' subject-year hidden states, the model reverts to a correct or plausible answer for the year-specific query.\nAs hinted by the heatmaps:\n\u2022 The largest gain in correct probability typically occurs after restoring subject+year hidden states. If corrupted, the model confuses or misaligns the year with the wrong subject, yielding off-target outputs (e.g. a different team or a random hallucination).\n\u2022 Other tokens (relation or object) produce smaller jumps when restored. Although they"}, {"title": "Year-Based Causal Tracing of Relation and Object Tokens", "content": "The middle and lower side six plots in Figures 6 replicate the above procedure for relation tokens (e.g., \"was a member of\") and object tokens (e.g., a team name). The heatmaps show weaker or narrower restoration effects when the year corruption is placed near those tokens:\n\u2022 Relation tokens only yield modest probability recovery upon restoration, implying that while they shape the factual link, they do not anchor the time dimension.\n\u2022 Object tokens affect final correctness but appear less coupled to the year. Overwriting their hidden states helps for precise object naming, yet does not fix when an event is said to occur."}, {"title": "Implications for Temporal-Subject Coupling", "content": "In line with prior studies (Meng et al., 2022), these findings confirm that the temporal aspect is mainly fused into the subject representation\u2014the model effectively treats \u201c(Subject in Year)\" as a unique entity. Restoring the subject+year region of hidden states yields the greatest improvement, implying that year tokens attach strongly to the subject slot. Conversely, relation and object tokens are comparatively less sensitive to time cues."}, {"title": "Limitations of Causal Tracing Alone", "content": "Despite highlighting which layer or token positions matter, causal tracing alone cannot pinpoint which heads or MLPs form the circuit that routes these time signals. For instance, a single layer might have multiple attention heads with different behaviors; or an MLP might selectively process the year dimension but remain obscure at the token-level. As we explore in (\u00a73), adopting a circuit-level perspective unveils specific Temporal Heads that systematically propagate year-conditioned knowledge throughout the model."}, {"title": "Details of Circuit Reproduction Score", "content": "CRS condenses relative performance differences and sign alignment into a single, intuitive 0\u2013100 metric, offering a streamlined assessment of circuit quality."}, {"title": "Motivation", "content": "Existing approaches such as logit diff or MatchNLL (Conmy et al., 2023; Yao et al., 2024) evaluate circuits by reporting two separate numbers: the baseline performance of the original model and the circuit's performance. However, this can obscure direct comparisons, especially when values are of different scales or signs. To address this, we introduce the Circuit Reproduction Score (CRS), a unified metric that normalizes these comparisons onto a 0\u2013100 scale. A score of 0 indicates a circuit that fails to retain meaningful model behavior, while 100 signifies equal or superior performance compared to the original model."}, {"title": "Definition", "content": "Let B represent the baseline performance of the original model and P the circuit's performance. CRS is computed as:\n$CRS(B, P) = 100 \\times S(B, P) \\times D(B, P)$,\n(14)\nwhere:\n\u2022 S(B, P) \u2208 (0, 1] is a sign-based adjustment factor.\n\u2022 D(B, P) = exp(-aR) scales based on deviation R.\n\u2022 a controls the sensitivity to deviations."}, {"title": "Handling Positive and Negative Baselines", "content": "\u2022 If B > 0 and P > B, CRS is 100, indicating that the circuit fully retains or improves upon original performance.\n\u2022 If P < B, the CRS score is exponentially reduced based on the relative performance gap.\n\u2022 If B < 0 (indicating the original model performed poorly), less negative performance is treated as an improvement.\n\u2022 If B and P differ in sign, CRS applies an intermediate weighting (e.g., 0.6-0.8) to avoid misleadingly high scores."}, {"title": "Implementation", "content": "We compute:\n$B = eval_{baseline}(G, D_{val}, logit_{diff})$,\n(17)\n$P = eval_{graph}(G, P, D_{val}, logit_{diff})$.\n(18)\nThese yield average performance values, which are then converted into:\n$CRS = one_{score}(B, P; \\alpha, S) \u2208 [0,100]$.\n(19)\nThe resulting CRS provides a concise and interpretable measure of circuit faithfulness:\n\u2022 Both negative: The circuit's score is capped (e.g., at most 100 \u00d7 0.5).\n\u2022 Both positive: The circuit may reach 100 if it fully retains baseline performance.\n\u2022 Mixed sign: An intermediate factor (e.g., 0.6-0.8) prevents inflated scores if the circuit behaves in an unintended manner."}, {"title": "Hyperparameters", "content": "The CRS computation relies on several hyperparameters that modulate its sensitivity to deviations and its handling of different sign scenarios:\n\u2022 \u03b1: Sensitivity to deviation, controlling how sharply CRS decreases as the circuit deviates from the baseline. Default: 1.0."}, {"title": "Details and Statistics of Dataset", "content": "Table 5 and 6 present the statistical details of the knowledge datasets used in our evaluation. For temporal knowledge, we utilize open-sourced WikiData as referenced. These datasets encompass a variety of knowledge categories, each consisting of multiple objects along with their associated time ranges."}, {"title": "Categorization of Knowledge Datasets", "content": "Each dataset category represents a specific type of structured knowledge:\nTemporal Knowledge. This category contains knowledge that varies over time, requiring temporal awareness for accurate retrieval. The definitions for each subcategory are as follows:\n\u2022 Sports: The teams associated with specific athletes over time.\n\u2022 Presidents: The names of country leaders for given years.\n\u2022 CEO: The chief executive officers of major companies in a given year.\n\u2022 Defense: The national defense budget of different countries.\n\u2022 Movies: The highest-grossing films by country for specific years.\n\u2022 GDP: The annual Gross Domestic Product (GDP) of various countries.\n\u2022 Inflation: The inflation rate of different countries for given years."}, {"title": "Attention Value Extraction and Injection", "content": "We employ a direct attention value addition method to influence the model's temporal knowledge representation. Though"}]}