{"title": "LLaVA-Surg: Towards Multimodal Surgical Assistant\nvia Structured Surgical Video Learning", "authors": ["Jiajie Li", "Garrett Skinner", "Gene Yang", "Brian Quaranto", "Steven Schwaitzberg", "Peter Kim", "Jinjun Xiong"], "abstract": "Multimodal large language models (LLMs) have achieved notable success across\nvarious domains, while research in the medical field has largely focused on uni-\nmodal images. Meanwhile, current general-domain multimodal models for videos\nstill lack the capabilities to understand and engage in conversations about surgical\nvideos. One major contributing factor is the absence of datasets in the surgical\nfield. In this paper, we create a new dataset, Surg-QA, consisting of 102,000\nsurgical video-instruction pairs, the largest of its kind so far. To build such a\ndataset, we propose a novel two-stage question-answer generation pipeline with\nLLM to learn surgical knowledge in a structured manner from the publicly avail-\nable surgical lecture videos. The pipeline breaks down the generation process\ninto two stages to significantly reduce the task complexity, allowing us to use a\nmore affordable, locally deployed open-source LLM than the premium paid LLM\nservices. It also mitigates the risk of LLM hallucinations during question-answer\ngeneration, thereby enhancing the overall quality of the generated data. We further\ntrain LLaVA-Surg, a novel vision-language conversational assistant capable of an-\nswering open-ended questions about surgical videos, on this Surg-QA dataset, and\nconduct comprehensive evaluations on zero-shot surgical video question-answering\ntasks. We show that LLaVA-Surg significantly outperforms all previous general-\ndomain models, demonstrating exceptional multimodal conversational skills in\nanswering open-ended questions about surgical videos. We will release our code,\nmodel, and the instruction-tuning dataset.", "sections": [{"title": "Introduction", "content": "Surgery, as a discipline with rich multimodal information in the medical field, diverges significantly\nfrom general medical diagnoses that often depend on static imagery, such as magnetic resonance\nimaging and chest X-ray. The dynamic nature of surgical procedures with complex sequence of\nactions and multi-stage processes, cannot be fully captured or understood through a single image.\nThe medical field has recently witnessed the significant impact of the Large Language Model\n(LLM), especially in the arena of medical question answering. Domain-specific LLMs like LLaVA-\nMed [11] and Med-PaLM [21], fused with publicly accessible medical question-answer data such\nas PubMed [31], can assist with inquiries about a biomedical image and meet the safety-critical\ndemands of the medical domain. Moreover, general purpose LLMs such as GPT [16], despite not\nbeing explicitly aligned to the medical field, have shown great potential and versatility when applied\nto some specific clinical knowledge areas. However, these models are still limited to processing\nsingle images, thus falling short of venturing into the surgical domain where the video modality plays\na crucial role."}, {"title": "Related Work", "content": "Surgical Video Question Answering (Surgical VQA) models can answer questions based on\nsurgical videos and offer assistance to practicing surgeons and surgical trainees. Early surgical\nVQA methods were largely discriminative [24, 5, 28], treating the task as a classification problem\nwhere answers were chosen from a predefined set. They excelled in identifying surgical steps,\ninstruments, and organs, but were limited to closed-set predictions and struggled with open-ended\nquestions and answers. Recent developments have shifted towards generative methods [20, 2, 19] that\nproduce free-form text sequences but are limited to single-turn conversations, preventing them from\nengaging in a dialogue or answering follow-up questions. Unlike these models, our LLaVA-Surg\nmodel can engage in meaningful multi-turn dialogues, answering surgical questions and providing\ncomprehensive surgical knowledge for an interactive learning experience.\nMultimodal LLM for Biomedical Image Conversations represents a significant advancement in\nthe field of medical artificial intelligence. These models combine text and image understanding\nto enable more nuanced and contextually aware interactions between clinicians and AI systems.\nFor instance, the LLaVA-Med model demonstrates the potential of multimodal LLMs to interpret\nand generate detailed medical image descriptions, thereby aiding both diagnostics and patient\ncommunication [11]. The application of such models extends to various tasks including VQA, where\nthey provide accurate and relevant answers based on medical images and related queries [32, 17]. This\nmultimodal approach also enhances the ability to perform complex reasoning and decision-making\nprocesses, which are critical in clinical settings [13]. Collectively, these developments underscore"}, {"title": "Surgical Video Instruction-tuning Data Generation", "content": "There is a significant deficiency in specialized datasets for training multimodal LLM as a conversa-\ntional assistant in the surgical domain. As illustrated in Figure 1, information in the surgical domain\ncan be categorized into four distinct levels: (1) basic identification of surgical objects such as organs\nand instruments, (2) recognition of discrete surgical actions, (3) higher-order reasoning of surgical\nactions, and (4) expert level deduction and planning.\nHowever, existing datasets [2, 30] lack level 3 and 4 information. To address this, we create Surg-QA,\nthe first surgical instruction-tuning dataset that contains all four levels of information. The proposed\ndataset consists of 100K video-text pairs from structured learning of surgical lecture videos and 2K\npairs focusing on the surgical visual concept alignment.\nSurgical Video Instruction-Tuning Data. For a surgical video X, and its transcript Xt, we prompt\nLlama-3-70B [1] through a two-step approach to create a set of questions Xq that can be answered\nonly when the video is provided, aiming to guide the assistant in describing the video content. A\nsingle-round instruction-tuning example can thereby represented by:\nUser: $X_q$ $X_v$<STOP>\\n Assistant : $X_a$<STOP>\\n\n(1)"}, {"title": "Surgical Visual Instruction Tuning", "content": "Architecture. LLaVA-Surg is a large vision-language model that aims to generate meaningful\nconversation about surgical videos. It employs the architecture of Video-ChatGPT [15], a general-"}, {"title": "Experiments", "content": "We conduct experiments to study two key components: the performance of LLaVA-Surg and the\nquality of the produced multimodal surgical instruction-tuning data. Our experiments focus on two\nevaluation settings: (1) How does LLaVA-Surg perform in surgical video question-answering, and\nhow does it compare to existing methods in the surgical domain? (2) How does the GPT evaluation\nframework compare to the human expert evaluation."}, {"title": "Implementation Details", "content": "Data. We collected 2,054 surgical procedures from WebSurg using the keyword \"intervention\" and\nan additional 97 procedures with the keyword \"gallbladder\" for future evaluation purposes, totaling\n2,151 procedures. These were randomly divided into a training set of 1,935 procedures and a test set\nof 216 procedures. In our instruction-tuning data generation pipeline, we use the 'large-v2' version\nof WhisperX [3] to transcribe the surgical lectures. We use Llama-3-70B-Instruct [1] for information\nextraction and data generation as mentioned in Section 3. We use 'gpt-3.5-turbo-0125' to perform the\nfollowing quantitative evaluation.\nTraining. We use LLaVA-Med as our pre-trained language backbone and finetune the model on\n90K surgical video instruction following data. We use CLIP ViT-L/14 as the image encoder and\nuse LLaVA-Med's language backbone as the initial weight of LLaVA-Surg. We update the linear\nlayer projecting the video features to the LLM's input space and the language backbone, while the\nCLIP encoder is kept frozen. We finetune the model for 5 epochs using a learning rate of 2e-5 and an\noverall batch size of 128. The training of our 7B model took around 6 hours on 8 A100 40GB GPUs.\nFor the rest of the hyperparameters, we follow the settings in [15]."}, {"title": "Quantitative Evaluation", "content": "Question-Answer Evaluation. We conducted a comprehensive quantitative evaluation on the test\nsplit of Surg-QA consisting of 4359 open-ended surgical video question-answer pairs. Following\nrecent works [12, 15, 11] that use GPT to evaluate open-ended questions, our evaluations employ"}, {"title": "Qualitative Evaluation", "content": "We performed an extensive evaluation of our model on various open-ended surgical video question-\nanswering tasks. Table 4 illustrates an example involving a gastric bypass procedure. LLaVA-\nSurg accurately identifies the procedure as a gastric bypass, noting the use of a suture and the\nclosing operation. It correctly answers the subsequent question regarding using a non-absorbable\nmonofilament suture to close the Petersen space. However, LLaVA-Med fails to correctly describe\nthe video, nor answer the following question. We provide more examples in Appendix C."}, {"title": "Conclusion", "content": "In this paper, we introduced Surg-QA, a surgical video instruction-tuning dataset of 102K video-text\npairs. Surg-QA is generated primarily through a cost-efficient, two-stage question-answer generation\npipeline, which effectively reduces hallucinations during question-answer generation by LLM. We\nthen trained LLaVA-Surg, a multimodal LLM in the surgical video domain, on Surg-QA. LLaVA-Surg\nshows great potential in understanding surgical videos and engaging in surgical video conversations,\noutperforming previous multimodal LLMs in our comprehensive evaluation. While LLaVA-Surg\nperforms competitively compared to existing methods in the surgical video domain, we note that"}, {"title": "More Discussions of LLaVA-Surg", "content": "Limitations of LLaVA-Surg The limitations of LLaVA-Surg include (1) hallucination, where it\nmay produce inaccurate but confident responses due to the lack of alignment with human preferences,\n(2) Its performance depends on the surgery procedures seen in SurgQA, with results varying widely\nbased on the surgery type. Additionally, since the data source Surg-QA, derived from WebSurg,\nincludes many rare cases, LLaVA-Surg's responses may lack universality and may not apply to a\nbroader range of clinical situations, (3) LLaVA-Surg might exhibit bias because of existing biases"}]}