{"title": "LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Surgical Video Learning", "authors": ["Jiajie Li", "Garrett Skinner", "Gene Yang", "Brian Quaranto", "Steven Schwaitzberg", "Peter Kim", "Jinjun Xiong"], "abstract": "Multimodal large language models (LLMs) have achieved notable success across various domains, while research in the medical field has largely focused on uni-modal images. Meanwhile, current general-domain multimodal models for videos still lack the capabilities to understand and engage in conversations about surgical videos. One major contributing factor is the absence of datasets in the surgical field. In this paper, we create a new dataset, Surg-QA, consisting of 102,000 surgical video-instruction pairs, the largest of its kind so far. To build such a dataset, we propose a novel two-stage question-answer generation pipeline with LLM to learn surgical knowledge in a structured manner from the publicly available surgical lecture videos. The pipeline breaks down the generation process into two stages to significantly reduce the task complexity, allowing us to use a more affordable, locally deployed open-source LLM than the premium paid LLM services. It also mitigates the risk of LLM hallucinations during question-answer generation, thereby enhancing the overall quality of the generated data. We further train LLaVA-Surg, a novel vision-language conversational assistant capable of answering open-ended questions about surgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations on zero-shot surgical video question-answering tasks. We show that LLaVA-Surg significantly outperforms all previous general-domain models, demonstrating exceptional multimodal conversational skills in answering open-ended questions about surgical videos. We will release our code, model, and the instruction-tuning dataset.", "sections": [{"title": "1 Introduction", "content": "Surgery, as a discipline with rich multimodal information in the medical field, diverges significantly from general medical diagnoses that often depend on static imagery, such as magnetic resonance imaging and chest X-ray. The dynamic nature of surgical procedures with complex sequence of actions and multi-stage processes, cannot be fully captured or understood through a single image.\nThe medical field has recently witnessed the significant impact of the Large Language Model (LLM), especially in the arena of medical question answering. Domain-specific LLMs like LLaVA-Med [11] and Med-PaLM [21], fused with publicly accessible medical question-answer data such as PubMed [31], can assist with inquiries about a biomedical image and meet the safety-critical demands of the medical domain. Moreover, general purpose LLMs such as GPT [16], despite not being explicitly aligned to the medical field, have shown great potential and versatility when applied to some specific clinical knowledge areas. However, these models are still limited to processing single images, thus falling short of venturing into the surgical domain where the video modality plays a crucial role."}, {"title": "2 Related Work", "content": "Surgical Video Question Answering (Surgical VQA) models can answer questions based on surgical videos and offer assistance to practicing surgeons and surgical trainees. Early surgical VQA methods were largely discriminative [24, 5, 28], treating the task as a classification problem where answers were chosen from a predefined set. They excelled in identifying surgical steps, instruments, and organs, but were limited to closed-set predictions and struggled with open-ended questions and answers. Recent developments have shifted towards generative methods [20, 2, 19] that produce free-form text sequences but are limited to single-turn conversations, preventing them from engaging in a dialogue or answering follow-up questions. Unlike these models, our LLaVA-Surg model can engage in meaningful multi-turn dialogues, answering surgical questions and providing comprehensive surgical knowledge for an interactive learning experience.\nMultimodal LLM for Biomedical Image Conversations represents a significant advancement in the field of medical artificial intelligence. These models combine text and image understanding to enable more nuanced and contextually aware interactions between clinicians and AI systems. For instance, the LLaVA-Med model demonstrates the potential of multimodal LLMs to interpret and generate detailed medical image descriptions, thereby aiding both diagnostics and patient communication [11]. The application of such models extends to various tasks including VQA, where they provide accurate and relevant answers based on medical images and related queries [32, 17]. This multimodal approach also enhances the ability to perform complex reasoning and decision-making processes, which are critical in clinical settings [13]. Collectively, these developments underscore"}, {"title": "3 Surgical Video Instruction-tuning Data Generation", "content": "There is a significant deficiency in specialized datasets for training multimodal LLM as a conversational assistant in the surgical domain. As illustrated in Figure 1, information in the surgical domain can be categorized into four distinct levels: (1) basic identification of surgical objects such as organs and instruments, (2) recognition of discrete surgical actions, (3) higher-order reasoning of surgical actions, and (4) expert level deduction and planning.\nHowever, existing datasets [2, 30] lack level 3 and 4 information. To address this, we create Surg-QA, the first surgical instruction-tuning dataset that contains all four levels of information. The proposed dataset consists of 100K video-text pairs from structured learning of surgical lecture videos and 2K pairs focusing on the surgical visual concept alignment.\nSurgical Video Instruction-Tuning Data. For a surgical video X, and its transcript Xt, we prompt Llama-3-70B [1] through a two-step approach to create a set of questions Xq that can be answered only when the video is provided, aiming to guide the assistant in describing the video content. A single-round instruction-tuning example can thereby represented by:\nUser: $X_q X_v \\text{<STOP>} \\backslash\\n$ Assistant : $X_a \\text{<STOP>} \\backslash\\n$   (1)"}, {"title": "4 Surgical Visual Instruction Tuning", "content": "Architecture. LLaVA-Surg is a large vision-language model that aims to generate meaningful conversation about surgical videos. It employs the architecture of Video-ChatGPT [15], a general-"}, {"title": "5 Experiments", "content": "We conduct experiments to study two key components: the performance of LLaVA-Surg and the quality of the produced multimodal surgical instruction-tuning data. Our experiments focus on two evaluation settings: (1) How does LLaVA-Surg perform in surgical video question-answering, and how does it compare to existing methods in the surgical domain? (2) How does the GPT evaluation framework compare to the human expert evaluation."}, {"title": "5.1 Implementation Details", "content": "Data. We collected 2,054 surgical procedures from WebSurg using the keyword \"intervention\" and an additional 97 procedures with the keyword \"gallbladder\" for future evaluation purposes, totaling 2,151 procedures. These were randomly divided into a training set of 1,935 procedures and a test set of 216 procedures. In our instruction-tuning data generation pipeline, we use the 'large-v2' version of WhisperX [3] to transcribe the surgical lectures. We use Llama-3-70B-Instruct [1] for information extraction and data generation as mentioned in Section 3. We use 'gpt-3.5-turbo-0125' to perform the following quantitative evaluation.\nTraining. We use LLaVA-Med as our pre-trained language backbone and finetune the model on 90K surgical video instruction following data. We use CLIP ViT-L/14 as the image encoder and use LLaVA-Med's language backbone as the initial weight of LLaVA-Surg. We update the linear layer projecting the video features to the LLM's input space and the language backbone, while the CLIP encoder is kept frozen. We finetune the model for 5 epochs using a learning rate of 2e-5 and an overall batch size of 128. The training of our 7B model took around 6 hours on 8 A100 40GB GPUs. For the rest of the hyperparameters, we follow the settings in [15]."}, {"title": "5.2 Quantitative Evaluation", "content": "Question-Answer Evaluation. We conducted a comprehensive quantitative evaluation on the test split of Surg-QA consisting of 4359 open-ended surgical video question-answer pairs. Following recent works [12, 15, 11] that use GPT to evaluate open-ended questions, our evaluations employ"}, {"title": "6 Conclusion", "content": "In this paper, we introduced Surg-QA, a surgical video instruction-tuning dataset of 102K video-text pairs. Surg-QA is generated primarily through a cost-efficient, two-stage question-answer generation pipeline, which effectively reduces hallucinations during question-answer generation by LLM. We then trained LLaVA-Surg, a multimodal LLM in the surgical video domain, on Surg-QA. LLaVA-Surg shows great potential in understanding surgical videos and engaging in surgical video conversations, outperforming previous multimodal LLMs in our comprehensive evaluation. While LLaVA-Surg performs competitively compared to existing methods in the surgical video domain, we note that"}]}