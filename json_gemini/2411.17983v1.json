{"title": "Optimized Conformal Selection: Powerful Selective Inference After Conformity Score Optimization", "authors": ["Tian Bai", "Ying Jin"], "abstract": "Model selection/optimization in conformal inference is challenging, since it may break the exchange-\nability between labeled and unlabeled data. We study this problem in the context of conformal selection,\nwhich uses conformal p-values to select \"interesting\" instances with large unobserved labels from a pool\nof unlabeled data, while controlling the FDR in finite sample. For validity, existing solutions require the\nmodel choice to be independent of the data used to construct the p-values and calibrate the selection\nset. However, when presented with many model choices and limited labeled data, it is desirable to (i)\nselect the best model in a data-driven manner, and (ii) mitigate power loss due to sample splitting.\nThis paper presents OptCS, a general framework that allows valid statistical testing (selection) after\nflexible data-driven model optimization. We introduce general conditions under which OptCS constructs\nvalid conformal p-values despite substantial data reuse and handles complex p-value dependencies to\nmaintain finite-sample FDR control via a novel multiple testing procedure. We instantiate this general\nrecipe to propose three FDR-controlling procedures, each optimizing the models differently: (i) selecting\nthe most powerful one among multiple pre-trained candidate models, (ii) using all data for model fitting\nwithout sample splitting, and (iii) combining full-sample model fitting and selection. We demonstrate\nthe efficacy of our methods via simulation studies and real applications in drug discovery and alignment\nof large language models in radiology report generation.", "sections": [{"title": "1 Introduction", "content": "Conformal inference is a versatile, distribution-free framework for predictive inference [Vovk et al., 2005].\nGiven any prediction model, it uses a conformity score to measure how well the outcomes \"conform\" to model\npredictions; it then leverages data exchangeability to deliver statistical evidence (p-value) for various tasks,\nincluding prediction set construction [Lei et al., 2018] and recent extensions in two-sample testing [Hu and\nLei, 2024] and multiple testing [Bates et al., 2023, Jin and Cand\u00e8s, 2023c]. Its strength lies in its flexibility\nas a \"wrapper\" for valid inference, regardless of the model or score used. However, despite this flexibility, a\ncommon challenge is model selection: determining the optimal conformity score and/or prediction model to\nimprove the performance for the specific statistical task at hand.\nWe study model selection in the context of model-free selective inference [Jin and Cand\u00e8s, 2023b], which\naims to identify unlabeled samples with large unobserved labels while controlling selection errors. Formally,\ngiven labeled data {(Xi, Yi)}=1 and unlabeled test samples {Xn+j}=1, the goal is to select a subset S C\n{1, ..., m} of test samples such that most of them obey Yn+j > Cn+j, where Cn+j are user-specified thresholds\nabove which the outcomes are considered interesting. This is formalized by false discovery rate (FDR) control:\nFDR := E(\\frac{\\sum_{j=1}^{m}1\\{j \\in S, Y_{n+j} < c_{n+j}\\} }{|S|} )<q, (1)"}, {"title": "2 Preliminaries", "content": "In this section, we first review the key ideas in the conformal selection framework [Jin and Cand\u00e8s, 2023c],\nand then discuss the challenges in optimizing the conformity score function, followed by a brief overview of\nrelated works in the field of selection inference and conformal inference.\n2.1 Recap: Conformal selection\nAssume access to a set of labelled data Dlabel = {(Xi, Yi)}_1 and a set of unlabeled test data Dtest =\n{Xn+j}=1 whose responses {Yn+j}}=1 are unobserved. Conformal selection begins by randomly splitting\nthe labelled data Dlabel into two disjoint subsets, the training set Dtrain := {(Xi, Yi)}11 and the calibration\nset Dcalib := {(Xi, Yi)}=n1+1. First, Dtrain is used to train a model and build a monotone conformity score\nfunction V : X \u00d7 Y \u2192 R where X and Y \u2286 R represent the sample space of {X} and {Y}, respectively.\nDefinition 1. A score function V : X \u00d7 Y \u2192 R is monotone if V(x, y) < V(x, y') whenever y \u2264 y' for any\nx \u2208 X and y, y' \u2208 \u0423.\nThe score function V can wrap around any prediction model (obtained in the training step), e.g., V(x, y) =\ny \u2212 \u03bc(x) where \u03bc(x) is a point prediction for y given x. Other popular choices include those based on\nquantile regression [Romano et al., 2019] and conditional cumulative distribution functions [Chernozhukov\net al., 2021]. We then compute the calibration scores Vi = V(Xn1+i, Yn1+i), i = 1,..., n\u2082, and test scores\nVn+j = V(Xn+j, Cn+j). These scores can be understood as \u201ctest statistics\u201d for large outcomes, and we will\nuse this term interchangeably. Conformal selection then builds a conformal p-value for each test point:\nPj = \\frac{\\sum_{i=n_1+1}^{n}1\\{V_i < V_{n+j}\\} + U_j \\cdot (1 + \\sum_{i=n_1+1}^{n} 1\\{V_i == V_{n+j}\\})}{n_2+1} (3)\nwhere U; id. Unif([0, 1]) is a random variable for tie-breaking, and n\u2082 = n - n\u2081. Intuitively, a small p-value\nsuggests that Vn+j is unusually large compared to {V;}, indicating that the corresponding Yn+j is likely to\nexceed cn+j, i.e., the unobserved label is \"of interest\". Formally, P(pj \u2264t, Yn+j \u2264 Cn+j) \u2264 t for all t \u2208 [0, 1].\nThe selection set S is then obtained by applying the Benjamini-Hochberg (BH) procedure [Benjamini\nand Hochberg, 1995] to {pi}-1 at the nominal FDR level q. As long as each test point (Xn+j, Yn+j) is\nexchangeable with the calibration data {(Xi, Yi)}=n1+1, it is shown in Jin and Cand\u00e8s [2023c] that S obeys\nthe FDR control (1) in finite samples.\nFrom now on, we refer to this procedure as Split Conformal Selection (SCS) for expositional convenience.\nAs suggested by Jin and Cand\u00e8s [2023c], we will exclusively focus on powerful \u201cclipped\u201d-type scores below.\nAssumption 1. The thresholds {ci}=1 are observed for the labeled data, such that {(Xi, Ci, Yi)}+mare\nexchangeable across i \u2208 [n + m]. All the score functions considered are \u201cclipped\u201d-type scores which obey\nV(X, Y) = V(Xi, ci) when Yi \u2264 Ci.\nWith observed {ci}=1, any monotone score function can be translated to a clipped version with strictly\nhigher power. Besides higher power, the property that V(Xi, Yi) = V(Xi, ci) when Yi < ci is crucial for\nflexible score optimization without observing the true test labels. Finally, the score function implicitly\ndepends on the thresholds, i.e., V(x, y, c), yet we suppress the dependence on c for notational simplicity.\n2.2 Challenges in optimizing conformity score function\nWe take a moment to review the theoretical underpinning of SCS and discuss the challenges in preserv-\ning FDR control while optimizing the score function with substantial data reuse. Readers may skip this\nsubsection without missing key concepts.\nAt a high level, the FDR control of SCS relies on two facts:"}, {"title": "3 Optimized Conformal Selection", "content": "We begin by introducing a general procedure that encompasses all use cases of OptCS, including model\nselection and full-sample model training and/or selection. The entire workflow consists of three steps:\n\u2022 Step 1: P-value construction. We introduce a novel approach to construct valid conformal p-values\nusing data-driven score functions, allowing for flexible model selection and optimization.\n\u2022 Step 2: Calibration of selection criteria. As preparation for multiple testing, we calibrate individual\nselection criteria for the p-values using \u201cauxiliary selection sizes\". This step addresses the complex\ndependencies among the p-values obtained in Step 1 that may arise from data-driven score functions.\n\u2022 Step 3: Multiple testing. Finally, we develop a multiple testing procedure that leverages the p-values\nand auxiliary selection sizes to construct the final selection set, ensuring finite-sample FDR control.\nThe high-level idea of OptCS is to \"individualize\" the p-values and selection criteria, constructed via\nslightly different model optimization processes for each test point. The key to validity is to ensure the opti-\nmization process for the j-th test sample is symmetric with respect to this sample and the calibration data,\nthereby separating the information used in the p-value and in the model optimization process for rigorous\nFDR control. This allows considerable flexibility in adapting to the data at hand, both in constructing pow-\nerful test statistics (via the score-generating functional) and in calibrating the multiple testing procedure\n(via the auxiliary selection sizes). We detail these steps in Section 3.1, followed by the general theoretical\nguarantee in Section 3.2. Concrete use cases of OptCS will be introduced in Section 4.\n3.1 General procedure\nStep 1: P-values. The first innovation of OptCS is a general construction of valid conformal p-values with\ndata-driven score functions. By \"data-driven\", we mean the choice of scores may depend on the calibration\nand test data. To enable such flexibility while ensuring the validity of the resulting p-values, we introduce\nthe notion of score-generating functional, which allows to use individual score functions for each test point.\nDefinition 2 (Score-generating functional). A score-generating functional is a function V : (X \u00d7 Y)n1+n2+m \u2192\n]Rm\u00d7(n2+m) with n1, n2, n, m \u2208 N, and n = n1+n2. For notational simplicity, we denote V(i) : (X\u00d7Y)n+m \u2192\nRn2+m such that V(i)(z) = V(z);,: for any z \u2208 (X \u00d7 V)n+m."}, {"title": "3.2 Theoretical guarantee", "content": "Our main theoretical result establishes finite-sample FDR control of OptCS, provided that the score-\ngenerating functional V(\u00b7) and the auxiliary selection function R(\u00b7) obey the appropriate conditions. The\nproof of Theorem 3.2 is in Appendix B.1.\nTheorem 3.2. Suppose for all j \u2208 [m], the following conditions hold:\n(1) (Z1,..., Zn, Zn+j) is exchangeable given {Zn+1}l\u2260j;\n(2) the score-generating functional V is monotone and permutation equivariant (Definitions 3 and 4);\n(3) the auxiliary selection functional R(\u00b7) is permutation invariant under the j-th null (Definition 5).\nThen, for any nominal FDR level q \u2208 (0,1) and any pruning method, the output S of OptCS obeys FDR\ncontrol (1) in finite sample, where the expectation is taken over both the calibration and test data.\nWe now provide some intuitions and intermediate technical ideas for Theorem 3.2. At a high level, all\n\"nuisance\" steps, including model optimization and auxiliary threshold calculation, are conducted in a way\nsuch that the conformal p-values remain valid (i.e. uniformly distributed) conditional on them. In particular,\nthis is achieved by the permutation equivariance of conformity scores and the permutation invariance of Rj.\nAll these support the use of OptCS across a wide range of applications, as we will see in the next section.\nTo begin with, under any pruning option, we obtain the following decomposition of FDR. Lemma 3.3\nadapts Lemma C.1 of Jin and Cand\u00e8s [2023c], and we include a proof in Appendix B.5 for completeness.\nLemma 3.3 (FDR decomposition). Under any of the three pruning options, the output of Algorithm 1 obeys\nFDR < E (\\frac{\\sum_{j=1}^{m}1\\{p_j <qR_j/m, Y_{n+j} \\leq c_{n+j}\\} }{R_j} ) (9)\nIt thus suffices to ensure that each expectation in the summation (9) is controlled. We achieve so by\nshowing that the p-value is uniformly distributed conditional on Rj on the null event. More specifically, let\n[Zj] = [Zn1+1,..., Zn, Zn+j] be the unordered set of the (n2+1) full observations, and define R; as the j-th\nelement of Ro (Z1:n1, Zn1+1:n, Zn+1:n+j\u22121, Zn+j, Zn+j+1:n+m). Under the conditions of Theorem 3.2, this is\na consequence of the following two facts:\n\u2022 P(pj \u2264t, Yn+j \u2264 Cn+j | [Zj]) \u2264 t for all t \u2208 [0, 1] that is measurable with respect to [Zj].\n\u2022 Yn+j \u2264 Cn+j implies R\u2081 = Rj, and Rj is measurable with respect to [Zj].\nFinally, we remark that the design of R; should aim to approximate the selection size of applying the\nBH procedure to our p-values (4) while perserving the permutation invariance property."}, {"title": "4 Instantiations of OptCS", "content": "In this section, we specify the construction of V(\u00b7) and R(\u00b7) in Algorithm 1, which leads to several concrete\ninstantiations of OptCS with various purposes. Each of them represents an approach to constructing or\nselecting powerful test statistics (scores) while maintaining the theoretical guarantees in Section 3.\nIn a nutshell, we exploit two ways model optimization can improve power: (i) select the most powerful\nmodel among multiple choices, and (ii) use all labeled data for model fitting by avoiding sample splitting.\nSection 4.1 proposes OptCS-MSel which allows (i) when multiple pre-trained models are available. Section 4.2\nproposes OptCS-Full which allows (ii) when a training model class is given. Finally, Section 4.3 proposes\nOptCS-Full-MSel which simultaneously achieves (i) and (ii) with multiple candidate model classes.\n4.1 OptCS-Msel: Model selection over trained models\nThe first scenario we consider is when multiple pre-trained models have been obtained in a separate training\nstage, and a scientist aims to use data at hand to select the best model among them and produce an FDR-\ncontrolling selection set. This is suitable, for instance, in many drug discovery tasks where pre-trained models\nfor drug properties are available to use yet too costly to retrain.\nFollowing the notations in Section 2.2, there are K candidate scores {V(\u00b7,\u00b7;k): X \u00d7 Y \u2192 R}=1 inde-\npendent of the calibration and test points. Without loss of generality, we assume they are obtained with\nthe fold {(X, Y)}11, independent of the calibration data {(Xi, Yi)}=n1+1. Also recall Vi(k) = V(Xi, Yi; k)\nand Vn+j(k) = V(Xn+j, Cn+j;k) are conformity scores using each model k \u2208 [K].\nAs we discussed in Section 2.2, greedily selecting the score function k \u2208 [K] that results in the largest\nselection set in SCS invalidates FDR control. In contrast, OptCS restores validity with a slightly modified\nprocedure. The key idea is to select a model separately for each j \u2208 [m], each in a \"greedy\" yet permutation-\ninvariant fashion. This selected model will be used to construct pj and its auxiliary selection size Rj. In this\nway, we ensure that the model selection process does not bias the p-values, thus maintaining FDR control.\nFor each j \u2208 [m], we will construct a permutation-invariant estimate of the SCS output using the k-th\nmodel, denoted as\nSj(k). Then, we greedily select kj = argmaxk Sj(k), and construct the functional V via\n(V(kj), ..., Vn (kj), Vn+1(kj),..., Vn+m(kj)) (10)\nin Line 3 of Algorithm 1, as well as the auxiliary selection sizes R\u2081 = |S;(k\u2081)| in Line 5 of Algorithm 1. We\nvisualize the comparison between OptCS and the naive approach in Figure 4.\nWe now specify S; (k) for each model index k \u2208 [K]. It is the output of the BH procedure applied to a\nset of slightly modified p-values {pi) (k)}e\u2260j \u222a {0} at the nominal level q, where\np_{l}^{(j)}(k) = \\frac{\\sum_{i\\in Z_{calib}} 1\\{V_i(k) \\leq V_{n+l}(k)\\} + 1\\{V_{n+j}(k) \\leq V_{n+l}(k)\\} }{n_2+1} , l \\in [m], l\\neq j. (11)\nNote that {pi) (k)}e\u2260; are very close to the conformal p-values in SCS using the k-th score function, except\nthe introduction of 1{Vn+j(k) < Vn+e(k)} to preserve permutation invariance.\nThe following result, whose proof is in Appendix B.2, ensures that V and R; satisfy the conditions\nrequired by OptCS, and thus it ensures finite-sample FDR control by Theorem 3.2."}, {"title": "4.2 OptCS-Full: Conformal selection without data splitting", "content": "In this part, we introduce OptCS-Full, a variant of OptCS that avoids sample splitting in conformal selection\nwhen model training with labeled data is needed. We will set aside the model selection issue and consider\na fixed model class for the conformity score. That is, the prediction model will be trained through a given\nprocess, and the conformity score wraps around this model in a given fashion. The goal is to improve\nthe scores/test statistics by training a more accurate model with more labeled data. A third method that\nsimultaneously conducts model training and model selection will be studied in the next part.\nFor clarity, we represent the training process via an algorithm A that takes as input a set of labeled data\nand output a trained model, e.g., A: UN\u22650 (X \u00d7 Y)N \u2192 {measurable functions \u00fb: X \u2192 Y}. Of course, the\noutput can be more general than a mapping from X to Y. For instance, it might consist of a point prediction\nmodel and a variance estimator to be used in the score function, or a conditional c.d.f. function. Here, we\nuse \u00fb to refer to the trained output for simplicity. We require that A treats the input data symmetrically:\nA((X_1,Y_1),\\ldots, (X_N, Y_N)) = A((x_{\\pi(1)}, Y_{\\pi(1)}),\\cdots, (X_{\\pi(N)}, Y_{\\pi(N)})) (12)\nfor any permutation \u03c0: [N] \u2192 [N]. The score function wraps around a fitted model \u00fb: X \u2192 Y in a given\nway. To emphasize this point, we write the conformity score function as\nV(\u00b7, \u00b7 |): X \u00d7 Y \u2192 R, (13)\nsuch that the role of \u00fb in the score function is fixed. For the ease of presentation, in this subsection, we\nrestrict our attention to binary classification problems with y = {0,1} and c = 0. The original problem can\nalways be reduced to this setting by a transformation Y = 1{Y < c}.\nRecall the preparatory data {Zi}11, the calibration data {Zi}=n1+1 and the test data {2n+j}-1 where\n2n+j = (Xn+j, 0). Our goal is to involve all the labeled data {Z}=1 to train an accurate prediction model\nwhile still producing a FDR-controlling selection set S \u2286 [m]. Similar to Full Conformal Prediction [Vovk\net al., 2005, FCP], the idea of OptCS here is to train the models in a way that is permutation invariant to\nthe calibration data and the j-th test point. Compared with FCP, we only need to plug in the null value\nYn+j = 0, instead of every hypothesized value y \u2208 Y, which makes the computation easier."}, {"title": "4.3 OptCS-Full-Msel: Model training and selection without data splitting", "content": "In this part, we combine the preceding two ideas to propose OptCS-Full-Msel, a variant of Algorithm 1 that\nallows model training, selection, and FDR-controlling conformal selection with all labeled data. It aims to\nconstruct the most powerful test statistic by both leveraging all labeled data and selecting the best model."}, {"title": "5 Simulation studies", "content": "5.1 Conformity score selection with pre-trained models\nWe first evaluate the performance of OptCS-MSel in the task of selecting conformity scores while producing\nFDR-controlled selection sets. We treat the set of candidate scores as given before \"seeing\" the calibration\nand test data, i.e., the latter two are not involved in the training process. All of the competing methods are:\n(i). Greedy: Select the candidate conformity score function which leads to the largest selection set in SCS.\n(ii). OptCS-MSel_homo and OptCS-MSel_hete: Our method with homogeneous and heterogeneous pruning.\n(iii). Base_random: Randomly pick a conformity score and use it in SCS.\n(iv). Base_cal_split: Randomly split the calibration set into three folds: Dcalib_sel (25%), Dtest_sel (25%),\nand Dealib (50%). We select the score which leads to the largest selection set in SCS with Dcalib_sel as\nthe calibration set and Dtest_sel as the test set. We then run SCS with calibration set Dealib\u00b7\n(v). Base_tr_split: Similar to (iv), but we split Dtrain into Dcalib_sel (25%), Dtest_sel (25%), and Dtrain\n(50%). After training models on Drain, we use Dcalib_sel (25%), Dtest_sel (25%) to select a score.\nWe exclude OptCS with deterministic pruning in result reporting since its power is often lower than the\nother two pruning options, consistent with Jin and Cand\u00e8s [2023b].\nSimulation settings. We construct 8 data generating processes, four adapted from Liang et al. [2024a],\nreferred to as Liang's settings, and four adapted from Jin and Cand\u00e8s [2023c], referred to as Jin's settings.\nIn all settings, the goal is to identify individuals whose unobserved responses exceed Cn+j = 0. We design\n11 model choices for Liang's settings and 24 model choices for Jin's settings. In Liang's settings, the model\nquality mostly depends on whether it includes certain features in regression modeling, hence the quality\ngap between candidate models is large. In Jin's settings, the model qualities are closer to each other. The\ndetailed data generating processes and model choices are summarized in Appendix C.1. In the experiments,\nwe fix the sample sizes at n\u2081 = n2 = m = 100 and vary the nominal FDR level q \u2208 {0.2,0.25, . . ., 0.45, 0.5}.\nSimulation results. We report the empirical FDR and power over N = 500 independent runs for Liang's\nsettings in Figure 6 and for Jin's settings in Figure 7, with |Dtrain| = 100, |Dcalib| = 100, and |Dtest| = 100.\nThe empirical FDR is the mean of |S\u2229 Ho |/(1 V |S|), and the empirical power is the mean of |S\u2229H1|/|H1|\nover all replica, where we define Ho = {j \u2208 [m]: Yn+j < c} and H\u2081 = {j \u2208 [m] : Yn+j > c}.\nWe observe drastic violation of FDR with the Greedy method due to the double-dipping bias. In contrast,\nall other methods control the FDR below the nominal levels. For all 8 settings and across all nominal\nFDR levels, OptCS-MSel_homo consistently outperforms all competing methods that maintain valid FDR\ncontrol. OptCS-MSel_hete outperforms baselines in Liang's settings. On the other hand, in Jin's settings\nwith larger nominal FDR levels, model qualities are similar since the random baseline outperforms other\nbaseline methods; in such cases, OptCS-MSel_homo maintains strong performance with higher power than\nOptCS-MSel_hete; we conjecture that the smoothing effect of homogeneous pruning is particularly useful.\nFinally, we include additional results in Appendix C.2 for the performance of these methods as (n1, n2)\nvaries, demonstrating consistent superior performance of OptCS-MSel over baselines.\n5.2 Conformal selection without data splitting\nNext, we evaluate OptCS-Full described in Section 4.2 and compare it against baseline methods that involves\ndata splits in settings with a fixed model class. Below, we outline all of the competing methods:\n(i). OptCS-Full_os: Our procedure in Algorithm 2 with over-sampling in training.\n(ii). OptCS-Full_sep: The second variant we introduce in Appendix A.2 that avoids involving all null test\nsamples in training but with more times of model training and additional pruning.\n(iii). Base_split_0.75: Randomly split the labeled data into Dtrain (75%) and Dcalib (25%). We train the\nmodel on Dtrain, and apply SCS using Dealib as calibration data."}, {"title": "5.3 Model training and selection with full data", "content": "Finally, we conduct simulation studies to demonstrate the performance of OptCS-Full-MSel, which uses all\nlabeled data in the entire process, including model training, model selection, and final multiple testing.\nWe compare OptCS-Full-MSel with a suite of sample splitting baselines with no techniques from OptCS.\nThese competing methods include:\n(i). OptCS-Full-MSel: Our OptCS-Full-MSel procedure where the training process uses over-sampling.\n(ii). Base_random_0.25: Randomly split the labeled data into Dtrain (25%) and Dcalib (75%). We randomly\nselect a model, train the model on Dtrain, and apply SCS using Dealib as calibration data.\n(iii). Base_random_0.75: Similar to (ii), but with Dtrain (75%) and Dcalib (25%).\n(iv). Base_split_112: Randomly split the labeled data into Dtrain, Dsel, and Dcalib with ratio 1:1:2. We\nuse Dtrain to train all the models, use Dsel to run SCS (after additional splitting) and select the model\nwith largest selection set, then use Dealib to run SCS with the test data and the selected model.\n(v). Base_split_121: Similar to (iii), with data split ratio 1:2:1 for Dtrain, Dsel, and Dcalib.\n(vi). Base_split_211: Similar to (iii), with data split ratio 2:1:1 for Dtrain, Dsel, and Dcalib.\n(vii). Base_split_111: Similar to (iii), with data split ratio 1:1:1 for Dtrain, Dsel, and Dcalib.\nTo investigate the relative contributions of the full-data training module and the model selection module\nto the power, we additionally include the following two \"partial\" baselines in our evaluation:\n(i). OptCS-Full_random: Our OptCS-Full procedure where the training process uses over-sampling, with\na randomly selected model class (i.e., full-data training but no model selection).\n(ii). OptCS-Full_split: We first split the data into Dsel (50%) and Dcalib (50%), then use Dsel to select\na model class, and run our OptCS-Full procedure using Dealib as the \"full data\" with the selected\nmodel class. Inside Dsel, we split the data into a \u201ccalibration\u201d fold (50%) and a \u201ctest\u201d fold (50%), run\nOptCS-Full with each model class, and select the one with the largest selection set.\nSimulation settings. We adopt the four Jin's settings adapted from [Jin and Cand\u00e8s, 2023c] and the\nfour Liang's settings adapted from [Liang et al., 2024a], with 9 model classes for Jin's settings and 7 model\nclasses for Liang's settings. We fix the total size of labeled data at n = 500 and test data at m = 100, and run\nthe experiments for N = 500 independent replica. More details on the model setups are in Appendix C.5.\nSimulation results. The empirical FDR and power in Liang's settings are reported in Figure 9. The\nperformance of OptCS-Full with homogeneous and heterogeneous pruning are in red and greed solid lines;\nthe two variants both control the FDR. While they yield similar power in Liang's settings, homogeneous\npruning is moderately more powerful in Jin's settings. Both variants of OptCS-Full consistently outperform\nall non-OptCS baselines (dashed lines). However, we note that these baselines do not exhaust the FDR\nbudget; this may be due to the fact that the resolution of p-values (determined by calibration data size) is\nreduced due to sample splitting.\nCompared with the two partial baselines (OptCS-Full_split which does not use full sample for model\nselection and OptCS-Full_random which misses the model selection component), OptCS-Full-MSel demon-\nstrates the benefits of combining the model selection and full-sample training modules. In particular, while\nthe contribution of full-sample training and calibration is significant (comparing OptCS-Full-MSel and\nOptCS-Full_split), the gap between OptCS-Full-MSel and OptCS-Full_random shows the contribution of\nmodel selection seems more significant in Liang's settings where the quality of models significantly differ."}, {"title": "6 Real data applications", "content": "We apply OptCS to two representative applications of Conformal Selection, namely, drug discovery (Sec-\ntion 6.1) and abstention of large language models in radiology report generation (Section 6.2).\n6.1 Drug discovery with model selection\nWe first apply OptCS to drug discovery tasks for selecting drug candidates with favorable biological prop-\nerties. In this task, controlling the FDR of the selection set ensures that subsequent investigations, such as\nwet-lab validation of their properties, are resource-efficient.\nConcretely, in this problem, each sample is a drug candidate (such as a small molecule, an antibiotic, or\na protein), whose physical and chemical features are encoded in X \u2208 X, and we are interested in certain\nbiological property Y \u2208 Y \u2286 R. While the ground-truth knowledge of Y typically needs to be evaluated via\nprocesses such as high-throughput-screening (HTS) or wetlab experiments [Lloyd, Macarron et al., 2011],\nmachine learning models are increasingly used to predict the properties to identify potentially viable candi-\ndates before such costly evaluations [Huang, 2007]. Given test drugs {Xn+j}}=1, we aim to select promising\nones with Yn+j > c for some pre-determined threshold c > 0 while controlling the false discovery rate.\nFor drug property prediction, state-of-the-art models provide a wide variety of pretrained molecule em-\nbeddings [Xiong et al., 2019, Li et al., 2021, Landrum, 2016]. These embeddings can be leveraged to train a\npredictor (e.g., a shallow neural network) in local datasets for a specific downstream task. With the many\nchoices, it is desired to build the selection set with the best model. Among the three procedures, OptCS-MSel\nis the most suitable for this setting where the models are typically costly to train."}, {"title": "6.2 Boosting LLM Alignment", "content": "Finally", "2024": "we use conformal selection to identify radiology images for which the LLM-\ngenerated reports meet certain alignment criterion. In this context", "prompt\" X \u2208 X, e.g., a radiology image. A vision-to-\nlanguage model f \u2208 X \u2192 L generates a report summarizing the findings from the image, where L is the space\nof reports. To address potential factual errors and other biases in f(X), Conformal Alignment [Gui et al.,\n2024": "defines the alignment status via an indicator Y = A(f(X)", "A": "L \u00d7 E \u2192 {0", "g": "X \u2192 [0, 1", "aligned": "eports (Yn+j > 0 should a reference report be acquired).\nThe power of the procedure depends on the choice of the predictor g and the conformity score, all with\nmany options in practice. Given the scarcity of high-quality reference data, effective use of limited labeled\ndata for model optimization is crucial. In this part, we focus on full-sample training variants which are\nparticularly suitable for the lightweight training processes often used for g (such as random forests).\nWe use a subset of the MIMIC-CXR dataset [Johnson et al., 2019"}, {"2024": "see Appendix C.6 for details on the datasets and language model.\nFollowing Gui et al. [2024"}]}