{"title": "SpectralGaussians: Semantic, spectral 3D Gaussian splatting for multi-spectral scene\nrepresentation, visualization and analysis", "authors": ["Saptarshi Neil Sinha", "Holger Graf", "Michael Weinmann"], "abstract": "We propose a novel cross-spectral rendering framework based on 3D Gaussian Splatting (3DGS) that generates realistic and seman-\ntically meaningful splats from registered multi-view spectrum and segmentation maps. This extension enhances the representation\nof scenes with multiple spectra, providing insights into the underlying materials and segmentation. We introduce an improved\nphysically-based rendering approach for Gaussian splats, estimating reflectance and lights per spectra, thereby enhancing accu-\nracy and realism. In a comprehensive quantitative and qualitative evaluation, we demonstrate the superior performance of our\napproach with respect to other recent learning-based spectral scene representation approaches (i.e., XNeRF and SpectralNeRF) as\nwell as other non-spectral state-of-the-art learning-based approaches. Our work also demonstrates the potential of spectral scene\nunderstanding for precise scene editing techniques like style transfer, inpainting, and removal. Thereby, our contributions address\nchallenges in multi-spectral scene representation, rendering, and editing, offering new possibilities for diverse applications.", "sections": [{"title": "1. Introduction", "content": "Accurate scene representation is an essential prerequisite for\nnumerous applications. The way we perceive our surroundings\nin terms of a mixture of light gives us a particular scene under-\nstanding, thereby determining how we interact with our envi-\nronment. However, representing scenes in terms of red, green\nand blue color channels suffers from both a bad reproduction of\nthe scene's appearance due to metamerism effects and lacking\ncharacteristics only observable in certain of the spectral bands.\nTherefore, multi-spectral scene capture and representation has\nbecome of high relevance, where light and reflectance spectra\nare given with a higher resolution thereby surpassing the limi-\ntations of the broad-band RGB color model.\nIn domains such as architecture, automotive industries, ad-\nvertisement, and design, accurate modeling of light transport\nand considering the full spectrum of light is crucial for vir-\ntual prototyping. Predictive rendering, which involves simu-\nlating the spectral transport of light, is necessary to assess and\nevaluate the visual quality of products before physical produc-\ntion. This ensures reliable assessment and enables color-correct\nscene reproduction. Furthermore, spectral information as cap-\ntured by multi-spectral (MS) cameras [1, 2], infrared (IR) cam-\neras [3], and UV sensors [4] extends scene understanding in\nterms of insights on underlying material characteristics and be-\nhavior (including anomalies, defects, etc.) revealed only in cer-\ntain sub-ranges of the light spectrum which empowers experts\nand autonomous systems to gain valuable insights and make\ninformed decisions in the respective scenarios. For precision\nfarming applications, multispectral scene monitoring enables\nearly detection and monitoring of harmful algal bloom in bodies\nof water [5], facilitates the detection and classification of plant\ndiseases [6, 7] to allow farmers to maintain crop health, opti-\nmize agricultural practices, and conduct quantitative and quali-\ntative analysis of agro products [8], and allows getting insights\non precise and objective plant parameters through 3D vision\nand multi-spectral imaging via phenotyping sensors like Plant-\nEye [9]. In the context of cultural heritage, multi-spectral infor-\nmation is essential for gaining insights on production processes\nof artifacts or artworks and used materials, as e.g. relevant for\nthe analysis of historical paintings [10, 11, 12] or for reveal-\ning hidden or altered features withing documents [13], thereby\nalso providing crucial hints on restoration of eroded parts by\nutilizing information from individual spectral bands that may\nexceed the visible range. Among the many further application\nscenarios where multi-spectral scene monitoring and represen-\ntation also allows for a more comprehensive understanding are\nfacial recognition systems [14], medical sciences, forensic sci-\nences and remote sensing [15], where land cover and usage can\nbe monitored more accurately.\nDepending on the respective scenario, multi-spectral infor-\nmation can be either stored in terms of multi-channel represen-\ntations [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27] (as typi-\ncally used for airborne or satellite-based surveillance), in terms\nof multi-spectral surface reflectance characteristics directly pa-\nrameterized on 3D point clouds [28, 29, 30, 31, 32, 33] or\nmeshes [34, 35], or in a volumetric manner as investigated with\nrecent learning-based neural radiance field (NERF) representa-\ntions [36]. Implicit scene representation using NeRFs [36] has\nbeen demonstrated to allow high-fidelity scene representation\nbased on training a neural network to predict view-dependent\ncolor and view-independent density information for points in\nthe scene volume and leveraging volume rendering to predict\nthe scene's appearance for particular viewpoints, while optimiz-\ning the network to produce images that match the original input\nimages. Beside the many extensions towards spatial represen-\ntations, recent NeRF approaches also have explored extensions\ntowards spectral scene representations [37, 38]. XNeRF and\nSpectralNeRF, despite their advancements in handling spectral\nscene representations, have limitations. XNeRF and Spectral-\nNerf do not include reflectance and lighting estimation, seg-\nmentation of the spectral scene, and explicit geometry. These\nlimitations can impact the accuracy, relightability, and com-\nprehensive understanding of spectral scenes. Moreover, The\n3DGS employs rasterization for rendering, which allows for\nreal-time performance compared to NeRF-based methods and\nadvanced 3DGS methods [39, 40] go beyond appearance and\ngeometry modeling by supporting open-world and fine-grained\nscene understanding. They exceed the capabilities of NeRF-\nbased approaches, like Semantic-NeRF [41], which incorporate\nsemantic information into radiance fields for 3D scene model-\ning. However, these methods struggle to generalize to open-\nworld scenarios. Distilled Feature Fields [42] and LERF [43]\nexplore distilling 2D features to aid in open-world 3D seman-\ntics, but they have limitations in accurate segmentation and can-\nnot match the segmentation quality and efficiency of Gaussian-\nbased methods [39, 40].\nThe recently introduced 3D Gaussian Splatting (3DGS) [44]\nhas been demonstrated to allow superior performance and qual-\nity compared to NERF-based scene representation and visual-\nization. This explicit scene representation replaces the neural\nnetwork used in NeRF approaches with a set of Gaussians and\nthe number and arrangement of Gaussians is optimized to best\nmatch the input data. Thereby, the representation results in im-\nproved rendering efficiency, while also offering interpretability\nin contrast to black-box neural network representations. How-\never, the extension of 3DGS towards spectral scene representa-\ntion and visualization has not been investigated so far.\nIn this paper, we present spectral 3D Gaussian splatting that\nallows efficient multi-spectral scene representation and visual-\nization. For this purpose, we present the following key contri-\nbutions:\n\u2022 We present a novel cross-spectral rendering framework\nthat extends the scene representation based on 3D Gaus-\nsian Splatting (3DGS) to generate realistic and semanti-\ncally meaningful splats from registered multi-view spec-\ntrum and segmentation maps.\n\u2022 We present an improved physically-based rendering ap-\nproach for Gaussian splats, estimating reflectance and\nlights per spectra, which enhances the accuracy and real-\nism of the rendered output by considering the unique char-\nacteristics of different spectra, resulting in visually con-\nvincing and physically accurate scene representations."}, {"title": "2. Related work", "content": "In recent years, significant advancements have been made\nin generating photo-realistic novel views through the use of\nnovel learning-based scene representations combined with vol-\nume rendering techniques. Neural Radiance Fields (NeRF)\n[36, 47] represent the scene based on a neural network that pre-\ndicts local density and view-dependent color for points in the\nscene volume. This information can then be used to synthe-\nsize images of the scene using volume rendering techniques.\nThe network representing the scene is trained by minimizing\nthe deviation of the predicted images to their respective given\ninput images under the respective view conditions, thereby ex-\nploiting the observation that an accurate scene representation\nby the network leads to an accurate image synthesis. The re-\nmarkable potential of the NeRF approach for novel view syn-\nthesis has given rise to several notable extensions. Researchers\nhave focused on improving rendering quality by addressing is-\nsues such as aliasing [48, 49, 50, 51], as well as accelerating\nnetwork training [52, 53, 54, 55, 56]. Furthermore, there have\nbeen efforts to handle more complex inputs, including uncon-\nstrained image collections [57, 58, 59], image collections re-\nquiring the refinement or complete estimation of camera pose\nparameters [60, 61, 62, 63], deformable scenes [64, 65] and\nlarge-scale scenarios [66, 67, 68]. Further works aimed at guid-\ning the training and handling textureless regions by incorporat-\ning depth cues [69, 70, 71, 72, 73].\nDespite the great success of NeRFs for novel view synthe-\nsis applications, the neural network lacks interpretability and\nthe extraction of surface information requires network evalu-\nations on a dense grid and a subsequent derivation of surface\ninformation from the volumetric density information based on\ntechniques like Marching Cubes [74], which limits real-time\napplications. Therefore, further works focused on representing\nscenes in terms of implicit surfaces [75, 76, 77], explicit rep-\nresentations using points [78], meshes [79], and 3D Gaussians\n[44]. Point-based neural rendering techniques, such as Point-\nNeRF [78], merge precise view synthesis from NeRF with the\nfast scene reconstruction abilities of deep multi-view stereo\nmethods. These techniques employ neural 3D point clouds to\nenable efficient rendering, thereby facilitating accelerated train-\ning processes. Furthermore, a recent approach [80] has shown\nthat point-based methods are well-suited for scene editing pur-\nposes. Recently, 3D Gaussian Splatting [44] has been intro-\nduced as the state-of-the-art, learning-based scene representa-\ntion based on optimized Gaussians for novel view synthesis,\nsurpassing existing implicit neural representation methods such\nas NeRFs in terms of both quality and efficiency. This approach\nutilizes anisotropic 3D Gaussians as an explicit scene represen-\ntation and employs a fast tile-based differentiable rasterizer for\nimage rendering.\nHowever, extending these novel scene representations to\nthe spectral domain beyond RGB channels remains an open\nchallenge, with only a few seminal works addressing this so\nfar. Spectral variants of NERF, such as xNERF [37] for\ncross-spectral spectrum-maps and SpectralNeRF [38] for multi-\nspectral spectrum-maps, have shown effectiveness in generat-\ning novel views across different spectral domains. The cross-\nspectral splats generated by our approach can be visualized via\nan interactive spectral viewer [81] based on Viser [82]. Besides\nview synthesis, the viewer allows to visualize splats, even with\nspectral characteristics, as well as visualizing residuals between\ndifferent versions of splats such as splats from different itera-\ntions during training or comparing differences between splats\nin different spectral ranges. Furthermore, the user study con-\nducted in their work [81] validates the effectiveness and prac-\nticality of the reconstructed 3D splats derived from the spec-\ntrum maps, confirming their utility in spectral visualization and\nanalysis. However, the framework of reconstructing a spectral\nGaussian Splatting scene representation is a novel contribution\nin this paper and has not been considered in their work [81]."}, {"title": "2.2. Radiance based appearance capture", "content": "Instead of focusing on the pure reproduction of a scene\naccording to the original NeRF formulation without explic-\nitly modeling reflectance and illumination characteristics, sev-\neral NeRF extensions focused on modeling reflectance by sep-\narating visual appearance into lighting and material proper-\nties. Respective approaches have the capability to jointly pre-\ndict environmental illumination and surface reflectance prop-\nerties even in the presence of unknown or varying lighting\nconditions [83, 84, 85, 86, 87, 88].\nOne notable contribution is Ref-NeRF [45], which in-\ntroduces a novel parameterization and structuring of view-\ndependent outgoing radiance, along with a regularizer on nor-\nmal vectors. This enhances the accuracy in predicting re-\nflectance properties. To address the challenge of learning ge-\nometry from highly specular surfaces, recent works [89, 90, 91]\nhave utilized SDF-based representations. This enables more\nprecise estimation of surface normals for physically based ren-\ndering. However, these methods suffer from time-consuming\noptimization and slow rendering speed, limiting their practical\napplication in real-world scenarios. Furthermore, NVDiffRec\n[92] is an explicit representation method that directly optimizes\ntriangle meshes with materials and environment map lighting,\nenabling real-time interactive applications, unlike MLP-based\nmethods that tend to be slower.\nRelightable Gaussians [93] presents a differentiable point-\nbased rendering framework for material and lighting decom-\nposition from multi-view images, enabling real-time relighting\nand editing of 3D point clouds. It surpasses existing material\nestimation approaches and offers improved results. Gaussian-\nShader [94] is another method that enhances neural rendering in\nscenes with reflective surfaces by applying a simplified shading\nfunction on 3D Gaussians. It addresses the challenge of accu-\nrate normal estimation on discrete 3D Gaussians, achieving a\nbalance between efficiency and rendering quality. Our shading\nmodel is inspired by this method where we use the model with-\nout the residual color in the reflectance estimation."}, {"title": "2.3. Sparse spectral scene understanding", "content": "Gaussian splatting based semantic segmentation frame-\nworks, such as Gaussian Grouping [39] and LangSplat [40],\nhave successfully utilized foundation models like Segment\nAnything [95] to segment scenes. LangSplat is a 3D lan-\nguage field that enables precise and efficient open-vocabulary\nquerying within 3D spaces by representing language features\nusing a collection of 3D Gaussians distilled from CLIP [96].\nGaussian Grouping extends Gaussian Splatting by incorporat-\ning object-level scene understanding and introducing Identity\nEncodings to reconstruct and segment objects in open-world\n3D scenes. We utilized this method for accurate semantic seg-\nmentation of spectral scenes. Segmenting the scene per spec-\ntra provides valuable information about regions that are visi-\nble in specific spectral ranges, enabling us to obtain finer de-\ntails that can be leveraged in various domains such as cultural\nheritage [10, 11, 12], smart farming [5, 8, 7], document anal-\nsis [13], face recognition [14], and other fields. This spectral\nsegmentation approach offers insights and solutions for diverse\napplications in these domains. In the scope of the evaluation,\nwe demonstrate that spectral scene understanding enables ef-\nficient and accurate scene editing techniques, including style\ntransfer, in-painting, and removal."}, {"title": "2.4. Spectral renderers", "content": "Spectral rendering engines such as ART [97], PBRT v3 [98],\nand Mitsuba [99] are commonly utilized by the scientific com-\nmunity. While CPU-based renderers are more prevalent, there\nis a growing trend of GPU-based spectral renderers that lever-\nage GPU acceleration. Some examples of GPU-based spec-\ntral renderers include Mitsuba 2 [100], PBRT v4 [101], and\nMalia [102]. These renderers play a crucial role in simulat-\ning real-world spectral data and are gaining recognition in the\nfield. To achieve computational efficiency in deep learning and\nfocus on relevant spectral information, we adopt a sparse spec-\ntral rendering approach using multi-view spectrum maps. This"}, {"title": "3. Background", "content": "The human eye is sensitive to only a certain range in the elec-\ntromagnetic spectrum (for wavelengths between about 380nm\nand 780nm) which varies between subjects. The response curve\nof the human eye is to the red, green and blue wavelengths\nwere determined using color matching functions which has\nbeen standardised by CIE in 1932 [103]. Given a spectral power\ndistribution L(A), its corresponding CIE tristimulus values X, Y\nand Z can be computed by convolution of the L(A) with the\nappropriate color matching functions $f_x(\\lambda)$,$f_y(\\lambda)$, $f_z(\\lambda)$ as rep-\nresented in the following equations [104]:\n$X = \\int_{380}^{780} f_x(\\lambda)L(\\lambda)d\\lambda$\n$Y = \\int_{380}^{780} f_y(\\lambda)L(\\lambda)d\\lambda$\n$Z = \\int_{380}^{780} f_z(\\lambda)L(\\lambda)d\\lambda$\n(1)\nThe spectral power distribution L(A) at a point x for incoming\nwavelength \u03bb; and outgoing wavelength A can be computed as\nfollows:\n$L(\\lambda, \\omega_i, \\omega_o, \\lambda_i, \\lambda_o) = \\int_{\\Omega} f_r(x, \\omega_i, \\omega_o, \\lambda_i, \\lambda_o) L_i(x, \\omega_i, \\omega_o, \\lambda_i) cos \\theta d\\omega_i$\n(2)\nwhere \u03a9 represents the hemisphere above a surface point x, $f_r$\nis the bidirectional reflectance function, $L_i$ is the incoming radi-\nance coming from incident direction $w_i$ and $w_o$ is the direction\nof the outgoing radiance.\nThe final RGB image is obtained based on the conversion\nfrom the XYZ color space to the sRGB space which involves\nthe following steps.\n\u2022 Conversion to linear RGB: This step involves using a ma-\ntrix multiplication to convert XYZ values to linear RGB\nvalues.\n$\\begin{bmatrix}\nR\\nG\\nB\n\\end{bmatrix} = (M')\\begin{bmatrix}\nX\\nY\\nZ\n\\end{bmatrix}$\n(3)\nThere are many methods [105] to convert XYZ to linear\nRGB and the value of the matrix M\u00b9 depends on it.\n\u2022 Gamma correction: Linear RGB values are gamma-\ncorrected to get sRGB values. This involves applying a\npower function with a specific gamma value (\u2248 2.2).\n\u2022 Clipping: All RGB values are clipped within the range [0,\n1]."}, {"title": "4. Methodology", "content": "We propose an end-to-end spectral Gaussian splatting ap-\nproach that enables physically-based rendering, relighting, and\nsemantic segmentation of a scene. Our method is built upon the\nGaussian splatting architecture [44] and leverages the Gaussian\nshader [94] for the accurate estimation of BRDF parameters\nand illumination. By employing Gaussian grouping [39], we\neffectively group 3D Gaussian splats with similar semantic in-\nformation. Our framework excels in generating full spectra ren-\ndering and conveniently initializes common features from other\nspectra trained to a specific iteration, ensuring improved recon-\nstruction of splats. In Figure 1, we showcase our proposed\nspectral Gaussian splatting framework, which uses a Spectral\nGaussian model to predict BRDF parameters, distilled feature\nfields, and light per spectrum from multi-view spectrum-maps.\nOur method combines segmentation, appearance modeling, and\nsparse spectral scene representation in an end-to-end manner.\nThereby it enhances BRDF estimation by incorporating spec-\ntral information. The framework has applications in material\nrecognition, spectral analysis, reflectance estimation, segmen-\ntation, illumination correction, and inpainting.\nIn the following subsections, we provide further details re-\ngarding the spectral model, covering topics such as appear-\nance modeling, spectral semantic scene representation, spectral\nscene editing, and the seamless integration of these aspects into\nthe 3DGS framework."}, {"title": "4.1. Spectral Gaussian splatting", "content": "The above steps can be combined to get the final transforma-\ntion matrix (M) directly get the sRGB values:\n$\\begin{bmatrix}\nR\\nG\\nB\\end{bmatrix} = (M)\\begin{bmatrix}\nX\\nY\\nZ\n\\end{bmatrix}$\n(4)\nBased on equations(1), (2), and (4), the RGB values per spec-\ntra maps [38] can be computed according to\n$\\begin{bmatrix}\nR\\nG\\nB_{\\lambda}\n\\end{bmatrix} = \\begin{bmatrix}\nM_{11}f_x(\\lambda) + M_{12}f_y(\\lambda) + M_{13}f_z(\\lambda) \\\nM_{21}f_x(\\lambda) + M_{22}f_y(\\lambda) + M_{23}f_z(\\lambda) \\\nM_{31}f_x(\\lambda) + M_{32}f_y(\\lambda) + M_{33}f_z(\\lambda) \\\n\\end{bmatrix} L(\\lambda)$\n(5)"}, {"title": "4.2. Spectral appearance modelling", "content": "In order to support material editing and re-lighting, we use an\nenhanced representation of appearance by replacing the spher-\nical harmonic co-efficients by a shading function, which incor-\nporates diffuse color, roughness, specular tint and normal in-\nformation and a differentiable environment light map to model\ndirect lighting similar to the Gaussian shader [94]\nThereby, the rendered color per spectrum of a Gaussian\nsphere can be computed by considering its diffuse color, spec-\nular tint, direct specular light, normal vector and roughness ac-\ncording to\n$c(w_o)_\\lambda = \\gamma (c_{d\\lambda} + s_\\lambda \\odot L_{s\\lambda}(w_o,n,\\rho_\\lambda))$\n(6)\nwhere,c(wo) represents the rendered color per spectrum for the\nviewing direction w. The function y is a gamma tone map-\nping function that adjusts the color values for display purposes.\n$c_{d\\lambda} \\in [0,1]^3$ denotes the diffuse color of the Gaussian sphere,\nspecifying the color appearance under diffuse lighting per spec-\ntrum. $s_\\lambda \\in [0,1]^3$ is the specular tint on the sphere, indicating\nthe color of the specular highlights per spectrum. $L_{s\\lambda}(w_o, n, \\rho_\\lambda)$\ndescribes the direct specular light for the Gaussian sphere in\nthe viewing direction wo per spectrum, considering the surface\nnormal n and roughness pr. n is the normal vector indicating\nthe surface orientation, and $\\rho_\\lambda \\in [0,1]$ represents the surface\nsmoothness or roughness per spectrum.\nThe shading model is motivated by two aspects:\n\u2022 The diffuse color ($c_{d\\lambda}$) represents the consistent colors of\nthe Gaussian sphere and remains unchanged with viewing\ndirections.\n\u2022 The terms $L_{s\\lambda}(w_o, n, \\rho_\\lambda)$ describes the interaction be-\ntween the intrinsic surface color $s_\\lambda$ (specular tint) and the\ndirect specular light Ls. This term accounts for most of\nthe reflections in rendering.\nTo compute the specular light per spectrum Ls in the shading\nmodel, the incoming radiance is integrated with the specular\nGGX Normal Distribution Function D [106]. The integral is\ntaken over the entire upper semi-sphere \u03a9 and is given by:\n$L_{s\\lambda}(w_o, n, \\rho_\\lambda) = \\int_{\\Omega} L(w_i)D(r, \\rho_\\lambda) (w_i\\cdot n) dw_i$\n(7)\nHere, \u03a9 represents the whole upper hemi-sphere, wi is the\ndirection for the input radiance, and D characterizes the specu-\nlar lobe (effective integral range). The reflective direction r is\ncalculated using the view direction wo and the surface normal n\nas $r = 2(w_o\\cdot n) n - w_o$. $L_{s\\lambda}$ represents the direct specular light\nper spectral band \u03bb."}, {"title": "4.3. Spectral semantic scene representation", "content": "Per-spectrum segmentation maps serve multiple purposes in\nvarious applications. They enable sparse scene representation,\nallowing for detailed identification of specific regions of inter-\nest and the detection of attributes like material composition or\ntexture. These maps are beneficial for tasks like inpainting and\nstatue restoration, where spectral information is crucial for ac-\ncurate and realistic results. Additionally, per-spectrum segmen-\ntation maps aid in anomaly detection by analyzing the spectral\nproperties of different regions and identifying deviations from\nexpected patterns. This approach of segmenting different spec-\ntra enables the identification of specific regions of interest, such\nas the detection of grey mould disease in strawberry plants [7].\nOverall, these maps provide valuable insights into the scene, al-\nlowing for more robust and precise image processing and analy-\nsis. Our framework utilizes the Gaussian grouping method [39]\nto generate per-spectrum segmentation of the splats. This en-\nsures consistent mask identities across different views of the\nscene and groups 3D Gaussian splats with the same seman-\ntic information. To create ground truth multi-view segmenta-\ntion maps for each spectrum, we employ the Segment Anything\nModel (SAM) [95] along with a zero-shot tracker [107]. This\ncombination automatically generates masks for each image in\nthe multi-view collection per spectrum, ensuring that each 2D\nmask corresponds to a unique identity in the 3D scene. By as-\nsociating masks of the same identity across different views, we\ncan determine the total number of objects present in the 3D\nscene.\nIn addition to the existing appearance and lighting properties,\na novel attribute called Identity Encoding is assigned to each\nspectral Gaussian, similar to Gaussian grouping [39]. The Iden-\ntity Encoding is a compact and learnable vector (of length 16)\nthat effectively distinguishes different objects or parts within\nthe scene. During training, similar to using Spherical Harmonic\ncoefficients to represent color, the method optimizes the Iden-\ntity Encoding vector to represent the instance ID of the scene.\nUnlike view-dependent appearance modeling, the instance ID\nremains consistent across different rendering views, as only the\ndirect-current component of the Identity Encoding is generated\nby setting the Spherical Harmonic degree to 0.\nThe final rendered 2D mask identity feature, denoted as Eid,\nfor each pixel per spectrum A is calculated by taking a weighted\nsum over the Identity Encoding (ei) of each Gaussian per spec-\ntrum. The weights are determined by the influence factor a of\nthe respective Gaussian on that pixel per spectrum. Mathemat-ically, this can be expressed as\n$E_{id} = \\sum_{i \\in N} e_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j)$\n(8)\nwhere N represents the total number of Gaussians.\nTo group the 3D Gaussians based on their object mask iden-\ntities, a grouping loss Lid, is computed per spectra. This loss\nhas two components, i.e. it can be formulated as\n$L_{id} = L_{2d\\lambda} + L_{3d\\lambda}$\n(9)\nwhere the first component $L_{2d\\lambda}$ is the 2D Identity Loss, which\ninvolves a softmax function to classify the rendered 2D features\nEid (see Equation 8) into K5 +1 categories, representing the total\nnumber of masks per spectrum in the 3D scene. The standard\ncross-entropy loss $L_{2d\\lambda}$ for the classification of K + 1 categories\nis applied. So given the rendered 2D features Eidx as input, a\nlinear layer is first applied f to restore its feature dimension\nback to K:\n$f(E_{id \\lambda}) = W\\cdot E_{id \\lambda} + b,$\n(10)\nwhere W represents the learnable weight matrix and b is the\nbias term.\nTo obtain the probabilities for each category, we apply the\nsoftmax function:\n$softmax(f(E_{id \\lambda})) = \\frac{exp(f(E_{id \\lambda}))}{\\sum_{k=1}^{K} exp(f(E_{id \\lambda}))}$\n(11)\nFor the identity classification task with K categories per spec-\ntrum d, we utilize the standard cross-entropy loss:\n$L_{2d\\lambda} = - \\sum_{i=1}^K y_i log(softmax(f(E_{id \\lambda})),$\n(12)\nwhere y is the ground truth label for each category.\nThe second component is the 3D Regularization Loss $L_{3d\\lambda}$,\nwhich capitalizes on the 3D spatial consistency to regulate the\nlearning process of the Identity Encoding $e_i$ per spectrum \u03bb.\nThis loss ensures that the Identity Encodings of the top k-\nnearest 3D Gaussians are similar in terms of their feature dis-\ntance, thereby promoting spatially consistent grouping. The 3D\ngrouping loss per spectrum A and sampled m points is computed\nas:\n$L_{3d\\lambda} = \\frac{1}{m} \\sum_{j=1}^{m} D_{KL}(P||Q) = \\frac{1}{m} \\sum_{j=1}^{m} \\sum_{i=1}^{k} F(e_{i,\\lambda}) log( \\frac{F(e_{i,\\lambda})}{\\sum_{e_{k,\\lambda} \\in Q} F(e_{k,\\lambda})})$\n(13)\nHere, P contains the sampled Identity Encoding en of a 3D\nGaussian, and $Q = e_{1,\\lambda}, e_{2,\\lambda}, \\ldots, e_{k,\\lambda}$, represents its k nearest neigh-\nbors in 3D Euclidean space."}, {"title": "4.4. Combined (Semantic and appearance) spectral model", "content": "Combined with the original 3D Gaussian loss [44] (we use\ny instead of as we use A to denote the spectral bands) on\nimage rendering (we use the appearance model as explained in\nthe Sec. 4.2 instead of spherical harmonics), the total loss per\nspectra Lrenders for fully end-to-end training is given by\n$L_{render} = (1 - \\gamma)L_\\lambda + \\gamma L_{D-SSIM \\lambda} + \\gamma_2 L_{2d\\lambda} + \\gamma_3 L_{3d\\lambda}$\n(14)\nThe total loss is given by\n$L_{render,total} = \\sum_{\\lambda=1}^{n_{\\lambda}} L_{render}$\n(15)\nwhere na is the total number of spectral bands.\nTo enhance the optimization process and improve robustness,\nthe model is initially trained for a specific warm-up iteration\n(1000 iterations) without incorporating the full-spectra spec-\ntrum maps. Following this, the common BRDF parameters and\nnormals for the full-spectra are initialized (see Fig. 1) using the\naverage values from all other spectra, and this initialization step\nis integrated into the training process. By including these ad-\nequate priors, the optimization of parameters is guided more\neffectively, leading to better outcomes as demonstrated in the\nquantitative and qualitative analysis."}, {"title": "4.5. Spectral scene editing", "content": "Our framework extends scene editing techniques, such as\nGaussian Grouping [39], into the spectral domain, unlocking\na wide range of possibilities. By leveraging the semantic in-\nformation present in any of the spectrum maps, we can achieve\nobject deletion, in-painting, and style-transfer. Figure 2 illus-\ntrates the utilization of segmentation maps obtained from the\n450 nm spectrum for the stylization of the splats across the full\nspectra.\nTo accomplish this, we transfer the style to the multi-view\nfull spectra maps and perform object in-painting through a fine-\ntuning of the splats, similar to Gaussian grouping [39], using\nthe new ground truth (multi-view semantic stylized maps). The\nsignificance of this capability is particularly evident in fields\nlike cultural heritage, where the retrieval of color information"}, {"title": "5. Experiments", "content": "To demonstrate the potential of our approach, we provide\nboth quantitative and qualitative evaluations with comparisons\nto baseline techniques."}, {"title": "5.1. Baseline techniques used for comparison", "content": "The techniques used as a reference in the scope of the eval-\nuation include several state-of-the-art variants of Neural Radi-\nance Fields (NeRF) (i.e., NeRF [36], MIP-NeRF [48], Aug-\nNeRF [108], Ref-NeRF [45]) (which considers appearance pa-\nrameters) and Gaussian splatting (i.e., Gaussian splatting with-\nout special reflectance modeling [44] and Gaussian Shader that\nspecifically models reflectance [94]) as well as the respective\nextensions of such modern scene representation approaches to\nthe spectral domain (i.e., SpectralNeRF [38] and Cross-spectral\nNeRF [37])."}, {"title": "5.2. Datasets", "content": "For the comparison with SpectralNeRF, we use both syn-\nthetic and real-world multi-spectral videos [38]. The poses\nfor the digger, spaceship, and vintage car models were esti-\nmated using DUSt3R [109] since reconstruction failed with\nCOLMAP [110]. For the remaining scene videos (kitchen, liv-\ning room, projector, and dragon doll), COLMAP was used to\ngenerate the poses."}, {"title": "5.3. Implementation details", "content": "The evaluations were conducted on an Nvidia RTX 3090\ngraphics card"}]}