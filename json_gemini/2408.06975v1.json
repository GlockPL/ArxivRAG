{"title": "SpectralGaussians: Semantic, spectral 3D Gaussian splatting for multi-spectral scene\nrepresentation, visualization and analysis", "authors": ["Saptarshi Neil Sinha", "Holger Graf", "Michael Weinmann"], "abstract": "We propose a novel cross-spectral rendering framework based on 3D Gaussian Splatting (3DGS) that generates realistic and seman-\ntically meaningful splats from registered multi-view spectrum and segmentation maps. This extension enhances the representation\nof scenes with multiple spectra, providing insights into the underlying materials and segmentation. We introduce an improved\nphysically-based rendering approach for Gaussian splats, estimating reflectance and lights per spectra, thereby enhancing accu-\nracy and realism. In a comprehensive quantitative and qualitative evaluation, we demonstrate the superior performance of our\napproach with respect to other recent learning-based spectral scene representation approaches (i.e., XNeRF and SpectralNeRF) as\nwell as other non-spectral state-of-the-art learning-based approaches. Our work also demonstrates the potential of spectral scene\nunderstanding for precise scene editing techniques like style transfer, inpainting, and removal. Thereby, our contributions address\nchallenges in multi-spectral scene representation, rendering, and editing, offering new possibilities for diverse applications.", "sections": [{"title": "1. Introduction", "content": "Accurate scene representation is an essential prerequisite for\nnumerous applications. The way we perceive our surroundings\nin terms of a mixture of light gives us a particular scene under-\nstanding, thereby determining how we interact with our envi-\nronment. However, representing scenes in terms of red, green\nand blue color channels suffers from both a bad reproduction of\nthe scene's appearance due to metamerism effects and lacking\ncharacteristics only observable in certain of the spectral bands.\nTherefore, multi-spectral scene capture and representation has\nbecome of high relevance, where light and reflectance spectra\nare given with a higher resolution thereby surpassing the limi-\ntations of the broad-band RGB color model.\nIn domains such as architecture, automotive industries, ad-\nvertisement, and design, accurate modeling of light transport\nand considering the full spectrum of light is crucial for vir-\ntual prototyping. Predictive rendering, which involves simu-\nlating the spectral transport of light, is necessary to assess and\nevaluate the visual quality of products before physical produc-\ntion. This ensures reliable assessment and enables color-correct\nscene reproduction. Furthermore, spectral information as cap-\ntured by multi-spectral (MS) cameras [1, 2], infrared (IR) cam-\neras [3], and UV sensors [4] extends scene understanding in\nterms of insights on underlying material characteristics and be-\nhavior (including anomalies, defects, etc.) revealed only in cer-\ntain sub-ranges of the light spectrum which empowers experts"}, {"title": "2. Related work", "content": "In recent years, significant advancements have been made\nin generating photo-realistic novel views through the use of\nnovel learning-based scene representations combined with vol-\nume rendering techniques. Neural Radiance Fields (NeRF)\n[36, 47] represent the scene based on a neural network that pre-\ndicts local density and view-dependent color for points in the\nscene volume. This information can then be used to synthe-\nsize images of the scene using volume rendering techniques.\nThe network representing the scene is trained by minimizing\nthe deviation of the predicted images to their respective given\ninput images under the respective view conditions, thereby ex-\nploiting the observation that an accurate scene representation\nby the network leads to an accurate image synthesis. The re-\nmarkable potential of the NeRF approach for novel view syn-\nthesis has given rise to several notable extensions. Researchers\nhave focused on improving rendering quality by addressing is-\nsues such as aliasing [48, 49, 50, 51], as well as accelerating\nnetwork training [52, 53, 54, 55, 56]. Furthermore, there have\nbeen efforts to handle more complex inputs, including uncon-\nstrained image collections [57, 58, 59], image collections re-\nquiring the refinement or complete estimation of camera pose\nparameters [60, 61, 62, 63], deformable scenes [64, 65] and\nlarge-scale scenarios [66, 67, 68]. Further works aimed at guid-\ning the training and handling textureless regions by incorporat-\ning depth cues [69, 70, 71, 72, 73]."}, {"title": "2.1. Learning-based scene representation", "content": "In recent years, significant advancements have been made\nin generating photo-realistic novel views through the use of\nnovel learning-based scene representations combined with vol-\nume rendering techniques. Neural Radiance Fields (NeRF)\n[36, 47] represent the scene based on a neural network that pre-\ndicts local density and view-dependent color for points in the\nscene volume. This information can then be used to synthe-\nsize images of the scene using volume rendering techniques.\nThe network representing the scene is trained by minimizing\nthe deviation of the predicted images to their respective given\ninput images under the respective view conditions, thereby ex-\nploiting the observation that an accurate scene representation\nby the network leads to an accurate image synthesis. The re-\nmarkable potential of the NeRF approach for novel view syn-\nthesis has given rise to several notable extensions. Researchers\nhave focused on improving rendering quality by addressing is-\nsues such as aliasing [48, 49, 50, 51], as well as accelerating\nnetwork training [52, 53, 54, 55, 56]. Furthermore, there have\nbeen efforts to handle more complex inputs, including uncon-\nstrained image collections [57, 58, 59], image collections re-\nquiring the refinement or complete estimation of camera pose\nparameters [60, 61, 62, 63], deformable scenes [64, 65] and\nlarge-scale scenarios [66, 67, 68]. Further works aimed at guid-\ning the training and handling textureless regions by incorporat-\ning depth cues [69, 70, 71, 72, 73]."}, {"title": "2.2. Radiance based appearance capture", "content": "Instead of focusing on the pure reproduction of a scene\naccording to the original NeRF formulation without explic-\nitly modeling reflectance and illumination characteristics, sev-\neral NeRF extensions focused on modeling reflectance by sep-\narating visual appearance into lighting and material proper-\nties. Respective approaches have the capability to jointly pre-\ndict environmental illumination and surface reflectance prop-\nerties even in the presence of unknown or varying lighting\nconditions [83, 84, 85, 86, 87, 88].\nOne notable contribution is Ref-NeRF [45], which in-\ntroduces a novel parameterization and structuring of view-\ndependent outgoing radiance, along with a regularizer on nor-\nmal vectors. This enhances the accuracy in predicting re-\nflectance properties. To address the challenge of learning ge-\nometry from highly specular surfaces, recent works [89, 90, 91]\nhave utilized SDF-based representations. This enables more\nprecise estimation of surface normals for physically based ren-\ndering. However, these methods suffer from time-consuming\noptimization and slow rendering speed, limiting their practical\napplication in real-world scenarios. Furthermore, NVDiffRec\n[92] is an explicit representation method that directly optimizes\ntriangle meshes with materials and environment map lighting,\nenabling real-time interactive applications, unlike MLP-based\nmethods that tend to be slower.\nRelightable Gaussians [93] presents a differentiable point-\nbased rendering framework for material and lighting decom-\nposition from multi-view images, enabling real-time relighting\nand editing of 3D point clouds. It surpasses existing material\nestimation approaches and offers improved results. Gaussian-\nShader [94] is another method that enhances neural rendering in\nscenes with reflective surfaces by applying a simplified shading\nfunction on 3D Gaussians. It addresses the challenge of accu-\nrate normal estimation on discrete 3D Gaussians, achieving a\nbalance between efficiency and rendering quality. Our shading\nmodel is inspired by this method where we use the model with-\nout the residual color in the reflectance estimation."}, {"title": "2.3. Sparse spectral scene understanding", "content": "Gaussian splatting based semantic segmentation frame-\nworks, such as Gaussian Grouping [39] and LangSplat [40],\nhave successfully utilized foundation models like Segment\nAnything [95] to segment scenes. LangSplat is a 3D lan-\nguage field that enables precise and efficient open-vocabulary\nquerying within 3D spaces by representing language features\nusing a collection of 3D Gaussians distilled from CLIP [96].\nGaussian Grouping extends Gaussian Splatting by incorporat-\ning object-level scene understanding and introducing Identity\nEncodings to reconstruct and segment objects in open-world\n3D scenes. We utilized this method for accurate semantic seg-\nmentation of spectral scenes. Segmenting the scene per spec-\ntra provides valuable information about regions that are visi-\nble in specific spectral ranges, enabling us to obtain finer de-\ntails that can be leveraged in various domains such as cultural\nheritage [10, 11, 12], smart farming [5, 8, 7], document anal-\nsys [13], face recognition [14], and other fields. This spectral\nsegmentation approach offers insights and solutions for diverse\napplications in these domains. In the scope of the evaluation,\nwe demonstrate that spectral scene understanding enables ef-\nficient and accurate scene editing techniques, including style\ntransfer, in-painting, and removal."}, {"title": "2.4. Spectral renderers", "content": "Spectral rendering engines such as ART [97], PBRT v3 [98],\nand Mitsuba [99] are commonly utilized by the scientific com-\nmunity. While CPU-based renderers are more prevalent, there\nis a growing trend of GPU-based spectral renderers that lever-\nage GPU acceleration. Some examples of GPU-based spec-\ntral renderers include Mitsuba 2 [100], PBRT v4 [101], and\nMalia [102]. These renderers play a crucial role in simulat-\ning real-world spectral data and are gaining recognition in the\nfield. To achieve computational efficiency in deep learning and\nfocus on relevant spectral information, we adopt a sparse spec-\ntral rendering approach using multi-view spectrum maps. This\ntechnique enables faster computations by reducing unimportant\nspectral data while preserving the necessary information for re-\nalistic rendering of spectral scenes. By leveraging spectrum\nmaps from multiple viewpoints, high-quality spectral render-\nings are generated with a reduced computational cost compared\nto full-resolution spectral rendering methods."}, {"title": "3. Background", "content": "The human eye is sensitive to only a certain range in the elec-\ntromagnetic spectrum (for wavelengths between about 380nm\nand 780nm) which varies between subjects. The response curve\nof the human eye is to the red, green and blue wavelengths\nwere determined using color matching functions which has\nbeen standardised by CIE in 1932 [103]. Given a spectral power\ndistribution L(\u03bb), its corresponding CIE tristimulus values X, Y\nand Z can be computed by convolution of the L(\u03bb) with the\nappropriate color matching functions fx(\u03bb),fy(\u03bb), fz(1) as rep-\nresented in the following equations [104]:\n$X = \\int_{380}^{780} f_x(\\lambda)L(\\lambda)d\\lambda$\n$Y = \\int_{380}^{780} f_y(\\lambda)L(\\lambda)d\\lambda$ \n$Z = \\int_{380}^{780} f_z(\\lambda)L(\\lambda)d\\lambda$\nThe spectral power distribution L(\u03bb) at a point x for incoming\nwavelength \u03bb; and outgoing wavelength \u03bb can be computed as\nfollows:\n$L(x, \\omega_i, \\omega_o, \\lambda_i, \\lambda_o)=\\int_{\\Omega} f_r(x, \\omega_i, \\omega_o, \\lambda_i, \\lambda_o) L_i(x, \\omega_i, \\omega_o, \\lambda_i) cos \\theta d\\omega_i$\nwhere \u03a9 represents the hemisphere above a surface point x, fr\nis the bidirectional reflectance function, L\u2081 is the incoming radi-\nance coming from incident direction w\u2081 and w is the direction\nof the outgoing radiance.\nThe final RGB image is obtained based on the conversion\nfrom the XYZ color space to the sRGB space which involves\nthe following steps.\n\u2022 Conversion to linear RGB: This step involves using a ma-\ntrix multiplication to convert XYZ values to linear RGB\nvalues.\n$\\begin{bmatrix}R\\\\G\\\\B\\end{bmatrix}=(M')\\begin{bmatrix}X\\\\Y\\\\Z\\end{bmatrix}$\nThere are many methods [105] to convert XYZ to linear\nRGB and the value of the matrix M\u00b9 depends on it.\n\u2022 Gamma correction: Linear RGB values are gamma-\ncorrected to get sRGB values. This involves applying a\npower function with a specific gamma value (\u2248 2.2).\n\u2022 Clipping: All RGB values are clipped within the range [0,\n1]."}, {"title": "4. Methodology", "content": "We propose an end-to-end spectral Gaussian splatting ap-\nproach that enables physically-based rendering, relighting, and\nsemantic segmentation of a scene. Our method is built upon the\nGaussian splatting architecture [44] and leverages the Gaussian\nshader [94] for the accurate estimation of BRDF parameters\nand illumination. By employing Gaussian grouping [39], we\neffectively group 3D Gaussian splats with similar semantic in-\nformation. Our framework excels in generating full spectra ren-\ndering and conveniently initializes common features from other\nspectra trained to a specific iteration, ensuring improved recon-\nstruction of splats. In Figure 1, we showcase our proposed\nspectral Gaussian splatting framework, which uses a Spectral\nGaussian model to predict BRDF parameters, distilled feature\nfields, and light per spectrum from multi-view spectrum-maps.\nOur method combines segmentation, appearance modeling, and\nsparse spectral scene representation in an end-to-end manner.\nThereby it enhances BRDF estimation by incorporating spec-\ntral information. The framework has applications in material\nrecognition, spectral analysis, reflectance estimation, segmen-\ntation, illumination correction, and inpainting.\nIn the following subsections, we provide further details re-\ngarding the spectral model, covering topics such as appear-\nance modeling, spectral semantic scene representation, spectral\nscene editing, and the seamless integration of these aspects into\nthe 3DGS framework."}, {"title": "4.1. Spectral Gaussian splatting", "content": "We propose an end-to-end spectral Gaussian splatting ap-\nproach that enables physically-based rendering, relighting, and\nsemantic segmentation of a scene. Our method is built upon the\nGaussian splatting architecture [44] and leverages the Gaussian\nshader [94] for the accurate estimation of BRDF parameters\nand illumination. By employing Gaussian grouping [39], we\neffectively group 3D Gaussian splats with similar semantic in-\nformation. Our framework excels in generating full spectra ren-\ndering and conveniently initializes common features from other\nspectra trained to a specific iteration, ensuring improved recon-\nstruction of splats. In Figure 1, we showcase our proposed\nspectral Gaussian splatting framework, which uses a Spectral\nGaussian model to predict BRDF parameters, distilled feature\nfields, and light per spectrum from multi-view spectrum-maps.\nOur method combines segmentation, appearance modeling, and\nsparse spectral scene representation in an end-to-end manner.\nThereby it enhances BRDF estimation by incorporating spec-\ntral information. The framework has applications in material\nrecognition, spectral analysis, reflectance estimation, segmen-\ntation, illumination correction, and inpainting.\nIn the following subsections, we provide further details re-\ngarding the spectral model, covering topics such as appear-\nance modeling, spectral semantic scene representation, spectral\nscene editing, and the seamless integration of these aspects into\nthe 3DGS framework."}, {"title": "4.2. Spectral appearance modelling", "content": "In order to support material editing and re-lighting, we use an\nenhanced representation of appearance by replacing the spher-\nical harmonic co-efficients by a shading function, which incor-\nporates diffuse color, roughness, specular tint and normal in-\nformation and a differentiable environment light map to model\ndirect lighting similar to the Gaussian shader [94]\nThereby, the rendered color per spectrum of a Gaussian\nsphere can be computed by considering its diffuse color, spec-\nular tint, direct specular light, normal vector and roughness ac-\ncording to\n$c(\\omega_o)_\\lambda = \\gamma \\left(c_{d\\lambda} + s_\\lambda \\odot L_{s\\lambda} (\\omega_o, n, \\rho_\\lambda)\\right)$ \nwhere,c(wo) represents the rendered color per spectrum for the\nviewing direction w. The function y is a gamma tone map-\nping function that adjusts the color values for display purposes.\ncda \u2208 [0,1]3 denotes the diffuse color of the Gaussian sphere,\nspecifying the color appearance under diffuse lighting per spec-\ntrum. sa \u2208 [0,1]3 is the specular tint on the sphere, indicating\nthe color of the specular highlights per spectrum. Ls, (\u03c9\u03bf, \u03b7, \u03c1\u03bb)\ndescribes the direct specular light for the Gaussian sphere in\nthe viewing direction wo per spectrum, considering the surface\nnormal n and roughness pr. n is the normal vector indicating\nthe surface orientation, and pr\u2208 [0,1] represents the surface\nsmoothness or roughness per spectrum.\nThe shading model is motivated by two aspects:\n\u2022 The diffuse color (cd) represents the consistent colors of\nthe Gaussian sphere and remains unchanged with viewing\ndirections.\n\u2022 The terms Ls (\u03c9\u03bf, \u03b7, \u03c1\u03bb) describes the interaction be-\ntween the intrinsic surface color sa (specular tint) and the\ndirect specular light Ls. This term accounts for most of\nthe reflections in rendering.\nTo compute the specular light per spectrum Ls in the shading\nmodel, the incoming radiance is integrated with the specular\nGGX Normal Distribution Function D [106]. The integral is\ntaken over the entire upper semi-sphere \u03a9 and is given by:\n$L_{s\\lambda} (\\omega_o, n, \\rho_\\lambda) = \\int_{\\Omega} L(\\omega_i)D(r, \\rho_\\lambda)(\\omega_i \\cdot n)d\\omega_i$\nHere, \u03a9 represents the whole upper hemi-sphere, wi is the\ndirection for the input radiance, and D characterizes the specu-\nlar lobe (effective integral range). The reflective direction r is\ncalculated using the view direction wo and the surface normal n\nas r = 2(wn) n wo. Lsa represents the direct specular light\nper spectral band \u03bb."}, {"title": "4.3. Spectral semantic scene representation", "content": "Per-spectrum segmentation maps serve multiple purposes in\nvarious applications. They enable sparse scene representation,\nallowing for detailed identification of specific regions of inter-\nest and the detection of attributes like material composition or\ntexture. These maps are beneficial for tasks like inpainting and\nstatue restoration, where spectral information is crucial for ac-\ncurate and realistic results. Additionally, per-spectrum segmen-\ntation maps aid in anomaly detection by analyzing the spectral\nproperties of different regions and identifying deviations from\nexpected patterns. This approach of segmenting different spec-\ntra enables the identification of specific regions of interest, such\nas the detection of grey mould disease in strawberry plants [7].\nOverall, these maps provide valuable insights into the scene, al-\nlowing for more robust and precise image processing and analy-\nsis. Our framework utilizes the Gaussian grouping method [39]\nto generate per-spectrum segmentation of the splats. This en-\nsures consistent mask identities across different views of the\nscene and groups 3D Gaussian splats with the same seman-\ntic information. To create ground truth multi-view segmenta-\ntion maps for each spectrum, we employ the Segment Anything\nModel (SAM) [95] along with a zero-shot tracker [107]. This\ncombination automatically generates masks for each image in\nthe multi-view collection per spectrum, ensuring that each 2D\nmask corresponds to a unique identity in the 3D scene. By as-\nsociating masks of the same identity across different views, we\ncan determine the total number of objects present in the 3D\nscene.\nIn addition to the existing appearance and lighting properties,\na novel attribute called Identity Encoding is assigned to each\nspectral Gaussian, similar to Gaussian grouping [39]. The Iden-\ntity Encoding is a compact and learnable vector (of length 16)\nthat effectively distinguishes different objects or parts within\nthe scene. During training, similar to using Spherical Harmonic\ncoefficients to represent color, the method optimizes the Iden-\ntity Encoding vector to represent the instance ID of the scene.\nUnlike view-dependent appearance modeling, the instance ID\nremains consistent across different rendering views, as only the\ndirect-current component of the Identity Encoding is generated\nby setting the Spherical Harmonic degree to 0.\nThe final rendered 2D mask identity feature, denoted as Eid,\nfor each pixel per spectrum A is calculated by taking a weighted\nsum over the Identity Encoding (ei) of each Gaussian per spec-\ntrum. The weights are determined by the influence factor a of\nthe respective Gaussian on that pixel per spectrum. Mathemat-\nically, this can be expressed as\n$E_{id} = \\sum_{i \\in N} e_{i\\lambda} a_{i\\lambda} \\prod_{j=1}^{i-1} (1 - a_j)$\nwhere N represents the total number of Gaussians.\nTo group the 3D Gaussians based on their object mask iden-\ntities, a grouping loss Lida is computed per spectra. This loss\nhas two components, i.e. it can be formulated as\n$L_{id} = L_{2d} + L_{3d}$\nwhere the first component L2dr is the 2D Identity Loss, which\ninvolves a softmax function to classify the rendered 2D features\nEid (see Equation 8) into K5 +1 categories, representing the total\nnumber of masks per spectrum in the 3D scene. The standard\ncross-entropy loss L2da for the classification of K + 1 categories\nis applied. So given the rendered 2D features Eida as input, a\nlinear layer is first applied f to restore its feature dimension\nback to K:\n$f(E_{id\\lambda}) = W \\cdot E_{id\\lambda} + b,$\nwhere W represents the learnable weight matrix and b is the\nbias term.\nTo obtain the probabilities for each category, we apply the\nsoftmax function:\n$softmax(f(E_{id\\lambda})) = \\frac{exp(f(E_{id\\lambda}))}{\\sum_{i=1}^{K} exp(f(E_{id\\lambda}))}$\nFor the identity classification task with K categories per spec-\ntrum d, we utilize the standard cross-entropy loss:\n$L_{2d\\lambda} = - \\sum_{i=1}^{K} y_i \\log(softmax(f(E_{id\\lambda}))) ,$\nwhere y is the ground truth label for each category.\nThe second component is the 3D Regularization Loss L3d,\nwhich capitalizes on the 3D spatial consistency to regulate the\nlearning process of the Identity Encoding ei per spectrum \u03bb.\nThis loss ensures that the Identity Encodings of the top k-\nnearest 3D Gaussians are similar in terms of their feature dis-\ntance, thereby promoting spatially consistent grouping. The 3D\ngrouping loss per spectrum A and sampled m points is computed\nas:\n$L_{3d} = \\frac{1}{m} \\sum_{j=1}^{m} D_{KL}(P||Q)  = \\frac{1}{mk} \\sum_{j=1}^{m} \\sum_{i=1}^{k} F(e_{i_{\\lambda}}) log(\\frac{F(e_{i_{\\lambda}})}{F(e_{j_{\\lambda}})})$  \nHere, P contains the sampled Identity Encoding en of a 3D\nGaussian, and Q = e1\u2081, e2\u2081, \u2026\u2026\u2026, ek, represents its k nearest neigh-\nbors in 3D Euclidean space."}, {"title": "4.4. Combined (Semantic and appearance) spectral model", "content": "Combined with the original 3D Gaussian loss [44] (we use\ny instead of as we use A to denote the spectral bands) on\nimage rendering (we use the appearance model as explained in\nthe Sec. 4.2 instead of spherical harmonics), the total loss per\nspectra Lrenders for fully end-to-end training is given by\n$L_{render} = (1 - \\gamma)L_{img} + \\gamma L_{D-SSIM} + \\gamma_2L_{2d\\lambda} + \\gamma_3L_{3d\\lambda}$\nThe total loss is given by\n$L_{render,total} = \\sum_{\\lambda=1}^{n_\\lambda} L_{render}$\nwhere na is the total number of spectral bands.\nTo enhance the optimization process and improve robustness,\nthe model is initially trained for a specific warm-up iteration\n(1000 iterations) without incorporating the full-spectra spec-\ntrum maps. Following this, the common BRDF parameters and\nnormals for the full-spectra are initialized (see Fig. 1) using the\naverage values from all other spectra, and this initialization step\nis integrated into the training process. By including these ad-\nequate priors, the optimization of parameters is guided more\neffectively, leading to better outcomes as demonstrated in the\nquantitative and qualitative analysis."}, {"title": "4.5. Spectral scene editing", "content": "Our framework extends scene editing techniques, such as\nGaussian Grouping [39], into the spectral domain, unlocking\na wide range of possibilities. By leveraging the semantic in-\nformation present in any of the spectrum maps, we can achieve\nobject deletion, in-painting, and style-transfer. Figure 2 illus-\ntrates the utilization of segmentation maps obtained from the\n450 nm spectrum for the stylization of the splats across the full\nspectra.\nTo accomplish this, we transfer the style to the multi-view\nfull spectra maps and perform object in-painting through a fine-\ntuning of the splats, similar to Gaussian grouping [39], using\nthe new ground truth (multi-view semantic stylized maps). The\nsignificance of this capability is particularly evident in fields\nlike cultural heritage, where the retrieval of color information\nfrom a specific spectral band enables the accurate restoration of\nmissing color details throughout the full-spectrum. By leverag-\ning these advancements, we can enhance various applications\nand open up new avenues for exploration."}, {"title": "5. Experiments", "content": "To demonstrate the potential of our approach, we provide\nboth quantitative and qualitative evaluations with comparisons\nto baseline techniques."}, {"title": "5.1. Baseline techniques used for comparison", "content": "The techniques used as a reference in the scope of the eval-\nuation include several state-of-the-art variants of Neural Radi-\nance Fields (NeRF) (i.e., NeRF [36], MIP-NeRF [48], Aug-\nNeRF [108], Ref-NeRF [45]) (which considers appearance pa-\nrameters) and Gaussian splatting (i.e., Gaussian splatting with-\nout special reflectance modeling [44] and Gaussian Shader that\nspecifically models reflectance [94]) as well as the respective\nextensions of such modern scene representation approaches to\nthe spectral domain (i.e., SpectralNeRF [38] and Cross-spectral\nNeRF [37])."}, {"title": "5.2. Datasets", "content": "For the comparison with SpectralNeRF, we use both syn-\nthetic and real-world multi-spectral videos [38]. The poses\nfor the digger, spaceship, and vintage car models were esti-\nmated using DUSt3R [109] since reconstruction failed with\nCOLMAP [110]. For the remaining scene videos (kitchen, liv-\ning room, projector, and dragon doll), COLMAP was used to\ngenerate the poses.\nTo demonstrate the adaptability of our method in han-\ndling cross-spectral data (infrared and multi-spectral), we con-\nducted a comparative analysis using the cross-spectral NeRF\ndataset [37]. We created the ground truth full spectrum image\nfrom the cross-spectral spectrum-maps. For this, we averaged\nthe images from all spectra and applied the colormaps viridis\nand magma for the multi-spectral and infrared dataset respec-\ntively, similar to the approach used in cross-spectral NeRF [37].\nTo further validate that the spectral appearance estimation pro-\nduces plausible results for different types of scenes (having\nalso highly-reflective objects in the scene), we created a syn-\nthetic multi-spectral dataset from the shiny blender dataset [45]\nand synthetic NeRF dataset [36] (see Figure 3). We generated\nthis multi-spectral dataset using Mitsuba [46] for 5 bands from\n460nm to 620nm similar to SpectralNeRF [38]. We generated\nthe data for the scenes where the shading model supported in\nMitsuba corresponded to the shading model in Blender in order"}, {"title": "5.3. Implementation details", "content": "The evaluations were conducted on an Nvidia RTX 3090\ngraphics card. In most scenes, we used a total of 30,000 itera-\ntions, except for the digger, spaceship, and vintage car scenes\nwhere we used 40,000 iterations. For the comparison to other\nmethods, we used the results reported in their original publica-\ntions."}, {"title": "5.4. Quantitative analysis", "content": "Quantitative analysis was performed on all datasets men-\ntioned in Section 5.2 and overview of the number of scenes,\nmulti-view images and number of iterations for which each\nscene was trained is presented in Table 2. We compute the\nPSNR [111], SSIM [112] and LPIPS [113] for all camera-views\nand report average the average result. The orange in the tables\nrepresents the best result and yellow represents the second best\nresults."}, {"title": "5.4.1. Comparison with radiance-field-based spectral methods", "content": "The quantitative analysis shows that our method overall out-\nperforms the existing spectral methods [37, 38] for both multi-\nspectral and cross-spectral data. The results presented in Ta-\nble 4.4 indicate that our method outperforms SpectralNeRF in\nmost scenes and on average for the synthetic dataset. Addi-\ntionally, our analysis, as shown in Table 5.4, reveals that our\nmethod also surpasses SpectralNeRF when applied to the real-\nworld dataset. It is important to note that due to the unavail-\nability of all datasets and test views from the original paper, our\nevaluation was limited to only one real-world dataset (see Table\n5.4) for the SpectralNeRF method. However, we also compare\nour method based on the Cross-spectral NeRF dataset which\ncontains only real-world scenes. Here, our method clearly per-\nforms better for all scenes (multi-spectral and infrared datasets)\nas presented in Table 5.4. This shows that our method produces\nplausible results with real-world scenes and outperforms state-\nof-the-art spectral methods."}, {"title": "5.4.2. Comparison with non-spectral radiance-field-based\nmethods", "content": "To demonstrate that our method produces plausible results\ncompared to existing state-of-the-art Gaussian splatting meth-\nods [94, 44], we conducted a comparison using spectral datasets\ncreated from both the NeRF synthetic dataset and the shiny\nBlender dataset, as described in Section 5.2. The analysis re-\nveals that our method consistently outperforms existing meth-"}, {"title": "5.6. Ablation study", "content": "In this section, we conduct ablations by eliminating the\nwarm-up iterations that we introduced to enhance reflectance\nand light estimations in the scene through the inclusion of\nappropriate priors from other spectra. For this, we use\nthree real-world scenes: dragon doll (from the Spectral-\nNeRF dataset [38]), orange, and tech scenes (from the Cross-\nSpectralNeRF dataset [37]). The dragon doll scene has 8 bands,\nwhile the orange and tech scenes have 10 bands.\nTo evaluate the impact of including priors from different\nspectra, we conducted a comprehensive analysis, encompass-\ning both quantitative measurements (see Table 5.6) and quali-\ntative observations (see Figure 7), after initializing the common"}, {"title": "5.7. Limitations", "content": "While the presented framework offers promising capabilities,\nit is important to acknowledge its limitations. One such limita-\ntion is the requirement for spectrum-maps to be co-registered,\nwhich can be a complex and time-intensive process. Moreover,\nas the resolution of images increases and more spectra are in-\ncorporated, the training time escalates significantly. To over-\ncome these challenges, future research can explore the integra-\ntion of alternative deep learning algorithms that support end-to-\nend training specifically for co-registering maps. Additionally,\nimproving the encoding methods to efficiently accommodate a\nlarger number of spectra would enhance the framework's capa-\nbilities.\nAnother limitation to consider is that the shading model cur-\nrently used in the framework is fixed. However, the framework\ncan be modified to have a flexible number of learnable param-\neters based on the shading model. This would allow users\nto configure the framework to their specific needs and enable\nmore customized and adaptable shading models. By address-\ning these limitations, the framework can be made more practi-\ncal and effective, enabling seamless co-registration, support for\nan expanded range of spectra, reduced training time for high-\nresolution images, and user-configurable shading models."}, {"title": "6. Conclusion", "content": "We presented 3D Spectral Gaussian Splatting, a cross-\\"}]}