{"title": "Multi-Objective Large Language Model Unlearning", "authors": ["Zibin Pan", "Shuwen Zhang", "Yuesheng Zheng", "Chi Li", "Yuheng Cheng", "Junhua Zhaot"], "abstract": "Machine unlearning in the domain of large language models (LLMs) has attracted great attention recently, which aims to effectively eliminate undesirable behaviors from LLMs without full retraining from scratch. In this paper, we explore the Gradient Ascent (GA) approach in LLM unlearning, which is a proactive way to decrease the prediction probability of the model on the target data in order to remove their influence. We analyze two challenges that render the process impractical: gradient explosion and catastrophic forgetting. To address these issues, we propose Multi-Objective Large Language Model Unlearning (MOLLM) algorithm. We first formulate LLM unlearning as a multi-objective optimization problem, in which the cross-entropy loss is modified to the unlearning version to overcome the gradient explosion issue. A common descent update direction is then calculated, which enables the model to forget the target data while preserving the utility of the LLM. Our empirical results verify that MoLLM outperforms the SOTA GA-based LLM unlearning methods in terms of unlearning effect and model utility preservation.", "sections": [{"title": "I. INTRODUCTION", "content": "The wide application of LLM in various fields raises significant concerns related to its safety, including but not limited to harmful responses [1], copyright infringement [2], hallucinations [3], and user privacy [4], [5]. An intuitive solution to address these concerns would be to retrain the model on an updated dataset that excludes undesirable sam-ples. However, such drastic measure is both time-consuming and financially prohibitive [6], [7]. Machine unlearning, in contrast, offers an efficient and cost-effective alternative by enabling the selective removal of data, allowing the LLM to \"forget\" specific information [8]\u2013[10].\nNumerous LLM unlearning techniques have been proposed recently [7]. For instance, [11] treats LLMs as a black box, and proposes an in-context unlearning method. However, the un-desirable behaviors still persist within the model, and prompt manipulation can cause the model to output undesirable in-formation [12]. Apart from that, Eldan and Russinovich [13] propose a relabeling fine-tuning method to unlearn the target information in LLM, where the label of the undesirable data is altered before fine-tuning the LLM on the updated dataset. RLHF (reinforcement learning from human feedback) and its variants [14], [15] utilize human-written outputs and fine-tuning to calibrate the responses of LLMs. Unfortunately, these methods are resource-intensive and computationally costly [7]. In resource-constrained scenarios, removing harmful responses should be prioritized over generating desirable responses [6].\nTo this end, Gradient Ascent (GA) method that increases the training loss Lfgt on the target forget data (i.e., the data that requires to be forgotten) to achieve unlearning [6], offers a resource-efficient and proactive alternative to complete retraining. Notably, GA has become a significant branch in the field of machine unlearning [16], [17]. However, two major challenges arise when adopting GA in LLM unlearning: gradient explosion and catastrophic forgetting.\nGradient explosion. Since the Cross-Entropy (CE) loss function has no upper bound, adopting GA to unlearn the target information in LLM would increase the gradient without bound and even lead to gradient explosion. To tackle this issue, one naive way is the gradient clipping method [18], which limits gradient norms with an extra hyper-parameter. Nevertheless, experimental tuning is required to find the optimal hyper-parameter. In contrast, we introduce an effective solution that replaces the Cross-Entropy loss with its unlearning version, and uses gradient descent to achieve the unlearning goal. In this way, the issue of gradient explosion could be overcome without the need of tuning extra hyper-parameters.\nCatastrophic forgetting in downstream tasks. This chal-lenge is not unique to GA-based approaches but is common across fine-tuning-based LLM unlearning methods. In LLM"}, {"title": "II. PRELIMINARIES", "content": "We follow the definition of LLM unlearning in [6], [10]. Suppose an LLM with parameters \u03b8 has been trained to conver-gence on the training data Dtr for a specific downstream task. Following the deployment of the model, some undesirable"}, {"title": "A. Unlearning Settings and Goal", "content": "samples Dfgt \u2286 Dtr are identified and need to be unlearned, while the model performance on the retain set Drt \u2286 Dtr should remain intact, with Drt \u222a Dfgt = Dtr. Unlearning is thus defined as a process that produces a new model that behaves as if it has never encountered Dfgt, while maintaining its utility on the retain set Drt."}, {"title": "B. Catastrophic Forgetting in LLM Unlearning", "content": "GA is a proactive way to unlearn Dfgt, which takes the inverse update of learning to maximize the model loss on the forget set Dfgt. It could be expressed as:\n\nLfgt:=\u2212\u2211(xfgt,yfgt)\u2208DfgtL(xfgt,yfgt;\u03b8),\n\nwhere xfgt and yfgt respectively correspond to the prompts and responses in the forget data Dfgt. L is the loss function, usually defined by Cross-Entropy loss.\n\nLCE=\u2212\u2211Kk=1\u2211Cc=1Yic\u22c5log(pic),\n\nwhere C denotes the size of the vocabulary, and K denotes the total number of tokens in a sequence. pic is the probability of token i belonging to class c. Yic is 1 if the actual token at position i belongs to class c and 0 otherwise. GA forces the model to forget the target data by driving pic closer to 0 when Yic = 1.\nHowever, such inverse updates could irrevocably damage the model utility on the retain data due to catastrophic forget-ting, in which a model forgets previously acquired knowledge or function during the process of learning new tasks. This occurs because unlearning requires removing specific data that may be integral to the model's effectiveness. To address this issue, one intuitive method named orthogonal gradient descent (OGD), derived from the field of continual learning, can be applied. OGD projects the gradients of new tasks g onto a subspace orthogonal to that of previous tasks. Unfortunately, this approach still suffers from model utility reduction since the direction is not gradient descent for earlier tasks.\nAnother way to counter this threat is to use KL divergence to constrain the unlearned model from diverging significantly from the original model. For instance, [6], [10] incorporates a KL divergence term and weighted-sum with the Lfgt, which aligns the output of the unlearned model with that of the original model on the retain data.\nHowever, a gradient conflict still arises from the simultane-ous optimization of two partially opposing objectives: unlearn-ing of certain knowledge and preservation of other essential information. That is, the gradient gfgt of the unlearning task and the gradient grt of retain data preservation may exhibit an inner product gfgt\u22c5grt < 0. Consequently, the model update direction would easily conflict with gfgt and grt, leading to compromised effectiveness in both the process of unlearning and knowledge retention. We verify it in the experimental results of Table I."}, {"title": "III. METHODOLOGY", "content": "To balance the trade-off between unlearning and model utility preservation, we formulate LLM unlearning as the following multi-objective optimization problem:\n\nmin{Lfgt(\u03b8),LKL(\u03b8),Lrt(\u03b8)},\n\nwhere LKL is the KL divergence between the outputs of the model and the original model on the retain data Drt, which is formulated by [6]:\n\nLKL:=\u2211(xrt,yrt)\u2208Drt\u2211Kk=1\u2211\u03a3KL(h\u03b8o(xrt,yi)||h\u03b8t(xrt,yi)),\n\nwhere \u03b8\u00b0 and \u03b8t correspond to the original model and the unlearned model at round t, xrt and yrt are non-harmful prompts and answers in the retain set, and h\u03b8(x,y<i):=P(Yi|(x,y<i);\u03b8). It is used to maintain the model utility [6].\nLrt represents the loss of maintaining the task performance, following the definition in [10]:\n\nLrt:=\u2211(xrt,yrt)\u2208DrtLCE(xrt,yrt;\u03b8).\n\nLfgt denotes the loss of unlearning. In GA, it takes the inverse of the CE loss (Eq. (1)). However, since the Cross-Entropy loss has no upper bound, directly minimizing Eq. (1) would lead to gradient explosion, as visualized in Fig. 2.\nTo address this issue, we design the following Unlearning Cross-Entropy (UCE) loss to replace the inverse CE loss (GA):\n\nLUCE=\u2211Kk=1\u2211Cc=1Yic\u22c5log(1\u2212(1\u2212\u03f5)pic),\n\nwhere \u03f5 is a small scalar used to slightly scale pic to prevent unbounded growth if pic = 1 in the beginning of unlearning. Following this, Lfgt is defined by Eq. 7, enabling model updates via the common gradient descent method. Since the UCE loss has the lower bound 0, it can achieve the goal of unlearning without causing gradient explosion.\n\nLfgt:=\u2211(xfgt,yfgt)\u2208DfgtLUCE(xfgt,yfgt;\u03b8)."}, {"title": "A. Problem Formulation", "content": null}, {"title": "B. Common Descent Update Direction", "content": "In the proposed MOLLM, we solve Problem (3) by iterating \u03b8t+1 = \u03b8t + \u03b7tdt in each round t, where \u03b7t represents the step size (i.e., the learning rate), and dt denotes the model update direction, which is common descent for each sub-objective of Problem (3). Specifically, Denote gfgt, gKL and grt as the gradients of the three sub-objective respective to the model parameters. dt satisfies dT \u22c5 gfgt < 0, dT \u22c5 gKL < 0, and dT \u22c5 grt < 0, allowing it to simultaneously minimize each sub-objective, thereby unlearning the forget data while preserving the model utility. Since dt is the common descent direction, based on the theorem of multiple gradient descent [21]\u2013[23], the model can reach the Pareto stationarity of the multi-objective optimization problem.\nDefinition 1 (Pareto Stationarity). \u03b8\u2217 is called Pareto stationary iff there exists a convex combination of \u2212gfgt, \u2212gKL, and \u2212grt that results in zero, i.e., \u2203\u03bei \u2265 0, \u22113i=1\u03bei = 1, such that \u03be1gfgt + \u03be2gKL + \u03be3grt = 0.\nWe propose a novel and efficient way to determine such a common descent direction dt. Let S be the space spanned by gfgt, gKL, and grt. The key idea is to obtain the dual vectors of gfgt, gKL, grt, denoted as g\u2021fgt, g\u2021KL, g\u2021rt, where the convex combinations of gfgt, gKL, grt form the dual space of S, as seen in Fig. 1. Afterward, the common descent direction dt for the model update can be obtained by taking the average of \u2212g\u2021fgt, \u2212g\u2021KL, and \u2212g\u2021rt.\nDefinition 2 (Dual Space) [24]. The space S\u2217 is called the dual space of S iff for any vector a lying in S, and for any b lying in S\u2217, it has a \u22c5 b < 0.\nAll vectors d lying inside the dual space of S satisfy d \u22c5 gfgt < 0, d \u22c5 gKL < 0, and d \u22c5 grt < 0. Since the dual vectors g\u2021fgt, g\u2021KL, g\u2021rt lie on the edge of the dual space S\u2217, one of them is orthogonal to the other two. Therefore, according to the matrix computation [25], g\u2021fgt can be obtained by projecting it to the null space of gKL and grt, following Eq. (8), and similarly for g\u2021KL and g\u2021rt.\n\ng\u2021fgt=gfgt\u2212AT(AAT)\u22121Agfgt,\n\nwhere A denotes a matrix concatenated by gKL and grt. After obtaining g\u2021fgt, g\u2021KL, and g\u2021rt, dt is obtained by dt = \u2212(g\u2021fgt + g\u2021KL + g\u2021rt). We scale the length of dt to min{||gfgt||, ||gKL||, ||grt||} in the end because the length of the gradients can be increased by the projection."}, {"title": "IV. EXPERIMENTS", "content": "We follow [6] to conduct the experiments on the well-known dataset PKU-SafeRLHF [26]. It contains harmful questions and harmful & non-harmful responses. We simulate a scenario in which an LLM is fine-tuned on a downstream dataset D, after which some harmful samples are identified from D and need to be unlearned. The learning rate is selected from {1e-5, 5e-6, 1e-6} with decay of 0.999 per round. We take the best performance of each method in comparison."}, {"title": "A. Experimental Setup", "content": null}, {"title": "V. CONCLUSION", "content": "In this paper, we study the proactive way of using Gradient Ascent for LLM unlearning, and analyze two significant challenges: gradient explosion and catastrophic forgetting. To address these issues, we incorporate multi-objective optimiza-tion into LLM unlearning, named MOLLM. Specifically, we design an unlearning version of Cross-Entropy loss to prevent gradient explosion, offering a novel and efficient way to calculate the common descent direction for the model update to unlearn the forget data while preserving the model utility. Experimental results demonstrate the effectiveness of MOLLM compared to the state-of-the-art baselines."}]}