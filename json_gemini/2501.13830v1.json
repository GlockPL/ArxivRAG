{"title": "A space-decoupling framework for optimization on bounded-rank matrices with orthogonally invariant constraints", "authors": ["Yan Yang", "Bin Gao", "Ya-xiang Yuan"], "abstract": "Imposing additional constraints on low-rank optimization has garnered growing interest. However, the geometry of coupled constraints hampers the well-developed low-rank structure and makes the problem intricate. To this end, we propose a space-decoupling framework for optimization on bounded-rank matrices with orthogonally invariant constraints. The \"space-decoupling\" is reflected in several ways. We show that the tangent cone of coupled constraints is the intersection of tangent cones of each constraint. Moreover, we decouple the intertwined bounded-rank and orthogonally invariant constraints into two spaces, leading to optimization on a smooth manifold. Implementing Riemannian algorithms on this manifold is painless as long as the geometry of additional constraints is known. In addition, we unveil the equivalence between the reformulated problem and the original problem. Numerical experiments on real-world applications\u2014spherical data fitting, graph similarity measuring, low-rank SDP, model reduction of Markov processes, reinforcement learning, and deep learning-validate the superiority of the proposed framework.", "sections": [{"title": "1 Introduction", "content": "Low-rank optimization, aiming to exploit the low-dimensional structure in matrix data for memory and computational efficiency, achieves success in a multitude of applications, e.g., factor analysis [Chu+05], system identification [Mar08; Zhu+22], large language models [Hu+22], synchronization [MB24]. With extra equality constraints in addition to the low-rank requirement, this paper is concerned with the following matrix optimization problem,\nmin_{XER^{m\\times n}} f(X)\ns. t. rank(X) \\leq r,\nh(X) = 0, (P)\nwhere the rank parameter $r < \\min{m,n}$, $f:R^{m\\times n} \\rightarrow R$ is twice continuously differentiable, and $h : R^{m\\times n} \\rightarrow R^q$ is orthogonally invariant in the sense that $h(XQ) = h(X)$ for all $Q$ in the orthogonal group $O(n)$. We denote the set of bounded-rank matrices by\nR^{<r}_{mxn} := \\{X \\in R^{mxn} : rank(X) \\leq r \\}\nand the level set of $h$ by\nH := \\{X \\in R^{mxn} : h(X) = 0 \\}.\nConsequently, the coupled feasible region can be written as $R^{<r}_{mxn} \\cap H$, which is both nonsmooth and nonconvex. In the vanilla scenario $h(X) = 0$, which implies $H = R^{m\\times n}$, problem (P) reduces to minimizing a function over bounded-rank matrices [SU15; LKB23]. Throughout this paper, the following blanket assumption is imposed on $h$.\nAssumption 1 (Blanket) The mapping $h: R^{m\\times n} \\rightarrow R^q$ is smooth and orthogonally invariant, and has full rank $q$ in the level set $H$, i.e., $\\text{rank}(D_hX) = q$ for all $X \\in H$, where $D_h$ denotes the Jacobian matrix."}, {"title": "1.1 Applications", "content": "The general formulation (P) satisfying Assumption 1 encompasses an array of structured optimization problems with low-rank matrix variables. We now present a brief overview on representative applications.\nSpherical data fitting. Finding a low-rank approximation of normalized data points lays a foundation for various problems, including concept mining, pattern classification, and information retrieval. Specifically, given a matrix $A \\in R^{m\\times n}$ of which rows represent data points and have unit length, Chu et al. [Chu+05] proposed the approximation task as follows,\nmin_{XER^{m\\times n}} \\frac{1}{2}||X - A||^2_F\ns. t. \\text{rank}(X) \\leq r,\n\\text{diag}(XX^T) - 1 = 0. (1.1)"}, {"title": "Graph similarity measuring", "content": "In chemical structure comparison, biological data analysis, and web searching, measuring the similarity of two graphs is a fundamental task. Given $X \\in R^{m\\times n}$ of which an entry evaluates the connection between two nodes in different graphs, Blondel et al. [Blo+04] proposed an iterative process to measure the desired node-to-node similarity. Furthermore, to explore the low-rank structure of the similarity matrix and to reduce the computational cost, Cason et al. [CAVD13] proposed the measuring problem as follows,\nmin_{XER^{m\\times n}} - \\text{tr}(XL(X))\ns. t. rank(X) \\leq r,\n||X||_F - 1 = 0, (1.2)\nwhere $L : R^{m\\times n} \\rightarrow R^{m\\times n}$ is a linear operator and $||\\cdot||_F$ denotes the Frobenius norm. Problem (1.2) falls into the scope of (P) with $h(X) = ||X||^2_F -1$ and the associated $H = SF(m, n) := \\{X \\in R^{m\\times n} : ||X||_F= 1\\}."}, {"title": "Low-rank semidefinite programming (SDP)", "content": "Applications across various fields can be addressed by the semidefinite program,\nmin \\langle C, Y \\rangle , s. t. Y\\in \\text{Sym}(n), Y \\geq 0, L(Y) = b, (1.3)\nwhere $\\text{Sym}(n)$ is the set of $n\\times n$ symmetric matrices, $L: R^{n\\times n} \\rightarrow R^q$ is a linear operator, and $b\\in R^q$. As shown in [Pat98], a low-rank solution for (1.3) exists when the feasible region is compact. Therefore, we can employ the Burer-Monteiro factorization $Y = XX^T$ [BM03] and impose a rank constraint on (1.3), resulting in an instance of (P),\nmin_{X\\in R^{n\\times n}} \\langle C, XX^T \\rangle\ns. t. rank(X) \\leq r,\nL(XX^T) - b = 0. (1.4)"}, {"title": "Model reduction of Markov processes", "content": "Numerous systems rely on Markov processes as the basic model, where each event's probability depends only on the previous state. However, a large state space $S = \\{s_1, s_2, ..., s_n\\}$, usually appearing in complicated systems, confines the identification of the Markov model $P\\in R^{|S|\\times |S|}$ in which an entry $P(s, s')$ represents the probability of the transition from state $s$ to state $s'$. Therefore, we can resort to the low-rank property of the Markov process to reduce system complexity and enable tractable computations. Additionally, since the probability matrix consists of nonnegative real numbers, with each row summing to one, it is reasonable to represent P using the Hadamard parameterization [LMY23], $X \\odot X$, $X \\in Ob(|S|, |S|)$. Consequently, imposing a rank constraint on X, we propose the following model to find reduced-dimension representations of Markov processes,\nmin_{X\\in S_{emis}} ||X \\odot X - P||\ns. t. rank(X) \\leq r,\ndiag(XX^T) - 1 = 0, (1.5)\nwhere $P$ is an empirical estimate of the ground-truth $P$. Formulation (1.5) provides fresh insights on identifying low-rankness in Markov processes, which will aid in extracting state features and thus facilitate various downstream tasks including state abstraction and reinforcement learning [Abe+18]."}, {"title": "Reinforcement learning (RL)", "content": "To make sequential decisions in Markov pro-cesses, RL is an effective framework, which aims for the optimal policy maximizing the expected cumulative reward. Specifically, with $A$ representing the action space, a policy $\\pi\\in R^{|S|\\times |A|}$ serves as an action selection rule, i.e., $\\pi(s, a)$ gives the probability of performing action $a$ at state $s$. Moreover, in a Markov decision process (MDP), a state-action pair $(s,a) \\in S \\times A$ incurs a reward $r(s,a)$ and transitions to state $s' \\in S$ with probability $P^a(s, a, s')$, where the tensor $P^a\\in R^{|S|\\times |A|\\times |S|}$ denotes the transition dynamics. Consequently, given $\\mu$ as the initial state distribution and $\\gamma\\in (0,1)$ as the discount factor, the central task of RL is to maximize the objective $J(\\pi) := E[\\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) | s_0 \\sim \\mu, \\pi]$. However, the optimization problem suffers from the curse of dimensionality when the state space $S$ and the action space $A$ are large [SB18]. To alleviate this, exploiting the low-rank structure in RL is an advisable approach. Specifically, recognizing that a policy is inherently a probability matrix, we adopt the Hadamard parameterization $\\pi(X) := X \\odot X$, $X \\in Ob(|S|, |A|)$, introduce a low-rank requirement on the variable X, and thus propose the following low-rank formulation for reinforcement learning,\nmin_{X\\in R^{|SX|A}} -J(\\pi(X))\ns. t. rank(X) \\leq r,\ndiag(XX^T) - 1 = 0. (1.6)"}, {"title": "Neural network training", "content": "Recent success in deep learning has witnessed the critical role of neural network architectures in both training efficiency and infer-ence performance. Among various designs, two principles receive increasing attention. Typically, one is weight normalization, which expresses the weight matrix $W \\in R^{m\\times n}$ by $W = (g \\odot 1)X$ with $g \\in R^m$ and $X \\in Ob(m, n)$. Therefore, each row of X encodes the direction, while $g$ is responsible for the scaling. It is reported that such a separation enhances training stability [SK16]. Meanwhile, another princi-ple, namely low-rank compression, harnesses the inherent low-rankness of large-scale network weights, effectively circumventing parameter redundancy and preserving decent generalization [ICP20]. Integrating the two principles-weight normaliza-tion and low-rank compression presents an enlightening avenue for further devel-opment. Specifically, for the i-th layer in a neural network, we train the normalized weight $X_i$ subject to the low-rank constraint, together with scale parameters $g_i$, leading to the following training model,\nmin_{\\{X_i,g_i\\}_{i=1}^l} f(\\{X_i, g_i\\}_{i=1}^l)\ns. t. rank(X_i) \\leq r_i,\ndiag(X_iX_i^T) - 1 = 0, \\text{ for }i = 1, 2, ..., l, (1.7)\nwhere $f$ is the loss function and $l$ is the number of layers."}, {"title": "1.2 Motivation and related work", "content": "Apart from the aforementioned real-world applications, many problems exhibit the structure of low-rankness and orthogonal invariance. However, a unified framework to address problem (P) is lacking, which essentially suffers from three pains.\n1. The optimality conditions of problem (P) remain unexplored, largely due to the involved geometry of the coupled constraints. This unawareness poses an impediment to defining stationarity measures, thereby hindering the development of algorithms.\n2. The feasible region inherits the nonsmoothness of $R^{<r}_{mxn}$, which gives rise to the undesirable phenomenon: there exist feasible sequences along which the station-arity measures tend to zero, but with limit points non-stationary [LKB23].\n3. Projections onto the coupled feasible set $R^{<r}_{mxn} \\cap H$ are unclear for general $H$, making it intractable to translate existing techniques in low-rank optimization, such as the projected gradient descent framework adopted by [SU15; OGA22; OW24].\nOptimality conditions. Research on optimality conditions gains momentum in low-rank optimization, where the variational geometry, including tangent and nor-mal cones to the feasible set, plays a pivotal role. Typically, several kinds of cones-the Mordukhovich normal cone [Luk13], the Bouligand tangent cone [CAVD13; SU15], and the Clarke tangent cone and the corresponding normal cone [HLU19; LSX19] to the bounded-rank set $R^{<r}_{mxn}$ have been well studied.\nHowever, an additional constraint set $C$ coupled with $R^{<r}_{mxn}$ renders the geometry of the feasible region more complicated. Specifically, when $m = n$ and $C = \\text{Sym}(n)$, Tam et al. [Tam17] derived the Mordukhovich normal cone to"}, {"title": "Optimization on bounded-rank matrices", "content": "In general, finding a global minimizer of a function solely subject to the bounded-rank constraint is NP-hard [GG11]. Existing literature [SU15; LKB23; OA23] reveals that it remains possible to compute a (first-order) B-stationary point, a zero of the B-stationarity measure- -norm of the projection of negative gradient onto the Bouligand tangent cone to $R^{<r}_{mxn}$. However, the nonsmooth and nonconvex nature of the bounded-rank set introduces the main obstacles. Typically, the apocalypse exists, which characterizes the sequence of points with B-stationarity measures converging to zero, but limit points not B-stationary [LKB23]. A line of algorithms is developed for the optimization on bounded-rank matrices, and they are categorized into three groups.\nThe first group is based on the projected gradient descent (PGD) framework, i.e., updating the iterate along an appropriate direction and then projecting it onto the bounded-rank set, which amounts to computing a truncated singular value decomposition (SVD) of a matrix. The PGD method in [OW24] has recently been proven to accumulate at B-stationary points. However, the well-known P2GD [SU15], additionally projecting negative gradient onto the tangent cone as the search direction, can result in the apocalypse; an explicit example was given in [LKB23]. To resolve this, Olikier et al. [OGA22] introduced a rank-adaptive strategy and obtained a modification P2GDR.\nThe second group evolves around the retraction-free principle. In detail, search directions are chosen from the so-called restricted tangent cone to $R^{<r}_{mxn}$ (see [OA23]) such that updates are performed along straight lines, and thus the algorithm gets rid of expensive projections onto the bounded-rank set. The original retraction-free descent (RFD) method [SU15] may encounter the apocalypse, and the variant RFDR [OA23] equipped with a rank reduction mechanism is guaranteed with the convergence to B-stationary points. Recently, Olikier and Absil [OA24] proposed a more efficient version named ERFDR, which preserves the benign convergence property of RFDR.\nThe third group resorts to the parameterization of the bounded-rank set, which constructs a smooth manifold $M$ and an associated mapping $\\phi$ such that $\\phi(M) = R^{<r}_{mxn}$. Subsequently, the original problem on the bounded-rank set can"}, {"title": "1.3 Contributions", "content": "In this paper, we develop a space-decoupling framework by taking advantage of the geometry of the bounded-rank set and the extra orthogonally invariant constraints. The space-decoupling principle features the following aspects.\nGeneral properties of the orthogonally invariant mapping $h$ are studied, which turn out to be closely related to rank information of the matrix variable. Specifically, Assumption 1 reveals that $H$\u2014the level set of $h$\u2014is a smooth manifold, and we show that $h$ also induces a series of manifolds $H^s$ embedded in lower-dimensional spaces $R^{m\\times s} (s = 1, 2, ..., n)$. Notably, these $H^s$ inherit the geometric structure from $H$, and the tangent space to $H$ at $X$ can be decoupled into a tangent space to $H^s$ and the space orthogonal to $X$ (Proposition 3). The results build up a connection between $R^{<r}_{mxn}$ and $H$, paving the way for investigating $R^{<r}_{mxn} \\cap H$.\nWe give a closed-form expression for the tangent cone to the coupled feasible set (Theorem 1), which essentially unveils the intersection rule the tangent (resp. normal) cone to $R^{<r}_{mxn} \\cap H$ can be decomposed as the intersection (resp. direct sum) of tangent (resp. normal) cones to each of them, i.e.,\nTx (Rmx H) = TxRmxn RM x n \\cap Tx H,\nNx (Rmx H) = NxRmxn <r \\oplus NxH.<r\nMoreover, the optimality conditions for (P) are developed.\nViewing the Grassmann manifold as embedded in $\\text{Sym}(n)$, i.e., $Gr(n, s) := \\{G \\in \\text{Sym}(n) : G^2 = G, \\text{rank}(G) = s\\}$, we propose the following space-decoupling param-eterization,\nM_h := \\{(X,G) \\in R^{m\\times n} \\times Gr(n, n - r) : XG = 0, h(X) = 0\\}\nwith the smooth mapping $\\phi: R^{m\\times n} \\times \\text{Sym}(n) \\rightarrow R^{m\\times n} : (X, G) \\rightarrow X$ satisfying $\\phi(M_h) = R^{<r}_{mxn} \\cap H$. We prove that the set $M_h$ is a smooth manifold (Theorem 2),"}, {"title": "2 Properties of the orthogonally invariant mapping", "content": "This section uncovers the connection between the orthogonal invariance of the mapping $h$ and the rank information of the variable $X$, and thus bridges the geometry of $H$ and $R^{<r}_{mxn}$.\nThe Bouligand tangent cone to a set $X$ at a point $X \\in X$ is\nTxX := \\{ \\eta = \\lim_{k\\rightarrow \\infty} \\frac{(X_k \u2013 X)}{t_k} : X_k \\in X, t_k > 0 \\text{ for all } k, \\lim_{k\\rightarrow \\infty} t_k = 0\\}.\nA vector $\\eta \\in TX$ is derivable if it admits a curve $\\gamma(t)$ on $X$ with $\\gamma(0) = X$ and $\\gamma'(0) = \\eta$, and the set $X$ is geometrically derivable at $X$ if any vector in $TX$ is derivable. Taking the polar operation on $TX$ introduces the Fr\u00e9chet normal cone, $NX := (TX)^\\circ$. Specifically, if $X$ is a smooth manifold, the tangent (resp. normal) cone reduces to a vector space, i.e., the tangent (resp. normal) space."}, {"title": "2.1 Geometry of H", "content": "The full-rankness of $h$ in Assumption 1 implies that the level set $H$ is a smooth manifold embedded in $R^{m\\times n}$; see [Lee12, Corollary 5.14]. Moreover, the orthogonal invariance of $h$ sheds light on the fact that the tangent space $T_XH$ contains a subspace that is independent of the choice of $h$; see the following proposition.\nProposition 1 Given X \u2208 H with rank(X) = s and the SVD X = U\u03a3VT, it holds that\nK_X := \\{U\\Sigma \\Omega V^T + BV : \\Omega \\in \\text{Skew}(s), B \\in R^{m\\times (n-s)}\\} \\subseteq T_XH. (2.1)\nProof The fact $T_XH = \\text{ker}(D_hX)$ shifts the focus to prove $K_X \\subseteq \\text{ker}(D_hX)$. Notice that $K_X$ is a linear space and any $\\eta \\in K_X$ can be expressed in the form of $\\eta = \\eta_1+\\eta_2$ where $\\eta_1 = U\\Sigma \\Omega V^T + UB_1V^\\perp$ and $\\eta_2 = U_\\perp B_2V^\\perp$ with $\\Omega\\in \\text{Skew}(s), B_1 \\in R^{s\\times (n-s)}$, and $B_2 \\in R^{(m-s)\\times (n=s)}.$"}, {"title": "2.2 Inheritance principle", "content": "Based on the above results, we demonstrate that the properties of $h$ can be in-herited from $R^{m\\times n}$ to lower-dimensional spaces. For $s = 1,2,...,n$, we define a sequence of natural embeddings $i^s : R^{m\\times s} \\rightarrow R^{m\\times n} : H \\rightarrow [H \\ 0_{m\\times (n-s)}]$, the associated mappings $h^s : hoi^s$, and the level sets\nH^s := \\{H \\in R^{m\\times s} : h^s(H) = 0 \\}.\nGiven $X \\in R^{m\\times n}$. The developments of this subsection rely on a decomposition $X = HV^T$ with $H \\in R^{m\\times s}$ and $V\\in St(n, s)$, which is always achievable in practice; e.g., singular value decomposition and QR factorization. Subsequently, $h$ and $h^s$ are connected via the values and differentials."}, {"title": "3 Optimality conditions", "content": "In this section, we explore the geometry of $R^{<r}_{mxn} \\cap H$, and then employ the results to identify the tangent and normal cones to $R^{<r}_{mxn} \\cap H$. Consequently, the optimality conditions for problem (P) are derived.\nThe geometry of low-rank sets is well developed; see [Van13; SU15]. As a fixed-rank layer of $R^{m\\times n}$, $R^{s}_{mxn}$ is indeed an analytic manifold. Given $X \\in R^{s}_{mxn}$ with the singular value decomposition $X = U\\Sigma V^T$, the tangent and normal spaces are outlined below,\nTxR^{s}_{mxn} \\subseteq \\left \\{\\left [\\begin{array}{cc} R^{s\\times s} & R^{s\\times(n-s)} \\\\ 0_{s\\times s} & 0_{s\\times(n-s)} \\end{array}\\right ] [VV^\\perp] \\right \\}, (3.1)\nNxR^{s}_{mxn} \\supseteq \\left [\\begin{array}{cc} U\\Sigma \\Sigma^* U^T & 0_{s\\times(n-s)} \\\\ 0_{s\\times(m-s)} & R^{(m-s)\\times(n-s)} \\end{array}\\right ] [VV^\\perp] , (3.2)\nAssembling the layers $R^{s}_{mxn} (s = 0,1,...,r)$ yields the bounded-rank set $R^{<r}_{mxn}$, with its tangent and normal cones at $X$ formulated as follows,\nTxR^{<r}_{mxn} = TxR^{s}_{mxn} + \\{N\\in NxR^{s}_{mxn} : \\text{rank} (N) \\leq r -s\\}, (3.3)\nNxR^{<r}_{mxn} = \\left \\{\\begin{array}{ll} NxR^{r}_{mxn}, & \\text{ if }s=r, \\\\ \\{0\\}, & \\text{ if }s <r. \\end{array} \\right. (3.4)\nLet $P_W := WW^T$. The projection of $E \\in R^{m\\times n}$ onto $TxR^{<r}_{mxn}$ is given by\nPTxR^{<r}_{mxn}(E) = EP_V + P_UEP_{V^\\perp} + P_{mon} (P_{U^\\perp}EP_{V^\\perp}). (3.5)"}, {"title": "3.1 Variational geometry", "content": "Armed with the properties of $R^{<r}_{mxn}$ and those of H obtained in section 2, we delve into the geometry of their intersection. To this end, we first revisit the basics for tangent and normal cones to the union (or intersection) of finite sets; see [RW09; Lee12].\n- If $X \\in \\cup_{i=1}^d X_i$, then it holds $Tx (\\cup_{i=1}^d X_i) = \\cup_{i=1}^d Tx X_i$;\n- If $X \\in X_1 \\cap X_2$, then it holds\nTx (X_1 \\cap X_2) \\subseteq Tx X_1 \\cap Tx X_2, \\text{ and }Nx (X_1 \\cap X_2) \\supseteq Nx X_1 + Nx X_2. (3.6)"}, {"title": "4 A Space-decoupling parameterization", "content": "The developed geometry in section 3 enhances the theory of optimization on bounded-rank matrices. However, the nonsmooth structure of $R^{<r}_{mxn} \\cap H$ and the"}]}