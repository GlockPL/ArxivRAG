{"title": "TORQUE-AWARE MOMENTUM", "authors": ["Pranshu Malviya", "Gon\u00e7alo Mordido", "Aristide Baratin", "Reza Babanezhad Harikandeh", "Gintare Karolina Dziugaite", "Razvan Pascanu", "Sarath Chandar"], "abstract": "Efficiently exploring complex loss landscapes is key to the performance of deep neural networks. While momentum-based optimizers are widely used in state-of-the-art setups, classical momentum can still struggle with large, misaligned gradients, leading to oscillations. To address this, we propose Torque-Aware Momentum (TAM), which introduces a damping factor based on the angle between the new gradients and previous momentum, stabilizing the update direction during training. Empirical results show that TAM, which can be combined with both SGD and Adam, enhances exploration, handles distribution shifts more effectively, and improves generalization performance across various tasks, including image classification and large language model fine-tuning, when compared to classical momentum-based optimizers.", "sections": [{"title": "INTRODUCTION", "content": "Despite the wide range of optimization methods available in the literature, stochastic gradient descent (SGD), typically augmented with momentum (Kingma & Ba, 2015; Nesterov, 1983; Qian, 1999), remains the go-to approach for practitioners. Momentum accelerates convergence, particularly in the presence of high curvature (Cutkosky & Mehta, 2020b), small but consistent gradients, or noisy gradients. It also helps the optimizer navigate the loss landscape and escape local minima or saddle points by maintaining consistent updates directions (Jin et al., 2018).\nWhile SGD with momentum (SGDM) has shown remarkable success in various scenarios, particularly in computer vision (Sutskever et al., 2013), it remains vulnerable to the adverse effects of large, misaligned gradients (Zhang et al., 2019). These gradients often stem from noisy data or abrupt changes in loss landscape curvature, especially in narrow basins where gradients frequently shift direction (Ortiz-Jim\u00e9nez et al., 2022). This can lead to oscillations, making it harder for the optimizer to escape sharp minima (Fu et al., 2023).\nIn this work, we propose that minimizing the influence of misaligned gradients during momentum updates can preserve valuable information and improve the exploration capabilities of momentum-based methods. To enable more consistent exploration of the loss landscape, particularly in noisy settings, we introduce a new approach that modifies the standard momentum update by incorporating a damping factor, inspired by the damping effect in mechanical systems (Fritzen, 1986).\nIn this analogy, momentum represents velocity in linear dynamics, and the gradient represents the applied force.\nThe damping term we introduce depends on the angle between the gradient and momentum, acting"}, {"title": "RELATED WORK", "content": "Momentum-based methods have been widely studied for their ability to improve convergence speed and exploration of the loss landscape. For instance, Xing et al. (2018) showed that as mini-batch gradients aligns with the top eigenvectors of the Hessian, SGD's exploration slows due to oscillatory behaviour, particularly at larger batch sizes. Similarly, Fu et al. (2023) showed that SGDM accelerates convergence by deferring this oscillation, referred to as abrupt sharpening, where gradients and the Hessian suddenly align, making SGDM more effective for larger learning rates.\nSeveral momentum variants aim to improve generalization by utilizing the curvature of the loss surface (Gilmer et al., 2021; Foret et al., 2021; Yao et al., 2021; Tran & Cutkosky, 2022; Kaddour et al., 2022).\nPopular optimizers like Adam (Kingma & Ba, 2015) combine adaptive learning rate with momentum for faster convergence, while Ziyin et al. (2020) proposed leveraging parameter updates, rather than gradients, to compute momentum. However, while these methods improve convergence speed, they do not specifically address the challenge of torqued gradients on noisy loss surfaces.\nLucas et al. (2018) introduced AggMo, an optimizer combining multiple momentum vectors with different decay rates, but requires storing multiple copies of model states (Cutkosky & Mehta, 2020a; Xie et al., 2021), unlike our method TAM, which maintains the same memory footprint as SGDM. Closest to our work, Roy et al. (2021) tackle gradient misalignment by considering angles between consecutive gradients. However, we argue that focusing on the angle between momentum and gradients is more criticial for stability, as demonstrated by our comparisons with their method, AngularGrad (seeSection 4)."}, {"title": "METHODOLOGY", "content": "Background: SGDM Momentum was first introduced to accelerate convergence in SGD (Polyak, 1964; Qian, 1999). Given a loss function $L_D(\\theta)$ and its gradients $g_t = \\nabla_{\\theta_t}L_D(\\theta_t)$ at time $t$, the momentum and parameter updates are:\n$m_t = \\beta m_{t-1}+ g_t; \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\theta_{t+1} = \\theta_t - \\eta m_t$ \t(1)\nwhere $\\beta$ is the momentum coefficient and $\\eta$ is the learning rate. The momentum accumulates past gradients, smoothing out noise and providing more weight to recent gradients. This helps accelerate"}, {"title": "Torque-Aware Momentum (TAM)", "content": "TAM modifies the momentum update in Eq. 1 to regulate the impact of new gradients. To handle the noisy nature of loss surfaces, we introduce a damping factor that adjusts the influence of gradients based on their directional alignment with the previous momentum. This acts like anisotropic friction (Tramsen et al., 2018), reducing the effect of torqued gradients, similar to how damping reduces torque in rotational systems.\nTo increase robustness against misaligned gradients and encourage exploration of dominant gradient directions, we define the correlation $S_t$ between the previous momentum direction and the current gradient as the cosine similarity:\n$S_t = \\frac{m_{t-1}.g_t}{||m_{t-1}|| ||g_t||}$ \t(2)\nWe apply smoothing to $S_t$ with a decay rate $\\gamma$ to account for stochasticity:\n$\\hat{S_t} = \\gamma \\hat{S}_{t-1} + (1 - \\gamma)S_t$. \t(3)\nNext, we normalize the smoothed correlation $\\hat{S_t}$ to the range [0, 1] and introduce a small constant $\\epsilon$ to ensure that new gradients still exert a small influence even when the momentum magnitude diminishes. We prioritize momentum update aligned with previous directions to reduce the influence of large opposing gradients:\n$d_t = \\frac{1 + \\hat{S_t}}{2} ; m_t = \\beta m_{t-1}+ (\\epsilon + d_t)g_t$. \t(4)\nThough TAM introduces the hyper-parameters $\\gamma$ and $\\epsilon$, they are fixed by default at 0.9 and 1e - 8, respectively, requiring no additional tuning. Figure 2 illustrates TAM's behaviour: when the alignment a1 is stronger (smaller a\u2081), the gradient g1 amplifies the momentum m\u2081. Conversely, when a2 is larger, the gradient g2 has less influence, resulting in a smaller momentum m2. The pseudo-code of TAM is given in Algorithm 1."}, {"title": "Learning Rate Transfer", "content": "Here we describe a simple heuristics to transfer a tuned learning rate from SGDM to TAM. We can do so by comparing effective learning rates, as derived in (Fu et al., 2023). For SGDM, the idea is that momentum changes the update magnitude in a way that can be approximated as t gets large as\n$\\frac{m_t}{1-\\beta} = \\sum_{s=1}^t \\beta^{t-s} g_s \\approx \\frac{1}{1-\\beta} g_t$\nThis suggests that the SGDM updates (1) with learning rate $\\eta$ have the same magnitude as the updates of SGD with effective learning rate $\\eta_{eff}^{SDM} = \\frac{\\eta}{1 - \\beta}$. Similarly, we derive the effective learning rate for TAM based on the update rule (4) with $|\\epsilon| \\ll 1$. Assuming that, as t in- creases, the cosine similarity $\\hat{s_t}$ stabilizes to a constant value $s^*$, TAM's effective learning rate becomes:\n$\\eta_{TAM}^{eff} \\approx \\frac{1+s^*}{2(1-\\beta)} \\eta$\t(5)"}, {"title": null, "content": "Under this assumption, a tuned learning rate $\\eta^{SGDM}$ for SGDM can be transferred to an optimal learning rate $\\eta^{TAM}$ for TAM by equating the corresponding effective learning rate. Solving for $\\eta^{TAM}$ yields:\n$\\eta^{TAM} = \\frac{2(1 - \\beta_{TAM})}{(1+s^*)(1 - \\beta_{SGDM})} \\eta_{SGDM}$\t(6)\nIn practice, we observed that $s^* \\approx 0$ as t increases (see Appendix A.2.1 for empirical evidence). In our experiments, we set $\\beta_{TAM} = \\beta_{SGDM}$, and found that $\\eta^{TAM} = 2\\eta_{SGDM}$ consistently yields optimal performance.\nThis equivalence means that in the neighborhood of optima, where $\\hat{s_t}$ has stabilized, TAM inherits the well-established convergence guarantees of SGDM (Yan et al., 2018; Liu et al., 2020). The damping factor $(1 + \\hat{s_t})/2$ remains bounded, ensuring the effective learning rate stays within a controlled range throughout training. This theoretical connection to SGDM, combined with our empirical evidence of $s^*$ stabilizing to 0, ensures TAM's convergence while maintaining its enhanced exploration capabilities during early training.\nAdaTAM We also introduce an adaptive variant of TAM, which combines Adam (Kingma & Ba, 2015) and the TAM update in Eq. 4. The update rule for AdaTAM is thus defined as\n$m_t = \\beta m_{t-1}+ (\\epsilon + d_t)g_t ; v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2;  \\theta_{t+1} = \\theta_t - \\eta \\frac{m_t}{\\sqrt{v_t} + c}$ \t(7)\nwhere $\\beta_2$ is the second-moment decay rate, and c is a small constant (typically le - 8 by default). Note that AdaTAM only modifies mt and keep the updates of vt the same as in Adam."}, {"title": "EXPERIMENTS", "content": "In this section, we present the results of our experiments evaluating TAM across various benchmarks. First, we compare TAM and AdaTAM with baseline optimizers including SGD (with and without momentum), Adam, and AngularGrad (Roy et al., 2021), in terms of generalization performance on image classification datasets (subsection 4.1). We also assess AdaTAM's performance in fine-tuning Bert-based models on the MTEB datasets (subsection 4.2). Additionally, we demonstrate TAM's robustness to distribution shifts in online learning settings ( subsection 4.3) and explore its use during a warm-up phase to facilitate loss landscape exploration in the early stages of training (subsection 4.4). All results of our experiments are averaged across five seeds, with additional experimental details provided in Appendix A.1."}, {"title": "IMAGE CLASSIFICATION", "content": "Setup. We run experiments on CIFAR 10, CIFAR100 (Krizhevsky & Hinton, 2009), and ImageNet (Deng et al., 2009). We train ResNet18, ResNet34 architectures on CIFAR10/100 for 200 epochs and ResNet50 on ImageNet for 90 epochs. We perform a learning rate grid search with a fixed compute budget assigned to each optimizer to obtain the best setup. We choose the ranges of these grid searches to be consistent with the learning rate transfer heuristic rule in Equation 6.\nResults. The validation accuracy for each optimizer is reported in Table 1. The results indicate that TAM and AdaTAM generally outperform their corresponding baselines across most configurations.\nAmong non-adaptive optimizers, the only exception is for CIFAR100 with the ResNet34 model, where TAM performs slightly below SGDM. In all other cases, TAM achieves higher accuracy. Although adaptive optimizers generally underperform compared to non-adaptive ones in these setups, we observe that AdaTAM achieves similar or even better results compared to Adam and AngularGrad, with the exception of ResNet34 on CIFAR10. Overall, while the effectiveness may vary depending on the specific model, these results indicate that TAM and AdaTAM provide consistent improvements in generalization across various models and datasets."}, {"title": "LLM FINE-TUNING", "content": "Setup. We compare AdaTAM with weight decay (AdaTAMW) to AdamW for fine-tuning LLMs. Specifically, we consider six pre-trained BERT-based models: BERT-base, BERT-large (Devlin, 2018),"}, {"title": "ONLINE LEARNING", "content": "In this section, we investigate whether TAM can handle distribution shifts in online learning, where non-IID setups typically cause deep learning models to struggle due to a loss of plasticity - the ability to adapt to new tasks. In such setups, distribution shifts alter the loss landscape, pushing parameters that performed well on a previous task into sub-optimal, higher loss regions for the new task, leading to plasticity loss (Lewandowski et al., 2024; Elsayed & Mahmood, 2024). Existing solutions to this problem focus on regularization (Kumar et al., 2023), reinitializing inactive parameters (Sokar et al., 2023), or adding normalization layers (Lyle et al., 2024b), often using SGD as the base optimizer.\nWe hypothesize that TAM's momentum from previous tasks can help push parameters out of sub-optimal regions by mitigating the torqued gradients that arise at the start of the new task, allowing for better exploration of the new task's loss landscape using knowledge from previous gradients. To test this, we compare TAM with SGD and SGDM in an online learning setup. Specifically, similar to (Lyle et al., 2024a;b), we also train multi-layered networks (MLP) on a sequence of tasks, where each task involves image classification on CIFAR10. We induce distribution shifts by flipping the labels between tasks, a common benchmark in online learning research (Elsayed & Mahmood, 2024; Lewandowski et al., 2024). We experiment with different degrees of label flipping, \u03b4\u2208 {40%, 80%, 100%}, to simulate soft and hard task boundaries. For each optimizer and each setup, a hyper-parameter grid search is conducted across different effective learning rates, selecting the best-performing setup is selected based on average online accuracy across all tasks, following Dohare et al. (2021). Each task is assigned a compute budget of 40 training epochs. We evaluate on two different sizes of MLP. Further setup details are provided in Appendix A.1."}, {"title": "WARM-UP WITH TAM", "content": "Exploring the loss surface is especially important during the initial phase of training, as it helps the optimizer effectively navigate the loss landscape and avoid getting stuck in local minima. TAM can be beneficial as a warm-up strategy, as it prioritizes important directions, helping to identify the basin of attraction early on.\nIn this section, we perform an ablation study to evaluate TAM warmup when training a ResNet18 on CIFAR-10. We begin by training the model with TAM and a constant learning rate for a specified number of steps (denoted as sw), then switch to SGDM while keeping the effective learning rate and optimizer state same. The learning rate of SGDM is set to half of TAM's learning rate, based on the effective learning rate analysis in section 3. Additionally, we include a baseline where training starts with SGDM, followed by a halving of the learning rate at step sw, while maintaining the optimizer state. Further implementation details are provided in Appendix A.1."}, {"title": "CONCLUSION", "content": "We propose Torque-Aware Momentum (TAM), an enhancement of classical momentum that mitigates the detrimental effects of torqued gradients, enabling more stable and consistent exploration of the loss landscape. By incorporating a damping factor that adjusts momentum based on gradient alignments, TAM helps models escape sharp minima and improve generalization across diverse tasks.\nOur evaluation of TAM spans multiple experimental setups, including image classification, large language model fine-tuning, and online learning with distribution shifts. Across these tasks, TAM consistently performs on par with, and often surpasses, traditional SGD and SGDM. In particular, TAM shows significant advantages in tasks involving distribution shifts, where it stabilizes learning and adapts more effectively than SGDM, especially when tasks share little overlap. Additionnaly, TAM proves valuable as a warm-up strategy, leading to faster convergence and lower loss barriers compared to SGDM.\nWhile our results demonstrate TAM's effectiveness in tasks with distribution shifts and gradient misalignment, further work is needed to test its capabilites in more challenging non-stationary envi- ronments, such as continual learning. Our preliminary continual learning experiments in Appendix A.2.3 highlight TAM's potential to address catastrophic forgetting by retaining gradient direction from previous tasks. However, a thorough investigation is required to fully understand and optimize TAM's performance in this domain. Another exciting avenue is to explore TAM's potential in other training paradigms, such as self-supervised learning and reinforcement learning, where effective exploration and stability is critical for model success."}]}