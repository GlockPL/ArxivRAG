{"title": "Visual Data Diagnosis and Debiasing with Concept Graphs", "authors": ["Rwiddhi Chakraborty", "Yinong Wang", "Jialu Gao", "Runkai Zheng", "Cheng Zhang", "Fernando De la Torre"], "abstract": "The widespread success of deep learning models today is owed to the curation of extensive datasets significant in size and complexity. However, such models frequently pick up inherent biases in the data during the training process, leading to unreliable predictions. Diagnosing and debiasing datasets is thus a necessity to ensure reliable model performance. In this paper, we present CONBIAS, a novel framework for diagnosing and mitigating Concept co-occurrence Biases in visual datasets. CONBIAS represents visual datasets as knowledge graphs of concepts, enabling meticulous analysis of spurious concept co-occurrences to uncover concept imbalances across the whole dataset. Moreover, we show that by employing a novel clique-based concept balancing strategy, we can mitigate these imbalances, leading to enhanced performance on downstream tasks. Extensive experiments show that data augmentation based on a balanced concept distribution augmented by CONBIAS improves generalization performance across multiple datasets compared to state-of-the-art methods. We will make our code and data publicly available.", "sections": [{"title": "1 Introduction", "content": "Over the last decade we have witnessed an unparalleled growth in the capabilities of deep learning models across a wide range of tasks, such as image classification [17, 47, 7], object detection [41, 55], semantic segmentation [20, 26, 43], and so on. More recently, with the introduction of large multi-modal models, these capabilities have improved further [25, 15]. However, such models, while demonstrating impressive performance on a wide range of tasks, have been shown to be biased in their predictions [30, 13]. These biases come in various forms, based in texture [14], shape [39, 32], object co-occurrence [51, 52, 48], and so on. In addition to exploring model biases, dataset diagnosis, or evaluating biases directly within the dataset, is particularly crucial as large datasets available today are beyond the scope of human evaluation, owing to their size and complexity. For example, ImageNet [6], a widely used dataset in deep learning literature, is known to have thousands of erroneous labels and a lack of diversity in its class hierarchy [33, 57]. Other popular datasets such as MS-COCO [23] and CelebA [27], have problematic social biases with respect to gendered captions and prejudicial attributes of people from different races. As a result, frameworks that effectively diagnose and debias these datasets are sought.\nWhile multiple works exist in the categorization and exploration of biases in visual data [9, 30], an end-to-end pipeline incorporating both diagnosis and debiasing has received relatively scant attention. ALIA [8] is the closest and most recent work exploring such a data-augmentation-based approach to debiasing, but it has two shortcomings - first, it does not diagnose the dataset which it aims to debias. Without such a diagnosis, it is challenging to identify the biases to be mitigated in the first place."}, {"title": "2 Related Work", "content": "Bias discovery in deep learning models. The identification of biases in trained deep learning models has a rich history, with early works exploring the texture and shape-bias tradeoff in ImageNet-pretrained ResNets [14, 21, 32, 39]. More recently, the field of worst group robustness has emerged, aiming to generalize classifier performance across multiple groups in the data that correspond to known spurious correlations [49, 45, 24, 42]. Debiasing and concept discovery in the feature space of the learned classifier is also common [1, 54, 58]. Testing model performance sensitivity to the presence of particular attributes has also been explored [53, 36]. With the recent rise in popularity of large language models, efforts have been made to identify learned biases using off-the-shelf captioning models [56], adaptive testing [11], and language guidance [19, 37]. Traditional data augmentation approaches such as CutMix [59], and RandAug [5], are used as baselines as well. Our work intervenes on the dataset directly, instead of operating in the model feature space or testing model sensitivity. This allows for a more intuitive and principled approach to bias discovery.\nData diagnosis. Our work is placed in the context of data diagnosis, i.e. identifying biases directly from the data without using the model as a proxy. One of the early influential works expounding the importance of datasets in deep learning research was a systematic review of the popular datasets in computer vision [51]. A modern appraisal categorizing more diverse types of biases in visual datasets exists in [9]. Additionally, works investigating possible issues with dataset labels have also received interest [33, 57]. Data diagnosis tools such as REVISE [52] compute object statistics (including co-occurrence) to generate high-level insights of the data. However, REVISE is not an end-to-end framework that at once diagnoses and debiases data. It is rather an exploratory tool for an overview of common concepts in the dataset. A more recent method, ALIA, uses a language model to populate diverse descriptions of the given dataset, consequently generating images from such descriptions. A more critical look on dataset bias lies in the field of fairness, particularly with regards to societal bias [12, 16]. Finally, benchmark datasets for data diagnosis have also been proposed [29, 28].\nObject co-occurrence bias in visual recognition. Objects are biased in the company they keep. This adage is well known in the computer vision literature, as outlined in [34, 10]. Modern efforts to mitigate object co-occurrence bias involve feature decorrelation [48], object aware contrastive representations [31], causal interventions [38], and fusing object and contextual information via attention [2]. The common theme in tackling contextual and co-occurrence bias lies entirely in using better models (feature representations) rather than intervening in the dataset directly. We place our debiasing method along the data augmentation direction, allowing for better controllability and interpretability of the debiasing stage, rather than relying on semantic features learned by a classifier, which may be difficult for humans to interpret."}, {"title": "3 Approach", "content": "Figure 2 illustrates the overall pipeline of our method. In this section, we begin with the problem statement in Section 3.1, and move to the three major stages in our method definition. Section 3.2 describes the procedure of concept graph construction. Section 3.3 illustrates the details of concept diagnosis. Finally, Section 3.4 presents our method for concept debiasing."}, {"title": "3.1 Problem Statement", "content": "We are given a dataset $D = \\{(x_i, Y_i)\\}_{i=1}^N$, a set of images and their corresponding labels. We also assume access to a concept set $C = \\{C_1, C_2, ..., c_k\\}$ that describes unique objects present in the data. An example concept set looks like the following: {alley, crosswalk, downtown, gas\nstation}, i.e. a list of unique objects present in each image in addition to the class label. Finally, we are given a classifier $f_\\theta(X)$ parameterized by network parameters $\\theta$. The central hypothesis of this work is that the class labels exhibit co-occurring bias with the concept set C, affecting downstream task performance. In this light, we wish to generate an augmented dataset $D_{aug}$ that is debiased with respect to the concepts and their corresponding class labels. Thus, given the new dataset $D' = D \\cup D_{aug}$, we wish to retrain $f_\\theta(X)$ in the standard classification setup:\n$f^* = arg \\min_f E_{(x,y) \\in D'}[L(y, f_\\theta(x))]$,"}, {"title": "3.2 Concept Graph Construction", "content": "We construct a concept graph $G = (V, E, W)$ from the data, where |V| is the node set of the graph,\nE is the edge set, and |W| is the set of weights for each edge in the graph. We first construct the node set V as a union of the label set Y and concept set C:\n$V = Y \\cup C$.\nNext, we construct the edge set E:\n$E = \\{(i, j) | \\exists image D_k such that both i and j appear in D_k\\}$.\nFinally, we construct the weight set W by computing the weights $w_{ij}$ for each edge (i, j) in G:\n$w_{ij} = \\sum_{n=1}^N \u2161(i \\in D_n and j \\in D_n)$,\nwhere I is the indicator function that returns 1 if both i and j are present in the n-th image in D, and\n0 otherwise, and N is the total number of images in the dataset.\nThe concept graph G encapsulates co-occurrence counts between nodes, thus providing an alternative representation of the (originally visual) data. As we show in the next section, this representation helps uncover novel imbalances (bias) contained in the dataset."}, {"title": "3.3 Concept Graph Diagnosis", "content": "In the previous section, we define how to build the concept graph. Here, we present how to leverage the concept graph for discovering co-occurrence biases. We present a principled approach to discovering concept-combinations across classes that co-occur in an imbalanced fashion."}, {"title": "Definition (Class Clique Sets)", "content": "For each class $Y_i \\in Y$, we construct a set of k-cliques using the concept graph G. The set of all possible k-cliques for class $Y_i$ is denoted as $K_i^k$:\n$K_i^k = \\{\\{C_{j_1}, C_{j_2},..., C_{j_k} \\} | C_{j_1}, C_{j_2}, ..., C_{j_k} \\in C and j_1 < j_2 < ... < j_k\\}$,\nwhere $j_1, j_2,...$ are the indices of concepts in C. Then, $K_i$ for class Yi can be successfully con-structed for k = 1, 2, ..., K, where K is the size of the largest clique in G containing $Y_i$. We construct class clique sets for every class in the dataset. An illustration of concept cliques in the Waterbirds dataset that help in bias diagnosis is provided in Figure 3."}, {"title": "Definition (Common Class Clique Sets)", "content": "Given $K_i$ for each class, we then compute the cliques common to all classes. These are the cliques of interest, whose imbalances we want to investigate:\n$K = \\bigcap_i K_i$\nwhere K encapsulates all common cliques enumerated across the dataset for all classes. Refer to Figure 2 for a broad illustration of the k-clique set construction from the concept graph G."}, {"title": "Definition (Imbalanced Common Cliques)", "content": "Given the set of common cliques across all classes K,\nwe compute the imbalanced class-concept combinations, i.e. the imbalanced clique set I:\n$I[K]_{M=1}^{Y_i} = \\{(|\\frac{F_{K}}{F_{K_{y_j}}} - \\frac{F_{K}}{F_{K_{y_m}}}|, arg\\min(F_{K_{y_m}}, F_{K_{y_i}}))\\}, \\forall i, j,$\nwhere $F_{K_{y_m}}$ and $F_{K_{y_i}}$ indicates the co-occurrence frequency of concepts in clique m with respect to class $y_i$ and $y_j$ respectively, and the arg min operator identifies the underrepresented class for the particular concept clique. Thus, each element in I is a number representing the imbalance of each common clique across all classes. For the special case where the size of clique m is 1, this equates to simply looking up the value $w_{ij}$ in G. For the case where the size of m > 2, it is straightforward to compute the co-occurrence of class $y_i$ with respect to concepts in m:\n$w_{ij...k} = \\sum_{n=1}^N \u2161(i \\in D_n and j \\in D_n ... and k \\in D_n)$,\nfor each image $D_n$ in the data. The set I holds rich information about the data. In addition to holding the imbalanced counts of concept combinations across all classes, the set I also holds which is the underrepresented class with respect to a particular concept clique.\nIntuitively, concept combinations that are common across all the classes, but do not co-occur uniformly across the classes are likely biased concept combinations. We provide an example from the Waterbirds dataset in Figure 4. The training set in Waterbirds is intentionally biased to the background: 95% of landbirds appear with land backgrounds, and 95% of waterbirds appear with water backgrounds. First, we find common cliques of varying sizes across the classes (Landbird, Waterbird). One example of a common clique of size 3 is (Landbird, Beach, Ocean) and (Waterbird, Beach, Ocean). We compute the co-occurrence of (Landbird, Beach, Ocean) and (Waterbird, Beach, Ocean) from the extracted metadata, and the imbalance is clear. Since"}, {"title": "3.4 Concept Graph Debiasing", "content": "We have, to this point, constructed a knowl-edge graph of the visual data, and diagnosed it for concept-based co-occurrence biases. Once the imbalanced clique set I is identified in G, we debias the data by gener-ating images containing under-represented concepts across classes.\nRecall that $I = \\{fi, Y_i\\}$ inherently holds the underrepresented class $Y_i$ and the fre-quency $f_i$ by which the original dataset needs to be adjusted with new images of class $Y_i$ with respect to concept clique i. Following the example in the previous sec-tion, we notice that the concepts (Beach, Ocean) are significantly over-represented in the Waterbird class than Landbird. Similarly, the concept Tree is significantly over-represented with the Landbird class than the Waterbird class. As a result, we sample $f_i$ instances of these under-represented cliques with respect to their classes, and prompt a text-to-image generative model for more images of the Waterbird class with the concept Tree. Similarly, we would prompt the model to generate images of Landbird with the concept Beach, Ocean. We generate images for all class based imbalances following this upsampling protocol. Typical prompts for our image-inpainting model would look like: An image of a ocean and a beach, An image of a tree, An image of a forest, etc. We use an inpainting-based method to make sure that the original object is not modified in the image, and that the new concepts are only injected into the non-object space in the image. See the supplementary material for the generated images and the prompts.\nUsing this upsampling protocol, we generate a set of images that leads to our augmented, debiased dataset $D_{aug}$. The original training data D can now be augmented using this data, and the classifier $f_\\theta(X)$ can be retrained on the dataset $D \\cup D_{aug}$. In the next section, we conduct experiments on three datasets to demonstrate our method's significant improvements of baselines."}, {"title": "4 Experiments", "content": "We validate our method on vision datatset diagnosis and debiasing across various scenarios. We begin by introducing the experimental setup including the datasets, baselines, tasks, and implementation details in Section 4.1. Section 4.2 presents the main results of our proposed framework, CONBIAS, compared with state-of-the-art methods. Finally, Section 4.3 details ablation studies and analyses."}, {"title": "4.1 Setup", "content": "Datasets. We use three datasets in our work: Waterbirds [45], UrbanCars [22], and COCO-GB [50], that are commonly used in the bias mitigation domain. We tackle background bias in the Waterbirds dataset, background and co-occuring object bias in the UrbanCars dataset, and finally gender bias in"}, {"title": "4.2 Main Results", "content": "In Table 1 we present the main results, averaged over three training runs. First, we note that for Waterbirds and UrbanCars, we observe significant improvements in both the Class-Balanced and OOD test sets over the typical augmentation methods such as CutMix and RandAug. Second, we note the significant improvement in performance over the most recent state-of-the-art augmentation method, ALIA. Third, for COCO-GB, while we notice slightly smaller difference in the CB and OOD accuracies between our method and the baselines, our hypothesis is that this happens because of limited number of samples used for augmentation. ALIA uses a confidence based filtering mechanism to remove generated samples. This leads to a small final number of 260 samples to be added for the retraining part. In the ablation section, we show this hypothesis to be true, and further demonstrate that on adding more images for the retraining step, we progressively increase the performance gap between our method and the baselines. These three observations taken together validate the usefulness of our approach. The next section provides additional insights on the usefulness of our method and the effect of ablating its components."}, {"title": "4.3 Ablations and Analyses", "content": "We further analyze our method along five axes: (1) The usefulness of the graph structure, (2) Robustness of our method to other evaluation metrics, (3) The impact on CB and OOD performance by increasing the number of added samples for the retraining step, (4) The usefulness of discovered concepts by our method on the trained classifier, and (5) The impact of the generative component in our work compared to ALIA, since the latter uses InstructPix2Pix [3] while we use a Stable Diffusion based inpainting protocol.\nEffect of the graph structure. Recalling the definition of Class Clique Sets, in principle one could only use cliques sizes of 1, i.e., the direct neighbors of each class node. This would be equivalent to computing the frequency of co-occurrence over a single hop neighborhood of the class node in the graph. In this ablation we show that one should use larger cliques, i.e. leverage the graph structure, instead of a simple direct neighborhood based frequency calculation. We trained three separate models on three different types of $D_{aug}$: Ours (BG), trained on images containing only background shortcuts, Ours (CoObj), images containing only the co-occurrence shortcuts, and Ours (Both), images containing both shortcuts, but not simultaneously.\nTable 2 shows the results. First, our approach of leveraging the graph structure provides improvement over simply using the frequency of a 1-hop neighborhood. Second, we note that all the methods outperform the baseline and ALIA, which shows that incorporating frequency based co-occurrences is in a broader sense much more useful than relying on diverse prompts generated by ChatGPT-4, which is the approach taken by ALIA.\nRobustness to evaluation metrics. The CB and OOD test accuracies test for generalization capa-bilities, but more direct evaluators of shortcut learning exist in the literature. In [22], for instance, the authors propose (i) The ID Accuracy - which is the accuracy when the test set contains common background and co-occurring objects, (ii) The BG-GAP - which is the drop in ID accuracy when the test set contains common co-occurring objects, but uncommon background objects, (iii) The CoObj-GAP, which is the drop in ID accuracy when the test set contains uncommon co-occurring objects, but common background objects, and finally (iv) The BG + CoObj GAP, which is the case when both background and co-occurring objects are uncommon in the test set. A multiple shortcut mitigation method should minimize the BG + CoOBj GAP metric, and also make sure it does not exacerbate any shortcut that the base model relies on. In Table 4, we present results of Base, Base (BG), Base (CoObj), Base(Both), and Base (CONBIAS) on these metrics for UrbanCars. We are able to post the lowest drops among all baselines on the CoObj-GAP and BG + CoOBj GAP metrics, suggesting mitigation of multiple shortcut reliance. This places our method in a more realistic context, as it is infeasible to assume that real world data will only have a single type of bias in them.\nScaling the number of images in $D_{aug}$. In Table 1, we commented on the fact that our method provides marginal improvement over the baselines in the COCO-GB dataset. Our hypothesis was that this was due to the low number of images in the augmented dataset. In Figure 5, we demonstrate the impact of adding more images to $D_{aug}$ for retraining. Clearly, our method benefits from this protocol, leading to significant differences over ALIA as we keep increasing the number of images. Note that, infinite enrichment is not recommended and has been found to be detrimental to classifier performance, as progressive addition of synthetic images will likely lead to addition of out-of-distribution examples"}, {"title": "5 Conclusion, Limitations, and Future Works", "content": "While CONBIAS is the first end-to-end pipeline to both diagnose and debias visual datasets, there are some limitations: First, that the enumeration of cliques grows exponentially with the size of the graph. For larger real world graphs, there could be more efficient strategies to find the concept combinations. Second, in this work we focus on biases emanating out of object co-occurrences. A variety of other biases exist in vision datasets, and future work would look to address the same. We add an extended section on broader impact of our work in the supplementary material. In summary, datasets in the real world are biased, and the exponential increase in dataset sizes over the past decade amplifies the challenge of investigating model and dataset biases. While both dataset and model diagnosis are exciting areas of research, an end-to-end diagnosis and debiasing pipeline such as CONBIAS offers a principled approach to diagnosing and debiasing visual datasets, in turn improving downstream classification performance. Our state-of-the art results open up numerous interesting possibilities for future work - incorporating more novel graph structures, and diagnosis under the regime where concept sets may be wholly or partially unavailable, remain interesting directions to pursue."}, {"title": "6 Broader Impact", "content": "Fairness in AI is rapidly gaining priority in current research as models and datasets grow exponentially larger, thus making it more and more complicated to diagnose them for biases. It is imperative to focus on understanding and mitigating biases learned by models, and inherent biases in the data, to ensure reliable and transparent predictions in the real world. The advent of generative models in particular, including large language models, and image generative models, invites new questions into how to reliably regulate such technologies. These models are trained on datasets in the order of hundreds of billions of data points. How do we ensure that problematic aspects of the data do not pass onto the models learning from them? How do we ensure that models do not generate synthetic data that is potentially harmful, misleading, and misinformative in nature? How do we evaluate the quality of generated data by such models? These are the pressing questions that our research direction is interested in."}, {"title": "7 Dataset Details", "content": "We use three datasets in our work - Waterbirds [45], UrbanCars [22], and COCO-GB [50].\nFor Waterbirds, the class labels are Landbird, Waterbird. The Waterbirds dataset has the background bias, i.e. 95% images of landbirds have land-based backgrounds, and 95% images of waterbirds have water-based backgrounds. For the concept set annotations, we use the captions extracted by authors of [8] captions available here.\nFor UrbanCars, the class labels are Urban, Country, defining the type of car. There are multiple biases in UrbanCars - (1) Background Bias, i.e. Urban cars appear with 95% correlation with urban backgrounds, and Country cars appear with 95% correlation with country backgrounds. (2) Co-Occurring object, i.e. Urban cars appear with 95% correlation with urban objects, and Country cars appear with 95% correlation with country objects.\nFor COCO-GB, the class labels are Man, Woman. The bias for the dataset are the set of objects in the MS-COCO dataset [23]. The authors of [50] find a strong bias of most objects in the data with respect to the \"Man\" class, and design a secret, gender-balanced test set to evaluate gender bias in classifiers."}, {"title": "7.1 Waterbirds", "content": "In Fig 7 we present examples from the Waterbirds training data. The classes are heavily biased to the backgrounds, i.e. Landbirds on Land, Waterbirds on Water."}, {"title": "7.2 UrbanCars", "content": "In Fig 8 we present examples from the UrbanCars training data. The classes are heavily biased to multiple shortcuts - Background and Co-Occurring objects."}, {"title": "7.3 COCO-GB", "content": "In Fig 9 we present examples from the COCO-GB training data. The \"Man\" class is known to be heavily biased in MS-COCO to everyday objects."}, {"title": "7.4 Splits", "content": "In Table 5 we present the train, validation, and test splits for our three datasets."}, {"title": "8 Concept Sets", "content": "In Table 6 we present the full concept sets for each dataset. The Waterbirds dataset has 64 unique concepts, the UrbanCars dataset has 17 unique concepts, and COCO-GB has 81 unique concepts, all from the MS-COCO dataset. Note that both MS-COCO and UrbanCars have ground truth concepts, while for Waterbirds, we use the extracted captions here."}, {"title": "9 Dataset Imbalances", "content": "In this section we shed more insight into what sort of concept imbalances ConBias discovers. These object level insights are also, to the best of our knowledge, the first of its kind, shedding more light on the secret co-ocurrence biases hidden in data."}, {"title": "9.1 Waterbirds", "content": "In addition to the main paper, we list some other category imbalances in Waterbirds in Figure 10. Some of these extreme imbalances appear in diverse 2-clique/3-clique combinations. For example, we see that concepts like forest, man, woman are significantly biased towards the Landbird class, while concepts like beach, man, sun, lake, mountain are biased towards the Waterbird class. This is the background bias that is known in the Waterbirds dataset, that ConBias successfully uncovers."}, {"title": "9.2 UrbanCars", "content": "In UrbanCars, the class labels (country car, urban car) are intentionally biased towards background and co-occurring objects. In Figure 11, we see that there exists an extreme imbalance betwee urban concepts such as driveway, traffic light towards urban cars, and country concepts such as forest road, field road, cow, horse towards country cars. These are exactly the background and co-occurring biases in the construction of the data, that ConBias successfully uncovers."}, {"title": "9.3 COCO-GB", "content": "The gender bias in COCO-GB has been extensively studied in [50]. In Figure 12, we show the extreme imbalance towards specific concepts in the MS-COCO dataset. Concepts such as baseball bat, sports ball, motorcycle, truck overwhelmingly correlate with images of men, which may be problematic for the classifier to learn."}, {"title": "10 Generative Model", "content": "Here we present more details of our generative model. We use Stable Diffusion based inpainting, as illustrated in Figure 13. Given the prompt, we first generate an image using Stable Diffusion [44]. Next, using ground truth masks of the object, we paste the object at the foreground of the generated image. In this way, we preserve the original object in the image, which is a challenge for traditional image editing methods such as InstructPix2Pix. We believe the inpainting method is a more principled approach to synthetic image generation, particularly if the downstream task is classification in nature."}, {"title": "11 Generated Images by ConBias", "content": "In this section we present examples of synthetic data generated by ConBias for Waterbirds, UrbanCars, and COCO-GB."}, {"title": "11.1 Waterbirds", "content": "In Figure 14 we present diverse images generated by ConBias for the two classes of Landbird and Waterbird. Due to the bias diagnosis stage where we found the overwhelming correlation between landbirds with land based backgrounds such as tree, forest, field, grass, etc, and waterbirds with water based backgrounds such as beach, ocean, boat, etc, ConBias was automatically able to decide which concept combinations to use to generate new, debiased images."}, {"title": "11.2 UrbanCars", "content": "In Figure 15 we present diverse images generated by ConBias for the two classes of Urban and Country cars. Due to the bias diagnosis stage, we were able to discover the overwhelming correlation between urban cars with urban based backgrounds such as gas station, driveway, alley, etc and urban co-occurring objects such as fireplug, stop sign, etc. Similarly, for country cars, we discovered bias towards country backgrounds such as desert road, field road, forest road, and, and country co-occurring objects such as cow, sheep, horse. As a result, ConBias helps generate urban cars with country based backgrounds and co-occurring objects, and vice versa."}, {"title": "11.3 COCO-GB", "content": "In Figure 16 we present diverse images generated by ConBias for the two classes of Man and Woman in COCO-GB. In this dataset, we were able to discover significant under-representation of women with respect to common, everyday objects in the MS-COCO dataset. Some examples include skateboard, motorcycle, car, truck, etc. These objects could have gendered assumptions and it is imperative for debiased datasets to have uniform representation across classes for such concepts.\nWe would also like to bring to the attention of our readers the successful nature of the inpainting procedure. We are able to consistently preserve the class label of interest in the synthetic images."}, {"title": "12 Confidence Intervals", "content": "In Table 7 we present the averaged results with standard deviations over three training runs. For both Waterbirds and UrbanCars, our improvements are large and significant. For COCO-GB, while originally did not observe statistically significant results, in the main paper we showed that increasing the number of images in $D_{aug}$ leads to significant improvements over the baselines."}, {"title": "13 Compute Details", "content": "We trained all models on a single NVIDIA RTX A4000 and used PyTorch [35] for all experiments. With the early stopping cosine learning scheduled described in the main paper, we observed fast training times, with 90 minutes for three runs on Waterbirds and UrbanCars, and 180 minutes for three runs on COCO-GB."}, {"title": "14 Sampling Algorithm", "content": ""}]}