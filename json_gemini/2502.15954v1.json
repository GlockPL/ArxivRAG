{"title": "MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning", "authors": ["Zaifu Zhan", "Jun Wang", "Shuang Zhou", "Jiawen Deng", "Rui Zhang"], "abstract": "Objective: To optimize in-context learning in biomedical natural language processing by improving example selection.\nMethods: We introduce a novel multi-mode retrieval-augmented generation (MMRAG) framework, which integrates four retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2) Top Mode, retrieving the most relevant examples based on similarity; (3) Diversity Mode, ensuring variation in selected examples; and (4) Class Mode, selecting category-representative examples. This study evaluates MMRAG on three core biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction (RE), and Text Classification (TC). The datasets used include BC2GM for gene and protein mention recognition (NER), DDI for drug-drug interaction extraction (RE), GIT for general biomedical information extraction (RE), and HealthAdvice for health-related text classification (TC). The framework is tested with two large language models (Llama-2-7B, Llama-3-8B) and three retrievers (Contriever, MedCPT, BGE-Large) to assess performance across different retrieval strategies.\nResults: The results from the Random mode indicate that providing more examples in the prompt improves the model's generation performance. Meanwhile, Top mode and Diversity mode significantly outperform Random mode on the RE (DDI) task, achieving an F1 score of 0.9669\u2014a 26.4% improvement. Among the three retrievers tested, Contriever outperformed the other two in a greater number of experiments. Additionally, Llama 2 and Llama 3 demonstrated varying capabilities across different tasks, with Llama 3 showing a clear advantage in handling NER tasks.\nConclusion: MMRAG effectively enhances biomedical in-context learning by refining example selection, mitigating data scarcity issues, and demonstrating superior adaptability for NLP-driven healthcare applications.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) such as the GPT series[1,2] and Llama models[3], are now playing an increasingly pivotal role in healthcare[4], demonstrating remarkable capabilities in processing biomedical text and electronic health records (EHRs)[5\u20137]. Their applications include extracting clinical insights[8,9], supporting patient monitoring[10], performing clinical text classification[11], addressing natural language inference[12], and facilitating medical information extraction[13\u201315].\nHowever, due to the need to protect patient privacy[16], research in medical NLP faces significant challenges related to insufficient training data[17,18]. Although hospitals and healthcare institutions often possess large amounts of raw EHR data, much of this data is unstructured and unlabeled[4]. The process of curating, annotating, and structuring this data into usable formats for training machine learning models is time-consuming, money-consuming and resource-intensive. Furthermore, for rare diseases[19,20]\u2014such as genetic disorders, orphan diseases, and conditions with limited geographic or demographic prevalence\u2014the lack of documented cases exacerbates the data scarcity issue. These conditions inherently have fewer reported instances, making it difficult to collect even raw EHR data, let alone labeled datasets. Finally, privacy regulations such as HIPAA or GDPR further restrict access to sensitive patient information, making it even harder to obtain and share data for research purposes. As a result, novel approaches are urgently needed to address these challenges.\nIn this context, in-context learning (ICL) might be a promising solution. Along with the development of LLMs, the size of these models has grown significantly, the amount of training data has increased, and the length of prompts has expanded[21]. As a result, LLMs have begun to demonstrate the ability to learn from context, a capability known as ICL[22]. This approach allows the model to learn patterns from a few examples provided within the prompt, enabling it to deliver higher-quality responses. In-context learning has several advantages. First, it eliminates the need for training or fine-tuning, which requires substantial computational resources and large amounts of data; instead, ICL only requires a few examples[23]. Second, designing prompts is much simpler than training a model[24], making this approach accessible to individuals without specialized knowledge of LLMs, such as doctors in hospitals. Third, providing examples aligns with the human brain's natural way of thinking-learning through analogy. This paradigm allows for the seamless incorporation of human knowledge into LLMs by simply adjusting demonstrations and templates[25,26]. Finally, a single model can address a wide range of tasks by designing different prompts and providing corresponding examples, showcasing its remarkable versatility[27]. Therefore, ICL is indispensable in the field of healthcare.\nBy leveraging the ability of large language models to learn patterns from a few carefully designed examples embedded in the prompt, ICL could mitigate the insufficient data challenges in the biomedical domain. However, despite its potential, the process of selecting relevant examples from existing verified or labeled data for ICL remains underexplored[28]. The careful curation of examples is crucial in biomedical contexts, as the examples not only encapsulate rich medical knowledge\u2014such as clinical guidelines, rare disease manifestations, or treatment pathways\u2014but also significantly influence the quality and accuracy of the model's responses based on their relevance to the input[23,25,28]. Therefore, developing effective methodologies for selecting high-quality examples becomes essential to unlocking the full potential of ICL in medical NLP.\nFortunately, Retrieval-Augmented Generation (RAG)[29,30], a technique designed to extract information directly relevant to an input query, offers a promising approach to address this challenge. RAG can enhance the example selection process by efficiently retrieving contextually similar data points[31,32], making it a natural complement to ICL. To further advance the application of ICL in medical NLP, we propose an innovative MMRAG framework that leverages RAG to extract and select similar examples using different strategies tailored to diverse use cases. This framework introduces four distinct modes for example selection: random mode, where examples are selected without any specific prioritization; top mode, which retrieves the most relevant examples based on similarity metrics; diverse mode, which ensures a broad range of examples to increase diversity; and classes mode, which focuses on selecting examples representative of specific categories or conditions. Each mode corresponds to a unique rule for selecting examples, providing flexibility and adaptability to a variety of medical NLP tasks. By integrating these strategies, the proposed framework offers a systematic and effective approach to maximize the potential of ICL in addressing critical challenges in medical NLP, particularly in domains constrained by data scarcity and privacy considerations. Our key contributions are summarized as follows:\n\u03a0 To the best of our knowledge, this is the first study to investigate the selection of examples in the biomedical domain for in-context learning.\n\u03a0 We propose a novel framework with four distinct modes (random, top, diverse, classes) to optimize example selection and explore the potential of ICL in biomedical contexts.\n\u03a0 We conduct extensive experiments, evaluating the framework across two LLMs on four datasets with three retrievers, providing comprehensive insights into its performance."}, {"title": "2 Methods", "content": "2.1 Overview of methods\nThis study introduces MMRAG (Multi-mode Retrieval-Augmented Generation) framework, designed to enhance ICL in biomedical NLP by optimizing example selection. MMRAG employs four retrieval strategies: Random Mode (arbitrary selection), Top Mode (most similar examples), Diversity Mode (balanced similarity and variation), and Class Mode (representative examples from different categories).\nThe framework integrates RAG with LLMs (Llama-2-7B[33], Llama-3-8B[34]) and three retrievers (Contriever[35], MedCPT[36], BGE-Large[37]) to systematically evaluate retrieval effectiveness. Our experiments cover Named Entity Recognition (NER), Relation Extraction (RE), and Text Classification (TC) using four biomedical datasets (BC2GM[38], DDI[39], GIT[40], HealthAdvice[41]). This framework demonstrates how tailored retrieval strategies can significantly improve ICL performance in biomedical NLP, addressing challenges such as data scarcity and privacy constraints.\n2.2 Tasks and datasets\nIn this work, we selected three core information extraction tasks in biomedical domains: NER, RE, TC. The main purpose of these tasks is to extract the useful information from the raw text, which is process from unstructured data to useful information for any downstream applications.\nNamed Entity Recognition (NER) - BC2GM[38]: The BC2GM dataset is a widely used benchmark for biomedical named entity recognition (NER), specifically focusing on gene and protein mentions in scientific literature. Originating from the BioCreative II Gene Mention task, it consists of manually annotated gene mentions extracted from PubMed abstracts. This dataset plays a pivotal role in biomedical text mining by enabling the development and evaluation of NER models that can accurately identify gene-related entities.\nRelation Extraction (RE) - DDI[39]: The DDI (Drug-Drug Interaction) dataset is a key resource for relation extraction in the biomedical domain, particularly designed to identify and classify drug-drug interactions from text. It includes sentences from biomedical literature and drug product information sources, annotated with drug entities and their interactions. This dataset is crucial for pharmacovigilance and drug safety, as accurate extraction of drug interactions supports clinical decision-making, prevents adverse drug reactions, and enhances patient safety.\nRelation Extraction (RE) - GIT[40]: The GIT (General BioMedical and Complementary and Integrative Health Triples) dataset is a high-quality biomedical triple extraction dataset specifically focused on non-drug therapies. It is characterized by its high-quality annotations and comprehensive coverage of relation types, making it a valuable resource for biomedical relation extraction. The dataset includes 22 relation types derived from SemMedDB, providing structured representations of relationships within biomedical texts. By supporting the extraction of meaningful entity-relation triples, GIT facilitates advancements in knowledge graph construction, automated reasoning, and biomedical text mining applications related to non-drug interventions.\nText Classification (TC) - HealthAdvice[41]: The HealthAdvice dataset is designed for text classification tasks related to health information and advisory content. It consists of a diverse collection of health-related advice. The dataset is structured to facilitate automatic classification of health advice into relevant categories, supporting applications such as misinformation detection, personalized health recommendations, and automated triaging of medical inquiries. By leveraging this dataset, researchers can enhance natural language processing (NLP) models tailored for health communication and decision support.\nThe combination of these datasets\u2014BC2GM for entity recognition, DDI and GIT for relation extraction, and HealthAdvice for text classification\u2014provides a comprehensive foundation for biomedical NLP research. These datasets enable the development of advanced NLP models capable of extracting critical biomedical knowledge from vast textual sources. By improving the accuracy of entity recognition, relationship extraction, and health-related text classification, they contribute significantly to biomedical informatics, clinical decision support, and evidence-based medicine.\n2.2 MMRAG Framework\nThe motivation for the proposed framework is to release the potential of ICL using the RAG technique for biomedical information extraction with LLMs. As depicted in Fig. 1, the retrieval process begins by embedding both the training set sentences and the input sentence. A retriever then ranks the training sentences based on similarity scores. The selection mode determines how the final examples are chosen from this ranked list. Then, we introduced four different modes for selecting examples: Random Mode, Top Mode, Diversity Mode, and Class Mode. These modes determine how retrieved sentences are selected based on their similarity rankings, striking different balances between similarity and diversity.\nRandom Mode (Few-shot): Random Mode selects examples arbitrarily from the training set without considering similarity ranking. This introduces randomness, which can improve model generalization but may result in irrelevant examples. While this mode maximizes diversity, it does not ensure the selection of highly relevant sentences.\nTop Mode: Top Mode selects the most similar examples by directly choosing the top-ranked sentences. This ensures high contextual similarity between the input sentence and the retrieved examples. However, it may lead to redundancy, as the examples are often very similar to each other, limiting the diversity of information.\nDiversity Mode: Diversity Mode introduces a skip-step selection mechanism, where examples are picked at intervals from the ranked list instead of consecutively selecting the top sentences. By skipping certain highly ranked examples, this method ensures that the retrieved examples maintain relevance while increasing the diversity of information.\nClass Mode: Class Mode selects the most relevant example from each predefined semantic class rather than purely ranking by similarity. This ensures that different types of sentences are represented in the final selection, improving coverage across multiple categories. While it maintains relevance, some selected examples may have slightly lower similarity scores compared to those chosen in Top Mode.\nEach mode provides a different trade-off between similarity and diversity. Random Mode cannot guarantee diversity and relevance, but it represents the most common method. Top Mode guarantees high similarity but lacks diversity. Diversity Mode balances both by selecting non-consecutive similar examples. Class Mode further enhances diversity by ensuring category coverage. Choosing the appropriate mode depends on the specific requirements of the retrieval task.\n2.3 Experimental Settings\nOur experiments were conducted using two large language models, Llama-2-7B[33] and Llama-3-8B[34], both of which have demonstrated strong performance in natural language processing tasks. To retrieve relevant examples, we employed three different retrievers: Contriever[35], a dense retriever trained with contrastive learning for general-purpose retrieval; MedCPT[36], a retriever optimized for medical and clinical text retrieval; and BGE-Large[37], an embedding-based retriever known for its effectiveness in similarity search. These retrievers were chosen to test the adaptability of our framework across different retrieval strategies.\nAll experiments were performed on NVIDIA A100 GPUs with 40GB memory, ensuring sufficient computational resources for training and inference. The batch size per device was set to 4 for both training and evaluation, and the inference was carried out sentence by sentence to optimize efficiency. For automated evaluation, we applied Low-Rank Adaptation[42], a parameter-efficient fine-tuning method, with a rank of 64, alpha set to 32, and a dropout rate of 0.1, to fine-tune the model. The models were optimized using the AdamW optimizer with a learning rate of 1e-5. Fine-tuning was conducted for 5,000 steps, with evaluations performed every 1,000 steps, and the best-performing model was selected for inference.\nFor evaluation, we adopted Micro Precision, Recall, and F1-score, in line with established studies. A prediction was considered correct only if the entire output exactly matched the ground truth, ensuring a strict and reliable assessment of model performance. This evaluation approach allowed us to measure the effectiveness of different retrieval strategies and selection modes in a controlled and reproducible manner."}, {"title": "3 Results", "content": "3.1 Performance in Random Mode\nIn Random Mode, examples were selected arbitrarily from the dataset, providing a baseline comparison for retrieval-enhanced selection strategies. Table 1 shows the mean and standard deviation of F1 scores for different tasks and datasets since the random mode introduces huge randomness. For the NER task on BC2GM, Llama-2-7B achieved an F1 score of 0.8766 with one example, improving to 0.9172 with ten examples. Llama-3-8B showed superior performance, reaching 0.9660 with one example and further increasing to 0.9782 with ten examples. In the RE task on the DDI dataset, Llama-2-7B fluctuated between 0.7049 and 0.7050 across different example counts, while Llama-3-8B exhibited marginal improvement, achieving a maximum F1 score of 0.7114 with five examples. In text classification on HealthAdvice, Llama-2-7B increased from 0.8954 (one example) to 0.9171 (ten examples), while Llama-3-8B ranged from 0.8848 to 0.9084. These results indicate that while Random Mode can yield reasonable performance improvements, it lacks the reliability and control of retrieval-based selection.\n3.2 Performance in Top Mode\nTop Mode retrieves the most similar examples based on similarity rankings. Table 2 presents the results for this mode, demonstrating notable performance improvements over Random Mode. In NER, Llama-2-7B reached an F1 score of 0.9098 with five examples when using the Contriever retriever, while Llama-3-8B achieved a peak score of 0.9726. The MedCPT retriever also contributed to strong results, with Llama-3-8B scoring 0.9706. For RE on the DDI dataset, Contriever retrieval led to the highest F1 score of 0.9669 for Llama-2-7B and 0.9573 for Llama-3-8B. Similarly, for text classification, Top Mode improved performance, with Llama-2-7B reaching 0.8969 and Llama-3-8B achieving 0.8923 when using Contriever. These results confirm that retrieval-driven selection significantly enhances model effectiveness, particularly when using domain-optimized retrievers.\n3.3 Performance in Diversity Mode\nDiversity Mode introduces a balance between similarity and variation by selecting examples at intervals within the ranked retrieval list. Table 3 presents the results, demonstrating that this approach improved model robustness across datasets. In NER, the highest F1 scores were achieved using the Contriever retriever, with Llama-2-7B reaching 0.9134 and Llama-3-8B achieving 0.9731 with ten examples. For the DDI dataset, Diversity Mode yielded stable results, with Llama-2-7B reaching 0.9666 and Llama-3-8B scoring 0.9623 using Contriever retrieval. Text classification results remained competitive, with Llama-2-7B achieving an F1 score of 0.8980 and Llama-3-8B reaching 0.8765. The results indicate that Diversity Mode helps mitigate overfitting while maintaining high task relevance.\n3.4 Performance in Class Mode\nTable 4 compares Llama2 and Llama3 across three datasets using three retrievers in Class Mode. Llama3 generally outperforms Llama2, especially in RE (GIT) where its F1-score with MedCPT reaches 0.6129, significantly higher than Llama2\u2019s 0.3786. In RE (DDI), Contriever achieves the best performance for both models, with Llama3 scoring 0.7060. For TC (HealthAdvice) both models perform well, with Llama2 slightly ahead using random retrieval, but Llama3 excels with Contriever (0.8917 F1-score). These results highlight Llama3's superior retrieval utilization and the impact of effective retrievers like MedCPT and Contriever on model performance."}, {"title": "4 Discussion", "content": "The proposed MMRAG framework is particularly valuable for the biomedical domain, where labeled data is scarce due to privacy constraints and annotation costs. Biomedical NLP tasks require high precision, as errors in entity recognition or relation extraction can lead to misinformation in clinical settings. By integrating retrieval-augmented generation, this framework improves in-context learning by dynamically selecting relevant and diverse examples. The results demonstrate that Top Mode provided the highest F1 scores in precision-sensitive tasks such as NER and RE, demonstrating that high-relevance retrieval improves performance. However, the Diversity Mode achieved more stable generalization across datasets while preserving task relevance. Class Mode, while slightly reducing F1 scores in some cases, ensured broader category representation, which is crucial for classification and multi-relation extraction tasks. Unlike traditional fine-tuning, this approach allows models to adapt to evolving biomedical knowledge. Its flexibility makes it suitable for applications in clinical decision support, drug discovery, and genomic research."}, {"title": "4.1 Error Analysis", "content": "Diving deeper into these promising results, our observations of the generated outputs reveal that LLMs mainly suffer from the following two errors:\nGeneration Errors: These include misclassifying entities, incorrectly assigning relationships between head and tail entities, and extracting non-entity words as entities. For example, LLMs wrongly identify the word \"Genetic\" as gene type but it is not.\nOver-Extraction: Due to our strict evaluation criteria, extracting extra entities beyond the ground truth or having boundary errors in entity spans can lead to a decline in performance scores. For example, \"CRE-binding-protein\u201d could be tokenized as ['CRE', '-', 'binding', '-', 'protein'] but the LLM recognizes \"-\" as not part of disease tokens."}, {"title": "4.2 Limitations and Future Directions", "content": "While this study focuses on example retrieval strategies, several limitations remain:\n\u03a0 Limited model diversity: Although the MMRAG framework is compatible with all generative LLMs, this study only evaluates two LLMs, Llama2 and Llama3. Future work should explore the effectiveness of retrieval strategies across a wider range of models, including newer architectures and domain-specific LLMs.\n\u03a0 Scalability to large datasets: The current framework is applicable to scenarios with large amounts of data. However, to fully leverage big data, future research should focus on developing more efficient retrieval algorithms that optimize performance while maintaining computational feasibility.\nBy addressing these limitations, retrieval-augmented in-context learning can be further enhanced for broader applications in AI-driven biomedical and other large-scale NLP tasks."}, {"title": "5 Conclusion", "content": "In this study, we proposed MMRAG, a novel multi-mode retrieval-augmented framework that enhances ICL in biomedical NLP by optimizing example selection. By systematically evaluating four retrieval strategies-Random, Top, Diversity, and Class Mode\u2014our approach demonstrates superior adaptability across multiple biomedical tasks. The results show that retrieval-enhanced selection significantly improves information extraction, particularly in precision-critical tasks. As AI-driven healthcare evolves, MMRAG provides a flexible and efficient method to address data scarcity and privacy challenges, paving the way for future advancements in clinical decision support, drug discovery, and biomedical text processing."}]}