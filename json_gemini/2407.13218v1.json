{"title": "LINR: Model Based Neural Retrieval on GPUs at LinkedIn", "authors": ["Fedor Borisyuk", "Qingquan Song", "Mingzhou Zhou", "Ganesh Parameswaran", "Madhu Arun", "Siva Popuri", "Tugrul Bingol", "Zhuotao Pei", "Kuang-Hsuan Lee", "Lu Zheng", "Qizhan Shao", "Ali Naqvi", "Sen Zhou", "Aman Gupta"], "abstract": "This paper introduces LiNR, LinkedIn's large-scale, GPU-based retrieval system. LiNR supports a billion-sized index on GPU models. We discuss our experiences and challenges in creating scalable, differentiable search indexes using TensorFlow and PyTorch at production scale. In LiNR, both items and model weights are integrated into the model binary. Viewing index construction as a form of model training, we describe scaling our system for large indexes, incorporating full scans and efficient filtering. A key focus is on enabling attribute-based pre-filtering for exhaustive GPU searches, addressing the common challenge of post-filtering in KNN searches that often reduces system quality. We further provide multi-embedding retrieval algorithms and strategies for tackling cold start issues in retrieval. Our advancements in supporting larger indexes through quantization are also discussed. We believe LiNR represents one of the industry's first Live-updated model-based retrieval indexes. Applied to out-of-network post recommendations on LinkedIn Feed, LiNR has contributed to a 3% relative increase in professional daily active users. We envisage LiNR as a step towards integrating retrieval and ranking into a single GPU model, simplifying complex infrastructures and enabling end-to-end optimization of the entire differentiable infrastructure through gradient descent.", "sections": [{"title": "INTRODUCTION", "content": "LinkedIn, the world's largest professional network, serves over a billion members globally, offering services from job searches to content engagement. This paper explores LiNR, LinkedIn's model-based GPU retrieval system, focusing on embedding-based retrieval (EBR). Traditional EBR uses unsupervised nearest neighbor search solutions [9, 14], indexing item vectors for fast retrieval. Our paper presents an innovative approach, combining exhaustive search with pre-filtering in a differentiable GPU model, using neural networks for distance learning and ranking. In LiNR, item vectors and model weights coexist within the same model binary, unlike traditional search indexing methods.\nWe believe the future of search and recommender systems lies in differentiable model-based serving, enabling joint optimization of retrieval and ranking. The K-nearest neighbor (KNN) search algorithm, an essential embedding-based retrieval method, uses learned query and item embeddings with a specific similarity metric to select the top-K closest items. Typically, KNN uses dot-product similarity, a form of matrix multiplication with normalized embeddings, which has been significantly sped up on modern GPUs (A100, H100, etc.) in frameworks like PyTorch and TensorFlow. Several challenges motivate us to propose model-based KNN algorithms implemented on GPUs:\n\u2022 Liquidity challenge: Real-time search systems rely on specific attributes to filter relevant items. In job recommendation systems, for example, filters like company names, locations, and skills are essential. Items meeting these conditions must be prioritized to avoid exclusion due to low KNN scores from embeddings alone.\n\u2022 Low latency requirement: Reducing retrieval latency and increasing throughput is a constant priority.\n\u2022 Huge memory cost: As number item embeddings and clauses increase, finding ways to lower memory usage and boost computational speed without compromising retrieval quality presents a significant challenge.\n\u2022 Freshness: Demonstrate that model-based approaches can enhance traditional nearest neighbor searches in quality and latency while supporting functionalities like live updates.\nIn this paper we discuss deployment of large-scale, neural model-based retrieval system, highlighting key challenges and solutions. A major challenge was the absence of efficient pre-filtering in PyTorch and TensorFlow, addressed by our custom indexing and filtering methods detailed in \u00a73.1, which also tackle latency issues. We also cover memory cost management through quantization techniques for larger indexes in \u00a73.2. LiNR enhanced search quality, utilizing multi-embedding retrieval algorithms discussed in \u00a73.3. Our work positions us among the pioneers in the industry in introducing a retrieval model-based serving infrastructure (\u00a74.2), showcasing the capability of such model-based retrieval systems to be effectively live-updated at scale (\u00a74.3). We perform our study of model-based index serving focuses on interest-based recommendations on LinkedIn's Feed, also known as out-of-network (OON) recommendations. These recommendations leverage member profiles and previous interactions with the Feed, enabling LinkedIn members to access highly relevant content. We integrate OON content into various LinkedIn surfaces, like Feed and Notifications, based on predicted user engagement likelihood. The effectiveness of OON recommendations is gauged by member interactions with OON content. We use two-tower neural networks to create embeddings for members and Feed Posts, forming a candidate selection vertical for OON in the Feed through EBR with a differentiable model-based search index. LiNR significantly outperforms FAISS-based [8] retrieval system in OON recommendations. We support full-scan model-based index serving on GPUs with latencies as low as 4 ms, handling indexes from 15 million to a billion entries. This capability, along with modeling enhancements, significantly boosts quality as detailed in \u00a75.2."}, {"title": "RELATED WORK", "content": "Industry focus has predominantly been on approximate neighbor search systems, with FAISS [8], ScaNN [4], SONG [23], RAFT [15] among notable examples. These support algorithms like HNSW [14], IVFPQ [9], CAGRA [16] on CPU and GPU platforms. Termed model-free, these methods use unsupervised algorithms for partitioning space using existing item embeddings, offering flexibility for any item set. In contrast, our approach employs deep neural networks for a model-based search index, fully operational on GPUs. We integrate item indexes with neural network weights within a PyTorch or TensorFlow model, training during index construction and using the model for retrieval.\nRecently with more performance and memory available on GPUs several publications have appeared considering model based nearest neighbor search such as [18, 20-22]. Mixture of logits (MoL) [21] in its production deployed form implements weighted combination of cosine similarities with neural network gates used to infer per distance component weights. The MoL paper does not provide information on examples of implementation of logits components, and which embedings have been used in production. We extend on top of MoL and introduce practical algorithms on how to learn components of MoL. Conversely, research by [18, 20, 22] has explored using transformers and generative techniques for search indexes. Unlike our system, which stores item embeddings directly, these studies create semantic structures through clustering and transformers to generate document IDs.\nA lot of research has been focused on representation learning with works representing posts and users in social networks [12, 17, 19]. As one of the components in MoL we have used approaches similar to [12, 17, 19], and additionally extended it with approaches for cold start infrequent users using clustering representations. Several previous works have explored the concept of model live-updates, which we expand on in this paper. These works include Monolith [13], PERSIA [11], and XDL [7]. In contrast, traditional search engines, as seen in Facebook Search EBR [6, 12] via Unicorn [3], and Lucene [2], have primarily focused on live-update functionality for unsupervised indexing techniques such as [8]. To the best of our knowledge, our paper represents one of the pioneering efforts in the realm of retrieval-based techniques for live-updating TensorFlow (TF) or PyTorch model-based retrieval indexes at a large-scale production level, with high QPS demands."}, {"title": "MODELING TECHNOLOGY", "content": "In this section we will describe how we modeled and developed exhaustive embedding-based search on GPU with attribute-based matching. We will provide details on how we scaled our model-based index to billion size on a single GPU with quantization. We extend Mixture of Logits (MoL) [21] by automatically training cluster embedding components and experimenting with different gating functions and variety of embedding components."}, {"title": "Exhaustive Search with Attribute-Based Matching (ABM)", "content": "Considering the post-filtering (filter after similarity-based retrieval) often suffers from the liquidity issue especially combining with ANN algorithms implemented on GPUs [24], we first focus on the KNN-based algorithm with attribute-based pre-filtering and introduce several basic approaches adopted to tackle the above challenges. Strategies to further improve the algorithm and tackle other online serving challenges including the live update problem will be introduced in \u00a74."}, {"title": "KNN with Similarity Masking", "content": "Our first KNN algorithm with ABM is a two-step similarity masking approach. As shown in Figure 1, given a query, we first compute the similarity between the query embedding with all item embeddings stored in a matrix to capture their semantic relationships in a similarity vector. Then, we filter out irrelevant items by multiplying it with the 0-1 mask vectors given by each clause to map the similarity scores of the filtered items to zero before the top-K selection. Each query clause could contain multiple attributes. Feasible items should satisfy all clauses, requiring at least one of the attribute in each clauses is matched. Reverse clauses are also supported (such as the company name attributes in Figure 1). As each item could contain different number of attributes for each clause, to effectively utilize the GPU memory for saving and update the clauses, we store all clause attributes in a single matrix in practice and have an extra counting matrix to record the number of attributes for each item in each clause similar to the counting matrix in a CSR format but for each item separately without having the indexing vector. Each item clause is sorted before the concatenation for faster judgement (as we can stop checking early as long as one attribute is matched). We implement the algorithm in CUDA and registered the clause filtering kernel as TensorFlow and PyTorch operations to integrate and serve with other modules."}, {"title": "KNN with Explicit Pre-Filtering", "content": "The second iteration of our KNN with ABM uses a new approach, incorporating explicit filtering before embedding multiplication, as illustrated in Figure 2. Initially, we slice the matrix to filter out irrelevant items, removing them early from subsequent computations. This method speeds up the process by reducing the computational burden during matrix multiplication and top-K selection, especially beneficial when the query filters result in a significantly smaller item set. We found that with custom CUDA implementation to merge the kernels, the speed could be generally faster than the first version introduced above. However, without customizing the masked matrix multiplication and kernel merging, simply adopting the matrix slicing in TensorFlow and PyTorch will introduce extra matrix copy and creation overhead, causing it to be slower than the first version when the pass rate is high."}, {"title": "Quantized KNN", "content": "Addressing memory constraints, we adopt a quantized KNN strategy using the Sign One Permutation One Random Projection (Sign-OPORP) method to compress embeddings to 1-bit and approximate dot-products via bitwise matching. This technique balances prediction accuracy and search speed, akin to typical ANN methods, but as an exhaustive search, it seamlessly integrates with attribute-based pre-filtering, circumventing liquidity issues. OPORP is a variant of count-sketch method. It leverages single random projection with fixed-length binning scheme to efficiently project embedding to a low-dimensional embedding. Sign-OPORP takes the sign of the projected embedding to generate 1-bit embedding that could accurately approximate the cosine similarity of the original floating-point embedding [10] via bit-wise matching, i.e., counting the number of matched bits of two quantized 1-bit embedding.\nAs the bit-wise matching operation is often much faster than regular matrix multiplication, we can replace the original embedding with quantized embedding and adopt the bit-wise matching operations in the above-mentioned KNN algorithm with pre-filtering, which can help greatly reduce the memory consumption. Compressing 1 billion fp16 embedding of dimension 64 to 1-bit embedding of the same dimension can reduce the memory by 16 times and help serve 1 billion items in single V100 GPU. We could adjust the size of the quantized embedding to balance the trade-off between the memory/speed and accuracy. Besides, if memory is not the concern, we could leverage the approximated similarity as an extra pre-filtering step reduce the computation of the full-precision matrix multiplication (see Figure 3), offering a unique perspective on exhaustive KNN with ABM. Note that, we call it exhaustive KNN to discriminate it from the regular ANN method with clustering such as HNSW and IVFPQ since our approach still computes all item similarity based on the quantized embeddings, which is easier to be combined with the pre-filtered ABM and live updates (see \u00a74.3)."}, {"title": "Similarity Modeling", "content": ""}, {"title": "Hadamard MLP", "content": "Dot Product or cosine similarity has been common in retrieval and it's computationally efficient. On the other hand, the multilayer perceptron (MLP)-based learned similarity functions has been reported inferior compared to properly tuned dot product. To balance the computation cost/latency and retrieval metrics, we attempted to boost the MLP-based learned similarity function through hadamard product. The architecture is shown on the left of Figure 4. A MLP block is applied to member and item embedding respectively, whose output performs hadamard product and then passes to another MLP block to output the final logit."}, {"title": "Mixture-of-Logits with Clustering", "content": "Mixture-of-logits [21] defines a model for computing high rank similarity based on adaptive gating of elementary logits across multiple embedding components:\n$MOL(x, u) = \\sum_{k} \\pi_{k,\\theta}(x, u) \\delta_{k,\\theta}(x, u)$ (1)\n$\\pi_{k,\\theta}(x, u)$ represents a learnt gating function, which gives per component weight using soft-max gate given input of user and item features. The parameters $\\theta$ are learnt through Adam optimization of gradients of a sampled soft-max loss.\nMixture-of-logits requires the availability of multiple features to leverage the gates, because the gates will collapse to a value of 1 if there is only one feature for user and item pair. We augment the feature with learnt cluster id embedding that obviate the necessity for having multiple features. In \u00a75.1 we show that learnt cluster id embedding leveraged through Mixture-of-Logits can significantly improve on top of dot product in production settings.\nAcross LinkedIn we observed variety of member behaviour with some members coming frequently and some coming from time to time. For the infrequent members we aimed to improve retrieval system performance. To achieve this we learn cluster id embeddings, which represent interests of cohorts of members and topics of posts. We describe the process on the right of Figure 4.\nFor training LiNR, we obtain two-tower embeddings for posts and members as part of the training data, along with available engagement labels. We initialize cluster ID embeddings using K-means on millions of post embeddings. During training for both members and posts, we find the closest cluster ID embedding based on cosine similarity to their two-tower embedding. These cluster IDs for members and posts are integrated into Mixture-of-Logits, along with the original two-tower embedding and other embeddings we developed for our use cases. We experimented with using K-means-initialized cluster ID embeddings as is and fine-tuning them through back propagation. We report the experiment results in \u00a75.1."}, {"title": "SYSTEM ARCHITECTURE", "content": ""}, {"title": "Out-of-Network Recommendations", "content": "Out of Network Recommendations is one of the many sources (first pass rankers) of Linkedin Homepage Feed. When a member visits Linkedin feed, a request is triggered from the front end and sent to feed service. Feed service passes this request to many first pass rankers including feed-OON mid tier (a.k.a. interest discovery). This service is responsible for retrieving the top-K most eligible items for the member to send back to feed. Today, the underlying index used for retrieval is a lucene based index. The runtime of the query of OON is depicted at Figure 5. For every member query, a embedding based search is performed across all eligible item embeddings, followed by a layer 1 (L1) ranking model, which decides top-K items. These are then sent to feed service and ranked by more sophisticated layer 2 (L2) ranking model for members consumption. LiNR aims to provide an online service to run model based retrieval algorithms that can outperform our baselines: (1) dot-product based EBR, and (2) FAISS-IVFPQ, which is supported at Linkedin for lucene systems.\nAs shown in the figure, interest-discovery will call model-cloud-L0 to fetch candidate items for the member. Model-cloud-L0 hosts the RAR model that does (1) item attribute-based filtering (2) embedding based retrieval with ranking using model. The model consists of the item embeddings, features needed for filtering and the trained model weights. Item embeddings are generated on a nearline fashion as and when a document is created at Linkedin so as to keep the index up to date. The filters required for filtering are also ingested nearline."}, {"title": "ML Infra Architecture", "content": "We enhance Model Cloud, our hosted solution for serving model inferences, to support retrieval as ranking as shown in Figure 5."}, {"title": "Retriever", "content": "This component performs attribute-based filtering and embedding-based retrieval of the top-k documents for a query. At startup, retriever initializes with the retrieval model and bootstrapped data. Its framework-agnostic design allows easy extension to any framework, such as Torch or TensorFlow. Al engineers can experiment with new methods by developing and deploying corresponding models to this system."}, {"title": "Ingestor", "content": "Model-based retrieval requires the entire document corpus to reside in GPU memory for low latency. To provide fresh results, this corpus must be updated near real-time (nearline). Several following components work together to achieve this functionality.\nIndex Store: Attributes and embeddings come from offline sources and nearline data streams. We use Apache Beam to join and transform feature data for the entire document corpus. Offline, the full corpus is batch-pushed to a Venice Store. Nearline updates are also written to this Venice Store.\nUpdator: Updator subscribes to the Index Store's Change-Data-Capture (CDC) Stream. As the feature data gets batch pushed and live updated, the Updator gets notified to further process them and write to the model.\nBootstrapper: At startup, the Ingestor bootstraps from the Venice CDC client by replaying all data from the beginning. The entire data corpus is transformed into the required format and copied to the GPU. To minimize bootstrapping time, we regularly compact the bootstrap data and store a snapshot on disk for a fast warm start."}, {"title": "Service", "content": "To meet our performance needs, we avoid the latency and unpredictability of managed, garbage-collected languages. We also minimize network hops, data copies, and transformations. Our Model Cloud LO service is written in a native language with minimal data transformations. User queries from the LO client land directly on our service, ensuring we meet latency requirements."}, {"title": "Model Live Update", "content": "Live Update Ingestor subscribes to Venice CDC [5] from the bootstrapped offset, classifying changes into upserts and deletes, then transforming and copying them to the GPU.\nThe system's effectiveness depends on the quality of the document index, which must remain fresh. This can be done by either regularly rebuilding the index or updating it in near-real-time via a data change stream. We chose the latter for two reasons: it keeps the corpus current, reflecting changes within seconds, and it's more efficient, avoiding the cost of rebuilding and replacing the entire index. To implement this, we modified the PyTorch model to expose Upsert and Delete APIs, ensuring safe and efficient concurrent index updates during inference. Techniques used include pre-allocating larger tensors, using a high-water mark to track the working set, and making thread-safe in-memory tensor manipulations with minimal data access serialization. These methods ensure that modifications have minimal to no impact on the inference path, as detailed in the Model Inference Benchmarking section."}, {"title": "Inference on Native Stack", "content": "We built a native serving system as we made performance and efficiency our top priorities. To serve the PyTorch model in this system, we had to convert it to a compatible format. There are a few alternatives for this purpose such as TorchScipt and torch.export. PyTorch supports two execution modes: eager mode and graph mode. Optimal performance is achieved by executing everything in graph mode as the operators are first synthesized into a graph, which are compiled and executed as a whole. We picked TorchScript for our initial implementation to execute the model in graph mode. However, by doing so we traded off the performance with ease of development. TorchScript is a subset of Python and comes up with some constraints. It requires static typing and does not support things like exceptions and data-dependent control flows. We found executing this conversion quite challenging and concluded that it should be a part of the model development rather than an afterthought. We also decided to pursue other options which are deemed to be more recent technologies such as torch.export."}, {"title": "EXPERIMENTS", "content": "In this section we provide results on modeling ablation studies, online A/B experiments with OON application and infrastructure model-based retrieval inference benchmarking for production indexes."}, {"title": "Model offline evaluation", "content": "We evaluated LiNR model on our internal dataset. The dataset consists of millions of examples where a member interacted with an item. The member and item are represented by embeddings learnt from a two-tower model. The two-tower model contains variety of features including member interaction history modeled by [19] and member profile features, which usually contains member job title, job location, company, skills and professional summary of the member. Posts usually contain text, image, video or external link information. Therefore, such content features from member and posts can help us to identify topics of interests of posts or professional topics of members.\nWe used Hit Rate @ 400 over evaluation dataset to report the metrics. We use cosine similarity with exhaustive search as baseline to evaluate against LiNR. We report results of Hadamard MLP for single embedding feature and extended Mixture-of-Logits with clustering for both single and multi-embedding features in Table 1. Hadamard MLP is favored for production due to its simplicity for deployment and low latency. However, we found Hadamard MLP is very sensitive to weight initialization and general initialization methods such as GlorotNormal or HeNormal can't stabilize the performance. Empirically we observed that the initial few steps determine the overall training trend, Thus we reinitialize the model if the first 100 steps go south.\nIn addition to the two-tower model and cluster ID features, we enhanced Mixture-of-Logits by introducing multiple embedding features developed at LinkedIn. We incorporated Graph Neural Network (GNN) embeddings for members and posts mapped to the same space using a heterogeneous GNN [1]. We found that adding more embeddings improved the Hit Rate @ 400 (see Table 1).\nOur extended Mixture-of-Logits with clustering perform well for both single and multi-embedding features. One surprise finding is that fixed clusters (non-trainble) outperform trainable clusters in all cases we explored, one possible explanation is the convergence pace of the clustering and other trainable parameters are different, we'll further investigate it in our future work. Another interesting observation is that it was important to carefully tune the number of clusters: having either too high or too low a value can cause performance to degrade."}, {"title": "A/B test of LiNR", "content": "For our baseline a dot-product EBR is done across all eligible items given member embedding query. We enabled cache on a cloud based storage for online lookup of computed results. This top K is retrieved by mid tier service when an online feed request is received. We leveraged this retrieval framework to test RAR based algorithms to understand relevance impact. The baseline for these experiments are full scan dot-product."}, {"title": "Model Inference Benchmarking", "content": "We conducted offline experiments to benchmark the effectiveness of different framework implementations of two KNN variants with ABM on datasets with different pass-rate scenarios. V1 represents KNN with similarity masking, and V2 represents KNN with explicit pre-filtering. Both are implemented in TF and PyTorch with CUDA kernel for attribute matching registered as a custom operator. We selected the Job recommendation index for benchmarking due to its variety of filters, providing multi-dimensional performance insights for LiNR. The high-pass-rate and low-pass-rate datasets are derived from job search tasks, containing around 15.5 million jobs with 25 thousand queries. The high-pass-rate dataset includes two clauses: geo-location matching and company name reverse matching (mismatched items are returned), with an average of 1.7 million items passing the clauses. The low-pass-rate dataset includes an additional job title exact matching clause, with a maximum pass rate of 1.2 million for single title matching and most queries having only thousands of passed items. Each item and query has one attribute per clause, converted to 64-bit integers before GPU comparison. The embedding dimension is 128, stored as fp16 values. Performance is measured by average latency, p95 latency in milliseconds, and recall label@2000."}, {"title": "High-Pass-Rate ABM Dataset Benchmarking", "content": "From Table 3, we see that on the high-pass-rate dataset, both TF and PyTorch implementations of V1 (exhaustive search) are faster than V2 (explicit pre-filtering). This is likely due to the native slicing and copying operations in TF and PyTorch, which are especially slow for large matrices, as in V2 with high-pass-rate filters. Benchmarking individual operations revealed that the top-K selection in the latest TF version is slower than in PyTorch, while large-matrix slicing is slower in PyTorch than in TF, leading to performance differences between frameworks. V1's implementation in the high-pass-rate dataset benefits more from the PyTorch implementation with increased batch sizes. Testing the V3 quantized KNN version showed further latency improvements with a trade-off in recall. In this experiment, we used 512-bit quantized embeddings and explored filtering different percentages of items based on quantized embedding similarity before full-precision similarity calculation. The trade-off between latency and recall, correlated with the filter size hyperparameter, is shown in Figure 7. By retaining 1% of items with an additional approximate ranking stage, we achieved around 10% further latency improvement with nearly parity performance."}, {"title": "Low-Pass-Rate ABM Dataset Benchmarking", "content": "As V2 version has the superiority on the low-pass-rate dataset compared to the other two versions (V3 may introduce redundant quantize matrix computation and filtering in the low-pass-rate case), we compare the its performance implemented with TF and PyTorch in Table 5. One single query, PyTorch still shows its advantage, but TF performs better on larger batch size. We attribute this to the fact the TF has better parallel schema for our case to conduct the retrieval in parallel. Though the queries are fetched in batch, since each query has different filters leading to different sets and number of retrieved items, we split the query batch and conduct the retrieval in parallel for each query independently. Considering that V2 is an exhaustive KNN search without liquidity issue, no recall drop and results are reported here."}, {"title": "Impact of Live Model Update on Inference", "content": "We run a benchmark to measure the impact of live model update on the inference latency on a single A100 GPU using our native serving system and bench marking tool. The bench marking tool uses a client for the native serving service and issues requests serially. We use plain KNN model with ABM (V1) and repeat the runs with various concurrent update rates and request batch sizes. We observe no measurable impact on the latency with increased update rate."}, {"title": "DEPLOYMENT LESSONS", "content": "Freshness: We initially deployed LiNR with offline inference and found it missed some fresh candidates. A/B tests revealed that live updates are crucial for serving newly created LinkedIn posts. Enabling live updates resulted in a +6% gain in our production systems, highlighting their importance for improved performance.\nPre-filtering: Our system employs EBR with pre-filtering, significantly enhancing retrieval quality. Many existing EBR infrastructures use KNN search with post-filtering, where results are first retrieved by KNN distance and then filtered. This post-filtering approach reduces system recall and quality by wasting candidate slots on items that don't meet attribute constraints. By enabling pre-filtering on GPU retrieval, we greatly improved the quality of results compared to our production FAISS and lucene-based systems.\nCustom filtering kernel: One lesson we learned early is that native TF or PyTorch do not effectively support filtering operations because deep learning frameworks weren't initially designed for model-based retrieval indexes. Native boolean masking and indexing cause a 100X latency increase, making them impractical for production. Therefore, we implemented a custom CUDA solution for pre-filtering on the GPU, which scans items in memory to find those that meet constraints. One approach is to create a CUDA filtering kernel and fuse it with the matrix multiplication kernel to perform masked-matrix multiplication for KNN with pre-filtering. However, this solution is hard to generalize to other similarity measures or operations, as each new architecture would require re-implementation and fine-tuning, slowing down model development and deployment. In practice, we could make a trade-off of fully fused kernels and separately implemented kernels. For products needing regular KNN support, fusing the entire kernel with top-k selection and quantization improves serving speed. For general use cases, we create individual custom operations, like pre-filtering and quantization, to allow flexible development and deployment of advanced selection strategy with native neural network operations supported by TF and PyTorch. It is noteworthy that all the results reported in the paper are based on the second solution (even for the three plain KNN version) without extra custom kernel fusion for the purpose of self-consistency and generalizability."}, {"title": "CONCLUSION", "content": "In this paper, we introduced LiNR, a state-of-the-art model-based embedding retrieval solution for LinkedIn's production system. Deploying LiNR to our online systems resulted in significant improvements in Out-Of-Network post recommendations on the LinkedIn Feed. We believe we are among the first in the industry to support live-updated, differentiable model-based indexing for recommendation and search applications. Looking forward, LiNR paves the way for unifying retrieval and ranking into a single GPU model, simplifying complex infrastructure and allowing end-to-end optimization of the entire differentiable system with gradient descent."}]}