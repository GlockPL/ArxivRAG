{"title": "Large Language Models Are Self-Taught Reasoners: Enhancing LLM Applications via Tailored Problem-Solving Demonstrations", "authors": ["Kai Tzu-iunn Ong", "Taeyoon Kwon", "Jinyoung Yeo"], "abstract": "Guiding large language models with a selected set of human-authored demonstrations is a common practice for improving LLM applications. However, human effort can be costly, especially in specialized domains (e.g., clinical diagnosis), and does not guarantee optimal performance due to the potential discrepancy of target skills between selected demonstrations and real test instances. Motivated by these, this paper explores the automatic creation of customized demonstrations, whose target skills align with the given target instance. We present SELF-TAUGHT, a problem-solving framework, which facilitates demonstrations that are \"tailored\" to the target problem and \"filtered\" for better quality (i.e., correctness) in a zero-shot manner. In 15 tasks of multiple-choice questions of diverse domains and the diagnosis of Alzheimer's disease (AD) with real-world patients, SELF-TAUGHT achieves superior performance to strong baselines (e.g., Few-shot CoT, Plan-and-Solve, Auto-CoT). We conduct comprehensive analyses on SELF-TAUGHT, including its generalizability to existing prompting methods and different LLMs, the quality of its intermediate generation, and more.", "sections": [{"title": "Introduction", "content": "Recently, large language models (LLMs) have emerged as an alternative knowledge source to human experts, popularizing the paradigm of prompting them to solve problems. In this context, methods such as chain-of-thought (CoT) prompting (Wei et al. 2022), which promotes LLMs to follow the step-by-step fashion of human problem-solving, have brought promising performances to LLM applications in diverse specialized domains (Singhal et al. 2023; Zaki, Krishnan et al. 2024; Liu et al. 2024), such as predicting crystal structures, clinical diagnosis, etc.\nDespite the success, most prompt-driven projects rely on human effort. That is, they require domain experts to select representative problems for the task, annotate their solutions (i.e., rationales & answers), and use them as demonstrations to guide LLMs in solving test instances (i.e., few-shot prompting). Such manual effort can make real-world applications costly and, more importantly, has no guarantee of optimal performances due to the one-size-fits-all selection of problem-solving demonstrations (Min et al. 2022), i.e., fixed and potentially unrelated (to the test instance) demonstrations used throughout the inference of the whole test set. A common remedy to such reliance on crafted demonstrations is zero-shot prompting, i.e., prompting LLMs without demonstrations. Upon this, studies have proposed to enhance LLMs' zero-shot reasoning (Kojima et al. 2022; Chae et al. 2024; Kong et al. 2024). For instance, Wang et al. (2023) present Plan-and-Solve (PS) prompting, where the LLM first devises a plan for the given problem and solves it according to the plan. However, the lack of problem-solution demonstrations still sometimes poses performance gaps between zero-shot approaches and their few-shot counterparts (Kojima et al. 2022). While there is a line of studies proposing to resolve this with automatic demonstration generation, they often require in-domain corpora (i.e., training/test sets) and do not explicitly address the alignment of knowledge/skills between demonstrations and test instances (Zhang et al. 2022; Wan et al. 2023; Li et al. 2024a).\nThis paper tackles the above bottlenecks in prompt-driven applications of LLMs for specialized domains. Specifically, we focus on invoking the LLM to self-create high-quality and tailored demonstrations for each test instance under a zero-shot setting, and using them to guide its own predictions. To this end, we present SELF-TAUGHT, a zero-shot framework of self-directed problem-solving.\nOur contributions are three-fold: (1) We present a simple and fully zero-shot framework, SELF-TAUGHT. Inspired by self-directed learning (SDL) in educational theories,2 SELF-TAUGHT starts by identifying information addressed in the target problem abstractively (Phase I). After that, it goes through a tailored creation phase (Phase II), where the LLM creates problems addressing similar information/knowledge to the target as well as their solutions with high certainty. Lastly, the self-created problems/solutions are used as tailored demonstrations for solving the target problem (Phase III); (2) In 13 QA tasks of specialized domains and 2 clinical datasets collected from real-world patients of Alzheimer's disease (AD), SELF-TAUGHT shows superior performances to strong baselines, including those powered by domain experts and in-domain demonstration pools; (3) In our analyses,"}, {"title": "Formulations", "content": "Many LLM-based projects (Singhal et al. 2023; Liu et al. 2024) use human-authored demonstrations $D_{human}$ to guide LLMs in generating the solution $S$ to a given problem $P$:\n$S \\sim P_\\theta(\\cdot|P, D_{human})$\nHere, $D_{human}$ often contains pairs of: (1) a selected representative problem of the task; (2) a solution including intermediate reasoning steps (i.e., rationale) and a final answer.\nHowever, the need for crafted demonstrations can make applications in specialized domains costly. More importantly, it is not feasible to customize demonstrations for each test instance, potentially yielding sub-optimal performance due to the discrepancy between them (Min et al. 2022) \u2013 problems used as demonstrations and the target may require completely different knowledge to solve, even if they are from the same domain. For instance, when solving physics problems, a demonstration of thermodynamics may not be beneficial to solving a problem of electronics.\nThus, we propose to create high-quality demonstrations $D_{self}$ via the knowledge of the LLM itself, which are tailored to each test instance. Formally, given $P$, the LLM first create $D_{self}$ tailored for $P$, and use it to guide the prediction of $S$:\n$D_{self} \\sim P_\\theta(I|P)$\n$S \\sim P_\\theta(\\cdot|P, D_{self})$"}, {"title": "Proposed Framework: SELF-TAUGHT", "content": "As shown in Figure 1, given a target problem $P$, our framework carries out the following phases in a zero-shot manner:\nPhase I: Information Identification\nTo facilitate tailored demonstrations for the target problem $P$, it is important for us to first know what $P$ is targeting. Thus, SELF-TAUGHT starts with an information identification, where we capture what kind of knowledge/skill is addressed by $P$. Formally, given $P$, the LLM lists the necessary information $I$ that one must know for solving $P$:\n$I = \\underset{I}{argmax} P_{LLM}(I|P)$\nNote that rather than print out the information specifically in the form of factual statements (e.g., \u201cAccording to the 2nd law of thermodynamics, idealized reversible processes produce no entropy and no process is...\"), the LLM lists the required information in an abstractive manner (e.g., \u201cUnderstanding the 2nd law of thermodynamics\u201d), as shown in Figure 1 bottom left. This approach is designed to mitigate the potential influences of hallucination (Lyu et al. 2022). We compare this method with a specific identification in Table 3.\nPhase II: Tailored Demonstration Creation\nNow, the LLM leverages the identified information $I$ to prepare tailored problem-solution demonstrations for $P$:\nII-1. Generating Pseudo Problems with High Relevancy. We first create pseudo problems that target the same knowledge/skills as $P$ based on the identified information $I$. Formally, given $P$ and $I$, the LLM generates a pseudo problem $p$ targeting information listed in $I$:\n$p = \\underset{p}{argmax} P_{LLM}(p|P, I)$\nII-2. Generating Pseudo Solutions with High Certainty. Intuitively, after generating $p$, we can obtain its solution $s$ via any zero-shot prompting approach (e.g., CoT or PS). However, since LLMs may produce non-factual statements, it is necessary to address the correctness of pseudo-solutions.\nFiltering low-quality outputs with the LLM itself in a zero-shot manner is challenging. Inspired by how LLMs can verbalize their confidence in their predictions, we apply a Certainty Filtering. Formally, given a pseudo problem $p$, the LLM first create a pseudo solution $s = (r, \\bar{a})$ consisting of a rationale $r$ and an answer $\\bar{a}$. Next, it outputs a certainty score $cs$ within 0-100 before ending the generation. The form of $s$ depends on the zero-shot prompting method of one's choice, e.g., Direct Prediction, COT, PS, etc:\n$s = \\underset{S}{argmax} P_{LLM}(s|p)$\n$\\Rightarrow cs = \\underset{CS}{argmax} P_{LLM}(cs|p, s)$\nwhere $\\Rightarrow$ indicates the sequential generation of tokens. To collect highly-confident $s$, we iterate this process (for at most $t$ times) until we get a $s$ top that yields a $cs \\geq \\lambda$.\nIn practice, we collect plural pairs (i.e., $N$ pairs) of $pp$ and $s$ to build a set of self-created tailored demonstrations $D_{self}$:\n$D_{self} = \\{(p_n, s_n)\\}_{n=1}^N$\nPhase III: Self-Directed Problem-Solving with Tailored Demonstrations\nWhile many works emphasize diverse and representative demonstrations of the test set (Zhang et al. 2022; Singhal et al. 2023; Li et al. 2024a), it may lead to unrelated demonstrations that provide sub-optimal or misleading guidance, eventually affecting the solving process of target problems (Lightman et al. 2024), especially in scenarios where each test instance requires different knowledge to solve.\nWe address this by guiding LLMs' problem-solving with its self-created demonstrations, which are tailored to the target. Formally, given $P$, $D_{self} = \\{p_n, s_n\\}_{n=1}$ is added to the LLM's input to guide the prediction of the solution $S$ to $P$:\n$S = \\underset{S}{argmax} P_{LLM}(S|P, K_{self})$\""}, {"title": "Datasets", "content": "To investigate the effectiveness of ours, we adopt several tasks from specialized domains, starting with multiple-choice questions of different emphases:\nQuestion Answering of Diverse Domains\nStrategyQA. This dataset contains questions that target multi-hop reasoning over a wide range of knowledge (Geva et al. 2021). For example, a question may require one to know facts about a celebrity and the properties of hydrogen.\nScienceQA. Proposed by Lu et al. (2022), questions are collected from elementary ~ high school curricula, targeting 26 topics including math, language, geography, etc. We exclude problems addressing the vision modality, i.e., images.\nMedQA. A popular benchmark datasets in the medical domain curated by Jin et al. (2021). The questions are collected from national medical licensing exams in several countries. We only adopt questions written in English.\nCollege-level problems of six domains. We include college-level problems of computer science (CS), medicine (Med), chemistry (Chem), math, physics (Phys), and biology (Bio) domains. The problems are collected from college textbooks and exams by Hendrycks et al. (2020).\nProfessional-level problems of four domains. We include four datasets addressing professional-level knowledge of accounting (Acct), medicine (Med), psychology (Psych), and legal (Law) domains. The problems are mainly obtained from diverse licensing/bar exams of the corresponding profession by Hendrycks et al. (2020).\nClinical Diagnosis with Real-World Patients\nBesides problems from academic scenarios, we further evaluate SELF-TAUGHT on a long-standing real-world challenge: the diagnosis of Alzheimer's disease (AD). For that, we incorporate two datasets of actual patients, collected by ADNI (Jack Jr et al. 2008) and AIBL (Ellis et al. 2009).\nAD diagnosis requires the LLM to reason over the electronic health records (EHRs) of patients and make the diagnosis accordingly, i.e., either AD, MCI (mild cognitive impairment), or Normal. The EHRs are structured following the practice of/obtained from Kwon et al. (2024), which list the findings from MRI scans (e.g., volume measurements of each brain region) and patient information such as the results of mental state exams, the presence of APOE4 allele, etc. Details and examples of all datasets are in Appendices."}, {"title": "Experimental Settings", "content": "Zero-shot Baselines\nZero-shot prompting has been widely utilized to mitigate human effort in LLM applications. Since SELF-TAUGHT also access to nothing but the target $P$ and the LLM's own knowledge, we consider these fair comparisons:\nDirect prediction (Direct). The LLM directly predicts the solution for $P$ without any intermediate process.\nChain-of-Thought prompting (CoT). Given a target problem $P$, this setting promotes the LLM to generate the intermediate reasoning steps towards the solution using the phrase \"Let's think step-by-step\u201d (Kojima et al. 2022).\nPlan-and-Solve prompting (PS). Wang et al. (2023) present PS prompting, which has shown promising performance in solving math problems. In PS, the LLM first devises a plan based on the problem instance and then solve it step-by-step according to the plan.\nReasoning-in-Conversation (RiC). Inspired by its performance in linguistic tasks such as humor detection (Wang et al. 2024), we modified it for our experiments. This setting prompts an LLM to first simulate a discussion between experts and then conclude the answer from the conversation.\nRole-Play prompting. Proposed by Kong et al. (2024), it outperforms zero-shot CoT in commonsense and math reasoning tasks. Here, the LLM and the user kick off the session with a short conversation that helps the LLM get into the role of an expert (e.g., a math teacher). Then, the problem-solving will be performed in a \"teaching the user\u201d manner.\nFew-shot Baselines (Oracles)\nWe further compare SELF-TAUGHT with few-shot prompting methods. Since these methods have access to demonstrations made with real problems from the dataset, we consider them oracles following Lyu et al. (2022):\nManual Chain-of-Thought (Manual CoT). A popular setting of few-shot CoT prompting in LLM applications, where the problem-solving is guided by demonstrations written by human domain experts (curated by prior work or our domain experts). Details are provided in Appendices.\nRetrieval CoT. Following Zhang et al. (2022), when given $P$, we retrieve top-N similar problems from the training set via text similarity. Then, we use the LLM to annotate CoT rationales/solutions for the retrieved problems, using them as demonstrations for solving $P$. Similar to SELF-TAUGHT, this setting also pursues demonstrations that address similar or identical knowledge/information to $P$.\nAutomatic CoT (Auto-CoT). It is widely applied for scenarios where demonstrations are unavailable (Zhang et al. 2022). It is similar to Retrieval-CoT, but we instead sample N most \"diverse\" problems from the training set via k-clustering as demonstrations that represent the whole task.\nModels and Implementation Details\nLarge language models. We use gpt-3.5-turbo-0125 and llama-3.1-8B (OpenAI 2023; Meta 2024), w/ temperature of 0.7. We report GPT's results in Table 1-3, Llama's summarized results in RQ6, and full results in Appendices.\nEncoder for Auto-/Retrieval CoT. Following Zhang et al. (2022), we use Sentence-BERT (Reimers and Gurevych 2019) to encode problems for clustering and retrieval.\nDemonstrations in few-shot baselines. If a dataset does not provide a training set, the demonstrations will be based on instances from the test set. They will be ignored during performance measurements. Also, we set N to 3 for QA. For AD diagnosis, we set it to 2 to match the radiologist demonstrations provided by Kwon et al. (2024).\nEvaluation Metrics. We report accuracy (%). For AD diagnosis, we further include precision, recall, and F1 score for a more comprehensive comparison.\nPreventing randomness. We report the median performance of three runs for all experiments in this work."}, {"title": "Results and Discussions", "content": "We present the results of the following Research Questions:\nRQ1: Can SELF-TAUGHT's tailored demonstrations enhance the LLM's reasoning in QA of diverse domains?\nRQ2: Is SELF-TAUGHT also beneficial in the clinical diagnosis with real-world patients of Alzheimer's disease?\nRQ3: How phases in SELF-TAUGHT affect performances?\nRQ4: Can SELF-TAUGHT also be applied to other zero-shot prompting methods besides CoT, and vice versa?\nRQ5: How cost-efficient is SELF-TAUGHT?\nRQ6: Can SELF-TAUGHT generalize to other LLMs?\nTailored demonstrations allow SELF-TAUGHT to outperform baselines in diverse QA tasks (RQ1). Table 1 shows model performances in 13 QA tasks. SELF-TAUGHT ranks first in 10 tasks, second in the rest of 3, and achieves better average acc than zero-shot baselines (top half).\nCompared with oracles, ours outperforms Manual and Auto-CoT in 10 and 11 tasks (out of 13). This suggests that using a fixed set of demonstrations (whether human-written or machine-generated) throughout the inference of all test data may yield sub-optimal performance, justifying our goal of tailored demonstrations. Retrieval CoT performs almost as well as ours when compared head-to-head (6 vs. 7 wins). We assume that it is because it also leverages problems relevant to $P$ as demonstrations. Still, ours yields a much higher Avg acc, indicating that our generative method can elicit better related demonstrations than similarity-based retrieval.\nIn Law, few-shot Direct outperforms all settings that involve intermediate reasoning (i.e., rationale). This may be because while problems are collected from the US, there is no clear regulation specifying which state's law should be referenced in each problem. This can cause misquotation of law and make the correct annotation/generation of rationales challenging, negatively affecting problem-solving. Interestingly, in Physics (Phys), SELF-TAUGHT and Manual CoT have much higher acc than others. This may suggest that ensuring the quality of demonstrations is relatively more important in physics domains than in other domains.\nOurs is less effective than Manual CoT in AD diagnosis due to the high similarity between instances (RQ2). In AD diagnosis (Table 2), SELF-TAUGHT beats all baselines in all metrics, except for Manual CoT. This pattern is much different from the findings in RQ1. We conjecture that it is because the discrepancy between each instance is extremely small here: all instances require LLMs to perform the same task w/ the same 3 output classes, via EHRs that are structured in the identical key-value format. This can marginalize the effect of tailored demonstrations and amplify the benefit of fixed human-crafted demonstrations.\nRegardless, ours presents a much smaller performance gap between it and Manual-CoT (6.18 percentage points of acc; avg of all classes/datasets) than other baselines (8.25 ~ 12.54 percentage points). In real clinical settings, each patient's EHR may be constructed diversely due to each radiologist's preference and other situational factors, thus requiring different reasoning to diagnose (Norman 2005). With insights from RQ2, we presume one can adjust SELF-TAUGHT for real-world applications by combining it with slight manual effort. For instance, including a minimal demonstration in the generation of pseudo problems/EHRs (Phase II), i.e., demonstration expansion, to tackle different EHR styles and diagnostic processes. We leave this to future work.\nDesigned phases contribute to performance improvement (RQ3). To investigate how our phasic design affects model performance, we evaluate ablated and modified SELF-TAUGHT in all 15 tasks in Table 3. First, ablations of information identification and certainty filtering both lead to worse performance, while the former has a larger impact. This shows: (1) having a phase for identifying what $P$ is targeting is crucial for creating tailored demonstrations, which is beneficial for CoT prompting; (2) when tailored pseudo problems are available, quality-controlling their solutions can further boost system performance.\nWe also report a version where the information identification (Phase I) is done by printing out the specific factual statements that are required to solve $P$. We find this performing worse than the SELF-TAUGHT and the 2nd ablation (both are equipped with an abstractive identification). This justifies our design of Phase I.\nSELF-TAUGHT enhances existing prompting methods (RQ4). So far, all solution generation of pseudo and target problems (i.e., Phase II-2 and III) has been driven by CoT prompting. As a formal study on improving LLM applications, it is necessary to validate SELF-TAUGHT's efficacy when implemented with different prompting methods. Thus, we repeat the above experiments with SELF-TAUGHT's variants, where the generation of solutions (Phase II-2 and III) is based on zero-shot direct and PS prompting.\nFigure 4 reports the improvement of average acc in all 15 tasks. Firstly, when incorporating SELF-TAUGHT to existing methods, we observe performance gains in all of them. The most significant improvement is in 0-shot Direct, where system performance increases by 2.35 percentage points of acc. Zero-shot PS benefits not as much from SELF-TAUGHT, we hypothesize that it is because the \"plan\" devised in PS has already worked as a self-created guidance for problem-solving, marginalizing the help of self-generated demonstrations from SELF-TAUGHT. We also present the improvement brought by SELF-TAUGHT w/o certainty filtering (CF). Performance gains are still achieved. This provides us with a relatively more cost-efficient implementation (than original SELF-TAUGHT) for incorporating existing methods.\nThere exists a cost-performance trade-off (RQ5). A concern of SELF-TAUGHT is its higher API cost. Regardless, we argue that ours is competitive when taking both performance and cost into account. Figure 2 plots accuracy against gpt-3.5-turbo-0125's API cost per instance (calculated based on input and output tokens in six college-level tasks). We find SELF-TAUGHT and its ablation (w/ certainty filtering) lying on the Pareto frontier, indicating an efficient cost-performance trade-off. This suggests SELF-TAUGHT'S value when performance is prioritized over the API cost.\nSELF-TAUGHT generalizes to a smaller open-source LLM (RQ6). To address RQ5, we test if ours' can generalize to an LLM that is both smaller and open-source. We report the performance of Llama-3.1-8B regarding the above Pareto-efficient methods in Figure 3. SELF-TAUGHT generally yield the best performance among the Pareto-efficient methods, suggesting that it can be a strong candidate method in settings with limited computational power and budget."}, {"title": "Comparing SELF-TAUGHT with Zero-shot CoT", "content": "Here, we investigate the improvement brought by SELF-TAUGHT (using 0-shot CoT in Phase II and III) over vanilla CoT in Figure 6 by comparing their prediction correctness. Among problems that 0-shot CoT is wrong, SELF-TAUGHT solve 15.4% of them correctly, even though they are both based on 0-shot CoT prompting. This exhibits the benefit of tailored demonstrations in LLM problem-solving."}, {"title": "Number of Pseudo Shots", "content": "We analyze the effect of varying the number of self-created pseudo shots using the six college-level tasks (Figure 7). First, SELF-TAUGHT outperforms Manual CoT and Retrieval CoT (both have N = 3) with fewer shots (N = 2). This suggests the effectiveness of tailored demonstrations generated with our framework, as well as the possibility of tuning N to further decrease our computational cost. Also, we observe that when N \u2265 3, model performance remains almost consistent. This pattern matches the findings from regular prompting with real demonstrations (Brown et al. 2020)."}, {"title": "SELF-TAUGHT and Task Difficulty", "content": "Lastly, we investigate the relation between SELF-TAUGHT and task difficulty (approximated by the most un-engineered setting, i.e., 0-shot Direct). Figure 8 shows a trend that as the performance of 0-shot direct decreases, the improvement increases, indicating that the LLM benefits more from SELF-TAUGHT in tasks that are initially harder for it w/o additional techniques. This provides a guideline for us to judge the priority when applying ours to LLM applications. Also, when plotting CoT against Direct, the regression coefficient $B = -0.14$, showing that SELF-TAUGHT generally brings more improvement ($\\beta = -0.21$) than CoT as the task gets difficult."}, {"title": "Related Work", "content": "General-purpose LLMs have been widely applied to diverse domains thanks to their ability to learn from user input: Li et al. (2024b) prompt LLMs to predict drug synergy; Chiang, Chou, and Riebesell (2024) build a prompt-driven system with material science APIs for crystal generation; Kwon et al. (2024) use LLMs to annotate raw patient data. However, they often rely on human demonstrations, which can be costly and yield sub-optimal performance when encountering the above-discussed demonstration-target discrepancy.\nBesides the mentioned works (e.g., PS, Auto-CoT, etc.), several studies have proposed to mitigate human efforts in prompt construction: Zhou et al. (2024) and Chae et al. (2024) invoke LLMs to generate task-level plans shared across all instances to guide instance-level inference. Lyu et al. (2022) use sentences from external related corpora along with random labels as pseudo demonstrations for text classification. Wan et al. (2023) use LLMs to create a large demonstration pool by repeatedly running the test set with 0-shot CoT and using majority-vote-based criteria to select good demonstrations for problem-solving. Similarly, Li et al. (2024a) use LLMs to generate a pseudo dataset of 5K questions with 29 designed topics from scratch and use it as the demonstration pool. Recently, Yang et al. (2024) use LLMs as optimizers, where the LLM iteratively updates the problem-solving instruction (e.g., \"Break this down\") until it yields a maximum accuracy on the training set. To address the lack of annotated rationales for CoT fine-tuning, Hwang et al. (2024) first run the training set with LLMs to collect correct/wrong CoT rationales. Then, the \u201cfirst wrong step\u201d in wrong rationales is identified with designed algorithms and used as fine-grained rewards for preference learning.\nSimilar to ours, Kim et al. (2022) and Chen et al. (2023) generate pseudo demonstrations by prompting LMs with phrases like \u201cgenerate a negative review\u201d or \u201cCome up with diverse creative instances for the task\". However, the former requires the same output span (e.g., positive & negative) across all test instances. The latter focuses on facilitating diverse representative problems following Auto-CoT (Zhang et al. 2022). It neglects the discrepancy (addressed information) between test and pseudo problems as well as the correctness of pseudo solutions (we show the effect of such neglection in Table 3). Inspired by them, SELF-TAUGHT resolve human effort by creating demonstrations that are both \"quality-controlled\u201d and \u201ctailored\" to each target instance, which is under-explored so far to the best of our knowledge.\""}, {"title": "Conclusions", "content": "We present SELF-TAUGHT, a problem-solving framework for specialized domains. It addresses the costly human effort and the demonstration-target discrepancy in LLM applications, by creating demonstrations that are quality-controlled and tailored to each test instance. It outperforms baselines in 15 tasks of QA and AD diagnosis. It is Pareto efficient regarding cost and performance and generalizable to different prompting methods and LLMs. The quality of self-created problems/solutions is confirmed in expert evaluations. We discuss the limitations of our work in Appendices."}, {"title": "Appendices", "content": "This work has limitations. First of all, SELF-TAUGHT can be less cost-efficient. Although we have shown that SELF-TAUGHT's cost-performance trade-off is acceptable (i.e., Pareto efficient), addressing the system cost more explicitly can be necessary for real-world deployment. A plausible direction is combining Retrieval CoT, SELF-TAUGHT, and an additional logic (e.g., a threshold of text similarity) that determines whether problems addressing similar knowledge to the target problem are available in the training/test corpora (for us to retrieve) and selectively generate tailored demonstrations only when such problems are absent or not enough for the desired number of shots.\nAnother concern is that although SELF-TAUGHT generally brings more performance gains to the adopted LLM in tasks that are initially more challenging for it (Figure 8), the final performance may still be far from optimal due to the lack of related knowledge in its parameter. One may address this by (1) adopting LLMs that are fine-tuned with corpora of the corresponding domains, e.g., running SELF-TAUGHT with BioMistral (Labrak et al. 2024) when solving medical problems, or (2) retrieving relevant information from external knowledge bases and use them to augment the generation of tailored demonstrations. We leave these to future work."}, {"title": "Supplementary Results", "content": "Combining SELF-TAUGHT with Other Zero-shot Prompting Methods\nWe hereby present the full result of Table 4:\n\u2022 Table 9: The results for SELF-TAUGHT that is combined with zero-shot Direct.\n\u2022 Table 10: The results for SELF-TAUGHT that is combined with zero-shot Plan-and-Solve (PS).\nCombining SELF-TAUGHT with a Smaller Open-source LLM\nWe show the detailed results of Figure 3 at:\n\u2022 Table 11: SELF-TAUGHT's performances when run with Llama-3.1-8B in question-answering.\n\u2022 Table 12: SELF-TAUGHT's performances when run with Llama-3.1-8B in the diagnosis of AD."}, {"title": "Further Details on the Datasets", "content": "Question Answering of Diverse Domains\nStrategyQA. This dataset contains questions that target multi-hop reasoning over a wide range of knowledge (Geva et al. 2021). The main feature of this dataset is that the necessary knowledge required for solving the question is not explicitly stated in the question text (e.g., \u201cYes or No: Did Aristotle Use a Laptop?\"). We adopted the test set (of 2,290 data) provided in BIG-bench.\nScienceQA. This dataset is proposed by Lu et al. (2022), questions are collected from elementary ~ high school curricula, targeting 26 topics including math, language, geography, etc. We adopt the test set and exclude problems addressing the vision modality, i.e., images (the final test set has a size of 2,224.) An example question is shown below:\nMedQA. This is a popular benchmark datasets in the medical domain curated by Jin et al. (2021). Since the questions are collected from national medical licensing exams in several countries (i.e., multi-lingual), we only adopt questions written in English. We use the test set with a size of 1,273. An example is shown below:\nCollege-level QA of 6 domains. We include college-level problems of computer science (CS), medicine (Med), chemistry (Chem), math, physics (Phys), and biology (Bio) domains. The problems are collected from college textbooks and exams by Hendrycks et al. (2020) as a part of the MMLU (Massive Multitask Language Understanding) dataset. The statistic is provided in Table 5 and an example is shown below:\nProfessional-level QA of 4 domains. We include four datasets addressing professional-level knowledge of accounting (Acct), medicine (Med), psychology (Psych), and legal (Law) domains. The problems are mainly obtained\""}, {"title": "AD Diagnosis with Real Patients", "content": "ADNI. The Alzheimer's Disease Neuroimaging Initiative (ADNI) (Jack Jr et al. 2008) is a long-term research project with the goal of addressing the diagnosis of AD. Data from ADNI has been widely used in prior works and significantly influenced the development of deep learning-based AD diagnosis (Ebrahimi, Luo, and Chiong 2020; Zhang et al. 2018; Jang and Hwang 2022; Ong et al. 2023).\nAIBL. The Australian Imaging, Biomarker and Lifestyle Flagship Study of Ageing (AIBL) (Ellis et al. 2009) is a project to investigate which biomarkers, cognitive characteristics, and health/lifestyle factors determine the subsequent progression of symptomatic AD. Data from AIBL is also one of the most widely used data for deep learning-based AD diagnosis (Qiu et al. 2020; Zhu et al. 2021; Jang and Hwang 2022).\nFeatures of both AD datasets. Each patient data from ADNI and AIBL has the following elements: (1) MRI scans of patients; (2) demographic information; (3) education level; (4) results from the mini-mental state examination; (5) the presence of APOE4 allele; (6) The ground-truth label of diagnosis.\nTextualized MRI data. Since this work focuses on mono-modal problem-solving, i.e., without considering the imaging modality, we incorporate the textualized ADNI and AIBL (only test sets) curated by Kwon et al. (2024), where the MRI scans are transformed into textual descriptions in the form of EHR via an automatic process based on the structural features of brain regions.10 We provide an example of such textualized data based on a patient from ADNI in Table 7. The data derived from AIBL has the exact same format."}, {"title": "Demonstrations for Manual CoT and Few-shot Direct", "content": "We provide the prompts for these baselines on our GitHub page.11 Here, we discuss how we acquire the few-shot demonstrations for them:"}, {"title": "Our Expert Annotation for MMLU Tasks.", "content": "For college-level QA of 4 domains and profession-level QA of 6 domains from MMLU, we manually annotated the few-shot examples (i.e., CoT rationales) for Manual CoT with a group of experts of corresponding domains. The names of the experts are listed in Acknowledgements. Note that because some of our experts do not speak English as their first language, generative AI may be applied during the annotation process purely for text translation/correction purposes."}, {"title": "Demonstrations Annotated by Prior Work", "content": "For StrategyQA, we adopt the human-annotated few-shot demonstration from Wei et al. (2022); For"}]}