{"title": "Large Language Models Are Self-Taught Reasoners: Enhancing LLM\nApplications via Tailored Problem-Solving Demonstrations", "authors": ["Kai Tzu-iunn Ong", "Taeyoon Kwon", "Jinyoung Yeo"], "abstract": "Guiding large language models with a selected set of human-\nauthored demonstrations is a common practice for improving\nLLM applications. However, human effort can be costly, es-\npecially in specialized domains (e.g., clinical diagnosis), and\ndoes not guarantee optimal performance due to the potential\ndiscrepancy of target skills between selected demonstrations\nand real test instances. Motivated by these, this paper explores\nthe automatic creation of customized demonstrations, whose\ntarget skills align with the given target instance. We present\nSELF-TAUGHT, a problem-solving framework, which facili-\ntates demonstrations that are \"tailored\" to the target problem\nand \"filtered\" for better quality (i.e., correctness) in a zero-\nshot manner. In 15 tasks of multiple-choice questions of di-\nverse domains and the diagnosis of Alzheimer's disease (AD)\nwith real-world patients, SELF-TAUGHT achieves superior\nperformance to strong baselines (e.g., Few-shot CoT, Plan-\nand-Solve, Auto-CoT). We conduct comprehensive analyses\non SELF-TAUGHT, including its generalizability to existing\nprompting methods and different LLMs, the quality of its in-\ntermediate generation, and more.", "sections": [{"title": "Introduction", "content": "Recently, large language models (LLMs) have emerged as\nan alternative knowledge source to human experts, popular-\nizing the paradigm of prompting them to solve problems.\nIn this context, methods such as chain-of-thought (CoT)\nprompting (Wei et al. 2022), which promotes LLMs to fol-\nlow the step-by-step fashion of human problem-solving,\nhave brought promising performances to LLM applications\nin diverse specialized domains (Singhal et al. 2023; Zaki,\nKrishnan et al. 2024; Liu et al. 2024), such as predicting\ncrystal structures, clinical diagnosis, etc.\nDespite the success, most prompt-driven projects rely on\nhuman effort. That is, they require domain experts to select\nrepresentative problems for the task, annotate their solutions\n(i.e., rationales & answers), and use them as demonstra-\ntions to guide LLMs in solving test instances (i.e., few-shot\nprompting). Such manual effort can make real-world appli-\ncations costly and, more importantly, has no guarantee of\noptimal performances due to the one-size-fits-all selection\nof problem-solving demonstrations (Min et al. 2022), i.e.,\nfixed and potentially unrelated (to the test instance) demon-\nstrations used throughout the inference of the whole test set.\nA common remedy to such reliance on crafted demon-\nstrations is zero-shot prompting, i.e., prompting LLMs with-\nout demonstrations. Upon this, studies have proposed to en-\nhance LLMs' zero-shot reasoning (Kojima et al. 2022; Chae\net al. 2024; Kong et al. 2024). For instance, Wang et al.\n(2023) present Plan-and-Solve (PS) prompting, where the\nLLM first devises a plan for the given problem and solves\nit according to the plan. However, the lack of problem-\nsolution demonstrations still sometimes poses performance\ngaps between zero-shot approaches and their few-shot coun-\nterparts (Kojima et al. 2022). While there is a line of stud-\nies proposing to resolve this with automatic demonstration\ngeneration, they often require in-domain corpora (i.e., train-\ning/test sets) and do not explicitly address the alignment\nof knowledge/skills between demonstrations and test in-\nstances (Zhang et al. 2022; Wan et al. 2023; Li et al. 2024a).\nThis paper tackles the above bottlenecks in prompt-driven\napplications of LLMs for specialized domains. Specifically,\nwe focus on invoking the LLM to self-create high-quality\nand tailored demonstrations for each test instance under a\nzero-shot setting, and using them to guide its own predic-\ntions. To this end, we present SELF-TAUGHT, a zero-shot\nframework of self-directed problem-solving.\nOur contributions are three-fold: (1) We present a simple\nand fully zero-shot framework, SELF-TAUGHT. Inspired by\nself-directed learning (SDL) in educational theories, SELF-\nTAUGHT starts by identifying information addressed in the\ntarget problem abstractively (Phase I). After that, it goes\nthrough a tailored creation phase (Phase II), where the LLM\ncreates problems addressing similar information/knowledge\nto the target as well as their solutions with high certainty.\nLastly, the self-created problems/solutions are used as tai-\nlored demonstrations for solving the target problem (Phase\nIII); (2) In 13 QA tasks of specialized domains and 2 clinical\ndatasets collected from real-world patients of Alzheimer's\ndisease (AD), SELF-TAUGHT shows superior performances\nto strong baselines, including those powered by domain ex-\nperts and in-domain demonstration pools; (3) In our analy-"}, {"title": "Formulations", "content": "Many LLM-based projects (Singhal et al. 2023; Liu et al.\n2024) use human-authored demonstrations $D_{\\text{human}}$ to guide\nLLMs in generating the solution $S$ to a given problem $P$:\n$S \\sim P_{\\theta}(\\cdot|P, D_{\\text{human}})$\n(1)\nHere, $D_{\\text{human}}$ often contains pairs of: (1) a selected represen-\ntative problem of the task; (2) a solution including interme-\ndiate reasoning steps (i.e., rationale) and a final answer.\nHowever, the need for crafted demonstrations can make\napplications in specialized domains costly. More impor-\ntantly, it is not feasible to customize demonstrations for each\ntest instance, potentially yielding sub-optimal performance\ndue to the discrepancy between them (Min et al. 2022) \u2013\nproblems used as demonstrations and the target may require\ncompletely different knowledge to solve, even if they are\nfrom the same domain. For instance, when solving physics\nproblems, a demonstration of thermodynamics may not be\nbeneficial to solving a problem of electronics.\nThus, we propose to create high-quality demonstrations\n$D_{\\text{self}}$ via the knowledge of the LLM itself, which are tailored\nto each test instance. Formally, given $P$, the LLM first create\n$D_{\\text{self}}$ tailored for $P$, and use it to guide the prediction of $S$:\n$D_{\\text{self}} \\sim P_{\\theta}(I|P)$\n(2)\n$S \\sim P_{\\theta}(P, D_{\\text{self}})$\n(3)"}, {"title": "Proposed Framework: SELF-TAUGHT", "content": "As shown in Figure 1, given a target problem $P$, our frame-\nwork carries out the following phases in a zero-shot manner:\nPhase I: Information Identification\nTo facilitate tailored demonstrations for the target problem\n$P$, it is important for us to first know what $P$ is targeting.\nThus, SELF-TAUGHT starts with an information identifica-\ntion, where we capture what kind of knowledge/skill is ad-\ndressed by $P$. Formally, given $P$, the LLM lists the neces-\nsary information $I$ that one must know for solving $P$:\n$I = \\underset{I}{\\operatorname{argmax}} P_{\\text{LLM}}(I|P)$\n(4)\nNote that rather than print out the information specifically in\nthe form of factual statements (e.g., \u201cAccording to the 2nd\nlaw of thermodynamics, idealized reversible processes pro-\nduce no entropy and no process is...\"), the LLM lists the\nrequired information in an abstractive manner (e.g., \u201cUnder-\nstanding the 2nd law of thermodynamics\u201d), as shown in Fig-\nure 1 bottom left. This approach is designed to mitigate the\npotential influences of hallucination (Lyu et al. 2022). We\ncompare this method with a specific identification in Table 3.\nPhase II: Tailored Demonstration Creation\nNow, the LLM leverages the identified information $I$ to pre-\npare tailored problem-solution demonstrations for $P$:\nII-1. Generating Pseudo Problems with High Relevancy.\nWe first create pseudo problems that target the same knowl-\nedge/skills as $P$ based on the identified information $I$. For-\nmally, given $P$ and $I$, the LLM generates a pseudo problem\n$p$ targeting information listed in $I$:\n$p = \\underset{p}{\\operatorname{argmax}} P_{\\text{LLM}}(p|P,I)$\n(5)\nII-2. Generating Pseudo Solutions with High Certainty.\nIntuitively, after generating $p$, we can obtain its solution $\\bar{s}$ via\nany zero-shot prompting approach (e.g., CoT or PS). How-\never, since LLMs may produce non-factual statements, it is\nnecessary to address the correctness of pseudo-solutions.\""}, {"title": "", "content": "Filtering low-quality outputs with the LLM itself in a\nzero-shot manner is challenging. Inspired by how LLMs can\nverbalize their confidence in their predictions, we apply a\nCertainty Filtering. Formally, given a pseudo problem $p$,\nthe LLM first create a pseudo solution $\\bar{s} = (r,\\bar{a})$ consist-\ning of a rationale $r$ and an answer $\\bar{a}$. Next, it outputs a cer-\ntainty score $cs$ within 0-100 before ending the generation.\nThe form of $\\bar{s}$ depends on the zero-shot prompting method of\none's choice, e.g., Direct Prediction, COT, PS, etc:\n$\\bar{s} = \\underset{S}{\\operatorname{argmax}} P_{\\text{LLM}}(s|p)$\n(6)\n$\\Rightarrow cs = \\underset{CS}{\\operatorname{argmax}} P_{\\text{LLM}}(cs|p, \\bar{s})$\n(7)\nwhere $\\Rightarrow$ indicates the sequential generation of tokens. To\ncollect highly-confident $\\bar{s}$, we iterate this process (for at\nmost $t$ times) until we get a $\\bar{s}$ top that yields a $cs \\geq \\lambda$.\nIn practice, we collect plural pairs (i.e., $N$ pairs) of $p$ and\n$\\bar{s}$ to build a set of self-created tailored demonstrations $D_{\\text{self}}$:\n$D_{\\text{self}} = \\{(\\bar{p}_n, \\bar{s}_n)\\}_{n=1}^N$\n(8)\nPhase III: Self-Directed Problem-Solving with\nTailored Demonstrations\nWhile many works emphasize diverse and representative\ndemonstrations of the test set (Zhang et al. 2022; Sing-\nhal et al. 2023; Li et al. 2024a), it may lead to unrelated\ndemonstrations that provide sub-optimal or misleading guid-\nance, eventually affecting the solving process of target prob-\nlems (Lightman et al. 2024), especially in scenarios where\neach test instance requires different knowledge to solve.\nWe address this by guiding LLMs' problem-solving with\nits self-created demonstrations, which are tailored to the tar-\nget. Formally, given $P$, $D_{\\text{self}} = \\{\\bar{p}_n, \\bar{s}_n\\}_{n=1}^N$ is added to the\nLLM's input to guide the prediction of the solution $S$ to $P$:\n$S = \\underset{S}{\\operatorname{argmax}} P_{\\text{LLM}}(S|P, K_{\\text{self}})$\n(9)"}, {"title": "Datasets", "content": "To investigate the effectiveness of ours, we adopt sev-\neral tasks from specialized domains, starting with multiple-\nchoice questions of different emphases:\nQuestion Answering of Diverse Domains\nStrategyQA. This dataset contains questions that target\nmulti-hop reasoning over a wide range of knowledge (Geva\net al. 2021). For example, a question may require one to\nknow facts about a celebrity and the properties of hydrogen.\nScienceQA. Proposed by Lu et al. (2022), questions are col-\nlected from elementary ~ high school curricula, targeting 26\ntopics including math, language, geography, etc. We exclude"}, {"title": "Experimental Settings", "content": "Zero-shot Baselines\nZero-shot prompting has been widely utilized to mitigate\nhuman effort in LLM applications. Since SELF-TAUGHT\nalso access to nothing but the target $P$ and the LLM's own\nknowledge, we consider these fair comparisons:\nDirect prediction (Direct). The LLM directly predicts the\nsolution for $P$ without any intermediate process.\nChain-of-Thought prompting (CoT). Given a target prob-\nlem $P$, this setting promotes the LLM to generate the in-\ntermediate reasoning steps towards the solution using the\nphrase \"Let's think step-by-step\u201d (Kojima et al. 2022).\nPlan-and-Solve prompting (PS). Wang et al. (2023)\npresent PS prompting, which has shown promising perfor-\nmance in solving math problems. In PS, the LLM first de-\nvises a plan based on the problem instance and then solve it\nstep-by-step according to the plan.\nReasoning-in-Conversation (RiC). Inspired by its perfor-\nmance in linguistic tasks such as humor detection (Wang\net al. 2024), we modified it for our experiments. This set-\nting prompts an LLM to first simulate a discussion between\nexperts and then conclude the answer from the conversation.\nRole-Play prompting. Proposed by Kong et al. (2024), it\noutperforms zero-shot CoT in commonsense and math rea-\nsoning tasks. Here, the LLM and the user kick off the session"}, {"title": "Results and Discussions", "content": "We present the results of the following Research Questions:\nRQ1: Can SELF-TAUGHT's tailored demonstrations en-\nhance the LLM's reasoning in QA of diverse domains?\nRQ2: Is SELF-TAUGHT also beneficial in the clinical diag-\nnosis with real-world patients of Alzheimer's disease?\nRQ3: How phases in SELF-TAUGHT affect performances?\nRQ4: Can SELF-TAUGHT also be applied to other zero-shot\nprompting methods besides CoT, and vice versa?\nRQ5: How cost-efficient is SELF-TAUGHT?\nRQ6: Can SELF-TAUGHT generalize to other LLMs?\nTailored demonstrations allow SELF-TAUGHT to out-\nperform baselines in diverse QA tasks (RQ1). Table 1\nshows model performances in 13 QA tasks. SELF-TAUGHT\nranks first in 10 tasks, second in the rest of 3, and achieves\nbetter average acc than zero-shot baselines (top half).\nCompared with oracles, ours outperforms Manual and\nAuto-CoT in 10 and 11 tasks (out of 13). This suggests that\nusing a fixed set of demonstrations (whether human-written\nor machine-generated) throughout the inference of all test\ndata may yield sub-optimal performance, justifying our goal\nof tailored demonstrations. Retrieval CoT performs almost\nas well as ours when compared head-to-head (6 vs. 7 wins).\nWe assume that it is because it also leverages problems rele-\nvant to $P$ as demonstrations. Still, ours yields a much higher\nAvg acc, indicating that our generative method can elicit bet-\nter related demonstrations than similarity-based retrieval.\nIn Law, few-shot Direct outperforms all settings that in-\nvolve intermediate reasoning (i.e., rationale). This may be\nbecause while problems are collected from the US, there is\nno clear regulation specifying which state's law should be"}, {"title": "", "content": "referenced in each problem. This can cause misquotation of\nlaw and make the correct annotation/generation of rationales\nchallenging, negatively affecting problem-solving. Interest-\ningly, in Physics (Phys), SELF-TAUGHT and Manual CoT\nhave much higher acc than others. This may suggest that en-\nsuring the quality of demonstrations is relatively more im-\nportant in physics domains than in other domains.\nOurs is less effective than Manual CoT in AD diagnosis\ndue to the high similarity between instances (RQ2). In\nAD diagnosis (Table 2), SELF-TAUGHT beats all baselines\nin all metrics, except for Manual CoT. This pattern is much\ndifferent from the findings in RQ1. We conjecture that it is\nbecause the discrepancy between each instance is extremely\nsmall here: all instances require LLMs to perform the same\ntask w/ the same 3 output classes, via EHRs that are struc-\ntured in the identical key-value format. This can marginalize\nthe effect of tailored demonstrations and amplify the benefit\nof fixed human-crafted demonstrations.\nRegardless, ours presents a much smaller performance\ngap between it and Manual-CoT (6.18 percentage points of\nacc; avg of all classes/datasets) than other baselines (8.25 ~\n12.54 percentage points). In real clinical settings, each pa-\ntient's EHR may be constructed diversely due to each radi-\nologist's preference and other situational factors, thus requir-\ning different reasoning to diagnose (Norman 2005). With in-\nsights from RQ2, we presume one can adjust SELF-TAUGHT\nfor real-world applications by combining it with slight man-\nual effort. For instance, including a minimal demonstration\nin the generation of pseudo problems/EHRs (Phase II), i.e.,\ndemonstration expansion, to tackle different EHR styles and\ndiagnostic processes. We leave this to future work.\nDesigned phases contribute to performance improve-\nment (RQ3). To investigate how our phasic design af-\nfects model performance, we evaluate ablated and modified\nSELF-TAUGHT in all 15 tasks in Table 3. First, ablations of"}, {"title": "Comparing SELF-TAUGHT with Zero-shot CoT", "content": "Here, we investigate the improvement brought by SELF-\nTAUGHT (using 0-shot CoT in Phase II and III) over vanilla\nCoT in Figure 6 by comparing their prediction correctness.\nAmong problems that 0-shot CoT is wrong, SELF-TAUGHT\nsolve 15.4% of them correctly, even though they are both\nbased on 0-shot CoT prompting. This exhibits the benefit of\ntailored demonstrations in LLM problem-solving."}, {"title": "Number of Pseudo Shots", "content": "We analyze the effect of varying the number of self-created\npseudo shots using the six college-level tasks (Figure 7).\nFirst, SELF-TAUGHT outperforms Manual CoT and Re-\ntrieval CoT (both have $N = 3$) with fewer shots ($N = 2$). This\nsuggests the effectiveness of tailored demonstrations gener-\nated with our framework, as well as the possibility of tuning\n$N$ to further decrease our computational cost. Also, we ob-\nserve that when $N \\geq 3$, model performance remains almost\nconsistent. This pattern matches the findings from regular\nprompting with real demonstrations (Brown et al. 2020)."}, {"title": "SELF-TAUGHT and Task Difficulty", "content": "Lastly, we investigate the relation between SELF-TAUGHT\nand task difficulty (approximated by the most un-engineered\nsetting, i.e., O-shot Direct). Figure 8 shows a trend that as the\nperformance of 0-shot direct decreases, the improvement in-\ncreases, indicating that the LLM benefits more from SELF-\nTAUGHT in tasks that are initially harder for it w/o additional\ntechniques. This provides a guideline for us to judge the pri-\nority when applying ours to LLM applications. Also, when\nplotting CoT against Direct, the regression coefficient $\\beta =$\n-0.14, showing that SELF-TAUGHT generally brings more\nimprovement ($\\beta$ = -0.21) than CoT as the task gets difficult."}, {"title": "Related Work", "content": "General-purpose LLMs have been widely applied to diverse\ndomains thanks to their ability to learn from user input: Li\net al. (2024b) prompt LLMs to predict drug synergy; Chi-\nang, Chou, and Riebesell (2024) build a prompt-driven sys-\ntem with material science APIs for crystal generation; Kwon\net al. (2024) use LLMs to annotate raw patient data. How-\never, they often rely on human demonstrations, which can be\ncostly and yield sub-optimal performance when encounter-\ning the above-discussed demonstration-target discrepancy.\nBesides the mentioned works (e.g., PS, Auto-CoT, etc.),\nseveral studies have proposed to mitigate human efforts in\nprompt construction: Zhou et al. (2024) and Chae et al.\n(2024) invoke LLMs to generate task-level plans shared\nacross all instances to guide instance-level inference. Lyu\net al. (2022) use sentences from external related corpora\nalong with random labels as pseudo demonstrations for text\nclassification. Wan et al. (2023) use LLMs to create a large\ndemonstration pool by repeatedly running the test set with\n0-shot CoT and using majority-vote-based criteria to se-\nlect good demonstrations for problem-solving. Similarly, Li\net al. (2024a) use LLMs to generate a pseudo dataset of 5K\nquestions with 29 designed topics from scratch and use it\nas the demonstration pool. Recently, Yang et al. (2024) use\nLLMs as optimizers, where the LLM iteratively updates the\nproblem-solving instruction (e.g., \"Break this down\") until\nit yields a maximum accuracy on the training set. To address\nthe lack of annotated rationales for CoT fine-tuning, Hwang\net al. (2024) first run the training set with LLMs to collect\ncorrect/wrong CoT rationales. Then, the \u201cfirst wrong step\u201d\nin wrong rationales is identified with designed algorithms\nand used as fine-grained rewards for preference learning.\nSimilar to ours, Kim et al. (2022) and Chen et al. (2023)\ngenerate pseudo demonstrations by prompting LMs with\nphrases like \u201cgenerate a negative review\u201d or \u201cCome up with\ndiverse creative instances for the task\". However, the for-\nmer requires the same output span (e.g., positive & negative)\nacross all test instances. The latter focuses on facilitating di-\nverse representative problems following Auto-CoT (Zhang\net al. 2022). It neglects the discrepancy (addressed infor-\nmation) between test and pseudo problems as well as the\ncorrectness of pseudo solutions (we show the effect of such\nneglection in Table 3). Inspired by them, SELF-TAUGHT re-\nsolve human effort by creating demonstrations that are both\n\"quality-controlled\u201d and \u201ctailored\" to each target instance,\nwhich is under-explored so far to the best of our knowledge.\""}, {"title": "Conclusions", "content": "We present SELF-TAUGHT, a problem-solving framework\nfor specialized domains. It addresses the costly human effort\nand the demonstration-target discrepancy in LLM applica-\ntions, by creating demonstrations that are quality-controlled\nand tailored to each test instance. It outperforms baselines in\n15 tasks of QA and AD diagnosis. It is Pareto efficient re-\ngarding cost and performance and generalizable to different\nprompting methods and LLMs. The quality of self-created\nproblems/solutions is confirmed in expert evaluations. We\ndiscuss the limitations of our work in Appendices."}, {"title": "Appendices", "content": "This work has limitations. First of all, SELF-TAUGHT can\nbe less cost-efficient. Although we have shown that SELF-\nTAUGHT's cost-performance trade-off is acceptable (i.e.,\nPareto efficient), addressing the system cost more explicitly\ncan be necessary for real-world deployment. A plausible di-\nrection is combining Retrieval CoT, SELF-TAUGHT, and an\nadditional logic (e.g., a threshold of text similarity) that de-\ntermines whether problems addressing similar knowledge to\nthe target problem are available in the training/test corpora\n(for us to retrieve) and selectively generate tailored demon-\nstrations only when such problems are absent or not enough\nfor the desired number of shots.\nAnother concern is that although SELF-TAUGHT gener-\nally brings more performance gains to the adopted LLM in\ntasks that are initially more challenging for it (Figure 8), the\nfinal performance may still be far from optimal due to the\nlack of related knowledge in its parameter. One may address\nthis by (1) adopting LLMs that are fine-tuned with corpora\nof the corresponding domains, e.g., running SELF-TAUGHT\nwith BioMistral (Labrak et al. 2024) when solving medical\nproblems, or (2) retrieving relevant information from exter-\nnal knowledge bases and use them to augment the generation\nof tailored demonstrations. We leave these to future work.\n2 Supplementary Results\nCombining SELF-TAUGHT with Other Zero-shot\nPrompting Methods\nWe hereby present the full result of Table 4:\n\u2022 Table 9: The results for SELF-TAUGHT that is combined\nwith zero-shot Direct.\n\u2022 Table 10: The results for SELF-TAUGHT that is combined\nwith zero-shot Plan-and-Solve (PS).\nCombining SELF-TAUGHT with a Smaller\nOpen-source LLM\nWe show the detailed results of Figure 3 at:\n\u2022 Table 11: SELF-TAUGHT's performances when run with\nLlama-3.1-8B in question-answering.\n\u2022 Table 12: SELF-TAUGHT's performances when run with\nLlama-3.1-8B in the diagnosis of AD.\n3 Further Details on the Datasets\nQuestion Answering of Diverse Domains\nStrategyQA. This dataset contains questions that target\nmulti-hop reasoning over a wide range of knowledge (Geva\net al. 2021). The main feature of this dataset is that the nec-\nessary knowledge required for solving the question is not\nexplicitly stated in the question text (e.g., \u201cYes or No: Did\nAristotle Use a Laptop?", "below": "nComplete the statement. Hydrogen chloride is\n(A)", "compound": "n(B)", "substance": "nMedQA. This is a popular benchmark datasets in the med-\nical domain curated by Jin et al. (2021). Since the questions\nare collected from national medical licensing exams in sev-\neral countries (i.e., multi-lingual), we only adopt questions\nwritten in English. We use the test set with a size of 1,273.\nAn example is shown below:\nA 21-year-old sexually active male complains of\nfever, pain during urination, and inflammation and\npain in the right knee. A culture of the joint fluid\nshows a bacteria that does not ferment maltose and\nhas no polysaccharide capsule. The physician orders\nantibiotic therapy for the patient. The mechanism of\naction of action of the medication given blocks cell\nwall synthesis, which of the following was given?"}]}