{"title": "MOSABench: Multi-Object Sentiment Analysis Benchmark for Evaluating Multimodal Large Language Models Understanding of Complex Image", "authors": ["Shezheng Song", "Chengxiang He", "Shasha Li", "Shan Zhao", "Chengyu Wang", "Tianwei Yan", "Xiaopeng Li", "Qian Wan", "Jun Ma", "Jie Yu", "Xiaoguang Mao"], "abstract": "Multimodal large language models (MLLMs) have shown remarkable progress in high-level semantic tasks such as visual question answering, image captioning, and emotion recognition. However, despite advancements, there remains a lack of standardized benchmarks for evaluating MLLMs performance in multi-object sentiment analysis, a key task in semantic understanding. To address this gap, we introduce MOSABench, a novel evaluation dataset designed specifically for multi-object sentiment analysis. MOSABench includes approximately 1,000 images with multiple objects, requiring MLLMs to independently assess the sentiment of each object, thereby reflecting real-world complexities. Key innovations in MOSABench include distance-based target annotation, post-processing for evaluation to standardize outputs, and an improved scoring mechanism. Our experiments reveal notable limitations in current MLLMs: while some models, like mPLUG-owl and Qwen-VL2, demonstrate effective attention to sentiment-relevant features, others exhibit scattered focus and performance declines, especially as the spatial distance between objects increases. This research underscores the need for MLLMs to enhance accuracy in complex, multi-object sentiment analysis tasks and establishes MOSABench as a foundational tool for advancing sentiment analysis capabilities in MLLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, multimodal large language models [1] (MLLMs) have made significant progress in image under-standing tasks, demonstrating immense potential, particularly in high-level semantic tasks such as visual question an-swering [2, 3, 4], image captioning [5, 6], and emotion recognition [7, 8, 9]. Commercial platforms like GPT40 [10] and Gemini [11], as well as open-source models such as Flamingo [12] and LLaVA [13], have achieved outstanding performance on a wide range of traditional tasks, even surpass-ing human-level performance in tasks like multimodal named entity recognition [14, 15, 16]. This impressive potential has driven the development of multimodal evaluation tasks. However, sentiment analysis [17, 18], as one of the key tasks in semantic understanding, still lacks a standardized benchmark specifically designed for evaluating MLLMs. Figure 1 shows that the performance of MLLMs on our MOSABench is unsatisfactory.\nAs shown in Figure 2 and 3, current sentiment analysis datasets have limitations in accurately evaluating the capa-bilities of MLLMs. Lack of focus on multi-object under-standing: In real-world social media, images typically contain multiple objects (such as people), each of which may convey different emotional information. Therefore, MLLMs are facing the challenge of multiple object sentiment analysis. However, most existing sentiment analysis benchmarks, such as Twit-ter15, Twitter17, and MSED [19], are dominated by single-object samples, which leads to models focusing primarily on single-object sentiment classification ability [20, 21]. When single-object data dominate the dataset, the model's good performance likely reflects its ability to classify emotions for individual objects, rather than its capacity to analyze emotions across multiple objects in an image. As a result, evaluating models on such imbalanced datasets fails to comprehensively assess their ability to understand multiple-object emotions, which limits the potential of multimodal sentiment analysis models. Lack of adaptability to MLLMs: Moreover, existing datasets [19, 22] have substantial limitations in evaluating the capabilities of MLLMs. Most of these datasets are designed for smaller models, lack instruction, and require extensive adaptation, making it challenging to comprehensively assess the performance of MLLMs. Additionally, since MLLMs gen-erate outputs in varying formats rather than strictly following predefined formats, accurately evaluating their performance is more difficult. Therefore, developing a standardized, multi-dimensional benchmark specifically designed for evaluating multi-object sentiment analysis tasks has become an urgent challenge."}, {"title": "II. RELATED WORK", "content": "The recent advancements in large language models (LLMs) [26, 27, 28, 29] have significantly improved multi-modal sentiment analysis, effectively handling complex sen-timent tasks across various modalities [30, 31]. Studies have demonstrated that LLMs achieve high accuracy on standard datasets, such as Twitter15 and Twitter17 [22], which are widely used to assess sentiment analysis capabilities by inte-grating text and image data to analyze social media posts. For example, the WisdoM[32] framework leverages large vision-language models (LVLMs) to enhance sentiment analysis by incorporating contextual world knowledge from images and text, thus improving LLMs interpretability and perfor-mance [33]. Similarly, the PSL [34] framework is a pipeline-based approach for aspect-based sentiment analysis, using small language models (SLMs) [35] for aspect extraction and MLLMs for sentiment analysis. This structured guidance enables MLLMs to focus on relevant image regions, effec-tively addressing the complexities of multimodal sentiment tasks [34]. These frameworks highlight the progress LLMS have made in multimodal alignment and sentiment understand-ing [20, 33]. However, current benchmarks, like Twitter15 and Twitter17 [22], reveal limitations in assessing MLLMS true multimodal comprehension capabilities. Primarily, these datasets often lack image-text consistency, where the targets mentioned in the text may not appear in the associated image, hindering accurate evaluations of MLLMs capacity to integrate visual context [33]. Additionally, these benchmarks are not equipped with LLM-specific instructions [36], making it chal-lenging to assess the impact of different prompting methods on sentiment prediction [34]. Lastly, they lack multi-object sentiment assessment, which is crucial for evaluating the abil-ity of MLLMs to independently analyze sentiments towards multiple entities within a single post [32, 34]. These gaps underscore the need for a scientifically designed benchmark that captures multimodal nuances, includes structured prompts tailored for LLM tasks, and offers multi-object sentiment assessment to comprehensively evaluate LLM performance in real-world multimodal sentiment applications.\nIn the evaluation of LLMs, tasks such as Named En-tity Recognition (MNER) [14, 15] have developed dedicated benchmarks to better measure model performance. However, sentiment analysis lacks a specialized benchmark tailored specifically for LLMs. While some comprehensive bench-marks, such as MM-SOC [37] and MM-BigBench [38], in-clude multimodal tasks related to sentiment analysis, they still exhibit notable limitations in design and evaluation [39, 40], particularly in areas of image-text consistency, instruc-tion design, and multi-object sentiment judgment [32, 34]. For instance, MM-SOC primarily targets multimodal tasks within social media environments, covering sentiment analysis, hate speech detection [41], and more, using the Memotion dataset [42] for joint image-text emotion detection. However, MM-SOC does not specifically address image-text consistency, which limits the evaluation of whether all entities mentioned in the text are also represented in the image. Additionally, MM-SOC lacks LLM-specific instructions, restricting its utility in large-scale multimodal tasks that require instruction-following capabilities [34]. MM-BigBench, on the other hand, covers a broad range of multimodal comprehension tasks, including visual question answering and multimodal sentiment analysis (MSA), focusing on multimodal information fusion and deep understanding. However, MM-BigBench does not provide detailed evaluations for multi-object sentiment analysis and does not adequately emphasize image-text consistency, which is essential for accurately identifying the sentiments toward multiple entities mentioned in the text. Furthermore, this benchmark lacks instructions specifically designed for LLMs, making it difficult to assess how different prompt structures might affect performance.\nIn summary, while current benchmarks have made progress in multimodal sentiment analysis, there remains significant room for improvement in image-text consistency, multi-object sentiment assessment, and instruction design specifically for LLMs."}, {"title": "III. MOSA BENCHMARK CONSTRUCTION", "content": "We construct the MOSABench dataset to address limita-tions in current MLLMs for multi-object sentiment analysis tasks. First, samples are selected from Twitter15, Twitter17, and TwiBot-20 that meet the requirements for multi-object sentiment analysis. To ensure data accuracy and diversity, strict filtering criteria are applied: each text must contain multiple distinct targets, and each target must also appear in the corresponding image, enabling the model to capture emotional cues from both visual and textual information. Abstract terms such as \"United Nations\" are excluded to ensure that each target in the sample has a clear emotional expression, thereby enhancing the model in distinguishing sentiments across multiple targets.\nDuring annotation, these samples are adapted from the original Multi-Aspect Based Sentiment Analysis (MABSA) format to a sentiment analysis format. Given a specified target, diverse question forms are designed, such as \"Please confirm the sentiment of X\" or \"Please judge the status of X,\" to simulate the model adaptability to different question types. This diverse design not only enhances the generalizability of the task but also prevents potential bias that may arise from a single question style, thereby enabling a more comprehensive evaluation of model capability.\nTo ensure objectivity and consistency in evaluation, the labeling system in our dataset is simplified by adopting a binary-choice structure in which \"A\" and \"B\" represent different sentiment categories, such as negative, neutral, and positive, using a fixed sentiment mapping. This structure reduces errors caused by inconsistent labels and streamlines model output processing, making sentiment analysis evaluation more straightforward. Additionally, the evaluation method is improved by implementing a novel scoring mechanism: for binary-choice questions, fully correct answers receive 3 points, partially correct answers receive 1 point, and fully incorrect answers receive no points. This scoring method enables a more comprehensive assessment of MLLM capabilities, enhancing the reliability and precision of sentiment analysis evaluation and providing a practical benchmark for multi-object sentiment analysis tasks.\nAs shown in Figure 5, we also annotate and analyze the spatial distances between targets within each image. This an-notation strategy aims to verify the relationship between target distance and task accuracy, revealing the potential limitations of MLLMs in multi-object sentiment analysis. Experimental results show that target distance significantly impacts senti-ment judgment accuracy; in particular, the accuracy decreases notably when targets are close to each other. This finding provides valuable insights for improving MLLM performance in complex scenarios.\nAs shown in this algorithm 1, we calculate the spatial relationship between two detected objects based on their bounding boxes. The bounding boxes, $B_1$ and $B_2$, are obtained using an object detection model, where only the two highest confidence detections of category \"person\" are selected as input. This ensures that the algorithm focuses on the spatial relation between two human figures within the image. The input image length L and threshold parameter k allow us to classify the relationship as Interlap, Close, or Far. Specifically, the parameter k serves as a tunable threshold to adjust the proximity level for determining Close and Far classifications."}, {"title": "A. Dataset Construction", "content": "We construct the MOSABench dataset to address limita-tions in current MLLMs for multi-object sentiment analysis tasks. First, samples are selected from Twitter15, Twitter17, and TwiBot-20 that meet the requirements for multi-object"}, {"title": "B. Distance Annotation", "content": "As shown in Figure 5, we also annotate and analyze the spatial distances between targets within each image. This an-notation strategy aims to verify the relationship between target distance and task accuracy, revealing the potential limitations of MLLMs in multi-object sentiment analysis. Experimental results show that target distance significantly impacts senti-ment judgment accuracy; in particular, the accuracy decreases"}, {"title": "Algorithm 1 Calculate Distance Type Between Two Bounding Boxes", "content": "Require: Image length L, Bounding boxes $B_1$ = $(x_1, y_1, x'_1, y'_1)$ and $B_2$ = $(x_2, y_2, x'_2, y'_2)$, threshold parameter k\nEnsure: Distance label: Interlap, Close, or Far\nStep 1: Compute center points $C_1$ and $C_2$:\n$C_1 = (\\frac{x_1+x'_1}{2}, \\frac{y_1+y'_1}{2})$\n$C_2 = (\\frac{x_2+x'_2}{2}, \\frac{y_2+y'_2}{2})$\nStep 2: Check for overlap (Interlap)\n1: if $x_1 < x'_2$ and $x'_1 > x_2$ and $y_1 < y'_2$ and $y'_1 > y_2$ then\nreturn Interlap\nend if\nStep 3: Calculate the Euclidean distance d between $C_1$ and $C_2$:\n$d = \\sqrt{(C_{1x} - C_{2x})^2 + (C_{1y} - C_{2y})^2}$\nStep 4: Determine distance type (Close or Far):\n4: if $d < L/k$ then\nreturn Close\nelse\nreturn Far\nend if"}, {"title": "C. Datasets Statistics", "content": "We conduct a statistical analysis of our MOSABench dataset, with results presented in Table I. The dataset contains a total of 1,047 samples, categorized into three distance groups: Close, Interlap, and Far, to capture varying spatial configurations. Among these samples, 57.11% are classified as Close, 31.18% as Interlap, and 11.71% as Far, providing a balanced representation across different target proximities. This distribution facilitates a comprehensive evaluation of MLLM performance under diverse spatial contexts.\nThe sentiment distribution in MOSABench reflects a sci-entifically consistent approach, aligning with the proportions observed in prior datasets. In MOSABench, Negative, Neutral, and Positive sentiments are represented by 15.44%, 49.43%, and 35.13%, respectively. This design aligns closely with the sentiment distributions in the Twitter15 and Twitter17 datasets [22], which feature similar proportions: Twitter15 in-cludes 12.06% Negative, 59.29% Neutral, and 28.65% Positive samples, while Twitter17 comprises 12.19% Negative, 45.68% Neutral, and 42.13% Positive samples. By mirroring these es-tablished distributions, MOSABench provides a scientifically robust benchmark for evaluating MLLMs sentiment analysis capabilities across multiple targets."}, {"title": "D. MLLM Baselines", "content": "Based on the MOSABench dataset we constructed, we conduct a comprehensive evaluation of existing MLLMs to assess their performance in multi-object sentiment analysis tasks. To this end, we propose two baseline methods and systematically test various mainstream model architectures, covering three distinct types. In terms of model selection, we choose representative MLLMs, categorized into three groups according to their architectures:\nOpen-sourced models: LLaVA1.6 [13], mPLUG-Owl [24], Qwen-VL [43], Qwen-VL2 [25], VisualGLM [44], BLIVA-FlanT5 [45], Monkey [46], GLM4V [47], InternLM2.5 [48].\nClose-sourced models: ERNIE-Bot [49], GPT-40 [10] and Gemini [11].\nThis diverse selection of models ensures the broad applicabil-ity and representativeness of the evaluation results."}, {"title": "E. Post-processing for LLM Scoring Assistance", "content": "In our experiments, we observe that not all large language models generate outputs with consistent formatting. Some models show variability in response length or randomness due to pre-training biases, which makes it difficult to score re-sponses effectively using simple regular expression matching.\nTo address this issue, we design a post-processing program that leverages a dedicated language model to simplify and standardize outputs before scoring them with a regex-based approach.\nThe post-processing program simplifies complex generated text, enabling it to conform to standardized scoring require-ments. Some generated responses contain lengthy explana-tions, which complicates direct regex matching. The dedicated language model simplifies and reformats these outputs to enable subsequent scoring automation.\nThis post-processing strategy improves scoring accuracy and reduces the need for extensive formatting instructions in multimodal sentiment analysis, minimizing potential interfer-ence with model performance. We select the Qwen2.5-7B-Instruct model for this post-processing, as it demonstrates strong capabilities in instruction-following and structured out-put generation. Other language models with structured output capabilities are also suitable for this purpose."}, {"title": "F. Metrics", "content": "In the Benchmark evaluation study, a novel scoring method is introduced to enhance the assessment of MOSABench per-formance. Specifically, each question is designed as a multiple-choice task where each sample requires separate emotion as-sessments for object1 and object2. Following an examination-style grading approach, if both assessments are correct, the sample receives a score of 3; if only one assessment is correct, it receives a score of 1; and if both are incorrect, no points are awarded, with a maximum score of 3 for each sample. To facilitate comparison with traditional benchmarks, each MLLM performance is also evaluated using standard metrics, including F1, Precision, and Recall. This combined assessment provides a comprehensive view of model effectiveness in multi-object sentiment analysis tasks."}, {"title": "IV. EXPERIMENTS AND ANALYSIS", "content": "As shown in Table II, we evaluate the performance of various MLLMs on MOSABench, focusing on multi-object sentiment analysis across different target distances. This table"}, {"title": "B. Score Distribution Across Distance Labels", "content": "Table III shows model performance across Interlap, Close, and Far categories, highlighting a decline in correct predictions (S3) and average scores (S) as object distance increases. This trend indicates that MLLMs struggle more with emotion recognition as targets become more distant, underscoring their limitations in complex, multi-object scenarios. Among open-sourced models, Qwen-VL2-7B achieves the highest average score (S) of 1.54, with strong S3 proportions across distances, showing effective emotion detection, especially for nearby objects. In contrast, Qwen-VL-7B performs the worst, with an average score of 0.31 and high SO proportions, particularly in"}, {"title": "C. Attention Visualization Analysis", "content": "Figure 8 visualizes the attention regions of various MLLMs during multi-object sentiment analysis, highlighting how each model interprets visual information, particularly in recogniz-ing facial expressions, which are crucial for accurate senti-ment detection. mPLUG-Owl, with the highest F1 (73.16), demonstrates the best focus, accurately targeting the facial expressions of both subjects. This focused attention suggests a strong understanding of task requirements, enabling it to avoid distractions and focus on sentiment-relevant features. Qwen-VL2, with an F1 of 58.39, also attends to the facial areas but with less intensity and consistency, indicating some missed or under-emphasized sentiment details, which may explain its"}, {"title": "D. Confusion Matrix Analysis", "content": "Figure 9 presents the confusion matrices for four MLLMs, illustrating their performance in multi-object sentiment analy-sis, with redder cells indicating higher counts for each senti-ment classification. mPLUG-Owl, the model with the highest performance, shows strong results along the diagonal, with the reddest cells indicating it accurately classifies sentiments across categories, including neutral (neu) and negative (neg) sentiments. Qwen-VL2 also performs well but has a tendency to misclassify neutral sentiments as negative, as indicated by the red cells off the diagonal in the neutral-negative area. LLaVA, in contrast, often misclassifies negative sentiments as neutral, as shown by the reddish cells in the negative-neutral category. VisualGLM, the model with the weakest performance, struggles significantly, frequently misclassifying negative sentiments as positive, with noticeable red shading in the off-diagonal cells of the negative-positive category. This analysis highlights the distinct ways each model handles sentiment classification errors, with mPLUG-Owl achieving"}, {"title": "V. CONCLUSION", "content": "We systematically investigate the limitations of MLLMs in multi-object sentiment analysis, particularly in scenarios with visually distinct and spatially varied targets. We introduce MOSABench, a benchmark dataset specifically designed to evaluate MLLM capabilities in independently and accurately assessing sentiments across multiple objects within a single image. MOSABench includes a wide range of spatial relation-ships, enabling a detailed analysis of how target proximity and feature diversity impact model performance. Our findings re-veal the significant challenges MLLMs face in complex multi-object environments, highlighting the need for architectural enhancements to improve their adaptability to these tasks. By providing a dedicated dataset and comprehensive evaluation framework, this work lays a foundation for future research aimed at advancing MLLM performance in nuanced, multi-target sentiment analysis. MOSABench serves as an initial exploration for future MLLM research, offering directions and insights for evaluating multi-object sentiment analysis. This benchmark aims to contribute to the development and assess-ment of models better suited for complex multimodal tasks, supporting progress in multi-object sentiment understanding."}]}