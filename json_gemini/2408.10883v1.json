{"title": "DAAD: Dynamic Analysis and Adaptive Discriminator for Fake News Detection", "authors": ["Xinqi Su", "Yawen Cui", "Ajian Liu", "Xun Lin", "Yuhao Wang", "Haochen Liang", "Wenhui Li", "Zitong Yu"], "abstract": "In current web environment, fake news spreads rapidly across online social networks, posing serious threats to society. Existing multimodal fake news detection (MFND) methods can be classified into knowledge-based and semantic-based approaches. However, these methods are overly dependent on human expertise and feedback, lacking flexibility. To address this challenge, we propose a Dynamic Analysis and Adaptive Discriminator (DAAD) approach for fake news detection. For knowledge-based methods, we introduce the Monte Carlo Tree Search (MCTS) algorithm to leverage the self-reflective capabilities of large language models (LLMs) for prompt optimization, providing richer, domain-specific details and guidance to the LLMs, while enabling more flexible integration of LLM comment on news content. For semantic-based methods, we define four typical deceit patterns: emotional exaggeration, logical inconsistency, image manipulation, and semantic inconsistency, to reveal the mechanisms behind fake news creation. To detect these patterns, we carefully design four discriminators and expand them in depth and breadth, using the soft-routing mechanism to explore optimal detection models. Experimental results on three real-world datasets demonstrate the superiority of our approach. The code will be available at: https://github.com/SuXinqi/DAAD.", "sections": [{"title": "Introduction", "content": "News is a crucial channel for the public to understand the world. With the rapid development of online social networks (OSNs) like Twitter and Weibo, the spread of fake news and distorted views has grown exponentially (Willmore 2016). This not only increases the information burden but can also cause panic, leading to significant negative impacts on society (Dong et al. 2024). To mitigate the adverse effects of fake news, automated fake news detection (FND) has become a research focus, helping network administrators prevent its spread (Ying et al. 2023).\nTweets with images are more engaging, which is why most content on current OSNs is presented in a multimodal format (Wang et al. 2023a; Wu et al. 2023). Additionally, multimodal fake news is more convincing due to its realism, and fake news creators exploit this feature to amplify their influence (Hu et al. 2022; Khattar et al. 2019). Therefore, this paper focuses on multimodal fake news detection (MFND). Many studies have been dedicated to detecting multimodal fake news and mitigating its harmful effects. Current MFND are primarily divided into two categories: (i) semantic-based, and (ii) knowledge-based methods.\nThe first category of methods argues that as technology advances, the style of fake news increasingly resembles that of real news, however MFND methods often lack critical information such as social context (Liao et al. 2023). To address this, some approaches incorporate knowledge bases (KBs) like Wikipedia (Hu et al. 2021; Tseng et al. 2022), knowledge graphs (Qian et al. 2021; Jin et al. 2022; Dun et al. 2021), and public APIs (Qi et al. 2021; Zhang et al. 2021) to determine the authenticity of news by capturing consistency with the KB or enriching the news content. However, as fake news becomes more sophisticated, static KBs struggle to provide reliable factual signals. With the emergence of large language models (LLMs) that possess strong reasoning and content understanding capabilities, community feedback indicates that LLMs' comments of news can enhance fake news detection systems (Liu et al. 2024b; Hu et al. 2024). Nonetheless, enabling LLMs to perform the intended tasks effectively remains a challenge. For instance, recent studies use prompt engineering to guide LLMs to focus on writing style and common-sense errors in news, allowing them to infer key clues (Liu et al. 2024a). However, there are two major drawbacks: 1) The prompts are manually crafted, overly reliant on human expertise, leading to limited generalization (Ma et al. 2024); 2) LLMs are highly sensitive to prompt formats, where semantically similar prompts can yield vastly different performances (Kojima et al. 2022; Wei et al. 2023), and the optimal prompt may be model- and task-specific (Lin et al. 2024; Ma et al. 2023; Hao et al. 2024). Thus, suitable prompts are crucial for LLMs to effectively analyze news content (Reynolds and McDonell 2021).\nThe second category of methods focuses on capturing the semantic features of news content to distinguish between real and fake news, which can be broadly divided into three subapproaches: 1) Research indicates that there are significant differences in the expression style of credible news versus fake news (Rashkin et al. 2017; Shu et al. 2017; Horne and Adali 2017). Consequently, some methods detect fake news by analyzing elements such as sentiment (Chen et al. 2023; Kwak, An, and Ahn 2020; Zhang et al. 2023) and logic (e.g., mismatches between semantics and grammar) (Li, Zhang, and Malthouse 2024; Xiao et al. 2024), as illustrated in Fig. 2a and Fig. 2b. 2) As shown in Fig. 2c, some fake news creators use techniques to manipulate images, leading to methods that focus on detecting image manipulation to identify fake news (Lao et al. 2024; Dong et al. 2024). 3) As illustrated in Fig. 2d, another common methods to creating multimodal fake news is the incorrect reuse of outdated images. Therefore, some methods assess the semantic consistency between images and text to detect fake news (Wang et al. 2023a; Wu, Liu, and Zhang 2023; Wu et al. 2023). Despite the significant performance improvements achieved by these subapproaches from different perspectives (Wu et al. 2021), two key limitations remain: 1) The detection models are manually crafted and rely on human knowledge and feedback, meaning that the optimal detection models have yet to be fully developed; 2) Existing detection methods are static, applying the same detection model to all samples, which prevents different detection patterns from complementing each other.\nTo address the aforementioned challenges, we propose a Dynamic Analysis and Adaptive Discriminator (DAAD) for fake news detection. First, we introduce the Monte Carlo Tree Search (MCTS) algorithm, strategically leveraging the self-reflective capabilities and error feedback of LLMs to optimize prompts, thereby introducing effective comments. To prevent the optimization process from getting trapped in local minima, we designed a 'MemoryBank' component that not only stores historical errors during the search but also compresses them into concise global guidance, encouraging LLMs to adjust prompts from a bird's eye view. Additionally, we use prompt resampling and batchprompt to expand both the prompt and sample spaces, respectively, further reducing the risk of local minima. Next, we meticulously designed four discriminators to target different deceit patterns: the ReLU and logical discriminator detect the reasonableness of emotion and contextual logic, respectively; the frequency-domain discriminator focuses on identifying image manipulation; and the semantic discriminator evaluates the alignment between image and text semantics. Finally, we expanded these discriminators in both depth and breadth, combining them with a soft-routing to adaptively explore the optimal detection model.\n\u2022 We propose a joint prompt optimization framework, namely DAAD, based on MCTS and the MemoryBank component to enhance the effectiveness and controllability of LLMs, while mitigating local minima during the optimization process.\n\u2022 We design four discriminators and employ a soft-routing mechanism to adaptively explore the optimal fake news detection model.\n\u2022 Extensive qualitative and quantitative experiments on three benchmarks, i.e., Weibo, Weibo21, and GossipCop, demonstrate the superiority of our approach."}, {"title": "Related Work", "content": "Knowledge-based Fake News Detection This category of methods employs external knowledge to aid in the detection of fake news. Some approaches use related entities from knowledge graphs (KGs) to enrich news content (Qian et al. 2021; Dun et al. 2021; Jin et al. 2022; Zhang et al. 2024). Other methods incorporate evidence from external knowledge bases (KBs) and capture their consistency to help distinguish between true and fake news (Hu et al. 2021; Qi et al. 2021; Liao et al. 2023). With the advent of large language models (LLMs), many approaches leverage their powerful knowledge and reasoning capabilities to analyze or enrich news content. For example, FakeNewsGPT4 (Liu et al. 2024b) and LEMMA (Xuan et al. 2024) utilize specific knowledge from LLMs to provide supplementary information for fake news detectors. DELL (Wan et al. 2024) and ARG (Hu et al. 2024) introduce LLM-based analysis to support decision-making in fake news detection. However, these"}, {"title": "Methodology", "content": "The architecture of our proposed DAAD, as shown in Fig. 3, predicts the label based on the multimodal news content and the comment provided by the LLM.\nDefinition 1 (Multimodal Fake News Detection): A piece of multimodal news is defined as \\(N = \\{I, T\\}\\), where I and T represent the image and text of the news, respectively. Our goal is to learn a probability distribution \\(P(Y|I, C)\\) that can effectively distinguish between fake and real news, where y = 1 indicates fake news, y = 0 indicates real news, and C denotes the comments from LLMs about the news.\nLearning Optimized Prompts\nHere, we will detail the process of prompt optimization to generate expert-level prompts in the news domain and produce insightful comments.\nDefinition 2 (Prompt Optimization): Given a LLM M, our goal is to find a prompt p that generates the best comments \\(C^{\\Diamond}\\) for a small set of sampled news \\(N^{\\Diamond} = \\{T\\}\\). Formally, this can be framed as an optimization problem. We aim to identify a prompt p within the natural language space S when M is prompted with p and T, the expected value of each sample's comment score \\(\\varphi(p, T^{\\Diamond})\\) is maximized.\n\\[p^* = arg \\underset{p \\in S}{max} E_{T^{\\Diamond}}[\\varphi(p, T^{\\Diamond})].\\]\nThe Markov Decision Process (MDP) We employ the reinforcement learning (RL) (Li 2017) for prompt optimization, modeling it as a MDP represented by a 4-tuple (S, \u0391, \u0393, \u03a5), where S denotes the state space, A represents the action space, \u0393 is the state transition function \u0393 : S \u00d7 A \u2192 S, and Y is the reward function Y : S \u00d7 A \u2192 R. To incorporate domain-specific knowledge into prompt revisions meaningfully, we propose action generation based on error feedback. Fig. 4a illustrates a state transition process. In the current state st, the agent generates an action at based on \\(a_t \\sim P_{M'}(a | s_t, M')\\), where M' is a LLM used for action generation. Subsequently, the agent obtains the new state st+1 based on the transition function \\(s_{t+1} \\sim P_{M''}(s | s_t, a_t, M'')\\), where M\" is a LLM used for state transition. The reward Y is defined as the accuracy on the validation set N.\nEnhancing Prompt Optimization To enhance the agent's exploration of the prompt p, we employ Monte Carlo Tree Search (MCTS) for optimization, inspired by (Wang et al. 2023b). Specifically, MCTS maintains a state-action value function Q: S \u00d7 A \u2192 R to estimate the potential rewards for state-action pairs along various paths. It iteratively performs selection, expansion, simulation, and backpropagation to update Q and expand the tree. After a predefined number of iterations, the path with the highest reward is selected as the final prompt p.\nSelection: At each iteration, the process starts from the root node s0 and traverses each layer of the tree, using Upper Confidence bounds applied to Trees (UCT) to select child nodes until reaching a leaf node.\n\\[a_t = arg \\underset{a \\in A(s_t)}{max} \\Big[Q(s_t, a) + k \\sqrt{\\frac{ln N(s_t)}{N(ch(s_t, a))}}\\Big],\\]\nwhere A(st) and N(st) represent the action set and visit count of node st, respectively, while ch(st, a) denotes the child node generated by action \\(a^*\\). The constant k is a hyper-parameter. Q(st, \\(a^*\\)) denotes the average accuracy/reward of node st on the validation set N.\nExpansion: Through the MDP, new child nodes are added to the selected leaf node. However, during expansion, the number of samples used to generate error feedback (action) are limited, making it particularly prone to injecting instance-specific details into the prompt (state).\nTo address this limitation, we developed Batchprompt, as shown in Fig. 4b, which processes multiple samples in a single prompt to generate multiple error feedbacks. Additionally, as illustrated in Fig. 4c, we created a MemoryBank for MCTS that not only stores a large volume of historical error feedback but also uses LLM to summarize errors with the prompt, \u201cSummarize the reasons and suggestions, focusing on key information.\u201d This forms a hierarchical memory, providing global guidance. Third, to increase the probability of generating successful prompts, we not only consider the current best prompt but also perform local exploration around it through Resampling. Specifically, we instruct the LLM to generate semantically similar variants with the prompt, \u201cGenerate num variants of the prompt while preserving the original meaning.\u201d\nSimulation: It is used to simulates the future trajectory of the selected node during expansion. To simplify, we directly generate multiple actions and select the node with the highest reward to proceed to the next layer of the tree structure.\nBackpropagation: Upon reaching the maximum depth, we backpropagate the rewards along the path from the root node to the terminal node and update the Q function.\n\\[Q^*(s_t, a_t) = \\frac{1}{Z} \\sum_{\\zeta = 1}^Z \\sum_{s' \\in S_{\\zeta}, a' \\in A_{a_t}} \\Upsilon (s', a').\\]\nwhere Z represents the number of future trajectories originating from st, \\(S_{\\zeta}\\) and \\(A_{\\zeta}\\) denote the \u03da-th state and action list in these trajectories, respectively. The MCTS algorithm and the specific meta-prompts for LLM, Memory-Bank, BatchBrompt, and Resampling can be found in Appendix A.\nAdaptive Discriminator Modeling\nFeature Representation Given an image I, we extract visual features using VGG-19 (Simonyan and Zisserman 2014). These features are then mapped to a d-dimensional space via a fully connected (FC) layer. The features for visual are represented as \\(V = [V_1, V_2,..., V_r] \\in R^{r \\times d}\\). For a given text T and comment C, we use a pre-trained BERT (Devlin et al. 2018) to extract word embeddings. These embeddings are also mapped to a d-dimensional space through a FC layer. The features for text and comment are represented as \\(T = [t_1, t_2,..., t_n] \\in R^{n \\times d}\\) and \\(C = [c_1, c_2,..., c_m] \\in R^{m \\times d}\\), respectively.\nAdaptive Comment Aggregation Module Due to issues such as hallucination resulting negative comments from LLM, which can affect training and inference, we propose an Adaptive Comment Aggregation Module. It includes two components: Comment Aggregation and Gated Fusion.\nComment Aggregation: We use text to identify key information in comments, facilitating positive passing between text and comments. Firstly, compute the affinity matrix between them.\n\\[A = (W_c C)(W_t T)^T,\\]\nwhere Wc and Wt are learnable weight matrices. The aggregated information flow from the comments is then summarized using the affinity matrix A:\n\\[\\tilde{C} = softmax(\\frac{A}{\\sqrt{d}}) C.\\]\nNext, we perform comment fusion.\nGated Fusion: We calculate the corresponding gate as:\n\\[g_i = \\sigma(t_i \\oplus \\tilde{c}_i), i \\in \\{1, ..., n\\},\\]\nwhere \u2295 denotes the element-wise product, \u03c3(\u00b7) represents the activation function, and \\(g_i \\in R^d\\) is the fusion gate, which promotes the integration of positive comments while suppressing negative ones. All gates can be represented as \\(G_t = [g_1,..., g_n] \\in R^{n \\times d}\\). Meanwhile, to retain the original feature of text that not requiring fusion, the fused features are further combined with the original features through residual connections:\n\\[T = W_f(G_t \\oplus (T \\oplus \\tilde{C})) + T,\\]\nwhere Wf is a learnable weight matrices, and \u2295 denotes element-wise add.\nDiscriminator As previously mentioned, we design four discriminators to detect different deception patterns. Since both images and text display these patterns, we primarily implement image-based versions.\nReLU Discriminator: Intuitively, a complex discriminator is unnecessary for detecting fake news with exaggerated emotions. Instead, we designed a simple ReLU-based discriminator to retain essential distinguishing features, formulated as: \\(H_0(V) = ReLU(V)\\).\nFrequency Domain Discriminator: To detect manipulated images or texts, we apply the Fast Fourier Transform (FFT) to convert spatial domain features into the frequency domain, enabling the detection of anomalous frequency components and identifying potential manipulations.\n\\[V[k] / T[k] = F_m(v_i / t_i) = \\sum_{i=0}^{n-1/m-1} (v_i / t_i) e^{-j2\\pi k i / m},\\]\nwhere \\(V \\in R^{r \\times d}\\) and \\(T \\in R^{n \\times d}\\) represent the complex vectors, and V[k] and T[k] are the spectrum of \\(v_i\\) and \\(t_i\\) at the frequency 2k and 2k, respectively. Inspired by (Lao et al. 2024), we enhance the image modality discrimination cues through textual features while suppressing irrelevant information. First, the frequency features T are pooled, and then a Temporal Convolutional Network (TCN) is used to extract important information from the text context, thereby filtering out irrelevant discriminative cues in the images:\n\\[\\hat{V} = V \\odot TCN(Avg (T \\odot W_x)),\\]\nwhere Wx is a trainable parameter matrix. Finally, we use the Inverse Fast Fourier Transform (IFFT) to restore the frequency domain features of image back to the spatial domain:\n\\[\\tilde{V} \\leftarrow F^{-1}(\\hat{V}).\\]\nTherefore, the above operations can be summarized as: \\(H_1(V, T) = [\\tilde{v}_1,..., \\tilde{v}_r]\\).\nLogical Discriminator: It is used to assess the contextual consistency between images and text (e.g., semantic and syntactic coherence of the text or global consistency of image features). We employ the multi-head attention mechanism to capture contextual dependencies across different subspaces feature of the image.\n\\[MultiHead(V) = Concat (head_1,..., head_h) W_o,\\]\nwhere the h-th head is represented as headh, and Wo denotes a learnable parameter matrix. Each attention head performs self-attention by computing the query (Q = Wq \u00d7 V), key (K = wk \u00d7 V), and value (V = wv \u00d7 V) vectors:\n\\[Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d}}) V.\\]\nNext, we use a feed-forward network (FFN) to combine the features produced by different head. The above operations can be summarized as: \\(H_2(V) = FFN (MultiHead(V))\\).\nSemantic Discriminator: It discriminates fake news by identifying fine-grained differences between image and text using cross-attention. First, compute the contextual vectors of the text that correspond to the image segments:\n\\[\\Tau_V = softmax(\\frac{[W_q V] [W_k T]^T}{\\sqrt{d}}) [W_v T].\\]\nTo finely identify differences between image and text, we refine the discriminative cues between image and text features. First, map the contextual vector \\(\\Tau_V\\) to generate scaling vector \\(\\beta_V\\) and shifting vector \\(\\gamma_V\\).\n\\[\\{\n    \\begin{aligned}\n        &\\gamma_V = \\sigma (W_{\\gamma} (\\Tau_V)), \\\\\n        &\\beta_V = W_{\\beta} (\\Tau_V),\n    \\end{aligned}\n\\]\nwhere \\(W_{\\gamma}\\) and \\(W_{\\beta}\\) is learnable parameter matrices and \u03c3 is activation function. Then, we compute the refined features using \\(\\gamma_V\\), \\(\\beta_V\\), MLP and residual connections.\n\\[\\hat{V}_i = MLP (V \\odot \\gamma_V + \\beta_V) + V.\\]\nThe above operations can be summarized as: \\(H_3(V, T) = [\\hat{V}_1,..., \\hat{V}_r]\\).\nSoft Router To adaptively utilize different discriminators for exploring optimal detection models, we expanded the proposed discriminators both in depth and width. The input image feature for the i-th discriminator is:\n\\[\\begin{aligned}\nV_i^{(l)} = \\{\n    \\begin{array}{ll}\nV, & l=0, \\\\\n    \\sum_{j=0}^{S_{l-1}} \\pi_{ji}^{(l-1)} \\cdot H_j^{(l-1)}, & l \\leq L,\n    \\end{array}\n\\end{aligned}\\]\nwhere \\(S^{(l)}\\) represents the total number of discriminators in each layer, \\(H_j^{(l-1)} \\in R^{r \\times d}\\) denotes the output of the j-th discriminator in the (l \u2013 1)-th layer, and \\(\\pi_{ji}^{(l-1)}\\) indicates the path probability from the j-th discriminator in the (l \u2013 1)-th layer to the i-th discriminator in the l-th layer:\n\\[\\pi_{ji}^{(l)} = ReLU \\{ [MLP (Avg(V))]_{ji} \\}.\\]\nwhen routing ends, the output feature is \\(X = V^{(L)}\\). To preserve the discriminative cues, we apply softmax pooling to X to obtain the final feature \\(X^*\\), which is then fed into the prediction layer to output the probability of the news being fake (Wu et al. 2021).\n\\[\\hat{y} = softmax (max (0, X^* W) W_o),\\]\nwhere W and Wo are learnable parameter matrices.\nObjective Function\nFor balanced datasets like Weibo and Weibo21, we use standard cross-entropy loss for training. For the imbalanced dataset like GossipCop, we have improved the traditional cross-entropy loss accordingly.\n\\[L = - \\frac{1}{N} \\sum_{i=1}^N y_i log (\\hat{y}_i) + (1 - y_i) \\hat{y} log (1 - \\hat{y})\\]\nIn \\((1 - y_i) \\hat{y} log(1 - \\hat{y})\\), the \\(\\hat{y}\\) reduces the loss value for easily recognizable true news, thus compelling the training process to focus more on challenging true and false news samples."}, {"title": "Experiments", "content": "Datasets Experiments were conducted on three widely used datasets: Weibo (Jin et al. 2017), Weibo-21 (Nan et al. 2021), and GossipCop (Shu et al. 2020). The Weibo dataset consists of 3,783 fake news and 3,749 real news samples for training, and 1,000 fake news and 996 real news samples for testing. The Weibo-21 dataset contains 4,487 fake news and 4,640 real news samples. For fair comparison, the dataset was split into training and testing sets at a 9:1 ratio, following the previously established data protocol (Ying et al. 2023). The GossipCop dataset is imbalanced, with 2,036 fake news and 7,974 real news samples for training, and 545 fake news and 2,285 real news samples for testing. For prompt optimization, 30% of the samples from each dataset were randomly selected for validation.\nImplementation Details We implemented DAAD using PyTorch 2.3.1 and conducted all experiments on a single NVIDIA Tesla A100 GPU. For text and comment feature extraction, we used 'bert-base-chinese' for the Weibo and Weibo-21 datasets with a maximum sequence length of 160, and 'bert-base-uncased' for the GossipCop dataset with a maximum sequence length of 394. Images were resized to 224 \u00d7 224 to match the input dimensions of the pre-trained VGG-19 model. In Prompt Optimization, the models M for the Chinese and English datasets were implemented with Chinese Llama3 and Llama3.1, respectively, while M' and M\" were implemented with GPT-3.5. The maximum tree depth was set to 10 and the exploration constant k was set to 2.5, with a batch size of 32, and training was conducted for 16 epochs. In Adaptive Discriminator Modeling, the dimensions of image and text features d were set to 256, with the number of heads set to 4 and a dropout rate of 0.5. We trained the model using AdaBelief (Zhuang et al. 2020) for 50 epochs with a batch size of 32 and an initial learning rate of 1e-4. In addition, for the GossipCop dataset, the hyperparameter A in the adjusted cross-entropy loss (Eq. 19) is empirically set to 3. Additional implementation details can be found in the code.\nPerformance Comparison\nTable 1 presents a comparison of our proposed DAAD method with mainstream approaches on the Weibo, Weibo-21, and GossipCop datasets. We evaluate the performance using Accuracy, Precision, Recall, and F1 score. The average accuracy of DAAD on Weibo, Weibo-21, and GossipCop is 93.2%, 94.2%, and 90.4%, respectively, representing improvements of 3.1%, 1.3%, and 0.9% over state-of-the-art models. Specifically, on the Weibo dataset, DAAD achieves the best performance across all metrics, with a notable increase of 4.3% in precision and 3.0% in F1 score for real news detection. Compared NSLM, Despite it relying on image tampering and cross-modal consistency for fake news detection, DAAD achieves a 4.7% improvement by considering a more comprehensive set of deceit patterns and employing adaptive routing that uses complementary and diverse discriminators. On the Weibo-21 and GossipCop datasets, although BMR leverages multi-view features of the news, DAAD achieves better performance through its more comprehensive and adaptive discriminators. On Gossip-Cop, DAAD improves average accuracy by 4.8% over AKA-Fake. While AKA-Fake introduces a dynamic knowledge via reinforcement learning, our method brings a greater enhancement by integrating dynamic comment from LLMs through prompt optimization. These improvements demonstrate the effectiveness of our adaptive discriminators and the integration of dynamic comments from the LLM through prompt optimization.\nAblation Study\nWe also conduct a detailed ablation study on the Chinese Weibo dataset and the English GossipCop dataset to examine the effectiveness of the proposed modules.\nHuman prompt v.s. Optimized prompt Table 2 illustrates the effectiveness of Prompt Optimization. DAAD refers to the use of MCTS to optimize prompts, incorporating dynamic and more detailed comments from LLMs. DAAD w/o MCTS indicates the"}, {"title": "Conclusion and Future Work", "content": "In this work, we address the issue of insufficient flexibility in existing knowledge-based and semantics-based fake news detection methods by proposing a Dynamic Analysis and Adaptive Discriminator (DAAD) approach. First, we introduce domain-specific comments from Large language models (LLMs) using Monte Carlo Tree Search (MCTS), and mitigated the risk of getting trapped in local minima during optimization through MemoryBank, Batchprompt, and Resampling. Second, we define four typical deceit patterns and design corresponding discriminators, allowing for flexible exploration of optimal detection models through dynamic routing. Finally, extensive experiments on three mainstream datasets demonstrate the superiority of our method.\nAlthough DAAD has demonstrated promising performance, it still has several limitations. First, while domain-specific prompts are introduced through prompt optimization, the meta-prompts used during the optimization process are manually defined, which may lead to suboptimal prompts. Future research could also focus on incorporating more detailed comments tailored to different news domains. Additionally, although various deception patterns and adaptive discriminators have been designed, these are still predefined. Future work could explore how to automatically discover more effective deception patterns and discriminators from the different domain and extend these methods to areas beyond fake news detection, such as Sarcasm and Harmful Meme Detection, aiming to develop a unified detection model."}, {"title": "A. Addtional Details for Prompt Optimization", "content": "A.1. Implementation Details\n1. Model Versions\n\u2022 GPT-3.5: gpt-3.5-turbo-0125\n\u2022 LLama 3: Llama3-8B-Chinese-Chat.\n\u2022 LLama 3.1: meta-llama/Meta-Llama-3.1-8B-Instruct.\nFor the Chinese datasets such as Weibo and Weibo-21", "input format": "s the actual input for model M used to generate error examples. The 'error string' refers to the format of each error example. The 'error feedback' serves as the actual input for model M'", "state transit": "uides the optimizer model M\" to perform state transitions (generate new prompts)", "prompt": "nPlease use your knowledge of journalism to determine the authenticity of the input news or social media message", "format": "n{prompt"}, "nNews or social media message: {news}.\nOptions:\nA:True\nB. False\nAt the end show the answer option between  and .error string:\nThe index of error news or social media message is: News or social media message: {news}\nBut this prompt gets the following examples wrong: {error strings}\nPlease carefully examine each wrong example and its incorrect answer. Provide detailed reasons explaining why the prompt leads to the incorrect answer.\nFinally, based on these reasons, summarize and list specific suggestions for improving the prompt according to the following guidelines:\n1.The suggestions should focus on how to modify the prompt to help the model classify better, without relying on external knowledge or inputting more information.\n2. The suggestions should avoid overfitting specific incorrect examples, ensuring generalizability to different news examples.\nstate transit:\nI am writing prompts for a large language model designed to detect fake news.\nMy current prompt is: {prompt}\nBut this prompt gets the following examples wrong: {error strings}\nBased on these errors, the problems with this prompt and the reasons are: {error feedback}\nThere is a list of former prompts including the current prompt, and each prompt is modified from its former prompts: {trajectory prompts}\nPlease consider the following potential causes of model classification errors and suggestions: {Memory}\nBased on the above information, please write {steps_per_gradient} new prompts following these guidelines:\n1. The new prompts should solve the current prompt's problems.\n2. The new prompts should consider the list of prompts and evolve based on the current prompt.\n3. Each new prompt should be wrapped with  and .\nThe new prompts are:\n2. Meta-prompts in MemoryBank\nSummarize the following reasons and suggestions concisely, focusing on key information. If there are multiple key points, summarize them in bullet points. The reason for the error and recommendation is:\nNumber :\nReasons and Suggestions: {Historical errors}\n3. Meta-prompts in Batchprompt\nTo accelerate the process and improve the efficiency of LLM utilization, Batchprompt is used for generating comments and generating error examples.\nBatchprompt for Comments Generation\n{prompt}\nYou will receive {batch_size} sentences from the news and social media message dataset as input. The format of the provided sentences is as follows:\nText 1: xxxxxx\nText 2: xxxxxx\nPlease generate the output according to the following format, with the result and reason enclosed in ["], "1": ["Result: X. Reason: xxxxxx"], "2": ["Result: X. Reason: xxxxxx"], "True": "r 'False,' and 'xxxxxx' is your reasoning for determining the authenticity of the news or social media message. and .4. Meta-prompts in Resampling\nGenerate {steps_per_gradient} variants of the prompt {prompt} while maintaining the same meaning. Each new prompt should be wrapped with  and .B. Visualize Experiment.\nB.1. Prompt Visualization\nIn this section, we present the optimized prompts for the Weibo and GossipCop datasets and provide a visual comparison between the optimized prompts and the standard manually crafted prompt, as shown in Table 6 and Table 7. Compared to the manually created prompts (highlighted in gray), our optimized prompts offer richer, domain-specific guidance (highlighted in green), and achieve superior performance. However, relying solely on the MCTS algorithm can introduce instance-specific details (highlighted in yellow), which may reduce generalization. This observation is consistent with our analysis in Table 4.\nB.2. Comment Visualization\nIn Table 8, we visualize the comments generated using optimized prompts. The results show that the comments produced by the LLM are not only effective and aligned with the fake news deceit patterns but also match the guidance provided in our optimized prompts. Additionally, we visualized the comments generated using both manually crafted prompt and optimized prompt to compare their differences"}