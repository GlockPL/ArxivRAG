{"title": "DAAD: Dynamic Analysis and Adaptive Discriminator for Fake News Detection", "authors": ["Xinqi Su", "Yawen Cui", "Ajian Liu", "Xun Lin", "Yuhao Wang", "Haochen Liang", "Wenhui Li", "Zitong Yu"], "abstract": "In current web environment, fake news spreads rapidly across online social networks, posing serious threats to society. Existing multimodal fake news detection (MFND) methods can be classified into knowledge-based and semantic-based approaches. However, these methods are overly dependent on human expertise and feedback, lacking flexibility. To address this challenge, we propose a Dynamic Analysis and Adaptive Discriminator (DAAD) approach for fake news detection. For knowledge-based methods, we introduce the Monte Carlo Tree Search (MCTS) algorithm to leverage the self-reflective capabilities of large language models (LLMs) for prompt optimization, providing richer, domain-specific details and guidance to the LLMs, while enabling more flexible integration of LLM comment on news content. For semantic-based methods, we define four typical deceit patterns: emotional exaggeration, logical inconsistency, image manipulation, and semantic inconsistency, to reveal the mechanisms behind fake news creation. To detect these patterns, we carefully design four discriminators and expand them in depth and breadth, using the soft-routing mechanism to explore optimal detection models. Experimental results on three real-world datasets demonstrate the superiority of our approach. The code will be available at: https://github.com/SuXinqi/DAAD.", "sections": [{"title": "Introduction", "content": "News is a crucial channel for the public to understand the world. With the rapid development of online social networks (OSNs) like Twitter and Weibo, the spread of fake news and distorted views has grown exponentially (Willmore 2016). This not only increases the information burden but can also cause panic, leading to significant negative impacts on society (Dong et al. 2024). To mitigate the adverse effects of fake news, automated fake news detection (FND) has become a research focus, helping network administrators prevent its spread (Ying et al. 2023).\nTweets with images are more engaging, which is why most content on current OSNs is presented in a multimodal format (Wang et al. 2023a; Wu et al. 2023). Additionally, multimodal fake news is more convincing due to its realism, and fake news creators exploit this feature to amplify their influence (Hu et al. 2022; Khattar et al. 2019). Therefore, this paper focuses on multimodal fake news detection (MFND). Many studies have been dedicated to detecting multimodal fake news and mitigating its harmful effects. Current MFND are primarily divided into two categories: (i) semantic-based, and (ii) knowledge-based methods.\nThe first category of methods argues that as technology advances, the style of fake news increasingly resembles that of real news, however MFND methods often lack critical information such as social context (Liao et al. 2023). To address this, some approaches incorporate knowledge bases (KBs) like Wikipedia (Hu et al. 2021; Tseng et al. 2022), knowledge graphs (Qian et al. 2021; Jin et al. 2022; Dun et al. 2021), and public APIs (Qi et al. 2021; Zhang et al. 2021) to determine the authenticity of news by capturing consistency with the KB or enriching the news content. However, as fake news becomes more sophisticated, static KBs struggle to provide reliable factual signals. With the emergence of large language models (LLMs) that possess strong reasoning and content understanding capabilities, community feedback indicates that LLMs' comments of news can enhance fake news detection systems (Liu et al. 2024b; Hu et al. 2024). Nonetheless, enabling LLMs to perform the intended tasks effectively remains a challenge. For instance, recent studies use prompt engineering to guide LLMs to focus on writing style and common-sense errors in news, allowing them to infer key clues (Liu et al. 2024a). However, there are two major drawbacks: 1) The prompts are manually crafted, overly reliant on human expertise, leading to limited generalization (Ma et al. 2024); 2) LLMs are highly sensitive to prompt formats, where semantically similar prompts can yield vastly different performances (Kojima et al. 2022; Wei et al. 2023), and the optimal prompt may be model- and task-specific (Lin et al. 2024; Ma et al. 2023; Hao et al. 2024). Thus, suitable prompts are crucial for LLMs to effectively analyze news content (Reynolds and McDonell 2021).\nThe second category of methods focuses on capturing the semantic features of news content to distinguish between real and fake news, which can be broadly divided into three subapproaches: 1) Research indicates that there are significant differences in the expression style of credible news versus fake news (Rashkin et al. 2017; Shu et al. 2017; Horne and Adali 2017). Consequently, some methods detect fake news by analyzing elements such as sentiment (Chen et al. 2023; Kwak, An, and Ahn 2020; Zhang et al. 2023) and logic (e.g., mismatches between semantics and grammar) (Li, Zhang, and Malthouse 2024; Xiao et al. 2024). 2) As shown in Fig. 2c, some fake news creators use techniques to manipulate images, leading to methods that focus on detecting image manipulation to identify fake news (Lao et al. 2024; Dong et al. 2024). 3) As illustrated in Fig. 2d, another common methods to creating multimodal fake news is the incorrect reuse of outdated images. Therefore, some methods assess the semantic consistency between images and text to detect fake news (Wang et al. 2023a; Wu, Liu, and Zhang 2023; Wu et al. 2023). Despite the significant performance improvements achieved by these subapproaches from different perspectives (Wu et al. 2021), two key limitations remain: 1) The detection models are manually crafted and rely on human knowledge and feedback, meaning that the optimal detection models have yet to be fully developed; 2) Existing detection methods are static, applying the same detection model to all samples, which prevents different detection patterns from complementing each other.\nTo address the aforementioned challenges, we propose a Dynamic Analysis and Adaptive Discriminator (DAAD) for fake news detection. First, we introduce the Monte Carlo Tree Search (MCTS) algorithm, strategically leveraging the self-reflective capabilities and error feedback of LLMs to optimize prompts, thereby introducing effective comments. To prevent the optimization process from getting trapped in local minima, we designed a 'MemoryBank' component that not only stores historical errors during the search but also compresses them into concise global guidance, encouraging LLMs to adjust prompts from a bird's eye view. Additionally, we use prompt resampling and batchprompt to expand both the prompt and sample spaces, respectively, further reducing the risk of local minima. Next, we meticulously designed four discriminators to target different deceit patterns: the ReLU and logical discriminator detect the reasonableness of emotion and contextual logic, respectively; the frequency-domain discriminator focuses on identifying image manipulation; and the semantic discriminator evaluates the alignment between image and text semantics. Finally, we expanded these discriminators in both depth and breadth, combining them with a soft-routing to adaptively explore the optimal detection model.\n\u2022 We propose a joint prompt optimization framework, namely DAAD, based on MCTS and the MemoryBank component to enhance the effectiveness and controllability of LLMs, while mitigating local minima during the optimization process.\n\u2022 We design four discriminators and employ a soft-routing mechanism to adaptively explore the optimal fake news detection model.\n\u2022 Extensive qualitative and quantitative experiments on three benchmarks, i.e., Weibo, Weibo21, and GossipCop, demonstrate the superiority of our approach."}, {"title": "Related Work", "content": "This category of methods employs external knowledge to aid in the detection of fake news. Some approaches use related entities from knowledge graphs (KGs) to enrich news content (Qian et al. 2021; Dun et al. 2021; Jin et al. 2022; Zhang et al. 2024). Other methods incorporate evidence from external knowledge bases (KBs) and capture their consistency to help distinguish between true and fake news (Hu et al. 2021; Qi et al. 2021; Liao et al. 2023). With the advent of large language models (LLMs), many approaches leverage their powerful knowledge and reasoning capabilities to analyze or enrich news content. For example, FakeNewsGPT4 (Liu et al. 2024b) and LEMMA (Xuan et al. 2024) utilize specific knowledge from LLMs to provide supplementary information for fake news detectors. DELL (Wan et al. 2024) and ARG (Hu et al. 2024) introduce LLM-based analysis to support decision-making in fake news detection. However, these"}, {"title": "Methodology", "content": "The architecture of our proposed DAAD, as shown in Fig. 3, predicts the label based on the multimodal news content and the comment provided by the LLM.\nDefinition 1 (Multimodal Fake News Detection): A piece of multimodal news is defined as N = {I, T}, where I and T represent the image and text of the news, respectively. Our goal is to learn a probability distribution P(Y|I, C) that can effectively distinguish between fake and real news, where y = 1 indicates fake news, y = 0 indicates real news, and C denotes the comments from LLMs about the news.\nHere, we will detail the process of prompt optimization to generate expert-level prompts in the news domain and produce insightful comments.\nDefinition 2 (Prompt Optimization): Given a LLM M, our goal is to find a prompt p that generates the best comments C\u25ca for a small set of sampled news N\u25ca = {T}. Formally, this can be framed as an optimization problem. We aim to identify a prompt p within the natural language space S when M is prompted with p and T, the expected value of each sample's comment score \u03c6(\u03c1, \u03a4\u03bf) is maximized.\np* = arg max E\u03c4\u03bf [\u03c6(\u03c1, \u03a4\u03bf)].\nPES\nThe Markov Decision Process (MDP) We employ the reinforcement learning (RL) (Li 2017) for prompt optimization, modeling it as a MDP represented by a 4-tuple (S, \u0391, \u0393, \u03a5), where S denotes the state space, A represents the action space, \u0393 is the state transition function \u0393 : S \u00d7 A \u2192 S, and Y is the reward function Y : S \u00d7 A \u2192 R. To incorporate domain-specific knowledge into prompt revisions meaningfully, we propose action generation based on error feedback. Fig. 4a illustrates a state transition process. In the current state st, the agent generates an action at based on at ~ PM'(\u0430 | St, M'), where M' is a LLM used for action generation. Subsequently, the agent obtains the new state st+1 based on the transition function St+1 ~ PM\" (s | st, at, M\"), where M\" is a LLM used for state transition. The reward Y is defined as the accuracy on the validation set N.\nEnhancing Prompt Optimization To enhance the agent's exploration of the prompt p, we employ Monte Carlo Tree Search (MCTS) for optimization, inspired by (Wang et al. 2023b). Specifically, MCTS maintains a state-action value function Q: S \u00d7 A \u2192 R to estimate the potential rewards for state-action pairs along various paths. It iteratively performs selection, expansion, simulation, and backpropagation to update Q and expand the tree. After a predefined number of iterations, the path with the highest reward is selected as the final prompt p.\nSelection: At each iteration, the process starts from the root node so and traverses each layer of the tree, using Upper Confidence bounds applied to Trees (UCT) to select child nodes until reaching a leaf node.\nat = arg max Q (st, at) + k\naEA(st) \u221a(ln N (st))\n\u221a(N (ch (st, a'))\nwhere A(st) and N(st) represent the action set and visit count of node st, respectively, while ch(st, a) denotes the child node generated by action a*. The constant k is a hyper-parameter. Q(st, a) denotes the average accuracy/reward of node st on the validation set N.\nExpansion: Through the MDP, new child nodes are added to the selected leaf node. However, during expansion, the number of samples used to generate error feedback (action) are limited, making it particularly prone to injecting instance-specific details into the prompt (state).\nTo address this limitation, we developed Batchprompt, as shown in Fig. 4b, which processes multiple samples in a single prompt to generate multiple error feedbacks. Additionally, as illustrated in Fig. 4c, we created a MemoryBank for MCTS that not only stores a large volume of historical error feedback but also uses LLM to summarize errors with the prompt, \u201cSummarize the reasons and suggestions, focusing on key information.\u201d This forms a hierarchical memory, providing global guidance. Third, to increase the probability of generating successful prompts, we not only consider the current best prompt but also perform local exploration around it through Resampling. Specifically, we instruct the LLM to generate semantically similar variants with the prompt, \u201cGenerate num variants of the prompt while preserving the original meaning.\u201d\nSimulation: It is used to simulates the future trajectory of the selected node during expansion. To simplify, we directly generate multiple actions and select the node with the highest reward to proceed to the next layer of the tree structure.\nBackpropagation: Upon reaching the maximum depth, we backpropagate the rewards along the path from the root node to the terminal node and update the Q function.\nQ* (St, at) =\n\u03a3\u03b6=1 \u03a3s'ES,a' Aat Y (s', a')\nwhere Z represents the number of future trajectories originating from st, S and As denote the \u03da-th state and action list in these trajectories, respectively. The MCTS algorithm and the specific meta-prompts for LLM, Memory-Bank, BatchBrompt, and Resampling can be found in Appendix A.\nFeature Representation Given an image I, we extract visual features using VGG-19 (Simonyan and Zisserman 2014). These features are then mapped to a d-dimensional space via a fully connected (FC) layer. The features for visual are represented as V = [V1, V2,..., Vr] \u2208 Rrxd. For a given text T and comment C, we use a pre-trained BERT (Devlin et al. 2018) to extract word embeddings. These embeddings are also mapped to a d-dimensional space through a FC layer. The features for text and comment are represented as T = [t1, t2,..., tn] \u2208 Rnxd and C = [C1, C2,..., Cm] \u2208 Rmxd, respectively.\nAdaptive Comment Aggregation Module Due to issues such as hallucination resulting negative comments from LLM, which can affect training and inference, we propose an Adaptive Comment Aggregation Module, it includes two components: Comment Aggregation and Gated Fusion.\nComment Aggregation: We use text to identify key information in comments, facilitating positive passing between text and comments. Firstly, compute the affinity matrix between them.\nA = (WC) (W+T),\nwhere We and Wt are learnable weight matrices. The aggregated information flow from the comments is then summarized using the affinity matrix A:\n\u010c = softmax(A^T )C.\nNext, we perform comment fusion.\nwhere denotes the element-wise product, \u03c3(\u00b7) represents the ac-tivation function, and gi \u2208 Rd is the fusion gate, which promotes the integration of positive comments while suppressing negative ones. All gates can be represented as Gt = [g1,..., gn] \u2208 Rnxd. Meanwhile, to retain the original feature of text that not requiring fusion, the fused features are further combined with the original features through residual connections:\nT = Wf(Gt (T \u2295 \u010c)) + T,\nwhere Wf is a learnable weight matrices, and \u2295 denotes element-wise add.\nDiscriminator As previously mentioned, we design four discriminators to detect different deception patterns. Since both images and text display these patterns, we primarily implement image-based versions.\nReLU Discriminator: Intuitively, a complex discriminator is unnecessary for detecting fake news with exaggerated emotions. Instead, we designed a simple ReLU-based discriminator to retain essential distinguishing features, formulated as: Ho (V) = ReLU(V).\nFrequency Domain Discriminator: To detect manipulated images or texts, we apply the Fast Fourier Transform (FFT) to convert spatial domain features into the frequency domain, enabling the detection of anomalous frequency components and identifying potential manipulations.\nV[k]/T[k] = Fm (vi/ti) = \u03a3 (vi/ti) e-12m,\ni=0\nwhere V \u2208 Rr\u00d7d and T \u2208 Rn\u00d7d represent the complex vectors, and V[k] and T[k] are the spectrum of v\u00bd and ti at the frequency 2k and 2k, respectively. Inspired by (Lao et al. 2024), we enhance the image modality discrimination cues through textual features while suppressing irrelevant information. First, the frequency features T are pooled, and then a Temporal Convolutional Network (TCN) is used to extract important information from the text context, thereby filtering out irrelevant discriminative cues in the images:\nV = V \u2299 TCN (Avg (T\u2299 Wx)),\nwhere Wx is a trainable parameter matrix. Finally, we use the Inverse Fast Fourier Transform (IFFT) to restore the frequency domain features of image back to the spatial domain:\nV\u2190 F\u00b9(V).\nTherefore, the above operations can be summarized as: H1 (V,T) = [1,..., \u1fe6r].\nLogical Discriminator: It is used to assess the contextual consistency between images and text (e.g., semantic and syntactic coherence of the text or global consistency of image features). We employ the multi-head attention mechanism to capture contextual dependencies across different subspaces feature of the image.\nMultiHead(V) = Concat (head1,..., head h) Wo,\nwhere the h-th head is represented as headh, and Wo denotes a learnable parameter matrix. Each attention head performs self-attention by computing the query (Q = Wq \u00d7 V), key (K = wk \u00d7 V), and value (V = wv \u00d7 V) vectors:\nAttention(Q, K, V) = softmax (\u221a) V.\nNext, we use a feed-forward network (FFN) to combine the features produced by different head. The above operations can be summarized as: H2(V) = FFN (MultiHead(V)).\nIt discriminates fake news by identifying fine-grained differences between image and text using cross-attention. First, compute the contextual vectors of the text that correspond to the image segments:\n\u03a4\u03bf = softmax( [WT] ) [WT].\nTo finely identify differences between image and text, we refine the discriminative cues between image and text features. First, map the contextual vector Tu to generate scaling vector \u1e9e and shifting vector \u03b3\u03c5.\n\u03b3\u03c5 = \u03c3 (W (T)),\n\u03b2\u03c5 = W\u03b2 (\u03a4),\nwhere Wy and WB is learnable parameter matrices and o is activation function. Then, we compute the refined features using \u03b3\u03c5, \u03b2\u03c5, MLP and residual connections.\nV\u2081 = MLP (V\u00a9 \u03b3\u03c5 + \u03b2\u03c5) + V.\nThe above operations can be summarized as: H3(V,T) = [V1,..., Ur].\nSoft Router To adaptively utilize different discriminators for exploring optimal detection models, we expanded the proposed discriminators both in depth and width. The input image feature for the i-th discriminator is:\nV (1)\ni =\n50-1(1-1)(1-1)\n\u03a3j=0 Tji\n(1)\nV j , l = 0,\n0, l > 0,\nwhere represents the total number of discriminators in each layer, H(l-1) \u2208 Rr\u00d7d denotes the output of the j-th discriminator in the (l \u2013 1)-th layer, and 7-1) indicates the path probability Tji from the j-th discriminator in the (l \u2013 1)-th layer to the i-th discriminator in the l-th layer:\nT = ReLU { [MLP (Avg(V)))]},\nwhen routing ends, the output feature is X = V\u00b9. To preserve the discriminative cues, we apply softmax pooling to X to obtain the final feature X*, which is then fed into the prediction layer to output the probability of the news being fake (Wu et al. 2021).\n\u0177 = softmax (max (0, X*W) W\uff61),\nwhere W and Ws are learnable parameter matrices.\nFor balanced datasets like Weibo and Weibo21, we use standard cross-entropy loss for training. For the imbalanced dataset like GossipCop, we have improved the traditional cross-entropy loss accordingly.\nL = - \u03a3 yi log (y) + (1 \u2212 yi) \u0177^ log (1 \u2212 \u0177)\nIn (1 \u2013 yi)\u0177 log(1 \u2013 \u0177), the \u0177 reduces the loss value for easily recognizable true news, thus compelling the training process to focus more on challenging true and false news samples."}, {"title": "Experiments", "content": "Experiments were conducted on three widely used datasets: Weibo (Jin et al. 2017), Weibo-21 (Nan et al. 2021), and GossipCop (Shu et al. 2020). The Weibo dataset consists of 3,783 fake news and 3,749 real news samples for training, and 1,000 fake news and 996 real news samples for testing. The Weibo-21 dataset contains 4,487 fake news and 4,640 real news samples. For fair comparison, the dataset was split into training and testing sets at a 9:1 ratio, following the previously established data protocol (Ying et al. 2023). The GossipCop dataset is imbalanced, with 2,036 fake news and 7,974 real news samples for training, and 545 fake news and 2,285 real news samples for testing. For prompt optimization, 30% of the samples from each dataset were randomly selected for validation.\nWe implemented DAAD using Py-Torch 2.3.1 and conducted all experiments on a single NVIDIA Tesla A100 GPU. For text and comment feature extraction, we used 'bert-base-chinese' for the Weibo and Weibo-21 datasets with a maximum sequence length of 160, and 'bert-base-uncased' for the GossipCop dataset with a maximum sequence length of 394. Images were resized to 224 \u00d7 224 to match the input dimensions of the pre-trained VGG-19 model. In Prompt Optimization, the models M for the Chinese and English datasets were implemented with Chinese Llama3 and Llama3.1, respectively, while M' and M\" were implemented with GPT-3.5. The maximum tree depth was set to 10 and the exploration constant k was set to 2.5, with a batch size of 32, and training was conducted for 16 epochs. In Adaptive Discriminator Modeling, the dimensions of image and text features d were set to 256, with the number of heads set to 4 and a dropout rate of 0.5. We trained the model using AdaBelief (Zhuang et al. 2020) for 50 epochs with a batch size of 32 and an initial learning rate of 1e-4. In addition, for the GossipCop dataset, the hyperparameter A in the adjusted cross-entropy loss (Eq. 19) is empirically set to 3. Additional implementation details can be found in the code.\nTable 1 presents a comparison of our proposed DAAD method with mainstream approaches on the Weibo, Weibo-21, and GossipCop datasets. We evaluate the performance using Accuracy, Precision, Recall, and F1 score. The average accuracy of DAAD on Weibo, Weibo-21, and GossipCop is 93.2%, 94.2%, and 90.4%, respectively, representing improvements of 3.1%, 1.3%, and 0.9% over state-of-the-art models. Specifically, on the Weibo dataset, DAAD achieves the best performance across all metrics, with a notable increase of 4.3% in precision and 3.0% in F1 score for real news detection. Compared NSLM, Despite it relying on image tampering and cross-modal consistency for fake news detection, DAAD achieves a 4.7% improvement by considering a more comprehensive set of deceit patterns and employing adaptive routing that uses complementary and diverse discriminators. On the Weibo-21 and GossipCop datasets, although BMR leverages multi-view features of the news, DAAD achieves better performance through its more comprehensive and adaptive discriminators. On Gossip-Cop, DAAD improves average accuracy by 4.8% over AKA-Fake. While AKA-Fake introduces a dynamic knowledge via reinforcement learning, our method brings a greater enhancement by integrating dynamic comment from LLMs through prompt optimization. These improvements demonstrate the effectiveness of our adaptive discriminators and the integration of dynamic comments from the LLM through prompt optimization."}, {"title": "Ablation Study", "content": "We also conduct a detailed ablation study on the Chinese Weibo dataset and the English GossipCop dataset to examine the effectiveness of the proposed modules.\nTable 2 illustrates the effectiveness of Prompt Optimization. DAAD refers to the use of MCTS to optimize prompts, incorporating dynamic and more detailed comments from LLMs. DAAD w/o MCTS indicates the use of manually crafted prompts. Compared to DAAD, DAAD w/o MCTS shows a 0.6% decrease in average accuracy across both datasets and a 2.8% decrease in F1 score for fake news detection on GossipCop. This demonstrates the importance of crafting domain-specific prompts when integrating comments from LLMs. The limited performance improvement may be attributed to the limitations of the open-source large models.\nThe impact of different discriminators is also shown in Table 2, where 'Full' represents the use of all discriminators simultaneously but not comment from LLM. 'w/o RD,' 'w/o FDD,' 'w/o LD,' and 'w/o SD' represent the removal of the ReLU-, Frequency Domain-, Logical-, and Semantic Discriminator, respectively. Compared to 'Full,' the performance of 'w/o SD' significantly decreases, with average accuracy dropping by 3.4% and 1.1% on the two datasets, respectively. This underscores the importance of integrating multimodal features for fake news detection. Additionally, the performance of 'w/o FDD' and 'w/o LD' also declines on both datasets, indicating that examining the frequency domain characteristics and logical consistency of news is effective. Overall, the removal of any discriminator results in a certain degree of performance drop, validating the effectiveness of all four discriminators.\nTable 3 illustrates the impact of different routing types on the Weibo dataset. 'Random' represents random routing, where is sampled from a uniform distribution. 'Hard' refers to discrete routing values of 7, meaning the output of the previous discriminator is routed to only one subsequent discriminator. 'Soft' indicates adaptive probability values of 7 learned by the network. It can be observed that 'Random' and 'Hard' show a decrease of 3% and 2%, respectively, compared to 'Soft', validating the importance of the complementary nature of the four discriminators.\nTable 4 demonstrates the effectiveness of each component in the Prompt Optimization process. It is important to note that the accuracy shown here reflects the performance of only using the LLM to classify news. 'Human' refers to LLM classification using manually designed prompts. 'MCTS' indicates the use of MCTS-optimized prompts for news classification by LLM. 'MCTS+Memory' incorporates the MemoryBank component on top of 'MCTS'. 'MCTS+Memory+Resample' further integrates the Resampling mechanism. The results reveal that using MCTS alone leads to a slight improvement or decrease in accuracy, likely due to the introduction of instance-specific details during optimization, which may reduce generalization. In contrast, 'MCTS+Memory' achieves accuracy improvements of 3% and 8.7% on the two datasets compared to 'MCTS', highlighting the importance of the MemoryBank. The introduction of the Re-sampling mechanism further enhances performance by 1.2% and 1.8% on the two datasets, demonstrating the effectiveness of further exploration around optimized prompts. The prompts obtained with each component are provided in Appendix B.\nTable 5 presents the impact of different routing layer numbers on Weibo dataset. The results indicate that moderately increasing the number of l can fully leverage the complementary strengths of the discriminators. However, further increasing the number of routing layers leads to a slight decline in performance, likely due to the increased complexity of the path space, which makes model learning more challenging or causes data overfitting, thereby limiting performance improvements.\nTo clearly illustrate the role of each discriminator, we discretized the learned routing using \u0430 \u0442 = 0.9 threshold and visualized the paths for selected fake news cases, as shown in Fig. 5. For blatantly fabricated images (as shown in the top row), the path primarily activates only the ReLU- and frequency domain discriminators. In contrast, for more complex news, the model explores relationships between different discriminators through more intricate paths."}, {"title": "Conclusion and Future Work", "content": "In this work, we address the issue of insufficient flexibility in existing knowledge-based and semantics-based fake news detection methods by proposing a Dynamic Analysis and Adaptive Discriminator (DAAD) approach. First, we introduce domain-specific comments from Large language models (LLMs) using Monte Carlo Tree Search (MCTS), and mitigated the risk of getting trapped in local minima during optimization through MemoryBank, Batch-prompt, and Resampling. Second, we define four typical deceit patterns and design corresponding discriminators, allowing for flexible exploration of optimal detection models through dynamic routing. Finally, extensive experiments on three mainstream datasets demonstrate the superiority of our method.\nAlthough DAAD has demonstrated promising performance, it still has several limitations. First, while domain-specific prompts are introduced through prompt optimization, the meta-prompts used during the optimization process are manually defined, which may lead to suboptimal prompts. Future research could also focus on incorporating more detailed comments tailored to different news domains. Additionally, although various deception patterns and adaptive discriminators have been designed, these are still predefined. Future work could explore how to automatically discover more effective deception patterns and discriminators from the different domain and extend these methods to areas beyond fake news detection, such as Sarcasm and Harmful Meme Detection, aiming to develop a unified detection model."}]}