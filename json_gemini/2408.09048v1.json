{"title": "mRNA2vec: mRNA Embedding with Language Model in the 5'UTR-CDS for mRNA Design", "authors": ["Honggen Zhang", "Xiangrui Gao", "June Zhang", "Lipeng Lai"], "abstract": "Messenger RNA (mRNA)-based vaccines are accelerating the discovery of new drugs and revolutionizing the pharmaceutical industry. However, selecting particular mRNA sequences for vaccines and therapeutics from extensive mRNA libraries is costly. Effective mRNA therapeutics require carefully designed sequences with optimized expression levels and stability. This paper proposes a novel contextual language model (LM)-based embedding method: mRNA2vec. In contrast to existing mRNA embedding approaches, our method is based on the self-supervised teacher-student learning framework of data2vec. We jointly use the 5' untranslated region (UTR) and coding sequence (CDS) region as the input sequences. We adapt our LM-based approach specifically to mRNA by 1) considering the importance of location on the mRNA sequence with probabilistic masking, 2) using Minimum Free Energy (MFE) prediction and Secondary Structure (SS) classification as additional pretext tasks. mRNA2vec demonstrates significant improvements in translation efficiency (TE) and expression level (EL) prediction tasks in UTR compared to SOTA methods such as UTR-LM. It also gives a competitive performance in mRNA stability and protein production level tasks in CDS such as CodonBERT.", "sections": [{"title": "Introduction", "content": "Messenger RNA (mRNA)-based gene therapeutics and vaccines have emerged as pivotal tools for developing a new class of drugs, as they do not present the safety concerns associated with genomic integration (To and Cho 2021). Recently, mRNA vaccines have been utilized in treating several diseases, including COVID-19 (Jackson et al. 2020) and cancer (Zeraati et al. 2017). mRNA therapeutics facilitate the production of missing or defective proteins, providing essential supplements to cure genetic diseases (Kwon et al. 2018). To advance mRNA-based treatments further, understanding and designing the appropriate mRNA sequences is crucial. Factors such as expression level and stability significantly impact the practical application of mRNA-based therapies.\nResearchers have primarily focused on the untranslated region (UTR) and the coding sequence (CDS) region to understand the mechanisms of mRNA in protein translation Cao et al. (2021); Li et al. (2023). Studies have demonstrated that UTR can regulate mRNA translation (Akiyama and Sakakibara 2022), with extensive exploration of the 5' UTR, which precedes the coding sequences (Chu et al. 2024; Wieder et al. 2024). The CDS region contains codons that are translated into amino acids, so codon optimization directly impacts mRNA translation.\nTechnological advancements using machine learning (particularly language models) with biology have enabled new methods to decode mRNA sequences (Diez et al. 2022; Cao et al. 2021). Ribosomes read sequences from left to right, similar to text sequences, suggesting that mRNA sequences could be modeled using language models and self-supervised learning techniques. Mapping mRNA to a high-dimensional representation space has the potential to enhance our understanding of its functions. However, significant differences exist between mRNA and natural text. For example, mRNA has a more limited vocabulary, consisting of 4 kinds of nucleotides and twenty amino acids. Therefore it is important to account for these differences when adapting language models for mRNA embedding.\nIn this paper, we propose an mRNA embedding method: mRNA2vec. Unlike previous research that examined the 5' UTR and CDS separately, we combine the 5' UTR and CDS as a single sequence input; the tail region of the 5' UTR and the head region of the CDS significantly regulate mRNA translation (Nieuwkoop et al. 2023). Utilizing information from both sides will enhance mRNA sequence representation. Instead of employing a single masked token prediction-based language model, we perform our model on a contextual level, such as data2vec, which considers both masked and unmasked sequences, providing a more suitable representation for downstream tasks as it reflects the unmasked nature of sequence prediction.\nIn addition to the unifying 5' UTR and CDS and contextual representation learning, we fully utilize the properties of the mRNA modality: 1) We specify a probabilistic hard-masking strategy that focuses more on the tail region of the 5' UTR and head of the CDS, increasing the likelihood of these regions being masked. 2) We incorporate additional modalities, such as Minimum Free Energy (MFE) and Secondary Structure (SS), to design better pre-training tasks to enhance the process.\nTo evaluate mRNA2vec, we consider translation expression tasks on both 5' UTR and CDS datasets. For the 5' UTR, we assess Translation Efficiency (TE) and Expression"}, {"title": "Related Work", "content": "mRNA Sequence-based Translation Prediction\nThe optimization of mRNA sequences to enhance expression and stability has become increasingly viable, thanks to the vast amount of measured and synthesized sequences available (Nikolados and Oyarzun 2023). Data-driven models have emerged as effective for predicting synonymous sequence expression, as they can detect highly nonlinear correlations between sequences and their functional outcomes. For instance, Leppek et al. (2022) introduced the RNA sequencing-based platform PERSIST-seq, which systematically predicts mRNA stability and ribosome load. Similarly, Diez et al. (2022) developed an evolutionary algorithm for codon optimization to address the challenge posed by the enormous number of possible synonymous sequences that encode the same peptide. In other studies, Cao et al. (2021) and Nieuwkoop et al. (2023) employed machine learning approaches, specifically random forests, to model the 5' UTR and CDS regions, predicting translation efficiency and protein production levels, respectively. Moreover, the length of the 5' UTR sequence itself can be a direct predictor of mRNA translation regulation (Wieder et al. 2024).\nmRNA Sequence Representation Learning\nSelf-supervised learning and language models have recently emerged as powerful tools for extracting sequence features from large, unlabeled datasets (Devlin et al. 2018; Raffel et al. 2020). DNA2vec(Ng 2017), inspired by word2vec (Mikolov et al. 2013), introduced a method for learning distributed representations of k-mers. Analogous to word2vec's property that $vec(king) \u2013 vec(man) + vec(woman) = vec(queen)$, DNA2vec aims to identify similar properties in k-mers, such as vec(ACGAT) \u2013 vec(GAT) + vec(ATC) = vec(ACATC). For base-by-base resolution embedding, RNA-BERT (Akiyama and Sakakibara 2022) adapts the pre-training algorithm BERT to non-coding RNA (ncRNA), embedding the four RNA bases while combining the Masked Language Model task with secondary structure alignment. Beyond mere representation, recent works try to understand the expression of mRNA by applying these representations. For example, Nikolados and Oyarzun (2023) developed a codon optimization language model for the Enterobacterales dataset by embedding codons, while CondoBERT (Li et al. 2023) further extended this approach by incorporating more pre-training datasets to measure mRNA expression, stability, and immunogenicity at the mRNA-cds level. By predicting masked tokens at the codon level, CondoBERT successfully obtains codon embeddings. Although the CDS region provides substantial information for predicting mRNA translation, the 5' UTR, as a regulatory sequence, also plays a critical role in learning translation efficiency (Nieuwkoop et al. 2023; Cetnar and Salis 2021; Sample et al. 2018). Recently, UTR-LM (Chu et al. 2024) was proposed to specifically focus on the 5' UTR sequence to train a language model."}, {"title": "Method:mRNA2vec", "content": "In this section, we introduce mRNA2vec, a novel mRNA language model involves concatenating the 5' UTR and CDS sequences to form a single input sequence. Unlike traditional models such as BERT(Devlin et al. 2018) or T5(Raffel et al. 2020), which use a masked sequence as input, mRNA2vec adopts a contextual target-based pre-training model inspired by data2vec. This approach allows the model to access both the unmasked and masked sequences simultaneously.\nWe employ a probabilistic hard masking strategy where specific regions of the sequence, particularly biologically important areas, have a higher likelihood of being masked. This ensures that these critical regions are more effectively learned during training. Additionally, domain-specific information, such as Minimum Free Energy (MFE) and Secondary Structure (SS), is incorporated through the definition of auxiliary pre-training tasks. This integration strengthens the model's ability to capture a representation across the entire sequence, enhancing its utility for translation-related downstream tasks on the both 5'UTR and CDS regions.\nPretrained Model\nPre-training has been very successful in advancing various domains such as natural language understanding (Devlin"}, {"title": "Pre-training Architecture", "content": "The pre-training architecture of mRNA2vec is illustrated in Figure 1. Instead of focusing solely on the straightforward task of predicting single masked tokens, we employ the data2vec pre-training architecture (Baevski et al. 2022) to learn contextual targets from the unmasked sequence. This approach enables the model to capture the entire sequence information rather than just the masked parts, leading to a more comprehensive representation for downstream tasks involving unmasked sequences.\nFor tokenization, we utilize specialized tokens tailored to mRNA sequences. Since mRNA is composed of only four type of nucleotides A, U, G, C, and is read by the ribosome in groups of three nucleotides, we design a tokenizer consisting of 64 tokens, each representing a specific sequence of three nucleotides (e.g., AAA, AAU, AAG, . . .).\nInstead of uniformly selecting masked tokens, as is common in NLP and CV tasks, we adopt a location dependent probabilistic hard-masking strategy. Research has demonstrated that regions such as the upstream CDS and the start of the CDS are crucial for mRNA translation (Nieuwkoop et al. 2023; Leppek et al. 2022). Therefore, token $X_{i}$ in these important regions are masked with a higher probability, as defined by:\n$p(X_{i} = masked) = q_{i} = \\begin{cases}\n\\frac{\\alpha h}{l-h} & \\text{if } i \\in [k, k + h] \\\\\n1-\\alpha & \\text{otherwise,}\n\\end{cases}$ (1)\nwhere $l$ is the length of the input sequence, $h$ is the length of the continuous important region, and $\\alpha$ adjusts the proportion of masked tokens selected from these regions.\nBy integrating the data2vec architecture with this specialized hard-masking strategy, mRNA2vec aims to reconstruct the entire sequence information based on the masked, crucial UTR-CDS regions. The loss function of the student model to predict the masked token given unmasked context from the teacher model for data2vec is defined as:\n$L = ||f_{t}(x) - y||_{2}^{2}$, (2)\nwhere $|| \\cdot ||$ denotes the squared L2 norm. Here, $f_{t}$ represents the predict of student encoder learning from the masked sequence, while $y$ is the representation obtained from the unmasked sequence, produced by a teacher model. The parameters of the teacher model are updated based on the student model through an exponentially moving average (EMA) (He et al. 2020). Multiple hidden states are averaged to form the contextual representation $y$ of the unmasked sequence."}, {"title": "Auxiliary Pretext Tasks with MFE and SS", "content": "In addition to the primary sequence reconstruction pretext task, we incorporate auxiliary tasks involving Minimum Free Energy (MFE) and secondary structure (SS), which are known to correlate with mRNA expression levels (Akiyama and Sakakibara 2022; Chu et al. 2024). Although Chu et al. (2024) argues that secondary structure does not significantly"}, {"title": "1. MFE Regression:", "content": "We apply the InfoNCE loss (Oord, Li, and Vinyals 2018) to capture the distance $d$ between the estimated MFE and the true MFE:\n$L_{mfe} = -log \\frac{exp(-d(\\hat{e_{i}}, e_{i}))}{\\sum_{j} exp(-d(\\hat{e_{j}}, e_{i}))}$ (3)\nHere, $\\hat{e_{i}}$ is the output scalar of the regressor $\\varphi(\\cdot)$, applied on top of the pre-trained model. In this context, $\\hat{e_{i}}$ and $e_{i}$ form a positive pair, while $\\hat{e_{j}}$ and $e_{i}$ form a negative pair. The goal is to minimize the distance $d(\\hat{e_{i}}, e_{i})$ while maximizing $d(\\hat{e_{i}}, e_{j})$, learning to rank the predictions rather than merely minimizing $d(\\hat{e_{i}}, e_{i})$ to zero. We use the mean token embedding from the last layer as the sequence representation, which serves as the input to $\\varphi(\\cdot)$."}, {"title": "2. Secondary Structure Classification:", "content": "We treat SS prediction as a token-wise classification problem. Similar to nucleotide tokenization, we tokenize consecutive groups of three \u201cdot-bracket\u201d characters, resulting in a 17-category classification problem:\n$L_{ss} = \\sum_{i} c_{i} log(p_{i})$ (4)\nIn this equation, $p_{i}$ represents the softmax output of the classifier $\\psi(\\cdot)$, applied on top of the pre-trained model, while $c_{i}$ is a 17-dimensional one-hot vector representing the true label. The same token representation used as input to $\\varphi(\\cdot)$ is used as input to $\\psi(\\cdot)$.\nThus, the overall loss function for our pre-training model, mRNA2vec, is defined as:\n$L_{mRNA2vec} = L + \\beta_{1}L_{mfe} + \\beta_{2}L_{ss}$ (5)\nHere, $\\beta_{1}$ and $\\beta_{2}$ are hyperparameters that regulate the contribution of MFE and SS to the language model. In this work, we set $\\beta_{1}$ = 0.01 and $\\beta_{2}$ = 0.001."}, {"title": "mRNA Translation Downstream Tasks", "content": "After obtaining the pre-trained embedding model, we can explore how the representation can be applied to the downstream task related to mRNA translation. Since we learned the embedding of the UTR and CDS region, we can evaluate our model on both UTR sequence optimization and CDS region codon optimization."}, {"title": "1. Evaluate mRNA2vec in 5' UTR:", "content": "It is essential to understand how the 5' UTR influences the rate at which mRNA is translated into protein. In the 5' UTR, we have two tasks to be predicted: mRNA Expression Level (EL) and Translation Efficiency (TE)(Chu et al. 2024). EL is measured in RNA-seq RPKM and TE is calculated as the ratio of Ribo-seq PRKM to RNA-seq RPKM. Ribo-seq is the ribosomal footprints observed on a given mRNA of interest, and RNA-seq is the relative abundance of mRNA transcript in the cell. PRKM represents Reads Per Kilobase of transcript per Million mapped reads (Cao et al. 2021)."}, {"title": "2. Evaluate mRNA2vec in CDS:", "content": "The codon optimization of CDS region will directly affect the translation of mRNA sequence. Codon optimization refers to choosing the optimal amino acid codes for increasing protein expression. However, some proteins might need a slower translation rate to fold properly and maintain protein stability (To and Cho 2021). Thus, in this region, Protein Production Level and mRNA Stability are used as two downstream tasks to measure the codon optimization of the CDS sequence."}, {"title": "Dataset", "content": "For pre-training, we collect five species (human, rat, mouse, chicken, and zebrafish) mRNA sequences from the NIH with the datasets API 1. To extract the 5'UTR and CDS sequence from it, we discard the sequences after the stop codon (UAG, UAA, UGA). Thus, we have the 5'UTR sequence before the start codon AUG and the CDS sequence after AUG until the stop codon. To avoid padding for the end of area of 5' UTR, we truncated 5' UTR sequence to a sequence with 102 nucleotides (34 tokens) from the start. After data processing, we obtain 510k sequences with an average length of 459 bp. To obtain the MFE value and SS, we use RNAfold from ViennaRNA(Lorenz et al. 2011).\nFor the downstream task 5' UTR data, we used three endogenous genes from three human cell types: Human embryonic kidney (HEK) 293T cell line, human prostate cancer cell line PC3 (PC3), and Human muscle tissue (Muscle) (Cao et al. 2021; Chu et al. 2024). HEK and PC3 have over 10k sequences but the Muscle only has 1k dataset for the downstream task. The average length is 91 bp for all 5 UTR data.\nFor the downstream task CDS data, we used two datasets: mRAN stability dataset (Diez et al. 2022) and the mRFP Expression dataset (Nieuwkoop et al. 2023). The mRNA stability dataset contains 25K sequences for all endogenous genes in Humans, zebrafish, Xenopus, and mouse. The mRFP Expression dataset contains 6k sequences to measure protein production levels for several gene variants in E.coil."}, {"title": "Experiment Setting", "content": "The mRNA2vec pre-training architecture is built upon the data2vec framework, utilizing the T5 encoder as the stu-"}, {"title": "Pre-training Result", "content": "Results In this section, we evaluate different pre-training strategies on mRNA sequences. After pre-training, the model weights are used to initialize the encoder, which outputs the representation of the mRNA sequence. On top of the pretrained model, a downstram task regressor consisting of a linear layer, a GELU layer, and a fully connected layer is used to predict the target values. Note that the encoder part is frozen, and only the parameters of the regressor are updated. The Spearman Rank used to measure the regression result between the outputs of the regressor and true values.\nFirst, we compare the data2vec approach using T5 as the encoder with a standalone T5 encoder on mRNA sequences. We also use an untrained (unloaded) model as the baseline, as shown in Figure 2. Our observations are as follows:\n1. While the mRNA representation learned by T5 can be trained effectively in the first few epochs, the data2vec method shows better performance after several epochs on the 5' UTR data. By incorporating the unmasked sequence during the pre-training process, data2vec extracts a more robust representation of the entire sequence, out-performing the results obtained by T5.\n2. Data2vec consistently improves results on both 5' UTR and CDS regions compared to the untrained model baseline. However, T5 shows limited effectiveness on the 5' UTR TE task. Contextual representation learning will have more advances for the mRNA compare the single mask token prediction such as T5.\nNext, we compare the performance of the pretrained model with and without the inclusion of MFE and SS-related losses. We evaluate four different pre-training models by adjusting $\\beta_{1}$ and $\\beta_{2}$. When $\\beta_{1}$ = $\\beta_{2}$ = 0, the base model $L_{mRNA2vec}$ = $L$ does not incorporate MFE or SS information. When $\\beta_{1}$ $\\ne$ 0 and $\\beta_{2}$ = 0, the model $L_{mRNA2vec}$ = $L$+$ \\beta_{1}L_{mfe}$ considers only MFE information. When $\\beta_{1}$ = 0 and $\\beta_{2}$ $\\ne$ 0, the model $L_{mRNA2vec}$ = $L$ + $ \\beta_{2}L_{ss}$ incorporates only SS information. As shown in Table 1, we found that:\n1. Incorporating both MFE and SS losses consistently improves the base model (without MFE and SS) across all downstream tasks. For instance, in the Muscle dataset, the TE task performance increased from 0.550 to 0.573, and the EL task performance improved from 0.619 to 0.663. This enhancement is attributed to the model's ability to consider sequence representation while aligning it with MFE and SS modalities.\n2. Analyzing the model under the scenarios where either $\\beta_{1}$ = 0 or $\\beta_{2}$ = 0, we observe that both cases generally improve the base model results. It is important to note that we used the same pre-training model for all tasks and datasets, with fixed hyperparameter combinations of ($\\beta_{1}$ = 0, $\\beta_{2}$ = 0.001) and ($\\beta_{1}$ = 0.01, $\\beta_{2}$ = 0). While this strategy may not represent the optimal hyperparameter settings for every dataset, it provides a more convenient and generalizable solution."}, {"title": "Discussion", "content": "1. Choosing the optimal $\\alpha$ for hard-masking: Since certain sub-sequences of the mRNA sequence are more critical than others, we employ a specialized region masking strategy with the hyperparameter $\\alpha$, as defined in Equation 1. In this paper, we designate the important region spanning from the 15th token ($k$ = 15) to the 45th token ($k$ + $h$ = 45). To assess how $\\alpha$ influences model learning, we experimented with different values ranging from 0 to 1, as shown in Table 2. When $\\alpha$, the masking strategy is equivalent to random masking. However, when $\\alpha$ > 0, the learning process accelerates, resulting in improved loss performance. Notably, it also enhances MFE learning, even without direct access to MFE information."}, {"title": "2. Design a better pretext tasks:", "content": "Contrary to the findings of UTR-LM (Chu et al. 2024), which suggested that incorporating secondary structure might degrade model performance, we explored a different approach by framing secondary structure as a token-wise classification problem with 27 categories (as opposed to the 3 categories used in UTR-LM). As shown in Figure 3, the pre-training process benefits from the inclusion of MFE and secondary structure information when appropriate pretext tasks are employed."}, {"title": "Downstram Tasks: Comparing to other benchmarks", "content": "In this section, we compare our method to several benchmarks that focus on TE and EL prediction by measuring the spearman rank for the 5' UTR: 1) CodonBERT (Li et al. 2023), 2) UTR-LM (Chu et al. 2024), 3) RNA-BERT (Akiyama and Sakakibara 2022), 4) RNA-FM (Chen et al. 2022), 5) Cao-RF (Cao et al. 2021).\nWe also compare our method to existing approaches that work on functional of sequence for the CDS: 1) Term Frequency-Inverse Document Frequency (TF-IDF) (Rajaraman and Ullman 2011), 2) RNA-BERT (Akiyama and Sakakibara 2022), 3) RNA-FM (Chen et al. 2022), 4) CodonBERT (Li et al. 2023)."}, {"title": "Finding an optimal input sequence length:", "content": "We examined the effect of different input sequence lengths on"}, {"title": "Conclusion", "content": "In this paper, we proposed a novel mRNA pre-training method, mRNA2vec, based on the data2vec architecture and mRNA domain knowledge. Due to the contextual learning and hard-mask strategy, the data2vec-based LM outperforms the mask token prediction LMs such as T5. We also find that using the domain knowledge MFE and SS properly can improve the learning result.\nMeanwhile, with optimal sub-sequence, better-hidden layer, and a well-designed regressor for the downstream task, mRNA2vec improves the best benchmark largely on the TE and EL task and obtains comparable result on the mRNA stability and E.coil protein production."}, {"title": "More results of Evaluation of Downstream Task Regressor", "content": "As shown in Table 7, it is consistent with the Table 5. More complex regressors generally improve performance over linear regressors."}, {"title": "SS Boosts Small Dataset More", "content": "We have observed in Table 1 that MFE and SS can better improve the performance on Muscle data than other datasets. Thus, we hypothesized that MFE and SS can contribute more on the small dataset. We downsampled the HEK dataset to train different regressors wiht the same setting of above experiments. Figure 6 shows the Spearman rank of TE on the test dataset after using a different sub-dataset as the training dataset. Our hypothesis could be verified by comparing the based model+MFE loss, model+MFE loss+ SS loss, and the base model. The margins between the based model and other models will be increased as the training data becomes smaller."}]}