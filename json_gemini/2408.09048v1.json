{"title": "mRNA2vec: mRNA Embedding with Language Model in the 5'UTR-CDS for\nmRNA Design", "authors": ["Honggen Zhang", "Xiangrui Gao", "June Zhang", "Lipeng Lai"], "abstract": "Messenger RNA (mRNA)-based vaccines are accelerating the\ndiscovery of new drugs and revolutionizing the pharmaceuti-\ncal industry. However, selecting particular mRNA sequences\nfor vaccines and therapeutics from extensive mRNA libraries\nis costly. Effective mRNA therapeutics require carefully de-\nsigned sequences with optimized expression levels and stabil-\nity. This paper proposes a novel contextual language model\n(LM)-based embedding method: mRNA2vec. In contrast to\nexisting mRNA embedding approaches, our method is based\non the self-supervised teacher-student learning framework of\ndata2vec. We jointly use the 5' untranslated region (UTR)\nand coding sequence (CDS) region as the input sequences.\nWe adapt our LM-based approach specifically to mRNA by\n1) considering the importance of location on the mRNA se-\nquence with probabilistic masking, 2) using Minimum Free\nEnergy (MFE) prediction and Secondary Structure (SS) clas-\nsification as additional pretext tasks. mRNA2vec demon-\nstrates significant improvements in translation efficiency (TE)\nand expression level (EL) prediction tasks in UTR compared\nto SOTA methods such as UTR-LM. It also gives a compet-\nitive performance in mRNA stability and protein production\nlevel tasks in CDS such as CodonBERT.", "sections": [{"title": "Introduction", "content": "Messenger RNA (mRNA)-based gene therapeutics and vac-\ncines have emerged as pivotal tools for developing a new\nclass of drugs, as they do not present the safety concerns\nassociated with genomic integration (To and Cho 2021). Re-\ncently, mRNA vaccines have been utilized in treating sev-\neral diseases, including COVID-19 (Jackson et al. 2020) and\ncancer (Zeraati et al. 2017). mRNA therapeutics facilitate\nthe production of missing or defective proteins, providing\nessential supplements to cure genetic diseases (Kwon et al.\n2018). To advance mRNA-based treatments further, under-\nstanding and designing the appropriate mRNA sequences is\ncrucial. Factors such as expression level and stability sig-\nnificantly impact the practical application of mRNA-based\ntherapies.\nResearchers have primarily focused on the untranslated\nregion (UTR) and the coding sequence (CDS) region to un-\nderstand the mechanisms of mRNA in protein translation\nCao et al. (2021); Li et al. (2023). Studies have demon-\nstrated that UTR can regulate mRNA translation (Akiyama\nand Sakakibara 2022), with extensive exploration of the 5'\nUTR, which precedes the coding sequences (Chu et al. 2024;\nWieder et al. 2024). The CDS region contains codons that\nare translated into amino acids, so codon optimization di-\nrectly impacts mRNA translation.\nTechnological advancements using machine learning\n(particularly language models) with biology have enabled\nnew methods to decode mRNA sequences (Diez et al. 2022;\nCao et al. 2021). Ribosomes read sequences from left to\nright, similar to text sequences, suggesting that mRNA se-\nquences could be modeled using language models and self-\nsupervised learning techniques. Mapping mRNA to a high-\ndimensional representation space has the potential to en-\nhance our understanding of its functions. However, signif-\nicant differences exist between mRNA and natural text. For\nexample, mRNA has a more limited vocabulary, consisting\nof 4 kinds of nucleotides and twenty amino acids. Therefore\nit is important to account for these differences when adapt-\ning language models for mRNA embedding.\nIn this paper, we propose an mRNA embedding method:\nmRNA2vec. Unlike previous research that examined the 5'\nUTR and CDS separately, we combine the 5' UTR and CDS\nas a single sequence input; the tail region of the 5' UTR and\nthe head region of the CDS significantly regulate mRNA\ntranslation (Nieuwkoop et al. 2023). Utilizing information\nfrom both sides will enhance mRNA sequence representa-\ntion. Instead of employing a single masked token prediction-\nbased language model, we perform our model on a contex-\ntual level, such as data2vec, which considers both masked\nand unmasked sequences, providing a more suitable repre-\nsentation for downstream tasks as it reflects the unmasked\nnature of sequence prediction.\nIn addition to the unifying 5' UTR and CDS and contex-\ntual representation learning, we fully utilize the properties\nof the mRNA modality: 1) We specify a probabilistic hard-\nmasking strategy that focuses more on the tail region of the\n5' UTR and head of the CDS, increasing the likelihood of\nthese regions being masked. 2) We incorporate additional\nmodalities, such as Minimum Free Energy (MFE) and Sec-\nondary Structure (SS), to design better pre-training tasks to\nenhance the process.\nTo evaluate mRNA2vec, we consider translation expres-\nsion tasks on both 5' UTR and CDS datasets. For the 5'\nUTR, we assess Translation Efficiency (TE) and Expression"}, {"title": "Related Work", "content": "mRNA Sequence-based Translation Prediction\nThe optimization of mRNA sequences to enhance expres-\nsion and stability has become increasingly viable, thanks\nto the vast amount of measured and synthesized sequences\navailable (Nikolados and Oyarzun 2023). Data-driven mod-\nels have emerged as effective for predicting synonymous se-\nquence expression, as they can detect highly nonlinear cor-\nrelations between sequences and their functional outcomes.\nFor instance, Leppek et al. (2022) introduced the RNA\nsequencing-based platform PERSIST-seq, which systemati-\ncally predicts mRNA stability and ribosome load. Similarly,\nDiez et al. (2022) developed an evolutionary algorithm for\ncodon optimization to address the challenge posed by the\nenormous number of possible synonymous sequences that\nencode the same peptide. In other studies, Cao et al. (2021)\nand Nieuwkoop et al. (2023) employed machine learning ap-\nproaches, specifically random forests, to model the 5' UTR\nand CDS regions, predicting translation efficiency and pro-\ntein production levels, respectively. Moreover, the length\nof the 5' UTR sequence itself can be a direct predictor of\nmRNA translation regulation (Wieder et al. 2024)."}, {"title": "mRNA Sequence Representation Learning", "content": "Self-supervised learning and language models have recently\nemerged as powerful tools for extracting sequence features\nfrom large, unlabeled datasets (Devlin et al. 2018; Raffel\net al. 2020). DNA2vec(Ng 2017), inspired by word2vec\n(Mikolov et al. 2013), introduced a method for learning dis-\ntributed representations of k-mers. Analogous to word2vec's\nproperty that  vec(king) \u2013 vec(man) + vec(woman) =\nvec(queen) , DNA2vec aims to identify similar proper-\nties in k-mers, such as  vec(ACGAT) \u2013 vec(GAT) +\nvec(ATC) = vec(ACATC) . For base-by-base resolution\nembedding, RNA-BERT (Akiyama and Sakakibara 2022)\nadapts the pre-training algorithm BERT to non-coding RNA\n(ncRNA), embedding the four RNA bases while combining\nthe Masked Language Model task with secondary structure\nalignment. Beyond mere representation, recent works try to\nunderstand the expression of mRNA by applying these rep-\nresentations. For example, Nikolados and Oyarzun (2023)\ndeveloped a codon optimization language model for the En-\nterobacterales dataset by embedding codons, while Con-\ndoBERT (Li et al. 2023) further extended this approach by\nincorporating more pre-training datasets to measure mRNA\nexpression, stability, and immunogenicity at the mRNA-cds\nlevel. By predicting masked tokens at the codon level, Con-\ndoBERT successfully obtains codon embeddings. Although\nthe CDS region provides substantial information for predict-\ning mRNA translation, the 5' UTR, as a regulatory sequence,\nalso plays a critical role in learning translation efficiency\n(Nieuwkoop et al. 2023; Cetnar and Salis 2021; Sample et al.\n2018). Recently, UTR-LM (Chu et al. 2024) was proposed\nto specifically focus on the 5' UTR sequence to train a lan-\nguage model."}, {"title": "Method:mRNA2vec", "content": "In this section, we introduce mRNA2vec, a novel mRNA\nlanguage model involves concatenating the 5' UTR and\nCDS sequences to form a single input sequence. Unlike\ntraditional models such as BERT(Devlin et al. 2018) or\nT5(Raffel et al. 2020), which use a masked sequence as\ninput, mRNA2vec adopts a contextual target-based pre-\ntraining model inspired by data2vec. This approach allows\nthe model to access both the unmasked and masked se-\nquences simultaneously.\nWe employ a probabilistic hard masking strategy where\nspecific regions of the sequence, particularly biologically\nimportant areas, have a higher likelihood of being masked.\nThis ensures that these critical regions are more effectively\nlearned during training. Additionally, domain-specific infor-\nmation, such as Minimum Free Energy (MFE) and Sec-\nondary Structure (SS), is incorporated through the defini-\ntion of auxiliary pre-training tasks. This integration strength-\nens the model's ability to capture a representation across the\nentire sequence, enhancing its utility for translation-related\ndownstream tasks on the both 5'UTR and CDS regions."}, {"title": "Pretrained Model", "content": "Pre-training has been very successful in advancing various\ndomains such as natural language understanding (Devlin"}, {"title": "et al. 2018; Rao et al. 2020), image invariant feature learning", "content": "(Chen et al. 2020; He et al. 2020), tabular datasets (Huang\net al. 2023; Rubachev et al. 2022) and time series (Wick-\nstr\u00f8m et al. 2022). With a properly designed pretext task\nin the pre-training process, we will obtain the generalized\nrepresentation for the data input. mRNA as the sequential\ndataset, we might discover some hidden information from\nthe large data pre-training similar to the language."}, {"title": "Pre-training Architecture", "content": "The pre-training architecture\nof mRNA2vec is illustrated in Figure 1. Instead of focus-\ning solely on the straightforward task of predicting single\nmasked tokens, we employ the data2vec pre-training archi-\ntecture (Baevski et al. 2022) to learn contextual targets from\nthe unmasked sequence. This approach enables the model to\ncapture the entire sequence information rather than just the\nmasked parts, leading to a more comprehensive representa-\ntion for downstream tasks involving unmasked sequences.\nFor tokenization, we utilize specialized tokens tailored to\nmRNA sequences. Since mRNA is composed of only four\ntype of nucleotides A, U, G, C, and is read by the ribosome\nin groups of three nucleotides, we design a tokenizer con-\nsisting of 64 tokens, each representing a specific sequence\nof three nucleotides (e.g., AAA, AAU, AAG, . . .).\nInstead of uniformly selecting masked tokens, as is com-\nmon in NLP and CV tasks, we adopt a location dependent\nprobabilistic hard-masking strategy. Research has demon-\nstrated that regions such as the upstream CDS and the start\nof the CDS are crucial for mRNA translation (Nieuwkoop\net al. 2023; Leppek et al. 2022). Therefore, token  X_i  in these\nimportant regions are masked with a higher probability, as\ndefined by:\n$p(X_i = masked) = q_i =\\begin{cases}\n\\alpha \\frac{h}{l} & \\text{if } i \\in [k, k + h] \\\\\n1 - \\alpha \\frac{h}{l} & \\text{otherwise,}\n\\end{cases}$\nwhere  l  is the length of the input sequence,  h  is the length\nof the continuous important region, and  \\alpha  adjusts the pro-\nportion of masked tokens selected from these regions.\nBy integrating the data2vec architecture with this special-\nized hard-masking strategy, mRNA2vec aims to reconstruct\nthe entire sequence information based on the masked, crucial\nUTR-CDS regions. The loss function of the student model\nto predict the masked token given unmasked context from\nthe teacher model for data2vec is defined as:\n$\\mathcal{L} = ||f_t(x) - y||^2,$\nwhere  || \\cdot ||  denotes the squared L2 norm. Here,  f_t  rep-\nresents the predict of student encoder learning from the\nmasked sequence, while  y  is the representation obtained\nfrom the unmasked sequence, produced by a teacher model.\nThe parameters of the teacher model are updated based on\nthe student model through an exponentially moving average\n(EMA) (He et al. 2020). Multiple hidden states are averaged\nto form the contextual representation  y  of the unmasked se-\nquence."}, {"title": "Auxiliary Pretext Tasks with MFE and SS", "content": "In addition\nto the primary sequence reconstruction pretext task, we\nincorporate auxiliary tasks involving Minimum Free En-\nergy (MFE) and secondary structure (SS), which are known\nto correlate with mRNA expression levels (Akiyama and\nSakakibara 2022; Chu et al. 2024). Although Chu et al.\n(2024) argues that secondary structure does not significantly"}, {"title": "benefit the pre-training process in UTR-LM, we include it", "content": "here to explore its potential impact.\nmRNA secondary structure consists of unpaired bases,\nrepresented by dots, which indicate single-stranded regions,\nand paired bases, represented by matching brackets (e.g.,\n()), which form stem structures. For example, the SS of\nthe sequence GGGAAACCC is represented as  \"(((...)))\u201d .\nMFE calculates base pair probabilities within thermody-\nnamic models, representing the energy required to form the\nmost stable secondary structure.\nTo enhance mRNA2vec's ability to represent mRNA se-\nquences for translation-related tasks, we introduce two addi-\ntional pretext tasks:\n1. MFE Regression: We apply the InfoNCE loss (Oord, Li,\nand Vinyals 2018) to capture the distance  d  between the\nestimated MFE  \\hat{e}_i  and the true MFE:\n$\\mathcal{L}_{mfe} = -\\log \\frac{\\exp(-d(e_i, \\hat{e}_i))}{\\sum_j \\exp(-d(e_j, \\hat{e}_i))}$\nHere,  \\hat{e}_i  is the output scalar of the regressor  \\varphi(\\cdot) , ap-\nplied on top of the pre-trained model. In this context,  \\hat{e}_i\nand  e_i  form a positive pair, while  \\hat{e}_i  and  e_j  form a neg-\native pair. The goal is to minimize the distance  d(e_i, \\hat{e}_i)\nwhile maximizing  d(\\hat{e}_i, e_j) , learning to rank the predic-\ntions rather than merely minimizing  d(e_i, \\hat{e}_i)  to zero. We\nuse the mean token embedding from the last layer as\nthe sequence representation, which serves as the input to\n$\\varphi(\\cdot)$.\n2. Secondary Structure Classification: We treat SS pre-\ndiction as a token-wise classification problem. Simi-\nlar to nucleotide tokenization, we tokenize consecutive\ngroups of three \u201cdot-bracket\" characters, resulting in a\n17-category classification problem:\n$\\mathcal{L}_{ss} = \\sum_i c_i \\log(p_i)$\nIn this equation,  p_i  represents the softmax output of the\nclassifier  \\psi(\\cdot) , applied on top of the pre-trained model,\nwhile  c_i  is a 17-dimensional one-hot vector representing\nthe true label. The same token representation used as in-\nput to  $\\varphi(\\cdot)$  is used as input to  \\psi(\\cdot) .\nThus, the overall loss function for our pre-training model,\nmRNA2vec, is defined as:\n$\\mathcal{L}_{mRNA2vec} = \\mathcal{L} + \\beta_1 \\mathcal{L}_{mfe} + \\beta_2 \\mathcal{L}_{ss}$\nHere,  \\beta_1  and  \\beta_2  are hyperparameters that regulate the\ncontribution of MFE and SS to the language model. In this\nwork, we set  \\beta_1 = 0.01  and  \\beta_2 = 0.001 ."}, {"title": "mRNA Translation Downstream Tasks", "content": "After obtaining the pre-trained embedding model, we can\nexplore how the representation can be applied to the down-\nstream task related to mRNA translation. Since we learned\nthe embedding of the UTR and CDS region, we can evalu-\nate our model on both UTR sequence optimization and CDS\nregion codon optimization."}, {"title": "1. Evaluate mRNA2vec in 5' UTR: It is essential to un-", "content": "derstand how the 5' UTR influences the rate at which\nmRNA is translated into protein. In the 5' UTR, we have\ntwo tasks to be predicted: mRNA Expression Level (EL)\nand Translation Efficiency (TE)(Chu et al. 2024). EL is\nmeasured in RNA-seq RPKM and TE is calculated as the\nratio of Ribo-seq PRKM to RNA-seq RPKM. Ribo-seq\nis the ribosomal footprints observed on a given mRNA\nof interest, and RNA-seq is the relative abundance of\nmRNA transcript in the cell. PRKM represents Reads\nPer Kilobase of transcript per Million mapped reads (Cao\net al. 2021).\n2. Evaluate mRNA2vec in CDS: The codon optimiza-\ntion of CDS region will directly affect the translation of\nmRNA sequence. Codon optimization refers to choosing\nthe optimal amino acid codes for increasing protein ex-\npression. However, some proteins might need a slower\ntranslation rate to fold properly and maintain protein sta-\nbility (To and Cho 2021). Thus, in this region, Protein\nProduction Level and mRNA Stability are used as two\ndownstream tasks to measure the codon optimization of\nthe CDS sequence."}, {"title": "Dataset", "content": "For pre-training, we collect five species (human, rat, mouse,\nchicken, and zebrafish) mRNA sequences from the NIH with\nthe datasets API 1. To extract the 5'UTR and CDS sequence\nfrom it, we discard the sequences after the stop codon (UAG,\nUAA, UGA). Thus, we have the 5'UTR sequence before the\nstart codon AUG and the CDS sequence after AUG until the\nstop codon. To avoid padding for the end of area of 5' UTR,\nwe truncated 5' UTR sequence to a sequence with 102 nu-\ncleotides (34 tokens) from the start. After data processing,\nwe obtain 510k sequences with an average length of 459 bp.\nTo obtain the MFE value and SS, we use RNAfold from Vi-\nennaRNA(Lorenz et al. 2011).\nFor the downstream task 5' UTR data, we used three en-\ndogenous genes from three human cell types: Human em-\nbryonic kidney (HEK) 293T cell line, human prostate can-\ncer cell line PC3 (PC3), and Human muscle tissue (Muscle)\n(Cao et al. 2021; Chu et al. 2024). HEK and PC3 have over\n10k sequences but the Muscle only has 1k dataset for the\ndownstream task. The average length is 91 bp for all 5 UTR\ndata.\nFor the downstream task CDS data, we used two datasets:\nmRAN stability dataset (Diez et al. 2022) and the mRFP Ex-\npression dataset (Nieuwkoop et al. 2023). The mRNA stabil-\nity dataset contains 25K sequences for all endogenous genes\nin Humans, zebrafish, Xenopus, and mouse. The mRFP Ex-\npression dataset contains 6k sequences to measure protein\nproduction levels for several gene variants in E.coil."}, {"title": "Experiment Setting", "content": "The mRNA2vec pre-training architecture is built upon the\ndata2vec framework, utilizing the T5 encoder as the stu-"}, {"title": "Pre-training Result", "content": "Results In this section, we evaluate different pre-training\nstrategies on mRNA sequences. After pre-training, the\nmodel weights are used to initialize the encoder, which out-\nputs the representation of the mRNA sequence. On top of the\npretrained model, a downstram task regressor consisting of\na linear layer, a GELU layer, and a fully connected layer is\nused to predict the target values. Note that the encoder part\nis frozen, and only the parameters of the regressor are up-\ndated. The Spearman Rank used to measure the regression\nresult between the outputs of the regressor and true values.\nFirst, we compare the data2vec approach using T5 as the\nencoder with a standalone T5 encoder on mRNA sequences.\nWe also use an untrained (unloaded) model as the baseline,\nas shown in Figure 2. Our observations are as follows:\n1. While the mRNA representation learned by T5 can be\ntrained effectively in the first few epochs, the data2vec\nmethod shows better performance after several epochs\non the 5' UTR data. By incorporating the unmasked se-\nquence during the pre-training process, data2vec extracts\na more robust representation of the entire sequence, out-\nperforming the results obtained by T5.\n2. Data2vec consistently improves results on both 5' UTR\nand CDS regions compared to the untrained model base-\nline. However, T5 shows limited effectiveness on the 5'\nUTR TE task. Contextual representation learning will\nhave more advances for the mRNA compare the single\nmask token prediction such as T5."}, {"title": "Next, we compare the performance of the pretrained", "content": "model with and without the inclusion of MFE and SS-related\nlosses. We evaluate four different pre-training models by ad-\njusting  \\beta_1  and  \\beta_2 . When  \\beta_1 = \\beta_2 = 0 , the base model\n$\\mathcal{L}_{mRNA2vec} = \\mathcal{L}$  does not incorporate MFE or SS informa-\ntion. When  \\beta_1 \\neq 0  and  \\beta_2 = 0 , the model  $\\mathcal{L}_{mRNA2vec} = \\\\\n\\mathcal{L} + \\beta_1 \\mathcal{L}_{mfe}$  considers only MFE information. When  \\beta_1 = 0\nand  \\beta_2 \\neq 0 , the model  $\\mathcal{L}_{mRNA2vec} = \\mathcal{L} + \\beta_2 \\mathcal{L}_{ss}$  incorpo-\nrates only SS information. As shown in Table 1, we found\nthat:\n1. Incorporating both MFE and SS losses consistently im-\nproves the base model (without MFE and SS) across all\ndownstream tasks. For instance, in the Muscle dataset,\nthe TE task performance increased from 0.550 to 0.573,\nand the EL task performance improved from 0.619 to\n0.663. This enhancement is attributed to the model's abil-\nity to consider sequence representation while aligning it\nwith MFE and SS modalities.\n2. Analyzing the model under the scenarios where either\n$\\beta_1 = 0$  or  $\\beta_2 = 0$ , we observe that both cases generally\nimprove the base model results. It is important to note\nthat we used the same pre-training model for all tasks\nand datasets, with fixed hyperparameter combinations of\n(\\beta_1 = 0, \\beta_2 = 0.001)  and  (\\beta_1 = 0.01, \\beta_2 = 0) . While\nthis strategy may not represent the optimal hyperparam-\neter settings for every dataset, it provides a more conve-\nnient and generalizable solution."}, {"title": "Discussion", "content": "1. Choosing the optimal  \\alpha  for hard-masking: Since cer-\ntain sub-sequences of the mRNA sequence are more crit-\nical than others, we employ a specialized region masking\nstrategy with the hyperparameter  \\alpha , as defined in Equa-\ntion 1. In this paper, we designate the important region\nspanning from the 15th token (k = 15) to the 45th to-\nken (k + h = 45). To assess how  \\alpha  influences model\nlearning, we experimented with different values ranging\nfrom 0 to 1, as shown in Table 2. When  \\alpha = 0 , the mask-\ning strategy is equivalent to random masking. However,\nwhen  \\alpha > 0 , the learning process accelerates, resulting\nin improved loss performance. Notably, it also enhances\nMFE learning, even without direct access to MFE infor-\nmation."}, {"title": "2. Design a better pretext tasks:", "content": "Contrary to the find-\nings of UTR-LM (Chu et al. 2024), which suggested that\nincorporating secondary structure might degrade model\nperformance, we explored a different approach by fram-\ning secondary structure as a token-wise classification\nproblem with 27 categories (as opposed to the 3 cate-\ngories used in UTR-LM). As shown in Figure 3, the pre-\ntraining process benefits from the inclusion of MFE and\nsecondary structure information when appropriate pre-\ntext tasks are employed."}, {"title": "Downstram Tasks: Comparing to other", "content": "benchmarks\nIn this section, we compare our method to several bench-\nmarks that focus on TE and EL prediction by measuring\nthe spearman rank for the 5' UTR: 1) CodonBERT (Li\net al. 2023), 2) UTR-LM (Chu et al. 2024), 3) RNA-BERT\n(Akiyama and Sakakibara 2022), 4) RNA-FM (Chen et al.\n2022), 5) Cao-RF (Cao et al. 2021).\nWe also compare our method to existing approaches that\nwork on functional of sequence for the CDS: 1) Term Fre-\nquency-Inverse Document Frequency (TF-IDF) (Rajaraman\nand Ullman 2011), 2) RNA-BERT (Akiyama and Sakak-\nibara 2022), 3) RNA-FM (Chen et al. 2022), 4) CodonBERT\n(Li et al. 2023)."}, {"title": "Results", "content": "For the 5'UTR dataset, after obtaining the pre-\ntrained model, we fine-tuned it on three 5' UTR datasets.\nAs shown in Table 3, our method outperforms all other ap-\nproaches on datasets related to the TE and EL tasks. For\nthe TE task, we achieved improvements of 13%, 12%, and\n14% over the previous state-of-the-art (SOTA) results on the\nHEK, PC3, and Muscle datasets, respectively. For the EL\ntask, our method yields improvements of 6%, 27%, and 31%\nover the best benchmarks.\nFor the CDS dataset, we evaluate these methods on the\nmRNA stability dataset and the mRFP-Expression dataset\n(protein production level of E. coli). As shown in Figure\n4, our method significantly improves the results compared\nto the codon representation method CodonBERT from 0.34\nto 0.53. For protein production levels, our method achieves\ncomparable results. However, unlike CodonBERT, our pre-\ntraining did not use E. coli mRNA sequences. This indicates\nthat our pre-trained model can be applied to species beyond\nthe five species used in the pre-training."}, {"title": "5: Two sub-sequnece selection strategies.", "content": "Pre-selected: change the\noutput length of encoder. Result will change as sequence length change."}, {"title": "translation efficiency (TE).", "content": "Notably, the 5' UTR datasets\n(HEK, PC3, Muscle) have an average sequence length of\n91 nucleotides (31 tokens after tokenization). We trun-\ncated the sequence from the left and kept only the right-\nmost M tokens. The solid line in Figure 5 shows the TE\nresults on the different datasets according to M. It is ap-\nparent that input sequence length affects the TE outcome,\nwith full sequences actually worsening the results. This\nfinding supports the idea that only a few bases in the 5'\nUTR play a crucial role in translation (Nieuwkoop et al.\n2023).\nInstead of selecting sub-sequences by truncating the in-\nput sequence, we directly selected the representation\nof the sub-sequence by choosing the tokens after pre-\ntraining, as shown by the dashed line in Figure 5. Both\ncurves suggest that 12, 14, and 18 tokens might be the op-\ntimal input sequence lengths for HEK, PC3, and Muscle,\nrespectively. However, this approach does not perform as\nwell as the strategy of controlling the input length."}, {"title": "2. Selecting the hidden state:", "content": "Different hidden layers can\nprovide different representations of the mRNA sequence\n(Chen et al. 2020). We studied the impact of selecting the\nhidden layer used for the downstream task. As shown in\nTable 4, using the last hidden state ( K = \u22121) harms the\nperformance compared to using the second last hidden\nlayer ( K = -2) or the third last hidden state( K = \u22123).\nIn this work, we use the second last hidden state as the\nmRNA embedding."}, {"title": "3. Designing a better downstram task regressor:", "content": "In this\nsubsection, we compare different regression heads on top\nof the pre-trained model, including a linear regressor,\na simple two-layer neural network, and a convolutional\nneural network. As shown in Table 5, the linear regressor\ncan outperform the UTR-LM results in Table 3. While\nmore complex regressors generally improve performance\nover linear regressors, their advantage diminishes with\nlarger datasets, such as HEK."}, {"title": "Conclusion", "content": "In this paper, we proposed a novel mRNA pre-training\nmethod, mRNA2vec, based on the data2vec architecture and\nmRNA domain knowledge. Due to the contextual learning\nand hard-mask strategy, the data2vec-based LM outperforms\nthe mask token prediction LMs such as T5. We also find that\nusing the domain knowledge MFE and SS properly can im-\nprove the learning result.\nMeanwhile, with optimal sub-sequence, better-hidden\nlayer, and a well-designed regressor for the downstream\ntask, mRNA2vec improves the best benchmark largely on\nthe TE and EL task and obtains comparable result on the\nmRNA stability and E.coil protein production."}, {"title": "More results of Evaluation of Downstream", "content": "Task Regressor\nAs shown in Table 7, it is consistent with the Table 5. More\ncomplex regressors generally improve performance over lin-\near regressors."}, {"title": "SS Boosts Small Dataset More", "content": "We have observed in Table 1 that MFE and SS can bet-\nter improve the performance on Muscle data than other\ndatasets. Thus, we hypothesized that MFE and SS can con-\ntribute more on the small dataset. We downsampled the HEK\ndataset to train different regressors wiht the same setting of\nabove experiments. Figure 6 shows the Spearman rank of TE\non the test dataset after using a different sub-dataset as the\ntraining dataset. Our hypothesis could be verified by com-\nparing the based model+MFE loss, model+MFE loss+ SS\nloss, and the base model. The margins between the based\nmodel and other models will be increased as the training data\nbecomes smaller."}]}