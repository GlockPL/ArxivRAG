{"title": "DeBaTeR: Denoising Bipartite Temporal Graph for Recommendation", "authors": ["Xinyu He", "Jose Sepulveda", "Mostafa Rahmani", "Alyssa Woo", "Fei Wang", "Hanghang Tong"], "abstract": "Due to the difficulty of acquiring large-scale explicit user feed-back, implicit feedback (e.g., clicks or other interactions) is widely applied as an alternative source of data, where user-item interactions can be modeled as a bipartite graph. Due to the noisy and biased nature of implicit real-world user-item interactions, identifying and rectifying noisy interactions are vital to enhance model performance and robustness. Previous works on purifying user-item interactions in collaborative filtering mainly focus on mining the correlation between user/item embeddings and noisy interactions, neglecting the benefit of temporal patterns in determining noisy interactions. Time information, while enhancing the models' utility, also bears its natural advantage in helping to determine noisy edges, e.g., if someone usually watches horror movies at night and talk shows in the morning, a record of watching a horror movie in the morning is more likely to be noisy interaction. Armed with this observation, we introduce a simple yet effective mechanism for generating time-aware user/item embeddings and propose two strategies for denoising bipartite temporal graph in recommender systems (DEBATER): the first is through reweighting the adjacency matrix (DEBATER-A), where a reliability score is defined to reweight the edges through both soft assignment and hard assignment; the second is through reweighting the loss function (DEBATER-L), where weights are generated to reweight user-item samples in the losses. Extensive experiments have been conducted to demonstrate the efficacy of our methods and illustrate how time information indeed helps identifying noisy edges.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems have been widely applied for studying user preferences [14]. As a powerful method for building recommender systems, collaborative filtering leverages the interaction history of users and items to mine latent information. Lately, with the development of Graph Neural Networks, neural graph collaborative filtering has received extensive attention, and many state-of-the-art methods have been proposed [7, 13, 27, 30] with their capability of capturing high-order interaction information. Despite the substantial progress, some challenges still exist, and one of them lies in the noisy nature of collected data [33], especially when the data is collected from implicit feedback (e.g., clicks) [25]. For example, users might inadvertently interact with some items (misclicks or merely out of curiosity), or some of the items might be intended for others (e.g., gifts). These noisy interactions are still considered as positive samples for model training, even though they do not align with the interacting user's preferences. Moreover, many malicious attacks [3, 11, 32] have been designed to craft fake interactions or users to bias recommender systems, resulting in sub-optimal prediction results. In the context of graph neural collaborative filtering, the problem becomes even worse as message passing in graph neural networks (GNNs) will amplify the impact of noisy interactions. Therefore, identifying and rectifying noisy interactions for neural graph collaborative filtering becomes important for building robust recommender systems that align with users' true interests.\nMany approaches have been proposed to tackle this issue. Depending on how samples are purified, they can be roughly divided into two categories. The first type of strategies directly remove the identified noisy samples from training data [4, 25]; while another group of works actively downweight those training samples during model training [5, 23, 26, 29]. To identify noisy interactions before rectifying, these works rely on trainable (e.g., a multilayer perceptron) or predefined functions (e.g., cosine similarity) to model the correlation between user/item embeddings and reliability.\nHowever, none of the existing work has focused on the natural advantage of time information for denoising. The advantages of leveraging interaction time for collaborative filtering are threefold. First, interaction times can be easily collected as every implicit feedback signal has an associated timestamp that we can leverage. Second, user-side temporal patterns can help identify some noisy interactions that cannot be distinguished without time information. For example, if a user usually watches horror movies at night and talk shows in the morning, a record of that user watching a horror movie in the morning is likely to be a noisy interaction, and this noisy interaction cannot be distinguished without the interaction timestamps. However, if the recommender system is aware of"}, {"title": "2 PRELIMINARIES", "content": "In this section, we first present notations used throughout the paper. Then we introduce the problem definition and briefly review neural graph collaborative filtering and denoising recommender system.\nProblem Definition. We formulate the temporal recommendation problem as follows.\nPROBLEM. Temporal Recommendation.\nInput: a temporal user-item interaction dataset D, a query from user u at time t;"}, {"title": "3 PROPOSED FRAMEWORK: DEBATER", "content": "To maximally improve the model's performance in prediction and denoising, we aim to find an effective and efficient way to incorporate time information into existing neural graph collaborative filtering frameworks by defining time-aware embeddings. However, it is highly challenging to ensure the dot product of embeddings to remain meaningful for preference prediction. In this section, we present our proposed method for yielding time-aware embeddings (Section 3.1) and how this method can be integrated with two denoising methods (Sections 3.2 and 3.3)."}, {"title": "3.1 Time-aware Embeddings", "content": "Given that most existing methods rely on embeddings to decide reliability or preferences, it is natural to define time-aware embeddings for boosting existing works' performance. However, the challenge lies in how to ensure the time-aware embeddings' dot products still preserve the user preference while reflecting temporal patterns as well. Therefore, we hope the dot products can be decomposed into two parts: $p_{ui}$ that represents user's general preference of item i, and $q_{ui}^t$ referring to user's temporal preference of item i at time t. Then we can define the overall time-aware preference $p_{ui}^t$ of user u over item i at time t as\n$p_{ui}^t = (e_u^t)^T e_i^t = p_{ui} + q_{ui}^t = e_u^T e_i + q_{ui}^t$\nwhere $e_u^t$, $e_i^t$ are time-aware embeddings of user and item. Similar to the formulation of $p_{ui}$, we define $q_{ui}^t$ as dot product\n$q_{ui}^t = (e_{ui})^T e_t$\nwhere $e_{ui}$ represents the joint embedding of user u and item i which can be defined through addition $e_{ui} = e_u + e_i$, and $e_t$ captures the temporal trends at time t. However, solving $e_u^t$ and $e_i^t$ in Eq.(11) is non-trivial, and embeddings instead of preferences are required for some loss functions (e.g., CL loss and AU loss) or weight generation functions (e.g., [26]). To resolve this issue, we redefine $q_{ui}^t$ as\n$q_{ui}^t = (e_{ui})^T e_t + ||e_t||_2^2$.\nAt a specific time t, $||e_t||_2^2$ is the same for all user-item pairs, therefore the additional term will not affect the ranking of items. With the new definition of $q_{ui}^t$, time-aware preference can be rewritten as\n$p_{ui}^t = (e_u^t)^T e_i^t = e_u^T e_i + q_{ui}^t = e_u^T e_i + (e_{ui})^T e_t + ||e_t||_2^2 = (e_u + e_t)^T (e_i + e_t)$\nand a natural choice for $e_u^t$ and $e_i^t$ is\n$e_u^t = e_u + e_t, e_i^t = e_i + e_t$.\nWith Eq. (14), $p_{ui}^t$ is capable of capturing both user-side and item-side temporal patterns. To capture item-side temporal patterns, we assume trained time embeddings can capture item temporal distribution. For example, we expect the embedding of a timestamp t near Christmas to be close to Christmas movies' embeddings. In this way, $e_i^T e_t$ will be positive for Christmas movies and users will likely be recommended with Christmas movies. Similarly, to capture user-side interaction temporal patterns, we expect the embedding of a timestamp to be close to the summation of user-item embeddings that tend to occur at this specific time.\nFinally, we design an encoder of timestamps to learn $e_t$. The idea of incorporating timestamps have been studied in sequential models [2, 12, 17], and we use a simplified variant of [17] to learn timestamp embeddings. For a timestamp $t \\in T$ where each dimension is a discrete value (e.g., representing [day, hour, minute, second]), each dimension will be first encoded into a one-hot vector and further embedded with a learnable matrix. Then embeddings of each dimension will be concatenated into the embedding of timestamp. Define learnable matrices as {$W_i$}$_{i=1,2,...,d_t}$, the embedding of timestamp t will be\n$e_t^i = one \\_ hot(t[i])W_i, i = 1, 2, ..., d_t$,\n$e_t = [e_t^1 || e_t^2 ||...|| e_t^{d_t}]$,\n$W_i \\in R^{m_i \\times (\\lfloor d/d_t \\rfloor + 1_{(i \\le d \\% d_t)})}$\nwhere $m_i$ is the number of distinct values in dimension i for all timestamps in T, $||$ denotes concatenation, % stands for modulo operator. $1_{(i \\le d \\% d_t)}$ is added to align the dimension of $e_t$ with the dimensions of $e_u$ and $e_i$. The additional space for storing {$W_i$}$_{i=1,2,...,d_t}$ is approximately $O((\\Sigma_i m_i)\\lfloor d/d_t \\rfloor)$, while the time complexity for calculating T embeddings of T timestamps is O(Td).\nIn the next two subsections, we will show how our proposed time-aware embeddings can be applied in two popular denoising strategies: reweighting the adjacency matrix and reweighting the loss function."}, {"title": "3.2 DEBATER-A: Reweighting the Adjacency Matrix", "content": "We generalize the reliability score function in the previous work [29] with time-aware embeddings as\n$r_{ui}^t = (cos(e_u^{(0)} + e_t, e_i^{(0)} + e_t) + 1)/2$\nNote that we differ from [29]: [29] uses the second and first layer embeddings $e_u^{(2)}$, $e_i^{(2)}$, $e_u^{(1)}$, $e_i^{(1)}$, while we use the zero layer embeddings for simplicity. Take [30] as the backbone model, the time-aware neural graph collaborative filtering and prediction process is formulated as\n$E_u^{(l)} = A E_u^{(l-1)}, E_i^{(l)} = A^T E_i^{(l-1)}, l = {1, ..., L}$\n$\\tilde{E}_u^{(l)} = A \\tilde{E}_u^{(l-1)} + \\Delta_u^{(l)}, \\tilde{E}_i^{(l)} = A^T \\tilde{E}_i^{(l-1)} + \\Delta_i^{(l)}, l = {1, ..., L}$.\n$p_{ui}^t = (e_u + e_t)^T (e_i + e_t), \\forall u \\in U, i \\in I, t \\in T$\nwhere A is the reweighted adjacency matrix in Eq.(9). We apply the noise sampling strategy that has been proved to be effective for denoising model with the reweighted adjacency matrix in [29],\n$\\hat{E}_u^{(l)} = Shuffle(\\tilde{E}_u^{(l)}), \\hat{E}_i^{(l)} = Shuffle(\\tilde{E}_i^{(l)}), l = {1, ..., L}$,\n$\\tilde{E}_u^{(l)} = RowNorm(\\hat{E}_u^{(l)}), \\tilde{E}_i^{(l)} = RowNorm(\\hat{E}_i^{(l)}), l = {1, ..., L}$,\nwhere $\\hat{E}_u^{(l)}$, $\\hat{E}_i^{(l)}$ are row-normalized $\\tilde{E}_u^{(l)}$, $\\tilde{E}_i^{(l)}$, and Shuffle(X) shuffles the rows in X. In this noise sampling strategy, noises are sampled from the embedding space and this helps mimicking real graph structural attacks. Finally, we jointly train the timestamp encoder and parameters of neural graph collaborative filtering model ($E_u^{(0)}$, $E_i^{(0)}$) with a weighted combination of BPR, CL and AU loss\n$L = L_{BPR} + \\lambda_1 L_{CL} + \\lambda_2 L_{AU}$\nwhere $\\lambda_1$, $\\lambda_2$ are weights of CL loss and AU loss. Compared to the loss function in [30], we replace $L2$ regularization term with the AU loss because AU loss has shown its effectiveness in previous denoising recommender systems [26]. In addition, time-aware embeddings are applied in loss functions. For example, the formula of"}, {"title": "3.3 DEBATER-L: Reweighting the Loss Function", "content": "In this section, we present how to integrate our time-aware embeddings with another type of denoising method by reweighting the loss functions. We extend the weight generator in the previous work [26] with timestamp embeddings as\n$w_{ui} = W(e_u||e_i||e_{t_{ui}})$\nwhere is W(.) is implemented as a 3-layer Multi-layer Perceptron (MLP). The backbone model is chosen as [30] with the same time-aware prediction process as in Eq. (22). The backbone model and weight generator W(\u00b7) are alternatively updated during the whole training process. Same as DEBATER-A, the loss function for training backbone model is formulated as a weighted combination of BPR, CL and AU loss, only that the loss terms are reweighted by $w_{ui}$. For example, the reweighted BPR loss with time-aware embeddings is\n$L_{BPR} = \\frac{1}{|O|} \\sum_{(u,i,j,t_{ui}) \\in O} - logo(w_{ui} (e_u + e_{t_{ui}})^T (e_i + e_{t_{ui}})\n\\\\ - w_{uj} (e_u + e_{t_{ui}})^T (e_j + e_{t_{ui}}))$.\nThen W() is trained by a gradient matching objective, following the training scheme of [26] with an extension to time-aware embeddings. The gradient matching objective matches the gradient of the BPR loss with the gradient of the AU loss with respect to backbone model parameters and is formulated as\n$\\theta = [E_u^{(0)} || E_i^{(0)}] \\in R^{(|U|+|I|) \\times d}$,\n$G_1 = \\nabla_{\\theta} L_{BPR}, G_2 = \\nabla_{\\theta} L_{AU}$\n$L_{GM} = \\sum_{i=1}^{|U|+|I|} (1 - \\frac{G_1[i,:]^T G_2[i,:]}{||G_1[i,:]||_2||G_2[i,:]||_2})$.\nwhere both $L_{BPR}$, $L_{AU}$ are reweighted objectives with time-aware embeddings from the backbone model."}, {"title": "4 EXPERIMENTS", "content": "In this section, we present the experimental evaluation of our proposed methods. The experiments are designed to answer the following questions:\n\u2022 RQ1. How does the proposed approach perform compared to state-of-the-art denoising and general neural graph collaborative filtering methods?\n\u2022 RQ2. To what extent does the time-aware embeddings contribute to the overall performance?\n\u2022 RQ3. Whether the proposed approach successfully capture the temporal patterns?"}, {"title": "4.1 Experimental Settings", "content": "Datasets. To evaluate the performance of DEBATER, we conduct experiments on four real-world public datasets: ML-100K [6], ML-1M [6], Yelp\u00b9, Amazon (Movies and TV) [15]. Data statistics are summarized in Table 4. The available timestamps in the datasets are preprocessed into the following formats\n\u2022 ML-100K: Day-Hour-Minute-Second\n\u2022 ML-1M: Month-Day-Hour-Minute-Second\nMetrics. We use Prec@k (Precision), Recall@k and NDCG@k (Normalized Discounted Cumulative Gain) as evaluation metrics, where k is set to 10 and 20.\nBaselines. We compare our proposed approach with four state-of-the-art general recommender systems and three state-of-the-art denoising recommender systems.\n\u2022 Bert4Rec [19] is a BERT based sequential recommendation model.\n\u2022 CL4SRec [28] applies contrastive learning to improve user representations quality in sequential models.\n\u2022 LightGCN [8] is a neural graph collaborative filtering model that simplifies GCNs for recommendation by removing feature transformation and nonlinear activation."}, {"title": "4.2 Main Results (RQ1)", "content": "The experimental results on vanilla datasets are shown in Table 2 and results on noisy datasets are shown in Table 3. Either DEBATER-A or DEBATER-L reaches the best performance in most cases, and DEBATER-L, which follows the training scheme of BOD [26], outperforms BOD. Although Bert4Rec outperforms our model on Amazon dataset, it performs much worse on others. The relative improvements in percentage against other 7 baselines per metric are shown in Table 7.\nComparing DEBATER-A and DEBATER-L, we can observe that DEBATER-L tends to have higher NDCG while DEBATER-A tends to have a higher precision and recall. This means that DEBATER-A is more suitable for retrieval tasks, while DEBATER-L is more suitable for ranking tasks. Furthermore, DEBATER-L is more robust compared with DEBATER-A, as DEBATER-L outperforms DEBATER-A on more metrics on noisy datasets compared to vanilla datasets."}, {"title": "4.3 Ablation Studies (RQ2)", "content": "We further conduct ablation studies on ML-100K and ML-1M dataset to evaluate the contribution of time-aware embeddings on the overall performance. We train DEBATER-A/DEBATER-L by removing time-aware embeddings from reliability score function or weight generator (R()/W()), and removing time-aware embeddings from loss functions and prediction. The experimental results on vanilla datasets and noisy datasets are shown in Table 5 and Table 6 respectively. For DEBATER-A, we can observe that removing time-aware embeddings in R() or losses and prediction both result in a decrease of performance on both vanilla and noisy datasets, indicating time-aware embeddings in DEBATER-A have effectively contribute to both model performance and robustness. Similarly, for DEBATER-L, we can observe that removing time-aware embeddings from losss and prediction leads to an overall worse performance on vanilla dataset. However, removing time information in W() reaches a slightly better performance on vanilla dataset, but suffers from a performance drop on noisy dataset. This means that time-aware embeddings in W(.) only contributes to robustness and might penalize model utility. On the other hand, removing time information in losses and prediction achieves better performance on noisy datasets, indicating time information in losses and prediction could affect model robustness as well."}, {"title": "4.4 Noise Sampling Strategy (RQ3)", "content": "To understand if our framework successfully recognizes the temporal patterns and learns what kinds of noise that our framework is detecting, we construct four additional noisy datasets based on ML-100K by: sampling items by hourly-based item popularity (\u221d $p$), sampling items by overall item popularity (\u221d $p_i$), reversely sample items by hourly-based item popularity (\u221d $1/p$) and reversely sampling items by overall item popularity (\u221d $1/p_i$). By sampling proportional to general/hourly item popularity, we enhance the item-side general/hourly patterns and make the noises harder to distinguish from the clean data; on the contrary, reversely sampled noises are more distinct from original patters and can be easily detected. Our experimental results in Table 8 align with this observation. We can also observe that DEBATER-A/DEBATER-L can easily separate the noise that is not coherent to items' temporal pattern (\u221d $1/p$) and reach comparable performance as on vanilla datasets. DEBATER-A/DEBATER-L are more effective on identifying noise sampled from general item popularity compared to identifying noise sampled from hourly item popularity. These observations reflect that DEBATER-A/DEBATER-L can indeed successfully learn item-side temporal patterns and use these patterns to help denoising."}, {"title": "5 RELATED WORKS", "content": "Denoising Recommender Systems. The goal of denoising recommender systems is to identify noisy edges according to different principles and purify the interactions in different learning phases. For example, [20, 29] determine interactions that connect dissimilar users and items as noisy and downweight those noisy edges in the adjacency matrix. [23] observes that noisy interaction tends to have larger losses, and prunes those positive samples with larger losses in the training stage. [5] instead memorizes clean samples in the early training stage, and reweights the objective function by the similarity between samples and those memorized clean samples. Another group of work achieves robustness by agreements. [26] trains a weight generator that maximizes the agreement between gradients of different objective functions, then the weights are applied to reweight objective functions to train backbone models. [24] maximizes the agreement between different models as they observe that different models make relatively similar predictions for clean samples. There also exist some works that are not explicitly designed for denoising recommender systems but potentially increase the models' robustness. [27] generates different views of graphs through graph structure augmentation, and maximizes the agreement between differen views through contrastive learning. [30] instead augments user and item embeddings by adding perturbation to GNN layers, and maximizes agreement between different views of embeddings.\nTime-aware Recommender Systems. Temporal order has been widely applied in sequential recommendation [10, 16, 19]. Lately, several works investigate the possibility of directly incorporating timestamps into sequential models [2, 12, 17]. However, limited attempts have been made to incorporate time information in the context of collaborative filtering. [1] formulates the temporal order of interactions into an item-item graph and leverages information from three different views: user-user, user-item and item-item."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose a simple yet effective mechanism, DE-BATER, for generating time-aware user/item embeddings, which integrates user/item embeddings with timestamp embeddings. To leverage this mechanism for denoising bipartite temporal graphs in recommender systems, we apply time-aware embeddings in two strategies: reweighting the adjacency matrix (DEBATER-A) and reweighting the loss function (DEBATER-L). DEBATER-A reweights the adjacency matrix through a cosine-based reliability score function, while DEBATER-L reweights the loss function through a weight generator. Both reliability score function and weight generator take time-aware embeddings as input, and time-aware embeddings are also integrated into the loss function and prediction. Extensive experiments on four real-world datasets demonstrate the efficacy of our proposed approach. The mechanism we propose for generating time-aware embeddings is not limited to these two strategies, but could be easily integrated with other neural graph collaborative filtering frameworks as well. Moving forward, future works will focus on investigating more time-aware neural graph collaborative filtering algorithms by integrating our method. We will also consider expanding the proposed method to denoise not only user-item interactions but also noise in user profiles or item attributes."}]}