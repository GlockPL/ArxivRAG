{"title": "On the Expressive Power of Sparse Geometric MPNNS", "authors": ["Yonatan Sverdlov", "Nadav Dym"], "abstract": "Motivated by applications in chemistry and other sciences, we study the expressive power of message-passing neural networks for geometric graphs, whose node features correspond to 3-dimensional positions. Recent work has shown that such models can separate generic pairs of non-equivalent geometric graphs, though they may fail to separate some rare and complicated instances. However, these results assume a fully connected graph, where each node possesses complete knowledge of all other nodes. In contrast, often, in application, every node only possesses knowledge of a small number of nearest neighbors.\nThis paper shows that generic pairs of non-equivalent geometric graphs can be separated by message-passing networks with rotation equivariant features as long as the underlying graph is connected. When only invariant intermediate features are allowed, generic separation is guaranteed for generically globally rigid graphs. We introduce a simple architecture, EGENNET, which achieves our theoretical guarantees and compares favorably with alternative architecture on synthetic and chemical benchmarks.", "sections": [{"title": "Introduction", "content": "Geometric graphs are graphs whose nodes are a 'position' vector in \\( \\mathbb{R}^d \\) (typically \\( d = 3 \\) ) and whose symmetries include the permutation symmetries of combinatorial graphs and translation and rotation of the node positions. Geometric graphs arise naturally in learning applications for chemistry, physical dynamics, and computer vision as natural models for molecules, particle systems, and 3D point clouds. These applications motivated many learning models for geometric graphs, which were often inspired by 'standard' graph neural networks (GNNs) for combinatorial graphs [30, 56, 27]. Sub-sequentially, several theoretical works aimed primarily at understanding the expressive power and limitations of GNNs for geometric graphs.\nGNNs for geometric graphs produce global graph features invariant to geometric graph symmetries. The expressive power of GNNs is typically assessed primarily by their ability to assign different global features to pairs of geometric graphs that are not geometrically isomorphic.\nRecent research on this problem has uncovered several interesting results, mainly assuming that the graphs are full (all distances between node features are considered). Under the full graph assumptions, several models for geometric GNNs are complete, that is, capable of separating all pairs of non-isomorphic graphs. However, this typically comes at a relatively high computational price. Examples of complete models include (d \u2013 1)-WL-based GNNs [47, 49, 58], GNNs based on sub-graph aggregation [59], and message passing models based on arbitrarily high-dimensional irreducible representations [13, 27, 17, 38]. This paper focuses on more efficient geometric GNNs: message-passing neural networks (MPNN) based on simple invariant and equivariant features. In [42, 58], it was shown that, even under the full graphs assumption, invariant MPNN-based networks are not complete. In contrast, [59, 49] proved that these models are 'generically complete,' which means"}, {"title": "Question 1.", "content": "For which graphs, other than the full graphs, are geometric message-passing neural networks generically complete?"}, {"title": "Question 2.", "content": "Can we devise a simple E-GGNN architecture that is maximally expressive hence, generically complete on connected graphs?"}, {"title": "2 Setup: Geometric Graph Neural Networks and Rigidity Theory", "content": "Geometric Graphs. In our discussion of geometric graphs and GNNs, we loosely follow the definitions of [50]. We define a geometric graph as a pair \\( G = (A, X) \\). The matrix \\( A \\in \\{0,1\\}^{n \\times n} \\) is an adjacency matrix, which fully defines a combinatorial graph. Accordingly, we sometimes use statements like 'the graph A.' We assume for simplicity that the graph nodes are the integers 1, ..., n. The matrix \\( X = (x_1,...,x_n) \\in \\mathbb{R}^{n \\times d} \\) denotes node positions. We denote the set of all geometric graphs with coordinates in \\( \\mathbb{R}^d \\) by \\( \\mathcal{G}(d) \\), and the subset of graphs with \\( \\leq N \\) nodes by \\( \\mathcal{G}_N(d) \\). In Appendix H, we discuss how to incorporate additional node and edge features into our model and theoretical results.\nThe symmetry group of geometric graphs is the product of the permutation group \\( S_n \\) and the group of rigid motions \\( \\mathcal{E}(d) \\). The action of a permutation matrix \\( P \\) on a geometric graph is given by \\( PG := (PAP^T, PX) \\). A rigid motion in \\( \\mathcal{E}(d) \\) is a rotation \\( Q \\in O(d) \\) and a translation vector \\( t\\in \\mathbb{R}^d \\). They act on \\( X \\) by applying the rotation and translation to all its coordinates, \\( x_i \\rightarrow Qx_i + t \\).\nTwo geometric graphs \\( G \\) and \\( H \\) are geometrically isomorphic if there exists a transformation in \\( S_n \\times \\mathcal{E}(d) \\) which can be applied to \\( G \\) to obtain \\( H \\). We will say that a function \\( F \\) defined on \\( \\mathcal{G}(d) \\) is invariant if \\( F(G) = F(H) \\) for all \\( G \\) and \\( H \\) which are geometrically isomorphic.\nThe expressive power of an invariant model \\( F \\) is closely related to its ability to separate pairs \\( G, H \\) that are not geometrically isomorphic: clearly, if \\( F \\) cannot separate \\( G \\) from \\( H \\), then \\( F \\) will not be a good approximation for functions \\( f \\) for which \\( f(G) \\neq f(H) \\). Conversely, if \\( F \\) can separate all pairs of geometric graphs, then it can be used to approximate all continuous invariant [55, 49] and equivariant [32, 57] functions. In Appendix F, we discuss the connection between approximation and separation in the context of our results.\nGeneric separation and Rigidity Theory As discussed in the introduction, our paper focuses on generic separation. The results we present in this section are primarily influenced by rigidity theory, so we also use this field's common notion of 'generic'. We will say that \\( X \\in \\mathbb{R}^{d \\times n} \\) is generic if it is not the root of any multivariate polynomial \\( p : \\mathbb{R}^{d \\times n} \\rightarrow \\mathbb{R} \\) with rational coefficients. The genericity assumption guarantees that \\( X \\) avoids many degeneracies, making separation easier. For example, for generic \\( X \\) all pairs of points have different norms, and every \\( d \\) tuple of points \\( x_{i_1},...,x_{i_d} \\) is full rank because \\( ||x_i|| - ||x_j||^2 \\) and \\( det[X_{i_1},...,X_{i_d}] \\) are polynomials with natural coefficients. We note that Lebesgue almost all point clouds in \\( \\mathbb{R}^{d \\times n} \\) are generic [23].\nA globally rigid graph is a central topic in rigidity theory:\nDefinition 2.1 (Globally rigid). ([39]) A geometric graph \\( G = (A, X) \\) is globally rigid in \\( \\mathbb{R}^d \\), if the only \\( X' \\in \\mathbb{R}^{n \\times d} \\) which satisfy that \\( ||x_i - x_j || = ||x'_i - x'_j|| \\) for all edges \\( (i, j) \\) in \\( A \\), are those that are related to \\( X \\) by a rigid motion.\nA combinatorial graph \\( A \\) is generically globally rigid in \\( \\mathbb{R}^d \\), if the geometric graph \\( G = (A, X) \\) is globally rigid in \\( \\mathbb{R}^d \\) for every generic \\( X \\in \\mathbb{R}^{n \\times d} \\)."}, {"title": "Definition 2.2.", "content": "Let \\( d \\) be a natural number. Following [59], we will say that an invariant function \\( F \\) defined on \\( \\mathcal{G}(d) \\), identifies a geometric graph \\( G \\), if for every \\( \\hat{G} \\in \\mathcal{G}(d) \\), we have that \\( F(G) = F(\\hat{G}) \\) if and only if \\( G \\) and \\( \\hat{G} \\) are geometrically isomorphic.\nWe say that \\( F \\) generically identifies a combinatorial graph \\( A \\), if \\( F \\) can identify \\( G = (A, X) \\) for every generic \\( X \\).\nWe say that \\( F \\) generically fails to identify a combinatorial graph \\( A \\), if \\( F \\) does not identify \\( G = (A, X) \\) for every generic \\( X \\).\nNote that the fact that \\( F \\) does not generically identify \\( A \\) only guarantees that there exists a generic \\( X \\) so that \\( F \\) fails to identify \\( (A, X) \\). The statement 'F generically fails to identify' is, apriori, significantly stronger. As discussed in the introduction, our goal will be to classify which graphs are generically identifiable by standard message-passing-based networks for geometric graphs, which can be classified as either I-GGNN or E-GGNN. We will now define these concepts."}, {"title": "E-GGNN", "content": "In the context of combinatorial graphs (with no geometric information), [8] showed that many famous graph neural networks are instantiations of MPNNs. The goal of [50] was to show, analogously, that many neural networks for geometric graphs follow a generalized MPNN framework. Loosely following [50], we define Equivariant Geometric Graph Neural Networks (E-GGNN) to be functions defined by a sequence of layers that propagates vector features from iteration \\( t \\) to \\( t + 1 \\) via a learnable function \\( f_\\theta^{(t)} \\) (at initialization we set \\( v_i^{(0)} = 0 \\)).\n\\[ v_i^{(t+1)} = f_\\theta^{(t)} (\\{\\{v_j^{(t)}, x_i - x_j\\}\\}, | j \\in N_i) . \\]\nTo ensure the equivariance of the construction, the function \\( f_\\theta^{(t)} \\) is required to be rotation equivariant. Translation invariance is implicitly guaranteed by the translation invariance of \\( x_i - x_j \\), and the requirement that \\( f_\\theta^{(t)} \\) is defined on multi-sets implicitly enforces permutation equivariance.\nWe note that for the sake of simplicity, we don't require a combine step that incorporates \\( v_i^{(t)} \\) in the process of computing \\( v_i^{(t+1)} \\) since we find that this step has little influence on our theoretical and empirical results. We also note that the vector \\( v_i^{(t)} \\) is not necessarily \\( d \\) dimensional, and the action of \\( O(d) \\) on \\( v_i^{(t)} \\) is allowed to change from layer to layer. For example, in the EGENNETarchitecture we will introduce later on, each \\( v \\) will be a \\( d \\times m \\) matrix, and the action of \\( O(d) \\) will be multiplication on each of the \\( m \\) coordinates.\nAfter \\( T \\) iterations of E-GGNNs, a permutation, rotation, and translation invariant feature vector can be obtained via a two-step process involving a rotation-invariant function \\( f_{inv} \\) and a multiset READOUT function:\n\\[ s_i = f_{inv}((v_i^{(T)})), s_{global} = READOUT\\{\\{s_1, .., s_n\\}\\} \\]\nAs shown in [50], many popular geometric graph neural networks can be seen as instances of E- GGNNs with a specific choice of functions \\( f_\\theta^{(t)} \\) and readout functions. Examples include EGNN [30], TFN [13], GVP [22], and MACE [35]. In Section 5.1, we present EGENNET, our instantiation of this framework."}, {"title": "I-GGNNS", "content": "An important subset of E-GGNNs are invariant GGNNs (I-GGNNs). These networks only maintain scalar features \\( s_i^{(t)} \\) and replace \\( x_i - x_j \\) with its norm to obtain\n\\[ s_i^{(t+1)} = f_\\theta^{(t)} (\\{\\{s_j^{(t)}, ||x_i - x_j||\\}\\}, | j \\in N_i) \\]\nA global invariant feature is obtained via a permutation invariant readout function as in equation 2. An advantage of I-GGNNs is that there are no rotation equivariance constraints on \\( f_\\theta^{(t)} \\). Therefore, any"}, {"title": "Theorem 3.1.", "content": "[expressive power of I-GGNN] Let \\( d \\) be a natural number. Let \\( F \\) be an I-GGNN. Let \\( A \\) be a graph not generically globally rigid on \\( \\mathbb{R}^d \\). Then, \\( F \\) generically fails to identify \\( A \\).\nConversely, if \\( A \\) is generically globally rigid on \\( \\mathbb{R}^d \\) and \\( F \\) is a maximally expressive I-GGNN with depth \\( T = 1 \\), then \\( F \\) generically identifies \\( A \\)."}, {"title": "4 Expressive power of E-GGNN", "content": "When using E-GGNN rather than I-GGNN, we will have generic separation if and only if the graph is connected.\nTheorem 4.1. [expressive power of E-GGNN] Let \\( d \\) be a natural number. Let \\( F \\) be an E-GGNN. Let \\( A \\) be a disconnected graph. Then, \\( F \\) generically fails to identify \\( A \\).\nConversely, if \\( A \\) is connected and \\( F \\) is a maximally expressive E-GGNN with depth \\( T > d + 1 \\), then \\( F \\) generically identifies \\( A \\)."}, {"title": "5 Building a generically maximally expressive E-GGNN", "content": "To obtain generic separation guarantees as in Theorems 3.1, 4.1 for a practical instantiation of E-GGNN (or I-GGNN), we will need to show that the instantiation is fully expressive. This type of question was studied rather extensively for combinatorial graphs [46, 33], and these results can be used to show that some simple realizations of I-GGNN are maximally expressive. We discuss this in more detail in Appendix C.\nFor E-GGNN, the requirement that \\( f_\\theta^{(t)} \\) is simultaneously injective (up to permutation) and equivariant to rotations seems to be challenging to attain with a simple E-GGNN construction, and indeed, [50] who suggested the notions of maximally expressive E-GGNN, did not give any indication of how they could be constructed in practice. Moreover, in Appendix D, we show that a maximally expressive E-GGNN can separate all pairs of full graphs and separate geometrically all pairs of connected graphs, indicating constructing such a network may be computationally expensive, as currently, such guarantees are only achieved by networks of relatively high complexity based on 2-WL subgraph aggregation techniques.\nIn our setting, however, the problem is less severe since, from the outset, we are only interested in analyzing network behavior on generic graphs. Accordingly, we would like to require our E-GGNN to be maximally expressive only for generic graphs. It turns out that this goal can be achieved with a straightforward E-GGNN architecture, which we name EGENNET. Our next step will be to present this architecture."}, {"title": "5.1 The EGENNET architecture", "content": "The EGENNET architecture we suggest resembles an EGNN architecture [30] with multiple equivariant channels as in [52]. The EGENNET architecture depends on two hyper-parameters: depth \\( T \\) and channel number \\( C \\). The input to the architecture is a geometric graph \\( (A, X) \\). The architectures then maintains, at each iteration \\( t \\) and node \\( i \\), an 'equivariant' node feature with \\( C \\) channels\n\\[ v_i^{(t)} = (v_{i,1}^{(t)},...,v_{i,C}^{(t)}), v_{i,c}^{(t)} \\in \\mathbb{R}^d, c = 1,..., C. \\]\nThese feature values are initialized to zero and are updated via the rotation and permutation equivariant aggregation formula\n\\[ v_{i,q}^{(t+1)} = \\sum_{j \\in N_i} \\psi_\\theta^{(t,q,0)} (||x_i - x_j ||, ||v_j||) (x_i - x_j ) + \\sum_{c=1}^C \\psi_\\theta^{(t,q,c)} (||x_i - x_j ||, ||v_{j,c}^{(t)}||) \\]"}, {"title": "Theorem 5.1.", "content": "Let \\( d, N, T \\) be natural numbers. Let \\( F \\) be a maximally expressive E-GGNN of depth \\( T \\), and let \\( F_\\theta \\) denote the EGENNET architecture with depth \\( T \\) and \\( C = 2N^{d+1} \\) channels. Then, for Lebesgue almost every choice of network parameters \\( \\theta \\), we have that for all generic \\( G, \\hat{G} \\in \\mathcal{G}_N(d) \\),\n\\[ F(G) = F(\\hat{G}) \\iff F_\\theta(G) = F_\\theta(\\hat{G}) \\]"}, {"title": "6 Experiments", "content": "6.1 Separation experiments\nWe begin with experiments to corroborate our theoretical findings. We consider examples where the graph A is a line graph, which is not generically globally rigid. We choose two pairs of geometric line graphs, Pair a and Pair b, depicted in Figure 2. These pairs have identical distances along graph edges but are not isomorphic. Therefore, they cannot be distinguished by I-GGNN but (at least when generic) can be distinguished by E-GGNN and our EGENNETarchitectures. Following the protocol from [50], we construct binary classification problems from pair a and pair b and apply our E-GGNN architecture EGENNET, as well as three popular I-GGNN architectures: SchNet [12], DimeNet [19], and SphereNet [28]."}, {"title": "7 Limitation and Future Work", "content": "In this work, we characterized the generic expressive power of I-GGNN and E-GGNN, showed that EGENNETis a generically maximally expressive E-GGNN, and showed the effectiveness of this architecture on chemical regression tasks. A limitation of our work is that it may struggle with symmetric and non-generic input. Future work could consider assessing the expressive power of non-NPNN models like geometric 2-WL on sparse graphs to address such input efficiently."}, {"title": "A Related Work", "content": "This section discusses related work that is not discussed in the main text.\nEquivariant models In the context of learning on geometric graphs, models should be equivariant to the joint action of permutations, rotations and translations. Besides the models discussed in the paper, some prominent models with this equivariant structure include Cormorant [15], Vector Neurons [26], and SE(3) transformers [18].\nCompleteness, universality and generic completeness The main text discusses related works on completeness focused on methods coming from the geometric learning community. Other complete methods, generally using similar ideas, appeared independently in the more chemistry-oriented community [51, 53, 7, 24]. Completeness in 2D was achieved in [36], and completeness with respect to rigid motions (without permutations) was achieved by Comenet [44]. Comenet also uses some genericity assumptions (e.g., that there is a well-defined notion of nearest neighbors).\nThe main text discussed how generic completeness can be achieved using I-GGNN when the graph is globally rigid. A similar result can be obtained when considering the list of all distances [39]. Generic completeness for full graphs can also be obtained using more complex distance-based features [45] or shape principal axes [29] (under the generic assumption that the principal eigenvalues are distinct)."}, {"title": "B Implementation details", "content": "Here, we detail the hyper-parameter choice. For all experiments, we use AdamW optimizer [9]. We use a varying weight decay of 1e-4, 1e-5 and learning rate of 1e-4. Reduce On Platue scheduler is used with a rate decrease of 0.75 with the patience of 15 epochs. We use two blocks with 40 channels in the challenging point cloud separation experiments. For k-chain experiments, we repeat each experiment 10 times, with 10 different seeds, and set the number of channels to be 60, and a varying number of blocks, as detailed in the table. For power graph experiments, we repeat each experiment 10 times, with 10 different seeds, and set the number of channels to be 60 and 3 blocks. For chemical property experiments, all the procedure is taken from [61]: each dataset is partitioned randomly into three subsets: 70% for training, 10% for validation, and 20% for test. We set the number of channels to be 256 and used 6 blocks. Each model is trained over 1,500 epochs. Experiments are repeated three times for all eight regression targets, and the results reported correspond to the model that performs best on the validation set in terms of Mean Absolute Error (MAE). In contrast to the experiments in [61], we didn't consider the EE dataset as it's currently not public on GitHub. We utilize PyTorch and PyTorch-Geometric to implement all deep learning models. All experiments are conducted on servers with NVIDIA A40 GPUs, each with 48GB of memory. All dataset details, including statistics, can be found in [61]."}, {"title": "C Building perfect I-GGNN", "content": "Here, we describe in detail how to construct a perfect I-GGNN. We will assume that we consider geometric graphs with up to N nodes. The main ingredients needed are aggregation functions and READOUT function that are injective on the space of all multisets of size \\( < N \\). [46] proved that the sum of projections and analytic activation can be used to construct such layers. Formally, be \\( M \\subseteq \\mathbb{R}^D \\) semi-analytic set of dimension D, and \\( N \\) vectors in \\( \\mathbb{R}^d \\) denoted by \\( X \\). Be \\( A \\in \\mathbb{R}^{(2D+1)\\times d} \\), \\( b \\in \\mathbb{R}^{2D+1} \\), \\( \\sigma: \\mathbb{R} \\rightarrow \\mathbb{R} \\) analytic non-polynomial activation, then for Lebesgue almost every \\( A, b \\)\n\\[ f_{A,b}(X) = \\sum_{i=1}^N \\sigma(A\\cdot x_i + b) \\]\nis injective up to permutation. Thus, the composition of such blocks and such READOUT function with dimension \\( D = d \\cdot N \\) yields a maximally expressive model on all input domain. Here, D is the intrinsic dimension of all features, ultimately produced by the \\( D \\) dimensional input space."}, {"title": "D Separation Power of Fully Maximally expressive E-GGNN", "content": "In the main text, we proposed EGENNET a simple generically maximal expressive E-GGNN. In this section, we show that if we have a maximally expressive E-GGNN on all input domain, we can reconstruct a full graph up to group action, and if the graph is connected, we can reconstruct its point cloud. This indicates that fully maximally expressive E-GGNN is as challenging as achieving completeness, which generally requires complex methods like 2-WL-based GNNs.\nTheorem D.1. Let \\( G = (A, X) \\) be a geometric graph, where \\( A \\) is the full graph on \\( n \\) nodes, and \\( X \\) is an arbitrary \\( d \\times n \\) matrix, then one iteration of maximally expressive E-GGNN can reconstruct \\( X \\).\nProof. Assume we run one iteration of E-GGNN on our geometric graph, then for each node \\( i \\), we know up to some rotation \\( Q_i \\)\n\\[ \\{Q_i \\cdot (x_i - x_j)|j \\in [n]\\} \\]\nChoosing \\( i = 1 \\), and \\( x_i = 0 \\), we have all other positions \\( Q_i \\cdot x_j, j \\neq i \\), thus up to translation (as we set \\( x_i \\)), rotation \\( Q_i \\) and node permutation, we can reconstruct our point cloud.\nWe now show that using \\( n \\) iterations of \\( E \\)-GGNN can reconstruct the point cloud of each connected graph.\nTheorem D.2. Assuming a connected geometric graph \\( G = (A, X) \\) with \\( n \\) nodes, then \\( n \\) iteration of maximally expressive E-GGNN can reconstruct the point cloud.\nProof. Assume we run \\( n \\) iterations of a maximally expressive E-GGNN on our connected graph. Then, by Theorem 4.1, we can reconstruct for each node \\( i \\) its \\( n \\)-hop up to rotation \\( Q_i \\). As the \\( n \\)-hop of a node is all other nodes (the graph's diameter is at most \\( n \\)), we reconstruct the full geometric graph up to rotation, and by theorem D.1, we can reconstruct the point cloud."}, {"title": "E I-GGNN+", "content": "Here, we present the definition of I-GGNN presented by [50], denoted by I-GGNN+, and show that it can separate all geometric full graphs. After, we explain why this model is strictly stronger than our I-GGNN.\n[50] defines (neglecting the update step also used there)\n\\[ v_i^{(t+1)} = f_\\theta^{(t)} (\\{\\{v_j^{(t)}, x_i - x_j\\}\\}, | j \\in N_i) \\) . \\]\nSuch that \\( f_\\theta \\) holds that for all \\( Q \\in O(d) \\)\n\\[ f_\\theta^{(t)} (\\{\\{v_j^{(t)}, x_i - x_j\\}\\}, j \\in N_i) = f_\\theta^{(t)} (\\{\\{v_j^{(t)}, Q \\cdot (x_i - x_j)\\}\\}, | j \\in N_i) \\)\nI-GGNN+ is maximally expressive if \\( f_\\theta^{(t)} \\) holds:\n\\[ f_\\theta^{(t)} (\\{\\{v_j^{(t)}, x_i - x_j\\}\\}, j \\in N_i) = f_\\theta^{(t)} (\\{\\{v_j^{(t)}, \\hat{x_i} - \\hat{x_j}\\}\\}, | j \\in N_i) \\) \\iff \\{\\{v_j^{(t)}, x_i - x_j\\}\\}, | j \\in N_i \\} = \\{\\{v_j^{(t)}, Q \\cdot (\\hat{x_i} - \\hat{x_j})\\}\\}, | j \\in N_i \\} \\exists Q\\in O(3) : \\{\\{v_j^{(t)}, \\hat{x_i} - \\hat{x_j}\\}\\}, | j \\in N_i \\} = \\{\\{v_j^{(t)}, Q \\cdot (\\hat{x_i} - \\hat{x_j})\\}\\}, | j \\in N_i \\} \\]\nWe now prove that if we run one iteration of maximally expressive I-GGNN+ on a full graph, we can reconstruct it.\nTheorem E.1. Let \\( G = (A, X) \\) be a geometric graph, where \\( A \\) is the full graph on \\( n \\) nodes, and \\( X \\) is an arbitrary \\( d \\times n \\) matrix, then one iteration of maximally expressive I-GGNN+ can reconstruct \\( X \\).\nProof. Assume we run one iteration of I-GGNN+. Then, for each node \\( i \\), we have\n\\[ f_\\theta^{(t)} (\\{\\{v_j^{(t)}, x_i - x_j\\}\\}, | j \\in [n]) \\) \\]\nthen up to some rotation \\( Q_i \\in O(3) \\), we know\n\\[ \\{\\{Q_i (x_i - x_j), | j \\in [n]\\}\\} \\]\nThen, we can reconstruct the point cloud by Theorem D.1."}, {"title": "F Approximation and separation", "content": "It is known that a tight connection exists between the approximation of invariant functions and complete invariant models [49].\nIn the context of our paper, we discussed generic completeness. In particular, we showed in Corollary 5.2 that EGENNET with random parameters is complete on \\( \\mathcal{G}_{generic}(d, N) \\), which denotes the set of all geometric graphs \\( G = (A, X) \\) with exactly \\( N \\) nodes, \\( A \\) connected, and \\( X \\in \\mathbb{R}^{d \\times n} \\) generic. Accordingly, we can obtain the following theorem:\nTheorem F.1. Let \\( K \\subseteq \\mathcal{G}_{generic}(d, N) \\) be a compact set, \\( f_{sep} : K \\rightarrow \\mathbb{R}^m \\) a continuous function invariant to rotations, translations, and permutations. Then \\( f_{sep} \\) is injective on \\( K \\) (up to symmetries) if and only if every continuous invariant \\( f : K \\rightarrow \\mathbb{R}^M \\) can be approximated uniformly on \\( K \\) using functions of the form \\( N(f_{sep}) \\), where \\( N \\) is a fully connected neural network.\nProof. Assume, on the one hand, \\( f_{sep} \\) is injective on \\( K \\) (up to symmetries). Given a continuous \\( f : K \\rightarrow \\mathbb{R}^M \\) we want to approximate, then, by [49], \\( \\forall\\epsilon > 0 \\exists \\) neural network \\( N \\):\n\\[ \\sup_{G \\in k}|f(G) \u2013 N(f_{sep}(G))| < \\epsilon \\]\nOn the other hand, assume by contradiction there exists \\( G_1 = (A_1, X_1) \\) and \\( G_2 = (A_2, X_2) \\) which are not related by translation, rotation or permutation, such that \\( f_{sep}(G_1) = f_{sep}(G_2) \\).\nDefine \\( f(G) = \\text{Inf}_{g \\in \\mathcal{G}}||(A, X) \u2013 g \\cdot (A_1, X_1)||, \\) where \\( G = (A, X) \\), and \\( \\mathcal{G} \\) is the group of permutations, rotations, and translation. Note that this minimum is obtained as the product group of rotation and permutation is compact, and the translation can be bounded by the sum of the norms of the two graphs. Then, according to the Tikhonov theorem, our group is compact, and we take a minimum continuous function over a compact set. Note that \\( f \\) is an invariant, continuous function such that \\( f(G_1) = 0 \\), and \\( f (G_2) > 0 \\), but for each neural network \\( N \\), \\( N(f_{sep} (G_1)) = N(f_{sep}(G_1)) \\), and we can't approximate our desired function f with \\( \\epsilon = \\frac{f(G_2)}{2} \\) accuracy, yielding a contradiction."}, {"title": "G Proofs", "content": "In this section, we restate and prove theorems that have not been fully proved in the main text.\nTheorem 3.1. [expressive power of I-GGNN"}]}