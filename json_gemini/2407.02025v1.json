{"title": "On the Expressive Power of Sparse Geometric MPNNS", "authors": ["Yonatan Sverdlov", "Nadav Dym"], "abstract": "Motivated by applications in chemistry and other sciences, we study the expressive power of message-passing neural networks for geometric graphs, whose node features correspond to 3-dimensional positions. Recent work has shown that such models can separate generic pairs of non-equivalent geometric graphs, though they may fail to separate some rare and complicated instances. However, these results assume a fully connected graph, where each node possesses complete knowledge of all other nodes. In contrast, often, in application, every node only possesses knowledge of a small number of nearest neighbors.\nThis paper shows that generic pairs of non-equivalent geometric graphs can be separated by message-passing networks with rotation equivariant features as long as the underlying graph is connected. When only invariant intermediate features are allowed, generic separation is guaranteed for generically globally rigid graphs. We introduce a simple architecture, EGENNET, which achieves our theoretical guarantees and compares favorably with alternative architecture on synthetic and chemical benchmarks.", "sections": [{"title": "Introduction", "content": "Geometric graphs are graphs whose nodes are a 'position' vector in $R^d$ (typically d = 3) and whose symmetries include the permutation symmetries of combinatorial graphs and translation and rotation of the node positions. Geometric graphs arise naturally in learning applications for chemistry, physical dynamics, and computer vision as natural models for molecules, particle systems, and 3D point clouds. These applications motivated many learning models for geometric graphs, which were often inspired by 'standard' graph neural networks (GNNs) for combinatorial graphs [30, 56, 27]. Sub-sequentially, several theoretical works aimed primarily at understanding the expressive power and limitations of GNNs for geometric graphs.\nGNNs for geometric graphs produce global graph features invariant to geometric graph symmetries. The expressive power of GNNs is typically assessed primarily by their ability to assign different global features to pairs of geometric graphs that are not geometrically isomorphic.\nRecent research on this problem has uncovered several interesting results, mainly assuming that the graphs are full (all distances between node features are considered). Under the full graph assumptions, several models for geometric GNNs are complete, that is, capable of separating all pairs of non-isomorphic graphs. However, this typically comes at a relatively high computational price. Examples of complete models include (d \u2013 1)-WL-based GNNs [47, 49, 58], GNNs based on sub-graph aggregation [59], and message passing models based on arbitrarily high-dimensional irreducible representations [13, 27, 17, 38]. This paper focuses on more efficient geometric GNNs: message-passing neural networks (MPNN) based on simple invariant and equivariant features. In [42, 58], it was shown that, even under the full graphs assumption, invariant MPNN-based networks are not complete. In contrast, [59, 49] proved that these models are 'generically complete,' which means"}, {"title": "Setup: Geometric Graph Neural Networks and Rigidity Theory", "content": "Geometric Graphs. In our discussion of geometric graphs and GNNs, we loosely follow the definitions of [50]. We define a geometric graph as a pair $G = (A, X)$. The matrix $A \u2208 {0,1}^{n\u00d7n}$ is an adjacency matrix, which fully defines a combinatorial graph. Accordingly, we sometimes use statements like 'the graph A.' We assume for simplicity that the graph nodes are the integers 1, ..., n. The matrix $X = (x_1,...,x_n) \u2208 R^{n\u00d7d}$ denotes node positions. We denote the set of all geometric graphs with coordinates in $R^d$ by $G(d)$, and the subset of graphs with \u2264 N nodes by $G_N(d)$. In Appendix H, we discuss how to incorporate additional node and edge features into our model and theoretical results.\nThe symmetry group of geometric graphs is the product of the permutation group $S_n$ and the group of rigid motions $&(d)$. The action of a permutation matrix $P$ on a geometric graph is given by $PG := (PAPT, PX)$. A rigid motion in $E(d)$ is a rotation $Q \u2208 O(d)$ and a translation vector $t\u2208 R^d$. They act on $X$ by applying the rotation and translation to all its coordinates, $x_i \u2192 Qx_i + t$.\nTwo geometric graphs $G$ and $H$ are geometrically isomorphic if there exists a transformation in $S_n \u00d7 E(d)$ which can be applied to $G$ to obtain $H$. We will say that a function $F$ defined on $G(d)$ is invariant if $F(G) = F(H)$ for all $G$ and $H$ which are geometrically isomorphic.\nThe expressive power of an invariant model $F$ is closely related to its ability to separate pairs $G, H$ that are not geometrically isomorphic: clearly, if $F$ cannot separate $G$ from $H$, then $F$ will not be a good approximation for functions $f$ for which $f(G) \u2260 f(H)$. Conversely, if $F$ can separate all pairs of geometric graphs, then it can be used to approximate all continuous invariant [55, 49] and equivariant [32, 57] functions. In Appendix F, we discuss the connection between approximation and separation in the context of our results.\nGeneric separation and Rigidity Theory As discussed in the introduction, our paper focuses on generic separation. The results we present in this section are primarily influenced by rigidity theory, so we also use this field's common notion of 'generic'. We will say that $X \u2208 R^{d\u00d7n}$ is generic if it is not the root of any multivariate polynomial $p : R^{d\u00d7n} \u2192 R$ with rational coefficients. The genericity assumption guarantees that $X$ avoids many degeneracies, making separation easier. For example, for generic X all pairs of points have different norms, and every d tuple of points $x_{i_1},...,x_{i_d}$ is full rank because $||x_i|| - ||x_j||^2$ and $det[X_{i_1},...,X_{i_d}]$ are polynomials with natural coefficients. We note that Lebesgue almost all point clouds in $R^{d\u00d7n}$ are generic [23].\nA globally rigid graph is a central topic in rigidity theory:\nDefinition 2.1 (Globally rigid). ([39]) A geometric graph $G = (A, X)$ is globally rigid in $R^d$, if the only $X' \u2208 R^{n\u00d7d}$ which satisfy that $||x_i - x_j || = ||x'_i - x'_j||$ for all edges $(i, j)$ in $A$, are those that are related to $X$ by a rigid motion.\nA combinatorial graph $A$ is generically globally rigid in $R^d$, if the geometric graph $G = (A, X)$ is globally rigid in $R^d$ for every generic $X \u2208 R^{n\u00d7d}$."}, {"title": "E-GGNN", "content": "In the context of combinatorial graphs (with no geometric information), [8] showed that many famous graph neural networks are instantiations of MPNNs. The goal of [50] was to show, analogously, that many neural networks for geometric graphs follow a generalized MPNN framework. Loosely following [50], we define Equivariant Geometric Graph Neural Networks (E-GGNN) to be functions defined by a sequence of layers that propagates vector features from iteration t to t + 1 via a learnable function $f^{(t)}$ (at initialization we set $v_i^{(0)} = 0$).\n$v_i^{(t+1)} = f^{(t)} ({{v_j^{(t)}, x_i - xj} | j \u2208 Ni})$\nTo ensure the equivariance of the construction, the function $f^{(t)}$ is required to be rotation equivariant. Translation invariance is implicitly guaranteed by the translation invariance of $x_i - x_j$, and the requirement that $f^{(t)}$ is defined on multi-sets implicitly enforces permutation equivariance.\nWe note that for the sake of simplicity, we don't require a combine step that incorporates $v_i^{(t)}$ in the process of computing $v_i^{(t+1)}$ since we find that this step has little influence on our theoretical and empirical results. We also note that the vector $v_i^{(t)}$ is not necessarily d dimensional, and the action of $O(d)$ on $v_i^{(t)}$ is allowed to change from layer to layer. For example, in the EGENNETarchitecture we will introduce later on, each $v_i^{(t)}$ will be a d \u00d7 m matrix, and the action of O(d) will be multiplication on each of the m coordinates.\nAfter T iterations of E-GGNNs, a permutation, rotation, and translation invariant feature vector can be obtained via a two-step process involving a rotation-invariant function $f_{inv}$ and a multiset READOUT function:\n$s_i = f_{inv}((v_i^{(T)})), s_{global} = READOUT{s_1, .., s_n}}$\nAs shown in [50], many popular geometric graph neural networks can be seen as instances of E-GGNNs with a specific choice of functions $f^{(t)}$ and readout functions. Examples include EGNN [30], TFN [13], GVP [22], and MACE [35]. In Section 5.1, we present EGENNET, our instantiation of this framework."}, {"title": "I-GGNNS", "content": "An important subset of E-GGNNs are invariant GGNNs (I-GGNNs). These networks only maintain scalar features $s_i^{(t)}$ and replace $x_i - x_j$ with its norm to obtain\n$s_i^{(t+1)} = f^{(t)} ({{s_j^{(t)}, ||x_i - x_j||} | j \u2208 Ni})$\nA global invariant feature is obtained via a permutation invariant readout function as in equation 2. An advantage of I-GGNNs is that there are no rotation equivariance constraints on $f^{(t)}$. Therefore, any"}, {"title": "Expressive power of I-GGNN", "content": "Our next goal is to analyze the separation abilities of I-GGNN.\nTheorem 3.1. [expressive power of I-GGNN] Let d be a natural number. Let F be an I-GGNN. Let A be a graph not generically globally rigid on $R^d$. Then, F generically fails to identify A.\nConversely, if A is generically globally rigid on $R^d$ and F is a maximally expressive I-GGNN with depth T = 1, then F generically identifies A.\nProof idea. if A is not globally rigid, then according to [6], for every generic X there exists X' for whom distances along edges are preserved, but X \u2260 X'. It's easy to see any I-GGNN, which is based only on distances along edges, can't separate such graphs.\nOn the other hand, generic geometric graphs have distinct pairwise distances. The full proof (see Appendix G) shows that this can be used to reconstruct the combinatorial graph A. We can then reconstruct X, up to rigid motion, by global rigidity.\nNote that this result is a generalization of [49] that showed maximally expressive I-GGNN generically identify full graphs. We now showed this is true for all generically globally rigid graphs.\nI-GGNN generic separation by power graph preprocessing A natural strategy to overcome the lack of generic separation for graphs that are not generically globally rigid, as proven in Theorem 3.1, is to replace these graphs with graphs that are generically globally rigid. This replacement can be done using power graphs: For a given graph G and natural number k, the power graph $G^k$ is defined to have the same nodes as the original graph G. A pair of nodes are connected by an edge in the power graph if and only if there is a path between them in the original graph of length < k.\nIn [41, 4], it was shown that if the original graph G is connected, then the power graph $G^{d+1}$ is generically globally rigid in Rd. Accordingly, when the input graph G is connected but not generically globally rigid, our theory suggests that applying I-GGNN not to the original graph but to its d + 1 power may benefit separation. An experiment illustrating this idea is presented in Table 1."}, {"title": "Expressive power of E-GGNN", "content": "When using E-GGNN rather than I-GGNN, we will have generic separation if and only if the graph is connected.\nTheorem 4.1. [expressive power of E-GGNN] Let d be a natural number. Let F be an E-GGNN. Let A be a disconnected graph. Then, F generically fails to identify A."}, {"title": "Building a generically maximally expressive E-GGNN", "content": "To obtain generic separation guarantees as in Theorems 3.1, 4.1 for a practical instantiation of E-GGNN (or I-GGNN), we will need to show that the instantiation is fully expressive. This type of question was studied rather extensively for combinatorial graphs [46, 33], and these results can be used to show that some simple realizations of I-GGNN are maximally expressive. We discuss this in more detail in Appendix C.\nFor E-GGNN, the requirement that $f^{(t)}$ is simultaneously injective (up to permutation) and equivariant to rotations seems to be challenging to attain with a simple E-GGNN construction, and indeed, [50] who suggested the notions of maximally expressive E-GGNN, did not give any indication of how they could be constructed in practice. Moreover, in Appendix D, we show that a maximally expressive E-GGNN can separate all pairs of full graphs and separate geometrically all pairs of connected graphs, indicating constructing such a network may be computationally expensive, as currently, such guarantees are only achieved by networks of relatively high complexity based on 2-WL subgraph aggregation techniques.\nIn our setting, however, the problem is less severe since, from the outset, we are only interested in analyzing network behavior on generic graphs. Accordingly, we would like to require our E-GGNN to be maximally expressive only for generic graphs. It turns out that this goal can be achieved with a straightforward E-GGNN architecture, which we name EGENNET. Our next step will be to present this architecture."}, {"title": "The EGENNET architecture", "content": "The EGENNET architecture we suggest resembles an EGNN architecture [30] with multiple equivariant channels as in [52]. The EGENNET architecture depends on two hyper-parameters: depth T and channel number C. The input to the architecture is a geometric graph (A, X). The architectures then maintains, at each iteration t and node i, an 'equivariant' node feature with C channels\n$v_i^{(t)} = (v_{i,1}^{(t)},..., v_{i,C}^{(t)}), v_{i,c}^{(t)} \u2208 R^d, c = 1,..., C.$\nThese feature values are initialized to zero and are updated via the rotation and permutation equivariant aggregation formula\n$v_{i,q}^{(t+1)} = \\sum_{j \u2208 N_i} \\phi^{(t,q,0)}(\\|x_i - x_j\\|, \\|v_j^{(t)}\\|) (x_i - x_j) +  \\sum_{c=1}^{C} \\phi^{(t,q,c)}(\\|x_i - x_j\\|, \\|v_j^{(t)}\\|)v_j^{(t)}.$\nwhere $\\|. \\|$ is the C dimensional vector containing the norms of the vectors $v_{j,c}, c = 1,..., C$. After T iterations, we obtain rotation and permutation invariant node features $s_i \u2208 R^C$ and global feature $s_{global} \u2208 R^C$ via\n$s_{i,q}^{(T)} = \\sum_{c=1}^{C} \\theta_{c,q}^{(1)} v_{i,c}^{(T)}, s_{global} =  \\sum_{i=1}^{n} \\theta_{q}^{(2)} s_i, q = 1,..., C$\nThe functions $\\phi^{(t,q,c)}, \\theta^{(g)lobal}$ are all fully connected neural networks with a single layer, an output dimension of 1, and an analytic non-polynomial activation (in our implementation, TanH activation). The complexity of EGENNET is linear in N, as we discuss in Appendix G.2.\nThe next theorem shows that EGENNET is generically maximally expressive:\nTheorem 5.1. Let d, N, T be natural numbers. Let F be a maximally expressive E-GGNN of depth T, and let F\u00f8 denote the EGENNET architecture with depth T and C = 2Nd + 1 channels. Then, for Lebesgue almost every choice of network parameters \u03b8, we have that for all generic $G, \\hat{G} \u2208 G_N(d)$,\n$F(G) = F(\\hat{G}) \u21d4 F_\u03b8(G) = F_\u03b8(\\hat{G})$\nProof. In the proof, we consider the set of all generic geometric graphs in $G_N(d)$. We will prove the theorem for an even larger set: the set $G_{distinct} (d)$ of all geometric graphs in $G_N(d)$, which additionally satisfies that for any distinct edges (i, j) and (s,t) in the graph, 0 < $\\|x_i - x_j\\| \u2260 \\|x_s - x_t\\|$. Since polynomial inequalities with integer coefficients define it, we note that $G_{distinct} (d)$ contains all generic graphs with \u2264 N nodes. The advantage of considering $G_{distinct} (d)$ is that the set of all geometric graphs in $G_{distinct} (d)$ of the same cardinality n \u2264 N can be thought of as a subset of $R^{n\u00d7n}\u2295 R^{n\u00d7d}$. Since a constant number of polynomial inequalities defines it, it is a semi-algebraic set, and hence, in particular, \u03c3-subanaltyic, so the finite witness theorem can be applied. In contrast, it is unclear that the set of all generic points of fixed dimension is a \u03c3 subanaltyic set.\nOur goal is to show that for the networks described in the theorem, for almost every choice of parameter values, all functions described in the procedure are injective. To be more accurate, let us introduce some notation given an arbitrary pair of geometric graphs\n$G = (A, X), \\hat{G} = (\\hat{A}, \\hat{X})$\nin our domain $G_{distinct} (d)$, we denote the features corresponding to the first graph by $v_i^{(t)}, s_i$ and $s_{global}$ as in the main text, and the features corresponding to the second graph by $\\hat{v}_i^{(t)}, \\hat{s}_i$ and $\\hat{s}_{global}$. The neighborhood of a node k in $\\hat{G}$ is denoted by $N_k$. The number of nodes in the two graphs is denoted by n and m, respectively.\nWe need to show that for all t = 0, ..., T \u2013 1 and all nodes i of G and k of $\\hat{G}$,\n$v_i^{(t+1)} = \\hat{v}_k^{(t+1)}$ if and only if {{(xi - xj, $v_j^{(t)}$) | j \u2208 Ni} = {{(xk - xj, $\\hat{v}_j^{(t)}$) | j\u2208 $\\hat{N}_k$}} (6)\n$s_i = \\hat{s}_k$ if and only if $\u2203Q \u2208 O(d), Qv_i^{(T)} = \\hat{v}_k^{(T)}$ (7)\n$s_{global} = \\hat{s}_{global}$ if and only if {{$s_i$ |i = 1, ..., n} = {{$ \\hat{s}_i$|i = 1, ..., m}} (8)\nOur first step for reaching this result is using the finite witness theorem, similar to what was done in [49]. We note that the features we are interested in are composed of $C = 2N^{d + 1}$ coordinates, denoted by q = 1, . . ., C, which are of the general form\n$v_{i,q}^{(t+1)} = F_t(X, v^{(t)}, \\hat{X}, \\hat{v}^{(t)}; a_{q,t}), t = 0, . . ., T \u2013 1$ (9)\n$s_{i,q}^{(T+1)} = F_T(v^{(T)}, \\hat{v}^{(t)}; a_{q,T})$ (10)\n$s_{global q}^{(g+lobal)} = F_{T+1}(s_1,..., s_n, \\hat{s}_1,..., \\hat{s}_m; a_{q,T+1})$ (11)\nwhere $a_{q,t}$ denote the parameters of the appropriate function, $v^{(t)}$ is the concatenation of all n features $v_i^{(t)}$, and $\\hat{v}^{(t)}$ is the concatenation of all m features $\\hat{v}_i^{(t)}$. In essence, we claim that by the finite witness theorem, for almost every choice of network parameters, we will have the following equalities for all geometric graphs G, $\\hat{G}$ satisfying the theorem's assumptions:\n$v_{i,q}^{(t+1)} - \\hat{v}_{k,q}^{(t+1)} = 0$ if and only if $F_t(X, v^{(t)}, \\hat{X}, \\hat{v}^{(t)}; a_t) = 0, \u2200\u03b1$ (12)\n$s_{i,q} - \\hat{s}_{k,q} = 0$ if and only if $F_T(v^{(T)}, \\hat{v}^{(t)}; \u03b1), \u03bd\u03b1$ (13)\n$s_{global} -  \\hat{s}_{global} = 0$ if and only if $F_{T+1}(s_1,..., s_n, \\hat{s}_1, . . ., \\hat{s}_m; a) = 0, \u03bd\u03b1$ (14)\nIn other words, the finite witness theorem enables us to reduce the problem to the problem of showing that some choice of network parameter exists, which enables separation."}, {"title": "Experiments", "content": "We begin with experiments to corroborate our theoretical findings. We consider examples where the graph A is a line graph, which is not generically globally rigid. We choose two pairs of geometric line graphs, Pair a and Pair b, depicted in Figure 2. These pairs have identical distances along graph edges but are not isomorphic. Therefore, they cannot be distinguished by I-GGNN but (at least when generic) can be distinguished by E-GGNN and our EGENNETarchitectures. Following the protocol from [50], we construct binary classification problems from pair a and pair b and apply our E-GGNN architecture EGENNET, as well as three popular I-GGNN architectures: SchNet [12], DimeNet [19], and SphereNet [28].\nThe results are shown in the first column in Table 1.\nAs expected, EGENNETperfectly separates pairs a and b, while SchNet fails to separate both. Note that DimeNet and SphereNet separate pair a but not pair b. This is because DimeNet and SphereNet compute angles at nodes, which utilizes information not contained in the graph G but only in the power graph G2. Indeed, distances between two-hop neighbors are sufficient for discriminating between the two geometric graphs in pair a but not in pair b, where the distance between the three-hop red and orange nodes is needed for separation. Next, as we suggested in Section 3, we apply the same algorithms to the powers of the original graph. As shown in Table 1, this improves the separation capabilities of the I-GGNN models. All invariant models succeed when the third power is taken, as expected since a connected graph's d + 1 power (here d = 2) is generically globally rigid."}, {"title": "Limitation and Future Work", "content": "In this work, we characterized the generic expressive power of I-GGNN and E-GGNN, showed that EGENNETis a generically maximally expressive E-GGNN, and showed the effectiveness of this architecture on chemical regression tasks. A limitation of our work is that it may struggle with symmetric and non-generic input. Future work could consider assessing the expressive power of non-NPNN models like geometric 2-WL on sparse graphs to address such input efficiently."}, {"title": "Acknowledgments", "content": "We thank Snir Hordan for going over our manuscript and for helpful discussions and suggestions. We would like to thank Idan Tankal for his technical help and support. N.D. and Y.S. are funded by Israeli Science Foundation grant no. 272/23."}, {"title": "Related Work", "content": "This section discusses related work that is not discussed in the main text.\nEquivariant models In the context of learning on geometric graphs, models should be equivariant to the joint action of permutations, rotations and translations. Besides the models discussed in the paper, some prominent models with this equivariant structure include Cormorant [15], Vector Neurons [26], and SE(3) transformers [18].\nCompleteness, universality and generic completeness The main text discusses related works on completeness focused on methods coming from the geometric learning community. Other complete methods, generally using similar ideas, appeared independently in the more chemistry-oriented community [51, 53, 7, 24]. Completeness in 2D was achieved in [36], and completeness with respect to rigid motions (without permutations) was achieved by Comenet [44]. Comenet also uses some genericity assumptions (e.g., that there is a well-defined notion of nearest neighbors).\nThe main text discussed how generic completeness can be achieved using I-GGNN when the graph is globally rigid. A similar result can be obtained when considering the list of all distances [39]. Generic completeness for full graphs can also be obtained using more complex distance-based features [45] or shape principal axes [29] (under the generic assumption that the principal eigenvalues are distinct)."}, {"title": "Implementation details", "content": "Here, we detail the hyper-parameter choice. For all experiments, we use AdamW optimizer [9]. We use a varying weight decay of 1e-4, 1e-5 and learning rate of 1e-4. Reduce On Platue scheduler is used with a rate decrease of 0.75 with the patience of 15 epochs. We use two blocks with 40 channels in the challenging point cloud separation experiments. For k-chain experiments, we repeat each experiment 10 times, with 10 different seeds, and set the number of channels to be 60, and a varying number of blocks, as detailed in the table. For power graph experiments, we repeat each experiment 10 times, with 10 different seeds, and set the number of channels to be 60 and 3 blocks. For chemical property experiments, all the procedure is taken from [61]: each dataset is partitioned randomly into three subsets: 70% for training, 10% for validation, and 20% for test. We set the number of channels to be 256 and used 6 blocks. Each model is trained over 1,500 epochs. Experiments are repeated three times for all eight regression targets, and the results reported correspond to the model that performs best on the validation set in terms of Mean Absolute Error (MAE). In contrast to the experiments in [61], we didn't consider the EE dataset as it's currently not public on GitHub. We utilize PyTorch and PyTorch-Geometric to implement all deep learning models. All experiments are conducted on servers with NVIDIA A40 GPUs, each with 48GB of memory. All dataset details, including statistics, can be found in [61]."}, {"title": "Building perfect I-GGNN", "content": "Here, we describe in detail how to construct a perfect I-GGNN. We will assume that we consider geometric graphs with up to N nodes. The main ingredients needed are aggregation functions and READOUT function that are injective on the space of all multisets of size < N. [46] proved that the sum of projections and analytic activation can be used to construct such layers. Formally, be M semi-analytic set of dimension D, and N vectors in Rd denoted by X. Be $A \u2208 R^{(2D+1)\u00d7d}, b \u2208 R^{2D+1}, \u03c3: R \u2192 R$ analytic non-polynomial activation, then for Lebesgue almost every A, b\n$f_{a,b}(X) = \\sum_{i=1}^{N} \u03c3(A\u00b7 Xi + b)$\nis injective up to permutation. Thus, the composition of such blocks and such READOUT function with dimension D = d \u00b7 N yields a maximally expressive model on all input domain. Here, D is the intrinsic dimension of all features, ultimately produced by the D dimensional input space."}, {"title": "Separation Power of Fully Maximally expressive E-GGNN", "content": "In the main text, we proposed EGENNET a simple generically maximal expressive E-GGNN. In this section, we show that if we have a maximally expressive E-GGNN on all input domain, we can reconstruct a full graph up to group action, and if the graph is connected, we can reconstruct its point cloud. This indicates that fully maximally expressive E-GGNN is as challenging as achieving completeness, which generally requires complex methods like 2-WL-based GNNs.\nTheorem D.1. Let G = (A, X) be a geometric graph, where A is the full graph on n nodes, and X is an arbitrary d \u00d7 n matrix, then one iteration of maximally expressive E-GGNN can reconstruct X.\nProof. Assume we run one iteration of E-GGNN on our geometric graph, then for each node i, we know up to some rotation $Q_i$\n${Q_i \u2022 (x_i - x_j)|j \u2208 [n]}$\nChoosing i = 1, and $x_i$ = 0, we have all other positions $Q_i x_j, j \u2260 i$, thus up to translation (as we set $x_i$), rotation $Q_i$ and node permutation, we can reconstruct our point cloud.\nWe now show that using n iterations of E-GGNN can reconstruct the point cloud of each connected graph.\nTheorem D.2. Assuming a connected geometric graph G = (A, X) with n nodes, then n iteration of maximally expressive E-GGNN can reconstruct the point cloud.\nProof. Assume we run n iterations of a maximally expressive E-GGNN on our connected graph. Then, by Theorem 4.1, we can reconstruct for each node i its n-hop up to rotation $Q_i$. As the n-hop of a node is all other nodes (the graph's diameter is at most n), we reconstruct the full geometric graph up to rotation, and by theorem D.1, we can reconstruct the point cloud."}, {"title": "I-GGNN+", "content": "Here, we present the definition of I-GGNN presented by [50], denoted by I-GGNN+, and show that it can separate all geometric full graphs. After, we explain why this model is strictly stronger than our I-GGNN.\n[50] defines (neglecting the update step also used there)\n$v_i^{(t+1)} = f^t ({{v_j^{(t)}, x_i - x_j} | j \u2208 Ni}})$\nSuch that ft holds that for all $Q \u2208 O(d)$\n$f^t ({{v_j^{(t)}, x_i - x_j} | j \u2208 Ni}}) = f^t ({{v_j^{(t)}, Q \u2022 (x_i - x_j)} | j \u2208 Ni}})$\nI-GGNN+ is maximally expressive if ft holds:\n$f^t ({{v_j^{(t)}, x_i - x_j} | j \u2208 Ni}}) = f^t ({{v_j^{(t)}, \\hat{x}_i - \\hat{x}_j} | j \u2208 N_i}})\n{\\exists Q\u2208 O(3) : {{$v_j^{(t)}, x_i - x_j} | j \u2208 N_i}} = {{$v_j^{(t)}, Q \u2022 (\\hat{x}_i - \\hat{x}_j)} | j \u2208 N_i}}\nWe now prove that if we run one iteration of maximally expressive I-GGNN+ on a full graph, we can reconstruct it.\nTheorem E.1. Let G = (A, X) be a geometric graph, where A is the full graph on n nodes, and X is an arbitrary d \u00d7 n matrix, then one iteration of maximally expressive I-GGNN+ can reconstruct X.\nProof. Assume we run one iteration of I-GGNN+. Then, for each node i, we have\n$f_t (v_j^{(t)}, x_i - x_j | j\u2208 [n]})$\nthen up to some rotation $Q_i \u2208 O(3)$, we know\n{${Q_i (x_i - x_j), | j \u2208 [n]}}$\nThen, we can reconstruct the point cloud by Theorem D.1."}, {"title": "Approximation and separation", "content": "It is known that a tight connection exists between the approximation of invariant functions and complete invariant models [49].\nIn the context of our paper", "theorem": "nTheorem F.1. Let $K \u2286 G_{generic"}, "d, N)$ be a compact set, $f_{sep} : K \u2192 R^m$ a continuous function invariant to rotations, translations, and permutations. Then $f_{sep}$ is injective on K (up to symmetries) if and only if every continuous invariant $f : K \u2192 R^M$ can be approximated uniformly on K using functions of the form N(fsep), where N is a fully connected neural network.\nProof. Assume, on the one hand, fsep is injective on K (up to symmetries). Given a continuous $f: K \u2192 R^M$ we want to approximate, then, by [49], $\u2200e > 0\u2203$ neural network N:\n$sup_{g\u2208k}|f(G) - N(f_{sep}(G))| < e$\nOn the other hand, assume by contradiction there exists $G_1 = (A_1, X_1)$ and $G_2 = (A_2, X_2)$ which are not related by translation, rotation or permutation, such that $f_{sep}(G_1) = f_{sep}(G_2)$.\nDefine $f(G) = Inf_{g\u2208G}\\|(A, X) - g \u00b7 (A_1, X_1)\\|$, where $G = (A, X)$, and G is the group of permutations, rotations, and translation. Note that this minimum is obtained as the product group of rotation and permutation is compact, and the translation can be bounded by the sum of the norms of the two graphs. Then, according to the Tikhonov theorem, our group is compact, and we take a minimum continuous function over a compact set. Note that f is an invariant, continuous function such that f(G1) = 0, and f(G2) > 0, but for each neural network N, N(fsep (G1)) = N(fsep(G1)), and we can't approximate our desired function```json\n{\n  \"title\": \"Approximation and separation\",\n  \"sections\": [\n    {\n      \"title\": \"Proofs\",\n      \"content\":", "In this section, we restate and prove theorems that have not been fully proved in the main text.\nTheorem 3.1. [expressive power of I-GGNN] Let d be a natural number. Let F be an I-GGNN. Let A be a graph not generically globally rigid on Rd. Then, F generically fails to identify A.\nConversely, if A is generically globally rigid on Rd and F is a maximally expressive I-GGNN with depth T = 1, then F generically identifies A.\nProof. Let G = (A, X) be a geometric graph, and assume that X is generic. First, we assume that A is generically globally rigid. Assume that there is some geometric graph G' which is assigned the same global feature as G after a single iteration. We need to show the two graphs are geometrically isomorphic. The multisets of si and s'i features in equation 2 are equal, and in particular, both graphs have the same number of nodes n, and by relabeling if necessary, we can assume that s_i = s'_i for all i = 1,...,n. This equation, in turn, implies that for every node i,\n{{||x_ij ||, j\u2208 Ni} = {{||x'ij||, j\u2208Ni}\nIn particular, the i'th node in G and G' have the same degree, so both graphs have the same number of edges. For any fixed (i, j) which is an edge of G, the distance ||x_ij|| will appear exactly once in the multiset corresponding to s_i and s'i; and will not appear in the other multisets. This observation implies that (i, j) is also an edge in the graph G'. Since the number of edges in both graphs is the same, we deduce that A = A'. By global rigidity, we deduce that all pairwise distances are the same, and by [30], the two graphs are geometrically isomorphic.", "n    },\n    {\n      \"title\": \"Complexity\",\n      \"content\":", "Running EGENNET for T iterations with C channels requires O(C. d. degi) elementary arithmetic operation for every node i, at every iteration t. Overall, this requires O(C\u00b7d\u00b7N\u00b7davg \u00b7T) to run EGENNET, where daug denotes the average degree in the graph. This is linear in N, which dominates the other terms in the complexity bound. If we set C = 2Nd + 1 as required for maximal expressivity in Theorem 5.1, then the complexity will be quadratic in N, which is less by a factor of n then [47, 58], and by a factor of n\u00b2 than [49]. We also note that the cardinality in the finite witness theorem, the main tool for our proof, depends on the intrinsic dimension of the input and not the ambient dimension. Thus, under the standard 'manifold assumption' [5], which asserts that data in learning tasks typically resides in a manifold of low dimension r, embedded in a high dimensional Euclidean space, the proof can be adapted to show that the number of channels necessary for maximal expressivity is only 2r + 1. Thus, under the manifold assumption, the complexity of EGENNET is linear in the number of nodes N.", "n    }"]}