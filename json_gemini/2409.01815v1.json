{"title": "LEARNING STATE-DEPENDENT POLICY PARAMETRIZATIONS\nFOR DYNAMIC TECHNICIAN ROUTING WITH REWORK", "authors": ["Jonas Stein", "Florentin D Hildebrandt", "Marlin W Ulmer", "Barrett W Thomas"], "abstract": "Home repair and installation services require technicians to visit customers and resolve tasks of\ndifferent complexity. Technicians often have heterogeneous skills and working experiences. The\ngeographical spread of customers makes achieving only perfect matches between technician skills\nand task requirements impractical. Additionally, technicians are regularly absent due to sickness.\nWith non-perfect assignments regarding task requirement and technician skill, some tasks may remain\nunresolved and require a revisit and rework. Companies seek to minimize customer inconvenience due\nto delay. We model the problem as a sequential decision process where, over a number of service days,\ncustomers request service while heterogeneously skilled technicians are routed to serve customers\nin the system. Each day, our policy iteratively builds tours by adding \"important\" customers. The\nimportance bases on analytical considerations and is measured by respecting routing efficiency,\nurgency of service, and risk of rework in an integrated fashion. We propose a state-dependent\nbalance of these factors via reinforcement learning. A comprehensive study shows that taking a few\nnon-perfect assignments can be quite beneficial for the overall service quality. We further demonstrate\nthe value provided by a state-dependent parametrization.", "sections": [{"title": "1 Introduction", "content": "One of the authors recently bought a new kitchen. After installation, the lighting did not work correctly and the\nafter-sales department of the kitchen company sent a technician to fix it. It did not work, and a few days later, another\ntechnician tried their luck, also unsuccessfully. Finally, the company sent one of their experts who resolved the issue\nwithin minutes. All this led to significant labor cost for the company and frustration for the author. The issue of\nunresolved repair or installation services and repeated technician visits for rework is not unique to kitchen installation.\nIt is also common in other services with complex technician tasks such as home appliance repair, the repair of home\nelectrics, or the installation of cable (van Moeseke et al. 2022). One reason can be a missing spare part (Pham and\nKiesm\u00fcller 2024). This issue can be resolved with remote diagnostic tools that can communicate the reason for failure\nquite accurately and the respective spare parts can be loaded to the technician's truck (Rippe and Kiesm\u00fcller 2023).\nHowever, even if the theoretical issue is known, there are a lot of practical circumstances involved. Here, another\nreason for services remaining unresolved is the individual skill level of the sent technicians (Han 2023). Some expert"}, {"title": "2 Related Literature", "content": "In our literature review, we present papers that closely align with our work. In Appendix A, we summarize the large\nbody of work on deterministic and multi-period service routing with heterogeneous workforce, and broader routing\nproblems that incorporate repeated customer visits.\nAs our work, Chen et al. (2016) consider the routing of technicians in a stochastic and dynamic context. On a daily\nbasis, a set of technician routes is scheduled to provide on-site services to newly requesting customers with the goal of\nminimizing completion time. Over time, technicians are able to improve their qualifications through learning, resulting\nin decreased service and completion times. By tuning a cost function approximation to handle anticipation more\ndirectly, the authors demonstrate that effective scheduling decisions should consider the individual learning rates of\nthe technicians. As in our work, the authors assign heterogeneous technicians to tasks of varying difficulty. However,\nsimilar to the existing body of literature, it is assumed that any technician can resolve any task with certainty, though\nwith varying service times and no rework.\nPham and Kiesm\u00fcller (2024) introduce an optimization problem involving spare parts planning and technician scheduling\nfor companies providing repair services for large household appliances. Over a sequence of days, a single technician\nvisits dynamically requesting customers whose equipment has failed and needs repair. The spare parts required are\nstochastic since they are only revealed when a technician first arrives at the customer. Technicians load a variety of\nspare parts into their vans before departing from the depot. If a technician does not have all the required spare parts\navailable, the repair remains unresolved and the customer needs to be revisited on a future day. The goal is to minimize\nthe average daily costs which involve travel, holding, delay and repair costs. The authors develop anticipatory solution\nmethods based on a value function approximation technique. The work shows similarities to ours since customer\nservices can fail. However, the key differences are that we model both a fleet and heterogeneous technicians, and our\nmethod is designed to tackle the resulting assignment decisions.\nKhorasanian et al. (2024) introduce a dynamic home care routing and scheduling problem. Over a rolling planning\nhorizon, a single nurse is assigned to referrals with each requiring multiple visits for treatment. On a daily basis, the\nnurse performs subsequent visits to patients located within a defined service area. The daily number of new referrals as\nwell as their required number of visits are unknown. New referrals can be rejected which comes with rejection costs.\nThe goal is to minimize the overall cost of service. The authors propose a reinforcement learning policy to decide about\nrejection of customers. They show that an effective policy rejects (all) customers who are too far away from the depot."}, {"title": "3 Problem Definition", "content": "In this section, we present the problem description. We give a formal definition of the decision problem followed by an\nillustrative example. Afterwards, we model our problem as a sequential decision process following the framework by\nPowell (2021)."}, {"title": "3.1 Formal Problem Description", "content": "Over multiple periods (e.g., days in one month), technicians serve customers within a defined service area. The set\nof technicians is fixed and their working hours per period are limited. Each technician has the same probability to\nbe unavailable on a given day due to sickness. Technicians also possess different qualification levels. Some regular\ntechnicians are less qualified due to limited practical experience or training. Other expert technicians are highly qualified\nwith extensive work experience.\nAt the beginning of each period, customers request on-site technician services. Each request contains three elements:\nthe location within the service area, the task difficulty, and the period the service is due (deadline). Related to the task\ndifficulty, we define two levels. The first level comprises manageable easy tasks that do not require advanced skills of\ntechnicians. Conversely, the second level comprises advanced tasks that require higher demands on technicians. In our\nwork, we assume that we know the task levels with certainty and that all tasks come with same service times. Besides\nthe tasks, customers have deadlines, e.g., two days after the request was made, representing a threshold for delayed\nservices. Customers are served on time if their requests are completed successfully before the deadline period, and vice\nversa, services are considered late when completed after the deadline. Starting from the deadline, customers experience\nincreasing inconvenience provoked by extended waiting times and unreliable service promises.\nEach period, the dispatcher determines which customers to serve, assigns customers to the available technicians, and\nplans their routes while considering the working time limitations. If assigned, customers with easy tasks are served\nsuccessfully regardless by the assigned technician. Customers with advanced tasks are served with certainty if assigned\nto an expert technician, but the tasks may remain unresolved and require rework with a known probability if assigned\nto a regular technician. As a result, we have two types of assignments. Safe assignments (advanced task to expert"}, {"title": "3.2 Example", "content": "In this section, we illustrate the decision process with a small example for a decision state with two potential decisions\nand their outcomes (see Figure 1). For the purpose of presentation, we assume a Manhattan-style grid with travel times\nof 10 minutes per segment. The depot is located in the center. Service times are set to 30 minutes and technicians have\na maximum working time of 210 minutes. We further assume, (all) two technicians are available in this period, one\nregular and one expert. The dotted and dashed lines show the tours for the regular and expert technician, respectively.\nThe customer inconvenience grows by 10% for each day a service is delayed. The decision state is set in period t = 4.\nAt that period, seven customers are in the system illustrated by the circles. The (green) scatter plots in the circles\nrepresent easy customers while the (red) dot matrix represents advanced customers. The individual deadline periods are\nrepresented by the numbers in the circles. In this example, one customer's deadline has already been exceeded by one\nperiod, and another customer has a deadline in the current period. Both represent urgent customers, as leaving their\nrequests incomplete would cause (additional) inconvenience. Two customers have their deadlines in the next period,\nand three customers have their deadlines in the period after next.\nOn the left side of Figure 1, we visualize two identical decision states with two different decisions. A decision involves\nassigning customers to technicians and determining their routing. In Decision I, top left of Figure 1, the regular\ntechnician visits two customers in the northeast of the service area. The working time is 160 minutes (10 \u00d7 10 minutes\n+ 2 \u00d7 30 minutes) and therefore feasible. The assigned advanced customer is a risky assignment which might result\nin rework. The expert technician visits one easy and one advanced customer in the south with a working time of 180\nminutes (12 \u00d7 10 + 2 \u00d7 30). Both assignments are safe. The remaining three customers in the west are not visited in\nthe period. The top right part of Figure 1 shows a potential realization of uncertainty and the resulting next state in\nperiod t = 5 following Decision I. A realization contains three parts: the outcomes of risky assignments, new requests\nand the technician availability in the next period. In the example, we see that the risky assignment remains unresolved\nand the corresponding customer located east to the depot requires rework. We further observe four new customers that\nrequest services, each with a deadline in period t = 7. The expert technician is absent in this state.\nThe alternative Decision II, at the bottom of Figure 1, shows only safe assignments. Three easy customers are served by\nthe regular technician with a working time of 210 minutes (12 \u00d7 10 + 3 \u00d7 30) and two advanced customers are served\nby the expert technician with a working time of 160 minutes (10 x 10 + 2 \u00d7 30). One easy and one advanced customer\nremain unassigned due to limited working hours and are postponed to the next period. Since this postponement exceeds\nthe deadline of the easy customer in the northeast, an increase in inconvenience of 1.1 is observed. However, compared\nto Decision I that causes no immediate increase in inconvenience, fewer (urgent) customers remain in the next state\nfollowing Decision II."}, {"title": "3.3 Sequential Decision Process", "content": "Our problem is both stochastic and dynamic. The availability of technicians, the rework probability of risky assignments,\nand the new customers in every period are stochastic. We decide every period about assignment and routing and\nsince decisions are interconnected, the problem is dynamic. Stochastic dynamic decision problems can be modeled as\nsequential decision processes, defining the problem as a sequence of states, decisions, rewards (costs), revelation of\ninformation, and transitions to next states (Powell 2021). As we are dealing with a minimization problem, we will use\nthe term \"costs\" instead of rewards throughout the paper. In the following, we define the components of the sequential\ndecision process. We start with preliminaries."}, {"title": "3.3.1 Preliminaries.", "content": "We assume access to a set of technicians W, defining their skills as $b_w \\in \\{0,1\\}$ with $b_w = 0$ ($b_w = 1$) representing\nw \u2208 W being an regular (expert) technician. We define the probability of risky assignments to remain unresolved as\n$p \\in (0, 1)$. Finally, we assume that customers can only call until period $T^c$ and a later leftover phase is exclusively used\nto serve remaining incomplete customers. Notably, in contrast to other problems, the leftover phase still follows the\nstructure of the sequential decision process."}, {"title": "3.3.2 Decision State.", "content": "A decision is made in every period ($t = 1, . . ., T^c, . . ., T$). Since the process only ends when all customer requests are\ncompleted, time T is a random variable. A decision state $S_t = (W_t, K_t, d_t, p_t, \\tau_t)$ in period t contains five types of\ninformation:\n\u2022 the set of available technicians $W_t \\subseteq W$ with $m_t = |W_t|$, respecting technician absences.\n\u2022 the set of customers $K_t$ with $n_t = |K_t|$, containing all customer requests."}, {"title": "3.3.3 Decisions.", "content": "In state $S_t$, a decision $x_t = (y_t, z_t) \\in X(S_t)$ comprises two parts. The first part, $y_t$, is the assignment of customers to\ntechnicians. The assignment variable $y_{wit} \\in \\{0, 1\\}$ is 1 if technician w visits customer i in period t (0 otherwise). The\nsecond part, $z_t$, is the routing of technicians to serve the customers. The routing variable $z_{wijt} \\in \\{0,1\\}$ is 1 if technician\nw traverses arc (i, j) to visit j coming from i (0 otherwise). Feasible decisions satisfy the routing constraints and ensure\nthat the working hour restrictions are respected. Consequently, our problem can be modeled as a heterogeneous team\norienteering problem with time limits. For a formal definition of the decision space, we refer to Appendix B.\nWe define $K_{x_t}^u = \\{i\\in K_t | \\Sigma_{w \\in W_t} y_{wit} = 0\\}$ as the set of unassigned customers following decision $x_t$. The set of risky\nassignments is defined as $K_t^r = \\{i\\in K_t | \\Sigma_{w \\in W_t} y_{wit} \\cdot p_{wit} > 0\\}$. The post-decision state $\\widetilde{S}_t$ when taking decision $x_t$\nin state $S_t$ is defined as $\\widetilde{S}_t = (\\widetilde{W}_t, \\widetilde{K}_t, \\widetilde{d}_t, \\widetilde{p}_t, \\widetilde{\\tau}_t)$ with the last three entries being the same as in the state $S_t$, but\nwithout the customers that were assigned safely by decision $x_t$.\nFollowing the framework by Powell (2021), each decision is associated with costs. For our problem, the immediate\ncost $C(S_t, x_t)$ of a decision $x_t$ in state $S_t$ is the increase in inconvenience for both unassigned customers and those\nassigned as risky. The value is therefore a random variable because the realized inconvenience depends on the successful\ncompletion of risky assignments. To reflect the exponentially growing customer inconvenience caused by late services,\nwe introduce a penalty term $\\eta > 1$. Then, the inconvenience function $f_i(t) = \\eta^{t-d_{it}+1}$ represents the increase in\ninconvenience experienced by customer i during the transition from period t to t + 1, given that customer i is urgent\nwith either a due or overdue deadline ($d_{it} < t$). If the deadline is not due ($d_{it} > t$), the function is defined as $f_i(t) = 0$.\nIn our problem, costs are only revealed once the outcome of all risky assignments is realized. Thus, we denote the\nexpected costs given a state $S_t$, decision $x_t$ and probability for unresolved services p as:\n$E[C(S_t, x_t)] = \\sum_{i \\in K_{x_t}^u} f_i(t) + p \\cdot \\sum_{i \\in K_{x_t}^r} f_i(t). $\n(1)"}, {"title": "3.3.4 Stochastic Information and Transition Function.", "content": "The stochastic information is threefold and defined as $w_{t+1} = (W_{t+1}, K_{t+1}, K_{t+1}^1)$. The first part is the set of available\ntechnicians, $W_{t+1} \\subseteq W$. The second part $K_{t+1}$ reveals a new set of customers with corresponding locations, tasks and\ndeadlines. We recall that $K_{t+1}$ is empty after the cutoff period, i.e., $t > T^c$. The third part relates to the realization of\nunresolved services, represented by the subset of risky assignments $K_{t+1}^1 \\subseteq K_t^r$. Only now, the real cost of a decision\nis revealed which we define as:\n$c(S_t, x_t, W_{t+1}) = \\sum_{i \\in \\{K_{x_t}^u \\cup K_{t+1}^1\\}} f_i(t).$\n(2)\nThe transition function T results in a new decision state in the next period $S_{t+1} = T(S_t, x_t, W_{t+1}) = (W_{t+1}, K_{t+1}, d_{t+1}, p_{t+1}, \\tau_{t+1})$ with $K_{t+1} = (K_{x_t}^u \\cup K_{t+1} \\cup K_{t+1}^1)$. The last three state entries are combined from the\ninformation of post-decision state $\\widetilde{S}_t$ and the set of new customers $K_{t+1}$. In the special case of $K_{t+1} = \\O \\forall t > T^c$, all\nrequests in $\\widetilde{S}_t$ were completed and the process terminates."}, {"title": "3.3.5 Solution.", "content": "The solution of our process is a decision policy $\\pi \\in \\Pi$, a sequence of decision rules $X^{\\pi}(S_t)$ assigning a decision $x_t$ to\nevery state $S_t$. A policy $\\pi^*$ is called optimal if it minimizes the expected overall customer inconvenience for all periods"}, {"title": "4 Methodology", "content": "In this section, we present our solution method. First, we provide a motivation and overview of our method and then\nintroduce our state-dependent parametrization. Finally, we conclude with the algorithmic details."}, {"title": "4.1 Motivation and Overview", "content": "Finding effective decisions for our problem is challenging. When recalling the two decisions illustrated in the\nsmall example in Subsection 3.2, it is not clear which one to select. The first decision avoids immediate customer\ninconvenience but leaves the system in the next period congested by serving fewer customers and risking rework.\nParticularly in case of fewer available technicians due to absences, this leads to more urgent customers in future and\nthus, to higher chances for inconvenience. The second decision routes more efficiently but accepts inconvenience by not\nserving an urgent customer on time. Still, the resulting next state is less congested and therefore may allow for less\nfuture inconvenience. Essentially, an effective policy must carefully balance the three competing goals of (i) ensuring\nsafe assignments and limited risk of rework, (ii) prioritizing service for urgent customers close to or over their deadlines\nto avoid inconvenience, and (iii) creating efficient routes to serve many customers and reduce postponements of (urgent)\ncustomers. While considering safe assignments is relevant in all states, the balance between routing efficiency and\nservice to urgent customers should ideally be adapted to the current state. That is, in states with more non-urgent\ncustomers, service to isolated urgent ones may be more important, while in congested states, the focus should shift\ntoward serving many customers efficiently. This is what we propose with our policy.\nIn the following, we give an overview of the functionality of our policy. In Figure 2, we depict the three goals (i)-(iii)\nwithin a triangle together with an illustration of our proposed policy's functionality in the center. Our policy dynamically\nbalances the goal focus based on state characteristics. Thus, it may position any two different states $S_1$ and $S_2$ differently\nwithin the triangle. The digits 1-7 in the black boxes correspond to a numeration of benchmark policies introduced\nin Section 5.2, indicating their positions within the triangle. We first discuss the three goals before demonstrating the\nfunctionality of our policy in detail.\n\u2022 Safe assignments. Prioritizing safe assignments means that advanced customers are served by expert\ntechnicians only. This will avoid any rework in future periods. However, it may prohibit consolidation\nopportunities for regular technicians, e.g., in areas with several easy and few advanced customers. In the worst\ncase, it leaves regular technicians underutilized or even idling while expert technicians travel the entire service\narea to serve advanced customers. Both leads to inconvenience for existing customers and a congestion of the\nsystem.\n\u2022 Service urgency. Service urgency comprises service of customers that are already delayed, but also service\nof customers with approaching deadlines. Focusing on deadlines only leads to similar issues as focusing on\nsafe assignments. Serving primarily urgent customers can help ensure that current period inconvenience is"}, {"title": "4.2 Analytical Derivation of Score Function", "content": "Our score function respects service urgency and routing efficiency, balanced according to the current state information\ncaptured by the parameter $a_{S_t}$. In this section, we evaluate properties of the Bellman equation and use these properties\nto motivate the design of the two parts of our score function (see Equation 5).\nService Urgency. The first part of the score function is motivated by the service urgency of customers. We measure\nservice urgency by the expected increase in inconvenience we can avoid by assigning a customer to a technician. If\nan assigned customers has a violated deadline, we can define these expected savings as $(1 \u2013 p_{wit}) \\cdot f_i(t)$. For safe\nassignments ($p_{wit} = 0$), savings of $f_i(t)$ are certain whereas for risky assignments ($p_{wit} = p$), savings are uncertain\nand result in an expected value of $(1 \u2013 p) \\cdot f_i(t)$. For any customer with an expiring deadline in the future, this measure\nis indifferent as no real inconvenience can emerge. However, as we show in the following proposition, there is value in\nconsidering (artificial) savings in inconvenience also for customers with deadline periods in the future."}, {"title": "Proposition 4.1 (Monotonicity of the value function in dt)", "content": "Given a post-decision state $S_t$ with deadlines $d_t$, we\nconstruct $S_t'$ such that $S_t$ and $S_t'$ are identical except for their corresponding customer deadlines, i.e., $d_i \\leq d'_i \\forall i \\in$\n$(K_{x_t}^u \\cup K_t^r)$. Then it holds that $V (S_t') \\leq V(S_t)$."}, {"title": "Corollary 4.1", "content": "Given a state $S_t$, that includes two customers $i, j \\in K_t$ with the same location, the same requirements,\nbut different deadlines $d_{it} < d_{jt}$, we define two decisions $x_t^{(i)}$ and $x_t^{(j)}$. Decisions $x_t^{(i)}$ and $x_t^{(j)}$ are constructed to have\nthe same assignment of customers to technicians and the same route except that decision $x_t^{(i)}$ includes customer i and\nnot j and vice versa for decision $x_t^{(j)}$ (in both cases, the customer is assigned the same technician and their position in\nthe route is equal). Then, it holds that $V (S_t^{(i)}) \\leq V (S_t^{(j)})$."}, {"title": "4.3 State-Dependent Parametrization", "content": "In this section, we explain how we learn a state-dependent parametrization of our score parameter $a_{S_t}$. To this end,\nwe employ the concept of proximal policy optimization (PPO, Schulman et al. 2017), the current state-of-the-art\nextension of trust-region policy gradient methods. The state-dependent parametrization is given by a function A that\nmaps a state $S_t$ to a state-dependent parameter $a_{S_t}$. We recall that $a_{S_t}$ balances service urgency and routing efficiency,\nsubstantially impacting the assignments of customers to technicians in the decision-making process. We represent our\nproblem knowledge whether $a_{S_t}$ is suitable given a state $S_t$ by the continuous probability density function $\\lambda(\\alpha | S_t)$.\nThen, parameters $a_{S_t}$, that direct decisions toward fewer accumulated costs arising from a given state $S_t$ onwards,\nshould have a high density $\\lambda(\\alpha | S_t)$. During training, A is a probability density function conditioned on the current\nstate $S_t$, from which we randomly sample parameters. However, during evaluation, we define our state-dependant\nparametrization function as $A(S_t) = arg \\max \\lambda(\\alpha | S_t)$ which represents a deterministic mapping of a state $S_t$ to a\nbest estimated parameter $a_{S_t}$. We improve A by shaping $\\lambda$ using offline training iterations in a RL framework. In\nthe training framework, we do not always choose the best $a_{S_t}$ according to our current problem knowledge $\\lambda(\\cdot | S_t)$ but\nrather sample the parameter $a_{S_t} \\sim \\lambda(\\cdot | S_t)$ to foster exploration. For that purpose, we set $\\lambda(\\cdot | S_t)$ to the probability\ndensity function of the normal distribution $N (\\mu_t, \\sigma_t^2)$. The mean $\\mu_t$ of the normal distribution is the output of a neural\nnetwork $d_{\\Theta}$ with parameters $\\Theta$ that takes state information as input. Thus, $\\mu_t$ is state-dependent. The variance $\\sigma_t^2$ of the\nnormal distribution is interpreted as a training parameter that describes how strongly we explore in training iteration k.\nFor the sake of simplicity, we represent the probability density function defined by the current network parameters $\\Theta$ as\n$\\lambda_{\\Theta}$."}, {"title": "4.4 Algorithmic Augmentation", "content": "Learning a state-dependant parametrization is immensely difficult. Even when following the original implementations\nof the state-of-the-art PPO algorithm as described in Schulman et al. (2017), the final result after thousands of\ncomputationally expensive learning iterations might be unsatisfactory. As Engstrom et al. (2019) argue, algorithmic\naugmentations in the implementation of PPO play a major role in the success of a training run but are, if at all, only\nmentioned hidden in the appendix. In our preliminary tests, we came to similar conclusions. Therefore, we want to\nprovide the community with our insights which \u201ctricks\u201d enabled us to learn the state-dependant parametrization when\nthe standard implementation may not have succeeded. Inspired by Engstrom et al. (2019), we have defined four different\nalgorithmic augmentations.\nScaling of Costs. Instead of using the actual costs for training, we scale them to the interval [0, 1] in order to reduce\nvariance in training.\nScaling of Observations. Instead of considering the actual observation from the environment, we scale the observation\nalong every dimension in order to improve the quality of the learning parameter space with respect to the optimizer we\napply.\nValue Function Clipping. We smooth the targets of the value network by considering the value loss\n$L^V = min \\{(V_{\\omega_k} (S_t) \u2013 \\widehat{C}_t)^2, (clip (V_{\\omega_k} (S_t), V_{\\omega_{k-1}} (S_t) - \\epsilon, V_{\\omega_{k-1}} (S_t) + \\epsilon) - \\widehat{C}_t)^2\\}$"}, {"title": "4.5 Score-Based Assignment and Routing Policy", "content": "As noted in Section 3.3.3, the decision space is a variant of a heterogeneous team orienteering problem with time limits,\nan NP-hard problem. Each state contains numerous customer requests and multiple technicians, making exact search\nmechanisms of the decision space computationally challenging. Since decisions need to be made in each state, we\nuse a routing heuristic, $\\psi : (X(S_t) \\times W_t \\times K_t) \\rightarrow X(S_t), (\\widehat{i}, w, i) \\rightarrow x$, that inserts a customer i into the position\nwithin technicians w's route $\\widehat{i}$ that causes the smallest increase in overall route duration. Thus, we expedite the solution\nprocess and obtain effective tours within reasonable runtime. For a fair comparison, we use $\\psi$ for our method and\nall benchmark methods. Even though it is relatively straightforward, we show in Appendix D.3 that the resulting\ntours are effective, and even more important, can capture different foci on service urgency, routing efficiency, and safe\nassignments.\nIn Algorithm 2, we show the conceptual procedure how decisions are derived in a state $S_t$. The state variable $S_t$ and a\nparametrization function A serve as the input parameters. The final decision $x$ represents the output, determining the\ntechnician routing for that state. A (feasible) starting solution is a decision $x_t$ that does not assign any customers, i.e.,\nempty routes for all technicians from the depot to depot ([0,0], . . . , [0,0]). Then, customers are added subsequently to the routing\nsolution $x_t$ as follows. From Line 6 to 12, we iteratively assign every remaining customer i to every technician w as we\ndetermine the score s of that assignment via our score function $\\zeta$, and update the (preliminary) routing decision $\\widehat{x}_t$ with\nfunction $\\psi$. If the score value is higher than all previously observed values and the decision is feasible (Line 9), the\ndecision is stored (Line 10-12). Once all score values are calculated, the routing decision x is updated according to the\nassignment with the highest score value (Line 15). The process is repeated until either all customers are assigned or no\nfeasible assignments remain."}, {"title": "5 Computational Evaluation", "content": "In this section, we first describe the test instances and benchmark policies. Then, we present the computational analysis,\nproviding insights into the objective value and examining both the methodology and the problem."}, {"title": "5.1 Test Instances", "content": "In our experiments, the fleet consists of three regular and three expert technicians. We assume an absence rate of 10%\nper technician and day (IWD 2024). Technicians do not work more than seven hours a day in the field. We assume\non-site service times of 30 minutes. Following the setting of the companies discussed in the introduction, customers are\nuniformly distributed across a quadratic service area of 200 \u00d7 200 kilometers, with the depot located in the center. We\nfurther assume Euclidean distances and an average driving speed of 60 kilometers per hour to capture the road network\nand a mix of highways, rural roads, and cities. We assume operations of a month with orders coming in the first three\nweeks (five working days per week) and a subsequent leftover phase. The cutoff period is therefore $T^c = 16$.\nWe expect 180 customer requests during a week leading to an expected number of six customer requests per technician\nand workday. Yet, as service requests accumulate over the weekend, we assume that the expected number of requests on\nMondays is three times as high as on the other days. Technically, the expected number of (new) daily customer requests,\nexcluding Mondays, follows the normal distribution $N(\\mu, \\sigma^2)$ with $\\mu = \\frac{180}{7} \\approx 25.7$, coefficient of variation $cv = $\n$\\frac{4}{7}$ and $\\sigma = cv \\times \\mu \\approx 4.3$. On Monday, the number is tripled.\nWe assume a time span of two days after customer requests are revealed during which no inconvenience arises. Thus,\na service provider has three periods to serve a request on time. After that, we assume an increase in inconvenience\nof 10% from period to period (see Figure 12). The likelihood of easy and advanced tasks is even, matching the skill\ndistribution within the technician fleet. Risky assignments remain unresolved in half the cases, i.e., with probability\n$p = 0.5$. Based on these parameters, we create 150 test instances, i.e., realizations of the entire sequential decision\nprocess, for evaluating our method."}, {"title": "5.2 Benchmark Policies", "content": "We compare our method, which we call DB (dynamic-balance) policy, to six problem-oriented benchmark policies and\none method-oriented policy. All policies follow the general procedure proposed in Algorithm 2, relying on routing\nheuristic $\\psi$, but with different and static foci on goal dimensions in the score function. According to the following\nnumbering 1-7, we have positioned each policy in Figure 2. The first three policies follow different ideas of a myopic\napproach to minimize the immediate increase in customer inconvenience (service urgency). The next three policies"}, {"title": "5.3 Objective Value and Average Delay", "content": "First, we compare the objective values of all policies. The grey bars of Figure 3 show the average inconvenience per\ncustomer. Each bar on the x-axis represents a policy, the y-axis shows the respective inconvenience value. For a detailed\nanalysis related to the development of inconvenience over time, we refer to Appendix D.2.\nWe observe significant differences in inconvenience. Policies SB and DB perform substantially better as they induce\nthe fewest average customer inconvenience. The myopic policies MYSF, MYEX, MYEF perform better than their\ncorresponding policies SF, EX, EF that do not consider customer deadlines. As expected, there is value in considering\ncustomer deadlines when aiming to minimize customer inconvenience. From the policies considering deadlines,\npolicy MYEF performs worst. This policy ignores heterogeneity in tasks and workforce, and only aims for the most\nefficient routing. This leads to many risky assignments and rework and eventually to more inconvenience for future\ncustomers.\nInterestingly, the MYSF policy performs worse than MYEX. MYSF avoids any risky assignments but allows assigning\neasy tasks to any technician. This leads to a backlog of the expert technicians and many unserved advanced tasks, while\nregular technicians remain idle. Consequently"}]}