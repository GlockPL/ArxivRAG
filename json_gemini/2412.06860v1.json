{"title": "Balancing Efficiency and Effectiveness: An LLM-Infused Approach for Optimized CTR Prediction", "authors": ["Guoxiao Zhang", "Yi Wei", "Yadong Zhang", "Huajian Feng", "Qiang Liu"], "abstract": "Click-Through Rate (CTR) prediction is essential in online advertising, where semantic information plays a pivotal role in shaping user decisions and enhancing CTR effectiveness. Capturing and modeling deep semantic information, such as a user's preference for \"H\u00e4agen-Dazs' HEAVEN strawberry light ice cream\" due to its health-conscious and premium attributes, is challenging. Traditional semantic modeling often overlooks these intricate details at the user and item levels. To bridge this gap, we introduce a novel approach that models deep semantic information end-to-end, leveraging the comprehensive world knowledge capabilities of Large Language Models (LLMs). Our proposed LLM-infused CTR prediction framework(Multi-level Deep Semantic Information Infused CTR model via Distillation, MSD) is designed to uncover deep semantic insights by utilizing LLMs to extract and distill critical information into a smaller, more efficient model, enabling seamless end-to-end training and inference. Importantly, our framework is carefully designed to balance efficiency and effectiveness, ensuring that the model not only achieves high performance but also operates with optimal resource utilization. Online A/B tests conducted on the Meituan sponsored-search system demonstrate that our method significantly outperforms baseline models in terms of Cost Per Mile (CPM) and CTR, validating its effectiveness, scalability, and balanced approach in real-world applications.", "sections": [{"title": "1 Introduction", "content": "Click-Through Rate (CTR) prediction plays an important role in recommender systems and online advertising [1, 4, 6, 7, 11, 13, 18]ks are proposed to capture semantic information by involving Pretrained Language Models (PLMs) [3, 9, 10, 12, 13, 15, 17]. As is shown in Figure 1, semantic information can be categorized as explicit information and implicit information, where explicit information is directly obtainable from the texture features and implicit information is inferred by relevant world knowledge. \nCompared with those works of BERT series [12, 13, 17], generative Large Language Models (LLMs) provide a fresh technological approach for recommender systems to capture deep semantic information [3, 9, 10, 15], due to their capacity for leveraging world knowledge to infer implicit information. Specifically, CTRL[9] integrates semantic information from LLMs into traditional ID-based models using contrastive learning. ClickPrompt [10] first aligns the collaborative and semantic knowledge from ID and textual features via the prompt interface, then tunes the CTR model without PLM for inference efficiency. KAR [15] acquires two types of external knowledge from LLMs-the reasoning knowledge on user preferences and the factual knowledge on items, which are transformed into augmented vectors to be compatible with the recommendation task. BAHE[3] employs the LLM's pre-trained low layers to extract embeddings of user behaviors and stores them in the offline database, then utilizes the deeper, trainable layers of the LLM to generate comprehensive user embeddings. However, it is important to note that, due to limited consideration of efficiency, most of these studies(ClickPrompt[10], KAR [15], CTRL [9]) remain largely experimental and have not been applied in practical industrial settings for CTR prediction. Besides, BAHE[3] chooses to partially utilize information from LLMs, which restricts the exploration of the effectiveness of LLMs for CTR estimation."}, {"title": "2 Methodology", "content": "In this section, we present our framework, depicted in Figure 2, which consists of two main modules: the Multi-level Knowledge Distillation Module and the Multi-level Knowledge Integration Module. We first introduce the process of distilling LLMs at both item and user levels, followed by integrating the distilled model into the CTR prediction framework."}, {"title": "2.1 Multi-level Knowledge Distillation Module", "content": "This module is designed to efficiently distill semantic knowledge at both item and user levels from LMs into a more efficient one. Knowledge generation and knowledge distillation are the main components of this module. We provide a detailed description of them in this section."}, {"title": "2.1.1 Knowledge Generation", "content": "High-quality and diverse data are essential to ensure that the distilled model generalizes well across different contexts and retains critical semantic information. To guide the LLMs produce high-quality data, we first manually selected outputs from the ChatGPT as reference, which include key phrases and the rationale. Then, the prompt will be enhanced by dynamically chosen reference output as an example for in-context learning [2]. Our prompt template for item level is illustrated in Figure 3. To ensure the dataset is representative and diverse, stratified and importance sampling are adopted, considering categorical information and posterior statistics. For user level data, the generation procedure follows a similar protocol."}, {"title": "2.1.2 Knowledge Distillation", "content": "Following [14], we leverage knowledge distillation to transfer semantic and contextual reasoning capabilities from the LLMs teacher model T to a more computationally efficient student model S. The student model is trained to replicate the output \\(y_T\\) of the teacher model by minimizing the distillation loss as follows:\n\\[L_{distill} = \\sum_{t=1}^{|y_T|} log (P_\u03b8 (y_{T,t} | y_{s,<t}, x)),\\]\n(1)\n\\[S^* = arg \\min_S L_{distill}\\]\n(2)\nwhere \\(y_{T,t}\\) represents the t-th token of the teacher model's output sequence, \\(y_{s,<t}\\) are the preceding tokens in the student model's output sequence and \\(x\\) are the input tokens."}, {"title": "2.2 Multi-level Knowledge Integration Module", "content": "The MKIM is designed to seamlessly integrate semantic insights at both the item and user levels within the CTR prediction framework. It comprises three key components: 1) LoRA, which enhances alignment and performance by fine-tuning distilled LLMs with Low-Rank Adaptors for computational efficiency; 2) Feature Adaptors, which compress and transform LLM output embeddings using a Multi-Layer Perceptron (MLP) to ensure effective integration into the CTR model; 3) Frequency-Adaptive Relevant Items Fusion, which improves item embedding robustness through masking and pooling operations. The transformed item and user embeddings are then concatenated to form the input for CTR tasks."}, {"title": "2.2.1 LoRA", "content": "Following [8], we finetune the distilled LLMs for CTR tasks using Low-Rank adaptors. Specifically, for a pre-trained weight matrix \\(W_o \\in \\mathbb{R}^{d \\times k}\\), we constrain its update by representing the latter with a low-rank decomposition \\(W_o + \\Delta W = BA\\), where \\(B \\in \\mathbb{R}^{d \\times r}\\), \\(A \\in \\mathbb{R}^{r \\times k}\\), and the rank \\(r < min(d, k)\\). During training, \\(W_o\\) is frozen and does not receive gradients, while the A and B contain trainable parameters."}, {"title": "2.2.2 Feature Adaptors", "content": "For the raw output embeddings of the LLMs, we define \\(e_u\\) as the user-level embedding, \\(L\\) as the length of user behavior sequence, \\(e_t\\) as the target item embedding and \\(e_i\\) as the item embeddings in user behavior sequence, where \\(i\\) ranges from 1 to \\(L\\). We apply a Multi-Layer Perceptron (MLP) as the feature adaptor to generate the projected embeddings \\(\\bar{e_u}\\), \\(\\bar{e_t}\\) and \\(\\bar{e_i}\\) as follows:\n\\[\\bar{e_u} = MLP(e_u)\\]\n(3)\n\\[\\bar{e_t} = MLP(e_t)\\]\n(4)\n\\[\\bar{e_i} = MLP(e_i), i \\in \\{1, ..., L\\}\\]\n(5)"}, {"title": "2.2.3 Frequency-Adaptive Relevant Items Fusion", "content": "To enhance the robustness of item embeddings during training, the item-level features \\(e_{item}\\) are obtained as follows:\n\\[sim(\\bar{e'_i}, \\bar{e_t}) = \\frac{\\bar{e'_{masked_i}} \\cdot \\bar{e_t}}{|\\bar{e'_{masked_i}}|| ||\\bar{e_t}||}\\]\n(6)\n\\[e_{top-k} = Top-k \\{sim(\\bar{e'_i}, \\bar{e_t}) | i \\in \\{1, ..., L\\}\\}\\]\n(7)\n\\[e_{item} = \\frac{\\sum_{i=1}^{e_{top-k}} \\bar{e'_i}}{k}\\]\n(8)\nwhere \\(\\bar{e_t}\\) represents the target item, \\(\\bar{e'_{masked_i}}\\) is gained from \\(\\bar{e_i}\\) with frequency-guided stochastic masking. The top-k similar item embeddings are then processed through a fusion layer to produce the final item embedding. We utilize the element-wise addition as our fusion operation."}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 Experimental Setup", "content": ""}, {"title": "3.1.1 Dataset", "content": "We conducted experiments using both a public dataset and one real-world business dataset. The key statistics of these datasets are presented in Table 1.\n\u2022 KDD Cup 2012: A CTR prediction dataset comprising advertising logs from Tencent, which include queries and user information. We randomly selected 10% of the users from the complete dataset for computation efficiency.\n\u2022 Meituan: A real-world dataset, sourced from Meituan's recommendation platform, contains extensive user-item interaction data that reflects actual business scenarios."}, {"title": "3.1.2 Baselines", "content": "To demonstrate the effectiveness of our proposed framework, we compare it with the popular CTR baseline models (parameter settings following [7]):\n\u2022 DIN [18]: a leading sequential CTR model that uses an attention mechanism to identify target-related interests.\n\u2022 DeepFM [6]: a popular CTR model that merges factorization machines with deep learning techniques for feature interaction.\n\u2022 DeepCharMatch [4]: a method that directly models semantic information at the character level using query-ad pairs.\n\u2022 SuKD [13]: a model that incorporates additional NLP knowledge in the fine-tuning of PLMs.\n\u2022 BERT4CTR [12]: an effective framework for integrating PLMs with non-textual features.\n\u2022 PRINT [7]: a BERT-based model that improves CTR prediction by accounting for personalized incentives in query-ad semantic relevance."}, {"title": "3.1.3 Evaluation Metrics", "content": "To thoroughly assess our approach, we adopted distinct metrics for evaluating the effectiveness of the distillation process and the model's overall performance while examining the relationship between these two evaluations.\nDistillation Evaluation Metric. To evaluate the effectiveness of our distillation process, we calculate the F1 score based on predicted key phrases. Cosine similarity is applied to determine phrase equivalence, leveraging embeddings generated by a BERT model. We manually annotated a subset of the LLM's outputs to establish a ground truth."}, {"title": "3.2 Overall Performance", "content": "In this section, we evaluate the performance of our novel LLM-infused CTR prediction framework against PRINT[7] and other baseline models across the two datasets. As detailed in Table 2, our approach demonstrates superior performance. Our framework leverages the deep semantic capabilities of Large Language Models as well as end-to-end training and inference processes. Compared to PRINT, our approach achieves a notable improvement in AUC, with increases of 0.25% and 0.63% over the KDD Cup 2012 and Meituan datasets, respectively."}, {"title": "3.3 Ablation Study for Our Framework", "content": "We performed a comprehensive ablation study to evaluate the contribution of each module within our framework. As depicted in Table 3, each module significantly contributes to the overall performance improvement of the complete model."}, {"title": "3.4 Effectiveness of LLM Distillation", "content": "Our experiments demonstrate a positive correlation between the knowledge distillation process and the performance of the CTR model, highlighting the critical role of an effective distillation strategy in enhancing model performance. Specifically, when the F1 score increases from 0.05 to 0.53, the AUC score of the CTR model shows a significant improvement. However, beyond an F1 score of 0.60, the rate of increase in the AUC score begins to diminish as shown in Figure 4. This observation suggests that the current method can enhance CTR model performance only up to a certain level. To further improve the CTR model, future research could explore more sophisticated designs for the distillation task to overcome this performance plateau."}, {"title": "3.5 Online Deployment and A/B Test Results", "content": "To reduce the inference time of the CTR model, we implement a combination of pre-computation and a hierarchical caching system, as inspired by PRINT[7]. For target item embeddings, we store pre-computed embeddings of top items ranked by exposure counts in Redis. If an item is not found in the cache, its embedding is computed in real time after the recall phase and before being fed into the CTR model. These newly computed embeddings are then cached in Redis, utilizing an eviction policy to manage storage efficiently."}, {"title": "4 Conclusion", "content": "In this paper, we present an innovative LLM-infused framework for CTR prediction designed to effectively utilize semantic information in CTR tasks. By introducing the KDM and KIM, our framework captures deep semantic insights at both item and user levels, addressing the limitations of traditional ID-based recommendation systems and existing methods that attempt to integrate LLMs or BERT into CTR tasks. Our model significantly enhances the AUC on the Meituan dataset and achieves a notable increase in system revenue by 2.59%, nd CTR by 2.12% in the Meituan sponsored-search system. This approach opens new avenues for incorporating LLMs into CTR prediction, and we hope our work inspires further research and exploration in this field."}]}