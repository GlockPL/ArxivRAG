{"title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces", "authors": ["Loris Gaven", "Thomas Carta", "Cl\u00e9ment Romac", "C\u00e9dric Colas", "Sylvain Lamprier", "Olivier Sigaud", "Pierre-Yves Oudeyer"], "abstract": "Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and LP online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces.", "sections": [{"title": "1. Introduction", "content": "Humans are open-ended learners, continuously exploring and developing new skills through their lifetime. A key mechanism to enable this remarkable capacity is curiosity-driven learning the intrinsic motivation to explore for the sake of learning and discovery (Berlyne, 1954; Kidd &\nHayden, 2015). Crucially, humans are autotelic learners intrinsically motivated to represent, invent, select and solve their own goals (Colas et al., 2022a). To navigate in a possibly infinite space of goals, without time to explore it exhaustively, they are equipped with intrinsic motivation signals (Baldassarre & Mirolli, 2012; Gottlieb & Oudeyer, 2018). Research on human curiosity has shown the key role of one such intrinsic signal: Learning Progress (LP), i.e. improvement of one's own ability to solve goals (Kaplan &\nOudeyer, 2007). Computational modeling work showed both how it enables efficient automatic curriculum learning (Lopes &\nOudeyer, 2012a; Poli et al., 2024b) and how it generates developmental trajectories that simulate key properties in the development of human infants (Oudeyer & Smith, 2016). Recently, several experimental paradigms where humans were free to explore various learning activities confirmed that humans use metacognitive LP monitoring to explore and prioritize goals (Ten et al., 2021; Leonard et al., 2023; Sayal\u0131 et al., 2023; Poli et al., 2024a).\nInspired by open-ended learning in humans and other natural systems, research in AI and artificial life has studied how"}, {"title": "2. Related Work", "content": "2.1. Goal selection in autotelic agents\nAutotelic agents exploring vast goal spaces face a critical challenge: they must prioritize which goals to pursue to efficiently develop general competence (Colas et al., 2022b). The automatic curriculum learning community has developed various approaches to address this challenge (Portelas et al., 2020b), leveraging different forms of intrinsic motivation: pursuing goals of intermediate difficulty (Held\net al., 2017; Racaniere et al., 2020; Castanet et al., 2023), seeking novelty or uncertainty (Warde-Farley et al., 2019;\nPong et al., 2019; Pitis et al., 2020), or maximizing learning progress (LP) (Stout & Barto, 2010; Matiisen et al.,\n2020; Fournier et al., 2018; Portelas et al., 2020a; Colas\net al., 2019; Kanitscheider et al., 2021; Kova\u010d et al., 2023;\nZhang et al., 2024). Among these, LP-based methods have proven particularly robust - they adapt to the agent's capabilities without requiring environment knowledge and\navoid common pitfalls like getting stuck on goals where progress plateaus or chasing uncontrollable novelty. The key challenge with LP approaches lies in efficiently estimating progress over large goal spaces, which is the focus of our work.\n2.2. Computing LP over goals\nLearning Progress (LP) measures the expected future improvement in achieving a goal through practice (Oudeyer"}, {"title": "3. Methods", "content": "In this section, we detail how MAGELLAN learns a metacognitive module that estimates and generalizes an agent's LP over language goal spaces. We then explain classic LP baselines against which MAGELLAN is compared. Finally, we introduce the Little-Zoo environment, specifically designed to study commonsense-based generalization abilities of LLM agents when facing large language goal space.\n3.1. Problem statement\nLet $M = (S, A, T, R)$ be an MDP, with $S$ a set of states, $T$\nthe transition function, $A$ the action space and $R$ the reward function. Let $G$ be a goal space and $\u03a0$ the policy space. We\ndefine a competence function $C_{M,\\pi} : G \\rightarrow \\mathbb{R}$ that indicates the competence of a policy $\\pi \\in \u03a0$ for a goal in $M$.\u00b9 The\nfinal aim is to find the optimal policy $\\pi^*$ that maximizes\n$J_{M}(\\pi) = E_{g \\sim U(G)}[C_{M,\\pi}(g)]$,\nwhere $U(X)$ is the uniform distribution over a set $X$.\nIn this paper, we focus on episodic online goal-conditioned RL with sparse and binary rewards, defined on a goal-augmented MDP $(S, A, T, G, R)$, with $R : S \\times G \\rightarrow \\{0; 1\\}$ a binary success function indicating whether a state $s$ satisfies a goal $g$. Here, we define $G = \\{S_{0} \\times I\\backslash S_{0} \\subseteq S\\}$, with $I$ an instruction space and $S_{0}$ the set of initial states. We consider a textual environment where a prompting function $\\phi : S \\times I \\rightarrow V^{K}$ is given to transform any pair (state, instruction) into a textual prompt of $K$ tokens in a given vocabulary $V$. Thus, from the agent side, the policy $\\pi$ selects any action $a_{h} \\in A$ by sampling from a categorical distribution $\\pi(.|\\phi(s_{h}, i))$ at any step $h$ of the episode.\nFor our competence function we use the success probability $P_{\\pi}(s_{0}, i)$ defined as the probability for $\\pi$, starting from $s_{0}$,\nto fulfill $i$ within $H$ steps: $P_{\\pi}(s_{0}, i) = E_{\\tau \\sim \\pi(\\tau|\\phi(s_{0},i))}[r_{\\tau,i}]$,\nwith $r_{\\tau,i} = \\mathbb{1}(\\exists s_{h} \\in \\tau, R(s_{h},i) = 1)$ the goal outcome\nof episode $\\tau$ for instruction $i$, $\\mathbb{1}$ the indicator function and $\\pi(\\tau|\\phi(s_{0}, i))$ the distribution of episodes of $H$ steps induced by $\\pi$ to fulfilling $i$ from $s_{0}$. In this setting our objective becomes:\n$J(\\pi) = E_{s_{0} \\sim U(S_{0}),i\\sim U(I)} [P_{\\pi}(s_{0}, i)]$.\nHowever, given the possibly huge number of goals $(s_{0}, i)$, the direct maximization of the problem becomes particularly inefficient. Our aim is to leverage transfer of competence between goals and focus during training on the ones maximizing LP. We denote as $\u03c0\u207a$ the policy obtained after $t$"}, {"title": "3.2. Metacognitive generalization of learning progress in language model agents", "content": "With MAGELLAN, we propose to learn estimators of the current and past policy's competence for any goal. As opposed to prior works, which either consider all goals independently or use goal groupings, we argue that learning goal-conditioned estimators would allow generalization between similar goals without defining any clear group. We propose to leverage the LLM used by our agent to learn the parameters $\\theta_{t}$ of a competence estimator $C_{\\theta_{t}}(g)$ for a policy $\\pi_{t}$ on a goal $g$. We compute $C_{\\theta_{t}}(g)$ by giving $g$ in the LLM's prompt, which produces a latent representation on top of its final decoder block for the last token. We use a Multi-Layer Perceptron (MLP) to output the estimated competence based on this representation. We train both the LLM and the MLP, leveraging the LLM's ability to project goals into a latent space where semantically similar goals are close. By updating the estimated competence of one goal, this allows MAGELLAN to also update close goals.\nIn practice, we maintain a buffer $D_{t}$ which contains, for the $M$ most recent training episodes at $t$ (i.e., $\\tau_{t-M} ...\\tau_{t}$), their corresponding pair of goal and outcome (i.e., $(g = (s_{0}, i), r_{\\tau,i})$ for each $t$). As this work focuses on success probability (i.e., we want $C_{\\theta_{t}}(g) \\approx P_{\\pi_{t}}(g)$), we train $C_{\\theta_{t}}$ using stochastic gradient descent to minimize the binary cross-entropy: $L(\\theta_{t}) = E_{(g,r)\\sim D_{t}} [BCE(r, C_{\\theta_{t}}(g))]$.\nWe maintain another buffer $B_{t}$ storing the last $N$ weights of our competence estimator: $B_{t} = [\\theta_{t-N}, \\theta_{t+1-N},..., \\theta_{t}]$. Weights are added to the buffer every time the competence estimator is updated, enabling access to estimations of the policy's competence from time $t$ to $t \\text{--} N$. Using this information, we estimate the absolute LP (ALP) Baranes &\nOudeyer (2013); Kanitscheider et al. (2021), tracking both progress and forgetting, as follows:\n$ALP_{\\pi_{t}}(g) = |C_{\\theta_{t}}(g) - C_{\\theta_{t-N}}(g)|$."}, {"content": "This ALP estimation can subsequently be used to structure the agent's curriculum. We apply the multi-armed bandit goal selection scheme introduced by (Lopes & Oudeyer, 2012b) where each arm is a goal, and its utility is MAGELLAN's estimate of the ALP of this goal. Goals are then sampled proportionally to their estimated ALP with an annealing $\\epsilon$-greedy scheme ($\\epsilon$ decreasing from 1 to 0.2). In\npractice, we train two separate versions of the same initial LLM (using LoRA adapters (Hu et al., 2022)): one for the policy and one for MAGELLAN's current competence estimator. We show in Appendix D.1 ablations on architectural\nchoices indicating that 1) keeping the LLM frozen leads to poor results, highlighting the need for a dynamic representation space (see also Figure 5), and 2) training separate\nLoRA adapters for the policy and MAGELLAN leads to more stability.\n3.3. Classic ALP baselines\nFollowing the literature on ALP in Section 2.2, we implement classic approaches, focusing on two dimensions. First, we consider Online (Baranes & Oudeyer, 2013; Matiisen et al., 2020) vs Evaluation-based ALP (Kanitscheider et al., 2021; Zhang et al., 2024) estimation. Then, we consider directly using the goal space (Portelas et al., 2020a; Kanitscheider et al., 2021) or using expert-defined groups of goals with assumed competence transfer (Stout & Barto, 2010; Colas et al., 2019). The latter requires extensive expert knowledge (EK) given the absence of automatic approach for discrete goal spaces. As expert-defined groups\nare created beforehand, no competence transfer is assumed across groups, which is likely to happen in spaces like natural language, where transfer occurs between semantically close goals regardless of groups.\nWe thus implement four baselines (see all details in Appendix C.3):"}, {"title": "4. Experiments", "content": "We provide empirical answers to our scientific questions using experiments with 8 different random seeds in the\nLittle-Zoo environment. For our LLM agent, we use SAC-GLAM (Gaven et al., 2024) to finetune Flan-T5 248M (Raffel et al., 2020) as per SAC-GLAM's experiments. We compare MAGELLAN to the classic approaches presented in 3.3. For methods accessing external expert knowledge,\nwe use Little-Zoo's hidden goal families (grasp any object, grow plant, grow herbivore, grow carnivore) but add only possible goals in these groups. These baselines would totally\nfail if given all goals from the same hidden family, regardless of their feasibility. We thus provide an additional group containing all impossible goals. In all our experiments, we use success rate (i.e. average outcome over multiple trials for a goal) noted SR as the observed competence.\nWe first study how well MAGELLAN's competence esti-"}]}