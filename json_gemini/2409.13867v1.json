{"title": "MAGICS: Adversarial RL with Minimax Actors Guided by Implicit Critic Stackelberg for Convergent Neural Synthesis of Robot Safety", "authors": ["Justin Wang", "Haimin Hu", "Duy P. Nguyen", "Jaime Fern\u00e1ndez Fisac"], "abstract": "While robust optimal control theory provides a rigorous framework to compute robot control policies that are provably safe, it struggles to scale to high-dimensional problems, leading to increased use of deep learning for tractable synthesis of robot safety. Unfortunately, existing neural safety synthesis methods often lack convergence guarantees and solution interpretability. In this paper, we present Minimax Actors Guided by Implicit Critic Stackelberg (MAGICS), a novel adversarial reinforcement learning (RL) algorithm that guarantees local convergence to a minimax equilibrium solution. We then build on this approach to provide local convergence guarantees for a general deep RL-based robot safety synthesis algorithm. Through both simulation studies on OpenAI Gym environments and hardware experiments with a 36-dimensional quadruped robot, we show that MAGICS can yield robust control policies outperforming the state-of-the-art neural safety synthesis methods.", "sections": [{"title": "1 Introduction", "content": "The widespread deployment of autonomous robots calls for robust control methods to ensure their reliable operation in diverse environments. Safety filters [21] have emerged as an effective approach to ensure safety for a broad spectrum of robotic systems, such as autonomous driving [48,45,25,24], legged locomotion [23,1,20,38], and aerial robots [14,43,49,8]. Model-based numerical safety synthesis methods [2,6] offer verifiable safety guarantees, but can only scale up to 5-6 state variables. In order to develop safety filters at scale, recent research efforts have been dedicated towards using neural representations of robot policies [15,41,3,22], showing potential for safety filtering with tens [38,20] even hundreds of state variables [26].\nWhile neural safety filters have shown significant progress in scalability, they are often challenging to analyze due to their black-box nature. Moreover, in adversarial settings, na\u00efve training of agent policies can lead to severe oscillatory behaviors, preventing the algorithm from converging. Recently developed game-theoretic machine learning algorithms guarantee convergence to mathematically meaningful equilibrium solutions (e.g., Nash or Stackelberg), even for training of black-box models such as deep"}, {"title": "2 Related Work", "content": "Game-Theoretic Learning Algorithms. Recent years have seen significant progress in machine learning problems formulated as games, such as generative adversarial networks (GANs) [17], adversarial RL [39], and hyperparameter optimization [35]. Those problems involve interacting agents with coupled and potentially competing objectives, which calls for careful design of the training algorithm. Fiez et al. [11] leverages the implicit function theorem to derive a provably convergent gradient-based algorithm for machine learning problems formulated as Stackelberg games. In their follow-up work [13], they show that the simple gradient descent-ascent with finite timescale separation (7-GDA) algorithm guarantees local convergence to a minimax solution in zero-sum settings, a result also reported by concurrent work [29]. More recent work [36] shows that convergence can be achieved in Stackelberg learning only with first-order gradient information. A closely related topic to Stackelberg learning is the exploitability of suboptimal strategies. Lei et al. [33] use unsupervised learning to estimate the exploited level of each trajectory in a dataset, which is then used in an offline learning process to maximize the influence of the dominant strategy.\nOur work builds on insights from the game-theoretic learning community and provides a novel, yet rigorous convergence analysis for adversarial RL."}, {"title": "Adversarial Reinforcement Learning.", "content": "Multi-agent robust policy synthesis is increasingly solved by adversarial RL methods due to their scalability. Pinto et al. [39] pioneered the idea of robust adversarial reinforcement learning (RARL), which improves robustness by jointly training a stabilizing policy and a destabilizing adversary. However, this approach ignores the coupling between the agents' objectives. Learning with Opponent-Learning Awareness (LOLA) [16] explicitly captures the coupled interests among agents in multi-agent RL by accounting for the influence of one agent's policy on the predicted parameter of the other agents. Huang et al. [27] model policy-gradient-based adversarial RL as a Stackelberg game and apply the Stackelberg learning algorithm from Fiez et al. [11] to solve the RL problem. Although these game-inspired adversarial RL algorithms have shown promising improvement over non-game baselines, they typically lack provable convergence guarantees. Recent work by Zheng et al. [50] models the hierarchical interaction between the actor and critic in single-agent actor-critic-based RL [30,18] as a two-player general-sum game, and provides a gradient-based algorithm that is locally convergent to a Stackelberg equilibrium.\nOur work combines the best of both worlds by building on the state-of-the-art game-theoretic RL [50] and adversarial learning [13] approaches to address the missing piece of provably convergent multi-agent adversarial RL in continuous state-action spaces."}, {"title": "Data-Driven Safety Filters for Robotics.", "content": "Deep learning has enabled scalable synthesis of robust control policies. Rapid Motor Adaptation (RMA) [31] uses deep RL to learn a base policy, and supervised learning to train an online adaptation module, allowing legged robots to quickly and robustly adapt to novel environments. Lai et al. [32] propose to train a transformer model for quadrupedal locomotion across diverse terrains. Specially focused on safety, data-driven safety filters [21,47] aim at providing scalable safety assurances. Robey et al. [41] use control barrier function (CBF) constraints as self-supervision signals to learn a CBF from expert demonstrations. A similar approach [10] learns a CBF parameterized as a deep neural network from labeled safe and unsafe state samples. Another line of work focuses on neural approximation of safety Bellman equations. Safety RL [15] proposes to modify Hamilton-Jacobi equation [2] with contraction mapping, rendering RL suitable for approximate safety analysis. Hsu et al. [22] extend the single-agent safety RL to the two-player zero-sum setting by offline co-training a best-effort safety controller and a worst-case disturbance policy. They use model-based rollout to verify safety at runtime. However, existing multi-agent neural safety synthesis approaches predominantly lack convergence guarantees, which can make the training notoriously difficult and time-consuming.\nTo the best of our knowledge, our proposed game-theoretic deep adversarial RL algorithm is the first provably-convergent multi-agent neural safety synthesis approach."}, {"title": "3 Preliminaries and Problem Formulation", "content": "Notation. Given a function $f$, we denote $D_{\\theta}f$ as the total derivative of $f$ with respect to $\\theta$, $\\nabla_{\\theta} f$ as the partial derivative of $f$ with respect to $\\theta$, $\\nabla_{\\theta \\phi}^2 f := \\frac{\\partial^2 f}{\\partial \\theta \\partial \\phi}$, and $\\nabla_{\\theta}^2 f := \\frac{\\partial^2 f}{\\partial \\theta^2}$. We denote $|| . ||$ as the 2-norm of vectors and the spectral norm of matrices. We indicate matrix $A$ is positive and negative definite with $A \\succ 0$ and $A \\prec 0$.\nWe assume throughout that all functions $f$ are smooth, i.e., $f \\in C^q$ for some $q \\geq 2$."}, {"title": "Zero-Sum Dynamic Games.", "content": "We consider discrete-time nonlinear dynamics that describe the robot motion:\n$x_{t+1} = f(x_t, u_t, d_t),$\nwhere $x_t \\in \\mathcal{X} \\subset \\mathbb{R}^n$ is the state, $u_t \\in \\mathcal{U} \\subset \\mathbb{R}^{n_u}$ is the controller input (belongs to the ego robot), $d_t \\in \\mathcal{D} \\subset \\mathbb{R}^{n_d}$ is the disturbance input, and $f : \\mathcal{X} \\times \\mathcal{U} \\times \\mathcal{D} \\rightarrow \\mathcal{X}$ is a continuous nonlinear function that describes the physical system dynamics. We model the adversarial interaction between the controller and disturbance as a zero-sum dynamic game with objective $J^{\\pi_u, \\pi_d}(x)$, where $\\pi_u, \\pi_d : \\mathcal{X} \\rightarrow \\mathcal{U}$ are control and disturbance policies, respectively, $x := x^{x_0, T}_{\\pi_u, \\pi_d} = (x_0, x_1, ..., x_{T-1})$ denotes the state trajectory starting from $x_0 = x$ under dynamics (1), policies $\\pi_u$, and $\\pi_d$. In this paper, we seek to compute a control policy $\\pi_u$ and disturbance policy $\\pi_d$ that constitute a feedback Stackelberg equilibrium (FSE) of a zero-sum dynamic game, where $\\pi_u$ maximizes $J^{\\pi_u, \\pi_d}(x)$ and $\\pi_d$ minimizes $J^{\\pi_u, \\pi_d}(x)$. This is formalized in the following definition."}, {"title": "Definition 1 (Feedback Stackelberg Equilibrium).", "content": "The strategy pair $(\\pi_u^*, \\pi_d^*)$ is a feedback Stackelberg equilibrium if\n$\\inf_{\\pi_d \\in r(\\pi_u^*)} J^{\\pi_u, \\pi_d}(x) \\geq \\inf_{\\pi_d \\in r(\\pi_u)} J^{\\tilde{\\pi}_u, \\pi_d}(x), \\quad \\forall \\tilde{\\pi}_u \\in \\Pi^u,$\nand $\\pi_d^* \\in r(\\pi_u^*)$, where $r(\\pi_u) := {\\pi_d \\in \\Pi^d | J^{\\pi_u, \\pi_d} > J^{\\pi_u, \\tilde{\\pi}_d}, \\forall \\tilde{\\pi}_d \\in \\Pi^d}$ is the optimal response map of the follower. Here, $\\Pi^u$ and $\\Pi^d$ are the sets of control and disturbance policies, respectively.\nDefinition 1 is aligned with our interest in worst-case robot safety analysis: by assigning the controller as the leader and disturbance the follower, we give the disturbance the instantaneous information advantage [28]. Note that such worst-case analysis leads to a least-restrictive safety filter [21, Sec. 2.3] in that it maintains system safety for all possible disturbance realizations, only overriding nominal control actions when they are incapable of preventing an imminent safety failure.\nAn FSE can be characterized using the first- and second-order optimality conditions (sufficient conditions of those in Definition 1), leading to the notion of a differential Stackelberg equilibrium (DSE) [11], which can be verified more easily. To this end, we adopt a finite-dimensional parameterization of players' policies: $\\pi_u = \\pi_{\\theta}$ and $\\pi_d = \\pi_{\\psi}$, where $\\theta \\in \\mathbb{R}^{n_\\theta}$ and $\\psi \\in \\mathbb{R}^{n_\\psi}$. In Section 4, we consider the neural representation of policies, i.e., $\\pi_{\\theta}$ and $\\pi_{\\psi}$ are deep neural networks. We denote the resulting game objective value as $J^{\\pi_{\\theta}, \\pi_{\\psi}}(\\theta, \\psi, x)$. We recall that, in zero-sum games, a DSE is equivalent to a local minimax equilibrium."}, {"title": "Definition 2 (Strict Local Minimax Equilibrium", "content": "Strategy pair $(\\pi_{\\theta}^*, \\pi_{\\psi}^*)$ is a differential Stackelberg equilibrium of a zero-sum dynamic game if conditions $\\nabla_{\\theta}J^{\\pi_{\\theta}, \\pi_{\\psi}}(\\theta^*, \\psi^*, x) = 0$, $\\nabla_{\\psi}J^{\\pi_{\\theta}, \\pi_{\\psi}}(\\theta^*, \\psi^*, x) = 0$, $S_1(\\mathcal{J}_{JJ}(\\theta^*, \\psi^*, x)) < 0$, and $\\nabla^2(\\mathcal{J}_{.j}(\\theta^*, \\psi^*, x)) > 0$ hold, where $\\mathcal{J}_{JJ}(\\theta, \\psi, x)$ denotes the Jacobian of the individual gradient vector:\n$\\mathcal{J}_{.j}(\\theta, \\psi, x) = \\begin{bmatrix} - \\nabla_{\\theta \\theta}^2 J^{\\pi_{\\theta}, \\pi_{\\psi}}(\\theta, \\psi, x) & - \\nabla_{\\theta \\psi}^2 J^{\\pi_{\\theta}, \\pi_{\\psi}}(\\theta, \\psi, x) \\\\ - \\nabla_{\\psi \\theta}^2 J^{\\pi_{\\theta}, \\pi_{\\psi}}(\\theta, \\psi, x) & \\nabla_{\\psi \\psi}^2 J^{\\pi_{\\theta}, \\pi_{\\psi}}(\\theta, \\psi, x) \\end{bmatrix},$\nand $S_1(\\mathcal{J}_{JJ})$ denotes the Schur complement of $\\mathcal{J}_{JJ}$ with respect to its lower-right block."}, {"title": "Reach-Avoid Robot Safety Analysis.", "content": "We consider the worst-case reach-avoid safety analysis for robot dynamics (1). We define the ego robot's target and failure sets as $\\mathcal{T} := {x | l(x) \\geq 0} \\subseteq \\mathbb{R}^n$ and $\\mathcal{F} := {x | g(x) < 0} \\subseteq \\mathbb{R}^n$, where $l(\\cdot)$ and $g(\\cdot)$ are Lipschitz continuous margin functions, encoding problem-specific safety and liveness specifications. We use Hamilton-Jacobi-Isaacs (HJI) reachability analysis to capture the interplay between the best-effort controller policy $\\pi^u$, i.e., one that attempts to reach the target set $\\mathcal{T}$ without entering the failure set $\\mathcal{F}$, and worst-case disturbance policy $\\pi^d$, i.e., one that prevents the controller from succeeding. To this end, we formulate an infinite-horizon zero-sum dynamic game with the following objective functional:\n$J^{\\pi, \\pi_d}(x) := \\max_{k} \\min_{\\tau \\in \\mathcal{E}[k, T]} l(x_{\\tau}), \\min_{T>k} g(x_k) \\}.$\nThe game's minimax solution satisfies the fixed-point Isaacs equation [28]:\n$V(x) = \\min\\{ g(x), \\max\\{ l(x), \\max_{u \\in \\mathcal{U}} \\min_{d \\in \\mathcal{D}} V(f(x, u, d))\\} \\}.$\nValue function $V(\\cdot)$ encodes the reach\u2013avoid set $\\mathcal{RA}(\\mathcal{T}, \\mathcal{F}) := {x | V(x) \\geq 0}$, from which the ego agent is guaranteed a policy to safely reach the target set without entering the failure set. Note that (5) implies that the disturbance has the instantaneous informational advantage and $V(x) = \\max_u \\min_d J^{\\pi, \\pi_d}(x)$ is known as the lower value of the zero-sum game (4). It corresponds to the minimal safety margin $g$ that the controller is able to maintain at all times under the worst-case disturbance.\nUnfortunately, the time and memory complexity required for numerically solving Isaacs equation (5) is exponential with respect to the dimension of the state space, rendering grid-based dynamic programming approaches impractical for realistic robot control problems. The recently developed prior state-of-the-art neural safety analysis algorithm-Iterative Soft Adversarial Actor-Critic for Safety (ISAACS) [22] uses adversarial RL to approximately solve the Isaacs equation (5). It alternates between updating the controller and disturbance policy parameters based on individual gradients of the players' objectives, which may lead to non-convergent training behaviors or stuck at a poor saddle point. In Section 5, we modify ISAACS with our proposed game-theoretic RL formalism in Section 4 so that it is provably convergent to a minimax equilibrium."}, {"title": "4 Approach: Stackelberg\u2013Minimax Adversarial RL", "content": "Soft Adversarial Actor\u2013Critic as a Three-Player Game. We consider a discrete-time adversarial Markov game governed by system (1). The initial state is determined by a given prior distribution $x_0 \\sim p_0(x)$. At time $t$, the controller and disturbance take actions according to their stochastic policies, i.e., $u_t \\sim \\pi_{\\theta}(\\cdot|x_t)$, $d_t \\sim \\pi_{\\psi}(\\cdot|x_t)$, the controller receives a bounded reward $r_t = r(x_t, u_t, d_t)$ emitted from the environment, and the disturbance receives reward $-r_t$. The single-player version of this Markov game (i.e., with $d_t = 0$) can be solved at scale using deep RL approaches. We will focus on both on- and off-policy actor-critic methods; specifically Advantage Actor-Critic (A2C) [37] and Soft Actor-Critic (SAC) [18], respectively. In the following, we present"}, {"title": "L(\u03c9, \u03b8, \u03c8) = \u0395 [(Vw(x) \u2013 V*(x))2],", "content": "X~X\nor\n$L(w, \\theta, \\psi) = \\mathbb{E}_{\\xi \\sim B} [(Q_{w_1} (x, u, d) \u2013 r \u2013 \\gamma Q_{w_2} (x', u', d'))^2],$ \nfor A2C and SAC, respectively, in which $V$ is the value function, $Q$ is the state-action value function, $w = (w_1, w_2) \\in \\mathbb{R}^{n_w}$ is the critic's parameter, $\\xi = (x, u, d, r, x')$ is the transition data, $B$ is a replay buffer, $\\gamma \\in (0, 1]$ is a discount factor, $u' \\sim \\pi_{\\psi}(\\cdot|x')$, $d' \\sim \\pi_d(\\cdot|x')$, and the function $V^\\pi(x)$ is approximated through a bootstrapped estimator (here we use generalized advantage estimation [42]). For A2C, the controller seeks to maximize the objective\n$J(\\omega, \\theta, \\psi) = \\mathbb{E}_{x \\sim (\\pi_u, \\pi_d)} [r(x_0, u_0, d_0) + V_w(x_1)],$\nwhile for SAC, the controller seeks to maximize entropy-regularized control objective\n$J(\\omega, \\theta, \\psi) =  \\mathbb{E}_{x \\sim B}  [\\min_{i \\in {1, 2}} Q_{w_i} (x, \\tilde{u}, d) - \\eta^u  [\\log \\pi^u (\\cdot|x) - H_u  ] + \\eta^d  [\\log \\pi^d (\\cdot|x) - H_d  ] \\],$ \nwhere $\\tilde{u} \\sim \\pi_{\\theta}(\\cdot|x)$, $\\tilde{d} \\sim \\pi_{\\psi}(\\cdot|x)$, $\\eta^u, \\eta^d > 0$ are the entropy regularization coefficients, and $H_0^u$ and $H_0^d$ are the minimum entropy (heuristically set to the dimension of the player action space, following [19]) of controller and disturbance policies, respectively. Finally, the disturbance minimizes $J(\\omega, \\theta, \\psi)$.\nWe cast MAGICS training procedure defined by (7)-(9) as a three-player general-sum (i.e., non-cooperative) Stackelberg game, which is modeled by the following trilevel optimization problem:\n$\\min_\\omega L(\\omega, \\theta, \\psi)$\ns.t. $\\theta \\in \\arg \\max_{\\tilde{\\theta}} J(\\omega, \\theta, \\psi),$\ns.t. $\\psi \\in \\arg \\min_{\\tilde{\\psi}} J(\\omega, \\theta, \\tilde{\\psi}),$\nwhere the critic is the leader, followed by the controller, and the disturbance plays last, consistent with its information advantage encoded in (5). The most natural way of viewing the three-player game is as follows: in line with [12], the two primary players are the controller $u$ and disturbance $d$, while the critic sits in judgment of their gameplay."}, {"title": "Solving the Zero-Sum Actor Game with 7-GDA.", "content": "In this section, we provide a subroutine that solves the inner zero-sum game between the two actors (10b)-(10c) using T-GDA [12,13] for a fixed critic parameter w. In essence, this approach scales up the learning rate of the follower so that it can adapt faster, which leads to guaranteed local convergence to a minimax solution of the zero-sum game (10b)-(10c). This subroutine is summarized in Algorithm 1."}, {"title": "MAGICS: Minimax Actors Guided by Implicit Critic Stackelberg.", "content": "We now return to the full game (10). Since the controller and the disturbance update their policy parameters using T-GDA, the critic sees them as two simultaneous (i.e., Nash) followers: the controller performs gradient ascent on return $J(\\omega, \\theta, \\psi)$, and the disturbance performs gradient descent on $\\tau J(\\omega, \\theta, \\psi)$. In the following, we derive the update rule for the MAGICS. We start by providing the critic's Stackelberg learning dynamics [11]. Invoking the implicit function theorem, the total derivative of the critic's cost $D_w L(w, \\theta^*, \\psi^*)$ at a minimax equilibrium $(\\theta^*, \\psi^*)$ is:\n$D_w L(\\omega, \\theta^*, \\psi^*) = \\nabla_w L(w) + \\nabla_w \\theta^*(\\omega) \\nabla_{\\theta}L(w, \\theta^*, \\psi^*) + \\nabla_w \\psi^*(\\omega) \\nabla_{\\psi}L(\\omega, \\theta^*, \\psi^*),$\nwhere $\\theta^*(\\omega) = r_{\\theta}(w)$ and $\\psi^*(\\omega) = r_{\\psi}(w)$ are the controller's and disturbance's rational response to the critic, respectively. The implicit differentiation terms $\\nabla_w \\theta^*(w)$ and $\\nabla_w \\psi^* (w)$ can be computed by solving the following linear system of equations:\n$\\begin{cases} 0 = \\nabla_{\\omega \\theta}J(\\omega, \\theta^*, \\psi^*) + \\nabla_{\\theta \\theta}^2 J(w, \\theta^*, \\psi^*) \\nabla_\\omega \\theta^*(\\omega) + \\nabla_{\\theta \\psi}J(\\omega, \\theta^*, \\psi^*) \\nabla_\\omega \\psi^*(\\omega), \\\\ 0 = \\nabla_{\\psi \\omega}J(\\omega, \\theta^*, \\psi^*) + \\nabla_{\\psi \\theta}J(w, \\theta^*, \\psi^*) \\nabla_\\omega \\theta^*(\\omega) + \\nabla_{\\psi \\psi}^2 J(\\omega, \\theta^*, \\psi^*) \\nabla_\\omega \\psi^*(\\omega). \\end{cases}$\nCompactly, the total derivative of the critic can be written as:\n$D_w L(\\omega, \\theta, \\psi) = \\nabla_w L(\\omega, \\theta, \\psi) - h_1(\\omega, \\theta,\\psi)^T H(\\omega, \\theta, \\psi)^{-1}h_2(\\omega, \\theta, \\psi),$\nwhere\n$h_1(\\omega, \\theta, \\psi) = \\begin{bmatrix} \\nabla_{\\omega \\theta}J(\\omega, \\theta,\\psi) \\\\ \\nabla_{\\omega \\psi}J(\\omega, \\theta,\\psi) \\end{bmatrix}, \\quad h_2(w, \\theta,\\psi) = \\begin{bmatrix} \\nabla_{\\theta}L(\\omega, \\theta, \\psi) \\\\ \\nabla_{\\psi}L(\\omega, \\theta, \\psi) \\end{bmatrix},$\nand\n$H (\\omega, \\theta, \\psi) = \\begin{bmatrix} \\nabla_{\\theta \\theta}^2 J(\\omega, \\theta, \\psi) & \\nabla_{\\theta \\psi}J(\\omega, \\theta, \\psi) \\\\ \\nabla_{\\psi \\theta}J(\\omega, \\theta, \\psi) & \\nabla_{\\psi \\psi}^2 J(\\omega, \\theta, \\psi) \\end{bmatrix}.$\nEach term in the Stackelberg gradient update rule can be computed directly and estimated by samples."}, {"title": "Theorem 1.", "content": "Given a Markov game with actor parameters $(\\theta, \\psi)$ and shared critic parameters $w$, if critic has objective function $L(\\omega, \\theta, \\psi)$ defined in (6), then $\\nabla_\\theta L(\\omega, \\theta, \\psi)$ is given by\n$\\nabla_\\theta L(\\omega, \\theta, \\psi) = \\mathbb{E}_{x \\sim (\\theta, \\psi)} [\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi^u (x_t | x) [V_w(x_0) - V^{\\pi^*}(x_t, u_t, d_t)]].$\nMAGICS-SAC, on the other hand, is an off-policy RL scheme, with critic's and actors' objective defined as an expectation over an arbitrary distribution from a replay buffer. An unbiased estimator for each term in the Stackelberg gradient can be computed directly from samples using automatic differentiation, e.g.,\n$\\nabla_w L(\\omega, \\theta, \\psi) = \\frac{1}{N} \\sum_{n=1}^N \\nabla_w ((Q_{w_1} (x_n, u_n, d_n) - r - \\gamma Q_{w_2} (x'_n, u'_n, d'_n))^2).$\nFor the second term in (11), we can estimate each term $h_1(\\cdot)$, $H(\\cdot)$, and $h_2(\\cdot)$ individually using samples from the replay buffer and resetting the simulator [44, Chapter 11].\nIn order to numerically determine convergence, we may check the magnitude of the gradient norms, i.e., $||D_w L(\\cdot)|| \\leq \\epsilon^c$, $||\nabla_\\theta J(\\cdot)|| \\leq \\epsilon^u$, $||\nabla_\\psi J(\\cdot)|| \\leq \\epsilon^d$, where $\\epsilon^c, \\epsilon^u, \\epsilon^d$ are small thresholds. The overall procedure of MAGICS is summarized in Algorithm 2."}, {"title": "Complexity and Gradient Computation Details.", "content": "The critic's Stackelberg gradient requires an inverse-Hessian-vector product (iHvp) and a Jacobian-vector product (jvp). The latter can be computed directly through a call to torch's automatic differentiation engine autograd.grad. The Hessian $H$ in (11) is a 2 \u00d7 2 block matrix composed of differentiating the loss functions of the critic and actor(s) against their respective"}, {"title": "Convergence Analysis of MAGICS.", "content": "Due to sample-based approximation of gradients, the MAGICS procedure can be modeled by the discrete-time dynamical system:\n$w_{t+1} = w_t - \\alpha_t^c (D_w L(w_t, \\theta_t, \\psi_t) + v_{w,t}),$\n$\\theta_{t+1} = \\theta_t + \\alpha_t^u (\\nabla_{\\theta}J(\\omega_t, \\theta_t, \\psi_t) + v_{\\theta,t}),$\n$\\psi_{t+1} = \\psi_t - \\tau \\alpha_t^u (\\nabla_{\\psi}J(\\omega_t, \\theta_t, \\psi_t) + v_{\\psi,t}).$\nIn the next, we show that the MAGICS procedure, modeled by system (14), locally converges to a game-theoretically meaningful equilibrium solution. We start by showing that T-GDA can robustly converge to a minimax equilibrium when the critic parameter $w$ varies within a local region, i.e., $\\omega \\in U_{\\bar{w}} := {\\tilde{\\omega} | ||\\tilde{\\omega} - \\bar{\\omega}|| < \\epsilon_w}$. This is formalized in the following Lemma, which extends the two-player T-GDA local convergence result [13, Theorem 1] to additionally account for a \"meta\" leader (i.e., the critic)."}, {"title": "Assumption 1", "content": "We assume that the following hold.\n(a) The maps $D_wL(\\cdot)$, $\\nabla_{\\theta} J(\\cdot)$, $\\nabla_{\\psi} J(\\cdot)$ are $L_1, L_2, L_3$ Lipschitz, and $||D_w L|| < \\infty$.\n(b) The critic learning rates are square summable but not summable, i.e., $\\sum_k \\alpha_k^c = \\infty$, $\\sum_k (\\alpha_k^c)^2 < \\infty$ for $i \\in {c, u}$.\n(c) The noise processes {$v_{w,t}$}, {$v_{\\theta,t}$}, and {$v_{\\psi,t}$} are zero-mean, martingale difference sequences (c.f. [50, Assumption 1])."}, {"title": "Lemma 1 (Robust Stability of DSE under \u03c4-GDA).", "content": "Consider the zero-sum game between the controller and disturbance $(-J, J)$ parameterized by $w \\in U_{\\bar{w}} = {\\tilde{\\omega} | ||\\tilde{\\omega} -"}, {"title": "Theorem 2", "content": "While Theorem 2 guarantees that MAGICS can converge to a DSE when the network parameters are initialized in a local region around it, we can leverage Lemma 2 to escape a non-DSE critical point.\nConsider the general-sum game $(L, -J, J)$ defined in (10). If parameters $(w^*, \\theta^*, \\psi^*)$ is a critical point of the game but not a DSE, there exists a $\\tau_0 \\in (0,\\infty)$ such that, for all $\\tau \\in (\\tau_0, \\infty)$, $(w^*, \\theta^*, \\psi^*)$ is an unstable equilibrium of the limiting system $(\\dot{\\omega}, \\dot{\\theta}, \\dot{\\psi}) = (-D_w L(w, \\theta, \\psi), \\nabla_{\\theta}J(\\omega, \\theta, \\psi), -\\tau \\nabla_{\\psi}J(\\omega, \\theta,\\psi)).$"}, {"title": "5 Convergent Neural Synthesis of Robot Safety", "content": "In this section, we apply MAGICS to high-dimensional robot safety analysis, and propose the MAGICS-Safety algorithm for convergent neural synthesis of safe robot policies. Following prior work on RL-based approximate reachability analysis [15,22], we consider a time-discounted version of Isaacs equation (5):\n$V^*(x) = \\min \\{ (1 - \\gamma)g(x), \\gamma \\max \\{ l(x), \\max_{u \\in \\mathcal{U}} \\min_{d \\in \\mathcal{D}} V^*(f(x, u, d))\\} \\}.$\nThe discount factor $\\gamma \\in (0, 1]$ leads to a probabilistic interpretation, i.e., there is $(1-\\gamma)$ probability that the episode terminates immediately due to loss of safety. Note that as $\\gamma \\rightarrow 1$, we recover the undiscounted Isaacs equation (5). Then, we apply SAC to approximately solve (15). The critic minimizes loss\n$L^\\gamma(\\omega,\\theta,\\psi) = \\mathbb{E}_{\\xi ~ B} [(Q_{w_1} (x, u, d) - (1 - \\gamma)g' - \\gamma \\min \\{g', Q_{w_2} (x', u', d')\\})^2],$\nwhere $g' := g(x')$, $\\xi = (x, u, d, g', x')$, $u' \\sim \\pi_\\psi(\\cdot|x')$, and $d' \\sim \\pi_d(\\cdot|x')$. The controller maximizes objective\n$J^\\gamma (\\omega, \\theta, \\psi) = \\mathbb{E}_{x ~ B} [Q_{w_1} (x, \\tilde{u}, d) - \\eta^u \\log \\pi^u (\\cdot|x) + \\eta^d \\log \\pi^d(\\cdot|x)],$\nwhere $\\tilde{u} \\sim \\pi_{\\theta}(\\cdot|x)$, $d \\sim \\pi_{\\psi}(\\cdot|x)$. The disturbance minimizes $J^\\gamma(\\omega, \\theta,\\psi)$.\nThe MAGICS-Safety training objectives (16)-(17) differ from [22] in that they explicitly capture the coupling among the critic and actors (hence their non-cooperative interactions). This game-theoretic formulation (c.f. (10)) of adversarial RL facilitates developing convergence guarantees using the MAGICS paradigm in Section 4. In the following theorem, we show that MAGICS-Safety locally converges to a DSE, which is a direct consequence of MAGICS convergence in Theorem 2."}, {"title": "Theorem 3 (Convergence of MAGICS-Safety).", "content": "Consider the general-sum game defined in (10) with game objectives $(L^\\gamma, -J^\\gamma, J^\\gamma)$ defined in (16)-(17). Let Assumption 1 hold. If $(w^*,\\theta^*,\\psi^*)$ is a DSE and $\\alpha^c = o(\\alpha^u)$, there exists a $\\tau_a \\in (0,\\infty)$ and a neighbourhood $U^0 = U^* \\times U^* \\times U^*$ around $(w^*,\\theta^*,\\psi^*)$ such that, for all $(\\omega_0,\\theta_0,\\psi_0) \\in U^0$ and all $\\tau_a \\in (T,\\"}]}