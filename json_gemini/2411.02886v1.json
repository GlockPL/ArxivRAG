[{"title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection", "authors": ["Wei Wu", "Zhuoshi Pan", "Chao Wang", "Liyi Chen", "Yunchu Bai", "Kun Fu", "Zheng Wang", "Hui Xiong"], "abstract": "With the development of large language models (LLMs), the ability to handle longer contexts has become a key capability for Web applications such as cross-document understanding and LLM-powered search systems. However, this progress faces two major challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues hinder the application of LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic, training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using Query-Key dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a small number of critical KV cache tokens in the attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we designed the Selection Cache based on observations of consecutive Query similarity and implemented efficient dot product kernel, significantly reducing the overhead of token selection. A comprehensive evaluation of TokenSelect demonstrates up to 23.84x speedup in attention computation and up to 2.28\u00d7 acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of large language models (LLMs), the number of parameters is no longer the sole factor significantly affecting model performance. The ability to effectively process longer context information has become one of the key metrics for evaluating LLMs' capabilities. The latest Web applications such as cross-document understanding [1], LLM-powered search systems [2], repository-level code completion [3, 4], and complex reasoning [5] have all placed higher demands on the long-context abilities of LLMs. There are two main difficulties in using pre-trained LLMs for long-context inference. On one hand, LLMs are limited by their context length during pre-training (e.g. Llama 3 only has 8192 tokens).\nDirectly inferencing on longer sequences can lead to severe performance degradation due to reasons including sequence lengths out-of-distribution [6, 7]. On the other hand, even if LLMs possess sufficiently large context lengths, the quadratic computational complexity of attention with respect to sequence length makes the response time for long-context inference unbearable.\nPrevious works have made numerous attempts to address these difficulties. To extend the context length of LLMs, the current common practice is to perform post-training on long texts [8\u201310]. However, this approach comes with significant computational costs, particularly in two aspects: the synthesis of high-quality long-text data and the training process on extended sequences. To accelerate long-context inference, many studies focus on the sparsity of attention, attempting to reduce the scale of KV Cache involved in computation. The key to this type of method lies in designing sparse patterns for attention, which can be mainly divided into two categories: one uses predefined sparse patterns [6, 7, 11, 12], while the other estimates the potential importance of KV Cache during the inference process [13\u201317], attempting to select relevant KV Cache tokens into attention calculations. However, the design of these sparse patterns is often heuristically based on historical criticality or coarse-grained criticality estimation of tokens, making it difficult to ensure that the selected tokens are truly critical, thus resulting in sub-optimal performance, as shown in Fig. 1.\nIn this paper, we further observe the non-contiguous sparsity of attention, revealing the importance of designing more fine-grained dynamic sparse patterns. To this end, we propose TokenSelect, a model-agnostic and training-free approach that utilizes token-level selective sparse attention for efficient long-context inference and length extrapolation. Specifically, for each Query, TokenSelect dynamically calculates token-level per-head criticality for the past KV Cache and selects the k most critical tokens through our head soft vote mechanism, involving them in the attention calculation. This reduces the scale of attention calculation to a constant length familiar to the model, while maintaining almost all of the long-context information, thereby simultaneously addressing the two main difficulties for long-context inference. To reduce the overhead of token selection, TokenSelect manages the KV Cache in token-level pages [18] and design efficient kernel for token selection based on Paged KV Cache management through Triton [19]. Furthermore, based on our observation of high similarity between consecutive queries, we have designed the Selection Cache, which allows consecutive similar queries to share token selection results, thereby reducing the selection frequency while ensuring its effectiveness.\nWe evaluate the performance and efficiency of TokenSelect on three representative long-context benchmarks [1, 20, 21] using three open-source LLMs [9, 22, 23]. The experimental results demonstrate that our TokenSelect can achieve up to 23.84\u00d7 speedup in attention computation compared to FlashInfer [24], and up to 2.28\u00d7 acceleration in end-to-end inference latency compared to state-of-the-art long-context inference method [15]. Simultaneously, it provides superior performance on three long-text benchmarks. In summary, we make the following contributions:\n\u2022 An observation on the non-contiguous sparsity of attention that highlights the importance of token-level selection.\n\u2022 TokenSelect, a model-agnostic and training-free method that achieves accurate and efficient long-context inference and length extrapolation, which is compatible with mainstream LLM serving systems and ready for Web applications.\n\u2022 A comprehensive evaluation of TokenSelect, demonstrating up to 23.84x speedup in attention computation and up to 2.28\u00d7 acceleration in end-to-end latency while exhibiting superior performance."}, {"title": "2 Related Works", "content": "Long-context LLMs. Due to computational complexity constraints, current LLMs based on Transformers often utilize limited context lengths during pre-training [9, 10, 22, 23, 25, 26]. To extend the long-context capabilities of LLMs, current methods can be broadly categorized into three approaches [27\u201329]: 1) Modifying positional encodings: A widely adopted method is positional interpolation [30]. Chen et al. first proposed linear scaling of ROPE [31] to map longer positional ranges within the original training window. Subsequent works [32, 33] further improved this method using Neural Tangent Kernel (NTK) theory [34], achieving longer context windows while maintaining model performance. Methods like YaRN [35] and Giraffe [36] optimize interpolation effects by adjusting frequency components or introducing temperature parameters. 2) Long-context post-training: This approach extends the model's context length through additional training steps on longer documents after pre-training [37, 38]. It has been widely adopted by leading LLMs [8\u201310] with the support of sequence parallelism techniques [39\u201341]. 3) Incorporating additional memory modules: Notable examples include Transformer-XL [42], Compressive Transformer [43], RMT [44] and Infini-attention [45]. Although these methods have expanded the context length of LLMs, long-context inference still faces the challenge of high computational costs.\nEfficient Long-context Inference. In state-of-the-art LLMs serving systems [18, 46\u201348], technologies such as Flash Attention [49, 50] and Paged Attention [46] have greatly optimized LLMs inference efficiency by improving GPU I/O bottlenecks. However, in long-context inference scenarios, the quadratic computational complexity of attention with respect to sequence length poses new challenges for LLMs inference. Numerous studies focus on the sparsity of attention, selecting partial KV Cache for attention calculations to improve long-context inference efficiency. Sliding window [11, 12]"}, {"title": "3 Preliminaries", "content": "In this section, we first introduce the inference process of LLMs, and then define the Selective Sparse Attention Problem."}, {"title": "3.1 LLMs Inference", "content": "Nowadays, mainstream LLMs are primarily based on the Decoder-only Transformer architecture, consisting sequentially of a word embedding layer, a series of transformer layers, and a token prediction head. Each transformer layer includes a multi-head attention (MHA) module and a feed-forward networks (FFN) module. The inference process of LLMs can be divided into two stages: the Prefill Stage and the Decode Stage.\nThe Prefill Stage is the preparatory phase of the inference process. In this stage, the user's input is processed layer by layer through a single forward pass of LLMs, generating KV Cache for each layer. The generation of KV Cache is completed by the MHA module. Assuming \\(X_{prefill} \\in \\mathbb{R}^{n_{in} \\times d}\\) is the input of a transformer layer, where \\(n_{in}\\) is the number of tokens in user's input sequence and d is the hidden size. The computation of MHA in the Prefill Stage is as follows (simplified to single-head mode):\n\\[[Q_{prefill}, K_{prefill}, V_{prefill}] = X_{prefill} \\cdot [W_q, W_k,W_v], \\tag{1}\\]\\[O_{prefill} = softmax(\\frac{Q_{prefill} K_{prefill}^T}{\\sqrt{d_h}})V_{prefill}, \\tag{2}\\]where \\(W_q, W_k, W_v\\) are linear projections, \\[.\\] represents tensor concatenation operation, and Eq.(2) is also known as Scaled Dot-Product Attention (SDPA). After these computation, \\(K_{prefill}\\) and \\(V_{prefill}\\) are stored as the KV Cache for current layer \\(K_{cache}\\) and \\(V_{cache}\\), and \\(O_{prefill}\\) is used for subsequent calculations.\nThe Decode Stage is the phase where LLMs actually generate the response. In the Decode Stage, LLMs load the KV Cache and generate \\(n_{out}\\) output tokens autoregressively through \\(n_{out}\\) forward passes. Assuming \\(X_{decode} \\in \\mathbb{R}^{1 \\times d}\\) is the input of a transformer layer in a forward pass, the computation of MHA in the Decode Stage is as follows (The calculation of \\(Q_{prefill}\\) and \\(O_{prefill}\\) is consistent with that in the Prefill Stage):\n\\[K_{decode} = [K_{cache}, X_{decode} \\cdot W_k], K_{cache} \\leftarrow K_{decode},\\tag{3}\\]\\[V_{decode} = [V_{cache}, X_{decode} \\cdot W_v], V_{cache} \\leftarrow V_{decode},\\\\]where \\(K_{decode}, V_{decode}\\) are composed of the KV Cache and the KV corresponding to the current input, which are then used to update the KV Cache of the current layer for use in the next forward pass.\nLLMs inference, unlike training, is memory-bound, necessitating frequent GPU I/O operations between HBM and SRAM while underutilizing processing units. This bottleneck is particularly evident in SDPA computation. Optimizing for I/O is crucial for enhancing LLMs inference efficiency, especially in long-context scenarios."}, {"title": "3.2 Selective Sparse Attention", "content": "As discussed in the Sec. 1, the high attention sparsity in LLMs suggests sparse attention as a promising solution for long-context inference challenges. Sparse attention can keep the number of tokens participating in attention computations at a constant scale, rather than increasing with sequence length. Given that predefined sparse patterns are detrimental to performance, we aim to dynamically select crucial tokens for attention computation at each step during the inference process. Therefore, we formalize this problem according to the following definition.\nDefinition 1 (Selective Sparse Attention Problem, informal). For current input of length C (C = 1 in the Decode Stage) and KV Cache of length N, assuming there are H attention heads with a head size of \\(d_h\\), let O be the output of the SDPA:\n\\[O = softmax(\\frac{Q^h \\cdot [K^h_{cache}, K^h_{current}]}{\\sqrt{d_h}})[V^h_{cache}, V^h_{current}] \\quad h\\in \\{1,...,H\\}\\tag{4}\\]where \\(Q^h, K^h, V^h \\in \\mathbb{R}^{C \\times d_h}\\) is the Query, Key, Value matrices of current input for head h and \\(K^h_{cache}, V^h_{cache} \\in \\mathbb{R}^{N\\times d_h}\\) represent the KV Cache. Let \\(\\hat{O}\\) be the output of the Selective Sparse Attention:\n\\[\\hat{O} = softmax(\\frac{Q^h \\cdot [K^h_{select}, K^h_{current}]}{\\sqrt{d_h}})[V^h_{select}, V^h_{current}] \\quad h\\in \\{1,...,H\\}\\tag{5}\\]where \\(K^h_{select}, V^h_{select} \\in \\mathbb{R}^{k\\times d_h}\\) are k selected KV Cache (k \\(\\ll\\) N). The selection of \\(K_{select}, V_{select}\\) is performed by selection function S:\n\\[S(Q, K_{cache}) = I, where I \\in \\mathcal{P}(\\{1,\\cdots, N\\}),\\tag{6}\\]\\[K_{select} = [(K_{cache})_i]_{i\\in I}, V_{select} = [(V_{cache})_i]_{i\\in I},\\\\]where I is the set of selected indices. The objective is to find an appropriate selection function S that minimizes the difference between the outputs of the SDPA and the selective sparse attention:\n\\[min_S \\|O - \\hat{O}\\|_2.\\tag{7}\\]"}, {"title": "4 Motivations and Observations", "content": "Attention is Sparse, Non-contiguous and Head-Distinctive. Previous works [6, 7, 11\u201317, 53] have demonstrated the sparsity of attention scores in LLMs, particularly when processing long texts. Recent approaches [15\u201317] partition the KV Cache into non-overlapping blocks, estimating block criticality for sparse attention calculations. These methods assume that tokens with higher attention scores tend to be contiguous. However, our further observations reveal that this assumption does not always hold true in practice. As illustrated in Fig. 2a, attention scores are sparsely distributed at the token-level, with critical tokens not necessarily contiguous. This non-contiguity leads to significant omissions in block-level token selection. Fig. 2b demonstrates that finer selection granularity improves recall of critical tokens, motivating us to perform token-level selection. For token-level selection, an intuitive approach would be to directly select the top-k tokens with the highest attention logits. However, observation in Fig. 2c reveals considerable disparity in the \\(L_1\\) norm of attention logits across attention heads. As a result, the selection result tends to be dominated by a few heads with disproportionately large attention logits, driving us to design a more robust selection function that maintains the independence of heads.\nConsecutive Queries are similar. As sparsity of attention is dynamic [13, 15\u201317], token selection should be performed for every Query, which inevitably increases the computational overhead of selective sparse attention. Fortunately, we observe that consecutive Queries exhibit high similarity, as shown in Fig. 3a. Intuitively, when two consecutive Queries are highly similar, their dot products with the Keys will also be similar, leading to substantial overlap in the token selection results. Due to space constraints, we provide an informal lemma about this below. The formal version and corresponding proof can be found in the Appendix A.\nLemma 1 (Informal). Consider Queries \\(Q_1, Q_2 \\in \\mathbb{R}^{1 \\times d}\\) that are consecutive and a Key set \\(\\{K_i\\}_{i=1}^N\\). Let \\(I_1\\) and \\(I_2\\) be the sets of indices of the top-k Keys selected by dot product for \\(Q_1\\) and \\(Q_2\\) respectively. If \\(cos (Q_1, Q_2) > \\epsilon\\), where e is a threshold, then \\(I_1 \\approx I_2\\).\nFig. 3b illustrates this lemma experimentally. It can be seen that the overlap rate of token selection tends to increase with query similarity. This key insight motivates us to reuse selection results for similar queries, improving computational efficiency. Moreover, the similarity distribution of consecutive Queries remains consistent"}, {"title": "5 Designs of TokenSelect", "content": "In this section, we will introduce the design details of TokenSelect, primarily encompassing the Selection Function, the Selection Cache, and efficient implementation of TokenSelect. The overall workflow of TokenSelect is illustrated in Fig. 4."}, {"title": "5.1 Selection Function", "content": "The simplest selection function is to determine the criticality of the tokens through the dot product of Q and \\(K_{cache}\\), then select the top-k critical ones as \\(K_{select}, V_{select}\\). The selected indices I are calculated as follow:\n\\[I_{topk} = TopK(Q \\cdot K_{cache}^T).\\tag{8}\\]However, as discussed in Sec. 4, this approach is prone to inaccuracies due to disparities in norm of attention logits between heads. To maintain independence between heads, a better approach is to have each head select the top-k most critical tokens, and then determine the final selection through voting among the heads:\n\\[I_{head\\text{-}vote} = TopK(\\sum_{h=1}^H\\mathbb{I}(i\\in TopK(Q^h \\cdot (K^h_{cache})^T))).\\tag{9}\\]where \\(\\mathbb{I}\\) is the indicator function. Unfortunately, despite better performance, this method relies on scatter_add and multiple topk operations, resulting in low efficiency on GPUs. Additionally, the 0/1 voting ignores the relative importance of tokens for each head.\nTherefore, we propose a head soft vote approach that offers better performance and efficiency. Specifically, we first calculate the per-head criticality, then normalize through softmax, and sum the results for all heads. The formalization is as follows:\n\\[I_{head\\text{-}soft\\text{-}vote} = TopK(\\sum_{h=1}^H softmax(Q^h \\cdot (K^h_{cache})^T)).\\tag{10}\\]"}, {"title": "5.2 Optimizing Selection Frequency", "content": "Although the aforementioned selection function can reduce the complexity of attention from O(N2) to O(k2) (k \\(\\ll\\) N), while maintaining performance, the execution time of the selection function itself still affects the latency of LLMs inference. To further accelerate long-context inference, based on our observations of the similarity of consecutive queries, we design optimization strategies for both the Prefill Stage and the Decode Stage to reduce the selection frequency while ensuring its effectiveness.\nIn the Prefill Stage, \\(Q_{prefill} \\in \\mathbb{R}^{n_{in}\\times d}\\) is inputed. In long-context scenarios, the number of tokens in the user's input sequence \\(n_{in}\\) may reach up to 1M, making it impractical to perform selection for each Query token. Considering the similarity of consecutive Queries, we use chunk-wise token selection, inputting \\(\\sum_{i=1}^c (Q_C)_i\\) into the selection function, where \\(Q_C \\in \\mathbb{R}^{c\\times d}\\) is the Query chunk and c is the chunk size. This method helps maintain the compute-intensive nature of the Prefill Stage, preventing it from becoming memory bound.\nIn the Decode Stage, due to the auto-regressive characteristic of LLMs, we need to frequently perform selection for \\(Q_{decode} \\in \\mathbb{R}^{1\\times d}\\),"}, {"title": "5.3 Efficient Implementation", "content": "To ensure that our proposed TokenSelect can be used for real-world Web applications, efficient implementation is crucial. We first analyze the computation time breakdown of representative block-level selective sparse attention method, InfLLM [15]. From (1)(2)(3) in Fig. 5, we can observe that although selective sparse attention can significantly reduce the complexity of attention calculations, the actual computation time is still highly dependent on the implementation. The incompatibility with efficient attention implementations such as Flash Attention has resulted in methods requiring historical attention scores [13\u201315, 53] being difficult to be applied in real-world Web applications. Through the analysis of InfLLM's Flash Attention-compatible version, we make several discoveries. The initial motivation for estimating token criticality at the block-level is to reduce the overhead of selection function (mainly considering dot product calculation). However, we find that dot product is not the primary performance bottleneck. Instead, a significant portion of the overhead comes from indexing the KV Cache using selected indices and making them contiguous in GPU memory, which frequently occurs during the updating of KV blocks and the concatenation of selected KV Cache. The extensive I/O required for this operation further exacerbates the memory-bound in LLMs inference. Based on this, we propose that Paged Attention is a more suitable implementation for selective sparse attention. Using Paged KV Cache management (with page size=1 for TokenSelect), we can reduce the I/O volume for selection results from the scale of all selected KV Caches O(2kd) to the scale of their indices O(k). However, by observing (4) in Fig. 5, we find that we encounter another bottleneck under Paged KV Cache management. Since logically contiguous KV Cache is not entirely contiguous in GPU memory, it also needs to be made contiguous before performing computational operations. To address this issue, we draw inspiration from the concept of Paged Attention and implement a Paged Dot Product"}, {"title": "6 Experiments", "content": "In this section, we first introduce the experimental setup of this paper, and then reveal the performance and efficiency of our TokenSelect in long-context inference through experiments."}, {"title": "6.1 Experimental Settings", "content": "Datasets. To evaluate TokenSelect's performance on long-context inference, we use the following datasets: (1) InfiniteBench [20]: The mainstream long-context benchmark consisting of multi-tasks. The average length of it exceeds 200K tokens. (2) RULER [21]: A challenging long-context benchmark containing 13 different tasks, with subsets of varying lengths up to 128K tokens. (3) LongBench [1]: Another mainstream long-context benchmark comprising 6 types of tasks. The 95% percentile for its lengths is 31K tokens. For each dataset, we use its recommended metrics, which are presented in the Appendix B.\nBaselines. To demonstrate the state-of-the-art (SOTA) performance of TokenSelect, we include the following methods for comparison: (1) Original models: We select three mainstream open-source LLMs - Qwen2-7B-Instruct [9], Llama-3-8B-Instruct [22], and Yi-1.5-6B-Chat [23] - utilizing their original context lengths without any modifications. (2) NTK-Aware Scaled ROPE: A nonlinear ROPE interpolation method. (3) SelfExtend: A ROPE interpolation method that reuses the position ids across neighboring tokens. (4) StreamingLLM: The SOTA method for long-context inference with predefined sparse patterns. (5) InfLLM: The SOTA method for long-context inference and length extrapolation using a block-level selective sparse attention method. (6) MInference: The SOTA method for long-context prefilling acceleration, utilizing three sparse patterns including block-level sparse attention. It's worth noting that since MInference doesn't support length extrapolation, we use an alternative evaluation method, applying it to Llama-3-8B-Instruct-262k (Llama3 after long-text post-training). Additionally, we do not include another SOTA method, QUEST [16], as it does not support Grouped Query Attention (GQA).\nImplementation details. In all experiments in this paper, we employ greedy decoding to ensure the reliability of the results. For our TokenSelect, we implement it on SGLang [18], which is a fast serving framework based on Flashinfer [24]. We implement our method using PyTorch [65] and Triton [19]. We follow the baseline"}, {"title": "6.2 Performance Comparisons", "content": "InfiniteBench. As shown in Table 1, our TokenSelect achieves significantly superior overall performance on InfiniteBench compared to all baseline methods, even though TokenSelect uses the smallest token budget (\\(<3K\\)). The fact that it significantly outperforms the original models demonstrates TokenSelect's strong length extrapolation capability. We analyze that this is due to our adoption of a fine-grained KV Cache selection strategy, while considering the equal contribution of each head to selection, which ensures that we can select most critical tokens. Observing the performance of other methods, we find that ROPE interpolation methods (NTK, Self-Extend) generally perform poorly unless used on specially trained models such as Qwen2-7B-Instruct. The better performance of Qwen2-7B-Instruct on the original model can also be attributed to this. The sparse attention method StreamingLLM, based on fixed sparse patterns, can guarantee some of the model's capabilities, but due to discarding a large amount of long-context information, it performs poorly on retrieval-related tasks (R.PK, R.Num, R.KV). The block-level selection method InfLLM can retain more long-context information compared to StreamingLLM. However, due to its sub-optimal block-level selection, it results in lower performance on most tasks compared to TokenSelect, even though we set a larger token budget for InfLLM. It is worth noting that Yi-1.5-6B does not perform normally on the R.KV task, as it is unable to correctly recite strings like the Universally Unique Identifier.\nRULER. To further demonstrate the long-context capability of TokenSelect, we conduct evaluation on the more challenging long-context benchmark RULER. Considering the increased difficulty of RULER and its substantial computational requirements, we include only comparable baseline methods. As shown in Table 2, our TokenSelect maintains significantly superior overall performance compared to other long-context inference methods. For all models, TokenSelect achieves length extrapolation while preserving the model's original capabilities, benefiting from our efficient utilization of the model's limited context length. Notably, due to the"}, {"title": "6.3 Ablation Studies", "content": "In ablation studies, we primarily analyze the impact of different Selection Functions S on performance. To compare the performance of different Selection Functions S under low token budgets (i.e., token efficiency), we maintain the 2K+512 configuration. From Table 4, we can observe that our proposed head soft vote mechanism performs significantly better across all tasks. This indicates that"}, {"title": "6.4 Hyper-parameter Analysis", "content": "Number of selected tokens k. As shown in Table 5, we fixed local to a relatively small value (512) to compare the performance when selecting different numbers of tokens. First, we observe that even selecting a very small number of tokens (e.g., 128, 256), our TokenSelect still demonstrates very comparable performance. Then, as k increases, the effectiveness of TokenSelect further improves, indicating that more moderately critical tokens also contribute to the retention of long-context information. Finally, we find that when k is set to larger values (e.g., 16K), our TokenSelect shows significant improvements in most tasks, further advancing the performance landscape of long-context inference methods.\nSimilarity threshold of the Selection Cache \u03b8. Fig. 6 shows that the Selection Cache hit rate increases significantly as the similarity threshold \u03b8 decreases, converging around \u03b8 = 0.5. This suggests potential for further acceleration of TokenSelect's Decode Stage by reducing \u03b8. Performance sensitivity to \u03b8 varies across tasks. While most tasks exhibit slight performance degradation with decreasing \u03b8, and R.PK in InfiniteBench shows no degradation, more challenging retrieval tasks like R.KV demonstrate significant performance deterioration. This indicates higher dynamicity requirements for token selection in these tasks."}, {"title": "6.5 Efficiency Comparisons", "content": "Efficiency of selective sparse attention. Fig. 7 demonstrates the significant acceleration of attention computation achieved by TokenSelect during long-context inference. With a KV Cache length of 1M, TokenSelect can provide up to 23.84\u00d7 speedup compared to FlashInfer, which is the inference kernel library we based on. This substantial improvement is attributed to our efficient kernel design.\nEnd-to-end efficiency. Fig. 8 compares the end-to-end latency of TokenSelect, InfLLM, and standard attention across various tasks. TokenSelect significantly accelerates long-context inference in real-world scenarios, achieving a maximum speedup of 4.70\u00d7 over standard attention and 2.28\u00d7 over the SOTA long-context inference method. Moreover, TokenSelect demonstrates superior performance compared to both of them."}, {"title": "6.6 Scaling Beyond 1 Million Context Length", "content": "To further explore TokenSelect's performance in extreme long-context scenarios, we design an extended benchmark with different text lengths following InfiniteBench. As illustrated in the Fig. 9, our TokenSelect demonstrates the ability to accurately capture critical information with a small token budget in contexts up to 2M tokens, underscoring its potential in more application scenarios."}, {"title": "7 Conclusion", "content": "In this paper, we introduces TokenSelect, a model-agnostic and training-free approach for efficient long-context inference and length extrapolation. TokenSelect addresses the two major challenges faced by LLMs in processing long texts: the context length limitation from pre-training and the computational complexity of attention. This is achieved through a novel token-level selective sparse attention mechanism. Experimental results demonstrate that TokenSelect can achieve up to 23.84\u00d7 speedup in attention computation and up to 2.28\u00d7 acceleration in end-to-end inference latency, while exhibiting superior performance across multiple long-context benchmarks. This approach significantly enhances LLMs' capability to handle long contexts, paving the way for efficient long-text processing in advancing Web applications."}, {"title": "A Formal Statement and Proof of Lemma 1", "content": "Lemma 1 (Invariant Top-k Key Selection under Cosine Similarity Threshold", "Formal).\nAssumptions": "n(1) Let \\(q_1", "as": "n\\[cos(a", "frac{ab}{\\|a\\|_2\\|b\\|_2},\\\\": "where \\(\\|\\|_2\\) denotes the Euclidean norm.\n(5) Define the top-k selection function based on dot product similarity as: \\(I(q) = arg \\underset{S \\subseteq \\{1", "1": "be a predefined threshold.\nLemma Statement: If the cosine similarity between the two query vectors \\(q_1\\) and \\(q_2\\) satisfies\n\\[cos(q_1", "epsilon,\\\\": "then the indices of the top-k keys selected by \\(q_1\\) and \\(q_2\\) are identical", "I(q_2).\\\\": "Proof: We start with the given condition:\n\\[\\underset{1 \\leq i \\leq k"}, {"hat{\\eta},\\\\": "which we aim to use to demonstrate that:\n\\[\\underset{1 \\leq i \\leq k"}, {"0.\\\\": "To facilitate our analysis", "notations": "n\\[\\hat{\\eta"}, "frac{\\eta}{\\|q_1\\|},\\quad \\hat{q}_1 = \\frac{q_1}{\\|q_1\\|},\\quad \\hat{q}_2 = \\frac{q_2}{\\|q_2\\|}.\\"], "becomes": "n\\[\\underset{1 \\leq i \\leq k"}, {"showing": "n\\[\\underset{1 \\leq i \\leq k"}, {"define": "n\\[\\hat{p"}, {"have": "n\\[\\underset{1 \\leq i \\leq k}{min} \\hat{q}_2k_i = \\underset{1 \\leq i \\leq k}{min} (\\hat{q}_1 cos \\theta + \\hat{p}_1 sin \\theta)k_i,\\\\]\\[> \\underset{1 \\leq i \\leq k}{min} \\hat{q}_1k_i cos \\theta - \\|k\\| \\underset{i}{\\text{max}} sin \\theta,\\\\]\\[\\geq \\hat{q}_{1k} cos \\theta - \\|k\\| \\underset{i}{\\text{max}} sin \\theta,\\]and\n\\[\\underset{j>k}{max} \\hat{q}_2k_j = \\underset{j>k}{max} (\\hat{q}_1 cos \\theta + \\hat{p}_1 sin \\theta)k_j\\\\]\\[\\leq \\underset{j>k}{max} \\hat{q}_1k_i cos \\theta + \\underset{j>k}{max} \\hat{p}_1k_i sin \\theta,\\\\]\\[\\leq \\hat{q}_{1k_{p+1}} cos \\theta + \\|k\\| \\underset{i}{\\text{max}} sin \\theta.\\]Therefore,\n\\[\\underset{1 \\leq i \\leq k}{min} \\hat{q}_2k_i - \\underset{j>k}{max} \\hat{q}_2k_j \\geq \\hat{q}_{1k"}]