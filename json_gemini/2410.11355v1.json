{"title": "Reducing Labeling Costs in Sentiment Analysis via Semi-Supervised Learning", "authors": ["MINOO JAFARLOU", "MARIO M. KUBEK"], "abstract": "Labeling datasets is a noteworthy challenge in machine learning, both in terms of cost and time. This research, however, leverages an efficient answer. By exploring label propagation in semi-supervised learning, we can significantly reduce the number of labels required compared to traditional methods. We employ a transductive label propagation method based on the manifold assumption for text classification. Our approach utilizes a graph-based method to generate pseudo-labels for unlabeled data for text classification task, which are then used to train deep neural networks. By extending labels based on cosine proximity within a nearest neighbor graph from network embeddings, we combine unlabeled data into supervised learning, thereby reducing labeling costs. Based on previous successes in other domains, this study builds and evaluates this approach's effectiveness in sentiment analysis, presenting insights into semi-supervised learning.", "sections": [{"title": "1 MOTIVATION", "content": "Obtaining labeled data for real-world applications is costly and time-consuming due to the need for human labor and specialized knowledge. In contrast, unlabeled data is abundant and inexpensive. This research uses label propagation, a graph-based semi-supervised learning method, to enhance performance in sentiment analysis tasks using deep learning techniques. Label propagation generates pseudo-labels for unlabeled data based on the proximity of data points within a graph, enabling cost-effective learning by leveraging data closeness and reducing the dependence on extensive labeled datasets [5].\nThe study focuses on achieving high performance with limited labeled data, a crucial aspect for industries where data labeling is scarce yet accuracy is critical. By experimenting with various label propagation parameters and techniques, such as similarity metrics, graph construction, and propagation strategies, we aim to tailor performance enhancements for specific natural language processing (NLP) tasks. We also emphasize the generalizability of our approach across"}, {"title": "2 LITERATURE REVIEW", "content": "Semi-supervised learning (SSL) is an intermediate between supervised and unsupervised learning, utilizing a small amount of labeled data alongside a large amount of unlabeled data to improve learning performance. Standard SSL methods include self-training, co-training, multi-view learning, and Transductive Support Vector Machines (TSVMs). These methods exploit the data's inherent structure to propagate labels from labeled to unlabeled instances, improving model performance with minimal labeled data [14].\nGraph-based methods in SSL leverage the relationships between data points, defining them as nodes in a graph. The edges between nodes are weighted based on similarity measures, such as word order, context similarity, and word frequency [19]. Graph Neural Networks (GNNs) have been remarkably effective in this domain, performing state-of-the-art results for node classification. InfoGNN, a novel informative pseudo-labeling framework, maximizes mutual information to label the most informative nodes, improving performance with few labels [9]. Another approach, as studied in [5], utilizes a transductive label propagation method based on the manifold assumption. This method iterates between creating a nearest neighbor graph of the dataset and generating pseudo-labels based on the network embeddings, improving performance on several datasets, particularly in the few labels scenarios.\nPseudo-labeling includes using a model's predictions on unlabeled data as true labels. This technique is specifically effective in SSL, where pseudo-labels can guide the training of deep neural networks [8]. The method encourages low-density detachment between classes, a principle known as entropy regularization. Recent advancements, such as PARS (Pseudo-Label Aware Robust Sample Selection), combine pseudo-labeling with sample selection and noise-robust loss functions to handle noisy labels and improve model accuracy significantly [3]. [16] presents Temporal Ensembling and Mean Teacher as advanced techniques in semi-supervised learning. Temporal Ensembling averages label predictions, while Mean Teacher averages model weights, improving accuracy and reducing the need for labeled data. Mean Teacher offers superior performance on benchmarks, demonstrating the importance of great network architecture in SSL. Another recent work, Virtual Adversarial Training (VAT), proposes a new regularization method based on virtual adversarial loss, a measure of local smoothness of the conditional label distribution given input. VAT represents the"}, {"title": "3 FUNDAMENTALS", "content": null}, {"title": "3.1 Label Propagation with Deep Learning", "content": "Label propagation is a graph-based semi-supervised learning method. In this approach, all data points, whether labeled or unlabeled, are represented as vertices on a graph within a d-dimensional feature space. Label propagation considers labeled data as 'sources. It assigns pseudo-labels to the unlabeled data based on the cluster assumption, which suggests that vertices close to each other on the graph should share similar labels. The' unlabeled' data becomes useful for further supervised learning by assigning these pseudo-labels to them. This method of assigning pseudo-labels to unlabeled data based on their proximity on the graph facilitates more cost-effective learning [5].\nThe label propagation process, illustrated in (Algorithm 1), involves placing assumed labels $y_i$ to data points $x_i$ within the unlabeled dataset $U$ through the process. This labeling is achieved using the label propagation function $LP$ to the hidden representations $V$, generating $y_i = LP(x_i, V)$. In this procedure, $LP$ utilizes the cluster assumption, considering the closeness of vertices in a graph constructed by data points within a d-dimensional feature space."}, {"title": "3.2 Dataset", "content": "The dataset utilized in this study is the \"Large Movie Review Dataset\" (IMDb). It is a leading resource for sentiment analysis and natural language processing tasks, consisting of 50 movie reviews. These reviews are separated into a training set of 25k reviews and a testing set of 25k reviews, balanced between positive and negative sentiments.\nClass imbalance, a prevailing issue in text classification tasks, can render label propagation sensitive to imbalances in the labeled data. This sensitivity leads to biased predictions, particularly when certain classes are underrepresented. The balanced nature of the IMDb dataset serves as a safeguard against such issues, guaranteeing an equitable representation of both classes."}, {"title": "3.3 Data Preprocessing and Cleaning", "content": "For dataset preparation, we present randomness by shuffling the dataset to provide a diverse distribution of data points. The dataset is then separated into training and validation sets with an 80:20 ratio and categorized into labeled and unlabeled subsets to support semi-supervised learning.\nDuring data cleaning, each sentence is converted to lowercase, leading or following spaces are removed, punctuation is separated with spaces, non-letter/punctuation characters are removed, and excessive spaces are cut.\nText data preparation involves three steps: tokenization customized to NLP models, vocabulary building limited to top and unique tokens, and data transformation converting tokenized text into numerical indices aligned with labels for efficient processing.\nTo enhance model performance, we incorporate pre-trained word embeddings. Word vectors are loaded from a specified file, an embedding matrix is created, and tokens are mapped to token IDs with pre-trained vectors or default vectors for unknown tokens. A vocabulary of the top 10K most frequent tokens is constructed to balance model complexity with linguistic relevance in most configurations."}, {"title": "3.4 Implementation", "content": "As discussed in the previous section, label propagation approach leverages both the geometrical distribution of the data and the available labels to extend labeling across a dataset. The process starts by calculating the k-nearest neighbors for each data point in the feature space. This involves normalizing the data features and using these to find the closest neighbors, which helps in understanding the local structure of the data. With the neighbors identified, the next step constructs a graph where nodes represent data points and edges are weighted by the distance between these points raised to a power determined by the gamma parameter. This weighted graph is then normalized to balance the influence each node has based on its connectivity. Using the graph, the algorithm propagates labels from the labeled data points to the unlabeled ones. It does this by solving a series of linear equations that diffuse the labels through the graph, smoothing out the label assignments based on the structure captured in the graph. After propagating the labels, each data point is assigned a pseudo label based on the highest probability from the label distribution. The reliability of these pseudo labels is further weighted by their respective entropy, providing a measure of confidence in each label. Finally, to address any class imbalance that might affect learning, the algorithm calculates weights for each class. This is done to ensure that each class is equally represented during any subsequent process."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "This section outlines the results of our investigations across different processes. We begin by evaluating the impact of various word embeddings on model performance, including Word2Vec, GloVe, and FastText. The analysis highlights the impact of each embedding on key performance metrics. We then examine the outcomes of tokenization variability, specifically focusing on how different token counts affect these metrics. Additionally, we compare the performance of deep learning models-BiGRU, BILSTM, and 1D CNN. Finally, our exploration of deep learning hyperparameter optimization for the BiGRU model details the effects of settings such as the number of layers, hidden dimensions, and label counts on the accuracy, F1 score, and AUC-ROC.\nThe outcomes are depicted through comparative bar charts for three baseline and label propagation metrics, with the fully supervised model serving as the upper benchmark. This analysis demonstrates how different embeddings, tokenization strategies, model selections, and hyperparameters influence the resulting performance metrics."}, {"title": "4.1 Results of Experimenting with Different Word Embeddings", "content": "Using pre-trained embeddings Word2Vec [11], FastText[1], and GloVe[13] helps to initialize the baseline model with rich semantic information learned from a large corpus of text. This initialization improves the model's performance, especially when dealing with a small amount of labeled data. In the label propagation stage, the model continues from the learned state achieved in the baseline stage, allowing it to benefit from the initially learned embeddings while focusing on improving the labels and model parameters through the label propagation process.\nThe experiments use 10 percent of the dataset configured with a BiGRU featuring 64 hidden dimensions and 2 layers, demonstrating the model's adaptability. Label propagation is implemented with 100 nearest neighbors."}, {"title": "4.2 Results of Tokenization Variability on Model Performance", "content": "These experiments, designed to compare the performance with different numbers of tokens (Fig. 2), have shown that 20k tokens consistently outperform 10k tokens across all three metrics (Accuracy, F1 Score, and AUC-ROC) for both the baseline and label propagation models. It is worth mentioning that 12846 out of 20002 tokens are matched with pre-trained fasttext word vectors."}, {"title": "4.3 Comparative Results of Deep Learning Models", "content": "In this experiment, we employed three distinct models: BiGRU (Bidirectional Gated Recurrent Unit), BiLSTM (Bidirectional Long Short-Term Memory), and 1D CNNs (One-Dimensional Convolutional Neural Networks) to investigate label propagation across all models. (Fig. 3) illustrates model comparisons of BiGRU, 1D CNN, and BiLSTM on 10 percent of data.\nIn the baseline model (Fig. 3), BiLSTM generally performs better regarding F1 Score and AUC-ROC, while BiGRU has a slight advantage in Accuracy. BiLSTM performs better in Accuracy in label propagation, while BiGRU has a slight"}, {"title": "4.4 Hyperparameter Optimization Results for BIGRU Model", "content": "This experiment considers the performance of baseline, fully supervised, and label propagation models-across different configurations on a BiGRU model. We examine these methods by varying the neural network layers, hidden dimensions, and data proportions used for label propagation. Although we considered all deep learning metrics, our analysis concentrates on key performance indicators: accuracy, F1 score, and AUC-ROC. The study seeks to determine the effectiveness and efficiency of each variable, delivering insights into their strengths and weaknesses under different experimental setups, mainly focusing on the impact of the number of labels in the label propagation method.\nIn every tested configuration (Fig. 5), label propagation exceeded the performance of the baseline models, which served as the lower benchmark. Label propagation consistently outperforms the baseline in configurations, showing improvements in F1 Score and AUC-ROC. Its performance is competitive with fully supervised models, which are the upper bound in this comparison. The performance of label propagation models lies between baseline and the fully-supervised models, indicating a robust progression in model performance. The study's success relies on the label propagation-based semi-supervised model, indicating enhancements over the baseline and gaining competitive performance close to the fully supervised benchmark. Expanding the quantity of data (from 10 percent to 20 percent and"}, {"title": "4.5 Discussion and Limitations", "content": "Label propagation encounters several challenges in text classification tasks that require attention. A primary issue is sensitivity to noise, as text data often possesses inaccuracies from misspellings, abbreviations, or ambiguous language. This issue is especially acute with user-generated or unstructured text.\nAdditionally, label propagation needs more discriminative capability when applied to text data with complex, non-linear relationships. This problem is also evident in tasks involving complicated language, where defining precise decision boundaries for accurate classification is challenging.\nAnother noteworthy problem for label propagation in text data is the reliance on an efficacious graph structure. Creating a graph representing relationships between text instances affects the propagation process's quality. Likewise, the computational complexity of label propagation poses challenges, especially with the large datasets typical in NLP tasks. Forming and updating graphs can be computationally intensive for large text corpora.\nHyperparameter tuning is vital in label propagation for text classification tasks. It involves optimizing parameters specific to constructing text-based graphs and the propagation processes. The choice of hyperparameters, such as the similarity measure and the number of neighbors in the graph, dramatically influences the model's performance, making the tuning process complex yet required for achieving optimal results in sentiment analysis. Determining the optimal number of k nearest neighbors is essential for achieving accurate results, which can be done similarly to the approach employed in [6]. Indeed, too few neighbors may introduce noise through inaccurate labels, while too many could result"}, {"title": "5 CONCLUSION", "content": "In conclusion, using unlabeled data alongside label propagation techniques has significantly enhanced semantic analysis performance, specifically when the number of labeled data was few. This research comprehensively evaluates label propagation's role, effectiveness, and practicality in enhancing sentiment analysis text classification tasks. The performance evaluation compares the model's performance against baseline and fully supervised models. The project's success is evident in the semi-supervised model based on label propagation, which improved over the baseline. Importantly, our ongoing project is committed to refining label propagation algorithms by integrating extra data, optimizing graph construction for better performance, incorporating domain adaptation strategies, and implementing regularization techniques to enhance robustness. In future work, we plan to combine label propagation with active learning strategies to identify and label the most insightful unlabeled data points to maximize performance yields while minimizing labeling endeavors."}]}