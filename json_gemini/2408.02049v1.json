{"title": "3D Single-object Tracking in Point Clouds with\nHigh Temporal Variation", "authors": ["Qiao Wu", "Kun Sun", "Pei An", "Mathieu Salzmann", "Yanning Zhang", "Jiaqi Yang"], "abstract": "The high temporal variation of the point clouds is the key\nchallenge of 3D single-object tracking (3D SOT). Existing approaches\nrely on the assumption that the shape variation of the point clouds and\nthe motion of the objects across neighboring frames are smooth, failing to\ncope with high temporal variation data. In this paper, we present a novel\nframework for 3D SOT in point clouds with high temporal variation,\ncalled HVTrack. HVTrack proposes three novel components to tackle the\nchallenges in the high temporal variation scenario: 1) A Relative-Pose-\nAware Memory module to handle temporal point cloud shape variations;\n2) a Base-Expansion Feature Cross-Attention module to deal with sim-\nilar object distractions in expanded search areas; 3) a Contextual Point\nGuided Self-Attention module for suppressing heavy background noise.\nWe construct a dataset with high temporal variation (KITTI-HV) by\nsetting different frame intervals for sampling in the KITTI dataset. On\nthe KITTI-HV with 5 frame intervals, our HVTrack surpasses the state-\nof-the-art tracker CXTracker by 11.3%/15.7% in Success/Precision.", "sections": [{"title": "1 Introduction", "content": "3D single-object tracking (3D SOT) is pivotal for autonomous driving [3,43] and\nrobotics [17, 21, 27, 46]. Given the target point cloud and 3D bounding box as\ntemplate, the goal of 3D SOT is to regress the target 3D poses in the track-\ning point cloud sequence. Existing approaches [6, 8, 10-13, 26, 30, 36, 41, 47-49]\nrely on the assumption that the point cloud variations and motion of the object\nacross neighboring frames are relatively smooth. They crop out a small search\narea around the last proposal for tracking, thus dramatically reducing the com-\nplexity of the problem. The template and search area features are then typically\ncorrelated as shown in Fig. 1a, and used to regress the 3D bounding box.\nIn practice, these approaches are challenged by the presence of large point\ncloud variations due to the limited sensor temporal resolution and the moving\nspeed of objects as shown in Fig. 1b. We refer to this significant variation in\npoint cloud and object position between two frames as the high temporal vari-\nation (HV). The high temporal variation challenge is non-negligible in existing\nbenchmarks, and exists in other scenarios not yet covered by them, such as:\nSkipped-tracking, which can greatly reduce computational consumption in\ntracking and serve a wide range of other tasks such as detection [22] and\nsegmentation [44].\nTracking in edge devices, which is essential for deploying trackers on common\ndevices with limited frame rate, resolution, computation, and power etc.\nTracking in highly dynamic scenarios [16], which is common in life. For ex-\nample, tracking in sports events, highway, and UAV scenarios.\nThere are three challenges for 3D SOT in HV point clouds, and existing ap-\nproaches are not sufficient to address these challenges. 1) Strong shape variations\nof the point clouds: Point cloud shape variations are usually caused by the oc-\nclusion and relative pose transformation between the object and the sensor. As\nillustrated in Fig. 1b, feature correlation in existing approaches fails because of\nthe dramatic change in the density and distribution of points. 2) Distractions\ndue to similar objects: When objects suffer from a significant motion, the search\narea needs to be enlarged to incorporate the target, thus introducing more dis-\ntractions from similar objects. Most of the existing trackers focus on local scale\nfeatures, which discards environmental spatial contextual information to handle\ndistractions. 3) Heavy background noise: The expansion of the search area fur-\nther reduces the proportion of target information in the scene. While aiming to\nfind the high template-response features in the feature correlation stage, existing\nmethods then neglect to suppress the noise interference and reduce the impact of\nnoise features. We evaluate state-of-the-art (SOTA) trackers [26,41,47,48] in the\nhigh temporal variation scenario as shown in Fig. 2. Their performance drops\ndramatically as the temporal variation of scene point clouds enlarges."}, {"title": "2 Related Work", "content": "2.1 3D Single-object Tracking\nMost of the 3D SOT approaches are based on a Siamese framework, because the\nappearance variations of the target between neighboring frames are not signifi-\ncant. The work of Giancola et al. [10] constitutes the pioneering method in 3D\nSOT. However, it only solved the discriminative feature learning problem, and\nused a time-consuming and inaccurate heuristic matching to locate the target.\nZarzar et al. [45] utilized a 2D RPN in bird's eyes view to build an end-to-end\ntracker. The P2B network [26] employs VoteNet [24] as RPN and constructs\nthe first point-based tracker. The following works [8, 12, 13, 30, 36, 47] develop\ndifferent architectures of trackers based on P2B [26]. V2B [12] leverages the\ntarget completion model to generate the dense and complete targets and pro-\nposes a simple yet effective voxel-to-BEV target localization network. BAT [47]\nutilizes the relationship between points and the bounding box, integrating the\nbox information into the point clouds. With the development of transformer\nnetworks, a number of works [6, 11, 13, 30, 41, 49] have proposed to exploit vari-\nous attention mechanisms. STNet [13] forms an iterative coarse-to-fine cross-and\nself-attention to correlate the target and search area. CXTrack [41] employs a\ntarget-centric transformer to integrate targetness information and contextual in-\nformation. TAT [18] leverages the temporal information to integrate target cues\nby applying an RNN-based [5] correlation module. Zheng et al. [48] presented\na motion-centric method M2-Track, which is appearance matching-free and has\nmade great progress in dealing with the sparse point cloud tracking problem.\nWu et al. [39] proposed the first semi-supervised framework in 3D SOT.\nWhile effective in their context, the above methods are designed based on\nthe assumption that the point cloud variation and motion of the objects across\nneighboring frames are not significant. In high temporal variation scenarios,\nthis assumption will lead to performance degradation because of the point cloud\nvariations and interference naturally occurring in large scenes. Here, we introduce\nHVTrack to tackle the challenges of 3D SOT in high temporal variation scenarios.\n2.2 3D Multi-object Tracking\n3D multi-object tracking (MOT) in point clouds follows two main streams:\nTracking-by-detection, and learning-based methods. Tracking-by-detection [2,\n4,34,37] usually exploits methods such as Kalman filtering to correlate the de-\ntection results and track the targets. CenterTrack [50], CenterPoint [43], and\nSimTrack [20] replace the filter by leveraging deep networks to predict the ve-\nlocity and motion of the objects. The learning-based methods [7,29,38] typically\napply a Graph Neural Network to tackle the association challenge in MOT.\nGNN3DMOT [38] leverages both 2D images and 3D point clouds to obtain a ro-\nbust association. 3DMOTFormer [7] constructs a graph transformer framework\nand achieves a good performance using only 3D point clouds."}, {"title": "3 Method", "content": "3.1 Problem Definition\nGiven the template of the target, the goal of 3D SOT is to continually lo-\ncate the poses of the target in the search area point cloud sequence  $\\mathcal{P}_s$\n=  {$\\mathcal{P}_1$, ..., $\\mathcal{P}_t$, ...,$\\mathcal{P}_P$ \u2208 $R^{N_\u00a3\u00d73}$}. Usually, the target point cloud with labels\nin the first frame is regarded as the template. Former trackers [6,8,10-13, 26, 30,\n36, 41, 47-49] leverage a 3D bounding box label  $B_o$  =  (x, y, z, w,l,h,$\\theta$) \u2208  $R^7$  to\ngenerate the template in the input. Here, (x, y, z), (w,l, h) and  $\\theta$  are the center\nlocation, bounding box size (width, length, and height), and rotation angle of\nthe target, respectively. As objects can be assumed to be rigid, the trackers only\nneed to regress the center and rotation angle of the target.\n3.2 Overview\nWe propose HVTrack to exploit both temporal and spatial information and\nachieve robust tracking in high temporal variation scenarios. As shown in Fig. 3,\nwe take the point cloud  $\\mathcal{P}_o^t$  at timet as the search area, and leverage memory\nbanks as the template. We first employ a backbone to extract the local spatial\nfeatures $X_o$ \u2208 $R^{N\u00d7C}$ of  $\\mathcal{P}_o^t$, with N and C the point number and feature channel,\nrespectively. Then, L transformer layers are employed to extract spatio-temporal\ninformation. For each layer l, (i) we capture the template information  $Mem_l$  \u2208\n$R^{K \\times N \\times C}$ from the Relative-Pose-Aware Memory module, with K the memory\nbank size (Sec. 3.3); (ii) the memory features and search area features  $X^{l-1}$  are\ncorrelated in the Base-Expansion Features Cross-Attention (Sec. 3.4); (iii) the\nContextual Point Guided Self-Attention (Sec. 3.5) leverages the attention map in\nthe Base-Expansion Features Cross-Attention to suppress the noise features; (iv)\nwe update the Layer Features memory bank using  $X^{l-1}$. After the transformer\nlayers, an RPN is applied to regress the location (xt, Yt, zt,  $\\theta$t), the mask  $M_t$ \u2208\n$R^{N \\times 1}$, and the observation angle  $\\alpha$  \u2208 $R^2$. Finally, the mask and observation\nangle memory banks are updated using the predicted results.\n3.3 Relative-Pose-Aware Memory Module\nAs shown in Fig. 1(b), rapid changes in relative pose lead to large variations\nin the shape of the object point cloud across the frames. Correlating the object"}, {"title": "3.4 Base-Expansion Feature Cross-Attention", "content": "Most of the existing trackers [12,26,30,36,41,47,49] employ a point based back-\nbone [25,35] and focus on local region features, which we call base scale features.\nUsing only base scale features in the whole pipeline is quite efficient and effective"}, {"title": "3.5 Contextual Point Guided Self-Attention", "content": "Most of the information in the search area will be regarded as noise, because we\nare only interested in one single object to be tracked. Existing trackers [12,26,30,\n36,47] aim to find the features with high template-response in the search area,\nbut neglect the suppress to the noise. Zhou et al. [49] proposed a Relation-Aware\nSampling for preserving more template-relevant points in the search area before\ninputting it to the backbone. By contrast, we focus on suppressing the noise\nafter feature correlation via a Contextual Point Guided Self-Attention (CPA).\nAs shown in Fig. 4b, we leverage the base and expansion scale attention maps\nto generate the importance map $I$ \u2208 $R^{N \\times 1}$ as\n$I = Mean(Attn_{l-1}^{base}) + Mean(Attn_{l-1}^{expan}).$\nThe higher the importance of the point, the more spatial context-aware informa-\ntion related to the target it contains. We sort the points according to the mag-\nnitude of their importance values. Then, all the points will be separated into G\ngroups according to their importance. For each group with points $P_G$ \u2208 $R^{G \\times C}$,\nwe aggregate the points into Ui clusters, which we call contextual points. Specif-\nically, we first reshape the points as  $P_G$ \u2208 $R^{U_i \\times C \\times G_i/U_i}$. Second, a linear layer is\nemployed to project the group to the contextual points  $P_U$ \u2208 $R^{U_i \\times C}$. We assign\nfewer contextual points for the groups with lower importance, and suppress the\nnoise feature expression. Finally, all the contextual points are concatenated and\nprojected into Key  $K^U$ \u2208 $R^{U \\times C}$ and Value  $V^U$ \u2208 $R^{U \\times C}$. We project $X^{l-1}$ to Q\nand perform a multi-head attention with  $K^U$ and $V^U$, and an FFN is applied\nafter attention. CPA shrinks the length of K and V, and leads to a computational\ncost decrease in self-attention."}, {"title": "3.6 Implementation Details", "content": "Backbone & Loss Functions. Following CXTrack [41], we adopt DGCNN [35]\nas our backbone, and apply X-RPN [41] as the RPN of our framework. We add\ntwo Shared MLP layers to X-RPN for predicting the observation angles ($\\alpha$) and\nthe masks. Therefore, the overall loss is expressed as\n$L = \\gamma_1L_{cc} + \\gamma_2L_{mask} + \\gamma_3L_{\\alpha} + \\gamma_4L_{rm} + \\gamma_5L_{box},$\nwhere  $L_{cc}$, $L_{mask}$, $L_{\\alpha}$, $L_{box}$, and $L_{box}$ are the loss for the coarse center,\nforeground mask, observation angle, targetness mask, and bounding box, re-\nspectively. We apply the  $L_2$ loss for $L_{cc}$, the standard cross entropy loss for\n$L_{mask}$ and $L_{rm}$, and the Huber loss for  $L_{\\alpha}$ and $L_{box}$.  $\\gamma_1$,  $\\gamma_2$,  $\\gamma_3$,  $\\gamma_4$, and  $\\gamma_5$\nare empirically set as 10.0, 0.2, 1.0, 1.0, and 1.0.\nTraining & Testing. We train our model on NVIDIA RTX-3090 GPUs with\nthe Adam optimizer and an initial learning rate of 0.001. Due to GPU memory\nlimitation, we construct point cloud sequences with 8 frames for training, and\nset K = 2 in training, and K = 6 in testing. Following existing methods [41,48],\nwe set N and C to 128. We stack L = 2 transformer layers and apply H = 4\nheads in BEA and CPA. We adopt G = 3 groups in CPA, and assign [32, 64, 32]\npoints and U = [4, 32, 16] contextual points for the groups, respectively."}, {"title": "4 Experiments", "content": "We leverage two famous 3D tracking benchmarks of KITTI [9] and Waymo [32]\nto evaluate the general performance of our approach in regular 3D SOT. In\naddition, we establish a new KITTI-HV dataset to test our performance in high\ntemporal variation scenarios.\nRegular Datasets. The KITTI tracking dataset comprises 21 training se-\nquences and 29 test sequences, encompassing eight object types. Following prior\nstudies [10, 26, 36, 41, 47, 48], we use the sequences 0-16 as training data, 17-\n18 for validation, and 19-20 for testing. The Waymo dataset is large-scale. We\nadopt the approach outlined in LiDAR-SOT [23] to utilize 1121 tracklets, which\nare subsequently categorized into easy, medium, and hard subsets based on the\nnumber of points in the first frame of each tracklet.\nHV Dataset. We build a dataset with high temporal variation for 3D SOT\nbased on KITTI, called KITTI-HV. Although high temporal variation scenarios\nare present in the existing benchmarks, there is no exact threshold to deter-\nmine whether the scenario is a high temporal variation scenario or not. Large\npoint cloud variations and significant object motions are two major challenges\nin high temporal variation scenarios. Sampling at frame intervals is a good way\nto simulate these two challenges. Also, the constructed KITTI-HV can provide a\npreliminary platform for exploring tracking in scenarios such as skipped-tracking,\nedge devices, and high dynamics. For a fairer comparison with existing methods,\nwe set the frame interval to 2, 3, 5, and 10. We set up more dense testings at\nlow frame intervals to exploit the performance of the existing methods in point\ncloud variations close to smooth scenarios. We train and test all methods from\nscratch individually on each frame interval.\nEvaluation Metrics. We employ One Pass Evaluation [40] to evaluate the\ndifferent methods in terms of Success and Precision. Success is determined by\nmeasuring the Intersection Over Union between the proposed bounding box and\nthe ground-truth (GT) bounding box. Precision is evaluated by computing the\nArea Under the Curve of the distance error between the centers of the two\nbounding boxes, ranging from 0 to 2 meters."}, {"title": "4.1 Comparison with the State of the Art", "content": "Results on HV tracking. We evaluate our HVTrack in 4 categories ('Car',\n'Pedestrian', 'Van', and 'Cyclist') following existing methods [26, 41, 47, 48] in\nthe KITTI-HV dataset. The methods we choose to compare with HVTrack are\nthe most representative SOT methods from 2020 to 2023 (Most cited methods\npublished in each year according to Google Scholar). As illustrated in Tab. 1, our\napproach consistently outperforms the state-of-the-art methods [26, 41, 47, 48]\nacross all frame intervals, confirming the effectiveness of the proposed tracking\nframework for high temporal variation scenarios. Notably, the performance gap\nbetween our HVTrack and existing trackers widens as variations are exacerbated.\nIn the particularly challenging scenario of 10 frame intervals, we achieve a sub-\nstantial 9.1%\u2191 improvement in success and a remarkable 10.4%\u2191 enhancement"}, {"title": "4.2 Analysis Experiments", "content": "In this section, we extensively analyze HVTrack via a series of experiments.\nAll the experiments are conducted on KITTI-HV with 5 frame intervals unless\notherwise stated.\nAblation Study. We conduct experiments to analyze the effectiveness of differ-\nent modules in HVTrack. As shown in Tab. 4, we respectively ablate OM, BEA,\nand CPA from HVTrack. We only ablate OM in RPM because LM and MM"}, {"title": "5 Conclusion", "content": "In this paper, we have explored a new task in 3D SOT, and presented the first\n3D SOT framework for high temporal variation scenarios, HVTrack. Its three\nmain components, RPM, BEA, and CPA, allow HVTrack to achieve robust-\nness to point cloud variations, similar object distractions, and background noise.\nOur experiments have demonstrated that HVTrack significantly outperforms the\nstate of the art in high temporal variation scenarios, and achieves remarkable\nperformance in regular tracking.\nLimitation. Our CPA relies on fixed manual hyperparameters to suppress noise.\nThis makes it difficult to balance the performance in different object and search\narea sizes, leading to a performance drop in tracking large objects. In the future,\nwe will therefore explore the use of a learnable function to replace the manual\nhyperparameters in CPA and overcome the large object tracking challenge."}, {"title": "A Implementation Details", "content": "KITTI-HV. KITTI-HV has the same size as the original KITTI. We can simply\nconstruct KITTI-HV with a few lines of code as in Algorithm 1. We set the inter-\nvals non-linearly ([2,3,5,10]) instead of the traditional linear setting ([2,4,6,8]).\nThus, we have denser tests in point cloud variations close to smooth scenarios\n(comparing [2,3,5] to [2,4,6]) for a fairer comparison with the existing methods.\nSearch areas. Former trackers [26,41,47,48] determine the search area by en-\nlarging the target bounding box in wide and length at the last frame by 2 meters\noffset. We follow their strategy to generate the search area with enlargement off-\nsets on KITTI [9] as shown in Tab. 7. We first statistically analyze the moving\ndistance in the xy-plane of 'Car' on KITTI with different frame intervals as\nshown in Tab. 8. We evaluate the performance of BAT [47] and M2-Track [48]\nwith different bounding box enlargement offsets in 5 frame intervals on KITTI-\nHV. The enlargement offsets are generated by slightly increasing the moving\ndistances under different quantiles in Tab. 8. As illustrated in Tab. 9, BAT and\nM2-Track reach the peak at the enlargement offset of 4 meters and 6 meters,\nrespectively. Thus, we choose the moving distances between quantiles of 50%\nand 75% as the enlargement offset for all the frame intervals and categories.\nFollowing [26, 41, 47, 48], we randomly sample 1024 points in the search area as\nthe input of the backbone.\nObservation angle. Instead of the original radian \u2208 $R^1$, we use the sine and\ncosine values \u2208 $R^2$ to represent the observation angle.\nAblation details. We construct the vanilla cross-attention and self-attention in\nthe ablation experiment as shown in Fig. 6 (a) and Fig. 7 (a). Compared to the\nBEA, vanilla cross-attention removes the expansion branch and assigns H heads\nfor the base branch. For the vanilla self-attention, we directly project X1-1 to K\nand V."}, {"title": "B More Comparisons", "content": "Comparison with latest SOTAs. In Tab. 10, we compare HVTrack with the\nlatest SOTAs on KITTI. There still exists a performance gap compared to them."}]}