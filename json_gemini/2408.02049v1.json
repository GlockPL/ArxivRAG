{"title": "3D Single-object Tracking in Point Clouds with High Temporal Variation", "authors": ["Qiao Wu", "Kun Sun", "Pei An", "Mathieu Salzmann", "Yanning Zhang", "Jiaqi Yang"], "abstract": "The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in the KITTI dataset. On the KITTI-HV with 5 frame intervals, our HVTrack surpasses the state-of-the-art tracker CXTracker by 11.3%/15.7% in Success/Precision.", "sections": [{"title": "1 Introduction", "content": "3D single-object tracking (3D SOT) is pivotal for autonomous driving [3,43] and robotics [17, 21, 27, 46]. Given the target point cloud and 3D bounding box as template, the goal of 3D SOT is to regress the target 3D poses in the tracking point cloud sequence. Existing approaches [6, 8, 10-13, 26, 30, 36, 41, 47-49] rely on the assumption that the point cloud variations and motion of the object across neighboring frames are relatively smooth. They crop out a small search area around the last proposal for tracking, thus dramatically reducing the complexity of the problem. The template and search area features are then typically correlated as shown in Fig. 1a, and used to regress the 3D bounding box.\nIn practice, these approaches are challenged by the presence of large point cloud variations due to the limited sensor temporal resolution and the moving speed of objects as shown in Fig. 1b. We refer to this significant variation in point cloud and object position between two frames as the high temporal variation (HV). The high temporal variation challenge is non-negligible in existing benchmarks, and exists in other scenarios not yet covered by them, such as:\n\u2013 Skipped-tracking, which can greatly reduce computational consumption in tracking and serve a wide range of other tasks such as detection [22] and segmentation [44].\n\u2013 Tracking in edge devices, which is essential for deploying trackers on common devices with limited frame rate, resolution, computation, and power etc.\n\u2013 Tracking in highly dynamic scenarios [16], which is common in life. For example, tracking in sports events, highway, and UAV scenarios.\nThere are three challenges for 3D SOT in HV point clouds, and existing ap- proaches are not sufficient to address these challenges. 1) Strong shape variations of the point clouds: Point cloud shape variations are usually caused by the oc- clusion and relative pose transformation between the object and the sensor. As illustrated in Fig. 1b, feature correlation in existing approaches fails because of the dramatic change in the density and distribution of points. 2) Distractions due to similar objects: When objects suffer from a significant motion, the search area needs to be enlarged to incorporate the target, thus introducing more dis- tractions from similar objects. Most of the existing trackers focus on local scale features, which discards environmental spatial contextual information to handle distractions. 3) Heavy background noise: The expansion of the search area fur- ther reduces the proportion of target information in the scene. While aiming to find the high template-response features in the feature correlation stage, existing methods then neglect to suppress the noise interference and reduce the impact of noise features. We evaluate state-of-the-art (SOTA) trackers [26,41,47,48] in the high temporal variation scenario as shown in Fig. 2. Their performance drops dramatically as the temporal variation of scene point clouds enlarges."}, {"title": "2 Related Work", "content": "Most of the 3D SOT approaches are based on a Siamese framework, because the appearance variations of the target between neighboring frames are not signifi- cant. The work of Giancola et al. [10] constitutes the pioneering method in 3D SOT. However, it only solved the discriminative feature learning problem, and used a time-consuming and inaccurate heuristic matching to locate the target. Zarzar et al. [45] utilized a 2D RPN in bird's eyes view to build an end-to-end tracker. The P2B network [26] employs VoteNet [24] as RPN and constructs the first point-based tracker. The following works [8, 12, 13, 30, 36, 47] develop different architectures of trackers based on P2B [26]. V2B [12] leverages the target completion model to generate the dense and complete targets and pro- poses a simple yet effective voxel-to-BEV target localization network. BAT [47] utilizes the relationship between points and the bounding box, integrating the box information into the point clouds. With the development of transformer networks, a number of works [6, 11, 13, 30, 41, 49] have proposed to exploit vari- ous attention mechanisms. STNet [13] forms an iterative coarse-to-fine cross-and self-attention to correlate the target and search area. CXTrack [41] employs a target-centric transformer to integrate targetness information and contextual in- formation. TAT [18] leverages the temporal information to integrate target cues by applying an RNN-based [5] correlation module. Zheng et al. [48] presented a motion-centric method M2-Track, which is appearance matching-free and has made great progress in dealing with the sparse point cloud tracking problem. Wu et al. [39] proposed the first semi-supervised framework in 3D SOT.\nWhile effective in their context, the above methods are designed based on the assumption that the point cloud variation and motion of the objects across neighboring frames are not significant. In high temporal variation scenarios, this assumption will lead to performance degradation because of the point cloud variations and interference naturally occurring in large scenes. Here, we introduce HVTrack to tackle the challenges of 3D SOT in high temporal variation scenarios."}, {"title": "2.2 3D Multi-object Tracking", "content": "3D multi-object tracking (MOT) in point clouds follows two main streams: Tracking-by-detection, and learning-based methods. Tracking-by-detection [2, 4,34,37] usually exploits methods such as Kalman filtering to correlate the de- tection results and track the targets. CenterTrack [50], CenterPoint [43], and SimTrack [20] replace the filter by leveraging deep networks to predict the ve- locity and motion of the objects. The learning-based methods [7,29,38] typically apply a Graph Neural Network to tackle the association challenge in MOT. GNN3DMOT [38] leverages both 2D images and 3D point clouds to obtain a ro- bust association. 3DMOTFormer [7] constructs a graph transformer framework and achieves a good performance using only 3D point clouds."}, {"title": "3 Method", "content": "Given the template of the target, the goal of 3D SOT is to continually lo- cate the poses of the target in the search area point cloud sequence Ps = {P\u2081,..., P\u209c,...,PP \u2208 RN\u00a3\u00d73}. Usually, the target point cloud with labels in the first frame is regarded as the template. Former trackers [6,8,10-13, 26, 30, 36, 41, 47-49] leverage a 3D bounding box label Bo = (x, y, z, w,l,h,0) \u2208 R\u2077 to generate the template in the input. Here, (x, y, z), (w,l, h) and 9 are the center location, bounding box size (width, length, and height), and rotation angle of the target, respectively. As objects can be assumed to be rigid, the trackers only need to regress the center and rotation angle of the target.\nWe propose HVTrack to exploit both temporal and spatial information and achieve robust tracking in high temporal variation scenarios. As shown in Fig. 3, we take the point cloud P\u2080 at timet as the search area, and leverage memory banks as the template. We first employ a backbone to extract the local spatial features X\u2080 \u2208 RN\u00d7C of P\u2080, with N and C the point number and feature channel, respectively. Then, L transformer layers are employed to extract spatio-temporal information. For each layer 1, (i) we capture the template information Memi \u2208 RKN\u00d7C from the Relative-Pose-Aware Memory module, with K the memory bank size (Sec. 3.3); (ii) the memory features and search area features X1\u208b\u2081 are correlated in the Base-Expansion Features Cross-Attention (Sec. 3.4); (iii) the Contextual Point Guided Self-Attention (Sec. 3.5) leverages the attention map in the Base-Expansion Features Cross-Attention to suppress the noise features; (iv) we update the Layer Features memory bank using Xi\u208b\u2081. After the transformer layers, an RPN is applied to regress the location (xt, Yt, zt, 0t), the mask Mt \u2208 RN\u00d71, and the observation angle \u03b1 \u2208 R\u00b2. Finally, the mask and observation angle memory banks are updated using the predicted results."}, {"title": "3.3 Relative-Pose-Aware Memory Module", "content": "As shown in Fig. 1(b), rapid changes in relative pose lead to large variations in the shape of the object point cloud across the frames. Correlating the object"}, {"title": "3.4 Base-Expansion Feature Cross-Attention", "content": "Most of the existing trackers [12,26,30,36,41,47,49] employ a point based back- bone [25,35] and focus on local region features, which we call base scale features. Using only base scale features in the whole pipeline is quite efficient and effective"}, {"title": "3.5 Contextual Point Guided Self-Attention", "content": "Most of the information in the search area will be regarded as noise, because we are only interested in one single object to be tracked. Existing trackers [12,26,30, 36,47] aim to find the features with high template-response in the search area, but neglect the suppress to the noise. Zhou et al. [49] proposed a Relation-Aware Sampling for preserving more template-relevant points in the search area before inputting it to the backbone. By contrast, we focus on suppressing the noise after feature correlation via a Contextual Point Guided Self-Attention (CPA).\nAs shown in Fig. 4b, we leverage the base and expansion scale attention maps to generate the importance map I \u2208 RN\u00d71 as\nI = Mean(Attnbase) + Mean(Attnexpan).\n(11)\nThe higher the importance of the point, the more spatial context-aware informa- tion related to the target it contains. We sort the points according to the mag- nitude of their importance values. Then, all the points will be separated into G groups according to their importance. For each group with points PG \u2208 RG\u00d7C, we aggregate the points into Ui clusters, which we call contextual points. Specif- ically, we first reshape the points as PG \u2208 RUi\u00d7C\u00d7G\u2081/Ui. Second, a linear layer is employed to project the group to the contextual points PU \u2208 RU\u00d7C. We assign fewer contextual points for the groups with lower importance, and suppress the noise feature expression. Finally, all the contextual points are concatenated and projected into Key KU \u2208 RU\u00d7C and Value VU \u2208 RU\u00d7C. We project X1\u208b\u2081 to Q and perform a multi-head attention with KU and VU, and an FFN is applied after attention. CPA shrinks the length of K and V, and leads to a computational cost decrease in self-attention."}, {"title": "3.6 Implementation Details", "content": "Backbone & Loss Functions. Following CXTrack [41], we adopt DGCNN [35] as our backbone, and apply X-RPN [41] as the RPN of our framework. We add two Shared MLP layers to X-RPN for predicting the observation angles (a) and the masks. Therefore, the overall loss is expressed as\nL = Y\u2081Lcc + Y\u2082Lmask + Y\u2083Lalpha + Y\u2084Lrm + Y\u2085Lbox, (12)\nwhere Lcc, Lmask, Lalpha, Lbox, and Lbox are the loss for the coarse center, foreground mask, observation angle, targetness mask, and bounding box, re- spectively. We apply the L2 loss for Lee, the standard cross entropy loss for Lmask and Lrm, and the Huber loss for Lalpha and Lbox. Y\u2081, Y\u2082, 73, 74, and Y5 are empirically set as 10.0, 0.2, 1.0, 1.0, and 1.0.\nTraining & Testing. We train our model on NVIDIA RTX-3090 GPUs with the Adam optimizer and an initial learning rate of 0.001. Due to GPU memory limitation, we construct point cloud sequences with 8 frames for training, and set K = 2 in training, and K = 6 in testing. Following existing methods [41,48], we set N and C to 128. We stack L = 2 transformer layers and apply H = 4 heads in BEA and CPA. We adopt G = 3 groups in CPA, and assign [32, 64, 32] points and U = [4, 32, 16] contextual points for the groups, respectively."}, {"title": "A Implementation Details", "content": "KITTI-HV. KITTI-HV has the same size as the original KITTI. We can simply construct KITTI-HV with a few lines of code as in Algorithm 1. We set the inter- vals non-linearly ([2,3,5,10]) instead of the traditional linear setting ([2,4,6,8]). Thus, we have denser tests in point cloud variations close to smooth scenarios (comparing [2,3,5] to [2,4,6]) for a fairer comparison with the existing methods.\nSearch areas. Former trackers [26,41,47,48] determine the search area by en- larging the target bounding box in wide and length at the last frame by 2 meters offset. We follow their strategy to generate the search area with enlargement off- sets on KITTI [9] as shown in Tab. 7. We first statistically analyze the moving distance in the xy-plane of 'Car' on KITTI with different frame intervals as shown in Tab. 8. We evaluate the performance of BAT [47] and M2-Track [48] with different bounding box enlargement offsets in 5 frame intervals on KITTI- HV. The enlargement offsets are generated by slightly increasing the moving distances under different quantiles in Tab. 8. As illustrated in Tab. 9, BAT and M2-Track reach the peak at the enlargement offset of 4 meters and 6 meters, respectively. Thus, we choose the moving distances between quantiles of 50% and 75% as the enlargement offset for all the frame intervals and categories. Following [26, 41, 47, 48], we randomly sample 1024 points in the search area as the input of the backbone.\nObservation angle. Instead of the original radian \u2208 R\u00b9, we use the sine and cosine values \u2208 R\u00b2 to represent the observation angle.\nAblation details. We construct the vanilla cross-attention and self-attention in the ablation experiment as shown in Fig. 6 (a) and Fig. 7 (a). Compared to the BEA, vanilla cross-attention removes the expansion branch and assigns H heads for the base branch. For the vanilla self-attention, we directly project X1\u208b\u2081 to K and V."}, {"title": "B More Comparisons", "content": "Comparison with latest SOTAs. In Tab. 10, we compare HVTrack with the latest SOTAs on KITTI. There still exists a performance gap compared to them."}]}