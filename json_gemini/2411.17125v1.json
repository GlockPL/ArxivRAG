{"title": "DOGE: Towards Versatile Visual Document Grounding and Referring", "authors": ["Yinan Zhou", "Yuxin Chen", "Haokun Lin", "Shuyu Yang", "Li Zhu", "Zhongang Qi", "Chen Ma", "Ying Shan"], "abstract": "In recent years, Multimodal Large Language Models (MLLMs) have increasingly emphasized grounding and referring capabilities to achieve detailed understanding and flexible user interaction. However, in the realm of visual document understanding, these capabilities lag behind due to the scarcity of fine-grained datasets and comprehensive benchmarks. To fill this gap, we propose the Document Grounding and referring data engine (DOGE-Engine), which produces two types of high-quality fine-grained document data: multi-granular parsing data for enhancing fundamental text localization and recognition capabilities; and instruction-tuning data to activate MLLM's grounding and referring capabilities during dialogue and reasoning. Additionally, using our engine, we construct DOGE-Bench, which encompasses 7 grounding and referring tasks across 3 document types (chart, poster, PDF document), providing comprehensive evaluations for fine-grained document understanding. Furthermore, leveraging the data generated by our engine, we develop a strong baseline model, DOGE. This pioneering MLLM is capable of accurately referring and grounding texts at multiple granularities within document images. Our code, data, and model will be open-sourced for community development.", "sections": [{"title": "1. Introduction", "content": "In recent years, Multimodal Large Language Models (MLLMs) [6, 14, 21, 23, 25, 26, 32, 33, 62-64] have achieved significant advancements in general visual understanding and reasoning by integrating pre-trained vision encoders with large language models. Leveraging the diverse fine-grained annotations of existing image datasets, some researchers further equip MLLMs with grounding and referring capabilities. These capabilities enhance the detailed visual understanding, increase the credibility of responses, and facilitate more efficient human-AI interaction.\nIn the field of visual document understanding, the dense textual content and complex layout significantly complicate fine-grained understanding.\nTo build a user-friendly and trustworthy document AI assistant, it is crucial to enable users to refer to specific regions of a document for more precise comprehension and provide accurate grounding of key details for more expressive and effective interaction.\nHowever, due to the lack of high-quality, fine-grained document datasets, the full potential of MLLMs in document grounding and referring remains largely unexplored. Existing efforts leverage the multi-granularity parsing annotations to enhance document detailed perception [12], or introduce region-level instruction-tuning tasks to achieve basic referring capabilities [28]. However, these works have two significant shortcomings:\n\u2022 Suboptimal annotation quality. To collect parsing annotations, the Optical Character Recognition (OCR) tools are typically employed to extract text and bounding boxes. Subsequently, random document regions and overlapping text boxes are selected to create multi-granular annotations. However, OCR tools have issues with inaccurate bounding box predictions, and the random selection can lead to truncated characters and imprecise bounding boxes. Furthermore, the text annotated using this approach often suffers from incomplete semantics, limiting its potential for application in developing instruction-tuning data.\n\u2022 Lack of diversity in task formats. Current instruction-tuning datasets primarily cater to fundamental referencing tasks, such as region-level OCR and summarization. However, these datasets fall short in supporting grounding tasks and fail to seamlessly incorporate grounding and referencing capabilities into the dialogue and reasoning processes of MLLMs. The limited range of tasks hinders the model's ability to perceive details flexibly and impacts the user interaction experience.\nIn order to advance the document referring and grounding capability, we propose the Document Grounding and rEferring data engine (DOGE-Engine) for high-quality fine-grained document data construction. With DOGE-"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. MLLMs for Visual Document Understanding", "content": "Visual Document Understanding focuses on comprehend-ing images with extensive text content. Recently, several Multimodal Large Language Models [6, 9, 56, 61] have been introduced to perform visual document understanding without relying on OCR tools. UReader [61] unifies a wide range of document understanding tasks with instruction-tuning format. A shape-adaptive cropping module is further designed to encode rich textual content in high-resolution image, where the raw image is cropped into multiple tiles, with each tile being individually encoded to represent fea-tures of the raw image. TextMonkey [34] employs shifted window to build connections between different image tiles, alleviating the issue of incoherence semantic caused by im-"}, {"title": "2.2. MLLMs for Grounding and Referring", "content": "In pursuit of fine-grained image understanding and convenient interaction, recent studies integrate grounding and referring abilities into MLLMs [4, 44, 65, 68]. Kosmos2 [44], Shikra [4] and Ferret [65] utilize bounding boxes or visual prompts to pinpoint specific regions of an image and generate responses with key objects being grounded, facilitating flexible content referring and interaction. Additionally, LLaVA-Grounding [68] and GLaMM [45] further employ finer-grained multi-granularity masks for pixel-level grounding across various semantic levels. These works achieve promising grounding and referring results on real-world images but cannot be adapted to visual document understanding due to the image domain gap. Recently, there are several attempts to develop visual document grounding and referring. mPLUG-1.5 [12] and Kosmos-2.5[35] enhance localization and fine-grained perception with bounding box annotations from OCR tools and PDF parsing tools, respectively. Fox [29] utilizes various visual prompts to refer to document regions and enables the MLLMs to extract or translate the region-level content. However, mPLUG-1.5 and Kosmos-2.5 only support basic text localization and recognition tasks. Fox further supports the referring translation task.\nThese methods, though effective, fall short of integrating both grounding and referring into the broader reasoning and dialogue processes. As a result, they leave the full po-"}, {"title": "3. DOGE-Engine", "content": "Well-annotated and diverse grounded data are crucial for improving the grounding and referring capabilities of MLLMs [27, 44, 52, 67]. Currently, there is a shortage of comprehensive and accurately labeled document grounded data. Manually annotating raw document images is both time-consuming and labor-intensive, as it requires not only marking the bounding boxes but also accurately annotating all the text within those boxes. To tackle this chal-lenge, we collect a substantial volume of documents and develop the DOGE-Engine to construct fine-grained doc-ument grounded datasets. Our document data sources primarily encompass three document data types: posters, charts, and PDF documents. As shown in the right of Fig. 4, we start by filtering the raw data to remove low-quality samples or those with missing or broken information. Then, we introduce the generation of two types of high-quality docu-ment data: The annotation process of multi-granular parsing data is detailed in Sec. 3.1, and the construction pipeline of instruction-tuning data is elaborated in Sec. 3.2. An overview of statistical information is provided in Sec. 3.3. Annotation results are presented in Appendix A."}, {"title": "3.1. Multi-granular Parsing Data Construction", "content": ""}, {"title": "3.1.1. Automatic Bounding box Annotation", "content": "Poster. We collect poster data from the Crello dataset [59], which consists of design templates from the design ser-vice website. Each poster contains the document meta-annotation, which includes rendered text blocks and their corresponding bounding boxes. However, we observe that"}, {"title": "3.1.2. Full-page Parsing Data Construction", "content": "In addition to the detailed annotations across word, phrase, line, and paragraph-level, we also construct full-page parsing annotations to enhance the comprehensive perception of the document content.\nPoster and Chart. The poster data features less text, mak-ing it straightforward to grasp the dependencies between text blocks. Therefore, we concatenate the paragraph-level text and their corresponding bounding boxes in a left-to-right, top-to-bottom scanning order to create the full-page parsing annotation in JSON format. Due to the highly structured nature of chart data, its full-page parsing annota-tion is organized as a JSON dictionary containing key-value pairs of chart, axis labels, legends, and their corresponding bounding boxes.\nPDF Document. PDF document contains extensive text and complex layouts, necessitating parsing in a logical reading order to comprehend the information accurately. MinerU contains a layout detection model and an OCR model, allowing for extracting text blocks with a certain reading order. However, it often fails to capture all content in documents, missing tables, footnotes, and other elements. In contrast, PyMuPDF can thoroughly extract content from documents, but it does not provide an appropriate reading order. We propose a Merge Strategy that combines the strengths of MinerU and PyMuPDF to achieve comprehen-sive, layout-aware annotations. As shown in Fig. 3b, 1) we first compare text blocks from ordered and unordered maps to eliminate duplicate text blocks. For the truncated blocks (red block 1 and red block 2 in the ordered map), we replace them with the corresponding complete block (blue block in the unordered map) to improve the semantic completeness within blocks; 2) In the ordered map, we construct an or-"}, {"title": "3.2. Instruction-Tuning Data Construction", "content": "To seamlessly integrate grounding and referring capabilities in the dialogue and reasoning, it is crucial to construct high-quality instruction-tuning data with accurate grounded text in both query and response. Based on the precise bounding boxes annotations from Sec. 3.1, we leverage GPT-40 [14] to generate diverse-formatted instruction-tuning data tai-lored to various text granularities, encompassing tasks such as question answering, summarization and reasoning. During construction, we also constrain the length and type of the generated response, resulting in different response cate-gories, e.g., short response, open-ended long response. Subsequently, we add a response format prompt to each query to improve the instruction-following quality of generated data.\nPoster and Chart. As introduced in Sec. 3.1.2, the full-page parsing annotation for poster and chart data includes all text blocks along with their corresponding bounding boxes, which contain sufficient information to construct ground-and-refer instruction-tuning data. Therefore, we provide GPT-40 with the full-page parsing data, and task GPT-40 to generate queries and responses in a grounded manner, where the generated content must include texts that originate from the given parsing annotations. Additionally, we require GPT-40 to warp the texts originating from pars-ing annotations with \u2018<ocr></ocr>' and append the cor-responding coordinates wrapped in '<bbox></bbox>'.\nPDF Document. PDF document often contains extensive text content. If we input all the text of various granularities into GPT-40, it would result in excessively long prompts. This not only incurs significant API calling costs but also increases the difficulty for GPT-40 in comprehending and completing the tasks, leading to low generation quality. To address these issues, we design a Post-annotating Strat-egy to generate high-quality data with minimal overhead. Specifically, 1) We first use the document image as input for GPT-40 instead of the text of document, which signifi-cantly reduces the token count; 2) Then, we task GPT-4o to generate queries and responses based on the content of doc-ument images. Additionally, any text within the queries and responses that originates from the document image must be wrapped with \u201c<ocr></ocr>\"; 3) We extract the texts\""}, {"title": "3.3. Data Verification and Splitting", "content": "Although we require GPT-40 to generate grounded responses in the format of \u201c<ocr> text </ocr> <bbox> x1, y1 ,x2,y2 </bbox>\", GPT-40 sometimes fails to follow our requirement, resulting in wrong coordinates format or missing \u201c<bbox></bbox>\u201d. Therefore, we implement a rule-based filter to remove these defective samples Additionally, for poster and chart data, we extract the grounded text from the generated content and compare them with the full-page parsing annotations, filtering out samples that contain incorrect grounded text. We also performed a detailed categorization of the data. As shown in Fig. 2, DOGE-Dataset includes 1.4M multi-granular parsing data and 700K diverse-formatted instruction-tuning data across three document types: poster, chart and PDF document. The multi-granular parsing data comprises four fine-grained levels (word, phrase, line, paragraph) and a full-page level. These grounded text at different granularity have precise bounding box annotation. The instruction-tuning data comprises four types: grounding, referring, grounding-and-referring, and plain Q&A. The plain Q&A is derived by removing the grounded content from a portion of the grounding data. We construct the plain Q&A subset to enhance the diversity of the data and maintain the traditional document understanding capabilities. Detailed dataset statistics are shown in Appendix D.\""}, {"title": "4. DOGE-Bench", "content": "We introduce the DOGE-Bench for the evaluation of grounding and referring capabilities of MLLMs on visual document understanding tasks.\nTask Definition. As shown in Fig. 4, we systematically construct our benchmark by categorizing our data into distinct classes based on both input and output formats. This classification helps in designing clear evaluation metrics. We divide the input formats into two categories based on the presence of bounding boxes: Grounded Question (GQ) with bounding boxes, and Plain-Text Question (PQ) without bounding boxes. The output formats are categorized into four classes:\n\u2022 Grounded Answer(GA): The response consists of a brief answer accompanied by its corresponding bounding box."}, {"title": "5. DOGE", "content": "Overall Architecture. As illustrated in Fig. 5, our model DOGE employ a general MLLM architecture, including a vision encoder, a projector, and a large language model. In the visual encoder component, to enable the model to handle high resolution, we first search for the best aspect ratio for the input image and dynamically segment images into multiple tiles. These tiles, along with a thumbnail of the input image, are provided as input to the vision encoder. We employ pixel shuffle[7] to improve the computational efficiency of the model when processing high-resolution im-ages. For bounding box representation, we simply discrete the continuous coordinates into discrete values from 0 to 999, avoiding the introduction of extra module or location tokens. During inference, we transfer the user-selected re-gions to coordinates of bounding boxes and insert them into the query for preprocessing. After obtaining the output, we utilize post-processing to overlay bounding boxes on orig-inal document images, thereby facilitating user interaction.\nTraining Strategy. We adopt a three-stage training strat-egy, including pre-aligning, pre-training, and fine-tuning. The pre-aligning stage focuses on aligning the feature space of vision and language rapidly. In this stage, we freeze both vision encoder and large language model, and train the pro-jector with a relatively large learning rate. The pre-training stage aims at document parsing capabilities. We unfreeze the vision encoder and the LLM, enabling the model to recognize diverse textual content and acquire text-reading capability. In the fine-tuning stage, We train the entire model using diverse instruction-tuning data, enhancing its instruction-following ability while activating its grounding and referring capabilities during dialogue and reasoning.\nTraining Dataset. In the pre-aligning stage, we utilize LLaVA-558K [33] to train the projector. For the pre-training dataset, we utilize DocStruct4M [12] along with our 1.4M multi-granular parsing data to enhance basic text reading, text grounding and text referring capabilities of DOGE. For the fine-tuning data, we adopt our ground-and-refer instruction-tuning data and meticulously selected"}, {"title": "6. Experiment", "content": ""}, {"title": "6.1. Implementation Details", "content": "DOGE utilizes the InternViT-300M-448px [6, 7] as the vi-sion encoder and Qwen2-7B-Instruct [60] as the LLM. In the pre-aligning stage, we only train the projector and the learning rate is set to 1e-3. In the pre-training and fine-tuning stage, all model parameters are trainable. The learn-ing rate for the vision encoder is 2e-6, while the learning rate for other components is le-5. Each stage is conducted for 1 epoch. In the pre-aligning stage, the batch size is set to 256, and we only use the thumbnail for vision encoder input. In the pre-training stage, the batch size is configured to 512. The number of image tiles and the max length of the input sequence to the large language model are set to 9 and 4096, respectively. In the fine-tuning stage, the batch size is adjusted to 256, and the image tile number and max input length increase to 16 and 6144. During inference, we set the image tile number to 20 for traditional understanding tasks and 16 for grounding and referring tasks."}, {"title": "6.2. Results on DOGE-Bench", "content": "\u300cDue to the scarcity of models that support document grounding and referring Q&A, we mainly present the per-formance of our model on DOGE-Bench, providing a per-formance reference for future research. We evaluate the grounding and referring capabilities of DOGE across three data types and seven tasks.\nPosters have a diverse range of font types and colors, making them ideal for assessing a model's adaptability to different font styles. DOGE shows strong performance in both grounding and referring tasks on poster data, validating its robustness in recognizing fonts with varied styles. Charts contain fine-grained and structural elements, necessitating precise text localization and understanding capabilities. Despite these challenges, DOGE still demonstrates consider-able performance. PDF document data typically features high-resolution and complex content, posing challenges on both referring and grounding tasks. Although DOGE per-forms well in simple question answering (Ga and GRa) and reasoning (G, and GRr), its Flall score in open-ended questions is relatively low. This can be attributed to the dif-ficulty of text localization in dense text images. Some texts could not be accurately located and their bounding boxes are missed during inference, limiting the model's ability to provide comprehensive grounded outputs. Additional quan-"}, {"title": "6.3. Traditional Document Understanding", "content": "To assess the overall capability of DOGE, we conduct ex-periments on 10 traditional document understanding bench-marks, including DocVQA [39], InfographicVQA [40], DeepForm [49] and KLC [48] for document comprehen-sion, WTQ [43] and TabFact [5] for table understanding, ChartQA [37] for chart comprehension, TextVQA [47] and TextCaps [46] for natural image interpretation, VisualMRC [50] for webpage understanding. For metrics, we use ANLS [2] for DocVQA and InfoVQA, F1 score for DeepForm and KLC, text-matching accuracy for WTQ, TabFact, TextVQA and ChartQA, CIDEr [53] for TextCaps and VisualMRC.\nAs detailed in Tab. 3, we observe that DOGE achieves competitive performance across all tasks. Notably, DOGE outperforms the previous state-of-the-art model IXC2.5 by +5.3% in accuracy on WTQ and +25.0% in CIDEr on Vi-sualMRC. It is important to highlight that our primary focus is on enhancing the grounding and referencing capabilities, and we do not engage in extensive pre-training and fine-tuning on large datasets as InternVL2 [7] and IXC2.5 [69]. Despite the limited data used for training, DOGE ranks sec-ond on four datasets while remaining highly competitive, showing that DOGE maintains a strong performance across general document understanding tasks. We believe that in-corporating more data for pre-training and fine-tuning could further improve DOGE's performance on these tasks."}, {"title": "6.4. Ablation Study", "content": "Effectiveness of multi-granular parsing data. We eval-uate the text recognition and grounding performance on DocLocal4K [12]. As shown in Tab. 4, DOGE surpasses mPLUG-DocOwl-1.5 across all granularities. We also train a model called DOGE\u00af, which is pre-trained without using our constructed multi-granular parsing data. Despite the do-main gap between our parsing data and DocLocal4K due to their different construction methods, it is evident that incor-"}, {"title": "7. Conclusion", "content": "In this paper, we introduce DOGE-Engine, a data construction pipeline for generating high-quality ground-and-refer data for fine-grained document understanding. Additionally, we construct DOGE-Bench as the first comprehensive benchmark for evaluating the document grounding and re-ferring capabilities of MLLMs. Furthermore, leveraging data generated by our engine, we develop DOGE, a pioneer-ing MLLM that integrates text grounding and referring abil-ities into the dialogue and reasoning process. Our results show that DOGE can achieve versatile document grounding and referring while achieving promising performance on traditional document understanding tasks. We hope our work will facilitate practical document AI assistants."}, {"title": "A. The Annotation Results", "content": ""}, {"title": "A.1. Poster and Chart Annotations", "content": "As shown in Fig. 6, we present a comparison between our annotations and the original annotations.\nIn the original poster annotations, some text image lay-ers are damaged, and some bounding boxes are inaccurate, which hinders precise text recognition and localization. In the annotations obtained using our Re-rendering Strategy, we reconstruct the text layers and achieve accurate bounding box annotations. Additionally, we include global content information from the original dataset, such as text for-mat, title and keywords to enhance global awareness dur-ing instruction-tuning data generation. We use the values of \"text_with_box\" as the annotations for poster's full-page parsing data.\nIn original chart annotations from ChartQA, some bounding boxes are associated with bars/lines rather than text value. This is inconsistent with our text and bounding box correspondence objectives. Additionally, the bounding boxes in the original annotations are not accurate. In our re-rendered annotations, we align the bounding boxes with the text value, and the bounding boxes are accurate. Furthermore, we randomly erase some of the values to ensure that the model can infer the missing values based on other visual information. We use the entire chart JSON dict as the annotations for the full-page parsing of the chart. Due to space constraints, we omit some of the content using '...'."}, {"title": "A.2. PDF Document Annotations", "content": "As shown in Fig. 7, we present a comparison between the ordered annotations from MinerU and the unordered but comprehensive annotations from PyMuPDF, along with the combined annotations using our Merge Strategy. We utilize green arrows to indicate the ordered annotations and gray arrows to indicate the naive scanning order. By combining the two annotation methods, we achieve full-page parsing annotations that are both comprehensive and as ordered as possible. Moreover, our method can become more effec-tive as the performance of the ordered annotation tools im-proves. Due to space constraints, we omit the content in the middle of this passage."}, {"title": "B. Prompts and Instructions", "content": ""}, {"title": "B.1. Prompt Details", "content": "In Fig. 8, we show three different prompts for poster, chart and PDF document:\nFor poster, we deploy plain text input as prompt. Besides format information and rule information, we also provide GPT-40 with some of the overall content and style informa-tion from the original Crello dataset. This helps in achieving"}, {"title": "B.2. Instruction Details", "content": "As shown in Fig. 9, we introduce the instruction uti-lized in Multi-granular Parsing tasks and the response format prompts which are followed by the questions for instruction-tuning data. For Bounding Box & Text Lo-calization, we introduce 4 instructions for each task, the answer is directly wrapped with \u201c<ocr></ocr>\u201d or \u201c<bbox></bbox>\u201d. For Full-page Parsing, we introduce 3 instructions for poster data, 1 for chart, and 1 for PDF document data. The responses are in the format shown in Fig. 6 and Fig. 7.\nWe add different response format prompts to different questions based on the format of the responses to help the model output corresponding results when users inter-act with it. For generated question and answer pairs, we combine different question and answer pairs with vari-ous response format prompts to obtain diverse grounded data. For Grounded Answering Data, we have 7 re-sponse format prompts to be added after the question, and the answer should be directly a simple text wrapped with \u201c<ocr></ocr>\u201d and followed by the coordinates wrapped with \u201c<bbox></bbox>\u201d. For Grounded Reasoning Data, we combine two random-chose prompts to-"}, {"title": "C. Qualitative Results", "content": ""}, {"title": "C.1. Analysis about Fig. 1", "content": "We present the inference results of DOGE for ground-ing, grounding-and-referring, and referring tasks. The spe-cific coordinates of the annotations are omitted, and the grounded text is highlighted with colored boxes. The colors of the boxes within the document image correspond to the colors of the text boxes. For the grounding task, we present four examples. The first sample demonstrates that DOGE can perform fine-grained question answering on general document images with rich content and complex layouts, successfully grounding the corresponding information. The second sample illustrates the model's excellent recognition and localization capabilities for diverse and small text in poster-type images. Additionally, we showcase two sam-ples of grounding in chart-type images, which indicate that DOGE possesses a certain level of mathematical ability, enabling it to provide grounded reasoning during calculation, as well as the capability to estimate values in charts that lack textual annotations based on the axes. For the grounding-and-referring and referring tasks, DOGE is able to recog-nize the content of user-selected regions and provide rea-sonable grounded or plain textual reasoning and responses. DOGE exhibits robust fine-grained grounding and referring capabilities, allowing for reliable grounded reasoning and accommodating diverse user interactions, significantly en-hancing the overall user experience."}, {"title": "C.2. More Qualitative Results", "content": ""}, {"title": "C.2.1. DOGE-Bench Examples", "content": "As shown in Fig. 10, we present three inference examples of DOGE-Bench, demonstrating unlabeled value reading ca-pability, grounding summarization capability, and referring summarization capability. The first figure is an example of reading unlabeled values. We add auxiliary mark boxes in red to the coordinate axes to facilitate reading the values, each box's height represents 5. It can be observed that the model's output values are accurate. The figure on the lower left is an example of grounding summarization for an en-tire document. DOGE is able to perceive the content of the entire document and perform grounding summarization ef-fectively. The figure on the right is an example of referring summarization for a specific area. The model can also accomplish this task, thereby helping users improve reading efficiency."}, {"title": "C.2.2. Other Examples", "content": "It is noteworthy that our model demonstrates strong gen-eralization capabilities beyond some training domains and tasks, as illustrated in Fig. 11.\nStrong generalization. As shown in the first row of exam-ples, we feed DOGE with screenshots of the paper content and the specially shaped fan chart that the model does not process before. DOGE is able to correctly respond, demon-strating its strong generalization and usability in actual doc-ument reading scenarios.\nHandwriting ground-and-refer ability. The middle sam-"}, {"title": "C.2.3. Failure Cases", "content": "Although DOGE demonstrates strong grounding and re-ferring capabilities, there are still some shortcomings, as shown in Fig. 12.\nReferring. The upper left figure illustrates an example of incorrect referring. It mistakenly associates the bounding box that should correspond to the text \u201cINITIATIVE\u201d in the question with the text \u201cLOYALTY\u201d below it. Additionally, its width is re-estimated according to the size corresponding to \"LOYALTY,\u201d resulting in an incorrect answer.\nGrounding. When encountering some unfamiliar text con-tent such as tables in the upper right image, DOGE can un-derstand the content, effectively identified the line breaks between \"Doc\u201d and \"VQA\u201d and merged them together, and"}, {"title": "D. Dataset Details", "content": ""}, {"title": "D.1. Dataset Statistic", "content": "As shown in Tab. 6, we present the detailed composition of the DOGE dataset's Multi-granular Parsing data and In-struction Tuning data. The Multi-granular Parsing data in-cludes five granularities: word, phrase, line, paragraph, and full-page parsing. The Instruction-Tuning data com-prises three types of grounded data: grounding referring, grounding-and-referring, and plain text Q&A. We provide the detailed data volume for each type.\nFig. 13 shows the distribution of block counts for poster, chart, and PDF document. Poster has fewer grounded blocks within each image, larger areas, diverse font styles, and larger font sizes. The average text length per block for poster is 3.25 words. Chart, on the other hand, has a higher number of blocks with small areas and small font sizes. Each block in a chart corresponds to a small compo-nent value within the chart, with an average text length of 1.34 words per block. PDF document has a moderate dis-tribution of block counts, larger areas, and small font sizes. Each block in a PDF contains relatively longer text, with an average length of 22.55 words per block."}, {"title": "D.2. Construction Cost Details", "content": "In terms of multi-granular parsing data construction, DOGE-Engine can achieve boundary box annotation and content extraction on any document dataset with a similar rendering process. It can also scale up to a larger volume of PDF source files without additional manual costs. When constructing instruction fine-tuning data, our main cost lies"}, {"title": "E. Training Data Details", "content": "For the pre-training data, we utilize DocStruct4M [12] along with our 2.1M multi-granular parsing data to enhance DOGE's foundational grounding and referring capabilities.\nFor the fine-tuning data, we compose our 703k Instruction-Tuning data with 575k DocDownStream-1.0 data from [12] and 716k other document-related data from various datasets, resulting in a final fine-tuning dataset of 2M. In Tab. 7, we show the detailed data source and sam-pled number of 716k other document-related data."}, {"title": "F. More Experiments", "content": "Effectiveness of our pretraining data. To better assess base performance of pretrained model using our multi-"}]}