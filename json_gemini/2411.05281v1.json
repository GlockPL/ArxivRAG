{"title": "FOX-1 TECHNICAL REPORT", "authors": ["Zijian Hu", "Jipeng Zhang", "Rui Pan", "Zhaozhuo Xu", "Salman Avestimehr", "Chaoyang He", "Tong Zhang"], "abstract": "We present Fox-1, a series of small language models (SLMs) consisting of Fox-1-1.6B and Fox-1-1.6B-Instruct-v0.1. These models are pre-trained on 3 trillion tokens of web-scraped document data and fine-tuned with 5 billion tokens of instruction-following and multi-turn conversation data. Aiming to improve the pre-training efficiency, Fox-1-1.6B model introduces a novel 3-stage data curriculum across all the training data with 2K-8K sequence length. In architecture design, Fox-1 features a deeper layer structure, an expanded vocabulary, and utilizes Grouped Query Attention (GQA), offering a performant and efficient architecture compared to other SLMs. Fox-1 achieves better or on-par performance in various benchmarks compared to StableLM-2-1.6B, Gemma-2B, Qwen1.5-1.8B, and OpenELM1.1B, with competitive inference speed and throughput. The model weights have been released under the Apache 2.0 license, where we aim to promote the democratization of LLMs and make them fully accessible to the whole open-source community.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Large Language Models (LLMs) show that the unified next-token prediction paradigm excels in improving performance across diverse text generation tasks, such as code generation Chen et al. [2021], math word problem solving Wei et al. [2022a], medical question answering Diao et al. [2023]. Furthermore, LLMs Brown et al. [2020], OpenAI [2023] are widely acknowledged for their System 2 capabilities Deng et al. [2023], Saha et al. [2023], Weston and Sukhbaatar [2023] in in-context learning Brown et al. [2020] and chain-of-thought prompting Wei et al. [2022a]. Generally, LLMs are developed by training a decoder-only transformer model on extensive text corpora Brown et al. [2020], Rae et al. [2021]. Currently, most state-of-the-art models achieve high performance through fine-tuning white-box LLMs Touvron et al. [2023a,b], Dubey et al. [2024] or using black-box LLM APIs OpenAI [2023], Ouyang et al. [2022].\nTo further enhance the capabilities of decoder-only language models, LLM training focuses more on their scaling laws Kaplan et al. [2020] and compute-optimal training schedules. The key components include model size, the volume of training data, and the corresponding training compute. Several studies Hoffmann et al. [2022] provide guidance on optimally balancing the increase in model size with the amount of training data, consistently showing that large models require extensive datasets and substantial training compute. Given that existing LLMs often reach hundreds of billions of parameters Touvron et al. [2023a,b], Ouyang et al. [2022], Dubey et al. [2024] for better performance, the training costs for these giant models are extremely high, leading to significant financial and environmental impacts. These costs also restrict most researchers from participating, and the high deployment costs further hinder the widespread application of these advanced AI technologies."}, {"title": "2 Pre-Training", "content": "Fox-1 pre-training involves curating and filtering a large corpus of open-sourced textual data, searching the model architecture, and developing the training recipe. In this section, we present the details of the above components."}, {"title": "2.1 Pre-Training Data", "content": "To pre-train Fox-1 we use 3 trillion tokens of textual data. To align with our proposed three-stage curriculum pre-training pipeline, we have reorganized the original data into three distinct collections. We have gathered an extensive range of textual data, encompassing unsupervised and instruction-tuning datasets, as well as diverse domains such as code, web content, mathematics, and science documents. To simplify the description of the data combination in the subsequent sections, we first list the dataset collection and then detail the data usage for each stage.\nRaw Data Collection We collected datasets from publicly released large and high-quality sources, including Redpajama Computer [2023], SlimPajama Soboleva et al. [2023], Dolma Soldaini et al. [2024], Pile Gao et al. [2020], and Falcon Penedo et al. [2023] datasets. Here are the detailed datasets:"}, {"title": "2.1.1 Tokenization", "content": "We hypothesize that it is easier to correctly predict a few consecutive tokens than many consecutive tokens Le Scao et al. [2023], Team et al. [2024], Tao et al. [2024], Kudo [2018]. Therefore, we believe using a large vocabulary could lead to better downstream performance. Arguably, a large vocabulary typically yields fewer tokens for a given text corpus Le Scao et al. [2023], Team et al. [2024], which could lead to better inference performance.\nAlthough Fox-1 is an English-only SLM, we aim to select a sufficiently general vocabulary to leverage intermediate checkpoints for future multilingual capabilities and domain-specific applications. We selected the tokenizer from Team et al. [2024], which offers a vocabulary size of 256K, making it one of the largest available at the time of this report."}, {"title": "2.1.2 Model Architecture", "content": "Fox-1 employs a decoder-only transformer architecture inspired by Touvron et al. [2023b,a], Dubey et al. [2024], Liu et al. [2024] with 1.6B total parameters while introducing various improvements and redesigns for better performance.\nDeeper Network A fundamental trade-off in model architecture design is depth versus width. While wider and shallower networks allow better memorization, deeper and thinner networks present stronger reasoning ability [Cheng et al., 2016, Liu et al., 2024]. In accordance with this principle, Fox-1 uses a deeper architecture than most modern SLMs. Specifically, Fox-1 consists of 32 self-attention layers, which is 78% deeper than Gemma-2B (18 layers), 33% deeper than StableLM-2-1.6B (24 layers), and 33% deeper than Qwen1.5-1.8B (24 layers).\nShared Embedding Fox-1 utilizes a large vocabulary of 256,000 with a hidden dimension of 2,048, resulting in approximately 0.5 billion parameters. Larger models typically use separate embedding layers for the input (vocabulary size to embedding size) and output (embedding size to vocabulary size) layers. For Fox-1, the embedding layers alone would require 1 billion parameters. To reduce the total number of parameters, we follow the approach of Zhang et al. [2022], Liu et al. [2024] and share the input and output embedding layer, maximizing weight utilization and reducing the parameter count by 0.5 billion, approximately 30.49% of the final model size.\nPre-normalization Similar to Touvron et al. [2023b], we use RMSNorm [Zhang and Sennrich, 2019] to normalize the input of each transformer layer. RMSNorm is the dominant choice of pre-normalization in modern large language models [Team et al., 2024, Bai et al., 2023, Touvron et al., 2023b], where it demonstrates better efficiency than LayerNorm [Ba, 2016]."}, {"title": "2.1.3 Training", "content": "The pre-training of long sequences is known to be challenging given the training inefficiency incurred by the quadratic complexity of the attention mechanism [Vaswani, 2017]. To mitigate this problem, a 3-stage curriculum learning strategy is introduced in the pre-training stage of Fox, where the chunk length of the training sample is gradually increased from 2K to 8K to ensure the long-context ability at a small cost.\nSpecifically, stage 1 comprises ~ 39% total data samples in the whole pre-training process, where the 1.05T-token dataset is chunked into 2K-length samples, with a batch size 2M tokens. We use 2,000 steps of linear warm-up for this stage. Stage 2 includes ~ 59% samples with 1.58T tokens and increases the chunk length from 2K to 4K and 8K. The actual chunking lengths vary across different data sources, which are decided based on the natural average lengths in each data source. The batch size also grows to 4M for improving the training efficiency, given stage 2 being the most time-consuming stage with diverse sources of different datasets. Finally, in stage 3, Fox is trained with 62B tokens (~ 0.02%) of high-quality data to lay the groundwork for different downstream task abilities, such as instruction-following, chitchat, domain-specific question-answer, etc. Across all stages, we utilize AdamW and WSD schedule [Hu et al., 2024] to train Fox, along with learning rate 4 \u00d7 10^{-4}, weight decay 0.1, AdamW \u03b2\u2081 = 0.9, \u03b22 = 0.95, WSD temperature 16,000."}, {"title": "3 Experimental Results", "content": "Following the evaluation setup of the Open LLM Leaderboard [Beeching et al., 2023], we evaluated Fox-1 and other SLMs on ARC Challenge (25-shot) [Clark et al., 2018], HellaSwag (10-shot) [Zellers et al., 2019], TruthfulQA (0-shot) [Lin et al., 2021], MMLU (5-shot) [Hendrycks et al., 2020], Winogrande (5-shot) [Sakaguchi et al., 2021], and GSM8k (5-shot) [Cobbe et al., 2021] on the a machine with 8\u00d7H100 GPUs and reported the average score of the 6 benchmarks.\nAs shown in figure 2 and table 6, Fox-1 performs better than or on par with Gemma-2B [Team et al., 2024], Qwen1.5-1.8B [Bai et al., 2023], StableLM-2-1.6B [Bellagente et al., 2024], and OpenELM1.1B [Mehta et al., 2024] across standard LLM benchmarks. For GSM8k, Fox-1 achieves 36.39%, outperforming all baselines. Fox-1 also surpasses Gemma-2B, StableLM-2-1.6B, and OpenELM 1.1B on MMLU despite being only half the size of Gemma-2B.\nInference Efficiency of Fox-1 We evaluated the end-to-end inference efficiency of Fox-1, Qwen1.5-1.8B, and Gemma-2B using vLLM with the TensorOpera serving platform on a single NVIDIA H100 in BF16 precision. To simulate real-world usage in multi-user scenarios, we use the OpenOrca [Lian et al., 2023] dataset and send concurrent requests to the same inference server. Performance was measured as output tokens per user per second, with each request averaging 234 tokens and each response 512 tokens. As shown in figure 3, Fox-1 achieves a throughput exceeding 200 tokens per second, surpassing Gemma-2B and matching Qwen1.5-1.8B in the same deployment environments. This high throughput can be attributed to Fox-1's architectural design, which incorporates Grouped Query Attention (GQA) for efficient query processing.\nWe did not include OpenELM, as it is unsupported by vLLM. With BF16 precision, Fox-1 only needs 3703MiB of GPU Memory, while Qwen1.5-1.8B, StableLM-2-1.6B, and Gemma-2B, respectively, requires 4739MiB, 3852MiB, and 5379MiB."}, {"title": "4 Conclusion", "content": "The Fox-1 series of small language models, including Fox-1-1.6B and Fox-1-1.6B-Instruct-v0.1, show advancement in efficient pre-training and instruction-following capabilities. Through the 3-stage data curriculum and a deep architecture with a large vocabulary size, Fox-1 excels across various benchmarks, outperforming or matching other small language models like StableLM-2-1.6B and Gemma-2B. The success of Fox-1 demonstrates the possibility of pre-training language models with competitive performance even with limited data resources."}]}