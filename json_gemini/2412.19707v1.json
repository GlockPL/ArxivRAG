{"title": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback", "authors": ["Sijia Chen", "Baochun Li"], "abstract": "Large language models (LLMs) have been routinely used to solve various tasks using step-by-step reasoning. However, the structure of intermediate reasoning steps, or thoughts, is rigid and unidirectional, such as chains, trees, or acyclic-directed graphs. Consequently, the resulting inflexible and forward-only reasoning may not address challenging tasks and fail when the LLM frequently gives false responses, i.e., \u201challucinations\". This paper proposes a new reasoning framework, called Thought Rollback (TR), allowing LLMs to adaptively build thought structure while maintaining effective reasoning toward problem-solving under \u201challucinations\". The core mechanism of TR is rolling back thoughts, which allows LLMs to perform error analysis on thoughts, and thus roll back to any previously mistaken thought for revision. Subsequently, by including such trial-and-error in the prompt to guide the LLM, each rollback leads to one more reliable reasoning path. Therefore, starting with a simple prompt without human annotations, LLM with TR adaptively and gradually explores thoughts for a correct solution. Comprehensive experiments on mathematical problems and multi-task reasoning demonstrate the state-of-the-art performance of TR in terms of problem-solving rate and interaction cost. For instance, the solving rate of GPT-4 with TR outperforms the current best by 9% on the MATH dataset.", "sections": [{"title": "1. Introduction", "content": "Large Language Models, initially designed for text generation with autoregression, are widely recognized to excel in a diverse array of natural language processing (NLP) tasks. Yet, at a particular model scale, their reasoning abilities, particularly in scaled-up versions like GPT-4 (OpenAI, 2023) and Llama 2 (Touvron et al., 2023), heavily depend on prompt engineering. With well-crafted prompts  even just a simple Let's think step by step (Kojima et al., 2022)  LLMs are able to perform step-by-step reasoning and achieved noteworthy success in mathematical, symbolic, and common sense tasks. With reasoning, LLMs are capable of producing coherent language sequences, called thoughts, which serve as intermediate reasoning steps toward solving the problem at hand. Extended from simple chain reasoning (Wei et al., 2022) with linear left-to-right thoughts, more complex reasoning became feasible in recent works by establishing thought structures that resembled trees (Yao et al., 2023) and graphs (Besta et al., 2023; Zhang et al., 2023; Luo et al., 2024).\nHowever, existing thought structures are unidirectional and thus allow a forward-only reasoning process, meaning that thoughts are generated sequentially from the start to the end. The efficacy of this reasoning process hinges on a redundant and, consequently, inefficient thought structure, requiring thorough explorations of each thought before progressing to the next. One major drawback of forward-only reasoning is that errors can propagate quickly (Yu et al., 2024). Consider the common case where one thought is incorrect or inaccurate: with forward-only reasoning, all thoughts derived from it can be misled. Even with revisions based on step-by-step evaluations (Weng et al., 2023), such propagation of errors can introduce further deviations from the correct path of reasoning, since LLMs have been found to confidently provide false information (i.e., \u201challucinations\u201d or \u201claziness\u201d) (Jiang et al., 2023).\nIndeed, humans also provide false information as frequently and randomly as LLMs do during reasoning but can still solve challenging problems eventually. This is attributed to adaptive reasoning, in which one does not pre-defined a fixed structure for thoughts and does not simply deduce forward but adaptively adjusts the thought structure after evaluating errors during reasoning. Such reasoning enables humans to begin with one simple or wrong thought but frequently introspect during reasoning, that is, to reconsider previous steps and build new reasoning paths from these reflections. In this paper, we argue that this iterative error correction nature of adaptive reasoning is essentially supported by rollback  jumping to previous steps with a new experience to reconsider the reasoning.\nTherefore, we propose a new reasoning framework, Thought Rollback (TR), relying upon the rolling back of thoughts to enable the adaptive reasoning of LLMs for general problem solving. TR embraces a rollback controller and a prompt enhancer that works seamlessly to enable the LLMs to generate an effective thought structure from one thought derived from a simple input prompt, as shown by Figure 1.\nLLMs with TR start with generating one thought from a simple zero-shot prompt containing only the question description. Subsequently, for each generated thought, the rollback controller allows the LLM to analyze the obtained chain of thoughts and thus determines whether to roll back and to which previous thought. Once rollback is triggered, prompt enhancer accumulates this error analysis as experience in the prompt. As a result, by avoiding making similar mistakes mentioned by experience, LLM is able to generate a new and more effective reasoning path from the chosen thought. Therefore, \u201challucinations\" that occur in thought or analysis of LLM may not influence reasoning due to the continuous thought revision guaranteed by the iterative rollbacks during reasoning. For example, in Figure 1, different from chain (Wei et al., 2022) and tree (Yao et al., 2023) structures, which assume a fixed and unidirectional structure, reasoning with rollbacks enables LLM to build a thought structure adaptively and revise thoughts to achieve complex but reliable reasoning. Specifically, after reaching the N-2 S-2 and N-6 S-4, LLM finds an error in 2-th step N-2 S-2 and thus rolls back to 1-th step N-1 S-1 to create two new reasoning paths. The rollback N-6 \u2192 N-1 leads to the revised thought N-7 S-2. The rollback N-2 \u2192 N-1 utilizes the error analysis to enhance the prompt and obtains N-3 S-2, leading to a correct answer 15.48 compared to the previous mistaken 3.87.\nWe observe four contributions of TR. First, it is a lightweight and generalized automatic prompting framework. TR allows LLMs to perform complex reasoning effectively on various tasks without introducing task-specific human annotations in the prompt or additional human-made designs. Second, the performance of TR is robust to the \"hallucinations\" as LLMs are able to reconsider and revise any existing thoughts adaptively and repeatedly during reasoning. Thus, third, TR is cost-friendly as the thought structure is built progressively to reach a solution instead of relying on bulky search structures (Yao et al., 2023) or question analogies (Yu et al., 2024). Finally, our evaluation of TR on mathematical problems and multi-task reasoning demonstrates that TR outperforms some state-of-the-art approaches while maintaining a lower interaction cost."}, {"title": "2. Related Work", "content": "By only guiding the reasoning behavior of LLMs, such as GPT-4 (OpenAI, 2023) and Llama2 (Touvron et al., 2023) with the text prompt, prompt engineering (Brown et al., 2020; Kojima et al., 2022) is parameter efficient and often matches or surpasses fine-tuning performance. Therefore, plenty of work has been proposed to enable LLMs to perform multi-step reasoning containing a series of intermediate steps, each known as the thought presented as a text sequence. Starting from chain-of-thought (CoT) (Wei et al., 2022) prompting, which provides reasoning examples in the prompt to deduce a chain of thoughts, subsequent endeavors, especially SC (Wang et al., 2022) and Complex CoT (Fu et al., 2023) augment the chain reasoning. Recent advances extend the chain structure into structured thoughts. ToT (Yao et al., 2023) and BoT (Sijia et al., 2024) pre-defines the thought structure as a tree, thus supporting exploring multiple thoughts in each step before generating the next. The graph of thoughts (GoT) (Besta et al., 2023) and cumulative reasoning (CR) (Zhang et al., 2023) further instantiate thoughts toward a solution as the graph structure. Another line of work focuses on the thought structure that allows the thought ensemble in each step. Thought propagation (Yu et al., 2024) explores analogous problems and then aggregates their results to update the solution for the given question, leading to a radial thought structure. To the best of our knowledge, none of the existing work supports the cyclic structure to allow LLMs to revise previous thoughts or recreate a new reasoning path from the previous step after being blocked at the current reasoning step. We fill this gap by proposing the rollback of thought, leading to a thought structure of directed cyclic graphs.\nDespite these achievements, LLMs often struggle with complex tasks, primarily due to the frequent occurrence of \"hallucinations\"-producing false outputs (Jiang et al., 2023; Yu et al., 2024), and \u201claziness\u201d\u2014yielding invalid or no output. Therefore, after noticing that LLMs have self-verification abilities and thus can analyze the answer for further correcting the errors in reasoning (Zheng et al., 2023; Wu et al., 2024). However, the most recent work (Huang et al., 2024) argues that LLMs cannot self-correct their reasoning, emphasizing the invalidity of applying simple verification to the reasoning path. Thus, most recent work either builds iterative-based verification (BoT (Sijia et al., 2024)) or focuses on step-by-step verification (Ling et al., 2023; Lightman et al., 2024). Combining these insights, we aim to exploit LLMs to analyze intermediate thoughts during reasoning to correct these thoughts and adjust reasoning direction adaptively. Continuous verification and revision may eliminate the negative impact of \"hallucinations\" or mistakes on the solutions.\nAnother related research stream is automatic prompting (Kojima et al., 2022; Zhang et al., 2022), which automatically constructs effective prompts to facilitate reasoning without human-made and task-specific demonstrations. As LLMs can learn from mistakes to become better reasoners (An et al., 2023; Sijia et al., 2024), this paper also releases human efforts from the prompt design by boosting the prompt with the error analysis of thoughts. We also show that by accumulating error analysis in the prompt during reasoning, LLMs are able to avoid making similar mistakes and explore correct solutions with interaction cost far less than ToT (Yao et al., 2023) and BoT (Sijia et al., 2024)."}, {"title": "3. Preliminary", "content": "3.1. Problem Statement\nGiven a pre-trained large language model (LLM) denoted as $f (\u00b7)$, the prompt engineering is to design the prompt $I (.)$ to make the model perform desired reasoning behavior toward addressing the given problem $x$. Specifically, multi-step reasoning contains $T$ intermediate steps $z_{0...T} = [z_0, z_1, ..., z_T]$ to bridge $z_0 := x$ and the answer $z_T := y$. To get $z_{0...T}$, we focus on step-wise thought generation in which each thought is a coherent language sequence $z_n$, behaving as one intermediate reasoning step, and $z_n$ is generated as $z_n \\sim f (z_n|I (z_{0,1...n-1}))$. Therefore, as thought is the LLM's output, we can define the bad thought caused by the \u201challucinations\u201d, \u201claziness\u201d, or false reasoning of LLMs as $z_n$.\nThese generated thoughts $z_{0...T}$ naturally follow a specific structure, such as a chain or tree. These structures are unidirectional and thus only support forward-only reasoning, which proceeds in a linear, sequential manner, meaning that LLMs only generate the subsequent thought $z_{n+1}$ from $z_n$. For instance, any edge $e_{n,m}$ of the structure is limited to $m = {n, n + 1}$ while $m >= n$.\nThis paper focuses on alleviating the effect of bad thoughts on the solution by making LLM not simply perform forward-only reasoning but achieve adaptive reasoning, which allows LLMs to 1) start from a simple prompt \u2161 ($z_0$), 2). self-organize the thought structure adaptively during reasoning, and thus 3). when $z_n$ occurs, LLM can make revisions and create better new reasoning paths till getting the solution. Specifically, not only advancing the reasoning sequentially, any previously generated thought will be reconsidered by continuously rolling back from n-th thought to one previous m-th thought, where $m \u2208 [0, n \u2013 1]$.\n3.2. Motivation: Forward-only reasoning fails in bad thoughts\nForward-only reasoning may fail as the bad thought $z_n$ is caused by the following three cases of error propagation.\nCase $[z_m, z_{m+1}, ..., z_n]$. A bad or illogical thought $z_m$ leads to all subsequent errors, where $z_n \\sim f (z_n|I (z_{0,...m...n-1}))$ and $m < n$.\nCase $[z_m, z_{m+1}..., z_n]$. $z_m$ does not lead to direct mistakes but causes a bad thought $z_n$ after many steps. For instance, this appears when $z_m$ behaves as one part of a solution.\nCase $[z_m...n-1, z_n]$. A bad thought $z_n$ may arise from one previous correct though $z_m$ because the wrong reasoning direction appears from this step.\nThe chain reasoning, typically in Chain-of-thought (CoT) prompting, generates a chain of thoughts $z_{0,1...T}$"}, {"title": "4. Thought Rollback Framework", "content": "4.1. Reasoning Overview\nIn contrast to existing approaches relying on pre-defined unidirectional thought structures, which are limited to forward-only reasoning, Thought Rollback (TR) generates a bidirectional thought structure by adaptively deducing forward and rolling back of thoughts. After reaching a reasoning step $z_n$, as shown in Figure 2, TR allows LLM to roll back to the bad thought $z_m$ after analyzing the existing thought chain $z_{0...n}$. As the error analysis $A_m$ is to be accumulated in the prompt, a new and more reliable reasoning path $z_m$ is generated from $z_m$. Therefore, iteratively performing this rollback develops the thought structure from a simple thought to a directed graph with cycles. Such adaptive reasoning is summarized into three stages.\nInitialization. Generate thought $z_1 \\sim f (z_1|\u2161 (z_0))$.\nRollback of thoughts. For each generated thought $z_n$, rollback controller exploits LLM to determine a rollback to one thought $z_m \u2208 z_{0...n}$.\n\u2022 Once the rollback to $m \u2208 [0, 1, ..., n]$ is triggered, the reasoning of LLM rolls back to the thought $z_{m-1}$ and prompt enhancer is used to enhance the prompt. Subsequently, the reasoning continues by creating a new m-th thought $z_m$ and generating $z_{n+1}$, where $z_m$ means a m-th thought deduced from a rollback from n.\n\u2022 When no rollback is required, generate $z_{n+1}$.\nEarly stopping. Stop reasoning when TR yields K number of solutions obtained. Otherwise, continue the Rollback of thoughts.\nSolution ensemble. Perform weighted majority voting on K solutions.\n4.2. Rolling Back Thought with Reasoning Analysis\nDuring reasoning, rollback controller enables an adaptive rollback by exploiting LLM to determine the rollback n \u2192 m. However, making LLM know the concept of rollback may introduce unnecessary complexity. Thus, TR supports the rollback mechanism by performing error analysis on thoughts. Specifically, with a task-agnostic prompt $I_R(R, [z_{0...n}])$, where R is a common error analysis instruction, LLM is guided to analyze a thought chain $[z_{0...n}]$, leading to the error analysis $A_m \\sim f (A_{IR}(R, [z_{0...n}]))$. Eventually, LLM is able to identify the indexes M of bad thoughts $m\u2208M$.\nTo get which thought to roll back to from $z_n$, TR follows the rule to roll back to the one step before the first bad thought. There are two reasons for this. First, generating the next thoughts from a bad thought is unreasonable. Second, we aim not to remove the bad thought but to generate a new reasoning path. Thus, we choose $z_{m-1}$ as the rollback destination, where $m = arg min M$. Besides, one thought will not be selected as the rollback destination more than U times to avoid all subsequent thoughts rolling back to the same previous thought. Thus, when the number of rollbacks to a thought reaches U, the next earliest one of $m = arg min M \\{a:arg min M\\}$ will be selected.\nTherefore, when M is not empty, TR generates the next thought $z_m$ from $z_{m-1}$, meaning that a new reasoning path $[z_0...z_{m-1}, z_m]$ derived from the rollback $n \u2192 m$ is created for the thought structure. As the existing $z_{0...n}$ remains unchanged, the reasoning continues by generating $z_{0...n+1}$. For ease of description, we define $n \u2192 m$ as the outgoing rollback for $z_{0...n+1}$ and the incoming rollback for the new reasoning path $[z_0...z_{m-1}, z_m]$.\n4.3. Enhancing the Prompt with Errors as Experience\nThrough iterative rollback from the n-th to the m 1-th step, TR gains the opportunity to address the three scenarios outlined in subsection 3.2. However, as discussed in 3.3, regenerating a next thought $z_m$ based on the same prompt may repeat existing mistakes in the new reasoning path. Especially considering that TR is built upon the prompt containing no human annotations, the thought regeneration after the rollback is equivalent to randomly exploring $z_m$ as in unidirectional structures.\nTherefore, TR embraces prompt enhancer to also roll back the error analysis $A_n$ to the m 1-th thought. Unlike BoT (Sijia et al., 2024) with outcome analysis, which utilizes error feedback on the final result, TR performs process analysis, i.e., rollback-by-rollback verification, to get error reports on intermediate thoughts, guiding the subsequent thought generation. With error analysis, each rollback is regarded as a trial on generating subsequent thoughts for $z_{m-1}$ because the analysis contains a trial experience: what mistakes may appear in the following steps of $z_{m-1}$. By including $A_n$ as an experience in the prompt, LLMs can avoid making existing bad thoughts after learning from mistakes. Eventually, each rollback $n \u2192 m$ creates the error experience $A_n$.\nExperience accumulation. The thoughts $Z_{X^()}^{z_{0...9-1}} = \\{z|z\u2208 Z_{0...q-1}, z_i \u2209 Z_{0...q\u22121}, j \u2208 [0, q \u2212 1], i \u2208 x (j)\\}$ of a reasoning path $z_{0...q-1}$ may derive from multiple incoming rollbacks, where x (j) is the set of rollbacks whose destination is j-th thought of this path. As each rollback creates an error experience from one trial of the given question, incoming rollbacks lead to a series of experiences $A_{X^{()}q-1}$. By accumulating an ensemble of trial-and-error reasoning experiences as the in-context learning examples in the prompt, LLM will learn from more experiences to generate the correct next thought $z_{q} \\sim f(g|I (A_{X^()q-1}, z_{0...q-1}))$, as shown in Figure 2 and two examples of Figure 8 and Figure 10.\n4.4. Ensembling Solutions\nTR may create massive final solutions as each adaptive triggered rollback leads to one more new reasoning path toward answering the question. Thus, we directly stop reasoning when there is K number of solutions $\\{z_{0...Tk}\\}_{Sk=1}$ obtained. Then, weighted majority voting (W-Voting) will be performed on them for a final solution. Specifically, for the solution $z_T$, the weight $w_t$ is higher when 1) it has a lower number of outgoing rollbacks denoted as $T_e$, meaning that fewer bad thoughts are identified; and 2) more experiences $B_T = A_{XT} \\leftarrow z_{0.T}$ are accumulated along this reasoning path. Eventually, TR outputs the final solution as: $arg max_{v\u2208V} \u2211^K_{k=1} \u0399 (\u03c5_\u03ba = \u03c5) (\u03b2_{\u03c4_\u03ba} - \u03b1_{\u03c4_\u03ba})$, where $V$ is the collection of solutions and $\u03c5_k$ is the value of k-th solution."}, {"title": "5. Experiments", "content": "Datasets. We conduct experiments on two streams of tasks. For the mathematical problems, we evaluate the performance of TR on test sets of GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), AQUA-RAT (Ling et al., 2017), MATH (Hendrycks et al., 2021b), TheoremQA (Chen et al., 2023b) datasets, where numerical subscripts indicate sample size. For TheoremQA, we specifically use half of the test set without visual information, leading to 400 samples. Following ToT (Yao et al., 2023), we utilize 100 challenging games of Game of 24. For multi-task reasoning, such as symbolic reasoning, we extract 900 samples from 56 categories of MMLU (Hendrycks et al., 2021a), i.e. MMLU.\nLarge language models. We utilize GPT-3.5-turbo (gpt-3.5-turbo-16k-0613), GPT-4 (gpt-4-1106-preview) (OpenAI, 2023) and Llama2 (Touvron et al., 2023), including Llama2-13b (Llama-2-13b-chat-hf) and Llama2-70b (Llama-2-70b-chat-hf) where 1b means one billion parameters. For LLMs with TR, the default settings for temperature and top-p are 0.7.\nBaselines. Apart from zero-shot prompting (Kojima et al., 2022), the comparison approaches include Chain-of-thought (CoT) (Wei et al., 2022), SC (Wang et al., 2022) and Complex CoT (Fu et al., 2023) (C-CoT), where the subscript 5 or 8 indicates the number of shots while the subscript sc denotes the number of sample paths. Also, TR is compared with the related approaches, such as Boosting of thoughts (BoT) (Sijia et al., 2024), Tree of thoughts (ToT) (Yao et al., 2023), Cumulative Reasoning (CR) (Zhang et al., 2023), and Progressive-Hint Prompting (PHP) (Zheng et al., 2023). ToT follows the best first search (BFS). The breadth limit of ToT is 6 while BoT performs 10 boosting iterations on 15 binary trees. We also include the state-of-the-art (SOTA) methods, such as CSV (Zhou et al., 2024) that relies on GPT-4 Code Interpreter, on each dataset as an additional comparison. We set K = 8 for possible early stopping of TR in all experiments.\nMetrics. All experiments report the Solve Rate (%) of the questions. We make LLM explicitly report the solution value after the strings, such as \"The solution is\" and \"The choice is\" in the z0...T. Thus, the value is directly extracted and compared with the ground truth. The Interaction Number refers to the frequency at which we must consult the LLM until we receive conclusive responses."}, {"title": "5.1. Main Evaluation Results", "content": "Adaptive reasoning. With zero-shot prompting and no pre-defined thought structures, such as chain of Chain Reasoning and the tree of ToT Reasoning, TR allows GPT-3.5-turbo, GPT-4, and Llama2 to self-organize and explore thought structures toward answering the question. Under challenging tasks, LLMs with TR adaptively build complex structures, as shown by examples in the Appendix, by continuously rolling back from thoughts with \"hallucinations\". For simpler tasks, lightweight structures are built by LLMs with TR. As such, with the ability to adjust thoughts and prompt the LLMs with accumulated experience of errors during reasoning, TR achieves a high solving rate and relatively lower resource cost.\nOverall comparison. We show, especially in Table 1, that compared to existing multi-step reasoning approaches, LLMs with TR achieve the best and the second best solving rate on AQUA-RAT & MATH and GSM8K & SVAMP respectively. Meanwhile, contrary to the resource-cost SOTA ones, such as BoT, which undertakes reasoning through massive tree thought structures, and CSV, which relies on GPT-4 Code Interpreter, TR yields notable performance by interacting less with relatively simpler LLMs. First, TR surpasses BoT by 6.3 on AQUA-RAT and 9.45 on MATH using GPT-4. In particular, TR requires only around 40 interactions compared to the 500+ interactions of BoT. Using TR with zero-shot prompting, GPT-4 and GPT-3.5-turbo outperform the ones using few-shot CoT prompting and self-consistency. Under hard math problems, especially MATH, the solving rate of TR is 17.99 and 3.28 higher than PHP+C-CoT under GPT-4 and GPT-3.5-turbo, respectively. Second, LLMs with TR adaptively explore thought structures, which is significantly better than pre-defined forward-only Chain Reasoning and ToT Reasoning. After large-scale interactions with LLMs, the performance of the latter two zero-shot prompting methods only approaches the 8 shots Complex CoT. Figure 3 (a) shows that compared to ToT reasoning, our TR requires one-third or less of the interactions to achieve a new state-of-the-art. Finally, with an average 28 interactions with GPT-4, TR yields a competitive solving rate 87.56 on the multi-task dataset MMLU, which contains symbolic reasoning.\nWe emphasize that TR is more effective in challenging problems, as shown in Table 2 and Table 3. In the level 5 difficulty of MATH, GPT-4 with TR is only 2.84 lower than the CSV that embraces the GPT-4 Code Interpreter as an auxiliary. The solving rate of GPT-4+TR is 4.35 higher than the current best in TheoremQA. Along with better performance, interaction cost is reduced to an acceptable range by TR. Another observation is that LLMs with TR introduce more interactions in hard problems than the simpler ones. For instance, as shown in Figure 3 (a) and the first two columns of Table 2, the average interaction cost increases to around 60. Also, in Table 3, the results of the Game of 24 dataset show that GPT-4 with TR requires an average of 32 interactions to reach a solving rate of 87%, which is only 7% lower than the CR (Zhang et al., 2023). This is a remarkable achievement as CR-related approaches rely on human-made demonstrations while TR is zero-shot prompting. Moreover, introducing the CR's demonstrations into the prompt of TR increases the solving rate to 93% while reducing the number of interactions to 24. In addition, the better performance of TR + CR-Prompt shows that including demonstrations reduces the reliance on majority voting.\nEffect of the rollback of thoughts. In Figure 3 (b) and (c), we specifically present the relation between rollbacks and the solving rate of reasoning paths and the decrease in the failure rates at the first step of the Game of 24. We define a reasoning path $z_{0...T}$ as In Rollback if a majority of its thoughts, represented by $Z_X$ in the subsection 4.3, are derived from incoming rollbacks. $z_{0...T}$ is defined as Out Rollback if more than two of its thoughts trigger outgoing rollbacks and as No Rollback if it includes no rollbacks. Figure 3 (b) presents these three types of reasoning paths TR generates during reasoning in four datasets. As TR allows the error analysis of each rollback to be accumulated in the prompt, as discussed in subsection 4.3, a In Rollback path generally benefits from exploiting more experiences during reasoning. Therefore, as Figure 3 (b) verifies, In Rollback paths consistently correspond to a higher solving rate because of prompting LLMs with these trial-and-error experiences. On the contrary, those Out Rollback paths have significantly low solving rates because they include more bad thoughts (\u201challucinations\u201d), which consequently trigger more rollbacks after being identified by LLMs. Similarly, when the first step of Game of 24 derives from the number of 0 to 5 incoming rollbacks, the failing rate decreases significantly from higher than 0.8 to lower than 0.3. The final observation is that longer spans of rolling back are more important for revising thoughts. Figure 3 (c) shows that the first step caused by a rollback 3 \u2192 0 has a higher success rate than the one caused by 2 \u2192 0 and 1 \u2192 0. Therefore, we argue that a rollback, especially generated from more latter reasoning steps (thoughts), contribute more to the thought revision. This may be because the error analysis brought by rollbacks of later reasoning steps contains more information and is more helpful in improving prompts."}, {"title": "Concerns.", "content": "First, TR contributes less to the performance enhancement when the LLMs inherently do not have solid ability. Especially in AQUA-RAT and MATH of Table 1, GPT-3.5-turbo with TR is 2.91 and 3.25 higher than PHP+C-CoT, which are significantly lower than these under GPT-4. Likewise, in Table 2, the solving rate of Llama2-70b with TR is around 4% higher than Llama2-70b but costs more interactions. Therefore, the performance of TR depends on LLMs' abilities to perform correct error analysis and understand the experience in the prompt. Second, to address harder problems, LLMs with TR tend to build over-complex thought structures, which generally contain more than 100 thoughts. The main reason is that a rollback generated by bad thoughts identified by LLMs or a mistaken rollback caused by hallucinations of LLMs leads to one more reasoning path. This appears frequently in challenging tasks; thus, LLMs self-organize a large-scale thought structure toward solutions. We present more visible examples in Figure 4, Figure 9, and Figure 10 of the Appendix.\n5.2. Main Insights\nThe notable performance enhancement of TR in terms of both solving rate and interaction cost shows the insight that adaptive adjusting thoughts supported by the rollback of thoughts during reasoning is core to the success of LLMs in complex mathematical reasoning. In addition, we can gain three more insights.\nExperience accumulation of error analysis from intermediate thoughts is better than that obtained by analyzing the whole reasoning path. Existing work (Huang et al., 2024) pointed out that LLMs are unable to revise reasoning based on the outcome analysis, which gives feedback on the final reasoning. Thus, BoT (Sijia et al., 2024), which relies on outcome analysis, had to embrace more careful-selected outcome analysis to prompt LLMs. Our TR opens a new direction of relying on process analysis, which provides error analysis for each intermediate reasoning step (rollback-by-rollback verification), to revise thought adaptively during reasoning. Besides, with the rollback of thought, outcome analysis is a special case of process analysis when analysis is used not to re-do reasoning but to adjust previous thought to create a correct reasoning path.\nExperience-guided Solution Ensemble is critical to the effectiveness of trial-and-error reasoning. After stopping reasoning, LLMs with TR yield K reasoning paths due to the adaptive exploration. Each reasoning path caused by one rollback of TR can be regarded as a trial for addressing the problem. When LLMs frequently make mistakes and occur \u201challucinations\u201d, the solution obtained in any trial may not be correct. Since TR exploits the error analysis of each incoming rollback as experience to prompt LLMs, a solution from the reasoning path with more incoming rollback is more acceptable. Therefore, we should ensemble these solutions by filtering out ones with limited experiences or many bad thoughts. As shown by comparisons between TR and TR + W-Voting in Table 1, Table 2, Table 3 and Figure 3 (b), such an experience-guided solution ensemble is critical.\nWeak LLMs may not identify multiple targets mentioned in the prompt. Including CoT examples in the prompt improves the solving rate of LLMs with TR, as shown in the GPT-4 column of Table 1. However, for the more challenging AQUA-RAT and MATH datasets in the GPT-3.5-turbo column, adding CoT causes a significant performance decrease. We argue that it may be hard for the weak LLMs to understand and distinguish instructions with different targets in the prompt. For example, the instruction of CoT examples emphasizes how to follow demonstrations, while the prompt with experiences in TR focuses on how to avoid given errors. Weak LLMs may not benefit from the enhanced prompt containing these two different guidances, especially when the reasoning is complex."}, {"title": "5.3. Token Cost Analysis", "content": "As shown in Table 4, when addressing questions from challenging datasets, the token cost of GPT-4 with TR is significantly higher than baseline approaches. Specifically, on the MATH dataset, the TR approach, on average, generates 35.26 times more tokens and requires 425.25 times more prompt tokens than the ZeroShot CoT. The corresponding ratios in the TheoremQA dataset are 28.09 and 358.46. This resource-intensive nature of our proposed TR derives from continuously identifying the errors and appending the prompt with error analysis during reasoning.\nHowever, these additional operations and the high token cost are necessary as hallucinations of LLMs frequently appear. First, since numerous erroneous thoughts are generated during reasoning, consistently identifying and revising them is crucial to ensuring the correctness of the answer. Second, in many cases, the error analysis of LLMs is invalid and even incorrect due to the inevitable hallucinations. Thus, accumulating error analysis derived from different reasoning paths decreases the negative impact of the flawed analysis on thought revisions. Third, many reasoning paths of the thought structure derive from rollbacks triggered by erroneous feedback from LLMs. Since mistaken rollbacks cannot be identified, the TR approach retains all generated paths and ultimately employs majority voting to enhance reliability.\nTherefore, we conclude that there is a trade-off between the high token cost and the problem-solving rate. On the one hand, since the TR approach requires many tokens to address a single question, its application may be limited for users with insufficient resources. On the other hand, compared to zero-shot GPT-4, GPT-4 with TR gains 27.19% and 13% improvements in solving rates on MATH and TheoremQA datasets. When users prioritize problem-solving rates, integrating GPT-4 with the TR approach ensures its applicability in many challenging scenarios."}, {"title": "6. Concluding Remarks", "content": "In this paper, we proposed Thought Rollback (TR), an effective reasoning framework supported by the rollback of thoughts that allows LLMs to perform adaptive reasoning to solve challenging problems. Without relying on human annotations and specific thought structure designs for reasoning, LLMs with TR can progressively self-organize and revise thoughts based on trial-and-error experiences until reaching a correct solution for various tasks. The rollback controller and prompt enhancer, together with the experience-guided weights majority voting, make TR achieve the state-of-the-art solving rate in many mathematical and multi-task reasoning datasets while maintaining a lower cost than the alternative leading approaches. We hope this work could shed light on the adaptive reasoning in LLMs toward addressing challenging tasks, especially when mathematical problems are involved."}, {"title": "Impact Statement", "content": ""}]}