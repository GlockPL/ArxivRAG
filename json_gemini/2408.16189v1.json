{"title": "A MORE UNIFIED THEORY OF TRANSFER LEARNING", "authors": ["Steve Hanneke", "Samory Kpotufe"], "abstract": "We show that some basic moduli of continuity \u03b4\u2014which measure how fast target risk decreases as source risk decreases-appear to be at the root of many of the classical relatedness measures in transfer learning and related literature. Namely, bounds in terms of \u03b4 recover many of the existing bounds in terms of other measures of relatedness-both in regression and classification\u2014and can at times be tighter.\nWe are particularly interested in general situations where the learner has access to both source data and some or no target data. The unified perspective allowed by the moduli \u03b4 allow us to extend many existing notions of relatedness at once to these scenarios involving target data: interestingly, while \u03b4 itself might not be efficiently estimated, adaptive procedures exist-based on reductions to confidence sets-which can get nearly tight rates in terms of \u03b4 with no prior distributional knowledge. Such adaptivity to unknown \u03b4 immediately implies adaptivity to many classical relatedness notions, in terms of combined source and target samples' sizes.", "sections": [{"title": "Introduction", "content": "Domain Adaptation or Transfer Learning refer generally to the problem of harnessing data from a source distribution P to improve prediction performance w.r.t. to a target distribution Q for which some or no data is available. This problem has been researched over the last few decades with a recent resurgence in interest driven by modern applications that are often characterized by a scarcity of perfect target data.\nA fundamental question in the theory of domain adaptation (and variant problems on distribution shifts) is how to measure the relatedness between source P and target Q distributions. Importantly, desired measures of relatedness should not only tightly capture the predictive information P has on Q, but have to be practically useful: that is, either the measure can be estimated from data to facilitate algorithmic design, or more generally, it should somehow admit adaptive procedures, i.e., procedures whose performance is adaptive to the a priori unknown level of relatedness between P and Q. Many notions have been proposed over the last few decades, starting with the seminal works of Mansour et al. [2009], Ben-David et al. [2010] on refinements of total-variation for domain adaptation in classification, to more recent proposals for domain adaptation in regression, e.g., Wasserstein distances Redko et al. [2017], Shen et al. [2018], or measures relating covariance structures across P and Q as in Mousavi Kalan et al. [2020], Zhang et al. [2022b], Ge et al. [2023]. These various notions of relatedness appear hard to compare at first glance, leading to a disparate theory of domain adaptation at present with no unified set of principles.\nInterestingly as we show, upon closer look at the existing literature\u2014whether in classification or regression\u2014it turns out that in fact, many seemingly distinct measures of relatedness proposed in domain adaptation actually implicitly bound the same fundamental quantities: we refer to these quantities as weak and strong moduli of transfer, and they roughly measure how fast the Q-risk of predictors decrease as their P-risk decreases. These moduli always yield as tight or tighter rates of transfer than many existing notions, while also admitting adaptive procedures in general settings, as shown via a reduction to the existence of certain confidence sets for the prediction problem at hand. These reductions, while of a theoretical nature, yield insights on general adaptive transfer approaches that are less tied to specific measures of relatedness between source P and target Q."}, {"title": "Preliminaries", "content": "Basic Definitions. Let X, Y be jointly distributed according to some measure \u03bc (later P or Q), where X is in some domain X and Y \u2208 Y. A hypothesis class is a set H of measurable functions X \u2192 Y. Given a loss function l : Y2 \u2192 R+, we consider risks R\u03bc(h) = E\u03bc l(h(X), Y), as measured under a given \u03bc.\nAssumption 1 (Finiteness). We assume throughout that R\u03bc(h) < \u221e,\u2200h \u2208 H, and for measures \u03bc considered.\nNote that the above is a mild assumption, as it does not require the loss to be uniformly bounded. For instance this admits the squared loss in linear regression.\nFor example, the case of binary classification corresponds to y = {\u00b11}, where we often choose l(y, y') = 1{y \u2260 y'}, and results below will depend on the VC dimension or Rademacher complexity of the class H (see, e.g., Vapnik and Chervonenkis [1971], Koltchinskii [2006]). As another example, the case of regression corresponds to y CR, and we may choose l(y, y') = (y \u2013 y')2, and results below may depend, for instance, on the covering numbers or pseudo-dimension of the class H (see, e.g. Anthony and Bartlett [1999]).\nRemark 1. Our general results will in fact capture quite abstract dependences on H, via an abstractly-defined notion of confidence sets, and any notion of complexity that allows for such confidence sets are therefore admissible."}, {"title": "Moduli of Transfer", "content": "As we will argue in Section 3, the above simple definition already captures the bulk of notions of discrepancies proposed in the literature on transfer learning.\nThe first notion considered below serves to capture the reduction in target Q-risk induced by small source P-risk, i.e., by a potentially large amount of P data. In particular, as we will see, e is to stand for the best risk achievable under P given a fixed amount of data from P."}, {"title": "Weak Modulus of Transfer", "content": "Here we consider a few examples of existing notions of discrepancy between source and target P, Q, and illustrate the types of bounds they imply on \u03b4(\u03b5). These bounds were already implicit in past work, even while the weak modulus \u03b4(\u03b5) was never explicitly defined as the main object of study, or as the implied notion of discrepancy."}, {"title": "Some Discrepancies in Classification", "content": "We first remark that some of the notions below can be stated generally beyond classification, e.g., the V-discrepancy and the transfer-exponent, however they usually appear in works on classification. In what follows assume l(a, b) = 1{a \u2260 b} and for the simplest case suppose Y = {-1,1} (though the claims are valid for any Y)."}, {"title": "Some Discrepancies in Regression", "content": "We note that, while we now focus on regression, i.e., h has continous output, the discussion below may be extended to classification in the usual way by viewing l(a, b) as a surrogate loss (and the classifier as sign(h) for example)."}, {"title": "Adaptive Transfer Upper-Bounds", "content": "We start with the following short notation for empirical risk minimizers over source or target samples Sp or SQ."}, {"title": "Examples of Weak Confidence Sets", "content": "Throughout this section, we let u generically denote P or Q, and recall the ERM \u0125\u03bc. Also, we define excess empirical risk as \u00ca\u00b5(h) = \u0154\u03bc(h) \u2013 infh'\u2208\u043d\u0125\u03bc(h')."}, {"title": "Classification with 0-1 loss", "content": "We first remark that (6, 7)-weak confidence, for \u0454 = O(n1/2) are easy to obtain for VC classes H. In fact, from Proposition 6 of the later Section 4.1, a simple set of the form \u0124 = { h \u2208 H : \u00ca\u00b5(h) \u2264 n 1/2} is a strong-confidence set (see Definition 10).\nThis section is dedicated to obtaining as tight a weak-confidence set as possible, while we remark that\nAssume here that y = {\u22121,1} and l(y, y') = 1{y \u2260 y'}. In what follows, we use the short-hand notation \u00b5(h \u2260 h') = \u03bc\u03c7 ({x \u2208 X : h(x) \u2260 h'(x)}), and for H' CH, define"}, {"title": "Regression with Squared Loss", "content": "We consider linear regression where H = {x \u2192 hw(x) = w\u00afx : w\u2208 Rd} and the squared loss l(y, y') = (y \u2212 y')2.\nDefine \u2211 = EXXT, assumed invertible. Also define \u2211 = EXXT, where \u017f\u00fb is the empirical version of \u03bc."}, {"title": "Lower-Bounds", "content": "Let l(a, b) = 1{a \u2260 b} and Y = {\u22121,1}."}, {"title": "Strong Modulus of Transfer", "content": "Let \u03bc denote either P or Q. Let 0 < r < 1, \u0454 > 0, and C > 1. We call a random set \u0124\u03bc = \u0124\u03bc(S\u03bc) an (e, T, C)-strong confidence set (under \u00b5) if the following conditions are met with probability at least 1"}, {"title": "Examples of Strong Confidence Sets", "content": "We present some examples of strong confidence sets for classical problems in classification and regression. We note in particular that, while Theorem 3 requires two such confidence sets for different values of e, the constructions below are for a single e but are stated in a way to make it clear that different values of e are admissible by varying constant factors in their definitions."}, {"title": "Classification Example", "content": "We consider classification with a VC class H. In what follows assume l(a, b) = 1{a \u2260 b} and for the simplest case suppose y = {\u22121,1}. We first note that a simple strong confidence set follows easily from usual \u221an results in this setting as stated in the firt proposition below. However, this corresponds to assuming \u03b2\u03bc = 0 (noise condition of Definition8); thus, for unknown \u03b2\u03bc, more sophisticated construction is required to achieve the corresponding fast rates. Such a construction is subsequently described."}, {"title": "Regression Example", "content": "We consider a linear regression setting with X \u2208 Rd and Y \u2208 R, jointly distributed under \u03bc. Assume throughout this section that ||X|| \u2264 1, and Y \u2212E[Y|x] is uniformly subGaussian with parameter \u03c3\u00b2 \u2265 1; see Condition 3 of Hsu et al. [2012]. We consider the squared loss l(y, y') = (y \u2212 y')\u00b2, and H = {x \u2192 hw(x) = w\u00afx : w\u2208 Rd}.\nDefine \u03a3 = \u0395\u03bcXXT, assumed invertible, and \u2211 = E\u00fbXXT, where \u017f\u00fb is the emprirical version of \u03bc on S\u03bc ~ \u03bc\u03b7\u03bd.\nWe make use of the following corollary to classical concentration results."}]}