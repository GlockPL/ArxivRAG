{"title": "FIT-Print: Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint", "authors": ["Shuo Shao", "Haozhe Zhu", "Hongwei Yao", "Yiming Li", "Tianwei Zhang", "Zhan Qin", "Kui Ren"], "abstract": "Model fingerprinting is a widely adopted approach to safeguard the intellectual property rights of open-source models by preventing their unauthorized reuse. It is promising and convenient since it does not necessitate modifying the protected model. In this paper, we revisit existing fingerprinting methods and reveal that they are vulnerable to false claim attacks where adversaries falsely assert ownership of any third-party model. We demonstrate that this vulnerability mostly stems from their untargeted nature, where they generally compare the outputs of given samples on different models instead of the similarities to specific references. Motivated by these findings, we propose a targeted fingerprinting paradigm (i.e., FIT-Print) to counteract false claim attacks. Specifically, FIT-Print transforms the fingerprint into a targeted signature via optimization. Building on the principles of FIT-Print, we develop bit-wise and list-wise black-box model fingerprinting methods, i.e., FIT-ModelDiff and FIT-LIME, which exploit the distance between model outputs and the feature attribution of specific samples as the fingerprint, respectively. Extensive experiments on benchmark models and datasets verify the effectiveness, conferrability, and resistance to false claim attacks of our FIT-Print.", "sections": [{"title": "1. Introduction", "content": "Deep learning models, especially deep neural networks (DNNs), have been widely and successfully deployed in widespread applications. In general, obtaining a well-performed model requires considerable computational resources and human expertise and is, therefore, highly expensive. In particular, some models are released to the open-source community (e.g., Hugging Face) for academic or educational purposes. However, the development of model reuse techniques, such as fine tuning and transfer learning, poses a potential threat to the intellectual property rights (IPR) of these models. With these methods, malicious developers can easily reuse open-source models for commercial purposes without authorization. How to protect their IPR becomes a vital problem.\nCurrently, ownership verification stands as a widely adopted post-hoc approach for safeguarding the IPR of model developers. This method intends to justify whether a suspicious third-party model has been reused from the protected model. Existing techniques to implement ownership verification can be broadly categorized into two main types: model watermarking and model fingerprinting. Model watermarking involves embedding an owner-specific signature (i.e., watermark) into the model. The model developer can extract the watermark inside the model to verify its ownership. On the contrary, model fingerprinting aims to identify the intrinsic feature (i.e., fingerprint) of the model instead of modifying the protected model. The fingerprint can be represented as the outputs of some testing samples at a particular mapping function. Comparing fingerprints enables comparing the source and suspicious models to examine whether the latter is a reused version of the former. Arguably, model fingerprinting is more convenient and feasible than model watermarking since the former does not necessitate any alteration to the parameters, structure, and training procedure and thus has no negative impact on the model.\nIn this paper, we reveal that existing model fingerprinting methods, no matter whether they are bit-wise or list-wise (i.e., extract the fingerprint bit by bit or as a whole list), are vulnerable to false claim attacks. In general, false claim attacks allow adversaries to falsely assert to have the ownership of third-party models that is not a reused model by creating a counterfeit ownership certificate (i.e., watermark or fingerprint)."}, {"title": "2. Revisiting Existing Model Fingerprinting", "content": "In this section, we first formally and comprehensively define the threat model of model fingerprinting. We further categorize existing methods into two types and describe their formulation. Subsequently, based on the aforementioned definition and formulation, we design a simple yet effective false claim attack and reveal the underlying vulnerability of existing fingerprinting methods."}, {"title": "2.1. Threat Model of Model Fingerprinting", "content": "In this paper, we consider three parties in our threat model, including model developer, model reuser, and verifier. Arguably, including a verifier is necessary and may improve the trustworthiness of model fingerprinting, although there are currently still no mature legal provisions about this. More justification of our threat model is in Appendix A.\nAssumptions of the Model Developer and Verifier. The model developer is the owner of the source model and can register its model and fingerprint to the trustworthy verifier with a timestamp. The verifier is responsible for fingerprint registration and verification. In case the model is reused by a model reuser, the model developer can ask the verifier for ownership verification. In particular, if two parties can simultaneously provide fingerprints and verify the ownership of a model, the fingerprint with a later timestamp will be deemed invalid. The model developer and the verifier are assumed to have (1) white-box access to its source model and (2) black-box access to the suspicious model.\nAssumptions of the Model Reuser. Model reusers aim to avoid having their authorized reuse detected by the verifier. To achieve this, they can first modify the victim model via various techniques, such as fine-tuning, pruning, transfer learning, and model extraction, before deployment."}, {"title": "2.2. The Formulation of Existing Fingerprinting", "content": "In this section, we outline the formulations of existing fingerprinting methods to aid in the analysis and design process in the subsequent sections of this paper and follow-up research. In this paper, we focus on black-box methods since they are more practical in the real world. In general, existing black-box model fingerprinting methods can be categorized into two types: adversarial example-based (AE-based) fingerprinting methods and testing-based fingerprinting methods. We also include a broader discussion about other fingerprinting methods in Appendix L.\nAE-based Fingerprinting Methods. AE-based fingerprinting methods assume that the independent model has a unique decision boundary. Based on this assumption, they exploit adversarial examples (AE) to characterize the properties of the decision boundary of a model. AE-based fingerprinting methods validate whether the AEs are misclassified by the source model and the suspicious model. If so, the suspicious model can be treated as a reused version of the source model. The proposition tested in AE-based methods can be formulated as follows.\nProposition 1 (Ownership Verification of AE-based Fingerprinting Methods). Let $M_O$ be the source model and $M_S$ be the suspicious model, and $g(x)$ is the function that always outputs the ground-truth label of any input data $x$. If for any testing sample $x \\in X_T$ ($X_T$ denotes the set of the testing samples), we have\n$M_O(x) = M(x) \\neq g(x)$,\nthe suspicious model $M_S$ can be asserted as a reused version of the source model $M_O$.\nTesting-based Fingerprinting Methods. Testing-based fingerprinting methods aim to compare the suspicious model with the source model on a specific mapping function $f(.)$. If the outputs are similar, the suspicious model can be regarded as being reused from the source model. As such, the core of testing-based fingerprinting methods is how to design the mapping function $f(.)$. The proposition used in testing-based methods can be formulated as follows.\nProposition 2 (Ownership Verification of Testing-based Fingerprinting Methods). Let $M_O$ be the source model and $M_S$ be the suspicious model. If for a specific mapping function $f(.)$ and any testing sample $x \\in X_T$ ($X_T$ is the set of the testing samples), we have\n$\\frac{1}{|X_T|} \\sum_{x \\in X_T} dist(f[M_O(x)], f[M_S(x)]) \\leq \\tau$,\nwhere $\\tau$ is a small positive threshold and $dist(\u00b7,\u00b7)$ is a distance function, the suspicious model $M_S$ can be asserted as reused from the source model $M_O$."}, {"title": "2.3. False Claim Attack against Model Fingerprinting", "content": "Existing model fingerprinting methods primarily assume that the model reuser is the adversary while paying little attention to the false claim attack where the model developer is the adversary. The formal definition of the false claim attack is as follows.\nDefinition 1. A false claim attack refers to a malicious attempt by a malicious model developer to falsely assert the ownership of an independent model $M_I$ by registering some fraudulent testing samples $x$ that can pass the ownership verification of Proposition 1 or Proposition 2.\nThe detailed process of false claim attacks is in Appendix A. Some terms (e.g., ambiguity attack and false positive rate) may have a similar definition to the false claim attack. We clarify their differences in Appendix M. Since registering the fingerprint with a timestamp can prevent any false claim after registration, its success hinges on generating a transferable fingerprint. For AE-based methods, have successfully implemented the false claim attacks by constructing transferable AEs. As such, we hereby mainly focus on designing false claim attacks against cutting-edge testing-based methods. Our primary insight is to craft inverse-AEs x which can be 'easily' classified, leading to\n$M_O(x) \\approx M_I(x)$\n$\\Rightarrow dist(f[M_O(x)], f[M_I(x)]) \\approx 0 \\leq \\tau$.\nTo execute this strategy, motivated by fast gradient sign method (FGSM) for AE generation, we propose to leverage Eq. (4) to generate malicious fingerprinting samples, as follows:\n$x' = x - \\gamma \\cdot sign(\\nabla_x J(M_O, x, y))$,\nwhere $sign(.)$ denotes the sign function, $J(.)$ represents the loss function associated with the original task of $M_O$, and $\\gamma$ signifies the magnitude of the perturbation. More powerful transferable adversarial attacks can be exploited here but we aim to show that using simple FGSM can also falsely claim to have ownership of some independent models.\nResults. We exploit 3 representative testing-based methods, i.e., ModelDiff, Zest and SAC to validate the effectiveness of our false claim attacks. The complete results can be found in Appendix E. As shown in Table 1, SAC is poor at identifying"}, {"title": "3. The Proposed Method", "content": "The objectives of model fingerprinting methods can be summarized in three-fold, including effectiveness, conferrability, and resistance to false claim attacks.\n\u2022 Effectiveness: Effectiveness means that the model developer can successfully verify the ownership of the source model through the model fingerprinting method.\n\u2022 Conferrability: Conferrability is defined to ensure that the model fingerprint needs to be conferable to the models that are reused from the source model. In other words, the fingerprints of the reused models and the source model need to be similar.\n\u2022 Resistance to False Claim Attacks: It requires that the fingerprints of independently trained models need to be different. Also, a malicious model developer cannot construct a transferable fingerprint that can be extracted from independently trained models."}, {"title": "3.2. The Insight of our FIT-Print", "content": "As discussed in Section 2.3, existing model fingerprinting methods are vulnerable to false claim attacks. We argue that the vulnerability stems primarily from the 'untargeted' characteristic of the fingerprinting methods. The untargeted characteristic leads to a large fingerprint space that can accommodate transferable adversarial fingerprints. In this paper, we propose FIT-Print, a targeted model fingerprinting framework to mitigate false claim attacks. Our main insight is that although it is tough to find the space that can only transfer among reused models, we can turn the fingerprint into a target one to restrict the fingerprinting space and reduce the adversarial transferability of fingerprints.\nGiven a mapping function $f(\u00b7)$ and a target fingerprint $F$, our goal is to make the fingerprint vector $v = f(M_S(x))$ to be close to F. Accordingly, the proposition of our FIT-Print can be defined as follows.\nProposition 3. Let $M_S$ be the suspicious model. If for a specific mapping function $f(\u00b7)$ and testing sample $x \\in X_T$ ($X_T$ is the set of the testing samples), we have\n$\\frac{1}{|X_T|} \\sum_{x \\in X_T} dist(f[M_S(x)], F) \\leq \\tau$,\nwhere $\\tau$ is a small threshold and $dist(\u00b7,\u00b7)$ is a distance function, the suspicious model $M_S$ can be asserted as reused from the owner of the fingerprint F.\nIn FIT-Print, we assume that the target fingerprint $F \\in \\{-1, 1\\}^k$ is a binary vector consisting of \u22121 or 1, and we can get the output logits of $M_S(x)$. The discussion on the label-only scenario where we can only get the Top-1 label can be found in Appendix G. We assume that the target fingerprint cannot be arbitrarily chosen and needs to be registered with a third-party institution. As shown in Fig. 2, FIT-Print can be divided into two stages: testing sample extraction and ownership verification. The technical details are described as follows."}, {"title": "3.3. Testing Sample Extraction", "content": "In the testing sample extraction stage, we aim to find the optimal testing sample set $X_T$ to make any reused models satisfy Eq. (5) in Proposition 3. Therefore, in FIT-Print, we first initialize the testing samples $X_T$ and the corresponding perturbations $R$. We denote the i-th element in $X_T$ and $R$ as $x_i$ and $r_i$ respectively. The element $x_i$ is set to an initial value $x$ and we can initialize the testing samples to any images. The testing samples in $X_T$ can be constructed by adding the perturbations to the initial values, i.e., $x_i = x + r_i$. After that, we need to optimize the perturbations $R$ to make the fingerprint vector $v$ close to the target fingerprint $F$. We can define the testing sample extraction as an optimization problem, which can be formalized as follows.\n$R=\\{min\\r_i\\}\\_{i=1}^{X_T} [\\frac{1}{|X_T|}\\sum_{i=1}^{|X_T|} [L(f(M_O(x+r_i), F)+\\lambda\\cdot||r_i||_2],$"}, {"title": "3.4. Ownership Verification", "content": "In the ownership verification stage, given a suspicious model $M_S$, FIT-Print examines whether the suspicious model $M_S$ is reused from the source model $M_O$ by justifying whether $M_S$ satisfies Eq. (5). Specifically, we first calculate the fingerprint vector $v$ of the suspicious model $M_S$ using the extracted testing samples in $X_T$. Each element $v_i = f(M_S(x + r_i))$. Since optimizing Eq. (8) makes the signs of the fingerprint vector $v$ represent the fingerprint F of the model, we need to transform $v$ into a binary vector by applying the sign function $sign(\u00b7)$ to get F, as Eq. (9).\n$\\hat{F} = sign(\\tilde{v_i}) = \\begin{cases} 1, & \\tilde{v_i} \\geq 0 \\\\ -1, & \\tilde{v_i} < 0 \\end{cases}$\nSubsequently, we leverage the bit error rate (BER) as the distance function $dist(\u00b7)$ in Eq. (5) and the BER is the distance between the extracted fingerprint and the target fingerprint, as follows.\n$BER = \\frac{1}{k} \\sum_{i=1}^{k} I\\{F_i \\neq \\hat{F_i}\\}$,\nwhere k is the length of the fingerprint and $I\\{\u00b7\\}$ is the indicator function. As Proposition 3, if the BER is lower than the threshold \u03c4, the suspicious model $M_S$ can be asserted as a reused model. For choosing the threshold \u03c4 to reduce false alarm and resist false claim attacks, we have Theorem 1.\nTheorem 1. Given the security parameter \u043a and the fingerprint $F \\in \\{-1, 1\\}^k$, if \u03c4 + satisfy that\n$\\sum_{d=0}^{\\lfloor \\tau k \\rfloor} \\binom{k}{d} (\\frac{1}{2})^k \\leq \\kappa$,\nwhere $\\binom{k}{d} = k!/[d!(k -d)!]$, the probability of a false alarm, i.e., the BER is less than \u03c4 with random testing samples, is less than \u043a.\nThe proof of Theorem 1 can be found in Appendix B. We also conduct an empirical evaluation on the resistance of FIT-Print against adaptive false claim attacks in Section 4.4."}, {"title": "3.5. Designing the Mapping Function in FIT-Print", "content": "In Section 3.3-3.4, we introduced the main pipeline and paradigm of our FIT-Print. The key to implementing FIT-Print is to design the mapping function $f(\u00b7)$. We hereby illustrate how to leverage the paradigm of FIT-Print and design two targeted model fingerprinting methods, including FIT-ModelDiff and FIT-LIME, as the representatives of bit-wise and list-wise methods, respectively."}, {"title": "3.5.1. FIT-MODELDIFF", "content": "FIT-ModelDiff is a bit-wise fingerprinting method that extracts the fingerprint bit by bit. The main insight of FIT-ModelDiff is to compare the distance between the output logits of perturbed samples $x + r_i$ and benign samples $x$. The vector of the distances is called the decision distance vector (DDV). Given the suspicious model $M_S$, DDV can be calculated as follows:\n$DDV_i = cos\\_sim(M_S(x + r_i), M_S(x)) = \\frac{M_S(x + r_i) \u00b7 M_S(x)}{||M_S(x + r_i)||\u00b7 ||M_S(x)||}$,\nwhere DDV represents the i-th element in the DDV and cos_sim(,) is the cosine similarity function. Since the output logits after softmax is always positive, the range of the DDV is [0, 1]. As proposed in Section 3.3, we aim to make the sign of the fingerprint vector v to be the same as the target fingerprint F. Therefore, to achieve this goal, we need to subtract a factor from DDV to make the range of v including both positive and negative values, as Eq. (13).\n$v_i = f(M_S(x + r_i), M_S(x)) = DDV_i - cos(\\alpha) = \\frac{M_S(x+r_i)\u00b7 M_S(x)}{||M_S(x+r_i)||\u00b7 ||M_S(x)||} - cos(\\alpha)$,\nwhere cos(\u00b7) is the cosine function and \u03b1 is the bias parameter. The final fingerprint vector v can be used for testing sample extraction or ownership verification."}, {"title": "3.5.2. FIT-LIME", "content": "FIT-LIME is a list-wise method that extracts the fingerprint as a whole list. FIT-LIME implements the mapping function $f(\u00b7)$ via a popular feature attribution algorithm, local interpretable model-agnostic explanation (LIME). LIME outputs a real-value importance score for each feature in the input sample x. We enhance the LIME algorithm and develop FIT-LIME to better cater to the needs of ownership verification. The details of FIT-LIME are elaborated as follows.\nThe first step of FIT-LIME is to generate c samples that are neighboring to the input image x. We also gather the adjacent pixels in the image into a superpixel. We uniformly segment the input space into k superpixels, where k is the length of the targeted fingerprint. Assuming that k = \u03bc\u00d7 v, the image can be divided into \u03bc rows and v columns. Then, we randomly generate c masks where each mask is a k-dimension binary vector, constituting $A \\in \\{0, 1\\}^{c\\times k}$. Each element in each row of the matrix A corresponds to a superpixel in the image x. After that, we exploit the binary matrix to mask the image x and generate the masked examples $X_m$. If the element in the i-th row of the mask A is 1, the corresponding superpixel preserves its original value. Otherwise, the superpixel is aligned with 0. Each row of the mask can generate a masked sample and the c masked samples constitute the masked sample set $X_m$.\nThe second step is to evaluate the output of the masked samples $X_m$ on the suspicious model $M_S$. Different from primitive LIME, we utilize the entropy of the outputs so that it no longer depends on the label of x. The intuition is that if the important features are masked, the prediction entropy will significantly increase. Following this insight, we calculate the following equation in this step.\n$p_i = H[M(X_m)]$,\nwhere H() calculates the entropy, $p_i$ is the i-th element in p, and $X_m$ is the i-th masked samples.\nAfter that, the final step is to fit a linear model and calculate the importance score of each superpixel. The importance scores can be calculated via Eq. (15). The importance score vector will be used as the fingerprint vector v in testing sample extraction and ownership verification.\n$v = (A^T A)^{-1} A^T p$."}, {"title": "4. Experiments", "content": "In this section, we evaluate the effectiveness, conferrability, and resistance to the false claim attack of our FIT-Print methods. We also include ablation studies on the hyperparameters in FIT-Print. More experiments about the resistance to adaptive attacks, FIT-Print with different targets, initializations, and different numbers of augmented models are shown in Appendix F. We also discuss applying FIT-Print in the label-only scenario and to other models and datasets in Appendix G and J. The analysis of the overhead of FIT-ModelDiff and FIT-LIME can be found in Appendix H."}, {"title": "4.1. Experimental Settings", "content": "Models and Datasets. Following prior works, we utilize two widely-used convolutional neural network (CNN) architectures, MobileNetV2 (mbnetv2 for short) and ResNet18, in our experiments. We train MobileNetv2 and ResNet18 using two different datasets, Oxford Flowers 102 (Flowers102) and Stanford Dogs 120 (SDogs120), in total 4 source models. We primarily focus on image classification models in our experiments. In particular, we provide a case study about implementing FIT-Print to text generation models in Appendix J.3.\nModel Reuse Techniques. We evaluate FIT-Print against the following five categories of model reuse techniques, including copying, fine-tuning, pruning, model extraction, and transfer learning. We further consider different implementations of these model reuse techniques in various settings and scenarios. For each source model, we train and craft three fine-tuning models, three pruning models, two extraction models, and three transfer learning models. These 12 models constitute the set of reused models. When experimenting on one source model, the other 36 models that are reused from other source models are treated as independent models. More details can be found in Appendix C.\nBaseline Methods. We consider both AE-based and testing-based fingerprinting methods. For the former, we implement two typical methods, IPGuard and MetaV. While for the latter, we take three different methods, ModelDiff, Zest, and SAC as the baseline methods. We also include a state-of-the-art (SOTA) white-box model fingerprinting method, i.e., ModelGiF, for reference.\nTarget Fingerprint. As default, we select a logo of a file and a pen as the targeted fingerprint F. We set the default length k of the fingerprint F to be 256 and thus F is resized to 16\u00d716. We set the security parameter $\u03ba = 10^{-9}$. According to Eq. (11), the threshold \u03c4 is 0.316 in our experiments."}, {"title": "4.2. Evaluation on Effectiveness and Conferrability", "content": "Table 2 illustrates the percentage of successfully identified reused models (ownership verification rate). Both FIT-ModelDiff and FIT-LIME can recognize the reused models under five reuse techniques with 100% ownership verification rates, which outperform existing fingerprinting methods and perform on par with the SOTA white-box method, ModelGiF. Also, FIT-ModelDiff and FIT-LIME achieve 0.0% ownership verification rates on the independent models, indicating that our methods do not lead to false alarms. Fig. 3 illustrates the BERs of the reused models, which are all less than the threshold T with a maximum of 0.227. The results validate the effectiveness and conferrability of FIT-Print."}, {"title": "4.3. Ablation Study", "content": ""}, {"title": "4.3.1. Effects of the Length of the Fingerprint", "content": "In this experiment, we investigate the impact of varying lengths of the fingerprint F. In addition to the default length of 256 = 16 \u00d7 16, we set the length to be 12 \u00d7 12, 20 \u00d7 20, and 24 \u00d7 24. The results illustrated in Fig. 4 indicate that both FIT-ModelDiff and FIT-LIME can recognize the reused models and the independent models with different lengths of fingerprints. Moreover, with the length of F increases, the BERs of both reused and independent models are more concentrated, signifying that a larger fingerprint length can reduce the probability of outliers and have better security."}, {"title": "4.3.2. Effects of the $l_2$-Norm Coefficient", "content": "\u03bb is the coefficient of the scale of the perturbations in the loss function Eq. (8). In this experiment, we study the effect of \u03bb on FIT-Print and adopt FIT-ModelDiff and FIT-LIME with five different \u03bb. From Table 3, since the scale of the perturbations in FIT-ModelDiff is quite small, varying \u03bb does not significantly affect the perturbations as well as the distances with reused models. While in FIT-LIME, a larger \u03bb can lead to a smaller perturbation. The $l_2$-norm of the perturbations reduces from 0.020 to 0.014. In the meantime, the average distances with reused models become larger."}, {"title": "4.4. The Resistance to Adaptive False Claim Attack", "content": "We hereby evaluate our FIT-Print against the adaptive false claim attack, where the adversary utilizes Eq. (8) to optimize the testing samples yet intentionally crafts some independent models as augmented models to enhance the transferability of the adversary's false fingerprint. We utilize the models trained on ImageNet and their corresponding reused models as the augmented models. Fig. 5 demonstrates that adding independent augmented models does not significantly enhance the transferability of the fingerprint in FIT-Print because the BERs on the independent models are nearly unchanged. However, as shown in Section 2.3, existing fingerprinting methods are vulnerable to false claim attacks due to their untargeted nature. In contrast, our targeted FIT-Print is significantly more resistant to false claims since targeted transferable fingerprints are much more difficult to craft (as analyzed in Section 3.2). Empirically, such"}, {"title": "5. Conclusion", "content": "In this paper, we first revisited existing model fingerprinting methods, designed a false claim attack by crafting some transferably easy samples, and revealed existing methods were vulnerable to the false claim attack. We found that the vulnerability can be attributed to the untargeted nature that existing methods compare the outputs of any given samples on different models rather than the similarities to specific signatures. To tackle the above issue, we proposed FIT-Print, a false-claim-resistant model fingerprinting paradigm. FIT-Print transformed the fingerprint of the model into a targeted signature by optimizing the testing samples. We correspondingly designed two fingerprinting methods based on FIT-Print, namely the bit-wise FIT-ModelDiff and the list-wise FIT-LIME. Our empirical experiments demonstrated the effectiveness, conferrability, and resistance to false claim attacks of our FIT-Print. We hope our FIT-Print can provide a new angle on model fingerprinting methods to facilitate more secure and trustworthy model sharing and trading."}, {"title": "Impact Statement", "content": "Ethic Statement. Unauthorized model reuse has posed a serious threat to the intellectual property rights (IPRs) of the model developers. Model fingerprinting is a promising solution to detect reused models. In this paper, we propose a new paradigm of model fingerprinting dubbed FIT-Print. Our FIT-Print is purely defensive and does not discover new threats. Moreover, our work utilizes the open-source dataset and does not infringe on the privacy of any individual. Our work also does not involve any human subject. As such, this work does not raise ethical issues in general.\nPotential Societal Impact. In terms of positive societal impact, this paper aims to address the challenges associated with false claim attacks in ownership verification through the utilization of targeted model fingerprinting methods. Our FIT-Print, as a method for protecting intellectual property rights (IPR) related to models, will assist both academia and industry in safeguarding the costly models' IPRs and preventing unauthorized model reuse and theft. Furthermore, FIT-Print has the potential to facilitate the emergence of new business models such as model trading.\nOn the other hand, one potential negative societal impact of our paper is that its insight can be used in designing targeted adversarial attacks to some extent. This is mostly due to the similarities between false claim attacks and targeted adversarial attacks. However, we notice that our FIT-ModelDiff changes only the difference between the outputs instead of directly their predictions, while FIT-LIME changes only the explanation. Neither of them directly turns the prediction labels into a target class. As such, although our insight might be transferred to (transferable) targeted adversarial attacks, we argue that the negative impact of our method is negligible to most of the AI applications."}, {"title": "Appendix", "content": "A. The Detailed Threat Model\nIn this section, we provide a detailed introduction to the threat models of model fingerprinting and false claim attacks. Three parties involved in the threat models are depicted in Figure 6.\nA.1. Detailed Threat Model of Model Fingerprinting\nThere are three parties involved in the threat model of model fingerprinting, including the model developer, the model reuser, and the verifier. The model developer trains a model and the model reuser attempts to steal and reuse this model. The verifier is responsible for fingerprint registration and ownership verification. The assumptions of these three parties can be found in Section 2.1.\nProcess of Model Fingerprinting. Model fingerprinting can be divided into three steps, including fingerprint generation, fingerprint registration, and ownership verification.\n1. Fingerprint Generation: The model developer trains its source model $M_O$ and generates the fingerprint of $M_O$.\n2. Fingerprint Registration: After generating the fingerprint, the model developer registers the fingerprint and the model with a timestamp to a trustworthy third-party verifier.\n3. Ownership Verification: For a suspicious model $M_S$ that could be a reused version of $M_O$, the verifier will first check the timestamps of these two models. If the registration timestamp of $M_S$ is later than $M_O$, the verifier will further check whether the fingerprint of $M_S$ is similar to the fingerprint $M_O$. If so, the suspicious model can be regarded as a reused version of $M_O$.\nA.2. Detailed Threat Model of False Claim Attacks\nThere are three parties involved in the threat model of false claim attacks, including the malicious developer, the verifier, and an independent developer. The formal definition of false claim attacks can be found in Section 2.3.\nAssumption of Malicious Developer. In false claim attacks, the malicious developer is the adversary who aims to craft and register a transferable fingerprint to falsely claim the ownership of the independent developer's model $M_I$. The malicious developer is assumed to have adequate computational resources and datasets to train a high-performance model and carefully craft transferable model fingerprints. The primary goal of the malicious developer is that the registered model fingerprints can be verified in as many other models as possible. By generating the transferable fingerprint, the malicious developer can (falsely) claim the ownership of any third-party models (that are registered later than that of the malicious developer).\nProcess of False Claim Attacks. The process of false claim attacks can also be divided into three steps, including fingerprint generation, fingerprint registration, and false ownership verification."}, {"title": "B. The Proof of Theorem 1", "content": "Theorem 1. Given the security parameter \u043a and the fingerprint $F \\in \\{-1, 1\\}^k$, if \u03c4 + satisfy that\n$\\sum_{d=0}^{\\lfloor \\tau k \\rfloor} \\binom{k}{d} (\\frac{1}{2})^k \\leq \\kappa$,\nwhere $\\binom{k}{d} = k!/[d!(k -d)!]$, the probability of a successful false claim attack, i.e., the BER is less than with the adversaries testing samples, is less than \u043a.\nProof. As mentioned in Section 2.2 and Section 2.3, registering the fingerprint with a timestamp can avoid any afterward false claim attack. As such, the adversary needs to craft the testing samples $X_T$ which can transfered to independent models in advance. We assume that the adversary extracts the fingerprint F from the independent model $M_I$ using the testing samples $X_T$, as follows.\n$\\hat{F} = sign(M_I(X_T))$,\nWe assume that F denotes the adversary's target fingerprint. Since $F \\in \\{-1, 1\\}^k$ is a k-bit binary vector and the adversary has no knowledge of the independent model $M_I$, the probability of any bit in F to match the corresponding bit in F is 1/2. Thus, to satisfy Eq. (5) in Proposition 3, i.e., making the BER between F and F less than \u03c4, there needs to have at least $k - [\u03c4\u00b7k]$ bits in F match F. Based on the binomial theorem, we have the probability of the aforementioned scenario, i.e., a successful false claim attack, is as follows.\n$Pr\\{BER(\\hat{F}, F) \\leq \\tau\\} = \\sum_{d=0}^{\\lfloor \\tau k \\rfloor} \\binom{k}{d} (\\frac{1}{2})^k$.\nSince the right-hand side of the Eq. (3) is less than 6, the probability of a successful false claim attack, i.e., the BER is less than with the adversarial testing samples, is also less than \u043a."}, {"title": "C. Implementation Details", "content": "C.1. Details of the Model Reuse Techniques\nIn our experiments", "Copying": "Copying refers to the adversary somehow gaining white-box access to the parameters and architecture of the victim model. Subsequently", "Fine-tuning": "Fine-tuning means the adversary re-trains the victim model with its own dataset which is related to the primitive task of the victim model. We consider three types of fine-tuning denoted as Fine-tuning(10%)", "Pruning": "Pruning intends to compress the model by removing redundant parameters. We leverage weight pruning as our pruning method, which prunes the neurons according to their activations. We"}]}