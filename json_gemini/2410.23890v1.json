{"title": "Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource languages", "authors": ["S\u00e9amus Lankford", "Andy Way"], "abstract": "In an evolving landscape of crisis communication, the need for robust and adaptable Machine Translation (MT) systems is more pressing than ever, particularly for low-resource languages. This study presents a com-prehensive exploration of leveraging Large Language Models (LLMs) and Multilingual LLMs (MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique challenges posed by crisis situations where speed, accuracy, and the ability to handle a wide range of languages are paramount, this research out-lines a novel approach that combines the cutting-edge capabilities of LLMs with fine-tuning techniques and community-driven corpus development strategies. At the core of this study is the development and empirical evaluation of MT systems tailored for two low-resource language pairs, illustrating the process from initial model selection and fine-tuning through to deployment. Bespoke systems are developed and modelled on the recent Covid-19 pandemic. The research highlights the importance of community involvement in creating highly specialised, crisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM models. It identifies fine-tuned MLLM models as offering superior performance compared with their LLM counter-parts. A scalable and replicable model for rapid MT system development in crisis scenarios is outlined. Our approach enhances the field of humanitarian technology by offering a blueprint for developing multilingual communication systems during emergencies.", "sections": [{"title": "1 Credits", "content": "This work was supported by ADAPT, which is funded under the SFI Research Centres Programme (Grant 13/RC/2016) and is co-funded by the European Regional Development Fund. The research was also funded by the Munster Technological University in Cork, Ireland."}, {"title": "2 Introduction", "content": "The excitement surrounding LLMs stems from their potential to revolutionise many fields, from lan-guage translation (Costa-juss\u00e0 et al., 2022) and con-tent generation (Brown et al., 2020) to chatbots\u00b9 and virtual assistants. Way (2024) observes that from the very outset, machine translation (MT) capability has been overhyped at each paradigm shift over the past 75 years, but with their ability to 'understand' language and generate complex responses, LLMs do appear capable of enhancing human communi-cation and productivity in ways that were unimagin-able with previous approaches, especially given that LLMs are not restricted to text-based use-cases, and can be used in creative applications such as generat-ing music\u00b2 or art.\nWhen building LLMs, the focus is on design-ing and training the model architecture. This in-volves selecting the appropriate neural network ar-chitecture and hyperparameters, as well as deciding on the training data and optimisation techniques to use.\nTuning an MLLM or LLM, in contrast, in-volves adjusting the parameters of a pre-trained model to improve its performance on a specific task. In neural networks such as MLLMs, the weights and biases are parameters that the network adjusts through training to minimise a cost function. This is performed by training the model on a task-specific dataset and adjusting the model's hyperparameters to optimise its performance. Tuning an MLLM can be a challenging task, as the model is often very complex and the training process can take a long time, but Lankford et al. (2023a) offer an open-source solution to fine-tuning pre-built MLLMs, with a particular focus on low-resource language pairs, thus overcoming much of this complexity. In contrast to predictions of their imminent demise (van der Meer, 2021), Way (2024) predicts that tools such as adaptMLLM will instead allow translators to gain a competitive edge, by building and tuning their own models with their own high-quality data, \"while retaining full control over the process, lead-ing to self-empowerment and an improved sense of well-being\".\nGiven their potential, this paper investigates whether tools such as adaptMLLM can be used to rapidly build good-quality MLLM-based MT sys-tems for deployment in crisis scenarios, where speed of development is crucial, but not at the expense of quality altogether. These deployments are con-trasted with the development of custom GPTs and fine-tuned LLMs. For two language pairs and four language directions, each featuring a minority lan-guage, we present and evaluate a pipeline that we hope can be used as a blueprint for rapid deployment in crisis scenarios to improve multilingual commu-nication."}, {"title": "3 Background", "content": "Way et al. (2020) observe that there \"have been alarmingly few attempts to provide automatic trans-lation services for use in crisis scenarios\". To the best of our knowledge, the first was Microsoft's ef-fort (Lewis, 2010) to build Haitian Creole systems following the devastating earthquake in 2010, as the title makes clear \u201cfrom scratch in 4 days, 17 hours, & 30 minutes\". Estimated casualties ranged from 100,000 to over 300,000 deaths, with around a third of all citizens affected in some way or other by the earthquake measuring 7.0 on the Richter scale. The main issues for the Microsoft team were a complete lack of knowledge of the language (grammatical structure, encoding, orthography etc), and no data at all to train high-quality statistical MT engines. However, the team quickly identified some available resources (the Bible is available in most languages), and a small number of native speakers to help with translation and, especially, validation of the MT out-put generated. Eventually, around 150,000 segments of training data were collected to build the system, which obtained a BLEU (Papineni et al., 2002) score of almost 30 for Creole to English, and 18.3 for En-glish to Creole, sufficiently high (especially for the into-English direction) for the system to be deployed for use by relief workers in the field.\nThis remarkable effort led to the writing of a cookbook for MT in crisis scenarios (Lewis et al., 2011), so that the lessons learned from the exercise could be put into practice when other crises arose, as they do all too commonly, regrettably. Importantly, Lewis et al. (2011) note that \u201cIf done right, MT can dramatically increase the speed by which relief can be provided\". In any such scenario, translation is al-most always needed, and despite its importance, it is often overlooked.\nIn response to the need for better preparation for translation readiness in crises, Sharon O'Brien coordinated the Interact project\u00b3 featuring partners from academia, industry, as well as NGOs. Federici et al. (2019) provide a set of recommendations within that project which apply mainly to human translation provision in crisis scenarios."}, {"title": "3.1 Multilingual Language Models-NLLB", "content": "MT has become a significant area of research with the aim of eliminating language barriers worldwide. However, the current focus is limited to a small number of languages, neglecting the vast majority of low-resource languages. In an effort to address this issue, the No Language Left Behind (NLLB) initia-tive was launched to try to overcome the challenges of using MT for low-resource language translation by developing datasets and models that bridge the performance gap between low- and high-resource languages. The NLLB team has also created archi-tectural and training enhancements tailored to sup-"}, {"title": "3.2 Large Language Models", "content": "The increasing availability of large datasets provides the raw material for LLM training (Radford et al., 2019; Conneau et al., 2020; Winata et al., 2021), en-abling performance improvement on a wide variety of NLP tasks.\nLLMs have the potential to improve the use of technology across a wide range of domains, includ-ing medicine, education and computational linguis-tics. In education, LLMs may be used for person-alised student learning experiences (Kasneci et al., 2023), while in the medical domain, analysing large amounts of medical files can assist doctors in treat-ing patients (Iftikhar et al., 2023). Of particular in-terest to our research is the manner in which LLMs can be used within the realm of NLP, more specifi-cally in the field of MT, and we now provide details of some of the main candidates in this space."}, {"title": "3.2.1 GPT-4", "content": "The primary distinction between GPT-3.5 and GPT-46 is that while the former is a text-to-text model, the latter is more of a data-to-text model, exhibiting the ability to perform tasks that its predecessor could not. For example, GPT-4 is capable of processing visual input as part of a prompt, such as images or web pages, and can even generate text that explains the humour in memes. Consequently, GPT-4 can be classified as a \"multimodal model\". Furthermore, GPT-4 has a longer memory than its previous ver-sions, with a short-term memory closer to 64,000 words, enabling it to maintain coherence during ex-tended interactions. GPT-4 also enables users to se-lect different personalities for the model's responses.\nOpenAI has not disclosed the number of pa-rameters used in the training of GPT-4, though many estimates suggest it may be around 1.76 trillion. However, other sources, such as AX Semantics,7 have estimated the number to be around 100 trillion, with such a large model costing around $100 mil-lion to build. AX Semantics maintains that such a number makes the language model (LM) more akin to the functioning of the human brain with respect to language and logic."}, {"title": "3.2.2 Gemini", "content": "Gemini comes in three versions tailored for vary-ing levels of complexity and application: Gemini Ultra for the most demanding tasks, Gemini Pro for a broad range of activities, and Gemini Nano for on-device applications. The Ultra variant, in particular, has demonstrated SOTA performance, outperform-ing human benchmarks in massive multitask lan-guage understanding (MMLU) across a suite of 57 subjects. Team (2024) documents the performance of Gemini on the \"Machine Translation from One Book (MTOB)\" benchmark (Tanzer et al., 2023), essentially how good a model is at learning a lan-guage from almost no resources. For an evaluation of Gemini 1.5 Pro on the FLORES-200 benchmark (Costa-juss\u00e0 et al., 2022) against Google Translate, GPT-3.5 and GPT-4, and other systems, see Akter et al. (2023) (p.12)."}, {"title": "3.2.3 CoPilot", "content": "Microsoft has introduced Microsoft 365 Copilot, a generative AI tool designed to enhance workplace productivity and creativity. Copilot integrates LLMs with user data from Microsoft Graph and Microsoft 365 apps, to allow users to utilise natural language commands across familiar Microsoft 365 applica-tions such as Word, Excel and PowerPoint.\nCentral to this announcement is the launch of Business Chat, which synergies with the LLM, Mi-crosoft 365 apps, and user data to generate out-puts such as status updates from natural language prompts, drawing from various data sources like emails, meetings, and documents. This ensures that users remain in control, enabling them to adjust or refine the outputs as needed."}, {"title": "4 Datasets", "content": "4.1 Language Pairs\nTo benchmark the translation performance of adaptMLLM in fine-tuning MLLMs for low-resource languages, we had to choose suitable lan-guage pairs for which appropriate datasets existed. The English-to-Irish (EN\u2194GA) and English-to-Marathi (EN MR) language pairs were selected since they fulfilled the criteria of low-resource lan-guages, and data was freely available from shared tasks featuring these language pairs in crisis scenar-ios. Therefore, these language pairs were very suit-able for evaluating our proposed pipeline for rapidly generating high-quality translations in crisis situa-tions by fine-tuning MLLMs.\nIrish is the first official language of the Repub-lic of Ireland, and is also recognised as a minority language in Northern Ireland. Irish is an official language of the European Union and a recognised minority language in Northern Ireland with an ISO code of \"GA\" 10\nThe dominant language spoken in India's Ma-harashtra state is Marathi, with an ISO code of \"MR\". It has over 83 million speakers, and it is a member of the Indo-Aryan language family. De-spite being spoken by a significant number of peo-ple, Marathi is considered to be relatively under-resourced when compared to other languages used in the region."}, {"title": "4.2 Shared Task Datasets", "content": "To benchmark the performance of our adaptMLLM-trained models, datasets from the LoResMT2021 shared task (Ojha et al., 2021) were used, since the shared task focused on low-resource languages in-cluding both EN\u2194GA and EN\u2194MR in the specific domain of translation of COVID-related data.\nThe datasets from the shared task provided 502 Irish and 500 Marathi validation sentences whereas 250 (GA\u2192EN), 500 (EN\u2192GA), and 500 (ENMR) sentences were made available in the test datasets. Training data consisted of 20,933 lines of parallel data for the EN\u2194MR language pair and 13,171 lines of parallel data were used to train the ENGA models. A detailed breakdown of all re-sources is available in Ojha et al. (2021)."}, {"title": "5 Approach", "content": "Our approach to enhancing MT in crisis situations involves three key elements. Initially, a custom GPT would be created on the ChatGPT platform immedi-ately after a crisis, enabling users to contribute to a specialised knowledge base with new terms relevant to the crisis, effectively crowd sourcing a dataset for crisis-specific language pairs. With this approach both in-domain corpora and simple first iteration models are developed in real time by disparate users entering source and reference translations. Within the custom GPT interface on ChatGPT the function-ality also exists to upload relevant documents which adds to the knowledge base of custom GPTs. An-other interesting feature of ChatGPT is its ability to publicly share custom GPTs by sharing links. In this manner, it is trivial to develop corpora by im-plementing a simple link-sharing strategy that in-vites community-wide, expert-only or an ensemble of contributions.\nAs the crisis evolves, these corpora are then used to develop more accurate MT models with new weights tailored to the specific language needs of the crisis by fine-tuning OpenAI models, or other LLM foundation models.\nFinally, a bespoke model could be created us-ing an open-source tool like adaptMLLM, fine-tuned with a custom dataset developed during the crisis. Such a phased approach allows for a rapid initial response and progressively more tailored MT solutions as the crisis unfolds, leveraging commu-nity input and specialised training to improve trans-lation accuracy in critical situations.\nOf course, a major consideration when design-ing an MT system in crisis scenarios is the avail-ability of suitable parallel corpora which contain new terminology associated with the unfolding cri-sis. However, it is precisely at these times when the production of such datasets presents the greatest challenge.\nThere are two parallel streams in this process, the first of which entails a community corpus devel-opment effort, involving multiple contributors, us-ing a collaborative, crowdsourced approach. In this phase, selected users and language experts interact with LLMs on an ad hoc basis by presenting text in the source language and providing the translation in the target language. In this manner, an in-domain parallel dataset relevant specifically to the particular crisis is rapidly developed for the chosen language pair.\nThe second stream, LLM ensemble, incor-porates several elements: models from ChatGPT, Copilot, Gemini and other foundation models. The corpus creation process is carried out by simply ex-porting and concatenating the conversation histories from each of the customised LLMs. Duplicate en-tries created in the corpus development stage are re-moved and the corpus is split into three datasets: \"Test\", \"Train\", and \"Validation\". The training dataset is used to fine-tune a pre-trained (M)LLM to create a bespoke in-domain crisis MT model. The validation dataset is also used as part of this fine-tuning process before the test set is used to evalu-ate the performance of the MT system using stan-dard BLEU, TER (Snover et al., 2006) and ChrF (Popovi\u0107, 2015) metrics.\nFinally, the output of the process feeds into a crisis MT GitHub which is the central repository for the development of MT systems for multiple lan-guage pairs. Models and datasets developed as part of this process would be shared on GitHub for open-source collaboration and distribution.\nA Colab notebook has been developed to help with this process and we have made it publicly avail-able as part of this paper's GitHub which is freely available for download.14 A Gradio-based web app is incorporated within this notebook which facili-tates the involvement of non-technical users in cor-pus creation. This is our first implementation of such a notebook for aiding crisis MT corpus de-velopment and as an open-source tool, improve-ments and contributions from the community are welcomed."}, {"title": "6 Empirical Evaluation", "content": "After outlining the details of our approach, the qual-ity of the models developed is evaluated by training models for the EN\u2194GA and the EN\u2194MR language pairs."}, {"title": "6.1 Infrastructure and Hyperparameters", "content": "All MLLM models were trained by fine-tuning a 3.3B parameter NLLB pre-trained model using the adaptMLLM application with a Google Colab Pro+ subscription. The DeepSpeed library enables our models to be loaded across both GPU and system memory, thus reducing the required compute re-sources. The optimal hyperparameters used for de-veloping models for both language pairs are the same as those identified by Lankford et al. (2023a).\nBoth the custom GPT models and the base-line models used the GPT-4 model under a stan-dard ChatGPT subscription. The OpenAI fine-tuned models were developed using a pay-as-you-go plan. In fine-tuning the OpenAI models, GPT-3.5-turbo-0125 was the chosen pre-trained model since GPT-4 was unavailable for fine-tuning. Default parameters were kept and the number of epochs was set to auto. For inference on these models, a temperature set-ting of 0.5 was chosen to ensure a more determin-istic output which aligns with the requirements for translation models."}, {"title": "6.2 Results: Automatic Evaluation", "content": "To determine the quality of our translations, auto-mated metrics were employed. For comparison with previous results, the performance of our new mod-els was measured using three automatic evaluation metrics: BLEU, TER, and ChrF. We report case-insensitive BLEU scores at the corpus level. Note that BLEU and ChrF are precision-based metrics, so higher scores are better, whereas TER is an error-based metric so lower scores indicate better transla-tion quality. All models, notebooks and translations generated as part of our experiments are freely avail-able for download."}, {"title": "6.2.1 Translation in the ENGA Directions", "content": "The experimental results in the EN\u2194GA_direc-tions are summarised in Tables 2-3 and are com-pared with the baseline highest scores from the LoResMT2021 Shared Task. 16\nThe highest-performing EN GA system in the LoResMT2021 Shared Task was submitted by ADAPT (Lankford et al., 2021). The model was developed with an in-house application, adapt-NMT (Lankford et al., 2023b) using a Transformer (Vaswani et al., 2017) architecture. It performed well across all key translation metrics (BLEU: 36.0, TER: 0.531 and ChrF3: 0.6).\nBy fine-tuning the NLLB MLLM, using the pa-rameters outlined in Table 1, a significant improve-ment in translation performance was achieved. The adaptMLLM EN\u2192GA system, shown in Table 2, achieves a BLEU score of 41.2, which is 5.2 BLEU points higher (14% relative improvement) than the score of the winning system in 2021.\nBoth the custom GPT-4 and GPT-4 baseline models performed well compared to the GPT-3 models. However, there was a significant differen-tial when compared to the adaptMLLM fine-tuned NLLB models which recorded an increase of 8.4 BLEU points which corresponds to a relative im-provement of 25%. In a crisis scenario, a GPT-4 baseline model would be available in real-time. A custom GPT could be available within a matter of minutes once a relevant training corpus is uploaded to the GPT's knowledge base. Such approaches would be suitable for assisting translators in the im-mediate aftermath of a crisis and would help in is-suing bilingual press releases. However, more de-tailed documentation would greatly benefit from the improved translation quality of a bespoke fine-tuned adaptMLLM solution.\nFor translation in the GA EN direction, illus-trated in Table 3, the best-performing model for the LoResMT2021 Shared Task was developed by IIITT with a BLEU score of 34.6, a TER score of 0.586 and ChrF3 score of 0.6. Accordingly, this serves as the baseline score by which we can benchmark our GA EN MLLM model, developed by fine-tuning a 3.3B parameter NLLB using adaptMLLM. Simi-lar to the results achieved in the EN\u2192GA direction, significant improvement in translation performance was observed using this new method. The perfor-mance of the adaptMLLM model offers an improve-ment across all metrics with a BLEU score of 75.1, a TER of 0.385 and a ChrF3 result of 0.71. In partic-ular, the 117% relative improvement in BLEU score against the IIITT system is very significant.\nThe results from our GA\u2192EN experiments re-inforce the findings derived from translating in the EN GA direction. The custom and baseline GPT-4 models immediately deliver a translation system with good quality BLEU scores of 53 points. How-ever, a higher-quality translation system with a 21.2 BLEU score improvement can delivered in a mat-ter of hours once a fine-tuned adaptMLLM NLLB sytem is put in place. The exact length of time for system development is dependent on the quality of the underlying training infrastructure and also, more importantly, on how rapidly the training corpus can be assembled."}, {"title": "6.2.2 Translation in the ENMR Directions", "content": "The experimental results from the LoResMT2021 Shared Task in the EN MR directions are sum-marised in Tables 4 and 5, and are compared with adaptMLLM. For the shared task, the highest-performing EN MR system was submitted by the IIITT team. Their model used a Transformer archi-tecture and achieved a BLEU score of 24.2, a TER of 0.59, and ChrF3 of 0.597.\nAgain the approach taken by adaptMLLM in fine-tuning a 3.3.B parameter NLLB MLLM yielded the best performance compared with other systems entered for the shared task. The EN MR adaptM-LLM system achieves the highest BLEU score of 26.4, a 2.2 point improvement (9% relative) com-pared with IIITT, the winning team in the EN\u2192MR shared task.\nThe MLLM-based system, trained using adaptMLLM, is also compared with GPT-4 and GPT-3.5 LLM-based systems. For the purposes of our experiments, the best-performing LLM used a custom GPT-4 model which recorded a BLEU score of 19.0 points in the EN\u2192MR direction. This was only a marginal improvement on the baseline GPT-4 model with a BLEU score of 18.5 points. Critically, however, this solution could be delivered in real time which makes such a model a potential starting point for an immediate crisis response. A relative im-provement of 42% and 7.9 BLEU points is available once sufficient time is given to developing the fine-tuned MLLM model.\nFor translation in the MR EN direction, the best-performing model for the LoResMT2021 Shared Task was developed by oneNLP-IIITT with a BLEU score of 31.3, a TER of 0.58 and ChrF3 of 0.646. This serves as the baseline against which our MR\u2192EN model, developed using adaptMLLM, can be benchmarked. The performance of the adaptM-LLM model offers a significant improvement across all metrics with a BLEU score of 52.6, a TER of 0.409 and a ChrF3 of 0.704. Again this represents a very strong relative improvement of 68% in BLEU compared with the winning team from the shared task.\nThe best-performing MLLM-based system in the MR EN direction is also compared with our LLM-based systems. The highest-performing LLM used a custom GPT-4 model which recorded a BLEU score of 38.8 points. This was only a marginal improvement on the baseline GPT-4 model with a BLEU score of 38.6 points. As previously noted, the GPT4 baseline solutions can be delivered in real time which makes this model the ideal start-ing point for an immediate crisis response. A rel-ative improvement of 36% and 14 BLEU points is available once sufficient time is given to developing the fine-tuned MLLM model."}, {"title": "7 Discussion", "content": "A significant finding of this research is the demon-strated capability to substantially improve trans-lation quality for low-resource languages through fine-tuning with crisis-specific datasets. The adapt-ability and speed of deployment offered by LLMs and MLLMs hold the promise of making such rapid response a standard practice in future crises, ensur-ing that linguistic barriers do not impede vital aid and information flow.\nHowever, this potential comes with its share of challenges, particularly concerning the assembly and quality of training datasets. This study's pro-posed solution, leveraging community input through custom GPTs to crowd-source and refine translation data, presents a scalable model for corpus develop-ment in crisis scenarios. Looking ahead, this re-"}, {"title": "8 Conclusion", "content": "In this paper, we outlined how the advent of LLMs has transformed our ability to rapidly develop MT systems for low-resource languages in crisis scenar-ios. A system for rapid corpus development was pre-sented which adopts a collaborative approach, em-phasising community involvement and open-source methodologies.\nThe appropriate response to developing MT systems at different phases of a crisis were high-lighted. Using the recent Covid pandemic as a reference crisis, MT systems were developed us-ing custom GPTs, fine-tuned models from OpenAI and fine-tuned MLLM models. We demonstrated that a custom GPT delivers a functioning MT sys-tem rapidly whereas a fine-tuned MLLM delivers a higher-quality solution given a longer time horizon.\nBy highlighting how a fine-tuned MLLM can provide SOTA accuracy during a crisis, our work demonstrates how LLMs and MLLMs can provide more inclusive communication. Language barriers in crisis communication will be diminished with the help of this approach which in turn helps minority communities in times of real need.\nOur paper introduces a pipeline which is ap-plicable to a broader range of NLP problems. As part of future work, the methodologies and insights derived from our research could extend beyond the scope of MT to other domains within NLP. Conse-quently, a versatile framework for addressing a vari-ety of language processing challenges in crisis sce-narios has been put forth in this study."}, {"title": "Limitations of study", "content": "The proprietary nature of MLLMs and LLMs such as NLLB and GPT-4, which do not disclose the specifics of their training datasets presents a prob-lem. When fine-tuning these models for specific tasks, there is a risk of overlapping data that can-not be easily identified or removed. This limitation underscores a broader issue within the field of NLP and MT research, where the exact composition of training data in SOTA models often remains opaque."}]}