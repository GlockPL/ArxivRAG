{"title": "Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization", "authors": ["Pengkun Jiao", "Na Zhao", "Jingjing Chen", "Yu-Gang Jiang"], "abstract": "Abstract-Open-set single-source domain generalization aims to use a single-source domain to learn a robust model that can be generalized to unknown target domains with both domain shifts and label shifts. The scarcity of the source domain and the unknown data distribution of the target domain pose a great challenge for domain-invariant feature learning and unknown class recognition. In this paper, we propose a novel learning approach based on domain expansion and boundary growth to expand the scarce source samples and enlarge the boundaries across the known classes that indirectly broaden the boundary between the known and unknown classes. Specifically, we achieve domain expansion by employing both background suppression and style augmentation on the source data to synthesize new samples. Then we force the model to distill consistent knowledge from the synthesized samples so that the model can learn domain-invariant information. Furthermore, we realize boundary growth across classes by using edge maps as an additional modality of samples when training multi-binary classifiers. In this way, it enlarges the boundary between the inliers and outliers, and consequently improves the unknown class recognition during open-set generalization. Extensive experiments show that our approach can achieve significant improvements and reach state-of-the-art performance on several cross-domain image classification datasets.\nIndex Terms-Open-set, single-source domain generalization, out of distribution, domain expansion, class boundary, edge map, binary classifier.", "sections": [{"title": "I. INTRODUCTION", "content": "WHEN the training and test data are independently and\nidentically distributed (i.i.d.), deep neural networks\nhave shown remarkable performances in various computer\nvision tasks [1]\u2013[4]. However, the performances will suffer\nfrom a catastrophic drop once the training and test data have\ndifferent distributions [5]\u2013[8]. Moreover, these distribution\nshifts are generally inevitable in real-world scenarios since\nthe collection of training datasets is inherently a sub-sampling\nof the real-world data; thus, it is impossible to have the\nsame distribution with unseen testing datasets that are usually\ncollected separately under different conditions (e.g. time and\nspace). Particularly, distribution shifts can appear as both\ndomain shifts [9], [10] and label shifts [11]\u2013[13] in the unseen\ntesting datasets, resulting in target domains with different\ndomain characteristics and larger label spaces compared to\nthe source domain. To generalize to unseen target domains\nwith unknown categories, open-set domain generalization (OS-\nDG) [14] is studied, with the aim of learning a generalizable\nmodel from multiple source domains.\nHowever, gathering annotated data from multiple domains\ncan be both difficult and costly. As a result, a more practical\nand appealing approach involves using a single-source domain.\nThis has spurred interest in the study of open-set single-\nsource domain generalization (OS-SDG) [15], where a model\nis trained solely on a single source domain to identify known\ncategories and discern unknown categories in unseen domains.\nDespite its appeal, this reliance on a singular source domain\nposes a significantly greater challenge compared to OS-DG.\nCrossMatch [15] is the first work solving OS-SDG. Built\nupon the adversarial data augmentation method [16], [17],\nCrossMatch generates two kinds of auxiliary training data:\nsamples outside the source data distribution and samples\noutside the source label space, which are used to extend\nthe source domain and act as unknown classes, respectively.\nHowever, the quality of samples generated through adversarial\ntraining can be unstable, resulting in unrealistic and subpar\nsamples, particularly for unknown classes. On the other hand,"}, {"title": null, "content": "SODG-Net [18] generates pseudo-open samples by combining\nthe immediate features of two known classes. Nevertheless,\nthis approach could potentially undermine the recognition of\nthe known classes, as pseudo-open samples incorporate partial\nfeatures from these known classes. Both of these prior works\nrely on the generation of pseudo-open samples, which may not\naccurately or sufficiently represent the actual open samples in\nthe unseen domains.\nIn this paper, we propose a novel Domain Expansion and\nBoundary Growth based method, named DEBUG, that only\naugments source samples in terms of known classes, without\ngenerating any pseudo-open samples. An intuition stems from\nthe observation that the capacity of the model in recogniz-\ning out-of-distribution known and unknown classes can be\nsimultaneously enhanced once the known class distributions\nin the source feature space are adequately discriminative and\nseparated by a large margin, as illustrated in Figure 1.\nThe profound rationale for DEBUG is rooted in the theory\nthat inductive bias aids in generalization [19], [20]. Enhancing\nthe induction in semantic attribution is beneficial for gen-\neralization to unseen domains [21], [22]. However, training\nsolely on a single source domain may inadvertently introduce\na bias tied to domain-specific characteristics, which could\nadversely affect cross-domain generalization. We classify these\ndomain characteristics into two distinct types: class-irrelevant\ncontent (primary background information) [23] and style [24].\nTo address these biases, we propose two techniques: back-\nground suppression (BS) and global probabilistic-based style\naugmentation (GPSA). More specifically, we use an off-the\nshelf foreground-background segmentation method\u00b9 to get a\ncoarse yet free foreground mask for each sample. Based on the\nresultant coarse mask, we remove the irrelevant background\nregions from the original samples, referred to as background\nsuppression. Additionally, we introduce a global probabilistic-\nbased style augmentation method that globally models the fea-\nture statistics (i.e. mean and standard deviation) of the original\nand background-suppressed samples as Gaussian distributions.\nWe augment both the original and background-suppressed\nsamples by replacing their instance-level style statistics with\nrandomly sampled style statistics from the global probabilistic\ndistributions. Subsequently, with the paired augmented sam-\nples with and without backgrounds, we force the model to\nlearn consistent representations via knowledge distillation. The\nconsistency regularization between two samples with different\ncontent and style augmentations promotes domain-invariant\nrepresentation learning. Moreover, the consistency with the\nbackground-suppressed augmentation can reduce the intra-\nclass variance since the removal of a background can mitigate\nthe spurious effect of irrelevant context.\nDespite the elimination of inductive biases related to domain\ncharacteristics, some class-unspecific features, such as com-\nmon object parts (e.g., furry tails), may still increase the risk\nof misclassification, particularly for unknown classes that have\nnot yet had their class-specific features captured. In response\nto this, we develop the Boundary Growth strategy, which"}, {"title": null, "content": "diminishes the emphasis on common but class-unspecific fea-\ntures, which subsequently leads to further sufficient separation\nof class distributions and creates more room for open-set\nrecognition. Specifically, we adopt the multi-binary classifiers\n[25] that train a one-vs-all classifier for each class to deal with\nopen-set recognition, and we present a novel strategy based\non edge maps that are considered as a new modality of the\nsamples to train the multi-binary classifiers. Specifically, we\nextract edge maps from the original samples, treating them\nas additional samples with the same semantic yet different\nspreads in the feature space (see Figure 3). When training\na binary classifier for a specific class, a boundary is learned\nbetween the edge maps of the positive samples and the hard\nnegative samples, either in their original form or as edge maps.\nWe conduct extensive experiments on a variety of cross-\ndomain datasets, including PACS [26], Office31 [27], Of-\nficeHome [27], and DomainNet126 [28]. Our proposed DE-\nBUG shows significant and consistent improvements over\nCrossMatch, especially on unknown class recognition. This\ndemonstrates the effectiveness of our proposed approach for\nopen-set single-source domain generalization.\nOur contributions can be summarized as follows:\n\u2022\n\u2022\n\u2022\n\u2022\nWe offer a novel insight into OS-SDG: the recogni-\ntion ability of out-of-distribution known and open-set\nunknown classes can be improved when there is clear\ndiscrimination and sufficient separation in the source\nfeature space's distribution of known classes. We achieve\nthe discrimination and separation through our domain\nexpansion and boundary growth design.\nWe introduce a comprehensive method for expanding the\nsingle-source domain: we augment the original samples\nby suppressing the background and adding style distur-\nbances sampled from global probabilistic style statistics,\nand guide models to learn content-invariant informa-\ntion by distilling knowledge from these background-\nsuppressed augmentations.\nWe propose a novel boundary growth technique to pre-\nserve spaces for unknown classes encountered in the\ntarget domains. We utilize edge maps as an additional\nmodality and integrate them into the training of multi-\nbinary classifiers. The edge maps expand the initial\ndistribution of each class, and the optimization of the\nbinary classifier for each class extends the class boundary\nof the initial distribution.\nWe demonstrate that our proposed approach consistently\noutperforms existing methods on various cross-domain\nimage datasets in the challenging open-set single-domain\ngeneralization setting."}, {"title": "II. RELATED WORK", "content": "A. Domain Generalization\nDomain Generalization (DG) aims to learn a robust model\nthat can generalize to unseen target domains. Great progress\nhas been made in DG over the years, and the existing methods\ncan be generally classified into three categories according\nto the techniques they use: 1) data augmentation [29]\u2013[33],"}, {"title": null, "content": "2) representation learning [34]\u2013[40], and 3) learning strate-\ngies, e.g., meta-learning [41]\u2013[44], ensemble learning [45]-\n[48], self-supervised learning [49]\u2013[51] and gradient operation\n[52]\u2013[54]. In the data augmentation-based methods, Zhou\net al. [29] augment the source domains by employing a\ndata generator to synthesize data from pseudo-novel domains.\nWang et al. [30] synthesize new samples by mixing up data\nfrom multiple source domains. DSU [32] models the feature\nstatistics within each training batch as a Gaussian distribution\nto capture domain shift uncertainty, and then samples new\nfeatures from this estimated distribution to simulate diverse\npotential domains. To learn domain invariant representation,\nMu et al. [37] learn domain invariant feature representations\nby a kernel-based method. Ilse et al. [38] propose a domain in-\nvariant variational auto-encoder that learns independent latent\nsubspaces for domain, class, and residual variations. In terms\nof learning strategies, Du et al. [42] adopt a probabilistic meta-\nlearning model for domain generalization, where they model\nshared classifier parameters as distributions to estimate predic-\ntion uncertainty and obtain domain-invariant representations\nusing their proposed meta variational information bottleneck\nprinciple. On the other hand, Bucci et al. [50] enhance cross-\ndomain robustness by adding self-supervised learning tasks,\nsuch as solving jigsaw puzzles and recognizing the image\norientation. Tian et al. [54] generalize the model to out-of-\ndistribution samples by maximizing the neuron coverage of\nthe model with the gradient similarity regularization between\nthe original data and the augmented data."}, {"title": null, "content": "Single Domain Generalization (SDG) is a special and ex-\ntreme case of DG, which assumes only one source domain is\navailable. The single-source assumption raises additional chal-\nlenges compared to multi-source DG. Existing works mainly\ntackle the SDG problem by synthesizing auxiliary training\ndata to increase the diversity of the training data. Adversarial\ndata augmentation (ADA) [16] is a commonly used technique,\nwhich generates training samples by backpropagating the in-\nverse gradient of the classifier. Maximum-entropy adversarial\ndata augmentation [17] further improves ADA by generating\nhard negative samples. Furthermore, ASR-Norm [55] designs\nan adaptive normalization scheme based on ADA to improve\nthe model generalization ability across domains. Peng et al.\n[56] employs a meta-learning-based scheme to efficiently or-\nganize the training of adversarially augmented \"fictitious\u201d do-\nmains. Unlike these ADA-based methods, L2D [57] proposes a\nstyle-complement module to enhance the generalization power\nof the model, and a progressive domain expansion network\n[58] designs a learnable generator to expand the source domain\nprogressively. Recently, Cugu et al. [59] randomly applied\nvisual corruptions on the training data, and enforced visual\nattention consistency between the model's activation maps\nfor the original and corrupted input images. SimDE [60]\nexpands the source domain by generating uncertain samples\nthrough a trade-off between entropy maximization and cross-\nentropy minimization, and enhances the diversity of domain\nexpansions by training a pair of generators that alternate the\nguidance of dual classifiers."}, {"title": "B. Open-Set Recognition", "content": "Open-Set Recognition (OSR) aims to identify \"unknown\"\nclasses that are unseen during training in the testing stage.\nPrior works tackle OSR mainly by: 1) modifying SoftMax for\npotential unseen classes [61], 2) generating open-set samples\n[62], [63], 3) employing placeholders for novel class distribu-\ntion anticipation [64], 4) prototype-based methods [65], and 5)\nenhancing closed-set accuracy [25], [66], [67]. In the direction\nof open-set samples generation, Neal et al. [62] use generative\nadversarial networks to generate examples resembling training\ndata but not belonging to any training category, thus refor-\nmulating open-set recognition as a classification task with an\nadditional \"unknown\" class. OpenGAN [63] creates synthetic\nopen data adversarially at the feature level rather than at the\npixel level. In contrast to these generation-based methods,\nsome methods circumvent the challenging open-set sample\ngeneration procedure by improving open-set recognition by\nenhancing the close-set classification. Vaze et al. [66] demon-\nstrate a strong correlation between closed-set and open-set\nperformance in OSR. They find that efficient identification of\nclosed-set classes contributes to open-set identification. OVA\n[67] trains a one-vs-all classifier for each class in the closed-\nset, aiming to reduce intra-class distance and increase inter-\nclass distance, consequently creating more room for effective\nrejection of unknown classes. Unfortunately, these existing\nOSR methods will fail to identify open-set classes when the\ntest distribution is different from the training one."}, {"title": "C. Open-Set Domain Generalization", "content": "Open-Set Domain Generalization (OS-DG) aims to identify\nunknown classes in target domains in addition to recognizing\nout-of-distribution known classes as in SDG. [14] is the first\nwork that introduces the task of OS-DG. It solves the task\nby first augmenting domains on both the feature-level and\nlabel-level via Dirichlet mixup and soft-labeling, and then\nconducting meta-learning over the domains to improve the\ngeneralization ability of the model. In addition to meta-\nlearning, some other learning strategies have been explored.\nFor example, Katsumata et al. [68] use metric learning to\ndiffuse feature representations of unknown samples. Shao\net al. [69] propose a supervised contrastive learning based\nframework to cope with open-set learning under domain shifts.\nYang et al. [70] propose a new training scheme to learn a\n(n+1)-way classifier to predict n sources and one unknown\nclass, and uses weighted entropy minimization to improve the\ngeneralization ability of the model. MEDIC [71] leverages\nthe gradient matching property inherent in meta-learning to\nestablish a well-balanced decision boundary among classes.\nDespite the fact that these methods can learn a relatively robust\nmodel that generalizes to cross-domain data and identifies\nunknown classes, their success heavily depends on the richness\nof multiple source domains, which might not be guaranteed\nin reality due to the heavy collection and annotation cost of\ncross-domain datasets.\nSimilar to the relation between DG and SDG, Open-set\nSingle-source Domain Generalization (OS-SDG) is an extreme\ncase of OS-DG. Research on OS-SDG is largely unexplored."}, {"title": "III. METHOD", "content": "In this paper, we study OS-SDG in the context of the\nimage classification task. Given only one source domain\n$S = \\{(x, y)\\}_{i=1}^{N_s}$ containing $N_s$ samples with the label space\n$C_s$, our objective is to learn a robust model $F(.)$ that can\ngeneralize to any target domain $T = \\{(x,y)\\}_{i=1}^{N_t}$, whose\nlabel space $C_t$ is a superset of $C_s$ (i.e. $C_s \\subset C_t$). Specifi-\ncally, $F$ should be able to recognize the target samples from\nknown categories $C_s$, and identify the samples from open-set\ncategories $C_t\\backslash C_s$ as unknown, which is formally formulated as:\n$F(x) = y$, if $y \\in C_s,$\n$F(x) = unknown$, if $y \\notin C_s.$\n(1)\nA. Framework Overview\nThe framework of our proposed DEBUG is illustrated in\nFigure 2. Our DEBUG consists of three learnable modules: a\nfeature encoder, a linear multi-class classifier, and $|C_s|$ one-\nvs-all classifiers (i.e. multi-binary classifiers). Our DEBUG\nis trained with the conventional cross-entropy loss $L_{ce}$ and\nother two losses (cf. Eq. 11 and 13 for the details) that are\ncomputed for the sake of domain expansion and boundary\ngrowth, respectively.\nB. Domain Expansion\nWe expand the limited source domain dataset by augmenting\nthe samples from both the content and style perspectives, using\nbackground suppression (Sec. III-B1) and global probabilistic-\nbased style augmentation (Sec. III-B2), respectively. With\nthese augmented samples, our model is able to learn domain-\ninvariant features by distilling consistent knowledge that resists\nboth content and style perturbations.\n1) Background Suppression: The background region in an\nimage is usually irrelevant to the semantic category of the\nimage, but it plays an adverse role in the representation learn-\ning [72]. If a model is not robust enough to disentangle such\nirrelevant information from the semantic-relevant information,\nthe resultant representation could be susceptible to domain\nperturbations such as background changes. In other words,\ndisentangling background effects from representation learning\ncan facilitate the acquisition of domain-invariant features.\nMotivated by this, we introduce a simple background sup-\npression scheme to coarsely mask out background regions.\nUnlike a few recent works that learn precise binary masks\nto separate foreground and background via complex adver-\nsarial training, we only use an off-the-shelf unsupervised\nforeground-background segmentation method to obtain coarse\nbinary masks. Specifically, we opt for DenseCLIP [73], an\nopen-vocabulary segmentation approach, due to its simplicity\nand efficacy. DenseCLIP is based on the pre-trained multi-\nmodal CLIP [74], and it leverages the powerful semantic\nencoding capability of CLIP to obtain coarse dense labels\n(i.e. segmentation result) for an image without training or"}, {"title": null, "content": "fine-tuning. By adopting this off-the-shelf segmentation tool,\nwe generate a free binary mask for each image according to\nthe semantic class of the image. Subsequently, we remove the\nbackground region from the image based on the coarse mask,\nresulting in a background-suppressed image.\n2) Global Probabilistic-based Style Augmentation: As\npointed out in AdaIN [24], the feature statistics, i.e. channel-\nwise mean and standard derivation encode the style informa-\ntion, which is one key type of domain shift. Previous works\n[75], [76] apply style augmentation by computing all the\nfeature statistics from the whole dataset and then swapping\n(or mixing) them with the original style feature statistics.\nAlthough these methods can generate additional samples, they\nrely on deterministic feature statistics that do not account for\nthe underlying distribution of the feature statistics. A recent\nwork DSU [32] proposes to explore the uncertain statistics by\nmodeling the feature statistics computed within a mini-batch\nas Gaussian distributions. During batch training, DSU replaces\nthe original statistics of the style features with the new ones\nsampled from the locally estimated Gaussian distributions.\nUnfortunately, the estimation of batch-based statistics in\nDSU can be prone to instability and bias, due to fluctuations\nin the data distribution across different training batches. More-\nover, the local uncertainty estimation in DSU is insufficient in\ndepicting the global distribution scope of probabilistic feature\nstatistics, which provide more diverse and smooth augmen-\ntations. The global probabilistic distribution is important for\nopen-set SDGs since we wish to expand the feature space of\nthe known classes from the source domain (via more diverse\naugmentations) but meantime leave some spaces in between\nthe known classes for the open-set classes in potential target\ndomains (via smooth augmentations). To this end, we propose\na global probabilistic modeling method to explore the style\nuncertainty within the source domain for style augmentation.\nInstead of adopting a naive way that computes the feature\nstatistics within the whole dataset offline for the probabilistic\nmodeling, we estimate the global probabilistic model in an\nonline fashion.\nMore specifically, for the intermediate feature of a mini-\nbatch $z \\in R^{B\\times C\\times H\\times W}$ produced by middle layers of the\nfeature encoder, where B is the batch size and C is the number\nof channel, we first compute its instance level channel-wise\nmean $\\mu \\in R^{B\\times C}$ and variance $\\sigma^2 \\in R^{B\\times C}$:\n$\\mu(z) = \\frac{1}{HW} \\sum_{h=1}^{H}\\sum_{w=1}^{W} z_{h,w},$\n$\\sigma^2(z) = \\frac{1}{HW} \\sum_{h=1}^{H}\\sum_{w=1}^{W} (z_{h,w} - \\mu(z))^2.$\n(2)\n(3)\nThen, following the non-parametric uncertainty estimation\nmechanism as in DSU, we respectively calculate the variance\nof statistics $\\mu(z)$ and $\\sigma(z)$ in one batch:\n$\\Sigma_{\\mu} = \\frac{1}{B} \\sum_{b=1}^{B} (\\mu(z)|_b - E_b [\\mu(z)])^2,$\n$\\Sigma_{\\sigma^2} = \\frac{1}{B} \\sum_{b=1}^{B} (\\sigma(z)|_b - E_b [\\sigma(z)])^2.$\n(4)\n(5)"}, {"title": null, "content": "Once obtaining $\\Sigma_{\\mu}$ and $\\Sigma_{\\sigma^2}$ of z, we use a moving average\nalgorithm to update global uncertainty statistics:\n$U_{\\mu} = \\alpha U_{\\mu} + (1 - \\alpha)\\Sigma_{\\mu},$\n(6)\n$U_{\\sigma^2} = \\alpha U_{\\sigma^2} + (1 - \\alpha)\\Sigma_{\\sigma^2},$\n(7)\nwhere $U_{\\mu} \\in R^C$ and $U_{\\sigma^2} \\in R^C$ denote the variance of global\nprobabilistic distributions w.r.t. $\\mu(z)$ and $\\sigma(z)$, respectively.\nThe values of $U_{\\mu}$ and $U_{\\sigma^2}$ are set as zeros at the beginning of\nthe training, and become steady along the training. $\\alpha$ is the\nmomentum hyper-parameter to update $U_{\\mu}$ and $U_{\\sigma^2}$.\nWe sample style perturbations (the mean and standard\ndeviation of new styles) from $N(\\mu,U_{\\mu})$ and $N(\\sigma,U_{\\sigma^2})$ using\nthe re-parameterization trick [77]:\n$\\beta(z) = \\mu(z) + \\xi_{\\mu} \\sqrt{U_{\\mu}},$\n(8)\n$\\gamma(z) = \\sigma(z) + \\xi_{\\sigma} \\sqrt{U_{\\sigma^2}},$\n(9)\nwhere $\\xi_{*} \\sim N(0,1)$ are random variables drawing from\na standard normal distribution. Subsequently, we augment\nintermediate features z by replacing the original style statistics\nin AdaIN [24] with the sampled style perturbations:\n$z^* = \\frac{z - \\mu(z)}{\\sigma(z)} \\gamma(z) + \\beta(z).$\n(10)\nHere $z^*$ is the augmented intermediate features from z.\nNote that we update the global uncertainty statistics with both\noriginal samples and background-suppressed samples.\n3) Knowledge Distillation: After obtaining the style-\naugmented samples with and without background, we per-\nform a knowledge distillation to encourage the encoder to\nlearn consistent representations regardless of the content and\nstyle perturbations. In particular, we take the background-\nsuppressed augmentation as the distillation target since the\nremoval of a background can restrict the spurious variable\ncaused by an irrelevant context, and consequently reduce the\nintra-class variance. Formally, we denote the style-augmented\nfeatures outputted by the encoder with and without background\nsuppression as $o_{BS} \\in R^{C'\\times H'\\times W'}$ and $o \\in R^{C'\\times H'\\times W'}$,\nrespectively. We distill consistent knowledge from $o_{BS}$ by\ncomputing the Kullback-Leibler (KL) divergence [78] between\no and $o_{BS}$:\n$L_{kd} = \\frac{1}{C'H'W'} \\sum_{c'=1}^{C'} \\sum_{h'=1}^{H'} \\sum_{w'=1}^{W'} d(o_{BS}/T) log \\{\\frac{d(o/T)}{d(o_{BS}/T)} \\},$\n(11)\nwhere d is the softmax function over the feature channel $C'$,\nand $\\tau$ is the temperature hyperparameter.\nC. Boundary Growth\nOur proposed domain expansion can help the model learn\ndomain-invariant representations, however, it does not specif-\nically take care of the open-set recognition problem in OS-\nSDG. Inspired by the recent success in addressing open-set\nidentification by using the one-vs-all classifier for each class\n[15], [25], [67], [79], we adopt multiple binary classifiers in\nthis paper. Specifically, let $w_i = \\{w_i^+, w_i^- \\}_{i=1}^{C_S}$ denote the\nmulti-binary classifiers, where $w_i^+$ and $w_i^-$ are the positive"}, {"title": null, "content": "and negative classifier of i-th class, respectively. The predic-\ntions (after softmax) of $w_i^+$ and $w_i^-$ are denoted as $p^i(y =$\n$1|x)$ and $p^i(y = 0|x)$, where $p^i(y = 1|x) + p^i(y = 0|x) = 1$.\n$p^i(y = 1|x)$ and $p^i(y = 0|x)$ reflect the probability of an input\nx belonging to the class i and all other classes, respectively.\nFor an image $x_s$ with label $y_s$, $x_s$ is the positive sample\nfor $w_{y_s}^+$ while might be the negative sample for other binary\nclassifiers $\\{w_j^+\\}_{j \\neq y_s}$ based on the hard negative classifier\nsampling strategy [25]. Each binary classifier is optimized by\nthe one-vs-all (OVA) loss:\n$L_{ova} = -log(p^{y_s}(y = 1|x^*)) - min_{i\\neq y_s} log(p^i(y = 0|x^*)).$\n(12)\nThe optimization of all multi-binary classifiers is inherently\nequivalent to extending the boundaries across all $|C_s|$ classes,\nand the efficacy of the boundary learning is heavily reliant on\nthe selection of effective samples, e.g. hard negative samples.\nIn view of this, we propose a novel classifier sampling\nstrategy that incorporates edge maps as positive and negative\nsamples to optimize the multi-binary classifiers. Since the edge\nmap of an image shares the same semantic information but\ndifferent characteristics with the corresponding image, it can\nbe considered as a new modality or view of the original image.\nWe also observe that the feature distribution of edge maps\nwithin one class is shifted away from the original distribution\nof that class, as demonstrated in Figure 3. This could be\nattributed to the fact that the removal of most appearance\ninformation from the edge map makes it become an out-\nof-distribution sample in the corresponding class, which is\na better choice as the positive and hard negative sample for\ntraining multi-binary classifiers.\nLet $\\varphi(.)$ denote the edge detector that extracts edge maps\nfrom images. Instead of the original $x^o$, we use $\\varphi(x^o)$ as the\npositive sample for $w_{y_s}^+$. For $\\{w_j^+\\}_{j \\neq y_s}$, we use both $x^o$ and\n$\\varphi(x^o)$ as the hard negative samples. Thus, the optimization of\neach binary classifier in Eq. 12 is modified as:\n$\\begin{aligned}\n&L_{ova} =\\\\\n&-log(p^{y_s}(y = 1|\\varphi(x^*))) \\\\\n&-\\frac{1}{2} min_{k\\neq y_s} log(p^k(y = 0|\\varphi(x^*))) \\\\\n&-\\frac{1}{2} min_{k\\neq y_s} log(p^k(y = 0|x^*)).\n\\end{aligned}$\n(13)\nSince edge maps roughly represent out-of-distribution sam-\nples in the corresponding class, the optimization of the multi-\nbinary classifiers with them can further grow the boundaries"}, {"title": null, "content": "across known classes and thus improve the robustness of the\nmodel on unknown class identification.\nD. Training and Inference\nDuring training, the model is optimized by the overall loss\nthat combines the three losses:\n$L_{all} = L_{ce} + \\lambda_1 L_{eova} + \\lambda_2 L_{kd},$\n(14)\nwhere $L_{ce}$ is the cross entropy loss computed between the\noutputs of multi-class classifier $w_m$ and the ground truths. $\\lambda_1$\nand $\\lambda_2$ are hyper-parameters to control the contributions of\nthe corresponding losses.\nDuring inference, we obtain the prediction score (after\nsoftmax) from $w_m$ for each test image, and then we calculate\nthe entropy of the prediction score. If the entropy value is\nsmaller than a threshold, it is determined as the known class\nwith the highest prediction score, otherwise, it is determined\nas an unknown class. To avoid threshold fine-tuning across\ndatasets, we set the threshold to $log_2 |C_s|$.\nIV. EXPERIMENTS\nA. Experimental Settings\n1) Datasets: Following CrossMatch [15], we evaluate our\nDEBUG on four cross-domain image classification datasets:\n\u2022 PACS [26] contains 9,991 images of 7 categories collected\nfrom four domains: art painting, cartoon, photo, and sketch.\nFour classes (i.e. dog, elephant, giraffe, and guitar) are\nselected as the known classes, and the remaining classes\n(i.e. horse, house, and person) are treated as unknown.\n\u2022 Office31 [27] contains 4,652 images of 31 categories col-\nlected from three domains: Amazon, DSLR and webcam.\nTen classes are selected as known (i.e. back pack, bike,\ncalculator, headphones, keyboard, laptop, monitor, mouse,\nmug, and projector), and another eleven classes are selected\nas unknown (i.e. ruler, punchers, stapler, scissors, trash can,\ntape dispenser, pen, phone, printer, ring binder, and speaker).\n\u2022 OfficeHome [80] contains 15,500 images of 65 categories\nfrom four domains: artistic, clip art, product, and real-\nworld. The first 15 classes (i.e. alarm clock, backpack,\nbattery, bed, bike, bottle, bucket, calculator, calendar, can-\ndles, chair, clipboards, computer, couch, and curtains) in\nalphabetic order are selected as known, and the rest 50\nclasses are treated as unknown."}, {"title": null, "content": "\u2022\nDomainNet126 [28", "81": ".", "domains": "real", "Details": "We adopt the ImageNet pre-\ntrained ResNet-18 [82"}]}