{"title": "DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning", "authors": ["Zhuoyuan Mao", "Mengjie Zhao", "Qiyu Wu", "Hiromi Wakaki", "Yuki Mitsufuji"], "abstract": "Recent advancements in music large language models (LLMs) have significantly improved music understanding tasks, which involve the model's ability to analyze and interpret various musical elements. These improvements primarily focused on integrating both music and text inputs. However, the potential of incorporating additional modalities such as images, videos and textual music features to enhance music understanding remains unexplored. To bridge this gap, we propose DeepResonance, a multimodal music understanding LLM fine-tuned via multi-way instruction tuning with multi-way aligned music, text, image, and video data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and Music4way-Any2T, three 4-way training and evaluation datasets designed to enable DeepResonance to integrate both visual and textual music feature content. We also introduce multi-sampled ImageBind embeddings and a pre-alignment Transformer to enhance modality fusion prior to input into text LLMs, tailoring DeepResonance for multi-way instruction tuning. Our model achieves state-of-the-art performances across six music understanding tasks, highlighting the benefits of the auxiliary modalities and the structural superiority of DeepResonance. We plan to open-source the models and the newly constructed datasets.", "sections": [{"title": "Introduction", "content": "Different modalities are often interwoven. In the context of music, humans typically experience music alongside complementary textual and visual signals, such as lyrics, the composition of a piece, live performances, or the arrangement of instruments in a band. These additional modalities significantly influence how humans perceive and understand music. Therefore, it is reasonable to assume that training a model to incorporate other modalities such as images, videos, and textual music features can enhance its performance on music understanding tasks (Manco et al., 2021; Gardner et al., 2024; Agostinelli et al., 2023; Liu et al., 2024).\nIn this work, we introduce the concept of multimodal music understanding, where multiple modality signals are leveraged to enhance the perception of music. We implement this concept through multi-way instruction tuning, inspired by code-switched (Song et al., 2022) and multi-way multilingual translation (Fan et al., 2021) that extend translation models beyond bilingual pairings. For multimodal music understanding, we establish re-"}, {"title": "Related Work", "content": "Music Understanding is an emerging topic that builds upon the foundational research efforts in music information retrieval (MIR), which traditionally focused on low-level music feature recognition tasks, such as identifying tempo, chords, keys, and instruments (Faraldo et al., 2016; Pauwels et al., 2019; Gururani et al., 2019; Schreiber et al., 2020). Early work in this area was centered on basic tagging tasks, such as determining the genre or version of a piece of music (Tzanetakis, 2001; Won et al., 2021; Yesiler et al., 2021). Over time, the focus shifted to high-level understanding tasks that require a more comprehensive interpretation of the content, sentiment, and insights conveyed by music. These tasks include captioning, reasoning, question answering, and tool using (Manco et al., 2021; Gardner et al., 2024; Agostinelli et al., 2023; Liu et al., 2024; Deng et al., 2024; Zhao et al., 2024).\nMultimodal Instruction Tuning and Music Foundation Models: Recently, multimodal pre-training has successfully bridged image, audio, and video modalities to text LLMs (Tang et al., 2024a; Wu et al., 2024; Gong et al., 2024; Tang et al., 2024b) through multimodal instruction tuning (Liu et al., 2023; Zhao et al., 2023) or universal multimodal"}, {"title": "Multi-way Instruction Tuning", "content": "We focus on the integration of multi-way information into each data for music understanding tasks, which conventionally contains only music and text."}, {"title": "Music-centric Multi-way Datasets Construction", "content": "M2UGen (Hussain et al., 2023) pioneered multimodal dataset construction for instruction-tuning music LLMs, creating music-centric pairs with text, images, videos, and other music for captioning, editing, and generation tasks. Drawing inspiration from studies on code-switched and multi-way translation (Song et al., 2022; Fan et al., 2021), which break away from English-centric bilingual pairing relationships, we propose extending traditional music LLM fine-tuning by incorporating multi-way relationships among music and other modalities, including image, video, and text. Building upon the music-centric paired dataset construction pipeline of M2UGen, we expand it with multi-way relationships to create multimodal-enriched training data."}, {"title": "Model Architecture Tailored for Multimodal Music Understanding", "content": "In this section, we introduce DeepResonance, our model designed to adapt general any-to-text LLMs for multimodal music understanding tasks, leveraging the multi-way datasets introduced in Sec. 3.2. We construct DeepResonance based on NEXT-GPT (Wu et al., 2024), a general any-to-any multimodal LLM. This framework integrates an ImageBind encoder (Girdhar et al., 2023) to process inputs from the image, video, and audio modalities, a Vicuna-7B (Chiang et al., 2023) model as the LLM backbone, and linear adaptors to bridge ImageBind to the Vicuna model. The vanilla version of DeepResonance, like NExT-GPT, models the text sequence generation task as follows:\n$P(W_n | X_m, X_v, X_i, X_t, Q, W_{1:n-1}) = LLM(A_m(e_m), A_v(e_v), A_i(e_i), \\{e_t\\}, \\{e_q\\}, \\{e_w\\}_{1:n-1})$  (1)\nwhere $W = \\{W_1, W_2, ..., W_n \\}$ represents the text sequence to be generated, and $X_m, X_v, X_i$ and $X_t$ denote the patch-level multimodal and text tokens for music, video, image, and text, respectively. Qindicates the query (i.e., instruction) input to the model. LLM and $A_*$ ($* \\in \\{m, v, i\\}$) represent the Vicuna-7B LLM and the linear adaptors for each modality. $e_*$ ($* \\in \\{m, v, i\\}$) denotes the embedding of music, video, and image produced by ImageBind, while $e_{\\#}$ ($* \\in \\{t, q, w\\}$) are the LLM's text embeddings of input, query, and output.\nHowever, the pooled single embedding from ImageBind may fail to capture the detailed information required to effectively interact with other modalities in downstream music understanding tasks, particularly for music and video modalities. This limitation arises because such multimodal encoders prioritize coarse-grained representations for cross-modal retrieval tasks (Balaji et al., 2022). Additionally, as NExT-GPT relies solely on modality-specific adaptors to map each modality into LLM's embedding space, it may not effectively model interactions between modalities. These interactions were never pre-trained, and the LLM itself only employs uni-directional attention (Zhou et al., 2024).\nTo address these challenges, we propose two modules for music LLMs: multi-sampled ImageBind embeddings and pre-alignment Transformer, as shown in Fig. 2. The former leverages embeddings from multiple clips sampled by ImageBind without pooling, while the latter introduces a Transformer to globally integrate and align information across modalities before feeding it into the LLM. Formally, the proposed model is defined as\n$P(W_n | X_m, X_v, X_i, X_t, Q, W_{1:n-1}) = LLM(T(A_m(\\e_m\\}_{1:N_m}), A_v(\\e_v\\}_{1:N_v}), A_i(\\e_i\\}_{1:N_i}), \\{e_t\\}, \\{e_q\\}, \\{e_w\\}_{1:n-1})$.  (2)\nHere, T represents the pre-alignment Transformer, and $e_*$ ($* \\in \\{m,v,i\\}$) from Eq. 1 is reformulated as $\\{e_*\\}_{1:N_*}$ to incorporate multi-sampled ImageBind embeddings for each modality, where $N_*$ denotes the number of sampled clips for a given modality. With these components, the multi-sampled ImageBind embeddings preserve finer-grained information for each modality, which is expected to enhance music understanding tasks (Sec. 5). The pre-alignment Transformer uses bidirectional attention to encode cross-modal dependencies, enhancing the interactions between music and other modalities, ultimately aiming to improve multimodal music understanding tasks (Sec. 5.3)."}, {"title": "Experiments and Results", "content": "Following the training strategy of NExT-GPT and M2UGen, we train DeepResonance in two stages. Subsequently, we evaluate DeepResonance across three music understanding tasks and three multimodal music understanding tasks in supervised settings. Additionally, we assess the model's zero-shot per-"}, {"title": "Baselines and Ours", "content": "Below are the baseline models and our models that we compare in this section.\nSALMONN (Tang et al., 2024a): A robust baseline model for audio understanding. It leverages a variety of audio datasets for training, including AudioCaps (Kim et al., 2019), WaveCaps (Mei et al., 2024), MusicNet (Thickstun et al., 2017), etc.\nMU-LLaMA (Liu et al., 2024): The first LLM instruction-tuned for music understanding with the MusicCaps and MusicQA as fine-tuning data.\nNEXT-GPT (Wu et al., 2024): The first any-to-any multimodal LLM trained on multimodal fine-tuning data, serving as the backbone for ours.\nM2UGen (Hussain et al., 2023): The first any-to-any multimodal LLM tailored to the music domain, trained on newly curated data derived from the music split of AudioSet (Gemmeke et al., 2017).\nNEXT-GPT w/ M2UGen: We train a NExT-GPT model on the same data as M2UGen, excluding MusicCaps, as its test split was inadvertently included in M2UGen's training data.\nNEXT-GPT w/ Music4way: We train a NExT-GPT model using the Music4way datasets (see Sec. 3.1), a more extensive and 4-way aligned dataset compared to M2UGen's training data. We exclude Music4way-MI2T and Music4way-MV2T (Table 1), aiming to evaluate the benefits of them.\nDeepResonance (Ours): The models introduced in Sec. 4, built upon NEXT-GPT and trained with datasets as shown in Table 1. We introduce two DeepResonance variants: DeepResonance-\u03b1, trained without the pre-alignment Transformer, and DeepResonance-\u00df, which includes it. The following sections examine their effectiveness, as the pre-alignment Transformer is specifically designed for multimodal music understanding tasks (Sec. 5.3)."}, {"title": "Music Understanding Tasks", "content": "Table 2 reports the performance of all models introduced in Sec.5.1 on music understanding tasks (music + text \u2192 text) using MusicQA(Liu et al., 2024), MusicCaps (Agostinelli et al., 2023), and our constructed Music4way-MusicCaps.\nFirst, we observe that DeepResonance outperforms baseline models on three datasets, demonstrating the effectiveness of our proposed training datasets and model architecture. Second, we find that applying the same training data to the NEXT-GPT framework yields a better performance than using the M2UGen framework, suggesting that NEXT-GPT is a more suitable backbone model. Third, by expanding M2UGen's training data with our constructed Music4way dataset, we observe an improvement in performance, further validating"}, {"title": "Multimodal Music Understanding Tasks", "content": "Table 3 lists the results on our proposed multimodal music understanding tasks (music + image/video + text \u2192 text), including Music4way-MI2T, Music4way-MV2T, and Music4way-Any2T. We compare the performance of DeepResonance with baseline models capable of processing multiple modalities\u2014music, text, image, and video\u2014such as the M2UGen and NEXT-GPT-based models.\nFirst, we observe that Music4way-MI2T and Music4way-MV2T represent novel downstream tasks for which no existing baseline models are inherently equipped. Through supervised fine-tuning with our curated training data for these datasets, the DeepResonance models gain the ability to generate unified multimodal captions successfully. Second, on Music4way-Any2T, which features flexible inputs and open-ended question-answer pairs, the baseline models perform poorly. Their generalization remains weaker than DeepResonance models, highlighting their limitations in handling diverse input patterns. Third, comparing DeepResonance-a and DeepResonance-\u00df, we find that the latter demonstrates a superior performance on Music4way-MI2T and Music4way-MV2T. This indicates that the pre-alignment Transformer, with its additional parameters, effectively integrates multiple modalities, thereby improving the supervised performance in structured multimodal music understanding tasks. However, DeepResonance-B exhibits reduced robustness on Music4way-Any2T, which highlights a trade-off, as the model's increased complexity may hinder its adaptability with limited instruction tuning data."}, {"title": "Zero-shot Evaluation", "content": "Fig. 4 present the zero-shot performance of Deep-Resonance and baselines on music understanding benchmarks, including GTZAN (Tzanetakis, 2001), MusicNet (Thickstun et al., 2017), MTG-Jamendo (Bogdanov et al., 2019), MusicInstruct-short, and MusicInstruct-long (Deng et al., 2024). We adopt the benchmark settings outlined in OpenMU-bench (Zhao et al., 2024), combining captioning and reasoning test sets where separate splits exist. All NExT-GPT-based models without"}, {"title": "Ablation Study", "content": "Figs. 5 and 6 present the results of ablation studies evaluating the effectiveness of key components in the proposed methods, including Music4way-MI2T and Music4way-MV2T instruction tuning data (MWIT), multi-sampled ImageBind embeddings (MIE), and the pre-alignment Transformer (PT). For music + text \u2192 text tasks (Fig. 6), we observe that MWIT, MIE, and PT each contribute positively to performance. However, when combined, PT does not consistently complement the other two components across all three benchmarks, with MWIT + MIE (DeepResonance-\u03b1) yielding the consistent improvements. For multimodal music understanding tasks (music + image/video + text \u2192 text), we compare settings with and without MIE and PT in Fig. 5, as MWIT serves as the in-domain data for these tasks. Integrating MIE and PT enhances performance, with PT proving most effective when limited to a single Transformer layer. This highlights the effectiveness of MIE and PT while suggesting that increasing PT's parameters may lead to overfitting on limited instruction tuning data. Moreover, PT with different layer settings negatively impacts performance on Music4way-Any2T (refer to Sec. 5.3)."}, {"title": "Conclusion", "content": "In this study, we introduced DeepResonance, a multimodal music understanding LLM capable of comprehending music through its connections with other modalities, such as image and video. To train and evaluate DeepResonance, we developed new music-centric multi-way instruction-following datasets. In addition, we proposed modules designed to enhance music-centric multimodal instruction tuning. Empirical results highlight the effectiveness of DeepResonance across six music understanding tasks and zero-shot scenarios. Future work will explore more refined instruction tuning datasets to improve the model's generalization capabilities for music understanding tasks."}, {"title": "Limitations", "content": "The proposed methods have the following limitations:\n(1) The input music training data is mostly limited to clips shorter than 30 seconds, with a significant portion (e.g., AudioSet) being 10 seconds. This may restrict the fine-tuned models' performance on longer music sequences. Additionally, the dataset's image frames are directly extracted from videos, assuming relevance within short clips (10s). For longer videos, selecting the most representative frames, such as cover images, should be explored in future work.\n(2) The effectiveness of DeepResonance has been verified using Vicuna (based on LLaMA) as the backbone LLM, but further studies are needed to assess its improvements when applied to other LLM architectures.\n(3) The extracted music features (e.g., downbeats, tempo, key, chords) rely on existing MIR algorithms, which may have precision limitations. Incorporating more signal-oriented features such as MFCCs or Chromagrams could be explored in future work.\n(4) While the proposed methods perform well on supervised datasets included in the training data, further the enhancing generalization capability to out-of-domain (distribution-shifted) music remains an open challenge.\n(5) Generating instruction tuning data with LLMs is a well-established and widely accepted approach, as seen in Self-Instruct (Wang et al., 2023). Our instruction tuning data construction process is relatively simple, ensuring the overall reliability. However, as with any LLM-generated data, biases may exist, and users should be mindful of potential biases."}, {"title": "Ethical Considerations", "content": "In this study, we leveraged publicly available datasets (without licensing issues) to create new datasets for multimodal music understanding. The newly generated content consists solely of text produced by LLMs such as GPT-40 mini, with no originally generated music, images, or videos. We fine-tuned the multimodal music understanding model through instruction tuning. While the model has been adapted to a specific domain, it may still generate hallucinations or biased content due to the nature of LLM-based text generation. Users should exercise caution when using the generated content, be aware of the potential risks associated with LLM outputs, and implement content safety checks as a post-processing measure."}, {"title": "Discussion on Multi-way Alignment", "content": "The 4-way alignment introduced in this work, which connects music, text, image, and video, is facilitated by pairing music with video and video with image. Each modality is further linked to text through captioning or feature extraction. Therefore, the 4-way relationship is constructed from several 2-way mappings, with any pair among four modalities being closely correlated as they stem from a single original music-video pair. Future work may develop finer-grained multi-way alignment for music understanding tasks. We encourage further discussion and research on how to establish improved multi-way relationships across different modalities."}, {"title": "Training and Evaluation Details", "content": "Following the training strategy of NEXT-GPT and M2UGen, we train DeepResonance in two stages. In the first stage, we fine-tune only the parameters of the linear adaptors and the proposed pre-alignment Transformer. This stage focuses on captioning tasks for music, image, and video modalities, utilizing images from COCO (Lin et al., 2014) and music and video clips from the constructed Music4way dataset.In the second stage, we fine-tune the linear adaptors and the pre-alignment Transformer while simultaneously performing LoRA-based fine-tuning (Hu et al., 2022) on Vicuna. This stage incorporates instruction tuning tasks using datasets including Alpaca (Taori et al., 2023), MusicCaps (Agostinelli et al., 2023), and MusicQA (Liu et al., 2024), along with our constructed Music4way, Music4way-MI2T, and Music4way-MV2T datasets. A summary of all datasets used for training and evaluation is provided in Table 1. As depicted in Fig. 2, instructions are fed directly into the text-LLM, bypassing the adaptors and the pre-alignment Transformer, as they do not require interaction with the input information.\nWe fine-tune for 5 and 2 epochs in the first and second stages, respectively, utilizing a learning rate of 1e-4 and a batch size of 16. Training is conducted on 8 NVIDIA A100 GPUs (40GB each). For LoRA, the rank and alpha are both set to 32, following NExT-GPT. We train the pre-alignment Transformer with various layer configurations (see Sec. 5.5) and find that a single Transformer layer achieves the best performance. Regarding the trainable parameters, the linear adaptors, LoRA, pre-alignment Transformer, and LLaMA embedding layers contain 4M, 33M, 157M, and 262M parameters, respectively, comprising 5.6% of the total model parameters. Regarding the training budget, stage 1 took 25.2 hours, while stage 2 took 20.3 hours for DeepResonance-a. For DeepResonance-B, the training times were 27.0 hours for stage 1 and 22.7 hours for stage 2.\nFor evaluation, we report the mean results from three inference runs and include BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and BERTScore (Zhang et al., 2020), following the setup of M2UGen (Hussain et al., 2023). We report BLEU-1, BLEU, ROUGE-L precision, ROUGE-L recall, ROUGE-L F1, BERTScore precision, BERTScore recall, and BERTScore F1 details in Appx. E, F, G, and H."}]}