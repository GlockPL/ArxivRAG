{"title": "BEHRNOULLI: A Binary EHR Data Oriented\nMedication Recommendation System", "authors": ["Xihao Piao", "Pei Gao", "Zheng Chen", "Lingwei Zhu", "Yasuko Matsubara", "Yasushi Sakurai"], "abstract": "The medical community believes binary medical event out-\ncomes in EHR data contain sufficient information for making a sensible\nrecommendation. However, there are two challenges to effectively utiliz-\ning such data: (1) modeling the relationship between massive 0,1 event\noutcomes is difficult, even with expert knowledge; (2) in practice, learn-\ning can be stalled by the binary values since the equally important 0\nentries propagate no learning signals. Currently, there is a large gap be-\ntween the assumed sufficient information and the reality that no promis-\ning results have been shown by utilizing solely the binary data: visiting\nor secondary information is often necessary to reach acceptable perfor-\nmance. In this paper, we attempt to build the first successful binary EHR\ndata-oriented drug recommendation system by tackling the two difficul-\nties, making sensible drug recommendations solely using the binary EHR\nmedical records. To this end, we take a statistical perspective to view the\nEHR data as a sample from its cohorts and transform them into con-\ntinuous Bernoulli probabilities. The transformed entries not only model\na deterministic binary event with a distribution but also allow reflect-\ning event-event relationship by conditional probability. A graph neural\nnetwork is learned on top of the transformation. It captures event-event\ncorrelations while emphasizing event-to-patient features. Extensive re-\nsults demonstrate that the proposed method achieves state-of-the-art\nperformance on large-scale databases, outperforming baseline methods\nthat use secondary information by a large margin. The source code is\navailable at https://github.com/chenzRG/BEHRMecom", "sections": [{"title": "1 Introduction", "content": "Medication errors are one of the most serious medical errors that could threaten\npatients' lives. Studies indicate that over 42% of these errors stem from physi-\ncians or doctors with insufficient experience or knowledge concerning specific\ndrugs and diseases [28]. This challenge becomes even more pronounced with\nmore new drugs and treatment guidelines. On the other hand, many patients"}, {"title": "2 Related Works", "content": "EHR-based Medication Recommendation. Historical Visit-based Meth-\nods: Many studies utilize deep learning to extract features from state variations\nacross hospital visits in EHR data [5, 31, 30, 2, 25]. For example, DoctorAI [5]\nemploys RNN to learn the dependencies of patients' historical information and\nthe doctor's medication recommendations record for recommendation. MICRON"}, {"title": "3 Problem Formulation", "content": "3.1 Recommendation System Based on EHR\nEHR data consists of various medical events, such as multiple symptoms, recorded\nchronically from patients. Each data sample contains binary outcomes of these\nevents. Let $x \\in {0,1}^M$ denote a patient EHR sample, where M represents the\ntotal number of medical events, 1 stands for the occurrence of the correspond-\ning event, and 0 for the absence. The EHR data is a matrix $X \\in {0,1}^{N\\times M}$\ncomposed by stacking V patient vectors {$x$}$_{1:N}$.\nIt is especially difficult to build effective recommendation systems using solely\nthe EHR data, mainly due to the following reasons: (1) binary data poses a chal-"}, {"title": "3.2 EHR as a Sample from the Population", "content": "To tackle the binary data, we take a statistical perspective in this paper. Several\nstudies show many prevalent diseases and common medical conditions are often\nassociated with commonalities among populations, such as shared living environ-\nments or behavioral habits [1]. In terms of epidemiology, temporal specificity and\nspatial specificity inevitably lead to a closer medical-relatedness among patients\nwithin the same sub-cohort compared to patients belonging to other sub-cohorts\n[26, 1, 19]. Inspired by this observation, we make two assumptions:\nAssumption 1 the outcome of a medical event $E_i$ of the entire population is\nsubject to a Bernoulli distribution.\nAssumption 2 an EHR dataset is an i.i.d. sample from the population.\nAs such, an intuitive solution to the signal-stopping 0 entries is to represent\nthem with the Bernoulli mean of the corresponding medical event $1-P(E_i = 1)$.\nThis Bernoulli mean can be empirically estimated from the EHR dataset which\nis assumed to be subject to the identical distribution as the population.\nBy mapping the 0-1 events to Bernoulli statistics, the equally important\nYes/No outcomes have continuous representations. This can be important since\nthe problem of data sparsity has been one of the most prominent obstacles to\neffectively mining the EHR data, as zero entries in practice do not propagate\nany learning signal. For instance, in deep learning models, a value of 0 cannot\neffectively activate the connection weights within a projection layer. Similarly,\nin graph-based models, zero values have no contribution to node attributes and\nmessage passing between nodes. As Bernoulli's mean can take on any value be-\ntween 0 and 1, this transformation also allows for more effective representations\nof the patient-event/event-event relationship. Quantifying such a relationship\nhelps reflect the intrinsic connection between medical events, and contributes\nmore to accurate medication recommendation than binary signals [16]."}, {"title": "4 Methodology", "content": "4.1 Patient-Event Graph Construction\nWe propose to learn by graph neural networks (GNNs) the inherent relation-\nship between medical events within each patient sample as well as between an\nevent and the patient population. A system overview has been shown in Fig.\n2. Specifically, for each patient a graph $G_n = (V_n, E_n)$ is constructed, where $V_n$\ndenote the set of vertices, corresponding to the patient-event relationship; $E_n$ the\nset of edges representing the event-event relationship. If we na\u00efvely initialized\nthe $V_n, E_n$ by the elements of X, the zero entries may cause significant loss of\ninformation of the non-occurrence of events since zero values typically stall the\npropagation of learning signals. Therefore, we propose to initialize them in the\nfollowing approach.\nPatient-to-patient correlation embedding. By assumption (i) in Section\n3.2, the occurrence of an event is subject to a Bernoulli distribution $P(E_j =$\n$k) = p^k (1 - p)^{k}$, where $k \\in {0,1}$, and p denotes the mean of the distribution.\nBy assumption (ii), the event in the EHR data sample is subject to the same\ndistribution $P(X_{\\cdot j} = k) = p^{k}(1 - p)^{k}$, where we denote the $X_{\\cdot j}$ denote the event\n$E_j$ for all patient samples. Therefore, we propose to initialize the vertices by the\nBernoulli mean p. While this quantity is not known, we can replace it with a\nsample estimate:\n$\\hat{p} := P(X_{\\cdot j} = 1) = \\frac{\\sum_{i=1}^N X_{ij}}{N}, P(X_{\\cdot j} = 0) = 1 - \\hat{p}$.\nEvent-to-event correlation embedding. Following this idea, we propose to\ninitialize the edges representing the event-to-event relationship by conditional"}, {"title": "4.2 Patient-to-Event Graph Learning", "content": "In Section 4.1, we mapped each patient sample $X_i.$ to a graph $G_i = (V_i, E_i)$, with\nempirical Bernoulli mean initialization. This transformation converts the original\ndrug recommendation binary classification problem into a graph classification\nproblem. Specifically, we first map the EHR data into a set of vertices and\nedges ${0,1}^M \\rightarrow V_i,E_i \\in [0,1]^M$, where B.I. denote Bernoulli Initialization. A\ngraph learning function is then applied to output final drug recommendations\n$F(\\cdot) : V_i, E_i \\rightarrow {0,1}^C$. We employ EGraphSage [12] as our F. We obtain the\nnode and edge embeddings $h_v, h_e$ by passing them through a simple MLP. Note\nthat the first layer of the MLP has an interpretation of nonlinearly transforming\nthe Bernoulli statistics residing on the nodes and edges we initialized. The graph\nlearning process aims to discern correlations between events through message"}, {"title": "4.3 Medication Recommendation", "content": "After K iterations, the last layer's updated node feature ($h'_v, \\forall v\\in V$) serves\nas the output of our graph embedding learning part. After obtaining the node\nembeddings $h'_v$ for each graph, we concatenate these embeddings to form a single\nfeature vector for each graph. This is represented as: $z_{G_i} = \\oplus(v\\in v)h'_v,$ Where $z_{G_i}$\nis the concatenated feature vector for graph $G_i$ and $\\oplus$ denotes the concatenation\noperation over all nodes (i.e., medical events).\nThe concatenated feature vectors $z_{G_i}$ for each graph are then passed through\na linear projection for classification. The linear layer has Y independent output\ndimensions, each corresponding to a binary classification task. Formally, for each\noutput dimension y, the operation is:\n$o_y = sigmoid (Linear (z_{G_i}))$\n$r_y = \\begin{cases}\n1 & \\text{if } o_y > 0.5 \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nUpon processing each graph's feature vector $z_{G_i}$, the system yields Y inde-\npendent outputs. Each of these outputs specifically corresponds to the recom-\nmendation decision for a drug. The decision is binary: either the medication is\nrecommended (represented by a value of 1) or not (represented by 0)."}, {"title": "5 Experiments", "content": "5.1 Databases\nWe evaluated the performance of BEHRNOULLI on two large-scale datasets: the\nMedical Information Mart for Intensive Care (MIMIC-III) dataset and the An-\ntimicrobial Resistance in Urinary Tract Infections (AMR-UTI). Both databases\nhave been extensively utilized for benchmarking and are available on PhysioNet.\nMIMIC-III represents a vast repository of de-identified health data from\nintensive care unit (ICU) patients at the Beth Israel Deaconess Medical Center\nin Boston, Massachusetts [10]. It contains a total of 46520 patients from 2001\nto 2012. In this database, the patient data we used was sourced from three sub-\ndatasets, 'ADMISSIONS', 'DIAGNOSES_ICD', and 'PROCEDURES_ICD'.\nAMR-UTI dataset contains EHR\ninformation collected from 51878 pa-\ntients from 2007 to 2016, with uri-\nnary tract infections (UTI) treated\nat Massachusetts General Hospital\nand Brigham & Women's Hospital\nin Boston, MA, USA [14]. Each pa-\ntient in this dataset provided a se-\nries of binary indicators for whether\nthe patient was undergoing a specific\nmedical event. There are four com-\nData preprocessing. For MIMIC-III, we referred to the data processing re-\nleased by [36, 33] for fairness. Only the patients with at least 2 visits are incor-\nporated. The medications were selected and retained based on their frequency of\noccurrence (the top 300). For AMR-UTI, we first excluded the basic demographic\ninformation including age and ethnicity. The observations that do not have any\nhealth event or drug recommendation were removed. For each observation, a\nfeature was constructed from its EHR as a binary indicator for whether the pa-\ntient is undergoing a particular medical event within a specified time window.\nAfter the above preprocessing, we divided both datasets into training, validation\nand testing by the ratio of 3/5, 1/5, and 1/5. More detailed description of data\nprocessing can be found in Appendix B.\n5.2 Baselines\nWe selected three categories of methods as the baseline: statistics-based, RNN-\nbased, and GNN-based models."}, {"title": "5.3 Ablation Study", "content": "We conducted several ablations for evaluating different modeling methods of\nnode/edge attributes. The main contributions of BEHRNOULLI lie in the trans-\nformation of binary medical events into continuous values (Bernoulli means).\nWe selected two classic methods for encoding categorical features into contin-\nuous values: Log Likelihood Ratio (LLR) and Target Encoding (TE) [21]. The\ndetailed descriptions of these two methods can be found in Appendix D. We\nfurther compared against random initialization of nodes and edges for ablation\nevaluation, and assessed BEHRNOULLI without node or edge construction. The\ndetails of ablation study are as follows:\nBEHRNOULLI : BM (Bernoulli means)w/ Post (posterior): Our proposal.\nBM w/o Post: we maintained the node embedding but replace the posterior\nedge embedding with the simple co-occurrence between medical events.\nLLR w/ Post: we replaced the Bernoulli means node embedding with LLR.\nLLR w/o Post: we used LLR as the simple co-occurrence embedding.\n\u03a4\u0395 w Post: we replaced the Bernoulli means node embedding with TE.\nTE w/o Post: we used TE as the simple co-occurrence embedding.\nBM w/ RE: we replaced the posterior edge embedding with random edge\n(RE) initialization.\nRN w/ Post: we replaced the Bernoulli means node embedding with random\nnode (RN) initialization."}, {"title": "6 Results", "content": "6.1 Comparison Against Baselines\nMain Results. Table 2 shows the main results for medication recommendation\non two databases. Overall, our proposed BEHRNOULLI outperforms all base-\nlines on all metrics (Jaccard, F1, PRAUC, and AUROC). Given that most re-\nsults were directly collected from the original study [33], BEHRNOULLI demon-\nstrates superior performance for the MIMIC-III dataset. It outperforms other\napproaches, even those that incorporate additional data types, secondary infor-\nmation, and visit modeling for medication recommendations. Some statistical\nmodels, such as LR and ECC, outperform early RNN-based baselines LEAP,\nDMNC, and RETAIN, demonstrating ECC achieves the best results in Jaccard\nand F1. The difficulties of RNN-based models lie in handling missing historical\ndata, inconsistent data formats in historical records, and the inability to handle\nsituations where no historical data is available. Similar results have been shown\nin AMR-UTI; given that six visit datasets could serve as the RNN's timesteps, it\noften vanishes gradients within various model fine-tunings. Graph-based meth-\nods tend to converge well and result in better performances compared to RNN\nbaselines. This shows the effectiveness of modeling event-event correlations and\nstructure information. While BEHRNOULLI is solely based on binary medical\nevents, further integrating this binary data modeling with external EHR infor-\nmation, like DDI, could enhance recommendation performance."}, {"title": "6.2 Ablation Study", "content": "For ablations on node and edge embeddings, Table 4 shows that among vari-\nous embedding strategies on both datasets, combining our Bernoulli mean node\nembedding with posterior probability edge embedding achieves the best results.\nIf we compare the contributions of node (Bernoulli mean) and edge (posterior\nprobability) embeddings to the graph-based drug recommendation results, we\nfind that node embedding has a relatively more substantial impact. The random\nnode destroys the recommendation performance even when considering poste-\nrior probabilities, resulting in a significantly low Jaccard score of 0.29. However,\nusing the Bernoulli mean for edge embedding initialization yields a much bet-\nter performance, with a Jaccard score of 0.5692. This could be because node\nembedding directly represents the original EHR data, while edges are inferred\nbased on the original data to model the relationship between different events.\nOn the other hand, the purpose of using GNN is to capture event-event correla-\ntions. Therefore, edge embeddings merely aid in GNN computations. The results\nshow that even without initializing edge embeddings with posterior probabili-\nties, GNN can still capture event-event correlations, though with a performance\ndecrease. For model ablation, GNN outperforms all baselines (i.e., MLP, LSTM,\nand Transformer) on two datasets."}, {"title": "6.3 Visualization of Recommendations", "content": "Fig. 4 shows representations learned by GNN for two different patients, on a sub-\nset of EHR medical events. GNN produces distinct feature values for different\nnodes, for visualization and better understanding, we processed the node feature\nvalues using the TOP-K approach, highlighting the top K nodes with the high-\nest feature values. It can be observed that GNN has learned completely different\nrepresentations for different patients and has emphasized different features on\nvarious nodes. For instance, the \"Hyperlipidemia\" node, which is highlighted in\nPatient B's data, receives a lower weight in Patient A's data. This indicates that\nthis event is more critical in Patient B's data compared to Patient A. Moreover,\nGNN has learned certain connections between important features for each pa-\ntient. This result highlights how the same event can exhibit entirely different\nlevels of importance across different patients' data. For example, in Patient B's\ndata, three identical series (MDII) of medical events are simultaneously high-\nlighted. Through this simple analysis, we confirmed that GNN learned different\nrepresentations for different patients, providing valuable information for down-\nstream tasks. More results can be found in Appendix F."}, {"title": "7 Conclusion", "content": "In this paper we proposed the first binary EHR data oriented drug recommen-\ndation system, by transforming the problematic 0,1 binary event outcomes to\ncontinuous Bernoulli means. We took a statistical perspective by viewing the\nEHR data as a sample from the greater population. In this manner, we modeled\nthe nodes and edges of GNN using the Bernoulli distribution. Extensive results\nshowed that our method attained the SOTA and outperformed all baselines by\na large margin; and some of the baselines learned from significantly more in-\nformation resources such as the visiting data and drug-drug interactions, while"}, {"title": "A Source Code", "content": "The code link is: https://github.com/chenzRG/BEHRMecom"}, {"title": "B Preprocessing", "content": "B.1 Data Profile\nData Sources: The data is sourced from the MIMIC dataset, a publicly avail-\nable dataset developed by the MIT Lab for Computational Physiology, compris-\ning de-identified health data from over 40,000 critical care patients. The primary\ndatasets in focus for preprocessing include:\nMedications ('PRESCRIPTIONS.csv\u2018): Contains records of medications pre-\nscribed to patients during their ICU stays.\nDiagnoses ('DIAGNOSES ICD.csv'): Lists diagnosis codes associated with\neach hospital admission.\nProcedures ('PROCEDURES ICD.csv'): Catalog procedures that patients\nunderwent during their hospital visits.\nData Structure: The data is structured in tabular format. Each table contains\nunique identifiers for patients (\u2018SUBJECT ID'), their specific hospital admissions\n('HADM ID'), and other clinical details pertinent to the dataset in question."}, {"title": "B.2 Medication Data Preprocessing", "content": "Data Loading and Initial Processing:\nLoad the Data: The medication data from 'PRESCRIPTIONS.csv' is loaded\ninto a data frame.\nFilter Columns: Only essential columns, namely 'pid', 'adm id', 'date', and\n'NDC', are retained.\nData Cleaning: Entries with 'NDC' equal to '0' are dropped, and any missing\nvalues are filled using the forward-fill method.\nData Transformation: The 'STARTDATE' field is converted to a DateTime\nformat for easier manipulation and analysis.\nSorting and Deduplication: The dataset is sorted by multiple columns, in-\ncluding 'SUBJECT ID', 'HADM ID', and 'STARTDATE', and any duplicate\nentries are removed.\nMedication Code Mapping:\nMapping NDC to RXCUI: The 'NDC' codes are mapped to the 'RXCUI'\nidentifiers using a separate mapping file.\nMapping RXCUI to ATC4: The 'RXCUI' identifiers are further mapped to\nthe 'ATC4' codes using another mapping file. The ATC classification system\nis crucial for grouping drugs into different classes based on their therapeutic\nuse."}, {"title": "B.3 Diagnosis Data Preprocessing", "content": "The diagnosis data from 'DIAGNOSES ICD.csv' is processed using the 'diag\nprocess' function, which involves:\nLoad the Data: Diagnosis data is loaded into a data frame.\nFilter Relevant Columns: Only essential columns, which might include diag-\nnosis codes and patient identifiers, are retained.\nHandle Missing or Erroneous Entries: Any missing values or errors in the\ndataset are addressed, ensuring data integrity.\nSorting and Deduplication: The dataset is sorted based on relevant columns,\nand duplicate entries are removed to ensure each record is unique."}, {"title": "B.4 Procedure Data Preprocessing", "content": "Similar to the diagnosis data, the procedure data from 'PROCEDURES ICD.csv':\nLoad the Data: Procedure data is loaded into a DataFrame.\nFilter and Clean: Only relevant columns are retained, and any erroneous or\nmissing entries are addressed.\nSorting and Deduplication: The data is organized by relevant columns, and\nany duplicate records are removed."}, {"title": "B.5 Data Integration", "content": "Combining Process: The datasets, once cleaned and preprocessed, need to be\nintegrated for comprehensive analysis:\nData Merging: The medication, diagnosis, and procedure data are merged\nbased on common identifiers like 'SUBJECT ID' and 'HADM ID'.\nFinal Cleaning: Any discrepancies resulting from the merge, such as missing\nvalues or duplicates, are addressed."}, {"title": "C Model Parameters", "content": "GNN Model (EGraphSage) The Graph Neural Network model is implemented\nusing the EGraphSage class. The configuration parameters for the model are in\nthe Table 5"}, {"title": "D Metrics", "content": "Target encoding is a popular technique in machine learning for encoding cat-\negorical features, where each category is replaced with its corresponding mean\ntarget value. This can be mathematically expressed as:\n$Target Encoding(c) = \\frac{\\sum_{i=1}^N y_i \\cdot \\delta(c, c_i)}{\\sum_{i=1}^N \\delta(c, c_i)}$"}, {"title": "E Implementation Details", "content": "The models mentioned were implemented by PyTorch 2.0.1 based on Python\n3.8.8, All experiments are conducted on an Intel Core i9-10980XE machine with\n125G RAM and an NVIDIA GeForce RTX 3090. For the experiments on both\ndatasets, we chose the optimal hyperparameters based on the validation set.\nModels were trained on Adam optimizer with learning rate $1 \\times 10^{-4}$ for 200\nepochs. The random seed was fixed as 0 for PyTorch to ensure the reproducibility\nof the models.\nFor a fair comparison, we employed bootstrapping sampling instead of cross-\nvalidation in the testing phase. Specifically, in each evaluation round, we ran-\ndomly sampled 80% of the data from the test set. We repeated this process 10"}]}