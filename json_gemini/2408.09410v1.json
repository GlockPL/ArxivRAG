{"title": "BEHRNOULLI: A Binary EHR Data Oriented Medication Recommendation System", "authors": ["Xihao Piao", "Pei Gao", "Zheng Chen", "Lingwei Zhu", "Yasuko Matsubara", "Yasushi Sakurai"], "abstract": "The medical community believes binary medical event outcomes in EHR data contain sufficient information for making a sensible recommendation. However, there are two challenges to effectively utilizing such data: (1) modeling the relationship between massive 0,1 event outcomes is difficult, even with expert knowledge; (2) in practice, learning can be stalled by the binary values since the equally important 0 entries propagate no learning signals. Currently, there is a large gap between the assumed sufficient information and the reality that no promising results have been shown by utilizing solely the binary data: visiting or secondary information is often necessary to reach acceptable performance. In this paper, we attempt to build the first successful binary EHR data-oriented drug recommendation system by tackling the two difficulties, making sensible drug recommendations solely using the binary EHR medical records. To this end, we take a statistical perspective to view the EHR data as a sample from its cohorts and transform them into continuous Bernoulli probabilities. The transformed entries not only model a deterministic binary event with a distribution but also allow reflecting event-event relationship by conditional probability. A graph neural network is learned on top of the transformation. It captures event-event correlations while emphasizing event-to-patient features. Extensive results demonstrate that the proposed method achieves state-of-the-art performance on large-scale databases, outperforming baseline methods that use secondary information by a large margin. The source code is available at https://github.com/chenzRG/BEHRMecom", "sections": [{"title": "1 Introduction", "content": "Medication errors are one of the most serious medical errors that could threaten patients' lives. Studies indicate that over 42% of these errors stem from physicians or doctors with insufficient experience or knowledge concerning specific drugs and diseases [28]. This challenge becomes even more pronounced with more new drugs and treatment guidelines. On the other hand, many patients"}, {"title": "2 Related Works", "content": "EHR-based Medication Recommendation. Historical Visit-based Meth-ods: Many studies utilize deep learning to extract features from state variations across hospital visits in EHR data [5, 31, 30, 2, 25]. For example, DoctorAI [5] employs RNN to learn the dependencies of patients' historical information and the doctor's medication recommendations record for recommendation. MICRON"}, {"title": "3 Problem Formulation", "content": "3.1 Recommendation System Based on EHR\nEHR data consists of various medical events, such as multiple symptoms, recorded chronically from patients. Each data sample contains binary outcomes of these events. Let $x \\in \\{0,1\\}^M$ denote a patient EHR sample, where M represents the total number of medical events, 1 stands for the occurrence of the corresponding event, and 0 for the absence. The EHR data is a matrix $X \\in \\{0,1\\}^{N \\times M}$ composed by stacking V patient vectors $\\{x\\}_{1:N}$.\nIt is especially difficult to build effective recommendation systems using solely the EHR data, mainly due to the following reasons: (1) binary data poses a chal-"}, {"title": "3.2 EHR as a Sample from the Population", "content": "To tackle the binary data, we take a statistical perspective in this paper. Several studies show many prevalent diseases and common medical conditions are often associated with commonalities among populations, such as shared living environments or behavioral habits [1]. In terms of epidemiology, temporal specificity and spatial specificity inevitably lead to a closer medical-relatedness among patients within the same sub-cohort compared to patients belonging to other sub-cohorts [26, 1, 19]. Inspired by this observation, we make two assumptions:\nAssumption 1 the outcome of a medical event $E_i$ of the entire population is subject to a Bernoulli distribution.\nAssumption 2 an EHR dataset is an i.i.d. sample from the population.\nAs such, an intuitive solution to the signal-stopping 0 entries is to represent them with the Bernoulli mean of the corresponding medical event 1-P(E\u2081 = 1). This Bernoulli mean can be empirically estimated from the EHR dataset which is assumed to be subject to the identical distribution as the population.\nBy mapping the 0-1 events to Bernoulli statistics, the equally important Yes/No outcomes have continuous representations. This can be important since the problem of data sparsity has been one of the most prominent obstacles to effectively mining the EHR data, as zero entries in practice do not propagate any learning signal. For instance, in deep learning models, a value of 0 cannot effectively activate the connection weights within a projection layer. Similarly, in graph-based models, zero values have no contribution to node attributes and message passing between nodes. As Bernoulli's mean can take on any value between 0 and 1, this transformation also allows for more effective representations of the patient-event/event-event relationship. Quantifying such a relationship helps reflect the intrinsic connection between medical events, and contributes more to accurate medication recommendation than binary signals [16]."}, {"title": "4 Methodology", "content": "4.1 Patient-Event Graph Construction\nWe propose to learn by graph neural networks (GNNs) the inherent relation-ship between medical events within each patient sample as well as between an event and the patient population. A system overview has been shown in Fig. 2. Specifically, for each patient a graph $G_n = (V_n, E_n)$ is constructed, where $V_n$ denote the set of vertices, corresponding to the patient-event relationship; $E_n$ the set of edges representing the event-event relationship. If we na\u00efvely initialized the $V_n, E_n$ by the elements of X, the zero entries may cause significant loss of information of the non-occurrence of events since zero values typically stall the propagation of learning signals. Therefore, we propose to initialize them in the following approach.\nPatient-to-patient correlation embedding. By assumption (i) in Section 3.2, the occurrence of an event is subject to a Bernoulli distribution $P(E_j = k) = p^k (1 \u2212 p)^{k}$, where $k \\in \\{0,1\\}$, and p denotes the mean of the distribution. By assumption (ii), the event in the EHR data sample is subject to the same distribution $P(X_{.j} = k) = p^k(1 \u2212 p)^{k}$, where we denote the $X_{.j}$ denote the event $E_j$ for all patient samples. Therefore, we propose to initialize the vertices by the Bernoulli mean p. While this quantity is not known, we can replace it with a sample estimate:\n$p := P(X_{.j} = 1) = \\frac{\\sum_{i=1}^{N} X_{ij}}{N}, P(X_{.j} = 0) = 1 \u2212 p$.\nEvent-to-event correlation embedding. Following this idea, we propose to initialize the edges representing the event-to-event relationship by conditional"}, {"title": "4.2 Patient-to-Event Graph Learning", "content": "In Section 4.1, we mapped each patient sample $X_i$ to a graph $G_i = (V_i, E_i)$, with empirical Bernoulli mean initialization. This transformation converts the original drug recommendation binary classification problem into a graph classification problem. Specifically, we first map the EHR data into a set of vertices and edges $\\{0,1\\}^M \\xrightarrow{\\text{B.I.}} V_i,E_i \\in [0,1]^M$, where B.I. denote Bernoulli Initialization. A graph learning function is then applied to output final drug recommendations $F(\u22c5): V_i, E_i \u2192 \\{0,1\\}^C$. We employ EGraphSage [12] as our F. We obtain the node and edge embeddings $h_v, h_e$ by passing them through a simple MLP. Note that the first layer of the MLP has an interpretation of nonlinearly transforming the Bernoulli statistics residing on the nodes and edges we initialized. The graph learning process aims to discern correlations between events through message"}, {"title": "4.3 Medication Recommendation", "content": "After K iterations, the last layer's updated node feature ($h'_v,\u2200v\u2208V$) serves as the output of our graph embedding learning part. After obtaining the node embeddings $h'_v$ for each graph, we concatenate these embeddings to form a single feature vector for each graph. This is represented as: $z_{g_i} = \u2295(v\u2208v)h'_v$, Where $z_{g_i}$ is the concatenated feature vector for graph $G_i$ and $\u2295$ denotes the concatenation operation over all nodes (i.e., medical events).\nThe concatenated feature vectors $z_{g_i}$ for each graph are then passed through a linear projection for classification. The linear layer has Y independent output dimensions, each corresponding to a binary classification task. Formally, for each output dimension y, the operation is:\n$o_y = \\text{sigmoid} (\\text{Linear} (z_{g_i}))$\n$r_y = \\begin{cases}\n1 & \\text{if } o_y > 0.5 \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nUpon processing each graph's feature vector $z_{g_i}$, the system yields Y inde-pendent outputs. Each of these outputs specifically corresponds to the recommendation decision for a drug. The decision is binary: either the medication is recommended (represented by a value of 1) or not (represented by 0)."}, {"title": "5 Experiments", "content": "5.1 Databases\nWe evaluated the performance of BEHRNOULLI on two large-scale datasets: the Medical Information Mart for Intensive Care (MIMIC-III) dataset and the An-timicrobial Resistance in Urinary Tract Infections (AMR-UTI). Both databases have been extensively utilized for benchmarking and are available on PhysioNet.\nMIMIC-III represents a vast repository of de-identified health data from intensive care unit (ICU) patients at the Beth Israel Deaconess Medical Center in Boston, Massachusetts [10]. It contains a total of 46520 patients from 2001 to 2012. In this database, the patient data we used was sourced from three sub-datasets, 'ADMISSIONS', 'DIAGNOSES_ICD', and 'PROCEDURES_ICD'.\nAMR-UTI dataset contains EHR information collected from 51878 pa-tients from 2007 to 2016, with uri-nary tract infections (UTI) treated at Massachusetts General Hospital and Brigham & Women's Hospital in Boston, MA, USA [14]. Each pa-tient in this dataset provided a se-ries of binary indicators for whether the patient was undergoing a specific medical event. There are four com-\nTable 1 shows some statistics on two databases. The sparsity of a matrix can be calculated using the formula: Sparsity = $1 - \\frac{\\text{Number of Non-Zero Elements}}{\\text{Total Number of Elements}}$.\nThe high degree of sparsity observed in the feature dimensions for both the MIMIC-III and AMR-URI datasets may constitute a significant impediment to the efficacy of the machine learning models.\nData preprocessing. For MIMIC-III, we referred to the data processing re-leased by [36, 33] for fairness. Only the patients with at least 2 visits are incor-porated. The medications were selected and retained based on their frequency of occurrence (the top 300). For AMR-URI, we first excluded the basic demographic information including age and ethnicity. The observations that do not have any health event or drug recommendation were removed. For each observation, a feature was constructed from its EHR as a binary indicator for whether the pa-tient is undergoing a particular medical event within a specified time window. After the above preprocessing, we divided both datasets into training, validation and testing by the ratio of 3/5, 1/5, and 1/5. More detailed description of data processing can be found in Appendix B."}, {"title": "5.2 Baselines", "content": "We selected three categories of methods as the baseline: statistics-based, RNN-based, and GNN-based models."}, {"title": "5.3 Ablation Study", "content": "We conducted several ablations for evaluating different modeling methods of node/edge attributes. The main contributions of BEHRNOULLI lie in the trans-formation of binary medical events into continuous values (Bernoulli means). We selected two classic methods for encoding categorical features into contin-uous values: Log Likelihood Ratio (LLR) and Target Encoding (TE) [21]. The detailed descriptions of these two methods can be found in Appendix D. We further compared against random initialization of nodes and edges for ablation evaluation, and assessed BEHRNOULLI without node or edge construction. The details of ablation study are as follows:\nBEHRNOULLI : BM (Bernoulli means)w/ Post (posterior): Our proposal.\nBM w/o Post: we maintained the node embedding but replace the posterior edge embedding with the simple co-occurrence between medical events.\nLLR w/ Post: we replaced the Bernoulli means node embedding with LLR.\nLLR w/o Post: we used LLR as the simple co-occurrence embedding.\n\u03a4\u0395 w Post: we replaced the Bernoulli means node embedding with TE.\nTE w/o Post: we used TE as the simple co-occurrence embedding.\nBM w/ RE: we replaced the posterior edge embedding with random edge (RE) initialization.\nRN w/ Post: we replaced the Bernoulli means node embedding with random node (RN) initialization.\nRN w/ RE: Both Bernoulli means and posterior embedding were replaced with random initialization.\nMoreover, we conducted model ablation to evaluate the event-event graphical correlation modeling compared to different model strategy of simple linear pro-jection (MLP), LSTM and Transformer. All of the implementation details of the baseline comparison and ablation study can be found in Appendix E."}, {"title": "6 Results", "content": "6.1 Comparison Against Baselines\nMain Results. Table 2 shows the main results for medication recommendation on two databases. Overall, our proposed BEHRNOULLI outperforms all base-lines on all metrics (Jaccard, F1, PRAUC, and AUROC). Given that most re-sults were directly collected from the original study [33], BEHRNOULLI demon-strates superior performance for the MIMIC-III dataset. It outperforms other approaches, even those that incorporate additional data types, secondary infor-mation, and visit modeling for medication recommendations. Some statistical models, such as LR and ECC, outperform early RNN-based baselines LEAP, DMNC, and RETAIN, demonstrating ECC achieves the best results in Jaccard and F1. The difficulties of RNN-based models lie in handling missing historical data, inconsistent data formats in historical records, and the inability to handle situations where no historical data is available. Similar results have been shown in AMR-UTI; given that six visit datasets could serve as the RNN's timesteps, it often vanishes gradients within various model fine-tunings. Graph-based meth-ods tend to converge well and result in better performances compared to RNN baselines. This shows the effectiveness of modeling event-event correlations and structure information. While BEHRNOULLI is solely based on binary medical events, further integrating this binary data modeling with external EHR infor-mation, like DDI, could enhance recommendation performance."}, {"title": "6.2 Ablation Study", "content": "For ablations on node and edge embeddings, Table 4 shows that among vari-ous embedding strategies on both datasets, combining our Bernoulli mean node embedding with posterior probability edge embedding achieves the best results. If we compare the contributions of node (Bernoulli mean) and edge (posterior probability) embeddings to the graph-based drug recommendation results, we find that node embedding has a relatively more substantial impact. The random node destroys the recommendation performance even when considering poste-rior probabilities, resulting in a significantly low Jaccard score of 0.29. However, using the Bernoulli mean for edge embedding initialization yields a much bet-ter performance, with a Jaccard score of 0.5692. This could be because node embedding directly represents the original EHR data, while edges are inferred based on the original data to model the relationship between different events. On the other hand, the purpose of using GNN is to capture event-event correla-tions. Therefore, edge embeddings merely aid in GNN computations. The results show that even without initializing edge embeddings with posterior probabili-ties, GNN can still capture event-event correlations, though with a performance decrease. For model ablation, GNN outperforms all baselines (i.e., MLP, LSTM, and Transformer) on two datasets."}, {"title": "6.3 Visualization of Recommendations", "content": "Fig. 4 shows representations learned by GNN for two different patients, on a sub-set of EHR medical events. GNN produces distinct feature values for different nodes, for visualization and better understanding, we processed the node feature values using the TOP-K approach, highlighting the top K nodes with the high-est feature values. It can be observed that GNN has learned completely different representations for different patients and has emphasized different features on various nodes. For instance, the \"Hyperlipidemia\" node, which is highlighted in Patient B's data, receives a lower weight in Patient A's data. This indicates that this event is more critical in Patient B's data compared to Patient A. Moreover, GNN has learned certain connections between important features for each pa-tient. This result highlights how the same event can exhibit entirely different levels of importance across different patients' data. For example, in Patient B's data, three identical series (MDII) of medical events are simultaneously high-lighted. Through this simple analysis, we confirmed that GNN learned different representations for different patients, providing valuable information for down-stream tasks. More results can be found in Appendix F."}, {"title": "7 Conclusion", "content": "In this paper we proposed the first binary EHR data oriented drug recommen-dation system, by transforming the problematic 0,1 binary event outcomes to continuous Bernoulli means. We took a statistical perspective by viewing the EHR data as a sample from the greater population. In this manner, we modeled the nodes and edges of GNN using the Bernoulli distribution. Extensive results showed that our method attained the SOTA and outperformed all baselines by a large margin; and some of the baselines learned from significantly more in-formation resources such as the visiting data and drug-drug interactions, while"}, {"title": "A Source Code", "content": "The code link is: https://github.com/chenzRG/BEHRMecom"}, {"title": "B Preprocessing", "content": "B.1 Data Profile\nData Sources: The data is sourced from the MIMIC dataset, a publicly avail-able dataset developed by the MIT Lab for Computational Physiology, compris-ing de-identified health data from over 40,000 critical care patients. The primary datasets in focus for preprocessing include:\nMedications ('PRESCRIPTIONS.csv\u2018): Contains records of medications pre-scribed to patients during their ICU stays.\nDiagnoses ('DIAGNOSES ICD.csv'): Lists diagnosis codes associated with each hospital admission.\nProcedures ('PROCEDURES ICD.csv'): Catalog procedures that patients underwent during their hospital visits.\nData Structure: The data is structured in tabular format. Each table contains unique identifiers for patients (\u2018SUBJECT ID'), their specific hospital admissions ('HADM ID'), and other clinical details pertinent to the dataset in question."}, {"title": "B.2 Medication Data Preprocessing", "content": "Data Loading and Initial Processing:\nLoad the Data: The medication data from 'PRESCRIPTIONS.csv' is loaded into a data frame.\nFilter Columns: Only essential columns, namely 'pid', 'adm id', 'date', and 'NDC', are retained.\nData Cleaning: Entries with 'NDC' equal to '0' are dropped, and any missing values are filled using the forward-fill method.\nData Transformation: The 'STARTDATE' field is converted to a DateTime format for easier manipulation and analysis.\nSorting and Deduplication: The dataset is sorted by multiple columns, in-cluding 'SUBJECT ID', 'HADM ID', and 'STARTDATE', and any duplicate entries are removed.\nMedication Code Mapping:\nMapping NDC to RXCUI: The 'NDC' codes are mapped to the 'RXCUI' identifiers using a separate mapping file.\nMapping RXCUI to ATC4: The 'RXCUI' identifiers are further mapped to the 'ATC4' codes using another mapping file. The ATC classification system is crucial for grouping drugs into different classes based on their therapeutic use."}, {"title": "B.3 Diagnosis Data Preprocessing", "content": "The diagnosis data from 'DIAGNOSES ICD.csv' is processed using the 'diag process' function, which involves:\nLoad the Data: Diagnosis data is loaded into a data frame.\nFilter Relevant Columns: Only essential columns, which might include diag-nosis codes and patient identifiers, are retained.\nHandle Missing or Erroneous Entries: Any missing values or errors in the dataset are addressed, ensuring data integrity.\nSorting and Deduplication: The dataset is sorted based on relevant columns, and duplicate entries are removed to ensure each record is unique."}, {"title": "B.4 Procedure Data Preprocessing", "content": "Similar to the diagnosis data, the procedure data from 'PROCEDURES ICD.csv':\nLoad the Data: Procedure data is loaded into a DataFrame.\nFilter and Clean: Only relevant columns are retained, and any erroneous or missing entries are addressed.\nSorting and Deduplication: The data is organized by relevant columns, and any duplicate records are removed."}, {"title": "B.5 Data Integration", "content": "Combining Process: The datasets, once cleaned and preprocessed, need to be integrated for comprehensive analysis:\nData Merging: The medication, diagnosis, and procedure data are merged based on common identifiers like 'SUBJECT ID' and 'HADM ID'.\nFinal Cleaning: Any discrepancies resulting from the merge, such as missing values or duplicates, are addressed."}, {"title": "C Model Parameters", "content": "GNN Model (EGraphSage) The Graph Neural Network model is implemented using the EGraphSage class. The configuration parameters for the model are in the Table 5"}, {"title": "D Metrics", "content": "Target encoding is a popular technique in machine learning for encoding cat-egorical features, where each category is replaced with its corresponding mean target value. This can be mathematically expressed as:\n$\\text{Target Encoding}(c) = \\frac{\\sum_{i=1}^{N} y_i \u22c5 \u03b4(c, c_i)}{\\sum_{i=1}^{N} \u03b4(c, c_i)}$"}, {"title": "E Implementation Details", "content": "The models mentioned were implemented by PyTorch 2.0.1 based on Python 3.8.8, All experiments are conducted on an Intel Core i9-10980XE machine with 125G RAM and an NVIDIA GeForce RTX 3090. For the experiments on both datasets, we chose the optimal hyperparameters based on the validation set. Models were trained on Adam optimizer with learning rate 1 \u00d7 10-4 for 200 epochs. The random seed was fixed as 0 for PyTorch to ensure the reproducibility of the models.\nFor a fair comparison, we employed bootstrapping sampling instead of cross-validation in the testing phase. Specifically, in each evaluation round, we ran-domly sampled 80% of the data from the test set. We repeated this process 10"}]}