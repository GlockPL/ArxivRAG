{"title": "AddrLLM: Address Rewriting via Large Language Model on Nationwide Logistics Data", "authors": ["Qinchen Yang", "Zhiqing Hong", "Dongjiang Cao", "Haotian Wang", "Zejun Xie", "Tian He", "Yunhuai Liu", "Yu Yang", "Desheng Zhang"], "abstract": "Textual description of a physical location, commonly known as an address, plays an important role in location-based services(LBS) such as on-demand delivery and navigation. However, the prevalence of abnormal addresses, those containing inaccuracies that fail to pinpoint a location, have led to significant costs. Address rewriting has emerged as a solution to rectify these abnormal addresses. Despite the critical need, existing address rewriting methods are limited, typically tailored to correct specific error types, or frequently require retraining to process new address data effectively. In this study, we introduce AddrLLM, an innovative framework for address rewriting that is built upon a retrieval augmented large language model. AddrLLM overcomes aforementioned limitations through a meticulously designed Supervised Fine-Tuning module, an Address-centric Retrieval Augmented Generation module and a Bias-free Objective Alignment module. To the best of our knowledge, this study pioneers the application of LLM-based address rewriting approach to solve the issue of abnormal addresses. Through comprehensive offline testing with real-world data on a national scale and subsequent online deployment, AddrLLM has demonstrated superior performance in integration with existing logistics system. It has significantly decreased the rate of parcel re-routing by approximately 43%, underscoring its exceptional efficacy in real-world applications.", "sections": [{"title": "1 Introduction", "content": "Addresses are crucial for logistics, ensuring the smooth operation of business by facilitating accurate and efficient delivery processes. In countries like India [50] and China [18], the prevalence of inaccurate or abnormal addresses poses a significant challenge. This issue arises from inadequate address regulatory frameworks, intricate address structures [51], and click farming fraud [60]. Abnormal Chinese addresses, defined as those that cannot be parsed into the standard hierarchy (Appendix A), often include errors such as missing administrative regions, nested addresses, unofficial aliases, irrelevant words, and misspellings (Appendix B). An illustrative example is an address that conflates Beijing and Nanjing, which we term a nested address. These are often exploited for region-specific discounts but lead to unreliable outcomes from Location-Based Services (LBS) due to their lack of systematic recording in databases.\nThis issue significantly impacts companies like JD Logistics, one of the largest logistics companies in the world, which faces around 25,000 daily re-routing events caused by abnormal addresses. These misrouted parcels, resulting from addresses that dispatch parcels to the wrong delivery stations, lead to additional transfers and re-routing, depicted in Figure 1 (red arrow). This process incurs annual losses exceeding $2 million for JD Logistics. Address rewriting, a critical procedure that aligns user-submitted addresses with standardized formats, can significantly reduce dispatching errors. Empirical evidence suggests that refining user-provided addresses through rewriting, as illustrated in Figure 1 (blue arrow), can substantially diminish these errors. Therefore, developing a comprehensive and adaptable address rewriting framework is imperative to process a wide array of erroneous user address inputs, enhancing the overall efficiency of LBS systems within the logistics sector."}, {"title": "2 Method", "content": "Address rewriting aims to refine user-input addresses into a standardized format that aligns with the user's original intent. We introduce a comprehensive framework for LLM-based address rewriting, AddrLLM, depicted in Figure 2. AddrLLM is composed of three key components: Supervised Fine-tuning(SFT) module, Address-centric Retrieval Augmented Generation(RAG) module and Bias-free Objective Alignment module. Initially, we leverage JD's sophisticated Location-Based Services(LBS) system to collect an extensive, high-quality dataset specifically for the SFT task associated with address rewriting. Subsequently, we design the objective alignment module to further calibrate the generation of rewrites to desired results. To prevent potential bias arising from reward model or manual annotation, we integrate the LBS system, which provides bias-free feedback that is directly derived from the model's performance in rewriting task. Finally, to enhance the rewriting process, we develop a customized RAG module designed to enrich the LLM with contextual information via targeted retrieval of relevant addresses."}, {"title": "2.1 Multi-instruction Supervised Fine-tuning", "content": "Because the semantics of address significantly diverges from the pre-training corpus of LLMs, directly employing these models for address processing may result in inaccuracies. To solve this challenge, we adopted a strategy that involves aggregating a range of tasks related to address to fine-tune LLMs, thereby improving their proficiency in understanding standard Chinese addresses. Here we describe the tasks and datasets for SFT. The prompt design for these tasks are detailed in Appendix C.\n\u2022 Address Parsing: Address parsing task involves the process of breaking down an address into its constituent components. The structure of a Chinese address is detailed in Appendix A. Our address parsing dataset comprises 20 million <address, address components> pairs obtained from JD's LBS system. Fine-tuning LLMs on the address parsing task is likely to help these models understand the structure of standard addresses, enabling them to generate addresses that conform to the standard hierarchy.\n\u2022 Address Entity Prediction(AEP): Address Entity Prediction is to infer absent administrative region in address. In logistics system, user-provided addresses frequently lack administrative regions, posing significant challenges to package dispatching. Thus, address entity prediction or filling is an important ability of address rewriting model. For the AEP task, we collect 20 million addresses from historical delivery orders in JD, and randomly delete administrative regions. Then the address rewriting model needs to predict the missing address entity.\n\u2022 Address Rewriting: Our address rewriting dataset is generated from two sources. The primary source is the JD LBS system. When a user inputs an address, the LBS system may do some basic rewriting, such as misspelling correction. We collect user-input addresses and rewritten ones and obtain 15 million samples from LBS system. The second source is address recommendation system integrated on JD e-commerce platform. When a user places an order with delivery address on JD e-commerce platform, the system recommends some related addresses from our standard address database. If user chooses to replace original address with a recommended address, we keep a record. However, this data can be noisy because the original address provided by user may only contain some keywords. Thus, we filter records by feeding original addresses into JD LBS system, and if geocoding succeeds(geocoding doesn't return error although returned coordinates may be incorrect), we can view original address as a complete address and put corresponding record into our address rewriting dataset. Following this process, 5 million samples are collected by the second source. In total, our address rewriting dataset contains 20 million samples. This dataset includes 77.8% samples that do not involve any rewriting, i.e. original address and rewritten address are the same.\nFinally, address parsing, address entity prediction and address rewriting datasets are mixed together to form the SFT dataset D.\n\u2022 Multi-instruction Supervised Fine-tuning (SFT): The process of generating text using large language model can be viewed as autoregressive sampling. In auto-regressive language generation, each word is predicted one at a time, and each prediction conditions on the prompt and previously generated words. Given model input (prompt) x and standard output y, the training objective is to find the model parameters $\\theta^*$ that maximizes the conditional probability $p(y|x) = \\Pi_{i=1} P(Y_i | Y_{0:i-1}, x)$. The training objective can be formulated as:\n$\\theta^* = arg max \\sum_{(x,y) \\in D} - log p(\\frac{y_i}{y_{0:i-1}}, x; \\theta)$\nwhere D is the SFT dataset, $\\pi$ is our large language model and $\\theta$ is its parameters."}, {"title": "2.2 Bias-free Objective Alignment", "content": "Through prior multi-instruction SFT, AddrLLM has developed understanding of standard Chinese addresses. Because address rewriting SFT dataset is generated based on JD LBS system, i.e. these abnormal addresses can be identified and rectified by current system, it indicates that these abnormal addresses have different distribution from those cannot be rectified by LBS system. As a result, AddrLLM's current skill set does not extend to effectively rewriting addresses that fall outside the correction capabilities of the JD LBS system. Moreover, the address groundtruth of unsuccessful delivery events is unknown to us. Therefore, to further enhance address rewriting capability of AddrLLM, particularly for those challenging addresses beyond the reach of the JD LBS system, reinforcement learning based objective alignment is necessary.\nIn previous works, frameworks for aligning the objectives of LLM typically rely on either manual annotations or trainable reward model [28, 54, 56] to guide LLM's training. However, these methods have some limitations that hinder their application in industrial settings. Firstly, manual annotations are not only biased, but also expensive to extend to large scale data. Additionally, the performance of LLM is highly tied to the effectiveness of reward model training, which produces scores that are not easily interpretable.\nTo overcome these limitations, we introduce a novel approach that utilizes JD LBS system for providing feedback. Our approach provides an unbiased and fast reward calculation that is directly tied to LLM's performance on address rewriting task. More importantly, the reward calculation is interpretable, which is vital for robust applications and further improvements in the industry. The reward r is comprised of three scores: semantics score, reverse geocoding score and geocoding score.\nFirstly, the rewritten address should not be too far from the original address in semantics. For example, if user inputs \"Outlets Store (Eastern door)\", the model should not simply rewrite it to \"Outlets Store\", even if the geocoding results of two addresses are close spatially. To prevent rewriting from deviating too far from user's intent, we design semantics score $seman(x, y)$:\n$seman(x, y) = cos(f(x), f(y))$\nwhere x is original address, y is rewritten address, cos is the function of cosine similarity, formulated as $cos(A, B) = \\frac{A*B}{||A||*||B||}$, f is semantics embedding model. At experiments, we choose pre-trained model BERT as f.\nSecondly, the rewritten address should be semantically close to the address obtained by reverse-geocoding on delivery coordinates. The result of reverse-geocoding may not be exact, for example, the room number or building number may be incorrect. However, the rewritten address should be semantically close to that address to some extent, for example, they are in the same community or road. To directly measure the rewritten address, we design reverse geocoding score $revgeo(y, c)$, formulated as:\n$revgeo(y, c) = cos(f(y), f(reverse(c)))$\nwhere y is rewritten address, c is coordinates of successful delivery, reverse is reverse-geocoding service, f is semantics embedding model.\nThirdly, we choose geocoding task to evaluate the rewriting result. Intuitively, the geocoding result of rewritten address should be close to the coordinates of successful delivery. Thus, we design geocoding score $geo(y, c)$, formulated as:\n$geo(y, c) = \\begin{cases}\n1 & k < \\theta_1\\\\\n1 - (k - \\theta_1)/\\theta_2 & \\theta_1 \\leq k < \\theta_2\\\\\n0 & k \\geq \\theta_2\\\\\n0 & geocoding failure\n\\end{cases}$\n$k = dis(geocoding(y), c)$\nwhere dis is Euclidean distance function, geocoding is JD geocoding system that maps from address to coordinates. We normalize geocoding score to [0,1] by weights $\\theta_1$ and $\\theta_2$. In experiments, we set $\\theta_1$ as 100 meters and $\\theta_2$ as 1000 meters. When the rewritten address cannot be recognized by JD geocoding service as an address (geocoding failure), the geocoding score is 0.\nFinally, three scores are added together with weights $\\lambda_1, \\lambda_2, \\lambda_3$:\n$r(x, y, c) = \\lambda_1seman(x, y) + \\lambda_2revgeo(y, c) + \\lambda_3geo(y, c)$\nIn experiments, $\\lambda_1, \\lambda_2$ and $\\lambda_3$ are set as 0.2,0.2 and 0.6 respectively.\nRL Task Formulation: At each time step t, large language model $\\pi$ generate next token as action $a_t$, based on current state $s_t$, which includes already generated tokens. Then the model obtains an immediate reward $r_t$ by a rewarding function R: SX A \u2192 R. The detailed Markov Decision Process(MDP) formulation is in Appendix D.\nTraining: We adopt Proximal Policy Optimization(PPO) [47] to optimize the large language model. The PPO algorithm can be formulated as:\n$\\max_{\\theta} E_{(s_t, a_t) \\sim \\pi_{\\theta'}} [min\\{k_{t, \\theta}, clip(k_{t, \\theta}, 1 - \\epsilon, 1 + \\epsilon)A^{\\theta'}(s_t, a_t)\\}]$\n$k_{t,\\theta} = \\frac{P_{\\theta}(a_t | s_t)}{P_{\\theta'}(a_t | s_t)}$\nwhere $\\theta'$ is parameters of fixed policy, $\\theta$ is parameters of updated policy, the clip function $clip(k_{t,\\theta}, 1 - \\epsilon, 1 + \\epsilon)$ limits the ratio $k_{t,\\theta}$ to the range [1 - $\\epsilon$, 1 + $\\epsilon$]. A is advantage function, which is formulated based on the estimation of value network $V_{\\phi}$. The value network $V_{\\phi}$ is initialized from the policy network $\\pi_0$. The formulation follows Generalized Advantage Estimation(GAE) [46]:\n$\\delta_t = R(s_t, a_t) + V_{\\phi} (s_{t+1}) - V_{\\phi}(s_t)$\n$A(s_t, a_t) = \\sum_{t'=0}^{\\infty} \\lambda^t\\delta_t$\nwhere $\\lambda$ is the bias-variance trade-off parameter.\nTo prevent the model from deviating too far form the initialization, we add a KL-divergence regularization to reward [42]:\n$R(s_t, a_t) = r(x, y, c) - \\beta KL(\\pi_{\\theta}||\\pi_0)$\nThe final loss function is composed of policy loss and value loss:\n$L_{\\theta} = - \\frac{1}{ST} \\sum_{T \\in S} \\sum_{t=0}^{T} min\\{k_{t,\\theta}(s_t, a_t), clip(k_{t,\\theta}, 1 - \\epsilon, 1 + \\epsilon)A^{\\theta'}(s_t, a_t)\\}$\n$L_{\\phi} = \\frac{1}{ST} \\sum_{T \\in S} \\sum_{t=0}^{T} (V_{\\phi}(S_t) - R_t)^2$\n$L_{PPO} = L_{\\theta} + \\lambda_{\\phi}L_{\\phi}$\nwhere S is sampling set, T is step numbers.\nFor the objective alignment, we utilize 4 million <address, location> samples, where address is the user-input address and location is the delivery coordinates reported by courier. The groundtruth address is unknown to us. The objective alignment module guides LLM to learn how to rewrite these user-input addresses to standard ones."}, {"title": "2.3 Address-centric RAG", "content": "Large Language Models (LLMs) often experience hallucination, particularly when generating content in domains unfamiliar to them, as highlighted in the studies by [4, 14, 57]. Given that LLMs are trained on datasets encompassing a broad range of scenarios, their expertise in specific tasks such as Chinese address rewriting is limited. Consequently, this make them prone to generating hallucinated content when tasked with rewriting Chinese addresses. Meanwhile, LLMs also suffer from misalignment [31, 38], which is also significant in address system since address database keeps updating. To solve these challenges, we develop an Address-centric Retrieval-Augmented Generation(RAG) module, which decouples reasoning ability and address knowledge storage of LLM, and maintains knowledge in external database.\nIn this section, we introduce the popular \"retrieve-then-read\" RAG pipeline and adapt it to our address rewriting scenario. Firstly, the retriever identifies and extracts a set of relevant addresses from the database. Subsequently, the generator, i.e. a fine-tuned LLM, bases its generated output on the addresses retrieved. We describe the retriever in more detail below.\n\u2022 Retriever: The retriever's function is to identify and prioritize all addresses relevant to the input. Formally, the retriever's role is encapsulated by the function M, which maps a query q and a database K to a subset K', such that M : [q, K] \u2192 K', where K' comprises the relevant addresses from K corresponding to the input q, ordered by decreasing relevance. In our scenario, q is an address we want to modify, K' is a set of addresses relevant to q. The foundation model of retriever is an encoder model E, which maps an address to a representation. Then a similarity score is computed between query address q and each sample in K:\n$s(p,q) = sim(E(q), E(p)), p \\in K$\nwhere sim: Rd \u00d7 Rd \u2192 R+ is the similarity function, such as cosine similarity. Finally, samples with highest similarity score, i.e. K', are returned by retriever.\nFor the sake of efficiency and scalability, retriever usually encode textual information into embedding space, where the retriever performs search [14, 21]. In previous RAG frameworks which mainly target at question-answering task, relevance is defined on semantics of paragraphs and sentences. For our address scenario, however, semantics relevance could introduce misleading information. Our objective necessitates a shift in the relevance criterion towards geographical proximity, ensuring that the retrieved addresses are within a close spatial range to the query address.\n\u2022 Spatial Encoding: Previous research has explored the spatial encoding of addresses [20, 55]. However, these techniques prioritize other NLP tasks like Masked Language Modeling and Hierarchical Text Classification, which generalize the pre-trained model(PTM) at the expense of its capacity for spatial encoding. Thus, fine-tuning the PTM is necessary. In our retrieval framework, we employ the widely adopted BERT encoder architecture [9] as foundation model E. The initialization of our retriever leverages parameters from the text encoder of G2PTL [55]. Subsequently we fine-tune BERT on geocoding task. To achieve this, we append several fully connected(FC) layers to existing BERT structure, which are designed to transform the embedding to geographic coordinates, namely longitude and latitude. Our geocoding dataset comprises 200 million <address, coordinates> pairs. Given that BERT comes pre-trained while the added FC layers are randomly initialized, we adopt a two-phase training strategy. In the first epoch of training, we freeze BERT parameters and concentrate on training FC layers. For subsequent epochs, we train the whole neural network, including BERT and FC layers. When retrieving addresses, the representation produced by BERT, a 768-element vector, is utilized as the spatial embedding."}, {"title": "3 Experiments", "content": "In this section, we conduct experiments to answer the following research questions:\n\u2022 RQ1: Does AddrLLM outperform other SoTA methods in offline experiments?\n\u2022 RQ2: Whether and how often does AddrLLM rewrite a standard address to incorrect one?\n\u2022 RQ3: How the components in AddrLLM contribute to the performance?\n\u2022 RQ4: How long is the duration required for SFT and objective alignment of LLM, and the responsiveness of our framework?\n\u2022 RQ5: Does the retriever in RAG module perform well?\n\u2022 RQ6: What improvements has the deployment of AddrLLM brought to JD's LBS system?"}, {"title": "3.1 Testing Datasets and Evaluation Metrics", "content": "In this section, we present the downstream applications that served as benchmarks for assessing our model, along with the associated metrics. The prompts employed for the LLMs across these applications are detailed in Appendix C. An overview of the datasets for the different tasks is provided in Table 1. While the training datasets have been extensively described in Section 2, in this part, we focus exclusively on the testing datasets. To test our model, we build three extensive datasets, each corresponding to one of the three applications.\nAddress Entity Prediction: Request the model to generate addresses with all necessary elements for those lacking administrative regions.\n\u2022 Trigger Prediction: This metric evaluates the ability of the model to discern which input addresses are missing administrative regions and require completion. If the model attempts to complete the address (e.g. add some words), we identify it as successful trigger prediction, regardless of whether the completion is correct.\n\u2022 Accuracy: Whether the model generates correct address. This metric is calculated as the ratio of accurately rewritten addresses to the total number of addresses missing regions.\nDirect: Directly evaluate the rewritten address.\n\u2022 Hit Rate: We derived the groundtruth for levels 1 to 4 (Appendix A) by reverse geocoding the delivery coordinates, complementing them with level 5 and 6 from the input addresses. After tokenizing both the rewritten addresses and groundtruth addresses, we then assessed the accuracy by calculating the percentage of address components predicted by the model that correspond with the groundtruth components.\nGeocoding: Geocoding is a popular GIS service that maps textual address to geospatial coordinates. In JD's LBS system, parcel dispatching relies heavily on the geocoding service. Here we rewrite the input address by model before feeding it to JD's geocoding service of LBS system.\n\u2022 Acc@300m: The percentage of coordinates that locates within 300-meter radius of the groundtruth coordinates, with the groundtruth being the delivery coordinates as reported by couriers.\n\u2022 Acc@500m: A metric akin to Acc@300m, except that the 300-meter radius is substituted with a 500-meter radius.\n\u2022 Acc@Station: This metric calculates the percentage of coordinates that locates within the responsible area of groundtruth delivery station. In logistics, if an address is correctly located within the area of corresponding station, we can view it as a successful geocoding process, because subsequently couriers in station do not relay on precise coordinates but textual addresses to perform last-mile delivery [5].\n\u2022 Robustness: Our geocoding testing dataset contains 90% addresses that can be correctly identified by JD's LBS system. In industrial scenario, an important metric is that the model should not rewrite a correct address to incorrect address. Thus, we design the metric Robustness, which is calculated as the percentage of originally correct addresses remaining correct after rewriting.\n\u2022 Correction: This metric measures the percentage of abnormal addresses that can be rectified by mode.\nThe datasets used for direct evaluation and geocoding contain an identical collection of addresses. Within this set, JD's LBS system accurately dispatches 90% of the addresses to delivery station, while the remaining 10% are subject to re-routing. Addresses within testing datasets do not overlap with those in training datasets."}, {"title": "3.2 Implementation Details", "content": "We choose AdamW [37] optimizer for LLM SFT and objective alignment. During SFT stage, LLM is trained for 1 epoch with learning rate 1e-5. During objective alignment stage, LLM is trained for 4 epochs with learning rate 1e-6. During retriever fine-tuning stage, BERT is trained for 4 epochs with learning rate initialized to 5e-5 and gradually decreased during the process of training, where Adam [26] is the optimizer. We compare Qwen-7B [2] and Baichuan-7B [3] as base LLM because of their outstanding performance in Chinese language related tasks. At retrieval stage, retriever select top-10 most relevant addresses from database, for which we use the open source vector database Vearch [34]. All of the offline experiments are conducted on a cloud-based computational cluster including 10 Nvidia H800 GPUs, 160 CPU cores of Intel Xeon 8468V."}, {"title": "3.3 Compared Methods", "content": "\u2022 BART [30]: A widely used transformer-based encoder-decoder Pre-Trained Model(PTM), which achieve remarkable gains in NLP tasks. We fine-tune it on our address rewriting dataset to enhance its ability in rewriting addresses.\n\u2022 BERT [9]: A popular transformer-based encoder PTM. We append Transformer decoder and fine-tune it on address rewriting dataset.\n\u2022 G2PTL [55]: The latest PTM pre-trained on logistics data and tasks. The built-in Address Entity Prediction module in G2PTL needs missing words marked as \"[MARK]\", which cannot be directly applied on our address rewriting scenario. Thus, we utilize the text encoder of G2PTL and append transformer decoder to return rewritten address. We fine-tune it on address rewriting dataset.\n\u2022 QWen-7B [2]: LLM baseline. Prompt Qwen-7B to rewrite address.\n\u2022 Baichuan-7B [3]: LLM baseline. Prompt Baichuan-7B to rewrite address.\n\u2022 SoP: Address geocoding service within JD's LBS system.\n\u2022 AddrLLM-Qwen: Utilize QWen-7B as the base LLM instead of Baichuan-7B.\n\u2022 AddrLLM w/o OA: Remove the objective alignment module from AddrLLM.\n\u2022 AddrLLM w/o SFT: Remove the SFT module from AddrLLM.\n\u2022 AddrLLM w/o RAG: Remove the RAG module from AddrLLM.\n\u2022 AddrLLM-RAG: Remove SFT and objective alignment modules from AddrLLM."}, {"title": "3.4 Offline Experiment Result", "content": "Our experimental results on direct evaluation, address entity prediction and geocoding tasks are shown in Table 2. By analyzing the results, we have the following findings:\n3.4.1 Overall Performance (RQ1). Upon evaluating the accuracy of AEP, the hit rate and geocoding accuracy, it becomes evident that our model, AddrLLM, outperforms all of baselines on our testing dataset, which consists of 90% standard and 10% abnormal addresses. The SoTA baseline G2PTL achieves second best average performance. Pose the application of AddrLLM for address rewriting, compared with the second best model G2PTL, we observed significant improvements across several metrics: trigger prediction of AEP increases by 7%, the accuracy of AEP increases by 10.8%, the hit rate increases by 15.4%, the geocoding accuracy at station level increases by 7.1%. This enhancements underscore the efficacy of our LLM-based address rewriting framework in detecting and correcting address inaccuracies. Notably, in the context of geocoding, a critical factor for successful parcel delivery in the logistics industry, compared with SoP method, AddrLLM reduces station-level inaccuracies by a substantial 43%.\n3.4.2 Robustness (RQ2). Since in the real-world scenario, percentage of incorrect addresses is greatly lower than that of our testing dataset(10%), it is important to test whether model rewrites a standard address to incorrect one. Thus, we design robustness metric on geocoding task. Robustness measures the percentage of standard addresses remaining correct after rewriting. From the table, AddrLLM achieves 99.9% robustness, which significantly outperforms other baselines. This suggests that AddrLLM is effective at recognizing correct addresses and avoids modifying them into incorrect ones.\n3.4.3 Ablation Study (RQ3). To understand the contribution of each component to AddrLLM's performance and to inform future enhancements and deployment strategies, we conduct an ablation study and meticulously analyze the experimental results. Our analysis yields the following insights:\nThe original Baichuan and QWen models exhibit suboptimal performance in address rewriting tasks, falling short of the SOTA baseline established by G2PTL, by an average of 7.3% and 8.7% respectively. This indicates that generalized LLMs have limited knowledge about standard addresses. In contrast, compared with Baichuan and QWen, AddrLLM and AddrLLM-QWen obtain improvements by 19.5% and 19%, respectively. This indicates that the integration of our novel RAG, SFT and objective alignment modules significantly enhance LLM's capacity for address rewriting.\nIn the absence of objective alignment(OA) component, AddrLLM attains the highest performance relative to other ablated models except AddrLLM-QWen. Even when OA is removed, AddrLLM still outshines the SoTA baselines, albeit it falls behind the SoP in geocoding task. The SFT process relies on groundtruth data generated by JD's LBS system(SoP). Therefore, without the OA module, it is challenging for AddrLLM to surpass the SoP. The OA module, however, introduces samples where SoP lacks the knowledge for the groundtruth addresses, providing a unique training opportunity for AddrLLM to refine its performance and ultimately exceed the SoP.\nExcluding the Supervised Fine-Tuning (SFT) module leads to a marked reduction in AddrLLM's effectiveness, particularly affecting the robustness of geocoding task. This deterioration is due to the prevalence of non-rewriting samples within the address parsing, rewriting and entity prediction datasets of SFT stage. These non-rewriting samples are instrumental in enhancing the model's proficiency in recognizing correct addresses and determining when not to perform rewrites.\nWhen the RAG component is removed, there is a noticeable downturn in AddrLLM's effectiveness, indicating the integral role of RAG in the model's overall functionality.\nObserving AddrLLM-RAG, where we remove SFT and OA modules, our model achieves the lowest performance relative to other ablated models. This is because the original Baichuan model lacks understanding of standard addresses. The average performance is only 2.6% higher than that of Baichuan-7B. This indicates that without fine-tuning the base model, AddrLLM is hard to utilize the relevant addresses retrieved by RAG module.\nIn conclusion, the synergy of SFT, OA, and RAG is essential for optimizing AddrLLM in the specialized task of rewriting addresses.\n3.4.4 Complexity Analysis (RQ4). In Table 3, we present a consolidated overview of the cumulative duration, measured in hours, required for Supervised Fine-Tuning (SFT), objective alignment, and evaluation phases. Notably, the objective alignment stage operates at an average rate of 13 samples per second. This relatively slow speed is attributed primarily to the reward calculation being executed on the CPU, whereas the training of the LLM occurs on the GPU. Consequently, there is an increased frequency of data transfers between the CPU and GPU compared to typical neural network training procedures. Additionally, this transfer and training process is inherently difficult to parallelize, further contributing to the reduced processing speed.\n3.4.5 Spatial Encoding in RAG (RQ5). The spatial encoding feature within the RAG module guarantees that the addresses retrieved are spatially close and subsequently influences the information supplied to the LLM. In this section, we assess the quality of spatial embeddings generated by our retriever in comparison to G2PTL, the most recent state-of-the-art Pre-Trained Model(PTM) for logistics addresses.\nFirstly, we select 12 delivery stations across Beijing and utilize the belonged addresses to evaluate model's proficiency in distinguishing between them. We employ t-SNE to reduce dimension and visualize the spatial embedding. Station-level classification is at a finer granularity than that of district-level, as a single district may contain dozens of delivery stations. While G2PTL [55] is reported to have excellent performance in district-level categorization, our observations from Figure 3 indicate that its capabilities at the station-level do not meet surrounding addresses retrieval in RAG module. Conversely, our retriever, which has been fine-tuned with a dataset comprising 200 million samples from nationwide geocoding database, demonstrates superior performance in classifying addresses at the station level."}, {"title": "3.5 Online Deployment (RQ6)", "content": "AddrLLM is incorporated to JD's current LBS system and deployed in Zhejiang province, China. Specifically, all packages sending from Zhejiang will be processed by the new system. Because the huge computing resources needed by Large Language Models, in the deployed system, AddrLLM is called to rewrite addresses only when abnormal addresses are detected [18]. Considering daily rewriting volume in Zhejiang province, AddrLLM is deployed on a computational cluster with 4 Intel Xeon 8468V CPUs, 4 NVIDIA RTX 4090D GPUs and 256 GB RAM.\nWe present monitoring data over the course of 60 days encompassing the \"618\", a sales event comparable to Black Friday. Daily rewriting volume and accuracy (percentage of abnormal addresses that can be corrected by AddrLLM) is depicted in Figure 5. On average, there are 754 instances of address rewriting each day. Our specialized address rewriting model, AddrLLM, has effectively corrected over 40% of these erroneous addresses, ensuring the proper delivery of corresponding parcels. Moreover, the accuracy curve reveals that the performance of AddrLLM remains stable when applied to real-world data streams.\nResults from Yulin city, Shaanxi Province and Yangjiang city, Guangdong Province are shown in Appendix E."}, {"title": "4 Discussion", "content": "Lessons Learned: We summarize key lessons learned from work:\n\u2022 Large Language Model shows powerful ability of reasoning, but still needs fine-tuning and objective alignment to meet the requirements of specific domain and task.\n\u2022 Retrieval-Augmented Generation(RAG) boosts LLM's performance in our address rewriting scenario.\n\u2022 After deployment of LLM, RAG decreases the workload of updating knowledge, especially for a rapidly updating geocoding database.\nLimitations:\n\u2022 For the offline experiment, because of vast amount of training data required by fine-tuning LLM, it's impractical to obtain a comprehensive analysis of all the incorrect address. For example, in Table 5, what are percentages of these errors in real-world address query.\n\u2022 Because of the black-box essential of LLM, we adopt a relatively conservative way to deploy our model. Instead of directly incorporating our model into core of JD's LBS system, which is the most critical system in a logistics company, we initially utilize it to rectify erroneous addresses after abnormal addresses detected. Though it is a good way to examine model's ability and robustness on real-world scenario, AddrLLM's capabilities is not fully utilized.\nFuture Work:\n\u2022 An autonomous method to analyze different types of errors in daily addresses.\n\u2022 After obtaining comprehensive analysis of erroneous addresses, train and test LLM's rewriting ability on individual type of error.\n\u2022 Further examine and enhance LLM's robustness, decrease its inference latency and incorporate it into the core of JD's LBS system."}, {"title": "5 Related Work", "content": "5.1 Address Processing\nAddress Processing is a critical task in the domain of geospatial analysis, intersecting with Natural Language Processing, information retrieval and Machine Learning. There have been significant researches advancing the field of address-related tasks. [45", "17": "infers geographic coordinates from textual addresses using data from e-commerce logistics. [18", "32": "introduces GeoGLUE, a benchmark for evaluating geographic natural language understanding. [25", "55": "developed PTMs for address related tasks, based on data from logistics"}]}