{"title": "A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies", "authors": ["Ryan Shea", "Zhou Yu"], "abstract": "Despite recent advancements in AI and NLP, negotiation remains a difficult domain for AI agents. Traditional game theoretic approaches that have worked well for two-player zero-sum games struggle in the context of negotiation due to their inability to learn human-compatible strategies. On the other hand, approaches that only use human data tend to be domain-specific and lack the theoretical guarantees provided by strategies grounded in game theory. Motivated by the notion of fairness as a criterion for optimality in general sum games, we propose a negotiation framework called FDHC which incorporates fairness into both the reward design and search to learn human-compatible negotiation strategies. Our method includes a novel, RL+search technique called LGM-Zero which leverages a pre-trained language model to retrieve human-compatible offers from large action spaces. Our results show that our method is able to achieve more egalitarian negotiation outcomes and improve negotiation quality.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in AI and NLP have led researchers to develop techniques to build au-tonomous agents which can achieve human-level performance in bargaining games such as Deal-or-no-Deal (Sengupta et al., 2021). These techniques can be separated into two broad categories: game theoretic approaches and data driven approaches.\nGame theoretic approaches to negotiation at-tempt to build negotiation agents without observing any human data. This class of algorithms is typi-cally applied to two-player zero-sum games which do not require agents to learn human-compatible strategies in order to be successful (Silver et al., 2018; Brown et al., 2020; Vinyals et al., 2019). However, other classes of games such as negotia-tion require cooperation with humans in order to be successful (Bakhtin et al., 2022). As a result, traditional game theoretic methods have failed to achieve quality performance in the realm of negoti-ation (Li et al., 2023).\nData driven approaches to negotiation learn a human-like strategy directly using data on a spe-cific negotiation domain (Verma et al., 2022; Lewis et al., 2017; He et al., 2018). Unfortunately, col-lecting human data is expensive and the strategies in the data may not effectively generalize to other negotiation domains. Furthermore, these methods lack the desirable properties that game theoretic methods offer such as controllability.\nWe propose an approach which bridges game the-oretic and data driven approaches called Fairness-Driven Human-Compatible (FDHC) bargaining. This method is designed to target egalitarian out-comes, specifically the Egalitarian Bargaining Solu-tion (EBS), which we use as a formal notion of fair-ness (Kalai, 1977). We target fair outcomes as prior work has shown that fairness is a key component of human strategies and has also served as a useful notion of optimality in general-sum repeated games (Tossou et al., 2020; DiGiovanni and Zell, 2021; Kroll et al., 2014). Our fairness-targeting strategy is learned with a novel LLM-Guided Monte Carlo tree search with Zero domain specific training data (LGM-Zero). LGM-Zero leverages the reasoning capabilities of LLM models (Kwon et al., 2023) to extract human-like negotiation offers from large action spaces without the need to collect additional human data. A value model trained via self-play then selects the best offer that the LLM proposes. The LLM and value model are used to guide a Monte Carlo tree search (MCTS) towards the de-sired outcome given by FDHC (Figure 1).\nWe say that an action/strategy is \"human-compatible\u201d if a human would take a similar action or apply a similar strategy if placed in the same scenario. For example, in a situation where partici-pants are negotiating over an item worth thousands of dollars, it would not be human-compatible for our model to offer a price down to the granularity"}, {"title": "2 Background", "content": "The Nash bargaining game is a game in which two or more players must divide a surplus between themselves. In the used car example given previ-ously, the surplus would be the difference between the buyer and seller's reservation prices. We use the term extensive form Nash bargaining game to refer to a game in which players can propose divisions of the surplus over the course of a series of time steps. A Nash equilibrium is a game state in which no player can benefit from a unilateral change in strategy.\nBargaining theory makes use of axioms which are rules that describe properties that a bargaining outcome satisfies (Nash, 1950). We make use of the following axioms when analysing the theoretical properties of our method. The axiom of symmetry says that if the players in the bargaining game are indistinguishable based on the description of the game, then they should all receive the same pay-off. A weak Pareto optimal solution is one where any change to the outcome will make at least one party no better off. Strong monotonicity states that any increase in the amount of surplus being bargained over should benefit all players involved in the negotiation. Formal definitions can be found in Appendix A.\nSurplus division is the process of dividing some commodity (often money) among a group of peo-ple. A utility function measures the welfare or satisfaction of a negotiator as a function of the amount of surplus they receive. A disagreement payoff is the amount of utility a negotiator receives if the negotiators do not reach an agreement. A reservation price is the minimum amount a seller is willing to sell an item for. The converse holds from the buyer's perspective. This term is specific to single-issue negotiation.\nAction space refers to the set of all valid actions available to an agent as it interacts with an environ-ment. In the context of negotiation, this is the set of actions available to participants in a negotiation. A value network is neural network that takes in a game state and outputs a scalar representing the quality of the state."}, {"title": "3 Related Work", "content": "Prior work in the field of negotiation has typically been centered on leveraging human data to learn ne-gotiation strategies. These methods involve collect-ing human-human dialogues for negotiation exer-cises such as Craigslist bargaining (He et al., 2018) or Deal-or-no-Deal (Lewis et al., 2017). This data can then be used to perform supervised learning or offline reinforcement learning on a negotiation model (Verma et al., 2022; Zhan et al., 2024). More recent work has focused on examining and enhanc-ing the negotiation capabilities of LLMs (Bianchi et al., 2022; Schneider et al., 2023; Fu et al., 2023; Xia et al., 2024). These methods use prompting to create negotiation agents and rely on the zero-shot/few-shot capabilities of LLMs to negotiate.\nData driven methods for negotiation are able to learn human-like negotiation strategies as they directly leverage human data. However they are often overly tailored to one particular domain and have a difficult time generalizing to other scenarios. Furthermore, data driven strategies lack theoretical guarantees such as convergence to a Nash equi-librium is which is a desirable attribute for any negotiation strategy.\nMethods grounded in game theory are able to provide the theoretical guarantees that data driven methods lack. As a result they are much more con-trollable and adapt better to different domains as no additional data collection is needed for training. However, training with no human involvement of-ten results in strategies which are incompatible with human play (Bakhtin et al., 2022). This has limited work in the area primarily to two-player zero-sum games such as chess where human-compatibility is not needed to ensure robust play (Silver et al., 2018). These methods are designed to ensure con-vergence to a Nash equilibrium, which does not necessarily result in a human-compatible strategy (Section 5). The little work that has attempted to apply game theoretic methods to the negotiation domain tends to ignore the dialogue aspect of nego-tiation, considering it to be \"cheap talk\" (Li et al., 2023). While the strategic aspect of negotiation can be modeled independently of dialogue, dialogue style has been shown to have a measurable effect on negotiation outcomes (Noh and Chang, 2024). Our method is designed to provide theoretical guar-antees similar to game theoretic methods while maintaining human-compatibility of data-driven methods by leveraging the reasoning capabilities of LLMs."}, {"title": "4 Method", "content": "In this section, we describe the FDHC framework which prioritizes egalitarian outcomes. We also describe LGM-Zero, which uses a value model trained with self-play and language model as a pol-icy network. Finally, we outline how we implement our setup for single-issue distributive bargaining."}, {"title": "4.1 FDHC Negotiation Framework", "content": "FDHC is designed to work within the context of the Nash bargaining game. Specifically, it is de-signed for an extensive form Nash bargaining game with imperfect information. In this game, players repeatedly request some portion of a surplus, if the sum of their requests at the end of the game is less than or equal to the total surplus then they both receive what they requested, if not they receive a disagreement payoff d. FDHC works by decom-posing this game in to a series of depth limited subgames. These subgames are identical to the original game, except they may be rooted at any game history and only extend for a limited number of actions in the future.\nBefore proceeding to our subgame, we make a guess at the size of the resource pool to be split and our opponent's utility function over these re-sources. The guess is made based on the history of the game and any initial information we are pro-vided before the game has begun. The specifics of how we do this are domain-dependent and for many games some of the information may be given. For example, in the game Deal-or-no-Deal we know the size of our resource pool but do not know our opponents preferences over the pool. Conversely, in distributive bargaining games we know our op-ponents preferences but do not know the size of the resource pool.\nAfter making our guess, we root our subgame at the corresponding belief state. This subgame is treated as a perfect-information game and the EBS is calculated as\nE(S, d) = arg max(min(xi - di))\nxEI(S,d) iEN\nwhere S denotes the bargaining set, I(S, d) is some individually rational payoff set, and di, xi are the disagreement payoff and payoff for player i, re-spectively.\nOur model then applies a strategy which targets this solution using LGM-Zero, described in the next section. We make moves according to this strategy until the subgame concludes. This can be as short as one move or as long as the entire game depending on our choice of subgame length. We then update our guess for the resource pool and utility function based on our opponents moves and transition to the next subgame. This process is repeated until the game concludes."}, {"title": "4.2 LGM-Zero", "content": "Now we describe LGM-Zero, which uses a MCTS guided by a LLM and value network to perform ne-gotiation actions. Under our setup the only model we train is our value network which is trained via self-play. We first describe how our method be-haves during inference time and then describe the process we use to train our value model."}, {"title": "4.2.1 Inference", "content": "Given the action history of a negotiation our algo-rithm searches for the best response by repeatedly performing selection, expansion, and backpropaga-tion. We describe these stages next.\nSelection During this stage we traverse the game tree by selecting the action, a, with the highest up-per confidence bound for its Q-value (Silver et al., 2018), calculated as\nU(s,a) = Q(s, a) + Cp * sqrt(\u03a3bN(s,b) / (1+ N(s, a)))\nwhere s is the current game state, Cp is a hyperpa-rameter which controls the degree of exploration, and N(s, a) denotes the number of times we have taken the action previously. The selection process is repeated until we reach a leaf node, which is a defined as a state whose children have not been explored yet (\u015awiechowski et al., 2021).\nExpansion In the expansion phase we feed a LLM a prompt to suggest five good actions given the current game state. The prompt used to gener-ate actions must be engineered specifically for the negotiation scenario the search is being applied to. We treat all the actions as having equal probability under the model and all other actions at the current state to have a probability of zero. If one of the actions results in a terminal state its value is set to the reward returned by the state, otherwise it's set to the output of our value model. These values are propagated back up the tree according to the next step.\nBackpropagation After expansion is concluded we update each node along the search path by in-crementing N(s, a) by one for each action taken during the search. We also update the Q-values along the search path as\nQ(s, a) \u2190 Q(s, a) + \u03c5(\u03c2) / N(s,a)\nwhere v(s) is the value of the state we evaluated, given either by our value model or the actual reward value depending on if the state is terminal.\nWe repeat this search for n iterations then make a move based on which child of the current state has the highest Q-value."}, {"title": "4.2.2 Training", "content": "Our value model is trained using a method sim-ilar to fictitious self-play (Heinrich et al., 2015). Fictitious self-play is an iterative method for com-puting an approximate Nash equilibrium. This is done by performing self-play with a mixed strategy that chooses between playing a best response to our opponent's strategy and the average strategy for the current player. The fictitious self-play set up traditionally learns the best response strategy with a deep Q-network (Mnih et al., 2013) and the average strategy via supervised learning."}, {"title": "4.3 Implementation", "content": "We implement our proposed method for a single-issue distributive bargaining exercise. This exercise involves two parties negotiating over the price of a used car and is used in graduate-level business classes (see Appendix F for the scenario). The buyer and seller are both given private reserva-tion prices which they cannot go beyond during the negotiation. In our scenario the seller cannot go below a price of $12,500 and the buyer can-not go above $13,500. The difference between the reservation prices is the surplus for the game. Our model is trained to act as the seller in this sce-nario. We assume that our opponents are risk neu-tral and have a disagreement payoff of $100. This disagreement payoff is chosen based on experimen-tal results which show that inefficient outcomes, such as disagreements, are common in negotiation (Feltovich and Swierzbinski, 2011; Ellingsen and Johannesson, 2004) suggesting that many humans may prefer to not reach a deal instead of agreeing to a outcome which gives little payoff.\nOur final design uses a modular framework where the negotiation acts are extracted from user responses using GPT-4. Our schema uses four acts: no_counteroffer, counteroffer, accept, reject. These acts are translated into our game state which con-sists of the offer history for the game (ex. [1500, 1100, 1450, 1200,... ]). If the user rejects an offer or gives no counter offer then we assume that they are maintaining their previous offer. If they accept the offer then we assume that their offer is equal to FDHC's offer. Then a counteroffer is generated us-ing FDHC and LGM-Zero. This offer is realized in natural language by prompting GPT-3.5 to generate a response incorporating the action.\nWe use GPT-3.5 as our LLM policy network and a transformer with 50 encoder layers and 50 decoder layers as our value network. Our initial subgame is rooted at the belief state for the sur-plus corresponding to the price range given in the initial description of the car. After the subgame concludes, our new guess for the surplus is equal to the difference between our current offer and the maximum between our opponent's offer and our reservation price. At the final turn of the negotia-tion we offer $100 (our disagreement payoff) above our reservation price or accept our opponents offer if it's above this price. Additional implementation details can be found in Appendix D."}, {"title": "5 Theoretical Analysis", "content": "In this section we analyse the theoretical properties of the FDHC framework. Our analysis assumes that, when needed, we can manipulate our LLM policy so that one of the offers it outputs is equiva-lent to the EBS.\nWe can ensure that our framework will result in a Nash equilibrium under fairly mild assumptions. We need to assume that the bargaining game is conducted during a finite number of time steps and that the number of steps is known to both players. This gives us the result in Theorem 1.\nTheorem 1. Let tn denote the FDHC's final turn in the negotiation, let a denote the outcome proposed at tn-1, and let EBS(x) denote the EBS value for some outcome x. Setting FDHC's estimate of S = arg max(EBS(a), EBS(d)) at tn will result in a Nash equilibrium outcome.\nThe proof for this result is straightforward and is presented in Appendix B. What this theorem says is that we can adjust our surplus estimate so that at its final turn, FDHC will either concede all of the surplus to its opponent(s) or accept the opponents' offer, so long as the offer is larger than"}, {"title": "6 Experiments", "content": "We test the effectiveness of our method using both automatic and human evaluations. Our results show that our method is able to generate fairer outcomes than existing negotiation baselines. Our human evaluation also shows that our method improves perceived negotiation quality while maintaining the same level of human-like negotiation as GPT-4."}, {"title": "6.1 Baselines", "content": "We test our method against six negotiation base-lines described below.\nSupervised Learning (SL) We use the SL agent described in He et al., 2018 as our first baseline. This method uses the Craigslist bargaining dataset (He et al., 2018) to train a negotiation agent via supervised learning. More details on this baseline can be found in Appendix D.\nOffline RL Our second baseline is based on the CHAI method given in Verma et al., 2022. This method uses the Craigslist bargaining dataset to train a negotiation agent with offline Q-learning in-stead of SL. Implementation details for this method can be found in Appendix D.\nGPT-3.5 and GPT-4 We setup GPT-3.5 and GPT-4 for negotiation by prompting them with a summarized version of the scenario in Appendix F. We find that giving them the full scenario results in oversharing information. We also explicitly tell the model not to reveal its reservation price.\nGPT-4 Self-Play We include another baseline using the method described in Fu et al., 2023. This method uses self-play to generate a prompt to im-prove the negotiation performance of GPT-4. Ad-ditional details can be found in Appendix D.\nVicuna-13b Our final baseline consists of a 13b parameter Vicuna model fine-tuned using synthetic data generated from GPT-4. We generate 108 nego-tiation transcripts for various negotiation scenarios and use them to train the Vicuna model with the goal of distilling a high quality strategy."}, {"title": "6.2 Automatic Evaluation", "content": "For our automatic evaluation we conduct 100 sim-ulated negotiations between our baselines and a GPT-4 buyer. We consider optimal outcomes in these negotiations to be ones which achieve the highest values for fairness, as we consider this out-come to be the most human-compatible. For our evaluations, fairness is defined as the difference in payoff between the buyer and seller.\nThe results of our evaluation are shown in Ta-ble 1 and Figure 2. Our results show that FDHC is able to achieve much higher values of fairness compared to our baselines. More than 50% of the deals reached in our negotiations achieve a payoff difference of zero.\nWe find that our LLM-based baselines gener-ally perform better than the methods trained using domain-specific data (SL and offline RL). This may be due to the fact that there is a slight mismatch between the negotiation scenarios described in the Craigslist bargaining dataset and the one in our ex-periment (dataset details can be found in Appendix D). The scenario in our experiment gives negotia-tors explicit reservation prices which we use to cal-culate utilities. However, in the Craigslist bargain-ing scenarios no reservation prices are given and instead must be inferred. He et al., 2018 provide a method for inferring these prices which we use here, however the lack of explicit reservation prices may still be harming negotiation performance. We believe this highlights an inherent weakness of data driven bargaining methods as new data must be col-lected in order to ensure high quality performance in new negotiation domains.\nOur LLM-based baselines all perform similarly in terms of fairness, with no statistically significant differences between the outcomes. Given the lack of differences as well as the fact that the average GPT-4 deal price is the most egalitarian out of all these models, we choose to use the GPT-4 baseline for comparison in our human evaluation."}, {"title": "6.3 Human Evaluation", "content": "Setup For our human evaluation we gathered 30 in-dividuals via in-person recruiting to test our models. Each person was asked to perform a negotiation with both bots giving us 30 dialogues per model. Each user was instructed to chat with our bot until they reached a deal then answer a post-chat survey where they rated \"How good of a negotiator is the bot?\" on a scale from 1-5 and \"How human-like is the bot's negotiation?\" on a scale from 1-5. They could also optionally answer \"Do you have any suggestions for improving the bot?\" in a text box.\nWe performed some filtering on our human con-versations to avoid low quality dialogues. We re-moved any conversations where the price detection and price realization modules in our FDHC method failed in order to isolate the actual performance of our framework. This resulted in the removal of all instances where the model agreed to a price below its reservation point. Therefore we also removed instances where GPT-4 agreed to a price below its reservation price so as to not skew the data dis-tribution to favor one condition. We also filtered out dialogues where human participants chose to end the negotiation instead of agreeing to a price which would result in a positive payoff for them, as"}, {"title": "7 Conclusion and Future Work", "content": "We presented a novel framework for build-ing human-compatible negotiation agents called FDHC. Our framework uses fairness as a notion of optimality along with a novel RL+search method called LGM-Zero to learn a human-like negotiation strategy. Our automatic and human evaluations show that our method is able to achieve more egali-tarian outcomes compared to several baselines. Our human evaluation also shows that our method is able to improve negotiation quality over the GPT-4 baseline while being similarly human-like despite using GPT-3.5 as its base model.\nAn interesting direction of future work is to ex-plore the use of alternate solutions to the Nash bargaining game. These include solutions such as the Nash bargaining solution (Nash, 1950) or the Kalai-Smorodinsky bargaining solution (Kalai and Smorodinsky, 1975). While our framework is de-signed to maximize convergence to the EBS, we can easily adjust it to target any feasible outcome."}, {"title": "8 Limitations", "content": "One limitation of our method is that we only have theoretical guarantees of convergence to the EBS solution under some fairly strong assumptions. The only guarantee we can provide under mild assump-tions is convergence to a Nash equilibrium, how-ever this does not necessary imply that our negotia-"}, {"title": "9 Ethical Concerns", "content": "While our method is designed to stress the impor-tance of fairness and cooperation in negotiation, our framework can be adjusted to have our model target a variety of different negotiation goals and tactics. This includes tactics which attempt to bully and exploit people. Such \"hardball tactics\u201c are eth-ically questionable and we do not condone the use our method in this way in any real-world negotia-tion scenarios.\nHowever, negotiation research has shown that hardball tactics ultimately result in worse negoti-ation outcomes for those using them as opposed to adopting a cooperative approach to negotiation (Lewicki et al., 2021). Therefore we believe that the best use of our method for all users will be to use it in its intended way of prioritizing fairness."}, {"title": "A Definitions", "content": "In this section we provide formal definitions for terms and concepts in bargaining theory. These concepts are used for our theoretical analysis of FDHC.\nDefinition 1. (d-Comprehensivity): Given a point d \u2208 Rn and a set S C R\", S is d-comprehensive if d \u2264 x \u2264 y and y \u2208 S then x \u2208 S.\nDefinition 2. (Comprehensive Hull): The compre-hensive hull of a set S C Rn w.r.t a point d \u2208 Rn is the smallest d-comprehensive set containing S.\nDefinition 3. (Permutation Operator): A permuta-tion operator, \u03c0, is a bijection from {1, . . ., n} to {1, ..., n}. Let \u03c0(x) = (X\u03c0(1), ..., \u03a7\u03c0(\u03b7)).\nDefinition 4. (Symmetry): A solution, F(S, d), sat-isfies symmetry if for all permutation operators, \u03c0(S) = S and \u03c0(d) = d, then F\u00bf(S,d) = Fj(S, d) for all i, j.\nDefinition 5. (Weak Pareto Optimality): A weak Pareto optimal solution, F(S, d), is any solution such that F(S, d) \u2208 {x \u2208 S | y > x = y \u2209 S}.\nDefinition 6. (Strong Monotonicity): Strong mono-tonicity says that if S C S' and d d' then F(S, d) \u2264 F(S', d').\nDefinition 7. (Translation Invariance): A solution is translation invariant if \u2200x \u2208 R, F(S+{x},d+ x) = F(S, d) + x."}, {"title": "B Proofs", "content": "The domain of bargaining problems (S, d) we con-sider are problems where: S is d-comprehensive, S is compact, and \u2203x \u2208 S such that x > d. To simplify the proofs we also assume that all prob-lems have been translated so that d = 0. Since the EBS is indeed translation invariant (Thomson and Lensberg, 1989) this has assumption has little effect on our analysis.\nLemma 1. Let U denote the total amount of sur-plus and let xi denote the amount of surplus de-manded by player i. Any outcome of the Nash bargaining game where E-1 xi = U is a Nash equilibrium.\nProof. The proof is straightforward. Assume there is a player in the Nash bargaining game where the outcome satisfies i=1 xi = U. If the player demands less surplus then they receive less than what they received from the outcome. If the player demands more surplus then the deal will fail and they will receive their disagreement price which is less that what they received from the outcome. \u220e\nTheorem 1 (restated). Let tn denote the FDHC's final turn in the negotiation, let a denote the out-come proposed at tn\u22121, and let EBS(x) denote the EBS value for some outcome x. Setting FDHC's estimate of S = arg max(EBS(a), EBS(d)) at tn will result in a Nash equilibrium outcome.\nProof. By Lemma 1 we know that any deal in the Nash bargaining game is a Nash equilibrium. Therefore we can prove Theorem 1 by showing that setting S = arg max(EBS(a), EBS(d)) at tn will result in a deal if one is feasible. If a deal is reached before tn then we are done. If not we can examine the two cases for tn.\nCase 1: tn corresponds to the last turn of the negotiation.\nIn this case, the only way there can be a feasible deal is if EBS(a) \u2265 EBS(d). Therefore FDHC's estimate of S will be equal to a, which corresponds to the outcome proposed in the previous turn. Given that the estimate of S is now a single point, a, the only possible choice for FDHC is to accept a, since no other divisions of the surplus are possible under its estimate of S.\nCase 2: tn occurs before the last turn of the negotiation.\nIf EBS(a) > EBS(d) then the reasoning proceeds as in case 1. If EBS(a) < EBS(d) then the only feasible action for FDHC is to propose an outcome where it receives no surplus. This will result in some positive surplus value given to its opponents at the end of the negotiation therefore they will accept the outcome. \u220e\nLemma 2. A bargaining outcome, F(S, d), satis-fies symmetry, weak Pareto optimality, and strong monotonicity if and only if it is E(S, d).\nProof. It's easy to show that E(S, d) satisfies these axioms therefore we omit it here. Now, let F(S, d) be a solution satisfying symmetry, weak Pareto op-timality, and strong monotonicity. Since we trans-late our bargaining problem so that d = 0, we can write E(S, d) = (a, . . ., a) = x for some a > 0. Now define T as the comprehensive hull of x with respect to point 0 and consider the bargaining problem (T,0). By weak Pareto optimality and"}, {"title": "C The Egalitarian Solution in a Non-Cooperative Framework", "content": "Our theoretical analysis of the EBS and conver-gence to an egalitarian outcome has so far been restricted to an axiomatic, cooperative setting. This approach abstracts away the specifics of the bar-gaining procedure and simply examines the prop-erties of the bargaining outcome. This has the advantage of being highly generalizable as it can be applied to any problem involving surplus shar-ing. However, it does not provide any theoreti-cal insights as to why targeting an egalitarian out-come would have a strategic justification in the non-cooperative setting. Prior work has explored this problem and we give a brief overview of some approaches here to provide additional justification for why targeting an egalitarian solution can con-stitute a robust strategy.\nBossert and Tan, 1995 outline a simple two-player arbitration procedure that results in the egal-itarian outcome in a noncooperative setting. In this procedure players first make simultaneous de-mands for portions of the surplus. If the demands are compatible then both players receive what they ask for. If not the game proceeds to the next time step and players make demands again. However, in this step the player that demanded more surplus is penalized by having their demand restricted. These penalties can be implemented in a variety of ways and Bossert and Tan, 1995 show that under this procedure the only Nash equilibrium strategy pair is the one where both players target the egalitarian solution. Chun, 1989 outlines another procedure where conflicts are instead revised by setting an agents claim to the maximum of all claims, includ-ing the agents own claim. Using this bargaining procedure along with a set of non-cooperative bar-gaining axioms, Chun, 1989 shows that targeting the egalitarian solution constitutes a dominant strat-egy in this setting.\nWhile the procedures outlined in these works do not encompass the entirety of real-world bar-gaining. It does demonstrate that the egalitarian solution is consistent with the the non-cooperative outcome of some plausible bargaining procedures. Therefore it may not be unreasonable to expect that human agents would target egalitarian outcomes in their negotiations and achieve egalitarian results against FDHC."}, {"title": "D Additional Implementation Details", "content": "As is the case with many methods designed around RL+search, our LGM-Zero contains many hyper-parameters. Our hyper-parameter settings and other implementation details vary during training and inference. We first describe the settings we use during training then inference. We also provide our source code which we will release upon accep-tance."}, {"title": "D.1 Training", "content": "All training was conducted on one NVIDIA RTX A4000. The total training process took about one hour. We perform four total iterations of training. Each iteration consists of playing 50 simulated ne-gotiation subgames to completion and training the value model for four epochs on the resulting out-comes. As outlined in Section 4.2 our training method is based on fictitious self-play which in involves mixing between a best response and av-erage strategy. We mix between these strategies with equal probability, for the average strategy with simply ask GPT-3.5 to suggest one move. For the best response strategy we perform the same search detailed in Section 4.2. We perform 50 iterations of the search with an exploration hyper-parameter, Cp, of two during the selection step. All calls to GPT-3.5 were made using a temperature of zero, we also cache the outputs for each game state to avoid repeated calls when possible. In total 200 games were generated for training, with manual inspection for quality. Convergence was measured by checking when all simulated games ended at the EBS.\nOur Q-network has 10.8M parameters in total, the final layer is a linear layer with a tanh activation function. The input to the Q-network is our game state as outlined in Section 4 and the output is simply a scalar value represnting the quality of the game state. The game state is also used to construct the prompt to our LLM policy network."}, {"title": "D.2 Inference", "content": "During inference we perform ten iterations of the search process outlined in Section 4.2 with a Cp of two. Another important setting for inference is our choice of subgame decomposition. We de-compose our game into three separate subgames of lengths ten, four, and finally two. The length of the subgame is the number of offers given by both the buyer and seller, so in a subgame of length ten our model will give five offers. At the end of the last subgame we offer our minimum possible price of $12,600 and continue to offer this price until the user either agrees or ends the negotiation.\nWe targeted a negotiation length of about 16-20 turns based on pre-experimental testing and consul-tations with business professors. We chose the first subgame to be the longest due to the fact that this is the point where our initial guess for the surplus size is the highest, therefore a longer subgame length is needed to ensure that our model does not concede too much too early. As the game proceeds, our surplus estimate shrinks therefore the subsequent subgames need to be shorter so that our model does not become too stingy and will still give mean-ingful concessions. These factors are the reasons for our chosen number of subgames and lengths, although they can be set to any arbitrary value."}, {"title": "D.3 Craigslist Bargaining Dataset", "content": "The Craigslist bargaining dataset consists of human-human dialogues where two users role-play as a buyer and seller negotiating over a product on Craigslist. The users are given the product post-ing which consists of photos, a description, and the listing price. The buyer is also given a target price to aim for during the negotiation. The users then chat until an agreement is reached. Users are given freedom in how to approach the negotiation and can quit at any time in which case no deal is reached. The dataset consists of 6,682 dialogues in total with an average turn length of nine.\nAs mentioned in Section 6.2, the Craigslist bar-gaining dataset does not give explicit reservation prices for the buyer and seller"}]}