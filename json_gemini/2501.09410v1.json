{"title": "MoE2: Optimizing Collaborative Inference for Edge Large Language Models", "authors": ["Lyudong Jin", "Yanning Zhang", "Yanhan Li", "Shurong Wang", "Howard H. Yang", "Jian Wu", "Meng Zhang"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. Exploiting the heterogeneous capabilities of edge LLMs is crucial for diverse emerging applications, as it enables greater cost-effectiveness and reduced latency. In this work, we introduce Mixture-of-Edge-Experts (MoE2), a novel collaborative inference framework for edge LLMs. We formulate the joint gating and expert selection problem to optimize inference performance under energy and latency constraints. Unlike conventional MoE problems, LLM expert selection is significantly more challenging due to the combinatorial nature and the heterogeneity of edge LLMs across various attributes. To this end, we propose a two-level expert selection mechanism through which we uncover an optimality-preserving property of gating parameters across expert selections. This property enables the decomposition of the training and selection processes, significantly reducing complexity. Furthermore, we leverage the objective's monotonicity and design a discrete monotonic optimization algorithm for optimal expert selection. We implement edge servers with NVIDIA Jetson AGX Orins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results validate that performance improvements of various LLM models and show that our MoE2 method can achieve optimal trade-offs among different delay and energy budgets, and outperforms baselines under various system resource constraints.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) represent a significant breakthrough in artificial intelligence, particularly in the field of natural language processing (NLP). Built primarily on the transformer architecture [1], LLMs are trained on vast and diverse datasets, enabling them to generate and contextualize human language to a remarkable extent. Prominent examples, such as OpenAI's GPT series [2], [3] and Google's PaLM [4], [5], [6], demonstrate the capability of LLMs to perform a wide range of tasks, including text generation, summarization, translation, and question answering. Their versatility stems from their ability to learn and generalize patterns in language,"}, {"title": "A. Background and Motivations", "content": "Large language models (LLMs) represent a significant breakthrough in artificial intelligence, particularly in the field of natural language processing (NLP). Built primarily on the transformer architecture [1], LLMs are trained on vast and diverse datasets, enabling them to generate and contextualize human language to a remarkable extent. Prominent examples, such as OpenAI's GPT series [2], [3] and Google's PaLM [4], [5], [6], demonstrate the capability of LLMs to perform a wide range of tasks, including text generation, summarization, translation, and question answering. Their versatility stems from their ability to learn and generalize patterns in language,\nmaking them indispensable for applications spanning from conversational AI to code generation and beyond. However, this capability comes at the cost of immense computational and memory requirements, as LLMs often contain billions of parameters. This resource-intensive nature creates challenges for their deployment, particularly in environments where computational resources are limited, such as edge devices. As the demand for real-time, localized AI services grows, addressing these challenges has become a critical focus for researchers and practitioners alike.\nOn the other hand, the convergence of edge intelligence, mobile edge computing, and LLMs is redefining the landscape of AI-driven applications by enabling powerful computational capabilities closer to end users. Mobile Edge Computing (MEC) is a paradigm that brings computational resources closer to end users by deploying processing power and storage at the edge of mobile networks, while edge intelligence leverages the processing power of edge devices to perform real-time, localized decision-making. However, the deployment of LLMs at the edge is inherently challenging due to their high computational and memory requirements, which often exceed the resource constraints of edge devices. This interplay between edge intelligence, mobile edge computing, and LLMs highlights the need for innovative task offloading techniques to balance performance, resource efficiency, and user experience. Understanding this dynamic is crucial for unlocking the full potential of LLMs in edge settings, enabling applications such as real-time translation, conversational AI, and intelligent assistance to flourish in resource-constrained environments.\nIn this paper, we aim to bridge this gap by answering the following key question:"}, {"title": "B. Key Challenges and Solution Approach", "content": "In this work, we propose deploying Mixture-of-Experts (MoE) paradigm at network edges to enable efficient collaborative edge inference. Originally introduced to tackle complex tasks by dividing them into subtasks and assigning them to specialized experts, MoE [7], [8] has evolved into a powerful approach for efficiently up-scaling models through sparse activation [9], [10], [11]. In this sense, MoE is particularly suited for addressing the computational and efficiency challenges posed by large-scale models, such as LLMs. Unlike traditional models, which activate all parameters during inference, MoE dynamically activates a small subset of specialized \"expert\" submodels for each task [7]. This dynamic routing mechanism significantly scales model capacity without a proportional increase in computational overhead, ensuring high efficiency for resource-intensive tasks. By leveraging only the most relevant experts for a given task, MoE not only achieves better performance but also optimizes resource utilization, such as memory and processing power. MoE's modular and sparse traits has great potential in reducing training and inference costs for LLMs.\nIn the context of edge networks, MoE provides a powerful approach for leveraging the heterogeneous capabilities of edge LLM experts by adapting computations to local resource constraints and task-specific requirements. By selecting a subset of edge LLM experts tailored to each inference task, \u041c\u043e\u0415 effectively minimizes computational overhead and latency. Furthermore, strategically deploying MoE at the edge enables optimization of LLM performance while reducing energy consumption and latency, thereby enhancing the overall user experience.\nHowever, several key challenges remain, including:\n1) Various Attributes across Edges: Traditional MoE ap- proach typically choose experts merely based on gating values. Although this approach may reduce computa- tional overhead, it cannot be deployed directly in net- works with edge LLMs that are heterogeneous in various attributes, including capability, energy consumption, and latency.\n2) Combinatorial Optimization: To satisfy the system constraints, we proposed an optimization problem to select the optimal subset of LLM experts for each query. However, this problem is challenging due to the expert subset selection's combinatorial nature, the loss func- tion's non-convexity, and the complex interplay between system constraints. Traditional optimization techniques may not be suitable to resolve these competing objec- tives simultaneously.\nIn this work, we introduce Mixture-of-Edge-Experts (MoE2), a novel collaborative inference framework for edge LLMs, as shown in Fig. 1. Our framework introduces a two-level expert selection mechanism. At the coarse-grained level, the selection is optimization-based, ensuring worst-case bounds on energy consumption and latency. At the fine-grained level, experts are dynamically selected based on input prompts through a routing network. This approach effectively leverages the heterogeneity in the capabilities of edge LLM experts to handle diverse tasks. Our main contributions are summarized as follows:\n\u2022 MoE Design and Problem Formulation: We formulate the first MoE-aided inference offloading problem for edge LLMs by optimally designing the gating network and ex- pert selection, subject to energy consumption and latency constraints. This problem is inherently combinatorial and challenging due to the heterogeneous nature of edge LLMs.\n\u2022 Optimal Solution Structures: To address the challenges of combinatorial optimization, we leverage the structural properties of the optimal solution. Specifically, we show that: (i) the optimality of gating parameters for the full LLM set extends to its subsets, and (ii) the objective value exhibits a monotonic improvement property during LLM set selection. These analytical insights demonstrate that the training of gating parameters can be decoupled from the selection of LLM experts, and facilitate our optimization algorithm design.\n\u2022 Algorithm Design. The MoE2 framework, including a training stage and an inference stage, is designed based on two derived theorems. We also propose a cluster- based domain identification method to create domain experts, which efficiently utilizes the strength of our MoE framework.\n\u2022 Implementation: We implement MoE2 on edge servers, and perform extensive experiments. Our results validate that performance improvements of various LLM models and show that MoE2 can adapt to different delay and energy budgets with optimal trade-offs, and achieves strong performance compared to baselines under various system resource constraints."}, {"title": "II. RELATED WORKS", "content": "In recent years, extensive research has explored MEC across various settings (e.g., [12], [13], [14], [15]). This work pri- marily reviews the collaborative inference literature in this domain, which focuses on deploying machine learning models at the edge to enable real-time data processing and decision-making [16]. The collaborative inference literature can be categorized into two main approaches: model partitioning and edge-cloud collaboration. In edge-cloud collaboration, edge devices and the cloud cooperate to balance system performance with resource constraints. Time-sensitive tasks are processed locally on edge devices, while more complex or less time-critical workloads are uploaded to the cloud"}, {"title": "A. Mobile Edge Computing and Collaborative Inference", "content": "In recent years, extensive research has explored MEC across various settings (e.g., [12], [13], [14], [15]). This work pri- marily reviews the collaborative inference literature in this domain, which focuses on deploying machine learning models at the edge to enable real-time data processing and decision-making [16]. The collaborative inference literature can be categorized into two main approaches: model partitioning and edge-cloud collaboration. In edge-cloud collaboration, edge devices and the cloud cooperate to balance system performance with resource constraints. Time-sensitive tasks are processed locally on edge devices, while more complex or less time-critical workloads are uploaded to the cloud\nfor additional computational support (e.g., [17], [18], [19]). This cooperative mechanism has been leveraged to accelerate inference for large language models (LLMs) [19]. To optimize edge computing, various algorithms have been developed to enable collaborative inference across multiple edge devices, effectively utilizing their combined computational capabili- ties to enhance model performance (e.g., [20], [21], [22], [23]). Integrating edge computing with AI and edge-cloud collaboration is highly promising for Internet of Things IoT applications requiring low latency, high bandwidth efficiency, and real-time processing [24]. Our proposed MoE\u00b2 introduces a novel framework for collaborative inference by leveraging the diverse capabilities of LLM experts specialized in handling different tasks, distinguishing it from existing approaches."}, {"title": "B. Mixture-of-Experts (MoE)", "content": "The concept of MoE was first introduced in [7], [8] to dynamically assign inputs to multiple expert networks using a gating network. Early MoE models improved adaptability and accuracy by dividing tasks into sub-tasks, with each expert specializing in specific regions of the input space. The gating network dynamically selects the most relevant experts, enabling efficient problem-solving.\nModern MoE architectures (e.g., [9], [10], [11], [25]), in- troduced sparse activation, where the gating network activates only a subset of experts for each input, enabling models to scale efficiently to trillions of parameters. Building on this foundation, MoE has recently been applied to address the issue of performance degradation when deploying LLMs on edge devices (e.g., [26], [27]). Our work extends these principles by designing an edge-based MoE system to balance performance, latency, and energy efficiency."}, {"title": "C. Edge LLMS", "content": "The superior performance of LLMs has increasingly cap- tured researchers' attention, leading to a growing focus on integrating LLMs with edge devices, as recently surveyed in [28].\nEdges/Networks for LLMs: Edge deployment of LLMs offers solutions to the challenges of cloud-based systems, such as high latency, data security concerns, and connectivity limitations. By leveraging edge computing, LLMs can process data locally, improving response times, enhancing privacy, and reducing bandwidth usage, which makes them particularly suitable for latency-sensitive and resource-constrained applica- tions [28]. Recent advancements in edge inference techniques for LLMs can be categorized into centralized edge inference, split inference, and collaborative inference. Centralized edge inference reduces communication overhead by optimizing to- ken representations (e.g., [29], [30]) and employing methods such as service placement and migration (e.g., [31], [32], [33], [34]). Split inference divides tasks between edge devices and servers to balance computation and communication, using strategies like token representation reduction (e.g., [35], [36], [37]), progressive offloading [38], early exit [39] and multi-hop architectures [40]. Collaborative inference employs speculative decoding (e.g., [41], [42]), where lightweight models on\nedge devices generate initial predictions that are refined by powerful servers. These techniques collectively enable scalable and resource-efficient edge inference for LLMs. However, when deploying LLMs at the edge, the trade-offs between performance, latency, and energy efficiency are not always fully considered. In addition, our approach did not require intrusive modifications, i.e., any modifications to edge LLM experts.\nLLMs for Edges/Networks: LLMs are transformative in wireless communications, addressing tasks like network configuration, traffic classification, and 6G optimization by reducing human effort and improving efficiency [43]. They are increasingly used for telecom-related tasks, such as domain knowledge generation (e.g., [44], [45]), code generation (e.g., [46], [47], [48], [49]), and network configuration generation (e.g., [50], [51], [52]). LLMs also excel in classification tasks, including network security (e.g., [53], [54], [55], [56], [57]), text (e.g., [58], [59]), image (e.g., [60], [61]), and network traffic classification (e.g., [62], [63]). In network optimization, LLM-enabled techniques like reinforcement learning (e.g., [64], [65], [66]), black-box optimization [67], convex opti- mization (e.g., [68], [69]), and heuristic algorithms (e.g., [70], [71]) enhance wireless network management. Additionally, LLMs apply time series models like pre-trained foundation models (e.g., [72], [73]), frozen pre-trained models (e.g., [74], [75]), fine-tuning (e.g., [76], [77]), and multi-modality approaches [78] to predict trends and demands. Wu et al. further pioneer LLMs as foundational models for networking with their NetLLM framework, reducing manual efforts and improving adaptability [79]. Our study fundamentally differs from the aforementioned works on \u201cLLMs for networks\", where the primary objective is to utilize LLMs to optimize edge networks, whereas our focus is on leveraging edge resources to support LLMs.\nClosely Related Works: Only a couple of studies are closely related. Yi et al. [26] proposed a framework which enables efficient on-device inference with a single LLM on a single edge server, with experts stored in external memory and loaded as needed. This work did not consider collabo- rative inference by exploiting heterogeneous LLM experts. A concurrent study by Li et al. [80] proposed a MoE model over edges for continual learning, while it did not consider the deployment of large models under a resource-constrained setting.\""}, {"title": "III. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "In this section, we present the system model for edge LLMs, the MoE2 architecture, and the two-level expert selection scheme. We then formulate the joint gating parameter training and LLM expert selection problem."}, {"title": "A. Edge LLM System Overview", "content": "System Overview. As shown in Fig. 2, we consider a system of a set N of specialized LLM experts (agents). Each LLM expert n \u2208 N is deployed on one edge server.\nTask Model. Mobile users with inference tasks may choose to offload them to edge LLM experts. Let X represent the\nset of all prompts requested by different users. Users may send prompts x \u2208 X to edge servers based on their specific applications. Future edge LLMs have diverse applications, in- cluding mobile health, humanoid robots, virtual assistants, and autonomous driving [28]. For example, Google's Med-PaLM 2 is an LLM fine-tuned on medical datasets, designed to provide answers to medical inquiries. Beyond this, healthcare LLMs can assist with tasks such as medical question answering, diagnosis, treatment planning, and medical report generation.\nWe consider a set M representing categories of LLM applications, which constitute M partitions of X. Specifically, we use Xm to denote the set of prompts (tasks) x associated with LLM application m. Each LLM application has its own latency requirements, which will be specified later.\nAutoregressive Process. The system processes \u00e6 and re- turns T prediction tokens through the autoregressive pro- cess. The length T is decided by the experts through the autoregressive process defined as follows. Starting with the prompt x, an LLM agent generates output tokens \u0177t at time step t \u2208 T sequentially. The system maintains a history h(x, t) = {x, \u01771,\u2026, \u0177t\u22121} that combines the original prompt and all previously generated tokens. At time step t, an LLM expert n \u2208 N may processes this history through the LLM output function fn to generate a probability distribution fn(h(x, t)) \u2208 P(V) for the next token \u0177t, where V is the used vocabulary space. The generation of \u0177t from fn(h(x,t)) will be discussed later.\nDistributions. Each prompt x is paired with a target answer y = [yt]t=1T. We let Px be the probability distribution of different prompts, requested by the users, and let Px,y denote\nthe joint distribution of (x, y). For each LLM application m\u2208 M, we use Px,m to denote the probability distribution of x conditioned on x \u2208 Xm."}, {"title": "B. The Mixture-of-Edge-Experts (MoE2) Architecture", "content": "MoE2. The system employs an MoE-like architecture to select a subset of edge LLM experts to generate token pre- dictions dynamically. MoE2 consists of two key components: a gating network and a two-level expert selection mechanism. Given a prompt x, the generated distributions fn(h(x, t)) are weighed by the gating network and aggregated at the user end to generate the final token \u0177t, which is subsequently added to the history h(x, t) for future generation. This iterative process continues, with each new token generation benefiting from the accumulated context of all previous predictions.\n1) Gating Network: Our system implements a distributed architecture where each edge server hosts a dedicated gating network that optimizes the routing weights for the accessible LLM experts. The gating network employs an embedding model to extract textual information and a multi-layer percep- tron (MLP) to generate gating values for the LLM experts. Upon receiving a prompt from a mobile user, the nearest edge server processes the request with its gating network, coordinates the token generation process, and synthesizes the final response for transmission back to the user. Let e denote the gating parameters of the gating network. Given prompt x, the gating network [9] outputs:\n$$g(x, \\theta) = [g_n(x, \\theta)]_{n\\in N},$$\nwhere $g_n(x, \\theta)$ is the n-th element of g(x, \u03b8). An example gating function $g_n(x,\u03b8)$ is the multilayer perceptron (MLP). Given the gating values, we compute the weights of experts with different expert selections. Given prompt x, for any select subset of experts S \u2286 N, we compute the normalized weights of S as:\n$$\\omega(x, \\theta, S) = [w_n(x, \\theta, S)]_{n\\in S} = \\frac{g_n(x, \\theta)}{\\sum_{n'\\in S}g_{n'}(x, \\theta)},$$\nThen, we can obtain the ensemble prediction by a weighted combination of expert outputs [9]. The fused probability distribution of token k is expressed as:\n$$F(x,\\theta,S,t) = \\sum_{n\\in S}\\omega_n(x, \\theta, S) f_n(h(x,t)).$$\nNote that F(x, 0, S, t) is a probability distribution of V, i.e., F(x,0,S,t) \u2208 P(V). A new token \u0177t+1 is sampled directly from F(x, 0, S, t), i.e., \u0177t+1 ~ F(x, 0, S, t). Then, \u0177t+1 will be added to the history information h(x, t + 1). This process repeats until a stop token is generated.\n2) Two-Level Expert Selection Mechanism: One of the key challenges, compared to conventional MoE architectures, lies in the joint selection of LLM experts and the training of gating parameters. Traditional MoEs typically choose experts merely based on gating values, employ additional routing networks to introduce sparsity. These routing networks often add tunable noise and retain only the top k values before applying the softmax function (as in (2)). Although this approach can reduce computational overhead, it cannot be directly deployed in networks with edge LLMs that are heterogeneous across various attributes, including capability, energy consumption, and latency.\nTo address the above challenge, we propose an LLM expert selection scheme comprising two levels: a coarse-grained level and a fine-grained level, as illustrated in Fig. 2:\n\u2022 The coarse-grained level selects a subset of LLM ex- perts, denoted by S, based on system constraints, such as through optimization-based methods. This selection ensures a worst-case bound on energy consumption and induced latency, accounting for scenarios where all LLM experts in S are utilized.\n\u2022 In the fine-grained level, LLM experts are further selected from the subset S dynamically based on prompts x via a routing network, leveraging the heterogeneity of LLM agents in handling different tasks while further reducing system costs.\nAs we will demonstrate later, the two-level expert selection mechanism, combined with a sufficiently sophisticated gating function g(x, 0), is crucial for enabling efficient algorithm design to jointly train and optimize S.\n3) Where to Deploy/Train MoE2: Based on our experimen- tal results presented later, a gating network with superior per- formance can be as small as 1.7GB, enabling its deployment on edge servers (alongside edge LLMs) to serve nearby users effectively. This also opens up the possibility of adopting edge federated learning or cloud-edge collaboration frameworks, where training is conducted in the cloud and deployment occurs at edges."}, {"title": "C. System Constraints", "content": "In reality, LLM applications typically have specific service requirements, which make deploying LLMs at the mobile edge a more efficient solution. The system incurs costs from two primary sources: system delay and energy consumption.\n1) System Delay: The system delay in our system can be categorized into three components: the computational time of the LLM experts, the transmission latency between edge devices and mobile devices, and the ensemble time when aggregating the outputs of LLM experts. For an edge LLM agent n, the computation time of prompt x can be represented as:\n$$T_n^{comp}(x) = \\frac{C^{token}(x)M_{size}}{C_{e}^{cap}} + \\frac{M_{size}}{b_n} + Covh,$$\nwhere:\n\u2022 $C^{token}(x)$: Average computational cost per token (in FLOPs) for edge LLM model n for input x.\n\u2022 $C_{e}^{cap}$: Computational capability (in FLOPs per second) of edge server e.\n\u2022 $M_{size}$: Average memory access size (in bytes).\n\u2022 $b_n$: Memory bandwidth (in bytes per second) of edge server n.\n\u2022 Covh: Constant overhead for operations, e.g., data transfer between CPU and GPU of edge server e, which cannot be overlapped with other operations (in seconds).\nThe transmission delay between LLM agent n and mobile devices can be calculated as:\n$$T_n^{tran}(x) = \\frac{D(x)}{R_n}$$\nwhere D(x) is the data size of the transmitted prompt x and Rn(t) is the achievable data rate between LLM agent n and users.\nFor an answer with T tokens, the delay between mobile user u and edge server with LLM model n can be expressed as the followings, where for all t\u2208 [T],\n$$T_n(x,t) = T_n^{comp}(h(x, t)) + T_n^{tran}(\\hat{y}_{t-1}) + T_n^{tran}(p_{n,t}),$$\nwhere we set h(x,0) = x and \u01770 = x for simplicity, and define [T] {1,\u2026\u2026,T}.\nWe use (x, S) to denote the overall delay for token-level ensemble generation of prompt x with LLM models subset S, given by:\n$$\\tau(x, S, t) = \\max_{n\\in S}{\\{T_n(x,t)\\}}.$$\nIn other words, the overall delay is determined by the the slowest edge LLM expert within the selected subset S, as the framework cannot generate the next token until all experts have completed their computations. The long-term system delay can be expressed as (rgate is the delay of gating network computation):\n$$\\Gamma(x, S) = \\sum_{t=1}^T\\tau(x, S, t) + r_{gate}.$$\nFinally, each LLM application m \u2208 M has its own latency requirement, given by the following constraint:\n$$E_{x\\sim P_{x,m}}[\\Gamma(x, S)] \\le T_{max,m}, \\forall m \\in M,$$\nwhere Tmax,m is the delay limit (deadline) in response to arbitrary prompt x for LLM application m.\n2) System Energy Consumption: The energy consumption of our system mainly depends on the computational energy consumption of inference for edge LLM experts. The total energy consumption for token-level ensemble generation is:\n$$E(x, S, t) = \\sum_{n\\in S} E_n^{comp} (h(x, t)),$$\nwhere $E_n^{comp}(h(x,t))$ represents the energy consumption to inference input h(x,t) for the LLM agent n. The long-term system energy consumption can be represented as:\n$$E(x, S) = \\frac{1}{T} \\sum_{t=1}^T E(x, S, t),$$\nwhich is constrained as:\n$$E_{x\\sim P_{x}}[E(x, S)] < E_{max},$$\nwhere Emax is the energy budget."}, {"title": "D. Problem Formulation", "content": "Our objective is to jointly optimize the (coarse-grained level) expert selection S and train the gating parameters \u03b8, considering the performance of LLM agents under the afore- mentioned system delay constraints and energy consumption constraint. To measure the quality of the ensemble answer generated by an expert subset S. For an answer with T tokens generated from S, the loss function of the ensemble prediction is defined as:\n$$L(\\theta, S, x, y) = - \\frac{1}{T} \\sum_{t=1}^T log\\left( \\sum_{n \\in S} \\omega_n(x, \\theta, S) f_{n,y_t}(h(x, t)) \\right),$$\nwhere fn,yt (h(x,t)) denotes the probability of yt in fn(h(x,t)). The expected loss function of the whole distri- bution can be denoted as:\n$$L_{exp}(\\theta, S) = E_{(x,y)\\sim P_{x,y}}[L(\\theta, S, x, y)].$$\nSince Px,y is not directly accessible in practice, we consider to use an empirical loss function instead:\n$$L(\\theta, S) = \\frac{1}{|D|} \\sum_{(x,y) \\in D} L(\\theta, S, x, y),$$\nwhere D is a training dataset. We now formulate the optimiza- tion problem:\nThe joint gating parameter training and LLM expert selection problem is formulated as:\nOP:\n$$\\min_{\\theta, S} L(\\theta,S)$$\ns.t.\n$$E_{x\\sim P_{x,m}}[\\Gamma(x, S)] \\le T_{max,m}, \\forall m \\in M,$$\n$$E_{x\\sim P_{x}}[E(x, S)] \\le E_{max}.$$\nThis problem is challenging due to the combinatorial nature of the expert subset selection, the non-convexity of the loss function, and the complex interplay between the system delay and energy consumption constraints. For a given input, the system must select the optimal subset of experts to minimize the loss function while satisfying the constraints on delay and energy consumption. Traditional optimization techniques may not be suitable for addressing these competing objectives simultaneously. Therefore, we propose a novel methodology to solve this problem effectively and efficiently.\nRemark 1. We note that Px, which is required in the con- straints of (16), is also not directly accessible in practice. However, based on our experimental results presented later, both (x, S) and E(x, S) primarily depend on the length of the prompts, and their statistical significance becomes notable only when the prompt length exceeds 1024 tokens. This implies that $E_{x\\sim P_{x,m}}[\\Gamma(x, S)]$ and $E_{x\\sim P_{x}}[E(x,S)]$ are much easier to estimate in practice compared to Lexp(0, S)."}, {"title": "IV. METHODOLOGY", "content": "MoE2 consists of two key stages training and inference. In training stage, we solve Problem (16) to attain the optimal overall response quality while subject to constraints. While in inference stage, we focus on balancing response quality with system costs."}, {"title": "A. Optimal Solution Structures", "content": "Define the optimal parameters 0* when fixing the subset S of edge LLMs:\n$$\\theta^*(S) \\triangleq \\arg \\min_{\\theta} L(\\theta, S), \\forall S \\subseteq N.$$\nWe provide the theoretical analysis of the relation between 0 and S. Then the following theorem extends\nTheorem 1 (Optimality for subset). Let the gating network g(x,0) be an MLP with a sufficiently large width. For any given subset $S \\subseteq N$ and the optimal parameters satisfy the following condition:\n$$\\frac{g_n(x, \\theta^*(S))}{\\sum_{n' \\in S} g_{n'}(x, \\theta^*(S))} = \\frac{g_n(x, \\theta^*(N))}{\\sum_{n' \\in N} g_{n'}(x, \\theta^*(N))}, \\forall n \\in S.$$\nThe key idea of proving Theorem 1 is conducted by contradiction, constructing a parameter 0' through universal approximation that satisfies the required conditions, ultimately leading to a contradiction. For a detailed proof, please refer to Appendix \u0412."}, {"title": "B. Algorithm Design", "content": "We present the algorithm for both training and inference stages.\n\u2022 The training stage focuses on optimizing the gating network parameters, 0, and subset selection to enhance system performance while adhering to system constraints.\n\u2022 As suggested by Theorems 1 and 2, the training process for can be decoupled from the expert selection process. For the coarse-grained expert selection problem in (16), we propose utilizing the discrete monotonic optimization algorithm [83].\n\u2022 In the inference stage, the goal is to perform fine-grained subset selection for each query, x, to fully exploit the heterogeneous capabilities of edge LLM experts. The structure of the proposed algorithm is illustrated in Fig. 2.\nRemark 2. Theorem 1 indicates that the optimal gating values 0*(S) for any subset S can be derived directly from the optimal gating values of the complete set N. Consequently, once the optimal parameter 0* (N) for the entire set N of edge LLMs is obtained, the optimal gating values for any subset S can be determined without the need for additional training.\nProblem (16) can be reduced to a combinatorial optimiza- tion over experts as:\n$$\\min_{S} L(\\theta^*(S), S)$$\ns.t.\n$$E_{x\\sim P_{x,m}}[\\Gamma(x, S)] \\le T_{max,m}, \\forall m \\in M,$$\n$$E_{x\\sim P_{x}}[E(x, S)] \\le E_{max},$$\nNote that when searching for the optimal S*, Theorem 1 suggests that it is not necessary to repeatedly train the gating networks to obtain 0*(S) for each S. This significantly reduces the complexity of the original problem.\nNow we aim to optimize subsets S to satisfy system constraints. We first introduce the property of monotonicity improvement of subsets as follows:\nTheorem 2 (Monotonic Improvement). Let the gating network g(x, 0) be an MLP with a sufficiently large width. For any pair of subsets S, S' \u2208 N such that S' \u2286 S, we have\n$$\\min_{\\theta} L(\\theta, S) \\le \\min_{\\theta} L(\\theta, S').$$\nProof. Consider the following function f : X \u2192 RINI, given by\n$$f_n(x) = \\begin{cases} \\frac{g_n(x,\\theta^*(S'))}{\\sum_{n' \\in S'} g_{n'}(x,\\theta^*(S'))}, & \\text{if } n \\in S', \\\\ 0, & \\text{if } n \\in S \\setminus S', \\end{cases}$$\nwhere h(x) = [hn(x)]n\u2208N. Since h(x) has at most countably many discontinuities, it is Borel measurable [81]. According to the universal approximation theorem [82], there exists \u03b8 such that the MLP g(x, 10) with a sufficiently large width can approximate h(x) to any degree of accuracy.\nThen, we have:\n$$L(\\theta^*(S), S) \\le L(\\hat{\\theta},S) = L(\\theta^*(S'), S').$$\n$$g(x, \\theta^*) = \\frac{g_n(x, \\theta^*(S))}{\\sum_{n' \\in S} g_{n'}(x, \\theta^*(S))}, \\forall S \\subseteq N.$$\nSince Px,y is not directly accessible in practice, we consider to use an empirical loss function instead."}, {"title": "V. SIMULATION RESULTS", "content": "In this section, we deploy MoE2 on local devices and test the model performance. We use a novel cluster-based method to create domain experts, which exploits the strength of the MoE mechanism."}, {"title": "A. Cluster-based Domain Experts", "content": "To utilize the strengths of the MoE mechanism", "reasons": "n\u2022 Comprehensive LLM training: Most existing LLM models are trained over diverse datasets", "domains": "The domain of a given prompt is not always clear", "datasets": "It is challenging to collect sufficient domain-specific data to train an expert even if the domain is distinguishable for prompts. To train a domain-specific expert, we need to label a large number of samples, which is time-consuming and expensive.\nTherefore, we propose a cluster-based"}]}