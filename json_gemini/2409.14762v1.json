{"title": "Do Large Language Models have Problem-Solving Capability under Incomplete Information Scenarios?", "authors": ["Yuyan Chen", "Tianhao Yu", "Yueze Li", "Songzhou Yan", "Sijia Liu", "Jiaqing Liang", "Yanghua Xiao"], "abstract": "The evaluation of the problem-solving capability under incomplete information scenarios of Large Language Models (LLMs) is increasingly important, encompassing capabilities such as questioning, knowledge search, error detection, and path planning. Current research mainly focus on LLMs' problem-solving capability such as \u201cTwenty Questions\u201d\u201d. However, these kinds of games do not require recognizing misleading cues which are necessary in the incomplete information scenario. Moreover, the existing game such as \"Who is undercover\" are highly subjective, making it challenging for evaluation. Therefore, in this paper, we introduce a novel game named BrainKing based on the \"Who is undercover\" and \"Twenty Questions\" for evaluating LLM capabilities under incomplete information scenarios. It requires LLMs to identify target entities with limited yes-or-no questions and potential misleading answers. By setting up easy, medium, and hard difficulty modes, we comprehensively assess the performance of LLMs across various aspects. Our results reveal the capabilities and limitations of LLMs in BrainKing, providing significant insights of LLM problem-solving levels.", "sections": [{"title": "1 Introduction", "content": "Incomplete information scenarios include missing information, uncertainty, and misinformation, encountered in fields such as business negotiations, military strategy, medical diagnosis, and legal judgments (Chen and Xiao, 2024; Gibbons and Gibbons, 1992; Chen et al., 2024f; Neo et al., 2024; Jin et al., 2024). The problem-solving capability under incomplete information refer to the capability to effectively handle available information, make rational inferences, and decisions in situations lacking comprehensive data. This capability is crucial in real life as we cannot possess all the necessary information for decision-making (Chen et al., 2023e, 2024d,a, 2022). It's also important for large language models (LLMs), which not only tests LLMs' logical reasoning capabilities but also involves adjusting strategies in constantly changing environments, significantly enhancing their robustness and quality of decision-making in various fields. Therefore, a natural question arises: Do LLMs have problem-solving capability under incomplete information scenarios?\nPrevious research on problem-solving capability under incomplete information scenarios focus on simulating complex decision-making environments through games (Jin et al., 2022; Yang et al., 2022; Zhao et al., 2024; Jin et al., 2023), such as \"Werewolf\" (Xu et al., 2023; Ri et al., 2022; Toriumi et al., 2017), \u201cPoker\u201d (Brown and Sandholm, 2019) and \"Avalon\" (Light et al., 2023), etc. These games require players to make decisions without full information, often involving deception and strategic planning to conceal their real identities. As shown in Fig. 1(a), \"Who is undercover\" is another incomplete information game which requires players to deduce whether they are the spy based on others' descriptions. In this running example, player with \"bread\" is the spy in this game and he needs to hide himself against being caught through twist the facts in his description like \"It typically requires more sugar, fats, and eggs.\". However, even advanced LLMs like GPT-3 and GPT-3.5, while excelling in general NLP tasks (Tao et al., 2024; Liu et al., 2023; Xiong et al., 2024; Chung et al., 2023; Chen et al., 2024e), revealing limitations in effective decision-making under incomplete information environments (Gigerenzer and Gaissmaier, 2011; Binz and Schulz, 2023; Chen et al., 2024c,b; Ni et al., 2024b,a; Li et al., 2024a).\nInformation processing is a crucial problem-solving capability under incomplete information scenarios, exemplified by the Minesweeper game (Li et al., 2023), the Twenty Questions game (Walsorth, 1882; Giordano et al., 1998), etc. As shown in Fig. 1(b), the Twenty Questions game requests the player to pose a series of yes-or-no questions to guess the given entity (i.e. \"George clooney\u201d), which can effectively evaluate LLMs' creativity (Hu et al., 2018), knowledge retrieval (Williams and Klamen, 2015; Szyma\u0144ski and Duch, 2012; Chen et al., 2023c; Li et al., 2024b; Zhou et al., 2024), multi-hop reasoning capabilities (Noever and McKee, 2023; Siegler, 1977; Chen et al., 2023d). However, the above-mentioned games, such as Twenty Questions, do not adequately assess LLMs' capabilities in processing information and solving problems because it lacks deception and strategic complexity that require recognizing misleading cues and formulating adaptive strategies based on limited or false information. Moreover, games like \u201cWerewolf\u201d and \"Who is undercover\" are highly subjective, making it challenging to evaluate LLMs' capabilities under incomplete information scenarios effectively.\nTherefore, in this paper, we introduce a new game named BrainKing by combining the \"Who is undercover\u201d and \u201cTwenty Questions\" game to assess LLMs' information processing and problem-solving capabilities under incomplete information scenarios. BrainKing challenges LLM participants to identify entities amidst potential misinformation through a limited set of yes-or-no questions across easy, medium, and hard difficulty modes. This game objectively assesses LLMs' world knowledge, reverse thinking, and error detection capabilities."}, {"title": "2 Datasets and Task Setups", "content": "The proposed BrainKing game, inspired by the Twenty Questions game, requires LLM participants to identify an entity with a limited number of yes-or-no questions despite potentially misleading answers. It contains three difficulty modes: easy, medium, and hard, as illustrated in Fig. 2. These modes are designed to thoroughly assess an LLM's world knowledge, reverse thinking, and error detection capabilities in identifying the target entity.\nSpecifically, we utilize an open-source Twenty Questions dataset \u00b9, which comprises 78,890 entities. To ensure the task is manageable and not hindered by a lack of knowledge by all LLM participants, we adopt GPT3.5 \u00b2 to score each entity for commonness from 1 (less common) to 3 (more common), retaining the top 10,000 common entities. Next, we also adopt GPT3.5 to generate a hierarchical concept list for each entity. This list must include at least three concepts, where each subsequent concept is a broader or more abstract"}, {"title": "2.1 Difficulty Modes", "content": "Easy mode is to provide a simple starting point which is the first concept from the hierarchical concept list of the target entity. It requires LLM participants to generate at most 20 yes-or-no questions to identify the target entity, which is the same as traditional Twenty Question game. For example, in Fig. 2, the simple starting point for the target entity \"Tiger\" is \"Pantherinae\u201d, which directly guides the LLMs to focus its inference within the realm of Pantherinae. The possible question is \u201cDoes it have stripes?", "Is it large in size?\" to confirm whether it belongs to Pantherinae.\nHard mode introduces a similar entity for generating wrong answers which is set as two in our task besides providing the simple starting point same as the easy mode. It requires LLM participants to generate at most 20 yes-or-no questions to identify the target entity with misleading information. For example, in Fig. 2, a possible question is \u201cDoes it often engage in activities in trees?\" and the correct answer is \u201cNo": "ecause the target entity tiger is usually active on the ground instead of the tree which indicates the similar entity \"Leopard\". LLM participants are expected not to navigate through this wrong answer and rethink the correct inference path."}, {"title": "2.2 Evaluation Metrics", "content": "We adopt accuracy and rounds as metrics to evaluate LLMs' performance in the proposed BrainKing. Accuracy measures whether an LLM can infer the target entity within 20 rounds of questioning, with a successful guess scored as 1 and an unsuccessful one as 0. Rounds measures how many questions it takes to infer the entity. Once the entity is inferred, the game stops regardless of whether it is correct or incorrect, preventing the LLMs from exploring the full range of possible entities. If the entity is not inferred within Twenty Questions, we set its rounds as 30, as it is not feasible to continue BrainKing indefinitely. We multiply the accuracy and the reciprocal of the rounds by 100 to obtain the accuracy win rate and rounds win rate, respectively. The average of the accuracy win rate and rounds win rate is then calculated to determine the total win rate. The optimal score for accuracy win rate, rounds win rate, and total win rate is 100. The higher the value, the higher the win rate.\nMoreover, we also introduce the ability to recognize confusion as an evaluation metric to determine whether an LLM can backtrack from misleading"}, {"title": "3 Experiments", "content": "In this section, we conduct extensive experiments to evaluate different LLMs' performance in the proposed BrainKing."}, {"title": "3.1 Experimental Setups", "content": "Our experiments are conducted on 8 Nvidia A100 GPUs, each with 80GB of memory, and we use PyTorch \u00b3 in Python \u2074. We set the maximum sequence length for both input and output sequences to maximum 200 tokens. we use GPT4 to respond to the questions posed by each LLM."}, {"title": "3.2 Datasets, Baselines and Metrics", "content": "The baseline LLMs for this evaluation are BLOOM-7B (Workshop et al., 2023) BLOOM-176B (Workshop et al., 2023), Claude2 (Bai et al., 2022), Falcon-7B (Almazrouei et al., 2023), Falcon-180B (Almazrouei et al., 2023), GPT3.5 (Brown et al., 2020), GPT4 (OpenAI, 2023), LLaMA2-7B (Touvron et al., 2023), LLaMA2-70B (Touvron et al., 2023), Vicuna-7B (Chiang et al., 2023), and Vicuna-33B (Zheng et al., 2023). The prompt for playing the BrainKing is shown in Table 1.\nWe recruit nine volunteers to participate in BrainKing. First, we select three entities, each representing a different mode of difficulty to test each volunteer's ability. Then, we rank their comprehensive scores from high to low. The top three scorers are assigned to hard mode, the middle three to medium mode, and the last three to easy mode. We then randomly distribute 1,000 entities, other than the initial three, among the nine volunteers and calculate their accuracy win rate, rounds win rate and total win rate. The highest total win rate from each of the three modes are averaged again to obtain the Human performance. Volunteers participate on a voluntary basis without compensation."}, {"title": "3.3 Main results", "content": "Question 1:Which LLM is the winner of the BrainKing? Answer 1: GPT4!"}, {"title": "3.4 Case study", "content": "We show a good running example with questions generated by the top two LLMs (i.e. GPT4 and Claude2) and human in Fig. 9 and Fig. 10, respectively. More cases are shown in Fig. 11, Fig. 12.\nIn Fig. 9, we find that both GPT4 and Claude2 make correct reasoning with just three questions in the easy mode. GPT4 focuses on determining the type of musical instrument and excluding percussion instruments, while Claude2 directly asks if it is a string instrument and if it belongs to classical music instruments. This shows that both are efficient in identifying with basic questions. In the medium mode with a harder starting point, GPT4 needs ten questions, while Claude2 only needs four. GPT4 starts by confirming it is a string instrument and narrows down to a guitar, including questions about whether it is played with a bow and if it is common in rock music. In contrast, Claude2's questions are more direct, quickly moving from whether it is a string instrument and played with a bow to identifying it as a guitar. This suggests that Claude2 is slightly more efficient and accurate in a more complex situation. In the hard mode, both GPT4 and Claude2 show the ability to narrow down gradually to identify the guitar. GPT4 takes ten questions, starting with confirming it is a string instrument and then asking if it is common in rock music and if it has frets to exclude the violin. Claude2 needs twelve questions, also starting with confirming it is a string instrument, but focusing more on physical characteristics like size and whether it has a hollow body, eventually asking if it is part of the guitar family. In this situation, GPT4 shows higher efficiency, although both are accurate.\nOverall, the differences between them may stem from their specific ways of processing information and decision-making. However, there are also some bad cases, including wrong logic reasoning that ask same questions which have been answered before, not following instructions that asks non-yes-or-no questions or self-questioning as shown in Fig. 12."}, {"title": "4 Related Work", "content": ""}, {"title": "4.1 Gaming abilities of LLMs", "content": "Recent research shows the capabilities of LLMs in various gaming scenarios. For example, Zhao and Anderson (2023) evaluate LLMs in solving and creating puzzles for the NPR Sunday Puzzle game show; Jiang et al. (2023) introduce BRAINTEASER to assess lateral thinking in LLMs; Lor\u00e8"}, {"title": "4.2 Evaluation for LLMs' Capabilities", "content": "Recent research has extensively explored the capabilities of LLMs across various domains (Chen et al., 2023a,b; Ren et al., 2024). For example, Ziems et al. (2023) demonstrate that LLMs can significantly contribute to Computational Social Science by classifying and explaining social phenomena; Zheng et al. (2023) explore the use of LLMs as judges to evaluate chat assistants, introducing benchmarks like MT-bench and Chatbot Arena; Zhong et al. (2023) focus on comparing the understanding ability of ChatGPT with fine-tuned BERT-style models using the GLUE benchmark; Laskar et al. (2023) present a comprehensive evaluation of ChatGPT on diverse academic datasets, including question-answering, text summarization, code generation, commonsense reasoning; Valmeekam et al. (2023a) introduce PlanBench, a benchmark for evaluating LLMs on planning and reasoning; Del and Fishel (2023) introduce a benchmark consisting of long-form mystery narratives in assessing LLMs' advanced reasoning abilities; Sawada et al. (2023) propose a benchmark containing advanced reasoning problems across multiple domains to evaluate the advanced reasoning capabilities of LLMs; Valmeekam et al. (2023b) investigate the planning abilities of LLMs in commonsense tasks and as heuristic guidance for other agents; While the aforementioned studies design different benchmarks in evaluating LLMs' capabilities, there is lack of a benchmark for evaluating the information processing and problem-solving capability of LLMs under incomplete information scenarios."}, {"title": "5 Conclusions and Future Work", "content": "In conclusion, our study highlights the importance of a multifaceted approach to evaluating the information processing and problem-solving capability of LLMs under incomplete information scenarios. The BrainKing game, as a novel benchmark, successfully challenges LLMs in various cognitive aspects, from basic knowledge retrieval to complex reasoning and confusion recognition. Our findings offer a detailed understanding of the strengths and limitations of current LLMs, underscoring the need for LLMs to not only process information accurately but also navigate through misleading information effectively. For future work, there is potential to extend the complexity and scope of the BrainKing, introducing more challenging scenarios to better mimic real-world conditions."}, {"title": "Limitations", "content": "There are a few key limitations. First, the Twenty Questions dataset used may not fully represent all possible types of entities. Moreover, selecting only the top 10,000 most common entities might limit the breadth and depth of the assessment. Second, in the hard mode of the game, LLMs are required to correctly identify the target entity based on potentially misleading information. This might be an overly challenging demand, especially for LLMs not yet fully trained to handle such complex tasks.\nThird, determining whether an LLM can recover from misleading information and return to the correct line of questioning can be a complex process. It involves a detailed analysis of the entire question-and-answer session, which could add to the complexity and subjectivity of the evaluation."}, {"title": "Acknowledgements", "content": "This work is supported by Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902), Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103), the National Natural Science Foundation of China (No.62072323), Shanghai Science and Technology Innovation Action Plan (No. 22511104700), and the Zhejiang Lab Open Research Project (NO. K2022NB0AB04)."}]}