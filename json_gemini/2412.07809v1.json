{"title": "Fine-Grained Graph Representation Learning for Heterogeneous Mobile Networks with Attentive Fusion and Contrastive Learning", "authors": ["Shengheng Liu", "Tianqi Zhang", "Ningning Fu", "Yongming Huang"], "abstract": "AI becomes increasingly vital for telecom industry, as the burgeoning complexity of upcoming mobile communication networks places immense pressure on network operators. While there is a growing consensus that intelligent network self-driving holds the key, it heavily relies on expert experience and knowledge extracted from network data. In an effort to facilitate convenient analytics and utilization of wireless big data, we introduce the concept of knowledge graphs into the field of mobile networks, giving rise to what we term as wireless data knowledge graphs (WDKGs). However, the heterogeneous and dynamic nature of communication networks renders manual WDKG construction both prohibitively costly and error-prone, presenting a fundamental challenge. In this context, we propose an unsupervised data-and-model driven graph structure learning (DMGSL) framework, aimed at automating WDKG refinement and updating. Tackling WDKG heterogeneity involves stratifying the network into homogeneous layers and refining it at a finer granularity. Furthermore, to capture WDKG dynamics effectively, we segment the network into static snapshots based on the coherence time and harness the power of recurrent neural networks to incorporate historical information. Extensive experiments conducted on the established WDKG demonstrate the superiority of the DMGSL over the baselines, particularly in terms of node classification accuracy.", "sections": [{"title": "Introduction", "content": "The emerging trend of convergence between AI and wireless technology is expected to bring along new research opportunities and better connectivity for people. Now commercial usage of 5G has reached maturity in the leading markets and has sparked a growing appetite for new services that imply extremely stringent requirements. The rapid evolution of networks' capabilities has introduced significant structural complexity, posing challenges for network management and maintenance. As such, the spotlight is on network automation as a prominent trend for forthcoming 6G networks projected for launch by 2030 (You, Huang et al. 2023), which integrates essential functions such as self-configuring, self-optimizing, self-protecting and self-healing (Chi et al. 2023). Achieving such a high degree of network automation requires a fusion of knowledge from both physical models and network big data. Fortunately, knowledge graphs (KGs), a powerful tool to integrate knowledge and data, offer a promising solution for network automation.\nWhile efforts have been made to establish wireless knowledge graphs (WDKGs) (Huang et al. 2024), the existing ones are constructed manually, which is a labor-intensive process with no guarantee of accuracy. The dynamic and heterogeneous nature of wireless communication networks further exacerbates these challenges. The primary hurdle lies in the unprecedentedly enormous and ever-expanding scale of wireless networks. The resultant WDKG includes a vast array of fields and relations, the number of which are both on the order of thousands. Moreover, heterogeneity abounds in the various attributes of nodes (e.g., block error rate from the physical layer and average throughput from the MAC layer) as listed in Appendix, alongside multiple types of edges (e.g., causal/explicit/implicit relations). Such extensive heterogeneity complicates manual differentiation and often prone to errors. Additionally, the edges of WDKGs keeps evolving with scene variation (e.g., the mobility of transmitter and receiver), which necessitates near real-time tracking, analysis, and updating. Given these complexities, the development of an automated technique for constructing and refining WDKGs becomes imperative.\nGraph structure learning (GSL) methods (Chen, Wu, and Zaki 2020; Franceschi et al. 2019) enable automatic topology construction but traditionally rely on labels for supervision, resulting in biased structures due to the neglect of unlabeled nodes or edges, which limits scalability. To address this issue, self-supervision GSL paradigms have emerged, which leverages supervision signals from contrastive (Liu et al. 2022) or generative learning (Fatemi, El Asri, and Kazemi 2021). Nevertheless, these approaches are primarily tailored for static homogeneous graphs such as Cora, CiteSeer and PubMed, presenting challenges for structure learning of dynamic and heterogeneous WDKGs. Motivated by these challenges, we propose a novel GSL paradigm that integrates attention mechanisms into self-supervised GSL. To capture heterogeneous information and evolutionary patterns between consecutive snapshots, we employ a hierarchical attention model and temporal attention model. Our contributions are summarized as follows.\n(1) We pioneer GSL on WDKGs. Specifically, we slice the wireless networks based on edge properties and segment-"}, {"title": "Related Work", "content": "Recent years has witnessed a growing interest in learning graph structures for graph neural networks (GNNs) by modeling the adjacency matrix with learnable parameters and optimizing them alongside GNNs for downstream tasks. Approaches to parameterize the adjacency matrix can be broadly categorized into three types. The first type is model-based (Wang et al. 2021; Franceschi et al. 2019), where the discrete nature of graph structures is taken into account by modeling them as probabilistic models such as Bernoulli and stochastic block models. The second type is based on the similarity matrix (Chen, Wu, and Zaki 2020; Yu et al. 2021), where node similarities are evaluated using various metric functions such as cosine similarity and dot product. The third type treats each entry of the adjacency matrix as a directly learnable parameter (Jin et al. 2020). However, these GSL methods heavily hinge on labeled data for supervision, which can yield biased structures as the learning process prioritizes labeled nodes and edges."}, {"title": "Self-supervision Learning", "content": "To extend the applicability of GSL to semi-supervised and unsupervised contexts, self-supervision has emerged. Self-supervision falls into two main categories: generative methods and contrastive methods. The former concentrates on minimizing the reconstruction error, typically achieved through autoencoder (Zhu, Jiao, and Tse 2020; Huang et al. 2022), which aim to preserve essential information of the original data at a pixel-level. Contrastive methods, taking a different approach, aim to train models capable of effectively distinguishing different inputs in the feature space. For instance, Liu et al. (2022) employ self-supervision via mutiview graph contrastive learning, where the mutual information between the anchor view and the learned view is maximized. In comparison to reconstructing the original data, the latter approach is more tractable and scalable."}, {"title": "Dynamic Heterogeneous Graphs Learning", "content": "The real-world graphs usually exhibit dynamic and heterogeneous characteristics. In general, the dynamic graphs can be modeled as snapshot sequences and timestamp graphs. Since the timestamp model can be transformed into a snapshot model with an appropriate granularity, research methods based on the snapshot model are more adaptable and will be the focus of our discussion. Recently there have been a multitude of researches on learning representations of dynamic heterogeneous graphs. There are two main types of approaches. The first type is the incremental method, which leverages the embedding of the last snapshot to learn the current embedding (Wang et al. 2022). This method is computationally efficient but suffers from error accumulation and can only capture short-term temporal information. The second type is the retrained method, which learns embeddings for each snapshot and designs neural networks to capture temporal information (Yang et al. 2020). This approach can capture long-term temporal information but becomes computationally intricate as the number of timesteps increases. To address the computational challenges, self-attention has emerged to selectively learn the most relevant historical information and disregard unnecessary information (Sankar et al. 2020). These methods have been verified efficient in the embedding learning for dynamic heterogeneous graphs, but few researches have focused on the task of structure learning in this context."}, {"title": "Problem Definition", "content": "Before the specific statement of our method, we first make a definition of WDKG. As mentioned before, the methods for learning dynamic heterogeneous graphs are divided into two main streams. In this paper, the snapshots method is adopted to fit the concept of coherence time in communication network. The coherence time $T_c$ is introduced for dividing dynamic graphs, during which the channel can be reasonably viewed as time-invariant. As a result, the dynamic heterogeneous WDKG can be viewed as a series of static heterogeneous snapshots, denoted as $G = \\{G_{t,T_c}|t = 1,2,...,T\\}$, where T is the number of snapshots. Each snapshot $G_t = (V, E_t) = (X_t, A_t)$ ($T_c$ is omitted for convenience, so as in the following of this paper) is a static heterogeneous graph where V is a shared node set and $n = |V|$ represents the number of nodes, $E_t$ represents edge set and $m = |E_t|$ is the number of edges at time step t, $X_t \\in \\mathbb{R}^{n \\times d}$ is the node feature matrix at time t (the i-th row $x_i^t$ is the feature vector of node $v_i$ at time step t), $A_t \\in [0,1]^{n \\times n}$ is the adjacency matrix (where $a_{ij}^t$ is the weight of the edge from $v_i$ to $v_j$ at time step t). Considering the heterogeneity of each snapshot, $A_t = \\{A_t^1, ..., A_t^s\\}$, where s is the number of edge categories. Given a dynamic heterogeneous graph G, our target is to refine the graph structure based on the existing graph structure and feature matrix."}, {"title": "Methodology", "content": "In this section, the proposed framework of DMGSL will be explained in detail, the framework is depicted in Fig. 1."}, {"title": "Hierarchical attention module", "content": "Given the heterogeneous nature of the wireless network, it is sliced in accordance with various relations (or edges). In terms of the dataset we use in this paper, three distinct sliced sub-networks are acquired. Then a hierarchical attention module is devised to independently learn each slice and subsequently merge them using an attention mechanism, which facilitates a nuanced understanding of the discrepancy in relations, allowing for fine-grained learning. The diagram of hierarchical attention module (HAT) is shown in Fig. 2.\nAs defined before, the dynamic heterogeneous WDKG is denoted as a series of static snapshots $G = \\{G_1, G_2, ..., G_T\\}$. As to the static graph at t-th snapshot $G_t$, the adjacency matrix $A_t$ is divided into three sub-adjacency matrices according to different kinds of relations (or edges), i.e., $A_t^1, A_t^2, A_t^3$. Combined with $X_t$ (the feature matrix at time t), we get $E_{1,a}^t = (X_t, A_t^1), E_{2,a}^t = (X_t, A_t^2), E_{3,a}^t = (X_t, A_t^3)$ as the initial matrices of anchor graphs.\nTo acquire the initial matrices of learned graphs, a full graph parameterization (FPG) learner is considered to generate sketchy adjacency matrix of WDKG from feature matrix at time t. The FGP learner parameterizes each element of the adjacency matrix independently, the learned adjacency matrix can be presented as $\\hat{A_t}$. Then $E_{1,l}^t = (X_t, \\hat{A_t}), E_{2,l}^t = (X_t, \\hat{A_t}), E_{3,l}^t = (X_t, \\hat{A_t})$ are obtained as the initial matrices of learned graphs.\nThe categories of relations between fields are different, including causal, explicit and implicit relations. Therefore, we slice the wireless network into three sub-networks and learn the information of them separately. The common method is to average the learned information from three slices, but in fact these slices are of different importance to structure learning. For instance, the direct influence of the causal relation (or edge) on the structure is higher than the indirect influence of the implicit relation (or edge). Therefore, we introduce a hierarchical attention model to learn the importance of different edges to GSL of wireless network, so as to integrate the information of the three network slices more explainably.\nSpecifically, the initial matrices of anchor graphs and learned graphs (i.e. $E_{1,a}^t, E_{2,a}^t, E_{3,a}^t, E_{1,l}^t, E_{2,l}^t, E_{3,l}^t$) are firstly entered into a nonlinear transformation function so that they map to the same feature space by $\\sigma(W \\cdot e_{s,a}^t + b)$, $\\sigma(W \\cdot e_{s,l}^t + b)$, where $e_{s,a}^t, e_{s,l}^t \\in \\mathbb{R}^{d + n}$ are the transpose of i-th row of initial matrices $E_{s,a}^t, E_{s,l}^t \\in \\mathbb{R}^{n \\times (d + n)}$ (i is omitted for convenience) which are two matrix representations of WDKG, $\\sigma$ denotes the activation function, W and b represent the weight matrix and bias vector, parameters of which are shared by anchor and learned graphs in the same edge-level (e.g., $E_{1,a}^t$ and $E_{1,l}^t$ share the parameters of W and b). The mapped graphs are denoted as $h_{1,a}^t, h_{2,a}^t, h_{3,a}^t$ and $h_{1,l}^t, h_{2,l}^t, h_{3,l}^t$. Then, the similarities between the mapped graphs and the edge-level attention parameterized vector are calculated to evaluate the importance of different slices to structure learning. Sequently, the normalized weight factors of the anchor graphs and learned graphs with edge type s at time t are figured up, which are denoted as $\\alpha_{s,a}^t, \\alpha_{s,l}^t$. The process can be defined as:\n$\\alpha_{s,a}^t = \\frac{exp \\left(q^T \\cdot \\sigma\\left(W \\cdot e_{s,a}^t + b\\right)\\right)}{\\sum_l exp \\left(q^T \\cdot \\sigma\\left(W \\cdot e_{l,a}^t + b\\right)\\right)}, s \\in [1, m]$\n$\\alpha_{s,l}^t = \\frac{exp \\left(q^T \\cdot \\sigma\\left(W \\cdot e_{s,l}^t + b\\right)\\right)}{\\sum_l exp \\left(q^T \\cdot \\sigma\\left(W \\cdot e_{l,l}^t + b\\right)\\right)}, s \\in [1, m]$   (1)\nLastly, the graphs with single communication relation can be merged with the normalized weight factors. The transpose of i-th row in the merged matrices $E_a^t, E_l^t \\in \\mathbb{R}^{n \\times (d + n)}$ are expressed as $e_a^t$ and $e_l^t$, which can be viewed as two kinds of embeddings of node i in the wireless communication network at time t. The merge can be formulized as:\n$e_a^t = \\sum_{s=1}^m \\alpha_{s,a}^t \\cdot e_{s,a}^t$\n$e_l^t = \\sum_{s=1}^m \\alpha_{s,l}^t \\cdot e_{s,l}^t$  (2)"}, {"title": "Temporal Attention Module", "content": "The wireless communication system encompasses a highly complex channel, including a series of channels caused by obstacles such as reflection, diffraction, scattering, coherence and shadowing. The mobility of transmitter or receiver renders time-invariance elusive. To address this, the notion of coherence time is introduced, representing the duration in which the channel can reasonably be viewed as time-invariant. Assuming the transmitter is fixed, the coherence time can be calculated based on the radio frequency and the radial velocity of receiver. Consequently, the dynamic WDKG is partitioned into several static snapshots accordingly. On this basis, the temporal attention module (TAT) is introduced to effectively integrate the abundant temporal features inherent in the WDKG.\nIn this paper, to model the dynamic information of WDKG, we employ a basic variant of RNN called long short-term memory (LSTM) which is qualified for conveying information during a long time. To cater to the limited memory units, LSTM preserves useful information that needs long-term memory, and forgets superfluous information. Moreover, a mechanism that can dynamically adjust memory is introduced to update the valuable information that needs to be remembered in time. Specifically, the LSTM model contains state vector $s_t$, forgetting vector $f_t$, memory vector $c_t$, input vector $i_t$, output vector $o_t$. The matrices output from hierarchical attention model are represented as $\\{E_1^t, E_2^t,..., E_T^t \\} \\in \\mathbb{R}^{n \\times D}$, $\\{\\hat{E_1^t}, \\hat{E_2^t},..., \\hat{E_T^t}\\} \\in \\mathbb{R}^{n \\times D}$, the transpose of their i-th row (denoted as $e^t, \\hat{e^t} \\in \\mathbb{R}^D$, where i is omitted) are entered into LSTM. An LSTM unit can be represented by the following formulas (omit the subscripts of anchor and learned graphs):\n$i_t = \\sigma \\left(W_i \\cdot [e^t || s_{t-1}] + b_i\\right)$,\n$f_t = \\sigma \\left(W_f \\cdot [e^t || s_{t-1}] + b_f\\right)$,\n$o_t = \\sigma \\left(W_o \\cdot [e^t || s_{t-1}] + b_o\\right)$,\n$c_t = tanh \\left(W_c \\cdot [e^t || s_{t-1}] + b_c\\right)$,\n$c_t = f_t \\odot c_{t-1} + i_t \\odot c_t, s_t = o_t \\odot tanh (c_t)$,   (3)\nwhere t \u2208 {1, 2, \u00b7\u00b7\u00b7 ,T}, $i_t, f_t, o_t, c_t \\in \\mathbb{R}^F$ (F is the output dimension of LSTM), $W_i, W_f, W_o, W_c \\in \\mathbb{R}^{F \\times 2D}$ and $b_i, b_f, b_o, b_c \\in \\mathbb{R}^F$ are trainable parameters, $\\sigma$ is the activation function, || is the concatenation operation, $\\odot$ is the Hadamard product. $s^0, c^0 \\in \\mathbb{R}^D$ need to be initialized, and in this article they are initialized to an all-one vector 1. The outputs of LSTM (i.e., the graph structure matrix at t time step that has integrated historical information) can be defined as $S_t = [\\left(s_1^t\\right)^T, \\left(s_2^t\\right)^T, ..., \\left(s_n^t\\right)^T] \\in \\mathbb{R}^{n \\times F}$.\nTo fuse the acquired wireless network topology across different snapshots, we employ a temporal attention model. This model calculates contribution factors to determine the impact of various snapshots on the overall structure learning process. For example, snapshots with lower doppler effect (i.e., a lower radial velocity between transmitter and receiver) may be more significant in the learning of network topology compared to other snapshots. These contribution factors enable us to effectively weigh the influence of each snapshot on the overall structure learning process. The i-th row of the graph structure at all times are extracted to constitute a fresh matrix $S_i = [\\left(s_i^1\\right)^T, \\left(s_i^2\\right)^T, ..., \\left(s_i^T\\right)]$, $s_i^1, s_i^2, ..., s_i^T \\in \\mathbb{R}$, which is the input. The popular scaling dot multiplication attention mechanism in natural language processing is adopted. The input $S_i \\in \\mathbb{R}^{T \\times F}$ is multiplied with three parameter matrices $W_Q, W_K, W_V \\in \\mathbb{R}^{F \\times F'}$, mapping into different feature spaces, represented as Q, K, $V \\in \\mathbb{R}^{T \\times F'}$, which is viewed as the linear transformation of the input. Using Q, K, V rather than $S_i \\in \\mathbb{R}^{T \\times F}$ in the calculation enhances the fitting ability of the model effectively. Then, multiply Q and $K^T$ to generate the similarity matrix. What needs to be emphasized is that when the dimension of K increases, the variance of $Q \\cdot K^T$ will become larger. In order to reduce the variance, each element of the similarity matrix is divided by $\\sqrt{F'}$ (F' is the dimension of K. The normalized similarity matrix can be regarded as a weight matrix. Finally, multiply the weight matrix with V and calculate the weighted sum, which is the output of the network. The above process can be formulized as follows:\n$Z_i = T_i V_i = softmax \\left(\\frac{Q \\cdot K^T}{\\sqrt{F'}} + M\\right) V$,   (4)\nwhere $\\Gamma_i \\in \\mathbb{R}^{T \\times T}$ is the weight matrix, $M \\in \\mathbb{R}^{T \\times T}$ is the masking matrix. If $M_{\\eta u} = - \\infty$, there is no effect from time u to \u03b7, and the corresponding element in the weight matrix is 0. If u is earlier than \u03b7, then $M_{\\eta u} = 0$; Otherwise, $M_{\\eta u} = - \\infty$. The output of the temporal attention model is defined as $Z_i = [\\left(z_i^1\\right)^T, \\left(z_i^2\\right)^T, ..., \\left(z_i^T\\right)]$, $z_i^1, z_i^2, ..., z_i^T \\in \\mathbb{R}^{F'}$, where $F'$ is the output dimension.\nIn order to enhance the performance, the extended multi-head attention mechanism is used. To be concrete, we define multiple groups (i.e. \u03ba groups) of $W_Q, W_K, W_V$, each group is calculated separately to generate different Q, K, V, and learn various parameters, the obtained multiple outputs are concatenated to get $Z_i = Concat(Z_i^1, Z_i^2, ..., Z_i^{\\kappa})$. In this paper, we take all the $z_i^t$ in $Z_i$ to constituent matrix $E = [\\left(z_1^t\\right)^T, \\left(z_2^t\\right)^T, ..., \\left(z_n^t\\right)^T] \\in \\mathbb{R}^{n \\times F'}$ as output. The anchor graph and learned graph formed from temporal attention module can be denoted as $h_a^t$ and $h_l^t$ which are regard as two graph representations of the wireless network."}, {"title": "Contrastive learning module", "content": "Data augmentation is significant method to mitigate overfitting and explore richer information. We use two common data augmentation schemes, edge dropping and feature masking, the augmented matrices $\\tilde{E_a}$ and $\\tilde{E_l}$ are obtained, corresponding to the two augmented views $\\tilde{h_a}$ and $\\tilde{h_l}$.\nNext, the augmented graphs are encoded and compressed, transforming the high-dimension into lower dimension. Graph convolutional network (GCN) is exploited as the encoder, the encoding process can be expressed as\n$H_a = GCN_{\\Theta} (\\tilde{E_a}), H_l = GCN_{\\Theta} (\\tilde{E_l}), s \\in [1, m]$,   (5)\nwhere \u0398 is the parameter of GCN encoder, $H_a, H_l \\in \\mathbb{R}^{n \\times d_1}$ ($d_1$ represents the output dimension of encoder) are the encoded structure matrices.\nFurthermore, to calculate the contrast loss function, we reflect the views to another latent space with the assistance of multiple layer projection (MLP), which is formalize as:\n$\\Upsilon_a = g_{\\varphi} (H_a), \\Upsilon_l = g_{\\varphi} (H_l)$   (6)\nwhere \u03c6 is the parameter of the projector $g_{\\varphi} (\\cdot)$, $\\Upsilon_a, \\Upsilon_l \\in \\mathbb{R}^{n \\times d_2}$ ($d_2$ is the output dimension of projector) are mapped graph matrices of the anchor and learned graphs. Then, a contrast learning loss function (van den Oord, Li, and Vinyals 2019) is used to maximize the similarity between each row vector of the two graph structures:\n$\\mathcal{L} = \\frac{1}{2n} \\sum_{i=1}^n[l(\\mathbf{y}_{a,i}, \\mathbf{y}_{l,i}) + l(\\mathbf{y}_{l,i}, \\mathbf{y}_{a,i})]$,   (7)\n$l(\\mathbf{y}_{a,i}, \\mathbf{y}_{l,i}) = -log \\frac{e^{sim(\\mathbf{y}_{a,i}, \\mathbf{y}_{l,i}) / \\rho}}{\\sum_{j=1}^n e^{sim(\\mathbf{y}_{a,i}, \\mathbf{y}_{l,j}) / \\rho}}$,   (8)\nwhere sim (\u00b7, \u00b7) is the cosine similarity function, \u03c1 is the temperature parameter. The loss function is minimized during the training process, where involves iteratively updating the model's parameters using gradient descent."}, {"title": "Experiments", "content": "In this section, we conduct a series of experiments to examine the effectiveness of the proposed framework for fine-grained graph representation learning from the following aspects. First, we compare the learned adjacency matrix with expert knowledge and other baselines. Second, we exploit the learned structure to perform node classification tasks and compare the results with baselines. Finally, we investigate the influence of key hyperparameters on the performance of our proposed method, aiming to provide guidelines for parameter tuning. The detailed setup of experiments can be found in Appendix A."}, {"title": "Baseline", "content": "We compare the proposed method with state-of-the-art methods of structure learning, including Sublime (Liu et al. 2022), GEN (Wang et al. 2021), IDGL (Chen, Wu, and Zaki 2020), SLAPS (Fatemi, El Asri, and Kazemi 2021), and their variant, IDGL-Anch and SLAPS-2s."}, {"title": "Dataset", "content": "The dataset is collected by an automated guided vehicle which travels in the park at a speed of 10 km/h for 500 m along a planned route, receiving signals with a frequency of 3300-3800 Hz, recording 40 data per second. We processed and organized the data, the details of dataset we use are provided in Appendix B."}, {"title": "Comparisons", "content": "In the proposed framework, each element of the output adjacency matrix represents the probability of an edge existing between two nodes; the higher the value, the closer the correlation between two nodes. We depict the heatmap of the original adjacency matrix and the adjacency matrices"}, {"title": "Impact of Key Hyperparameter", "content": "Since the data fields (or nodes) and relations (or edges) of wireless communication data is less than that of general public datasets such as Cora, Citeseer, it is significant to prevent overfitting in the learning process. To this end, we perform data augmentation by feature masking before encoding and mapping. The feature masking rate of the anchor graph"}, {"title": "Ablation Study", "content": "In order to verify the validity of the hierarchical attention model and temporal attention model introduced in our proposed method, we conduct an ablation study. Node classification performance is employed to evaluate if each component positively contributes to the final learned structure, as shown in Table 1 and Fig. 5. First of all, considering only the hierarchical attention model (DMGSL (w/o TAT)), the classification accuracy is higher than other baselines and comparable to Sublime, but the comprehensive performance considering precision and F1-score is better than Sublime. Then, considering only the temporal attention model (DMGSL (w/o HAT)), the performance is significantly higher than the other baselines in the case of each indicator. Finally, the hierarchical attention model and temporal attention model are introduced together (DMGSL), it is obvious that the classification performance is further improved. Meanwhile, it can be seen in Fig. 5 that when there is no attention models, the performance is relatively worst compared to other configurations. Therefore, both the hierarchical attention model and temporal attention model help enhance classification performance.\nSo far, we have demonstrated that DMGSL performs well on node classification. But whether it means good performance on structure learning? It is true to some degree. Although the process of node classification seems that only node attributes are paid attention, it is important to note that the impact of edges is also considered in the learning of the network structure and node embedding. Therefore, to a certain extent, the high performance achieved in the node classification task indicates that the learned network structure is meaningful. It not only incorporates the attributes of wireless network nodes but also captures the relations between nodes through edges, indeed supplementing and optimizing expert knowledge. By successfully constructing the network structure, our method provides a potential foundation for network automation."}, {"title": "Conclusion", "content": "The avenue of AI is fast evolving and will impact the telecom industry in the years to come. This study selects network automation as a cutting-in point and considers the problem of automatic graph representation for the next-generation mobile networks. Based on edge properties, the presented framework incorporates coherence time to partition dynamic networks into static snapshots. Hierarchical attention independently learns and merges slices, facilitates nuanced understanding of relationships, while the temporal attention model integrates temporal features for enhanced learning. Furthermore, the method employs LSTM and multi-head attention mechanisms to capture temporal dynamics. Data augmentation techniques such as edge dropping and feature masking are utilized to mitigate overfitting and extract richer information from data. Overall, this research marks significant progress in autonomous learning and refinement of network topology for wireless communications, which paves the way for advancements in network automation."}, {"title": "A. Setup of experiments", "content": "The pre-processed network data, i.e., feature matrix, is input into the proposed framework together with the adjacency matrix of expert knowledge for training, which yields the output of the learned adjacency matrix and node embedding. To intuitively reflect the reliability of the learned structure, we use the learned node embedding for the node classification task. The training, validation, and test sets are divided by hierarchical sampling at a ratio of 6:2:2. To comprehensively evaluate the performance of node classification, we calculate four commonly used metrics: accuracy, precision, recall, and F1-score. The proposed method was implemented using an NVIDIA GeForce RTX 3060 Laptop GPU with 13.8 GB of memory. The implementation was based on PyTorch 1.9.0, utilizing the SGD and Adam optimizers."}, {"title": "B. Dataset", "content": "The final dataset can be summarized as follows, with details shown in Table 2 and Table 3.\n\u2022 The adjacency matrix of the uplink throughput knowledge graph, generated by expert knowledge, includes 82 nodes and 133 relations, with three types of relations: causal relation (1), implicit relation (2), and explicit relation (3).\n\u2022 The measured uplink throughput data (with 82 data fields, including throughput capacity, block error rate, frame structure value, etc.) are collected with a window of 15 minutes and 35 minutes, resulting in 38,250 and 120,418 pieces of data, respectively. The data dimensions are 38,250 \u00d7 82 and 120,418 \u00d7 82."}, {"title": "C. Influence of key hyperparameters", "content": "To prevent the learned graph from over-fitting the anchor graph (i.e.", "0,1": "is a key parameter for adjusting the degree of updating: the closer \u03c4 is to 1, the larger the proportion of the anchor graph in update process, meaning the degree of updating is lower. We varied the size of \u03c4 and conducted several experiments, plotting the curves of contrastive loss versus training epochs with different \u03c4 values, as shown in Fig. 6(a). We readily observe that, when \u03c4 is 0.9, the contrastive loss shows a rapid downward trend initially but continues to decrease without a convergence trend as epochs increase. This is due to the rapid variation of the anchor graph, which causes unstable learning. When \u03c4 is greater than or equal to 0.99, the curves gradually converge as epochs increase. The curve for \u03c4 = 0.999 converges the slowest; despite the curve for \u03c4 = 0.99 converges slowly at the beginning, it achieves a stable value faster than \u03c4 = 0.9999 and \u03c4 = 0.99999. Therefore, we conclude that when \u03c4 = 0.99, the model converges to a fair performance and is scalable.\nThe learning rate, denoted as lr, is another crucial parameter in deep learning, as it affects both the convergence speed and the effectiveness of learning. In order to find the optimal lr, we set lr to [10-1, 10-2, 10-3, 10-4, 10-5"}]}