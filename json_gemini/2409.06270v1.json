{"title": "Towards Robust Uncertainty-Aware Incomplete Multi-View Classification", "authors": ["Mulin Chen", "Haojian Huang", "Qiang Li"], "abstract": "Handling incomplete data in multi-view classification is challenging, especially when traditional imputation methods introduce biases that compromise uncertainty estimation. Existing Evidential Deep Learning (EDL) based approaches attempt to address these issues, but they often struggle with conflicting evidence due to the limitations of the Dempster-Shafer combination rule, leading to unreliable decisions. To address these challenges, we propose the Alternating Progressive Learning Network (APLN), specifically designed to enhance EDL-based methods in incomplete MVC scenarios. Our approach mitigates bias from corrupted observed data by first applying coarse imputation, followed by mapping the data to a latent space. In this latent space, we progressively learn an evidence distribution aligned with the target domain, incorporating uncertainty considerations through EDL. Additionally, we introduce a conflict-aware Dempster-Shafer combination rule (DSCR) to better handle conflicting evidence. By sampling from the learned distribution, we optimize the latent representations of missing views, reducing bias and enhancing decision-making robustness. Extensive experiments demonstrate that APLN, combined with DSCR, significantly outperforms traditional methods, particularly in environments characterized by high uncertainty and conflicting evidence, establishing it as a promising solution for incomplete multi-view classification.", "sections": [{"title": "Introduction", "content": "In the field of multi-view classification, handling incomplete data has always been a significant challenge. For the task of incomplete multi-view classification (IMVC), existing studies can be broadly divided into two primary categories. The first category comprises methods that perform classification using only the available views without imputing the missing data. Although these methods (Lee and van der Schaar 2021; Zhang et al. 2019) avoid the complexity of data reconstruction, they often struggle when faced with high missing rates, as they fail to fully exploit the correlations between views. Consequently, these approaches are typically less effective and robust in scenarios where a significant portion of the data is missing. The second category of methods (Wu and Goodman 2018; Mattei and Frellsen 2019; Thung, Yap, and Shen 2018; Cai et al. 2018) seeks to reconstruct the missing data using deep learning techniques,"}, {"title": "Related Work", "content": ""}, {"title": "Incomplete Multi-View Learning", "content": "Incomplete multi-view learning, a central challenge in multi-view classification, focuses on effectively managing missing views. Existing methods fall into two categories: 1) approaches that work directly on available views, learning a common latent representation without reconstructing missing data (Zong et al. 2020; Li, Jiang, and Zhou 2014; Zhao, Liu, and Fu 2016; Zhang et al. 2019; Lee and van der Schaar 2021); and 2) generative methods that impute missing views before downstream tasks, using techniques like VAEs and GANs (Wu and Goodman 2018; Hwang et al. 2021; Mattei and Frellsen 2019; Zhang et al. 2022; Xu et al. 2019; Wang et al. 2018). However, both approaches have limitations. The former often suffers from performance degradation due to limited view correlations, while the latter tends to rely on deterministic imputation (Rai et al. 2010; Gao, Peng, and Jian 2016; Lin et al. 2021), failing to capture the inherent uncertainty of missing data, which can result in unstable outcomes, particularly in high-dimensional scenarios. To address these challenges, we propose a multi-stage interpolation approach that refines imputation by first performing coarse interpolation in the observed space, followed by further refinement in the latent space. This allows for better uncertainty assessment, enabling more robust inference. Moreover, while EDL (Sensoy, Kaplan, and Kandemir 2018) has been effective in modeling uncertainty, its application to incomplete multi-view classification (IMVC) is hindered by sensitivity to evidence conflicts in the observed domain. To mitigate this, we introduce a conflict-aware DSCR with a new consistency loss, ensuring more coherent evidence fusion and improved performance in IMVC tasks."}, {"title": "Uncertianty-Based Deep Learning", "content": "Numerous studies have focused on developing models capable of estimating uncertainty to enhance reliability and trustworthiness in decision-making (Zhang et al. 2021; Xiao et al. 2021; Li 2022; Izmailov et al. 2021; Gal and Ghahramani 2016; Amini et al. 2020; Sensoy, Kaplan, and Kandemir 2018; Liu, Huang, and Letchmunan 2023; Liu et al. 2023; Chen et al. 2024; Ma et al. 2024). Among these, EDL (Sensoy, Kaplan, and Kandemir 2018) has gained attention for its ability to model \"second-order probabilities\" over logits using Dempster-Shafer Theory (Shafer 1992) and Subjective Logic (Jsang 2018). This approach captures uncertainty conveniently and accurately across various domains (Qin et al. 2022; Shao, Dou, and Pan 2024; Holmquist, Klas\u00e9n, and Felsberg 2023; Huang et al. 2024c,b,a). However, when applying EDL-based methods to IMVC (e.g. (Xie et al. 2023a)), existing approaches often overlook the uncertainty introduced by the corrupted nature of the observed domain. Relying on samples drawn from the observed distribution for imputation can inadvertently introduce conflicts. This is a notable weakness of vanilla EDL, as the DSCR is highly sensitive to evidence conflicts (Huang et al. 2023). Even a single piece of conflicting evidence can lead to anomalous fusion results, ultimately degrading the performance of downstream IMVC tasks. To this end, we propose a novel conflict-aware DSCR, incorporating a new consistency loss to achieve more coherent evidence fusion."}, {"title": "Method", "content": "Our method focuses on leveraging correlations among multiple views while addressing uncertainty and conflicts due to missing data, ultimately improving multi-view fusion classification. We first define IMVC and introduce the Uncertainty Mitigation through Alternating Evidence Learning (UMAE) model architecture, then discuss the training details, and finally, explore the reasons behind the model's effectiveness."}, {"title": "Background", "content": "Given $N$ training inputs ${X_n}_{n=1}^N$ with $V$ views, i.e., $X = {x_n^v}_{v=1}^V$, and the corresponding class labels ${y_n}_{n=1}^N$ multi-view classification aims to construct a mapping between input and label by exploiting the complementary multi-view data. In this paper, we focus on the IMVC task defined in Definition 1"}, {"title": "Definition 1. Incomplete Multi-View Classification.", "content": "A complete multi-view sample is composed of $V$ views $X = {x^v}_{v=1}^V$ and the corresponding class label $y$. An incomplete multi-view observation $\\tilde{X}$ is a subset of the complete multi-view observation (i.e., $\\tilde{X} \\subseteq X$) with arbitrary possible $V$ views, where $1 < V < V$. Given an incomplete multi-view training dataset ${\\tilde{X}_n, y_n}_{n=1}^N$ with $N$ samples, IMVC aims to learn a mapping between the incomplete multi-view observation $\\tilde{X}$ and the corresponding class label $y$."}, {"title": "UMAE Model Architecture", "content": "Imputation of Missing Views. We first perform random imputation on the missing view data, then project each view $x^v$ into a unified feature space using a set of linear transformations ${f_v}_{v=1}^V : {x^v} \\rightarrow {z^v}$. Within this feature space, random masks $m_n = {m_n^v}_{v=1}^V$ are generated to simulate missing features, with the missing parts of $z_n$ set to 0 to obtain the masked features. By concatenating the masked features with the mask $m_n$, the input $\\tilde{z}_n$ for the VAE model is obtained.\nThe VAE model takes $\\tilde{z}_n$ as input and outputs $\\hat{z}$, with the ground truth features denoted as $z$. Training the VAE involves minimizing the negative Evidence Lower Bound (ELBO), represented by the loss function $\\mathcal{L}_{ELBO}$. The ELBO consists of two key components: a reconstruction error term $\\mathbb{E}_{q(\\hat{z}|\\tilde{z}_n)}[\\log p(z|\\hat{z})]$, which measures the log-likelihood of the true features $z$ given the model's latent variables $\\hat{z}$, and a KL divergence term $KL(q(\\hat{z}|\\tilde{z}_n) || p(z))$, which quantifies the divergence between the approximate posterior $q(\\hat{z}|\\tilde{z}_n)$ and the prior $p(z)$. The ELBO is thus defined as:\n$\\mathcal{L}_{ELBO} = \\mathbb{E}_{q(\\hat{z}|\\tilde{z}_n)}[\\log p(z|\\hat{z})] - KL (q(\\hat{z}|\\tilde{z}_n) || p(z)),$ (1)\nwhere $p(z)$ represents the prior distribution, often a standard normal distribution $\\mathcal{N}(0, I)$, while $q(\\hat{z}|\\tilde{z}_n)$ denotes the approximate posterior distribution parameterized by a neural network. $p(z|\\hat{z})$ is the distribution used to reconstruct the observed data from the latent space.\nTo enhance robustness, we sample multiple imputations from the learned distribution, offering a comprehensive approach to handling uncertainty in missing views. By projecting views into a unified feature space, the method effectively learns a reliable model that leverages feature correlations."}, {"title": "Multi-View Opinion Fusion.", "content": "Formally, let the VAE multiple sampling outputs for view $v$ be denoted as $\\hat{z}^v$, where $\\hat{z}^v$ represents the reconstructed feature for the $v$-th view. We then create the reconstructed feature $z^{rc}$ for each sample $n$ by combining the observed and reconstructed features. For each sample $n$, the reconstructed feature is computed as:\n$z^{rc} = m^v \\cdot z + (1 - m^v) \\cdot \\hat{z}^v$ (2)\nThese features are then projected into the evidence space using a linear transformation ${f^v}_{v=1}^V$\n$\\epsilon^v = f^v(\\hat{z}^{rc})$ (3)\nwhere $f^v$ is a linear layer that maps the reconstructed features into the evidence space.\nFor $K$ classification problems, the multinomial opinion over a specific view of an instance $(x^v)^i$ is represented as a triplet $w = (b, u, a)$. Here, $b = (b_1, ..., b_K)^T$ denotes the belief masses assigned to possible values based on evidence support, $u$ represents the uncertainty mass reflecting evidence ambiguity, and $a = (a_1, ..., a_K)$ is the prior probability distribution for each class. According to subjective logic, both $b$ and $u$ must be non-negative and their sum must equal one:\n$\\sum_{k=1}^K b_k + u = 1, \\forall k \\in [1,..., K]$ (4)\nwhere $b_k \\geq 0$ and $u \\geq 0$. The projected probability distribution of multinomial opinions is given by:\n$p_k = b_k + a_k u, \\forall k \\in [1, ..., K]$ (5)\nOn opinion aggregation, let $w^A = (b^A, u^A, a^A)$ and $w^B = (b^B, u^B, a^B)$ be the opinions of views A and B over the same instance, respectively. The conflictive aggregated opinion $w^{A+B}$ is calculated as follows:\n$\\mathcal{E}_{A\\oplus B} = \\mathcal{E}_A \\oplus \\mathcal{E}_B = (b^{A+B}, u^{A\\oplus B}, a^{A+B}),$ (6)\n$b_k^{A \\oplus B} = \\frac{b_k^A u^B + b_k^B u^A}{u^A + u^B}$ (7)\n$u^{A\\oplus B} = \\frac{2u^A u^B}{u^A + u^B}$ (8)\nThe opinion $w^{A+B}$ is equivalent to averaging the view-specific evidences:\n$\\epsilon^{A \\oplus B} = \\frac{1}{2} (\\epsilon^A + \\epsilon^B)$. (9)\nrepresents the combination of the dependent opinions of A and B. This combination is achieved by mapping the belief opinions to evidence opinions using a bijective mapping between multinomial opinions and the Dirichlet distribution. Essentially, the combination rule ensures that the quality of the new opinion is proportional to the combined one. In other words, when a highly uncertain opinion is combined, the uncertainty of the new opinion is larger than the original opinion.\nThen, we can fuse the final joint opinions $w$ from different views with the following rule:\n$w = w^1 \\oplus w^2 \\oplus ... \\oplus w^V$, (10)\nAccording to the above fusion rules, we can get the finalmulti-view joint opinion, and thus get the final probabilityof each class and the overall uncertainty."}, {"title": "Conflict-Aware Consistency Loss.", "content": "We introduce a new conflict degree measure that unifies probability distribution and uncertainty, ensuring a symmetric measure within the range [0, 1]. The new distributions are defined as $q^A = p(1 - u^A)$ and $q^B = p(1 - u^B)$. Using the Jensen-Shannon divergence:\n$Dis(q^A||q^B) = \\frac{1}{2} D_{KL}(q^A||M) + \\frac{1}{2} D_{KL}(q^B ||M)$ (11)\nwhere $M = \\frac{1}{2} (q^A + q^B)$. The KL divergence is given by $D_{KL}(q^A||M) = \\sum_{k=1}^K q_k^A \\log \\frac{q_k^A}{M_k}$ and $D_{KL}(q^B||M) = \\sum_{k=1}^K q_k^B \\log \\frac{q_k^B}{M_k}$. The new conflict degree measure is then:\n$\\mathcal{C}(w^A, w^B) = 1 - D_{JS}(q^A||q^B)$ (12)\nThus, the conflict loss for multi-view fusion is:\n$\\mathcal{L}_{con} = \\frac{1}{V-1} \\sum_{A=1}^V \\sum_{B \\neq A}^V \\mathcal{C}(w^A, w^B)$ (13)\nThe proposed conflictive degree measure improves upon the original by ensuring symmetry through the Jensen-Shannon divergence, which keeps the conflict assessment balanced regardless of input order. It operates within a clear range of [0, 1], with 0 representing maximum conflict and 1 indicating no conflict, thus providing an intuitive and interpretable scale. By unifying probability distributions and uncertainties into a single metric, the measure offers a more comprehensive understanding of conflict, directly reflecting the degree of disagreement between opinions in a way that is both holistic and easy to interpret."}, {"title": "Alternating Pro-Gressive Learning Network", "content": "To optimize the performance of our model, we developed an Alternating Progressive Learning Network, which consists of three phases: Feature Training Phase (UMAE-F), VAE-EDL Training Phase (UMAE-V), Joint Training Phase (UMAE-J). The model parameters $\\Theta$ consist of the following components: $\\theta_c$ for the feature extraction module, $\\theta_e$ for the EDL module, and $\\theta_v$ for the VAE module."}, {"title": "UMAE-F.", "content": "In this phase, we use noise to fill in the missing feature views and leverage labels for coarse alignment of the feature space, optimizing $\\theta_c$. This step accelerates the subsequent alignment between latent and target distribution. The loss function $\\mathcal{L}_{acc}$ is used and we firstly introduced $\\mathcal{L}_{ace}$ as below:\n$\\mathcal{L}_{ace} (a_n) = \\frac{1}{\\vert \\mathcal{B}(a_n)\\vert} \\sum_{j=1}^{\\vert \\mathcal{B}(a_n)\\vert} [-\\sum_{k=1}^K Y_{nj} \\log P_{nj}]$\n$= \\frac{1}{\\vert \\mathcal{B}(a_n)\\vert} \\sum_{j=1}^{\\vert \\mathcal{B}(a_n)\\vert} \\sum_{k=1}^K Y_{nj} (\\psi(S_n) - \\psi(A_{nj}))$, (14)\nwhere $\\psi(\\cdot)$ is the digamma function. Nevertheless, this loss function does not ensure lower evidence for incorrect labels. To address this, we introduce the Kullback-Leibler (KL) divergence:"}, {"title": null, "content": "$\\mathcal{L}_{KL}(a_n) = KL [D(p_n||a_n)||D(p_n||1)]$\n$= \\log [\\frac{\\Gamma(\\sum_{k=1}^K a_{nk})}{\\Gamma(K) \\prod_{k=1}^K \\Gamma(a_{nk})}] + \\sum_{k=1}^K (a_{nk} - 1) (\\psi(a_{nk}) - \\psi (\\sum_{j=1}^K a_{nj}))$ , (15)\nwhere $D(p_n||1)$ is the uniform Dirichlet distribution, $\\hat{a} = Y_n + (1-Y_n) a_n$ is the Dirichlet parameter after removal of the non-misleading evidence from predicted parameters $a_n$ for the n-th instance, and $\\Gamma(\\cdot)$ is the gamma function. Therefore, given the Dirichlet distribution with parameter $a_n$ for the n-th instance, the loss function is defined as:\n$\\mathcal{L}_{acc}(a_n) = \\mathcal{L}_{ace} (a_n) + \\lambda_t \\mathcal{L}_{KL}(a_n)$, (16)"}, {"title": "UMAE-V.", "content": "In this stage, we freeze the pre-trained ${f^v}_{v=1}^V$ and use EDL to reduce the bias in the latent space representations introduced by the observed domain. The loss function used here optimizes $\\theta_v$ and $\\theta_e$:\n$\\mathcal{L}_{edl} = \\mathcal{L}_{acc} + \\mathcal{L}_{con}$, (17)"}, {"title": "UMAE-J.", "content": "In UMAE-J, we unfreeze the feature layers and introduce a VAE reconstruction loss, enabling all model components to coordinate effectively for optimal performance. The combined loss optimizes $\\theta_c, \\theta_v, and \\theta_e$:\n$\\mathcal{L}_{J} = \\mathcal{L}_{edl} + \\mathcal{L}_{elbo}$, (18)\nwhere\n$\\mathcal{L}_{ELBO} = -\\mathbb{E}_{q(z|\\tilde{z}_n)}[\\log p(z|\\hat{z})] + KL (q(\\hat{z}|\\tilde{z}_n) || p(z))$ (19)"}, {"title": "Experimental Setup", "content": "Datasets. To validate the effectiveness of UMAE, experiments were conducted on six datasets. The NUS(Chua et al. 2009) dataset is a large-scale 3-view dataset with 30,000 samples and 31 categories, used for multi-label classification. The YaleB(Georghiades, Belhumeur, and Kriegman 2001) dataset contains 3 views with 10 categories, each with 65 facial images. The Handwritten(Perkins and Theiler 2003) dataset consists of 6 views covering 10 categories from digits \"0\" to \"9,\" with 200 samples per category. The ROSMAP(Wang et al. 2020a) dataset includes 3 views with two categories\u2014Alzheimer's disease (AD) patients and normal control (NC)\u2014with 182 and 169 samples, respectively. The BRCA(Wang et al. 2020b) dataset comprises 3 views for Breast Invasive Carcinoma (BRCA) subtype classification, containing 5 categories with 46 to 436 samples per category. Lastly, the Scene15(FeiFei Li 2005) dataset has 3 views with 15 categories for scene classification, with each category containing 210 to 410 samples."}, {"title": "Comparison Methods.", "content": "This paper compares UMAE with the following methods: (1) GCCA(Kettenring 1971), an extension of Canonical Correlation Analysis (CCA) for handling data with more than two views; (2) TCCA(Luo et al. 2015), which maximizes canonical correlation across multiple views to obtain a shared subspace; (3) MVAE(Wu and Goodman 2018), which extends the variational autoencoder to multi-view data using a product-of-experts strategy to find a common latent subspace; (4) MIWAE(Mattei and Frellsen 2019), which adapts the importance-weighted autoencoder for multi-view data to impute missing data; (5) CPM-Nets(Zhang et al. 2019), which directly learns joint latent representations for all views and maps these to classification predictions; (6) DeepIMV(Lee and van der Schaar 2021), which applies the information bottleneck framework to extract marginal and joint representations and constructs view-specific and multi-view predictors for classification; and (7) UIMC(Xie et al. 2023b), which uses the K-nearest neighbors method to form a sampling distribution, repeatedly samples missing data, and employs the EDL fusion method to explore and leverage uncertainty from imputation for effective and reliable classification representations."}, {"title": "Experimental Configuration and Construction", "content": "We conduct extensive research on the proposed model across multiple datasets with varying missing rates, defined as $\\eta = \\frac{\\sum_{v=1}^V M}{V \\times N}$. The evaluation focuses on the model's ability to handle different levels of missing data and view conflicts. To generate conflict datasets, 40% of samples have one view randomly replaced with views from other categories. Experiments were performed on an NVIDIA RTX 4090 GPU."}, {"title": "Quantitative Experimental Results", "content": "Comparison on Normal Datasets. We first evaluated model effectiveness using standard datasets with varying missing rates $\\eta = [0, 0.1, 0.2, 0.3, 0.4, 0.5]$. All methods were implemented using the same network architecture. As presented in Table1, UMAE consistently outperformed all existing methods, including the state-of-the-art UIMC. This is particularly notable as the K-nearest neighbors imputation method used in UIMC showed diminishing effectiveness with sparse samples, larger sample distances, and higher missing rates. UMAE's superior performance can be attributed to its learning-based imputation approach, which operates on feature layers of missing views and robustly leverages global information to generate high-quality imputed views, thereby achieving leading-edge performance."}, {"title": "Comparison on Conflictive Datasets.", "content": "To validate the effectiveness of the proposed conflict loss module, we conducted experiments on conflict datasets."}, {"title": "Ablation Study.", "content": "To evaluate each stage of APLN, we conducted experiments using Zero-Imputation (ZIMP) and Mean-Imputation (MIMP) as baselines, alongside UMAE-F, UMAE-V, and UMAE-J.As shown in Table 2, UMAE-F with its noise imputation method at the feature layer, achieve competitive performance with regard to baseline methods. This stage primarily enables a coarse feature alignment function, allowing UMAE-V to refine the aligned feature domain. In UMAE-V, vae and edl module is learned, significantly enhancing the model at this stage. UMAE-J, where the feature layers ${f^v}_{v=1}^V,VAE and EDL moudle are jointly trained, improves coordination and adaptability among model components, leading to overall performance optimization."}, {"title": "Qualitative Experimental Results", "content": ""}, {"title": "t-SNE Visualization of Model Stages.", "content": "To visually and qualitatively assess the effectiveness of our approach across the three stages, we conducted a t-SNE experiment on the Handwritten dataset. As illustrated in Fig. 4, the t-SNE visualizations demonstrate the progressive improvement of the model's classification abilities across the stages of APLN, ultimately achieving state-of-the-art results."}, {"title": "Kernel Density Estimation Analysis of Uncertainty Across Model Stages.", "content": "To quantitatively and qualitatively assess the reduction in output uncertainty across the three stages of our approach, we performed a kernel density estimation (KDE) analysis on the 4 datasets under a missing rate of $\\eta = 0.4$. As shown in Fig. 5, the KDE plots clearly illustrate the progressive decrease in model output uncertainty as the APLN process unfolds. These findings underscore the effectiveness of our method in refining the model's confidence in its predictions over the course of the three stages."}, {"title": "Conclusion", "content": "In this work, we introduced the APLN to address challenges in incomplete multi-view classification, focusing on mitigating bias and managing uncertainty in corrupted observed domains. Our approach refines imputation in the latent space and integrates a conflict-aware DSCR to enhance decision-making robustness. Experimental results on benchmark datasets demonstrate that APLN significantly outperforms traditional methods, particularly in environments with high uncertainty and conflicting evidence. However, the computational demands of our method, especially during the progressive learning and evidence fusion stages, suggest potential challenges in scaling to very large datasets. Future work will focus on improving the computational efficiency of APLN, making it more accessible for larger-scale applications while maintaining its robustness in IMVC."}]}