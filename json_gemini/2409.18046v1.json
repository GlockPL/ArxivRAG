{"title": "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "authors": ["Soeun Lee", "Si-Woo Kim", "Taewhan Kim", "Dong-Jin Kim"], "abstract": "Recent advancements in image captioning have explored text-only training methods to overcome the limitations of paired image-text data. However, existing text-only training methods often overlook the modality gap between using text data during training and employing images during inference. To address this issue, we propose a novel approach called Image-like Retrieval, which aligns text features with visually relevant features to mitigate the modality gap. Our method further enhances the accuracy of generated captions by designing a Fusion Module that integrates retrieved captions with input features. Additionally, we introduce a Frequency-based Entity Filtering technique that significantly improves caption quality. We integrate these methods into a unified framework, which we refer to as IFCap (Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning). Through extensive experimentation, our straightforward yet powerful approach has demonstrated its efficacy, outperforming the state-of-the-art methods by a significant margin in both image captioning and video captioning compared to zero-shot captioning based on text-only training.", "sections": [{"title": "Introduction", "content": "The task of image captioning generates appropriate textual descriptions for images by combining computer vision (CV) and natural language processing (NLP). With the emergence of Large Language Models (LLMs) and Vision and Language Models (VLMs), various works have studied efficient training methods for image captioning (Mokady et al., 2021; Luo et al., 2023; Ramos et al., 2023). These approaches develop effective captioning by using pre-trained models with few parameters or lightweight networks. However, these works rely on paired image-text data, which is costly (Kim et al., 2019b, 2024). To overcome this, recent studies have explored text-only training methods for image captioning, aiming to solve the problem using only textual data (Nukrai et al., 2022; Li et al., 2023; Fei et al., 2023; Zeng et al., 2024; Wang et al., 2023; Liu et al., 2023; Ma et al., 2023).\nText-only training introduces a new direction in which models are trained solely using text data. Recent existing works have studied what to use as extra cues, such as extracted nouns (Fei et al., 2023), generated synthetic images (Liu et al., 2023; Ma et al., 2023) for training, and extracted tags from object detectors (Liu et al., 2023). However, existing methods that rely on object information are sensitive to incorrect data, and utilizing large external models (e.g., stable diffusion Rombach et al., 2022 or object detectors Carion et al., 2020) incurs additional costs. Thus, we aim to address the problem by acquiring diverse information cost-effectively without additional models.\nThe retrieval task involves finding relevant information in a database for a given query. Initially rooted in NLP (Lewis et al., 2020), the field has expanded into CV and into multi-modal retrieval. Depending on the input data and database, various retrieval methods are possible, such as image-to-text (Ramos et al., 2023) and text-to-text retrieval (Wang et al., 2023). In the existing text-only training study, there have been attempts to use the text-to-text retrieval method. However, existing works can't address the modality gap inherent in text-only training settings, where training is performed with text and inference with images. In addition, such works rely too much on retrieved captions without considering visual information. This modality gap and the use of a narrow scope of information may lead to performance degradation.\nTo verify this, we visualize the analysis result of the CLIP embedding feature of retrieved captions that the model uses in training via t-SNE in Fig. 2. The analysis is done on the COCO (Chen et al.,"}, {"title": "Related work", "content": "Text-only Captioning\nThe advantage of CLIP (Radford et al., 2021) has been utilized in a variety of tasks, such as image captioning, image generation, and object detection. In the realm of image captioning, text-only training research is emerging that uses only text data for learning without image data, taking advantage of the CLIP characteristic that image embeddings and text embeddings are learned to be close. DeCap (Li et al., 2023) trains a text decoder using only textual data and introduces a support memory mechanism to project input images into the text embedding space during inference, facilitating the generation of captions. ViECap (Fei et al., 2023) recognizes the main entity of text data that comes as input and configures it as a prompt, allowing LLM to perform object-agnostic learning based on open vocabulary retrieval using CLIP.\nModality Gap\nVision language models such as CLIP aim to embed images and text closely in a shared space. How-"}, {"title": "Modality Gap", "content": "ever, it has been shown that these embeddings are located in two separate regions, with a significant gap between the modalities (Liang et al., 2022). This modality gap hinders the interaction between vision and text modalities and limits the quality of generated captions. Among the notable approaches addressing this issue, CapDec (Nukrai et al., 2022) assumes that the image embeddings paired with text embeddings are located within a small radius around the text embeddings and mitigates the gap with noise injection. CLOSE (Gu et al., 2022) highlights the low cosine similarity between images and their paired texts and uses a hyper-parameter-scaled noise injection technique to bridge the gap.\nWe focus on the modality gap for retrieval from a new perspective. Our goal is to perform text retrieval similar to image-to-text retrieval, considering the modality gap. The distinction from existing methods can be observed in Fig. 2 left."}, {"title": "Retrieval Augmented Generation", "content": "Retrieval has been used in diverse ways in NLP. Image captioning also benefits from retrieval modules by incorporating novel objects and new information into captions, allowing access to new domains without additional training. Retrieval is applied in various ways in image captioning models. For instance, Smallcap (Ramos et al., 2023) retrieves captions relevant to the input image and uses them as instructions for the text decoder. In text-only image captioning, ViECap (Fei et al., 2023) retrieves novel objects from the input image and uses them as prompts, while Knight (Wang et al., 2023) uses retrieved captions as text features.\nMost retrieval methods are based on image-to-text retrieval, but text-only captioning performs text-to-text retrieval. However, during inference, the modality gap caused by the input image leads to poor performance. Our method carefully addresses this issue to improve performance by considering the gap between image and text."}, {"title": "Methods", "content": "We propose a new text-only image captioning model, IFCap, which is illustrated in Fig. 4. During training, the model only utilizes text data, as is standard for text-only training models. First, we embed the input text using a text encoder. The text embeddings are then fed into a mapping network to close the gap between different modalities. Finally, the processed embeddings go through a caption decoder to generate the output caption.\nOur IFCap utilizes a simple yet powerful retrieval mechanism and addresses the modality gap between image and text with Image-like Retrieval (Section 3.1). After performing Image-like Retrieval, we employ a Fusion Module (Section 3.2) to merge input embeddings with the retrieved features. During inference, we use the retrieved captions from the image to find accurate and detailed entities with Frequency-based Entity Filtering (Section 3.3)."}, {"title": "Image-like Retrieval (ILR)", "content": "While text-to-text retrieval can be effectively performed during training, it is likely to suffer from performance degradation during inference when an image is provided as input due to the modality gap. Therefore, Image-like Retrieval (ILR) aims to perform text-to-text retrieval in a manner that resembles image-to-text retrieval outcomes, given text input. For this, we propose an approach that inserts noise into the feature space of the input text, bringing it closer to the image feature space. The augmentation process is as follows:\nFirst, we utilize the CLIP to embed the input text \\(t_i\\) and the text corpus \\(T = \\{t_i\\}_{i=1}^{n}\\) with a text encoder \\(E_T\\). Then, we introduce noise \\(\\epsilon_T \\sim N(0, \\sigma^2)\\) into"}, {"title": "Image-like Retrieval (ILR)", "content": "the embedding of input text \\(T_i\\), aiming to adjust the text features to align more closely with the image feature space:\n\\[T'_i = E_T(t_i), T'_i = T_i + \\epsilon_T.\\]\nNext, the retrieval step is performed using the noise-injected input text \\(T'_i\\). To identify the descriptions most relevant to \\(T'_i\\), the top-k descriptions are retrieved by calculating the cosine similarity between \\(T'_i\\) and all sentence embeddings in the text corpus. This process closely follows previous methods in image-to-text retrieval (Ramos et al., 2023), with the distinction that we perform retrieval based on \\(T'_i\\) instead of images.\nBy utilizing this approach during training, we can enhance the ability of a model to provide image-like information even in a text-only training setting, thereby narrowing the modality gap and improving performance."}, {"title": "Fusion Module (FM)", "content": "In text-only image captioning, choosing which additional information to inject into the model and dealing with new representations with given data appropriately are important issues. To handle this problem, we use the attention mechanism (Vaswani et al., 2017) to fuse input text features and retrieved captions features to extract their meaningful interaction. The attention mechanism emphasizes certain important features, and due to its effectiveness, it has been widely utilized in the field of captioning (Xu et al., 2015).\nWe first encode input text and retrieved captions using CLIP (Radford et al., 2021) text encoder, then inject a Gaussian noise \\(\\epsilon \\sim N(0, \\sigma^2)\\) to input text feature for relieving the modality gap between image and text. Then we adjust the dimension of the input text feature and retrieved captions feature to the embedding space of caption decoder with linear layers \\(f_{l_1}\\) and \\(f_{l_2}\\) respectively, and apply cross-attention \\(f_{Att}\\) with \\(T_e\\) as query and \\(R_e\\) as key, then create fusion representation \\(F_e\\) containing input text and retrieved captions. Finally, \\(F_e\\) is fed into a trainable Mapping Network, which encodes the overall contents of the given input. We can summarize this process with equations.\n\\[T_e = T_i + \\epsilon, R_e = E_T(ILR(T_i)),\\]\n\\[F_e = f_{Att}(f_{l_1}(T_e), f_{l_2}(R_e)),\\]\n\\[F = Map(F_e; \\theta_{\\phi}).\\]"}, {"title": "Fusion Module (FM)", "content": "The noun implies intuitive and explicit information about objects in the image. For employing the property of nouns, we extract entities in each training text corpus and input images. We build a hard prompt h with extracted entities \\(E = \\{e_1, e_2, ..., e_n\\}\\) to make the model aware of existing entities in the image. With retrieved captions and hard prompts with entities, the model can learn the ability to generate proper captions without images. We use auto-regressive loss to optimize our projector and caption decoder. (Details about the Fusion Module are in Sec. 4.1).\n\\[L_e = \\frac{1}{N} \\sum_{i=1}^{N} log(y_i|F_i; h; y_{<i}; \\theta).\\]"}, {"title": "Frequency-based Entity Filtering (EF)", "content": "After retrieving \\(l\\) captions from an image, we use grammar parser tools (e.g., NLTK Bird and Loper, 2004) to extract nouns from the retrieved sentences and calculate the frequency of these extracted nouns as \\(F = [f_1, f_2, ..., f_n]\\). We then select nouns that have a frequency larger than a predefined threshold and place them into a hard prompt.\nHeuristic threshold: Since frequency is discrete, we can manually find the best threshold by conducting experiments with every possible threshold. This allows us to determine the global optimal threshold.\nAdaptive threshold: We can use a heuristic threshold, but these thresholds are often unsuitable for different environments, and performing extensive experiments incurs unnecessary costs. Instead, we can estimate the common distribution of noun frequencies as certain probability distributions. We can assume frequencies follow \\(N(\\mu_F, \\sigma_F).\\)\n\\[\\tau_{adap} = \\mu_F + \\sigma_F.\\]\nAny nouns with a frequency larger than \\(\\tau_{adap}\\), which places them in the upper 15%, can be considered outliers. Using this adaptive threshold, we can implement a flexible threshold that fits various settings. However, it does not guarantee global optima, leading to a trade-off relationship between heuristic thresholds and adaptive thresholds."}, {"title": "Experiments", "content": "Implementation Details\nWhile verifying the state-of-the-art performance of our model, we use CLIP (ViT-B/32) as the image encoder and GPT2base (Radford et al., 2019) as the text decoder. Parameters in the image encoder are frozen during training, and the text decoder and Fusion Module are trained. We train a total of 5 epochs, learning rate of \\(2 \\times 10^{-5}\\), use scheduler for learning rate scheduler, AdamW optimizer (Kingma and Ba, 2014), and set batch size 80. We use a single NVIDIA RTX4090 with 24GB VRAM; it takes about an hour and uses 12GB of VRAM during training.\nImage-like Retrieval: We first discover adequate \\(\\sigma_i\\) for Image-like Retrieval. Based on our experiment (Fig. 5), we choose \\(\\sigma_r\\) as 0.04 in most cases. We retrieve k sentences with noise-injected input text feature \\(T_e\\).\nFusion Module: We project \\(T_e \\in \\mathbb{R}^{d}\\) and \\(R_e \\in \\mathbb{R}^{d \\times k}\\) with \\(f_{l_1}, f_{l_2}\\) into \\(\\mathbb{R}^{d_{gpt}}, \\mathbb{R}^{d_{gpt} \\times k}\\) respectively where d is the CLIP dimension and \\(d_{gpt}\\) is the dimension of GPT-2 embedding space. We use projected \\(T_e\\) as query and \\(R_e\\) as key in \\(f_{Att}\\) layer. Finally, \\(F_e\\) and \\(\\Theta_q\\) are concatenated and fed into"}, {"title": "Experiments", "content": "the Mapping Network, which consists of 8 layered transformers (Vaswani et al., 2017).\nFrequency-based Entity Filtering: From the input image, we retrieve l sentences and extracted nouns to obtain frequency F. With the predefined threshold, we filter entities and build hard prompt h, providing more accurate and diverse entities to the caption decoder.\nDatasets, metrics We evaluate our model in human-annotated datasets. For in-domain generalization, we test our model on MS-COCO (Chen et al., 2015), Flickr30k (Young et al., 2014), and utilize Karpathy split (Karpathy and Fei-Fei, 2015). Also, to check the model's performance in the unseen scenarios, we use the No-Caps (Agrawal et al., 2019) validation set. For metrics, we use common image captioning metrics CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016), BLEU@n (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005). More details about datasets and metrics are included in the appendix (Sec. \u0410)."}, {"title": "Text-only Captioning", "content": "We compare our model with other state-of-the-art text-only image captioning models. CapDec (Nukrai et al., 2022) and ViECap (Fei et al., 2023) are based on Clipcap (Mokady et al., 2021). They use predefined Gaussian noise for aligning text and image features. Similarly, CLOSE (Gu et al., 2022) uses various noise settings, and DeCap (Li et al., 2023) uses a memory bank. And a recent approach to text-only image captioning, Knight (Wang et al., 2023) only utilizes text features with a retrieval mechanism, also MeaCap (Zeng et al., 2024) processes retrieved sentences into Subject-Predicate-Object triplets and employs them as additional information. ICSD (Ma et al., 2023) and SynTIC (Liu et al., 2023) utilize text-to-image generation models like Stable Diffusion (Rombach et al., 2022) to close the gap."}, {"title": "In-domain Captioning", "content": "We benchmark our IFCap on in-domain settings in Table 1 including COCO and Flickr30k. We compare our methods with previous state-of-the-art in text-only image captioning. Our IFCap dominates every metric in the COCO dataset compared to models that utilize larger models (Gu et al., 2022; Wang et al., 2023) and have complex training time (Ma et al., 2023; Liu et al., 2023). Also, in Flickr30k, IFCap shows decent performance in BLEU@4 and METEOR and achieves the best scores in CIDEr and SPICE."}, {"title": "Cross-domain Captioning", "content": "We validate IFCap's transfer ability through diverse domains, including the NoCaps validation set and cross-domain from COCO \u2192 Flickr30k and vice versa. In NoCaps, we use the same model trained in the COCO domain to test how the model recognizes unseen objects during training. In the NoCaps validation split, our IFCap performs the best in every metric and every domain compared to previous state-of-the-art text-only image captioning models (Li et al., 2023; Nukrai et al., 2022; Fei et al., 2023). Also, in cross-domain settings between COCO and Flickr30k, IFCap wins state-of-the-art in most metrics and the second best in some metrics."}, {"title": "Video Captioning", "content": "In video captioning, we train our model in the same manner as previous experiments. First, we perform Image-like Retrieval on the corpus from each video captioning dataset MSVD (Wu et al., 2017) and MSR-VTT (Xu et al., 2016). For inference time, we sample 5 images from input video and calculate the average of their CLIP image features. We also retrieve 5 sentences from each sampled image, 25 in total, and also calculate the average of CLIP text features per image. Most of the met-"}, {"title": "Ablation Study", "content": "We conduct extensive experiments to identify the impact of each key component in IFCap, Image-like Retrieval (ILR), Fusion Module(FM), and Frequency-based Entity Filtering(EF). Also, for each component, we search the best hyperparameter in the COCO test split with an in-domain setting.\nKey Components: We check the strength of each component by detaching from our best model, which consists of all 3 components Table 5. First, removing FM, we simply concatenate the input text feature and retrieved features after applying dimension mapping layers \\(f_{l_1}\\) and \\(f_{l_2}\\) and passing it to the caption decoder. Removing EF is simply applying entity extraction via CLIP classifier like (Fei et al., 2023) does. Demounting ILR makes inaccessible to retrieval features solely using input features; hence FM can't exist without ILR. Adding more components into the baseline, we can explicitly notice performance improvement. So, using all three key components constitutes a state-of-the-art model, which is IFCap. Note that IFCap has 2 variants, IFCap and IFCap*, with EF and without EF respectively. To see a full comparison of various datasets, including in-domain, cross-domain, and"}, {"title": "Ablation Study", "content": "video captioning, refer to Table 14.\nImage-like Retrieval: It is crucial to identify adequate timing for injecting noise into text features for successful text-to-text retrieval that imitates image-to-text retrieval. We can split injecting timing into Pre-e and Post-e. We find that our setting which injects noise before performing retrieval is the best among all possible combinations. We can verify this in Table 6. The first column of the table indicates how the model performs retrieval, just for easy understanding of noise injection in retrieval.\nFusion Module: We utilize a cross-attention layer and transformer layer for mapping the network. In Table 8, we try multiple combinations of the number of each layer. The more layers we use, the more performance gain we can get until the number of transformer layers is 4. The performance gain is also observed when we use 8 transformer layers but it is so slight. Increasing the number of cross-attention layers is effective when the transformer layer is small, but the tendency does not last while the transformer layer grows. We conclude using 8 transformer layers and"}, {"title": "Conclusion", "content": "In this paper, we propose a zero-shot captioning method, IFCap, through text-only training. IFCap performs Image-like Retrieval to address the gap between image-to-text retrieval and text-to-text retrieval, Fusion Module for interaction between existing and additional representations, and Frequency-based Entity Filtering during inference time to extract frequently occurring entities from the retrieved sentences. Our method can be easily applied to various tasks and provides valuable guidance for retrieval-based methods in a text-only setting. It offers clear and precise information to LLMs without relying on a limited vocabulary. The simplicity and robustness of IFCap are demonstrated through state-of-the-art performance across various datasets in image and video captioning. The future direction of our method includes the extension of our method on more complex datasets, such as region-based captioning (Kim et al., 2019a, 2021) or visual question answering (Cho et al., 2023a,b), which suffer from data issues."}, {"title": "Limitations", "content": "We demonstrate that IFCap exhibits superior performance across various image captioning and video captioning datasets compared to other zero-shot image captioning models with text-only training. However, the optimal value of \\(\\epsilon_r\\), for Image-like Retrieval currently requires a heuristic approach to determine. We leave the task of finding a more convenient method for determining the optimal \\(\\epsilon_r\\), as future work to further improve image captioning models with text-only training."}, {"title": "Image-like Retrieval", "content": ""}]}