{"title": "Can Language Models Enable In-Context Database?", "authors": ["Yu Pan", "Tianjiao Zhao", "Hongfeng Yu", "Jianxin Sun"], "abstract": "Large language models (LLMs) are emerging as few-shot learners capable of handling a variety of tasks, including comprehension, planning, reasoning, question answering, arithmetic calculations, and more. At the core of these capabilities is LLMs' proficiency in representing and understanding structural or semi-structural data, such as tables and graphs. Numerous studies have demonstrated that reasoning on tabular data or graphs is not only feasible for LLMs but also gives a promising research direction which treats these data as in-context data. The lightweight and human readable characteristics of in-context database can potentially make it an alternative for the traditional database in typical RAG (Retrieval Augmented Generation) settings. However, almost all current work focuses on static in-context data, which does not allow dynamic update. In this paper, to enable dynamic database update, delta encoding of database is proposed. We explore how data stored in traditional RDBMS can be encoded as in-context text and evaluate LLMs' proficiency for CRUD (Create, Read, Update and Delete) operations on in-context databases. A benchmark named InConDB is presented and extensive experiments are conducted to show the performance of different language models in enabling in-context database by varying the database encoding method, prompting method, operation type and input data distribution, revealing both the proficiency and limitations.", "sections": [{"title": "1 INTRODUCTION", "content": "The recent surge in enthusiasm for LLMs (Large Language Models) stems from their growing ability to handle and interpret textual data, as demonstrated by notable studies from [7, 11, 31, 33, 34, 38, 39, 44]. Originally designed to process sequential text, these models have now been adapted to performing a wide range of tasks across different modalities, such as voices, images and videos [6, 9, 44]. Modern LLMs are evolving as few-shot (or zero-shot) learners [7] capable of handling a variety of tasks, ranging from question answering, semantic comprehension, planning, reasoning, arithmetic calculations and more [2-5, 10, 20, 26, 37]. Collectively, these developments have reinforced the belief that LLMs are critical milestones on the journey toward achieving artificial general intelligence (AGI) [8]. Researchers believe LLMs' capability rely on their representations and comprehension of various structures from outside world, which can be collected and expressed as structural data such as graphs and tables.\nRecently numerous studies have shown that LLMs are capable of doing in-context reasoning on graphs [13, 16, 32, 40, 42] and tables [15, 21, 25, 35, 36, 43]. Given the structural data and the description of a task, possibly with extra examples and instructions, LLMs need to reply with the correct result of the task on the structural data. By serializing the structural data using various encoding scheme and adopting different prompting technologies, these work try to find out the optimal protocol through which the input can maximize LLMs' capability of reasoning on structural data. However, almost all of the exiting work does not take into account of the dynamic feature of the structural data, which may be updated frequently in the actual application scenarios. For instance, the nodes and edges may be added/removed to/from the graph, and new knowledge will be inserted into the knowledge graph while outdated knowledge will be removed. Similarly, tabular data may also be edited to reflect the updated situation. So the question arises naturally: can LLMs enable in-context update of structural data? In this case, not only a querying task is allowed as input to LLMs, but an updating task will also be allowed. If the update is acceptable and does not violate any constraint of the structural data, LLMs need to register the update, otherwise LLMs need to reply with an error message. In this setting, LLMs should not only be capable of comprehending the structures of the data, but also understanding the updates along the temporal dimension.\nAs the context length of LLMs increases rapidly with some technologies being proposed to even scale the context to infinitely long [28], it is promising that LLMs' context window can accommodate way larger dataset in the near future. Combined with LLMs' in-context reasoning capability on dynamic structural data, we believe it is possible for LLMs to enable in-context database, which is a lightweight alternative of traditional database in which CRUD (Create, Read, Update and Delete) operations are handled by LLMs, rather than pre-programmed procedures as in traditional DBMS. In the prevalent settings of RAG (Retrival Augmented Generation), where the whole dataset is stored on external database and sampled as required by the specific task, in-context database provide an alternative solution. Also the adoption of in-context database and traditional database in RAG is not necessarily exclusive, they can complement with each other. For instance, external database can store the full dataset which has lower rate of updates whereas in-context database can act as a partial cache which is also more fresh. In a typical multi-agent [17, 24] configurations, there can be an agent which is mainly responsible for data management, which act as a database, but can understand and execute queries more intelligently.\nIn this work, we perform the first comprehensive evaluation about how the data stored in the traditional RDBMS database can be encoded as text and LLMs' proficiency for CRUD operations on in-context database. A benchmark named InConDB is proposed which includes dataset and queries for RDBMS databases. We conduct"}, {"title": "2 FRAMEWORK", "content": "2.1 Problem Formalization\nLet f represents the function of a language model, in which f: W \u2192 W maps the input tokens to output tokens, both from the set of all series of tokens W. A relational database D = {Ti|i = [1, N]} contains N tables Ti, where i \u2208 [1, N]. Q represents a CRUD (Create, Read, Update and Delete) operations on database D. And S = s(D, Q) denotes the ground truth solution when performing query Q on database D, where s represents a real world database management system (DBMS). For select operations, S will be the query result of Q and for modification operations such as insert, update or delete, S will be either \"Succeed\" when the query succeeds, or \"Fail\" when the query fails.\nTo enable the in-context database, we present both the database D and the query Q to the language model. To optimize the performance of language model, we introduce encoding method d of database D and query Q, where d can be either SQL or natural language (NL), which means the database D and query Q can be described in either SQL commands or in natural language. Also we introduce prompting method p, which could be one of the three"}, {"title": "2.2 Delta Encoding of Database", "content": "Different from existing works which use plain tables to represent tabular data or relational databases, here in our paper we use \"delta\" encoding. That is, we use a sequence of commands to represent the current status of the database, in which each command encodes a minor modification of the status of the database in previous step. Then d(D) is a command sequence [C1, ..., Ci, ..., Ck], and d(Q) = Cj, where Ci, Cj \u2208 I and I is the set of all database operations. Formally, we can get the following equation:\nD = d\u207b\u00b9([C\u2081, ..., C\u1d62, ..., C\u2096]) = C\u2096 (...(C\u2082(C\u2081(\u00d8))))\nwhere \u00d8 represent the initial status of all databases containing no data. The current status of database D is equivalent to the composition function of C\u1d62 for i from 1 to k, starting from empty database.\nThe benefit of using delta representation is obvious: new database operation can be appended to the end of historical command sequence immediately. It is very convenient to represent the current status of a database in such accumulative fashion, without any real database operations. So the insert, update or delete will always be an O(1) operation. The cost for the delta encoding is that it requires high reasoning capability of language models, to infer the current status of the database from the command sequence D and conduct the query Q on it accordingly. This process not only requires large language model to understand the structure of the database, but decipher the changes along the time axis as well. So generally speaking, our work also evaluate large language model's capability to understand the accumulating effects of historical events."}, {"title": "2.3 Evaluation Framework", "content": "To evaluate the performance of language models as in-context database, for various combinations of encoding methods and prompting methods, we propose a two-branch evaluation framework illustrated in Figure 1. First we prepare a bunch of JSON files, each containing a bunch of CRUD operations for one database schema, then a data sample generator is utilized to sample a tuple: (instruction sequence D, query Q). The sampled tuple (D,Q) is subject to a distribution D:\n(D, Q) ~ D(l, b, o)\nin which the distribution D has parameters l, b, o. l represents the number of commands in the command sequence D, b represents the ratio of insert operations in the whole command sequence, and o represents the overlap ratio between insert operations and non-insert operations. Figure 2 illustrates the concepts of these three parameters. In our experiment, we'll evaluate the performance of language models as in-context database, by varying these three parameters. Intuitively, by increasing"}, {"title": "2.4 Prompting and Encoding", "content": "As introduced in 2.1, we evaluate the performance of different languages models by varying the combination of encoding and prompting methods. There are 2 encoding methods: SQL and NL (natural language) and 3 prompting methods: zero-shot, zero-COT and few-shot.\nFigure 3 illustrates a case of the model input and output for encoding method of SQL and prompting method of zero-shot. All the commands will be given in SQL syntax. In the system prompt, we instruct the language model to imitate a relational database and we also specify the format of user input and model output. Since this case is zero-shot, the user is expected to feed in the SQL instruction sequence (in shallow blue) and query (in shallow green) at once, without any model output in between each command. We can see the instruction sequence contains insert, delete, select and update queries. The language model is required to synthesize the execution of all the commands and directly reply with the correct"}, {"title": "3 EXPERIMENTS", "content": "3.1 Experiments Configuration\n3.1.1 Benchmark. As introduced in 2.3, We create 20 JSON files, each contains the schema and CRUD operations for a relational database, which contains 3-5 tables, some of which have foreign keys referencing other tables. Each database file contains several insert, delete, update and select operations, and the select operations are further divided into several categories such as select queries with different number of filtering conditions, joint queries with different number of tables, range queries, queries with ranking, queries with count. In the following experiments, we evaluate the"}, {"title": "3.2 Experiment 1. Comparing Language Models", "content": "In this experiment, we evaluate and compare the performance of different language models for CRUD database operations: update, delete, insert and select, by varying different combinations of prompting and encoding methods. We evaluate 3 types of prompting methods: zero-shot, zero-COT and few-shot, and 2 types of encoding methods: SQL and NL. The exact meaning of the prompting and encoding methods are already introduced in 2.4. We choose l, b and o defined in 2.3 as 100, 0.5 and 0.5 respectively. For each parameter setting, we sample 300 tuples of (D,Q), calculate the accuracy defined in 2.3, and get the average accuracy for all 300 tuples. Since we only fine-tune LLama3.1-8B for the prompting method of few-shot, so we merely show the result for this case. We render the background color intensity based on the accuracy.\nFigure 7 illustrates the comparison of different models for CRUD database operations, for prompting method of few-shot and encoding method of SQL. We group the data by each operation. For insert and select operations, GPT-40 and the fine-tuned LLama3.1-8B are the top 2 models; for update, the situations are different that Gemma2 outperforms other models and fine-tuned LLama3.1-8B is the second; for delete operation, all the performance of all the models, except Mistral, is close to each other. The performance of Mistral is the lowest for all the CRUD operations.\nThe comprehensive evaluation results are listed in Table 1. The best case for each operation is enclosed in yellow box. Generally speaking, GPT-40 has best performance and our fine-tuned LLama3.1-8B also has great performance by using few-shot prompting. Gemma2 only performs well when few-shot is adopted. LLama3.2-3B performs a little better than LLama3.1-8B. Mistral performs worst among all the models.\nWe can also see few-shot is the best prompting method for all the models. To our surprise, zero-shot outperforms zero-COT for all the models, we think the reason is that in COT, we asked the model to output the correct answer in the last line, which may be too rigorous (see 4). So even the thinking process of a model is correct, it may still fail to output the correct result in the last line. As for the encoding methods, for zero-shot or zero-COT, natural language (NL) performs similarly or even better than SQL encoding when the query is update, delete and insert. It looks when there's no example of results given, natural language can describe the query more clearly for update, select and insert. But for select query, SQL outperforms natural language because SQL can define the output format of a select query more easily. For few-shot, natural language generally performs worse than SQL. We think this is because by"}, {"title": "3.3 Experiment 2. Comparing Data Retrieval Methods", "content": "In this experiment, we evaluate and compare the performance of different language models for 10 categories of select operations such as select queries with 0-3 filtering conditions in the \"where\" statement, join queries with 1-3 tables, range queries, select queries with \"order by\" predicates, and queries with \"count\", by varying different combinations of prompting and encoding methods. We evaluate 3 types of prompting methods: zero-shot, zero-COT and few-shot, and 2 types of encoding methods: SQL and NL. We choose l, b and o defined in 2.3 as 100, 0.5 and 0.5 respectively. For each parameter setting, we sample 300 tuples of (D,Q), calculate the accuracy defined in 2.3, and get the average accuracy for all 300 tuples. Since we only fine-tune LLama3.1-8B for the prompting method of few-shot, so we only show the result for this case. We render the background color intensity based on the accuracy. Table 2 contains the detailed results for each model. The best performer for each category of select operation in each column is enclosed in yellow box.\nGPT-40 dominates almost all the categories, except fine-tuned LLama3.1-8B performs best for count operation. All the best performance happens when few-shot prompting and SQL encoding are adopted. GPT-40 also outperforms other models when zero-shot and zero-cot are adopted. The fine-tuned LLama3.1-8B is second to GPT-40, which is impressive given its model scale is way smaller than GPT-40. The performance of Gemma2-9B and LLama3.2-3B are close to each other. Still Mistral performs worst among all the models.\nAs the number of filtering conditions increases, the performance generally decreases. This agrees with our intuition, because more filtering conditions generally means more complicated query. The only exception is that the performance for 3-Filter is better than 2-Filter, 1-Filter or even 0-Filter. This is because select queries with 3-Filter normally return empty set of results, which reduces the prediction complexity.\nSimilarly, when the number of joined tables increases, the performance generally decreases, for more tables means more complicated queries. 3-Table query has some exception, in which its performance outperforms 2-Table or 1-Table. We think the reason is still the same as the filtering case: 3-Table joining frequently returns empty set of results, which is easier for the models to predict.\nRange queries perform similar to filtering operations, and rank queries perform"}, {"title": "3.4 Experiment 3. Varying Input Scale", "content": "In this experiment, we evaluate the performance of language models by varying the input scale. Figure 8 illustrates the performance of different models as a function of input scale. We change the input scale l from 10 commands to 400 commands, and fix the encoding method to SQL, prompting method to few-shot and query method to no-filtering select query. We choose b and o as 0.5 and 0.5 respectively. For each input scale and model, we sample 300 (D,Q) pairs and calculate the average accuracy for all the pairs.\nWhen the input scale increases, the performance of all models drops for all models. The trend of dropping is slower when the number of commands exceeds 250, or the accuracy approaches 20%. GPT-40 and fine-tuned LLama3.1-8B is the best performer among all the models, except when the input scale exceeds 250, the performance of latter drops drastically. We think this is because we fine tune LLama3.1-8B with training data from l in [10, 100], so when the input scale exceeds certain bound, the data distribution becomes too different than the distribution of training data. But still, we can observe for l\u2208 [100, 200], fine-tuned LLama3.1-8B performs as expected, which proves the fine-tuning can be generalized when l exceeds 2x the upper bound of l in the training data."}, {"title": "3.5 Experiment 4. Varying the Ratio of Insert Operations", "content": "In this experiment, we evaluate the performance of language models by varying the ratio of insert operations. Figure 9 illustrates the performance as a function of the ratio of insert operations b. We change the ratio of insert operations b from 0 to 1, and fix the encoding method to SQL, prompting method to few-shot and query"}, {"title": "3.6 Experiment 5. Varying the Overlap between Insert and Non-Insert Operations", "content": "In this experiment, we evaluate the performance of language models by varying the overlap between insert and non-insert operations. Figure 10 illustrates the performance as a function of overlap between insert and non-insert operations. We change the overlap o between insert and non-insert operations from 0 to 1, and fix the encoding method to SQL, prompting method to few-shot and query method to no-filtering select query. We choose b and o as 0.5 and 0.5 respectively. For each parameter setting, we sample 300 (D,Q) pairs and calculate the average accuracy for all the pairs.\nWhen the overlap between insert and non-insert operations increases, the performance of all models changes not much, varying between 0.4 and 0.8, which means even we interleave the non-insert operation with insert ones, it does not change the complexity of the database much.\nGPT-40 and fine-tuned LLama3.1-8B is the best performer among all the models. The performance of other four models keeps almost constant, around 0.4."}, {"title": "4 RELATED WORK", "content": "4.1 In Context Learning\nA lot of studies have shown LLMs are capable of acting as few-shot learners [2, 7, 12], in which LLMs exhibit the capability to learn a novel task given the in context examples. These examples presented to LLMs is analogy to the program given to a traditional computer, but in an abstract and declarative way, instead of detailed and imperative way. Though still limited, it is nevertheless the first time a deep model can achieve such a high-order ability. Researchers"}, {"title": "4.2 Prompt Engineering", "content": "The goal of prompt engineering is to optimize the the output of LLMs for a specified task. There has been several methods being proposed: zero-shot in which the model is provided with a task description without any further examples, few-shot [7] in which a small number of examples along with their corresponding outputs are presented to the model and the model can conduct in-context learning to generate output on new inputs, chain-of-thought(CoT) [41] where the model is provided with a number of examples, each showing how to solve the corresponding task step by step, zero-shot COT [22] which is similar to CoT except the model is not presented with any examples but simply with a simple prompt like: \"let us think step by step\". In this paper, we'll evaluate LLMs' ability as in-context database by adopting the above prompting methods."}, {"title": "4.3 In Context Graph Reasoning", "content": "Recently there are emerging studies evaluating LLMs' capability of doing in-context reasoning on general graphs or knowledge graphs [13, 16, 23, 32, 40, 42]. By presenting a description of a graph and a query on the graph, these studies are curious about how well LLMs can answer the query. Majority of the work conclude that LLMs can demonstrate the capability of in-context reasoning on graphs (though limited) while giving feasible suggestions and methods on how to optimize such capability. All the existing work focus on in-context learning on static graphs, which does not allow any updates."}, {"title": "4.4 In Context Tabular Data Reasoning", "content": "Similar to graph reasoning, there are also studies evaluating LLMs' capability of in-context reasoning on tabular data [15, 21, 25, 35, 36, 43], by presenting a serialized description of a table and various tasks to language models. These studies have shown LLMs perform well on some of the tasks while still have limited capability on other tasks. Various prompting and encoding frameworks are developed to optimize the performace of LLMs. Also all of the existing work"}, {"title": "4.5 Retrieval Augmented Generation", "content": "Retrieval Augmented Generation (RAG) [14, 18, 23, 27] is proposed to solve the hallucination problems by augmenting LLMs with external knowledge which is more fresh and domain-specific. It is more lightweight compared with fine-tuning, because no model parameters need to be updated and the external knowledge can be appended along with the query sent to the language model. RAG often relies on external databases which have already been populated with domain knowledge and can be sampled in queried time. That is, only the knowledge pertinent to current query will be retrieved and presented to the language model. Our work, on the other hand, tries to put all the data in the context of LLMs and utilized LLMs' own capability to extract necessary information."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose dynamic in-context database. A benchmark named InConDB is presented and we investigate how data stored in traditional RDBMS databases can be represented as text and evaluate the capability of large language models (LLMs) to perform CRUD (Create, Read, Update, Delete) operations on in-context databases. We introduce a benchmark called InConDB and conduct extensive experiments to evaluate the performance of LLMs in enabling in-context database interactions. Our study highlights how performance varies depending on factors such as database encoding techniques, query encoding strategies, prompt engineering, operation type, and data distribution, uncovering both strengths and limitations of LLMs in this context. We find few-shot and SQL is the best combination of prompting and encoding method. Different types of queries also have effect on the accuracy of model prediction. GPT-40 outperforms all other evaluated models, and fine-tuned LLama3.1-8B achieves competitive performance. The number of input commands has a negative impact on the performance of language models. We also find that larger ratio of insert operations result in better performance and overlap between insert and non-insert operation has no obvious impact on the performance of models.\nIn current stage, in-context database is still a challenge for SOTA language models. But we believe as the size of context window and the reasoning capability of language models increases, in-context database will be enabled and as a result, for some light-weighted application scenarios, in-context database can replace traditional dataase in the near future."}]}