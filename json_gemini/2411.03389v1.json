{"title": "Neurons for Neutrons:\nA Transformer Model for Computation Load Estimation on\nDomain-Decomposed Neutron Transport Problems", "authors": ["Alexander Mote", "Todd Palmer", "Lizhong Chen"], "abstract": "Domain decomposition is a technique used to reduce memory overhead on large neutron transport\nproblems. Currently, the optimal load-balanced processor allocation for these domains is typi-\ncally determined through small-scale simulations of the problem, which can be time-consuming\nfor researchers and must be repeated anytime a problem input is changed. We propose a Trans-\nformer model with a unique 3D input embedding, and input representations designed for domain-\ndecomposed neutron transport problems, which can predict the subdomain computation loads\ngenerated by small-scale simulations. We demonstrate that such a model trained on domain-\ndecomposed Small Modular Reactor (SMR) simulations achieves 98.2% accuracy while being able\nto skip the small-scale simulation step entirely. Tests of the model's robustness on variant fuel\nassemblies, other problem geometries, and changes in simulation parameters are also discussed.\nKeywords Monte Carlo, Neutron transport, Machine learning, Domain decomposition", "sections": [{"title": "I. INTRODUCTION", "content": "Particle transport problems are a common application of high performance computing re-\nsources within the computational physics community. GPU acceleration allows for these problems\nto be solved orders of magnitude more rapidly, particularly for Monte Carlo simulations, the oper-\nations of which are known to be \u201cembarrassingly parallel\u201d [1]. Traditional Monte Carlo simulation\nmethods use a process known as domain replication, where each processor holds a copy of the\nfull problem geometry in memory, and tracks a subset of the particles within that geometry; for\nGPUs, this leads to a memory bottleneck as the size and complexity of the problem increases. This\nmemory bottleneck is mitigated by the use of domain decomposition [2], where each processor is\ninstead given a portion of the full problem geometry (referred to as a \u201csubdomain\u201d) and only sim-\nulates particles within that subdomain. Any particles that travel out of a processor's subdomain\nare communicated to the neighboring subdomain on a different processor using message passing.\nThis method of simulation allows GPUs to be efficiently utilized on large-scale neutron transport\nproblems, such as k-eigenvalue problems in modern nuclear reactors.\nHowever, not all subdomains are created equal. The composition of a neutron transport do-\nmain may create an imbalance in computation load across different subdomains; as such, uniformly\nassigning an equal number of processors to each subdomain in a given problem could drastically\nimpact runtimes. To balance this computational load, processors are instead allocated to subdo-\nmains such that each subdomain has a roughly equivalent overall runtime. The computational load\nof each subdomain is typically estimated by way of a small-scale version of the simulation, which\nruns with some fraction of the total number of particles needed for the full problem. However,\nthis small-scale simulation can still be very computationally expensive and time-consuming for\nexperimenters; additionally, new computational load estimates must be gathered when nearly any\nchange is made to the input parameters of a simulation. Even when the problem input itself is the\nsame, hardware differences or increased particle interactions within a given subdomain can change\nthe results between a small-scale and large-scale simulation.\nThis paper describes a deep learning model, specifically a novel Transformer model, which has\nbeen trained on domain-decomposed Small Modular Reactor (SMR) simulations, and predicts a\ncomputation load value for each subdomain in the full problem geometry. This model is uniquely\ndesigned to handle 3-dimensional reactor domains, using a 3-dimensional input embedding and"}, {"title": "II. PREVIOUS WORK", "content": "Shift [3] is a state-of-the-art Monte Carlo radiation transport code designed at ORNL to sim-\nulate neutron physics. It is massively parallel, and has demonstrated utilization of GPU architec-\ntures to achieve a 100-times speedup compared to CPU runtimes [4]. On larger problem domains,\nhowever, GPUs become infeasible, as the memory for all nuclides and tallies being simulated on\na GPU must be allocated upfront. To reduce this memory overhead, domain decomposition was\nimplemented in Shift, allowing complex problem geometries, like that of a depleted SMR, to be\nsimulated on GPUs [5].\nMonte Carlo code developers at ORNL have previously developed a processor allocation al-\ngorithm [6] for use with Shift. This algorithm uses an \u201callocation rank\u201d diagnostic value that is\nderived from small-scale simulation data to estimate computation load; this value is a normalized\nestimation of particle simulation time. An allocation rank value is generated for each subdomain\nduring a simulation, and these values are used in a post-processing algorithm to balance compu-\ntational load. The pseudocode for this algorithm can be found in Algorithm 1."}, {"title": "III. MOTIVATION", "content": "As discussed in section I, domain decomposition requires a processor allocation algorithm to\nensure that computational load is balanced across all subdomains in a simulation. The processor\nallocation algorithm described in section II achieves this by minimizing the proportion of subdo-"}, {"title": "IV. METHODOLOGY", "content": null}, {"title": "IV.A. Representing the Domains", "content": "The NuScale SMR domain consists of a 7x7 grid of \"core assemblies\", each containing different\nmaterials. Some of these materials are nuclear fuel rods, which generate the particles we are\nconcerned with simulating; these rods have varying amounts of enriched uranium that are defined\nby the user when the domain is created in VERA. These assemblies extend uniformly in the z-axis\nof the reactor; therefore, the z-axis can be arbitrarily subdivided for presentation to our model\nwithout changing the material properties of these assemblies.\nWe define an elementary volume within\nthe domain as the smallest volume that a do-\nmain can be subdivided into; in other words,\nthe elementary volume is the \"pixel\" of our\ninput representation, defining the resolution\nwith which our model can \"see\" the domain.\nFor this problem, we take each core assembly\nand divide it along the z-axis into 7 equivalent\npieces. This one-seventh of a core assembly is\nour elementary volume for this problem, creat-\ning a 7x7x7 3-dimensional grid of elementary\nvolumes that can be grouped into subdomains.\nTransformer models accept \"embedded\" \ninput tokens of any value; this means that a\nreactor with complex fuel compositions or a multitude of nuclides that contribute to computational"}, {"title": "IV.B. Dataset Generation", "content": "A script was created that randomly generates a domain decomposition that aligns with\nthe 7x7x7 grid created in section IV.A; that is to say, each subdomain contains at least one\nelementary volume, and each elementary volume can be assigned to one and only one subdomain.\nRandom generation was used to ensure that the dataset represented a subset of the entire possible\nsearch space of subdomains, and reduced the risk of overfitting on selected or conventionally\nused decompositions. This domain decomposition information is saved to a file that can later be\nused to run a Shift k-eigenvalue simulation on this subdomain allocation; aside from this domain\ndecomposition, the simulations are identical.\nA new tensor is generated for this domain decomposition, containing a row for every subdo-\nmain in the simulation. Each row is filled with the enriched fuel concentrations of all elementary\nvolumes contained in that subdomain; these volumes are indexed using the method seen in Figure\n1. All other elements of the row are filled with a constant embedding value representing parti-\ncle communication between \"out-of-subdomain\" elements. After the last subdomain is filled in\nthis way, a row of \"end of sequence\" values is added to inform the model that it has reached\nthe final subdomain in this input. Once filled with the relevant input information, these tensors\nare then padded with zeroes to ensure that they are all the same size for easier batch processing\nduring training; this allows the transformer model to process inputs with an arbitrary number of\nsubdomains.\nOnce the domain decompositions have been generated, a small-scale simulation is performed"}, {"title": "IV.C. Model Architecture", "content": "As mentioned in section III, a Transformer model was identified as the ideal machine learning\nmodel for this application. The model used in this paper was constructed using PyTorch modules;\nFigure 2 shows the architecture and flow of the Transformer model. Normally, Transformer models\naccept natural language sentences as an input, requiring the words to be \"embedded\" into numeric\ntokens before being processed by the model; however, our inputs are already embedded into numeric"}, {"title": "V. RESULTS", "content": null}, {"title": "V.A. Ablation Study", "content": "An ablation study [11] was performed on the model to ensure that the hyper-parameters\nused for training were the optimal values for this application. These studies are regularly used in\nmachine learning to examine the effect of modifying each hyper-parameter within a given range\nand determine its impact on model performance. All studies trained the model for 2100 epochs,\nstepping down the size of the learning rate every 700 epochs. Figure 3 shows the validation loss\nof the model across 2100 epochs when a number of different initial learning rates were used. The\nminimum loss value from these candidates arose from a learning rate of 0.5; this learning rate gave\nus a minimum loss value of 27.04 using a Mean Squared Error (MSE) loss function.\nFigure 4 shows the validation loss of the model when a number of different learning rate\nstep factors were used. These step factors scale the learning rate as training continues, allowing"}, {"title": "V.B. Fresh-Fuel SMR Results", "content": null}, {"title": "V.B.1. Model Performance", "content": "Figure 7 shows the training and validation losses of the model's training run over 3000 epochs\non four AMD MI-250X GPUs; this training run lasted approximately six hours on this hardware.\n\nThe model started this run with a learning rate of 0.5, reducing by a factor of 0.5 every 1000\nepochs. This training run used a Mean Absolute Error (MAE) loss function, shown in equation 6:\n\n$MAE(x) = \\frac{1}{n}\\sum |x_n - y_n|$ (6)\n\nwhere x is the model's prediction, and y is the target value. The model achieved a minimum\nvalidation loss of 3.42; this loss was achieved on non-normalized output targets, with a range of\nvalues in the single-digits to hundreds, which leads to larger loss values than traditionally seen in\nmachine learning applications. The model's accuracy was also measured with an $R^2$ score; across\nthe entire validation set, the model achieves an $R^2$ score of 98.23%. The model was trained on\n2000 datapoints; each point represents an input/output pair of an SMR domain decomposition\nand its corresponding computation load, as described in section IV.B.\nOne other area of performance to consider is the runtime of the model itself; the model must\nbe able to generate predictions at the same speed as small-scale simulations or faster in order to\nbe useful to researchers. On average, the small-scale simulations used in the model's dataset took\n112.5 seconds to complete. Comparatively, the Transformer model takes 55 seconds to generate\npredictions; however, the majority of this time is spent initializing the model and loading saved\nweights from training. Once initialized, this model can generate predictions in under a second;"}, {"title": "V.B.2. Simulation Runtimes", "content": "The following results focus on three resolutions of domain decomposition that would real-\nistically be used in a k-eigenvalue simulation: 8, 16, and 32 subdomains. Multiple simulations\nwere performed for each of these resolutions, all with the subdomain divisions placed at different\npositions along the x-, y-, and z-axes."}, {"title": "V.C. SMR Fuel Variants", "content": "To demonstrate the flexibility of the model on changes to the problem being simulated, several\nvariations of the SMR domain were created, and the model's performance on these domains was\nanalyzed. No additional training was performed on the model to generate these predictions; the\nsame training results generated in section V.A were used for these simulations, and a matching\nrepresentation was simply used as input to this version of the model."}, {"title": "V.C.1. Depleted Fuel", "content": "Figure 9 shows the results of a suite of large-scale depleted fuel SMR simulations, comparing\nthe model's predictions to those generated by a small-scale simulation of the problem at several\nsubdomain resolutions. As the figure shows, the model's predictions modestly outperform the\nsmall-scale predictions on this problem, achieving a speedup of approximately 2.3%. This perfor-\nmance increase is most prominent in simulations with a higher number of subdomains; notably,\nthe processor allocations generated by the small-scale predictions led to a crash in the large-scale\nsimulations in both the 16- and 32-subdomain cases. The model's predictions seemingly generated\nmore stable processor allocations, which were able to complete a full large-scale simulation. Due\nto the large amount of memory required to simulate the depleted fuel problem, it is possible that\nthe current load prediction method is not sufficient for a domain of this complexity; this issue is"}, {"title": "V.C.2. Shuffled Fuel", "content": "A suite of shuffled fuel SMR domains were created by randomly assigning one of the three\nfuel assembly types found in the fresh-fuel SMR domain to each assembly space in the SMR\ncore architecture. To ensure that this random assignment did not introduce a bias for either"}, {"title": "V.D. C5G7 Results", "content": "In order to show the model's performance on other types of problem geometries, a new\ndataset was created for the C5G7 domain. This dataset was generated using a similar method\nto the one described in Section IV.B; however, due to the structure of the C5G7 domain (seen in\nFigure 11), the domain was instead divided into a 6x6x6 grid of elementary volumes. The model\narchitecture described in Section IV.C was similarly modified to account for this new input size."}, {"title": "V.D.1. Model Performance", "content": "Figure 7 shows the training and validation losses of the model's training session over 4000\nepochs on four AMD MI-250X GPUs; this training run lasted approximately five hours on this"}, {"title": "V.D.2. Simulation Runtimes", "content": "Similarly to the results seen in section V.B.2, these results focus on the large-scale cycle\nruntimes of a suite of 8, 16, and 32-subdomain C5G7 simulations, using 10 inactive cycles followed\nby 10 active cycles, with an initial particle count of 50 million, executed on 40 compute nodes\non Lassen (800 MPI ranks). Figure 13 shows the average cycle times of the test simulations\nwith a resolution of 8 subdomains. The model's predictions generated processor allocations that\ncompleted at 99.3% the speed of simulations using the small-scale results. Much like the SMR\nsimulations, we see similar results across all subdomain resolutions; in the 16-subdomain suite, the"}, {"title": "V.E. Parameter Sensitivity", "content": "Our final study of the model's capabilities was to examine its sensitivity to changes in simu-\nlation parameters; in other words, to see if it has a robustness to these changes that is not present\nin the current methodology, as discussed in section III. It should be noted that this robustness\nrepresents a major challenge, due to the wider variance in computation load created by modifying\nthe parameters of these simulations. In order to test this robustness, we added a set of small-scale\nfresh-fuel SMR simulations to the dataset which contained a varying number of processors, cycles,\nand particle histories. This dataset was used to re-train the original model, using its pre-existing\nweights as a starting point for fine-tuning. As this re-training was performed, it became clear that\nthe model needed to be given the simulation parameters as part of its input in order to be able to\nmake accurate predictions. This led to a new challenge: adjusting the model architecture in a way\nthat retains the training that has been done, while incorporating the simulation parameters into"}, {"title": "VI. CONCLUSION", "content": "In this paper, we sought to train a Transformer model on domain-decomposed SMR simula-\ntions to generate processor allocations without the need to run a small-scale simulation. In that\nregard, this model was largely successful; large-scale simulations using the predictions generated\nby the model ran at roughly 98% the speed of simulations using state-of-the-art load prediction\nmethods, while skipping the small-scale simulation step entirely. These predictions were generated\nmore quickly than the current methodology, providing an estimation of computational load with\nsimilar accuracy at half of the overall runtime, even when accounting for time taken to initialize\nthe model and load its trained weights. In addition, on several of the domain decompositions\ntested in section V, the model's predictions surpassed the current state-of-the-art methodology,\nleading to faster runtimes. A low number of particle histories were used in the simulations that\ncomprise the model's dataset, to more evenly compare the model's predictions to those generated\nby the current methodology; however, the model's accuracy on large-scale simulations could likely\nbe improved by training on simulations with a higher number of particle histories. Currently, the\nmodel does not appear to be robust enough to predict accurately on simulations with a variety\nof parameters; after a preliminary study, it was determined that more work will be needed in the\nfuture to improve this robustness.\nFurther developments could include the creation of a \"domain-agnostic\" version of the model,\nwhich can accept any LWR geometry generated by ORNL's VERA tool. Once such a model is\nsuccessfully created, a method of selecting an optimal domain decomposition from a given problem\ninput naturally follows, given this model will have the necessary data to make that decision for an\nend user without the need for \"guess-and-check\". Additionally, there are a number of Monte Carlo\nparticle transport codes that simulate problems and domains similar to those seen in Shift; this\nmodel could easily be transferred to other code bases and re-trained for their specific applications.\nThe model may also be able to generate predictions on other particle transport problems, such as\nfixed-source or charged particle simulations, without significant additional work.\nLastly, the processor allocation algorithm used in this project may merit re-evaluation after"}]}