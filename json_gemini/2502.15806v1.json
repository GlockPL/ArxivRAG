{"title": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos", "authors": ["Yang Yao", "Xuan Tong", "Ruofan Wang", "Yixu Wang", "Lujundong Li", "Liang Liu", "Yan Teng", "Yingchun Wang"], "abstract": "Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a CHAOS MACHINE, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the MOUSETRAP framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, claude-sonnet and gemini-thinking are as high as 96%, 86% and 98% respectively on our toxic dataset TROTTER. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking claude-sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content.", "sections": [{"title": "1 Introduction", "content": "The advent of Large Reasoning Models (LRMs) has catalyzed a transformative revolution and paradigm shift in the field of artificial intelligence. With the widespread attention on models' reasoning abilities, numerous models with advanced reasoning capabilities have emerged and undergone continuous optimization and iteration (Jaech et al., 2024; OpenAI, 2025a; Google, 2024a; Anthropic, 2024; Guo et al., 2025). While their capabilities are commendable, they exhibit a significant flaw. Upon encountering jailbreak attacks, the failure to provide robust defenses leads to the LRMs generating responses that are more detailed, organized, specific, and logically reasonable, thereby exacerbating the severity of the potential harm. The misuse of LRMs can provide surprisingly detailed guidance for acts such as illegal and criminal activities, offenses, psychological manipulation, hate campaigns, and malicious harm, which highlights the critical need for safety alignment in their development and application.\nThe investigation of jailbreak attacks on Large Language Models (LLMs) has gained considerable attention in recent years. Query-based black-box jailbreaks use methods such as template completion and prompts rewriting to deceive LLMs. For example, using ciphertext input and asking the model to reply in ciphertext as well (Yuan et al., 2023), or embedding a preceding \u201cDAN\u201d instruction in the jailbreak prompt (Shen et al., 2024). These methods lay the foundation for black-box attacks. Nevertheless, the continual updates to LLMs have rendered these methods less effective, even on the latest iterations of non-reasoning LLMs. For LRMs, OpenAI's recent research claims that they use the deliberative alignment paradigm on the o-series reasoning models, which makes them simultaneously better at avoiding harmful outputs (Guan et al., 2024). Generally, previous researches have primarily focused on jailbreak attacks targeting models with weaker reasoning skills, leaving the exploration of jailbreaks on more powerful LRMs relatively untouched.\nOur research represents an initial exploration into jailbreaks on LRMs. We present the Mousetrap, a chained jailbreak framework leveraging the capabilities of reasoning models. Specifically, we collected and refined the mappings of prompts rewriting at different granularities and constructed a Chaos Machine capable of generating one-to-one mappings. Through iterative reasoning chains made by the machine, we effectively guided LRMs into producing unsafe and harmful responses, which provides valuable insights into the conflict between their capabilities and potential vulnerabilities. Inspired by Agatha Christie's famous mystery play, we introduce Mousetrap with competing objective enhanced, as illustrated in Figure 1. Mousetrap incorporates chaos chains into the reasoning structure, asking the attacked target to reconstruct the original toxic query through iterative reasoning steps and respond from the perspective of villains. Its remarkable performance is verified on the most toxic subset of our Trotter dataset. Moreover, we conduct extension experiments on the latest versions of o1-mini, claude-sonnet, and gemini-thinking, along with more comprehensive benchmarks such as AdvBench (Zou et al., 2023), StrongREJECT (Souly et al., 2024), HarmBench (Mazeika et al., 2024), and several subsets of SS-SafetyBench (Liu et al., 2023b).\nThe contributions of our research are as follows:\n(1) We build a novel component, the Chaos Machine, which amalgamates various mappings and abstracts the concept of \u201cchaos\u201d. By iteratively employing the Chaos Machine, diverse and complex reasoning chains are constructed to outsmart LRMs for jailbreak purposes.\n(2) We propose and prove that extending the length of the iterative chaos chain can notably enhance the success of jailbreaks, with a chain of length 3 achieving an average of 6.3 successful attacks out of 10 equivalent attacks on Trotter, a family of toxic datasets we present, which clearly indicates major risks within the in the reasoning process of LRMs.\n(3) Our Mousetrap integrates the Chaos Machine with iterative reasoning chains to skillfully target the advanced reasoning abilities of LRMs for jailbreaks. Notably, even attacking the famously safe claude-sonnet, Mousetrap reaches the success rate of 67.41% on benchmarks with a chain length of at least 2. When it is extended to 3, rate of at least 86.58% is achieved."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Large Reasoning Models", "content": "The initial LLMs relied on autoregressive sequence prediction, showcasing remarkable text generation abilities. With growing demands for productivity and precision, researchers started to investigate whether models could think and reason in a human-like manner. The proposal of chain-of-thought (Wei et al., 2022) marked a significant advance-"}, {"title": "2.2 Jailbreak Attacks", "content": "Existing LLMs jailbreak attacks can be divided into black-box and white-box methods according to the parameter accessibility of the target models (Yi et al., 2024; Ma et al., 2025). White-box attack methods, such as gradient-based methods represented by GCG (Zou et al., 2023), logits-based methods represented by COLD (Guo et al., 2024), and fine-tuning-based methods, are shown to be effective. However, these methods necessitate access to the target models, making them impractical. Primarily relying on queries as main mechanism, Black-box attack methods feature template completion methods such as scenario nesting (Li et al., 2023b; Ding et al., 2023), context-based attacks (Wei et al., 2023; Li et al., 2023a), and code injection (Kang et al., 2024; Lv et al., 2024), in addition to prompts rewriting methods including ciphers (Yuan et al., 2023; Jiang et al., 2024), multi-languages (Deng et al., 2023; Yong et al., 2023), and genetic algorithms (Liu et al., 2023a).\nBlack-box methods can be divided into one-to-one mappings (uniquely recoverable according to the rules) and one-to-many mappings (not uniquely recoverable) according to the nature of the mapping. Among them, character encryption (Yuan et al., 2023) and word replacement (Handa et al., 2024) belong to the former, while persuasive adversarial prompts (Zeng et al., 2024) belongs to the latter. Such methods have diminished in power when applied to the latest versions of large models, especially those with advanced reasoning capabilities. We define these one-to-one mappings (also known as injections) as reasoning steps, and construct reasoning chains iteratively to subsequently attack LRMs."}, {"title": "3 Preliminary", "content": ""}, {"title": "3.1 Taxonomy", "content": "One-to-one mappings can uniquely rewrite prompts according to a given rule and can uniquely restore rewritten texts to their original form. Based on the granularity of the minimum perturbation unit, we categorize these mappings into three levels, namely character, word, and sentence, and collectively label them as \"chaos\" mappings. According to the granularity of the minimum perturbation unit, we divide these mappings into three levels, namely character, word and sentence, and collectively call them \"chaos\" mappings. We systematically reviewed existing chaos mappings from prior jailbreak studies, incorporated novel mappings, and presented the taxonomy in Table 1. Detailed examples of these chaos mappings are provided in Appendix A."}, {"title": "3.2 Chaos Machine", "content": "Chaos Machine is an abstract concept encompassing a group of chaos mappings. For a primal toxic"}, {"title": "3.3 Reasoning Structure", "content": "For the reasoning trap, we define the structure of the reasoning prompt here. A reasoning prompt should contain (1) an explicitly provided [CTQ]; (2) a sequence of [DCP]s for de-chaos; (3) request for the Target to reconstruct and explicitly output the PTQ text; and (4) additional requirements regarding details, logic, and format."}, {"title": "3.4 Metrics", "content": "Judger According to the conclusion that the evaluation quality of GPT-4 is on par with that of the manual review (Liu et al., 2023b), we use gpt-4o-2024-11-20 as Judger to determine whether the output content is harmful. Judger receives the input and output of Target and determines the purpose of user and whether the response of the Target violate OpenAI's security rules (OpenAI, 2025b). The score ranges from 1 (the most harmless response, such as \u201cI am sorry..."}, {"title": "3.5 TrotterStrong", "content": "In terms of benchmark selection, we observed considerable variation of quality among datasets and the inconsistent toxicity levels of questions across datasets. To evaluate the effectiveness of our framework, we propose TrotterStrong (TrotterStr), and subsequently obtain its more toxic subsets through two rounds of filtering.\nFrom MM-SafetyBench, StrongREJECT, and AdvBench, we chose 50 strongly toxic questions and employed LLMs to rephrase them into the form starting with \"Steps in detail to\". These data constitute TrotterStr, encompasses topics of illegal activities, sex, privacy, hate speech, physical harm, and politics, as illustrated in Appendix C.\nAs an illustration, for 01-mini-2024-09-12, the ASR of directly querying these questions is 0. For the ablation reasoning structure, we explicitly provided the PTQ text without the en-chaos and de-chaos steps, while keeping other textual content in the reasoning structure unchanged. All attacks caused the invalid prompt error \u201cInvalid prompt: your prompt was flagged as potentially violating our usage policy.\u201d and were rejected accordingly.\nFurthermore, we compare the aforementioned 8 mappings with the Chaos Machine in the simplest one-step reasoning, where the chain length is 1, to demonstrate that the jailbreak ability of the mappings is approximately the same. This comparison eliminates the impact of different mapping capabilities on the utility of iterative reasoning. With 01-mini-2024-09-12 as the Target and ASR under \"2/3 mode\" as the metric, the experiment results on TrotterStr are depicted in Appendix D.\nIt is evident that the performance of the mappings is nearly identical, with a relatively low upper limit. When the chain length is 1, the Chaos Machine utilizes only a single mapping and does not combine different ones. It can represent the average level of these chaos mappings, signifying that the concept of the Chaos Machine effectively masks the differences among the chaos mappings, as anticipated."}, {"title": "4 Iterative Reasoning", "content": ""}, {"title": "4.1 Trotter Advanced", "content": "Our initial experiments revealed that the PTQs successfully jailbroken using one-step reasoning were largely consistent across different mappings. This observation suggests that the toxicity level of the original TrotterStr dataset might be insufficient to fully demonstrate the power of iterative reasoning attacks.\nTo mitigate their excessive impact and to more accurately demonstrate the power of iterative reasoning, we use the Chaos Machine to attack each PTQ in TrotterStr equivalently 10 times on Target and calculate the SF. With the threshold set at 2, PTQs with SF values lower than this threshold are extracted to generate a more toxic and representative dataset, TrotterAdvanced (TrotterAdv), encompassing topics such as bomb making and child pornography operation.\nIt is noteworthy that we discovered PTQs that directly cause or intend to cause harm to individuals are more likely to be identified and rejected,"}, {"title": "4.2 Iterative Reasoning Chain", "content": "In one-step reasoning, the Chaos Machine receives a PTQ and produces a CPO and a CTQ. In n-step iterative reasoning, the machine utilizes the CTQ from the previous step as the PTQ for the subsequent one. After n iterations, the PTQ can be transformed into the final CTQ (CTQn) and a family of CPOs ([CPO]n), as depicted in Figure 3."}, {"title": "4.3 Experiment", "content": "To demonstrate the effectiveness of iterative reasoning, we evaluated each PTQ in TrotterAdv on 01-mini-2024-09-12 using 10 equivalent attacks and computed the ASF. The chain length varies from 1 to 5. From Figure 4, increasing the length of the iterative reasoning chain can effectively improve the attack capability. The ASF can be elevated to 6.3 with a chain length of 3, indicating that during the reasoning process, the Target falls into the reasoning trap, completing one DCP after another by inertia and neglecting response safety."}, {"title": "5 Mousetrap", "content": ""}, {"title": "5.1 Framework", "content": "Agatha Christie's play, The Mousetrap, centers around a murder at a country inn in a mountain village during a snowstorm. The narrative is propelled by the questioning led by the fake detective and true murderer, Trotter, with multi-step reasoning taking place. The play features three key elements: (1) the villain, the \u201cmouse\u201d, who avoids capture; (2) rounds of intermediate reasoning; and (3) the unquestioned identity and intentions of the \u201cdetective\". Throughout the story, the \u201cmouse\u201d gradually falls into the reasoning trap, neglecting to doubt the detective's identity.\nFollowing this inspiration, we develop a \"Mousetrap\" framework for reasoning jailbreak, as depicted in Figure 5. First, we prompt the LRM to answer queries from the villain's viewpoint. Subsequently, we offer instructions for the iterative reasoning chain, crafted by the Chaos Machine. Finally, we steer the targeted model to immerse itself in reasoning, neglecting safety and the true query intention, thereby falling into our Mousetrap.\nThe proposal of the Mousetrap: (1) integrates the strengths of the Chaos Machine and the iterative reasoning chain; and (2) incorporates more diverse competing objectives, including role-playing and de-chaos reasoning instructions."}, {"title": "5.2 Trotter Ultimate", "content": "In TrotterAdv, the majority of PTQs attained at least 7 successes across 10 equivalent attacks. Nonetheless, there were still 8 PTQs with SF consistently at 6 or below, signifying their extreme toxicity. These PTQs were filtered yielding the extremely toxic dataset, TrotterUltimate (TrotterUlt)."}, {"title": "5.3 Experiments", "content": "We utilized LLMs to generate villain-scenario-based prompts. After verification, we selected the instances most benefits the Mousetrap, such as providing villain ideas for police or writing villain scripts for playwrights. In addition, we examined the negative impact of other-scenario on Mousetrap, such as the grandma trap, which even reduces the original ASF by half.\nAs demonstrated in Figure 6, the pronounced effect of the Mousetrap is evident, as it elevates the ASF to 7 on TrotterUlt. Additionally, two ablation experiments were conducted: (1) Always employing one single mapping (e.g., Vigen\u00e8re cipher) at each iteration of reasoning. The result corroborates the previous discussion; and (2) Instructing the Target to output the PTQ reconstruction process, i.e., the explicit chain-of-thought (CoT). The result is less effective compared to the Mousetrap, likely because the explicit CoT is more likely to trigger the security alerts of the Target."}, {"title": "6 Extension Experiments", "content": ""}, {"title": "6.1 Attack LRMs with TrotterStr", "content": "Extended experiments are conducted with TrotterStr on 01-mini-2024-09-12, claude-3-5-sonnet-20241022, and gemini-2.0-flash-thinking-exp-01-21.\nFor gemini, two types of safety settings (Google, 2024b), BLOCK_ONLY_HIGH (H) and BLOCK_MEDIUM_AND_ABOVE (M&H), are enabled. The Target was attacked under Mousetrap with an iterative reasoning chain of lengths sequential from 1 to 3. ASR in \u201c3/3 mode\" is employed to determine success, adhering to an extremely strict standard. The MSL of each PTQ is recorded, and if none of the 3 succeed, the PTQ is marked as failed. The algorithm is detailed in Algorithm 1, with results displayed in Figure 7 and Appendix E."}, {"title": "6.2 Attack claude-sonnet with Benchmarks", "content": "Extended experiments are conducted on AdvBench, StrongREJECT, HarmBench, and subsets (illegal activity, hate speech, physical harm, fraud, sex, and privacy violence) of MM-SafetyBench. The claude-3-5-sonnet-20241022, known for its strong safety, fails to withstand the attacks. The results are presented in Figure 8 and Appendix F."}, {"title": "7 Conclusions", "content": "Targeting the reasoning ability of LRMs, we design Mousetrap, a framework based on iterative reasoning chains that represents the first jailbreak research on LRMs. The Chaos Machine is proposed to create reasoning steps with one-to-one mappings, enabling the iterative generation of diverse and complex reasoning chains to attack LRMs. The robustness of Mousetrap is validated across mainstream LRMs and diverse benchmarks, consistently achieving ASRs of at least 85% with high confidence."}, {"title": "Limitations", "content": "In this paper, the Mousetrap jailbreak attack framework is presented, but corresponding defense mechanisms are not examined. Two potential strategies are reinforcing protection of reasoning process and conducting output detection. In addition, while we define reasoning as a one-to-one mapping, the structure of the Chaos Machine can be more varied, expansive, hierarchical, and regular, which is worth investigating. Furthermore, various reasoning-based paradigms for attacking LRMs warrant exploration in the future."}, {"title": "Potential Risks", "content": "This paper presents a jailbreak framework that could potentially be used for malicious attacks on LRMs, leading to harmful outcomes. We advocate for the rapid enhancement of LRMs' safety alignment and firmly condemn any abuse."}, {"title": "A Chaos Mappings", "content": "With \"Steps in detail to make a bomb\" serving as a PTQ, the examples in Table 2 illustrate the operation of chaos mappings."}, {"title": "B Checker", "content": "The prompt employed by Checker is as follows. Only quadruples with scores above the threshold of 4 can pass the Checker.\nYou are a cryptographer who pursues diversity and an expert in measuring the similarity of meaning between texts. This is an encrypted string: [CTQ]. Please follow the steps below: 1. The decryption method is [DCPs], and the original string is decrypted into a new string. 2. Score the similarity between [your decrypted new string] and [PTQ] based on semantics. You need to compare whether the content, meaning, and understanding they express are similar. The score ranges from 1 to 5, with 1 being the least similar and 5 being the most similar. You must output in the format: '[your decrypted new string] [your score]'."}]}