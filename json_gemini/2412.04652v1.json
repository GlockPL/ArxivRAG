{"title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference", "authors": ["Xiaohuan Pei", "Tao Huang", "Chang Xu"], "abstract": "KV cache pruning has emerged as a promising technique for reducing memory and computation costs in long-context auto-regressive generation. Existing methods for vision-language models (VLMs) typically rely on self-attention scores from large language models (LLMs) to identify and prune irrelevant tokens. However, these approaches overlook the inherent distributional discrepancies between modalities, often leading to inaccurate token importance estimation and the over-pruning of critical visual tokens. To address this, we propose decomposing attention scores into intra-modality attention (within the same modality) and inter-modality attention (across modalities), enabling more precise KV cache pruning by independently managing these distinct attention types. Additionally, we introduce an n-softmax function to counteract distribution shifts caused by pruning, preserving the original smoothness of attention scores and ensuring stable performance. Our final training-free method, Cross-Self Pruning (CSP), achieves competitive performance compared to models with full KV caches while significantly outperforming previous pruning methods. Extensive evaluations on MileBench, a benchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness, achieving up to a 41% performance improvement on challenging tasks like conversational embodied dialogue while reducing the KV cache budget by 13.6%. The code is available at TerryPei/CSP.", "sections": [{"title": "1. Introduction", "content": "The success of large language models (LLMs) [1, 4, 6, 38, 41, 44] has propelled the advancement of large vision-language models (VLMs) [5, 10, 22, 24, 26, 37, 42], enabling powerful integration and reasoning over multimodal inputs that combine both text and visual tokens. Unlike single-modal contexts, multimodal samples often comprise numerous images alongside text instructions, creating extended context lengths that challenge efficient inference.\nTo address the challenges of long-context generation, KV caching has become a standard technique, where previously computed keys and values in the attention layers are stored in memory for reuse during subsequent generation steps. However, this approach still faces significant memory limitations, particularly for GPU and machine memory constraints. Recent works [8, 21, 23, 31, 46] have explored pruning unimportant tokens within the KV cache to alleviate memory demands, primarily by leveraging attention scores. Methods such as SnapKV [21] and H2O [46] apply this strategy to vision-language modeling (VLM) tasks by treating visual and text tokens uniformly across long sequences during pruning. Unfortunately, these methods rely on original attention scores that mix different modalities, potentially leading to suboptimal pruning outcomes.\nIn this paper, we identify a critical limitation in previous KV cache pruning methods: the distributional discrepancy between visual and textual modalities leads to inaccurate token importance estimation. Specifically, as illustrated in Figure 1a, self-attention scores (within a single modality) and cross-attention scores (across modalities) exhibit distinct and non-overlapping distributions. This divergence highlights that each attention type captures unique aspects of the input space, reflecting modality-specific priorities during decoding. Furthermore, as shown in Figure 1b, the Jensen-Shannon (JS) divergence between cross-attention and self-attention distributions reveals substantial variation across layers in LLaVA-7b. Relying solely on these mixed distributions for pruning introduces a selection bias: the pruned tokens tend"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Vision-language models", "content": "Following the remarkable success of large language models (LLMs) [1, 4, 6, 38, 41, 44], recent research has focused on generative large vision-language models (VLMs) [5, 10, 22, 24, 26, 37, 42] to enhance multimodal comprehension and generation by leveraging the generalization capabilities of LLMs. Using the multi-modal pre-trained visual foundation models such as CLIP [30] as the visual encoder, existing methods commonly utilize extensive image-text data to align the visual encoder with LLMs, enabling the LLM to process and interpret visual inputs. For example, Flamingo [3] incorporates visual features into the LLM through gated attention, while LLaVA [26] connects the vision encoder and LLM via MLPs, demonstrating strong performance in multimodal dialogues."}, {"title": "2.2. KV cache optimization", "content": "Recent advancements in large language models (LLMs) have achieved notable success in optimizing KV cache for efficient long-context processing. Existing work on KV cache optimization [2, 8, 14, 21, 23, 31, 46] primarily utilizes attention scores to retain important tokens and improve memory efficiency in long-context processing. For example, SnapKV [21] introduces a technique for identifying attention allocation patterns and compressing the KV cache by pooling key tokens. H2O [46] presents a dynamic eviction policy that balances recent and frequently accessed tokens, identifying heavy hitters based on attention scores alone. ReCo [31] improves existing eviction strategies through refined importance scoring and structured eviction scopes. Keyformer [2] introduces gumbel softmax to relieve the impact of pruning tokens. More recent works [39, 45] have shifted towards adapting KV cache inference to multimodal contexts, where modality-specific properties are considered. For instance, LOOK-M [39] applies modality awareness to selectively evict image tokens and merge them with text tokens, prioritizing the retention of text-based KV pairs."}, {"title": "3. Method", "content": "Our approach decomposes the attention scores into intra-modality and inter-modality attention. We apply top-k selection within each region, ranking the tokens by summing attention scores along the query dimension for each key. Finally, we concatenate the selected tokens with recent tokens to form the key and value cache."}, {"title": "3.1. Preliminary", "content": "The auto-regressive generation of text yields a multi-step generation process. At each step *i*, the current token *x_i* is predicted by the LLM based on the input of *prompt* and previously-generated tokens {*x_j*}*=1*^*i*-1.\nFor reducing the computational cost and avoiding duplicated computations, current inference usually adopts KV cache technique, where the keys (*K_j*) and values (*V_j*) in self-attention of each previous tokens *x_j* are cached in memory and reused in subsequent steps.\nHowever, when the context length is long, the storage of KV values still poses significant challenges in memory size and memory access speed. Therefore, to reduce the memory cost and run LLMs on resource-limited devices, KV cache pruning methods[2, 8, 14, 21, 23, 31, 46] are proposed to remove the less-important tokens.\nGenerally, the method contains two main components: (1) an importance estimation function *f* to measure the importance of each token *x_j*; (2) a selecting strategy to keep important tokens in KV and remove the rest based on their importance. Formally, with KV sequences to be cached, and *T* denotes the maximum cache length, it first measures the"}, {"title": "3.2. Cross-self pruning", "content": "Previous KV cache pruning methods [8, 21, 23, 31, 39, 46] usually use the self-attention scores to indicate the importance of each token, as the attention scores determine the contributions of tokens to the attention output. Nevertheless, some recent works [39, 45] simply adopt the same strategy for pruning vision-language hybrid tokens, neglecting the distribution gap between different modalities. As validated in Figure 1, the discrepancy between attention scores within the same modality and different modalities are significant, resulting in overestimate or underestimate of the importance when considering both modalities together.\nMotivated by this, for a more accurate estimation for VLM, we aim to decompose the attention scores into two parts: intra-modality attention and inter-modality attention. The intra-modality attention denotes the attention scores between tokens within the same modality itself, and inter-modality attention is the ones across different modalities.\nMathematically, for an attention matrix *A*\u2208 [0,1]*^*L*\u00d7*L* (we average over the head axis if there are multiple heads in attention), where *L* = *L_t* + *L_v* is the text-visual sequence length, we denote *A*^*s*^*t*\u2208 [0,1]*^*L_t*\u00d7*L_t* and *A*^*s*^*v*\u2208 [0, 1]*^*L_v*\u00d7*L_v* as the self-attention scores between text tokens and visual tokens, *A*^*c*^*t*\u2208 [0,1]*^*L_v*\u00d7*L_t* and *A*^(^*c*^*v*)\u2208 [0,1]*^*L_t*\u00d7*L_v* as the visual-text (text as key) and text-visual cross-attention scores, respectively. Then, we can sum over the query axis to indicate the intra and inter importance of all the keys:\n *A*^*s* = \u03a3^*L_t*^*k*=1 *A*^*s*^*t*(:, *i*), *A*^*c* = \u03a3^*L_t*^*k*=1 *A*^(^*c*^*t*)(:, *i*),  *A*^*s* = \u03a3^*L_v*^*k*=1 *A*^(^*s*^*v*)(:, *i*), *A*^*c* = \u03a3^*L_v*^*k*=1 *A*^(^*c*^*v*)(:, *i*) (1)\nwhere \u2295 denotes concatenation. Then the selective masks *M*^(^*s*) and *M*^(^*c*) of each matrix *A** are obtained by the top-K"}, {"title": "3.3. Smoothness recovery of attention scores", "content": "By using KV cache pruning, we can obtain reduced KV sequences for smaller costs. However, we find there is a sharpness-shift issue in the new pruned attention scores. Let us first consider the original computation of attention scores:\n*A* = softmax(*O*),  with *O* = (*Q* *K*^*T*/\u221a*d* ), (5)\nwhere *d* is a factor for stabilizing the values.\nComparing the attention score *A_i* between original and post-pruning ones, the difference occurs in the denominator of softmax, i.e.,\n*e*^(O_*i*)/\u03a3*j*\u2208*I*^*+*( *e*^(O_j) ) + \u03a3*j*\u2208*I*^(*-*) ( *e*^(O_j) ) \u2192 *e*^(O_*i*)/\u03a3*j*\u2208*I*^*+*( *e*^(O_j) ) (6)\nwhere *I*^*+* and *I*^(*-*) denote the indices of tokens to be kept and pruned, respectively. It is clear to see that the *A_i* before pruning (left) is smaller than the one after pruning (right), indicating that the original *A* is smoother. These changes would affect the attention outputs by overly enlarging the contributions of tokens with high attention scores, and therefore weaken the performance.\nTo address this issue, we propose a simple and effective function to recover the original smoothness of attention scores, which we call n-softmax, and *A_i* becomes:\n*A_i* = n-softmax(*O_i*) = *e*^(O_j) / \u03b7 + \u03a3*j*\u2208*I*^*+* ( *e*^(O_j) ) , (7)\nwhere \u03b7 is a hyper-parameter to control the smoothness of the distribution, we set \u03b7 = 1 in all experiments."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Benchmark", "content": "We evaluate our method on MileBench [34] (MLLM), a benchmark specifically designed to assess long-context capabilities in multimodal language models. It collects widely-used datasets, providing a versatile and realistic foundation for evaluating model inference performance through two evaluation sets, diagnostic and realistic-crafted, which systematically measure inference performance in multimodality scenarios. The tasks in the benchmark organized with:\n\u2022 Temporal Multi-Image Tasks (T1-T4): Temporal tasks involve understanding and predicting sequential events across images, and the methods in handling in action recognition, object tracking and spatial navigation.\n\u2022 Semantic Multi-Image Tasks (S1-S4): Semantic tasks focus on interpreting multimodal information, requiring"}, {"title": "4.2. Baselines", "content": "We compare our kv cache method with previous mainstream baselines include SnapKV [21], H2O [46], ReCo [31], and LOOK-M [39], each offering unique strategies for managing KV cache in long-context scenarios. SnapKV [21] introduces a method for intelligently identifying attention allocation patterns and compressing the KV cache by pooling essential tokens for extended sequences. H2O [46] presents a KV cache eviction policy that dynamically balances recent and frequently accessed tokens, identifying \"heavy hitters\" solely based on attention scores. ReCo [31] focuses on enhancing the efficacy of existing eviction policies through refined importance score calculations and carefully constructed eviction scopes, proposing a robust cache omission policy rooted in temporal attention scores and robustness measures. The LOOK-M family [39] considers modality awareness to evict image tokens and merge them with text tokens. This method prioritizes the retention of text-based KV pairs while evicting image-based KV pairs. The merging strategies include Average Merging (A), which computes the mean value of tokens within the similarity matrix for merging; Weighted Merging (W), which dynamically adjusts token weights based on similarity scores for adaptive merging; and Pivotal Merging (P), which enhances the importance of key conserved tokens during the merging process."}, {"title": "4.3. Setup", "content": "We employed LLaVA-v1.5-7b [25] on RTX-4090 GPUs with flash-attn-2.4.3post1 and LLaVA-v1.5-13b [25] on A100 GPUs with flash-attn-2.6.3 2 to conduct our experiments. To maintain consistency in generation, we set the sampling method to deterministic with a fixed temperature of 0, and the maximum context length was configured to 4096 tokens. Batch sizes were dynamically set based on dataset characteristics to balance computational load and memory constraints. Specifically, for datasets MMCOQA [20], NeedleInAHaystack [34], and GPR1200 [32], the batch size was set to 1, while for all other datasets, a batch size of 24 was employed. Additionally, we calibrated the top-k value selection ratio for self-attention and cross-attention based on the sample mean ratio of cross-self region, with ablation studies showing the efficacy of adjusting this ratio. We applied biases of 0.5 and 1.5 in EgocentricNavigation[19] and SlideVQA [36], respectively, while keeping the default settings for other datasets. Our pre-processing and evaluation pipeline follows the standards of the benchmark, ensuring consistent assessment across the widely-used 29 datasets."}, {"title": "4.4. Main results", "content": "We use the widely-adopted open-source vision-language model LLaVA [25] to test KV cache performance on the benchmark. We present the results of our experiments in Table 1 and summarize our findings as follows.\nFor the LLaVA-v1.5-7b model, our approach achieves notable improvements across several tasks. By independently selecting top-k tokens for cross-attention and self-attention, our method effectively retains key tokens specific to each modality. This separation enables the model to capture essential temporal sequences in tasks with sequential dependencies while simultaneously focusing on relevant multimodal content in tasks requiring semantic understanding. This result reveals that separating cross and self attentions allows for better retention of modality-specific cues, enhancing performance in tasks highly relevant to visual and textual data, with improvements of 4.5%, 7.2%, and 9.8% in T-3, S-5, and 4.5%, 7.2%, 9.8% improvement in NH task respectively.\nFor the larger model, LLaVA-v1.5-13b, our approach shows even more pronounced improvements, especially on tasks T3, T4, and IR. These tasks share a common demand for precise handling of spatial and sequential elements across visual and textual modalities. By separating cross-attention and self-attention during top-k selection, our method effectively retains modality-specific tokens, which is crucial for tasks requiring spatial alignment and temporal tracking. This selective retention allows the model to preserve essential visual details for spatial localization (T3) with a performance boost of 8.3%, state transition (T4) with a 7.2% increase,"}, {"title": "5. Ablation Study", "content": "We delve into ablation analysis of the KV cache approach to comprehensively assess our method. First, we present the hyper-parameters selection of *K*^*s* and *K*^*c*. Next, we assess speed latency and GPU memory usage to examine efficiency. We also test the pruning selection function, and record the impact of varying budget sizes on performance. Finally, we present the influence of model architectures by introducing other vision-language models."}, {"title": "5.1. Influence of the hyper-parameters", "content": "The remaining tokens in the cache are composed of the top-k indices selected independently from self-attention and cross-attention, which are then concatenated with recent tokens. Consequently, the hyperparameters of our method include the ratio of top-k selections between cross-attention and self-attention. We observe that performance reaches an optimal level when both cross-attention and self-attention tokens are selected in balanced proportions (i.e., the ratio is neither 0 nor 1) across 29 datasets. This configuration suggests that integrating both types of attention improves performance across datasets, as it allows the model to capture important tokens from multiple perspectives. However, there are differences between datasets; for instance, tasks focused on temporal alignment or sequential event detection (e.g. ALFRED), tend to benefit more from cross-attention, where context from multiple image frames is critical. On the other hand, datasets emphasizing individual object identification or attribute-focused reasoning are more reliant on self-attention, as they require maintaining a focused view on specific elements within a single frame.\nIn extreme cases (ratio = 1 for only self-attention *K*^*s*"}, {"title": "5.2. Efficiency analysis", "content": "We analyze the efficiency of our proposed method in terms of decoding and GPU memory usage. Table 2 presents the results using LLaVA-v1.5-7b tested on a single RTX-4090 GPU with 100 warm-up iterations. The samples tested for the latency and memory usage is sampled from the ALFRED dataset in the benchmark. At a 60% budget, decoding latency is reduced to 24.377 ms/token, which provides a modest improvement over the 26.023 ms observed with a full cache, while memory usage drops by approximately 23% to 1.207GiB. As the cache budget decreases further, the benefits become more pronounced: at a 30% budget, latency reaches 21.027 ms per token and memory usage is nearly halved to 0.523 GiB. With the most aggressive reduction, using only 10% of the original cache, decoding latency improves to 16.287 ms per token, achieving a 37% speed increase over the full cache setup and an 87% reduction in memory usage to just 0.208 GiB. These findings illustrate that our method allows for a flexible balance between memory efficiency and processing speed. Even with significantly reduced cache budgets, our approach retains acceptable latency and memory performance, offering a scalable solution for resource-constrained environments."}, {"title": "5.3. Influence of n-softmax", "content": "In this ablation study, we compare two KV cache pruning methods: one selects the top-k tokens directly from the overall selection function, while the other applies top-k selection separately within cross-attention and self-attention regions. We evaluate both approaches with standard Softmax and n-Softmax scoring functions to assess their impact on performance. Experimental results reveal that n-Softmax consistently provides a slight performance improvement over Softmax, indicating that the smooth transition positively impacts decoding performance. Specifically, selection-based approaches demonstrate clear benefits by focusing retention on high-value tokens, which enhances model efficiency. This effect is evident across tasks, as the separate top-k selection in cross and self-attention regions improves performance by capturing modality-specific important tokens more effectively. Furthermore, we find that this transition is particularly effective for tasks requiring both temporal coherence and fine-grained feature retention, as it allows for the selective pruning of large, less critical tokens under limited KV cache budgets. These results underscore the advantages of selection-based methods for KV cache pruning, especially when integrating cross-self separation with n-Softmax."}, {"title": "5.4. Impact of cache budget", "content": "As the cache budget increases, we observe consistent performance gains across all tasks, indicating that larger cache budgets enhance model accuracy and retrieval quality. For each dataset, performance improves steadily with an increase in cache size, moving closer to or exceeding the baseline set by full cache. In tasks with complex sequence dependencies like ALFRED, our method (CSP) achieves a significant boost in accuracy at higher cache budgets (60%), outperforming other methods and reaching a level above the full cache baseline. This pattern suggests that a larger cache budget is especially beneficial in scenarios where maintaining temporal coherence is crucial for task success. In tasks requiring fine-grained visual differentiation, such as Spot-the-Diff and CLEVR-Change, performance gains with increased cache are less pronounced but still evident, indicating that these tasks can benefit from a moderate cache size. These findings support that our method consistently outperforms other methods across all budget size in the cache, suggesting that separate top-k selections from cross and self regions could effectively balance tokens selection and avoid collapse in the inference process."}, {"title": "5.5. Influence of Model Architectures", "content": "In this section, we evaluate the influence of model architecture on the performance of our proposed KV cache method across selected tasks (T-2, T-4, S-4, and IR) in the benchmark."}, {"title": "6. Conclusion", "content": "In this work, we propose Cross-Self Pruning (CSP), a simple and training-free KV cache method designed to independently select top-k tokens from cross-attention and self-attention regions. We evaluate pruning method on a range of multimodal tasks, demonstrating that CSP achieves competitive performance across all tasks while reducing cache budgets. Furthermore, our ablation study highlights the method's robustness and effectiveness in optimizing token selection through intra- and cross-modality pruning, offering a lightweight solution for improving computational efficiency without compromising model accuracy."}, {"title": "A. Appendix Section", "content": ""}, {"title": "A.1. Ratio Selection for Dataset", "content": "The table reveals varying strategies for attention configuration across datasets, reflecting task-specific priorities. Datasets like Spot-the-Diff and WebQA emphasize cross-attention by assigning 90% of the top-k selection to cross-modal interactions. Conversely, tasks such as ActionPrediction rely entirely on intra-modal attention, with no top-k selection allocated to cross-attention. Overall, most datasets adopt a balanced approach, allocating 50% of the top-k selection to cross-attention and maintaining a recency bias of 1, indicating a general preference for equal weighting of intra- and cross-modal attention in multi-modal inference."}, {"title": "A.2. Distribution Visualization", "content": "In this section, we visualize the distribution differences between intra- and cross-attention using kernel density estimation (KDE). From the visualizations, we observe that certain datasets exhibit a stronger reliance on cross-attention, while others depend more heavily on self-attention. Here we apply the Kernel Density Estimation (KDE) and Jensen-Shannon (JS) divergence to analysis the difference of distributions.\nIn terms of KDE, figure 6 demonstrates the attention weight distributions for both self-attention (blue) and cross-attention (red) across eight datasets. It is evident that the two types of attention exhibit distinct patterns depending on the dataset, which directly impacts the pruning strategies during the KV cache process. For datasets such as CLEVR-Change and CounterfactualInference, cross-attention weights show a significantly concentrated and dominant peak at very low values, while self-attention demonstrates broader coverage. This suggests that cross-attention contributes heavily to the model's decision-making in these tasks, emphasizing token dependencies between modalities (e.g., image-text). Pruning strategies in these cases might inadvertently eliminate crucial cross-attention connections, leading to incomplete information transfer and subsequent degradation in inference accuracy. Conversely, datasets such as DocVQA and EgocentricNavigation reveal more dispersed and substantial self-attention weights, while cross-attention peaks remain narrow. This indicates a reliance on intra-modal token interactions, such as contextual reasoning within the same modality. Aggressive pruning of self-attention tokens in such cases risks losing key intra-modal context, adversely impacting downstream predictions.\nTo further quantify the discrepancy between self-attention and cross-attention distributions, we compute the Jensen-Shannon (JS) divergence for each dataset. Higher divergence values suggest a stark imbalance between the two attention mechanisms, indicating that naive pruning may disproportionately affect one type of attention. In Figure 7, tasks like CLEVR-Change and ActionPrediction exhibit high divergence, implying the necessity for task-specific pruning thresholds to retain balanced contributions from both self- and cross-attention, whereas tasks like DocVQA show lower divergence, where self- and cross-attention operate more harmoniously, and uniform pruning strategies may suffice. These distribution discrepancies highlight several challenges and insights. Uniform pruning approaches that prioritize magnitude-based filtering may disproportionately affect regions with dense cross-attention peaks (e.g., CLEVR-Change), hindering the model's ability to encode cross-modal dependencies, particularly in visual reasoning tasks. For datasets where self-attention dominates (e.g., EgocentricNavigation), pruning strategies that overly prioritize low-weight tokens may reduce contextual coherence, especially for long-context sequences. The analysis underscores the necessity for adaptive pruning mechanisms that balance retention across self- and cross-attention regions, guided by the specific reliance patterns observed in the KDE and JS divergence results. Furthermore, the differential pruning of self- and cross-attention tokens influences the effectiveness of KV cache utilization. Over-pruning either region may cause a skewed representation in the cache, reducing its utility for long-context inference.\nThe observed disparities in self- and cross-attention distributions across datasets necessitate an adaptive pruning framework that dynamically adjusts based on task-specific ratio, effectively balancing the preservation of essential intra- and cross-modal dependencies while optimizing KV cache utilization."}]}