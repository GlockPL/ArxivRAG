{"title": "Are We There Yet? Revealing the Risks of Utilizing Large Language Models in Scholarly Peer Review", "authors": ["Rui Ye", "Xianghe Pang", "Jingyi Chai", "Jiaao Chen", "Zhenfei Yin", "Zhen Xiang", "Xiaowen Dong", "Jing Shao", "Siheng Chen"], "abstract": "Scholarly peer review is a cornerstone of scientific advancement, but the system is under strain due to increasing manuscript submissions and the labor-intensive nature of the process. Recent advancements in large language models (LLMs) have led to their integration into peer review, with promising results such as substantial overlaps between LLM- and human-generated reviews. However, the unchecked adoption of LLMs poses significant risks to the integrity of the peer review system. In this study, we comprehensively analyze the vulnerabilities of LLM-generated reviews by focusing on manipulation and inherent flaws. Our experiments show that injecting covert deliberate content into manuscripts allows authors to explicitly manipulate LLM reviews, leading to inflated ratings and reduced alignment with human reviews. In a simulation, we find that manipulating 5% of the reviews could potentially cause 12% of the papers to lose their position in the top 30% rankings. Implicit manipulation, where authors strategically highlight minor limitations in their papers, further demonstrates LLMs' susceptibility compared to human reviewers, with a 4.5\u00d7 higher consistency with disclosed limitations. Additionally, LLMs exhibit inherent flaws, such as potentially assigning higher ratings to incomplete papers compared to full papers and favoring well-known authors in single-blind review process. These findings highlight the risks of over-reliance on LLMs in peer review, underscoring that we are not yet ready for widespread adoption and emphasizing the need for robust safeguards.", "sections": [{"title": "1 Introduction", "content": "Scholarly peer review serves as a fundamental pillar of scientific progress, where experts offer rigorous and objective assessments to ensure the integrity and reliability of research before publication [1, 2, 3, 4]. However, with the surge in manuscript submissions [5, 6], the peer review system is under immense pressure [7], compounded by the difficulty in securing a sufficient number of qualified reviewers [8, 9, 10, 11]. Meanwhile, peer review is an inherently labor-intensive process. In 2020 alone, it is estimated that the cumulative time spent on peer review worldwide surpassed 15,000 years, equivalent to an economic cost exceeding 1.5 billion USD [12, 13].\nIn this context, academia has increasingly explored the automation of the peer review process [14, 15]. Large Language Models (LLMs) [16, 17] have emerged as promising tools in this realm, due to their extraordinary capability for understanding and generating natural language [18, 19]. Researchers have proposed automated pipelines using LLMs such as GPT-4 to review scientific manuscripts [20, 21, 22, 23, 24], with studies showing substantial overlap between LLM-generated reviews and those produced by human reviewers (e.g., over 30% overlap on Nature journals) [20]. Additionally, research has documented the increasing reliance on LLMs in writing peer reviews for AI conferences: for four recent Al conferences, between 6.5% and 16.9% of peer reviews are substantially influenced by LLMs [25]; for a machine learning conference (ICLR 2024), at least 15.8% of reviews are believed to be written with LLM assistance [26]. These observations indicate a growing trend towards integrating LLMs into the peer review process [27].\nWhile the community has seen a growing number of individuals utilizing LLMs for the peer review process [25, 26], we still lack a clear and comprehensive understanding of the potential risks associated with their use. LLMs, despite their impressive capabilities, are still susceptible to manipulation [28, 29, 30] and can reflect inherent flaws/biases [31, 32, 33, 34] in generations, which may lead to skewed and unfair evaluations of scientific papers. Given the critical role of peer review in maintaining scientific integrity, the unchecked integration of LLMs into this process poses a significant risk that must be seriously considered before widespread adoption, which strongly motivates our work.\nIn this work, we present a series of analysis that reveals the potential risks associated with employing LLMs in scholarly peer review. Specifically, building upon three established reviewing pipelines that has demonstrated to exhibit substantial alignment with human reviewers [20, 22, 23], we devise two series of experiments that critically evaluate the reliability and validity of LLM-generated assessments through manipulation and examining the inherent flaws. In our manipulation experiments, we investigate two types of manipulation: explicit manipulation and implicit manipulation. For explicit manipulation, we develop a review injection attack method that embeds manipulative review content into the manuscript PDF using extremely small white font, rendering it nearly invisible against the background. This covert injection is designed to go unnoticed by human reviewers while remaining readable for PDF parsers in the automated review process. For implicit manipulation, we consider scenarios where authors proactively disclose limitations or weaknesses in their manuscripts, as encouraged by some conferences such as NeurIPS [35]. By intentionally highlighting insignificant limitations, authors can subtly influence LLM reviews, constituting a form of implicit manipulation. Regarding experiments on inherent flaws, we design controlled studies to explore potential flaws due to inherent limitations of LLMs, including hallucination, bias against paper length and authorship. For instance, we find that LLMs may hallucinate when presented with incomplete content or even an empty paper, that authorship can influence LLM judgments in a single-blind review setting, and that longer papers tend to receive more favorable feedback.\nBased on these setups, we carry out extensive experiments based on the open-accessed ICLR 2024 reviews, which reveal several key findings: (1) LLMs are vulnerable to both explicit and implicit manipulation, leading to reviews that can be significantly swayed by intentional authors. In our explicit manipulation experiments, we find that LLM-generated reviews can be almost entirely controlled by the injected content, with agreement rates reaching 90%; while significantly deviating from human reviews (from 53% to 16%). Notably, explicit manipulation can result in all papers receiving positive feedback, shifting ratings from 5.34 to 7.99 on average according to a rating model. (2) Our implicit manipulation experiments further reveal that LLMs are more susceptible than human reviewers to the limitations authors proactively disclose in their manuscripts. Specifically, we notice that LLM reviews are 4.5\u00d7 more consistent with authors' proclaimed limitations than human reviews. (3) LLMs exhibit inherent flaws that may compromise the objectivity of scholarly reviews. As an example of hallucination, LLMs could give higher ratings to papers with incomplete content than"}, {"title": "2 Results", "content": "We conduct experiments to explore the effects of manipulation and inherent flaws on the review provided by LLMs. Our experiments focus on ICLR 2024, one premier machine learning conference with open-access human-generated peer reviews. For evaluation, on the one hand, we use the consistency metric from existing literature, which quantifies the overlap between two reviews (see details in Section A). On the other hand, we report a paper rating on a 1-10 scale, which is a more direct factor in paper decision-making."}, {"title": "2.1 Explicit Manipulation", "content": "For experiments of explicit manipulation, we inject a sequence of manipulative texts after the conclusion part of each paper. The injected content aims to manipulate LLMs towards generating reviews that lean towards clear acceptance, emphasizing the paper's strengths while downplaying its weaknesses; see Figure 13. The modified papers are then feed into an LLM-based review system.\nLLM reviews are susceptible to explicit manipulation, which can significantly reduce the consis-tency between LLM and human reviews, making LLM review unreliable. Here, we examine how explicit manipulation affects the process of using LLMs for paper review by comparing LLM-human consistency with and without manipulation; see the details for measuring consistency in Section A. In Table 1, we report two types of consistency: the proportion of key points in the LLM's reviews that are also mentioned by humans (denoted by human-LLM-matched / LLM's), and the proportion of key points in human's reviews that are also mentioned by the LLM (denoted by human-LLM-matched /"}, {"title": "2.2 Implicit Manipulation", "content": "In the previous section, we demonstrate how paper authors can explicitly manipulate LLM-based reviews by embedding small white text within the article, a tactic that is difficult for human reviewers to detect. However, conservative authors might still refrain from employing such strategies, fearing that they could be classified as unethical or as cheating. In light of this, we further identify a potentially more subtle form of manipulation: the disclosure of a paper's limitations by the authors themselves, which is exactly encouraged by some official guidelines [35].\nTo investigate this, we collect 500 papers that explicitly presented their limitations and extract the corresponding sections from the PDFs for consistency measurement. We then compare the consistency between human reviews and the limitations content, as well as the consistency between LLM reviews and the limitations content. These comparisons are illustrated in Figure 7, where we present both overall results and results categorized by acceptance, rejection, and withdrawal outcomes.\nAs shown in the figure, (1) overall, the consistency between LLM reviews and the limitations content is significantly higher than the consistency between human reviews and the limitations content. This"}, {"title": "2.3 Inherent Flaws", "content": "Hallucinations in LLM review. Here, we examine whether hallucination issues [36, 37] might exist during the review process of LLMs. Targeting this, we first feed the three LLM review systems with an empty paper respectively. To our surprise, we find that LLM Review [20] generates fluent review content even though there is no paper content provided. Specifically, it still mentions that \u2018the paper presents a novel methodology' and 'the paper is well-written'; see detailed review content in Figure 6. In contrast, both the review systems in AI Scientist [22] and AgentReview [23] successfully detect it as an empty paper.\nHowever, does it mean that the review systems in AI Scientist [22] and AgentReview [23] are robust against hallucination issues? To answer this question, we further conduct experiments by gradually adding content into the empty paper: 1) adding title only, 2) adding title, abstract, and introduction. We report the results of three review systems in Table 4. From the table, we see that 1) on average, compared to the whole paper, these systems give lower ratings to the content with title only. While this relative relation is reasonable, the difference is not statistically significant, as indicated by the overlapping confidence intervals (e.g., AI Scientist [22]). This suggests the limitations of LLMs in reviewing given that the title provides extremely less information. 2) When we continue adding the abstract and introduction into the content, we notice that all the three systems give comparable review ratings compared to the whole paper (e.g., 5.76 v.s. 5.82 for AgentReview [23]). This result clearly indicates the unreliability of LLMs for replacing humans for peer review since the LLM could give similar rating regardless of the completeness of the paper."}, {"title": "Bias in LLM-review regarding paper length.", "content": "To investigate whether LLM reviewers show a preference for longer papers, we conduct an experiment with 1000 papers reviewed by each system, with ratings assigned using the rating LLM. The papers are grouped into six categories based on their total token count, and the proportion of papers receiving positive ratings was calculated for each group. As shown in Figure 10, the results reveal a monotonic increase in the proportion of positive ratings for longer papers, particularly in the LLM Review and AI-Scientist review systems. While longer papers may generally provide more detailed content, the observed trend suggests that the LLM reviewer tends to favor longer papers, indicating a potential bias toward longer submissions in LLM-based review processes. Further, this finding implies that, in the future, to enhance the credibility of LLM-based peer review, it will be essential to explore review methods that address length-based biases [38]."}, {"title": "Bias in LLM-review regarding authorship.", "content": "In this experiment, we investigate whether LLM reviewers show a preference for papers from pres-tigious institutions or well-known researchers in single-blind review scenarios. Here, a total of 500 papers are evaluated by the LLM Review system [20]. The metric used for evaluation is the positive rating ratio, defined as the proportion of papers receiving a score of 6 or higher from the rating LLM, based on the ICLR scoring system, where a score of 6 represents a borderline accept. As shown in Figure 9, when the authors' affiliations are replaced with those of well-known universities, companies, or researchers (see details in the third part of Section 3.4), the average positive rating in-creases from 36.8% to 40.8%, 41.6%, and 41.2%, respectively. This indicates that the LLM review system tends to favor papers associated with pres-tigious authors, suggesting an inherent bias towards well-known institutions and researchers. This result indicates that the introduction of LLMs into the peer review process may exacerbate issues of unfairness."}, {"title": "2.4 Behaviors of Different LLMS", "content": "In previous experiments, the used LLM to generate review is GPT-4o-0806, one of the state-of-the-art LLMs. Here, we further examine whether similar risks exist when we use other LLMs to generate reviews. For this, we conduct experiments to examine issues of implicit manipulation and hallucination, using three open-source LLMs produced by different companies, Llama-3.1-70B-Instruct [17], DeepSeek-V2.5 [39], and Qwen-2.5-72B-Instruct [40].\nThe consistency between LLM-generated and human-generated is positively correlated with the capabilities of LLMs. Here, we report the con-sistency of LLM-generated and human-generated reviews of four LLMs, which measures the degree of overlap in key points. Additionally, we report the Chatbot Arena Score2, which is a widely recog-nized metric in representing the general capabilities of LLMs [41]. From Figure 12, we see that the LLM-human consistency is positively correlated with the capabilities of LLMs, where the strongest GPT-4o also achieves the highest consistency value. For this consistency metric, the preference ranking is: GPT-4o > Qwen-2.5-72B-Instruct > DeepSeek-V2.5 > Llama-3.1-70B-Instruct.\nDifferent LLMs exhibit varying risk degrees of being implicitly manipulated. Here, we report the consistency between LLM-generated reviews and limitations disclosed by authors in the paper in\nFigure 11(a), where we also report the consistency between human-generated reviews and limitations as a reference line. From the figure, we see that all these LLMs are more likely to reiterate the limitations disclosed by the authors, indicating the risks of being implicitly manipulated. For this metric, the preference ranking is: Llama-3.1-70B-Instruct > DeepSeek-V2.5 > GPT-4o > Qwen-2.5-72B-Instruct.\nDifferent LLMs exhibit varying degrees of hallucination issues. We implement the same experi-ments in Table 4 on four LLMs. To facilitate a clearer comparison across the different LLMs, we present the ratings for each type of content (title, introduction, and full) as a percentage relative to the \"full\" content rating. From Figure 11(b), we see that (1) all LLMs face similar hallucination issues as"}, {"title": "3 Methods", "content": "\"introduction\" content rating reaches the percentage of over 95% relative to the \"full\" content rating. Specifically, for Llama-3.1-70B-Instruct [17], the rating assigned to \"introduction\" content is even higher than that of \"full\" content. (2) Generally, GPT-4o-0806 and DeepSeek-V2.5 exhibit greater robustness against the hallucination issue, as their ratings for title, introduction, and full content show a reasonable stepped pattern. Considering this metric, the preference ranking is: GPT-4o > DeepSeek-V2.5 > Qwen-2.5-72B-Instruct > Llama-3.1-70B-Instruct.\nOverall, GPT-4o-0806 is the most appropriate candidate in serving as a reviewer."}, {"title": "3.1 Leveraging LLMs in Scholarly Peer Review", "content": "Here we introduce the existing LLM-based review systems, taking LLM Review [20] as an example. The system begins by processing an academic paper in PDF format, utilizing a machine-learning-based parser, ScienceBeam [42], to extract key sections of the paper, including the title, abstract, figure and table captions, and main text. Based on this extracted content, the system constructs a review prompt designed to guide the LLM in generating feedback; see full prompt in Figure 17. Through a single pass, the LLM provides structured feedback addressing four critical aspects: significance and novelty, potential reasons for acceptance, potential reasons for rejection, and suggestions for improvement. We adopt the latest GPT model GPT-4o-0806 to capitalize on recent advancements in model capabilities and extended context window sizes, facilitating content understanding and the process of lengthy paper."}, {"title": "3.2 Explicit Manipulation", "content": "The goal of this manipulation technique is to insert content into the paper in such a way that the corresponding LLM-generated review systematically emphasizes the paper's significant strengths while diminishing the impact of its weaknesses. Specifically, the injected content firstly directs the LLM reviewer to highlight strengths such as notable novelty and significant practical impact. Simultaneously, it downplays weaknesses by reframing them as minor and easily fixable issues, such as minor writing improvements and small implementation details. Finally, the content instructs the LLM to emphasize these strengths, diminish the weaknesses, and conclude the review with strong advocacy for the paper's acceptance; refer to the full injected content in Figure 13. This approach increases the likelihood that the resulting review presents the paper as exceptional and highly deserving of acceptance.\nAuthors intending to employ this technique embed the manipulation text at the end of their papers. The text is formatted in a white font with a minimal size, making it virtually invisible in the final PDF and appearing as subtle blank space. For human reviewers, such hidden text remains undetectable during a standard reading of the paper. However, during the LLM review process, this text is extracted and parsed along with the conclusion content, seamlessly integrating into the LLM's review prompt. By incorporating such an injected content into the review process, the LLM is deceived into generating highly positive feedback with minimal criticism, which significantly increases the chances of the paper being accepted."}, {"title": "3.3 Implicit Manipulation", "content": "Explicit manipulation is an effective but ethically questionable strategy for influencing LLM reviews. To explore subtler, seemingly legitimate alternatives, we examined whether implicit manipulation could guide LLM reviews without overt violations. Surprisingly, a common practice encouraged by academic committees\u2014explicitly addressing a paper's limitations\u2014can be used to achieve this.\nWhile policies requiring authors to state their work's limitations aim to promote transparency, they also enable covert manipulation. Authors may strategically frame the \"Limitations\" section of their paper to preemptively identify weaknesses that are either trivial or easily addressed during the rebuttal phase. By doing so, they can prepare effective responses in advance, making it easier to counter any concerns raised by reviewers during the rebuttal process.\nNotably, human reviewers typically evaluate a paper holistically, forming independent judgments about its weaknesses. In contrast, LLM reviewers disproportionately rely on explicitly stated"}, {"title": "3.4 Inherent Flaws", "content": "LLM-based evaluation paradigms suffer from various inherent flaws and biases [32, 43, 44, 45]. When utilizing LLMs for the review process, it is crucial to examine how these issues might influence the fairness and reliability of the review system. One significant concern is the tendency of LLMs to generate hallucinations, particularly when provided with incomplete or improperly parsed input papers. Additionally, we explore two specific biases inherent in LLM-based reviews: bias in LLM-review regarding paper length and bias in LLM-review regarding authorship. Understanding and addressing these flaws is critical for evaluating the robustness of LLM-based peer review systems.\nHallucinations in LLM review. LLMs are known to generate hallucinations-outputs that appear plausible but are factually incorrect or unsupported [46]. To investigate this phenomenon within the context of LLM-based review systems, we explore scenarios where the input paper is incomplete or improperly parsed. Specifically, we analyze the feedback generated when the input consists only of the title, or the title and abstract with the main content limited to the introduction.\nUsing the review prompt in Figure 17 as an example, we manually simulate such scenarios by providing incomplete inputs. For one experiment, only the title is provided while all other sections are left empty. For another experiment, we supply the title and abstract but restrict the main content to the introduction alone. The review system's feedback is then analyzed to determine how the LLM responds to these limited inputs and whether it generates coherent yet unsupported feedback based on hallucinations.\nBias in LLM-review regarding paper length. LLMs often display a preference for longer re-sponses [47], which raises the question of whether this bias might lead the review system to favor longer papers. Here we investigate the impact of paper length in token on review outcomes. By analyzing the review results for papers with varying lengths, we aim to determine whether LLM-based reviews disproportionately favor longer papers.\nBias in LLM-review regarding authorship. In single-blind review settings, where the reviewer can see the authors' names and affiliations, we investigate whether LLM reviewers show favoritism toward papers from more prestigious institutions or authored by well-known researchers. To study"}, {"title": "4 Discussions", "content": "Faced with the inevitable trend of researchers increasingly using LLMs in the scholarly peer re-view process, this paper aims to uncover the associated risks before their widespread adoption. We comprehensively evaluate these risks from three key perspectives: explicit manipulation, implicit manipulation, and inherent flaws. Specifically, we demonstrate that LLMs can be explicitly manipu-lated to generate review content stealthily injected into manuscripts by authors. In a subtler manner, LLMs are significantly more susceptible than human reviewers to being influenced by limitations proactively disclosed by authors, introducing the risk of implicit manipulation. Furthermore, we identify several inherent flaws in LLMs for academic paper review. For instance, LLMs can produce seemingly plausible reviews even when presented with an empty paper.\nOur findings underscore a critical conclusion: LLMs, in their current state, are insufficiently robust to replace human reviewers in scholarly peer review. The risks of manipulation, inherent biases, and flaws make them unfit for serving as the sole or primary means of assessment in this essential process. As such, we strongly advocate for a moratorium on the use of LLMs for executing peer review until these risks are more fully understood and effective safeguards are put in place. This pause should provide time for further research and development, as well as the implementation of policies to mitigate these risks.\nIn addition to halting the use of LLMs in peer review, we call on journal and conference orga-nizers to take immediate action to ensure the integrity of the review process. Firstly, we believe that committees should introduce comprehensive detection tools and accountability measures to identify and address both malicious author manipulation and the use of LLMs by reviewers in place of human judgment. Furthermore, we believe it is essential to introduce punitive measures to deter such behaviors. By imposing clear penalties for authors who engage in manipulation or reviewers who replace their judgments with LLM-generated content, we can reduce the likelihood of these risks materializing.\nWhile LLMs should not replace human reviewers, they can support the review process. Looking to the future, we recognize that the number of manuscript submissions is continually increasing, and the potential for automation in the review process is undeniable. While LLMs are not yet capable of fully replacing human reviewers, they could still play a valuable role in supporting the review process, if used judiciously. For example, LLMs could be introduced as a supplementary tool, providing reviewers with enhanced feedback and insights that could improve the quality of the review process. We are already seeing early signs of this in conferences like ICLR 2025, where LLMs are being used to offer reviewers suggestions for improving their evaluations [49]. However, such uses should always be considered supplementary, rather than as a replacement for the nuanced judgment of human experts.\nAs we move forward, it is crucial to continue exploring ways to make LLM-assisted review systems more robust and secure. In the long term, the goal should be to develop a peer review process that integrates LLMs in a way that maximizes their potential while safeguarding against the risks we have identified. This includes implementing defensive mechanisms, such as content"}, {"title": "Ethical Statement", "content": "Our study explores the potential risks and vulnerabilities associated with using LLMs for peer review, including possible manipulations that could artificially influence their evaluations. However, it is essential to emphasize that the primary aim of this research is not to provide actionable methods for exploitation but to advance the understanding of these vulnerabilities within the community. By shedding light on these issues, we seek to foster the development of stronger safeguards and ethical frameworks, ultimately contributing to the responsible and secure use of LLMs for scientific peer review and beyond."}, {"title": "Human Discrepancy", "content": "This gives the average discrepancy between all pairs of human ratings across all papers, which can be used as a baseline to compare the model's performance.\n\n$\nHuman Discrepancy = \\frac{\\sum_{p} \\sum_{i0"}]}