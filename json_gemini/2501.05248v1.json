{"title": "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient Pruning", "authors": ["Laura Puccioni", "Alireza Farshin", "Mariano Scazzariello", "Changjie Wang", "Marco Chiesa", "Dejan Kosti\u0107"], "abstract": "Large Language Models (LLMs) have demonstrated their exceptional performance in various complex code generation tasks. However, their broader adoption is limited by significant computational demands and high resource requirements, particularly memory and processing power. To mitigate such requirements, model pruning techniques are used to create more compact models with significantly fewer parameters. However, current approaches do not focus on the efficient extraction of programming-language-specific sub-models. In this work, we explore the idea of efficiently deriving coding-specific sub-models through unstructured pruning (i.e., Wanda). We investigate the impact of different domain-specific calibration datasets on pruning outcomes across three distinct domains and extend our analysis to extracting four language-specific sub-models: Python, Java, C++, and JavaScript. We are the first to efficiently extract programming-language-specific sub-models using appropriate calibration datasets while maintaining acceptable accuracy w.r.t. full models. We are also the first to provide analytical evidence that domain-specific tasks activate distinct regions within LLMs, supporting the creation of specialized sub-models through un-structured pruning. We believe that this work has significant potential to enhance LLM accessibility for coding by reducing computational requirements to enable local execution on consumer- grade hardware, and supporting faster inference times critical for real-time development feedback.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have become the forefront of technological advancements, making significant strides in various fields thanks to their exceptional performance across diverse and complex tasks. Recently, LLMs have been extensively used in code generation [1], [2], either acting as co-pilots (i.e., assisting developers) [3], [4] or even automating parts of the development process [5], [6]. However, despite their growing use in code-related tasks, their broader adoption is limited by significant computational demands and high resource requirements of forefront models, especially in terms of memory and processing power [7]. Over the past year, model sizes have continuously increased, with Llama-3.1, the largest open-source model, reaching 405 billion parameters [8]. As the size of these models continues to grow, with GPT-5 allegedly exceeding 10 trillion parameters [9], the challenge of efficiently inferencing becomes significant, especially as LLMs deployment poses an ever-increasing energy demand that may soon become unsustainable [10].\nOne model does not fit all. General-purpose LLMs are trained on vast datasets spanning multiple domains, with their parameter count carefully set to optimize the balance in the loss function, largely influenced by the volume of training data. However, certain applications (e.g., coding or math) do not require broad knowledge but instead benefit from expertise in a specific domain. For example, developers might find a model specialized in a particular programming language, such as Python, more helpful for coding tasks, without needing knowledge on unrelated subjects like physics or chemistry. In such instances, a domain-specific sub-model is ideal, as it can maintain essential human language understanding along with specialized domain expertise.\nEfficiency via compression. In this context, model com- pression techniques have proven effective. These approaches aim to produce a compact/sparse model with significantly fewer parameters, enhancing deployment efficiency in terms of both cost and resource usage. One prominent approach in this domain is pruning, which involves removing unnecessary parameters from a model without significantly degrading its performance [11]. However, many existing pruning meth- ods [12], [13] are not specifically devised for LLMs, causing significant inaccuracies, and often require retraining, making them less feasible for models of such scale and complexity. Such limitations have prompted the need for more tailored approaches that can address the specific challenges posed by these models. Wanda [14] presents a novel technique for pruning LLMs by leveraging a small set of calibration data to estimate activation norms. This method efficiently identify less critical weights/parameters in the model, allowing for pruning with lower computational/memory demands and minimal accuracy drops of the pruned LLM. Importantly, Wanda achieves this without the need to retrain or fine-tune the full model. Although Wanda offers several advantages, it is primarily designed to extract general-purpose sub-models that maintain strong performance across multiple tasks, rather than focusing on domain-specific sub-models. Furthermore, authors do not investigate the impact of calibration sample selection on pruning outcomes, relying exclusively on samples from the C4 dataset [15], a broad English-language text source."}, {"title": "II. RELATED WORK", "content": "LLMs are decoder-only models, meaning they only utilize the decoder part of the Transformer architecture [16]. In such models, input tokens are transformed into embeddings that are directly fed into the decoder, which processes them through a series of matrix multiplications involving a large number of parameters. Given the high computational demand of these multiplications, significant research efforts have focused on model compression, which aims to transform large, resource- intensive models into more compact versions [17]. These smaller models are suitable for deployment on devices with less sophisticated hardware and are optimized for faster execution with minimal latency [18].\nModel compression approaches. In recent research efforts, five primary model compression approaches have emerged: (i) quantization, (ii) knowledge distillation, (iii) Neural Architecture Search (NAS), (iv) low-rank factorization, and (v) pruning. Each of these methods offers unique advantages and challenges, and are complementary. Quantization is a compression method that reduces model size by representing weights and activations with lower-bit representations, decreasing computational complexity and storage requirements [19]. The most common approach is post-training quantization, which converts pretrained weights to lower bit precision [20]. However, quantization may introduce errors that impact the model's ability to generalize or maintain performance across varied tasks, lowering overall accuracy. Low-rank factorization compresses models by decomposing weight matrices into lower-rank matrices, minimizing redundancy and facilitating faster computations by parallelizing memory access of dense matri- ces [21], [22]. Knowledge distillation transfers knowledge from a large model (teacher) to a smaller model (student), aiming to replicate the teacher's behavior with fewer resources [23]- [25]. NAS is similar to knowledge distillation and often involves training a super-network once, enabling the sampling of sub-networks through the weight-sharing principle [26], [27]. Low-rank factorization, knowledge distillation, and NAS all involve retraining or significant modifications to the model architecture, making them challenging to implement for LLMs. Among these approaches, pruning has proven to offer the best trade-off between computational gains and minimal loss in accuracy. It aims to reduce the size or complexity of a model systematically, eliminating redundant or less influential parameters, thereby reducing the model's computational and storage requirements [28]. In this work, we mainly focus on unstructured pruning, i.e., induce sparsity at a finer granularity by eliminating individual weights rather than entire units or blocks (i.e., neurons or layers) [29]."}, {"title": "III. METHODOLOGY", "content": "Prior works examined how calibration data affect the effectiveness of model compression methods [34]\u2013[37], offering two key insights: (i) no single dataset consistently performs best across all tasks, emphasizing the sensitivity of compression outcomes to the choice of calibration data, and (ii) the optimal size for calibration sets is relatively small, as larger datasets yield minimal performance improvements, suggesting that a carefully selected, compact set is sufficient for effective pruning. D-Pruner [32] is the first work to use task-specific calibration sets to extract domain-specific sub-models, focusing primarily on financial and medical domains, but not addressing program- ming or code generation tasks. Furthermore, as discussed in \u00a7II, D-Pruner relies on full-parameter fine-tuning and gradient updates, which results in high computational demands.\nFollowing the approach shown in Fig. 1, we assess whether combining efficient pruning techniques (i.e., Wanda) with task- specific calibration datasets can effectively generate domain- specific LLMs that maintain strong performance in specialized tasks while minimizing memory and computational demands. We first validate our approach by extracting sub-models in the domains of math, CSR, and language translation. Then, we focus on sub-model extraction for code generation, i.e., Python, Java, C++, and JavaScript.\nOur methodology consists of three main steps, which will be detailed in the following.\n1 Model selection. We begin by selecting one or more models for pruning. To ensure the study's validity and broad applicability, we chose a diverse range of models based on factors such as novelty, size, and purpose. First, we selected the latest versions of models (available at the time of writing) to enable fair and relevant comparisons. Second, our focus is on relatively compact models, as our goal is to create domain-specific sub-models that can be efficiently deployed on commodity hardware with limited resources. Finally, models were chosen based on their performance on widely recognized evaluation tasks, ensuring a trustable baseline for fair comparison between the original model and its pruned counterpart. While the original Wanda paper focused solely on Llama-2 [34], this study includes four diverse models, specifically: CodeLlama-7B and CodeLlama-13B [38], Mistral- 7B-v0.1 [39], and Gemma-1.1-7B [40]. CodeLlama is fine- tuned specifically for code, while Mistral and Gemma are more general-purpose models. We include two CodeLlama versions to validate our findings on a larger model. All the LLMs were chosen in their \u201cinstruct\u201d version, as it is designed to follow natural language instructions more seamlessly.\n2 Datasets selection. As mentioned in \u00a7II, Wanda utilizes a calibration set to estimate input activations essential for identifying weights to prune, making the quality and selection of these datasets critical for obtaining sub-models with high accuracy. For code generation, we employed: (i) three Python- specific datasets to validate the approach (see Table IIa), and (ii) four datasets covering different programming languages to generalize our findings to other coding tasks, reported in Table IIb. As for math, CSR, and language translation tasks, we chose the three datasets listed in Table II\u0430.\n3 Pruning process. We apply the Wanda pruning technique to the selected models and calibration datasets. Following prior best practices [34], we use 128 samples as calibration data for each dataset considered. Samples are randomly selected with a fixed seed. Given the diverse datasets and models presented in Step 2, the pruning process is performed iteratively. This iterative approach systematically adjusts parameters related to"}, {"title": "IV. EVALUATION AND DISCUSSION", "content": "In this section, we discuss the results of the domain-specific sub-models created by pruning the foundational models de- scribed in \u00a7III with the chosen calibration datasets. We begin by reviewing the performance of sub-models generated for the four selected domains, followed by an analysis of the programming- language-specific sub-models (\u00a7IV-A). Additionally, we dig into the internal structure of the obtained sub-models using both visual and analytical metrics (\u00a7IV-B).\nA. Task-Specific Pruning Evaluation\nPruning with domain-specific calibration sets brings clear benefits. The proposed approach shows an overall improvement over the original Wanda paper's results. Notably, the C4 dataset does not yield the best outcomes for any task across all models and domains assessed. In each case, the top-performing sub- models are those pruned using domain-specific calibration datasets. Table III summarizes the best results achieved for the selected models\u00b9, showing the sub-model with the highest accuracy and the calibration dataset used. Although the sub- models generally exhibit lower accuracy than the original models, this is an expected trade-off to increase computational efficiency. Nonetheless, there is a consistent pattern across all models: the highest accuracy is obtained with calibration datasets tailored to each task, confirming our intuition that domain-specific datasets are essential for the creation of accurate, domain-specific sub-models.\nRobust training yields strong post-pruning results. Looking at the 7B models results, an interesting finding emerges in the code generation task: the Gemma model shows a relatively smaller accuracy drop after pruning compared to code-specific models, i.e., CodeLlama. Although CodeLlama, specifically designed for coding tasks, was expected to preserve most of its original performance, it is actually Gemma that retains a better accuracy, with a drop of ~5.83%, compared to a ~7.47% drop for CodeLlama. We speculate that this result may be due to Gemma's greater resilience to pruning, likely a result of its broader training data. Unlike CodeLlama, which is primarily fine-tuned on coding data [64], Gemma has been exposed to a diverse range of sources, including web documents, code, and math [40]. This wider training data could have helped Gemma develop a deeper understanding of language, enhancing its adaptability in the pruning process. This finding is consistent with previous research [65] suggesting that a wider pre-training knowledge base contributes to better model's robustness.\nPruning retains strong CSR for user query processing. Unlike other domains, the CSR task exhibits relatively consis- tent accuracy levels across different calibration datasets, even those from unrelated domains. This finding suggests that the sub-models may not gain significant benefits from the usage of CSR-specific calibration samples during pruning. The consistent accuracies across sub-models indicate that they possess a robust understanding of common knowledge and everyday reasoning principles, likely due to their training on web documents or datasets rich in general knowledge. Therefore, additional calibration with CSR-specific samples during pruning appears to offer limited benefits. Since each model requires a strong base of common knowledge to effectively process user queries, it is likely that essential weights for CSR are consistently retained across all sub-models.\nTask complexity influences post-pruning accuracy. In the language translation task, we observe a similar trend to that of code generation: translation-specific sub-models retain higher accuracy, confirming the effectiveness of our approach. Interestingly, translation sub-models experience a much smaller accuracy drop (\u22643%) than those focused on code generation tasks (\u22655%). We hypothesize that this notable difference arises from the fundamental nature of the tasks. Translation typically involves a more direct mapping between input and output sequences across languages, while code generation requires transforming natural language into executable code, which demands a deep understanding of programming logic, syntax, and semantics. Consequently, code-generation sub-models may lose essential information during pruning, leading to the ob- served larger accuracy drop. This observation also suggests that resulting sub-models could be utilized for performing specific sub-tasks within a domain (e.g., line-by-line code translation between programming languages) rather than broader tasks (e.g., generating complete code blocks).\nLogical reasoning skills retain cross-task accuracy. We observe that code and math sub-models experience only a slight drop in accuracy when evaluated on tasks from the other domain (i.e., math models on code tasks and vice versa). Fig. 2 shows the accuracy of code (blue line) and math (green line) tasks conducted with non-code and non-math sub-models for the selected 7B models. It is evident that code-specific sub-models perform slightly better on math tasks, and the same applies for math-specific sub-models on coding tasks. This interesting behavior can be explained by the shared skills and similarities between coding and mathematical reasoning. Both areas require logical thinking, problem-solving skills, and a solid understanding of numerical concepts. As a result, sub- models specialized in one domain may retain sub-areas of knowledge relevant to the other, enabling them to achieve higher accuracy levels.\nProper calibration sets extract programming-language- specific sub-models. After evaluating the impact of task-specific calibration sets in pruning, which supports our approach for extracting domain-specific sub-models effectively, we explore whether similar outcomes could be achieved with programming-language-specific sub-models. In previous experi-ments, we solely focused on Python in the code generation task. Here, we expand our approach to extract three additional sub-models: Java, C++, and JavaScript. We select CodeLlama-13B as the base model, given the higher accuracy of its sub-models. Table IV presents the performance of each sub-model pruned with different programming language datasets as calibration sets. The evaluation spans four distinct versions of HumanEval, each tailored to a specific programming language. A significant observation is the strong language specificity of each sub-model: they achieve high accuracy within their own language but show an accuracy drop when applied to other languages. This finding demonstrates that it is possible to effectively extract programming-language-specific sub-models from foundational models with suitable calibration sets, and that such sub-models retain acceptable accuracy w.r.t. base models.\nB. Sub-Models Structure Comparison\nIn order to determine whether our approach is effectively cre- ating domain-specific sub-models, we conducted a comparison of the parameter masks of each layer of the pruned models.\nDistinct weight patterns emerge in domain-specific sub-models. We analyzed the internal structure of the obtained domain-specific sub-models to determine whether the pruning method influences their specificity to certain domains. Due to space limitations, we include results only for Gemma- 7B, which displays the most significant differences and has fewer layers, providing a clearer visualization. Fig. 3 presents visual representations of the post-pruning weights of the q_proj matrix\u00b2 in the 12th layer of the model, generated using the penzai tool [62]. Specifically, Fig. 3a compares two math-specific sub-models, while Fig. 3b contrasts one math-specific sub-model with a translation-specific sub-model. In each figure, the left and center plots report weights as colored squares, where darker squares indicate higher values of the corresponding weights, while the right-most plot highlights the differences in weights between the two matrices (red dots). This visualization allows us to qualitatively assess that sub-models from distinct domains exhibit notable disparities compared to those within the same domain, as the number of red dots in Fig. 3a (math vs. math) is markedly lower than the ones in Fig. 3b (math vs. translation).\nQuantitative analysis of sub-models weight patterns. We deepen the understanding of weight differences through an analytical per-layer comparison using the Jaccard distance. Fig. 4 depicts the per-layer Jaccard distance of the same Gemma sub-models shown in Fig. 3. Specifically, Fig. 4a reports the comparison between sub-models within the same domain (math), while Fig. 4b compares sub-models from different domains (math and translation). The clear contrast indicates that sub-models within the same domain tend to have more similar weight distributions, while those from different domains show more divergent weight distributions. These results suggest"}, {"title": "V. CONCLUSIONS", "content": "In this study, we demonstrate that domain-specific sub- models can be efficiently extracted from foundational LLMs using unstructured pruning (i.e., Wanda). This approach was applied across three distinct domains: math, CSR, and language translation. Our findings reveal that the resulting sub-models maintain acceptable accuracy when compared to their full- model counterparts, and that sub-models derived using domain- specific calibration sets outperform those created with unrelated data, highlighting the importance of selecting appropriate calibration datasets. We are the first ones to extract four programming-language-specific sub-models, further confirming our findings. Furthermore, we provide the first analytical evi- dence that domain-specific tasks activate unique LLMs regions with unstructured pruning, indicating that certain weights are essential for maintaining domain-specific performance.\nFuture work could extend these experiments by incorporating additional models of different sizes and exploring a wider range of programming languages. Additionally, we also plan to develop a novel inference system that utilizes the extracted sub-models and dynamically selects the most appropriate according to user requirements (similarly to Composition of Experts [66])."}]}