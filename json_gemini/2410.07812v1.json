{"title": "TEMPORAL-DIFFERENCE VARIATIONAL CONTINUAL LEARNING", "authors": ["Luckeciano C. Melo", "Alessandro Abate", "Yarin Gal"], "abstract": "A crucial capability of Machine Learning models in real-world applications is the\nability to continuously learn new tasks. This adaptability allows them to respond\nto potentially inevitable shifts in the data-generating distribution over time. How-\never, in Continual Learning (CL) settings, models often struggle to balance learn-\ning new tasks (plasticity) with retaining previous knowledge (memory stability).\nConsequently, they are susceptible to Catastrophic Forgetting, which degrades\nperformance and undermines the reliability of deployed systems. Variational Con-\ntinual Learning methods tackle this challenge by employing a learning objective\nthat recursively updates the posterior distribution and enforces it to stay close to\nthe latest posterior estimate. Nonetheless, we argue that these methods may be\nineffective due to compounding approximation errors over successive recursions.\nTo mitigate this, we propose new learning objectives that integrate the regulariza-\ntion effects of multiple previous posterior estimations, preventing individual errors\nfrom dominating future posterior updates and compounding over time. We reveal\ninsightful connections between these objectives and Temporal-Difference meth-\nods, a popular learning mechanism in Reinforcement Learning and Neuroscience.\nWe evaluate the proposed objectives on challenging versions of popular CL bench-\nmarks, demonstrating that they outperform standard Variational CL methods and\nnon-variational baselines, effectively alleviating Catastrophic Forgetting.", "sections": [{"title": "INTRODUCTION", "content": "A fundamental aspect of robust Machine Learn-\ning (ML) models is to learn from non-stationary\nsequential data. In this scenario, two main\nproperties are necessary: first, models must\nlearn from new incoming data potentially\nfrom a different task with satisfactory\nasymptotic performance and sample complex-\nity. This capability is called plasticity. Second,\nthey must retain the knowledge from previously\nlearned tasks, know as memory stability. When\nthis does not happen, and the performance\nof previous tasks degrades, the model suf-\nfers from Catastrophic Forgetting (Goodfellow\net al., 2015; McCloskey & Cohen, 1989). These\ntwo properties are the central core of Contin-\nual Learning (CL) (Schlimmer & Fisher, 1986;\nAbraham & Robins, 2005), being strongly rel-\nevant for ML systems susceptible to test-time\ndistributional shifts.\nGiven the critical importance of this topic, ex-"}, {"title": "RELATED WORK", "content": "Continual Learning has been studied throughout the past decades, both in Artificial Intelligence\n(Schlimmer & Fisher, 1986; Sutton & Whitehead, 1993; Ring, 1997) and in Neuro- and Cognitive"}, {"title": "PRELIMINARIES", "content": "Problem Statement. In the Continual Learning setting, a model learns from a streaming of tasks,\nwhich forms a non-stationary data distribution throughout time. More formally, we consider a task\ndistribution \\( \\mathcal{T} \\) and represent each task \\( t \\sim \\mathcal{T} \\) as a set of pairs \\( \\{(x_t, y_t)\\} \\)_{N_t} \\), where \\( N_t \\) is the dataset\nsize. At every timestep \\( t \\), the model receives a batch of data \\( \\mathcal{D}_t \\) for training. We evaluate the model\nin held-out test sets, considering all previously observed tasks.\nIn the Bayesian framework for CL, we assume a prior distribution over parameters \\( p(\\theta) \\), and the\ngoal is to learn a posterior distribution \\( p(\\theta \\mid \\mathcal{D}_{1:T}) \\) after observing T tasks. Crucially, given the\nsequential nature of tasks, we identify a recursive property of posteriors:\n\\[\np(\\theta \\mid \\mathcal{D}_{1:T}) \\propto p(\\theta)p(\\mathcal{D}_{1:T} \\mid \\theta) \\overset{i.i.d}{=} p(\\theta) \\left[ \\prod_{t=1}^{T} p(\\mathcal{D}_t \\mid \\theta) \\right] \\propto p(\\theta \\mid \\mathcal{D}_{1:T-1})p(\\mathcal{D}_T \\mid \\theta),\n\\]\nwhere we assume that tasks are i.i.d. Equation 1 shows that we may update the posterior estimation\nonline, given the likelihood of the subsequent task.\nVariational Continual Learning. Despite the elegant recursion, computing the posterior \\( p(\\theta \\mid\n\\mathcal{D}_{1:T}) \\) exactly is often intractable, especially for large parameter spaces. Hence, we rely on an\napproximation. VCL achieves this by employing online variational inference (Ghahramani & Attias,\n2000). It assumes the existence of variational parameters \\( q(\\theta) \\) whose goal is to approximate the\nposterior by minimizing the following KL divergence over a space of variational approximations \\( \\mathcal{Q} \\):"}, {"title": "TEMPORAL-DIFFERENCE VARIATIONAL CONTINUAL LEARNING", "content": "Maximizing the objective in Equation 3 is equivalent to the optimization in Equation 2, but its\ncomputation relies on two main approximations. First, computing the expected log-likelihood term\nanalytically is not tractable, which requires a Monte-Carlo (MC) approximation. Second, the KL\nterm relies on a previous posterior estimate, which may be biased from previous approximation\nerrors. While updating the posterior to account for the next task, these biases deviate the learning\ntarget from the true objective. Crucially, as Equation 3 solely relies on the very latest posterior\nestimation, the error compounds with successive recursive updates.\nAlternatively, we may represent the same objective as a function of several previous posterior esti-\nmations and alleviate the effect of the approximation error from any particular one. By considering\nseveral past estimates, the objective dilutes individual errors, allows correct posterior approximates\nto exert a corrective influence, and leverages a broader global context to the learning target, reducing\nthe impact of compounding errors over time."}, {"title": "VARIATIONAL CONTINUAL LEARNING WITH N-STEP KL REGULARIZATION", "content": "We start by presenting a new objective that is equivalent to Equation 2 while also meeting the\naforementioned desiderata:\nProposition 4.1. The standard KL minimization objective in Variational Continual Learning (Equa-\ntion 2) is equivalently represented as the following objective, where \\( n \\in \\mathbb{N}_0 \\) is a hyperparameter:\n\\[\n\\begin{aligned}\nq_t(\\theta) = \\arg\\max_{\\theta \\in \\mathcal{Q}} \\mathbb{E}_{\\theta \\sim q_t(\\theta)} \\left[ \\sum_{i=0}^{n-1} \\frac{(n - i)}{n} \\log p(\\mathcal{D}_{t-i} \\mid \\theta) \\right] - \\sum_{i=0}^{n-1} \\frac{1}{n} D_{KL}(q_t(\\theta) \\mid\\mid q_{t-i-1}(\\theta)).\n\\end{aligned}\n\\]\nWe present the proof of Proposition 4.1 in Appendix A. We name Equation 4 as the n-Step KL\nregularization objective. It represents the same learning target of Equation 2 as a sum of weighted\nlikelihoods and KL terms that consider different posterior estimations, which can be interpreted as\n\"distributing\" the role of regularization among them. For instance, if an estimate \\( q_{t-i} \\) deviates too\nfar from the true posterior, it only affects 1/n of the KL regularization term. The hyperparameter n\nassumes integer values up to t and defines how far in the past the learning target goes. If n is set to\n1, we recover vanilla VCL.\nAn interesting insight comes from the likelihood term. It contains the likelihood of different tasks,\nweighted by their recency. Hence, the idea of re-training in old task data, commonly leveraged as a\nheuristic in CL methods, naturally emerges in the proposed objective. Additionally, we may estimate\nthe likelihood term by replaying data from different tasks simultaneously, alleviating the violation\nof the i.i.d assumption that happens given the online, sequential nature of CL (Hadsell et al., 2020)."}, {"title": "FROM N-STEP KL TO TEMPORAL-DIFFERENCE TARGETS", "content": "The learning objective in Equation 4 relies on several different posterior estimates, alleviating the\ncompounding error problem. A caveat is that all estimates have the same weight in the final ob-"}, {"title": "CLOSING REMARKS", "content": "In this work, we have presented a new family of variational objectives for Continual Learning,\nnamely Temporal-Difference VCL. TD-VCL is an unbiased proxy of the standard VCL objective\nbut leverages several previous posterior estimates to alleviate the compounding error caused by\nrecursive approximations. We have shown that TD-VCL represents a spectrum of continual learning\nalgorithms and is equivalent to a discounted sum of n-step Temporal-Difference targets. Lastly, we\nhave empirically displayed that it helps address Catastrophic Forgetting, surpassing vanilla VCL and\nother baselines in improved versions of popular CL benchmarks.\nLimitations. Despite being theoretically principled and attaining superior performance, TD-VCL\npresents limitations. First, the hyperparameters n and \\( \\lambda \\) depend on the evaluated setting, which may\nrequire certain tuning. Second, the objectives require maintaining a copy of the past n posterior\nestimates, increasing the memory requirements. Still, we believe this is not a major limitation as\nTD-VCL suits well modern deep Bayesian architectures that target smaller parameter subspaces for\nposterior approximation (Yang et al., 2024; Dwaracherla et al., 2024; Melo et al., 2024).\nFuture Work. While presenting connections with Temporal-Difference methods, our work does\nnot claim that TD-VCL is an RL algorithm. Further mathematical connections with Markov Deci-\nsion/Reward Processes formalism are left as future work. Another interesting direction is to apply\nTD-VCL objectives for probabilistic meta-learning (Finn et al., 2018; Zintgraf et al., 2020)."}, {"title": "A DERIVATION OF THE N-STEP KL REGULARIZATION OBJECTIVE", "content": "In this Section, we prove Proposition 4.1:\nProposition 4.1. The standard KL minimization objective in Variational Continual Learning (Equa-\ntion 2) is equivalently represented as the following objective, where \\( n \\in \\mathbb{N}_0 \\) is a hyperparameter:\n\\[\n\\begin{aligned}\nq_t(\\theta) = \\arg\\max_{\\theta \\in \\mathcal{Q}} \\mathbb{E}_{\\theta \\sim q_t(\\theta)} \\left[ \\sum_{i=0}^{n-1} \\frac{(n - i)}{n} \\log p(\\mathcal{D}_{t-i} \\mid \\theta) \\right] - \\sum_{i=0}^{n-1} \\frac{1}{n} D_{KL}(q_t(\\theta) \\mid\\mid q_{t-i-1}(\\theta)).\n\\end{aligned}\n\\]\nProof. Starting from Equation 2, we can expand it as a sum of equal terms and utilize the recursive\nproperty (Equation 1) to expand these terms:\n\\[\n\\begin{aligned}\nq_t(\\theta) &= \\arg \\min_{\\theta \\in \\mathcal{Q}} D_{KL}(q(\\theta) \\mid\\mid \\frac{1}{Z_t} q_{t-1}(\\theta)p(\\mathcal{D}_t \\mid \\theta))\n\\\\&= \\arg \\min_{\\theta \\in \\mathcal{Q}} - \\frac{1}{n} D_{KL}(q(\\theta) \\mid\\mid \\frac{1}{Z_t} q_{t-1}(\\theta)p(\\mathcal{D}_t \\mid \\theta))\n\\\\&= \\arg \\min_{\\theta \\in \\mathcal{Q}} - \\frac{1}{n} D_{KL}(q(\\theta) \\mid\\mid \\frac{1}{Z_t} q_{t-1}(\\theta)p(\\mathcal{D}_t \\mid \\theta))\n\\\\&+ \\frac{1}{n} D_{KL}(q(\\theta) \\mid\\mid \\frac{1}{Z_tZ_{t-1}} q_{t-2}(\\theta)p(\\mathcal{D}_{t} \\mid \\theta) p(\\mathcal{D}_{t-1} \\mid \\theta)) + ...\n\\\\&+ \\frac{1}{n} D_{KL}(q(\\theta) \\mid\\mid \\frac{1}{\\prod_{i=0}^{n-1}Z_{t-i}} q_{t-n}(\\theta) \\left[ \\prod_{i=0}^{n-1} p(\\mathcal{D}_{t-i} \\mid \\theta) \\right])\n\\\\&= \\arg \\min_{\\theta \\in \\mathcal{Q}} - \\frac{1}{n} \\left[ D_{KL}(q_t(\\theta) \\mid\\mid q_{t-1}(\\theta)) - \\mathbb{E}_{\\theta \\sim q_t(\\theta)} [\\log p(\\mathcal{D}_t \\mid \\theta)] \\right]\n\\\\&+ \\frac{1}{n} \\left[ D_{KL}(q_t(\\theta) \\mid\\mid q_{t-2}(\\theta)) - \\mathbb{E}_{\\theta \\sim q_t(\\theta)} [\\log p(\\mathcal{D}_t \\mid \\theta) + \\log p(\\mathcal{D}_{t-1} \\mid \\theta)] + ...\n\\\\&=\\arg \\min_{\\theta \\in \\mathcal{Q}} \\frac{1}{n} \\left[ \\sum_{i=0}^{n-1} D_{KL}(q_t(\\theta) \\mid\\mid q_{t-i}(\\theta)) - \\mathbb{E}_{\\theta \\sim q_t(\\theta)} \\left[ \\sum_{i=0}^{n-1} \\log p(\\mathcal{D}_t \\mid \\theta) \\right] \\right]\n\\\\&+ (n - 1) \\log p(\\mathcal{D}_{t-1} \\mid \\theta) + \\cdots + \\log p(\\mathcal{D}_{t-n+1} \\mid \\theta)\n\\\\&= \\arg \\max_{\\theta \\in \\mathcal{Q}} \\mathbb{E}_{\\theta \\sim q_t(\\theta)} \\left[ \\sum_{i=0}^{n-1} \\frac{(n - i)}{n} \\log p(\\mathcal{D}_{t-i} \\mid \\theta) \\right] - \\sum_{i=0}^{n-1} \\frac{1}{n} D_{KL}(q_t(\\theta) \\mid\\mid q_{t-i-1}(\\theta)).\n\\end{aligned}\n\\]"}, {"title": "B DERIVATION OF THE TEMPORAL-DIFFERENCE VCL OBJECTIVE", "content": "Before proving Proposition 4.2, we start by presenting a well known result for the sum of geometric\nseries:\nLemma B.1. The finite sum of a geometric series with n terms, common ratio \\( \\lambda \\) and initial term a\nis given by:\n\\[\n\\sum_{k=0}^{n-1} \\lambda^k a = \\frac{a(1 - \\lambda^n)}{(1 - \\lambda)}\n\\]\nProof. Let \\( s_n = \\sum_{k=0}^{n-1} \\lambda^k a \\). Hence,\n\\[\n\\begin{aligned}\ns_n - \\lambda s_n &= \\sum_{k=0}^{n-1} \\lambda^k a - \\lambda \\sum_{k=0}^{n-1} \\lambda^k a = a - a \\lambda^n\\\\\n\\Leftrightarrow s_n(1 - \\lambda) &= a(1 - \\lambda^n)\\\\\n\\Rightarrow s_n &= \\frac{a(1 - \\lambda^n)}{(1 - \\lambda)}\n\\end{aligned}\n\\]\nNow, we prove Proposition 4.2.\nProposition 4.2. The standard KL minimization objective in VCL (Equation 2) is equivalently rep-\nresented as the following objective, with \\( n \\in \\mathbb{N}_0 \\), and \\( \\lambda \\in [0, 1) \\) hyperparameters:\n\\[\n\\arg\\max_{\\theta \\in \\mathcal{Q}} \\mathbb{E}_{\\theta \\sim q_t(\\theta)} \\left[ \\sum_{i=0}^{n-1} \\frac{\\lambda^i (1 - \\lambda^{n-i})}{1 - \\lambda^n} \\log p(\\mathcal{D}_{t-i} \\mid \\theta) \\right] - \\sum_{i=0}^{n-1} \\frac{\\lambda^i (1 - \\lambda)}{1 - \\lambda^n} D_{KL}(q_t(\\theta) \\mid\\mid q_{t-i-1}(\\theta)).\n\\]\nProof. We can use Lemma B.1 to expand the sum of KL terms:"}, {"title": "C THE CONNECTION OF TD TARGETS IN TD-VCL AND REINFORCEMENT\nLEARNING", "content": "In the Section 4, we formalize the concept of n-Step Temporal-Difference for the Variational CL\nobjective (Definition 4.3). In this Section, we reveal the connections between this definition and the\nwidely used Temporal-Difference methods in Reinforcement Learning. Our aim is to clarify why\nEquation 6 indeed represents a temporal-difference target, both in a broad and strict senses.\nIn a broad sense, bootstrapping characterizes a Temporal-Difference target: building a learning\ntarget estimate based on previous estimates. Crucially, the leveraged estimates are functions of\ndifferent timesteps. TD-VCL objectives applies bootstrapping in the KL regularization term, by\nconsidering one or more of posteriors estimates from previous timesteps.\nIn a strict sense, we can show that Equation 6 deeply resembles TD targets in Reinforcement Learn-\ning. RL assumes the formalism of a Markov Decision Process (MDP), defined by a tuple M =\n(S, A, P, R, P\u03bf, \\( \\gamma \\), \u0397), where S is a state space, A is an action space, \\( P : S \\times A \\times S \\rightarrow [0, \\infty) \\) is a\ntransition dynamics, \\( R : S \\times A \\rightarrow [-R_{max}, R_{max}] \\) is a bounded reward function, \\( P_o : S \\rightarrow [0, \\infty) \\)\nis an initial state distribution, \\( \\gamma \\in [0, 1] \\) is a discount factor, and H is the horizon.\nThe standard RL objective is to find a policy that maximizes the cumulative reward:\n\\[\narg \\max_{\\pi} \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{H} \\gamma^k R(s_{t+k}, a_{t+k}) \\right],\n\\]\nwith \\( a_t \\sim \\pi_\\theta(a_t \\mid s_t) \\), \\( s_t \\sim P(s_t \\mid s_{t-1}, a_{t-1}) \\), and \\( s_o \\sim P_o(s) \\), where \\( \\pi_\\theta : S \\times A \\rightarrow [0, \\infty) \\)\nis a\npolicy parameterized by \\( \\theta \\). Hence, we can define the following learning target, which represents a\n\"value\" function at each state \\( s_t \\):\n\\[\nv_{\\pi}(s_t) := \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{H} \\gamma^k R(s_{t+k}, a_{t+k}) \\mid s = s_t \\right], \\forall s_t \\in S.\n\\]\nNaturally, it follows that \\( \\pi^*_\\theta = \\arg\\max v_{\\pi}(s), \\forall s \\in S \\). Crucially, we can expand Equation 14 as\nfollows:\n\\[\n\\begin{aligned}\nv_{\\pi}(s_t) &= \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{H} \\gamma^k R(s_{t+k}, a_{t+k}) \\mid s = s_t \\right] \\\\\n&= \\mathbb{E} \\left[ R(s_t, a_t) + \\sum_{k=1}^{H} \\gamma^k R(s_{t+k}, a_{t+k}) \\mid s = s_t \\right] \\\\\n&= \\mathbb{E} [R(s_t, a_t) + \\gamma v_{\\pi}(s_{t+1})], \\\\\n&= \\mathbb{E} [R(s_t, a_t) + \\gamma R(s_{t+1}, a_{t+1}) + \\gamma^2 v_{\\pi}(s_{t+2})], \\\\\n&= \\mathbb{E} \\left[ \\sum_{k=0}^{n-1} \\gamma^k R(s_{t}, a_{t}) + \\gamma^n v_{\\pi}(s_{t+n}) \\right], \\forall s_t \\in S, n \\leq H.\n\\end{aligned}\n\\]\nTemporal-Difference methods estimates a learning target directly from Equation 15:\n\\[\n\\hat{v}_{\\pi}(s) = TDRL(n) = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{n-1} \\gamma^k R(s_{t}, a_{t}) \\right] + \\gamma^n v_{\\pi}(s_{t+n}) , \\forall s_t \\in S, n \\leq H.\n\\]\nNow, we turn our attention back to our Variational Continual Learning setting. The standard VCL\nobjective is given by Equation 2:"}, {"title": "DTD(X)-VCL IS A DISCOUNTED SUM OF N-STEP TD TARGETS", "content": "In Section 4, we mention that the TD-VCL learning target is a compound update that averages n-step\ntemporal-difference targets, as per Proposition 4.4, which we prove below.\nProposition 4.4. \\( \\forall n \\in \\mathbb{N}_0, n \\leq t \\), the objective in Equation 2 can be equivalently represented as:\n\\[\nq_t(\\theta) = \\arg \\max_{\\theta \\in \\mathcal{Q}} TD_t(n),\n\\]\nwith TD+(n) as in Definition 4.3. Furthermore, the objective in Equation 5 can also be represented\nas:\n\\[\nq_t(\\theta) = \\arg \\max_{\\theta \\in \\mathcal{Q}} \\frac{1-\\lambda}{1 - \\lambda^n} \\left[ \\sum_{k=0}^{n-1} \\lambda^k TD_t(k + 1) \\right]\n\\]\nProof. We start by proving the equivalence between Equation 2 and Equation 7:\n\\[\n\\begin{aligned}\nq_t(\\theta) &= \\arg \\min_{\\theta \\in \\mathcal{Q}} D_{KL}(q(\\theta) \\mid\\mid \\frac{1}{Z_t} q_{t-1}(\\theta)p(\\mathcal{D}_t \\mid \\theta))\\\\\n&= \\arg \\min_{\\theta \\in \\mathcal{Q}} \\left[ \\sum_{i=0}^{n-1} \\mathbb{E}_{\\theta \\sim q_t(\\theta)} \\log p(\\mathcal{D}_{t-i} \\mid \\theta) \\right] - \\sum_{k=0}^{n-1} D_{KL}(q_t(\\theta) \\mid\\mid q_{t-n}(\\theta))\\\\\n&= \\arg \\max_{\\theta \\in \\mathcal{Q}} TD_t(n).\n\\end{aligned}\n\\]\nNow, we show that Equation 5 is a discounted sum of n-Step targets:\n\\[\n\\begin{aligned}\nq_t(\\theta) &= \\arg \\max_{\\theta \\in \\mathcal{Q}} \\frac{1-\\lambda}{1 - \\lambda^n} \\mathbb{E}_{\\theta \\sim q_t(\\theta)} [\\log p(\\mathcal{D}_t \\mid \\theta) - D_{KL}(q_t(\\theta) \\mid\\mid q_{t-1}(\\theta))]\n\\\\&+ \\lambda \\mathbb{E}_{\\theta \\sim q_t(\\theta)} [\\log p(\\mathcal{D}_t \\mid \\theta) + \\log p(\\mathcal{D}_{t-1} \\mid \\theta)] - \\lambda D_{KL}(q_t(\\theta) \\mid\\mid q_{t-2}(\\theta)) + ...\\\\\n&- + \\lambda^{n-1} \\mathbb{E}_{\\theta \\sim q_t(\\theta)} \\left[ \\sum_{i=0}^{n-1} \\log p(\\mathcal{D}_{t-i} \\mid \\theta) \\right] - \\lambda^{n-1} D_{KL}(q_t(\\theta) \\mid\\mid q_{t-n}(\\theta))\n\\\\&= \\arg \\max_{\\theta \\in \\mathcal{Q}} \\frac{1-\\lambda}{1 - \\lambda^n} [TD_t(1) + \\lambda TD_t(2) + ... + \\lambda^{n-1} TD_t(n)]\n\\\\&= \\arg \\max_{\\theta \\in \\mathcal{Q}} \\frac{1-\\lambda}{1 - \\lambda^n} \\left[ \\sum_{k=0}^{n-1} \\lambda^k TD_t(k + 1) \\right]\n\\end{aligned}\n\\]"}, {"title": "E TD-VCL: A SPECTRUM OF CONTINUAL LEARNING ALGORITHMS", "content": "In this Section, we describe how TD-VCL spans a spectrum of algorithms that mix different levels\nof Monte Carlo approximation for expected log-likelihood and KL regularization. Our goal is to\nshow that by choosing specific hyperparameters for Equation 5, one may recover vanilla VCL in\none extreme and n-Step KL regularization in the opposite.\nLet us consider the TD-VCL objective in Equation 5:\n\\[\n\\arg \\max_{\\theta \\in \\mathcal{Q}} \\mathbb{E}_{\\theta \\sim q_t(\\theta)} \\left[ \\sum_{i=0}^{n-1} \\frac{\\lambda^i (1 - \\lambda^{n-i})}{1 - \\lambda^n} \\log p(\\mathcal{D}_{t-i} \\mid \\theta) \\right] - \\sum_{i=0}^{n-1} \\frac{\\lambda^i (1 - \\lambda)}{1 - \\lambda^n} D_{KL}(q_t(\\theta) \\mid\\mid q_{t-i-1}(\\theta)).\n\\]\nTrivially, if we set \\( \\lambda = 0 \\), assuming \\( 0^0 = 1 \\), it recovers the Vanilla VCL objective, as stated in\nEquation 3, regardless of the choice of n.\nMore interestingly, we investigate the learning target as \\( \\lambda \\rightarrow 1 \\):\n\\[\n\\lim_{\\lambda \\rightarrow 1} \\mathbb{E}_{\\theta \\sim q_t(\\theta)} \\left[ \\sum_{i=0}^{n-1} \\frac{\\lambda^i (1 - \\lambda^{n-i})}{1 - \\lambda^n} \\log p(\\mathcal{D}_{t-i} \\mid \\theta) \\right] - \\sum_{i=0}^{n-1} \\frac{\\lambda^i (1 - \\lambda)}{1 - \\lambda^n} D_{KL}(q_t(\\theta) \\mid\\mid q_{t-i-1}(\\theta))\n\\]\n\\[\n= \\mathbb{E}_{\\theta \\sim q_t(\\theta)} \\left[ \\lim_{\\lambda \\rightarrow 1} \\frac{\\lambda^i (1 - \\lambda^{n-i})}{1 - \\lambda^n} \\log p(\\mathcal{D}_{t-i} \\mid \\theta) \\right] - \\sum_{i=0}^{n-1} \\lim_{\\lambda \\rightarrow 1} \\frac{\\lambda^i (1 - \\lambda)}{1 - \\lambda^n} D_{KL}(q_t(\\theta) \\mid\\mid q_{t-i-1}(\\theta))\n\\]\nLet us develop (I) and (II) separately by applying the L'H\u00f4pital's rule. First, for (I):\n\\[\n\\lim_{\\lambda \\rightarrow 1} \\frac{\\lambda^i (1 - \\lambda^{n-i})}{1 - \\lambda^n} = \\lim_{\\lambda \\rightarrow 1} \\frac{i \\lambda^{i-1}(1 - \\lambda^{n-i}) - \\lambda^{n-i} (n - i) \\lambda^{n-i-1}}{-n \\lambda^{n-1}}\n\\]\n\\[\n= \\lim_{\\lambda \\rightarrow 1} \\frac{i \\lambda^{i-1} - i \\lambda^{i-1} \\lambda^{n-i} - (n - i) \\lambda^{n-1}}{-n \\lambda^{n-1}} = \\frac{n - i}{n}\n\\]\nNow, for (II):\n\\[\n\\lim_{\\lambda \\rightarrow 1} \\frac{\\lambda^i (1 - \\lambda)}{1 - \\lambda^n} = \\lim_{\\lambda \\rightarrow 1} \\frac{i \\lambda^{i-1} (1 - \\lambda) - \\lambda^{i-1}}{-n \\lambda^{n-1}} = \\frac{1}{n}\n\\]\nApplying Equations 22 and 23 to TD-VCL objective, we obtain:\n\\[\n\\arg \\max_{\\theta \\in \\mathcal{Q}} \\mathbb{E}_{\\theta \\sim q_t(\\theta)} \\left[ \\sum_{i=0}^{n-1} \\frac{(n - i)}{n} \\log p(\\mathcal{D}_{t-i} \\mid \\theta) \\right] - \\sum_{i=0}^{n-1} \\frac{1}{n} D_{KL}(q_t(\\theta) \\mid\\mid q_{t-i-1}(\\theta)),\n\\]"}, {"title": "F IMPLEMENTATION DETAILS", "content": "Operationalization. For all experiments, we use a Gaussian mean-field approximate posterior and\nassume a Gaussian prior \\( N(0, \\sigma^2 I) \\). We parameterize all distributions as deep networks. For all\nvariational objectives, we compute the KL term analytically and employ the Monte Carlo approx-\nimations for the expected log-likelihood terms, leveraging the reparametrization trick (Kingma &\nWelling, 2014) for computing gradients. Lastly, we employed likelihood-tempering (Loo et al.,\n2021) to prevent variational over-pruning (Trippe & Turner, 2018).\nModel Architecture and Hyperpatameters. We adopted fully connected neural networks. We\nchose different depths and sizes depending on the benchmark, and we provide a full list of hyper-\nparameters in Appendix G. For training, we used the Adam optimizer (Kingma & Ba, 2015). We\nimplemented early stopping with a patience parameter of five epochs, which drastically reduced the\nnumber of epochs needed for each new task.\nWe initialize the prior with variance le-5. In contrast to what was reported by Nguyen et al. (2018),\nwe found no gains in initializing the variational parameters with the maximum likelihood estimate\nparameters. Therefore, we started by sampling from the prior.\nHyperparamter Tuning Protocol. For the proposed methods, we mainly tuned three hyper-\nparameters: n (as in n-Step KL), \\( \\lambda \\) (as in TD-VCL), and \\( \\beta \\) (the likelihood tempering param-\neter). We conducted a grid search for each evaluated benchmark, with \\( n \\in \\{1,2,3,5,8, 10\\} \\),\n\\( \\lambda \\in \\{0.0,0.1, 0.5, 0.8, 0.9, 0.99\\} \\), and \\( \\beta \\in \\{1e^{-5}, 1e^{-4}, 1e^{-3}, 5e - 3, 1e - 2, 5e - 2, 1e^{-1}, 1.0\\} \\).\nFor VCL, we tuned the \\( \\beta \\) hyperparameter in the same way.\nReproducibility. Reported results are averaged across ten different seeds. Error bars represent 95%\nconfidence intervals, while Table 1 shows 2-sigma errors up to two decimal places. We execute all\nexperiments using a single GPU RTX 4090. We provide our implementation code for the proposed\nmethods (TD-VCL and n-Step), as well as considered baselines (Batch MLE, Online MLE, VCL,\nand VCL CoreSet) in https://github.com/luckeciano/TD-VCL."}, {"title": "G HYPERPARAMETERS", "content": "Table 2 provides the shared hyperparameters used in each benchmark. Tables 3 and 2 provided the\nspecific hyperparameters for n-Step KL and TD-VCL methods, respectively."}, {"title": "H SPLITMNIST: ADDITIONAL RESULTS", "content": "H.1 MULTI-HEAD NETWORK\nFigure 7 presents the results for the SplitMNIST benchmark. The first five plots bring the results\nper each task", "benchmark": "even Online\nMLE presents a final average accuracy of around 90%. All variational methods present almost\nperfect results, ranging between 97% and 98%. As stated in Section 5.1, these positive results are\na consequence of using multi-head networks, which end up training different classifiers on top of\nthe same backbone. This architecture would naturally disregard negative transfer and, therefore,\nCatastrophic Forgetting. They also assume a priori knowledge about the number of tasks, which is\nunrealistic for CL settings.\nH.2 SINGLE-HEAD NETWORK\nIn this Section, We re-evaluate the SplitMNIST benchmark by only allowing single-head architec-\ntures.Figure 8 presents the results. As expected, the performance of"}]}