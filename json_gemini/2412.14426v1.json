{"title": "All-in-One Tuning and Structural Pruning for Domain-Specific LLMs", "authors": ["Lei Lu", "Zhepeng Wang", "Runxue Bao", "Mengbing Wang", "Fangyi Li", "Yawen Wu", "Weiwen Jiang", "Jie Xu", "Yanzhi Wang", "Shangqian Gao"], "abstract": "Existing pruning techniques for large language models (LLMs) targeting domain-specific applications typically follow a two-stage process: pruning the pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on specific domains. However, the pruning decisions, derived from the pretrained weights, remain unchanged during fine-tuning, even if the weights have been updated. Therefore, such a combination of the pruning decisions and the fine-tuned weights may be suboptimal, leading to non-negligible performance degradation. To address these limitations, we propose ATP: All-in-One Tuning and Structural Pruning, a unified one-stage structural pruning and fine-tuning approach that dynamically identifies the current optimal substructure throughout the fine-tuning phase via a trainable pruning decision generator. Moreover, given the limited available data for domain-specific applications, Low-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In ATP, we introduce LoRA-aware forward and sparsity regularization to ensure that the substructures corresponding to the learned pruning decisions can be directly removed after the ATP process. ATP outperforms the state-of-the-art two-stage pruning methods on tasks in the legal and healthcare domains. More specifically, ATP recovers up to 88% and 91% performance of the dense model when pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.", "sections": [{"title": "1 Introduction", "content": "Domain-specific LLMs have become indispensable for handling professional tasks such as legal, healthcare, and finance applications (Ling et al., 2024; Jeong, 2024; Zheng et al., 2023). By fine-tuning the general-purpose pretrained LLMs (Wang et al., 2024a,b,c) on domain-specific datasets (Zheng et al., 2024; Susnjak et al., 2024; Xie et al., 2024), the domain-specific LLMs can adapt to the unique terminologies and nuanced contextual requirements of the given domain, producing high-quality outputs of the relevant domain.\nDue to the limited size of domain-specific datasets, full-parameter fine-tuning of LLMs is usually prone to significant knowledge forgetting and performance degradation (Christophe et al., 2024; Wang et al., 2024a; Lin et al., 2024b). To mitigate this issue, Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA) (Hu et al., 2021), have been widely adopted. These techniques enable effective domain alignment while retaining the knowledge and capabilities of the general-purpose LLMs to the greatest extent. However, domain-specific LLMs inherit the substantial computational and memory costs of their general-purpose counterparts, posing significant challenges for deployment (Wan et al., 2023; Stojkovic et al., 2024). Therefore, effective compression techniques are important to deploy the LLMs onto domain applications.\nPruning is one of the most promising solutions to compress the LLMs by systematically removing less significant parameters (Xia et al., 2024; Guo et al., 2023; Gao et al., 2024; An et al., 2024). To construct compact models for cost-efficient deployment, structural pruning is usually preferable to weight-level pruning. Although structural pruning can effectively reduce computational and memory overhead, it often incurs non-negligible performance degradation, especially when the sparsity level is high (e.g., \u2265 0.4). To address it, current pruning techniques typically follow a two-stage pipeline: (1) pruning the general-purpose pretrained model to produce a compact model, and (2) fine-tuning the pruned model, aiming to recover the performance loss incurred by pruning while aligning the model with domain knowledge. The major issue of this pipeline is that the pruning decisions obtained from the pretrained weights during the pruning stage remain unchanged during the fine-tuning stage. However, the optimal substructure of the pretrained model may evolve during fine-tuning since the importance of weights can change when the weights are kept updated. Ignoring such structural evolution is likely to result in sub-optimal pruning decisions for domain-specific applications.\nIn this paper, we propose ATP, All-in-One Tuning and Pruning, a unified one-stage framework for domain-specific structural pruning of LLMs that integrates pruning-decision search and LoRA-based fine-tuning, targeting for the domain application where limited fine-tuning data is available. ATP establishes a dynamic interplay between structural pruning and parameter fine-tuning by continuously updating the pruning decision everytime the weight is updated while constraining the fine-tuning of weights with the current pruning decision simultaneously. More specifically, a pruning-decision generator is introduced to continuously generate pruning decisions based on the updated weights, enabling the exploration of optimal substructures throughout the tuning process. The effect of the current pruning decision on the model output is simulated through a LoRA-aware forward pass without actual model compression. Concurrently, the LoRA-aware structural sparsity constraints are proposed to penalize the LoRA weights associated with the pruned structures indicated by the pruning decisions. The penalization gradually diminishes the contribution of the relevant weights to the final output, such that both the LoRA weights and their corresponding pretrained weights can be effectively removed at the end. The output of ATP is a compact model tailored to the given domain, derived from the final pruning decisions.\nWe evaluated ATP in the HealthCare and Legal domains. Experimental results demonstrate that ATP outperforms conventional two-stage structural pruning in both language modeling and problem-solving capabilities across different domains. Notably, even at high sparsity levels (\u2265 0.4), the performance of models pruned with ATP remains comparable to that of the domain-specific counterpart without pruning.\nOur contributions are summarized as follows:\n\u2022 We propose ATP, a one-stage approach that integrates structural pruning with LoRA-based fine-tuning, optimized for domain-specific applications with limited fine-tuning data.\n\u2022 We design a novel sparsity-constrained tuning method tailored for LoRA by introducing a LoRA-aware forward pass and structural sparsity regularization to LoRA weights.\n\u2022 We conduct extensive experiments in the HealthCare and Legal domains. The results show that ATP outperforms the conventional two-stage structural pruning in domain-specific applications in most of the evaluated settings. More specifically, it recovers up to 88% and 91% performance of the dense model when pruning 40% parameters of LLaMA2-7B and LLaMA3-8B, respectively."}, {"title": "2 Related Work", "content": "LLM Pruning. To reduce the computational cost of machine learning algorithms, model pruning was proposed and has achieved great success in the training and inference stage of conventional machine learning models (Bao et al., 2020, 2022a,b) and convolutional neural networks (CNNs) (Wang et al., 2021; Wu et al., 2020b,a; Wang et al., 2023). Therefore, applying the pruning techniques to LLMs seems to be an intuitive and promising method. However, the new architecture from transformer layers, the huge amount of parameters and the higher expectation of the capability of the pruned model pose new challenges to the pruning of LLMs and the effective method to prune LLMs is still under exploration. Existing pruning methods for LLMs can be classified into unstructured and structured pruning. Unstructured pruning removes individual weights, resulting in a sparse model that maintains the original structure. In contrast, structured pruning eliminates entire channels or layers, producing a smaller model with reduced dimensionality.\nUnstructured pruning methods (Frantar and Alistarh, 2023; Sun et al., 2024; Zhang et al., 2023, 2024b) have shown promising results for general and domain-specific applications, retaining comparable performance with the dense model. Structured pruning further enhances deployment compatibility by removing groups of weights, but identifying optimal pruning patterns while preserving performance is more challenging.\nRecent structural pruning techniques focus on finding optimal pruning patterns. For example, Ma et al. (2023) calculates grouped importance scores to eliminate less significant components, Lin et al. (2024a) utilizes combined matrix decomposition to determine pruning pattern without propagation, Ashkboos et al. (2024) uses orthogonal transformations for matrix slicing, and van der Ouderaa et al. (2024) employs multi-step search strategies to optimize pruning patterns. Most of them typically incorporate an additional post-pruning fine-tuning stage for performance recovery at higher sparsity levels. Such separation prevents mutual interaction between pruning and fine-tuning, failing to consider the change in the optimal pruning pattern due to weight updates (Wu et al., 2024). Hence, bridging such a connection between pruning decisions and tuning is the main focus of ATP.\nLLM Fine-Tuning. Pretraining a representation model via self-supervised learning (Wu et al., 2021b,c) and then fine-tuning the pretrained model to downstream tasks follows the principles of transfer learning, which have been extensively used in computer vision. Transfer learning enables leveraging knowledge gained from a source domain (e.g., a large-scale, general-purpose corpus) to enhance performance in a target domain (e.g., domain-specific datasets) (Zhang et al., 2024c; Bao et al., 2024). This approach reduces the training cost for domain-specific applications compared with training the model from scratch (Wu et al., 2021a, 2022, 2021d; Zhang et al., 2022; Wu et al., 2023). In the development of large language models (LLMs), a similar transfer learning paradigm is adopted. The LLM is first pretrained on a massive general-purpose corpus and then fine-tuned on small domain-specific datasets to align the pretrained general models toward specialized domains. Full-parameter fine-tuning of LLMs (Lv et al., 2023) updates all model parameters, but it presents challenges like high computational demands and the risk of over-fitting, especially with limited data (Zhang et al., 2024a).\nTransfer learning-based PEFT methods (Houlsby et al., 2019; He et al., 2021; Lester et al., 2021; Liu et al., 2021; Hu et al., 2021; Liu et al., 2024; Wang et al., 2024a) have been thus developed to address these challenges, which generally adapt LLMs without model weight update via prompt tuning and pre-fix tuning (Li and Liang, 2021), or train only a small subset of parameters while keeping the rest of the model frozen with extra modules. Among these, LORA (Hu et al., 2021) stands out as a representative method for PEFT due to its efficiency and compatibility. LoRA inserts trainable low-rank matrices into each Transformer layer, allowing adaptation with minimal additional parameters. This reduces computational and memory overhead and helps preserve the generalization of the LLM after parameter tuning. In this work, we build the foundation of ATP based on LoRA-tuning to satisfy the usually limited fine-tuning data within a specific domain."}, {"title": "3 Methodology", "content": "ATP dynamically searches for the optimal pruning decision via a trainable pruning-decision generator alongside the LoRA-tuning process. Upon convergence, the LoRA-weights corresponding to the pruned structures approach zero, allowing for the direct extraction of a compressed and fine-tuned domain-specific LLM guided by the finalized pruning decision.\n3.1 Notations\nTo clarify our methodology, we define the following notations. For a linear projection in LLMs, let $W \\in \\mathbb{R}^{m \\times n}$ be the pretrained weight matrix, and $X \\in \\mathbb{R}^{l \\times m}$ be the input feature. The corresponding LORA module's weights are denoted as $W_a \\in \\mathbb{R}^{m \\times r}$ and $W_b \\in \\mathbb{R}^{r \\times n}$, where m and n are the input and output dimensions, respectively, l is the number of tokens, and r is the LoRA rank. Let $D \\in \\mathbb{R}^{n \\times n}$ be a diagonal pruning-selection matrix with entries in {0, 1}, satisfying $D^2$ = D. We further denote the diagonal vector of D as the pruning decision vector $d \\in \\{0,1\\}^n$, where $d_i$ = 1 signifies that the i-th output dimension is retained, and $d_i$ = 0 indicates pruning. If the output dimension remains unpruned, then D = I, where $I \\in \\mathbb{R}^{n \\times n}$ is the identity matrix.\n3.2 Prunable Groups in LLMs\nWe perform structural pruning in an LLM by group-wise removal on linear projections within decoder layers. Specifically, each row and column of W is treated as an individual prunable group.\nA decoder layer typically consists of two sequential blocks: Attention and Multi-Layer Perceptron"}, {"title": "3.3 Pruning-Decision Generator", "content": "To dynamically generate optimal pruning decisions, we introduce a trainable pruning-decision generator G, which outputs $d_{all}$ = {$d_1$,\u2026\u2026,$d_n$,\u2026\u2026,$d_N$}, a set of N pruning-decision vectors corresponding to each of the N decoder layers. $d_n$ for n-th layer is the concatenation of $d_{QK}$, $d_V$, and $d_{GU}$.\nThe generator G is constructed sequentially with Transformer encoder blocks, followed by fully connected layers that project the output to the dimension of len($d_n$). The Gumbel-Sigmoid (Jang et al., 2016) function, combined with the straight-through estimator (STE), serves as the final output layer to produce decision vectors that closely approximate a binomial distribution. Given trainable weights M of G, the generator produces the set of pruning-decision vectors as:\n$d_{all}$ = G(M).\nSuch design ensures discrete decision generation while maintaining differentiability. The detailed structure of G is shown in the Appendix A.1."}, {"title": "3.4 LoRA-Aware Designs", "content": "We integrate the generated pruning decisions with LoRA-based tuning through two key designs: (1) LoRA-aware forward pass and (2) LoRA-aware sparsity regularization.\nForward pass for training G. Training G aims to search for optimal pruning decisions, making it crucial to simulate the actual pruning effect of the decisions on the output behavior of the LoRA-integrated LLM. To achieve this, we formulate the forward pass of a LoRA-linear projection as:\n$f_G(X) = X(W + W_aW_b)D$,\nwhere the pruned portions of both the pretrained weights W and the LoRA module $W_aW_b$ are ignored based on the current pruning decisions.\nForward pass for LoRA weights tuning. However, during LoRA-based tuning, directly applying D disrupts gradient flow to the currently pruned dimensions, preventing updates to the corresponding LORA parameters. The update stagnation of LORA weights narrows the search space of D, ignoring potentially better pruning decisions. To address this, we formulate the forward pass for LoRA-Linear training as:\n$f_L(X) = X(WD + W_aW_b)$,\nwhere the pretrained weights are temporarily masked by D, while the LoRA parameters remain fully trainable.\nLoRA-aware sparsity regularization. As we aim to directly remove G after the ATP process with minimal negative effects, we incorporate sparsity regularization on the LoRA weights to ensure that weights groups pruned by D approaches 0. Specifically, we apply LoRA-aware group lasso regularization to drive the pruned portions of the LORA weights toward zero during training.\nSpecifically, we separately constrain the rows in $(I - D_{prev})W_a$ and the columns in $W_b(I \u2013 D)$ at the decided pruned positions to achieve alignment of structural sparsity between LoRA weights and W, where $D_{prev}$ denotes the pruning matrix of the previous layer, consequentially pruning the input dimensions of the current layer. Thus, the LoRA-aware regularization term is defined as:\n$L_{gl} = \\sum_{g_a \\in G_a} \\|g_a\\|_2 + \\sum_{g_b \\in G_b} \\|g_b\\|_2$,\nwhere $G_a$ represents the set of row groups in $(I \u2013 D_{prev})W_a$, $G_b$ represents the set of column groups in $W_b(I \u2013 D)$, respectively and $||\\cdot||_2$ denotes the $L_2$-norm. The unpruned groups are not affected by this sparsity regularization.\nBy employing LoRA-aware sparsity regularization, we ensure that the model gradually approaches the structural sparsity dictated by the pruning decisions during the LoRA tuning process while preserving the flexibility for decision updates within the ATP process."}, {"title": "3.5 ATP Algorithm", "content": "The overall pipeline of ATP is shown in Fig.2. We further introduce the ATP algorithm design, as detailed in Alg.1 and Alg.2.\nWe first construct the pruning-decision generator G according to the configuration of the target LLM. The initial pruning-decision vectors are all ones, indicating no pruning at the start, and are gradually refined to identify optimal pruning decisions."}, {"title": "4 Experiment and Analysis", "content": "4.1 Experiment Setup\nWe primarily adopt the experimental setup from Zhang et al. (2024b) for two specific domains: HealthCare and Legal.\nWe evaluate the performance of the pruned and domain-aligned LLM from: language modeling capability, and mult-task solving ability of natural language inference (NLI), question answering (QA), and summarization under the corresponding domain. We construct the domain-specific training and calibration datasets from MedNLI (Romanov and Shivade), PubMedQA (Jin et al., 2019), HQS (Abacha and Demner-Fushman, 2019) for HealthCare, and from CaseHold (Zheng et al., 2021), BillSum (Kornilova and Eidelman, 2019) for Legal.\nWe mainly compare ATP against two state-of-the-art structural-pruning methods LLM-Pruner (Ma et al., 2023) and SliceGPT (Ashkboos et al., 2024), where both can be categorized as two-stage pruning and tuning methods. We choose LLaMA2-7B (Touvron et al., 2023) and LLaMA3-8B (Dubey et al., 2024) from the LLaMA model family as the pretrained dense models.\nDetailed dataset construction, sample template, evaluation metrics, and hyperparameters are provided in Appendix A.2.\n4.2 Main Results\nThe overall evaluation results are shown in Tab.1. From the table, we observe that ATP greatly outperforms LLM-Pruner and SliceGPT, two-stage pruning methods, in domain-specific language modeling and summarization capability. For language modeling, ATP achieves the lowest perplexity under most settings, with its advantages being especially evident in the Legal domain, where the samples are long enough to contain rich domain-specific contexts. For summarization tasks, ATP maintains performance comparable to dense models even at higher sparsity levels while LLM-Pruner and SliceGPT struggle to capture the critical meanings and generate high-quality summaries. Although pruning impacts deterministic label prediction tasks like MedNLI and PubMedQA, ATP achieves the best results in most cases, with minor exceptions. Notably, ATP retains 75% ~ 88% of the relative performance of the original dense model for LLaMA3-8B when pruning 40% to 50% of the parameters. For LLaMA2-7B, ATP achieves an even higher relative performance of 81% ~ 91%, significantly outperforming SliceGPT and LLM-Pruner in both cases."}, {"title": "4.3 Analysis", "content": "Finalized Pruning Decisions. In Fig.4, we visualize the final pruning-decision set $d_{all}$ generated from G for the LLaMA2-7B model at a 50% sparsity level under the HealthCare domain. The distribution of the layer-wise decision vectors $d_n$ reveals a general trend where earlier layers are pruned less, and later layers more. This aligns with the hierarchical nature of LLMs: early layers capture general features essential for understanding the input, while later layers specialize in domain-specific details that can be pruned more aggressively. Interestingly, the first layer stands out as an outlier with a significantly higher pruning ratio than neighboring layers. We suspect this is due to the dataset consisting entirely of domain-specific tasks, where the first layer plays a relatively smaller role in domain adaptation.\nOptimal Subtructural Evolution. To demonstrate that changes in weight importance through parameter tuning can influence pruning decisions, we train the pruning-decision generator G using the same initialized M, based on the HealthCare calibration dataset targeting at 50% sparsity level, on the pretrained LLaMA2-7B model without LoRA fine-tuning. This allows us to identify the optimal pruning decisions on the pretrained weights only. We then compare these generated decisions, $d_{all_{pt}}$ with $d_{all}$ from ATP, which accounts for the fine-tuning process. We mathematically evaluate the decision difference ratio via the normalized Hamming distance between two decision vectors. As shown in Fig.3, the difference ratio of each pruning decision ranges from 20% to even 55%. Mathematically, this can be attributed to the changes in weight importance due to updates during the fine-tuning process. From a more abstract perspective, such decision evolution could be explained as fine-tuning enables the pruned model to recover certain capabilities more easily, while other capabilities are harder to restore. Consequently, pruning decisions are adjusted to retain weights associated with less recoverable capabilities, while pruning those that can be more easily re-established during fine-tuning. Thus, incorporating dynamically-adjusted pruning decisions together with parameter tuning is meaningful as conducted in ATP.\nEffect of sparsity level p. In Fig.5(a), we visualize ATP's performance on LLaMA2-7B in the HealthCare domain under different sparsity levels, p = {0.3,0.4, 0.5,0.6}. As expected, all metrics degrade as p increases, consistent with the general behavior of pruning. Notably, when p increases from 0.3 to 0.5, the loss in language modeling capability and the closely related summarization performance remains minimal, while the decline in label prediction tasks such as NLI and QA stays within an acceptable range. This highlights ATP's ability to maintain performance even under significant sparsity constraints. We recommend a 50% sparsity level with ATP as it achieves a good balance between model size and specialized performance. However, when p increases to 0.6, a sharp performance drop across all metrics occurs, suggesting that fine-tuning can no longer effectively align a heavily pruned pretrained model to the target domain under the same configuration.\nTraining Dynamics. In Fig.5(b), we visualize the loss curves of ATP for HealthCare on LLaMA2-7B targeting a 50% sparsity level. The first row and the second row refer to the loss curves of G and the target LLM in Eq.6 and Eq.7, respectively. The dynamics of $L_s$, which correlates with the current sparsity level, are primarily determined by \u03b1, while other losses are observed to be mutually affected by both \u03b1 and \u03b2. In general, lower values of \u03b1 and \u03b2 tend to facilitate more stabilized training and better language modeling on the calibration dataset but result in a slower approach toward the desired sparsity level. Moreover, through joint analysis of $L_{gl}$ and LLM_G, we observe that due to the small size of the calibration dataset for G training, a small portion of the pruning decisions may change depending on the calibration samples. This is because it is difficult to find the theoretically global optimal decision for every sample. Thus, freezing G and increasing \u03b2 are crucial to facilitate the coverage of the pruned portions of LoRA weights towards zero and to achieve the desired structural sparsity after the ATP process.\nCase Study. We perform a summarization case study on a vaccine-related HealthCare question using LLaMA2-7B with 50% sparsity level across all methods. As shown in Tab.2, the reference summary highlights two critical points: extra dose and infant. However, SliceGPT overlooks the infants, providing a more generic response, while LLM-Pruner fails to capture the concern of the extra dose. In contrast, ATP successfully incorporates both key points, demonstrating the stronger language modeling capability over other methods. More case study examples are given in the Appendix A.3."}, {"title": "5 Conclusion", "content": "In this work, we introduce ATP, a novel one-stage structural pruning and tuning method for domain-specific LLM compression. More specifically, ATP integrates dynamically adjusted pruning decisions with sparsity-regularized fine-tuning, leveraging a LoRA-aware design to effectively address the challenges posed by limited tuning data in specialized domains. Extensive experiment results demonstrate that ATP greatly outperforms the conventional two-stage methods in language modeling and multi-task solving capabilities in specific domains under diverse settings. We believe such a unified one-stage framework and the related findings would open new possibilities for domain-specific LLM pruning."}, {"title": "6 Limitations", "content": "While ATP demonstrates its effectiveness in domain-specific LLM compression, it still has some common issues observed in existing pruning techniques. For example, in summarization tasks, for a small subset of input samples, the pruned model may occasionally get stuck repeating certain words or short phrases although the likelihood of this issue is reduced compared to other methods.\nFurthermore, achieving extremely high sparsity levels (i.e., p \u2265 0.6) remains a significant challenge as we observe a relatively substantial performance drop when transitioning from p = 0.5 to p = 0.6, indicating that the aggressive removal of parameters at such sparsity levels can disproportionately affect the model's ability to retain and recover critical knowledge. Such degradation remains difficult to mitigate quickly, suggesting that additional strategies are required to ensure the model's resilience under extreme sparsity constraints.\nFuture work may aim to address these limitations to unlock the full potential of our proposed ATP method for practical and scalable deployment in domain-specific applications."}, {"title": "7 Ethical Considerations", "content": "In this paper, we focus on pruning and fine-tuning domain-specific LLMs. Data samples for specialized domains such as healthcare, legal, medical, and finance often have the potential to contain sensitive or private information, raising critical ethical and privacy concerns. Here, we make it clear that all training and testing samples used in our experiments for both the HealthCare and Legal domains are drafted exclusively from public open-source datasets. These datasets have undergone rigorous pre-processing before being made publicly to eliminate any personal or sensitive information to minimize the risk of private data leakage. For example, in the HQS dataset, a raw sample would look like: \u201cMy name is [NAME] from [LOCATION], my 10-year-old boy has Duchenne muscular dystrophy...\u201d with all personal information removed."}, {"title": "A Appendix", "content": "A.1 Pruning-Decision Generator G\nIn general, G does not take external inputs to generate the pruning-decision vectors. The adjustment of pruning-decision vectors is achieved by updating the trainable parameters in G.\nNetwork Architecture\nIn our implementation, the pruning-decision generator G, as shown in Tab.3, sequentially consists of the following components:\nSelf input. The input of G is a frozen orthogonally-initialized nn.Parameter with shape (N, 64), where N is the total number of decoder layers of the target LLM.\nTransformer Encoder. The input is firstly fed into 2 sequential Transformer encoder blocks (Vaswani et al., 2017). Each block uses a multi-head self-attention mechanism with 4 attention heads and a feed-forward network with an intermediate dimension of 256 and the ReLU activation function (Krizhevsky et al., 2012). The encoders process the input into a tensor of shape (N, 64), representing the layer-wise intermediate representations.\nLayer Normalization. A layer normalization (Ba et al., 2016) module is then applied to the intermediate representation, ensuring stabilized training subsequent computations. The output of this step maintains the shape (N, 64).\nLayer-Wise Decision Projection. The output of the previous LayerNorm is then projected to the desired length, counting for all pruning decisions for a single decoder layer in the target LLM. Specifically, the projected dimension = dhead * 2 + dint, where dhead is the head-wise attention dimension and dint is the intermediate projection dimension in MLP. For example, for LLaMA2-7B, dhead = 128, dint = 11008, thus the projected dimension is 128 * 2 + 11008 = 11264, counting for total the pruning decisions for DQK, Dv and DGU as mentioned in the paper. The output shape of the decision projection is (N, dhead * 2 + dint).\nGumbel-Sigmoid Sampling and Binary Mask Conversion via STE. Then the projected output is sampled via Gumble-Sigmoid with sampling temperature T = 0.4 and offset base = 3 to approximate binomial distribution for each decision. We consider the decision elements in the output tensor after Gumbel-Sigmoid Sampling with the shape (N, dhead *2+ dint) as 'soft' decisions."}, {"title": "A.2 Detailed Experiment Setup", "content": "Dataset Construction\nWe construct the domain-specific training datasets following the same settings of D-Pruner (Zhang et al., 2024b).\nFor the HealthCare domain, we create the fine-tuning dataset Dt by combining MedNLI, PubMedQA, and HealthQuestionSummary (HQS) in a ratio of 7:7:1, resulting in a total of 15,000 samples. For the Legal domain, we select data from CaseHold and BillSum in a ratio of 13:2, also totaling 15,000 samples. All training samples are obtained by selecting the first n samples from the 'train' split of each respective dataset.\nThe calibration dataset De for both domains contains 1,000 samples, with a ratio of 1:3:1 from MedNLI, PubMedQA, and HQS for the Health-Care domain, and 1:1 from CaseHold and BillSum for the Legal domain. Inspired by the concept that general weight importance obtained from open-domain datasets enhances the model's adaptability and generalization across multiple tasks, we further extract the first 300 samples from the C4 (Raffel et al., 2020) dataset and append them to each of the two domain-specific calibration datasets, resulting in a total of 1,300 samples for each domain-specific calibration dataset.\nIt is worth mentioning that we DO NOT include any samples from HarrisionTextBook (a widely recognized and authoritative medical textbook for HealthCare) and MultiLegalPile (Niklaus et al., 2023) (a large dataset consisting various documents relevant to law and legal proceedings for Legal) in either Dt or De.\nFor HealthCare performance evaluation, we choose the first 300 paragraphs from HarrisionText-Book, the entire 1422 test samples in MedNLI \u2018test' split, the first 500 test samples in PubMedQA 'test' split, and the entire 100 test samples in HQS 'test' split. For Legal performance evaluation, we choose the first 300 samples in the 'en_legislation_US' split of MultiLegalPile, and the first 200 test samples in BillSum 'test' split.\nSample Template\nD-Pruner (Zhang et al., 2024b) mentions that all the samples are formulated following Alpaca template (Bommasani et al., 2021), thus we follow this for all the samples except for those from HarrisonTextBook and MultilegalPile because they are for perplexity evaluation only. For better understanding, we provide our formulated template for MedNLI as an example:\n\"Below is an instruction that describes a task related to HealthCare, paired with an input that provides further context. Write a response that appropriately completes the request.\nInstruction: Determine the relationship between the HealthCare Premise and the Hypothesis from 'entailment', 'contradiction', 'neutral'.\nInput: Premise: \u2018{sentence1}', Hypothesis: '{sentence2}'.\nResponse: Their relationship is {label}. \u201c,\nwhere sentencel, sentence2, and label are the corresponding Premise, Hypothesis, and ground-truth labels extracted from each drafted sample from MedNLI. Samples from other datasets are also formulated in a similar way.\nEvaluation Metrics\nWe evaluate the pruned models' language modeling (linguistic) capabilities using perplexity scores on HarrisonTextBook and MultiLegalPile, corresponding to the HealthCare and Legal domains, respectively. The natural language inference (NLI) abilities are assessed through prediction accuracy on MedNLI. For question answering (QA), we evaluate the models using Macro-F1 scores on PubMedQA since the data distribution in PubMedQA is super biased and the majority of the answers are 'yes'. Summarization capabilities are measured using ROUGE scores (Lin, 2004) on HQS and Bill-Sum for HealthCare and Legal domain. Considering the randomness in next-token sampling, we conduct each evaluation 3 times except for perplexity evaluation (as perplexity calculation does not involve token sampling), to report the average score of it.\nTo better demonstrate the overall performance of the pruned model compared to the dense model, we formulate the Relative Performance of a pruned model as follows: For metrics within a domain,\nRelative Performance = $\\frac{1}{n} \\sum_{i=1}^{n} \\frac{Score_{p,i}}{Score_{d,i}}$\nWhere Scorep,i and Scored,i are scores for the pruned and the dense model for ith task separately. For summarization metrics such as ROUGE R1, R2, and RL, we define the performance ratio between the pruned model and the dense model as: $\\frac{Summary p}{Summary d} = \\frac{1}{3} (\\frac{R1_p}{R1_d} + \\frac{R2_p}{R2_d} + \\frac{RL_p}{RL_d})$. This ratio,$\\frac{Summary p}{Summary d}$, is then inserted into Eq. 9 and averaged with scores from other tasks..\nBaselines\nTo ensure a fair comparison, both LLM-Pruner and SliceGPT use the same calibration dataset for stage 1 (pruning), the same training dataset for stage 2 (post-pruning tuning), and the same LoRA tuning hyperparameters as ATP. To achieve this, we hand-craft the dataset formulation part of their provided post-pruning tuning Python scripts to integrate into our formulated domain-specific datasets as their original scripts only support general datasets from Huggingface.\nTraining Details\nWe conduct the training for every method on 8\u00d7 NVIDIA A100s with 80G cuda memory. For every method, no samples were truncated.\nLLM Loss Modeling. Unlike D-Pruner (Zhang et al., 2024b), which models the next-token prediction loss solely on the reponse, we adopt the tuning approach from Shi et al. (2024) in our implementation, where the next-token prediction loss is computed on the entire sentence (instruction + response). We consider this adjustment necessary because we find a large portion of response in the\""}, {"title": "A.3 More Case Study", "content": "We provide additional case studies on summarization tasks on both domains as shown in Tab.4 and Tab.5 based on LLaMA2-7B under 50% sparsity level. The results demonstrate that our method produces summaries that better capture the key points of the input with better domain-specific language consistency, while SliceGPT and LLM-Pruner relatively miss critical details or fail to align with the specialized terminology required for each domain."}, {"title": "A.4 Reported Results of LLM-Pruner", "content": "D-Pruner (Zhang et al., 2024b) also"}]}