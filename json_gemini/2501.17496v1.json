{"title": "SemML: Enhancing Automata-Theoretic LTL\nSynthesis with Machine Learning", "authors": ["Jan K\u0159et\u00ednsk\u00fd", "Tobias Meggendorfer", "Maximilian Prokop", "Ashkan Zarkhah"], "abstract": "Synthesizing a reactive system from specifications given in\nlinear temporal logic (LTL) is a classical problem, finding its applications\nin safety-critical systems design. We present our tool SEMML, which won\nthis year's LTL realizability tracks of SYNTCOMP, after years of domi-\nnation by STRIX. While both tools are based on the automata-theoretic\napproach, ours relies heavily on (i) SEMantic labelling, additional informa-\ntion of logical nature, coming from recent LTL-to-automata translations\nand decorating the resulting parity game, and (ii) Machine-Learning\napproaches turning this information into a guidance oracle for on-the-fly\nexploration of the parity game (whence the name SEMML). Our tool fills\nthe missing gaps of previous suggestions to use such an oracle and provides\nan efficeint implementation with additional algorithmic improvements.\nWe evaluate SEMML both on the entire set of SYNTCOMP as well as\na synthetic data set, compare it to STRIX, and analyze the advantages\nand limitations. As SEMML solves more instances on SYNTCOMP and\ndoes so significantly faster on larger instances, this demonstrates for\nthe first time that machine-learning-aided approaches can out-perform\nstate-of-the-art tools in real LTL synthesis.", "sections": [{"title": "1 Introduction", "content": "Synthesis of finite systems from their logical specifications has been one of\nthe central topics of theoretical computer science since the times of Church [7]\nand B\u00fcchi [5], being closely linked to developments of the automata theory [40].\nIndeed, the logical formula would be translated to an automaton, in fact a game\nover this automaton played by the environment and system players, where the\nstrategy of the latter corresponds to an implementation of the specified system.\nSince Pnueli's suggestion to use Linear Temporal Logic (LTL) [30] for describ-\ning relevant properties of reactive systems, LTL synthesis [31] has become an\nappealing alternative to manual implementation followed by LTL model checking.\nIndeed, the tedious and error-prone implementation and debugging could be\ncircumvented by automated construction of systems or their controllers, which\nare then correct \u201cby construction\u201d. Nevertheless, the 2-EXPTIME-completeness\nof LTL synthesis, stemming from the doubly exponentially sized parity automata\nfor the LTL formulae, has been challenging the practical applicability of the\nwhole concept. Fortunately, this has also led to numerous advances, such as\nidentification of subclasses of properties for which the problem becomes easier,\ne.g. [2,29,3], or methods avoiding the notoriously expensive step of determinizing\nthe automata, e.g. [23,41] or employing antichain-based methods, e.g. [4,6].\nThe breakthrough of directly constructing deterministic automata orders\nof magnitudes smaller [18,13] has brought the classical automata-theoretic ap-\nproach back on the stage, and indeed as the most efficient approach available.\nThis is witnessed by STRIX [26], a synthesis tool based on the translations\nof Rabinizer/OWL[22,21] tools, winning the LTL tracks of the main synthesis\ncompetition SYNTCOMP[16].\nSemantic Labelling and Previous Work. The dramatic improvements in the\nsize came with an interesting side-effect. In contrast to determinization of Safra\n[35] and others [28,36], the new constructions are following the logical structure\nof the formula. Consequently, the states of the generated automaton/game are\nlabelled by this additional information. It consists of the formula describing\nthe property yet to be satisfied, i.e. monitoring the progress of satisfaction of\nthe original formula, and formulae capturing progress of all its subformulae.\nFor example, an input formula \u00aba VGF(a^ X b) labels the initial state of the\nautomaton together with the sub-goal aXb to be satisfied infinitely often,\nsee Fig. 1. After reading a, the successor state is labelled by the remaining goal\nGF(aXb) as well as the progressing sub-goal b left to be satisfied. Under b the\nautomaton then moves to the state with the first component remaining forever\nthe same goal GF(aXb), but the second component, now being satisfied fully,\nsignals that one repetition of the sub-goal has been successfully finished."}, {"title": null, "content": "While this labelling was left unused for years, it clearly offers additional\ninformation. Indeed, for instance, seeing \u00aca VGF(a/Xb) as a goal, it seems\neasier to choose a in order to satisfy it than to take care of the infinitely\nrepeating sub-goal. Similarly, if progress is made in satisfying a X b by choosing\nan a, it seems wasteful not to follow with a b, although the overall goal of\nGF(aXb) remains unaffected either way. Such a guidance can be used to\nexplore the automaton/game on-the-fly and possibly finding a winning strategy\nbefore the whole state space is constructed. In contrast, from the traditional\nperspective of solving games on graphs, one can either solve the whole game,\nor possibly explore it on-the-fly \"blindly\" since there is no observable difference\nbetween taking a transition, say, to the left or right. Note that the guidance need\nnot be reliable, the correctness is still guaranteed by solving (a part of) the game,\nhinting at possible use of machine learning."}, {"title": "In [19], two attempts have been made to explore the automaton/game in a\nprofitable order using this additional information. Firstly, the first component\nis subject to a na\u00efve heuristic called trueness, estimating the ease to satisfy a\nformula (by considering every formula as a Boolean combination, ignoring the\ntemporal structure, and counting the percentage of satisfying assignments), and\nthen transitions with higher trueness are explored first. Secondly, reinforcement\nlearning has been used with rewards being related to satisfying sub-goals in\nthe second component. While the former is also implemented in STRIX, both\nare ad-hoc heuristics with limitations. In [20], a machine-learning approach has\nbeen suggested, which learns from solving games for other formulae estimating\nwhich transition is more often \"winning\" than others. This allows for superior\nprecision, also dealing natively with more convoluted choices where the hand-\nwritten heuristic struggles. However, only this oracle was implemented, not a\n(competitive) synthesis procedure.\nOur Contribution. In the present tool paper, we show how we incorporate this\napproach into the whole synthesis pipeline, closing the gaps explicitly left open.\nBesides, we report on our tool efficiently implementing the approach and, even\nin a preliminary version, winning this year's edition of the realizability track\u00b9 of\nthe SYNTCOMP competition. In more detail, our contribution is as follows:\n- We implement a machine-learning heuristic guiding the on-the-fly exploration.\nIn contrast to [20] using SVM, we evaluate various models and choose the most\nadequate option. Besides, we adapt it to the state-of-the-art semantic labelling.\n- We incorporate it into our synthesis pipeline, which improves over the approach\nof STRIX in several (traditionally algorithmic) aspects.\n-We report on the performance of our tool SEMML (short for SEMantic-labelling-\nbased Machine Learning) and analyze why it performs better than STRIX. It\nis worth noting that SEMML is faster on the SYNTCOMP benchmark set\nwhile being trained only on synthetic data. On this synthetic data, it is even\nan order of magnitude faster.\nNote that other lines of research that use LTL synthesis solvers as blackbox, e.g.\nLTL modulo theories synthesis [34] or portfolio solvers such as NEUROSYNT [8],\ndirectly profit from these improvements.\nFurther Related Work. Besides STRIX, the closest to our work is, on the\none hand, SPOT (with 1tlsynt) [33], following the same automata-theoretic\napproach, but constructing the whole automaton; and on the other hand, purely\nmachine-learning approaches such as the deep-learning-based [37,9], implemented\nin NEUROSYNT [8], which guesses circuits using ML, falling back to STRIX to\nachieve completeness. Before the automata-theoretic approach, further winning\napproaches included bounded synthesis, e.g. [11,14], or even earlier safraless"}, {"title": "2 Tool Description", "content": "In this section, we provide an overview of our tool SEMML. We formally state the\nproblem it is solving, describe how to use the tool, and its high-level approach.\n2.1 Problem Description: LTL Synthesis and Realizability\nThe problem of LTL reactive synthesis is defined as follows. We are given an LTL\n[30] formula \u03c6 over a set of atomic propositions AP together with a partition of AP\ninto environment and system propositions AP = APENVUAPsYs. The environment\nand system generate an infinite word as follows. In each step i, the environment\nchooses ei CAPENV and then the system chooses si \u2286 APsys, generating a\nsequence e1, 81, C2, S2, .... The combined word over AP is then e\u2081 U81, 62 U $2,...\nand the system wins if this word satisfies 4. The central question is whether the\nsystem has a winning strategy, i.e. a way to choose si based on the current prefix\nso that the combined word always satisfies the given formula.3 In that case, the\ninstance is called realizable and unrealizable otherwise. For example, the formula\n$ = G(r\u21d4 X g) with APsys = {r} and APENV = {g} prescribes that whenever\nthe environment sends a request, the system should in the next step grant the\nrequest, and only then. This formula is realizable, and a winning strategy is to\nremember whether the environment sent a request in the previous step.\nDeciding whether a formula is realizable or not is called LTL realizability. In\nsynthesis in the narrower sense, we want to output such a strategy for the winning\nplayer, i.e. a procedure that at every step outputs the next choice, typically in\nthe form of a finite state machine, e.g. a Mealy machine or an AIGER circuit [1].\nWhile our tool can output the strategy in a straightforward way, we refrain from\ndiscussing it further, as it neither the focus of the tool nor of our advancements.\n2.2 Functionality\nInputs/Outputs SEMML accepts the standard format TLSF [15] (used in SYNT-\nCOMP), converted to LTL using syfco, as well as explicit input, i.e. an LTL\nformula together with a partition of the atomic propositions. It supports both\nrealizability and synthesis (with the strategy encoded as AIGER circuit).\nUsage To streamline interaction, SEMML is invoked through a Python wrap-\nper. For TLSF input, use main.py --tlsf <path to tlsf file>, and for ex-\nplicit input main.py --ins=<ins> --outs=<outs> -f=<formula>, where ins"}, {"title": null, "content": "In line with the automata-theoretic approach, SEMML employs on-the-fly con-\nstruction of the corresponding parity game. In SEMML, this process comprises\nthree main components, namely frontier exploration, partial game solving, and\nbacktracking, also outlined in Fig. 2. In a nutshell, starting from an empty\ngame, SEMML explores a \"minimal\" frontier. Here, we employ a sophisticated\nmachine-learning (ML) guidance that, based on the semantic labelling, decides\nwhich parts of the game to explore first. Then, our parity game solver tries to\nfind a solution, interpreting unexplored states as losing for either player. If the\nsolver finds a solution, we are done. If not, we need to explore more of the game.\nWe refer to a backtracking heuristic to identify candidate states and, starting\nfrom there, go back to frontier exploration. In this setup, the main purpose of\nthe new ML component is to tackle hard cases, where the automaton is too large\nto be constructed in its entirety, by trying to identify a small part where one of\nthe players already wins.\n3 Advancements in Detail\nIn this section we describe the major advancements of SEMML. Recall that\nour technical goal is to employ ML to guide on-the-fly synthesis towards \"easily\nwinning\" regions and thus improve scalability. To describe our approach and\ncontributions, we first discuss the state of the art. Here, we consider the tool\nSTRIX, which is currently the only implementation of this approach competitive\non the standard SYNTCOMP benchmarks, and [20], which provides the first\nML-based approach to exploiting the semantic labelling. In particular, we discuss\ntheir individual shortcomings and incompatibilities. Then, we outline how we\nsolve these issues, and describe our solution approach in detail.\n3.1 State of the Art and its Shortcomings\nSTRIX alternates between exploring the parity game and trying to solve the\nexplored part. To decide which states to explore, STRIX uses a global double-"}, {"title": "3.2 Locally Guided Exploration", "content": "Recall that the overall approach is to alternate between exploring parts of the\nparity game and checking whether there exists a winning strategy in the current\npart of the game already, as depicted in Fig. 2. For the exploration, our aim is\nto follow \"good\" choices for one player that work well against all options of its\nopponent. As such, we run the exploration for both \u201cperspectives\" separately, and\nregularly switch between them (further motivation and details in Sec. 3.4). In the\nfollowing, we take the perspective of the system (aiming to prove realizability);\nthe dual part for the environment is analogous.\nAs hinted in Sec. 2.3, our exploration approach comprises two parts, namely\nfrontier exploration and backtracking. The components of frontier exploration\nand its interplay with both backtracking and the game solver is depicted in Fig. 3.\nDuring frontier exploration, the core idea is to only explore a necessary minimum\nso that a strategy of the system can be at all properly defined. In particular, we\nwant to reach a point where every known system state has at least one of its\nsuccessors explored and every known environment state has all of its successors\nexplored. If that is the case, we call the (partial) parity game closed. Clearly,\nto obtain a closed game, we repeatedly need to explore states. To this end, we\nmaintain a queue of automaton states (the current frontier) which we still need\nto explore. After taking a state from the queue, we compute the immediate\nautomaton successors, using an adapted implementation of OWL, and split this\nautomaton transition (under a subset of AP) into the two moves of the players\nunder a subset of APENV and APsys, respectively. While we hardly have a choice\nfor environment states (we need to explore all successors in the game), in system\nstates we can select which successor to explore. Thus, in these states we ask our\nexploration heuristic for advice, and add all newly reached states to the queue.\nFor this local guidance, we employ the new ML-based approach, which we later\nexplain in depth. For an example, see Fig. 3 (bottom). From left to right, we\nobtain a state qo from the queue and construct its successors 91 and q2 in the\nautomaton. Splitting it into the game introduces the two system states s\u2081 and\n82. In both states, the exploration heuristic recommends going towards 92, and\nwe only add that state to the frontier queue.\nOverall, we repeat this process until the current game is closed, and then\nattempt to solve it. If we cannot determine a winner, then in at least one of\nthe system states a \"wrong\" successor was chosen (the system cannot win the\ncurrent partial game). Thus, subsequently, we ask the backtracking oracle which\nstates might have been \"wrong\". Concretely, we heuristically choose a subset of\nall non-fully explored states with the highest trueness. For each of these, we\nexplore their next best successors according to our heuristics. Now, the game\nmight not be closed, and thus we switch back to frontier exploration. We repeat\nthis process until a winner is found (which always happens, as eventually the\nentire game is explored)."}, {"title": "3.3 Exploration Guidance Through Machine Learning", "content": "In this section, we describe our ML approach used to guide the frontier exploration.\nRecall that for a given state the exploration heuristic is supposed to give a ranking\npreferring \"good\" edges which lead to a winning strategy: Initially, we follow the\nhighest ranked choice, then, when backtracking in this state, the second one, and\nso on. Thus, we would like this heuristic to prefer edges that can be part of a\nwinning strategy. Additionally, as we also want to obtain small games, among the\npossibly winning edges we would like to prefer edges leading to smaller strategies\n(hence exploring a smaller part of the game). Note that one can also employ\nhandcrafted heuristics instead, e.g. the score computed by STRIX (which we also\nimplement and evaluate).\nSimilar to [20], we employ a supervised learning approach. As usual for ML,\nwe start by describing the dataset used to train our models. We then discuss\nthe overall architecture of the model(s), how we obtain the ground truth, and\nhow we extract features. Finally, we discuss the training method. Of course, our\napproach is not the only possible way to tackle this problem. Yet, while designing\nit, we discovered several subtle pitfalls and tried alternative approaches which\nproved to be suboptimal. We provide further details within this section.\nData Our explicit aim is to exploit structure in real-world formulae. Here,\nexisting datasets such as the SYNTCOMP set seem a natural choice. However,\nwe want to evaluate our approach on the entire SYNTCOMP set (in order to\nfaithfully replicate the SYNTCOMP evaluation). Thus, \u201cshowing"}, {"title": null, "content": "\u201cbuilding blocks\u201d, they generate formulae by sampling assumptions and guarantees\nand assemble them in the form of \u201cconjunction of assumptions implies conjunction\nof guarantees\u201d, which adds some comprehensible structure. Formulae of this kind\ncan be interpreted as \u201cif the environment adheres to one behaviour profile, the\nsystem should adhere to another\u201d.\nWe extend this idea a bit further by sampling several options for system and\nenvironment, which diversifies the formulae while maintaining comprehensibility.\nIn particular, we sample multiple sets of assumptions and guarantees and assemble\na formula in the form of \u201cDNF of assumptions implies DNF of guarantees\u201d.\nIntuitively, these formulae mean \u201cIf the environment follows one of these behaviour\nprofiles, the system should adhere to one of their behaviour profiles\u201d. In particular,\nthis introduces options for the system to which behaviour profile it should adhere\nto, which in turn might depend on the profile the environment chooses.\nWe filter our generated data into two groups depending on the size of the\ncorresponding automaton. The training and validation group consists of 1000\nformulae where the automaton has at most 500 states. We introduce this limitation\nto keep the ground truth and feature computation feasible (described later). While\n1000 formulae may seem like a small data set, note that we learn from the local\ndecisions in each state of the 1000 associated parity games, which give several\nmillion data points in total. For evaluation, we also identified 200 formulae of\nwhich the automaton size is not known except that it is larger than 20,000 states.\nWhile this yields a decent data foundation for our venture, the synthetic data\ndefinitely is quite different from SYNTCOMP. Thus, for practical purposes,\none should consider including SYNTCOMP and other data during learning.\nSince this is orthogonal to the evaluation in this paper (including any part\nof SYNTCOMP in our training data would introduce unwanted biases), we\ndeliberately do not include this.\nModel Architecture Similar to [20], we rank outgoing edges through all\npairwise comparisons. Formally, we employ a pair classifier p : E \u00d7 E \u2192 R\nwhere the sign denotes whether the first or second edge is preferred and the\nmagnitude denotes the confidence in that prediction. In a state, every pair of\nedges is compared and each edge is ranked according to the sum of confidences\nin its favour. However, since this scales quadratically in the number of outgoing\nedges, we approximate the above for states with more than 16 edges. For them,\na number of pivot edges are chosen that every other edge is compared to, in\norder to obtain a \"first guess\" at a ranking. From that guess, the best 8 are\nselected to enter the second round which now is a full round of comparisons. The\nfinal ranking comprises the ranking among the top 8 followed by the other edges\naccording to the first ranking.\nMoreover, similar to [20], we pre-classify states into groups with conceptual\ndifferences and train a separate model for each group. Intuitively, we distinguish\n(i) whether a state is owned by the system or the environment, and (ii) whether\nthe long-term goals are trivially structured, e.g. a single liveness condition, which\nsimplifies some decisions; leading to 4 models in total. We discuss the concrete\nimplementation of the pair classifier in the \"Training\" section below."}, {"title": "Ground Truth For the supervised learning of our models, we need meaningful\nlabels that denote the quality of an edge so that we can determine the better one\nof any given pair. But which edges are \"good\"? At first, this may seem obvious\nsimply take all edges which are part of a winning strategy. This however is\nproblematic for multiple reasons, as already outlined in [20].\nFirst, parity games do not allow for maximally permissive strategies. This\nmeans that there simply is no one \"local truth\"; whether an edge is good or\nbad may depend on decisions in other states, as we exemplify in Fig. 4. While\nthe edges leading to the goal from v\u2081 and v2 are always winning choices, edge\nV102 is only winning if the goal-edge is chosen in v2 (and vice-versa for v201).\nConsequently, different solution approaches may yield different sets of winning\nedges, and just considering one of them would bias the model to behave alike\nto that concrete solution method and not to \"understand\" semantically labelled\nparity games in general. Relating to the previous example, even though V102 and\n0201 are symmetric, a solver may only mark one of them as winning, but never\nboth. As such, using the output of one solver would actively try to make our\nmodel believe that one of the two is better and imitate that solver's bias.\nSecondly, even if multiple edges are indeed winning, this does not inform us\nabout the \"complexity\" required to win after playing one of them. For example,\nconsider two edges where one leads to a trivially winning sink within two steps\nand the other leads to a large and complicated, but ultimately also winning\nregion. Qualitatively, both edges are equivalent, but we prefer the former as it\nyields smaller solutions and requires fewer correct decisions in the future. We also\nprovide an illustration in Fig. 4. There, both choices in u are winning, however\nwe prefer moving to v\u2081 over moving to w, as we can win \"faster\".\nThus, in order to determine the quality of any given edge of the game, we\nanalyze the game tree after playing said edge. Constructing the entire game\ntree and applying min-max is practically infeasible already for rather small\ninstances. Therefore, we apply an improved version of the decayed Monte Carlo\ntree search suggested in [20]. In particular, we only deeply expand the tree for\ncritical paths (the ones where either player fancies their chances) and thus can\nidentify longer shortest winning paths. Conveniently, we thereby no longer require\nthe \"optimal stalling\" strategy that [20] uses for the opponent, as the opponent\nprefers longer losing paths over shorter ones by default due to the decay. In the\nend, we effectively get a score between -1 and 1 for each edge in the game which\nindicates the \"quality\" of this edge. Intuitively, an edge directly leading to tt\ngets a 1. An edge leading to a region where the system can win but may require\na lot of steps to do so yields a small, positive value, while edges after which the\nenvironment can quickly force a losing cycle yield a score close to -1."}, {"title": "Feature Extraction The feature extraction transforms a transition in the game\ninto a vector of numbers so that it can be processed by an ML model. The features\nare based on all the information that is available at the time of deciding which\nedge to explore further. In particular, this includes the semantic labelling of the\ntransition's source and target, the colour/priority, and also labelling associated\nto its sibling transitions.\nWe deliberately aimed at manually designing a (large) set of features derived\nfrom the semantics and then prune it via feature selection. While automatic\nfeature extraction is a powerful tool, in our use case the feature extraction needs\nto be extremely efficient, since we need to call it hundreds of times per second\nto remain even remotely viable. (Recall that we need to extract the features for\nevery edge in the game and already the games obtained from reasonably simple\nformulae can easily reach thousands of states and significantly more edges.)\nState Features We first introduce twelve \"formula features\", which transform a\nsingle LTL formula into a number. Intuitively, they can be thought of as proxies\nfor higher level concepts. These concepts include formula-complexity (syntactic\nproperties such as height and size of the syntax tree), formula-sat-difficulty (how\n\"difficult\" is it to satisfy the formula, capturing variants of trueness [19]), or\nformula-controllability (how much influence does a player have on the truth value\nof the formula with only their variables). Formula features are then aggregated\nfor a state (which comprises several formulae) in one of two ways. Either, we\nselect a single formula of the labelling and yield the value of the base feature on\nthe selected formula as the state's overall value (e.g. selecting the formula that\nmaximizes the value of another base feature). Or, we apply the base feature to\nall formulae of the semantic labelling and aggregate the results in several ways,\nexploiting the non-trivial structure of the state labelling. Intuitively, this captures\nthe respective concept (e.g. controllability) over the entire state.\nEdge Features Edge features are obtained from state features by either taking\nthe state feature of the edge's successor or the change of the feature along the\nedge, i.e. the difference of the feature in the successor and predecessor. Further,\nwe can compare that value against the value of all other edges of the same state\nby normalizing the feature to the [0, 1]-interval, so that a normalized value of 1\ndenotes that it is the highest among its sibling edges. This may help learning\nrelative comparisons, but loses all information on the absolute value of the feature.\nAs confirmed in our final models, a mixture of normalized and non-normalized\nfeatures seems to be desirable. Aside from features derived from states, we also\nconsider features based on the edge priority as suggested by [20]. However, we\ninclude the parity information in an \"ML-friendly\" way, for example by mapping\nit to a linear scale.\nIn total, we obtain well above 150k different features for edges. These include,\nfor example, the change in the syntax tree height of the most controllable formula\nor the aggregated trueness, normalized across all successors. For more details on\nthe features we refer to App. A.5.\nTraining Applying ground truth and feature methods to the generated formulae\nyields a dataset for supervised learning with way over a million samples for every"}, {"title": "state class. We bootstrap these down to roughly 100k samples per state class\nin order to make the training take reasonable amounts of time. The following\nprocedure was done for every state class individually.\nFeature Elimination As using our entire set of thousands of features is absolutely\nimpractical in several regards, especially with our performance constraints in\nmind, we perform multiple stages of feature elimination. First, we randomly select\nbetween 50 an 100 features per major category and add some hand-picked features.\nThis leaves us at an algorithmically manageable, yet practically infeasible amount\nof about 700 features. For further reductions, we perform a variant of recursive\nfeature elimination, adjusted to our pairwise setting (see App. A.1). As different\nfeatures might be more or less important for different model types and state\nclasses, we ran a separate feature elimination for each of these. For details on\nwhat kind of features remained after the elimination, we refer to App. A.5.\nModels As for model types, we evaluated (kernel-)SVMs, neural networks, random\nforests, and gradient boosted trees. The input for each model is the concatenation\nof the two feature vectors of the respective edges that we want to compare\npairwise. For tree models, we additionally concatenated the pointwise differences\nof the features in order to allow them to compare the same feature of both edges\nin one decision node. For every model type, we performed several smaller runs to\nobtain suitable hyper-parameters for the large scale feature elimination.\nUltimately, gradient boosted trees proved to be the best choice for imple-\nmenting our pair classifier in all four state classes. Together with random forests,\nthey clearly outperformed the non-tree methods like SVM or NN. However, in\ncontrast to random forests, they required less features (3-10, depending on state\nclass) to do so. Further details on this experiment can be found in App. A.1.\n3.4 Engineering\nTo conclude, we provide details on our implementation and engineering im-\nprovements. First and foremost, as a major practical improvement, SEMML is\nimplemented in pure Java (built on top of OWL). In contrast, STRIX is developed\nas a hybrid between Rust and Java, with native compilation of Java through\nGraalVM, and a complex interplay between the two code bases. This adds, among\nothers, complexity due to working with two languages, subtle performance over-\nheads when crossing language boundaries, and setup difficulties due to requiring\na rather particular set of tools. As such, SEMML is significantly easier to use,\nmaintain, and extend. For easy incorporation of third party tools such as syfco,\nwe also include a Python wrapper. For learning, we use the Python library\nSKLEARN [27], and store our models in the established PMML format.\nAside from structural improvements and pure-Java implementation, we also\nadded several engineering changes compared to the approaches of STRIX and\n[20], of which we list a few notable ones.\nState Merging In parity games, states are fully determined by their set of\nedges, i.e. if two system states transition to the same successor for every"}, {"title": "system assignment while emitting the same priority, they are equivalent and\ncan be merged. This equivalence check is very efficient due to our internal\nrepresentation of states. Interestingly, by applying this reduction we observed\na decrease of game sizes by up to two orders of magnitude.\nDual Perspectives As mentioned in Sec. 3.2, our approach alternates between\nthe perspectives of both players. While this is not required for correctness, it\nhelps in practice, as, for example, we can quickly find a small winning region\nfor the environment even if the system player explores in a different direction.\nExploration Scheduling Usually, we switch perspectives whenever the game\nis closed and the solver is consulted. However, when following a \"bad\" edge\nleads the algorithm off the track, we might spend a lot of time trying to close\nthe game. Instead of insisting on continuing this process to the end, we also\nswitch to the other perspective after too many states have been explored\nwithout closing the game.\nResult Sharing We re-use information discovered from one \"perspective\" for\nthe other part, where appropriate, for example already constructed parts of\nthe automaton. Moreover, if one side finds a set of states to be winning for\nthem, the exploration of the other side directly treats them as losing.\nCaching We trade memory for time by caching all computed features.\nBDD We implemented complement edges and several further engineering im-\nprovements in the underlying pure-Java BDD library JBDD [24].\n4 Experimental Evaluation\nIn this section, we evaluate the two central research questions of interest, namely:\nRQ1 Can SEMML solve LTL realizability more efficiently than state-of-the-art\ntools, in particular STRIX?\nRQ2 How much of the improvements are caused by algorithmic and engineering\nchanges and how much by employing ML-guided exploration?\nWe first introduce considered tools, metrics of interest, and our benchmark sets.\nThen, we present our results and discuss each research question separately.\nTools We consider our tool SEMML and the state-of-the-art tool STRIX4. \u03a4\u03bf\nfurther distinguish algorithmic and engineering improvements from those due to\nthe ML-based exploration heuristic, we also consider SEMA\u0141 (\u201cSEMML without\nML\"), which uses the exploration score of STRIX as guidance instead.\nNote that both SEMML and STRIX internally construct a strategy even\nwhen \"only\" solving LTL realizability. The problem of exporting the (already\nconstructed) strategy into a particular format, e.g. AIGER circuits, is completely\northogonal. Thus, we explicitly focus on the time to find the solution (i.e. let the\ntools run in their \"realizability\" configuration).\""}, {"title": null, "content": "Table 5. Scores of all pairings on the Synthetic dataset separated by realizability\nSynthetic\nReal\nUnreal\nAll\nSTRIX/SEMML\n13.07[8+0/27", "n7.33[22+0/26": "n8.56[30+0/53", "nSTRIX/SEMM\u043c\u044c\n10.44[8+0/19": "n8.66[22+0/18", "n9.1[30+0/37": "nSEMME/SEMML\n1.67[25+2/10", "n1.19[38+2/10": "n1.36[63+4/20"}]}