{"title": "The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety Analysis", "authors": ["Wenbo Pan", "Zhichao Liu", "Qiguang Chen", "Xiangyang Zhou", "Haining Yu", "Xiaohua Jia"], "abstract": "Large Language Models' safety-aligned behaviors, such as refusing harmful queries, can be represented by linear directions in activation space. Previous research modeled safety behavior with a single direction, limiting mechanistic understanding to an isolated safety feature. In this work, we discover that safety-aligned behavior is jointly controlled by multi-dimensional directions. Namely, we study the vector space of representation shifts during safety fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal directions in the space, we first find that a dominant direction governs the model's refusal behavior, while multiple smaller directions represent distinct and interpretable features like hypothetical narrative and role-playing. We then measure how different directions promote or suppress the dominant direction, showing the important role of secondary directions in shaping the model's refusal representation. Finally, we demonstrate that removing certain trigger tokens in harmful queries can mitigate these directions to bypass the learned safety capability, providing new insights on understanding safety alignment vulnerability from a multi-dimensional perspective. Code and artifacts are available at https://github.com/BMPixel/safety-residual-space.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in different domains through extensive pre-training on web-scale text data (Brown et al., 2020; Zhao et al., 2023; Qin et al., 2024). However, toxic content in"}, {"title": "2. Preliminaries", "content": "Linear Representation We build our framework on the Linear Representation Hypothesis from Park et al. (2023). A one-dimensional feature value W (e.g., \"gender\", \"harmfulness\") is defined as a latent variable that instantiates context w through a generator function, denoted as $w := G(W)$. In safety analysis, w typically represents user queries with varying safety aspects - from benign questions like \"What leads to a united society?\" to harmful ones like \"how to make a handgun\". Probability of feature W presenting in output is denoted as P(W) (e.g., safe or unsafe responses).\nLet $X : w \\rightarrow R^d$ be a mapping from context w to its representation. We say that $x \\in R^d$ is a feature direction of feature W if there exists a pair of contexts $w_0, w_1 := G(W)$ such that $(w_1) \u2013 \\lambda(w_0) \\in {ax : a > 0}$ satisfying:\n$\\frac{P(W = 1 | \\lambda(W1))}{P(W = 1 | \\lambda(w_0))} > 1$. \n(1)\nThis inequality ensures that the direction positively contributes to feature W.\nSafety Directions In LLM safety alignment, researchers have identified distinct feature directions for various safety aspects including bias, toxicity, and refusal behavior. To find such directions, we first construct two sample sets with only difference being W presenting: G(W) and G(\u00acW). The feature direction vw is then obtained by maximizing the distance between these two distributions. To verify causality, we can intervene by suppressing this direction in the activation space:\nx := x - avw. (2)"}, {"title": "3. Safety Residual Space", "content": "We first define our framework of safety residual space. Our approach is motivated by recent research on training dynamics of alignment algorithms (Jain et al., 2024; Lee et al., 2024), which shows that representation shifts during training are both meaningful and interpretable. We focus specifically on the effects of safety fine-tuning by comparing representation dynamics before and after the fine-tuning process, limiting our scope to a single forward pass. Let x denote vectors in the representation space X \u2282 Rd. Then $T : X \u2192 X$ describes the representation shift from unaligned (before fine-tuning) to aligned (after fine-tuning) states. We define the safety residual space as the linear span of representation shifts during safety fine-tuning. Formally:\nDefinition 3.1 (Safety Residual Space). Consider T from training on unaligned samples whose representation is x ~ P(Xu). The safety residual space VT is defined as the optimal affine transformation parameterized as Vr(x) = Wx + b that minimizes:\n$VT = argmin_{\u0174,6} Exx||T(x) - (Wx+b)||^2$.\nIntuitively, this definition captures the linear activation shifts that the model learns from the training. We consider the safety feature directions as linear and ignore non-linear error between VT and T. We use activations from transformer block outputs at the position of the first generated token from each layer. We compute activations from the training data as an approximation of the unaligned distribution Xu.\nExtracting Principal Components To identify important directions in the residual space, we apply Singular Value Decomposition (SVD) to W \u2013 I and take the first k right vectors (components) Vik. This describes the span of the largest k orthogonal representation shifts from the input space (i.e., the model before training).\nNotation We denote different components as LN-CK, where LN is the layer number and K is the Kth largest right vector from SVD. Specifically, we refer to the LN-C1 as the dominant component, while others are non-dominant components. We use component and direction interchangeably in this paper."}, {"title": "3.1. Component as Feature Direction", "content": "A key question is whether the components in the residual space contain interpretable features, similar to probe vectors. We first establish the connection between feature directions and residual space. We show that residual direction contributes to the training goal under convergence assumption:\nTheorem 3.2 (Convergence Ensures Utility). Suppose training converges to parameters W* with\n$J(W^*) \u2265 sup_{w\u2208M} J(W) \u2013 \u0454,$\n$J(W) := E_{(x,y)~D}[log P_w (y|x)]$\nFor any residual direction v \u2208 VT from alignment shift T, there exist contexts wo, W1 := G(C) (where C = 1 iff fw* (X) = Y) such that\n$\u039e\u03b1 > 0, \\lambda(\u03c9_1) \u2013 \\lambda(\u03c9_0) = \u03b1\u03c5$\nwith the difference satisfying Equation 1."}, {"title": "3.2. Experimental Setup", "content": "Now, we describe the experiment setup, focusing on how models learn to recognize and handle unaligned harmful queries through safety fine-tuning.\nDataset We construct a comprehensive preference dataset incorporating various challenging jailbreak methods and alignment blindspots from recent research (Ding et al., 2023; Yu et al., 2023; Zou et al., 2023; Chao et al., 2023; Liu et al., 2024). To generate harmful examples, we apply these jailbreak methods to toxic samples from STRONG REJECT (Souly et al., 2024). We further incorporate 50% samples from or-bench (Cui et al., 2024) as harmless samples to balance the dataset. Detailed dataset specifications are provided in the Appendix C.1.\nEvaluation Metrics Following established practices in jailbreak research (Zou et al., 2023; Souly et al., 2024), we evaluate model responses along two dimensions: refusal accuracy and response harmfulness. We measure refusal accuracy across both harmless and harmful test samples, while quantifying response harmfulness using STRONG REJECT scores (Souly et al., 2024).\nSafety Fine-tuning We perform safety fine-tuning on Llama 3.1 8B Instruct using both SSFT and DPO approaches for one epoch. For SSFT, we follow Inan et al. (2023) to optimize the model to generate refusal for harmful queries"}, {"title": "4. Linearity of Safety Residual Space", "content": "In this section, we analyze the residual space derived from the SSFT and DPO experiments. We focus on two key linear characteristics of orthogonal directions in the residual space:\n\u2022 Effective Rank: We measure the linear dimensionality of the residual space using effective rank k. Given an energy threshold \u03c4, we calculate k as the minimum number of orthogonal components needed to explain T percent of the variance in the representation shift. Here, $\\sigma_i$ denotes the singular values of the matrix W \u2013 I.\n$k = min \\{ i: \\frac{\\sum_{j=1}^{i} \\sigma_j^2}{\\sum_{j=1}^{N} \\sigma_j^2} > \u03c4 \\}$\n\u2022 Dominant Component: We define this as the first component of SVD(W \u2013 I), the direction of which explains the majority of the shift's variability. We show that this dominant direction predicts the model's aligned behavior (i.e., refusal of harmful requests). We compare it to the refusal direction (Arditi et al., 2024), a probe vector in the activation space that best explains the model's refusal behavior. To evaluate these vectors' predictive power, we use them as weights in linear binary classifiers that distinguish between compliant and refusing responses."}, {"title": "5. Feature Directions in Safety Residual Space", "content": "So far, we have focused on examining the dominant direction in the safety residual space, which predicts the model's aligned behavior. In this section, we will investigate how non-dominant directions represent different features.\nProblem Unlike probe vectors, arbitrary directions lack pre-defined semantic meanings (Bricken et al., 2023), making it challenging to observe outcome changes through intervention experiments. While previous works (Ball et al., 2024; Lee et al., 2024) have used Logit Lens (Nostalge-braist, 2020) to map representations to the projection layer in transformers, the faithfulness of this approach relies on vector similarity to the vocabulary space, which does not apply to residual directions.\nOur Approach To determine features represented by directions, we introduce a theoretically grounded method within the LRP framework. We refer it as Partial Layer-wise Relevance Propagation (PLRP): given a set of directions {v} and representations X\u00b9, we first project X\u00b9 onto the span of {vi}. We then decompose its Euclidean norm into relevance scores R and back-propagate the relevance scores. To ensure relevance conservation, we apply the epsilon rule (Bach et al., 2015) for handling projections. Formally we have:\n$Pv (X\u00b2) = \\sum_{VEV} ||v\u00b2 X'||^2 x R_i$\nThe relevance score Ri is then back-propagated to either (1) input tokens in training data or (2) projections on directions of activation in earlier layers. For input tokens t, we follow Achtibat et al. (2024) and sum up relevance scores of all elements in the token embedding, i.e., $R = \\sum_{t=1} R$. To compute relevance scores of directions v\u2081 in X' of earlier layers, we first compose an linear reconstruction term with first k SVD components V:k \u2208 Rd\u00d7k: \u0176l' = V:kW+\u20ac,"}, {"title": "5.1. Interpreting Directions via Token Relevance", "content": "We demonstrate that relevance scores of training input tokens help understand the semantic meaning of directions in the safety residual space. Table 1 visualizes the relevance distribution for several directions using a handcrafted example on layer 14. 1 We provide observations on the dominant and non-dominant directions in the following.\nDominant Direction We evaluate dominant directions (i.e. LN-C1) and non-dominant directions (i.e. L14-CK in Table 1) separately. The TOP TOKEN column shows the most relevant training tokens that activate each direction. For L14-C1 and L15-C1, we observe that the dominant direction primarily relates to harmful subjects, such as divisive ideologies. This aligns with our earlier finding that the dominant direction best predicts harmfulness.\nNon-Dominant Direction For non-dominant directions, we find they are activated not by toxicity or harmfulness, but rather by features characteristic of specific jailbreak patterns. For instance, tokens like Imagine, fictional and hypothetical in L14-C2 establish a hypothetical tone. This negatively correlates with the dominant component in layer 25, reducing the probability of refusal. Meanwhile, L14-C5 is triggered by explicit mentions of ChatGPT and positively correlates with the dominant direction, likely due to its"}, {"title": "5.2. Layer-Wise Dynamics of Safety Residual Space", "content": "We now examine the evolution of safety feature directions in the space. Using PLRP, we can measure how one direction influences another by attributing feature directions to directions in earlier layers. Figure 5 visualizes the relevance score of different components between adjacent layers.\nEarly Phase: Development of Safety Features We analyze how feature directions evolve across layers using PLRP to trace relevance scores through the transformer network."}, {"title": "6. Toward Multi-dimensional Concept of Safety Fine-tuning Vulnerabilities", "content": "Previous analysis presents a multi-dimensional framework for understanding learned safety behaviors, where distinct features and dynamics emerge along different directions in residual space. In this section, we demonstrate how this framework provides practical insights into safety fine-tuning vulnerabilities by showing manipulating non-dominant directions can bypass learned safety capabilities. We explore two methods to circumvent the learned safety capabilities while preserving the model's refusal ability: (1) suppressing non-dominant components and (2) removing or rephrasing trigger tokens from jailbreak prompts. Here, we define \"trigger tokens\" as specific token sequences that induce changes in feature directions, as demonstrated in Table 1.\nSuppressing Non-Dominant Directions As shown in subsection 5.1, removing L14-C6 explains the model's learned ability to refuse PAIR-like jailbreaks. Building on this insight, we investigate the effect of suppressing most non-dominant components while leaving dominant components untouched. Formally:\n$x := x- \\sum \u03b1v_i$\n$ViEVt:$\nThis approach allows us to examine whether safety alignment can be reversed by blocking only indirect features. To preserve the model's ability to refuse plainly harmful prompts, we exclude component directions with harmfulness correlations above 0.7.\nTrigger Removal Attack We next introduce a procedure to remove trigger tokens from jailbreaks. First, we apply token-wise PLRP to dominant directions of the final layers to identify a list of top trigger tokens that explain the refusal output. Then, we employ another LLM to iteratively rephrase the harmful prompt while avoiding these trigger tokens, similar to TAP (Mehrotra et al., 2023). These modified jailbreak prompts are incorporated into the safety fine-tuning dataset, and we evaluate the detection accuracy on a validation split. The detailed algorithm is provided in the Appendix B."}, {"title": "6.1. Results", "content": "Disrupting Non-dominant Directions Reduces Refusal In Figure 6, we analyze how different attacks affect the projection values compared to default prompts (Harmful and Benign). Both non-dominant suppression and trigger"}, {"title": "7. Discussion", "content": "Connection with Linear Representation Hypothesis Our work builds upon the Linear Representation Hypothesis, which posits that studied features can be expressed through linear projections. Recent works have shown that not all feature directions are linear (Engels et al., 2024). We observe that some directions occasionally flip between different layers, and feature directions cannot be extended indefinitely without degrading generation quality. Neverthless, we identify several linear feature directions in the safety residual space and verify their linearity."}, {"title": "8. Related Work", "content": "LLM Alignment & Jailbreak Attacks Multiple algorithms have been proposed to align LLMs with human preferences, with the most prevalent approach being reinforcement learning from human feedback, such as DPO (Rafailov et al., 2024). Recent work has explored tuning-free alignment methods, including pruning (Wei et al., 2024), model fusion (Yi et al., 2024), weight editing (Uppaal et al., 2024), and decoding process modifications (Xu et al., 2024). In parallel, numerous studies have focused on compromising these safety alignments through jailbreak attacks (Ding et al., 2023; Yu et al., 2023; Zou et al., 2023; Chao et al., 2023; Liu et al., 2024; Jiang et al., 2024). Our work primarily investigates jailbreak attacks due to their widespread study and proven effectiveness against even the most recent models.\nMechanistic Interpretation & Representation Learning Another line of research seeks to understand LLM safety alignment by analyzing internal model mechanisms. Through supervised approaches, researchers have localized safety behaviors in various model components: activations (Wei et al., 2024; Zhou et al., 2024a; Li et al., 2024b), attention patterns (Zhou et al., 2024b), and parameters (Lee et al., 2024; Arditi et al., 2024). Additionally, unsupervised methods based on superposition and dictionary learning (Bricken et al., 2023) have identified meaningful safety-related feature directions (Ball et al., 2024; Balestriero et al., 2023). Several works related to ours examine the effects of safety fine-tuning (Jain et al., 2024; Lee et al., 2024; Yang et al., 2024) by analyzing representation shifts. Our work advances this understanding by comprehensively characterizing these shifts and attribute clear semantic meaning to the identified directions."}, {"title": "9. Conclusion", "content": "In this work, we provide a multi-dimensional mechanistic understanding of what LLMs learn from safety fine-tuning. We identify multiple feature directions that jointly control safety behavior\u2014a hidden dimension previously invisible"}, {"title": "Impact Statement", "content": "Our research shows methods for analyzing and bypassing LLM safety mechanisms, which could enable harmful content generation. We acknowledge these risks and emphasize the need for careful use of our methods. However, since multiple effective jailbreaks for the studied models are already public, our work does not create new safety concerns. We therefore believe sharing our code and methods openly benefits the research community by supporting reproducibility and future safety research."}, {"title": "A. Proof of Theorem 3.2", "content": "Let W* be the converged parameter point of the model such that\n$J(W^*) \u2265 sup_{WEM} J(W) \u2013 \u03b5,$\nwhere $J(W) = E_{(x,y)~D}[log P_w (y | x)]$, and \u025b \u2265 0. Denote by \u5165(\u00b7) the pre-trained representation, and let T : A(x) \u2194 Xaligned(x) be the residual shift learned during safety fine-tuning. The space VT comprises the linear directions {v} CRd that describe how T modifies A(x) to encourage aligned outputs.\nWe first define a zero-utility direction v \u2208 Vr as one where no pair (wo, w\u2081) satisfies both:\n$\\lambda(\u03c9_1) \u2013 \\lambda(\u03c9_\u03bf) \u2208 {\u03b1\u03bd : \u03b1 > 0},$\n$\\frac{P(C = 1 | \\lambda(W1))}{P(C = 1 | \\lambda(w_0))} > 1.$\n(3)\nHere, C = 1 indicates that fw* (x) = y (the model's output is correct/aligned); C = 0 otherwise.\nFor any zero-utility direction v, we can construct a modified parameter W* by projecting out v:\nW* = \u03a8(W*,\u03c5),\nwhere \u03a8(\u00b7,\u00b7) removes components along v from the internal representations.2\nSince v is zero-utility, this projection preserves the model's behavior:\n$Pw* (yx) = Pw*(yx)$\n(4)\nfor all training samples (x, y). This means the training objective J(W) remains unchanged:\nJ(W*) = J(W*)\nGiven that W* converged with J(W*) \u2265 supw\u2208M J(W) \u025b, it cannot contain any zero-utility directions. If it did, we could remove such a direction without affecting performance, contradicting the convergence assumption. This holds even for \u025b-suboptimal solutions.\nBy contraposition, every direction v \u2208 Vr must contribute positively to the training objective for some pair of contexts, matching (Equation 1) in the main text."}, {"title": "B. Algorithm for Trigger Removal Attack", "content": "Algorithm 1 Removing Shortcut Triggers\nRequire:\n1: p: harmful prompt to rewrite\n2: n: number of iterations\n3: LM: victim language model\n4: eval(x) : evaluates output harmfulness\n5: resample(p, B) : resamples p excluding tokens in blacklist B\n6: plrp(LM, p): extract top tokens in p with Partial LRP\nEnsure: Rewritten prompt p*\n7: Striggers \u2190\n8: for i = 1 to n do\n9: Pvariants \u2190 resample(p, Striggers)\n10: score \u2190 eval(LM, Pvariants)\n11: S'\u2190 plrp(LM, Pvariants with top k score)\n12: Striggers \u2190 Striggers US'\n13: end for\n14: p* \u2190 Pvariants[i] where i = argmax(score[i])\nWe implement our trigger removal attack on Llama 3 8B using an iterative approach. For each harmful prompt from STRONGREJECT, we perform n = 3 iterations of trigger identification and removal. In each iteration, we first generate 10 rephrased variants using Llama 3 405B as the resampling model\u00b3. These variants maintain the harmful intent while varying the style and expression. We then evaluate each variant using the Strong Reject Score metric to identify which rephrasing attempts successfully bypass the model's safety mechanisms. The variants with the lower scores (more likely to be rejected) are analyzed using Partial Layer-wise Relevance Propagation (PLRP) to extract tokens that contribute most to circumventing safety guardrails. These identified trigger tokens are added to a growing blacklist. In practice, we found that generating multiple variants per iteration is crucial, as the trigger tokens identified from training data alone are insufficient for consistently bypassing the model's safety mechanisms. The process continues until either the maximum iterations are reached or a successful bypass is achieved."}, {"title": "C. Dataset and Training", "content": "C.1. Dataset Construction and Composition\nComposition of the Dataset. For the composition of the training set, we selected jailbreak methods or alignment blindspots, which are challenging in recent research, as methods for generating harmful examples. We applied all jailbreak methods on STRONG REJECT (Souly et al., 2024) to generate harmful samples. Additionally, we collected harmless samples from the OR-Bench (Cui et al., 2024) dataset to balance the dataset.\nAll the baseline jailbreak methods we applied include:\n\u2022 PAIR (Chao et al., 2023) guides the LLM through a carefully designed system prompt template to iteratively refine harmful input based on the target LLM's responses, in order to attack the black-box model.\n\u2022 ReNellm (Ding et al., 2023) enhances stealth by rewriting the expression form of the harmful prompt and nesting it in three general task scenarios to attack black-box models.\n\u2022 GPTFuzz (Yu et al., 2023) systematically generates and filters adversarial harmful prompts to attack black-box models by automating a fuzz testing framework, combining semantic variation and feedback iteration.\n\u2022 GCG (Zou et al., 2023) train an attack suffix for a specific white-box model, with the goal of maximizing the model's"}, {"title": "C.2. Training Procedure and Results", "content": "Metrics. We use the Strong Reject Score (Ding et al., 2023) as the evaluation metric for the training effectiveness of N-SHOT Security Training. The Strong Reject Score is a metric for assessing jailbreak methods, taking two inputs: a dangerous task, and outputs from the model after the application of a jailbreak method. It utilizes a carefully designed"}, {"title": "C.3. Experiment on Models of Different Scale and Architecture", "content": "To understand how model scale and architecture affect safety alignment, we conducted comparative experiments with two additional models: Llama-3.2-3B-Instruct and Ministral-8B-Instruct (Mistral). We applied identical DPO training, with results shown in Figures 11 and 12 respectively.\nBoth models exhibited similar safety improvement trends to the Llama-3.1-8B-Instruct baseline, but demonstrated weaker capabilities in recognizing and handling unseen harmful queries across all jailbreak methods. The 3B-parameter Llama variant showed faster initial convergence rates, particularly evident in Figure 11, but consistently failed to recognize Trigger"}, {"title": "C.4. Impact of Model Intervention on General Ability", "content": "We evaluated the impact of model interventions on general task performance by measuring perplexity degradation on the Alpaca dataset (Taori et al., 2023). Specifically, we calculated perplexity solely on the output part of samples to assess whether the interventions compromised foundational capabilities.\nResults are shown in Figure 13. Notably, interventions using DPO showed consistently higher perplexity (8.42) compared to the base Llama model (7.10) and SSFT baseline (6.59), indicating a more substantial impact on general capabilities. Among the directional interventions, SSFT variants demonstrated relatively modest perplexity"}]}