{"title": "EEG-EMG FAConformer: Frequency Aware Conv-Transformer for the fusion of EEG and EMG", "authors": ["ZhengXiao He", "Minghong Cai", "Letian Li", "Siyuan Tian", "Ren-Jie Dai"], "abstract": "Motor pattern recognition paradigms are the main forms of Brain-Computer Interfaces(BCI) aimed at motor function rehabilitation and are the most easily promoted applications. In recent years, many researchers have suggested encouraging patients to perform real motor control execution simultaneously in MI-based BCI rehabilitation training systems. Electromyography (EMG) signals are the most direct physiological signals that can assess the execution of movements. Multimodal signal fusion is practically significant for decoding motor patterns. Therefore, we introduce a multimodal motion pattern recognition algorithm for EEG and EMG signals: EEG-EMG FAConformer, a method with several attention modules correlated with temporal and frequency information for motor pattern recognition. We especially devise a frequency band attention module to encode EEG information accurately and efficiently. What's more, modules like Multi-Scale Fusion Module, Independent Channel-Specific Convolution Module(ICSCM), and Fuse Module which can effectively eliminate irrelevant information in EEG and EMG signals and fully exploit hidden dynamics are developed and show great effects. Extensive experiments show that EEG-EMG FAConformer surpasses existing methods on Jeong2020 dataset, showcasing outstanding performance, high robustness and impressive stability.", "sections": [{"title": "I. INTRODUCTION", "content": "Our research focuses on motor pattern recognition [1] using EEG and EMG signals. Since the 1990s, top research institutions have advanced brain-computer interface (BCI) systems. Early leaders included Graz University of Technology, University of T\u00fcbingen, and the Wadsworth Center [2]. Graz University, led by Pfurtscheller [3], pioneered using event-related potentials for motor imagery tasks. Since 2003, B\u0421\u0406 competitions have furthered data processing and classification techniques.\nDespite progress, processing EEG signals remains challenging due to low signal-to-noise ratio, noise, artifacts, and individual differences. Extracting critical information and improving algorithm interpretability are key challenges. Recent deep learning methods like Liu [4], Fan [5], EEG Conformer [6], EEG-Deformer [7], CNN-LSTM [8], EEGNet [9], EEG-TCNet [10], and LMDA-Net [11] have shown promise, out-performing traditional methods. However, these methods often overlook the correlation between EEG and EMG in motor pattern recognition. Although previous works can capture both short period and long period features in some way, they did not effectively get rid of the rebundant information in irrelevant frequency band and noises mixed in the signals. That's what we are looking for: a method that can eliminate irrelevant information and noises. Also, it can grasp temporal and frequency information in both short periods and long periods for discerning dynamics embedded in the EEG and EMG signals.\nTo mitigate the above-mentioned issues and enhance the perception of temporal dynamics in EEG data and EMG data, we introduce EEG-EMG FAConformer, a novel convolutional Transformer. We propose fusing EMG features with EEG features for better recognition, an area that has received limited attention. Our model is divided into two branches. For the EMG section, we just adopted resblocks to encode information. As for the EEG section, the workflow consists of a EEG-Denoise module and an EEG branch. We would like to introduce the EEG branch completely. First, the frequency band attention will arrange an adaptive weight to different frequency which will eliminate a lot of irrelevant information. Then the Multi Scale Feature Fusion module is comprised of 1D kernels with different sizes. In that way, we can extract different hidden temporal patterns. Next the independent channel-specific convolution module is used to extract features in independent channel, which shows superior performance over normal convolution layers. Lastly, it will go through Squeeze-and-Excitation block(SEBlock) to pay more attention to related channels. Then we will fuse the EEG features and EMG features. We concat these features first and apply a fuse module to fuse these features. It will capture both short term and long term correlations hidden in signals, which is quite critical in motor pattern recognition.\nOur contributions can be summarized as follows:\n1.  We propose a lightweight network named EEG-EMG FAConformer which is comprised of convolutions and different attention modules for motor pattern recognition decoding using EEG and EMG signals. We especially introduce a Frequency Band Module to effectively encode EEG information and elimintate noises and irrelevant information.\n2.  We develop some useful modules like Multi-Scale Fusion Module, Independent Channel-Specific Convolution Module (ICSCM), and Fuse Module which can effectively eliminate irrelevant information in EEG and EMG signals and fully exploit hidden patterns. Each of them can play an important part in the future work about the fusion of EEG and EMG in motor pattern recognition and greatly improve motor pattern recognition performance.\n3.  Through extensive experiments and ablation study on Jeong2020 dataset, the efficacy and robustness of EEG-EMG FAConformer is proved. Our results show its superiority compared with other state-of-the-art (SOTA) methods."}, {"title": "II. RELATED WORKS", "content": "The motor pattern recognition task has a long history. It used to be mainly in machine learning. However, with the boom of deep learning, more and more methods have been proposed to enhance the decoding accuracy of motor pattern recognition. Methods using machine learning adopt a CSP+SVM structure. Barachant et al. (2011) introduced a BCI framework using motor imagery (MI) with a focus on Riemannian geometry to classify EEG signals directly through their spatial covariance matrices. Common Spatial Pattern (CSP) [12] analyzes the spatial distribution of multi-channel EEG signals to differentiate brain activities across various tasks. Then the method utilizes svm to make a classification.\nTo enhance CSP's effectiveness, Optimal Spatial-Temporal Patterns (OSTP) [13] employs mutual information for feature selection to optimize both frequency band and time segment selection for CSP filtering. Additionally, Ozdenizci et al. [14] proposed an information-theoretic learning approach to improve neural feature interpretation and address conventional feature selection limitations.\nAnother advanced method, Filter Bank CSP (FBCSP) [15], segments EEG data into multiple frequency bands and applies CSP to each. This method uses a feature selection algorithm that automatically identifies features tailored to individual subjects, significantly boosting classification accuracy. FBCSP is highly regarded and frequently used in comparative analyses within the field.\nAs we have mentioned above, multiple deep learning methods have been proposed to demonstrate promising results in the motor imagery task. EEGNet [9] is the first compact deep learning model which is used in this sphere. This model is comprised of some easy modules which guarantee it is lightweight. It begins with some convolutions along the temporal dimension and then uses some depthwise convolutions along spatial fields and ends with depthwise seperable convolutions to extract features. Then some methods like FBCNet [16] appears, which manually extracts temporal features by calculating variance. Since motor imagery appears mainly in some of the regions of our brains, the attention mechanism shows great performance in this task. EEG Conformer [6] which use self-attention to learn global features dominated this task. And some other methods have been proposed continuously to solve this problem. LightConvNet [17] and some other methods have been proposed to reduce the number of parameters and increase precision.\nHowever, few researchers cast attention on multi-modal fusion into the motor pattern recogniton. Some researchers in the biomedical engineering field and medical field like Grospr\u00eatre [18] and Decety [19] have found that when human beings move their upper limbs or do some motor imagery, related neuron groups will be activated. Inspired by these works, we analyze and fuse this information to make a more accurate estimation of one's intention for upper limb moves."}, {"title": "III. PROPOSED METHODS", "content": "As shown in Fig. 1, we introduce the overall structure of the model, which is divided into two branches: the EMG branch and the EEG branch. We extract information from both sources and make an effective fusion to make a more accurate motor pattern decoding."}, {"title": "B. Details of EEG-Branch", "content": "1) EEG Denoise Net: Our design for EEG-Denoise is relatively straightforward. We have incorporated the IC-U-Net model proposed by Chuang et al. [20], which has notably enhanced the signal-to-noise ratio of the dataset and improved classification efficiency. Despite its simplicity, this module is lightweight and demonstrates measurable improvements.\n2) Details of EEG Branch: As shown in Fig. 1, we can get an overview of the EEG-Branch structure. Motivated by Qin [21] and Ang [22], we designed a frequency band attention module which derives valuable information from relevant frequency bands.\nInitially, we adopted multiple band-pass filters to the raw EEG data utilizing the Chebyshev Type II filter.\nNext, we used point-wise convolution to integrate multi-band information. This process allows the network to leverage complementary information from each frequency band. At the same time, an adaptive weight is assigned to each frequency band to reduce noise in redundant frequency bands and enhance information in other frequency bands.\nBy integrating the frequency band information, we obtain the final output. For the single-trial input data $X \\in R^{C \\times T}$, let $C$ represent the number of channels and $T$ represent the time points. We applied multiple Chebyshev Type II filters to the raw data to achieve frequency splitting, resulting in data represented as $X_{MB(n)} = X *h(n) \\in R^{N_b \\times C \\times T}$, where $N_b$ represents the number of frequency bands.\nFor each frequency band, we first applied a self-attention mechanism across channels. Then, we used point-wise convolution to merge the multiband information. This process enables the network to utilize the complementary information from each frequency band effectively. Simultaneously, an adaptive weight $W(n)$ is assigned to each frequency band to attenuate noise from redundant bands and improve useful information from other bands.\nAfter merging the frequency band information, we obtain the fused frequency band data. This approach effectively extracts valuable information from relevant frequency bands while suppressing noise and irrelevant information from other bands, leading to a refined output $X_{FS}$. And $W(n)$ represents the weight of respective frequency.\n\n$X_{FS} = \\sum_{t=1}^{N_b} X_{MB(n, i, j)} * w(n) \\in R^{C \\times T}$\n\nNext, we applied a set of 1D residual convolutional layers with varying kernel sizes to the extracted features for temporal feature extraction. Through experimental evaluation, we determined that the performance of 2D convolutions was inferior to that of 1D convolutions. Additionally, 1D convolutions have a relatively lower parameter count. Consequently, we utilized 1D convolutions for this purpose. By performing frequency segmentation and applying self-attention mechanisms across different channels, the primary objective is to extract features from the temporal sequence. We adopted a convolutional kernel size of $X_5$.\nDrawing inspiration from multi-scale fusion techniques, we propose a multi-scale convolutional module to learn patterns with different temporal lengths and fuse features effectively. This approach enhances recognition accuracy by employing convolutional layers of four different sizes and subsequently fusing the features to recognize patterns across various scales. Let the kernel sizes be denoted as $S_1, S_2, S_3$ and $S_4$. The formula for extracting the features on this temporal sequence is:\n\n$X_{LS_k (i)} = \\sum_{m=0}^{S_k - 1} X_{FS(i + m)} \\cdot w_k(m) + b_k, k=1,2,3,4$\n\n$X_{fuse} = concat(X_{LS_1}, X_{LS_2}, X_{LS_3}, X_{LS_4})$\n\n$X_{fuse} = Conv1D(X_{fuse}, kernel\\_size=1, filters=128)$\n\nThese convolutional kernels enhances our network's ability to recognize spatiotemporal information. After the fusion we adopted an independent channel-specific convolution module to lower the dimension and reduce redundant information. Then We utilized SEBlock [23] to assign different weights to different EEG channels, improving the accuracy of our classification. The operation here is also straightforward.\nFirst, we pool the features along the temporal dimension, then learn the relationships between different channels through linear layers. This operation is part of a very lightweight gating mechanism, formulated as:\n\n$Z_c = F_{sq} (X_{fuse}) = \\frac{1}{T} \\sum_{t=1}^{T} X_{fuse}(t)$\n\n$X_c = F_{ez} (Z_c, w) = \\sigma (w_2 \\sigma (W_1 Z_c))$\n\nThis simple operation helps maintain the lightweight nature of the model.\n3) Details of EMG-Branch: We adopt 1D convolutional residual layers to extract the EMG signals. While EMG signals can provide valuable information, their role in motor imagery tasks is primarily auxiliary. Therefore, the EMG branch is relatively smaller in comparison to the EEG branch."}, {"title": "C. Details of Fuse Module", "content": "In this section, we use a relatively simple method of concatenating (concat) and multihead attention mechanisms to perform feature fusion of EEG and EMG signals. We can express the signal fusion module using the following formula:\n\n$X_{fuse} = MultiheadAttention(Concat(X_{emg}, X_{eeg}))$\n\nHere, our multi-head attention mechanism reassigns weights to different EEG and EMG channels, improving the effectiveness of our classification tasks."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "The experimental data for this study was obtained from the publicly accessible Jeong2020 database [24].\nThe Jeong2020 dataset was recorded by a team of cognitive science professors from Korea University, including Schalk and McFarland. This dataset includes data from each participant's motor execution tasks and motor imagery tasks.\nThese data were provided by 25 participants, each contributing 3300 trials throughout the experiment. Each experiment included motor imagery, actual movement, and rest intervals. During the experiment, EEG signals were collected through 60 electrode channels with a sampling frequency of 2500 Hz, in addition to 6-channel EMG signals."}, {"title": "B. Implementation Details and Evaluation Metrics", "content": "We used multiple sets of hyperparameters in our experiments to achieve the best experimental results. We tested and recorded the number of epochs, the learning rate, the selection of the optimizer, and the batch size. Here, we use the five-fold cross-validation method for comparison, as it is commonly used in related papers. During training, we use one-fifth of the data as the test set and four-fifths as the training set, then perform five-fold calculations and average the accuracy results. The evaluation metrics for the experiment include the most commonly used metrics for multiclassification tasks: accuracy and the kappa coefficient. According to the classification results [25], let T (true) be the number of samples where the predicted results match the actual results and F (false) be the number of samples where the predicted results differ from the actual results. The accuracy and kappa coefficient are defined as follows:\n\n$acc = \\frac{T}{T+F}$\n\n$\\\u041a = \\frac{p_o - p_e}{1 - p_e}$\n\nIn the kappa coefficient formula, $p_o$ is the accuracy, used to evaluate the balance of classification. The calculation method for $p_e$ is shown in the formula below: a represents the actual number of each class, and b represents the number of predictions for each class.\n\n$p_e = \\frac{a_1 \\times b_1 + a_2 \\times b_2 + ... + a_n \\times b_n}{n \\times n}$"}, {"title": "C. Experimental Results and Ablation Studies", "content": "1) Experimental Results: Fig. 2 and Table II presents the classification accuracy results of various models across different tasks.\nIn the three-class motor execution task, our model achieved an accuracy of 94.7%, significantly higher than other models. The accuracies of the other models were as follows: EEG Conformer at 92.6%, CNN-LSTM at 91.7%, LMDAnet at 93.5%, and BaseCNN at 87.0%.\nIn the six-class motor execution task, our method (EEG-EMG FAConformer) achieved the highest accuracy at 90.0%. The accuracies of EEG Conformer and CNN-LSTM were 89.1% and 88.0%, respectively. LMDAnet achieved an accuracy of 89.5%, and BaseCNN achieved 85.0%.\nFor the binary classification task of motor execution, our method (EEG-EMG FAConformer) achieved an outstanding accuracy of 98.1%. The accuracies of EEG Conformer and CNN-LSTM were 97.8% and 97.1%, respectively. LMDAnet achieved an accuracy of 98.0%, close to our model. BaseCNN achieved 96.5%.\nIn the motor imagery dataset, as shown in Fig. 2, the proposed EEG-EMG FAConformer significantly outperformed in the three-class motor imagery task. EEG-EMG FAConformer achieved an accuracy of approximately 62.5%, while EEG Conformer achieved 56%, CNN-LSTM achieved 43.45%, BaseCNN achieved 38%, LMDAnet achieved 61.5% and EEG-TCNet achieved 59.427%.\nIn the binary classification task of motor imagery, the proposed EEG-EMG FAConformer achieved an accuracy of 61.23%, while other methods lagged behind significantly. The accuracies of EEG Conformer, CNN-LSTM,EEG-TCNet and BaseCNN were only 53.2%, 51.707%, 53.333% and 40.2%, respectively. LMDAnet achieved an accuracy of 54.867%, leading other methods but still far behind our model.\nThe lower classification accuracy in the motor imagery dataset is primarily due to the weak EMG signals during the data collection process, as the muscles were only slightly activated. In motor execution, EMG signals decode movements more effectively, making decoding easier. Therefore, we plan to create a motor imagery dataset with slightly increased force to address the issue of relatively weak EMG signals.\nThese comparisons demonstrate that our method (EEG-EMG FAConformer) excels in multiple tasks, with leading accuracies, highlighting its strong feature extraction and classification performance. LMDAnet also performed well in some tasks, but overall, it still lagged behind our method. These results further validate the effectiveness and robustness of our method in handling complex tasks.\n2) Visualization: Fig. 3 shows the raw brain region images (left side) and the brain region visualizations processed by our frequency-band attention mechanism (right side) for different subjects (Subject 2 to Subject 7). These images demonstrate the significant effect of our method in enhancing feature extraction and classification tasks.\nAfter processing with our method, it is evident that the features of the relevant activated regions are significantly enhanced. Specifically, in the raw data, the distribution of the activated regions is not very distinct. However, after applying the frequency-band attention mechanism, the signal intensity in these regions is also enhanced.\nThese improvements are highly beneficial for classification tasks. Enhanced features help the model better identify and distinguish different categories, thereby improving classification accuracy. The comparison of data for each subject in Fig. 3 clearly shows the effectiveness of our frequency-band attention mechanism in extracting key features.\nFurther ablation study also verified the role of the frequency band attention mechanism. In the ablation experiments, when the frequency band attention mechanism was removed, the model's performance significantly declined, demonstrating the critical role of the attention mechanism in feature extraction and enhancement. These experimental results not only showcase the superiority of our method but also provide strong support for its effectiveness in practical applications.\nFig. 3 shows significant differences in motor imagery EEG patterns among subjects, leading to lower test accuracies for some individuals. For example, Subject 6 exhibits distinct brain region patterns compared to others. This phenomenon can be attributed to several factors: first, variations in adherence to experimental guidelines and environmental factors might have compromised EEG signal quality. Equipment accuracy and stability also play a role. Second, EEG signals are highly individualized, influenced by physiological and psychological states, and motor imagery abilities. Some subjects may have weaker or atypical motor imagery patterns, impacting the classifier's performance. These individual differences pose a significant challenge in motor imagery EEG signal classification, as varying motor imagery abilities and styles among subjects result in lower classification accuracy. To address this, future research should focus on improving data collection to enhance signal quality and consistency, developing robust algorithms for feature extraction and classification to better handle individual differences, and implementing personalized modeling approaches to customize training based on unique EEG characteristics, thereby improving classification accuracy.\nt-Distributed Stochastic Neighbor Embedding (t-SNE) [26] is a commonly used statistical dimensionality reduction and feature visualization method. After training with LMDAnet, LSTMCNN, and our method, Fig. 4 and 5 show the t-SNE projections of the data into the two paradigms, 2D S01 and 2D S11, respectively. The visual representations in Fig. 4 and 5 highlight the superior feature extraction capabilities of the EEG-EMG FAConformer model, providing a reasonable explanation for its superior classification performance.\nAs seen in Fig. 4 and 5, the features extracted by the EEG-EMG FAConformer model exhibit clear boundaries and more distinct clustering, indicating that this model effectively captures sparse and distinct features, enhancing the separability between different categories. Specifically, compared to other models, the EEG-EMG FAConformer can achieve better classification results with fewer training samples. These advantages are not only visually evident but also quantitatively supported by metrics such as accuracy and kappa coefficient, where the EEG-EMG FAConformer model performs better.\nIn contrast, the LSTMCNN and LMDAnet models show more chaotic feature extraction with unclear boundaries and poor class distinction. This indicates that the EEG-EMG FAConformer model can better utilize the critical information in EEG and EMG signals during feature extraction, thereby enhancing overall classification performance.\nIn conclusion, the visual results and quantitative analysis from Fig. 4 and 5 demonstrate the superiority of the EEG-EMG FAConformer model in feature extraction and classification tasks. This model not only improves the efficiency of feature extraction but also maintains high classification accuracy with fewer training samples. This is significant for practical applications, as obtaining large amounts of labeled data is often challenging in real-world scenarios. Therefore, the superior characteristics of the EEG-EMG FAConformer model provide strong support for multimodal data processing in complex scenarios.\nFig. 6 shows the confusion matrices of four models on the three-class motor imagery task for Subject 8, including CNNL-"}, {"title": "D. Ablation Study", "content": "In this study, we conducted an ablation study on the multi-feature attention module of the EEG-EMG FAConformer to evaluate its impact on model performance. We used the Jeong2020 dataset for the experiments, specifically removing the frequency band attention, multiscale fusion and independent channel-specific convolution module. To compare the overall model with each module, we selected data from 9 participants for detailed display. The experimental results are shown in Fig. 7 and Table III.\nSpecifically, after removing the modules, the accuracy of the model decreased to varying degrees. After removing the frequency band attention module, the average accuracy decreased by 2%. After removing the multiscale fusion module, the average accuracy decreased by 0.25%. After removing the independent channel-specific convolution module, the average accuracy decreased by 0.2%. In almost every subject's experimental results, removing the frequency band attention module resulted in a significant loss in accuracy, such as a 6% drop in S01. This indicates that the frequency band attention module plays an important role in improving the classification accuracy for specific tasks, particularly in motor imagery tasks."}, {"title": "V. CONCLUSION", "content": "In this work we have introduced a lightweight and effective model tailored for the extraction and fusion of EEG and EMG signals to recognize motor patterns. With the combination of Frequency Band Attention, Multi-Scale Fusion, Independent Channel-Specific Convolution Module (ICSCM) and Fuse Module, we effectively extract important information for recognition and get rid of redundant and irrelevant information. Our method shows astonishing robustness and superiority in Jeong2020 dataset and then verify the potential of EEG-EMG fusion in motor pattern recognition.\nHowever, there is still a lot to be done. Firstly, there is no enough dataset with both EEG and EMG recorded in relevant tasks, which make the development of fusion of EEG and EMG signals quite difficult. Still the transfer learning in relevant area remains undeveloped. Since different people have different EEG signals, the application of transfer learning in the decoding of EEG signals is quite important.\nLastly, we are currently developing a dataset focused on mild muscle activation during motor imagery to address the problem of low EMG intensity in previous motor imagery datasets. This dataset aims to provide more diverse and realistic data, enhancing the training and performance of EEG-EMG fusion models in capturing subtle muscle activities."}]}