{"title": "ProMRVL-CAD: Proactive Dialogue System with Multi-Round\nVision-Language Interactions for Computer-Aided Diagnosis", "authors": ["Xueshen Li", "Xinlong Hou", "Ziyi Huang", "Yu Gan"], "abstract": "Recent advancements in large language models (LLMs) have demonstrated extraordinary comprehension\ncapabilities with remarkable breakthroughs on various vision-language tasks. However, the application of\nLLMs in generating reliable medical diagnostic reports remains in the early stages. Currently, medical\nLLMs typically feature a passive interaction model where doctors respond to patient queries with little or no\ninvolvement in analyzing medical images. In contrast, some ChatBots simply respond to predefined queries\nbased on visual inputs, lacking interactive dialogue or consideration of medical history. As such, there is\na gap between LLM-generated patient-ChatBot interactions and those occurring in actual patient-doctor\nconsultations. To bridge this gap, we develop an LLM-based dialogue system, namely proactive multi-round\nvision-language interactions for computer-aided diagnosis (ProMRVL-CAD), to generate patient-friendly\ndisease diagnostic reports. The proposed ProMRVL-CAD system allows proactive dialogue to provide patients\nwith constant and reliable medical access via an integration of knowledge graph into a recommendation\nsystem. Specifically, we devise two generators: a Proactive Question Generator (Pro-Q Gen) to generate\nproactive questions that guide the diagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report\nGenerator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating two real-world publicly\navailable datasets, MIMIC-CXR and IU-Xray, our model has better quality in generating medical reports.\nWe further demonstrate the performance of ProMRVL achieves robust under the scenarios with low image\nquality. Moreover, we have created a synthetic medical dialogue dataset that simulates proactive diagnostic\ninteractions between patients and doctors, serving as a valuable resource for training LLM.\nAutomatically generating patient-friendly diagnos-\ntic reports is crucial for mitigating clinical shortages\nand facilitating patient-doctor communication. De-\nspite increasing interest, few studies have investigated\nbuilding reliable interactive medical dialogue systems\nfor automatic diagnostic report generation, especially\nwith the consideration of both clinical visuals and\nmedical history. Existing work on interactive medi-\ncal services mainly targets question-answering (QA)\ntasks [1, 2, 3, 4], which only takes textual inputs but\nleaves visuals unsupported. However, medical images,\nsuch as radiography, are considered critical references\nfor disease diagnosis due to their rich visual and tex-\ntual features. Thus, these models could only serve as\nknowledge retrieval systems to answer health-related\nquestions, rather than medical dialogue systems for", "sections": [{"title": "1 Methodology", "content": null}, {"title": "1.1 System Overview", "content": "The overall structure of ProMRVL-CAD is illus-\ntrated in Figure 2. The proposed system consists\nof two major components, Proactive Question Gen-\nerator (Pro-Q Gen) for proactive conversation and\nMulti-Vision Patient-Text Diagnostic Report Gener-\nator (MVP-DR Gen) for disease diagnosis. For the\nPro-Q Gen, we generate a synthetic and proactive\nmedical dialogue dataset that is conditional on the\nmedical history from a real-world medical dataset to\nmimic doctor-patient conversations. Then, we fine-\ntune a pre-trained LLM with the synthetic dialogue\ndataset and medical histories to boost its reasoning\nand interaction capability in disease analysis. In the\nMVP-DR Gen, we feed the medical visuals and the\ntextual inputs (medical history and dialogue) to the\nsecond LLM. We confer the multi-modality capability\non the model by leveraging a MiniLM [24] for dialogue\nanalysis and deploying a vision transformer followed\nby an alignment layer for visual processing. Lastly,\nwe freeze the parameters of the second LLM to fully\nutilize its language and reasoning capability. We train\nthe rest of the components with the knowledge of\nthe diagnostic reports to grant the proposed model a\nprofessional understanding of medical inputs."}, {"title": "1.2 Step 1: Proactive Question Gener-\nation (Pro-Q Gen)", "content": "The design of ProMRVL-CAD is inspired by the\nconventional diagnostic procedure in which doctors\ninquire about the health conditions of patients. The\ngoal of our proposed Pro-Q Gen is to proactively pose\nqueries to acquire patient's health status information\nthat underlying the potential disease. This requires\nPro-Q Gen with strong disease reasoning capability to\nperform disease diagnosis, making it more challenging\nthan classic recommendation tasks that mainly target\nfeature embedding and pattern recognition. Differ-\nent from existing work where the ChatBot passively\nanswers queries, the Pro-Q Gen model generates ques-\ntions from the ChatBot side to lead the conversation\nthrough a better understanding of patients' health\nconditions. In particular, we developed a dialogue\nrecommendation system that generates proactive ques-\ntions during conversations. Unlike a traditional end-\nto-end doctor agent, our system offers more precise\nquery selection and refinement. As illustrated in Fig-\nure 2, our system operates in two stages: generating\nquery candidates and ranking them using a knowledge\ngraph. This approach enables the proposed model\nto thoroughly explore the candidate query space gen-\nerated by the foundation model and select the most\nrelevant queries for the current response.\nDuring the training process, we use cross-entropy\nto fine-tune the proposed LLMs to generate query\ncandidates. The loss function is defined as\n$L = \\sum_{i=1}^{L} \\log p_{\\Theta}(x_i; X, X_{<i}),$ (1)\nwhere $\\Theta$ stands for trainable parameters, $x_i$ repre-\nsents current predicted tokens with i = 1, ..., L indi-\ncating the location of current token, X stands for"}, {"title": "Candidate Ranking through Clinical Concept\nKnowledge Graph", "content": "In our study, we propose to\nuse knowledge graph [27] to enhance the performance\nof our recommendation system. Inspired by [28], we\ndevelop a clinical concept knowledge graph to embed\nthe structural knowledge between various diseases and\nsymptoms. Note that our clinical concept graph is\nbuilt upon an item graph, with no patient feature\ninvolved to protect users' privacy. The diseases and\nthe corresponding symptoms are extracted from real-\nworld clinical dialogue and medical records to ensure\nclinical professionalism. In Figure 3, we show an\nexample of our knowledge graph with edge widths\nindicating the correlation between the disease and the\nsymptom.\nTo fully utilize the language and reasoning capabil-"}, {"title": "Proactive Medical Dialogue Dataset Genera-\ntion and Data Training on Hybrid Dataset", "content": "Ex-\nisting medical dialogue datasets only contain textual\ndiscussions of health conditions with limited descrip-"}, {"title": "1.3 Step 2:\nMulti-Vision Patient-\nText Diagnostic Report Genera-\ntion (MVP-DR Gen)", "content": "The unique advantage of our proposed Multi-Vision\nPatient-Text Diagnostic Report Generator, namely\nMVP-DR Gen, lies in its multi-modality capability\nto simultaneously and synthetically process textual\nand visual inputs. Particularly, MVP-DR Gen could\nprocess medical visuals with different views if a single\nview is insufficient for disease diagnosis. As shown\nin Figure 2, it employs a MiniLM [24, 37] for textual\nanalysis and a vision transformer (ViT) [38, 39, 40] to\nlearn the high-dimensional visual and texture features\nfrom the multi-view images. Inspired by [41], the\nfeatures extracted from different images/views are\ndirectly averaged to ensure a fixed input size for the\nfollowed vision-language generating task. Our vision-\nanalysis task can be further divided into two sub-\ntasks, the disease identification task and the report\ngeneration task. Specifically, the report generation\nmodule aims to generate the diagnostic report from\nthe embedded features, while the disease identification\ntask contributes to improving the diagnosis capability.\nIn the report generation task, we use an alignment\nlayer, which serves as a soft prompt, to align the"}, {"title": "2 Experiments", "content": null}, {"title": "2.1\nExperiment Setup", "content": "Datasets. We carry out experiments using two\npublicly available datasets: MIMIC-CXR [22] and IU-\nXray [23]. We follow the same data partition policy\nin [22] for the MIMIC-CXR dataset and the partition\npolicy in [43] for the IU-Xray dataset. Additionally,\nwe build two subset datasets, MIMIC-V2 and IU-\nV2, with cases that only contain frontal and lateral\nviews of Xray images. Moreover, as mentioned in Sec-\ntion 1.2, we generated a clinical dialogue dataset that\ncontains both synthetic and real clinical dialogues\nto fine-tune our proposed Pro-Q Gen. Our clinical\ndialogue dataset consists of 78399 (66149 synthetic\nand 12250 real) clinical conversation records. For the\nsynthetic dialogues, we used ChatGPT to generate di-\nalogues using medical history from the MIMIC-CXR\ndataset. For the real clinical dialogues, we used part\nof the Huatuo-26M [36] and CMtMedQA [44] dataset,\nwhich is collected based on real conversations between\nthe patient and doctor. Our diagnostic report gener-\nation task is evaluated on the MIMIC-CXR dataset\nand IU-Xray dataset, as they are among the largest\nreal-world datasets consisting of Xray images and di-\nagnosis reports. In the MIMIC-CXR dataset, each\npatient is associated with one or multiple Xray images,\nalong with a diagnosis report containing impressions,\nfindings, medical history, etc. The IU-Xray dataset\nconsists of 7,470 Xray images, with 3,955 study cases\nand corresponding reports.\nFine-tuning of Pro-Q Gen. In the Pro-Q Gen\nmodule, we use Llama-3-8B Instruct [45] as the back-\nbone model for dialogue generation. During the fine-\ntuning process, we decompose the network parameter\n$\\theta$ to $\\theta_0 + \\Delta\\theta(\\Theta)$ using low-rank adaption with N = 16\nranks.\nTraining of MVP-DR Gen. We set $\\alpha$ = 1 for\nour overall loss function in Eq. 4. In the MVP-DR\nGen module, we adopt the Swin Transformer [46]\npre-trained on ImageNet [47] as the ViT for feature\nembedding with a dimension of 1024. We deploy a\nlinear layer for the alignment layer in the report gen-\neration and a fully connected layer as the classifier\nfor disease identification. The disease identification\nresults are transferred to the corresponding texts and\nintegrated into the end of the generated report. We\nfurther deploy the MiniLM-L6-v2 [24] model to gen-\nerate the word embeddings, with an output of 384\ndimensions for each sentence. Similar to the visual\ntask, the embeddings of sentences in the medical his-\ntory and dialogues are averaged before sending to an\nalignment layer for downstream analysis. Lastly, the"}, {"title": "2.2 Evaluation on Diagnostic Report\nGeneration", "content": "Comparison with SOTAs. We compare the di-\nagnostic report generation performance of ProMRVL-\nCAD with the following SOTA: R2GCMN [8], \u039c\u0395-\nTransformer [54], ChatCAD+ [5], and R2GenGPT\n[55]. In Table 1, we present the overall performance\nof ProMRVL-CAD and baselines on both report text\nquality. Our model achieves the best performance in\nall evaluation metrics regarding BLEU and ROUGE,\nindicating high text quality in comparison with origi-\nnal medical report. Moreover, we comapre the clinical\nefficacy results in Table 2. We observed that the clin-\nical efficacy of the proposed model is much higher\nthan baselines. Particularly, our model shows a signif-\nicant improvement in Recall. These results evidently\ndemonstrate our effectiveness in disease diagnosis and\nabnormality detection, as Recall directly indicates the\ndisease detection capability."}, {"title": "Ablation Study", "content": "We conduct an ablation analysis\nto show the necessity of our modules and strategies.\nIn Table 3, we present the performance of MVP-DR\nGen with different model inputs. Compared with\nsingle-view images, we observe that using images with\nmultiple views can potentially improve the diagnosis\nperformance in the medical report generation. It is\nalso noticed that textual input boosted the report gen-\neration even on the baseline models, which is also a\ndemonstration of scalability of ProMRVL, indicating\nthe feasibility of integrating the framework with other\nVLM. More importantly, these results confirm the ne-\ncessity of using textual health status information for\ndiagnosis report generation. To further demonstrate\nthe impact of text input, we add two additional abla-\ntion studies on the latest two approaches, ChatCAD+\nand R2GenGPT. It is worth noting that baselines\nof ChatCAD+ and R2GenGPT don't support text\ninput themselves. We embedded the textual input in\nthe same way of MVP-DR. We similarly noticed that\ntext input could improve the performance of report\ngeneration. However, the improved performances in\nboth ChatCAD+ and R2GenGPT are still lower than\nProMRVL. This results indicated that although text\ninput could improve the text quality, it it the unique\nmulti-modal design in ProMRVL that boost the per-\nformance in report generation. Other methods (i.e.,\nChatCAD+ and R2GenGPT) could benefit from text\ninput but not as much as ProMRVL."}, {"title": "2.3 Evaluation on Proactive Medical\nDialogue", "content": null}, {"title": "Quantitative Evaluation on Synthetic Medi-\ncal Dialogue Dataset", "content": "We generated 119,276 syn-\nthetic medical conversations based on corresponding\nfiles in the MIMIC-CXR dataset. In Table 4, we show\nthe quantitative evaluation of our synthetic medical\ndialogue dataset with two widely used medical dia-\nlogue datasets, MTS-Dialog [33] and MedDialog [32].\nWe use Professionalism and Conciseness, which are\nconsidered key properties for natural and effective dia-\nlogue, to validate the quality of our synthetic dataset,\nProDial. Same as [36, 44], the Professionalism and\nConciseness of the synthetic medical dialogue dataset\nare automatically evaluated by the quantitative scores\nprovided by ChatGPT. As shown, our synthetic med-\nical dialogue dataset has similar professionalism and\nconciseness properties as natural dialogue datasets. In\nFigure 5, we show two representative dialogue samples\nfrom our proposed Pro-Q Gen model, which provide\nimmediate responses to the previous reactions."}, {"title": "2.4 Evaluation on Robustness of\nProMRVL", "content": "Performance on a New Dataset. We con-\nduct experiments on a similar Xray dataset, the IU-\nXray dataset to show the robustness of our proposed\nProMRVL-CAD system. The study cases with two\nviews (frontal and lateral) images for the IU-Xray\nwere selected and formed a new IU-V2 dataset. Addi-\ntionally, we create a sub-dataset of the MIMIC-CXR\ndataset, namely MIMIC-V2, which contains the study\ncases with two views. In Table 6, we show our evalua-\ntion results conducted on MIMIC-V2 and IU-V2 with\ndifferent settings.\nWe confirm that the MVP-DR outperforms exist-\ning methods. Moreover, the textual input can further\nimprove the performance of MVP-DR, using the same\nimage input. Note that the text quality in Table 6 is\nhigher than the multi-view results in Table 1. This is\nbecause MIMIC-V2 is a subset of the MIMIC dataset\nwhere each data subject has two images. For com-\nparison, the dataset used in Table 6 has a portion\nof subjects that only has one image to ensure a fair\ncomparison with other approaches. The discrepancy\nbetween Table 6 and Table 1 highlights the need of\nbuilding a multi-view dataset for diagnosis. Besides,\nthe MVP-DR achieves much higher recall in detect-\ning the top-6 diseases (lung opacity, pleural effusion,\natelectasis, pneumonia, cardiomegaly, and edema) in\nthe MIMIC-CXR dataset as shown in Figure 6, which\ntakes up 16.9%, 14.8%, 13.5%, 13.3%, 11.1%, and\n10.2% of the positive cases in the MIMIC-V2 dataset.\nThis indicates that our proposed MVP-DR Gen is\nless likely to generate misdiagnosed medical reports.\nLastly, our model achieves similar high performance\non both datasets, which evidently shows its general-\nization capability on diagnostic report generation.\nScalability and Complexity. ProMRVL is\ngeneric and it can be easily scalable to other modules\nin terms of embedding, alignment, language model,\netc. Streamlining the system's architecture could facil-\nitate easier deployment and maintenance, enhancing\nits scalability. We notice that the integration of text\ninput with a vision model in ProMRVL can also im-\nprove performance in ChatCAD+ and R2GenGPT,\nas we demonstrated in Table 3. To further improve\nthe scalability, we conduct additional experiments\nto demonstrate the current network has room for a\nsimplified implementation. We further reduce the\ncomplexity of the system using LoRA [25, 26] for Vi-\nsion and LLM models. This approach reduces the\nparameters from 90.9M to 5M. The model perfor-\nmance, after reducing complexity nearly 20 times less,\nis satisfactory (less than 4% overall performance\ndrop) as shown in Table 7.\nRobustness to Data Variability. The general-"}, {"title": "3 Discussion", "content": "In this study, we presented a novel ProMRVL model\nby introducing a proactive dialogue framework and\nmulti-modal analysis, addressing significant challenges\nin medical dialogue and image integration. The frame-\nwork's research paradigm and practical applications\noffer substantial advancements, such as improving\ninteraction quality and mimicking real-world medi-\ncal evaluations. These contributions highlight ProM-\nRVL's capability to handle complex dialogic interac-\ntions and its potential for broader impact in medical\nAI.\nEvaluation results validate the system's effective-\nness and robustness. Comparative experiments reveal\nthat ProMRVL outperforms baseline methods, even\nwhen enriched with textual input as shown in Table\n3. Additionally, the inclusion of a knowledge graph\nsignificantly enhances diagnostic precision and recall"}, {"title": "4 Conclusion", "content": "This paper devises a proactive dialogue system,\nProMRVL-CAD, with the unique multi-modality fea-\nture of processing both clinical visuals and medical\ndialogue for disease diagnosis. The proposed model\ndeploys a novel proactive question generator to mimic\nthe nature of proactive dialogue during conventional\npatient-doctor interactions to collect the patients'\nhealth conditions. Our model has the superior per-\nformance of handling multi-image input over conven-\ntional LLMs with a nature of multi-round conversa-\ntion. Evaluating on two publicly available datasets,\nwe demonstrate that ProMRVL-CAD outperforms\nstate-of-the-arts in generating medical reports. Our\nframework also creates the first synthetic proactive\ndialogue that integrates both medical visuals and\npatients' textual health status information from the\nexisting clinical dataset. Future work will focus on\nimplementing the system across multiple datasets em-\npowered by federated learning."}]}