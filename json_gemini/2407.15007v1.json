{"title": "Is Behavior Cloning All You Need?\nUnderstanding Horizon in Imitation Learning", "authors": ["Dylan J. Foster", "Adam Block", "Dipendra Misra"], "abstract": "Imitation learning (IL) aims to mimic the behavior of an expert in a sequential decision making task\nby learning from demonstrations, and has been widely applied to robotics, autonomous driving, and\nautoregressive text generation. The simplest approach to IL, behavior cloning (BC), is thought to incur\nsample complexity with unfavorable quadratic dependence on the problem horizon, motivating a variety\nof different online algorithms that attain improved linear horizon dependence under stronger assumptions\non the data and the learner's access to the expert.\nWe revisit the apparent gap between offline and online IL from a learning-theoretic perspective, with\na focus on general policy classes up to and including deep neural networks. Through a new analysis of\nbehavior cloning with the logarithmic loss, we show that it is possible to achieve horizon-independent\nsample complexity in offline IL whenever (i) the range of the cumulative payoffs is controlled, and (ii)\nan appropriate notion of supervised learning complexity for the policy class is controlled. Specializing\nour results to deterministic, stationary policies, we show that the gap between offline and online IL is\nnot fundamental: (i) it is possible to achieve linear dependence on horizon in offline IL under dense\nrewards (matching what was previously only known to be achievable in online IL); and (ii) without further\nassumptions on the policy class, online IL cannot improve over offline IL with the logarithmic loss, even\nin benign MDPs. We complement our theoretical results with experiments on standard RL tasks and\nautoregressive language generation to validate the practical relevance of our findings.", "sections": [{"title": "1 Introduction", "content": "Imitation learning (IL) is the problem of emulating an expert policy for sequential decision making by\nlearning from demonstrations. Compared to reinforcement learning (RL), the learner in IL does not observe\nreward-based feedback, and must imitate the expert's behavior based on demonstrations alone; their objective\nis to achieve performance close to that of the expert on an unobserved reward function.\nImitation learning is motivated by the observation that in many domains, demonstrating the desired behavior\nfor a task (e.g., robotic grasping) is simple, while designing a reward function to elicit the desired behavior\ncan be challenging. IL is also often preferable to RL because it removes the need for exploration, leading\nto empirically reduced sample complexity and often much more stable training. Indeed, the relative ease\nof applying IL (over RL methods) has led to extensive adoption, ranging from classical applications in au-\ntonomous driving (Pomerleau, 1988) and helicopter flight (Abbeel and Ng, 2004) to contemporary works that\nleverage deep learning to achieve state-of-the-art performance for self-driving vehicles (Bojarski et al., 2016;\nBansal et al., 2018; Hussein et al., 2017), visuomotor control (Finn et al., 2017; Zhang et al., 2018), navigation\n(Hussein et al., 2018), and game AI (Ibarz et al., 2018; Vinyals et al., 2019). Imitation learning also offers\na conceptual framework through which to study autoregressive language modeling (Chang et al., 2023; Block\net al., 2024a), and a number of useful empirical insights have arisen as a result of this perspective. However,\na central challenge limiting broader real-world deployment is to understand and improve the reliability and\nstability properties of algorithms that support general-purpose (deep/neural) function approximation.\nIn more detail, imitation learning algorithms can be loosely grouped into offline and online approaches.\nOffline imitation learning algorithms only require access to a dataset of logged trajectories from the expert,\nmaking them broadly applicable. The most widely used approach, behavior cloning, reduces imitation learning"}, {"title": "1.1 Background: Offline and Online Imitation Learning", "content": "To motivate our results, we begin by formally introduce the offline and online imitation learning frame-\nworks, highlighting gaps in current sample complexity guarantees concerning horizon dependence. We take\na learning-theoretic perspective, with a focus on general policy classes.\nMarkov decision processes. We study imitation learning in episodic Markov decision processes. Formally,\na Markov decision process $\\mathcal{M} = (\\mathcal{X}, \\mathcal{A}, P,r, H)$ consists of a (potentially large) state space $\\mathcal{X}$, action space\n$\\mathcal{A}$, horizon $H$, probability transition function $P = \\{P_h\\}_{h=0}^H$, where $P_h : \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{X})$, and reward function"}, {"title": "1.1.1 Offline Imitation Learning: Behavior Cloning", "content": "Let $\\pi^* = \\{\\pi_h^* : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})\\}_{h=1}^H$ denote the expert policy. In the offline imitation learning setting, we receive a\ndataset $D = \\{\\sigma^i\\}_{i=1}^n$ of (reward-free) trajectories $\\sigma^i = (x_1, a_1), ..., (x_H, a_H)$ obtained by executing $\\pi^*$ in the\nunderlying MDP $\\mathcal{M}^*$. Using these trajectories, our goal is to learn a policy $\\pi$ such that the rollout regret $J(\\pi^*) \u2014\nJ(\\pi)$ to $\\pi^*$ is as small as possible. We emphasize that $\\pi^*$ is an arbitrary policy, and is not assumed to be optimal.\nBehavior cloning. Behavior cloning, which reduces the imitation learning problem to supervised pre-\ndiction, is the dominant offline imitation learning paradigm. To describe the algorithm in its simplest\nform, consider the case where $\\pi^* := \\{\\pi_h^* : \\mathcal{X} \\rightarrow \\mathcal{A}\\}_{h=1}^H$ is deterministic. For a user-specified policy class\n$\\Pi \\subset \\{\\pi_h : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})\\}_{h=1}^H$, the most basic version of behavior cloning (Ross and Bagnell, 2010) solves the\nsupervised classification problem\n$\\hat{\\pi} = \\arg \\min_{\\pi \\in \\Pi} \\frac{1}{n} \\sum_{i=1}^n \\sum_{h=1}^H \\mathbb{I}\\{\\pi_h(x_h^i) \\neq a_h^i\\} =: \\mathcal{L}_{bc}(\\pi)$\nNaturally, other classification losses (e.g., square loss, logistic loss, or log loss) may be used in place of the\nindicator loss. To provide sample complexity bounds for this algorithm, we make a standard realizability\nassumption (e.g., Agarwal et al. (2019); Foster and Rakhlin (2023)).\nAssumption 1.1 (Realizability). The policy class $\\Pi$ contains the expert policy, i.e. $\\pi^* \\in \\Pi$.\nThis assumption asserts that $\\Pi$ is expressive enough to represent the expert policy; depending on the\napplication, $\\Pi$ might be parameterized by simple linear models, or by flexible models such as convolutional\nneural networks or transformers. To simplify presentation, we adopt a standard convention in RL theory and\nfocus on finite classes with $|\\Pi| < \\infty$ (Agarwal et al., 2019; Foster and Rakhlin, 2023). A standard uniform\nconvergence argument implies that if we define $\\mathcal{L}_{bc}(\\pi) = \\sum_{h=1}^H P^{\\pi^*}\\[\\pi(x_h) \\neq \\pi^*(x_h)]$, then with probability"}, {"title": "1.1.2 Online Imitation Learning and Recoverability", "content": "The aforementioned limitations of behavior cloning have motivated online approaches to IL (Ross and\nBagnell, 2010; Ross et al., 2011; Ross and Bagnell, 2014; Sun et al., 2017). In the online framework, learning\nproceeds in $n$ episodes in which the learner can directly interact with the underlying MDP $\\mathcal{M}^*$ and query the\nexpert advice. Concretely, for each episode $i \\in [n]$, the learner executes a policy $\\pi^i = \\{\\pi_h^i : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})\\}_{h=1}^H$\nand receives a trajectory $\\sigma^i = (x_1, a_1, a_1'), ..., (x_H, a_H, a_H')$, in which $a_h \\sim \\pi_h^i(x_h)$, $a_h' \\sim \\pi^*(x_h)$, and\n$x_{h+1} \\sim P_h(x_h, a_h)$; in other words, the trajectory induced by the learner's policy is annotated by the expert's\naction $a_h' \\sim \\pi^*(x_h)$ at each state $x_h$ encountered. After all $n$ episodes conclude, the learner produces a final\npolicy $\\hat{\\pi}$ whose regret to $\\pi^*$ should be small. Online imitation learning can avoid error amplification and\nachieve improved dependence on horizon for MDPs that satisfy a recoverability condition (Ross et al., 2011;\nRajaraman et al., 2021a).\nDefinition 1.1 (Recoverability parameter). The recoverability parameter for an MDP $\\mathcal{M}^*$ and expert $\\pi^*$ is\ngiven by\n$\\mu = \\max_{x \\in \\mathcal{X}, a \\in \\mathcal{A}, h \\in [H]} \\{ (Q_h^*(x, \\pi^*(x)) - Q_h^*(x, a))_+ \\} \\in [0, R]$\nUnder recoverability, the Dagger algorithm of Ross et al. (2011) leverages online interaction by interactively\nquerying the expert and learning to correct mistakes on-policy, leading to sample complexity"}, {"title": "1.2 Contributions", "content": "We present several new results that clarify the role of horizon in offline and online imitation learning.\n1. Horizon-independent analysis of log-loss behavior cloning. Through a new analysis of behavior\ncloning with the logarithmic loss (LogLossBC), we show that it is possible to achieve horizon-\nindependent sample complexity (Jiang and Agarwal, 2018; Wang et al., 2020; Zhang et al., 2021,\n2022) in offline imitation learning whenever (i) the range of the cumulative payoffs is normalized, and\n(ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Our result\nis facilitated by a novel information-theoretic analysis which controls policy behavior at the trajectory\nlevel, supporting both deterministic and stochastic expert policies.\n2. Deterministic policies: Closing the gap between offline and online IL. Specializing LogLossBC to\ndeterministic stationary policies (more generally, policies with parameter sharing) and cumulative rewards in\nthe range [0, H], we show that it is possible to achieve sample complexity with linear dependence on horizon\nin offline IL in arbitrary MDPs, matching was was previously only known of online IL. We complement\nthis result with a lower bound showing that, without further structural assumptions on the policy class\n(e.g., no parameter sharing (Rajaraman et al., 2020)), online IL cannot improve over offline IL with\nLogLossBC, even for benign MDPs. Our results are summarized in Table 1. Nonetheless, as observed in prior\nwork (Rajaraman et al., 2020), online imitation learning can still be beneficial for non-stationary policies.\n3. Stochastic policies: Tight understanding of optimal sample complexity. For stochastic expert\npolicies, our analysis of LogLossBC gives the first variance-dependent sample complexity bounds for imita-\ntion learning with general policy classes, which we prove to be tight in a problem-dependent and minimax\nsense. Using this result, we show that for stochastic stationary experts, (i) quadratic dependence on the\nhorizon is necessary when cumulative rewards lie in the range [0, H], in contrast to the deterministic\nsetting, but (ii) LogLossBC\u2014through our variance-dependent analysis\u2014can sidestep this hardness and\nachieve linear dependence on horizon under a recoverability-like condition. Finally, we show that, as in the"}, {"title": "2 Horizon-Independent Analysis of Log-Loss Behavior Cloning", "content": "This section presents the first of our main results, a horizon-independent sample complexity analysis of\nlog-loss behavior cloning for the case of deterministic experts. Our second main result, handles the case of\nstochastic experts, builds on our results here, and is presented in Section 3."}, {"title": "2.1 Log-Loss Behavior Cloning and Supervised Learning Guarantees", "content": "The workhorse for all of our results (both for deterministic and stochastic experts), is the following simple mod-\nification to behavior cloning. For a class of (potentially stochastic) policies $\\Pi$, we minimize the logarithmic loss:"}, {"title": "2.2 Horizon-Independent Analysis of LogLossBC for Deterministic Experts", "content": "We first consider the case where the expert \u03c0* is deterministic. Our main result is the following theorem,\nwhich translates the supervised learning error $\\mathcal{D}_\\frac{1}{2}(\\mathbb{P}_{\\pi}, \\mathbb{P}_{\\hat{\\pi}})$ into a bound on rollout performance in a horizon-\nindependent fashion.\nTheorem 2.1 (Horizon-independent regret decomposition (deterministic case)). For any deterministic policy\n$\\pi^*$ and potentially stochastic policy $\\pi$,\n$J(\\pi^*) \u2013 J(\\hat{\\pi}) \\leq 4R \\cdot D_\\frac{1}{2}(\\mathbb{P}_{\\hat{\\pi}}, \\mathbb{P}_{\\pi^*})$\nThis result shows that horizon-independent bounds on rollout performance are possible whenever (i) rewards\nare appropriately normalized, and (ii) the supervised learning error $\\mathcal{D}_\\frac{1}{2}(\\mathbb{P}_{\\pi}, \\mathbb{P}_{\\pi^*})$ is appropriately controlled.\nIt is proven using novel trajectory-level control over deviations between $\\hat{\\pi}$ and $\\pi^*$; we will elaborate upon this\nin the sequel. We emphasize that this result would be trivial if squared Hellinger distance were replaced by\ntotal variation distance in (6); that the bound scales with squared Hellinger distance is crucial for obtaining\nfast 1/n-type rates and linear horizon dependence. We further remark that this reduction is not specific to\nLogLossBC, and can be applied to any IL algorithm for which we can bound the Hellinger distance. Combining\nTheorem 2.1 with Proposition 2.1, we obtain the following guarantee for finite policy classes."}, {"title": "2.3 Interpreting the Sample Complexity of LogLossBC", "content": "To understand the behavior of the bound for LogLossBC in Corollary 2.1 in more detail, we consider two\nspecial cases (summarized in Table 1).\nStationary policies and parameter sharing. If $\\log |\\Pi| = O(1)$, the bound in Eq. (7) is independent\nof horizon in the case of sparse rewards (R = O(1)), and linear in horizon in the case of dense rewards\n(R = O(H)). In other words, our work establishes for the first time that:\nO(H) sample complexity can be achieved in offline IL under dense rewards for general II,\nas long as $\\log |\\Pi|$ is appropriately controlled. This runs somewhat counter to intuition expressed in prior work\n(Ross and Bagnell, 2010; Ross et al., 2011; Ross and Bagnell, 2014; Rajaraman et al., 2020, 2021a,b; Swamy\net al., 2022), but we will show in the sequel that there is no contradiction.\nGenerally speaking, we expect to have $\\log |\\Pi| = O(1)$ if $\\Pi$ consists of stationary policies or more broadly,\npolicies with parameter sharing across steps $h \\in [H]$ (as is the case in transformers used for autoregressive text\ngeneration). As an example, for a tabular (finite state/action) MDP, if $\\Pi$ consists of all stationary policies,\nwe have $\\log |\\Pi| = |\\mathcal{X}| \\log |\\mathcal{A}|$, so Eq. (7) gives $J(\\pi^*) \u2013 J(\\hat{\\pi}) < R \\frac{|\\mathcal{X}| \\log(|\\mathcal{A}| \\delta^{-1})}{n}$; that is, stationary policies can\nbe learned with horizon-independent samples complexity under sparse rewards and linear dependence on\nhorizon under dense rewards.\nSimilar behavior holds for non-stationary policies with parameter sharing. For example, we show (Ap-\npendix C.1) that for linear policy classes of the form $\\pi_h(a \\mid x) \\propto \\exp((\\phi_h(x, a), \\theta))$ for a feature map\n$\\phi_h(x, a) \\in \\mathbb{R}^d$, one can take $\\mathcal{D}_\\frac{1}{2}(\\mathbb{P}_{\\hat{\\pi}}, \\mathbb{P}_{\\pi^*}) = \\tilde{O}(\\frac{d}{n})$, so that Theorem 2.1 gives $J(\\pi^*) \u2013 J(\\hat{\\pi}) \\leq \\tilde{O}(d)$.\nNon-stationary policies or no parameter sharing. For non-stationary policies or policies with no\nparameter sharing across steps $h$ (e.g., product classes where $\\Pi = \\Pi_1 \\times \\Pi_2 \\cdots \\times \\Pi_H$), we expect $\\log |\\Pi| = O(H)$\n(more generally, $\\mathcal{D}_\\frac{1}{2}(\\mathbb{P}_{\\hat{\\pi}}, \\mathbb{P}_{\\pi^*}) = O(\\frac{H}{n})$). For example, in a tabular MDP, if $\\Pi$ consists of all non-stationary\npolicies, we have $\\log |\\Pi| = H |\\mathcal{X}| \\log |\\mathcal{A}|$. In this case, Eq. (7) gives linear dependence on horizon for sparse\nrewards ($J(\\pi^*) \u2013 J(\\hat{\\pi}) \\leq R \\frac{H |\\mathcal{X}| \\log(|\\mathcal{A}| \\delta^{-1})}{n}$) and quadratic dependence on horizon for dense rewards ($J(\\pi^*) \u2013\nJ(\\hat{\\pi}) \\leq \\frac{H^2 |\\mathcal{X}| \\log(|\\mathcal{A}| \\delta^{-1})}{n}$). The latter bound is known to be optimal (Rajaraman et al., 2020) for offline IL."}, {"title": "2.4 Optimality and Consequences for Online versus Offline Imitation Learning", "content": "We now investigate the optimality of Theorem 2.1 and discuss implications for online versus offline imitation\nlearning, as well as connections to prior work. Our main result here shows that in the dense-reward regime\nwhere $r_h \\in [0,1]$ and $R = H$, Theorem 2.1 cannot be improved when $\\log |\\Pi| = O(1)$\u2014even with online access,\nrecoverability, and known dynamics.\nTheorem 2.2 (Lower bound for deterministic experts). For any $n \\in \\mathbb{N}$ and $H \\in \\mathbb{N}$, there exists a (reward-free)\nMDP $\\mathcal{M}^*$ with $|\\mathcal{X}| = |\\mathcal{A}| = 2$, a class of reward functions $\\mathcal{R}$ with $|\\mathcal{R}| = 2$, and a class of deterministic policies\n$\\Pi$ with $|\\Pi| = 2$ with the following property. For any (online or offline) imitation learning algorithm, there"}, {"title": "2.5 Proving Theorem 2.1: How Does LogLossBC Avoid Error Amplification?", "content": "The central object in the proof of Theorem 2.1 is the following trajectory-level distance function between\npolicies. For a pair of potentially stochastic policies $\\pi$ and $\\pi'$, define\n$\\rho(\\pi \\| \\pi') := \\mathbb{E}^{\\pi} \\mathbb{E}_{a_{1:H} \\sim \\pi'(x_{1:H})} [\\mathbb{I}\\{ \\exists h : a_h \\neq a_h' \\}]$\nwhere we use the shorthand $a_{1:H} \\sim \\pi'(x_{1:H})$ to indicate that $a_1 \\sim \\pi'(x_1), ..., a_H' \\sim \\pi'(x_H)$. We begin by\nshowing (Lemma D.2) that for all (potentially stochastic) policies $\\pi^*$ and $\\hat{\\pi}$,\n$J(\\pi^*) \u2013 J(\\hat{\\pi}) < R\\cdot \\rho(\\pi^* \\| \\hat{\\pi}).$"}, {"title": "3 Horizon-Independent Analysis of LogLossBC for Stochastic Experts", "content": "In this section, we turn out attention to the general setting in which the expert policy \u03c0* is stochastic.\nStochastic policies are widely used in practice, where they are useful for modeling multimodal behavior\n(Shafiullah et al., 2022; Chi et al., 2023; Block et al., 2024b), but have received relatively little exploration in\ntheory beyond the work of Rajaraman et al. (2020) for tabular policies.\nOur main result for this section, Theorem 3.1, is a regret decomposition based on the supervised learning\nerror $\\mathcal{D}_\\frac{1}{2}(\\mathbb{P}_{\\hat{\\pi}}, \\mathbb{P}_{\\pi^*})$ that is horizon-independent and variance-dependent (Zhou et al., 2023; Zhao et al., 2023;\nWang et al., 2024). To state the guarantee, we define the following notion of variance for the expert policy:\n$\u03c3^2_{\\pi^*} := \\sum_{h=1}^H \\mathbb{E}^{\\pi^*} [(Q^*(x_h, \\pi^*(x_h)) \u2013 Q^*(x_h, a_h))^2]$.\nWe can equivalently write this as $\\sigma^2_{\\pi^*} = \\sum_{h=1}^H \\mathbb{E}^{\\pi^*} [(V^*(x_h) \u2013 Q^*(x_h, a_h))^2]$. Our main result is as follows.\nTheorem 3.1 (Horizon-independent regret decomposition). Assume $R > 1$. For any pair of (potentially\nstochastic) policies $\\pi^*$ and $\\hat{\\pi}$ and any $\u03b5 \\in (0, e^{-1})$,\n$J(\\pi^*) \u2013 J(\\hat{\\pi}) \\leq 4\\sqrt{6} \\sqrt{\u03c3^2_{\\pi^*} \\mathcal{D}_\\frac{1}{2}(\\mathbb{P}_{\\hat{\\pi}}, \\mathbb{P}_{\\pi^*})} + O(R \\cdot \\log(Re^{-1})) \\cdot \\mathcal{D}_\\frac{1}{2}(\\mathbb{P}_{\\hat{\\pi}}, \\mathbb{P}_{\\pi^*}) + \u03b5$.\nApplying this result with LogLossBC leads to the following guarantee.\nCorollary 3.1 (Regret of LogLossBC). For any expert $\\pi^* \\in \\Pi$, the LogLossBC algorithm in Eq. (5) ensures\nthat with probability at least $1 \u2013 \u03b4$,\n$J(\\pi^*) \u2013 J(\\hat{\\pi}) \\leq O(1) \\cdot \\sqrt{\\frac{\u03c3^2_{\\pi^*} \\log(|\\Pi| \\delta^{-1})}{n}} + O(R \\log(n)) \\cdot \\frac{\\log(|\\Pi| \\delta^{-1})}{n}$"}, {"title": "3.1 Horizon-Independence and Optimality for Stochastic Experts", "content": "To understand the dependence on horizon in Corollary 3.1, we restrict our attention to the \"parameter sharing\"\ncase where $\\log |\\Pi| = O(1)$, and separately discuss the sparse and dense reward settings (results summarized in\nTable 2).\nConsider the sparse reward setting where $R = O(1)$. Here, at first glance it would appear that the variance\n$\u03c3^2_{\\pi^*}$ should scale with the horizon. Fortunately, this is not the case: The following result\u2014via a law-of-total-\nvariance-type argument (Azar et al., 2017)\u2014implies that Corollary 3.1 is fully horizon-independent, with no\nexplicit dependence on horizon when $R = O(1)$ and $\\log |\\Pi| = O(1)$. For a function $f(x_{1:H}, A_{1:H})$, let $\\text{Var}^\\pi[f]$\ndenote the variance of $f$ under $(x_1, a_1), ..., (x_H, a_H) \\sim \\pi$.\nProposition 3.1. We have that $\u03c3^2_{\\pi^*} < \\text{Var}^{\\pi^*}\\[\\sum_{h=1}^H r_h] \\leq R^2$.\nFor the dense-reward regime where $R = H$, Proposition 3.1 gives $J(\\pi^*) \u2013 J(\\hat{\\pi}) \\leq H \\sqrt{\\frac{\\log |\\Pi|}{n}}$. This is\nsomewhat disappointing, as we now require $\u03a9(H^2)$ trajectories (quadratic sample complexity) to learn a\nnon-trivial policy, even when $\\log |\\Pi| = O(1)$. The following result shows that the dependence on the variance\nin Corollary 2.1 cannot be improved in general, which implies that the quadratic horizon dependence in this\nregime is tight."}, {"title": "3.2 Proof Sketch for Theorem 3.1", "content": "When the expert is stochastic, the trajectory-wise distance in Eq. (8), is no longer useful (i.e., $\u03c1(\\pi^* \\| \\hat{\\pi}) \\neq 0$),\nwhich necessitates a more information-theoretic analysis. Our starting point is the following scale-sensitive\nchange-of-measure lemma for Hellinger distance.\nLemma 3.1 (Change-of-measure for Hellinger distance (Foster et al., 2021, 2022)). Let P and Q be probability\ndistributions over a measurable space $(\\mathcal{X}, \\mathcal{F})$. Then for all functions $h : \\mathcal{X} \\rightarrow \\mathbb{R}$,\n$|\\mathbb{E}_P\\[h(X)] \u2013 \\mathbb{E}_Q\\[h(X)]| \\leq \\sqrt{(\\mathbb{E}_P\\[h^2(X)] + \\mathbb{E}_Q\\[h^2(X)])} \\cdot D_\\frac{1}{2}(P, Q)$.\nIn particular, if $h \\in [0, R]$ almost surely, then\n$\\mathbb{E}_P\\[h(X)] \\leq 2 \\mathbb{E}_Q\\[h(X)] + R \\cdot D_\\frac{1}{2}(P, Q)$.\nWe first sketch how to use this result to prove a weaker version of Theorem 3.1, then explain how to strengthen\nthis argument. Define the sum of advantages for a trajectory $o = (x_1, a_1), ..., (x_H, a_H)$ via\n$\\Delta(o) = \\sum_{h=1}^H Q^*(x_h, \\pi^*(x_h)) \u2013 Q^*(x_h, a_h) = \\sum_{h=1}^H V^*(x_h) \u2013 Q^*(x_h, a_h)$.\nBy the performance difference lemma, we can write $J(\\pi^*) \u2013 J(\\hat{\\pi}) = \\mathbb{E}\\[\\Delta(o)]$, so applying Eq. (14) yields\n$J(\\pi^*) \u2013 J(\\hat{\\pi}) = \\mathbb{E}\\[\\Delta(o)] \\leq \\mathbb{E}^{\\pi^*}\\[\\Delta(o)] + \\sqrt{(\\mathbb{E}^{\\hat{\\pi}}\\[\\Delta^2(o)] + \\mathbb{E}^{\\pi^*}\\[\\Delta^2(o)])} \\cdot D_\\frac{1}{2}(\\mathbb{P}_{\\hat{\\pi}}, \\mathbb{P}_{\\pi^*})$.\nFrom here, we observe that $\\mathbb{E}^{\\pi^*}\\[\\Delta(o)] = 0$ and $\\mathbb{E}^{\\pi^*}\\[\\Delta^2(o)] = \u03c3^2_{\\pi^*}$ (this follows because advantages are a\nmartingale difference sequence under $\\mathbb{P}^{\\pi^*}$), so all that remains is to bound the term $\\mathbb{E}^{\\hat{\\pi}}\\[\\Delta^2(o)]$. A crude\napproach is to observe that $|\\Delta(o)| \\leq H$, so that applying Eq. (15) gives\n$\\mathbb{E}^{\\hat{\\pi}}\\[\\Delta^2(o)] \\leq \\mathbb{E}^{\\pi^*}\\[\\Delta^2(o)] + (H)^2 \\cdot D_\\frac{1}{2}(\\mathbb{P}_{\\hat{\\pi}}, \\mathbb{P}_{\\pi^*}),$"}, {"title": "4 To What Extent is Online Interaction Beneficial?", "content": "Our results in Sections 2 and 3 show that the benefits of online interaction in imitation learning to the\nextent that horizon is concerned are more limited than previously thought. We expect that \u2018in practice,\nonline interaction may still lead to benefits, but in a problem-dependent sense. To this end, we now highlight\nseveral special cases in which online interaction does indeed lead to benefits over offline imitation learning,\nbut in a policy class-dependent fashion not captured by existing theory. In particular, we identify three\nphenomena which lead to improved sample complexity: (i) representational benefits; (ii) value-based feedback;\nand (iii) exploration. Our results in this section can serve as a starting point toward developing a more\nfine-grained understanding of algorithms and sample complexity of imitation learning.\nRepresentational benefits. The classical intuition behind algorithms like Dagger and Aggrevate (which\nDefinition 1.1 attempts to quantify) is recoverability: through online access, we can learn to correct the\nmistakes of an imperfect policy. Our results in Sections 2 and 3 show that recoverability has limited benefits\nfor stationary policy classes as far as horizon is concerned. In spite of this, the following proposition shows\nthat recoverability can have pronounced benefits for representational reasons, even with constant horizon.\nProposition 4.1 (Representational benefits of online IL). For any $N \\in \\mathbb{N}$, there exists a class $\\mathcal{M}$ of MDPs\nwith $H = 2$ and a policy class $\\Pi$ with $\\log |\\Pi| = O(N)$ such that"}, {"title": "5 Experiments", "content": "In this section, we validate our theoretical results empirically. We first provide a detailed overview of our\nexperimental setup, including the control and natural language tasks we consider, then present empirical\nresults for each task individually."}, {"title": "5.1 Experimental Setup", "content": "We evaluate the effect of horizon on the performance of LogLossBC in three environments. We begin by\ndescribing our training and evaluation protocol (which is agnostic to the environment under consideration),\nthen provide details for each environment.\nIn each experiment, we begin with an expert policy \u03c0* (which is always a neural network; details below) and\nconstruct an offline dataset by rolling out with it n times for H timesteps per episode. To train the imitator\npolicy, we use the same architecture as the expert, but randomly initialize the weights and use stochastic\ngradient descent with the Adam optimizer to minimize the LogLossBC objective for the offline dataset. We\nrepeat this entire process for varying values of H.\nTo evaluate the regret $J(\\pi^*) \u2013 J(\\hat{\\pi})$ after training, we approximate the average reward of the imitator\npolicy by selecting new random seeds and collecting n trajectories of length H by rolling out with $\\hat{\\pi}$; we\napproximate the average reward of the expert $\\pi^*$ in the same fashion, and we also compute several auxiliary\nperformance measures (details below) that aim to capture the distance between $\\hat{\\pi}$ and $\\pi^*$. In all environments,\nwe normalize rewards so that the average reward of the expert is at most 1, in order to bring us to the\nsparse reward setting in Section 1.1 and keep the range of the possible rewards constant as a function of the\n(varying) horizon.\nWe consider four diverse environments, with the aim of evaluating LogLossBC in qualitatively different domains:\n(i) Walker2d, a classical continuous control task from MuJoCo (Towers et al., 2023; Todorov et al., 2012)\nwhere the learner attempts to make a stick figure-like agent walk to the right by controlling its joints; (ii)\nBeamrider, a standard discrete-action RL task from the Atari suite (Bellemare et al., 2013), where the learner\nattempts to play the game of Beamrider; (iii) Car, a top-down discrete car racing environment where the car\nhas to avoid obstacles to reach a goal, and (iv) Dyck, an autoregressive language generation task where the\nagent is given a sequence of brackets in $\\{\\{,\\},\\[,\\],(,)\\})$ and has to close all open brackets in the correct order.\nWe emphasize diversity in task selection in order to demonstrate the generality of our results, covering\ndiscrete and continuous actions spaces, as well as both control and language generation. For some of the\nenvironment (Walker2d, Beamrider), the task is intended to be \u201cstateless\u201d, in the sense that varying the horizon\nH does not change the difficulty of the task itself (e.g., complexity of the expert policy \u03c0*), allowing for an\nhonest evaluation of the difficulty of the learning problem as we vary the horizon H. For other domains, such"}, {"title": "5.2 Results", "content": "We summarize our main findings below.\nEffect of horizon on regret. Figures 1 and 2 plot the relationship between expected regret and the number\nof expert trajectories for the Walker2d (MuJoCo), and BeamriderNoFrameskip (Atari) environments, as the\nhorizon H is varied from 50 to 500. For both environments, we find regret is largely independent of the horizon,\nconsistent with our theoretical results. In fact, in the case"}]}