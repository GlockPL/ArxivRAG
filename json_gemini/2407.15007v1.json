{"title": "Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning", "authors": ["Dylan J. Foster", "Adam Block", "Dipendra Misra"], "abstract": "Imitation learning (IL) aims to mimic the behavior of an expert in a sequential decision making task by learning from demonstrations, and has been widely applied to robotics, autonomous driving, and autoregressive text generation. The simplest approach to IL, behavior cloning (BC), is thought to incur sample complexity with unfavorable quadratic dependence on the problem horizon, motivating a variety of different online algorithms that attain improved linear horizon dependence under stronger assumptions on the data and the learner's access to the expert.\nWe revisit the apparent gap between offline and online IL from a learning-theoretic perspective, with a focus on general policy classes up to and including deep neural networks. Through a new analysis of behavior cloning with the logarithmic loss, we show that it is possible to achieve horizon-independent sample complexity in offline IL whenever (i) the range of the cumulative payoffs is controlled, and (ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Specializing our results to deterministic, stationary policies, we show that the gap between offline and online IL is not fundamental: (i) it is possible to achieve linear dependence on horizon in offline IL under dense rewards (matching what was previously only known to be achievable in online IL); and (ii) without further assumptions on the policy class, online IL cannot improve over offline IL with the logarithmic loss, even in benign MDPs. We complement our theoretical results with experiments on standard RL tasks and autoregressive language generation to validate the practical relevance of our findings.", "sections": [{"title": "1 Introduction", "content": "Imitation learning (IL) is the problem of emulating an expert policy for sequential decision making by learning from demonstrations. Compared to reinforcement learning (RL), the learner in IL does not observe reward-based feedback, and must imitate the expert's behavior based on demonstrations alone; their objective is to achieve performance close to that of the expert on an unobserved reward function.\nImitation learning is motivated by the observation that in many domains, demonstrating the desired behavior for a task (e.g., robotic grasping) is simple, while designing a reward function to elicit the desired behavior can be challenging. IL is also often preferable to RL because it removes the need for exploration, leading to empirically reduced sample complexity and often much more stable training. Indeed, the relative ease of applying IL (over RL methods) has led to extensive adoption, ranging from classical applications in au- tonomous driving (Pomerleau, 1988) and helicopter flight (Abbeel and Ng, 2004) to contemporary works that leverage deep learning to achieve state-of-the-art performance for self-driving vehicles (Bojarski et al., 2016; Bansal et al., 2018; Hussein et al., 2017), visuomotor control (Finn et al., 2017; Zhang et al., 2018), navigation (Hussein et al., 2018), and game AI (Ibarz et al., 2018; Vinyals et al., 2019). Imitation learning also offers a conceptual framework through which to study autoregressive language modeling (Chang et al., 2023; Block et al., 2024a), and a number of useful empirical insights have arisen as a result of this perspective. However, a central challenge limiting broader real-world deployment is to understand and improve the reliability and stability properties of algorithms that support general-purpose (deep/neural) function approximation.\nIn more detail, imitation learning algorithms can be loosely grouped into offline and online approaches.\nOffline imitation learning algorithms only require access to a dataset of logged trajectories from the expert, making them broadly applicable. The most widely used approach, behavior cloning, reduces imitation learning"}, {"title": "1.1 Background: Offline and Online Imitation Learning", "content": "To motivate our results, we begin by formally introduce the offline and online imitation learning frame- works, highlighting gaps in current sample complexity guarantees concerning horizon dependence. We take a learning-theoretic perspective, with a focus on general policy classes.\nMarkov decision processes. We study imitation learning in episodic Markov decision processes. Formally, a Markov decision process \\( M = (\\mathcal{X}, \\mathcal{A}, P,r, H) \\) consists of a (potentially large) state space \\( \\mathcal{X} \\), action space \\( \\mathcal{A} \\), horizon \\( H \\), probability transition function \\( P = \\{P_h\\}_{h=0}^H \\), where \\( P_h : \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{X}) \\), and reward function"}, {"title": "1.1.1 Offline Imitation Learning: Behavior Cloning", "content": "Let \\( \\pi^* = \\{\\pi_h^* : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})\\}_{h=1}^H \\) denote the expert policy. In the offline imitation learning setting, we receive a dataset \\( D = \\{\\sigma^i\\}_{i=1}^n \\) of (reward-free) trajectories \\( \\sigma^i = (x_1, a_1), ..., (x_H, a_H) \\) obtained by executing \\( \\pi^* \\) in the underlying MDP \\( M^* \\). Using these trajectories, our goal is to learn a policy \\( \\pi \\) such that the rollout regret \\( J(\\pi^*) - J(\\pi) \\) to \\( \\pi^* \\) is as small as possible. We emphasize that \\( \\pi^* \\) is an arbitrary policy, and is not assumed to be optimal.\nBehavior cloning. Behavior cloning, which reduces the imitation learning problem to supervised pre- diction, is the dominant offline imitation learning paradigm. To describe the algorithm in its simplest form, consider the case where \\( \\pi^* := \\{\\pi_h^* : \\mathcal{X} \\rightarrow \\mathcal{A}\\}_{h=1}^H \\) is deterministic. For a user-specified policy class \\( \\Pi \\subset \\{\\pi_h : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})\\}_{h=1}^H \\), the most basic version of behavior cloning (Ross and Bagnell, 2010) solves the supervised classification problem\n\n\n\nNaturally, other classification losses (e.g., square loss, logistic loss, or log loss) may be used in place of the indicator loss. To provide sample complexity bounds for this algorithm, we make a standard realizability assumption (e.g., Agarwal et al. (2019); Foster and Rakhlin (2023)).\nAssumption 1.1 (Realizability). The policy class \\( \\Pi \\) contains the expert policy, i.e. \\( \\pi^* \\in \\Pi \\).\nThis assumption asserts that \\( \\Pi \\) is expressive enough to represent the expert policy; depending on the application, \\( \\Pi \\) might be parameterized by simple linear models, or by flexible models such as convolutional neural networks or transformers. To simplify presentation, we adopt a standard convention in RL theory and focus on finite classes with \\( |\\Pi| < \\infty \\) (Agarwal et al., 2019; Foster and Rakhlin, 2023). A standard uniform convergence argument implies that if we define \\( \\mathcal{L}_{bc}(\\pi) = \\sum_{h=1}^H \\mathbb{P}^{\\pi^*}[\\pi(x_h) \\neq \\pi^*(x_h)] \\), then with probability"}, {"title": "1.1.2 Online Imitation Learning and Recoverability", "content": "The aforementioned limitations of behavior cloning have motivated online approaches to IL (Ross and Bagnell, 2010; Ross et al., 2011; Ross and Bagnell, 2014; Sun et al., 2017). In the online framework, learning proceeds in n episodes in which the learner can directly interact with the underlying MDP \\( M^* \\) and query the expert advice. Concretely, for each episode \\( i \\in [n] \\), the learner executes a policy \\( \\pi^i = \\{\\pi_h^i : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})\\}_{h=1}^H \\) and receives a trajectory \\( \\sigma^* = (x_1^i, a_1^i, a_1^{i'}), ..., (x_{H}^i, a_{H}^i, a_{H}^{i'}), \\) in which \\( a_h^i \\sim \\pi_h^i(x_h^i) \\), \\( a_h^{i'} \\sim \\pi^*(x_h^i) \\), and \\( x_{h+1}^i \\sim P_h(x_h^i, a_h^i) \\); in other words, the trajectory induced by the learner's policy is annotated by the expert's action \\( a_h^{i'} \\sim \\pi^*(x_h^i) \\) at each state \\( x_h^i \\) encountered. After all n episodes conclude, the learner produces a final policy \\( \\hat{\\pi} \\) whose regret to \\( \\pi^* \\) should be small. Online imitation learning can avoid error amplification and achieve improved dependence on horizon for MDPs that satisfy a recoverability condition (Ross et al., 2011; Rajaraman et al., 2021a).\nDefinition 1.1 (Recoverability parameter). The recoverability parameter for an MDP \\( M^* \\) and expert \\( \\pi^* \\) is given by\n\\mu = \\max_{x \\in \\mathcal{X}, a \\in \\mathcal{A}, h \\in [H]} \\{(Q_h^*(x, \\pi_h^*(x)) - Q_h^*(x, a))^+\\} \\in [0, R].\nUnder recoverability, the Dagger algorithm of Ross et al. (2011) leverages online interaction by interactively querying the expert and learning to correct mistakes on-policy, leading to sample complexity"}, {"title": "1.2 Contributions", "content": "We present several new results that clarify the role of horizon in offline and online imitation learning.\n1.  Horizon-independent analysis of log-loss behavior cloning. Through a new analysis of behavior cloning with the logarithmic loss (LogLossBC), we show that it is possible to achieve horizon- independent sample complexity (Jiang and Agarwal, 2018; Wang et al., 2020; Zhang et al., 2021, 2022) in offline imitation learning whenever (i) the range of the cumulative payoffs is normalized, and (ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Our result is facilitated by a novel information-theoretic analysis which controls policy behavior at the trajectory level, supporting both deterministic and stochastic expert policies.\n2.  Deterministic policies: Closing the gap between offline and online IL. Specializing LogLossBC to deterministic stationary policies (more generally, policies with parameter sharing) and cumulative rewards in the range [0, H], we show that it is possible to achieve sample complexity with linear dependence on horizon in offline IL in arbitrary MDPs, matching was was previously only known of online IL. We complement this result with a lower bound showing that, without further structural assumptions on the policy class (e.g., no parameter sharing (Rajaraman et al., 2020)), online IL cannot improve over offline IL with LogLossBC, even for benign MDPs. Nonetheless, as observed in prior work (Rajaraman et al., 2020), online imitation learning can still be beneficial for non-stationary policies.\n3.  Stochastic policies: Tight understanding of optimal sample complexity. For stochastic expert policies, our analysis of LogLossBC gives the first variance-dependent sample complexity bounds for imita- tion learning with general policy classes, which we prove to be tight in a problem-dependent and minimax sense. Using this result, we show that for stochastic stationary experts, (i) quadratic dependence on the horizon is necessary when cumulative rewards lie in the range [0, H], in contrast to the deterministic setting, but (ii) LogLossBC-through our variance-dependent analysis can sidestep this hardness and achieve linear dependence on horizon under a recoverability-like condition. Finally, we show that, as in the"}, {"title": "1.3 Paper Organization", "content": "Section 2 presents the first of our main results, a horizon-independent sample complexity analysis for LogLossBC for deterministic experts, and discusses implications regarding the gap between offline and online IL as it concerns horizon. Section 3 presents analogous results and implications for stochastic experts. Section 4 discusses mechanisms through which online IL can have benefits over offline IL, beyond horizon dependence, highlighting directions for future research. Section 5 presents an empirical validation, and we conclude with open problems and further directions for future research in Section 6.3. Proofs and additional results are deferred to the appendix.\nNotation. For an integer \\( n \\in \\mathbb{N} \\), we let \\( [n] \\) denote the set \\( \\{1, ..., n\\} \\). For a set \\( \\mathcal{X} \\), we let \\( \\Delta(\\mathcal{X}) \\) denote the set of all probability distributions over \\( \\mathcal{X} \\). We use \\( I_x \\in \\Delta(\\mathcal{X}) \\) to denote the direct delta distribution, which places probability mass 1 on x. We adopt standard big-oh notation, and write \\( f = \\tilde{O}(g) \\) to denote that \\( f = O(g \\cdot \\max\\{1, polylog(g)\\}) \\) and \\( a \\leq b \\) as shorthand for \\( a = O(b) \\)."}, {"title": "2 Horizon-Independent Analysis of Log-Loss Behavior Cloning", "content": "This section presents the first of our main results, a horizon-independent sample complexity analysis of log-loss behavior cloning for the case of deterministic experts. Our second main result, handles the case of stochastic experts, builds on our results here, and is presented in Section 3."}, {"title": "2.1 Log-Loss Behavior Cloning and Supervised Learning Guarantees", "content": "The workhorse for all of our results (both for deterministic and stochastic experts), is the following simple mod- ification to behavior cloning. For a class of (potentially stochastic) policies \\( \\Pi \\), we minimize the logarithmic loss:"}, {"title": "2.2 Horizon-Independent Analysis of LogLossBC for Deterministic Experts", "content": "We first consider the case where the expert \\( \\pi^* \\) is deterministic. Our main result is the following theorem, which translates the supervised learning error \\( D_\\triangle(P_{\\pi}, P_{\\pi^*}) \\) into a bound on rollout performance in a horizon- independent fashion."}, {"title": "2.3 Interpreting the Sample Complexity of LogLossBC", "content": "To understand the behavior of the bound for LogLossBC in Corollary 2.1 in more detail, we consider two special cases (summarized in Table 1).\nStationary policies and parameter sharing. If log\\(|\\Pi|\\) = O(1), the bound in Eq. (7) is independent of horizon in the case of sparse rewards (R = O(1)), and linear in horizon in the case of dense rewards (R = O(H)). In other words, our work establishes for the first time that:\nO(H) sample complexity can be achieved in offline IL under dense rewards for general \\( \\Pi \\),\nas long as log\\(|\\Pi|\\) is appropriately controlled. This runs somewhat counter to intuition expressed in prior work (Ross and Bagnell, 2010; Ross et al., 2011; Ross and Bagnell, 2014; Rajaraman et al., 2020, 2021a,b; Swamy et al., 2022), but we will show in the sequel that there is no contradiction.\nGenerally speaking, we expect to have log\\(|\\Pi|\\) = O(1) if \\( \\Pi \\) consists of stationary policies or more broadly, policies with parameter sharing across steps \\( h \\in [H] \\) (as is the case in transformers used for autoregressive text generation). As an example, for a tabular (finite state/action) MDP, if \\( \\Pi \\) consists of all stationary policies, we have log\\(|\\Pi|\\) = \\(|\\mathcal{X}|\\)log\\(|\\mathcal{A}|\\), so Eq. (7) gives \\( J(\\pi^*) - J(\\hat{\\pi}) <R\\frac{|\\mathcal{X}|log(|\\mathcal{A}|\\delta^{-1})}{n} \\); that is, stationary policies can be learned with horizon-independent samples complexity under sparse rewards and linear dependence on horizon under dense rewards.\nSimilar behavior holds for non-stationary policies with parameter sharing. For example, we show (Ap- pendix C.1) that for linear policy classes of the form \\( \\pi_h(a \\mid x) \\propto \\exp(\\langle \\phi_h(x,a), \\theta \\rangle) \\) for a feature map \\( \\phi_h(x, a) \\in \\mathbb{R}^d \\), one can take \\( D_\\triangle(P_{\\pi}, P_{\\hat{\\pi}}) = \\tilde{O}(\\frac{d}{n}) \\), so that Theorem 2.1 gives \\( J(\\pi^*) - J(\\hat{\\pi}) \\leq \\tilde{O}(d) \\).\nNon-stationary policies or no parameter sharing. For non-stationary policies or policies with no parameter sharing across steps h (e.g., product classes where \\( \\Pi = \\Pi_1 \\times \\Pi_2 \\cdots \\times \\Pi_H \\)), we expect log\\(|\\Pi|\\) = O(H) (more generally, \\( D_\\triangle(P_{\\pi}, P_{\\hat{\\pi}}) = O(H/n) \\)). For example, in a tabular MDP, if \\( \\Pi \\) consists of all non-stationary policies, we have log\\(|\\Pi|\\) = \\( H|\\mathcal{X}|\\)log\\(|\\mathcal{A}|\\). In this case, Eq. (7) gives linear dependence on horizon for sparse rewards \\( (J(\\pi^*) - J(\\hat{\\pi}) \\leq \\frac{RH|\\mathcal{X}|log(|\\mathcal{A}|\\delta^{-1})}{n}) \\) and quadratic dependence on horizon for dense rewards \\( (J(\\pi^*) - J(\\hat{\\pi}) \\leq \\frac{H^2|\\mathcal{X}|log(|\\mathcal{A}|\\delta^{-1})}{n}) \\). The latter bound is known to be optimal (Rajaraman et al., 2020) for offline IL."}, {"title": "2.4 Optimality and Consequences for Online versus Offline Imitation Learning", "content": "We now investigate the optimality of Theorem 2.1 and discuss implications for online versus offline imitation learning, as well as connections to prior work. Our main result here shows that in the dense-reward regime where \\( r_h \\in [0,1] \\) and \\( R = H \\), Theorem 2.1 cannot be improved when log\\(|\\Pi|\\) = O(1)\u2014even with online access, recoverability, and known dynamics."}, {"title": "2.5 Proving Theorem 2.1: How Does LogLossBC Avoid Error Amplification?", "content": "The central object in the proof of Theorem 2.1 is the following trajectory-level distance function between policies. For a pair of potentially stochastic policies \\( \\pi \\) and \\( \\pi' \\), define\n\\rho(\\pi \\|\\| \\pi') := \\mathbb{E}^{\\pi} \\mathbb{E}_{a_{1:H} \\sim \\pi'(x_{1:H})}[\\mathbb{I}\\{\\exists h : a_h \\neq a'_h\\}],\nwhere we use the shorthand \\( a_{1:H} \\sim \\pi'(x_{1:H}) \\) to indicate that \\( a_1 \\sim \\pi'(x_1), ..., a'_H \\sim \\pi'(x_H) \\). We begin by showing (Lemma D.2) that for all (potentially stochastic) policies \\( \\pi^* \\) and \\( \\pi \\),\nJ(\\pi^*) - J(\\hat{\\pi}) < R\\cdot \\rho(\\pi^* \\|\\| \\hat{\\pi})."}, {"title": "3 Horizon-Independent Analysis of LogLossBC for Stochastic Experts", "content": "In this section, we turn out attention to the general setting in which the expert policy \\( \\pi^* \\) is stochastic. Stochastic policies are widely used in practice, where they are useful for modeling multimodal behavior (Shafiullah et al., 2022; Chi et al., 2023; Block et al., 2024b), but have received relatively little exploration in theory beyond the work of Rajaraman et al. (2020) for tabular policies.\nOur main result for this section, Theorem 3.1, is a regret decomposition based on the supervised learning error \\( D_\\triangle(P_{\\pi}, P_{\\pi^*}) \\) that is horizon-independent and variance-dependent (Zhou et al., 2023; Zhao et al., 2023; Wang et al., 2024). To state the guarantee, we define the following notion of variance for the expert policy:\n\\sigma_{\\pi^*}^2 := \\sum_{h=1}^H \\mathbb{E}_{\\pi^*} [(Q^*(x_h, \\pi^*(x_h)) - Q^*(x_h, a_h))^2].\nWe can equivalently write this as \\( \\sigma_{\\pi^*}^2 = \\sum_{h=1}^H \\mathbb{E}_{\\pi^*} [(V^*(x_h) - Q^*(x_h, a_h))^2] \\). Our main result is as follows."}, {"title": "3.1 Horizon-Independence and Optimality for Stochastic Experts", "content": "To understand the dependence on horizon in Corollary 3.1, we restrict our attention to the \"parameter sharing\" case where log\\(|\\Pi|\\) = O(1), and separately discuss the sparse and dense reward settings (results summarized in Table 2).\nConsider the sparse reward setting where \\( R = O(1) \\). Here, at first glance it would appear that the variance \\( \\sigma_{\\pi^*}^2 \\) should scale with the horizon. Fortunately, this is not the case: The following result-via a law-of-total- variance-type argument (Azar et al., 2017)-implies that Corollary 3.1 is fully horizon-independent, with no explicit dependence on horizon when \\( R = O(1) \\) and log\\(|\\Pi|\\) = O(1). For a function \\( f(x_{1:H}, A_{1:H}) \\), let \\( \\text{Var}^\\pi[f] \\) denote the variance of f under \\( (x_1, a_1), ..., (x_H, a_H) \\sim \\pi \\).\nFor the dense-reward regime where \\( R = H \\), Proposition 3.1 gives \\( J(\\pi^*) - J(\\hat{\\pi}) \\leq H\\sqrt{\\frac{\\sigma_{\\pi^*}^2 \\log(|\\Pi|)}{n}} \\). This is somewhat disappointing, as we now require \\( \\Omega(H^2) \\) trajectories (quadratic sample complexity) to learn a non-trivial policy, even when log\\(|\\Pi|\\) = O(1). The following result shows that the dependence on the variance in Corollary 2.1 cannot be improved in general, which implies that the quadratic horizon dependence in this regime is tight."}, {"title": "3.2 Proof Sketch for Theorem 3.1", "content": "When the expert is stochastic, the trajectory-wise distance in Eq. (8), is no longer useful (i.e., \\( \\rho(\\pi^* \\|\\| \\hat{\\pi}) \\neq 0 \\)), which necessitates a more information-theoretic analysis. Our starting point is the following scale-sensitive change-of-measure lemma for Hellinger distance.\nLemma 3.1 (Change-of-measure for Hellinger distance (Foster et al., 2021, 2022)). Let P and Q be probability distributions over a measurable space (X,F). Then for all functions h : X \u2192 R,\n\\left|\\mathbb{E}_P[h(X)] - \\mathbb{E}_Q[h(X)]\\right| \\leq \\sqrt{\\left(\\mathbb{E}_P[h^2(X)] + \\mathbb{E}_Q[h^2(X)]\\right)} \\cdot D_\\triangle(P, Q).\nIn particular, if h \u2208 [0, R] almost surely, then\n\\mathbb{E}_P[h(X)] \\leq 2 \\mathbb{E}_Q[h(X)] + R \\cdot D_\\triangle(P, Q).\nWe first sketch how to use this result to prove a weaker version of Theorem 3.1, then explain how to strengthen this argument. Define the sum of advantages for a trajectory \\( o = (x_1, a_1), ..., (x_H, a_H) \\) via\n\\Delta(o) = \\sum_{h=1}^H Q^*(x_h, \\pi^*(x_h)) - Q^*(x_h, a_h) = \\sum_{h=1}^H V^*(x_h) - Q^*(x_h, a_h).\nBy the performance difference lemma, we can write \\( J(\\pi^*) - J(\\hat{\\pi}) = \\mathbb{E}[\\Delta(o)] \\), so applying Eq. (14) yields\nJ(\\pi^*) - J(\\hat{\\pi}) = \\mathbb{E}[\\Delta(o)] \\leq \\mathbb{E}^{\\pi^*}[\\Delta(o)] + \\sqrt{\\left(\\mathbb{E}^\\pi[\\Delta^2(o)] + \\mathbb{E}^{\\pi^*}[\\Delta^2(o)]\\right)} \\cdot D_\\triangle(P_{\\pi}, P_{\\pi^*}).\nFrom here, we observe that \\( \\mathbb{E}^{\\pi^*}[\\Delta(o)] = 0 \\) and \\( \\mathbb{E}^{\\pi^*}[\\Delta^2(o)] = \\sigma_{\\pi^*}^2 \\) (this follows because advantages are a martingale difference sequence under \\( P^{\\pi^*} \\)), so all that remains is to bound the term \\( \\mathbb{E}^\\pi[\\Delta^2(o)] \\). A crude approach is to observe that \\( |\\Delta(o)| \\leq H \\), so that applying Eq. (15) gives"}, {"title": "4 To What Extent is Online Interaction Beneficial?", "content": "Our results in Sections 2 and 3 show that the benefits of online interaction in imitation learning-to the extent that horizon is concerned are more limited than previously thought. We expect that 'in practice, online interaction may still lead to benefits, but in a problem-dependent sense. To this end, we now highlight several special cases in which online interaction does indeed lead to benefits over offline imitation learning, but in a policy class-dependent fashion not captured by existing theory. In particular, we identify three phenomena which lead to improved sample complexity: (i) representational benefits; (ii) value-based feedback; and (iii) exploration. Our results in this section can serve as a starting point toward developing a more fine-grained understanding of algorithms and sample complexity of imitation learning.\nRepresentational benefits. The classical intuition behind algorithms like Dagger and Aggrevate (which Definition 1.1 attempts to quantify) is recoverability: through online access, we can learn to correct the mistakes of an imperfect policy. Our results in Sections 2 and 3 show that recoverability has limited benefits for stationary policy classes as far as horizon is concerned. In spite of this, the following proposition shows that recoverability can have pronounced benefits for representational reasons, even with constant horizon."}, {"title": "5 Experiments", "content": "In this section, we validate our theoretical results empirically. We first provide a detailed overview of our experimental setup, including the control and natural language tasks we consider, then present empirical results for each task individually."}, {"title": "5.1 Experimental Setup", "content": "We evaluate the effect of horizon on the performance of LogLossBC in three environments. We begin by describing our training and evaluation protocol (which is agnostic to the environment under consideration), then provide details for each environment.\nIn each experiment, we begin with an expert policy \u03c0* (which is always a neural network; details below) and construct an offline dataset by rolling out with it n times for H timesteps per episode. To train the imitator policy \\( \\hat{\\pi} \\), we use the same architecture as the expert, but randomly initialize the weights and use stochastic gradient descent with the Adam optimizer to minimize the LogLossBC objective for the offline dataset. We repeat this entire process for varying values of H.\nTo evaluate the regret J(\u03c0*) - J() after training, we approximate the average reward of the imitator policy by selecting new random seeds and collecting n trajectories of length H by rolling out with \\( \\hat{\\pi} \\); we approximate the average reward of the expert \u03c0* in the same fashion, and we also compute several auxiliary performance measures (details below) that aim to capture the distance between and \u03c0*. In all environments, we normalize rewards so that the average reward of the expert is at most 1, in order to bring us to the sparse reward setting in Section 1.1 and keep the range of the possible rewards constant as a function of the (varying) horizon.\nWe consider four diverse environments, with the aim of evaluating LogLossBC in qualitatively different domains: (i) Walker2d, a classical continuous control task from MuJoCo (Towers et al., 2023; Todorov et al., 2012) where the learner attempts to make a stick figure-like agent walk to the right by controlling its joints; (ii) Beamrider, a standard discrete-action RL task from the Atari suite (Bellemare et al., 2013), where the learner attempts to play the game of Beamrider; (iii) Car, a top-down discrete car racing environment where the car has to avoid obstacles to reach a goal, and (iv) Dyck, an autoregressive language generation task where the agent is given a sequence of brackets in {{,}, [, ], (,)} and has to close all open brackets in the correct order.\nWe emphasize diversity in task selection in order to demonstrate the generality of our results, covering discrete and continuous actions spaces, as well as both control and language generation. For some of the environment (Walker2d, Beamrider), the task is intended to be \"stateless\", in the sense that varying the horizon H does not change the difficulty of the task itself (e.g., complexity of the expert policy \u03c0*), allowing for an honest evaluation of the difficulty of the learning problem as we vary the horizon H. For other domains, such"}, {"title": "5.2 Results", "content": "We summarize our main findings below.\nEffect of horizon on regret. Figures 1 and 2 plot the relationship between expected regret and the number of expert trajectories for the Walker2d (MuJoCo), and BeamriderNoFrameskip (Atari) environments, as the horizon H is varied from 50 to 500. For both environments, we find regret is largely independent of the horizon, consistent with our theoretical results. In fact, in the case of BeamriderNoFrameskip, we find that increasing the horizon leads to better regret. To understand this, note that our theory provides horizon-agnostic upper bounds independent of the environment. Our lower bounds are constructed for specific worst-case environments, and not rule out the possibility of improved performance with longer horizons environments with favorable structure. We conjecture that this phenomenon is related to the fact that longer horizons yield fundamentally more data, as the total number of state-action pairs in the expert dataset is equal to nH.\nFigure 3(a) plots our findings for the Dyck environment. Here, we see that with the number of trajectories n fixed, regret does increase with H, which might appear to contradict our theory at first glance. However, we note that the policy class itself must become larger as H increases, as the task itself becomes more difficult (equivalently, the supervised learning error \\( D^2 (P_{\\pi^*}, P_{\\pi}) \\) must grow with H). As a result, the regret is not expected to be independent of H for this environment, in spit of parameter sharing. To verify whether supervised learning error is indeed the cause for horizon dependence for Dyck, Figure 3(b) plots the logarithm of the product of the Frobenius norms of the weight matrices of the expert for varying values of H, as a proxy for supervised learning performance (Bartlett et al., 2017; Golowich et al., 2018). We find that the log-product-norms do in fact grow with H, consistent with the fact that the regret grows with H in this case.\nFor the Car environment, we observe similar behavior to the Dyck environment, visualized in Figure 4. We find that performance degrades slightly as a function of the horizon H, but that this increase in regret can be explained by an increase in the log-product-norm (Figure 3(b)). However, the effect is mild compared to Dyck."}, {"title": "6 Discussion", "content": "We conclude with additional technical remarks and directions for future research."}, {"title": "6.1 When is Indicator-Loss Behavior Cloning Suboptimal?", "content": "Our discussion in Section 1.1.1 suggests that indicator-loss behavior cloning, which solves \\( \\hat{\\pi} = \\text{arg}\\min_{\\pi\\in \\Pi} \\mathcal{L}_{bc}(\\pi) := \\sum_{i=1}^n \\sum_{h=1}^H \\mathbb{I}\\left{\\pi_h(x_h) \\neq a_h\\right\\} \\), can have suboptimal horizon dependence compared to LogLossBC. This turns out to be a subtle point. Suppose that \\( \\pi^* \\) is deterministic, that \\( \\Pi \\) exactly satisfies realizability in the sense that \\( \\pi^* \\in \\Pi \\), and that \\( \\Pi \\) only contains deterministic policies. In this case, we observe that \\( \\mathcal{L}_{bc}(\\hat{\\pi}) = 0 \\) (i.e., \\( \\hat{\\pi} \\) agrees with \\( \\pi^* \\) on every instance in the dataset). Consequently, \\( \\hat{\\pi} \\) can also be viewed as minimizing an empirical version of the trajectory-wise loss in Eq. (8), i.e.\n\nFrom here, a standard uniform convergence argument implies that \\( \\rho(\\pi \\|\\| \\pi^*) \\leq \\sqrt{\\frac{\\text{log}(|\\Pi|\\delta^{-1})}{n}} \\), and by combining this with Eq. (9), we obtain the following result."}, {"title": "6.2 The Role of Misspecification", "content": "This paper (for both deterministic and stochastic experts```json\n{\n  \"title\": \"Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning\"", "authors": ["Dylan J. Foster", "Adam Block", "Dipendra Misra"], "abstract": "Imitation learning (IL) aims to mimic the behavior of an expert in a sequential decision making task by learning from demonstrations, and has been widely applied to robotics, autonomous driving, and autoregressive text generation. The simplest approach to IL, behavior cloning (BC), is thought to incur sample complexity with unfavorable quadratic dependence on the problem horizon, motivating a variety of different online algorithms that attain improved linear horizon dependence under stronger assumptions on the data and the learner's access to the expert.\nWe revisit the apparent gap between offline and online IL from a learning-theoretic perspective, with a focus on general policy classes up to and including deep neural networks. Through a new analysis of behavior cloning with the logarithmic loss, we show that it is possible to achieve horizon-independent sample complexity in offline IL whenever (i) the range of the cumulative payoffs is controlled, and (ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Specializing our results to deterministic, stationary policies, we show that the gap between offline and online IL is not fundamental: (i) it is possible to achieve linear dependence on horizon in offline IL under dense rewards (matching what was previously only known to be achievable in online IL); and (ii) without further assumptions on the policy class, online IL cannot improve over offline IL with the logarithmic loss, even in benign MDPs. We complement our theoretical results with experiments on standard RL tasks and autoregressive language generation to validate the practical relevance of our findings.", "sections": [{"title": "1 Introduction", "content": "Imitation learning (IL) is the problem of emulating an expert policy for sequential decision making by learning from demonstrations. Compared to reinforcement learning (RL), the learner in IL does not observe reward-based feedback, and must imitate the expert's behavior based on demonstrations alone; their objective is to achieve performance close to that of the expert on an unobserved reward function.\nImitation learning is motivated by the observation that in many domains, demonstrating the desired behavior for a task (e.g., robotic grasping) is simple, while designing a reward function to elicit the desired behavior can be challenging. IL is also often preferable to RL because it removes the need for exploration, leading to empirically reduced sample complexity and often much more stable training. Indeed, the relative ease of applying IL (over RL methods) has led to extensive adoption, ranging from classical applications in au- tonomous driving (Pomerleau, 1988) and helicopter flight (Abbeel and Ng, 2004) to contemporary works that leverage deep learning to achieve state-of-the-art performance for self-driving vehicles (Bojarski et al., 2016; Bansal et al., 2018; Hussein et al., 2017), visuomotor control (Finn et al., 2017; Zhang et al., 2018), navigation (Hussein et al., 2018), and game AI (Ibarz et al., 2018; Vinyals et al., 2019). Imitation learning also offers a conceptual framework through which to study autoregressive language modeling (Chang et al., 2023; Block et al., 2024a), and a number of useful empirical insights have arisen as a result of this perspective. However, a central challenge limiting broader real-world deployment is to understand and improve the reliability and stability properties of algorithms that support general-purpose (deep/neural) function approximation.\nIn more detail, imitation learning algorithms can be loosely grouped into offline and online approaches.\nOffline imitation learning algorithms only require access to a dataset of logged trajectories from the expert, making them broadly applicable. The most widely used approach, behavior cloning, reduces imitation learning"}, {"title": "1.1 Background: Offline and Online Imitation Learning", "content": "To motivate our results, we begin by formally introduce the offline and online imitation learning frame- works, highlighting gaps in current sample complexity guarantees concerning horizon dependence. We take a learning-theoretic perspective, with a focus on general policy classes.\nMarkov decision processes. We study imitation learning in episodic Markov decision processes. Formally, a Markov decision process \\( M = (\\mathcal{X}, \\mathcal{A}, P,r, H) \\) consists of a (potentially large) state space \\( \\mathcal{X} \\), action space \\( \\mathcal{A} \\), horizon \\( H \\), probability transition function \\( P = \\{P_h\\}_{h=0}^H \\), where \\( P_h : \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{X}) \\), and reward function"}, {"title": "1.1.1 Offline Imitation Learning: Behavior Cloning", "content": "Let \\( \\pi^* = \\{\\pi_h^* : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})\\}_{h=1}^H \\) denote the expert policy. In the offline imitation learning setting, we receive a dataset \\( D = \\{\\sigma^i\\}_{i=1}^n \\) of (reward-free) trajectories \\( \\sigma^i = (x_1, a_1), ..., (x_H, a_H) \\) obtained by executing \\( \\pi^* \\) in the underlying MDP \\( M^* \\). Using these trajectories, our goal is to learn a policy \\( \\pi \\) such that the rollout regret \\( J(\\pi^*) - J(\\pi) \\) to \\( \\pi^* \\) is as small as possible. We emphasize that \\( \\pi^* \\) is an arbitrary policy, and is not assumed to be optimal.\nBehavior cloning. Behavior cloning, which reduces the imitation learning problem to supervised pre- diction, is the dominant offline imitation learning paradigm. To describe the algorithm in its simplest form, consider the case where \\( \\pi^* := \\{\\pi_h^* : \\mathcal{X} \\rightarrow \\mathcal{A}\\}_{h=1}^H \\) is deterministic. For a user-specified policy class \\( \\Pi \\subset \\{\\pi_h : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})\\}_{h=1}^H \\), the most basic version of behavior cloning (Ross and Bagnell, 2010) solves the supervised classification problem\n\\hat{\\pi} = \\text{arg} \\min_{\\pi \\in \\Pi} \\frac{1}{n} \\sum_{i=1}^n \\sum_{h=1}^H \\mathbb{I}\\{\\pi_h(x_h^i) \\neq a_h^i\\}.\n\n\nNaturally, other classification losses (e.g., square loss, logistic loss, or log loss) may be used in place of the indicator loss. To provide sample complexity bounds for this algorithm, we make a standard realizability assumption (e.g., Agarwal et al. (2019); Foster and Rakhlin (2023)).\nAssumption 1.1 (Realizability). The policy class \\( \\Pi \\) contains the expert policy, i.e. \\( \\pi^* \\in \\Pi \\).\nThis assumption asserts that \\( \\Pi \\) is expressive enough to represent the expert policy; depending on the application, \\( \\Pi \\) might be parameterized by simple linear models, or by flexible models such as convolutional neural networks or transformers. To simplify presentation, we adopt a standard convention in RL theory and focus on finite classes with \\( |\\Pi| < \\infty \\) (Agarwal et al., 2019; Foster and Rakhlin, 2023). A standard uniform convergence argument implies that if we define \\( \\mathcal{L}_{bc}(\\pi) = \\sum_{h=1}^H \\mathbb{P}^{\\pi^*}[\\pi(x_h) \\neq \\pi^*(x_h)] \\), then with probability"}, {"title": "1.1.2 Online Imitation Learning and Recoverability", "content": "The aforementioned limitations of behavior cloning have motivated online approaches to IL (Ross and Bagnell, 2010; Ross et al., 2011; Ross and Bagnell, 2014; Sun et al., 2017). In the online framework, learning proceeds in n episodes in which the learner can directly interact with the underlying MDP \\( M^* \\) and query the expert advice. Concretely, for each episode \\( i \\in [n] \\), the learner executes a policy \\( \\pi^i = \\{\\pi_h^i : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})\\}_{h=1}^H \\) and receives a trajectory \\( \\sigma^* = (x_1^i, a_1^i, a_1^{i'}), ..., (x_{H}^i, a_{H}^i, a_{H}^{i'}), \\) in which \\( a_h^i \\sim \\pi_h^i(x_h^i) \\), \\( a_h^{i'} \\sim \\pi^*(x_h^i) \\), and \\( x_{h+1}^i \\sim P_h(x_h^i, a_h^i) \\); in other words, the trajectory induced by the learner's policy is annotated by the expert's action \\( a_h^{i'} \\sim \\pi^*(x_h^i) \\) at each state \\( x_h^i \\) encountered. After all n episodes conclude, the learner produces a final policy \\( \\hat{\\pi} \\) whose regret to \\( \\pi^* \\) should be small. Online imitation learning can avoid error amplification and achieve improved dependence on horizon for MDPs that satisfy a recoverability condition (Ross et al., 2011; Rajaraman et al., 2021a).\nDefinition 1.1 (Recoverability parameter). The recoverability parameter for an MDP \\( M^* \\) and expert \\( \\pi^* \\) is given by\n\\mu = \\max_{x \\in \\mathcal{X}, a \\in \\mathcal{A}, h \\in [H]} \\{(Q_h^*(x, \\pi_h^*(x)) - Q_h^*(x, a))^+\\} \\in [0, R].\nUnder recoverability, the Dagger algorithm of Ross et al. (2011) leverages online interaction by interactively querying the expert and learning to correct mistakes on-policy, leading to sample complexity"}, {"title": "1.2 Contributions", "content": "We present several new results that clarify the role of horizon in offline and online imitation learning.\n1.  Horizon-independent analysis of log-loss behavior cloning. Through a new analysis of behavior cloning with the logarithmic loss (LogLossBC), we show that it is possible to achieve horizon- independent sample complexity (Jiang and Agarwal, 2018; Wang et al., 2020; Zhang et al., 2021, 2022) in offline imitation learning whenever (i) the range of the cumulative payoffs is normalized, and (ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Our result is facilitated by a novel information-theoretic analysis which controls policy behavior at the trajectory level, supporting both deterministic and stochastic expert policies.\n2.  Deterministic policies: Closing the gap between offline and online IL. Specializing LogLossBC to deterministic stationary policies (more generally, policies with parameter sharing) and cumulative rewards in the range [0, H], we show that it is possible to achieve sample complexity with linear dependence on horizon in offline IL in arbitrary MDPs, matching was was previously only known of online IL. We complement this result with a lower bound showing that, without further structural assumptions on the policy class (e.g., no parameter sharing (Rajaraman et al., 2020)), online IL cannot improve over offline IL with LogLossBC, even for benign MDPs. Nonetheless, as observed in prior work (Rajaraman et al., 2020), online imitation learning can still be beneficial for non-stationary policies.\n3.  Stochastic policies: Tight understanding of optimal sample complexity. For stochastic expert policies, our analysis of LogLossBC gives the first variance-dependent sample complexity bounds for imita- tion learning with general policy classes, which we prove to be tight in a problem-dependent and minimax sense. Using this result, we show that for stochastic stationary experts, (i) quadratic dependence on the horizon is necessary when cumulative rewards lie in the range [0, H], in contrast to the deterministic setting, but (ii) LogLossBC-through our variance-dependent analysis can sidestep this hardness and achieve linear dependence on horizon under a recoverability-like condition. Finally, we show that, as in the"}, {"title": "1.3 Paper Organization", "content": "Section 2 presents the first of our main results, a horizon-independent sample complexity analysis for LogLossBC for deterministic experts, and discusses implications regarding the gap between offline and online IL as it concerns horizon. Section 3 presents analogous results and implications for stochastic experts. Section 4 discusses mechanisms through which online IL can have benefits over offline IL, beyond horizon dependence, highlighting directions for future research. Section 5 presents an empirical validation, and we conclude with open problems and further directions for future research in Section 6.3. Proofs and additional results are deferred to the appendix.\nNotation. For an integer \\( n \\in \\mathbb{N} \\), we let \\( [n] \\) denote the set \\( \\{1, ..., n\\} \\). For a set \\( \\mathcal{X} \\), we let \\( \\Delta(\\mathcal{X}) \\) denote the set of all probability distributions over \\( \\mathcal{X} \\). We use \\( I_x \\in \\Delta(\\mathcal{X}) \\) to denote the direct delta distribution, which places probability mass 1 on x. We adopt standard big-oh notation, and write \\( f = \\tilde{O}(g) \\) to denote that \\( f = O(g \\cdot \\max\\{1, polylog(g)\\}) \\) and \\( a \\leq b \\) as shorthand for \\( a = O(b) \\)."}, {"title": "2 Horizon-Independent Analysis of Log-Loss Behavior Cloning", "content": "This section presents the first of our main results, a horizon-independent sample complexity analysis of log-loss behavior cloning for the case of deterministic experts. Our second main result, handles the case of stochastic experts, builds on our results here, and is presented in Section 3."}, {"title": "2.1 Log-Loss Behavior Cloning and Supervised Learning Guarantees", "content": "The workhorse for all of our results (both for deterministic and stochastic experts), is the following simple mod- ification to behavior cloning. For a class of (potentially stochastic) policies \\( \\Pi \\), we minimize the logarithmic loss:\n\\hat{\\pi} = \\text{arg} \\min_{\\pi \\in \\Pi} \\frac{1}{n} \\sum_{i=1}^n \\sum_{h=1}^H -\\text{log}(\\pi_h(a_h^i \\mid x_h^i))."}, {"title": "2.2 Horizon-Independent Analysis of LogLossBC for Deterministic Experts", "content": "We first consider the case where the expert \\( \\pi^* \\) is deterministic. Our main result is the following theorem, which translates the supervised learning error \\( D_\\triangle(P_{\\pi}, P_{\\pi^*}) \\) into a bound on rollout performance in a horizon- independent fashion."}, {"title": "2.3 Interpreting the Sample Complexity of LogLossBC", "content": "To understand the behavior of the bound for LogLossBC in Corollary 2.1 in more detail, we consider two special cases (summarized in Table 1).\nStationary policies and parameter sharing. If log\\(|\\Pi|\\) = O(1), the bound in Eq. (7) is independent of horizon in the case of sparse rewards (R = O(1)), and linear in horizon in the case of dense rewards (R = O(H)). In other words, our work establishes for the first time that:\nO(H) sample complexity can be achieved in offline IL under dense rewards for general \\( \\Pi \\),\nas long as log\\(|\\Pi|\\) is appropriately controlled. This runs somewhat counter to intuition expressed in prior work (Ross and Bagnell, 2010; Ross et al., 2011; Ross and Bagnell, 2014; Rajaraman et al., 2020, 2021a,b; Swamy et al., 2022), but we will show in the sequel that there is no contradiction.\nGenerally speaking, we expect to have log\\(|\\Pi|\\) = O(1) if \\( \\Pi \\) consists of stationary policies or more broadly, policies with parameter sharing across steps \\( h \\in [H] \\) (as is the case in transformers used for autoregressive text generation). As an example, for a tabular (finite state/action) MDP, if \\( \\Pi \\) consists of all stationary policies, we have log\\(|\\Pi|\\) = \\(|\\mathcal{X}|\\)log\\(|\\mathcal{A}|\\), so Eq. (7) gives \\( J(\\pi^*) - J(\\hat{\\pi}) <R\\frac{|\\mathcal{X}|log(|\\mathcal{A}|\\delta^{-1})}{n} \\); that is, stationary policies can be learned with horizon-independent samples complexity under sparse rewards and linear dependence on horizon under dense rewards.\nSimilar behavior holds for non-stationary policies with parameter sharing. For example, we show (Ap- pendix C.1) that for linear policy classes of the form \\( \\pi_h(a \\mid x) \\propto \\exp(\\langle \\phi_h(x,a), \\theta \\rangle) \\) for a feature map \\( \\phi_h(x, a) \\in \\mathbb{R}^d \\), one can take \\( D_\\triangle(P_{\\pi}, P_{\\hat{\\pi}}) = \\tilde{O}(\\frac{d}{n}) \\), so that Theorem 2.1 gives \\( J(\\pi^*) - J(\\hat{\\pi}) \\leq \\tilde{O}(d) \\).\nNon-stationary policies or no parameter sharing. For non-stationary policies or policies with no parameter sharing across steps h (e.g., product classes where \\( \\Pi = \\Pi_1 \\times \\Pi_2 \\cdots \\times \\Pi_H \\)), we expect log\\(|\\Pi|\\) = O(H) (more generally, \\( D_\\triangle(P_{\\pi}, P_{\\hat{\\pi}}) = O(H/n) \\)). For example, in a tabular MDP, if \\( \\Pi \\) consists of all non-stationary policies, we have log\\(|\\Pi|\\) = \\( H|\\mathcal{X}|\\)log\\(|\\mathcal{A}|\\). In this case, Eq. (7) gives linear dependence on horizon for sparse rewards \\( (J(\\pi^*) - J(\\hat{\\pi}) \\leq \\frac{RH|\\mathcal{X}|log(|\\mathcal{A}|\\delta^{-1})}{n}) \\) and quadratic dependence on horizon for dense rewards \\( (J(\\pi^*) - J(\\hat{\\pi}) \\leq \\frac{H^2|\\mathcal{X}|log(|\\mathcal{A}|\\delta^{-1})}{n}) \\). The latter bound is known to be optimal (Rajaraman et al., 2020) for offline IL."}, {"title": "2.4 Optimality and Consequences for Online versus Offline Imitation Learning", "content": "We now investigate the optimality of Theorem 2.1 and discuss implications for online versus offline imitation learning, as well as connections to prior work. Our main result here shows that in the dense-reward regime where \\( r_h \\in [0,1] \\) and \\( R = H \\), Theorem 2.1 cannot be improved when log\\(|\\Pi|\\) = O(1)\u2014even with online access, recoverability, and known dynamics."}, {"title": "2.5 Proving Theorem 2.1: How Does LogLossBC Avoid Error Amplification?", "content": "The central object in the proof of Theorem 2.1 is the following trajectory-level distance function between policies. For a pair of potentially stochastic policies \\( \\pi \\) and \\( \\pi' \\), define\n\\rho(\\pi \\|\\| \\pi') := \\mathbb{E}^{\\pi} \\mathbb{E}_{a_{1:H} \\sim \\pi'(x_{1:H})}[\\mathbb{I}\\{\\exists h : a_h \\neq a'_h\\}],\nwhere we use the shorthand \\( a_{1:H} \\sim \\pi'(x_{1:H}) \\) to indicate that \\( a_1 \\sim \\pi'(x_1), ..., a'_H \\sim \\pi'(x_H) \\). We begin by showing (Lemma D.2) that for all (potentially stochastic) policies \\( \\pi^* \\) and \\( \\pi \\),\nJ(\\pi^*) - J(\\hat{\\pi}) < R\\cdot \\rho(\\pi^* \\|\\| \\hat{\\pi})."}, {"title": "3 Horizon-Independent Analysis of LogLossBC for Stochastic Experts", "content": "In this section, we turn out attention to the general setting in which the expert policy \\( \\pi^* \\) is stochastic. Stochastic policies are widely used in practice, where they are useful for modeling multimodal behavior (Shafiullah et al., 2022; Chi et al., 2023; Block et al., 2024b), but have received relatively little exploration in theory beyond the work of Rajaraman et al. (2020) for tabular policies.\nOur main result for this section, Theorem 3.1, is a regret decomposition based on the supervised learning error \\( D_\\triangle(P_{\\pi}, P_{\\pi^*}) \\) that is horizon-independent and variance-dependent (Zhou et al., 2023; Zhao et al., 2023; Wang et al., 2024). To state the guarantee, we define the following notion of variance for the expert policy:\n\\sigma_{\\pi^*}^2 := \\sum_{h=1}^H \\mathbb{E}_{\\pi^*} [(Q^*(x_h, \\pi^*(x_h)) - Q^*(x_h, a_h))^2].\nWe can equivalently write this as \\( \\sigma_{\\pi^*}^2 = \\sum_{h=1}^H \\mathbb{E}_{\\pi^*} [(V^*(x_h) - Q^*(x_h, a_h))^2] \\). Our main result is as follows."}, {"title": "3.1 Horizon-Independence and Optimality for Stochastic Experts", "content": "To understand the dependence on horizon in Corollary 3.1, we restrict our attention to the \"parameter sharing\" case where log\\(|\\Pi|\\) = O(1), and separately discuss the sparse and dense reward settings (results summarized in Table 2).\nConsider the sparse reward setting where \\( R = O(1) \\). Here, at first glance it would appear that the variance \\( \\sigma_{\\pi^*}^2 \\) should scale with the horizon. Fortunately, this is not the case: The following result-via a law-of-total- variance-type argument (Azar et al., 2017)-implies that Corollary 3.1 is fully horizon-independent, with no explicit dependence on horizon when \\( R = O(1) \\) and log\\(|\\Pi|\\) = O(1). For a function \\( f(x_{1:H}, A_{1:H}) \\), let \\( \\text{Var}^\\pi[f] \\) denote the variance of f under \\( (x_1, a_1), ..., (x_H, a_H) \\sim \\pi \\).\nFor the dense-reward regime where \\( R = H \\), Proposition 3.1 gives \\( J(\\pi^*) - J(\\hat{\\pi}) \\leq H\\sqrt{\\frac{\\sigma_{\\pi^*}^2 \\log(|\\Pi|)}{n}} \\). This is somewhat disappointing, as we now require \\( \\Omega(H^2) \\) trajectories (quadratic sample complexity) to learn a non-trivial policy, even when log\\(|\\Pi|\\) = O(1). The following result shows that the dependence on the variance in Corollary 2.1 cannot be improved in general, which implies that the quadratic horizon dependence in this regime is tight."}, {"title": "3.2 Proof Sketch for Theorem 3.1", "content": "When the expert is stochastic, the trajectory-wise distance in Eq. (8), is no longer useful (i.e., \\( \\rho(\\pi^* \\|\\| \\hat{\\pi}) \\neq 0 \\)), which necessitates a more information-theoretic analysis. Our starting point is the following scale-sensitive change-of-measure lemma for Hellinger distance.\nLemma 3.1 (Change-of-measure for Hellinger distance (Foster et al., 2021, 2022)). Let P and Q be probability distributions over a measurable space (X,F). Then for all functions h : X \u2192 R,\n\\left|\\mathbb{E}_P[h(X)] - \\mathbb{E}_Q[h(X)]\\right| \\leq \\sqrt{\\left(\\mathbb{E}_P[h^2(X)] + \\mathbb{E}_Q[h^2(X)]\\right)} \\cdot D_\\triangle(P, Q).\nIn particular, if h \u2208 [0, R] almost surely, then\n\\mathbb{E}_P[h(X)] \\leq 2 \\mathbb{E}_Q[h(X)] + R \\cdot D_\\triangle(P, Q).\nWe first sketch how to use this result to prove a weaker version of Theorem 3.1, then explain how to strengthen this argument. Define the sum of advantages for a trajectory \\( o = (x_1, a_1), ..., (x_H, a_H) \\) via\n\\Delta(o) = \\sum_{h=1}^H Q^*(x_h, \\pi^*(x_h)) - Q^*(x_h, a_h) = \\sum_{h=1}^H V^*(x_h) - Q^*(x_h, a_h).\nBy the performance difference lemma, we can write \\( J(\\pi^*) - J(\\hat{\\pi}) = \\mathbb{E}[\\Delta(o)] \\), so applying Eq. (14) yields\nJ(\\pi^*) - J(\\hat{\\pi}) = \\mathbb{E}[\\Delta(o)] \\leq \\mathbb{E}^{\\pi^*}[\\Delta(o)] + \\sqrt{\\left(\\mathbb{E}^\\pi[\\Delta^2(o)] + \\mathbb{E}^{\\pi^*}[\\Delta^2(o)]\\right)} \\cdot D_\\triangle(P_{\\pi}, P_{\\pi^*}).\nFrom here, we observe that \\( \\mathbb{E}^{\\pi^*}[\\Delta(o)] = 0 \\) and \\( \\mathbb{E}^{\\pi^*}[\\Delta^2(o)] = \\sigma_{\\pi^*}^2 \\) (this follows because advantages are a martingale difference sequence under \\( P^{\\pi^*} \\)), so all that remains is to bound the term \\( \\mathbb{E}^\\pi[\\Delta^2(o)] \\). A crude approach is to observe that \\( |\\Delta(o)| \\leq H \\), so that applying Eq. (15) gives"}, {"title": "4 To What Extent is Online Interaction Beneficial?", "content": "Our results in Sections 2 and 3 show that the benefits of online interaction in imitation learning-to the extent that horizon is concerned are more limited than previously thought. We expect that 'in practice, online interaction may still lead to benefits, but in a problem-dependent sense. To this end, we now highlight several special cases in which online interaction does indeed lead to benefits over offline imitation learning, but in a policy class-dependent fashion not captured by existing theory. In particular, we identify three phenomena which lead to improved sample complexity: (i) representational benefits; (ii) value-based feedback; and (iii) exploration. Our results in this section can serve as a starting point toward developing a more fine-grained understanding of algorithms and sample complexity of imitation learning.\nRepresentational benefits. The classical intuition behind algorithms like Dagger and Aggrevate (which Definition 1.1 attempts to quantify) is recoverability: through online access, we can learn to correct the mistakes of an imperfect policy. Our results in Sections 2 and 3 show that recoverability has limited benefits for stationary policy classes as far as horizon is concerned. In spite of this, the following proposition shows that recoverability can have pronounced benefits for representational reasons, even with constant horizon."}, {"title": "5 Experiments", "content": "In this section, we validate our theoretical results empirically. We first provide a detailed overview of our experimental setup, including the control and natural language tasks we consider, then present empirical results for each task individually."}, {"title": "5.1 Experimental Setup", "content": "We evaluate the effect of horizon on the performance of LogLossBC in three environments. We begin by describing our training and evaluation protocol (which is agnostic to the environment under consideration), then provide details for each environment.\nIn each experiment, we begin with an expert policy \u03c0* (which is always a neural network; details below) and construct an offline dataset by rolling out with it n times for H timesteps per episode. To train the imitator policy \\( \\hat{\\pi} \\), we use the same architecture as the expert, but randomly initialize the weights and use stochastic gradient descent with the Adam optimizer to minimize the LogLossBC objective for the offline dataset. We repeat this entire process for varying values of H.\nTo evaluate the regret J(\u03c0*) - J() after training, we approximate the average reward of the imitator policy by selecting new random seeds and collecting n trajectories of length H by rolling out with \\( \\hat{\\pi} \\); we approximate the average reward of the expert \u03c0* in the same fashion, and we also compute several auxiliary performance measures (details below) that aim to capture the distance between and \u03c0*. In all environments, we normalize rewards so that the average reward of the expert is at most 1, in order to bring us to the sparse reward setting in Section 1.1 and keep the range of the possible rewards constant as a function of the (varying) horizon.\nWe consider four diverse environments, with the aim of evaluating LogLossBC in qualitatively different domains: (i) Walker2d, a classical continuous control task from MuJoCo (Towers et al., 2023; Todorov et al., 2012) where the learner attempts to make a stick figure-like agent walk to the right by controlling its joints; (ii) Beamrider, a standard discrete-action RL task from the Atari suite (Bellemare et al., 2013), where the learner attempts to play the game of Beamrider; (iii) Car, a top-down discrete car racing environment where the car has to avoid obstacles to reach a goal, and (iv) Dyck, an autoregressive language generation task where the agent is given a sequence of brackets in {{,}, [, ], (,)} and has to close all open brackets in the correct order.\nWe emphasize diversity in task selection in order to demonstrate the generality of our results, covering discrete and continuous actions spaces, as well as both control and language generation. For some of the environment (Walker2d, Beamrider), the task is intended to be \"stateless\", in the sense that varying the horizon H does not change the difficulty of the task itself (e.g., complexity of the expert policy \u03c0*), allowing for an honest evaluation of the difficulty of the learning problem as we vary the horizon H. For other domains, such"}, {"title": "5.2 Results", "content": "We summarize our main findings below.\nEffect of horizon on regret. Figures 1 and 2 plot the relationship between expected regret and the number of expert trajectories for the Walker2d (MuJoCo), and BeamriderNoFrameskip (Atari) environments, as the horizon H is varied from 50 to 500. For both environments, we find regret is largely independent of the horizon, consistent with our theoretical results. In fact, in the case of BeamriderNoFrameskip, we find that increasing the horizon leads to better regret. To understand this, note that our theory provides horizon-agnostic upper bounds independent of the environment. Our lower bounds are constructed for specific worst-case environments, and not rule out the possibility of improved performance with longer horizons environments with favorable structure. We conjecture that this phenomenon is related to the fact that longer horizons yield fundamentally more data, as the total number of state-action pairs in the expert dataset is equal to nH.\nFigure 3(a) plots our findings for the Dyck environment. Here, we see that with the number of trajectories n fixed, regret does increase with H, which might appear to contradict our theory at first glance. However, we note that the policy class itself must become larger as H increases, as the task itself becomes more difficult (equivalently, the supervised learning error \\( D^2 (P_{\\pi^*}, P_{\\pi}) \\) must grow with H). As a result, the regret is not expected to be independent of H for this environment, in spit of parameter sharing. To verify whether supervised learning error is indeed the cause for horizon dependence for Dyck, Figure 3(b) plots the logarithm of the product of the Frobenius norms of the weight matrices of the expert for varying values of H, as a proxy for supervised learning performance (Bartlett et al., 2017; Golowich et al., 2018). We find that the log-product-norms do in fact grow with H, consistent with the fact that the regret grows with H in this case.\nFor the Car environment, we observe similar behavior to the Dyck environment, visualized in Figure 4. We find that performance degrades slightly as a function of the horizon H, but that this increase in regret can be explained by an increase in the log-product-norm (Figure 3(b)). However, the effect is mild compared to Dyck."}, {"title": "6 Discussion", "content": "We conclude with additional technical remarks and directions for future research."}, {"title": "6.1 When is Indicator-Loss Behavior Cloning Suboptimal?", "content": "Our discussion in Section 1.1.1 suggests that indicator-loss behavior cloning", "mathcal{L}_{bc}(\\pi)": "sum_{i=1}^n \\sum_{h=1}^H \\mathbb{I}\\left{\\pi_h(x_h) \\neq a_h\\right\\} \\), can have suboptimal horizon dependence compared to LogLossBC. This turns out to be a subtle point. Suppose that \\( \\pi^* \\) is deterministic, that \\( \\Pi \\) exactly satisfies realizability in the sense that \\( \\pi^* \\in \\Pi \\), and that \\( \\Pi \\) only contains deterministic policies. In this case, we observe that \\( \\mathcal{L}_{bc}(\\hat{\\pi}) = 0 \\) (i.e., \\( \\hat{\\pi} \\) agrees with \\( \\pi^* \\) on every instance in the dataset). Consequently, \\("}]}]}