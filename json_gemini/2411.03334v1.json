{"title": "Neural network prediction of strong lensing systems\nwith domain adaptation and uncertainty quantification", "authors": ["Shrihan Agarwal", "Aleksandra \u0106iprijanovi\u0107", "Brian D. Nord"], "abstract": "Modeling strong gravitational lenses is computationally expensive for the complex\ndata from modern and next-generation cosmic surveys. Deep learning has emerged\nas a promising approach for finding lenses and predicting lensing parameters,\nsuch as the Einstein radius. Mean-variance Estimators (MVEs) are a common\napproach for obtaining aleatoric (data) uncertainties from a neural network predic-\ntion. However, neural networks have not been demonstrated to perform well on\nout-of-domain target data successfully e.g., when trained on simulated data and\napplied to real, observational data. In this work, we perform the first study of the\nefficacy of MVEs in combination with unsupervised domain adaptation (UDA) on\nstrong lensing data. The source domain data is noiseless, and the target domain\ndata has noise mimicking modern cosmology surveys. We find that adding UDA\nto MVE increases the accuracy on the target data by a factor of about two over an\nMVE model without UDA. Including UDA also permits much more well-calibrated\naleatoric uncertainty predictions. Advancements in this approach may enable future\napplications of MVE models to real observational data.", "sections": [{"title": "1 Introduction and Related Work", "content": "Strong gravitational lensing provides critical insights into galaxy evolution, dark matter, and dark\nenergy [4, 112, 111, 72, 57, 41, 110, 100]. Modern cosmological surveys [20, 35, 3, 81, 24, 60, 55,\n32, 98, 15, 29, 62, 119] are expected to contain 103 - 105 lensing systems [85, 102, 21]. Traditional\nlens finding techniques have relied heavily on human-intensive image reviewing [87, 86, 94, 95, 77],\nand modeling has relied on computationally-intensive analytic likelihood-fitting [13, 65, 58, 27, 31].\nThis has motivated supervised deep learning-based techniques like neural network classification and\nregression to be applied to strong lensing in addition to a wide variety of cosmology topics [80, 105,\n49]. Obtaining uncertainties is important for these areas of study [68]. They can be obtained in\nnetwork regression through a variety of methods e.g., MC Dropout [36, 47, 116, 64], Bayesian\nNeural Networks [BNNs; 19, 18, 8, 114, 43], mean-variance estimation [103, 109, 25, 106], Deep\nEnsembles [17, 40, 106, 63, 28, 37, 2], Deep Evidential Regression [124, 79, 78, 6], and Simulation-\nBased Inference [23, 66, 67, 117]. Once trained, these methods are typically very fast compared\nto traditional parametric modeling methods [68]. However, all of these models face the challenge\nthat there is insufficient observational data for training and instead rely on realistic simulations\n[12, 82, 5, 70, 91, 93, 92]."}, {"title": "2 Methods: Lensing, Mean-variance Networks, and Domain Adaptation", "content": "Physics of strong lensing: Galaxy-scale strong lensing occurs when a foreground lens galaxy deflects\nlight from a background galaxy, creating a magnified and warped image of the background object.\nThis distorted image is the primary observable (Fig. 1(a)) for predicting physics parameters. Multiple\nkinds of noise sources e.g., atmosphere, sky brightness, CCD readout, and photon counting \u2014 can\nfurther distort the image. The Einstein radius \\theta_E indicates the spatial scale of the lensing system and\ndepends on the lens galaxy mass distribution, which is complex but can often be modeled with a\n5-parameter singular isothermal ellipsoid (SIE), including the Einstein radius [84]. We predict \\theta_E.\nMean-variance Estimation Networks: Mean-variance Estimators (MVEs) estimate the mean and\nvariance of data labels, where the variance is the square of the aleatoric uncertainty \\sigma_{al} [103, 25, 99].\nThe MVE loss function is set to the \\beta-negative log-likelihood: L_{MVE} = L_{\\beta-NLL}(\\beta_{NLL}), where\n\\beta_{NLL} is a hyperparameter. For the NLL loss, the gradient becomes small for high-variance data\npoints, causing them to be undersampled. The \\beta-NLL approach resolves this by multiplying a\nvariance-re-weighting term \\beta^2_{NLL} [99]. For \\beta_{NLL} = 1, the gradient is equivalent to that for the\nmean-squared error (MSE) loss. For \\beta_{NLL} = 0, the original NLL loss is recovered.\nUnsupervised Domain Adaptation (UDA): In UDA, the source data have labels, while the target\ndata do not have labels. Common UDA approaches include adversarial methods [74, 42, 38, 39]\nand distance-based methods [44, 61, 107, 122]. We use the distance-based method, Maximum\nMean Discrepancy (MMD), wherein the loss L_{UDA} is a multi-dimensional distance between latent\nembeddings of the source and target data sets [44, 108]. In minimizing the MMD loss, these\nembeddings of the source and target data become aligned and include domain-invariant features,\nwhich allows the model to perform well on domain-shifted data."}, {"title": "3 Experiments", "content": "Data: We use the deeplenstronomy [82, 12, 14] to simulate ground-based telescope images of\ngalaxy-scale strong lenses. Images have a pixel scale 0.263\u2033/pixel, matching the Dark Energy Survey\n(DES) [1]. The lens galaxy light profile (S\u00e9rsic) is assumed to be centered on the lensing mass.\nWe use theoretically and empirically inspired uniform priors for typical strong lensing parameter\ndistributions. For the lens mass, we use SIE profile, Einstein radius \\theta_E, eccentricity {e_{1,1}, e_{1,2}}, and\nexternal shear {\\gamma_{1,2}} [84]. Two-dimensional source eccentricity is {e_{s,1}, e_{s,2}}. For the lens and\nsource light profiles, we use S\u00e9rsic profiles with distribution index n, and scale radius R. The relative\nangular positions between the background and lens galaxies are {x,y}. Prior ranges for all simulation\nparameters can be found in Table 1.\nWe use three photometric bands (g, r, z) to get rich image morphologies during training. To generate\nrealistic galaxy colors, each simulated lens galaxy is assigned a redshift in the range z_l < 0.7, and\neach background galaxy a redshift in the range 1.27 < z_s < 2 according to the DES Y3 Gold\nCatalog [101, 125]. Each galaxy is randomly assigned a color from a real galaxy according to the\nassigned redshift [26]. So that each lens galaxy is visible but not saturated, we use a lower limit on the\napparent magnitude for all bands ({m_g, m_r, m_z} > 17.5) and an upper limit for any one of the bands\n({m_g, m_r, m_z} < 21). For the background galaxy, we use the limits {m_g, m_r, m_z} > 17.5 and\nm_g < 22 [125]. Redshifts are used solely for colors and are independent of the lensing configuration.\nWe induce a domain shift between the source and the target domains in terms of image noise. The\nsource data has noise characteristics that represent a nearly noiseless image: read noise is 0 e\u00af; no\nsky brightness is added; the exposure time is 1000 seconds (set high to minimize Poisson/shot noise);\nthe number of exposures is 10; the zero-point magnitude is 30; the CCD gain is 6.083 e\u00af/count;\nseeing is 0.9\" (moderate for modern optical cosmic surveys) [1, 45, 82]. In contrast, the target data\nhas noise that mimics DES: the read noise is 7.0 e\u00af, the exposure time is 90 seconds (typical of\nmodern optical cosmic surveys) [1, 26], and the number of exposures, the magnitude zero point, the\nsky brightness, and the seeing are sampled from empirical distributions [1]. Our dataset has 100,000\nobjects each for the source and target data. We use a 70/10/20 (training/validation/test) split for all\ndata. The test set is used for all results in this paper. All images have a shape of 40\u00d740 pixels. The\ndataset uses ~ 9 GB. Project data can be found on Zenodo. An example image is shown in Fig. 1(a)."}, {"title": "Model Optimization:", "content": "We build our models using PyTorch [89]. The network has three convolution\nblocks (each with a convolution, maxpooling, and batch normalization layer) followed by three dense\nlayers with 128, 32, and two nodes, respectively. MVE techniques present challenges for training\ne.g., highly fluctuating loss functions [99]. We found that a batch size of 128, a learning rate of 0.001,\nand default settings for Adamw provided optimal model performance [75]. We considered scheduling\nof the hyperparameters for the UDA and MVE losses. We found the best results with \\beta_{NLL} = 0.5 [99]\nand \\lambda_{UDA} = 1.4. Over 150 epochs of training, we selected the best model as the one that minimized\nthe MVE loss on source data. For some seeds, the MVE-UDA model pathologically predicts a mean"}, {"title": "4 Results: UDA improves MVE performance on the target domain", "content": "We trained five models that differed in their weight initialization. The median results across ini-\ntializations are consistent with the \"Selected\" model (Table 2). Therefore, unless otherwise stated,\nwe refer only to results of the \"Selected\" model for clarity of presentation. For the mean residual\n(\\langle\\delta \\theta_E\\rangle = \\langle \\theta_{E,pred} - \\theta_{E, true}\\rangle) and aleatoric uncertainty (\\langle \\sigma_{al}\\rangle), we take the mean over all the data\nfor a single model. Ideally, the successful combination of MVE and UDA (MVE-UDA) would\nperform comparably to the MVE-only model on the source data. When applied to source data, the\nMVE-UDA model has a higher mean residual (\\langle\\delta \\theta_E\\rangle) by ~ 0.015\" compared to the MVE-only model\n(Table 2(a), Fig. 2(a; four left plots)). The mean uncertainty of the MVE-UDA model is approximately\ntwice that of the MVE-only model (Table 2(b), Fig. 2(a; fifth, right plot)). At the same time, the\nMVE-UDA model is better calibrated (less underconfident) than the MVE-only model (Fig. 2(b)).\nFor target data, however, the MVE-only model has a high mean residual (\\langle\\delta \\theta_E\\rangle = 0.0818\", twice\nthe MVE-UDA model's mean residual \\langle\\delta \\theta_E\\rangle = 0.0425\" (Table 2(a)). In contrast, the MVE-only\nmodel has a low mean uncertainty (\\langle \\sigma_{al}\\rangle = 0.0253\u2033, half the MVE-UDA model's mean uncertainty\n(\\langle \\sigma_{al}\\rangle = 0.0503\" (Table 2(b)). Commensurately, the MVE-only model is significantly overconfident,\nwhile the MVE-UDA model is only slightly overconfident (Fig. 2(b)) on target data.\nThe MVE-UDA model uncertainty is higher at both low and high values of \\theta_E (Fig. 2(a) and Fig. 2(a;\nfifth, right plot)). The high uncertainty for the MVE-UDA model at low \\theta_E may be due to low image\nresolution or high seeing, such that smaller lensing arcs could be obscured. The high uncertainty\nat high \\theta_E may be caused by the image being too small to contain the lensing arcs. The residuals\nand uncertainties for both models are slightly larger than uncertainties assumed in some studies\n~ 0.01\" [71] but comparable to those from traditional modeling techniques ~ 1-5% [97, 104]. In\nFig. 1(b), we find that the target and source embeddings do not overlap for the MVE-only model. In\ncontrast, the embeddings overlap almost completely for the MVE-UDA model, and the points exhibit a\ngradient in the Einstein radius. These items indicate that the embedding vectors of both are correlated\nwith \\theta_E, but only the MVE-UDA embedding has accurate alignment across domains. Lastly, the\ncoverage of the MVE-only model varies significantly on the target data across initializations, but\nperformance is stable for MVE-UDA (Fig. 2(b)). We find DA is essential to MVE for better calibrated,\nconsistent, and accurate performance on domain-shifted datasets."}, {"title": "5 Summary and Outlook", "content": "In this work, we provide the first demonstration that unsupervised domain adaptation (UDA) signifi-\ncantly improves the performance of mean-variance estimator (MVE) models on unlabeled target data.\nWe predicted the Einstein radius of strong gravitational lenses with MVEs (\u00a72). We incurred a domain\nshift between the source and target domains so that the source images are approximately noiseless,\nand the target images have noise characteristics similar to DES (\u00a73). When applied to the noisy\ntarget data, the MVE-UDA model is significantly better calibrated, more consistent across weight\ninitialization, and more accurate than the MVE-only model (Fig. 2(a) and Table 2(c,d)). Similar\napproaches may improve neural network model performance when applied to real, observational data."}, {"title": "Acknowledgments and Disclosure of Funding", "content": "A Funding\nWe acknowledge the Deep Skies Lab as a community of multi-domain experts and collaborators\nwho've facilitated an environment of open discussion, idea generation, and collaboration. This\ncommunity was important for the development of this project.\nWork supported by the Fermi National Accelerator Laboratory, managed and operated by\nFermi Research Alliance, LLC under Contract No. DE-AC02-07CH11359 with the U.S. Department\nof Energy. The U.S. Government retains and the publisher, by accepting the article for publication,\nacknowledges that the U.S. Government retains a non-exclusive, paid-up, irrevocable, world-wide\nlicense to publish or reproduce the published form of this manuscript, or allow others to do so, for\nU.S. Government purposes.\nThis material is based upon work supported by the Department of Energy under grant No.\nFNAL 21-25.\nB Author Contributions\nAgarwal: Methodology, Formal analysis, Software, Validation, Data Curation, Investigation, Writing\n- Original Draft\n\u0106iprijanovi\u0107: Conceptualization, Methodology, Formal analysis, Writing Review & Edit-\ning, Supervision, Project administration\nNord: Conceptualization, Methodology, Formal analysis, Resources, Writing - Original\nDraft, Writing - Review & Editing, Supervision, Project administration, Funding acquisition\nWe thank the following colleagues for their insights and discussions during the development of this\nwork: Rebecca Nevin.\nC Software Attribution\nWe used the following software packages: Astropy [11, 9, 10], deeplenstronomy [82],\nlenstronomy [12, 14], Matplotlib [54], Numpy [46], Pandas [88] Python [115], PyTorch [89],\nScipy [56, 121], Seaborn [120], Sklearn [16, 90], Torch [22], Torchvision [76],\nD MVE Network Architecture\nSee Table 3 for the detailed MVE network architecture. There are 112,866 trainable parameters. We\nnote that the activation function for the final dense layers is chosen to be sigmoid rather than ReLU,\nsince ReLU predicts a value of zero for any negative input, encouraging predictions of zero mean or\nvariance. This issue can also be solved by alternative approaches, such as the use of Leaky ReLU or\nother activation functions that disincentivize a prediction of zero.\nE Model inference with varied weight initializations\nWe performed experiments five times, each with a different random seed for the network weight\ninitialization. All models received the same optimization procedure (\u00a73). The performance of\nMVE-only model on the target data sets is inconsistent across the seed choices. In contrast, the\nMVE-UDA model performs consistently slightly worse than the MVE-only model on the source data"}, {"title": "F Computational costs for experiments", "content": "All computing was executed on an NVIDIA A100 GPU with 40GB memory. These computations\nwere performed on the Fermilab Elastic Analysis Facility [EAF; 50]. Training with and without UDA\nrequire the same amount of time, ~ 2.5 hours."}]}