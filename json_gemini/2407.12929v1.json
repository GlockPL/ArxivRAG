{"title": "The Foundation Model Transparency Index v1.1", "authors": ["Rishi Bommasani", "Kevin Klyman", "Sayash Kapoor", "Shayne Longpre", "Betty Xiong", "Nestor Maslej", "Percy Liang"], "abstract": "Foundation models are increasingly consequential yet extremely opaque. To char- acterize the status quo, the Foundation Model Transparency Index was launched in October 2023 to measure the transparency of leading foundation model developers. The October 2023 Index (v1.0) assessed 10 major foundation model developers (e.g. OpenAI, Google) on 100 transparency indicators (e.g. does the developer disclose the wages it pays for data labor?). At the time, developers publicly dis- closed very limited information with the average score being 37 out of 100. \u03a4\u03bf understand how the status quo has changed, we conduct a follow-up study (v1.1) after 6 months: we score 14 developers against the same 100 indicators. While in v1.0 we searched for publicly available information, in v1.1 developers submit reports on the 100 transparency indicators, potentially including information that was not previously public. We find that developers now score 58 out of 100 on average, a 21 point improvement over v1.0. Much of this increase is driven by developers disclosing information during the v1.1 process: on average, developers disclosed information related to 16.6 indicators that was not previously public. We observe regions of sustained (i.e. across v1.0 and v1.1) and systemic (i.e. across most or all developers) opacity such as on copyright status, data access, data labor, and downstream impact. We publish transparency reports for each developer that consolidate information disclosures: these reports are based on the information disclosed to us via developers. Our findings demonstrate that transparency can be improved in this nascent ecosystem, the Foundation Model Transparency In- dex likely contributes to these improvements, and policymakers should consider interventions in areas where transparency has not improved.", "sections": [{"title": "1 Introduction", "content": "Foundation models are the epicenter of artificial intelligence (AI) as AI begins to shape how the economy and society function (Bommasani et al., 2021). For such a high-impact technology, transparency is vital to facilitate accountability, competition, and collective understanding. As an illustrative example, the current lack of transparency regarding the data used to build foundation models makes it difficult to assess what copyrighted information is used to train foundation models. Governments around the world are intervening to increase transparency: for example, the EU AI Act and the US's proposed AI Foundation Model Transparency Act take major strides by mandating a number of disclosure requirements (Bommasani et al., 2024, Appendix A).\nTo characterize the transparency of the foundation model ecosystem, Bommasani et al. (2023a) introduced the Foundation Model Transparency Index (FMTI). Launched in October 2023, the first iteration of the index (FMTI v1.0) scored 10 major foundation developers (e.g. OpenAI, Google, Meta) based on publicly available information regarding 100 transparency indicators. These 100 indicators span matters such as the data, labor, and compute used to build models; the capabilities, limitations, and risks associated with models; and the distribution of models as well as the impact of their use. FMTI v1.0 established that, in the status quo, the foundation model ecosystem was opaque: the average score was 37 points out of 100. Yet FMTI v1.0 also identified heterogeneity in public disclosures: while the top score was a 54, for 82 indicators at least one developer scored a point.\nTo understand how the landscape has evolved in the last 6 months, we conduct a follow-up study (FMTI v1.1).\u00b9 To enable direct comparison, we retain the 100 transparency indicators and the associated threshold for awarding a point from FMTI v1.0. However, instead of searching for public information as was done in FMTI v1.0, we request that developers report the relevant information for each indicator. We implemented this change for three reasons: (i) completeness: we obviate the concern that information was missed when searching the Internet; (ii) clarity: we reduce uncertainty by having developers affirmatively disclose information; and (iii) scalability: we remove the effort required for researchers to conduct an open-ended search for decentralized public information.\nWe contacted 19 foundation model developers, and 14 provided reports related to the 100 transparency indicators (Adept, AI21 Labs, Aleph Alpha, Amazon, Anthropic, BigCode/Hugging Face/Servi- ceNow, Google, IBM, Meta, Microsoft, Mistral, OpenAI, Stability AI, Writer).\u00b2 Given each devel- oper's initial report, we provided scores based on whether each disclosure satisfied the associated indicator. Developers responded to these initial scores, engaging in dialogue via email and virtual meetings, and clarifying matters in many cases. Following this iterative process, for each developer we publish a transparency report that consolidates the information it discloses. These reports contain new information, which developers had not disclosed publicly prior to the start of FMTI v1.1. On average, developers disclosed information related to 16.6 indicators that was not previously public.\nThe FMTI v1.1 results demonstrate ample room for improvement, as well as tangible improvements in transparency over half a year. On average, developers disclose information that satisfies 58 of the 100 transparency indicators. Developers are least transparent with respect to the upstream resources required to build their foundation models, scoring 46%, in comparison to 65% on downstream indicators and 61% on model-related indicators. Developers can become more transparent by drawing on the transparency practices of other developers\u2014at least one developer scores a point on 96 of the 100 indicators, and multiple developers score a point on 89 indicators.\nWe find that developers' scores improved significantly over FMTI v1.0, with a 21 point improvement in the mean overall score. Scores improved across every domain, with upstream, model, and downstream scores improving by 6-7 points. Each of the 8 developers that were evaluated in both v1.0 and v1.1 improved their scores, with an average increase of 19 points. While some developers disclosed substantially more (e.g. AI21 Labs's score increased by 50 points), others made fairly marginal changes (e.g. OpenAI's score increased by just 1 point).\nThese findings affirm that greater transparency is feasible in the foundation model ecosystem. Further, they suggest that the Foundation Model Transparency Index in tandem with other interventions drives improvements in transparency. However, given that there has been very little progress on specific"}, {"title": "2 Background", "content": "To contextualize our effort, we describe FMTI v1.0 and prior work on multi-iteration indices."}, {"title": "2.1 The Foundation Model Transparency Index", "content": "Bommasani et al. (2023a) launched the Foundation Model Transparency Index in October 2023. To conceptualize transparency for foundation models, they introduced a hierarchical taxonomy aligned with the foundation model supply chain (Bommasani et al., 2023b). This taxonomy featured three top-level domains: the upstream resources involved in developing a foundation model, the foundation model itself and its properties, and the downstream use of the foundation model. These domains aggregate 23 subdomains (e.g. the upstream domain contains data, labor, data access, and compute as subdomains) and 100 binary transparency indicators. Using public information identified through a systematic search protocol, FMTI v1.0 scored 10 companies (AI21 Labs, Amazon, Anthropic, Cohere, Google, BigScience/Hugging Face, Inflection, Meta, OpenAI, Stability AI) from 0\u2013100 on the 100 indicators. Companies were sent initial scores and allowed to contest them before FMTI v1.0 was released.\nFMTI v1.0 showed a pervasive lack of transparency in the foundation model ecosystem, with the highest-performing developer scoring just 54 out of 100. Developers disclosed very little information about the labor or compute used to build their foundation models (scoring just 17% on these subdomains), or their real world impact (scoring 11% on this subdomain). Open foundation model developers\u2014which refers to developers who released their flagship foundation model openly (i.e. with widely available model weights; Kapoor et al., 2024)\u2014outperformed closed developers by a wide margin: all three developers with open flagship foundation models were among the top four scoring developers. Several companies disclosed almost no information about their flagship foundation models, with three companies scoring 25% or less."}, {"title": "2.2 Indices over time", "content": "A central objective of an index is to track a concept over time to characterize changes. In doing so, many notable indices have evolved over time to reflect changing circumstances and priorities. For instance, the Human Development Index (HDI) was changed multiple times in the 1990s and 2000s, largely in response to academic criticism (Klasen, 2018; Stanton, 2007). Despite these changes, which complicate direct comparisons across index iterations, the HDI remains one of the most trustworthy and popular indices for human development.\nAs an index is conducted repeatedly, and the world changes as measured by the index, a natural question is how the index contributes to this change. Attributing why corporate behavior (such as disclosure practices) changes is notoriously difficult. Companies generally do not reveal why they make changes and changes generally reflect a confluence of multiple factors. Kogen (2022) provides a unique demonstration of an index's impact, analyzing the 2018 Ranking Digital Rights Index (RDR), which ranked the freedom of expression and privacy policies of 26 of the world's largest ICT companies. By reviewing internal RDR documents and interviewing relevant stakeholders (e.g. representatives from 11 companies and 14 civil society groups), Kogen concluded that RDR had clear influence and that indexes can be useful resources for social movements. In the case of FMTI, we expect that the Index brings attention to the disclosure practices of companies, making it easier for media, policymakers, investors, customers and the public to apply additional pressure that engenders greater transparency. Additionally, the Index provides clarity to companies by setting concrete targets and empowers employees within companies to push for greater transparency.\nTo reason about an index's impact over time, we also draw inspiration from Raji and Buolamwini (2019). In 2017, Buolamwini and Gebru (2018) demonstrated significant performance disparities across demographic groups in 3 face recognition systems (from IBM, Microsoft, and Megvii). A"}, {"title": "3 Methods", "content": "FMTI v1.1 involves four steps: indicator selection, developer selection, information gathering, and scoring. We describe these steps and how they relate to their implementation in FMTI v1.0 below."}, {"title": "3.1 Indicator selection", "content": "To concretize transparency, we use the 100 indicators from FMTI v1.0: Bommasani et al. (2023a) defined these indicators based on the literature regarding foundation models and AI. The 100 indicator are listed by name in Figure 8.\u2074 These indicators span three domains. First, 32 upstream indicators address transparency related to the ingredients and processes of model development, including data, compute, and labor. Second, 33 model indicators address transparency related to the properties and function of the model, including model access, capabilities, risks, and safety mitigations. Third, 35 downstream indicators address transparency related to the release and deployment of models, including usage policies, distribution, privacy protections, and impact. Prior work has strongly motivated the importance of each area of evaluated transparency, from labor (Gray and Suri, 2019a; Crawford, 2021; Hao and Seetharaman, 2023), data (Bender and Friedman, 2018; Gebru et al., 2018; Longpre et al., 2023b,a), compute (Lacoste et al., 2019; Schwartz et al., 2020; Patterson et al., 2021; Luccioni and Hern\u00e1ndez-Garc\u00eda, 2023), evaluation (Liang et al., 2023), safety (Cammarota et al., 2020; Longpre et al., 2024a), privacy (EU, 2016; Brown et al., 2022; Vipra and Myers West, 2023; Winograd, 2023), policies (Kumar et al., 2022; Weidinger et al., 2021; Brundage et al., 2020), and impact (Tabassi, 2023; Weidinger et al., 2023)."}, {"title": "3.2 Developer selection", "content": "In FMTI v1.0, Bommasani et al. (2023a) selected 10 foundation model developers: all 10 were companies developing salient foundation models with consideration given for diversity (e.g. type of company, type of foundation model). Further, for each foundation model developer, Bommasani et al. (2023a) designated a flagship foundation model that was used as the basis for scoring the developer. In FMTI v1.1, we require companies to submit transparency reports: we reached out to leadership at 19 companies: 01.AI, Adept, AI21 Labs, Aleph Alpha, Amazon, Anthropic, BigCode/Hugging Face/ ServiceNow, Cohere, Databricks, Google, IBM, Inflection, Meta, Microsoft, Mistral, OpenAI, Stability AI, Writer, and xAI.\u2075 14 developers agreed to prepare reports and designated their flagship foundation model.\u2076\nTable 1 describes the developers and their flagship foundation models. 8 of the 14 developers are hold-overs from FMTI v1.0.\u2077 Three developers are assessed for the same models as v1.0 (Jurassic-2 for AI21 Labs, Llama 2 for Meta, GPT-4 for OpenAI), whereas five are assessed for new models (Titan Text Express for Amazon, Claude 3 for Anthropic, StarCoder for BigCode/HuggingFace/ServiceNow, Gemini 1.0 Ultra API for Google, and Stable Video Diffusion for Stability AI). The six new developers"}, {"title": "3.3 Information gathering", "content": "In FMTI v1.0, Bommasani et al. (2023a) identified publicly-available sources of information for each developer through a systematic protocol for searching the Internet, which provided the information for all scoring decisions. This approach has four potentially undesirable properties. First, given that information is decentralized across the Internet, the researchers may have missed information.\u2078 Second, the relationship between a piece of public information and an indicator may be indirect and oblique, leading to greater subjectivity in scoring. Third, focusing on public information aligns with a developer's current level of transparency but does not provide developers with an opportunity to disclose further information. Finally, and most fundamentally, this search significantly adds to the cost of executing the index.\nIn FMTI v1.1, we request transparency reports from each developer that directly address each of the 100 indicators. This change in the information gathering process alters the dynamics for the four aforementioned considerations. First, if we assume developers are strongly incentivized to be their own best advocates and are certainly the most knowledgeable entities about their models, then the information they compile should be complete. Second, by having developers directly clarify information on indicators affirmatively, uncertainties that contributed to more subjective scoring are addressed. Third, by allowing developers to include information that was not-previously public, which is made public through this process, opportunities arise for greater transparency. Finally, by having developers gather information, the cost we bear is reduced."}, {"title": "3.4 Scoring", "content": "In FMTI v1.0, once information was identified, two researchers independently scored each of the 1000 (indicator, developer) pairs. The agreement rate was 85.2% (148 disagreements): in the event of"}, {"title": "3.5 Timeline", "content": "We summarize the execution of FMTI v1.1 as follows:\n1. Developer solicitation (December 2023 \u2013 January 2024). We contacted leadership at 19 companies developing foundation models, requesting they submit transparency reports.\n2. Developer reporting (February 2024). 14 developers designated their flagship foundation model and submitted transparency reports in relation to each of the 100 transparency indicators for their flagship model.\n3. Initial scoring (March 2024). We reviewed the developers' reports, ensuring a consistent horizontal standard across all developers in terms of how each indicator was scored.\n4. Developer response (April 2024). We returned the scored reports to developers, who then contested scores on specific indicators (potentially including the disclosure of additional information to justify a different score). Following this process, we finalized the transparency reports, which were validated by the developers prior to public release."}, {"title": "4 Results", "content": "We analyze this iteration of the Index on its own (\u00a74.1), in relation to the first iteration (\u00a74.2), and specifically in terms of new disclosures (\u00a74.3)."}, {"title": "4.1 Standalone results of FMTI v1.1", "content": "While the average score on the Index has significant room for improvement, there is high variability among developers. Based on the overall scores (right of Figure 1), 11 of the 14 developers score below 65, showing that there is a significant lack of transparency in the foundation model ecosystem and substantial room for improvement across developers. The mean and median are 57.93 and 57 respectively, with a standard deviation of 13.98. The highest-scoring developer scores points for 85 of the 100 indicators, while the lowest-scoring developer scores 33. The 3 top-scoring developers (BigCode/Hugging Face/ServiceNow, Aleph Alpha, and AI21 Labs) are more than one standard deviation above the mean, the next 9 are near the mean (IBM, Microsoft, Meta, Stability AI, Writer, Anthropic, OpenAI, Mistral, Google), and the 2 lowest-scoring developers are more than one standard deviation below the mean (Amazon, Adept).\nImprovement is feasible for each developer. In spite of significant opacity, for 96 of the 100 indicators there exists some developer that scores points, and of these there are 89 where multiple developers score points. The disclosures that developers make to satisfy these indicators provide a concrete example of how all developers can be more transparent. If developers emulate the most-transparent developer for each indicator, overall transparency would improve sharply.\nDevelopers disclose significant new information, which contributes to their scores. A developer's total score on the Foundation Model Transparency Index reflects the information that it discloses"}, {"title": "4.2 Comparative results between FMTI v1.1 and v1.0", "content": "Transparency increased across the board from v1.0 to v1.1. Foundation model developers significantly improved their scores between October 2023 and May 2024, with the average score rising from 37 to 58 out of 100. Scores improved on every domain, with average upstream scores improving by the greatest margin (+7.6 points) followed by downstream (+7.2) and model (+6.1). As a result, there is significantly more information publicly available about the upstream resources developers use to build foundation models, the models themselves and how they are evaluated, and their downstream distribution and use.\nTransparency increased in nearly all subdomains from v1.0 to v1.1. Developers improved their scores on every subdomain with the exception of data access. The largest improvements in subdomain scores were in compute (average increase of +2.4 indicators per developer), data labor (+2.3), and risks (+1.6). This broad improvement demonstrates that the overall trend in recent years toward reduced transparency is more nuanced than is commonly understood, though transparency is still"}, {"title": "4.3 New information in FMTI v1.1", "content": "A key feature of the FMTI v1.1 methodology is that companies were able to disclose new information, meaning information that was not public at the onset of the FMTI v1.1 process. In some cases, this information is directly made public for the first time via the FMTI v1.1 transparency reports. In other cases, this information was incorporated into preexisting or new publicly available documentation from companies. For example, as we noted previously, AI21 Labs released the first model card for Jurassic-2 and Stability AI significantly updated the model card for Stable Video Diffusion.\nNew information constitutes a large fraction of the score for several companies. Figure 6 breaks down each developer's overall FMTI v1.1 scored based on which indicators were awarded for new information vs. information that was previously publicly available. For three developers (AI21 Labs, Aleph Alpha, Writer), new information constitutes roughly half the points awarded. In the case of Aleph Alpha, several new disclosures are made about data labor: all laborers are employed by Aleph Alpha, work in Germany, and are afforded labor protections as stipulated by German law. For Writer, new information is provided on compute: models are trained on 1024 NVIDIA A100 80GB GPUs for 74 days (910k GPU hours) on the Writer cluster, amounting to 8.2 \u00d7 10\u00b2\u00b3 FLOPs in compute, 812 MWh in energy, and 207tCO2eq in emissions."}, {"title": "5 Discussion", "content": "Having characterized how transparency has changed from October 2023 to April 2024, we discuss how we reason about these changes (\u00a75.1), what we recommend going forward (\u00a75.2), and ways in which the Foundation Model Transparency Index could evolve in subsequent iterations (\u00a75.3)."}, {"title": "5.1 Interpretation of findings", "content": "There is significant room for improvement in the transparency of foundation model developers. Developers on average score just 58 out of 100, with major gaps in multiple subdomains for most developers. Developers' transparency reports are incomplete, lacking disclosures on many important matters.\nNevertheless, our findings provide significant reasons for optimism about the prospects for improved transparency. Foundation model developers shared a significant amount of new information about how they build, evaluate, and deploy their models via FMTI v1.1. Some of the areas of the index that were least transparent in v1.0 show significant improvement in v1.1, including subdomains such as compute, methods, risks, and usage policy. Several companies have become much more transparent, with some releasing model cards or other documentation for their flagship foundation models for the first time. By engaging directly with foundation model developers, we have shown that many firms are willing to disclose more information about some of their most powerful technologies.\nStill, the overall state of transparency in the foundation model ecosystem remains poor. Developers are opaque about the data, labor, and compute used to build their models, they often do not release reproducible evaluations of risks or mitigations, and they do not share information about the impact their models are having on users or market sectors. Transparency in the foundation model ecosystem has been declining for several years, and this trend is unlikely to reverse in the near term.\nWhere developers do disclose information, they sometimes disclose information for only the least onerous indicators within a subdomain. For example, in the risks subdomain, most developers describe risks (12 of 14 developers) and many demonstrate risks (8 of 14). However, fewer developers evaluate unintentional harm (5 of 14) or intentional harm (4 of 14). We see the same trends for model mitigations and data labor, where developers disclose information regarding less intensive indicators but no others.\nThere are a variety of different reasons why a developer might not disclose information related to a specific indicator. Developers could face legal exposure if they disclose a substantial amount of information related to some indicators, as is the case with data. Some developers argue that disclosure of certain information could amount to ceding a developer's edge to its competitors. The process of releasing information also presents a potential coordination problem for large developers that need to consult with lawyers, engineers, product managers, and executives before doing so. There were many instances in which it appeared that a developer did not fully understand an indicator and so did not disclose the information needed to satisfy that indicator. In other cases, developers noted that they do not have access to the information in question as it is collected only by deployers or end users. Our results show that these and other factors combine to limit the overall transparency of foundation model developers."}, {"title": "5.2 Recommendations", "content": "We present recommendations aimed at different stakeholders based on the findings of FMTI v1.1 as well as changes in the world over the past six months, building on a more extensive set of recommendations made in Bommasani et al. (2023a, \u00a78)."}, {"title": "5.2.1 Foundation model developers", "content": "As part of FMTI v1.1, we publish transparency reports that consolidate information disclosures from developers and that we release subject to their validation. In light of voluntary codes of conduct promulgated by the White House and the G7 that include commitments for foundation model"}, {"title": "5.2.2 Customers of foundation models", "content": "Purchasers (e.g. downstream developers, enterprise users of chatbots, or government entities) can exert negotiating power in procurement to increase transparency. Notably, some foundation model developers stated that their participation was driven by requests from customers to understand the transparency of their products. Others mentioned that their practices related to transparency with their customers are much better compared to the data they can share publicly\u2014an example of the influence customers can have on business practices. The Foundation Model Transparency Index provides a structured way for customers to advance transparency from foundation model developers\u2014both in terms of the information developers share with their clients and with the broader public.\nIn addition, governments can play a dual role as customers of foundation models (Quay-de la Vallee et al., 2024). As influential (and lucrative) procurers of technology, this can help them play a standard-setting role. For example, the U.S. government is one of the largest purchasers of various goods and services, which allows it to set the standards for how these goods and services are sold, shaping business practices across industries (Vinsel, 2019), including around transparency. While requirements on transparency by governments may lead to legal concerns around government overreach, such as concerns in the US related to the first amendment implications of compelled speech (Bankston and Hodges, 2024), standard-setting via procurement circumvents these concerns by relying solely on the voluntary commitment to these standards by model developers who want to enter a contract."}, {"title": "5.2.3 Transparency advocates", "content": "The transparency reports we release can enable transparency advocates in academic and civil society organizations to better understand developer practices. While for the purposes of the Index scores we set a threshold for constitutes sufficient disclosure to award a point, the underlying disclosures are considerably richer. We encourage researchers and journalists to investigate this information, which includes considerable variation across companies that we do not explore in this paper."}, {"title": "5.2.4 Policymakers", "content": "Policymakers can use our results to identify areas of pervasive opacity\u2014including areas with sustained opacity (across FMTI v1.0 and v1.1) as well as areas with systematic opacity (across the developers we score). This can also highlight perverse business incentives that might lead to such a lack of transparency, and in turn, can inform regulation that addresses them. For example, sharing information about the data used to train foundation models might open up companies to liability concerns, such as due to the legal uncertainty around copyright violations. Regulation intended to address such perverse incentives, such as mandatory disclosure of training data, could help address these impediments to transparency.\nVarious policy efforts in the last two years have focused on addressing transparency in the founda- tion model ecosystem, including Canada's Code of Conduct on the Responsible Development and Management of Advanced Generative AI Systems, the EU AI Act, and the US Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. Still, Bommasani et al. (2024) find that these efforts can lack specificity. Our iterative process with model developers provides an insight into how such specificity might arise in practice-government bodies might"}, {"title": "5.3 Next steps", "content": "We plan to conduct future versions of the Foundation Model Transparency Index on a regular basis. As stated by Bommasani et al. (2023c), as part of future iterations we will make changes to the Foundation Model Transparency Index to reflect changes in the foundation model ecosystem and the organizations that build and deploy foundation models.\nFor example, moving forward, the Foundation Model Transparency Index may change the indicators or their thresholds. For the purposes of clear comparisons, the indicators and scoring thresholds are the same across FMTI v1.0 and v1.1. Due to ambiguity (as evidenced by developers misinterpreting indicators), one possible change could include splitting existing indicators into multiple different ones in order to more clearly delineate the information required in each indicator. Due to saturation (as evidenced by most developers satisfying many indicators), possible changes could include increasing the scoring thresholds for existing indicators as well as introducing new indicators about information that is not currently assessed by the Index. And due to changes external to FMTI in the foundation model ecosystem, such as growing interest in the encoding of human values (Scherrer et al., 2024) and the implementation of reward models for aligning foundation models (Lambert et al., 2024), possible changes could include adding indicators that specifically reflect transparency in the values encoded in models as well as information about how reward models are trained and operationalized. Most fundamentally, while the initial 100 indicators were decided upon by Bommasani et al. (2023a) with guidance from others in the community, a more open-ended process for community-driven indicator proposals may be implemented."}, {"title": "6 Limitations", "content": "In general, transparency as a construct and indices as an approach have well-known limitations (Birchall, 2014; Valdovinos, 2018; Alloa and Thom\u00e4, 2018; Alloa, 2018; Sagar and Najam, 1998; Boldin, 1999; Santeramo, 2017; Greco et al., 2019; Schlossarek et al., 2019). There are also a number of well-known limitations of transparency in AI specifically (Ananny and Crawford, 2018a; Phang et al., 2022; Hartzog, 2023). These limitations are discussed at length by Bommasani et al. (2023a) and they apply equally to FMTI v1.1.\nIn \u00a72.2, Bommasani et al. (2023a) write: \u201cTransparency is far from sufficient on its own and it may not always bring about the desired change (Corbett and Denton, 2023). Salient critiques of transparency include:\n\u2022 Transparency does not equate to responsibility. Without broad based grassroots movements to exert public pressure or concerted government scrutiny, organizations often do not change bad practices (Boyd, 2016; Ananny and Crawford, 2018b).\n\u2022 Transparency-washing provides the illusion of progress. Some organizations may mis- appropriate transparency as a means for subverting further scrutiny. For instance, major technology companies that vocally support transparency have been accused of transparency- washing, whereby \"a focus on transparency acts as an obfuscation and redirection from more substantive and fundamental questions about the concentration of power, substantial policies and actions of technology behemoths\" (Zalnieriute, 2021).\n\u2022 Transparency can be gamified. Digital platforms have been accused of performative trans- parency, offering less insightful information in the place of useful and actionable visibility (Ghosh and Faxon, 2023; Mittelstadt, 2019). As with other metrics, improving transparency can be turned into a game, the object of which is not necessarily to share valuable informa- tion.\u00b9\u2075\n\u2022 Transparency can inhibit privacy and promote surveillance. Transparency is not an apolitical concept and is often instrumentalized to increase surveillance and diminish privacy (Han, 2015; Mohamed et al., 2020; Birchall, 2021). For foundation models, this critique under-"}]}