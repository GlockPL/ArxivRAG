{"title": "Prompting Video-Language Foundation Models with Domain-specific Fine-grained Heuristics for Video Question Answering", "authors": ["Ting Yu", "Kunhao Fu", "Shuhui Wang", "Qingming Huang", "Jun Yu"], "abstract": "Video Question Answering (VideoQA) represents a crucial intersection between video understanding and language processing, requiring both discriminative unimodal comprehension and sophisticated cross-modal interaction for accurate inference. Despite advancements in multi-modal pre-trained models and video-language foundation models, these systems often struggle with domain-specific VideoQA due to their generalized pre-training objectives. Addressing this gap necessitates bridging the divide between broad cross-modal knowledge and the specific inference demands of VideoQA tasks. To this end, we introduce HeurVidQA, a framework that leverages domain-specific entity-action heuristics to refine pre-trained video-language foundation models. Our approach treats these models as implicit knowledge engines, employing domain-specific entity-action prompters to direct the model's focus toward precise cues that enhance reasoning. By delivering fine-grained heuristics, we improve the model's ability to identify and interpret key entities and actions, thereby enhancing its reasoning capabilities. Extensive evaluations across multiple VideoQA datasets demonstrate that our method significantly outperforms existing models, underscoring the importance of integrating domain-specific knowledge into video-language models for more accurate and context-aware VideoQA.", "sections": [{"title": "I. INTRODUCTION", "content": "THE evolution of large-scale text pre-training has significantly advanced the capabilities of Large Language Models (LLMs) [1], [2], [3] such as GPT-3 [4] and BERT [5], empowering them with an unprecedented ability to comprehend and process complex linguistic structures. These breakthroughs have paved the way for sophisticated natural language processing applications [6], [7]. Building on the success of LLMs, the development of Large Multi-modality Pre-trained Models (LMMs) [8], [9], [10], [11], exemplified by models like CLIP [12] and DALL-E [13], represents a significant leap forward by integrating textual and visual data during pre-training. This fusion has driven remarkable progress in fields requiring deep cross-modal understanding, including robotics [14], medical diagnostics [15], [16], and interactive gaming environments [17].\nVideo Question Answering (VideoQA) [18], [19], [20], [21] stands out as a particularly challenging yet rapidly growing field within vision-language bridging research [22], [23]. It demands both discriminative unimodal understanding [5], [24], [25] and comprehensive cross-modal interaction [12], [22], [26], [27], integrating video understanding with language processing to infer reliable answers. While traditional approaches have explored enhanced video-linguistic models [28], [29] and adaptive cross-modal interactions [30], the advent of Video Language Foundation Models (VFMs) [22], [31], [32], such as ALPRO [33] and SiaSamRea [34], has brought about a paradigm shift. These models, pre-trained on extensive video-text datasets [35], [36], have been pivotal in equipping machines with broad cross-modal knowledge, setting new standards for video content interpretation and advancing the boundaries of VideoQA [18], [19], [20].\nDespite the advanced capabilities of VFMs, their generalized pre-training often underperforms in specialized VideoQA domains. This gap underscores the need for precise, context-aware adaptations to fully harness these models. Integrating prompt engineering with VFMs offers a promising solution by providing tailored prompts that steer the models toward domain-specific challenges, ensuring more accurate outcomes. However, the success of prompt-based approaches heavily relies on the design and quality of the prompts themselves. This is particularly true in VideoQA, where understanding the interplay between entities and their actions within videos is paramount. Generic prompts may not suffice for tasks that demand deep, context-specific insights, underscoring the need for a more refined strategy.\nIn this study, we introduce HeurVidQA, a novel framework designed to enhance video-language foundation models with domain-specific entity-action heuristics to address the complex demands of VideoQA. These complex demands arise from the need to process diverse and intricate video content, requiring not only an understanding of temporal sequences and spatial relationships but also the ability to handle varied question types that demand nuanced, context-aware reasoning. Our approach transforms these large-scale models into dynamic knowledge engines capable of navigating such intricate content with high precision and adaptability. We employ a strategy that integrates domain-specific, fine-grained heuristics into the prompt design, refining focus on essential cross-modal elements. As depicted in Figure 1, we utilize instantiated prompt templates and specific regions within video frames to enable the prompter to accurately identify dynamic actions that vary spatially and evolve entities over time. This strategic use of heuristic-based prompts, informed by a deep understanding of key entities and their interactions, sharpens the model's focus and enhances analytical depth. Subsequently, leveraging these finely designed, context-aware entity-action heuristics, we prompt VFMs to generate coherent and contextually aligned responses.HeurVidQA underscores the criticality of recognizing key entities and actions for effective video content analysis. By ensuring the model's focus aligns with the question intricacies specific to a video, we enable a more contextually informed and impactful analysis, yielding answers that are accurate and deeply contextualized.\nIn summary, our contributions are threefold:\n\u2022 We introduce HeurVidQA, a framework that enhances video-language foundation models for VideoQA by integrating general implicit priors with domain-specific, fine-grained knowledge, thereby improving cross-modal understanding and reasoning capabilities.\n\u2022 We develop a domain-specific prompting mechanism utilizing fine-grained entity-action heuristics to guide the model in accurately identifying and interpreting dynamic actions and evolving entities within videos.\n\u2022 We validate the effectiveness of our approach through comprehensive evaluations on four VideoQA datasets across different domains, demonstrating robust and generalizable performance compared to existing methods."}, {"title": "II. RELATED WORK", "content": "A. Video Question Answering\nVideo Question Answering (VideoQA) has emerged as a critical area in cross-modal research, driven by significant advancements in vision-language integration. Early efforts in this domain focused on adapting ImageQA models to video contexts, primarily leveraging LSTM-based encoders [37], [38]. However, these approaches were limited by their reliance on unimodal understanding, often failing to capture the inherent temporal dynamics of video content. This shortcoming underscores the necessity for advancing research towards cross-modal understanding approaches. To address this, attention mechanisms [39], [40] have been investigated to extract linguistically guided visual features. Further advancements include the development of hierarchical attention mechanisms [38], [41], co-memory networks [42], and heterogeneous memory-enhanced models [43], which have proven effective in modeling the complex interplay between video and question features. The advent of the Transformer model [44] brought a revolutionary shift in natural language processing (NLP) and found application in VideoQA by Li et al. [39]. This advancement significantly enhanced unimodal understanding within VideoQA. However, the visual modality's limitations necessitated the development of novel cross-modal frameworks. Leveraging self-attention and co-attention mechanisms, Transformers facilitated deeper cross-modal interaction, enabling the extraction of pertinent visual features. Additionally, structured representations, including heterogeneous graph alignment networks [45], structure-aware interaction models [46], and Dynamic Graph Transformers [18], further advanced reasoning capabilities in VideoQA. Recent studies in VideoQA have also tackled challenges related to confounders and compositional reasoning. Grounding indicators [47] have been developed to mitigate spurious correlations, while question de-composition engines [48] offer valuable insights into compositional reasoning. To address confounding factors, multimodal causal reasoning frameworks [19] have been introduced, along with adaptive spatial-temporal attention mechanisms [20]. As research into multi-granularity advances, emerging approaches increasingly explore the impact of varying granularities on VideoQA performance [49], [50], [51], [52], focusing on multi-level intra- and inter-granularity relations to enhance cross-modal comprehension. Another approach [47], [53] to solving VideoQA tasks focuses on identifying video frames relevant to the question, drawing inspiration from similar text-to-video retrieval (TVR) tasks. For example, Wu et al. [54] explored both text-free and text-guided frame selection strategies in TVR, determining an optimal strategy that balances accuracy and computational efficiency. In the context of multi-modal pre-training on large-scale vision-text data [55], [22], [9], [33], [26], [56], [12], Transformer-based models have demonstrated remarkable advancements. While prompt learning and engineering [12] have enhanced pre-training, the customization of prompts specifically tailored to entities and actions in videos remains underexplored. This paper addresses this gap by constructing task-specific prompts finely tuned to entities and actions, thereby enabling superior spatiotemporal"}, {"title": "B. Vision-and-Language Pre-training", "content": "The paradigm of pre-training on large-scale visual-language data followed by fine-tuning for specific downstream tasks has demonstrated significant success in cross-modal applications, including VideoQA. Vision-and-Language Pre-trained (VLP) models [12], [57], such as CLIP [12] and ALIGN [57], commonly use standardized pre-training objectives like masked language modeling (MLM) and image-text matching (ITM) on extensive vision-text datasets. However, earlier VLP models, such as OSCAR [56], depended heavily on pre-trained object detectors for visual feature extraction. This reliance led to increased computational overhead and introduced noise into the image-text data, which ultimately impacted downstream performance. Despite their strong performance across various tasks, these VLP models face two significant limitations: first, they often struggle to effectively model complex vision-text interactions, and second, they tend to overfit to the noisy data prevalent in large-scale image-text datasets, which ultimately hampers their generalization performance. To overcome these limitations, various strategies have been proposed. For instance, Li et al. [26] introduced an intermediate image-text contrastive loss to enhance cross-modal alignment and semantic understanding while mitigating the effects of noisy data using Momentum Distillation. Similarly, BLIP [22] advanced the quality of vision-text pairs through the Captioning and Filtering (CapFilt) approach, which fine-tunes a captioner using language modeling and a filter with cross-modal tasks, thereby improving the model's handling of vision-text relationships. In the domain of video processing, VideoBERT [58] extended the BERT model to video analysis but neglected the crucial roles of textual cues and cross-modal interactions. Meanwhile, ActBERT [59] depended on a pre-trained object detector to generate object pseudo-labels, but its limited detection categories and high computational costs resulted in suboptimal performance. Moving beyond one-directional transfer models, unified transformer-based architectures, as proposed by Wang et al. [31], have emerged to handle multimodal input sources, enabling joint pre-training for image-language and video-language tasks, thus benefiting both image and video-related assignments. More recently, with the advancement of prompt learning, VLP models have benefited from the integration of carefully designed prompts, which have significantly enhanced their performance in downstream tasks. This approach allows for greater control and precision, enabling models to produce task-specific outcomes more effectively. The success of prompt learning lies in its ability to address challenges such as limited data and complex annotations, positioning it as a promising direction for advancing cross-modal research. By incorporating these techniques, researchers have achieved impressive results, further narrowing the gap between vision and language understanding. Our proposed HeurVidQA framework marks a significant departure by incorporating novel visual heuristics that enhance the VLP model with fine-grained perceptual capabilities. Leveraging the robust cross-modal learning strengths of these models, our approach not only improves their ability to capture intricate visual nuances but also bridges the gap between pre-training and downstream task execution. This integration of new visual cues within HeurVidQA represents a pivotal advancement in refining and advancing state-of-the-art visual language processing."}, {"title": "C. Prompt Learning", "content": "Prompt learning [4], [60], [61] has emerged as a powerful technique in the cross-modal field, allowing models to achieve task-specific results by incorporating targeted prompts during training. This approach enhances control and accuracy, providing a promising solution to challenges posed by limited data and complex annotations. At its core, prompt learning can be likened to a \"fill-in-the-blank\" task for pre-training models. By masking certain words in sentences and prompting the model to predict the missing tokens, the model learns to comprehend contextual relationships between words in a sentence. CLIP [12], in particular, showcases the efficacy of prompt-based learning for image recognition. Through contrastive learning on a large-scale noisy dataset, it aligns relevant image-text pairs and distinguishes unrelated ones, effectively achieving image-text alignment. For image recognition, it incorporates the label into a descriptive sentence, enabling the model to predict specific words (e.g., \"An image about a [MASK]\u201d). Remarkably, CLIP achieves impressive accuracy in image recognition tasks even without fine-tuning, thanks to the flexibility of prompt design, making it applicable to various image-text tasks. Building on the success of CLIP, ALIGN [57] further advances vision-language models by training on a massive dataset comprising 1.8 billion image-text pairs. ALIGN surpasses CLIP's predictive performance, achieving superior results. Meanwhile, CoOp [62] introduces a prompt fine-tuning method, adapting NLP techniques to the cross-modal domain. By employing learnable tokens as prompts and minimizing classification losses, CoOp effectively learns Soft Prompts with minimal annotated data, overcoming the limitations of manually tuned prompts. This innovation yields significant performance improvements and greater flexibility. However, CoOp's learned context struggles to generalize to unseen classes, a challenge addressed by CoCoOp [63]. CoCoOp generates input-conditional vectors via a lightweight network, dynamically adjusting prompts to improve generalization across varying classes. This method enhances performance on unseen classes, solidifying CoCoOp's robustness. Additionally, Ma et al. [64] tackles the overfitting issue in CoOp by projecting gradients onto a low-rank subspace during back-propagation. Exploring prompt techniques further, researchers have focused on enhancing tasks through verb"}, {"title": "III. METHOD", "content": "The overall architecture of our HeurVidQA is illustrated in Figure 2. In our approach, we seek to accurately predict the answer y to a given question Q, posed in natural language, based on the content of a raw video V. The following equation formalizes this objective:\n$\\hat{y} = \\text{argmax}_{y \\in A} F_w(y|V, Q, A),$ (1)\nwhere $F_w$ represents the VideoQA model parameterized by weights W. The function's output, $\\hat{y}$, varies based on the VideoQA task configuration, offering an open-ended response from a global answer set or a selected choice from multiple alternatives. For multiple-choice QA, we establish a linkage between the question and candidate answers with a [PAD] token, supplemented by the inclusion of a specialized [CLS] token positioned at the inception of the textual sequence:\n$T^{MC}(Q, a_i | a_i \\in A_{MC}) = [CLS] Q [PAD] a_i,$ (2)\nwhere TMC is the conversion function that converts multiple-choise question Q and its candidate answers $a_i$ to the input format. For open-ended QA, where no predefined candidate answers exist, the [CLS] token is directly associated with the commencement of the question. The formulation of the conversion function is as follows:\n$T^{OE}(Q) = [CLS] Q.$ (3)\nOur model is designed to be adaptable across these varied formats, enabling comprehensive applicability within the field of VideoQA.\nIn the following, we outline the workflow of the proposed HeurVidQA framework, organized into three key phases. The Data Engineering phase establishes the foundation by optimizing visual inputs for the EAPrompter through strategic clipping and crafting video entity-action templates. Next, the Entity-Action Heuristics Generation subsection introduces the EAPrompter's architecture and training, focusing on synthesizing heuristic insights from the processed data. Finally, the Heuristic Boosting Answer Inference phase utilizes these heuristics to enable synergistic interaction between VFMs and the knowledge engine, enhancing answer inference performance.\nA. Data Engineering\nOur HeurVidQA framework begins by preparing videos and prompts for analysis. We segment the videos temporally and spatially, targeting key moments and areas likely to hold relevant information. Simultaneously, we generate domain-specific prompts using action-related verbs and object-focused nouns anticipated in the video content. These tailored prompts guide the foundation models in identifying critical elements. This phase is crucial for data preparation, ensuring the subsequent heuristic generation stages are well-informed and effective. To enhance both temporal and spatial sensitivity in video cropping, we employ distinct strategies tailored to each dimension. Temporal sensitivity is addressed by consistently"}, {"title": "B. Entity-Action Heuristics Generation", "content": "After data engineering, our HeurVidQA framework progresses to the crucial stage of generating domain-specific fine-grained heuristics. This phase employs a two-pronged approach: (1) Action Heuristics Generation focuses on capturing verbs and their temporal dynamics within video content, enabling the model to comprehend ongoing activities; (2) Entity Heuristics Generation targets the identification and tracking of nouns and their spatial attributes, ensuring the accurate reference of key subjects within the visual space. This dual approach ensures that both temporal and spatial aspects are effectively captured, enhancing the model's capacity for precise video question answering.\n1) EntityActionPrompter: The core of our heuristic generation lies within the EntityActionPrompter (EAPrompter), which operates through two sub-modules: the action prompter (AP) and the entity prompter (EP), each tailored to capture distinct yet complementary aspects of the video content's entity-action landscape. The AP enhances temporal precision by randomly cropping identical spatial regions across sparsely selected video frames, while the EP augments spatial acuity by extracting varied regions across different frames. Both modules share an identical architecture, adopting standard clip-based visual-language model structures with a video branch and a text branch. For the video branch, both AP and EP employ a 12-layer TimeSformer224 [24] to capture video embeddings. It takes as input a clip $V \\in R^{C \\times F \\times H \\times W}$, which undergoes random sampling, scaling, cropping, and random masking to enhance computational efficiency and consistent performance.\nTo facilitate the formula expression, the unimodal visual embeddings derived from AP are denoted {vas, ,\u2026\u2026\u2026, }, where vas and correspond to video global temporal embedding and temporal cropping numbers respectively. Visual embeddings from EP are denoted as {vels,vi,\u2026\u2026\u2026, }, where was and are video global spatial embedding and spatial cropping numbers. For the text branch, we employ a multi-layer bidirectional transformer [5] to encode text semantics in a hierarchical collaborative parsing pattern. This procedure produces corresponding prompts embeddings, {tas, tais.....} and {testis,...} for action and entity prompts, respectively. For cross-modality alignment, a similarity function $s(\\cdot)$ is optimized between the video and textual embeddings as follows:\n$s(v_{cls}, t_{cls}) = f_v(v_{cls}) \\cdot f_t(t_{cls}),$ (4)\nwhere a higher similarity score is obtained for matching videos and prompts.\nBoth AP and EP are pre-trained on webly sourced video-text pairs with video-text contrastive (VTC) loss. As depicted in Figure 3, the symmetric contrastive loss plays a key role in enhancing cross-modal alignment. By pulling positive pairs closer (on the diagonal) and pushing negative pairs further apart (off-diagonal), this mechanism drives the encoder representations to converge in a low-dimensional space, thereby optimizing inter-modal alignment. The symmetric contrastive loss for the vision-to-text modality is calculated as follows:\n$L_{v2t} = \\sum_{i=1}^B \\log \\frac{\\exp(s(v_{cls}^i, t_{cls}^i) / \\tau)}{\\sum_{j=1}^B \\exp(s(v_{cls}^i, t_{cls}^j) / \\tau)},$ (5)\nThe symmetric contrastive loss for the text-to-vison contrastive loss is computed as:\n$L_{t2v} = \\sum_{i=1}^B \\log \\frac{\\exp(s(t_{cls}^i, v_{cls}^i) / \\tau)}{\\sum_{j=1}^B \\exp(s(t_{cls}^i, v_{cls}^j) / \\tau)},$ (6)\nwhere B represents the batch size and $\\tau$ signifies a learnable temperature parameter, the prompter, after being pre-trained on extensive datasets, exhibits strong alignment capabilities for video and textual data. To prevent external noise from affecting its performance, we freeze the prompter's parameters during the entity-action heuristic generation stage. Through this approach, the EAPrompter effectively constructs a detailed entity-action map of the video content, providing a heuristic-driven analysis that greatly enhances the accuracy and relevance of the subsequent answer inference stage.\n2) Heuristic Generation: In the Heuristic Generation subsection, we introduce a key innovation in our approach: the utilization of latent heuristic information embedded within"}, {"title": "C. Heuristic Boosting Answer Inference", "content": "Heuristic boosting answer inference stage leverages heuristic labels from the previous phase to strengthen interactions between VFMs and the heuristic knowledge engine, refining the model's ability to infer answers accurately. HeurVidQA utilizes VFMs to capture contextual video-language embeddings, followed by a QA reasoner to conduct question-answering interaction and communication. The QA reasoner stacks cross-modality alignment layers to fuse visual-textual embeddings to explore in-depth, informative clues. Specifically, it comprises several self-attention blocks, cross-attention layers, and feed-forward networks to deliver cross-modal fusions {ects,1,...,\u2208N\u300f+N+}, where els serves as a pivotal link between the EAPrompter and the QA Reasoner. Instead of directly utilizing the fused embedding ecls to predict the answer, HeurVidQA skillfully introduces the generated domain-specific entity-action heuristics as supervisory targets with Temporal Action Modeling (TAM) and Spatial Entity Modeling (SEM) losses. TAM Loss enhances the model's ability to understand temporal action sequences, while SEM Loss improves the recognition of spatial relationships among entities. These loss functions are pivotal in transferring the latent knowledge from the prompters to the baseline model, guiding the VFMs toward more precise reasoning in VideoQA tasks. Through two distinct classifiers $c_1(.)$ and $c_2(.)$, we initially obtained the normalized confidence scores of action and entity heuristics:\n$p_{a,am} = c_1(e_{cls}), \\quad p_{e,en} = c_2(e_{cls}),$ (9)\nwhere $p_{a,am}$ and $p_{e,en}$ denote the classifier-derived probability distributions for action and entity heuristics, respectively. The subscripts am and en correspond to the $m^{th}$ action and $n^{th}$ entity within the respective distributions.\nThe TAM loss is calculated as the cross-entropy between the classifier-derived action heuristic distribution $p_a$ and the target distribution $h\\hat{u}_{cls,am}$:\n$L_{TAM} = - \\sum_{m=1}^M h\\hat{u}_{cls,am} \\log p_{a,am}.$ (10)\nSimilarly, the SEM loss is calculated as the cross-entropy between the classifier-derived entity heuristic distribution $p_e$ and the target distribution $h\\hat{u}_{cls,en}$:\n$L_{SEM} = - \\sum_{n=1}^N h\\hat{u}_{cls,en} \\log p_{e,en}.$ (11)\nGuided by the supervision of entity-action heuristics, the fusion embedding retrieves the unimodal information that might have been lost during cross-modal interaction, enabling HeurVidQA to achieve fine-grained spatiotemporal reasoning. To predict the final answer $\\hat{y}$, we use an independent classifier, which takes the heuristic-enhanced fusion representation as input, formulated as follows:\n$\\hat{y} = C_3(e_{cls}).$ (12)\nComparing the predictive answers $\\hat{y}$ with the ground truth y, we calculate the prediction loss as follows:\n$L_{pred} = - \\sum y \\log \\hat{y}.$ (13)"}, {"title": "IV. EXPERIMENTS", "content": "We have conducted an extensive analysis to evaluate the performance of HeurVidQA across several established VideoQA benchmark datasets, including NExT-QA [67], MSVD-QA [38], MSRVTT-QA [38], and SUTD-TrafficQA [68].\nA. Datasets\n\u2022 NEXT-QA [67] presents complex challenges requiring advanced causal and temporal reasoning. It comprises 5,440 videos averaging 44 seconds in length, with approximately 52K annotated QA pairs categorized into causal, temporal, and descriptive questions. We also consider the ATPhard split from Buch et al. [69], featuring questions that demand sophisticated video comprehension beyond single-frame analysis.\n\u2022 MSVD-QA [38] is derived from the MSVD dataset and includes 10s video clips across 1,970 videos. It features around 51K QA pairs automatically generated based on video captions, covering diverse topics like video content, object recognition, and scene understanding.\n\u2022 MSRVTT-QA [38] is similar to MSVD-QA but on a larger scale, with 10K videos averaging 15 seconds each and containing 244K QA pairs. These are generated from video descriptions and serve to train and evaluate models in open-ended question answering.\n\u2022 SUTD-TrafficQA [68] serves as a critical resource for advancing research in traffic-related question answering, requiring a deep understanding of traffic events and their causal relationships. It comprises over 10,000 videos depicting diverse traffic incidents, supplemented by 62,535 human-annotated QA pairs\u201456,460 for training and 6,075 for testing. This comprehensive dataset rigorously evaluates the cognitive abilities of models in understanding complex traffic scenarios. All tasks follow a multiple-choice format without constraints on the number of possible answers.\nB. Implementation Details\nOur method builds on ALPRO [33] as the video-language foundation model, fine-tuned using 4 NVIDIA GeForce 3090 GPUs. We used weights pre-trained on WebVid-2M and CC-3M datasets, alongside prompter weights pre-trained on noisy video-text pairs. The AdamW optimizer was employed, with a weight decay of 0.001 and a unified learning rate of 5e-5 across datasets. A linear decay schedule was used to adjust the learning rate dynamically, promoting rapid convergence initially and precise convergence during later training stages. For raw video processing, videos were rescaled to 224 \u00d7 224, and a random sparse sampling strategy was applied to extract 16 frames while preserving their sequential order. The EAPrompter followed a distinct processing strategy, resizing videos to 256 \u00d7 256 before cropping a 224 \u00d7 224 area. After random sampling, 50-70% of the original spatial area was randomly masked to capture specific actions and entities effectively. For the Entity-Action Heuristic Generation, spaCy was used to extract the top 1,000 frequent verbs and nouns as action and entity candidates. Heuristics were discarded if the highest-scoring entity had a normalized similarity score below 0.1. The training was conducted over 10 epochs for NEXT-QA and SUTD-TrafficQA, and 15 epochs for MSVD-QA and MSRVTT-QA datasets.\nC. Experimental Results\nWe evaluated our method against several state-of-the-art (SoTA) models on four VideoQA datasets: NExT-QA, SUTD-TrafficQA, MSVD-QA, and MSRVTT-QA. Our comparisons included non-pretrained models like VGT [18] and CoVGT [73], as well as pre-trained VL models, such as MIST [20] and our baseline, ALPRO [33].\n1) Comparison on NExT-QA: Table I presents the comparative results on the NExT-QA dataset, encompassing both non-pretrained and pretrained VL models. Notably, we emphasize the baseline method, ALPRO, to demonstrate the improvements achieved through heuristic prompts. The results indicate that pretrained visual-language (VL) models generally surpass non-pretrained models, as evidenced by the performance of VGT and CoVGT compared to their pretrained counterparts, VGT (PT) [18] and CoVGT (PT) [73]. Our method consistently achieves optimal or near-optimal results across overall performance metrics (Acc@All) and specific question types (Causal, Temporal, and Descriptive)."}, {"title": "The complete training objective of HeurVidQA can be obtained by combining the prediction loss with the heuristic losses as follows:", "content": "$L = L_{pred} + \\alpha L_{TAM} + (1 - \\alpha) L_{SEM}.$ (14)\nWhere $\\alpha$ represents a hyperparameter set to 0.5 in our experiments, it serves to balance the weighting between action and entity prompts. To explore other configurations, we introduced a dynamic gating mechanism that adjusts the influence of action and entity prompts based on the input data and training stage. This question-guided gating mechanism involves the following steps: processing the text input via VFMs to extract the global tcls embedding and passing this embedding through a gating network composed of two multi-layer perceptrons, a dropout function, and a sigmoid activation function.\n$g(t_{cls}) = \\text{Sigmoid}(MLP2(\\text{Dropout}(MLP1(t_{cls}))))$. (15)\nThrough the gating mechanism, we obtain a scalar value ranging from 0 to 1, representing the gate weight for the action prompt. Next, the gate weight for the entity prompt is calculated by subtracting the action prompt's weight from 1. Finally, these gate weights are applied to the TAM loss and SEM loss. The combined loss function is then formulated as follows:\n$L = L_{pred} + g L_{TAM} + (1 - g) L_{SEM}.$ (16)\nThis adaptive gating scheme has demonstrated its effectiveness in enhancing both the model's adaptability and stability by dynamically adjusting the influence of action and entity prompts."}, {"title": "V. CONCLUSIONS AND FUTURE DIRECTIONS", "content": "This paper presents the HeurVidQA model, which leverages principles of prompt learning and engineering to enhance the capabilities of pre-trained visual language models in VideoQA tasks. By integrating heuristic prompts that target both action information across temporal frames and entity information across spatial regions, we introduce two novel loss functions: Prompt Action Modeling and Prompt Entity Modeling. Additionally, we propose a dynamic gating mechanism to maintain a balanced emphasis between action and entity prompts,optimizing the model's reasoning and inference capabilities.\nWhile HeurVidQA demonstrates substantial improvements in VideoQA tasks, it is not without limitations. A key limitation lies in its reliance on domain-specific heuristic prompts, which, while effective, may restrict its generalization across significantly different domains. Additionally, the model's performance in handling highly abstract or counterfactual reasoning scenarios remains limited, as it predominantly leverages explicit action-entity heuristics derived from video content. To address these limitations, future work could explore adaptive prompt learning mechanisms that dynamically adjust to diverse domains without requiring manual heuristic definitions. Incorporating more advanced reasoning techniques, such as causal inference and counterfactual analysis, could further enhance the model's robustness in complex scenarios. Additionally, integrating knowledge graphs or external world knowledge may provide the model with a richer contextual understanding, enabling it to handle a broader range of VideoQA challenges. Finally, reducing the model's dependency on large-scale pre-training data could improve its scalability and applicability in resource-constrained environments."}, {"title": "F. Qualitative Analysis", "content": "Figure 9 showcases the prediction outcomes generated by HeurVidQA on various question categories within the NEXT-QA dataset. Each question is prefixed with a letter indicating its type, C for Causal, T for Temporal, and D for Descriptive. To comprehensively evaluate our approach, we analyze HeurVidQA with the comparative model ALPRO. Three key observations emerged from this analysis: (1) Both HeurVidQA and ALPRO effectively address questions with extended temporal scopes, such as comprehending the lady's behavior in Example 5. (2) HeurVidQA surpasses ALPRO in localizing smaller visual objects and reasoning about challenging questions. For instance, in Example 4, where the video captures a confined spatial and temporal presence of women, HeurVidQA's enhanced performance is attributed to its heuristic prompts, which enable it to detect and identify diverse entities and actions, leading to improved sensitivity in temporal reasoning questions. (3) HeurVidQA outperforms ALPRO on more complex long-term questions. This advantage stems from its augmented language modeling capabilities, facilitated by heuristic prompts, allowing the model to manage multiple actions and physical information, ultimately contributing to superior performance. (4) While HeurVidQA demonstrates excellent performance in addressing descriptive questions, its efficacy is constrained when tackling intricate causal and temporal scenarios. This limitation is primarily attributed to the model's reliance on explicitly derived action entities from video content as primary sources of heuristics. Such an approach, while effective for straightforward descriptive tasks, does not inherently equip HeurVidQA with the nuanced inferential capabilities requisite for dissecting and"}]}