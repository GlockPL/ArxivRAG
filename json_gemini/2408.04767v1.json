{"title": "Data-Driven Pixel Control: Challenges and Prospects", "authors": ["Saurabh Farkya", "Zachary Alan Daniels", "Aswin Raghavan", "Gooitzen van der Wal", "Michael Isnardi", "Michael Piacentino", "David Zhang"], "abstract": "Recent advancements in sensors have led to high resolution and high data throughput at the pixel level. Simultaneously, the adoption of increasingly large (deep) neural networks (NNs) has lead to significant progress in computer vision. Currently, visual intelligence comes at increasingly high computational complexity, energy, and latency. We study a data-driven system that combines dynamic sensing at the pixel level with computer vision analytics at the video level and propose a feedback control loop to minimize data movement between the sensor front-end and computational back-end without compromising detection and tracking precision. Our contributions are threefold: (1) We introduce anticipatory attention and show that it leads to high precision prediction with sparse activation of pixels; (2) Leveraging the feedback control, we show that the dimensionality of learned feature vectors can be significantly reduced with increased sparsity; and (3) We emulate analog design choices (such as varying RGB or Bayer pixel format and analog noise) and study their impact on the key metrics of the data-driven system. Comparative analysis with traditional pixel and deep learning models shows significant performance enhancements. Our system achieves a 10X reduction in bandwidth and a 15-30X improvement in Energy-Delay Product (EDP) when activating only 30% of pixels, with a minor reduction in object detection and tracking precision. Based on analog emulation, our system can achieve a throughput of 205 megapixels/sec (MP/s) with a power consumption of only 110 mW per MP, i.e, a theoretical improvement of ~30X in EDP.", "sections": [{"title": "1 Introduction", "content": "Imaging systems are generating increasingly large amounts of data, driven by advancements in sensor technology and algorithms requiring higher frame rate data collection, high dynamic range data, and continuous uninterrupted surveillance. Edge devices, constrained by size, weight, and power (SWaP), struggle to process high-bandwidth data streams with low latency. We propose leveraging data-driven learning to reduce data bandwidth, energy usage, and latency on edge hardware, enabling effective real-time analysis of complex scenes.\nOur system follows the Dynamic Data Driven Applications Systems (DDDAS) paradigm [11], which tightly integrates an architecture for dynamic sensing with intelligent data"}, {"title": "2 Technical Approach", "content": "We aim to move processing into the sensor pixel to avoid power consumption due to data movement. In prior work [39], we presented an analog design that computes salient patch features before analog-to-digital conversion. Our framework for dynamic scene understanding is versatile, but this paper's focus is Multiple Object Tracking (MOT) [8].\nThe system architecture (Fig. 2) illustrates the process from front-end sensing through back-end prediction to feedback control. The anticipatory sensing mechanism tells the sensor which patches to sense. The in-pixel processor uses patch-level linear projections to extract basic features for the sensed patches. By only processing a subset of patches and compressing this information into feature vectors, the front-end reduces the amount of data sent to the back-end. These features are sent to a Vision Transformer (ViT) [12], which uses self-attention [34] to process the partially sensed image, aggregating and transforming the features for downstream tasks. The ViT features are fed into a Feature Pyramid Network (FPN) [24] and Mask-RCNN [18] to perform object detection. The object detections along with ViT features feed into a DeepSORT model [37] to perform MOT. The object detector features are re-used with a recurrent neural network (RNN) [7] to forecast which patches will contain salient objects in future frames. RNN-based saliency scores, detection uncertainties, and tracking uncertainties are ensembled to prioritize patch sensing for the next frame, promoting efficient identification of new objects and minimizing the uncertainty of existing object tracks. Every $N = 16$ frames, our system fully senses the image to avoid missing new objects."}, {"title": "2.2 Masked Autoencoder and Object Detection", "content": "Mask-RCNN [18] serves as the object detector. To extend the model's feature extractor for processing partial information, we adapt the randomized masking technique from Masked ViT [19], which randomly drops patches from the input image during training and forces the model to reconstruct the missing information. By dropping $p$% of the total image patches $n$ (features), the computational complexity of the ViT's attention mechanism is reduced to $O((p*n)^2)$, $p < 1$. For further improvement, we move from random to targeted salient masking.\nIn conventional detectors, the FPN [24] is commonly used to facilitate detection across different scales. Fang et al. [14] extend ViT and FPN to work with masked models for detection but require expensive fine-tuning of the pre-trained ViT model. Our design avoids retraining by leveraging features extracted from various layers of the ViT encoder, reconstructing missing features, and integrating with Mask-RCNN. Our current approach doesn't fully leverage partial information post-FPN, but future work could utilize models like DeTR [6] to overcome this limitation.\nWe utilize the Masked ViT model as a fixed feature extractor while fine-tuning the overall model using the Mask-RCNN objective function. During training, we mask > 70% of patches within images, extracting features from ~ 30%, mimicking our anticipatory sensing framework for MOT. This method reduces the region-of-interest for the detector, streamlining object detection (see Table 1). The anticipatory mechanism may not always provide a perfect set of patches. To fortify the detector, during training we introduce noise by: 1) randomly excluding patches inside large objects and 2) including a few non-salient background patches."}, {"title": "2.3 Object Tracking", "content": "The MOT model consists of a straightforward implementation of DeepSORT. SORT uses Kalman filtering to predict object trajectories in image space and associate objects via the Hungarian algorithm using bounding box overlap. DeepSORT improves association using visual feature matching (in our system, using ViT encoder features)."}, {"title": "2.4 Anticipatory Sensing", "content": "The tracking model predicts where an object will appear in the next frame. Our anticipatory attention mechanism scores the importance of each object for the next time frame to rapidly identify new objects and reduce uncertainty in existing detections/tracks:\nPredicted saliency: Using global image features, we train an RNN to predict which patches will contain salient objects in the next frame using ground truth segmentations during training. We assign a probability of saliency to each patch and compute an object-level score by averaging over all patches overlapping the object.\nDetection uncertainties: We use one minus the confidence output by the detector as a score, forcing the system to focus on objects with high detection uncertainty.\nTracking uncertainties: The Kalman filter of DeepSORT outputs covariance matrices representing uncertainty in each object's position. We measure the change in the size of the covariance matrix from the previous to the current time step t: $\\max(\\text{det}(\\text{cov}(\\text{obj},t))-\\text{det}(\\text{cov}(\\text{obj}_{t-1})),0)$. The system should focus on objects where by not sensing the object, the uncertainty in the object's position grows."}, {"title": "2.5 Model Compression", "content": "To optimize our model for deployment on edge devices, we employ a two-stage approach. In stage 1, we downsized ViT from 768 dimensions to 240 dimensions by applying knowledge distillation [20]. This process involves using the l2 loss across the entire FPN to ensure a smooth convergence while maintaining the same Mask R-CNN model between the 768D and 240D ViT models. In stage 2, with the FPN reduced to 30% of its original size from stage 1, we further decrease the dimensions of all Mask R-CNN modules by 25%, and fine-tune the model using the Mask R-CNN loss function."}, {"title": "3 Experimental Results", "content": ""}, {"title": "3.1 Importance of Salient Patches", "content": "Our first set of experiments focuses on understanding the importance of salient patch selection. Additional studies can be found in Farkya et al. [15]. In these experiments, we do not utilize the full system; instead, we train an RNN to predict salient patches in future frames and use off-the-shelf task-specific models on partially sensed images. We evaluate object recognition from video clips using the DAVSOD dataset [13], which contains videos annotated with human eye fixations. We consider: 1) random patch selection, 2) oracle-based patch selection (determined by human fixation), and 3) RNN-based patch selection. The evaluation uses per-frame classification accuracy over four classes (human, animal, artifact, and vehicle). The RNN reasonably mimics human fixation (AUROC of 0.78 for anticipatory salient patch prediction) and in Fig. 3 (left), the RNN-based selection (blue line) noticeably outperforms random selection (orange line) and slightly under-performs human attention (gray line).\nWe perform a similar experiment with pedestrian tracking (MOT17 dataset [25]). We combine the RNN-based anticipatory patch selection model with a pre-trained transformer-based tracking model [29]. Foreground object segmentation masks are used as ground truth for training the RNN. The MOTA and MOTP metrics [25] are used for evaluation. We see improvement in MOTA (higher better) and MOTP (lower better) (Fig. 3 (right)) when using learned selection over random selection."}, {"title": "3.2 Validation of Architectural Design Choices for Detection", "content": "We want to understand how intelligent patch selection and model compression affect object detection. We vary input type (RGB or Bayer), the percent of pixels sensed, the compression of the features, and the type and level of noise. Experiments are conducted on a subset of the MS-COCO validation dataset [23] using mean average precision (mAP, higher is better) as the evaluation metric. Because MS-COCO consists of static images, we use ground truth segmentation maps as a proxy for the patch selection. Results appear in Table 1 (left). We observe: 1) Bayer pixels with reduced features hurts detection accuracy but still achieves high performance (> 40 mAP), suggesting a trade-off between efficiency and detection performance; 2) focusing on salient patches aids detection, improving the mAP by 2-4 points; and 3) the system is relatively robust to small amounts of noise, decreasing mAP by only ~3 points.\nWe hypothesize the loss of performance from model compression is due to: 1) information loss because Bayer pixels discard one-third of the data; 2) error due to domain shift from distilling the RGB model to work with Bayer pixels, and 3) a mismatch between the architectural bias of the feature extractor and structure of Bayer pixels."}, {"title": "3.3 Validation of Architectural Design Choices for Multi-Object Tracking", "content": "Extending the previous experiment, we explore how the full system (including the learned patch selection) performs on MOT as input type, percent of pixels sensed, compression of the features, and type and level of noise are varied. Experiments are conducted on a subset of the BDD100K MOTS dataset [38]. This challenging dataset for MOT is captured from the perspective of moving vehicles in crowded environments. We process 32 high-resolution (1280x720) videos at 30FPS. Evaluation metrics include MOTA (higher better), MOTP (lower better), AUROC for anticipatory patch-level saliency prediction (1.0 is perfect), and detection precision and recall. From Table 1 (right), we see little performance loss from sensing partial images vs full images in most metrics. The anticipatory patch selection always achieves over 0.92 AUROC, suggesting the anticipatory sensing mechanism is not the cause of low MOT performance. We expect MOT performance could improve substantially with state-of-the-art detection and tracking models. For reasons discussed in the previous section, using Bayer pixels and shrinking the in-pixel feature size lowers the tracking performance (by ~12 points in terms of MOTA), suggesting there is a trade-off between faster processing and higher performance. I.e., the partially-sensed RGB model achieves performance on par with the full model with 3.3X bandwidth reduction, and if further bandwidth reduction (10X) is needed, then the Bayer model can be used with some loss in performance."}, {"title": "3.4 Analysis of Anticipatory Sensing Mechanism for Multi-Object Tracking", "content": "We perform ablations to understand each component of the anticipatory patch selection mechanism (Table 2), using RGB images without model compression or added noise. Random patch selection appears to be a strong baseline (0.338 MOTA), suggesting that our model is relatively robust to noisy selection of patches. Using any of the three score functions (RNN-based saliency prediction, object detection uncertainty, and change in tracking uncertainty) improves MOTA by 4-5 points and saliency AUROC by up to 40-45 points, suggesting that each score function is useful on its own. The RNN component results in the highest saliency AUROC (0.948) of any individual component. Unfortunately, we find ensembling the score functions with equal weights does not noticeably improve metrics. To improve the anticipatory sensing mechanism, we may want to dynamically learn to weigh the score components on a per-frame basis."}, {"title": "3.5 Understanding Errors in Multi-Object Tracking", "content": "We look at errors the system makes (Table 3). We measure: how frequently an object track ID switches, the number of frames (on average) until each object is detected for the first time, what percent of objects are detected at least once, overall instance-level recall of object detection, the number of frames (on average) until the model attends to the patches containing each object, and what percent of objects are sensed at least once based on the anticipatory attention mechanism. We analyze results by size of the object (small: < 32x32 pixels, medium: 32x32 to 96x96 pixels, large > 96x96 pixels) and whether the object is occluded or truncated. The system performs well on medium and large objects but struggles on small objects. The system is not overly sensitive to occluded or truncated objects. The system does well anticipating object location (attention is typically placed on the object within 1-2 frames). Objects often appear from the horizon or the periphery, so it takes several frames (mean of 4-5 with high std dev for RGB setting) before most objects are large enough to be initially detected. Once an object is detected, association is decent (1-2 track ID switches per object on average for RGB). Compression (Bayer + 240-dim features) leads to longer time-to-detect, more ID switches, and lower detection recall, which explains the loss in tracking performance."}, {"title": "3.6 Energy and Latency Benefits", "content": "Our system (anticipatory sensing + model compression) uses 103 GFLOPs/frame, 19.3M model parameters, 113 W of board power (NVIDIA A6000), and 61 ms/frame processing time. An equivalent system without anticipatory sensing and compression uses 1022 GFLOPs/frame, 118M model parameters, 173 W of board power (A6000), and 180 ms/frame processing time. We compute the Energy Delay Product (EDP), which summarizes energy consumption and execution time as a single value. Looking at the ideal EDP (assumes no loss of power to data movement) and measured EDP (uses measured board power), we find energy-latency savings of 15-30X (Table 4)."}, {"title": "4 Conclusions", "content": "We presented a system that integrates 1) an architecture for dynamic data-driven sensing at the pixel-level with 2) intelligent processing for computer vision in a feedback control loop. This study highlighted some challenges of data-driven pixel-level control: how to leverage DDDAS for anticipatory attention, how intelligent pixel-level control leads to less complex ML models, and how analog design choices affect predictive performance. Our solution, based on in-pixel processing and anticipatory sensing, led to a 10X reduction in bandwidth and a ~15-30X improvement in energy-latency savings compared to traditional hardware. We demonstrated the effectiveness of ML-based anticipatory patch selection, highlighted accuracy vs efficiency trade-offs, and suggested avenues for improvement (e.g., better handling of small objects and Bayer pixels). Intelligent, dynamic anticipatory sensing combined with novel energy-efficient hardware is useful beyond electro-optic scene understanding. The framework is compatible with other hardware (e.g., pixel-level exposure control), modalities (e.g., RF sensing), domains (e.g., situational awareness via satellite-based sensing), and applications where data reduction is important. The framework for partial adaptive sensing with flexible control can also be extended to manage and reduce data transmission between nodes in a sensor network with limited communication."}]}