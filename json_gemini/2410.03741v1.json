{"title": "Towards Democratization of Subspeciality Medical Expertise", "authors": ["Jack W. O'Sullivan", "Anil Palepu", "Khaled Saab", "Wei-Hung Weng", "Yong Cheng", "Emily Chu", "Yaanik Desai", "Aly Elezaby", "Daniel Seung Kim", "Roy Lan", "Wilson Tang", "Natalie Tapaskar", "Victoria Parikh", "Sneha S. Jain", "Kavita Kulkarni", "Philip Mansfield", "Dale Webster", "Juraj Gottweis", "Joelle Barral", "Mike Schaekermann", "Ryutaro Tanno", "S. Sara Mahdavi", "Vivek Natarajan", "Alan Karthikesalingam", "Euan Ashley", "Tao Tu"], "abstract": "The scarcity of subspecialist medical expertise, particularly in rare, complex and life-threatening diseases, poses a significant challenge for healthcare delivery. This issue is particularly acute in cardiology where timely, accurate management determines outcomes. We explored the potential of AMIE (Articulate Medical Intelligence Explorer), a large language model (LLM)-based experimental AI system optimized for diagnostic dialogue, to potentially augment and support clinical decision-making in this challenging context. We curated a real-world dataset of 204 complex cases from a subspecialist cardiology practice, including results for electrocardiograms, echocardiograms, cardiac MRI, genetic tests, and cardiopulmonary stress tests. We developed a ten-domain evaluation rubric used by subspecialists to evaluate the quality of diagnosis and clinical management plans. Evaluation was blinded to whether plans were produced by general cardiologists or AMIE, the latter enhanced with web-search and self-critique capabilities. AMIE was rated superior to general cardiologists for 5 of the 10 domains (with preference ranging from 9% to 20%), and equivalent for the rest. Access to AMIE's response improved cardiologists' overall response quality in 63.7% of cases while lowering quality in just 3.4%. Cardiologists' responses with access to AMIE were superior to cardiologist responses without access to AMIE for all 10 domains. Qualitative examinations suggest AMIE and general cardiologist could complement each other, with AMIE thorough and sensitive, while general cardiologist concise and specific analogous to the combination of a sensitive initial diagnostic test (AMIE) followed by a highly specific confirmatory test (general cardiologist). Overall, our results suggest that specialized medical LLMs such as AMIE have the potential to augment general cardiologists' capabilities by bridging gaps in subspecialty expertise, though further research and validation are essential for wide clinical utility.", "sections": [{"title": "1 Introduction", "content": "Globally, there is a significant shortage of speciality medical expertise [1]. The World Health Organization (WHO) predicts a deficit of 18 million providers by 2030, with shortages being most acute in resource-limited and rural areas [2]. This disparity is exacerbated for rarer and more complex conditions, particularly those for which timely treatment prevents morbidity and mortality. For instance, Hypertrophic Cardiomyopathy (HCM) is one of the leading causes of sudden cardiac death in young adults [3], yet, more than half of US states do not have a HCM subspecialist center [4]. Lack of subspecialist access has led to 60% of HCM patients undiagnosed in the US [5]. With premature mortality highly preventable with implanted cardiac defibrillators (ICDs) [3], cardiac conditions such as HCM exemplify this urgent unmet need in healthcare delivery: timely and widely-available access to subspecialist expertise [6]. While cardiac conditions serve as an indicative example, the consequences of delayed access to subspecialist care are significant across all specialities, often resulting in increased morbidity and mortality as patients miss critical diagnostic and treatment windows. Navigating the cascade of referrals required to access subspecialist expertise creates undue stress and anxiety while presenting a time-consuming and resource-intensive process for both patients and healthcare providers.\nLarge language models (LLMs) have emerged as potential assistive tools for an array of healthcare issues [7, 8]."}, {"title": "2 Methods", "content": "To assess AMIE's ability, we developed a blinded fully-counterbalanced reader study, as described in Figure 1. The study comprised four main phases: 1. Acquisition of clinical data: Recruitment and de-identification of clinical data from a subspecialized inherited cardiovascular center, 2. Domain Adaptation of AMIE (Articulate Medical Intelligence Explorer), 3. General cardiologist and AMIE assessment of each case, followed by revised assessment of the cases by general cardiologists when given access to the AMIE assessment outputs, and 4. Subspecialist evaluation and analysis. General cardiologists and AMIE were tasked with interpreting clinical text data from 204 real-world patients, including electrocardiograms (ECGs), rest and stress transthoracic echocardiograms (TTEs), genetic testing, cardiac MRIs (CMRs), ambulatory Holter monitors, and cardiopulmonary exercise tests (CPX). \u03a4\u03bf facilitate scientific progress and reproducibility of our results, we have made all data publicly available (see Data Availability).\nThis data was obtained from patients referred to the Stanford Center for Inherited Cardiovascular Disease, encompassing patients with both suspected and confirmed inherited cardiovascular diseases, and general cardiology patients. We recruited three board-certified general cardiologists not affiliated with Stanford, who have not had dedicated training in cardiovascular genetics and had not cared for any of the studied patients. These general cardiologists and AMIE were asked to diagnose, triage, and manage these patients using the provided text reports. The general cardiologists assessed the data twice: initially without AMIE support and later with assistance from AMIE's assessment. Afterwards, blinded subspecialist cardiologists used a multi-domain evaluation rubric to compare the responses from AMIE to those from general cardiologists, as well as the general cardiologist's responses before and after assistance from AMIE."}, {"title": "2.1 Clinical data", "content": "The Stanford Center for Inherited Cardiovascular Disease (SCICD) is one of the world's largest centers focused on inherited cardiovascular disease. Patients with suspected cardiovascular disease are referred for diagnosis and management. Referrals typically are from general cardiologists. The patient population largely contains patients with suspected inherited cardiovascular disease, however the clinic also sees general cardiology patients. All physicians at SCICD have received subspeciality training in genetic cardiac disease.\nThe assessment of patients involves review of the patient's history and review of tests such as cardiac MRIs, rest and stress echocardiograms, cardiopulmonary stress tests, ECGs, ambulatory Holter monitors, and genetic testing. For each of these tests, a text report outlining in-depth and summarized test results is produced. Text results from these investigations were available and utilized for all patients included in this study. We utilized 204 real-world patient cases in the test set after first exposing the model to 9 different patient cases for model refinement. No cases utilized in model refinement were utilized for model testing."}, {"title": "2.2 AMIE: An LLM based AI system for diagnostic dialogue", "content": ""}, {"title": "2.2.1 Model development", "content": "Articulate Medical Intelligence Explorer (AMIE), is an experimental LLM-based medical artificial intelligence (AI) system optimized for clinical history-taking and diagnostic dialogue [9]. AMIE was trained with a self-play based simulated learning environment for diagnostic medical dialogues, enabling the scaling of AMIE's knowledge and capabilities across a multitude of medical conditions and contexts (see Figure 2). Specifically, we previously used this environment to iteratively fine-tune AMIE with a corpus of medical question answering, reasoning, summarization tasks and real-world medical dialogue data in addition to an evolving set of simulated dialogues. At inference time, AMIE utilizes a chain-of-reasoning strategy adaptable to various medical tasks, enabling the integration of automated feedback mechanisms and tool use to refine its response."}, {"title": "2.2.2 Domain adaptation of AMIE", "content": "Adaptation of AMIE to this subspecialist domain required clinical data from just nine patients. Five cases, chosen at random, were used to design an optimal approach to model prompting through iterative review and expert feedback. Several prompting strategies were tested: zero vs. few-shot prompting, with vs. without search retrieval augmentation, providing an AMIE generated summary of the cardiac testing information vs. providing raw cardiac testing information, and using the original response vs. allowing the model to self-critique and revise its answer. After AMIE produced responses on the 5 patients using various combinations of these strategies, we had a specialist cardiologist review the responses and select their preferred set of responses. Ultimately, the specialist preferred the \"few-shot with self-critique and search retrieval augmentation\" method, in which AMIE drafts an initial response to the case with few-shot exemplars, conducts web search to retrieve relevant guidelines, and then critiques and revises its initial drafted response. The few-shot exemplars used were four additional patient cases for which a specialist cardiologist provided a \"gold-standard\" case assessment. Note that none of the nine patient cases used as either few-shot exemplars or for validating inference strategies were part of the test set."}, {"title": "2.3 Study design and evaluation", "content": ""}, {"title": "2.3.1 Cardiologist and AMIE assessment", "content": "We exposed both AMIE and board-certified general cardiologists without subspecialty expertise in genetic cardiovascular disease to test results from 204 consecutive, real-world patients. This test population consisted of patients suspected or confirmed to have inherited cardiovascular disease, as well as a mixture of patients without genetic cardiovascular disease. Test results included: cardiac MRIs, rest and exercise echocardiograms, cardiopulmonary stress tests, ECGs, and ambulatory Holter monitor results. Genetic test result data were also available for some of the patients.\nBoth AMIE and general cardiologists completed the same standardized assessment form shown in Figure 3. After AMIE and the general cardiologists completed their assessments individually, the cardiologists underwent a washout period of 2 months. Then, the cardiologists were again provided with the clinical data, as well as AMIE's and their own responses, and asked to revise their assessment if needed.\nThe assessment form (Figure 3) aims to evaluate the respondents across a range of domains including triage, diagnosis, and management of these patients with potential inherited cardiovascular disease. Respondents provide an 'Overall impression' of the patient's case and answer a 'Consult Question' regarding the likelihood of a genetic cardiomyopathy. The 'Triage Assessment' section prompts respondents to determine the necessity of referral to a specialist center. In the 'Diagnosis' section, the respondent is asked to list their most likely diagnosis as well as any additional information they would need to ask the patient or gather from tests. In the 'Management' section, the respondent describes their management plan and any additional information they would need. After answering these previous sections, the respondent is provided with genetic test results (where available) and asked if and how they would change any prior answers."}, {"title": "2.3.2 Subspecialist evaluation", "content": "We performed a series of comparisons to examine the utility of AMIE to appropriately triage, diagnosis, and manage patients with suspected inherited cardiovascular disease:\n1. General cardiologist response vs. AMIE response;\n2. Individual assessment of AMIE and general cardiologist responses;\n3. General cardiologist response vs. general cardiologist response with AMIE assistance.\nThe responses of 204 real-world cases were evaluated by four subspecialist cardiologists, who were blinded to source of responses. We conducted two types of evaluation: direct A/B preference comparisons and an individual assessments, described further in Section 2.3.3.\nThe statistical analysis of the results were performed via bootstrapping (see Section 2.3.4)."}, {"title": "2.3.3 Development of an evaluation rubric for subspecialist case interpretation", "content": "We developed rubrics for subspecialists to evaluate responses, under two conditions: Direct A/B comparison of general cardiologist's and AMIE's responses for each domain, and individual evaluation of specific responses in isolation.\nFor the direct A/B comparison (see Figure 4), the domains of the evaluation rubric mirrored the clinical assessment form completed by general cardiologists and AMIE. For each domain, evaluators indicated a direct preference between general cardiologists and AMIE with an option for a tie. We chose this direct comparison with a third option for a tie to facilitate greater potential discrimination in performance between general cardiologists and AMIE compared with Likert scales, while also allowing equivalence to be expressed. To investigate more nuanced qualities of each response, subspecialists also evaluated general cardiologist and AMIE responses individually. To do this we developed an individual evaluation rubric (see Figure 5). This was developed through identification of response quality themes from the existing literature [8], followed by an iterative combination of semi-structured feedback from LLM domain experts and cardiologist experts not involved in the evaluation or the remainder of the study design. Themes from the existing literature were shared with respective experts. Experts then provided feedback in a semi-structured manner. Themes were narrowed and refined in response to expert feedback until thematic saturation (no new themes emerged) was reached, which in our case lead to five major themes. Instructions for evaluators were written and piloted using the 9 development cases, these cases were piloted and used as worked examples in interactive feedback sessions with expert evaluators. Feedback on instructions, and the evaluation rubric was sought from experts and changes implemented if concordance from experts was present. The above approach led to iterative improvement to our individual evaluation rubric, spanning five crucial domains of LLM evaluation: 1. Errors, 2. Addition, 3. Omission, 4. Reasoning and intelligence, and 5. Bias. Subspecialist experts were asked to answer Yes or No to direct questions spanning these 5 domains and then given free text responses to quantify and explain their selections."}, {"title": "2.3.4 Statistical analysis", "content": "For each preference or independent rating, we determined significance by using bootstrapping to estimate the sampling distribution of our test statistic [15]. For individual evaluation criteria, this test statistic was computed as (% yes for AMIE - % yes for Cardiologist), while for the preference ratings, it was computed as (% AMIE preferred - % Cardiologist preferred) or (% Assisted preferred - % Unassisted preferred). Note that we discarded ties for the purpose of this analysis. We created 10,000 re-samples of our data and calculated these statistic for each re-sample. We then determined significance by observing whether a difference of zero fell below the fifth percentile of the bootstrap distribution."}, {"title": "3 Results", "content": "204 consecutive patients were assessed by both general cardiologists and AMIE. The median age of patients was 59 years (range: 18-96 years). The number and percentage of patients with available clinical text data for each test was as follows: CMR: 121 (59.3%), CPX: 115 (56.4%), resting TTE: 172 (84.3%), exercise TTE: 131 (64.2%), ECG: 188 (92.2%), ambulatory holter monitor: 151 (74.0%), and genetic testing: 147 (72.0%) (see Table 1). Of the 204 patients, 75 (36.8%) had a variant adjudicated to be pathogenic or likely pathogenic as per American College of Medical Genetics and Genomics (ACMG) interpretation of variants criteria [16]."}, {"title": "3.1 Direct preference: AMIE vs. Cardiologist", "content": "The subspecialist evaluators first directly compared AMIE's assessment to the general cardiologists' assessments using the evaluation form in Figure 4. For the 204 patients, across the 10 evaluation domain considered, AMIE was rated superior to general cardiologists across five domains while being equivalent for the remaining domains (Figure 6a and Table A.1). The domains in which AMIE responses were preferred were: 'consult question explanation', 'additional patient information', 'additional test information', 'management', and 'genetic explanation'."}, {"title": "3.2 Individual assessment", "content": "The subspecialist evaluators evaluated AMIE and the general cardiologist responses individually on the set of five 'yes'/'no' question in Figure 5. As shown in Figure 6b and Table A.2, AMIE's responses were less likely to omit relevant content, though this difference was not statistically significant. AMIE responses were more likely to contain unnecessary extra content or a clinically significant error. On the other hand, AMIE's responses were less likely to be inapplicable/inaccurate for particular medical demographics, and were rated equivalent to general cardiologist responses in evidence of correct reasoning."}, {"title": "3.3 Direct preference: Cardiologist before and after seeing AMIE's response", "content": "Of the 204 patient assessments, 195 of the assessments (95.6%) were changed by the general cardiologists after seeing AMIE's response. While for a few assessments this consisted of relatively inconsequential wording changes, for many cases this revision substantially improved the appropriateness of the diagnosis, management plan, and/or genetic testing interpretation. When considering the entire response, the assisted 'Cardiologist + AMIE' responses were ranked over cardiologists' responses without AMIE, preferred 63.7% of the time, compared with 3.4% of the time for responses by unassisted general cardiologists. Across the remaining 9 specific domains, the AMIE-assisted responses were preferred for all domains when directly compared to the general cardiologists alone, though 'Tie' was the most common evaluation for 8 of the 10 domains (see Figure 7 and Table A.3).\nThe domains that had the greatest improvement were 'management', 'additional patient information', and 'additional test information'. In particular, the management domain, which improved the most with AMIE assistance, had many cited causes for improvements including newer medications for rare diseases (e.g., cardiac myosin inhibitors for hypertrophic cardiomyopathy), education around family screening, and identification of further risk factors for sudden cardiac death."}, {"title": "3.4 Analysis of clinical feedback", "content": "To understand the rationale behind the preferences and individual ratings provided by subspecialists, we analyzed the free-text comments left by subspecialists for AMIE's and the general cardiologists' responses. We combined all of the feedback received and summarize the reasons for preference for both AMIE and Cardiologists. While AMIE and cardiologists had similar overall preferences (see Figure 6), the types of feedback they each received were quite different; AMIE was seen as thorough, and sensitive to a broad differential diagnosis, while the general cardiologists were often seen as specific, and concise, but risked anchoring on a certain diagnosis (see Figure 8). In this way, the approach of AMIE and general cardiologist might complement each other: AMIE is thorough and sensitive, general cardiologist concise and specific - analogous to combining a sensitive initial diagnostic test with a highly specific confirmatory test. We quantified this difference in specialist feedback of AMIE and general cardiologist responses: 92% of general cardiologist's clinically significant errors described by subspecialists were omission errors and only 8% of errors related to unnecessary extra care. These omission errors were a combination of: missing the correct diagnosis, overlooking imaging findings suggestive of disease or requiring further investigation (e.g., \u2018biventricular systolic dysfunction\u2019), or not recommending the appropriate diagnostic or follow up tests (e.g., no family screening, or no holter to investigations and assessment of sudden cardiac death risk). In contrast, only 35.5% of AMIE'S errors were omission, however 64.9% were related to suggestions of potentially unnecessary care. The majority of these suggestions were additional or repeat non-invasive tests (such as holter monitors, echocardiograms and cardiac MRIs), with 9.1% of described errors related to unnecessary suggestions of invasive tests/procedures (such as transesophageal echocardiogram and EP study). Both AMIE and general cardiologists' clinically significant errors are described in Figure A.3. We also performed a similar analysis on the 5 individual assessment criteria, finding that the subspecialists described very different and often complementary strengths and weaknesses of AMIE and cardiologists for each criteria (see Figure A.2)."}, {"title": "3.5 Qualitative clinical applications", "content": "To explore potential future clinical uses of technology such as AMIE, we present four qualitative examples of how capabilities in dialogue could be utilized to communicate with patients or up-level generalists. The first hypothetical scenario in Figure 9 shows AMIE assisting a general cardiologist in the assessment of real-world clinical ECG and ambulatory Holter monitor text data (Figure 9a). Figure 9b shows the responses to this data by the general cardiologist and AMIE respectively, and Figure 9c shows an example dialogue of AMIE assisting a general cardiologist. As Figure 9b shows, the initial general cardiologist assessment of the patient had a low likelihood of genetic heart disease and no referral to a speciality genetic center was recommended. AMIE's response better reflected the subspecialist cardiologist assessment: that this patient's data suggests they may have hypertrophic cardiomyopathy and should be referred to a specialist center. This example shows how AMIE may assist a general cardiologist to manage this patient, highlighting that hypertrophic cardiomyopathy can present with a modest left ventricular outflow tract obstruction and can be asymptomatic.\nFor the remaining three hypothetical scenarios, AMIE was given a prompt, akin to a \u201cone-line\u201d summary of a patient (Table A.4) along with clinical data for the corresponding patient and asked to produce dialogues mirroring various potential use cases for AMIE:\n1. AMIE reaches a diagnosis and then explains it to a patient (Figure A.4);\n2. AMIE providing assistive dialogue for a general cardiologist (Figure A.5);\n3. AMIE assumes the role of the specialist cardiologist and presents an assessment (Figure A.6)."}, {"title": "4 Discussion", "content": "In this study, we probe the ability of LLMs to provide additive support to generalists in the assessment of rare, life-threatening cardiac diseases that typically require subspeciality cardiac care. Further, we address the unmet need of evaluation of LLMs in specialist medical domains. To this end, we curate and open-source a de-identified real-world clinical dataset for patients suspected to have inherited cardiomyopathies and propose an evaluation rubric for the quality of diagnosis, triage and clinical management for such patients. Using this evaluation rubric, blinded subspecialists evaluate AMIE and general cardiologists before and after viewing AMIE's assessment. Across these 204 patients, AMIE is rated as equivalent (and for some domains, superior) in standalone performance, though with an increased rate of clinically significant errors. Furthermore, by improving assessment quality in over 60% of patients, AMIE demonstrates significant potential to enhance general cardiologists' diagnoses and management of these challenging cases.\nOur results demonstrate the feasibility of utilizing LLMs, specifically AMIE, an experimental research LLM optimized for diagnostic dialogue, to assess patients with rare and life-threatening cardiac conditions. Adapting AMIE to this subspecialist and rarified domain was highly data-efficient, leveraging iterative feedback from subspecialist experts to enhance the quality of AMIE's responses using just 9 cases. This iterative process, combined with self-critique and the incorporation of search functionality, enabled AMIE to provide assessments equivalent (and for some domains, better) than those of general cardiologists. This contrasts with prior studies using generic, non-specialized LLMs, which did not achieve comparable clinical performance [14].\nWhile AMIE provided more thorough assessments and exhibited fewer content omissions or biases, its increased comprehensiveness did come at the expense of a modest increase in clinically-significant errors. AMIE's errors were generally additive, such as suggesting further investigations (i.e., such as regular monitoring with a cardiac MRI), whereas general cardiologist's errors tended to be omission. Most commonly missing diagnoses, overlooking pathological imaging findings, and not recommending appropriate further investigations. In fact, 92% of general cardiologist's errors were omission, whereas the majority of AMIE errors were related to suggestions of unnecessary care (65%). Given the potential for AMIE's errors and the need for further prospective validation, our results do not support the deployment of LLMs like AMIE autonomously. The results instead indicate that a more appropriate use case for LLMs in this domain may be as assistive tools for generalist providers. AMIE is thorough and sensitive and its errors tend to be detectable and revolve around additional, potentially unnecessary testing. General cardiologists are specific, but prone to omission. In this way, AMIE's assistive value could be in thorough sensitive assessments, which then can be refined by cardiologists, who tend to be more specific. Importantly, AMIE's errors are detectable by cardiologists and"}, {"title": "5 Ethics approval", "content": "The clinical subspecialist evaluator component of this research involved the participation of physicians. This study adhered to the principles outlined in the Declaration of Helsinki. Informed consent was obtained from each physician before their participation. This study used only retrospective, de-identified data that fell outside the scope of institutional review board oversight."}, {"title": "6 Reporting summary", "content": "Further information on research design is available in the separate reporting summary"}]}