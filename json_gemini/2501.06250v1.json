{"title": "Generative AI for Cel-Animation: A Survey", "authors": ["Yunlong Tang", "Junjia Guo", "Pinxin Liu", "Zhiyuan Wang", "Hang Hua", "Jia-Xing Zhong", "Yunzhong Xiao", "Chao Huang", "Luchuan Song", "Susan Liang", "Yizhi Song", "Liu He", "Jing Bi", "Mingqian Feng", "Xinyang Li", "Zeliang Zhang", "Chenliang Xu"], "abstract": "Traditional Celluloid (Cel) Animation production pipeline encompasses multiple essential steps, including storyboarding, layout design, keyframe animation, inbetweening, and colorization, which demand substantial manual effort, technical expertise, and significant time investment. These challenges have historically impeded the efficiency and scalability of Cel-Animation production. The rise of generative artificial intelligence (GenAI), encompassing large language models, multimodal models, and diffusion models, offers innovative solutions by automating tasks such as inbetween frame generation, colorization, and storyboard creation. This survey explores how GenAI integration is revolutionizing traditional animation workflows by lowering technical barriers, broadening accessibility for a wider range of creators through tools like AniDoc, ToonCrafter, and AniSora, and enabling artists to focus more on creative expression and artistic innovation. Despite its potential, issues such as maintaining visual consistency, ensuring stylistic coherence, and addressing ethical considerations continue to pose challenges. Furthermore, this paper discusses future directions and explores potential advancements in AI-assisted animation. For further exploration and resources, please visit our GitHub repository: https://github.com/yunlong10/Awesome-AI4Animation", "sections": [{"title": "I. INTRODUCTION", "content": "Animation, as a powerful medium for storytelling and artistic expression, has evolved significantly over the past century. Celluloid (Cel) animation, established as a cornerstone of traditional animation in the early 1920s, has shaped the foundation of the modern animation industry through its distinctive frame-by-frame approach that combines artistic vision with technical precision. This methodology laid the foundation for modern animation while highlighting a fundamental tension: the challenges of maintaining high artistic quality while improving production efficiency: 1) Time-intensive manual labor: The creation of keyframes, inbetween-ing, and colorization often requires extensive manual effort. 2) Technical complexity: Coordinating multiple stages, from scripts to storyboard, from keyframe animation to coloring, involves a high degree of expertise and meticulous planning. 3) Creativity constraints: Repetitive and labor-intensive tasks can detract from the time and energy animators devote to higher-level creative decisions. These challenges pose barriers to efficiency and scalability, particularly in modern animation workflows where production timelines are tightening.\nRecent advances in generative artificial intelligence (GenAI) have introduced transformative solutions to these challenges.\nGenAI, including large language models (LLMs) [1]-[3], multimodal large language models (MLLMs) [4]-[9], and diffusion models [10]-[14], has demonstrated its ability to produce images, videos, and even animations autonomously or semi-autonomously. By automating repetitive tasks, such as generating inbetween frames or applying color schemes, GenAI enables animators to focus more on creative storytelling and artistic innovation. Its applications in various creative fields are expanding rapidly, showing promise in addressing the inherent challenges of Cel-Animation production.\nTo better understand how GenAI is transforming Cel-Animation, we examine its historical evolution through three major stages, as shown in Figure 1:\n1) The Handcrafted Cel Age (1920s-2010s): Cel-Animation emerged in the early 20th century as a groundbreaking art form that revolutionized animation production. The innovation of using celluloid sheets enabled animators to separate dynamic characters from static backgrounds, allowing for unprecedented depth in scenes and efficient reuse of background elements [15]. This process established fundamental techniques still used today: storyboarding for narrative planning, keyframe and inbetween frame drawing for motion, and hand-painting cels for final visuals."}, {"title": null, "content": "While masterpieces like Snow White and the Seven Dwarfs (1937) showcased the artistic potential, they also highlighted its limitations-the film required over 200,000 hand-drawn frames and three years to complete. The labor-intensive process of creating 24 frames per second demanded large teams of specialized artists, presenting significant challenges in maintaining consistency and scaling production.\n2) The Computer-Assisted Cel Age (1980s-present): The 1980s marked a significant shift in Cel-Animation with the introduction of digital tools that revolutionized traditional workflows. Disney's Computer Animation Production System (CAPS), developed in collaboration with Pixar, pioneered the automation of coloring and compositing while preserving the aesthetic of hand-drawn animation [16]. Films like Beauty and the Beast (1991) and The Lion King (1994) demonstrated CAPS' ability to enhance precision and complexity. Concurrently, Japanese studios adopted tools like Toonz [17], OpenToonz [18], Adobe Flash [19], and Clip Studio Paint [20] to optimize inbetweening and compositing while maintaining their signature hand-drawn style, as seen in Nausica\u00e4 of the Valley of the Wind (1984) and Neon Genesis Evangelion (1995). This era showcased contrasting strategies: U.S. studios continued to use full animation, while Japanese studios sufficiently utilized \"limited animation\" [21] further reduced frame counts and maximized resource efficiency, which brought significant productivity. While these digital tools enhanced efficiency and expanded creative possibilities, they primarily served as assistive technologies, with core artistic execution still heavily dependent on manual input.\n3) The AIGC Cel Era (2020s onward): The emergence of Artificial Intelligence Generated Content (AIGC) has redefined Cel-Animation. Unlike previous digital tools that mainly augmented manual processes, AIGC actively participates in technical execution, such as inbetween frame generation, complex effects creation, and stylistic consistency maintenance [22]-[24]. These tools reduce repetitive tasks, allowing animators to focus on creative decisions [25]. For instance, AniDoc employs video diffusion models to automate inbetweening and colorization in 2D animation [23], while ToonCrafter [24] efficiently handles exaggerated non-linear motions and occlusions in cartoons. Platforms like Storyboarder.ai streamline pre-production by generating storyboards using AI [26]. Experiments by Netflix Japan (The Dog and the Boy, 2023) and tools like CogCartoon [27] demonstrate AIGC's potential to democratize animation by enabling practical visualization with minimal resources. Research in animation video generation models, such as AniSora [28], and innovations in generative storytelling [25], further highlight the potential for AIGC to revolutionize animation as an art form and a production methodology.\nWhile AIGC has dramatically improved animation production efficiency and creative possibilities, significant challenges persist in maintaining visual consistency, and preserving stylistic coherence [22]-[24, 29]. As the field continues to evolve rapidly, understanding these challenges and opportunities requires a systematic examination of both traditional animation techniques and emerging AI technologies.\nRecently, several surveys have explored related fields in generative content creation. Li et al. [30] focus on long video generation, addressing challenges and methodologies for extended video content. Lei et al. [31] review human video generation, emphasizing realistic motion synthesis. Xing et al. [32] survey video diffusion models, highlighting their application to temporal coherence. Zhou et al. [33] explore generative AI and large language models in video generation, and Cho et al. [34] provide a comprehensive review of text-to-video generation. Zhao et al. [35] present a comprehensive survey on cartoon image processing, including a limited coverage of topics related to cartoon animation. In contrast, our work specifically targets Cel-Animation, systematically reviewing the integration of GenAI tools into traditional animation pipelines, analyzing its role across pre-production, production, post-production stages, and discussing future research directions, with particular attention to how these technologies are reshaping the balance between creativity and productivity in animation."}, {"title": "II. PRELIMINARY", "content": "The Cel-Animation production pipeline is a meticulously structured process, encompassing multiple stages from conceptualization to final delivery. Each phase plays a vital role in shaping the quality and visual appeal of the final animation. This section provides a comprehensive overview of the conventional Cel-Animation workflow, which includes pre-production, production, post-production, and other auxiliary processes, as illustrated in Figure 2. More interpretation of terms can be found in Appendix C.\n1) Pre-production: Pre-production sets the foundation for an animation project by defining its creative direction and technical framework. It is a highly collaborative phase that determines the overarching vision of the work.\n\u2022 Scripting: The script serves as the blueprint for the story, defining the time, place, characters, events, and dialogue. It also specifies which props and items need to appear in the scenes, along with their respective colors. In cases of adaptations or original works, initial meetings involve directors, scriptwriters, and producers discussing the overall story structure, world-building, and required narrative elements. Decisions made at this stage influence the project's trajectory.\n\u2022 Setting: Building upon the script, the setting defines the world in which the story takes place, encompassing both the physical and thematic environment. This phase involves conceptualizing characters, items, and background elements that visually represent the story's mood and themes (see Figure 3).  They are used as templates by animators in order to stay on the model when drawing.\n\u2022 Storyboarding: The storyboard translates the script into a visual sequence, determining shot composition, character acting, and scene timing. Directors and storyboard artists work closely during storyboard meetings to finalize the visual narrative. This step establishes the creative groundwork for subsequent production phases. An example of storyboard can be seen in Figure 4 on the left.\n2) Production: Production is where pre-production ideas take physical form, transitioning from conceptualization to the creation of animation assets. Multiple teams collaborate"}, {"title": null, "content": "simultaneously on distinct elements of the animation.\n\u2022 Layout (L/O): Layouts (L/O) specify the placement and relationships between characters, objects, and back-grounds within a shot, based on storyboards. Artists ensure precise framing, perspective, and camera angles during this step. Layout approval marks the transition into more detailed work streams.\n\u2022 The 1st Keyframe Animation (Genga/1st Key): The First keyframe animation (Genga/1st Key) provides the structural foundation for character movement and acting. Key animators produce rough sketches representing critical moments within a sequence, which serve as references for further refinement.\n\u2022 The 2nd Keyframe Animation (Nigen/2nd Key): The Second keyframe animation (Nigen/2nd Key) builds upon Genga, adding detail to poses and smoothing out motion. This step bridges gaps in movement and ensures continuity. Corrections by animation directors are often incorporated at this stage. After this step, the resulting line drawing highlights are represented in red, while shadow lines are depicted in blue, as shown in the second row of Figure 4.\n\u2022 Inbetweening: Inbetweening involves drawing transitional frames between keyframes to achieve smooth and natural motion. This stage, typically handled by assistant animators, forms the bulk of traditional Cel-Animation and is critical for maintaining flow and consistency.\n\u2022 Colorization: Once line work is finalized, frames are colored based on established color models. Colorization not only enhances visual appeal but also reinforces the emotional and thematic tone of the animation. Teams often use digital tools for efficient and precise coloring.\n3) Post-production: Post-production integrates all animated elements into a cohesive final product, adding effects, sound, and other finishing touches.\n\u2022 Compositing & Photography: Compositing combines characters, backgrounds, and visual effects into finalized scenes. Traditionally involving photography of physical cel layers, modern workflows rely on digital compositing software to achieve similar results."}, {"title": null, "content": "Cutting (CT): Cutting organizes animated sequences into a narrative flow, ensuring proper timing, pacing, and synchronization with sound. Editors fine-tune scenes for coherence and alignment with the director's vision.\n\u2022 Music & Sound Effects: Sound design includes creating and syncing background music (BGM) and sound effects (SE) to the visuals. These elements enhance emotional depth and provide a more immersive viewing experience.\n\u2022 After Recording (AR) & Dubbing (DB): The dubbing phase involves recording and synchronizing voice acting with the completed animation. Additionally, this step integrates character performances with the visual flow of the animation.\n4) Other Processes:\n\u2022 Background Design: Background design relies on the BG Setting obtained during the Setting stage and Mood Board, as well as the L/O from the Layout stage. In addition to manual drawing, 3D assistance is sometimes required.\n\u2022 3D Assistance: Place buildings and characters in 3D space to recreate the actual scene, and position a virtual camera within the 3D space to finalize the layout. The layout created in 3D is printed onto paper, and from there, the process of adding character movements by hand continues.\n\u2022 Clean-Up: Clean-up is the process of refining rough sketches, aiming to transform the initial, hasty lines into clear and polished final line art. This step ensures the accuracy of details in characters and scenes and is typically carried out during the 2nd Keyframe Animation.\n\u2022 Quality Control & Inspection: The animation production pipeline incorporates multiple inspection stages to ensure consistent quality. The details can be found in Appendix B."}, {"title": "B. Generative Al", "content": "1) Large Language Models (LLMs): Language models learn the joint probability $p(x_{1:L})$ over a token sequence $X_{1:L}$ via the chain rule:\n$P(X_{1:L}) = \\prod_{i=1}^{L} P(X_i|X_{1:i-1}),$ (1)\nwhere L is the sequence length. With billions of parameters, LLMs utilize tokenizers and self-attention layers to predict token probabilities autoregressive: $M(x_{1:i\u22121}) = P(X_i | X_{1:i\u22121})$. Decoding strategies, such as greedy decoding:\n$x_t = arg \\max_{s \\in S} logP_{M(s|x_{1:t-1})},$ (2)\ncontrol token selection, while sampling strategies promote diversity.\nKey characteristics of LLMs include:\n\u2022 Scaling Laws [36]: Model performance grows predictably with increased parameters, data, and compute.\n\u2022 Emergent Abilities [37]: At scale, models exhibit novel behaviors like in-context learning, instruction following, and chain-of-thought reasoning."}, {"title": null, "content": "LLMs have been applied in animation and filmmaking to streamline tasks like scriptwriting, character backstory creation, and generating visual descriptions for storyboarding. Multimodal LLMS (MLLMs or LMMs) [9, 38] extend LLMs by integrating visual encoders and cross-modal aligners, excelling in tasks that require both visual and textual reasoning.\n2) GAN, VAE, and Diffusion Models: Generative Adversarial Networks (GANs) [39] involve a generator G and a discriminator D in a minimax game:\n$\\min_{G} \\max_{D} E_{x \\sim p_{data}}[log D(x)]+E_{z \\sim p_{z}}[log(1-D(G(z)))].$ (3)\nApplications include background generation and style transfer. StyleGAN [40] refines stylistic consistency, while Pix2Pix [41] transforms sketches into images.\nVariational Autoencoders (VAEs) [42] model latent distributions by maximizing the Evidence Lower Bound (ELBO):\n$L(\\theta, \\phi) = E_{q_{\\phi}(z|x)} [log p_{\\theta}(x|z)] \u2013 D_{KL}(q_{\\phi}(z|x)||p(z)),$ (4)\nwhere $D_{KL}$ is the Kullback-Leibler divergence. VAEs excel in generating controllable character poses and expressions. Diffusion models [10, 43] iteratively denoise samples drawn from Gaussian noise to approximate data distributions. A forward process adds noise in steps:\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 \u2013 \\beta_t}x_{t-1}, \\beta_tI),$ (5)\nand a neural network $ \\epsilon_{\\theta}(x_t,t)$ predicts noise in the reverse process. The model minimizes the following Mean Square Error (MSE) loss:\n$L = E_{x_0,\\epsilon,t} [||\\epsilon \u2013 \\epsilon_{\\theta}(x_t,t)||^2],$ (6)\nwhere $x_t = \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 \u2212 \\bar{a}_t} \\epsilon$, with $a_t = \\prod_{i=1}^{t}(1 \u2212 \\beta_{\\epsilon})$.\nModels like Stable Diffusion [44] and ControlNet [12] extend this framework to tasks such as animation and colorization. Video-specific diffusion models address temporal consistency, further bridging key production gaps."}, {"title": "III. GENAI FOR CEL-ANIMATION", "content": "In this section, we explore how GenAI approaches are designed for each specific process in Cel-Animation, alongside those that show promising potential for Cel-Animation production despite their original development for other applications. We also collect related datasets, as shown in Table I.\nA. GenAI for Pre-production\n1) Script Generation: The script traditionally relies on human writers' creativity and narrative skills. However, the application of generative AI methods varies between original animations and adaptations. LLMs can be primarily used for original animations to generate story settings, expand details, and create dialogues from scratch. In contrast, for adaptations, such as those based on novels, manga, or games, LLMs first need to understand the source material's plot, character relationships, and narrative style before creating or adapting the script. With the advancements in proprietary (e.g., ChatGPT [5], Claude [47], Gemini [46]) and open-source pre-trained LLMs (e.g., LLaMA [1], Qwen [49]), generative"}, {"title": null, "content": "AI has demonstrated strong adaptability in both original and adapted scriptwriting, providing flexible and efficient solutions for animation script generation. The HoLLMwood [45] framework illustrated in Figure 8 introduces a role-playing mechanism, utilizing pre-trained LLMs to automate various stages of scriptwriting, including plot development and dialogue generation. This framework can generate structured scripts tailored to the requirements of different animation projects while optimizing character interactions and narrative logic.\n2) Setting Generation: While there is no dedicated research focusing on character/object setting generation, many artists have already experimented with tools like Midjourney [50] and Stable Diffusion [44] to generate designs for items, characters, scenes, and mechas through text prompts. As shown in Figure 9, these generated contents demonstrate a certain degree of consistency (for example, multiple views of characters from different angles maintain good identity preservation). However, there are notable imperfections in the details. For instance, the generated character designs may include objects that are either absent from the character or lack practical significance; furthermore, the text appearing in these generated design sheets often consists of meaningless gibberish characters rather than coherent text.\n3) Storyboard Generation: Storyboard generation primarily focusing on live-action film storyboards [58] while also addressing non-photorealistic storyboard generation such as animation [27, 51]. These systems typically accept textual scripts as input and generate visual storyboards in the form of sequential images or video sequences. To develop an automated storyboard production pipeline utilizing AI tools, users can provide high-level narrative guidelines to LLMs, which then generate detailed scene descriptions. These descriptions subsequently serve as input for image generation models, which produce the final sequential visual narrative in either static or animated form. The main challenge lies in achieving consistent identity in the generated images and ensure the"}, {"title": null, "content": "in static images. Video generation models with camera motion control [70] enable simultaneous foreground and background motion, addressing the limitations of static layout models, as shown in the bottom of Figure 11. These approaches are useful for Cel-Animation, offering dynamic layouts that support various camera angles, distances, and continuity across frames, bridging gaps in traditional static layout methods.\n2) Keyframe Animation: The keyframe animation mainly focuses on the moving elements within a scene. Therefore, we mainly collect and review methods focused on character animation generation [71]. To achieve this goal, recent works [137] leverage 2D body landmarks for the target avatar pose control, which provides explicit spatial guidance for human pose generation. For example, Animate-Any-One [71], Champ [72], and MusePose [138] introduce ReferenceNet to incorporate the spatial body information for the target pose generation, effectively bridging the gap between source and target poses. These approaches demonstrate significant improvements in pose transfer accuracy and natural motion synthesis. MimicMotion [73] and Animate-X [74] further advance this direction by considering the different body sizes and proportions to achieve better generalization across realistic persons and cartoon characters. This consideration of body morphology variations enables more robust pose transfer between subjects with different physical characteristics, expanding the applicability of these methods to a broader range"}, {"title": null, "content": "of scenarios. These methods aim to regulate the movement of the body, while there are also many methods that focus on the movement control of the face. Together, they form a comprehensive framework for character animation. Recent works [29, 139]\u2013[141] animate the photo-realistic/toonification face from 3D Gaussian splatting or neural representation. For example, TextToon [29] and Emo-Avatar [141] adopt the 3D Gaussian representation with LLMs, and enabling the adaptation of facial cartoon style through text-based modifications. Ada-TalkingHead [139] employs neural keypoints to drive head animation, while Editable-Head [140] generates head animations using explicit landmark-based representations. In contrast to coarse body control, these methods are more refined, offering greater sophistication and intricate details for facial motion animation.\n3) Inbetweening: Inbetweening is one of the most mechanically straightforward yet labor-intensive and time-consuming steps in the animation pipeline. This makes it particularly suitable for efficiency enhancement through GenAI. Consequently, compared to other steps in the animation production process, there has been a relatively substantial of research in AI for inbetweening. The current mainstream approach, such as Tooncraft [24], AutoFI [80], etc., draws inspiration from video interpolation models, where diffusion models are employed to predict intermediate frames given the first and last frames."}, {"title": null, "content": "4) Colorization: Colorization is another labor-intensive step that has been extensively studied. Since our survey primarily focuses on Cel-Animation, we have concentrated our review on anime video colorization rather than static anime image colorization. The input typically consists of line drawings along with a colored line drawing as guidance, enabling the colorization of all black-and-white line drawings [23]. More recently, this transformation has been approached as a style transfer problem in computer vision. TextToon [29], StyleGANEX [88] and VToonify [87] simplifying the process through few-shot learning, where models learn domain transfer from source sketches to target styles using only a small set of example animations. However, these approaches often struggle with maintaining temporal consistency across video sequences and only limited to avatars. The latest researches leveraging large-scale pretrained Diffusion Models [89, 90, 142] has achieved video-level style transfer even in zero-shot scenarios, significantly reducing manual labor requirements. Recent works [94, 96]\u2013[98] are capably of achieving more complex colorizations for the whole scene with precise control. Notably, some current models can simultaneously perform both inbetweening and colorization steps, requiring only one colored line drawing as reference along with first and last frames as input to generate intermediate frames that are already colored. For example, ToonCraft [24] demonstrates this capability by supporting both inbetweening and colorization in a single model."}, {"title": "C. GenAI for Post-production", "content": "1) Compositing & Photography: After animation completes inbetweening and colorization, it needs to be composited with backgrounds and other materials. Compositing involves more than simply overlaying foreground elements onto backgrounds; it requires careful attention to maintaining consistency between foreground and background elements. Although the spatial relationships and perspective between foreground and background are unified during the layout phase, specific adjustments are still necessary. For instance, in twilight scenes, the environmental lighting and color tones of foreground and background elements need to be harmonized; the variations in light and shadow, as well as the reflection of light on the character's body, are particularly challenging to handle. In other cases, particle effects, special textures, or flame effects may need to be added. All these processing steps must be completed during the Photography phase. Considerable researches [100]\u2013[104, 106] have already explored methods for harmonizing foreground and background composition (not limited to video). For example, [143] attempted to adjust objects when compositing them into backgrounds to better integrate them into the environment, while [99] explored controllable lighting editing, as shown in Figure 15.\n2) Music & Sound Effects: Audio plays an essential role in animation production, creating an immersive experience and engaging the audience in the story being told. Specifically, audio can be categorized into three primary sources: speech, music, and sound effects. Among these, speech, commonly pre-recorded, undergoes integration during the dubbing process. This section focuses on the production of music and sound effects."}, {"title": null, "content": "Music Generation: Music in animation often serves to amplify character emotions and enhance the atmosphere. For instance, an energetic song can heighten the intensity of a battlefield scene, while a melancholic tune can evoke empathy and draw the audience into a poignant narrative. Recently, AI-generated music has emerged as a significant area of innovation, attracting considerable attention for its potential to revolutionize audio production in animation. A prominent approach in this domain involves generating music from text descriptions, leveraging techniques such as diffusion models [144]\u2013[146] or sequence-to-sequence modeling [147]\u2013[149]. However, text-guided music generation often proves suboptimal for video, where synchronization between visual content and generated music is critical. This limitation has spurred advancements in video-guided audio generation.\nRelevant to this survey is the development of video-guided music generation methods, including FoleyMusic [114], Video2Music [115], V2Meow [116], MelFusion [117], and VidMuse [118]. While these methods are not specifically tailored for anime music generation, they offer valuable, generalizable tools that lay the foundation for future research in AI-driven animation.\nSound Effects Generation: Sound effects encompass a wide range of non-speech and non-music sounds from daily life, such as animal noises, traffic sounds, clanging or hitting effects and more. These sounds are typically specific to the scenes depicted in the video, presenting significant challenges in video-audio alignment. Specifically, generating sound effects that match the semantic content of the scene and synchronizing them with precise timing is crucial to achieve harmony between the audio and visual elements. Several approaches have addressed this challenge using auto-regressive transformer models to generate audio from visual features, including methods such as SpecVQGAN [119], Im2Wav [120], and FoleyGen [121]. Diffusion models have also gained popularity in this domain [122, 123, 150].\nBy leveraging automatic tools for generating audio synchronized with video content, AI-driven animation holds the"}, {"title": null, "content": "potential to significantly reduce human labor while enriching the audio-visual experience.\n3) Dubbing (DB): In animation production, dubbing encompasses the broader process of ensuring synchronization between audio and visual elements, particularly focusing on the correspondence between speech and character movements. Traditionally, this process follows two main approaches: after-recording, where the animation is adjusted to match pre-recorded voice acting, and dubbing, which involves post-production audio processing to synchronize with existing animations. Recent AI-powered dubbing models like EmotiVoice [124] have demonstrated the potential to automate this synchronization process. These systems employ a comprehensive approach: first detecting and isolating the character's facial region, then analyzing the target audio to guide the deformation or regeneration of facial features, particularly mouth movements and expressions. The process involves analyzing silent video, text/subtitles, and reference audio to generate emotionally expressive speech that aligns with the visual elements. These models achieve natural synchronization by combining acoustic characteristics from reference voices, pronunciation patterns from phoneme-level analysis, and emotional expressions derived from visual cues, enabling the generation of synchronized and emotionally appropriate speech-motion correspondence for animated characters.\n4) Cutting: Cutting refers to the process of arranging shots produced during the photography phase according to the storyboard script, and adjusting the duration, rhythm, and sequence of shots based on directorial intent to ensure proper pacing and visual presentation while conforming to television format and runtime requirements. Currently, there is no GenAI work specifically focused on animation cutting. As shown in"}, {"title": "D. Others", "content": "1) Cel-Animation Editing: Recent advances in context-aware translation have enabled more precise and controllable editing of cel-animations. Re:Draw [130] proposes a novel framework that combines the benefits of inpainting and image-to-image translation while respecting both original content and contextual relevance. This approach allows for local edits while maintaining global consistency, particularly useful for enhancing details like character eyes without compromising the overall animation style. The method employs dual discriminator structures and novel adversarial losses to ensure both artistic control and contextual coherence, demonstrating significant improvements over traditional editing approaches in preserving animation consistency. ScalingConcept [131] introduces a training-free editing method for images using pre-trained text-guided diffusion models. Among its diverse applications, one is specifically tailored for anime editing. During the photography and post-production stages of anime production, cumulative errors in line processing can lead to blurred lines, causing the image to appear fuzzy. This issue is often exacerbated by filters used in scenes like sunsets, which cannot be resolved by merely increasing the resolution or bitrate of the anime. With the ScalingConcept method, such issues can be addressed, resulting in a significant improvement in the overall visual quality of the anime.\n2) Cels Decomposition: Decomposing animation into individual layers or \u201csprites\u201d is crucial for both analysis and editing. Sprite-from-Sprite [132] introduces a self-supervised framework that can automatically decompose cartoon ani-"}, {"title": null, "content": "mations into reusable sprite elements. The method recognizes that sprites in real-world cartoons can vary between simple, computationally-tractable animations and complex, hand-drawn sequences. Similarly, Generative Omnimatte [133] advances this concept by incorporating a generative video prior to handle dynamic backgrounds and complete occluded regions, enabling more robust decomposition of animation elements while preserving associated effects like shadows and reflections.\n3) 3D Assistance: Three-dimensional assistance tools have emerged to support traditional cel-animation production. DrawingSpinUp [135] introduces a method for automatically animating character drawings in 3D space, enabling more complex movements while maintaining artistic integrity. Likewise, Toon3D [134] focuses on reconstructing 3D scenes from cartoon drawings, allowing for novel viewpoint generation and better planning of complex animation sequences. These approaches bridge the gap between 2D and 3D animation techniques, providing artists with tools to create more dynamic sequences while preserving the distinctive qualities of hand-drawn animation."}, {"title": "IV. DISCUSSION", "content": "Despite the advancements and potential applications of GenAIs in Cel-Animation, several limitations hinder their effectiveness and adoption.\n1) Expressiveness and Prompt Limitations: The reliance on natural language prompts to direct generative models [60, 66] often fails to encapsulate the nuances of Cel-Animation. Elements such as exaggerated expressions, dynamic perspectives, and intricate actions are difficult to describe effectively in textual form, limiting the scope of creativity achievable through prompt-based generation.\n2) Insufficient Reference Utilization: Current GenAIs struggle to incorporate references like color charts and settings of items, characters, or scenes. This limitation results in outputs that lack fidelity to established design elements, creating inconsistencies that require additional manual intervention.\n3) Background Integration and Input Deviations: Current models for tasks like colorization and inbetweening produce results with backgrounds, which complicates post-generation editing. Additionally, the input sketches for colorization or inbetweening are frequently derived from models rather than original keyframe animation.\n4) Limited Control and Artistic Freedom: While automation provides efficiency, it limits the granular control artists can exercise over the final product. Full reliance on prompt-based systems undermines the creative autonomy that animators value, as these systems fail to allow detailed adjustments at varying levels of granularity.\n5) Legal and Copyright Concerns: The use of Cel-Animations for training may raise copyright issues, which pose ethical and legal challenges for widespread adoption."}, {"title": "B. Future Directions", "content": "This section examines future directions and highlights key areas that warrant further improvement. As noted earlier, contemporary GenAI methods have been successfully applied to pre-production, production, and post-production processes and have achieved promising results. However, these approaches have yet to address all of the fundamental challenges inherent in cel-animation. In this study, we discuss four key areas that can be improved by data curation and model design."}, {"title": null, "content": "1) Building Large-Scale and Comprehensive Celluloid Animation Dataset: Datasets play an important role in building robust celluloid animation production models. Although some datasets are available [85, 98, 151, 153], they remain limited for several critical processes, leaving gaps that hinder further advancement in this domain. For instance, script generation frequently relies on text corpora derived from fiction and cartoons, but such resources rarely capture the nuanced interplay between visual storytelling and narrative structure unique to hand-drawn animation. As a result, many GenAI models struggle to generate consistent, contextually rich scripts that can be directly integrated into a production pipeline. A promising way to address these limitations is to develop new large-scale, comprehensive datasets that mirror real-world animation workflows. Such datasets should not merely expand in scale but also diversify in content to cover a broad range of animation styles, genres, and production stages. For example, by incorporating both preliminary sketches and final colored frames along with corresponding scripts or dialogue-researchers can more effectively train models to understand the full spectrum of visual and narrative elements that define hand-drawn animation.\n2) Advanced LLM-based Content Generation Methods: While there have been numerous efforts to use LLMs for script generation, existing approaches have limitations in the use of LLMs and have not incorporated multimodal content understanding. Recent advancements in MLLMs [5, 7, 8, 38, 155] have demonstrated exceptional performance in vision-language tasks [156]\u2013[160], underscoring the necessity of holistic, multi-phase methods that seamlessly integrate textual and visual elements. Such approaches will facilitate Cel-Animation with MLLMs' powerful multimodal understanding and reasoning capabilities. A key challenge lies in aligning textual narrative with the fine-grained visual components that define Cel-Animation, such as character expressions, background design, and color schemes. Existing methods often treat these tasks independently (e.g., generating a script first and later producing accompanying images) leading to potential mismatches in style, tone, or narrative context. Instead, an advanced LLM-based approach could employ multimodal reasoning, where scene descriptions and dialogue generation run in tandem with storyboard sketches, character poses, and layout proposals. By iteratively refining both text and images together, the framework ensures that each creative decision ranging from dialogue shifts to lighting changes is consistently reflected across the entire production pipeline.\n3) Unified Multiprocess Generation Frameworks: Ensuring coherence in model-generated animation content is a fundamental requirement. Certain processes in the celluloid animation production workflow can be unified within a single model, thereby enhancing consistency across each stage. Recent advances in autoregressive multimodal models [161]\u2013[164] have further enabled language models to generate interleaved text and images, paving the way for more streamlined production pipelines. By producing storyline elements alongside corresponding visuals in a single pass [165]\u2013[167], these models can streamline traditionally separate tasks such as script generation, storyboard layout, and character design."}, {"title": null, "content": "For instance, while generating a dialogue, the same model can simultaneously propose camera angles or background sketches, ensuring textual and visual narratives evolve together rather than in isolation. These interleaved frameworks"}]}