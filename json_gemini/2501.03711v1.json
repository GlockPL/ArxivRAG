{"title": "Unsupervised Speech Segmentation: A General Approach Using Speech Language Models", "authors": ["Avishai Elmakies", "Omri Abend", "Yossi Adi"], "abstract": "In this paper, we introduce an unsupervised approach for Speech Segmentation, which builds on previously researched approaches, e.g., Speaker Diarization, while being applicable to an inclusive set of acoustic-semantic distinctions, paving a path towards a general Unsupervised Speech Segmentation approach. Unlike traditional speech and audio segmentation, which mainly focuses on spectral changes in the input signal, e.g., phone segmentation, our approach tries to segment the spoken utterance into chunks with differing acoustic-semantic styles, focusing on acoustic-semantic information that does not translate well into text, e.g., emotion or speaker. While most Speech Segmentation tasks only handle one style change, e.g., emotion diarization, our approach tries to handle multiple acoustic-semantic style changes. Leveraging recent advances in Speech Language Models (SLMs), we propose a simple unsupervised method to segment a given speech utterance. We empirically demonstrate the effectiveness of the proposed approach by considering several setups. Results suggest that the proposed method is superior to the evaluated baselines on boundary detection, segment purity, and over-segmentation. Code is available at https://github.com/avishaiElmakies/ unsupervised_speech_segmentation_using_slm.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech segmentation is the task of breaking a speech signal into individual sound units. It plays an essential role in a variety of speech and audio applications such as Automatic Speech Recognition [1], [2], speaker diarization [3], and speech science [4], [5]. Most prior work on speech segmentation focused on spectral changes in the spoken utterance, as a result, it mainly captures phone or phoneme segments [2], [6]\u2013[10]. Although much progress has been made with spectral segmentation approaches, most of these methods focus on a single acoustic-semantic style change, e.g., emotion diarization [11], usually using supervised methods.\nSpeech Language Models (SLMs) are a promising research direc- tion in the field of speech and audio processing [12]\u2013[15]. SLMs first represent speech and audio signals as discrete acoustic units, on which a language model is applied to maximize the sequence likelihood [12], [16]. This method was shown to be beneficial in several speech modeling and generation tasks [12], [13], [15]. Improving the semantic and acoustic abilities of SLMs is an ongoing topic of research in the field [17]. The ability to directly obtain good estimates of the distributions of speech signals without converting them into text allows us to use SLMs for applications that cascading methods are not well equipped to handle, e.g., emotion-related tasks.\nIn this work, we define an unsupervised approach for speech segmentation, focusing on acoustic-semantic style changes that do not translate well into text. We leverage the progress in the field of SLMs and show a simple pipeline that tackles this new approach for unsupervised speech segmentation. Specifically, we first break the au- dio into segments (\"acoustic-sentences\"). Then, we score consecutive sentences using the probabilities obtained from the SLMs. Finally, we select acoustic-sentences to merge using the calculated scores.\nWe examine the proposed approach considering two case studies of"}, {"title": "II. BACKGROUND", "content": "In speech segmentation, we get a speech signal $s \\in \\mathbb{R}^T$, where T is the length of the sequence, and divide it into m segments $y_1,..., y_m$, where the length varies between each $y_i$. In our setup, we aim to segment the speech utterance into acoustic-semantic style segments. As we consider the unsupervised setup, the acoustic-semantic style segments can have different forms, e.g., a change of emotion, a change of gender, a change of speaker, etc.\nLanguage models (LMs) have been a topic of recent discussion in various fields of AI, ranging from NLP [18] to visual LMs [19]. Recently, Speech Language Models (SLMs) also emerged as an interesting line of research [12], [13], [17].\nSLMs operate similarly to textual LMs using next-token prediction. However, unlike text, speech signals are continuous in nature; hence, it is not clear how to represent speech in a discrete manner. One com- mon approach to mitigate this, is to quantize contextualized speech representation obtained from a pre-trained self-supervised model [12], [13], [15]. This is often done using the k-means algorithm. The tokens generated following the aforementioned approach have been shown to correlate with phonemes [20]. Next, such speech tokens can be directly used for the task of next-token prediction in a similar way to textual LMs. Using both a discretization model and a language model allows us to approximate the distribution of our data, for an"}, {"title": "III. METHOD", "content": "In this section, we introduce a simple and flexible pipeline for tackling Unsupervised Speech Segmentation. This pipeline is inspired by text-based segmentation methods, which work on sentences and scores to segment the text [21]. The pipeline can be seen in Fig. 1 and is composed of 3 parts: (a) a sentencer that splits the audio into segments (\"acoustic-sentences\"), (b) a scorer that scores consecutive sentences, and (c) a span-selector that uses the scores and the sentences to select the final spans/segments."}, {"title": "A. Sentencer", "content": "Similarly to [21] we use a naive and straightforward approach to perform the initial segmentation. We segment the audio into equally sized segments, which we denote as acoustic-sentences. This process can be considered as an initial guess for the segments, where in later stages we will refine such segmentation."}, {"title": "B. Scorer", "content": "Following [21] we use the Point-Wise Mutual Information (PMI) which is defined for two sentences x, y as follows:\n$PMI(x, y) = log \\frac{P(x, y)}{P(x) P(y)}$ (2)\nWe use this score since PMI is a measure of association, it compares the probability of x, y showing one after the other to what this probability would be if x and y were independent. If the PMI is small or negative, it suggests that x and y are more likely to be independent, making the placement of a boundary between them a reasonable hypothesis. PMI has also been shown to work well for segmentation and boundary detection in other fields [21], [22].\nAs we have seen, using SLMs we can approximate $P_{real}$ of audio, hence we can use both (1) and (2) to define the following score:\n$score(y_i, y_{i+1}) = log \\frac{P_{\\theta}(f_{\\phi}(y_i, y_{i+1}))}{P_{\\theta}(f_{\\phi}(y_i)) \\cdot P_{\\theta}(f_{\\phi} (y_{i+1}))}.$ (3)\nLater, this score will be used to select the number of segments and what acoustic-sentences should be merged to reach the final segmentation of the speech utterance."}, {"title": "C. Span selector", "content": "We present three methods for selecting both the number of seg- ments (denoted by k) and the segments themselves using the scores obtained from the Scorer."}, {"title": "Constant number of segments.", "content": "Under this method, we first set the value of k. Then, boundaries are placed on the k - 1 smallest scores to yield k segments. We denote this method as C(k). We explored values of $k \\in \\{10, 15, 20\\}$."}, {"title": "Adaptive number of segments.", "content": "Here, the number of segments is based on the number of initial segments (i.e., acoustic-sentences) in the speech utterance. Meaning that each speech signal will have a varying amount of segments. The overall number of segments is defined as follows,\n$k = max(0, \\frac{m}{v} - 20) + 4$, (4)\nwhere m is the number of acoustic-sentences in a given signal and v represents the number of acoustic-sentences needed to increase the number of segments by 1. We subtract 20 from the number of acoustic-sentences to prevent over-segmentation for short files and include 4 as the minimum number of segments. We select those numbers assuming less than 10 seconds of speech (with acoustic-sentences of 0.5 seconds) is short for semantic segmentation of more than 4 segments. Finally, following (4) we get k and choose the boundaries as seen previously. We denote this method as A(v). We experimented with $v \\in \\{5, 10, 15, 20\\}$."}, {"title": "Threshold.", "content": "The last span selection approach also selects the number of segments, k, dynamically. First, a threshold t is used, then all scores smaller than t are selected as boundaries. We denote this method with T(t). We experimented with $t \\in \\{-5, -8, -10, -12.5, -15\\}$."}, {"title": "IV. DATASETS", "content": "We use two benchmarks to create synthetic datasets, which will be used to evaluate the proposed method: (i) The Emotional Voices Database (EmoV-DB) [23] data set contains English speech record- ings. The recordings were obtained from four speakers (two male and two female) and contains five different emotions for most of the speakers (i.e. Neutral, Amused, Angry, Sleepy, and Disgust); (ii) The Interactive Emotional Dyadic Motion Capture (IEMOCAP) [24] dataset which contains 12 hours of English audio-visual data (impro- vised and scripted) from ten speakers (5 men, 5 women). We use the emotions of happy (we converted excited to happy as they are highly similar and hard to differentiate), sad, angry and neutral.\nGiven these two benchmarks, we concatenate recordings from the same or different emotions to simulate acoustic-semantic style changes. We focus on the aspects of emotional style and gender style."}, {"title": "Experiment 1: change of emotion.", "content": "In the first experiment, we focus on the change of emotion as our acoustic-semantic style change. For this, we extract the relevant utterances for each speaker and concatenate them. The number of segments is randomly chosen between 4 and 30, where the segments for each file are randomly chosen. All speech signals are resampled to 16kHz whenever needed. For EmoV-DB we generated 2000 files (500 files for each speaker) while for IEMOCAP 2500 we generated 2500 files (250 per speaker)."}, {"title": "Experiment 2: change of gender.", "content": "For this experiment, we focus on the change of gender. For each speaker, we group their utterances based on the utterances emotions. In IEMOCAP we select speakers based on the sessions in the dataset(each session contains one male and one female), while in EmoV-DB we select both speakers of the opposite gender equally. Later on, we select a random emotion. From the group of utterances for the emotion we select random segments for both speakers (in the range between 4 to 30). The files for both speakers are resampled to 16kHz whenever needed and concatenated in an alternating order. We verify that there are an equal number of files that start with male and female. For EmoV-DB we generated 2000 files (250 for each combination) while for IEMOCAP we generated 2500 files (500 for each pair)."}, {"title": "V. EXPERIMENTAL SETUP", "content": "We leverage the pre-trained TWIST SLM model as described in [15], which were optimized over ~ 200k hours of spoken data. All results in this paper are reported using the 350M parameter version. We experimented with larger models (i.e., 1.3B and 7B), however we did not observe significant improvements. The spoken data was first discretized using a k-means quantizer optimized over representations extracted from a pre-trained HuBERT model [15]."}, {"title": "A. Baselines", "content": "We evaluate the proposed method against the following baselines:"}, {"title": "Equal Length (EL) Segmentor.", "content": "The first baseline is the simplest one, segmenting the audio into equal-length segments. We test this approach using both a constant number of segments and an adaptive approach to the number of segments."}, {"title": "Contrastive Learning Scorer (CLS).", "content": "Next, we examine the effect of a more advanced method. We explore the usage of a scoring method as described in III-B. For that, we leverage a self-supervised phoneme segmentation model based on Constractive Learning to score frames. Specifically, we consider the model proposed by [25]. Scoring is done via cosine similarity between adjacent frames as suggested by [25]."}, {"title": "Diarization Methods (DM)", "content": "We additionally compare the proposed approach to SOTA supervised methods used for a single acoustic- semantic style change: (i) Speaker Diarization (SD) [26]; (ii) Emotion Diarization (ED) [11]. While ED needed no changes since it gives segmentation as defined above, SD needed hyper-parameters calibration to better match our setup (i.e., SD often introduces small and unnatural segments during speaker activity which causes over- segmentation). For that, we removed segments shorter than 0.25s and combined consecutive segments that were predicted to have the same speaker with a distance of less than 0.5s."}, {"title": "B. Evaluation", "content": "We evaluate the proposed method considering three different eval- uation metrics: (i) Recall Precision F1 (RP-F1), where we compute the recall and precision of boundary selection of the methods and compute their F1 score. For this metric we consider a tolerance of 0.5 seconds; (ii) R-Value, as proposed by [25], [27]. We additionally consider the R-Value score to mitigate the sensitivity of the F1 score to over-segmentation; (iii) Purity Coverage F1 (PC-F1). As suggested by Bredin et al. [28], similarly to clustering, one can use segment- wise purity and coverage to test our segmentation. We include these metrics for completeness. It is important to note that only the ED model was trained on the EMOV-DB and IEMOCAP datasets."}, {"title": "VI. RESULTS", "content": "Results are summarized in Table I. Results suggest that the pro- posed approach is superior to the evaluated baselines considering both PR-F1 and R-Val, meaning the proposed approach is better at finding segmentation boundaries while not performing over-segmentation. As for PC-F1, the proposed approach achieves inferior performance to"}, {"title": "VII. ABLATION", "content": "Ablation results are presented in Fig. 2. We present results using EmoV-DB only, as results for IEMOCAP show similar trends. We report PR-F1 and R-Val only as we observe a high positive correlation (0.55) between PR-F1 and PC-F1.\nThe effect of k. Fig. 2(a) visualizes the effect of changing the number of segments, k, in a constant span selector. Results suggest that increasing the number of segments slightly increases PR-F1 while decreasing R-Val. This is especially noticeable in emotion change (experiment 1), where the decrease in R-val is much sharper than the decrease in gender change (experiment 2). This drop in performance is due to the samples in experiment 1 usually having fewer segments, which results in a much lower R-Val when increasing k.\nThe effect of v. Fig. 2(b) presents the effect of increasing v on \u0391(v) span selector. Results suggest that increasing v slightly lowers the PR-F1 while significantly improving R-Val. Using v = 5 yields a very low R-val. However, when increasing v to 10, R-Val increases drastically (at a slight cost of PR-F1). A(10) seems to be the best at balancing between PR-F1 and R-Val.\nThe effect of t. Fig. 2(c) presents the effect of changing the threshold parameter in the threshold span selector. We observe that increasing the threshold increases PR-F1 in both experiments. The threshold seems to have a different effect on R-Val. Increasing the threshold increases R-val up to some point, in which it starts to have a negative effect. T(-10) seems to be the most balanced threshold.\nThe effect of acoustic-sentence size: In Fig. 2(d) we visualize the effect of changing the acoustic-sentences sizes. Results suggest that increasing the size of the acoustic-sentences decreases both PR-F1 and R-val, where the size of 0.5s reaches the best performance."}, {"title": "VIII. DISCUSSION", "content": "We presented a new approach for Unsupervised Speech Segmen- tation and propose a simple and efficient (no training is needed) unsupervised baseline for the approach. We used two data sets and defined four benchmarks that can be used to test and evaluate the per- formance of different models on this new approach. We empirically show that this pipeline is superior to the evaluated baselines under most metrics while leaving room for improvement in future work. The pipeline also seems to work better for the new approach than pipelines designed for a single acoustic-semantic use case. Our work presents the first steps in exploring this new approach. We believe and hope this proposed approach will be interesting and valuable to the spoken language modeling community, as it captures higher levels of acoustic-semantic style in speech signals, which may also be unique properties of speech. We believe improvements made on this approach may be beneficial in developing hierarchical speech models, spoken dialogue systems, and spoken language understanding.\nLimitations. The proposed research has three main limitations. (i) Inference time may be relatively long. The proposed pipeline leverages a 350M parameter SLM which requires about 1-2 hours of processing per experiment using a 24GB GPU (A5000). (ii) Although simple and efficient, following the proposed approach to convert raw speech into equally sized acoustic-sentences, is far from ideal and limits the performance of the overall system. (iii) This work defines and sets a benchmark for this approach of Unsupervised Speech Segmentation. However, the datasets we provide are relatively simple and focus only on two acoustic-semantic style changes (emotion and gender). Creating a larger dataset with more complex acoustic-semantic style changes could be an interesting next step that will benefit the community.\nFuture work. For future work we plan to mitigate most of the limitations described in the paragraph above. Specifically, we plan to explore variable-length acoustic-sentences by first performing segmentation via common methods, such as [25]. Additionally, we would like to explore and develop more advanced SLMs directly ded- icated for this speech segmentation approach, incorporating prosodic features as part of the modeling pipeline (e.g., F0, duration)."}]}