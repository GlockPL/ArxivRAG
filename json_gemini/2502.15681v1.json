{"title": "One-step Diffusion Models with f-Divergence Distribution Matching", "authors": ["Yilun Xu", "Weili Nie", "Arash Vahdat"], "abstract": "Sampling from diffusion models involves a slow iterative process that hinders their practical deployment, especially for interactive applications. To accelerate generation speed, recent approaches distill a multi-step diffusion model into a single-step student generator via variational score distillation, which matches the distribution of samples generated by the student to the teacher's distribution. However, these approaches use the reverse Kullback\u2013Leibler (KL) divergence for distribution matching which is known to be mode seeking. In this paper, we generalize the distribution matching approach using a novel f-divergence minimization framework, termed f-distill, that covers different divergences with different trade-offs in terms of mode coverage and training variance. We derive the gradient of the f-divergence between the teacher and student distributions and show that it is expressed as the product of their score differences and a weighting function determined by their density ratio. This weighting function naturally emphasizes samples with higher density in the teacher distribution, when using a less mode-seeking divergence. We observe that the popular variational score distillation approach using the reverse-KL divergence is a special case within our framework. Empirically, we demonstrate that alternative f-divergences, such as forward-KL and Jensen-Shannon divergences, outperform the current best variational score distillation methods across image generation tasks. In particular, when using Jensen-Shannon divergence, f-distill achieves current state-of-the-art one-step generation performance on ImageNet64 and zero-shot text-to-image generation on MS-COCO.", "sections": [{"title": "1. Introduction", "content": "Diffusion models [12, 58] are transforming generative modeling in visual domains, with impressive success in generating images [1, 43, 44], videos [13, 53], 3D objects [32, 75], motion [74, 76], etc. However, one of the key limitations of deploying diffusion models in real-world applications is their slow and computationally expensive sampling process that involves calling the denoising neural network iteratively.\nEarly works on accelerating diffusion models relied on better numerical solvers for solving the ordinary differential equations (ODEs) or stochastic differential equations (SDEs) that describe the sampling process of diffusion models [15, 17, 29, 54, 67]. However, these methods can only reduce the number of sampling steps to around tens of steps, due to the discretization error, accumulated with fewer steps.\nMore recently, distillation-based approaches aim at the ambitious goal of reducing the number of sampling steps to a single network call. These approaches can be generally grouped into two categories: 1) trajectory distillation [9, 23, 28, 55, 59] which distills the deterministic ODE mapping between noise and data intrinsic in a diffusion model to a one-step student, and 2) distribution matching approaches [71, 72, 79, 82] that ignore the deterministic mappings, and instead, matches the distribution of samples generated by a one-step student to the distribution imposed by a pre-trained teacher diffusion model. Among the two categories, the latter often performs better in practice as the deterministic mapping between noise and data is deemed complex and hard to learn. Naturally, the choice of divergence in distribution matching plays a key role as it dictates how the student's distribution is matched against the teacher's. Existing works [71, 72] often use variational score distillation [64] that matches the distribution of the student and teacher by minimizing the reverse-KL divergence. However, this divergence is known to be mode-seeking [2] and can potentially ignore diverse modes learned by the diffusion model.\nIn this work, we propose a novel generalization of the distribution matching distillation approach using the f-divergence, termed f-distill. The f-divergence represents a large family of divergences including reverse-KL, forward-KL, Jensen-Shannon (JS), squared Hellinger, etc. These divergences come with different trade-offs on how they penalize the student for missing modes in the teacher distribution and how they can be estimated and optimized using Monte Carlo sampling. Within our framework, we evaluate various f-divergences based on these properties and observe different tradeoff. For instance, forward-KL has a better mode coverage, but has a large gradient variance; JS demonstrates moderate mode-seeking and gradient saturation, particularly in early training stages, but exhibits low variance. Our analysis reveals that no single f-divergence consistently outperforms others across all datasets. We observe divergences with better mode coverage tendencies generally perform better on the CIFAR-10 dataset. However, on"}, {"title": "2. Background", "content": "The goal of f-distill is to accelerate the generation of pre-trained (continuous-time) DMs [12, 58]. In this paper, we follow the popular EDM framework [17] for the notations and forward/backward processes. DMs perturb the clean data x0 ~ Pdata in a fixed forward process using \u03c3\u00b2(t)-variance Gaussian noise, where x \u2208 Rd and t denotes the time along the diffusion process. The resulting intermediate distribution is denoted as pt(xt) with xt \u2208 Rd. For notation simplicity, we will use x to replace xt, unless stated otherwise, throughout the paper. For sufficiently large \u03c3max, this dis-"}, {"title": "2.1. Diffusion models", "content": "tribution is almost identical to pure random Gaussian noise. DMs leverage this observation to sample the initial noise \u2208max ~ N(0, \u03c3maxI), and then iteratively denoise the sample by solving the following backward ODE/SDE, which guarantees that if \u03c3(0) = 0, the final x follows the data distribution Pdata:\ndx = \u2212\u03b2(t)\u03c3(t) \\nabla_x \\log p_t(x)dt \\tag{1}\n\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\text{Probability Flow ODE}\n-\u03b2(t)\u00b2 \u03c3(t)\u2207x log pt(x)dt + \\sqrt{2\u03b2(t)\u03c3(t)}dwt,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\text{Langevin Diffusion SDE}\nwhere wt is a standard Wiener process and \u2207x log pt (x) is the score function of the intermediate distribution pt(x). The score function is learned by a neural network s4(x; \u03c3(t)) trained with the denoising score matching objective [56, 62]. In Equation (1), the first term is the Probability Flow ODE, which guides samples from high to low noise levels. The second term is a Langevin Diffusion SDE, which acts as an equilibrium sampler across different noise levels \u03c3(t), effectively refining the samples and correcting errors during the sampling process [17, 67]. This component can be scaled by the time-dependent parameter \u03b2(t). Setting \u03b2(t) = 0 leads to pure ODE-based synthesis. However, solving the diffusion ODE and SDE typically involves a considerable number of iterations (often tens or hundreds), posing a significant challenge to the practical deployment of diffusion models. Although different kinds of accelerated samplers for diffusion ODE [17, 29, 54] and SDE [15, 17, 67] have been proposed, they usually still require > 20 sampling steps in practice to produce decent samples."}, {"title": "2.2. Variational score distillation", "content": "A recent line of works [71, 72] aim to distill the teacher diffusion models so into a single step generator Ge, through variational score distillation (VSD), which is originally introduced for test-time optimization of 3D objects [64]. The goal is to enable a student model Go to directly map the noise z from the prior distribution p(z) = N(z; 0, I) to the clean sample xo at \u03c3 = 0 using xo = Go(z), effectively bypassing the iterative sampling process. Let p$ denote the distribution obtained by plugging in pre-trained diffusion models 84(x; \u03c3(t)) in Equation (1), and let qe denote the output distribution by the one-step generator Go (in the following text, we drop the subscript in pp and qe for notation simplicity). Then, the gradient update for the generator can be formulated as follows:\nEt,z,e [(84(x; \u03c3(t)) \u2013 \u2207x log qe (x; \u03c3(t))) VeGe(z)] \\tag{2}\nwhere x = Go(z) + \u03c3(t)e and \u2208 ~ N(0, I). Intuitively, the gradient encourages the generator to produce samples that lie within high-density regions of the data distribution. This is achieved through the teacher score term, s$(x; \u03c3(t)), which guides the generated samples towards areas where the teacher model assigns high probability. To prevent mode collapse, the gradient also incorporates a term that discourages the generator from simply concentrating on a"}, {"title": "2.3. f-divergence", "content": "In probability theory, an f-divergence [41] quantifies the difference between two probability density functions, p and q. Specifically, when p is absolutely continuous with respect to q, the f-divergence is defined as:\nDf(p||q) = \u222b q(x)f(p(x)/q(x)) dx\nwhere f is a convex function on (0, +\u221e) satisfying f(1) = 0. This divergence satisfies several important properties, including non-negativity and the data processing inequality. Many commonly used divergences can be expressed as special cases of the f-divergence by choosing an appropriate function f. These include the forward-KL divergence, reverse-KL divergence, Hellinger distance, and Jensen-Shannon (JS) divergence.\nIn generative learning, f-divergence has been widely applied to popular generative models, such as GANs [37], VAEs [63], energy-based models [73] and diffusion models [60]."}, {"title": "3. Method: general f-divergence minimization", "content": "In this section, we introduce a general distillation framework, termed f-distill, based on minimizing the f-divergence between the teacher and student distributions. Since the student distribution q is the push-forward measure induced by the one-step generator Go, it implicitly depends on the generator's parameters \u03b8. Due to this implicit dependency, directly calculating the gradient of f-divergence, Df(p||q), w.r.t \u03b8 presents a challenge. However, the following theorem establishes the analytical expression for this gradient, revealing that it can be formulated as a weighted version of the gradient employed in variational score distillation. Notably, these weights are determined by the density ratio of the generated samples. We state the theorem more generally by providing the gradient for pt and qt, where pt is the perturbed distribution through the diffusion forward process for the"}, {"title": "4. Comparing properties of f-divergence", "content": "In this section, we compare the properties across different distance measures in the f-divergence family, in the context of diffusion distillation. We will inspect their three properties: mode-seeking, saturation, and variance during training. We summarize the comparison of different fs, and their corresponding weighting function h, in Table 1.\nMode-seeking. Mode-seeking divergences [2, 24], such as reverse-KL, encourage the generative distribution q only to capture a subset of the modes of data distribution, and tend to avoid assigning probability mass to regions of low data density when solving minq Df(p||q) = \u222b qf(p/q)dx. This behavior, however, is undesirable for generative models"}, {"title": "5. Experiment", "content": "We evaluate f-distill on CIFAR-10 [21] and ImageNet 64\u00d764 [7] for class-conditioned image generation, and on zero-shot MS COCO 2014 [25] for text-to-image generation. We use COYO-700M [4] as the training set for text-to-image generation. We use pre-trained models in EDM [17] as teachers for CIFAR-10 / ImageNet-64, and Stable Diffusion (SD) v1.5 [42] for text-to-image generation. For hyper-parameters, we use a batch size of 2048 / 512 / 1024 for CIFAR-10 / ImageNet-64 / COYO-700M. As DMD2 [71] is a special case (reverse-KL) under the f-distill framework, we borrow their tuned hyper-parameters, including learning rates, CFG guidance weight, update frequency for fake score and discriminator, and the coefficient for GAN loss in generator update. We posit that hyperparameters tuned for reverse-KL should generalize effectively to other divergences. In the text-to-image experiment, we observe that the estimation of density ratio (and thus the weighting function h) by the discriminator is inaccurate at the early stage of training. To address this, we \"warm up\" the discriminator by initializing the model with a pre-trained reverse-KL model, which has a constant h. We defer the training details to Section B in the supplementary material.\nOur baseline comparisons include multi-step diffusion models and existing diffusion distillation techniques. We also re-implemented DMD2 [71] (reverse-KL) within our codebase. Furthermore, to isolate the impact of our proposed f-distill objective, we conducted an ablation study by removing it and training solely with the GAN objective (denoted"}, {"title": "5.1. Image generation", "content": "as \"Adversarial distillation\" in the tables).\nEvaluations. We measure sample quality with Fr\u00e9chet Inception Distance (FID) [11]. For diversity, we use the Recall score [22]. For image-caption alignment, we report the CLIP score. We defer more evaluations to Appendix D.2 and D.3 on diversity and image quality.\nResults. We first experiment with all the f-divergences in Table 1 on CIFAR-10. Table 2 shows that (1) all the variants under f-distill outperform the adversarial distillation baseline, validating the effectiveness of distributional matching by f-distill in addition to GAN objective. (2) f-divergences with milder mode-seeking behavior generally yield better performance. Specifically, forward-KL and Jeffreys divergences, which lack mode-seeking properties, achieve significantly lower FID scores, and higher Recall scores, than divergences with mode-seeking characteristics (e.g.,, reverse-KL, softened RKL). We also list some recent works for comparison. Note that GDD-I and CTM utilize external pre-trained feature extractors and extensive tuning for their GAN objective. In contrast, our approach simply employs the teacher model as feature extractor, as we primarily use this dataset to analyze the relative performance of different fs.\nTable 3 and Table 4 report FID, Recall and CLIP score in two more challenging datasets. We report the inference latency on a single NVIDIA A100 GPU for fair comparison on text-to-image generation, as in [71]. Our main findings are: (1) f-distill with JS divergence achieves the current state-of-the-art one-step FID score on both ImageNet-64 and zero-shot MS COCO. Concretely, JS"}, {"title": "5.2. Behavior of h in non-mode-seeking divergences", "content": "As discussed in Section 4, f-divergence with medium or no mode-seeking property has a faster increasing second derivative f\", resulting in an increasing weighting function h(r) = f\"(r)r\u00b2. This means that generated samples in the low-density regions of the data distribution p will be down-weighted accordingly, and the teacher models are prone to inaccurate score estimation in these regions [18].\nTo further understand the behavior of h, we study its relation with the score difference between a teacher and fake score, i.e., ||s$(x; \u03c3(t)) \u2212 sy(x,\u03c3(t))||2 on real datasets. Recall that the teacher s approximates the true score, i.e., 84(x; \u03c3(t)) \u2248 \u2207x log pt(x), and the online fake score approximate the generated distribution, i.e., sy(x, \u03c3(t)) \u2248 \u2207x log qt (x). We compute both h and the score difference for 6.4k generated samples and sort them in ascending order of their score difference. Fig. 5 shows that the sample's weighting h generally goes in the opposite direction with its score difference when using non-mode-seeking divergences. This observation suggests that when using non-mode-seeking divergences, f-distill effectively downweights samples in regions where the teacher and fake scores exhibit substantial discrepancies, which typically correspond to low-density regions of the true data distribution."}, {"title": "6. Related work", "content": "As the sampling process in diffusion models is essentially solving the ODEs or SDEs [58], many early works focus on reducing the sampling steps with faster numerical solvers [17, 26, 30, 54, 81]. However, they usually still require more than 20 steps due to the discretization error. Diffusion distillation has recently attracted more attention due to its promising goal of reducing the number of sampling steps to one single network call. It mainly includes two classes of distillation approaches:\n(1) Trajectory distillation, which trains a one-step student model to mimic the deterministic sampling process of the teacher diffusion model. Knowledge distillation [31, 80] learns a direct mapping from noise to data. Progressive distillation [35, 46] iteratively halves the number of sampling steps via distillation. Consistency models [9, 23, 28, 55, 59] lean a consistency function that maps any noisy data along an ODE trajectory to the associated clean data.\n(2) Distribution matching, which aligns the distribution of the one-step student with that of the teacher diffusion model. Adversarial distillation [50, 51, 70] mainly relies on the adversarial training [10] to learn teacher output's distribution. Another line of approaches implicitly minimizes various divergences, often via variational score distil-"}, {"title": "7. Conclusions", "content": "We have proposed f-distill, a novel and general framework for distributional matching distillation based on f-divergence minimization. We derive a gradient update rule comprising the product of a weighting function and the score difference between the teacher and student distributions. f-distill encompasses previous variational score distillation objectives while allowing less mode-seeking divergences. By leveraging the weighting function, f-distill naturally downweights regions with larger score estimation errors. Experiments on various image generation tasks demonstrate the strong one-step generation capabilities of f-distill."}, {"title": "A. Proofs", "content": "In this section, we provide proofs for Theorem 1 and Proposition 1 in the main text. We will start with Lemma 1 before proving Theorem 1.\nLemma 1. Assuming that sampling from x ~ qt(x) can be parameterized to x = Go(z) + \u03c3(t)e for z ~ p(z), \u2208 ~ N(0, I) and Go, g are differentiable mappings. In addition, g is constant with respect to \u03b8. Then \u222b\u2207oqt(x)g(x)dx = \u222b \u222bp(\u2208)p(z)\u2207xg(x)VoGo(z)dedz.\nProof. As qt and g are both continuous functions, we can interchange integration and differentiation:\n\u222b\u2207oqt(x)g(x)dx = Ve \u222b qt(x)g(x)dx\n= Ve\u222b \u222bp(\u2208)p(z)g(Ge(z) + \u03c3(t)e)dedz\n=\u222b \u222bp(\u2208)p(z) Veg(Ge(z) + \u03c3(t)e)dedz \\tag{5}\n= \u2212\u222b \u222bp(\u2208)p(z)\u2207xg(Ge(z) + \u03c3(t)e)VoGo(z)dedz\n=\u222b \u222bp(\u2208)p(z)\u2207xg(x)VoGo(z)dedz\nwhere x = Go(z) + \u03c3(t)e. We can interchange integration and differentiation again in Eq. (5) as g is a differentiable function.\nTheorem 1. Let p be the teacher's generative distribution, and let q be a distribution induced by transforming a prior distribution p(z) through the differentiable mapping Ge. Assuming f is twice continuously differentiable, then the gradient of f-divergence between the two intermediate distribution pt and qt w.r.t 0 is:\nVoDf(Pt||qt) = Ez,\u20ac\\left[f\"\\left(\\frac{p_t(x)}{q_t (x)}\\right) \\left(\\frac{p_t(x)}{q_t (x)}\\right)^2  \\nabla_x \\log p_t (x) - -\\nabla_xlog \\sqrt{x} \\log q_t q(x) (x) \\right) VaGa(z)\\right] \\tag{6}\nwhere z ~ p(z), \u0454 ~ N (0, I) and x = Go(z) + \u03c3(t)\u03b5\nProof. Note that both the intermediate student distribution qt and the sample x have a dependency on the generator parameter \u03b8. In the proof, we simplify the expression \u222b(\u2207oqt(x))g(x)dx as \u222b\u2207oqt(x)g(x)dx for clarity. The total derivative of f-divergence between teacher\u2019s and student\u2019s intermediate distribution is as follows:\nVoDf(Pt(x)||qt(x)) = Vo\\int q_t(x)f\\left(\\frac{p_t(x)}{q_t (x)}\\right)dx\n= \\int V_o q_t(x)f\\left(\\frac{p_t(x)}{q_t (x)}\\right)dx+\\int q_t(x)V_ef\\left(\\frac{p_t(x)}{q_t (x)}\\right)dx\n= \\int V_o q_t(x)f\\left(\\frac{p_t(x)}{q_t (x)}\\right)dx + \\int q_t(x) f'\\left(\\frac{p_t(x)}{q_t (x)}\\right) \\frac{p_t(x)}{q^2_t (x)} V_o q_t(x) dx\n= \\int V_o q_t(x)f\\left(\\frac{p_t(x)}{q_t (x)}\\right)dx+\\int V_o q_t(x) f'\\left(\\frac{p_t(x)}{q_t (x)}\\right) \\frac{p_t(x)}{q_t (x)} dx\\tag{7}"}, {"title": "B. Training details", "content": "In this section, we provide training details for f-distill on CIFAR-10, ImageNet-64 and COYO-700M (w/ SD v1.5 model). Table 5 shows the values of common training hyper-parameters on different datasets. For most hyper-parameters, we directly borrow the value from [71], which is a special case in the f-distill framework. Inspired by the three-stage training in [71], we also divide the ImageNet-64 training process into two stages with different learning rates. In the first stage, we train the model with a learning rate of 2e-6 for 200k iterations, then fine-tune it with a learning rate of 5e-7 for 180k iterations. We apply TTUR [71] for all the models. We further provide an algorithm box in Alg 1 for clarity.\n[71] uses the online fake score network as the feature extractor for the GAN discriminator. This complicates the training process, as there is an additional hyper-parameter balancing the denoising score-matching loss and GAN loss for updating the fake score network. To simplify the use of GAN in our framework, we use the fixed teacher network as the feature extractor, similar to LADD [51]. Unlike [71], including the fake score network as part of the learnable parameter in the GAN discriminator, the learnable parameter in the new setup is a small classification head whose input is the feature from the teacher network. We empirically observe that the modification leads to better performance on CIFAR-10, as shown in Fig. 7a. We also include an ablation study (green line in Fig. 7b), which uses the fake score as a feature extractor but does not update it in the GAN loss. The model behaves poorly in this case, as the fake score is constantly getting updated with denoising score-matching loss, validating the benefits of using the fixed teacher score as feature extractor."}, {"title": "C. Properties of f-divergence", "content": "Mode-seeking, as described in Section 10.1.2 in [2], refers to the tendency of fitted generative models to capture only a subset of the dominant modes in the data distribution. This occurs during the minimization of the f-divergence minq Df (p||q) between the true data distribution (p) and the learned generative distribution (q). An f-divergence is considered \u201cmode-seeking\u201d if its minimization leads to this mode-seeking behavior in the corresponding generative model. The mode-seeking behavior in generative models translates into a lack of diversity in practice. Most of the previous classifications of mode-seeking divergences are mainly based on empirical observations. For example, reverse-KL is widely considered mode-seeking, and forward-KL aims for the opposite (i.e., mode-coverage) [38]. Here, we applied the criteria proposed in [24] (see Definition 4.1 in the paper) to roughly classify the f-divergence based on mode-seeking. Intuitively, a smaller limit indicates a higher tolerance of the corresponding f-divergence for large density ratios (r = p/q). This allows the generative distribution q to assign less probability mass to regions where the true distribution p has low density without incurring a significant penalty. Consequently, this behavior can lead to mode-seeking, where the model focuses on capturing only the dominant modes of the data distribution. Hence, we use the rate of the limit to classify divergence in the mode-seeking column in Table 1.\nAnother paper [52] classifies the mode-seeking divergence based on the increasing rate of the limits lim,\u2192\u221e f\"(r) and limro f\"(r), through a concept of \u201ctail weight\u201d."}, {"title": "C.1. Mode-seeking behavior in f-divergence", "content": ""}]}