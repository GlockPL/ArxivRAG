{"title": "M\u00b2M: Learning controllable Multi of experts and multi-scale operators are the Partial Differential Equations need", "authors": ["Aoming Liang", "Zhaoyang Mu", "Pengxiao Lin", "Cong Wang", "Mingming Ge", "Ling Shao", "Dixia Fan", "Hao Tang"], "abstract": "Learning the evolutionary dynamics of Partial Differential Equations (PDES) is critical in understanding dynamic systems, yet current methods insufficiently learn their representations. This is largely due to the multi-scale nature of the solution, where certain regions exhibit rapid oscillations while others evolve more slowly. This paper introduces a framework of multi-scale and multi-expert (M2M) neural operators designed to simulate and learn PDEs efficiently. We employ a divide-and-conquer strategy to train a multi-expert gated network for the dynamic router policy. Our method incorporates a controllable prior gating mechanism that determines the selection rights of experts, enhancing the model's efficiency. To optimize the learning process, we have implemented a PI (Proportional, Integral) control strategy to adjust the allocation rules precisely. This universal controllable approach allows the model to achieve greater accuracy. We test our approach on benchmark 2D Navier-Stokes equations and provide a custom multi-scale dataset.", "sections": [{"title": "1 Introduction", "content": "Many challenges require modeling the physical world, which operates under established physical laws [Karniadakis et al., 2021, Brunton and Kutz, 2024]. For example, the Navier-Stokes equations form the theoretical foundation of fluid mechanics and have widespread applications in aviation, shipbuilding, and oceanography [Vinuesa and Brunton, 2022]. Various numerical approaches exist to tackle these equations. These include discretization methods such as finite difference [Godunov and Bohachevsky, 1959], finite volume [Eymard et al., 2000], finite element [Rao, 2010], and spectral methods [Shen et al., 2011]. Although classical physical solvers based on first principles can achieve high accuracy, they must recalculate when faced with new problems, failing to generalize and resulting in inefficient solutions. Artificial intelligence-based surrogate models effectively address these issues by providing more adaptable and efficient solutions. Understanding and learning the data that embodies these physical laws is crucial for controlling and optimizing real-world applications [Lv et al., 2022, Kim and Boukouvala, 2020]. Mastery of such data-driven insights enables more precise predictions, enhanced system performance, and significant advancements in how we interact with and manipulate the application in the fields of engineering and science [No\u00e9 et al., 2020]. The growing interest in efficient PDE solvers and the success of deep learning models in various fields has sparked significant attention, such as neural operator methods [Li et al., 2020, Kovachki et al., 2021, Bonev et al., 2023, Liu et al., 2024a]. Um et al. [2020] proposed a spatial resolution solver to reduce the computational and accelerate physical simulations. Wu et al. [2022], Sanchez-Gonzalez et al. [2020] through reducing the dimensions of latent space to map the solution in the surrogate models.\nExploring how to integrate and fully leverage performance across different scales while controlling complex learning dynamics is a promising area of research. Based on the frequency principle [Xu et al., 2019], our primary motivation is to enable smaller or more general models to learn low-frequency dynamic data, while delegating high-frequency data to more capable models. The router (distribution policy) regulates this allocation, which sets our approach apart from other methods. In this work, we introduce the multi-scale and multi-expert (M2M) neural operators as the effective surrogate model to learn the dynamics of PDEs and optimize the appropriate allocation law for the different scales with different expert models. Our critical insight lies in leveraging the divide-and-conquer approach among models to learn the capabilities across different scales quickly. Divide and conquer is a fundamental algorithmic technique for solving complex problems by breaking them down into simpler, more manageable sub-problems [Smith, 1987, Huang et al., 2017, Ganaie et al., 2022, Emirov et al., 2024]. This approach works on the principle that a large problem can often be divided into two or more smaller problems of the same or similar type. Each of these smaller problems is then solved independently. Once solutions are obtained for all the sub-problems, they are combined to form a solution to the original, more extensive problem. In addition,"}, {"title": "2 Problem Setting and Related Work", "content": "We consider temporal Partial Differential Equations (PDEs) w.r.t. time $t \\in [0,T]$ and multiple spatial dimensions $x = [x_1,x_2,...x_D] \\in X \\subset \\mathbb{R}^D$. We follow a similar notation as in [Brandstetter et al., 2022].\n$\\partial_t u = F(a(t), x, u, u_x, u_{xx}, ...), \\quad (t, x) \\in [0, T] \\times X$\n$u(0, x) = u_0(x), \\quad x \\in X$\n$B[u](t, x) = 0, \\quad (t, x) \\in [0,T] \\times \\partial X$\nwhere $u: [0, T] \\times X \\rightarrow \\mathbb{R}^n$ is the solution, which is an infinite-dimensional function. $a(t)$ is a time-independent parameter of the system, which can be defined on each location x, e.g. diffusion coefficient that varies in space but is static in time, or a global parameter. $F$ is a linear or nonlinear function. Note that in this work we consider time-independent PDEs where $F$ does not explicitly depend on t. $u^0(x)$ is the initial condition, and $B[u](t, x) = 0$ is the boundary condition when x is on the boundary of the domain $X$ across all time $t \\in [0,T]$. Here $u_x, u_{xx}$ are first- and second-order partial derivatives, which are a matrix and a 3-order tensor, respectively (since x is a vector). Solving such temporal PDEs means computing the state $u(t, x)$ for any time $t\\in [0, T]$ and location $x \\in X$ given the above initial and boundary conditions.\nThe fundamental problem can be succinctly represented for tasks involving partial differential equations by the following formula.\n$(\\partial, u_0) \\xrightarrow[]{f_{\\theta}} (\\partial_1, u_1) ... \\xrightarrow[]{f_{\\theta}} (\\partial_T, u_T)$,\nwhere $f_\\theta$ represents the model and $\\partial$ denotes the boundary conditions.\nDeep Learning-based Surrogate Methods. There are two fundamental approaches:\n\u2022 Autoregressive Model Approach: The model learns the mapping function $f_{\\theta}$ from a given $u_t$ to the next $u_{t+1}$, acquiring discrete representations. This method involves learning the model to predict subsequent time steps based on previous inputs. Such frameworks include CNN-based models [Wang et al., 2020b, Kemeth et al., 2022], GNN-based models [Pfaff et al., 2020, Li et al., 2024], and transformer-based models [Cao, 2021, Geneva and Zabaras, 2022, Takamoto et al., 2023].\n\u2022 Neural Operator Approach: Unlike autoregressive models, the neural operator method [Lu et al., 2021] allows the model to map through multiple time steps, learning infinite-dimensional representations. This approach enables the model to handle more complex temporal dynamics by learning continuous representations. Apart from vanilla FNO, there are other operator learning methods such as U-FNO (U-Net Fourier Neural Operator, [Wen et al., 2022]), UNO (U-shaped neural operators, [Azizzadenesheli et al., 2024]), WNO (Wavelet Neural Operator, [Navaneeth et al., 2024]), and KNO (Koopman Neural Operator, [Xiong et al., 2024]).\nIn addition to these two conventional methods, researchers have developed several hybrid approaches that combine elements of both [Watters et al., 2017, Zhou et al., 2020, Keith et al., 2021, Hao et al., 2023, Kovachki et al., 2024, Wang et al., 2024]. For multi-scale PDEs problems, Liu et al. [2020] developed multi-scale deep neural networks, using the idea of radial scaling in the frequency domain and activation functions with compact support. Hu et al. [2023] propose the augmented physics-informed"}, {"title": "3 The Proposed Method", "content": "In this section, we detail our M2M method. We first introduce its architecture in sec. 3.1. Then we introduce its learning method (sec. 3.2), including learning objective training, and a technique to let it learn to adapt to the varying importance of error and computation. The high-level schematic is shown in figure 1."}, {"title": "3.1 Model architecture", "content": "The model architecture of M2M consists of three components: multi-scale segmentation and interpolation, Experts Net, and Gate router. We will detail them one by one.\nMulti-scale Segmentation and Interpolation. Multi-scale segmentation involves strategically decomposing the input into multiple scales or resolutions to facilitate detailed analysis and processing. This technique benefits applications that require fine-grained analysis on various scales, such as traditional image processing [Emerson, 1998, Sunkavalli et al., 2010] and deep learning methods [Zhong et al., 2023, Yuvaraj et al., 2024]. Consider a discrete form input represented as $u_t^{h\\times w}$, $u_t^{p\\times w}$, where $h$ and $w$ denote the spatial domain resolution at time step $t$. In multi-scale segmentation, the $u_t^{p\\times w}$ first needs to be segmented into smaller, non-overlapping scale patches. For example, segmenting a tensor $u_t^{h\\times w}$ into $2 \\times 2$ patches results in four distinct segments. Each segment corresponds to a quarter of the original tensor, assuming that $h$ and $w$ are evenly divisible by 2. Secondly, suppose that we wish to perform an interpolation on these segmented patches to restore them to the original $h \\times w$ dimensions. Mathematically, this operation can be expressed as:\n$u_t^{h\\times w} \\xleftarrow[\\substack{\\text{Segmentation}\\\\ \\{ ui, j\\in \\{1,2\\}}} \\\\ ]{} { \\xrightarrow[\\substack{\\text{Interpolation} \\\\ {\\Phi_{i,j} \\in \\{1,2\\}}} \\\\ ]{} }$"}, {"title": "Experts Net", "content": "Experts Net. In theory, an expert net is composed of multiple distinct models. However, our sub-expert networks are structured in a parallel configuration for rigorous comparison in this study. Importantly, we have opted for a non-hierarchical architecture. All constituent models are based on the Fourier Neural Operator (FNO), with potential variations in the number of modalities. Formally, let $E = \\{E_1, E_2, ..., E_n\\}$ represent the set of expert models, where each $E_i$ is an FNO. The input to each expert is a different patch $P_i \\in \\mathbb{R}^{h\\times w}$. The output of each expert maintains the same dimensionality as the input. The primary function of the expert system is to model the temporal evolution of the system state as shown in Eq. 2. We employ a divide-and-conquer strategy, where each expert $E_i$ operates on a subset of the input space:\n$E_i: P_i^{h\\times w}\\xrightarrow[]{}P_i'^{h\\times w}$,\nwhere $P_i^{h\\times w}$ is a patch of the input and $P_i'^{h\\times w}$ is the corresponding output patch. The predication solution $\\hat{u}^{h\\times w}$ involves the aggregation of these individual patch predictions to reconstruct the full system state:\n$\\hat{u}^{h\\times w} = A(\\hat{P_1}, \\hat{P_2}, ..., \\hat{P_n})$,\nwhere $A$ is an aggregation function that combines the individual patch predictions into a coherent global state, this approach allows for parallelization and potentially more efficient processing of complex spatio-temporal dynamics while maintaining consistency across all or sparse expert models."}, {"title": "Gate Router Mechanism in MoE", "content": "Gate Router Mechanism in MoE. The Gate Router Mechanism is a crucial component in the MoE architecture and is responsible for distributing input patches across expert models. The primary objectives of this mechanism are:\n1. To efficiently allocate different patches to different models and to route complex problems to more sophisticated networks. (Divide and Conquer)\n2. To avoid overloading a single model, which could lead to high computational complexity. (Simplicity is the ultimate sophistication)\n3. Optionally, the router could be set as the top-k and strong prior, which we encourage the sparse experts to apply the different regions.\nLet $X = \\{u_t^{1,1}, u_t^{1,2}, u_t^{2,1} u_t^{2,2}\\}$ be a set of 4 input scale domain. The router function $R$ is defined as:\n$R: X \\rightarrow [0,1]^{N\\times M}$,\nwhere $R(x_i)$ represents the probability of routing input $x_i$ to expert $E_j$. The ideal routing strategy aims to optimize the following objectives:\n$\\underset{R}{min} E_{x\\sim D}  \\sum_{j=1}^{N} R(x)_{i,j} \\cdot Error(E_j, x)$"}, {"title": "3.2 Learning objective and control strategy", "content": "The training objective is defined as follows:\n$\\mathcal{L}(t) = \\lambda(t) \\mathcal{L}_{router} + \\mathcal{L}_{experts}$,\nwhere the $\\mathcal{L}_{router}$ and $\\mathcal{L}_{experts}$ represent the training loss for the router and experts net, respectively. The $\\lambda(t)$ is a hyperparameter related to the training epoch $t$. Our assumption is as follows: in the initial stage of the model, the router should allocate data evenly to the experts, allowing each expert to receive sufficient training. Once the expert networks have been adequately trained, if the router is not performing well, feedback should be used to train the router. This will enable the router to select the well-performing experts for further training, thereby fully leveraging the potential of the experts.\nRouter Loss. The training objective for the router can be formulated as:\n$\\mathcal{L}_{router} = KL(R(x)||P(E)) + \\mathcal{L}_{load}$,\nwhere KL is the Kullback-Leibler divergence, and this formulation allows the router to start from the prior distribution and gradually adapt to the optimal routing strategy as training progresses. The KL divergence term encourages the router to maintain some similarity to the prior, which can help prevent all inputs from being routed to a single expert. To promote the sparsity of the router and the computational tradeoff, we introduce a load-balancing entropy loss as $\\mathcal{L}_{load}$:\n$\\mathcal{L}_{load} =  \\sum_{i=1}^{M}p_{ij} \\log p_{ij}$,\nwhere $p_{ij}$ represents the probability $R(x_i)_{ij}$ of assigning the i-th data point to the j-th expert.\nExpert Learning Loss. Each expert model should be trained using supervised learning to approximate the solution of the PDE at a given time step. To achieve this, we define the loss function for each expert model using the Mean Squared Error (MSE) between the predicted solution and the true solution of the PDE at each time step."}, {"title": "4 Experiments", "content": "In the following experiments, we set out to answer the following questions on our proposed M2M:\n\u2022 Multi-scale effect and allocate mechanism: Can the M2M model dynamically allocate the spatial domain to concentrate computational resources on regions with higher dynamics, thus enhancing prediction accuracy?\n\u2022 Pareto frontier improvement: Does M2M enhance the Pareto frontier of Error versus Computation compared to deep learning surrogate models (SOTA)?\n\u2022 Controllable training: Is M2M capable of adapting its learning results based on the dynamics of the problem, as indicated by the parameter \u03bb?\nWe evaluate our M2M on two challenging datasets: (1) a custom 2D benchmark nonlinear PDEs, which tests the generalization of PDEs with the different spatial frequencies; (2) a benchmark-based Naiver-Stokes simulation generated in [Li et al., 2020]. Both datasets possess multi-scale characteristics where some domains of the system are highly dynamic while others are changing more slowly. We use the relative L2 norm (normalized by ground-truth's L2 norm) as a metric, the same as in [Li et al., 2020]. Since our research primarily focuses on control methods combined with multi-expert models, we aim to utilize the foundation modes of Fourier operators. In the following sections, we will consistently employ FNO32, FNO128, FNO32, and FNO16, with the goal of achieving a higher-order operator FNO256."}, {"title": "4.1 The comparison of custom multi-scale dataset v.s. PID-control effect", "content": "Data and Experiments. In this section, we test M2M's ability to balance error vs. computation tested on unseen equations with different parameters in a given family. For a fair comparison, we made the model size of the different methods as similar as possible. We use the custom dataset for testing the Multi-scale effect and Controllable training. The multi-scale dataset is given by\n$\\nabla^2u(x,y) = f(x, y)$,\nwhere $u(x, y)$ is the unknown solution to be solved, and $f(x, y)$ represents the source term, which varies for different regions. More details about the multi-scale dataset are available in the Appendix 6.1.\nMain Results. The compared baseline methods are FNO [Li et al., 2020], UNO [Azizzadenesheli et al., 2024], CNO [Raonic et al., 2024], and KNO [Xiong et al., 2024]. Please refer to the appendix for baseline visualization results in the appendix 6.4.3. The M2M approach achieves Pareto optimality, as demonstrated in the Pareto frontier detailed in figure 10. As a heuristic choice, we set the target to 0 and defined the loss $\\mathcal{L}(t)$ as the RMSE in the training stage."}, {"title": "4.2 The Naiver-Stokes (NS) dataset and comparison of SOTA", "content": "Here we evaluate our M2M performance in a more challenging dataset in the Naiver-Stokes dataset, the description is shown in Appendix 6.2. In this experiment, to ensure a fair comparison and leverage the M2M method's ability to enhance FNO's inherent capabilities, we selected the baseline model of FNO-3D instead of the auto-regressive style in FNO-2D. Since FNO-3D operates at least three times faster than FNO-2D, this choice significantly shortened the experimental cycle. As shown in table 4.2, our M2M can allocate high FNO modes to the high-frequency region and achieve better accuracy than the baselines. Specifically, M2M outperforms the strong baseline of FNO128 in the performance a little. This shows that the router could learn a proper allocation policy, allowing the evolution model to evolve the system more faithfully. However, the multi-scale effect did not perform well on this complex dataset, especially in the boundary. The reason is that there are strong temporal scale dependencies between patches, and as the spatial partitioning increases, the divide-and-conquer approach becomes less effective in the appendix 6.2. We applied our method to a cylinder wake flow in the appendix 6.4.4, close to real-world data which has the prior on the fluid mechanic. By incorporating prior distributions in regions where vortex shedding forms around the cylinder, the prediction is quite accurate."}, {"title": "5 Conclusion and Limitation", "content": "In this work, we have introduced M2M. This first intense learning-based surrogate model jointly learns the evolution of the physical system and optimizes assigning the computation to the most dynamic regions. In multi-scale and Naiver-Stokes datasets, we show that our PID method can controllably train the expert's net and router, which improves long-term prediction error than strong baselines of deep learning-based surrogate models. The fitting error has been demonstrated to converge based on control theory in the Appendix 6.5. Furthermore, the PID-based M2M could improve the convergence speed, showing that this intuitive baseline is suboptimal. Finally, M2M outperforms its ablation without multi-scale segmentation, showing the divide-and-conquer strategy which can significantly reduce the prediction error. We hope M2M can provide valuable insight and methods for machine learning and physical simulation fields, especially for applications requiring scalability and multi-physics models. The limitation of M2M is shown in appendix 6.6."}, {"title": "6 Appendix", "content": "6.1 Custom Multi-scale Poisson equation dataset\nThe custom multi-scale dataset is designed to simulate a complex scenario, where data in some regions change slowly while changing more rapidly in others. We define the task as follows: the input is the solution $u^{low}$ to a relatively low-frequency equation, while the output is the solution $u^{high}$ to a corresponding high-frequency equation. To idealize this dataset, we adopted the form of the classical Poisson equation and used the finite difference method to solve the problem. The concise discrete form is [1, 128, 128] $\\rightarrow$ [1, 128, 128]. The time step is set to 1 and the spatial domain is set to [128, 128]."}, {"title": "6.1.1 Frequency Distribution on different regions", "content": "The source term for each region is a sinusoidal function with a systematically varying frequency. Specifically, the source term $f_{ij}(x, y)$ is defined as follows for $i, j = 1, 2, 3, 4$:\n$f_{11}(x, y) = sin(\\pi\\cdot (1\\cdot \\mu_x)) sin(\\pi\\cdot (1\\cdot \\mu_y))$,\n$f_{12}(x, y) = sin(\\pi\\cdot (2\\cdot \\mu_x)) sin(\\pi\\cdot (2\\cdot \\mu_y))$,\n$f_{21}(x, y) = sin(\\pi\\cdot (3\\cdot \\mu_x)) sin(\\pi\\cdot (3\\cdot \\mu_y))$,\n$f_{22}(x, y) = sin(\\pi\\cdot (4\\cdot \\mu_x)) sin(\\pi\\cdot (4\\cdot \\mu_y))$.\nThe initial solution of PDEs will be decided by the dimensionless frequency $\\mu$ and the other solution for the high frequency is $7\\cdot \\mu$. In this dataset, we sampled 1000 cases with different values of $\\mu$, which were drawn from a normal distribution $\\mathcal{N}(1,0.1)$ using Monte Carlo sampling. Out of the 1000 samples, 700 are allocated for the training dataset, while the remaining 300 are reserved for the test dataset. To increase the complexity in the varying time PDEs, we assume that the solutions include a two-step solution and that the ground truth (the second time-solution) is the high spatial frequency to be predicted, $7\\cdot \\mu$ of each low-frequency domain corresponding to the input domain."}, {"title": "6.1.2 Solver Implementation and setting of grids", "content": "The Poisson equation is solved numerically using a finite-difference method on each block. The boundary conditions and the source term $f(x, y)$ determine the solution $u(x, y)$ within each block. The computational grid is set into a 128 \u00d7 128 grid and divided into 2\u00d72 blocks, each of size 64\u00d764. After calculation, the boundary condition $g(x, y) = 0$ is assigned in each block boundary."}, {"title": "6.2 2D Naiver Stokes", "content": "6.2.1 2D Naiver Stokes Datasets\nThe Navier-Stokes equation has broad applications in science and engineering, such as weather forecasting and jet engine design. However, simulating it becomes increasingly challenging in the turbulent phase, where multiscale dynamics and chaotic behavior emerge. In our work, we specifically test the model on a viscous, incompressible fluid in vorticity form within a unit torus. The concise discrete form is [10,64,64] \u2192 [10,64,64]. The input time step is set to 10 and the spatial domain is set to [64, 64].\n$\\partial_t\\omega(t, x) + u(t, x) \\cdot \\nabla\\omega(t, x) = \\nu\\Delta\\omega(t, x) + f(x), \\quad x \\in (0, 1)^2, t \\in (0, T]$\n$\\nabla \\cdot u(t, x) = 0, \\quad x \\in (0,1)^2, t \\in [0, T]$\n$\\omega(0,x) = \\omega_0(x), \\quad x \\in (0,1)^2$\nwhere $\\omega(t, x) = \\nabla \\times u(t, x)$ is the vorticity, $\\nu \\in \\mathbb{R}^+$is the viscosity coefficient. The spatial domain is discretized into 64 \u00d7 64 grid. The fluid is turbulent for $\\nu = 10^{-5}$ (Re = 105)."}, {"title": "6.2.2 Results of M2M at different scales", "content": "Figure 6 and 7 below show the prediction results at two different scales. As can be seen, there are some sharp edges at the boundaries."}, {"title": "6.3 Detailed configuration of the M\u00b2M and baseline models", "content": "This section provides a detailed configuration of M2M, baseline methods, and the hyperparameters used for training in Table 3 and Table 4."}, {"title": "6.3.1 Hyper-parameters for training of M2M", "content": "The policy network is implemented as a classifier, with the output corresponding to the weights distribution of the expert network."}, {"title": "6.4 Extra Visualization", "content": "6.4.1 MoE and PID trajectory details\nIn this section, we present results concerning the two types of priors in the router during the initial phase, along with different PID parameters and scaling factors. One type of strong prior, such as [0100] to add the output of the router, indicates that the router assigns each patch to four experts by incorporating the prior directly into the router's output through hard constraints, followed by a softmax function. The other type of weak prior represented as [0000], relies entirely on the router's output without any prior constraints. As for the second prior, the results has shown in the figure 3."}, {"title": "6.4.2 Ablation study on the multi-scale effect", "content": "We compared the performance of multi-scale models on the custom dataset, where the model is directly routed to different experts by a router, with the prior set to [0000]. It is worth noting that these comparisons were made without the inclusion of the PID algorithm, to ensure fairness in the table 5. Both interpolation and extrapolation methods in the multi-scale stage were chosen to be linear for the sake of computational efficiency."}, {"title": "6.4.3 Baseline results", "content": "Here, we show the Pareto Frontier with different models in the figure 10. It can be observed that our M2M model, represented by the blue stars, lies on the Pareto frontier, demonstrating that our computational speed and accuracy are quite competitive. Performances of baseline models CNO, FNO, UNO, and KNO on the custom dataset are"}, {"title": "6.5 Control theory on PID", "content": "A PID controller regulates the control input $u(t)$ by combining three terms: proportional (P), integral (I), and derivative (D). The controller aims to minimize the tracking error $e(t)$, defined as the difference between the desired reference signal $r(t)$ and the system output $y(t)$:\n$e(t) = r(t) - y(t)$"}, {"title": "3. Derivative Term", "content": "The derivative term $K_d \\frac{de(t)}{dt}$ anticipates future error based on the rate of change of the error. It provides a damping effect that helps reduce overshoot and oscillations in the system's response. The term is proportional to the velocity of the error, thus slowing down the system's response as the error decreases."}, {"title": "4. Combined Dynamics and Stability", "content": "The overall system dynamics, taking all three terms into account, can be modeled as:\n$\\frac{de(t)}{dt} = -K_p e(t) - K_i \\int_0^t e(\\tau)d\\tau - K_d \\frac{de(t)}{dt}$\nFor a properly tuned system, the combination of the proportional, integral, and derivative terms ensures that the error will decrease over time. Specifically, the integral term guarantees that any steady-state error will be driven to zero, while the proportional and derivative terms ensure fast response and stability. Thus, as $t \\rightarrow \\infty$, $e(t) \\rightarrow 0$."}, {"title": "6.6 Limitation", "content": "Our approach may have certain limitations in the following areas:\n1. It may require some prior knowledge of physics, such as frequency decomposition and domain-specific knowledge embedding;\n2. There may be competitive interactions between expert models, where the quality of initialization plays a decisive role in model performance;\n3. The PID approach may not be suitable for more complex models and datasets, and methods like Model Predication Control or reinforcement learning might need to be explored in the future;\n4. To reduce patch boundary effects, especially in scenarios where performance degrades with a high time and spatial dynamic (e.g. Turbulence) dataset;"}, {"title": "6.7 Mathematical Notations", "content": "This section lists the mathematical notations used in the paper for reference."}]}