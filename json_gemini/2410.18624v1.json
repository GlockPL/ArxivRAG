{"title": "Prompting and Fine-Tuning of Small LLMs for Length-Controllable Telephone Call Summarization", "authors": ["David Thulke", "Yingbo Gao", "Rricha Jalota", "Christian Dugast", "Hermann Ney"], "abstract": "This paper explores the rapid development of a telephone call summarization system utilizing large language models (LLMs). Our approach involves initial experiments with prompting existing LLMs to generate summaries of telephone conversations, followed by the creation of a tailored synthetic training dataset utilizing stronger frontier models. We place special focus on the diversity of the generated data and on the ability to control the length of the generated summaries to meet various use-case specific requirements. The effectiveness of our method is evaluated using two state-of-the-art LLM-as-a-judge-based evaluation techniques to ensure the quality and relevance of the summaries. Our results show that fine-tuned Llama-2-7B-based summarization model performs on-par with GPT-4 in terms of factual accuracy, completeness and conciseness. Our findings demonstrate the potential for quickly bootstrapping a practical and efficient call summarization system.", "sections": [{"title": "I. INTRODUCTION", "content": "In many industries, particularly those involving customer service, healthcare, and finance, summarizing telephone conversations is a crucial task. It is common practice for agents to summarize calls as part of call wrap procedures to ensure proper documentation and outline potential follow-ups. This process can take around 10% of the total call handling time, representing a significant operational overhead.\nBesides reducing this overhead, automatic summarization of telephone calls offers several advantages: It can provide higher consistency and reduce the influence of individual agent biases. Additionally, automated summaries can still be reviewed and post-edited by agents if necessary, maintaining a balance between efficiency and accuracy.\nLarge language models (LLMs) have shown considerable promise in generating coherent and contextually relevant text. Very large frontier LLMs like GPT-4 [1] and Llama 3.1 405B [2] have demonstrated strong zero-shot performance in various tasks, including summarization. However, more efficient, smaller LLMs still struggle to match this level of performance. Task-specific training data may enable these smaller models to close the performance gap, offering a more resource-efficient alternative for practical applications.\nThis paper explores the rapid development of a telephone call summarization system utilizing LLMs. Our approach involves initial experiments with prompting existing LLMs to generate summaries of telephone conversations. These experiments provide insights into effective strategies and limitations, guiding the creation of a tailored training dataset using advanced frontier LLMs such as GPT-4.\nA critical aspect of our research is the ability to control the length of the generated summaries to meet use-case specific requirements. We use Llama-2-7B [3] as our base model. By fine-tuning the model and employing strategic prompting techniques, we aim to produce high-quality summaries that adhere to desired length constraints.\nTo evaluate the effectiveness of our method, we utilize state-of-the-art LLM-as-a-judge-based evaluation techniques. The results show that Llama-2-7B model, when fine-tuned on summarization-specific data is able to produce summaries that are on par with those generated by GPT-4 in terms of factual accuracy, completeness, and conciseness. In contrast, when the model is fine-tuned on task-agnostic, general data, its performance significantly deteriorates even lagging behind that of the Llama-2-Chat-7B model. This highlights the usefulness of training with task-specific data. Finally, our experiments on length-control reveal that training on uniform task-specific data can weaken the model's instruction-following ability. However, incorporating varying length-specific instructions into synthetic data generation can help restore this capability. Our findings highlight the potential for quickly bootstrapping a practical and efficient call summarization system.\nThis paper is structured as follows: Section II introduces the telephone conversation corpus used in this work. Section III details the prompting techniques and synthetic data generation process. Section IV describes the models and approaches used for fine-tuning. Section V presents the evaluation methods and results, focusing on length adherence and utilizing LLM-as-a-judge evaluation. Section VI reviews related work in the field of call summarization and LLMs. Finally, Section VII concludes the paper with a discussion of our findings and future research directions."}, {"title": "II. TELEPHONE CALL CORPUS", "content": "For this work, we utilized a corpus of 2,331 simulated telephone recordings. We did not use real customer conversations to avoid any privacy concerns. The simulated corpus was originally created to augment AppTek's automatic speech"}, {"title": "III. SYNTHETIC DATA GENERATION AND PROMPTING", "content": "Due to the high costs and effort involved in human data annotation, we opted to generate synthetic summaries of call transcripts using GPT-4 [1], which is a strong external model capable of producing high-quality summaries. This approach allowed us to automatically generate a large volume of summaries for model supervision while minimizing costs.\nTo generate high-quality summaries, we first preprocess the call transcripts by extracting speaker tags from the raw data and incorporating them into the transcript. The inclusion of speaker information aims to provide additional context that the model can utilize when generating summaries. While our system is entirely text-based, this step also considers potential future applications, such as multimodal models that process audio inputs from call recordings to produce text summaries, where speaker information could be valuable. We anonymize the speaker tags, labeling them as speaker 0 and speaker 1 when two speaker channels are present. Although more detailed speaker information could be included, it falls outside the scope of this work.\nIn addition to including speaker tags, we also account for longer context lengths. Given that GPT-4 supports a larger context window (8k) compared to our Llama2 [3] base model (4k), generating synthetic data with a context size larger than our model's capacity is inefficient. Therefore, when necessary, we truncate the left side of the context to ensure that the combined call transcript and summarization prompt fit within our model's context window. While this may impact summarization quality, it is a necessary compromise we have to make.\nPrompting is an important part of the LLM pipeline as it aligns the model behavior with the user purpose [4]. Due to our above-mentioned context pruning method and findings from our preliminary experiments\u2014where prompts positioned closer to the current content produced better results compared to those positioned further away (e.g., \"Summarize the call transcript above\" at the end yielded better outcomes than \"Summarize the call transcript below\" at the beginning)-we adopt a transcript-first-and-prompt-last format for prompting.\nWe define three categories for summarization-specific prompts, namely \"default\", \"general\" and \"length-oriented\". Detailed prompts can be found in Table III. The \"default\" category includes a straightforward command that instructs the model to summarize the call transcript. This serves as the baseline prompt. For the \u201cgeneral\u201d category, we used GPT-4 to generate a range of summarization prompts focusing on various aspects such as content, sentiment, next steps, and so on. We then reviewed these prompts manually and selected those that met our quality criteria.\nFinally, for the \"length-oriented\" category, we used six prompt variants, each imposing specific constraints in sentence count, word count or paragraph count to control the summary length. The objective of this category was to train the model to adhere to length-specific instructions. We anticipated that exposure to such length-specific prompts during training would enable the model to follow these instructions, thereby providing a 'soft' mechanism for length control during testing.\nThis way, in total, we considered 20 summarization-specific prompts, and for each transcript during training, we randomly sampled five variants from this pool of prompts.\nNext, we selected a simple and straightforward system prompt for telephone call summarization:\u201cYou are good at summarizing call transcripts.\" It is arguable if such a system prompt is even needed for a task-specific model, especially when alternative system prompts are foreign to the model. However, considering that our models also see general instruction data during training (described in Section IV), i.e. other instructions that are not summarization-specific, we nonetheless include this system prompt to better align the model to the summarization objective."}, {"title": "IV. FINE-TUNING", "content": "In this section, we discuss the fine-tuning process that we employed to enhance the model's summarization performance. As we want to preserve the general-domain instruction following capabilities of the model, we also train our model on non-"}, {"title": "A. General-domain Instruction Fine-Tuning Data", "content": "Besides the summarization-specific instruction fine-tuning (IFT) data, we extend the training data with general-domain data to improve the instruction following capabilities and robustness of the model. We refer to this data as Our IFT data and use the same data mixture described in our previous work [5]. Specifically, we include the following subsets:\n\u2022\tan internal high-quality set of 700 prompt-completion pairs originally collected by AppTek\n\u2022\tDatabricks Dolly [6] the first openly available human-generated IFT dataset with a permissive license consisting of 15,001 prompt and completion pairs across 7 task categories\n\u2022\tthe English subset of OpenAssistant Conversations 1 (OASST-1) [7] taking only the best-rated conversations, resulting in 3,783 conversations.\n\u2022\t3,282 question-and-answer pairs from StackExchange\n\u2022\ta subset of 45,000 samples from FLAN v2 and CoT [8]\n\u2022\tLlama-2 Safety consisting of 939 refusals to prompts in the Do-Not-Answer dataset [9] synthetically generated using Llama-2-Chat-70B [10]\nIn the following we refer to all Llama-2-7B models that are fine-tuned on this data as Llama-2-7B-Our."}, {"title": "B. Summarization-specific Instruction Fine-Tuning Data", "content": "Using the three categories of summarization-specific prompts outlined in Table III and following the methodology described in Section III, we generate three distinct types of summarization-specific IFT data using GPT-4: Default, General, and Length-specific Summarization IFT, which contain 580, 7,248, and 3,327 instances, respectively. We then fine-tune Llama-2-7B [3] using various combinations of this data, resulting in seven different summarization-specific IFT model variants, as shown in Table IV."}, {"title": "C. Training Setup", "content": "We use a fork of NVIDIA's Megatron-LM [11] by the EPFL LLM Team [12, 13] for IFT training. We use a cosine learning rate schedule with a peak learning rate of $10^{-5}$ and a warm-up of 100 steps. The batch size is set to 64 and we use the full sequence length of 4096 tokens. For regularization, we use weight decay of $10^{-2}$ and dropout as used for LIMA [14]. Since the exact summarization instruction data seen by each model variant differs, the number of training steps also differ. That said, because our general-domain instruction data (in comparison to summarization-specific instruction data) still makes up the majority of the total instruction data, this difference is not significant among the models, ranging from 1814 training steps to 2337 training steps. All models are trained on 4xA100 80GB GPUs utilizing pipeline parallelism and using full weight fine-tuning.\nWe use Chat Markup Language1 as prompt template to format the IFT data, which results in the following format:\n<|im_start|>system\n[system_prompt]<|im_end|>\n<|im_start|>user\n[call_transcript]\n[summarization_prompt]<|im_end|>\n<|im_start|>assistant\n[call_summary]<|im_end|>"}, {"title": "V. EVALUATION", "content": "For evaluation, we focus on two aspects. First, the general quality of the generated summaries is evaluated using LLM-as-a-Judge approaches. Second, we analyze the effect of our approaches on the length adherence of the resulting models. As baselines, we consider GPT-4 [1] and Llama-2-Chat-7B [10], which was instruction fine-tuned on general domain data as well as further tuned using reinforcement learning from human feedback. We perform greedy decoding and set the maximum completion length to 256. If not stated otherwise all models are prompted with the default summarization prompt described in Section III."}, {"title": "A. LLM-as-a-Judge", "content": "We evaluate our models using LLM-as-a-judge based evaluation methods, specifically, Prometheus-Eval [15] and FineSurE [16]. These approaches provide a framework for evaluating models against various task-relevant criteria. For summaries to be useful, it is important that they contain a gist of all the key facts discussed in the call, without any misleading information. To ensure this, we perform a multi-dimensional evaluation.\nFor evaluation with Prometheus-Eval [15], we utilize two predefined rubrics to determine if the summaries are non-misleading (HONESTY) and factually accurate (FACTUAL_VALIDITY). Additionally, we introduce a custom rubric to assess if a summary includes all the main points discussed in the call (COMPLETENESS) (see Figure 1). The summaries are evaluated using the Prometheus-8x7B model [15], which assigns a Likert-scale score from 1 to 5 for each rubric per summary.\nFineSurE [16], unlike Prometheus-Eval, is specifically designed for fine-grained summarization evaluation. It assesses summaries from three perspectives: Faithfulness, Completeness, and Conciseness. FAITHFULNESS is evaluated by determining the proportion of sentences in the summary that are factually correct, ensuring the summary is free from factual errors. COMPLETENESS is measured as the ratio of the number of key facts from the transcript that are effectively covered in the summary to the total number of key facts extracted from the summary, ensuring the summary is thorough. CONCISENESS is determined by the ratio of the number of sentences that contain relevant key facts to the total number of sentences in the summary, thereby evaluating the brevity of the summary. We report the percentage scores for each metric.\nKey fact extraction and evaluation of the summaries is done using GPT-4o (gpt-4o-2024-05-13). The default prompt for key fact extraction in FineSurE requires a reference summary. However, since we do not have reference summaries in our test set, we extract key facts directly from the call transcript, using the prompt template shown in Figure 2.\nTable IV presents the results with average scores per metric. Bold numbers highlight the best overall score per metric, while the second-best scores are underlined. We first note that the Llama-2-7B-Our model trained only on the general-domain IFT data slightly lacks behind Llama-2-Chat-7B across all metrics. This can be explained by the larger amount of IFT data and additional RLHF fine-tuning that Llama-2-Chat-7B received. Including summarization-specific IFT data significantly improves the performance of the model across all metrics with results that are on-par with GPT-4. While our models still slightly lagging behind GPT-4 in faithfulness and factual validity but surpassing it in completeness and honesty. Notably, all variants of our summarization IFT models significantly outperform Llama-2-Chat-7B across all metrics from"}, {"title": "B. Length Adherence", "content": "We further assess the effectiveness of length-oriented summarization IFT by testing the IFT variants with prompts that include specific length constraints and report the percentage of summaries that adhered to these constraints. The prompts used for this evaluation are: Summarize the call transcript above in 50 words and Summarize the call transcript above in 100 words which were both part of the length-specific summarization IFT data.\nThe results are shown in Table VI. We first observe that the model trained on general-domain IFT data only (Llama-2-7B-Our) adheres to the length constraints. When incorporating summarization-specific IFT data without length constraints (Default and General), the model's adherence to the length constraints significantly degrades. We assume this is caused by the model overfitting to summarization lengths that occur in this data. However, incorporating length-specific instructions into the summarization-specific data restores this capability. Finally, we observe that even the best performing models only achieve adherence rates of around 60%, indicating that there is still room for improvement in this area. Possible solutions could include filtering the synthetically generated data or incorporating even more and diverse length-specific data."}, {"title": "VI. RELATED WORK", "content": "While there is some previous work studying both extractive [17] as well as abstractive [18] telephone call summarization, most recent work using LLMs focuses one news summarization [19, 20]. Most related to our work is recent efforts on using LLMs for written dialog summarization [21, 22]. Asi et al. utilize commercial API models for telephone conversation [23] while in our approach we distill the summarization capabilities of a frontier LLM to a small LLM. Concurrently to our work, Mullick et al. explore the use of synthetic data for domain-specific document summarization [24].\nLength control is a common requirement for abstractive summarization systems. While LLMs allow to define arbitrary constraints in natural language, previous systems often relied on special control tokens [25]. In addition to supervised fine-tuning, previous work also utilized reinforcement learning [26] or preference optimization [27] to enable length control in text generation models."}, {"title": "VII. CONCLUSION", "content": "In this paper, we explored the rapid development of a telephone call summarization system utilizing LLMs, with a particular emphasis on achieving high performance by utilizing small models. Our experiments demonstrate that it is possible to significantly close the gap between small use-case-specific LLMs and frontier models like GPT-4, even in the absence of use-case-specific data. By generating a tailored synthetic training dataset, we are able to fine-tune a 7B parameter model to achieve summarization capabilities approaching or even slightly exceeding the performance of GPT-4 in key metrics. Our results show that the diversity of the prompts in the synthetically generated data is crucial to improving performance even if just a single prompt is used for evaluation. Further, our analysis and experiments on length control show that training on homogeneous task-specific data can deteriorate instruction following capabilities. We show that, at least for length control, this specific capability can be recovered by augmenting the synthetic data generation.\nWhile our results are promising, they represent an initial step in the broader development of a robust call summarization system. Future research should focus on addressing several important areas that were not fully explored in this study. These include the impact of speech recognition errors, the impact of text normalization (e.g. conversion of spoken to written numbers) and the impact of personal identifiable information that is often redacted in contact center transcripts. Additionally, previous work has shown that pre-training or continued pre-training on in-domain data (i.e. call transcripts in case) can improve the downstream performance of LLMS [5, 13, 28]. Further, while our synthetic dataset was effective for fine-tuning, real-world data from the specific domain of application is expected to provide additional benefits, leading to even higher levels of performance and reliability. Finally, while LLM-as-a-judge approaches provide a useful evaluation framework, human evaluation is still necessary to assess the real-world utility and to make deployment decisions."}]}