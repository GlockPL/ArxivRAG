{"title": "Mitigating Unauthorized Speech Synthesis for Voice Protection", "authors": ["Zhisheng Zhang", "Qianyi Yang", "Derui Wang", "Pengyang Huang", "Yuxin Cao", "Kai Ye", "Jie Hao"], "abstract": "With just a few speech samples, it is possible to perfectly replicate\na speaker's voice in recent years, while malicious voice exploita-\ntion (e.g., telecom fraud for illegal financial gain) has brought huge\nhazards in our daily lives. Therefore, it is crucial to protect publicly\naccessible speech data that contains sensitive information, such\nas personal voiceprints. Most previous defense methods have fo-\ncused on spoofing speaker verification systems in timbre similarity\nbut the synthesized deepfake speech is still of high quality. In re-\nsponse to the rising hazards, we devise an effective, transferable, and\nrobust proactive protection technology named Pivotal Objective\nPerturbation (POP) that applies imperceptible error-minimizing\nnoises on original speech samples to prevent them from being ef-\nfectively learned for text-to-speech (TTS) synthesis models so that\nhigh-quality deepfake speeches cannot be generated. We conduct\nextensive experiments on state-of-the-art (SOTA) TTS models uti-\nlizing objective and subjective metrics to comprehensively evaluate\nour proposed method. The experimental results demonstrate out-\nstanding effectiveness and transferability across various models.\nCompared to the speech unclarity score of 21.94% from voice syn-\nthesizers trained on samples without protection, POP-protected\nsamples significantly increase it to 127.31%. Moreover, our method\nshows robustness against noise reduction and data augmentation\ntechniques, thereby greatly reducing potential hazards.", "sections": [{"title": "1 INTRODUCTION", "content": "Fueled by advancements in generative artificial intelligence (AI),\nvoice cloning has become a hot topic in recent years. Well-trained\nspeech synthesis models can now synthesize realistic speech with\nspecific text and speaker features (e.g., speaker ID, and some simple\nspeech samples) at test time. These advancements stem from the\ndevelopment of deep neural networks (DNNs), leading to models\nlike Tacotron2 [42] and Transformer-TTS [29] for single-speaker\nsynthesis, and GlowTTS [23], VITS [24] and MB-iSTFT-VITS [22]\nfor multi-speaker scenarios with awesome synthetic speech. As a re-\nsult, the synthesized speech produced by these models is becoming\nincreasingly indistinguishable from real human speech, faithfully\nreplicating timbre, speaking rhythm, and pitch. Moreover, some\nzero-shot TTS models, such as YourTTS [7] and MegaTTS2 [21],\nhave been proposed with the capacity to clone a speaker's voice\nfrom one short speech sample. Fine-tuning these pre-trained mod-\nels can achieve superior performance with fewer computing and\ndata resources compared to training from scratch.\nVoice Protection. The synthesized speech referred to as deep-\nfake audio is challenging to differentiate from real speech without\nproper attention, posing significant data security risks. Moreover,\nin recent years, large language models (LLMs) [1, 45] have been\ncontinuously developed, and their high-quality human-like text out-\nput has led to widespread usage among the public. However, LLMs\ncan generate human-like text responses for TTS models, which\nfurther creates a need for voice data protection, as voiceprint be-\ncomes increasingly important for distinguishing these semantically\nenhanced synthetic content. Unauthorized exploitation of public\naudio files for malicious voice synthesis on the internet by attackers\ncan lead to serious security and legal issues, including social en-\ngineering spoofing, illegal authentication, and telecom fraud with\nlethal consequences.\nExisting Defense Methods. To deal with the hazards of deepfake\naudio, the existing defensive methods mainly focus on detection and\nprevention techniques. Detection technologies are usually aimed\nat cases where the deepfake audio has been synthesized. Some\nauxiliary measures are adopted to determine whether the audio is\ngenerated, such as using MFCC features [16] and other acoustic\nsignal analysis [5] simulating auditory effects of the human ear,\nor by supplementary methods like emotion recognition [9] and\nspeaker verification [36]. Detecting the speech [30] can improve\npeople's awareness of preventing deepfake audio, but the reactive\nnature of detection usually implies that the audio has already been\nexploited and cannot achieve protection at the data level. In recent\nyears, researchers have proposed methods [18, 46, 52] to protect"}, {"title": "2 RELATED WORK", "content": "2.1 Text-To-Speech Synthesis\nModern text-to-speech systems use the basic framework of statis-\ntical parametric speech synthesis (SPSS), which consists of three\nmain core components: a text analysis module, an acoustic model,\nand a vocoder [4, 57].\nThe text analysis module is responsible for transforming the\ninput text into linguistic features suitable for further processing.\nTraditional SPSS typically employs Hidden Markov Models (HMM)\nfor acoustic modelling [51, 56], while modern TTS systems moving\nto more efficient neural network models instead. [12, 38, 54, 55].\nThe vocoder is responsible for synthesizing the generated spec-\ntrogram into audio. Early DNN-based synthesis methods tended\nto perform poorly on long sentences, often dropping words. To\novercome these challenges, researchers have introduced alignment\nmechanisms [3, 43]. Among these, autoregressive models achieve\nalignment through the attention mechanism [42, 43, 47], while\nnon-autoregressive models achieve alignment more efficiently in\nother ways, such as the Montreal Forced Aligner (MFA) [8, 33, 40].\nThe end-to-end TTS system alleviates the problem of one-to-many\nmapping and further improves speech synthesis [40, 42].\n2.2 Privacy Preservation\nThe abuse of speech synthesis leaks the privacy of the speaker,\nand there are some ways to defend against it, such as speaker\nanonymization and data-based protection.\nSpeaker anonymization [13, 50] is an effective way to protect the\nprivacy of the speaker, and the simplest and most direct method is to\nconvert the original speech to text before resynthesizing it through\nthe TTS model to completely remove the voiceprint. In addition,\nFang et al. [13] and Han et al. [17] to hide voiceprint by modifying\nthe x-vector of the original waveform, Yao et al. [49] can effectively\nhide the identity of the speaker by modeling the speaker as a matrix\nand then modifying the values of the matrix. The existing data-\nbased approach [18, 28, 46, 52], leverages protective algorithms to\nmake some perturbations or changes to the original audio data to\nachieve voice anti-cloning.\nHowever, both of the current privacy preservation methods have\ncertain limitations, that is, their results bypass the speaker verifi-\ncation systems and achieve dissimilarity in voice quality, but the\nsynthetic audio after model training still has usability. This may\nlead to attackers obtaining deepfake audio and replacing predeter-\nmined targets, thus flooding potential victims. Therefore, we hope\nto ensure that the deepfake audio is not similar and low-usability."}, {"title": "3 THREAT MODEL", "content": "In this section, we outline the capability and limitations of the\ndefender and adversary."}, {"title": "3.1 Defender Capability and Limitations", "content": ". To achieve comprehensive and effective protection of released\ndata at the source stage, our audio data protection scenario is that\nbefore users publish their audio files on public platforms such as\nsocial media, they can leverage our designed data protection strat-\negy for privacy-preserving. Therefore, the adversary cannot obtain\nthe sensitive unprotected original audio. In this paper, the defender\nand users perturb the audio at a fixed position for better effect.\nTo more accurately simulate real-world training scenarios, we re-\nstrict the defender's knowledge: they are not privy to the specific\nText-to-Speech (TTS) model that the adversary might employ for\nspeech synthesis. This requires the defender to fully consider the\ntransferability and robustness of the generated perturbation across\ndifferent TTS models when designing the protection method, that\nis, to choose a surrogate model and generate noise that still has\nhigh generalization on other models."}, {"title": "3.2 Adversary Capability.", "content": "The advanced TTS models can synthesize realistic voices after train-\ning clean samples from victims. We consider the two capabilities of\nthe adversary in the real world.\nCapability of Data Access. With the continuous development of\ninternet technology, people are more likely to share their videos\npublicly on the network, such as vlogs. These videos containing\nsensitive information can usually be directly downloaded, and ad-\nversaries can obtain unauthorized data through techniques (e.g.,\nweb crawler), from social media platforms such as YouTube, TikTok,\nand Facebook.\nCapability of Speech Synthesis. When the adversaries obtain our\nprotected audio, they can train different TTS models with conven-\ntional settings and they can use various models for effective speech\nsynthesis training. At the same time, due to the lack of a prior\nknowledge about the obtained audio, they may obverse the embed\nperturbation after protection. Therefore, the adversary may employ\nsome perturbation removal and data transformation techniques, to\ndisrupt the structural information or backdoor information of the\nadded perturbation, to achieve high-quality speech synthesis."}, {"title": "4 OVERVIEW OF METHODS", "content": "Figure 2 illustrates our aim of voice anti-cloning and the protective\nresult of our methods. In this section, we illustrate the problem\ndefinition of unlearnable audio and introduce the primary methods\nincluding the speaker selection strategy for fine-tuning and the\nspecific perturbation generation method for TTS systems."}, {"title": "4.1 Problem Definition", "content": "The design of voice protective methods cannot be separated from\nthe scenarios to be protected in this paper and the motivation for\nprivacy-preserving, while we also need to characterize theoretically\nthe results that can be achieved by our designed audio protection\nstrategy as well as the problem itself.\nMotivation. Facing malicious speech synthesis and unauthorized\ndata access, effective data defensive methods can better reduce the\nharm and hazards of the public in the face of attackers. However, on\nthe one hand, with the abuse of technologies such as web crawlers,\nunauthorized access to large amounts of data is becoming increas-\ningly rampant, which exposes public privacy to a greatly dangerous\nsituation. On the other hand, an effective and generalized voice\nanti-cloning strategy has not been well proposed. Previous methods\nhave more or less imperfections, such as only deceiving speaker ver-\nification models [18, 46] without considering that deepfake speech\nmay not target a specific victim. Therefore, preventing unautho-\nrized abuse of audio and defending against synthetic speech have\nbecome the two main motivations of our work.\nUnlearnable Audio. We assume a clean dataset with n samples to\nbe protected as $D_c = \\{(x_i, y_i, z_i)|(x_i, y_i, z_i) \\in X \\times Y \\times Z\\}_{i=1}^n$, where\n$x_i$ is the i-th speech sample from speaker victim $y_i$ and $z_i$ is the\ncorresponding speech text. Given a TTS model G, G can effectively\nsynthesize high-quality and realistic deepfake audio that is similar\nto the timbre of the specified speaker, after training the speech\nsynthesis model on $D_c$. Users utilize the data protection method we\nproposed to protect the clean audio dataset $D_c$, adding an imper-\nceptible perturbation $\\delta$ to each sample at a protective position I to\nobtain the protected dataset $D_u$. After training the model G on $D_u$\nwith the same configuration parameters, the same prompt pair will\nresult in unusable audio filled with background noise at position\nI, thereby achieving the goal of making the protected dataset $D_u$\nunlearnable for the TTS model and preserving privacy. Let a speech\nsample x to safeguard, the protective method can be specifically\ndescribed using the following formula:\n$\\arg \\min_\\delta L(G_I(x + \\delta), x)$,\ns.t. $H(x + \\delta) \\approx H(x)$ and $||\\delta||_p \\leq \\epsilon$, (1)"}, {"title": "4.2 Speaker Selection", "content": "It requires more computing and data resources when training from\nscratch [44]. Based on the pre-training of many samples, it can\naccelerate and optimize the training process with layers adaptation.\nFor better training, we take into account the degree of voiceprint\nsimilarity between the chosen speakers and the pre-trained model\nof a single speaker during the selection process. Specifically, for the\nselected speaker denoted as j, the similarity between their speaker\nembeddings can be quantified by cosine similarity, expressed by:\n$d_j = D(E_s(x_j), E_s(x_0))$, (2)\nwhere $x_j$ and $x_0$ represent the speech of the j-th and targeted\nspeaker, and $E_s(.)$ is the speaker encoder that computes the speaker's\ninformation features and outputs the embeddings containing per-\nsonal voiceprint information from the input audio. D(.) computes\nthe cosine similarity of the two vectors.\nMeanwhile, we add a limitation that each speaker should own\nmore than 50 samples when selecting speakers to obtain a quanti-\ntative balance between different speakers."}, {"title": "4.3 Unlearnable Audio", "content": "In the field of computer vision, the imperceptible perturbations\ngenerated based on $l_p$ norm constraint can make the protected\ndataset unlearnable for the DNNs [19] in the classification task. The\noptimization structure is a bi-level loop, with the outer structure\nfor optimizing model $f_\\theta$ and the inner loop for noise generation,\nwhich can be written as follows:\n$\\arg\\min_\\theta E_{x, y} [\\min_\\delta L(f_\\theta(x + \\delta), y)]$s.t. $||\\delta||_p \\leq \\epsilon$, (3)\nwhere (x, y) denotes the input data and its label.\nThe bi-level structure will optimize both the model parameters\nand the embedded perturbations to improve the generalization of\nthe noises across different model parameters. However, this brings\na huge time overhead to optimize the model's parameters in large-\nparameter and large-data-set scenarios such as TTS synthesis. The\ncore of the bi-level error-minimizing is the internal optimization\nloop, which crafts noise to reduce the error of the model by simulat-\ning the training process so that it learns more about the added noise\nwhen learning from protected data. Based on the above, we can\nsimplify the bi-level error-minimizing method and use the inner\nloop as the targeted optimization function which can reduce the\ncost of training resources. The formulation can be described as\n$\\arg\\min_\\delta L(f(x + \\delta), y)$ s.t. $||\\delta||_p \\leq \\epsilon$. (4)\nGenerative TTS models, such as variational autoencoder (VAE),\nemploy an encoder-decoder structure that learns the distribution of"}, {"title": "4.4 Pivotal Objective Perturbation", "content": "Due to the diversity of objective functions across various TTS\nmodels and our strategy of introducing noise on the waveform to\nmaintain semantic consistency in speech, it is insufficient if we\nleverage all training objective functions of g as the target for noise\noptimization. This is because components of the objective function\nunrelated to input audio would be unable to optimize the noise.\nTypically, TTS models prioritize the reconstruction function be-\ntween synthetic and real audio as their primary objective. Moreover,\nfor the generative TTS model, it is conventional to compute the\nl1 (or l2) distance between the synthetic and real audio to guide\nbetter synthesis which is a learning distribution and outputting it\nprocess [21-24, 29, 42].\nFor better noise generation, we propose a Pivotal Objective Per-\nturbation approach to synthesize effective protective perturbation.\nPOP selects the reconstruction loss as the objective in the er-\nror minimization to generate unlearnable speech samples. We craft\nSOTA and the backbone TTS model named VITS [24] as an example\nto expound the reason and strategy of our method. VITS contains\nthe structure of a decoder and a vocoder, and its generator's objec-\ntive optimization function can be expressed as follows:\n$L_{vits} = L_{recon} + L_{kl} + L_{dur} + L_{adv}(G) + L_{fm}(G)$, (6)\nwhere $L_{recon}$ denotes the reconstruction loss between real and\nsynthetic speech. $L_{kl}$ and $L_{duration}$ represent the KL divergence\nloss and duration loss. $L_{adv}(G)$ and $L_{fm}(G)$ are the adversarial\ntraining loss and feature-matching loss of the generator.\nIn the following, we will give a brief illustration of these loss\nfunctions in Eq. (6). KL divergence $L_{kl}$ is a common loss function\nof VAE architecture, and it learns the relation between phoneme c\nand text, etc., which can be expressed as:\n$L_{kl} = \\log q_\\phi (z | X_{lin}) - \\log p_\\theta (z | C_{text}, A)$,\nz ~ $q_\\phi (z | x_{lin}) = N (z; \\mu_\\phi (X_{lin}), \\sigma_\\phi (X_{lin}))$ (7)\nThe duration loss is a negative variational lower bound, relative\nto the input text and the length of the text. It can not be completely\naffected by speech waveform content, which is formulated as:\n$\\log p_\\theta(d|c_{text}) \\geq E_{q_p(u,v|d,C_{text})} \\log (\\frac{p_\\theta(d - u, v|C_{text})}{q_p(u, v|d, C_{text})})$. (8)"}, {"title": "5 EXPERIMENTS AND ANALYSES", "content": "In this section, we outline the datasets and experimental details.\nFurthermore, we present and discuss the anti-cloning effectiveness,\ntransferability, and robustness across different SOTA models."}, {"title": "5.1 Experimental Settings", "content": "Datasets. We utilize two famous and clean speech datasets, Lib-\nriTTS [53] and CMU ARCTIC [25]. For the LibriTTS dataset, we\nselect the top 50 speakers that are the most similar to the pre-\ntrained speaker from the train-clean-100 subset derived from the\nLibriSpeech [34] corpus and use ECAPA-TDNN [11] as speaker en-\ncoder. The CMU ARCTIC dataset comprises 18 speakers, from each\nof whom we select 300 samples for fine-tuning. During fine-tuning,\n80% of the samples are used for training randomly, while 20% are\nreserved for evaluation.\nModels. The experimental models selected show SOTA perfor-\nmance in the TTS domain. These models include:\n\u2022 GlowTTS [23]: GlowTTS is a two-stage speech synthesis model\nthat learns to synthesize mel-spectrograms as a synthesizer;\n\u2022 VITS [24]: VITS is a backbone TTS model based on the VAE\nstructure in the speech system domain. It operates as an end-to-\nend synthesis system, learning directly from the input waveform\nto produce similar waveform distributions;\n\u2022 MB-iSTFT-VITS [22]: MB-iSTFT-VITS is an enhanced VITS model\nutilizing multi-band inverse Short-Time Fourier Transform which\nmakes the model efficient.\nFor GlowTTS, we choose WaveGlow [37] and HiFiGAN [26], the\nSOTA processor conversing mel-spectrogram to waveform, respec-\ntively as a vocoder. Three models are trained on the LJSpeech [20]\ndataset containing samples from a single speaker.\nExperimental Details. We keep the conventional settings as in\nprevious papers [22-24] and perform position-fixed cropping dur-\ning noise generation and training procedure with a batch size of\n15. To ensure the effectiveness of fine-tuning, we set the number of\ntraining iterations to 200, with 200 iterations for noise generation.\nWe set the noise boundary e as 8/255 to strike a balance between\nenhancing the anti-cloning protective effect and maintaining hu-\nman perceptibility about the embedded noise. The experiments are\nconducted on one NVIDIA A800 GPU with 80GB memory.\nMetrics. To comprehensively evaluate the effectiveness of training\non both clean and protected datasets, we consider objective metrics\nsuch as mel-cepstral distortion (MCD) with dynamic time warping\nand word error rate (WER) (%), along with subjective evaluation\nusing mean opinion score (MOS) with 95% confidence intervals.\nFor objective assessment, WER reflects the recognizability of pro-\nnunciation in a given speech, utilizing the pre-trained medium\nsize of Whisper [39], an open-source speech recognition model\nfrom OpenAI. MCD can reflect the differences in terms of timbre,\nspeech content, and duration time between the generated and syn-\nthetic speech. For the subjective metric, MOS denotes the most\nintuitive reflection of the usability of these deepfake speeches be-\ncause malicious speech synthesis aims to spoof human perception\nby marking a 0 to 5 score for each audio. The higher value of WER\nrepresents more unclarity of a speech, and the higher value of MCD\nreflects less-than-ideal training results. Moreover, for speech im-\nperceptibility, we consider the signal-to-noise ratio (SNR)(dB) and"}, {"title": "5.2 Effectiveness of POP Method", "content": "In this section, to verify the protective effectiveness of our method,\nwe conduct experiments on two large-scale multi-speaker datasets\nwith each of the three TTS models as we mentioned previously.\nThe evaluation involves comparing ground truth with generated\nsynthesized audio by different methods using the same sentence\nspoken by the same speaker.\nWe process these training samples in different ways. First, we use\na clean dataset without perturbation to verify the model's outstand-\ning speech synthesis performance. In addition, we utilize baseline\nmethods and our proposed data protection method POP to generate\nspecific perturbations for each TTS model and evaluate the results\nwhen training on the protected dataset.\nTable 1 presents our experimental results. It shows that models\nperform well with relatively moderate fine-tuning on the selected\ndatasets. For instance, the WER of samples generated by the MB-\niSTFT-VITS model differs from ground truth by only 6.5%, which is\nalso reflected in subjective evaluations in Section 5.4, indicating the\neffectiveness of our fine-tuning approach. When random noises are\nadded, the synthesized audio by the model exhibits a small number\nof noises resembling background sounds, yet the articulation and\nclarity of the speaker's pronunciation remain comparable to clean\ndata. For example, the synthesized speech quality difference is\nnegligible for VITS trained on the clean and randomly noisy CMU\nARCTIC dataset, suggesting that random noise does not render the\ndata samples unlearnable. The random noise-added dataset shows\nthat undesigned perturbations cannot interfere with the model to\nachieve data protection.\nAs shown in Table 1, the audio data protected by our proposed\nPOP data protection method can be employed on different TTS\nmodels and achieve a remarkably effective anti-cloning effect. On\nthe LibriTTS dataset, the WER and MCD of the MB-iSTFT-VITS\nmodel increase significantly from 5.830% and 21.939% when trained\non clean samples to 13.646% and 127.310% trained on POP-protected\ndataset, indicating that the model merely learns any relevant voice\ntimbre and text alignment information from the protected wave-\nform. The synthesized audio is almost entirely noise without rele-\nvant audio information from an auditory perspective. At the same\ntime, similar effects are observed on other models and datasets. The\nexperiments in this section demonstrate that the POP method has\nsignificant effectiveness in preventing audio data from high-quality\nmalicious voice cloning."}, {"title": "5.3 Transferability Analyses", "content": "When generating noise using different models, we employ the same\nmodel in fine-tuning and noise generation. However, there are\nvarious advanced TTS models. Therefore, the question arises:\nCan the unlearnable samples produced by one surrogate\nmodel be effective on other models as well?\nIn this section, we utilize MB-iSTFT-VITS as the surrogate model\nto protect datasets. Then we train on two other models using sur-\nrogate model-protected audio to verify the transferability and gen-\neralization of the specific perturbation across even unseen models."}, {"title": "5.4 User Study", "content": "In Sections 5.2 and 5.3, we performed a comprehensive objective\nverification study of our proposed method using three distinct TTS\nmodels. A crucial aspect of the method evaluation was examin-\ning the human perception of deepfake speeches generated by TTS\nmodels when training on different perturbation-embedded datasets.\nFor each protection method, we randomly selected two audio sam-\nples and created a questionnaire via the Credamo platform. We\ninvited 73 participants to complete the questionnaire, ultimately\nobtaining 61 valid responses. To best reduce subjective biases, we\nimplemented three key measures: (1) anonymizing the purpose of\nthe questionnaire and the names of the audio samples, (2) including\ntwo arithmetic questions to ensure participants' attentiveness and\n(3) randomizing the order of the samples to prevent the order of the\nsamples from having an impact on participant ratings. Finally, we\ncalculated the subjective metric MOS with 95% confidence intervals,\nfollowing the principles outlined in [27].\nTable 2 shows the subjective experimental results across three\nTTS models and various methods. From the results in Table 2, it\ncan be found that unprotected clean data can easily be exploited to\nsynthesize high-quality realistic audio. On the LibriTTS dataset, the\nsubjective evaluation result of MB-iSTFT-VITS is 4.50\u00b10.25, which\nis almost comparable to the 4.71\u00b10.21 of real audio. Unprotected\ndata brings the owner of the audio into a potentially dangerous\narea, causing leakage of sensitive data such as voiceprint. On the\ncontrary, employing POP to protect the dataset cannot synthesize\naudio that deceives the human ear perception, greatly reducing\nthe risk of data being in a dangerous area. On the VITS model,\nits MOS value is only 0.13\u00b10.21, meaning that the synthesized\naudio is just the noisy audio content to the human ear and they\nwill not be deceived by these \"bad\" audio. This demonstrates that\nour approach is delivering significant results in securing data and\neffectively preventing the leakage of sensitive information."}, {"title": "5.5 Ablation Study", "content": "In this section, we conduct the ablation study on the selection of\ndifferent objective functions and random segment training with a supplemental defensive strategy to improve the protection effect in\nthis scenario. Moreover, we use Eq. (2) to select and conduct this\nexperiment on the single speaker most similar to the pre-trained\nspeaker, which will be detailed in Section 5.6.\nObjective Analyses. In Section 4.4, we have introduced the chal-\nlenges when using the perturbative protection strategy, and regard\na backbone model VITS as an example to illustrate the reasons for\npivotal function selection and the detailed information about POP\nstrategy. Our method POP selects $L_{recon}$ in Eq. (6) as the optimiza-\ntion objective, because choosing this function can bring about the\nbest protection effect, and generally speaking, generative TTS mod-\nels usually regard the reconstruction loss as one of the objectives\nin multi-objective optimization. In this experiment, we will explore"}, {"title": "5.6 Robustness against Adaptive Attackers", "content": "In the real world, we cannot predict the training methods that\nvarious adaptive attackers will adopt. A stronger adversary can\nkeenly perceive the difference between audio protected by the\nPOP method and unprotected clean audio. Therefore, they can use\nvarious attack methods targeting perturbations, with perturbation\nremoval technology and data augmentation technology being the\nmost commonly used. In the previous experiments in Sections\n5.2, 5.3, and 5.4, we utilize large-scale multi-speaker datasets for\nevaluation, typically with thousands of training samples. There is\na more realistic scenario that attackers cannot access such a large\nnumber of samples. Therefore, we select a single speaker with the\nclosest voiceprint similarity to the pre-trained speaker, containing\n117 samples that only last 16 minutes, as used in Section 5.5 to\nsimulate the most challenging protection scenarios. In this section,\nwe utilize the VITS model to conduct noise reduction and speech\naugmentation techniques.\nNoise Reduction. From the experiment in Section 5.2, it can be\nfound that the addition of perturbations to clean samples, whether\ngenerated specifically or randomly, will have a certain weakening\neffect on the TTS models. When an attacker obtains audio protected\nby POP, the experienced attackers will detect the embedded abnor-\nmal perturbations and become suspicious of the obtained training\nsamples. In the audio field, their most effective way is to directly\nuse perturbation removal methods on the added perturbations to\neliminate the protection effect caused by noise. However, due to\nthe lack of clean samples from the adversary, they cannot obtain\nunprotected samples based on our public perturbation generation\nmethods. Usually, they employ perturbation removal techniques\nin the audio field. In this experiment, we simulate the adversary\nusing the specific noise reduction technique [41] using spectral\ngating which computes the spectrogram of a speech and estimates\na perturbation threshold for each frequency band to denoise.\nAfter applying the noise reduction method to denoise the sam-\nples protected by POP, the resulting samples do not contain a noisy\nbackground, and the effect of the perturbations we embedded has\nbeen highly destroyed. When training the VITS model using the\naudio after denoising with noise reduction, the values of MCD and\nWER are 8.699 and 83.419% respectively compared to 10.838 and\n109.571% without the noise reduction. This indicates that after de-\nnoising the embedded perturbation, the POP method still maintains"}, {"title": "6 DISCUSSIONS AND LIMITATIONS", "content": "In this section, we outline some discussion points about the detailed\ninformation and limitations of our proposed method.\nAudibility Analyses. The perturbation we embedded on the origi-\nnal audio should satisfy a better imperceptibility to guarantee audio\nusability in realistic scenarios and reduce the adversary's alertness\nto detect anomalously embedded noise. Figure ?? and ?? shows the\nperceptibility of the perturbation for the three models, where the\nPOP method provides the best protective effect, with the SNR and\nPESQ of 17.894 and 3.551, respectively, in the VITS model, which\nrepresents high audio usability. The POP approach causes the least\nchange to the dataset and has the highest imperceptibility compared\nto the EM approach and whole-segment audio protection.\nIn addition, the added noises are constrained by the lp norm\nstrictly so that the maximum value of the perturbation does not\nexceed the radius e, limiting the perturbation interference effect\nwith the audio. We calculate the mean square error (MSE) between\nthe original and protected audio to be 99%, which means that the"}, {"title": "7 CONCLUSION", "content": "In this paper, we focus on the defense mechanism against unautho-\nrized exploitation of audio samples. It may bring huge threats in\nthe real world when uploading our sensitive information on social\nplatforms without protection. To cope with this and achieve privacy-\npreserving, we devise an effective and transferable perturbative\ndata protection method named Pivotal Objective Perturbation. It\naims to make training samples unlearnable by applying specific\nimperceptible error-minimizing perturbation. Compared to the EM\nmethod in PAP, our proposed POP can not only reduce the com-\nputing resources but also achieve a better data protection effect\nbecause not all the functions can be optimized by the perturbation\nand the pivotal optimization can reduce the model error better.\nWe validate our method's effectiveness on common datasets with\nadvanced TTS models. Subjective and objective evaluations have\nshown their outstanding validity in protecting audio data. The gen-\nerated noise transferred well across models, showing our method's\nimprovement. Moreover, our method remains highly robust against\nadaptive adversaries. Compared to existing works, our approach\noffers a convenient, novel, and transferable audio protection mech-\nanism in the TTS domain by rendering samples unlearnable. This\nrepresents a step toward the security and privacy of audio data,\nmitigating the potential risks associated with voice cloning."}, {"title": "8 ACKNOWLEDGEMENT", "content": "We thank the reviewers for their insightful and helpful feedback\non our work. This work was supported in part by the National\nKey Research and Development Program of China under Grant\nNo. 2020YFB1805400 and the Fundamental Research Funds for the\nCentral Universities under Grant No. 2024ZCJH05."}, {"title": "A DETAILED INFORMATION", "content": "In this section, we illustrate the detailed descriptions of speech\ntransformation techniques conducted in Section 5.6 by Table 6."}]}