{"title": "Pre-training Graph Neural Networks on Molecules by Using Subgraph-Conditioned Graph Information Bottleneck", "authors": ["Van Thuy Hoang", "O-Joun Lee"], "abstract": "This study aims to build a pre-trained Graph Neural Network (GNN) model on molecules without human annotations or prior knowledge. Although various attempts have been proposed to overcome limitations in acquiring labeled molecules, the previous pre-training methods still rely on semantic subgraphs, i.e., functional groups. Only focusing on the functional groups could overlook the graph-level distinctions. The key challenge to build a pre-trained GNN on molecules is how to (1) generate well-distinguished graph-level representations and (2) automatically discover the functional groups without prior knowledge. To solve it, we propose a novel Subgraph-conditioned Graph Information Bottleneck, named S-CGIB, for pre-training GNNs to recognize core subgraphs (graph cores) and significant subgraphs. The main idea is that the graph cores contain compressed and sufficient information that could generate well-distinguished graph-level representations and reconstruct the input graph conditioned on significant subgraphs across molecules under the S-CGIB principle. To discover significant subgraphs without prior knowledge about functional groups, we propose generating a set of functional group candidates, i.e., ego networks, and using an attention-based interaction between the graph core and the candidates. Despite being identified from self-supervised learning, our learned subgraphs match the real-world functional groups. Extensive experiments on molecule datasets across various domains demonstrate the superiority of S-CGIB.", "sections": [{"title": "Introduction", "content": "Graph Neural Networks (GNNs) have recently emerged in computational chemistry, offering powerful tools for predicting molecular properties (Gilmer et al. 2017; Kearnes et al. 2016). While GNNs have shown remarkable performance in molecular property prediction, their effectiveness depends on the availability of abundant labeled molecules for model training (Hao et al. 2020; Hoang et al. 2023).\nRecently, pre-training strategies have offered considerable potential in overcoming the challenges of the scarcity of labeled molecular data (Rong et al. 2020; Luong and Singh 2023). The existing pre-training strategies can be categorized into three primary groups: node-level pre-training, contrastive learning, and subgraph-level pre-training. Node-level pre-training mainly focuses on node-level prediction, e.g., node attribute reconstruction or edge prediction, which may not fully leverage the high-order structure of molecules, i.e., functional groups (Rong et al. 2020). The second strategy is contrastive learning, which focuses on learning representations by contrasting multiple views of molecules based on random or heuristic augmentations (You et al. 2021; Xu et al. 2021). More recently, subgraph-level strategies focus on semantic subgraphs, which can capture both local and global structural patterns by identifying functional groups (Subramonian 2021; Zhang et al. 2021; Rong et al. 2020; Liu et al. 2023; Inae, Liu, and Jiang 2023). The main idea is to use human annotations or prior knowledge to extract the semantic subgraphs, e.g., frequent subgraphs across molecules, to enhance recognizing significant substructures and molecular property prediction (Luong and Singh 2023; Degen et al. 2008). To sum up, most recent pre-training strategies aggregate information from node-level or subgraph-level to generate graph-level representations.\nHowever, two challenges limit the existing pre-training strategies on molecules. First, the existing strategies lack the ability to generate well-distinguished graph-level representations. Most node-level strategies mainly focus on the local structure and then adopt a pooling function, e.g., mean, max, or sum, to aggregate information, resulting in poor-distinguished graph-level representations as information from noisy and redundant nodes can be aggregated to form graph-level representations (Hu et al. 2020a). Besides, while subgraph-level strategies (Subramonian 2021; Zhang et al. 2021) can capture specific subgraphs at multiple scales, they could overlook the entire graph-level distinctions (Rong et al. 2020; Inae, Liu, and Jiang 2023). That is, subgraph-level representations based on discrete pre-defined patterns, e.g., frequent subgraphs, ignore global interactions between important nodes that derive the molecule's entire structure. For contrastive learning strategies, applying augmentation schemes, e.g., edge perturbation, could potentially disrupt the structures and properties of molecules (Lee, Lee, and Park 2022). Second, it is challenging for subgraph-based strategies to cover all possible functional groups given in diverse molecule datasets. To capture functional groups, recent subgraph-level strategies create dictionaries based on pre-defined rules, e.g., counting the discrete occurrences of subgraphs across molecules to decompose molecules into a bag of subgraphs, commonly prioritizing subgraphs with larger sizes and frequent occurrences (Luong and Singh 2023; Kong et al. 2022). The prioritization and static dictionaries could limit the model's ability to capture new and uncommon functional groups.\nIn this paper, we overcome the above challenge of pre-training strategies by considering how to (1) generate well-separated graph-level representations and (2) automatically capture significant subgraphs without explicit annotations or prior knowledge. The fundamental idea behind our strategy is that, across the chemical domain, molecules share universal core subgraphs that can combine with specific significant subgraphs to robust representations of molecules.\nTo generate well-distinguished graph-level representations, this initiates a problem in recognizing a set of important nodes (graph cores) that can allow robust and well-separated representations (Hu et al. 2020a). We interpret this problem as the Graph Information Bottleneck (GIB) principle, which aims to compress an input graph into a core subgraph that keeps sufficient and compressed information about the input graph (Yu et al. 2021). However, current GIB methods learn compressed subgraphs by minimizing information loss in predicting the graph labels, which still require the label information. To solve it, we propose a Subgraph-conditioned Graph Information Bottleneck (S-CGIB) for self-supervised pre-training to compress an input graph into a graph core conditioned on specific significant subgraphs without using label information. First, the graph cores contain important nodes, which could generate robust representations under the S-CGIB principle. Second, toward capturing functional groups without prior knowledge, we suppose that graph cores then could reconstruct input graphs conditioned on specific significant subgraphs across molecules.\nTo discover the significant subgraphs, as we intentionally ignore using prior knowledge about functional groups, we propose to generate a set of functional group candidates, i.e., ego networks rooted at each node. The reason is that the graph core of molecules typically consists of central substructures composing important nodes only for generating well-separated representations. In other words, the compression process ignored unimportant nodes, which can be a part of functional groups in terms of molecular properties. Then, we propose an attention-based interaction between the graph core and candidates to recognize significant subgraphs as functional groups. The attention coefficients can highlight significant subgraphs, which benefit from recognizing functional groups across molecules."}, {"title": "Related work", "content": "We now discuss how the existing pre-training strategies can learn the molecular structure and chemical properties in the context of Self-Supervised Learning compared to our method. Early pre-training strategies focus on learning node representations, which are then aggregated into a single graph-level representation through pooling mechanisms, e.g., min, max, or sum, (Hu et al. 2020b,a). The node-level strategies can be mainly grouped into node-level structure reconstruction, e.g., neighborhood prediction (Hu et al. 2020b; Hoang and Lee 2024), or node feature reconstruction (Devlin et al. 2019). However, these methods focus on learning to distinguish individual node representations, which do not directly handle the challenge of incorporating node embeddings into a single graph-level representation. Furthermore, node-level pre-training strategies could be limited to capture the high-order structures, i.e., functional groups.\nAnother line is contrastive learning, which is generally grouped into two categories: node-level contrastive methods and graph-level contrastive methods (Liu et al. 2022; St\u00e4rk et al. 2022; Qiu et al. 2020). The node-level contrastive learning strategies seek to generate multiple node views by applying augmentation schemes, e.g., edge perturbation, which modifies the graph connectivity while preserving the node's identity. For example, GraphCL (You et al. 2020) adopts multiple augmentation schemes to generate multiple views of the input graph and uses contrastive loss to obtain embeddings of these views closer. The idea of JOAO (You et al. 2021) is to automatically search schemes to find the most effective augmentations. In contrast, graph-level methods generate multi-views of an input graph and guarantee similar representations while discriminating them from the other graph-level representations (Xu et al. 2021). Such augmentations often change the molecular connectivity and structure, failing to preserve the molecule's properties.\nRecent pre-training approaches on molecules emphasize recognizing and learning semantic patterns, such as functional groups, across molecule data (Zhang et al. 2021; Liu et al. 2023; Inae, Liu, and Jiang 2023). The semantic subgraphs can be discovered under pre-defined rules with the use of prior knowledge or human annotation. For example, GROVER (Rong et al. 2020) extracts 85 frequent functional groups and employs a Self-supervised Learning (SSL) task to predict the presence of the functional groups. MGSSL (Zhang et al. 2021) employs depth-first or breadth-first search to discover the semantic subgraphs at multiple scales. Recently, GraphFP (Luong and Singh 2023) decomposes molecules into bags of functional groups by composing a dictionary via frequent subgraph mining, i.e., common and large frequent subgraphs across molecules. Such strategies rely on pre-defined vocabulary with discrete counting frequent subgraphs in molecules, making it challenging to build a complete dictionary to cover new or less common functional groups. In contrast, we automatically discover functional groups via attention-based interaction between the molecular core and a set of functional group candidates."}, {"title": "Problem Descriptions", "content": "We study the problem of pre-trained GNNs on molecules, which recognize graph cores conditioned on significant subgraphs (functional group candidates) under the S-CGIB principle. Thus, we first present notations and then the definition of S-CGIB, which is a modification of GIB and CGIB.\nA molecule can be represented as a graph $G = (V, E)$, where $V = \\{V_1, V_2, \u2026\u2026\u2026, v_n\\}$ represents the set of atoms and $E$ denotes the set of bonds. $G$ is associated with its adjacency matrix $A$ and feature matrix $X$. Let $N_k(v)$ be a set of neighboring nodes within a k-hop distance from the root node $v$. The set of functional group candidates in $G$ is defined as: $S = \\{G[N_k(v)] | v \u2208 V\\}$, where $G[N_k(v)]$ is the k-hop ego network rooted at node $v$.\nRecently, the Information Bottleneck (IB) principle (Tishby, Pereira, and Bialek 2000) has been used on graphs, called GIB, to discover a core subgraph from an input graph.\nDefinition 1 (GIB) The GIB principle was originally introduced by Yu et al. (2021) to recognize a compressed and informative subgraph from an input graph. Given an input graph $G$ and its label $Y$, the compressed graph, as graph core $G_c$, is discovered as:\n$\\min_{G_c} - I(Y; G_c) + \u03b2I(G; G_c),$ (1)\nwhere $\u03b2$ is a Lagrange multiplier used to balance the two terms. The first term encourages $G_c$ to be informative to the graph label $Y$, and the second term is the compression term, which minimizes the mutual information of $G$ and $G_c$.\nIn the context of the presence of side information $T$, several studies have accounted for the conditional information $T$, named CIB (Chechik and Tishby 2002; Gondek and Hofmann 2003). We then apply CIB on graphs as Conditional Graph Information Bottleneck (CGIB).\nDefinition 2 (CGIB) Given an input graph $G$ and its label $Y$, the graph core $G_c$ is discovered given the side information $T$, as:\n$\\min_{G_c} - I(Y; G_c\\|T) + \u03b2I(G; G_c).$ (2)\nThe first term quantifies how much information the graph core $G_c$ retains about $Y$, given the side information $T$. The variable $T$ is supposed to be known as prior knowledge.\nTo employ the CGIB principle under an SSL task without prior knowledge, we propose to minimize the first term of Eq. 2 by replacing the prediction with a reconstruction task. As the compression process ignored unimportant nodes, which can be a part of functional groups, we suppose they can be side information for a graph reconstruction task. The key idea is that molecules share universal graph cores, which can reconstruct the molecule structure conditioned on specific significant subgraphs. Specifically, we suppose that $G$ is formed by combining a graph core $G_c$ and a set of functional group candidates $S$, such that $G = G_c \\cup S$. Then, the Subgraph-conditioned Graph Information Bottleneck can be defined below:\nDefinition 3 (S-CGIB) Given an input graph $G$ and a set of functional group candidates $S$, we define the S-CGIB principle conditioned on the subgraph $S$ as:\n$\\min_{G_c} I (G; G_c\\|S) + \u03b2I (G; G_c),$\n(3)\nBy conditioning on $S$, the first term encourages the graph core $G_c$ to capture sufficient information for reconstructing input graph $G$ (conditional reconstruction), while the graph core also needs to be compressed from the input graph (compression). Overall, jointly optimizing the two terms allows $G_c$ to be compressed and preserve the input graph structure conditioned on $S$. Since $S$ consists of all ego networks rooted at each node, to discover important ego networks, i.e., functional groups, we propose an attention-based strategy detailed in the following Section."}, {"title": "Methodology", "content": "Graph Compression The overall architecture of S-CGIB is shown in Figure 1. Given an input graph G with adjacency matrix A and node feature matrix X, we learn node representations in G through a GNN encoder as:\n$H = f_\u03b8(X, A),$ (4)\nwhere $H \u2208 R^{N\u00d7d}$ refers to node embeddings, $f_\u03b8(\u00b7,\u00b7)$ is a GNN encoder, e.g., GIN (Xu et al. 2019). Inspired by recent VGIB principle (Yu, Cao, and He 2022) that injects noise information into an input graph to obtain important nodes, we compress $G$ by injecting noise into $H$ to obtain a graph core $G_c$ with new node representations $Z$, which is a bottleneck to distill the important nodes, thereby generating well-distinguished graph-level representations. The key idea is that the important nodes will be injected with less noise information compared to unimportant nodes. Specifically, given the embedding matrix $H$, for each node $v_i$, S-CGIB learns a probability $p_i$ with a multi-layer perceptron (MLP) followed by a Sigmoid(\u00b7) function. We then replace the node representation $h_i$ by $\u03f5$ with probability $p_i$, as:\n$p_i = Sigmoid (MLP(H_i)),$ (5)\n$\u03f5 ~ N (\u00b5_H, \u03c3_H^2 I),$ (6)\n$\u03bb_i ~ Bernoulli (p_i),$ (7)\n$z_i = \u03bb_ih_i + (1 \u2212 \u03bb_i)\u03f5,$\nwhere $\u03bb$ is obtained by sampling from Bernoulli distribution parameterized with the probability $p_i$, $\u00b5_H$ and $\u03c3_H$ are the mean and variance of $H$, $\u03f5$ is sampled from $H$ based on Gaussian distribution. Therefore, the information of the input graph is compressed into $Z$ with the probability of $p_i$ by masking unimportant nodes with noisy information. That is, the compression process ensures that $Z$ focuses on the important nodes (graph core) while discarding irrelevant and unimportant nodes. To allow the differentiable sampling, we adopt the Gumbel sigmoid method (Jang, Gu, and Poole 2017; Maddison, Mnih, and Teh 2017), i.e., $\u03bb_i = Sigmoid(^{1/\u03c4} [\u2212log[pi/(1 \u2212 pi)] + log [q/(1 - q)]]$ where $q ~ Uniform (0, 1)$ and $\u03c4$ is the temperature parameter. For the detailed compression optimization process, we refer readers to the Model Optimization Section.\nSubgraph Learning The next problem is to extract functional group candidates and encode them to obtain vector representations. Let $G[N_k(v)]$ denotes the k-hop ego network rooted at the node $v$. We first apply a GNN encoder on nodes within the $G[N_k(v)]$. Then, the subgraph-level representation rooted at node $v$ is obtained via a pooling function $POOL(\u00b7)$. Formally, the representations of ego networks in the input graph $G$ can be computed as:\n$h_v^{(l+1)} = g_\u03b8^{(l)} (G [N_k (v)]), l = 0, \u2026, L \u2212 1,$ (8)\n$h_v = POOL (h_v^{(L)} | v \u2208 N_k (v)),$ (9)\n$H_S = [h_v | v \u2208 V],$ (10)\nGraph Core and Subgraph Interaction Note that each functional group has distinct chemical characteristics that contribute differently to the overall molecule behavior. To capture significant subgraphs across molecules, we then propose an attention-based interaction between the graph core and subgraph candidates to highlight specific significant subgraphs. Specifically, we first employ a pooling function to aggregate important node features $Z$ into $z$, i.e., $z = POOL(Z)$, to obtain a single vector representation of the graph core. We then calculate the normalized attention coefficients for each functional group candidate. Formally, the coefficient of a subgraph candidate rooted at a node $v_i$ can be computed as:\n$\u03b1_i = \\frac{exp ((z||H_S) W^T)}{\\sum_{j=1}^N exp ((z||H_S) W^T)},$\n(11)\nwhere $W_a \u2208 R^{1\u00d72d}$ refers to learnable projection and $H_S$ is the representations of the ego network rooted at node $v_i$. Thus, the interaction could capture the correlation between the graph core and functional group candidates. The final representations are then concatenated along with the coefficients as:\n$H_i = z|| (\u03b1_iH_S).$ (12)\nIn a nutshell, given an input graph G, our model jointly learns the graph core and the significant subgraphs under the S-CGIB principle. We first inject noisy information into H to obtain the graph core Z under the compression process. Meanwhile, we capture the significant specific subgraphs from the subgraph candidates (ego networks rooted at each node) through the attention-based interaction between the graph core and the ego networks under the graph reconstruction."}, {"title": "Model Optimization", "content": "To train the model while optimizing the graph core conditioned on $S$, we optimize the objective function:\n$G = arg min I(G; G_c\\|S) + \u03b2I(G; G_c),$ (13)\nwhere each term denotes the conditional graph reconstruction and compression, respectively. Then, we present upper bounds for each term to guide the optimization.\nMinimizing \u2212I(G; Gc\\|S) The first term of Eq. 13 denotes a reconstruction of the input graph G, given Gc conditioned on S. Thus, we utilize the chain rule for mutual information on the Conditional Graph Reconstruction term as follows:\n$-I(G; G_c\\|S) = min -I(G; G_c, S) + I(G; S).$\n(14)\nWe observed that including the second term, i.e., $I(G; S)$, into our objectives severely degrades the overall model performance (Appendix D.1). That is, the model performs worse as we push the input graph G and its functional group candidates $S$ far apart. Therefore, we only minimize the first term of Eq. 14.\nThe first term of Eq. 14 can be bounded as follows:\n$-I(G; G_c, S) \u2264 E_{G;G_c,s}[\u2212log p_s (G|G_c, S)],$ (15)\nwhere $p_s (G|G_c, S)$ is a variational approximation of $p(G|G_c, S)$, which outputs the input graph G (see Appendix A.1). Thus, we model $p_s (G|G_c, S)$ as a graph structure reconstruction parametrized by $\u03b8_s$, which outputs the graph $G$ based on the $G_c$ and $S$. Therefore, we can minimize the upper bound of \u2212I(G; Gc, S) by minimizing the graph reconstruction loss $L_{rec}(G; G_c, S)$, which can be modeled as graph structure recovery. Specifically, given the output representation $H$, to minimize the graph structure loss, we first capture the similarity between any two nodes $v_i$ and $v_j$ in $H$ by employing cosine similarity, i.e., $\u00c2_{i,j} = \\frac{(H_i \u22c5 H_j )}{\\|H_i\\|\\|H_j \\|}$ (Zhang et al. 2020). The reconstruction loss then can be defined as follows:\n$L_{rec} = \\sum_{i,j} {||A - \u00c2||_F},$ (16)\nwhere $A$ refers to the original adjacency matrix of $G$ and $|| \u00b7 ||_F$ is the Frobenius norm.\nMinimizing I (G; Gc) To minimize the second term of Eq. 13, we employ the sufficient encoder assumption that the latent representation Z is lossless in the encoding process, i.e., $I(Z\\|H) \u2248 I(G_c\\|G)$. To optimize the graph core $G_c$, we can employ a variational upper bound that is tractable and can be minimized during training (Yu, Cao, and He 2022). Formally, given the mutual information $I(G; G_c)$, the variational upper bound of $I(G; G_c)$ is:\n$L_{MI} (G,G_c) \u2264 E (\u2212\\frac{1}{2\u03c3} log P + \\frac{1}{2\u03c4} log P + \\frac{1}{2\u03c3_H^2} log Q),$ (17)\nwhere $P = \\sum_{j=1}^N (1 \u2212 \u03bb_j)^2, Q = \\sum_{j=1}^N \u03bb_j \\frac{(H_j\u2212\u00b5)^2}{\u03c3_H^2} $, and $\u03bb$ is computed from Eq. 6. For the proof of the upper bound, we refer readers to Appendix A.2.\nContrastive Learning We note that minimizing the upper bound of I(G,Gc) from Eq. 17 could lead to overcompression with sufficient information on S. That is, the graph core $G_c$ can be too distinguished from its input graph $G$ compared to other graphs. We argue that the ideal core should at least satisfy the high mutual information with its input graph compared to others during the compression process, as $I (G_c, G) > I (G_c, \\backslash G)$, where $\\backslash G$ refers to remaining graphs, excluding $G$. Thus, we propose to use a contrastive learning-based method to maximize the agreement between the graph core and its input graph. Specifically, the contrastive objective is computed as:\n$L_{con} = - \\frac{1}{B} \\sum_{i=1}^B log \\frac{exp (s (z^i, H^i))}{\\sum_{j=1, j \\neq i}^B exp (s (z^i, H^j))},\\$\n(18)\nwhere B denotes the number of graphs in a mini-batch, s(\u00b7,\u00b7) is the cosine similarity between graph core and input graph, $Z = POOL(Z)$, and $H = POOL(H)$.\nThe overall loss for the pre-training task is as follows:\n$L_{total} = L_{con} + L_{rec} + \u03b2L_{MI},$\n(19)\nwhere $\u03b2$ is a hyperparameter to balance the compression and structure preservation trade-off.\nDomain Adaptation We note that our pre-trained model can learn significant subgraphs only on the domains of pre-training datasets. However, generalizing toward varied downstream tasks can be challenging due to the node attribute distinctions in specific downstream datasets. That is, while pre-training primarily focuses on structural molecular features, node feature reconstruction is also essential and helps our model adapt to learn node feature characteristics in downstream datasets. Therefore, we employ an unsupervised domain adaptation after pre-training, which acts as a domain-oriented generalization for downstream datasets. Then, we utilize a loss function to reconstruct node feature information for each graph as:\n$L_{att} = \\frac{1}{\\|V\\|} \\sum_{v_i\u2208V} {\\|x_i - \\hat{x_i}\\|_2},$ (20)\nwhere $x_i$ is the initial feature of node $v_i$ and $x_i = MLP(\u0124_i)$."}]}