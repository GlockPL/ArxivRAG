{"title": "Feature Importance Depends on Properties of the Data: Towards Choosing the Correct Explanations for Your Data and Decision Trees based Models", "authors": ["C\u00e9lia Wafa AYAD", "Thomas Bonnier", "Benjamin Bosch", "Sonali Parbhoo", "Jesse Read"], "abstract": "In order to ensure the reliability of the explanations of machine learning models, it is crucial to establish their advantages and limits and in which case each of these methods outperform. However, the current understanding of when and how each method of explanation can be used is insufficient. To fill this gap, we perform a comprehensive empirical evaluation by synthesizing multiple datasets with the desired properties. Our main objective is to assess the quality of feature importance estimates provided by local explanation methods, which are used to explain predictions made by decision tree-based models. By analyzing the results obtained from synthetic datasets as well as publicly available binary classification datasets, we observe notable disparities in the magnitude and sign of the feature importance estimates generated by these methods. Moreover, we find that these estimates are sensitive to specific properties present in the data. Although some model hyper-parameters do not significantly influence feature importance assignment, it is important to recognize that each method of explanation has limitations in specific contexts. Our assessment highlights these limitations and provides valuable insight into the suitability and reliability of different explanatory methods in various scenarios.", "sections": [{"title": "1 Introduction", "content": "Decision tree-based models such as random forest [1] are widely used machine learning algorithms in data science. Although deep learning has been increasingly popular, especially in domains such as computer vision and natural language processing, random forest, for example, continues to be a competitive option in many types of tabular data in a diverse number of domains, including biology [2] and medicine [3], where interpretation is paramount. Small decision trees operating on understandable feature spaces are naturally interpretable, and although this interpretability is diluted across a large forest, it can be recovered in terms of feature importance, which is a major tool that can be used in practical applications for data understanding, model improvement, or model explainability. However, practitioners may lose trust in the importance scores provided for random forest [4], or simply be unable to use them to answer their research questions from the feature importance result due to a number of reasons [5, 6], for example: (1) a relative lack of training examples leads to instability where the importance scores change due to only minor changes or additions to the dataset or hyperparameters. (2) Even with a large training set, multiple (possibly equivalent) feature scores can be presented. (3) the feature importance scoring mechanism is thrown off by particular properties of the data distribution such as noise, imbalance, and feature type (in particular, the importance of continuous features is often overestimated). (4) results where the importance of the feature is assigned to spurious or even random features. Practitioners are thus often right to be reluctant to draw conclusions from or place trust in off-the-shelf feature-importance scorers, and we aim to remedy this to some extent with a benchmarking study.\nTo remedy this, researchers proposed explainability methods such as LIME [7] and SHAP [8] to explain black-box models by attributing feature importance estimates as explanations of the model's predictions. While prior research [9\u201313] has already taken the first steps towards analyzing the disagreement of explanation methods for models such as deep neural networks, analyzing the behavior of the wide range of existing explanation methods for random forest or in general ensemble trees still insufficiently explored, with regard to particular data properties and model parameters [14].\nCompared to other work, we study the explainability methods suited to explain decision tree-based models. Some of these methods are specific to tree ensembles and the rest are general model-agnostic (which, thus, can also be applied to random forest). We do so with extensive experiments on synthetic alongside real-world datasets, and certain manipulations thereof, which we carry out to isolate and identify aspects which lead to particular results insofar as feature importance. This provides a more thorough understanding, which we use to highlight some limitations of existing methods, and formulate a number of recommendations for practitioners. The contribution in this paper is twofold.\n\u2022 Conduct a thorough evaluation of various explainability methods in the context of specific data properties, such as noise levels, feature correlations, and class imbalance, elucidating their strengths and limitations.\n\u2022 Providing valuable guidance for practitioners and researchers on selecting the most suitable explainability method based on the characteristics of their dataset."}, {"title": "2 Background", "content": "Post-hoc local explanations for tree based models\nPost-hoc explanation methods can be classified based on explanation scope (global vs. local), model architecture (specific vs. agnostic) and basic unit of explanation (feature importance vs. rule-based). This paper focuses on local post-hoc explanation methods based on feature importance. It analyzes four model agnostic methods in Table 1 (LIME, LSurro, Kshap and Sshap) and two tree-specific methods (Tshap and TI).\nLIME, LSurro, Kshap and Sshap construct local interpretable models such as a linear regression in the neighborhood of the instance that is being explained. These methods differ in the kernel used for the generation of the local neighborhood and the objective that is being optimized. For example, LIME uses an exponential kernel while SHAP explainers use Shapley kernel, and both use the squared error to minimize the loss between the black box model and the surrogate model. On the other hand, Tshap and TI both are designed to explain tree based methods such as random forest. Tshap computes the Shapley value for each node in the tree based on the decision splits. While TI decomposes the prediction into the sum of feature contributions and the bias, i.e; the mean given by the root of the decision tree that covers the entire training set.\nProperties (metrics) of local explanations\nSince we cannot measure the accuracy of feature importance estimates due to the absence of ground truth feature importance, prior research proposed objectively assessing the quality of explanations through various metrics, such as examining stability and compactness of the local explanations, the faithfulness of the interpretable model to the black-box predictions [18], robustness to input perturbations [12] and fairness across subgroups [19]. The stability metric [18] compares the explanation given to an instance in its neighborhood. If two instances have similar feature values and predictions, they should have similar explanations. While the compactness metric shows whether it is possible to explain the model's prediction with fewer features, so that it is easily understood by humans. To measure the robustness of the interpreters, local fidelity and stability were also proposed [20, 21]. The faithfulness can also be measured by the consistency, the feature and rank agreements [9] between the explanation and the ground truth feature importance or between pairs of explanations generated by different methods."}, {"title": "3 Related Work To explainability Benchmarking Frameworks", "content": "The landscape of explainable artificial intelligence has witnessed a surge in research efforts aimed at understanding and evaluating the diverse methodologies employed for interpreting complex machine learning models. Several survey and benchmarking papers, including XAI-survey [22] and BenchXAI [23, 24], have played a crucial role in shedding light on the disagreement problem within existing explainability methods [9, 11, 13, 25, 26]. Notably, these contributions have been important to the understanding of the challenges and nuances associated within the field of machine learning explainability.\nWhile the majority of existing benchmarks have primarily focused on explaining neural networks for text and image data with feature importance generation methods such as [10, 12, 27\u201330], the research community has introduced several frameworks to facilitate the transparent evaluation of explainability methods. Examples include OpenXAI [31], Captum [32], Quantus [33], and many others such as [34, 35]. In addition, [36] introduced a quantitative framework with specific metrics for assessing the performance of post-hoc interpretability methods, particularly in the context of time-series classification. This research provides a targeted approach to evaluating the temporal aspects of interpretability. These frameworks aim to provide a structured approach to assess the effectiveness and reliability of various explainability techniques.\nDespite these advancements, the evaluation of post-hoc interpretability methods for ensemble trees predictions, taking into account different data properties, remains unexplored. This paper seeks to fill this gap by addressing the specific question of how existing interpretability methods designed for ensemble trees predictions perform under varying data conditions. This research aims to contribute valuable insights and further enrich the evolving field of explainability evaluation."}, {"title": "4 Synthetic Data Generation Framework", "content": "We wish that the decision function of a model f make predictions \u0177 = f(X) to minimize some expected loss;\n$E_{X,Y~P(Y,X)} [l(Y, f(X))]$"}, {"title": "", "content": "where P(Y, X) is the data distribution and l is some loss function. A decision function f : X \u2192 Y is essentially some function of P(X, Y). For example, to maximize classification accuracy,\ny = f(X) = argmax P(y|X)\ny\nTherefore, the feature importance outcome depends significantly on different mechanisms/properties of this generating distribution P(X, Y). Namely, the properties of each variable (and its distribution), and the properties of the relations among variables. Considering only two features, we could represent the concept as a Bayesian network (Fig. 1 illustrates).\nP(X,Y) = P(X1, X2,Y) = P(Y|X1,X2)P(X2|X1)P(X1)\ni.e., a Full factorization of the joint, and thus we could consider the following properties (as nodes and edges):\n1. P(X1): specifying the type of the feature X1;\n2. P(X2 X1): the amount of conditional dependence of X2 on X1; and\n3. P(Y|X1, X2): the amount and type of correlation between features and target, revealing the special case of P(Y|X1, X2) = P(Y) when there is no correlation.\nPartial-XOR and Partial-NOT exhibit feature independence (features are independent from each other - when the target is observed), and both features are required to make a perfect prediction for Partial-XOR, and only X\u2081 is required to perfectly predict Partial-NOT; in this case deterministic.\nIn real-world settings, Full represents the case where one feature is related to another feature and both participate to make the prediction of the output, for example in ADULT INCOME dataset (that we later include in our experiments) the feature OCCUPATION is correlated to AGE and both predict the output INCOME. Partial-CAUS on the other hand, may represent a causal relationship between one feature and the outcome through a chain of causality; or an indirect correlation of one feature on the output through another feature(or even multiple features), forming a chain of correlations.\nFull and Partial-CAUS are specific cases of respectively Partial-XOR and Partial-NOT when X1 is dependent on X2, thus for the rest of this paper, we denote XOR to refer to both Full and Partial-XOR, and use NOT to refer to Partial-CAUS and Partial-NOT. The data can be generated as:\nP(X) ~ \u039d (\u03bc, \u03a3)\nWhere N is a bi-variate normal distribution and \u2211 is the covariance matrix, representing the amount of correlation between the two random variables X1 and X2. In order to introduce noise, we apply a mask to invert a percentage of labels in the output y. Let's denote the percentage of labels to invert as e. Let Mi be a binary mask defined as:\n$M_i =\\begin{cases}\n1 \\quad \\text{with probability } e\\\\\n0 \\quad \\text{with probability } 1 \u2013 \u0454\n\\end{cases}$"}, {"title": "", "content": "The perturbed output y can be obtained with:\n$\\hat{y}$ = y (1 \u2013 M) + (1 \u2212 y) \u2299 M\nWith y = f(X) and f represents the logical operators XOR or NOT.\nGround truth feature importance\nWe use \u03d5x(f*) to denote the ground truth feature importance that are given by the true model f* to which we compare \u03d5x (f), the feature importance estimates that is generated by each of the local explainability methods to explain the predictions of the learned model f. Intuitively, the true model f* can be illustrated with a D-depth decision tree. With D = 2 for XOR dataset variants (the first split on X1 and the second on X2) and D = 1 for NOT dataset variants (only one split on X1).\nWhen \u20ac = 0, the ground truth feature importance for all variants of XOR datasets are fixed as \u03d5\u2081 = \u03d5x\u2082 = .5, because both X\u2081 and X2 are necessary to make the prediction of XOR. The amount of the correlation p between X1 and X2 doesn't affect the importance as both are necessary to make the prediction of XOR. Meanwhile, only X\u2081 is necessary to make the prediction of NOT, thus \u03d5x\u2081 = 1 and \u03d5x\u2082 = p, because when X2 is correlated to X1, X2 have an indirect influence estimated by p to predict NOT.\nOn the other hand, when \u0454 \u2260 0, \u03d5x\u2081 = \u03d5x\u2082 = .5 * \u0454 for XOR dataset variants, and \u03d5x\u2081 = 1 * \u0454 and \u03d5x\u2082 = p * \u0454 for NOT dataset variants.\nFor the XOR function, both 11 and 12 are equally important in determining the output, and their importance scores should ideally converge to 0.5 when considering a large amount of data points. Consider a decision tree model that aims to predict the XOR function using features 21 and 22. For simplicity, let's assume that the decision tree splits on both 21 and 22 at each level. The decision tree's predictions can be expressed as:\ny = f(x1,x2)\nNow, let's define the feature importance scores (\u03a6\u2081\u2081 and \u03a6\u2081\u2082) using the Gini impurity criterion, a common metric for decision trees:\n$\\Phi_{x_1} = \\frac{\\sum_{nodes \\text{ splitting on } x_1} \\text{Gini decrease at the node}}{\\text{Gini decrease at the node}}$\n$\\Phi_{x_2} = \\frac{\\sum_{nodes \\text{ splitting on } x_2} \\text{Gini decrease at the node}}{\\text{Gini decrease at the node}}$\nIn a large dataset, the decision tree will be able to accurately capture the XOR relationship, and both 21 and 22 should contribute equally to the impurity decrease, leading to similar importance scores. For a balanced decision tree, these Gini decreases would be distributed among the splits involving 21 and 22. In the limit of a large dataset, we would expect:\n$\\lim_{\\text{large dataset}} \\Phi_{x_1} = \\lim_{\\text{large dataset}} \\Phi_{x_2} = 0.5$"}, {"title": "5 Empirical Setup", "content": "To carry out our experiments, we demonstrate our findings on four real-world datasets: HEART DIAGNOSIS, CERVICAL CANCER, ADULT INCOME and GERMAN CREDIT RISK. These datasets include properties such as feature interactions (dependence or independence),noise, random irrelevant variables and class imbalance.\nWe generate 24 synthetic datasets (Figures 2 and 3) expressing different combinations of these properties by varying several parameters such as the correlation amount of the normal distribution from which the data points are drawn and the probability of each class, thus, the amount of generated noise and class imbalance. Each dataset is divided to 80% for training and 20% for testing. We report the results of the feature importance estimates on the test set. We compute feature importance estimates on the true model f* and learned model f, so that we can compare the generated feature importance estimates to their ground truth values. Finally, we analyze the advantages and limitations of each explainability method on the synthetic datasets and we run larger experiments on the above real-world datasets from the UCI repository [37]."}, {"title": "5.1 Datasets, models and metrics", "content": "Datasets\nFigures 2 and 3 show the generated datasets by varying the parameters in Eq 4:\n\u03bc\u2208 {[0, 1], [1,0]}, \u03a3\u2208 {[[1,0], [0, 1]], [[1, .1], [.1, 1]], [[1, .9], [.9, 1]], [[1, 1], [1, 1]]}, and e \u2208 {0,.25, .5}.\nIn addition, Table 2 summarizes the properties of the four real-world datasets that we use to demonstrate our findings.\nModels\nFor the synthetic datasets, we compute the feature importance scores of the learned model f on datasets with 1000 instances. The learned model f can be either a decision tree or a random forest. On the other hand, we use the random forest model with parameters learned using grid search and evaluated with 10-fold cross-validation for"}, {"title": "5.2 Experiments", "content": "5.2.1 Synthetic data\nFigures 4 and 5 show the normalized feature importance estimates attributed by the selected explainability methods. After the normalization of the absolute importance of X1 and X2, their contributions sum to one. We perform the normalization to faithfully compare the feature attributions to their ground truth values.\nExplainability methods based on learning surrogate models overestimate the importance to irrelevant variables, Tree interpreter is sensitive to noise and SHAP explainers always favor one feature over the other\nOverall, all explainers except local surrogates overestimate the importance of X1 over X2 across the XOR datasets. Also, none of these methods perfectly matches ground truth feature importance on average across all datasets. Moreover, LSurro and LIME feature importance attributions are the least affected by noise and feature correlation. Indeed, LSurro and and LIME attribute comparable importance to X1 and X2 for XOR and NOT dataset variants, and both overestimate the importance of unimportant features (such as X2 in case of NOT). Notably, TI is the most affected by noise, that is confirmed in its decomposition of the the feature and noise contributions to the prediction. Additionally, feature correlations increase the importance and instability of X2 importance in XOR datasets attributed by SHAP explainers, and noise lowers the importance of X1 and X2 for all the explainers. Finally, SHAP explainers and TI have the highest variance of feature importance estimates in the NOT datasets."}, {"title": "SHAP explainers yield very comparable explanations", "content": "Figure 6 shows the faithfulness of the explanations to the ground truth measured by mean consistency and mean feature agreements across the XOR and NOT generated datasets.\nSHAP explainers yield consistent explanations due to the same feature importance attribution mechanism they all employ. However their explanations are the most inconsistent with respect to the ground truth values. Furthermore and for both XOR and"}, {"title": "LSurro is the most locally stable, overestimates unimportant features and achieves better model accuracy with less features", "content": "Table 4 shows mean consistency, mean stability and compactness across the XOR and NOT datasets. For XOR and NOT datasets respectively, Kshap is the most consistent to"}, {"title": "5.2.2 Real-world data", "content": "For the real-world datasets the ground truth feature importance is unavailable, we perform evaluation of the different metrics in Section 2."}, {"title": "Local surrogates achieves 100% of model accuracy with 5 features on Adult Income dataset.", "content": "Figure 7 shows feature agreements for ADULT INCOME Income dataset. Kshap and Sshap have exactly the same top-10 feature attributions and ranking. Tl and Tshap share the same set of top-10 features. The rest of the explainers share 90% of the top 10 most important features. LIME share the lowest of top-10 important features with TI, Kshap and Sshap on the ranking of the top 10 features. Additionally, Table 5 illustrates the compactness, mean consistency and stability of the different methods on the ADULT INCOME Income dataset. LSurro explains 100% of model output with only 5 features. Kshap, Tshap and TI are the most stable for this dataset and LIME have the highest mean consistency across all the datasets."}, {"title": "SHAP explainers generate the most consistent explanations for German Credit Risk dataset.", "content": "Figure 8 and Table 6 show the computed metrics on GERMAN CREDIT RISK dataset. Overall, all the explainers achieve 90% of model accuracy with only 1 feature and SHAP explainers are the most consistent among the methods. LIME is the most stable among the explainers. Moreover, SHAP explainers have same top-10 most important features as they all use the same mechanism of computation of the Shapley values the feature importance estimates, LIME have only 6 top features in common with SHAP explainers, and Kshap and Tshap share 7 features with the same rankings."}, {"title": "SHAP explainers and TI share the top-10 most important feature for Heart Diagnosis dataset", "content": "In Figure 9 and Table 7, SHAP explainers and TI share the top-10 most important feature, contrary to LIME which doesn't share same ranking of the top features with other explainers. Kshap and TI have exactly the same top-10 features and in the same rankings. On the other hand, LSurro estimates 100% of model accuracy with only 5 features and SHAP explainers and TI generate the most stable and consistent explanations."}, {"title": "Local surrogates are the most consistent and explains 100% of model output with 5 features for Cervical Cancer dataset.", "content": "Figure 10 and Table 8 illustrate above metrics on the CERVICAL CANCER dataset. Tshap and Kshap share the top 10 features. LIME and Sshap have the lowest top features in common. LIME and LSurro have no comparable rankings of the the features with TI, although they both learn a surrogate linear model in the neighborhood of each instance but use different mechanisms of the generation of the local neighborhood, which can explain the disagreement in their features importance estimates. Additionally, LSurro is the most consistent and explains 100% of model output with 5 features, and SHAP explainers have the highest mean stability."}, {"title": "6 Conclusions and Future Work", "content": "The experiments on the synthetic and real-world datasets showed that feature importance attribution can be affected by multiple factors such as data properties, the black-box model and the assumptions on which the explainable method is built to attribute feature contributions. We restricted our study to the first factor with a focus on some data properties, decision tree based models, on tabular data and for a binary classification task. For the matter of simplicity and ease of understanding of the model's behavior, we restricted our generation model to two variables in order to easily track the feature interactions, which can be a limit in real-world scenarios because of the need to handle high-dimensional data in many situations.\n\u2022 For datasets with irrelevant variables, avoid using LSurro and LIME because both overestimate the importance of unimportant features.\n\u2022 We recommend to avoid using TI for highly noisy datasets because Tl is the most unstable compared to other explainers in such datasets. This can be justified by the decomposition of of the feature importance used by TI, which allocates importance to the noise.\n\u2022 Kernel, Sampling and Tree SHAP explainers give very similar explanations, thus we recommend using Sshap or Tshap for faster computations and adaptability for decision trees.\nPerspectives\nFuture work should further focus on each single explainability method separately to be able to explore in depth the effect of its assumptions and its inner workings for a single model parameter and on one specific data property on the feature importance attribution. Also, it is of interest to assess these feature importance estimates on other data parameters such as the number of instances in the test set and the number of features. Our work can be applied on other tasks such as regression and multi-class classification, to image and text data."}]}