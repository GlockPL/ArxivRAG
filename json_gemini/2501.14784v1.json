{"title": "DESERVE: TOWARDS AFFORDABLE OFFLINE LLM INFERENCE VIA DECENTRALIZATION", "authors": ["Linyu Wu", "Xiaoyuan Liu", "Tianneng Shi", "Zhe Ye", "Dawn Song"], "abstract": "The rapid growth of generative AI and its integration into everyday workflows have significantly increased the demand for large language model (LLM) inference services. While proprietary models remain popular, recent advancements in open-source LLMs have positioned them as strong contenders. However, deploying these models is often constrained by the high costs and limited availability of GPU resources. In response, this paper presents the design of a decentralized offline serving system for LLM inference. Utilizing idle GPU resources, our proposed system, DeServe, decentralizes access to LLMs at a lower cost. DeServe specifically addresses key challenges in optimizing serving throughput in high-latency network environments. Experiments demonstrate that DeServe achieves a 6.7x-12.6x improvement in throughput over existing serving system baselines in such conditions.", "sections": [{"title": "1 INTRODUCTION", "content": "Interest in generative AI, especially text-generating chatbots, has increased significantly recently. Millions of individuals (Orth, 2023) and many companies (Marr, 2023) are now using AI chatbots in their daily workflows, boosting the demand for large language model (LLM) services. Closed-source models are popular, but open-source models have recently improved and now show competitive performance in chatbot evaluations (Chiang et al., 2024). This has drawn industry attention, leading to the emergence of companies providing paid services for open-source LLM inference.\nHowever, despite the wide applicability of LLM services, the platform inference price is typically still too high for daily usage. Users who want to self-host open-source models to reduce costs have two options to consider: personal devices or rented cloud machines. Personal devices, even with powerful consumer-grade GPUs, often cannot locally run large models due to memory constraints and can only efficiently run smaller or compressed models with lower performance. On the other hand, cloud GPU machines are also very expensive for long-term usage.\nInstead of self-hosting the model, a potential solution is for multiple personal device owners to collaboratively host the model together. By connecting GPUs in a decentralized network, the group may have enough GPU memory to run LLM inference tasks much more efficiently than running them separately. Such a decentralized computation paradigm has also been explored in prime number search (GIMPS) and crypto mining (Nakamoto, 2008). Furthermore, by integrating with decentralized on-chain payment, it is possible to build a framework that matches users who have inference needs with miners who possess compute resources. This concept is illustrated in Figure 1 and detailed in Section 6.2.\nHowever, the high network latency in decentralized environments prevents existing serving systems from achieving high throughput. To adapt to network conditions and reach practical performance, in this work, we have built a serving system named DeServe that tackles the throughput optimization problem in a decentralized environment. Specifically, our contributions are threefold.\n1. We identify major performance challenges caused by high-latency decentralized environments and design an efficient offline serving algorithm suitable for such environments. To the best of our knowledge, this is the first work that optimizes the throughput of the offline serving system in a high-latency network environment.\n2. We conduct both simulated and real-world experiments to validate our algorithm design. Our results demonstrate that, compared to the existing serving system baseline, the DeServe system achieves a 6.7x to 12.6x throughput improvement in high-latency environments.\n3. We discuss the correctness protection problem in decentralized collaboration and design a modular DeServe framework for practical deployment integrating with on-chain components."}, {"title": "2 BACKGROUND", "content": "This section covers the basics of LLM serving and model pipelining, with a focus on the network architecture of the widely-used Llama 3 model (Dubey et al., 2024) as an example."}, {"title": "2.1 Transformer Layers and Auto-regressive Decoding", "content": "As a text generation model, the Llama 3 model takes the input of text and continuously generates sequences of tokens in a pre-determined dictionary. The model follows an auto-regressive decoding procedure (Graves, 2013), generating one token in each round of inference until reaching a special termination token (eos). In each round, the previously generated tokens are appended to the input. During the inference, the input is processed through different network layers to generate the probabilistic distribution on the dictionary for the next token. Then, a sampling procedure decides which token should be chosen next. To optimize performance, the model employs a KV cache (Pope et al., 2023), which stores intermediate states from previous tokens, to reduce redundant calculations and speed up the inference procedure.\nThe Llama 3 model consists of three types of layers: one embedding layer, multiple transformer layers, and one output layer. The number of transformer layers varies depending on the size of the model. In practice, the processing of these transformer layers takes most of the time during the inference procedure. To run model inference efficiently, the model parameters for these layers as well as the KV cache are often loaded into GPU memory when a serving system starts. However, it's important to note that the KV cache can consume a significant amount of GPU memory, especially for long sequences. For instance, in the Llama 3 70B model, the KV cache for a sequence of length 4096 can occupy 1.25 GB of GPU memory. The large GPU memory consumption potentially limits the number of concurrent requests that can be processed simultaneously, which is commonly referred to as the batch size."}, {"title": "2.2 Online and Offline Serving System", "content": "An LLM serving system usually takes some inference requests as input and returns responses as output, aiming to optimize the performance. Online and offline serving are two different approaches for delivering inference results, each optimized for different operational needs. Online serving, also known as real-time inference, is designed for scenarios that need quick responses like chatbots or interactive systems where users expect quick feedback. It often optimizes for low latency, allowing users to interact with the LLM in real time and receive prompt responses.\nOn the other hand, offline serving, also called batch inference, is used when processing large volumes of data without the need for instant results like bulk text analysis or generating responses for a large dataset. Its optimization methods prioritize throughput and efficient use of computational resources. It's often more cost-effective for processing substantial amounts of data. An example of the cost benefits of offline serving is OpenAI's batch API, which offers a 50% discount compared to their standard API pricing for users willing to wait up to 24 hours for responses (OpenAI, a). The evaluation metrics for such an offline serving system usually target total throughput."}, {"title": "2.3 Model Parallelism and Partition", "content": "Limitation in GPU memory is the major challenge when serving a large model such as the Llama 3 70B, which needs approximately 130 GB of GPU memory. However, widely-used GPUs, like H100, only have less than 80 GB of GPU memory, not to mention that consumer-grade GPUs have even less memory. As a result, it is often infeasible to serve the entire model using a single device.\nTo address this challenge, the widely adopted approach is model parallelism, a technique commonly employed in both distributed training (Shazeer et al., 2018; Shoeybi et al., 2019; Zheng et al., 2022), and serving (Yu et al., 2022; Li et al., 2023) phases. For transformer-based LLMs, there exist two primary categories of model parallelism, differentiated by their methods of partitioning the models: intra-layer and inter-layer parallelism. We introduce them below and discuss why we choose inter-layer parallelism in the decentralized environment.\nIntra-layer Parallelism Transformer models perform high-dimensional matrix multiplications on tensors. Intra-layer parallelism, also known as tensor parallelism, splits these operations and their parameters over multiple devices (Shoeybi et al., 2019). This approach allows faster computation on large models that wouldn't fit on a single GPU, making it particularly suitable for models with massive parameter sizes.\nInter-layer Parallelism Since a transformer model is executed layer by layer, the inter-layer parallelism divides the model into smaller stages with a layer as the smallest unit and then distributes them to different devices (Zheng et al., 2022). The stages are executed in a pipeline way (Huang et al., 2019; Li et al., 2021) to avoid waiting for the completion of the previous stage and reduce the idle time of devices during multiple rounds of execution. So it can also be referred to as pipeline parallelism. This strategy substantially reduces communication overhead because only a small amount of data needs to be forwarded from the output of one stage to the input of the next stage.\nConsidering that intra-layer parallelism can lead to large communication overhead due to data exchange between GPUs (Li et al., 2023), a decentralized serving system can derive significant benefits from inter-layer parallelism, especially under slow and unreliable network conditions. Thus, when serving models that require GPU memory exceeding the maximum available GPU memory in any node in the network, we select a proper set of nodes and use model pipelining to distribute the layers. We will then dive into finer-grain optimizations in Section 4."}, {"title": "3 MOTIVATION AND COST MODEL", "content": "A LLM-serving system processes users' inference tasks, generating tokens for each task until reaching the special termination token, thereby earning the corresponding user payment. As an offline-mode service, given a set number of tasks and their associated profit, the serving system consumes hardware compute power within a specific time frame as the cost in exchange for revenue. The construction of a serving system is well motivated when hardware costs remain below revenue.\nIn this section, we first separately explain the cost of different compute resources and the LLM service pricing model. Then, combined together, we calculate the profit requirement for a LLM serving system. Based on the profit requirement, we motivate the need for decentralized serving."}, {"title": "3.1 Cost of Different Compute Resources", "content": "Running large models, such as llama-70b which consumes large amounts of GPU memory, often requires multiple GPUs across several machines. Such resource cost estimation generally follows two approaches.\nThe first approach considers direct procurement, listing corresponding hardware purchase prices, space requirements, power costs, and estimated lifespan to calculate comparative compute power. This approach is centralized and benefits from relatively stable costs. However, this long-term estimation approach is less feasible given the dynamic compute demands of continuously evolving models and the increasing power of ever-releasing GPUs.\nThe second approach is to consider the dynamic pricing of pay-per-use resources, such as renting GPU instances on cloud platforms. This approach outsources the main cost components to the platform, allowing for relatively flexible price comparisons. However, due to different platform profit strategies, costs can vary significantly depending on the compute resource provider. This cost model is more practical for profit reasoning. Thus, in this work, we mainly focus on the second approach.\nSpecifically, we consider three distinct sources of compute resources: cloud platforms, decentralized compute platforms, and the decentralized mining paradigm."}, {"title": "3.2 Pricing for LLM Services", "content": "Users typically access LLM services via a web UI or API, with the former involving subscription-based payment and cost estimates based on usage frequency. Therefore, we primarily focus on API services here. There are two major pricing models for LLM API services: on-demand pricing and provisioned pricing. The former charges based on token usage (e.g., OpenAI (OpenAI, b), Anthropic (Anthropic)), while the latter provides a stable maximum throughput for a committed period (e.g., AWS Bedrock's provisioned model (AWS, b)). We primarily consider on-demand pricing, given that most platforms adopt this model.\nThe on-demand pricing model varies based on the service scenario and the modeling of computational tasks. Service scenarios include online interfaces for immediate returns and asynchronous offline-serving interfaces. For example, OpenAI's batch API allows up to 24 hours to return results at half the price of its online version (OpenAI, a). In terms of task modeling, due to differences in resource utilization between the prefill stage (input processing) and the decode stage (output generation) (Agrawal et al., 2024), some platforms differentiate pricing between input tokens and output tokens, while others use a unified pricing strategy for simplicity. Next, we adopt this simplified model to discuss the profit requirement for a serving system."}, {"title": "3.3 Profit Model", "content": "With an understanding of the cost model and existing pricing structures, we now construct the profit model for an offline LLM serving system. The key is to parameterize the conversion rate from compute resources to token processing, reflecting the serving system's performance.\nConsider a single compute resource unit forming a pipeline that serves the target LLM, with a per-time cost C from a specified source. For a given workload of requests containing $N_I$ input tokens and expecting $N_O$ output tokens, if the system completes the workload with $N = N_O + N_I$ tokens in total within time T, we evaluate its performance using two metrics: input throughput, $M_I = \\frac{N_I}{T}$, and output throughput, $M_O = \\frac{N_O}{T}$. To align with a simplified pricing model using unified input and output pricing, we consider the total throughput $M = M_I + M_O$.\nAssuming the workload follows an on-demand pricing model with input price per token $P_I$ and output price per token $P_O$, the total revenue generated is $R = N_I \\cdot P_I + N_O \\cdot P_O$. Alternatively, under a simplified pricing model where $P = P_I = P_O$, we have $R = N \\cdot P$.\nThe system is profitable when $R > C \\cdot T$, which sets a performance challenge. For example, in the simplified pricing model, the system must satisfy the following requirement: $R > C \\cdot T \\Leftrightarrow M > \\frac{C}{P}$. Table 2 displays the calculated profit thresholds across various settings. As shown in Table 2, mining platforms offer dramatically lower compute costs at just $0.35 per hour compared to $13.88 for cloud platforms, requiring only 108 tokens/second throughput to break even - making them an attractive option for cost-effective LLM"}, {"title": "4 OPTIMIZE DECENTRALIZED SERVING", "content": "Compared to centralized serving, decentralized environments often introduce higher network latency and limited bandwidth. For instance, for GPUs located in US east and US west to collaborate, it takes around 60 milliseconds for a packet to arrive from one to the other. The high network latency between GPUs greatly impacts inference efficiency. For example, using pipelining in vLLM between machines across US, there is a 58% reduction in overall throughput compared with centralized setting.\nAdditionally, decentralized settings often involves more consumer-grade GPUs with less memory. Thus, serving a model with the same memory requirement takes more GPUs. For instance, an NVIDIA H100 has 80GB of memory, while the RTX 4090 offers only 24GB. As a result, two H100 GPUs can serve a 70B model, whereas it requires eight RTX 4090 GPUs to handle the same workload. These factors complicate efforts to maintain high throughput in decentralized serving. Specifically, we identify two major challenges for the serving algorithm design.\nLimited memory for each batch. In pipeline parallelism, multiple microbatches are present on each machine, but only one is actively executed at any given time. Since these microbatches share the same GPU memory, each can only own a portion of it. Consequently, the batch size for each microbatch is limited, which in turn restricts the overall GPU utilization rate. In Table 3, we illustrate the relationship between batch size and execution time for decoding with same prefix.\nNetwork latency bubbles. High-latency networks have a significant impact on the efficiency of LLM inference."}, {"title": "4.1 Overview", "content": "We highlight two major designs in DeServe to overcome the above challenges: KV cache offloading and microbatch scheduling.\nKV cache offloading increases available GPU memory by transferring unused KV cache to CPU memory, enabling larger batch sizes per microbatch and more efficient memory allocation. Additionally, the system adjusts the number of microbatches in the pipeline to fill bubbles caused by network latency, ensuring optimal GPU resource utilization."}, {"title": "4.2 KV Cache Offloading", "content": "We illustrate the GPU memory layout during serving in Figure 3. The memory layout accommodates model weights and KV cache similar to some existing works (Strati et al., 2024). Specifically, when using $N_M$ machines with M memory and $N_B$ microbatches to serve a Llama 3 70B model, the distributed model weights occupy $M_W$, leaving $M_{KV} = M - M_W$ for KV cache allocation. Microbatches need to allocate their own KV cache, thus sharing only a portion of GPU memory for KV cache, which is formulated as $M_B = \\frac{M_{KV}}{N_B}$.\nTo increase GPU memory for each microbatch, we use KV cache offloading. Specifically, we define two global page pools, $G_0$ and $G_1$, each with $M_G$ GPU memory, to swap KV cache between CPU and GPU. Additionally, $N_B$ local page pools are allocated for each microbatch, which remain on the GPU without offloading. By using this strategy, each microbatch can utilize one global page pool and its own local page pool. Thus, available GPU memory for each microbatch will become:\n$M'_B = \\frac{M_{KV} - 2M_G}{N_B} + M_G$ (1)\nThe size of $M_G$ is determined by the PCIe bandwidth W"}, {"title": "4.3 Microbatch Scheduling", "content": "We propose microbatch scheduling to address pipeline bubbles caused by network latency. This solution is illustrated in Figure 2(c). In the example showed in the figure, there are four machines inside the pipeline, and the network latency is $T_s$. The bubbles brought by the network latency can be resolved through adding two more extra microbatches to fill the blank.\nBy combining microbatch scheduling with KV cache offloading, the system is able to yield larger optimization space. Without KV cache offloading, doubling the number of microbatches would halve the average batch size per microbatch, rendering this optimization ineffective. However, as shown in Formula 1, increasing the number of microbatches still guarantees a lower bound of $M_G$ GPU memory per microbatch, determined by PCIe bandwidth. This ensures a minimum throughput level, regardless of the number of microbatches we use to cater network condition.\nIn conclusion, by scheduling microbatches according to the network latency, we can maintain high throughput even in extreme network condition. We provide the experiment result in the next section."}, {"title": "5 EXPERIMENTS", "content": "We use a static workload with an average prompt length of 256, randomly selecting lengths from [0, 512], with generated lengths chosen similarly. We send $N_R$ requests simultaneously for prefilling and decoding, replenishing them as the previous requests are completed. The system is benchmarked for 20 minutes with statistics collected from the last 16 minutes. Since the average input and output lengths are the same, we expect the output throughput to match the input throughput. DeServe is implemented in PyTorch (Paszke et al., 2019) with CUDA kernels from FlashInfer (Ye et al.). The codebase for the experiments has been published in https://github.com/CoLearn-Dev/deserve.\nWe run real-world experiments by machines across us-east-1 and us-west-4 in Google Cloud with 58.4ms latency, and use the machines in the same region with code patches to simulate latency for other experiments.\nTable 4 shows the main experiment result. The vLLM threw out an error during the benchmark when we tried tensor parallelism in machines across two regions. It might be because of the high latency or insufficient bandwidth be-"}, {"title": "6 DISCUSSION", "content": "Unlike cloud or decentralized computation platforms, in a decentralized mining paradigm, individual compute resource contributors may deviate from the protocol, providing incorrect values during execution. For instance, malicious participants may falsely claim GPU usage but instead run the protocol on a low-cost CPU instance, producing out-"}, {"title": "6.1 Protect Inference Correctness", "content": "Existing protection mechanisms primarily focus on three main approaches: optimistic verification, voting-based consensus, and zero-knowledge proof. opML (Conway et al., 2024) employs an optimistic approach to protect LLM inference correctness on the blockchain. The approach is similar to Agatha (Zheng et al., 2021) which protects the inference correctness of DNN. It adapts the bisection protocol, similar to Arbitrum's (Kalodner et al., 2018) method for smart contracts, to pinpoint errors in the computation process, either at the granularity of tensor operations or element-wise operations. On the other hand, spML (Zhang et al., 2024b) utilizes a voting-based consensus mechanism. It relies on a decentralized network of validators to determine the accuracy of inference results through a voting process. Both methods need deterministic computation, such as software-based floating-point operations, to ensure consistent results across different hardware platforms. zkLLM (Sun et al., 2024) introduces a parallelized lookup argument to bring zero-knowledge proof to LLM inference. However, the computation cost still very high.\nAdditionally, we further identify the following design principles that are vital for the practicability of a decentralized serving system.\n1. The correctness protection should not introduce heavy extra computation costs for the inference service provider. Both opML and spML rely on the deterministic computation which slow downs the inference, and zkLLM rely on computationally expensive zero-knowledge proofs. All these previous solutions fail to meet this principle.\n2. Both inference input and output should remain off-chain by default to avoid extra costs. Storing large data on-chain often incurs substantial blockchain computation fees (i.e., gas fees), potentially exceeding the cost of the inference itself. This makes solutions like opML, which require storing all results on-chain, cost-prohibitive.\n3. The inference service provider should not be required to respond to verification challenges from arbitrary parties. opML requires the inference service provider to respond to challenges from any verifier, potentially exposing the provider to denial-of-service (DoS) attacks."}, {"title": "6.2 DeServe Framework", "content": "Our DeServe framework (cf. Figure 1) is designed with theabove principles in mind.\nBased on the above principles, we provide the DeServe framework design in Figure 1. In addition to the serving system discussed in Section 4, the framework introduces four more on-chain components to handle revenue distribution and correctness protection. The task registry and GPU registry enable service discovery and record offline inference tasks from the user and available compute resources from miners. The payment module accepts and locks user payment in cryptocurrencies on-chain and transfers them to the corresponding miner after the inference results are returned to the user. The arbitration module is not triggered during an honest collaboration. However, if the user receives incorrect inference results, given the signature from the miner that produces the result, the user can invoke arbitration procedure that eventually claims the staking of"}, {"title": "6.3 Open Problems", "content": "While the DeServe framework design addresses challenges in offline serving algorithms and integrates mechanisms for correctness protection, several open challenges remain. For instance, the responsibility for deploying and maintaining on-chain component smart contracts, as well as matching compute resources with tasks, remains centralized. Additionally, fair task distribution strategies for miners have yet to be fully explored. Furthermore, criteria for verifying result correctness - an area challenging for most correctness protection mechanisms - are not yet well-defined. Future research into these issues would greatly enhance the practicality of decentralized serving solutions."}, {"title": "7 RELATED WORK", "content": "Recent work has focused extensively on enhancing GPU utilization efficiency. Some approaches prioritize computation reorganization (Agrawal et al., 2024; Cheng et al., 2024; Hu et al., 2024; Strati et al., 2024; He et al., 2024) at GPU usage level, as outlined in a comprehensive survey (Li et al., 2024), while others optimize at the cluster level by refining device selection and scheduling for inference requests, targeting both edge (Yang et al., 2024; Zhang et al., 2024a) and decentralized environments (Mei et al., 2024). In this work, we specifically target GPU usage level optimizations tailored for decentralized environments.\nThere is also work on the decentralized use of consumer-grade GPUs for LLMs, where some focus on training (Tang et al., 2023; 2024), and others on network construction (Olshansky et al., 2024). However, these efforts do not address the specific technical challenges associated with orchestrating LLMs over high-latency networks. Additional work has explored communication compression and model quantization to optimize performance (Borzunov et al., 2023), which affects the model behavior, and thus is orthogonal to our approach."}]}