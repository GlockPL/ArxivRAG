{"title": "Fact or Fiction? Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retrievals", "authors": ["Tobias A. Opsahl"], "abstract": "Despite recent success in natural language processing (NLP), fact verification still remains a difficult task. Due to misinformation spreading increasingly fast, attention has been directed towards automatically verifying the correctness of claims. In the domain of NLP, this is usually done by training supervised machine learning models to verify claims by utilizing evidence from trustworthy corpora. We present efficient methods for verifying claims on a dataset where the evidence is in the form of structured knowledge graphs. We use the FACTKG dataset, which is constructed from the DBpedia knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval process, from fine-tuned language models to simple logical retrievals, we are able to construct models that both require less computational resources and achieve better test-set accuracy.", "sections": [{"title": "1 Introduction", "content": "As the volume of information generated continues to grow, so does the risk of misinformation spreading, which has made automatic fact verification a crucial task in NLP (Cohen et al., 2011; Hassan et al., 2015; Thorne and Vlachos, 2018; Bekoulis et al., 2021). Traditionally, fact verification has been tackled in journalism by experts manually researching topics and writing articles about their findings. Some specific websites dedicated to this approach are FactCheck.org and PolitiFact.com. However, it is time consuming and labor intensive, and is not able to follow the pace of the creation of information in digital media (Cohen et al., 2011; Hassan et al., 2015).\nOne of the most popular datasets for fact verification is the Fact Extraction and VERification (FEVER) dataset (Thorne et al., 2018). It consists of claims supported by a corpus of Wikipedia articles. Models trained on the dataset need to extract the relevant evidence and use it to classify claims as supported, refuted or not enough information."}, {"title": "2 Related Work", "content": "Despite its popularity, several issues have been discovered. Due to the manual construction of claims, the structure of the language is inherently biased with respect to the classes, and therefore it is possible to achieve good performance without using the evidence at all (Schuster et al., 2019). It has also been shown that models trained on FEVER experience a significant drop in performance when the factual evidence is changed in a way that influences the validity of claims (Hidey et al., 2020). These issues can be improved by accordingly adjusting the validation and test dataset to contain less biased data (Schuster et al., 2019; Hidey et al., 2020), but we believe it is important to develop models on other datasets as well.\nA less studied approach to process evidence is by structured data. In many real-world examples, data is available in large structured databases, rather than unstructured articles. This is relevant for domains such as social networks, logistics, management systems and database systems. The dataset TabFact (Chen et al., 2019) is created with this intent, consisting of claims with tabular evidence extracted from Wikipedia.\nWe will dedicate this article to increase the performance of models trained on the FACTKG dataset (Kim et al., 2023), a dataset created for fact verification with structured evidence in the form of knowledge graphs (KGs). The claims are created with evidence from DBpedia (Lehmann et al., 2015), a large KG extracted from Wikipedia. A KG consists of nodes and edges linked together to represent structural concepts. Nodes represent entities, such as persons, things or events, and edges represent relations, conveying how entities are related, as shown in Figure 1. For instance, a node can be the company Meyer Werft, and since it is located in the city Papenburg, they are connected with the edge location. We refer to Meyer Werft, location, Papenburg as a knowledge triple.\nSince the task of fact verification with KGs re-"}, {"title": "2.1 Fact Verification", "content": "The FEVER dataset is one of the most popular datasets used for fact verification (Thorne et al., 2018), and has influenced several model architectures. Graph-based Evidence Aggregating and Reasoning (GEAR) (Zhou et al., 2019) works by finding relevant articles with entity linking, giving them a relevance score, embedding the claim and sentences in the relevant evidence with a pre-trained BERT (Devlin et al., 2018), and then using a GNN to reason over the embeddings. The Neural Semantic Matching Network (NSMN) (Nie et al., 2019) used three homogenous neural networks used for document retrieval, sentence selection and claim verification. By using a transformer based architecture, Generative Evidence REtrieval (GERE) (Chen et al., 2022) combined the evidence retrieval and sentence identifying into a single step.\nSeveral other datasets for fact verification have also been proposed. The Fake News Challenge (Hanselowski et al., 2018) were aimed towards predicting the relevance and agreement of a title and text. VitaminC (Schuster et al., 2021) focuses on representing changing evidence, and was created by constructing claims based on different revisions of Wikipedia articles. The dataset FAVIQ (Park et al., 2021) explored ambiguous parts of claims, while TabFact (Chen et al., 2019) used tabular data as evidence. There have also been proposed multimodular dataset for fact verification, combining claims and images (Zlatkova et al., 2019; Mishra et al., 2022)."}, {"title": "2.2 The FactKG Dataset", "content": "The FACTKG dataset (Kim et al., 2023) consists of 108 000 English claims for fact verification, where the downstream task is to predict whether the claims are true or false. The claims are constructed from the DBpedia KG (Lehmann et al.,"}, {"title": "2015), which is extracted from Wikipedia and represents how entities are related to each other.", "content": "The claims are constructed on either of the following five reasoning types:\n\u2022 One-hop: To answer a one-hop claim, one only needs to traverse one edge in the KG. In other words, only one knowledge triple is needed to verify the validity of the claim.\n\u2022 Multi-hop: As opposed to one-hop claims, one needs to traverse multiple steps in the KG to verify multi-hop claims.\n\u2022 Conjunction: The claim includes a combination of multiple claims, which are often added together with the word and.\n\u2022 Existence: These claims state that an entity has a relation, but does not specify which entity it relates to.\n\u2022 Negation: The claim contains negations, such as not.\nThe dataset is split in a train-validation-test set of proportion 8:1:1. The train and validation set includes relevant subgraphs for each claim, but not the test set. All claims include a list of entities present in the claim and as nodes in the KG."}, {"title": "2.3 Question Answer Graph Neural Networks", "content": "The question answer graph neural network (QA-GNN) (Yasunaga et al., 2021) is a hybrid language and GNN model that both uses a pre-trained language model to process the text, and couples it with a GNN reasoning over a subgraph. It is given text and a subgraph as input. The text, consisting of a question and possible answers, is added as a node to the subgraph. The language model embeds the text, and assigns a relevance score to each node in the subgraph. The relevance scores are multiplied with the node features, before being sent into the GNN. The GNN output, text-node and the text embedding are concatenated before being put into the classification layer."}, {"title": "3 Methods", "content": "remains relatively unexamined, we want to explore several different approaches to the problem. We use the following three model architectures:\n\u2022 Textual Fine-tuning: Fine-tuning pretrained encoder models on text data for claim verification. We use BERT (Devlin et al., 2018) by concatenating the claims with subgraphs represented as strings.\n\u2022 Hybrid Graph-Language Model: Using a modification of a question answer graph neural network (QA-GNN) (Yasunaga et al., 2021), which both uses a pretrained encoder model to embed the claim, and a graph neural network (GNN) to structurally process the subgraphs.\n\u2022 LLM Prompting: Deploying state-of-the-art language models in a few-shot setting, without the need for additional finetuning. We use ChatGPT 4o (Achiam et al., 2023; Open AI, 2024) for this setting.\nThe textual finetuning serves as a simple and conventional method, while the QA-GNN can handle graph based data efficiently and is more specifically constructed for the task of interest. In contrast, the LLM prompting displays how well general purpose language models can perform on the task. It does not require any further training and does not use any evidence. Therefore, it will serve as a baseline and give insight to how difficult the task is.\nOur main contribution is that we increase the accuracy and computational efficiency of models trained on FACTKG. By utilizing efficient subgraph"}, {"title": "3.1 Efficient Subgraph Retrieval", "content": "We experiment with different ways of retrieving relevant subgraphs for the claim, focusing on computational efficiency. Each datapoint in the FACTKG dataset consists of a claim and a list of entities"}, {"title": "3.2 Finetuning BERT", "content": "We use BERT (Devlin et al., 2018) as our pretrained language model. We first train a baseline model using only the claims and no subgraphs, and then with all of the different methods for retrieving subgraphs. The subgraphs are converted to strings, where each knowledge triple is represented with square brackets, and the name of the nodes and edges are the same as they appear in DBpedia. The order of the knowledge triples is determined by the order of the list of entities in the FactKG dataset and the order of the edges in DBpedia. The subgraphs are concatenated after the claims and a \"|\" separation token."}, {"title": "3.3 QA-GNN Architecture", "content": "In order to adapt the QA-GNN to the fact verification setting, we perform some slight modifications. Because the possible answers are always \u201ctrue\u201d or \"false\", we embed only the claims, instead of the question and answer combination. Additionally, we do not connect the embedded question or claim to the subgraph.\nWe use a pre-trained BERT (Devlin et al., 2018) as the language model to embed and calculate the relevance scores. In order to reduce the complexity of the model, we use a frozen BERT to calculate the embeddings for the nodes and the edges in the graph. This way, all of the embeddings in the graph can be pre-calculated. We use the last hidden layer representation of the CLS token, which is of length 768. The BERT that calculates the relevance scores and the embedding of the claim is not frozen. The relevance scores are computed as the cosine similarity between the claim embedding and the embedding of the text in the nodes.\nWe use a graph attention network (Veli\u010dkovi\u0107 et al., 2017) for our GNN. Since the subgraphs are quite shallow, we only use two layers in the GNN, and apply batch norm (Ioffe and Szegedy, 2015). Each layer has 256 features, which is mean-pooled and concatenated with the BERT embedding and sent into the classification layer. We add dropout (Srivastava et al., 2014) to the GNN layers and the classification layer."}, {"title": "3.4 ChatGPT Prompting", "content": "We construct a prompt for ChatGPT 4o in order to answer a list of claims as accurately as possible. This is done by creating an initial prompt and validating the results on 100 randomly drawn claims from the validation set, and by trying different configurations of the prompt until we do not get a better validation set accuracy. We then use the best prompt with 100 randomly drawn unseen test-set questions, and attempt to ask 25, 50 and 100 claims at a time, to see if the amount of claims at a time influences the performance. We run the testing three times.\nSince we do not have access to vast enough computational resources to run an LLM, this analysis is limited by only using 100 datapoints from the test set. In order to get access to a state-of-the-art LLM, we used the ChatGPT website with a \"ChatGPT Plus\" subscription to perform the prompting. This model is not seeded, so the exact answers are not reproducible, but every prompt and answer are available in the software provided with this article. We used the ChatGPT 4o model 30th of May 2024. Every prompt was performed in the \"temporary chat\" setting, so the model did not have access to the history of previous experiments.\nDue to the inability to use the entire test set and the lack of reproducibility, we do not directly compare this experiment to the other models. However, we still believe it serves as a valuable benchmark. Recently, the performance of LLMs has rapidly improved, which suggests that their applications will continue to broaden. Additionally, this approach is not fine-tuned, and may work as an interesting benchmark that can contextualize the results of the other models."}, {"title": "3.5 Benchmark Models", "content": "We will compare the results against the best benchmark models from Kim et al. (2023) and the best performing models known to the authors, found in Gautam (2024). These comparisons include both baselines that use only the claims and models that also incorporate subgraph evidence.\nClaim-Only Models:\n\u2022 FactKG BERT Baseline: The baseline model from Kim et al. (2023) uses a fine-tuned BERT, training only on the claims.\n\u2022 ROBERTa Baseline: Similarly to above, the baseline from Gautam (2024) uses a fine-"}, {"title": "3.6 Further Experimental Details", "content": "Due to computational constraints, we tuned the hyperparameters one by one, instead of performing a grid search. All the training was performed on the University of Oslo's USIT ML nodes (University Centre for Information Technology, 2023), using an RTX 2080 Ti GPU with 11GB VRAM. The BERT model has 109 483 778 parameters, which all were fine-tuned. The QA-GNN used a total of 109 746 945 parameters. The FACTKG dataset comes with a lighter version of DBpedia that only contains relevant entries, which was used for this paper. Further details can be found in Appendix A."}, {"title": "4 Results", "content": "tuned language model with claims only, but uses ROBERTa (Liu et al., 2019) as the base model.\nModels Utilizing Subgraphs:\n\u2022 GEAR-Based Model: The benchmark model from Kim et al. (2023) is inspired by GEAR (Zhou et al., 2019), but has been adapted to handle graph-based evidence. It uses two fine-tuned language models to retrieve the subgraphs. One of them predicts relevant edges, the other predicts the depth of the subgraph.\n\u2022 FactGenius: This model combines zero-shot LLM prompting with fuzzy text matching on the KG (Gautam, 2024). The LLM filters relevant parts of the subgraphs, which are then refined using fuzzy text matching. Finally, a fine-tuned ROBERTa is used to make the downstream prediction."}, {"title": "4.1 Improved Performance and Efficiency", "content": "The test results for our best model configurations and the benchmark models can be found in Table 1. The best performing model is the fine-tuned BERT with single-step subgraphs. The fine-tuned BERT without any subgraphs were able to achieve slightly higher performance than the one from Kim et al. (2023), which we suggest is due to finding better hyperparameters.\nAdditionally, our models were much faster to train. While the GEAR model used 2-3 days to train on an RTX 3090 GPU (reported by the authors by email), our QA-GNN only used 1.5 hours. The training time of our fine-tuned BERT model was greatly influenced by the size of the subgraphs we used. With no subgraphs, it took about 2 hours to train, while with the one-hop subgraph it took 10 hours. FactGenius was reported to use substantially more computational resources, running the LLM inference on a A100 GPU with 80GB VRAM for 8 hours."}, {"title": "4.2 Successful Subgraphs Retrievals", "content": "We now look at the different configurations for the subgraph retrievals, which greatly influenced the performance of the models. Since the direct and contextual approach only includes subgraphs if a certain requirement is fulfilled, it will result in some of the claims having empty subgraphs. In the training and validation set, 49.0% of the graphs were non-empty for the direct approach, and 62.5% were non-empty for the contextual approach. The single-step method resulted in vastly bigger subgraphs.\nWhile the QA-GNN could handle the big subgraphs efficiently, the fine-tuned BERT was severely slowed down when the size of the subgraphs got bigger. Therefore, we substituted any empty subgraphs with the single-step subgraph when using QA-GNN, but kept the empty graphs when using fine-tuned BERT. This means that some claims for the direct and contextual BERT models were predicted only using the bias in the language model and the claim."}, {"title": "4.3 Competitive ChatGPT Performance", "content": "The results for the ChatGPT prompting can be found in Table 4. The accuracy is substantially lower than from our best models, but better than the baselines using only the claims. The accuracy is fairly consistent over the three runs, and we do not see a big difference between the amount of questions asked at a time."}, {"title": "5 Discussion", "content": "We were able to train better and more efficient models by simplifying the subgraph retrieval methods, both by using a fine-tuned BERT and a slightly modified QA-GNN model. While the QA-GNN models trained the fastest, the fine-tuned BERT clearly performed the best, significantly outperforming the benchmark models. This suggests that the simple logical subgraph retrievals worked better than the complex trained approaches in previous work. We suggest that the performance gain in the claim-only benchmark was due to slightly better hyperparameters.\nAll of the models performed better the bigger the subgraphs were. This suggests that the model architectures are able to utilize the relevant parts of the subgraphs, without needing an advanced subgraph retrieval step. This is emphasized by our fine-tuned BERT model achieving a 93.49% test set accuracy by only using the single-step subgraphs, while the GEAR model from Kim et al. (2023), which trained two language models to perform graph retrieval, achieved a 77.65% test-set accuracy.\nWhen examining the precision and recall metrics in Table 3, we see that most of the models has a higher precision than recall, except for the best performing model, the single-step BERT. However, the single-step BERT does have a lower recall for the multi-hop claims, which it performs significantly worse on. Therefore, the models mostly has a higher precision than recall when their performance is not so good, suggesting they are slightly more likely to predict \"false\" on claims that they are not confident about.\nA limitation of our subgraph retrieval methods is that they never include nodes that are more than one step away from an entity node, while the trained approach from Kim et al. (2023) is dynamic and may include more. This might make the hypothesis that the simple subgraph retrieval methods will"}, {"title": "6 Conclusion and Future Work", "content": "Our results show that with simple, yet efficient methods for subgraph retrieval, our models were able to improve fact verification with knowledge graphs with respect to both performance and efficiency. The fine-tuned BERT model with single-step subgraphs clearly achieves the best performance, while the QA-GNN models are more efficient to train.\nThis indicates that complex models can work well with simple subgraph retrieval methods. Since the single-step subgraphs mostly contain information not relevant for the claims, the models are themselves able to filter away irrelevant information, and complex subgraph retrieval methods may hence not be necessary for accurate fact verification. Additionally, since the best performing model performed the poorest with multi-hop claims, future research could explore simple subgraphs retrieval methods allowing for bigger depths than one. Additionally, future work should also be directed towards running similar experiments on other datasets.\nWe also encourage researchers that have access to bigger computational resources to further explore the performance of LLMs for fact verification. A core limitation of our ChatGPT prompting was the inability to use the full test-set, and we consider this crucial for further development. We also think it would be especially interesting to make LLM and KG hybrid models. Since our results indicate that simple single-step subgraph retrievals outperform more complex methods, a promising path of future research would be to simply use both the claims and the single-step subgraphs as input to the LLM. If possible, the LLM could also be fine-tuned on the dataset. We also encourage future work to create fully reproducible results with LLMs, which we were unable to do."}, {"title": "7 Limitations", "content": "Our experiments with ChatGPT were done on a small sample of test questions, with a model that was not possible to seed, and therefore is not reproducible. Due to the small sample size, we are not able to directly compare the performance to other approaches. The lack of reproducibility, which stems from the state-of-the-art model that was available to the author is not fully publicly available, makes it impossible for other researchers to completely verify our findings. Additionally, the process for creating prompts were not standardized, we simply tried different configurations based on our own experience with using LLMs until we could not increase the validation accuracy further. Due to these limitations, one should therefore be very hesitant to make any confident conclusions based on the experiments we performed with ChatGPT.\nBecause our intention was to specifically explore different language models' abilities of fact verification with knowledge graphs on the FACTKG dataset, we did not conduct any experiments on other datasets. It is possible that our results will not be consistent with other datasets.\nAdditionally, our selection of models and hyperparameter settings could be more diverse. Due to computational constraints, we did not perform a grid search for hyperparameters, but tuned hyperparameters one by one. Which parameters we searched for were not decided in advance. A predefined grid search might lead to a fairer and more reproducible approach. We did not experiment with different orderings of the knowledge triples for the fine-tuned BERT models, which could influence the performance."}, {"title": "A Hyperparameter Details", "content": "We used an AdamW optimizer (Loshchilov and Hutter, 2017) and a linear learning rate scheduler with 50 warm up steps. We used the model from the epoch with lowest accuracy loss. The hyperparameters were tuned in a line search, first testing different learning rates, and testing all the other hyperparameters with the best learning rate. We searched for learning rates in {1e \u2013 3, 5e \u2013 4, 1e -"}, {"title": "B Scientific Artifacts", "content": "We conducted the experiments using several python libraries, including PyTorch version 2.0.1 (Paszke et al., 2019) with CUDA version 11.7, Hugging-Face Transformers (Wolf et al., 2020), NumPy (Harris et al., 2020), SpaCy (Honnibal and Montani, 2017) and NLTK (Bird et al., 2009). We will make all the code used for this paper publicly available."}]}