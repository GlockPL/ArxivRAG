{"title": "ProgramAlly: Creating Custom Visual Access Programs via Multi-Modal End-User Programming", "authors": ["Jaylin Herskovitz", "Andi Xu", "Rahaf Alharbi", "Anhong Guo"], "abstract": "Existing visual assistive technologies are built for simple and common use cases, and have few avenues for blind people to customize their functionalities. Drawing from prior work on DIY assistive technology, this paper investigates end-user programming as a means for users to create and customize visual access programs to meet their unique needs. We introduce ProgramAlly, a system for creating custom filters for visual information, e.g., 'find NUMBER on BUS', leveraging three end-user programming approaches: block programming, natural language, and programming by example. To implement ProgramAlly, we designed a representation of visual filtering tasks based on scenarios encountered by blind people, and integrated a set of on-device and cloud models for generating and running these programs. In user studies with 12 blind adults, we found that participants preferred different programming modalities depending on the task, and envisioned using visual access programs to address unique accessibility challenges that are otherwise difficult with existing applications. Through ProgramAlly, we present an exploration of how blind end-users can create visual access programs to customize and control their experiences.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial intelligence (AI)-based assistive technologies can help blind people gain visual access in a variety of common scenarios, such as reading printed text and identifying objects. These applications tend to be designed for simple and common use cases to maximize their broad usability, and prior work has demonstrated that there is still a long-tail of diverse scenarios that automated assistive technologies cannot account for [26]. This leads to users having to shoulder additional cognitive load and adjust how they use the technology to get usable results. Depending on the application, users may need to sift through irrelevant or repetitive information, ask follow up questions, or re-take photos to find specific pieces of information they are looking for. While these methods can be acceptable in some situations, they can be especially difficult in situations where people want a specific piece of information quickly. For example, situations that are repetitive (like sorting mail), time-sensitive (like catching a bus), or require scanning an entire object or room with the camera (like finding an expiration date) can all become burdensome with general purpose assistive technology.\nDo-It-Yourself (DIY) assistive technology research has sought to address the related issue of a lack of customizability in assistive devices [29]. To this end, a variety of approaches have been developed aiming to make it easier for non-experts to create adaptive 3D models for themselves or a family member [12, 28]. The same concept has yet to be fully applied to the space of assistive software. From prior work, we know that blind people already put a significant degree of effort into customizing, hacking, or simply envisioning new assistive technologies [26]. Yet, there is a gap between the technologies and customizations that people desire to create, and the systems that can support them in doing so with various degrees of technical expertise.\nEnd-user programming is a potential method for supporting users in customizing and DIY-ing AI assistive software. Ko et al. define end-user programming as a form of programming done by non-professionals, 'to support some goal in their own domains of expertise', further, 'to achieve the result of a program primarily for personal, rather [than] public use' [34]. This definition is aligned with assistive technology needs: blind people are domain experts in designing and using assistive technology [4], and there is a long-tail of unique scenarios that require personalization to meet individual needs. End-user programming approaches, while powerful for enabling users to do more complex tasks, have not yet been applied to the domain of visual accessibility. Doing so presents new research challenges, namely in making tools that are approachable, accessible, and expressive. In this work, we demonstrate the potential of end-user programming approaches for assistive technology creation and customization.\nWe introduce ProgramAlly, an end-user programming tool for creating and customizing reusable visual information filters (see Figure 1). ProgramAlly is a mobile application that provides a multi-modal interface for creating and iteratively editing short block-based programs (e.g., 'find NUMBER on BUS'). It is built on a generalizable program representation of similar filtering tasks, derived from a dataset of real-world scenarios from blind people's everyday experiences. ProgramAlly provides a set of methods for implementing programs, using multiple interaction modes: direct input, speech, and camera input. These modes implement common end-user programming approaches: block-based programming, natural language programming, and programming by example. ProgramAlly integrates a set of on-device and cloud models for generating and running programs, and can easily be extended to support new, specialized models.\nIn a study of ProgramAlly with 12 blind participants, we assessed its three program creation modes, comparing ProgramAlly to existing AI-powered assistive applications, and gathering participants' thoughts on programming and DIY-ing assistive technology more broadly. Four of these participants were consulted as ProgramAlly was being developed, providing design feedback and suggestions for new features, as well as evaluating the programming interfaces and concept. The remaining participants performed a final evaluation of ProgramAlly in both in-person and remote settings.\nWe found that participants were receptive to the idea of customizing and programming their assistive technology, even if they had no programming experience. Participants envisioned using different programming interfaces depending on the program they wanted to write, the setting, and their experiences with technology. We observed that each interface requires different cognitive and technical skills, and outline specific challenges faced by blind end-user programmers when creating visual programs.\nOverall, ProgramAlly is an investigation of how end-user programming techniques can be used to create and customize AI-based assistive technology. ProgramAlly aims to inform how AI models may be directly used as building blocks by blind people in order to support new, complex tasks. This work aims to promote the democratization of AI technology creation and support blind people in having greater control over the AI-based technologies in their lives. This paper makes the following contributions:\n(1) A generalized representation of visual information filtering tasks, informed by real-world scenarios from blind people's everyday experiences, that can be easily extended to support new object classes.\n(2) ProgramAlly, a system instantiating this representation and providing a set of multi-modal interaction methods for creating visual information filtering programs: block-based programming, natural language programming, and programming by example.\n(3) A study of ProgramAlly with blind users, assessing the application of end-user programming approaches to the DIY assistive technology space and highlighting new challenges faced by blind end-user programmers."}, {"title": "2 RELATED WORK", "content": "ProgramAlly builds upon a body of prior research on accessibility and programming tools. We first review the need to express specific intents in assistive technology. Then, we review various approaches to technology personalization: personalization in assistive technologies, DIY assistive technology, and end-user programming."}, {"title": "2.1 Information Seeking in Assistive Technology", "content": "Searching visual scenes for specific pieces of information has always been an important aspect of assistive technology design. In early remote human assistance approaches like VizWiz, users would submit a question along with an image, and assistants would use their human intelligence to determine a relevant answer [5]. This need for specific information is present across a variety of accessibility contexts: Find My Things and Kacorri use teachable object recognizers to help blind users locate specific possessions [32, 50], VizLens helps blind users search for specific buttons on physical interfaces [23], and CueSee highlights products of interest for people with low vision [72]. Even outside of accessibility contexts, the Ctrl-F shortcut for 'find' is ubiquitous. General-purpose and specific assistive technologies each have their uses; compare the ambient audio cues in Microsoft's Soundscape [47] to navigation directions from Google Maps, neither is a direct replacement for the other.\nYet, current automated assistive applications present challenges to getting specific information quickly. Whether they run on a live camera feed (e.g, Seeing AI [46]) or on a static image (e.g, the GPT-4 powered 'Be My AI' [18]), commercial applications have taken a general approach to describing visual information, conveying all results from the underlying OCR or object detection models, or generating as rich of a description about the visual content as possible. While this is sometimes desirable, it risks slowing down and increasing the cognitive burden on users who are looking for something specific [20]. In this work, we aim to target this need for specificity. ProgramAlly is a live assistive technology that can provide continuous feedback, but it also aims to capture a user's explicit intent through the creation of filtering programs."}, {"title": "2.2 Methods for Personalizing Assistive Technology", "content": "In accessibility research, personalization of technology to meet user needs is used to reduce the burden of accessibility on users [21, 61, 62]. This typically leaves the function of the technology unchanged, but aims to automatically map the input and output mechanisms to new systems or modalities [19, 69]. For example, Yamagami et al. recently considered how people with motor impairments would create personalized gesture sets that map to common input mechanisms [71].\nWork customizing the functionality of assistive technology is more limited in comparison. In AI assistive technology, teachable object recognizers have been used to allow users to personalize recognition models themselves [33]. Users capture their own image or video data of unique objects that can be stored and later recognized [50, 64]. These approaches can be more useful than off the shelf object recognition models as they are customized to user's specific needs [9, 32]. However, for commercial applications, users have limited avenues for customization. While screen readers can be personalized through a variety of settings, shortcuts, and add-ons [49], AI powered assistive applications are typically part of closed software ecosystems. While they may have some settings within the application for things like language and output speed, this is typically the extent of the customization. Through this work, we hope to demonstrate new methods for personalizing assistive technology functionality to meet unique user needs."}, {"title": "2.3 DIY Assistive Technology", "content": "DIY communities have adopted an approach to making centered around personalization, democratization, and collaboration [37, 63]. For assistive technology, DIY approaches can help to address assistive technology adoption due to unique or changing needs [29]. To this end, prior research on DIY assistive technology has sought to make the process of prototyping and making more accessible to participants with a range of technical skills [44, 56].\nMost of this research focuses on making physical tools for accessibility (e.g., making 3D-printed devices like an ironing guide, right angle spoon, or tactile graphics [10], prototyping custom prosthetics [27]), rather than software tools. Some tools are being developed to support blind people in DIY-ing more high-tech hardware sensing systems, such as AllyBits [25] or the Blind Arduino Project [30]. While these raise the ceiling of high-tech DIY creation, little research has focused on DIY-ing new software systems for existing devices users already own. In their original case studies of DIY assistive technology, Hurst and Tobias highlighted one instance of 'high-tech custom-built assistive technology', wherein a team of professional programmers worked with an artist with ALS to create software that used eye-tracking input for drawing [29]. Lowering the barrier to entry for creating technically complex assistive software is an important next step in enabling people to DIY personally meaningful assistive technology."}, {"title": "2.4 End-User Programming", "content": "Decades of end-user programming research has sought to understand and support programming work done by people who are not trained as programmers [51]. While initially focusing on end-user programming in professional contexts (e.g., using spreadsheets or other domain-specific tools [59]), a variety of approaches have been developed to support programming for personal utility as well. For example, Marmite is an end-user programming tool that allows users to create new applications by combining data and services from multiple existing websites [70]. Here, we describe previous end-user programming approaches that work towards the goal of making programming more approachable for novices. In this work, we aim to apply these existing end-user programming approaches to the domain of visual assistive technology, enabling blind people to have a new level of control over assistive software.\nBlock-Based Programming. Visual, block-based programming approaches allow users to create programs by graphically organizing elements. These approaches often aim to support novices by providing pre-structured statements to reduce or eliminate syntax errors [48], for example, as in Scratch [52]. While these approaches are commonly used in educational settings [68], they are also used in commercial mobile automation tools to provide sets of components that users can arrange as they wish to create time-saving automations, as in Shortcuts on iOS [1] and Google Assistant [42].\nNatural Language Programming. Further work has aimed to synthesize programs from natural language alone. These approaches commonly require a set of training data consisting of queries and desired automations [17, 39]. Large language models have also been used for program synthesis, with mixed results [2, 67].\nProgramming By Example. Programming by example approaches alternatively allow users to create programs by providing demonstrations of desired functionality, without the need for any code [16, 40]. Programming by example has been implemented in a range of domains, for instance, Rousillon automates web scraping with a demonstration from users on how to collect the first row of a data table [11], and Sugilite automates actions on mobile interface using a demonstration and natural language request [38].\nEnd-User Programming and Accessibility. The accessibility of programming tools is a nascent area [53-55] that has largely focused on developers rather than end users. End-user programming approaches have occasionally been applied to accessibility contexts for the purposes of sharing accessibility bugs and teaching blind children. For example, for web accessibility, demonstration has been"}, {"title": "3 PROGRAMALLY", "content": "ProgramAlly is a mobile application that implements end-user programming techniques to allow users to create block-based visual information filtering programs (e.g., 'find NUMBER on BUS'). ProgramAlly is implemented as a native iOS application, and consists of the following components, as shown in Figure 2:\n(1) A program representation, as the framework for implementing and running programs with on-device models.\n(2) Program creation interfaces provide a multi-modal set of tools for users to create and iterate on programs.\n(3) A program generation server provides the components for automatically generating programs based on images or natural language text."}, {"title": "3.1 Design Goals", "content": "Overall, we designed ProgramAlly based on three primary goals: Expressiveness, Approachability, and Accessibility.\nD1: Expressiveness. ProgramAlly's goal is to be an interface where users can customize off-the-shelf models for their own uses. Programs should be able to support a wide variety of real-world use cases through a flexible structure and range of models.\nD2: Approachability. ProgramAlly needs to be approachable for non-experts. To this end, it includes a set of methods to create and iterate on programs through multiple modalities, and users can choose what fits their needs. ProgramAlly should aim to have as little technical jargon as possible and explain program parameters in natural terms.\nD3: Accessibility. ProgramAlly needs to be VoiceOver and Braille display accessible for users, both while creating and running programs. ProgramAlly's VoiceOver implementation groups related parameters together to provide context for each statement. Additionally, ProgramAlly provides visual context while running programs to help the user aim and know what is in frame."}, {"title": "3.2 Visual Filtering Programs in ProgramAlly", "content": "ProgramAlly is built on a generalizable representation of visual filtering tasks. Here, we describe how that represntation was designed, how it is implemented, and how it is used to run visual filtering programs."}, {"title": "3.2.1 Designing a Representation of Filtering Tasks", "content": "ProgramAlly's scope of programming visual filtering tasks was determined based on prior work indicating it to be a possible domain for assistive technology customization [26]. We aimed to understand features of filtering tasks in order to build a program representation that could capture a variety of user needs (D1: Expressiveness). Herskovitz et al. captured a dataset of scenarios where blind participants described cases of wanting to create or customize assistive technology [26]. From this dataset, we labeled specific instances as filtering tasks: cases where the participant was searching for a certain type of information. We considered a task to require filtering if using a general scene description tool like Be My AI [18] or OCR like Seeing Al's Document Mode [46] would produce extraneous or distracting information beyond the intended task, but could produce useful results with additional processing.\nOut of the original set of 201 scenarios, we identified 29 as filtering. These were fairly evenly spread across all 12 participants from the dataset, with each participant describing at least one. Scenarios fell roughly into two types: finding specific types of text, or specific items. For searching for text, this could be finding specific strings (i.e., a name on a package, a room number in a hotel), finding certain types of text (i.e., a number of miles on a treadmill, the number of calories on a package), or finding text in a specific location (i.e., on a thermostat display, on a license plate). For searching for objects, this could be finding a specific type of item (i.e., a trash can in a mall, a stairway), or items in a specific location (i.e., a person in a chair, an obstacle on a sidewalk).\nFrom this analysis, we determined two key aspects to include in our filtering program representation: (1) the ability to filter by type of object or text, and (2) the ability to filter by an item's location. Our representation includes two types of statements to address this: a 'find' statement, and an 'on' statement to convey objects overlapping. We confirmed this representation by analyzing a random sample of questions from the VizWiz Question Answering dataset, a dataset of images and questions asked by blind people [24]. We found that the two statements in our representation could represent a significant portion (approximately half) of the 100 queries we analyzed, without the need for additional operators that would increase program complexity."}, {"title": "3.2.2 Program Representation", "content": "Programs in ProgramAlly are generally in the form 'find ITEM on ITEM', with any number of 'find' or 'on' statements. For example, a program can range from 'find CAR' to 'find COLOR on CAR' to 'find TEXT on LICENSE PLATE on CAR'. Adding multiple 'find' statements runs each 'find' statement in parallel and produces a similar effect to an OR operator. For example, the program 'find COLOR on CAR, find TEXT on LICENSE PLATE on CAR' for locating a ride share would announce both the color and license plate number of a car if visible.\nAdditionally, each item in the statements can consist of both a target item (e.g., an object, a type of text), and an optional adjective to describe that target. ProgramAlly supports adjectives denoting color, size, or location. For example, 'find NUMBER on RED BUS', 'find LARGEST TEXT on SIGN', or 'find ADDRESS on CENTER ENVELOPE' are programs where the output would be further restricted to match specific conditions. Programs in ProgramAlly are stored as lists of these items (adjective and target pairs)."}, {"title": "3.2.3 Running Programs", "content": "ProgramAlly uses this representation to run programs to generate live output. ProgramAlly does this by iterating over the list of items in a program backwards, cropping or filtering the source image at each step. For example, in the 'find NUMBER on BUS' program shown in Figure 2, ProgramAlly first runs an object detection model that has the class 'bus'. The model will output a series of bounding boxes that have that class label. Then, the next item in the program is processed. In this case, for each bus bounding box, the frame is cropped and passed into a text detection model. The resulting text is then filtered for strings that only consist of numbers. ProgramAlly keeps track of points where filtering fails (say, if no buses are found), and later uses that information to generate descriptive program output.\nProgramAlly currently leverages a set of models and functions for processing each piece of a program. Each item that can be included in a program is stored in a dictionary associating it with the relevant model. The two primary types of targets, objects and text, are both recognized with a set of on-device models, though this could be extended in the future to support cloud models as well. For object detection, ProgramAlly uses a set of YOLO models as they have low latency on a range of iPhones. This includes the default set of 80 object classes detected by YOLOv8 [57, 66]. Additionally, we included a set of modified YOLO-World models, a version of YOLOv8 that can be extended with new detection classes without any fine-tuning through a vision-language modeling approach [13, 65]. We added four additional models with classes that we chose to be relevant to accessibility tasks (D1: Expressiveness) [8]: an outdoor navigation model ('sign', 'license plate'), an indoor navigation model ('door', 'stairs', 'hallway', 'exit sign', 'trash can'), a reading model ('envelope', 'package', 'document\u201d, 'poster'), and a product identification model ('package', 'can', 'bottle', 'box', 'product', 'jar'). For this last set of classes, we made them available under one super-class called 'grocery item' for flexibility. New models can easily be added to ProgramAlly as it searches the dictionary for the appropriate model when running a program.\nIn addition to recognizing objects, ProgramAlly also detects text with iOS's native text recognition. Programs can include the item 'any text', but can also include various more specific types of text such as 'address', 'email', 'phone number', 'date', etc. These types can all be used within programs, for example, 'find ADDRESS on PACKAGE'. These text types are detected by a combination of Google's Entity Extraction API [22] and regex functions.\nFinally, adjectives in ProgramAlly are then used to further filter object or text results. Adjectives include color (red, blue, etc.), size (largest, smallest), and location (center, upper left, etc.). These can be used alongside any item, for instance in the program 'find LARGEST TEXT on BLUE SIGN'. Adjectives in ProgramAlly were implemented natively: color is detected by matching the most common pixel colors within an object's bounding box to a set of strings; size is determined by comparing an object's bounding box to others of its type and then filtering for the lower or upper quartile; and item location is determined based on a quadrant system, breaking down the parent object (either the image frame or a bounding box) into sections to label the location of a child item (i.e., \"text on upper left\"). While these implementations are naive, they are meant to demonstrate that a variety of sources of classification can be used in ProgramAlly, and could eventually be replaced by more robust models or algorithms."}, {"title": "3.2.4 Program Output", "content": "While running programs, ProgramAlly keeps track of where target items were found in order to give context for each piece of information. For example, if two buses are found in the frame, the output could be: \"Found number 73 on bus, left of frame, found number 21 on bus, right of frame.\" This system also tracks where the program failed if the target was not found. If the first item in the program is not found, ProgramAlly will attempt to provide output for the second item, and so on. For example, if a bus is found with no number on it, the output would be, \"Found bus, no number.\" In this case, because 'number' results are filtered from the more general text detection model, ProgramAlly would also read strings that are not numbers as a backup, for example, the route name. Unique messages are generated for each failure point, for example, if no buses are found (\u201cNo bus found\"), or if a bus is found but the adjective does not match (\"Found white bus, no red bus visible\"). This information is used to provided helpful backup information for understanding the scene and aiming the camera.\""}, {"title": "3.3 Block-Based Programming Mode", "content": "ProgramAlly's first method for creating new filtering programs is a block-based programming interface, shown in Figure 1. This block mode can be used to create a program from scratch, or to edit a program that was generated automatically by one of the other two methods. When first creating a new program, the authoring interface will display a program with two empty items in order to provide a default structure for users to fill in. There are two sections of this interface. First, a heading called 'Program Summary', which includes a natural language summary of the implemented program for users to refer back to as they edit. For the default program, this will initially read, \"Find any object on any object\", and will update as users fill in the program with their desired items.\nNext, under a heading called 'Edit Program Directly', users can read through each statement in the program, and edit them with actions in VoiceOver. For example, when VoiceOver focus is on the first 'find' statement, it will announce: \"Find any object, actions available: Edit adjective, Edit object, Delete this item.\" If parameters for the item have already been selected, the VoiceOver description changes to reflect what has been chosen. For example, for the statement 'find RED BUS', the description would be: \"Find red bus, actions available: Edit adjective 'red', Edit object 'bus', Delete this item.\" Grouping these together as one single element with multiple actions, rather than having 'edit adjective' and 'edit object' as separate VoiceOver elements, is meant to clarify that the different parameters in the 'find' statement are functionally related, without relying on the visual aspect of them each being on one line (D3: Accessibility). This design was also based on the VoiceOver experience of Apple's Shortcuts app [1], where each block is read as a separate element and editing parameters can be similarly accessed through actions. When either of the edit actions are activated, a new page will appear listing the possible adjectives or objects to fill in the program (shown in Figure 1). The menu includes buttons that can be used to filter the items by type, or a search bar for finding a specific item."}, {"title": "3.4 Natural Language: Question Mode", "content": "Inspired by natural language programming approaches, ProgramAlly includes 'Question Mode', which generates a program from a question or statement (D2: Approachability). Users can type or dictate a question, and the resulting program will appear in the block-based interface for them to review and refine further. For instance, the query, 'What does this bottle say?' would result in the generated program: 'find ANY TEXT on BOTTLE'. This result could then be modified with a follow up question: 'Actually, just read the biggest text' changes the program to 'find LARGEST TEXT on BOTTLE'.\nTo prototype this interaction, we use a few-shot prompting approach with GPT-4. We provide a custom system prompt describing how to extract items, and listing the possible item classes. Then, we provide a set of approximately 20 queries and their correct JSON program representation that we wrote based on examples from accessibility datasets [24, 26]. Without developing a custom entity extraction workflow, we found that this approach works well. However, GPT-4 will sometimes produce errors. The most common issue is the model hallucinating new object classes. In this case, the block interface will alert the user that there is an unsupported field and open the editing menu for users to select an alternative. The model very rarely produces programs with an incorrect structure. If the model fails to extract entities, which can happen if the question is vague (e.g., \"What is this?\") it will occasionally respond with natural language rather than a program (e.g., \"I'm sorry, I don't know what you mean, can you clarify?\"). Future work could use additional fine-tuning to create a conversational approach to clarifying ambiguous language.\nProgramAlly also includes a method for users to edit programs with a follow-up question, rather than by manually editing a generated program using the block-mode. When editing a generated or pre-existing program, there is a text box where users can type or dictate a follow-up question (Figure 1). Various follow up questions are included in our system prompt to GPT. Based on feedback in our formative studies, we also included an option to access this feature while a program is running. If the program output is not as expected, users can directly access the option to edit the program with natural language, for rapid iteration. For example, when running the program 'find NUMBER on BUS', the user could provide the statement, \"Read the route name instead\", and the program would be modified to be 'find TEXT on BUS'."}, {"title": "3.5 Programming-By-Example: Explore Mode", "content": "ProgramAlly's Explore Mode allows users to automatically generate a program by selecting a target feature detected in the camera feed (D2: Approachability). Explore mode lists all object and text features in the image, and users select an item to filter for, effectively providing a demonstration of the filtering behavior. Explore mode was included to address the challenge of unknown-unknowns [7]: without knowing what visual features or information is present, blind users would not necessarily have all of the information available to write a working filtering program.\nIn this mode, ProgramAlly runs all object and text detection models at once, with the goal of outputting everything that a user might want to create a program to find. Users then select the information they are looking for to demonstrate filtering behavior, and a program is generated which aims to filter for that type of information in the future. For example, as shown in Figure 3, the camera is pointing at a bus stop. If the user selects '30', which is a route number, the resulting generated program will be 'find NUMBER on BUS', because that is where the text '30' was found in the frame. Once a program is generated, the app again displays the result in the block-based interface, with the program summary and the option to edit the generated program further either with a question or with blocks."}, {"title": "3.5.1 Generating Programs from Demonstrations", "content": "Programs are generated using ProgramAlly's server, which represents images as a tree of items. First, on the device, ProgramAlly maintains a list of each frame where an item was detected. When the user selects an item, a frame is then chosen that contains that item, which is then sent to the generation server. ProgramAlly's server uses a set of models to then generate the tree structure. These are slightly different than the models used on device, and includes Mask R-CNN under Detectron2 [45] and Google's Cloud Vision API [15] for object detection and Google Cloud's OCR model [14] for text. Additionally, to label adjectives and other properties associated with each item, the server runs DenseCap [31], a model that creates rich language descriptions of image regions. Because DenseCap produces natural language descriptions associated with bounding boxes, we use a few-shot prompting approach to GPT-4 [58] to extract and label the relevant objects and their associated adjectives.\nNext, each item is stored as a node in a scene graph hierarchy based on bounding box overlap. The parent node is the entire image, and child nodes can either be text or objects, stored with their associated adjectives. Finally, from this scene graph, the originally selected node is then used to generate a program. The selected node is located in the scene graph, and all of its ancestor nodes (not including the root image) are then selected, representing a branch of the graph (see Figure 3). Traversing up this branch, each node then becomes an item in the program. Each node in this set is converted into an adjective and object pair, and ordered based on their parent-child relationship in the source graph. This generated program is then sent to the device as JSON. While ProgramAlly currently uses a strict tree structure to avoid any ambiguity (ensuring that a single branch can always be chosen), this does limit this generation technique to supporting only the current 'find' and 'on' operators. To support more complex programs, new synthesis techniques would need to be developed.\nBecause the server includes the addition of DenseCap for describing objects, there may in rare cases be a class in the generated program that is not present in the app, although we aim to filter these classes out when possible. In this case, the block interface will again alert the user that the field is unsupported and surface the menu for selecting a replacement."}, {"title": "4 USER STUDY PROTOCOL", "content": "To understand how ProgramAlly can be used as a tool for creating and customizing assistive technology, we conducted a study with 12 blind participants. Our goals were to (1) assess the accessibility and approachability of ProgramAlly, and (2) understand unique challenges faced by blind end-user developers creating visual technology. This study was approved by our Institutional Review Board (IRB). Participants were compensated $25 per hour for their time and expertise. This ranged from 1.5 to 3 hours in total, with an average time of 2 hours."}, {"title": "4.1 Participants", "content": "Participants were recruited using email lists for local accessibility organizations, prior contacts, and snowball sampling. Participants were required to be over 18 years old, have some level of visual impairment, and regularly use a screen reader to access their devices. Participants were also required to have an iPhone so that they could download ProgramAlly via TestFlight.\nDemographic information for participants is shown in Table 1. Of the 12 participants, two had some prior programming experience for their coursework or career. However, participants had a range of experiences with technology and VoiceOver, and not all were experts. For example, R2 and R4 were assistive technology professionals, while F2 was new to using VoiceOver and had not previously used any mobile assistive applications. We recognize that recruiting remote participants can create a bias for people who are technically savvy, as they need to have a desktop and be familiar with Zoom. To try diversifying our sample, we also recruited in-person participants."}, {"title": "4.2 Procedure", "content": "After a brief introductory interview, participants were introduced to ProgramAlly by reading through a pre-written program to familiarize themselves with the concept and interface. Participants were then asked to modify the example program slightly by adding an adjective. We tried to keep verbal instructions minimal to let participants reason for themselves about the program. The only pointer that we gave to participants was that they could swipe up or down on the program elements to hear the different editing actions available, because depending on the verbosity settings of their device, VoiceOver may not have spoken this information.\nAfter this introduction, participants then used ProgramAlly to create and run three programs. Participants used each of the three programming interfaces (block mode, explore mode, question mode), and were assigned tasks to create a program for (i.e., 'create a program that will find addresses on mail'). While creating each program, participants were prompted to think aloud to explain their thoughts or concerns, or to ask questions. Participants then ran the programs they created, with remote participants using sample images and in-person participants using props. These props included books, grocery items, and packages and mail. In-person participants were also given the option to compare their program to either Be My AI or Seeing AI, depending on what they would normally use for the same task. After creating each program, participants were asked to rate the ease of creating the program, and how accurate they felt the program was.\nFinally, participants were asked to rank the three creation modes on different factors. The study concluded with an open-ended interview about the prototype app and the experience of using end-user programming methods for DIY assistive technology overall. Participants were asked to imagine how such an app could fit into their existing assistive technology workflows, and the pros and cons of creating programs to customize assistive technology."}, {"title": "4.3 Data Collection and Analysis", "content": "Remote participants joined a Zoom call that was recorded, and when testing the app they were asked to share their phone's screen in the call. Similarly, the face-to-face participants interviews were audio recorded, and their devices were screen recorded. The audio was later transcribed and used for analysis. We then created written descriptions of participant's strategies for completing each tasks from the video data.\nSince participants were encouraged to use a think-aloud method and take their time to fully explore the functions, ask usability questions, and give feedback, performing an analysis of task completion time does not provide much insight about how ProgramAlly works in practice. Instead, we primarily report qualitative data on participants' strategies and workflows while using ProgramAlly, and their general feedback."}, {"title": "5 USER STUDY RESULTS", "content": "Here we present results from our user study. First"}]}