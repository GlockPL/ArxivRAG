{"title": "VIBIDSAMPLER: ENHANCING VIDEO INTERPOLATION USING BIDIRECTIONAL DIFFUSION SAMPLER", "authors": ["Serin Yang", "Taesung Kwon", "Jong Chul Ye"], "abstract": "Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, need adaptation for two-frame (start & end) conditioned generation, which is essential for effective bounded interpolation. Unfortunately, existing approaches that fuse temporally forward and backward paths in parallel often suffer from off-manifold issues, leading to artifacts or requiring multiple iterative re-noising steps. In this work, we introduce a novel, bidirectional sampling strategy to address these off-manifold issues without requiring extensive re-noising or fine-tuning. Our method employs sequential sampling along both forward and backward paths, conditioned on the start and end frames, respectively, ensuring more coherent and on-manifold generation of intermediate frames. Additionally, we incorporate advanced guidance techniques, CFG++ and DDS, to further enhance the interpolation process. By integrating these, our method achieves state-of-the-art performance, efficiently generating high-quality, smooth videos between keyframes. On a single 3090 GPU, our method can interpolate 25 frames at 1024\u00d7576 resolution in just 195 seconds, establishing it as a leading solution for keyframe interpolation.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models (Blattmann et al., 2023a;b; Wu et al., 2023; Xing et al., 2023; Bar-Tal et al., 2024) have made it possible to generate high-quality videos that closely match a given text or image conditions. Various efforts have been made to leverage the powerful generative capabilities of these video diffusion models, especially in the context of keyframe interpolation, to improve perceptual quality significantly. Specifically, diffusion-based keyframe interpolation (Voleti et al., 2022; Danier et al., 2024; Huang et al., 2024; Feng et al., 2024; Wang et al., 2024) focuses on generating intermediate frames between two keyframes, aiming to create smooth and natural motion dynamics while preserving the keyframes' visual fidelity and appearance. Image-to-video diffusion models are particularly well-suited for this task because they are designed to maintain the visual quality and consistency of the initial conditioning frame.\nWhile image-to-video diffusion models are designed for start-frame conditioned video generation,\nthey need to be adapted for start and end frame conditioned video generation for keyframe interpolation. One line of works (Feng et al., 2024; Wang et al., 2024) addresses this issue by introducing a new sampling strategy that fuses the intermediate samples of the temporally forward path, conditioned on the start frame, and the temporally backward path, conditioned on the end frame. The fusing strategy ensures smooth and coherent frame generation in-between two keyframes using image-to-video diffusion models in a training-free (Feng et al., 2024) or a lightweight fine-tuning manner (Wang et al., 2024).\nIn the geometric view of diffusion models (Chung et al., 2022), the sampling process is typically described as iterative transitions $M_t \\rightarrow M_{t-1}, t = T, \u00b7\u00b7\u00b7, 1$, moving from the noisy manifold $M_T$ to the clean manifold $M_0$. From this perspective, fusing two intermediate sample points through linear interpolation on a noisy manifold can lead to an undesirable off-manifold issue, where the generated samples deviate from the learned data distribution. TRF (Feng et al., 2024) reported that this fusion strategy often results in undesired artifacts. To address these discrepancies, they apply multiple rounds of re-noising and denoising to the fused samples, which may help correct the off-manifold deviations.\nUnlike the prior works, here we introduce a simple yet effective sampling strategy to address off-manifold issues. Specifically, at timestep t, we first denoise $x_t$ to obtain $x_{t-1}$ along the temporally forward path, conditioned on the start frame ($I_{start}$). Then, we re-noise $x_{t-1}$ back to $x_t$ using stochastic noise. After that, we denoise $x'_t$ to get $x'_{t-1}$ along the temporally backward path, conditioned on the end frame ($I_{end}$), where the ' notation indicates that the sample has been flipped along the time dimension. Unlike the fusing strategy, which computes two conditioned outputs in parallel and then fuses them, our bidirectional diffusion sampling strategy samples two conditioned outputs sequentially, which mitigates the off-manifold issue.\nFurthermore, we incorporate advanced on-manifold guidance techniques to produce more reliable interpolation results. First, we employ the recently proposed CFG++ (Chung et al., 2024), which addresses the off-manifold issues inherent in Classifier-Free Guidance (CFG) (Ho & Salimans, 2021). Second, we incorporate DDS guidance (Chung et al., 2023) to ensure proper alignment of the last frame of the generated samples with the given frames, as the ground-truth start and end frames are already provided. By combining bidirectional sampling with these guidance techniques, our method achieves stable, state-of-the-art keyframe interpolation performance without requiring fine-tuning or multiple re-noising steps. Thanks to its efficient sampling strategy, our method can interpolate between two keyframes to generate a 25-frame video at 1024\u00d7576 resolution in just 195 seconds on a"}, {"title": "2 RELATED WORKS", "content": "Video interpolation. Video interpolation is a task that generates the intermediate frames based on two bounding frames. Conventional interpolation methods have utilized convolutional neural networks (Kong et al., 2022; Li et al., 2023; Lu et al., 2022; Huang et al., 2022; Zhang et al., 2023b; Reda et al., 2019), which are typically trained in a supervised manner to estimate the optical flows for synthesizing an intermediate frame. However, they primarily focus on minimizing $L_1$ or $L_2$ distances between the output and target frames, emphasizing high PSNR values at the expense of perceptual quality. Furthermore, the train datasets generally consist of high frame rate videos, limiting the model's ability to learn extreme motion effectively.\nDiffusion-based methods and time reversal sampling. Diffusion-based methods have been pro-posed (Danier et al., 2024; Huang et al., 2024; Voleti et al., 2022) to leverage the generative priors of diffusion models to produce high-quality perceptual intermediate frames. Although these methods demonstrate improved perceptual performance, they still struggle with interpolating frames that contain significant motion. However, video keyframe interpolation methods that build on the robust performance of video diffusion models have been more successful in handling ambiguous and non-linear motion (Xing et al., 2023; Jain et al., 2024), largely due to the incorporation of the temporal attention layers in these models (Blattmann et al., 2023a; Ho et al., 2022; Chen et al., 2023; Zhang et al., 2023a).\nRecent advancements in video diffusion models, particularly for image-to-video diffusion, have introduced new sampling techniques that leverage temporal and perceptual priors. These techniques reverse video frames in parallel during inference and fuse bidirectional motion from both the temporally forward and backward directions. TRF (Feng et al., 2024) proposed a method that combines forward and backward denoising processes, each conditioned on the start and end frames. Similarly, Generative Inbetweening (Wang et al., 2024) introduced a method that extracts temporal self-attention maps and rotates them to sample reversed frames, enhancing video quality by fine-tuning diffusion models for reversed motion. However, these methods rely on a fusion strategy that often results in an off-manifold issue. Moreover, although methods such as multiple noise injections and model fine-tuning have been employed to address these challenges, they continue to exhibit off-manifold issues and substantially increase computational costs. In contrast, we introduce a simple yet effective sampling strategy that eliminates the need for multiple noise injections or model fine-tuning."}, {"title": "3 VIDEO INTERPOLATION USING BIDIRECTIONAL DIFFUSION", "content": "Although our method is applicable to general video diffusion models, we employ Stable Video Diffusion (SVD) (Blattmann et al., 2023a) as a proof of concept in this paper. By introducing SVD, we aim to provide a clearer understanding of our approach. SVD is a latent video diffusion model employed in EDM-framework (Karras et al., 2022) with micro-conditioning (Podell et al., 2023) on frame rate (fps). For the image-to-video model, SVD replaces text embeddings with the CLIP image embedding (Radford et al., 2021) of the conditioning.\nIn EDM-framework, the denoiser $D_\\theta$ computes the denoised estimate from the U-Net $\\epsilon_\\theta$:\n$D_\\theta(x; \\sigma, c) = C_{skip}(\\sigma)x + C_{out}(\\sigma)\\epsilon_\\theta(C_{in}(\\sigma)x; C_{noise}(\\sigma), c),$        (1)\nwhere $C_{skip}$, $C_{out}$, $C_{in}$, and $C_{noise}$ are $\\sigma$-dependent preconditioning parameters and $c$ is the condition. In practice, the denoiser $D_\\theta$ takes concatenated inputs $[x, x]$ to return c-conditioned estimate and null-conditioned estimate $[x_c(x), x_\\emptyset(x)]$, where $x_c$ is then updated using $w$-scale classifier-free guidance (CFG) (Ho & Salimans, 2021):\n$x_c(x) \\coloneqq x_\\emptyset(x) + w[x_c(x) \u2013 x_\\emptyset(x)].$                     (2)\nFor sampling, SVD employs Euler-step to gradually denoise from Gaussian noise $x_T$ to get $x_0$:\n$x_{t-1}(x_t; \\sigma_t, c) := c(x_t) + \\frac{\\sigma_{t-1}}{\\sigma_t}(x_t - x_c(x_t)),$      (3)"}, {"title": "3.1 BIDIRECTIONAL SAMPLING", "content": "Prior approaches such as TRF (Feng et al., 2024) and Generative Inbetweening (Wang et al., 2024) have employed a fusion strategy that linearly interpolates between samples from the temporally forward path, conditioned on the start frame ($I_{start}$), and the temporally backward path, conditioned on the end frame ($I_{end}$):\n$x_{t-1,C_{start}} = x_{t-1}(x_t; \\sigma_t, C_{start}),$      (4)\n$x'_{t-1,\\mathcal{C}_{end}} = x_{t-1}(x'_t; \\sigma_t, C_{end}),$      (5)\n$x_{t-1} = \\lambda x_{t-1,C_{start}} + (1 - \\lambda)(x'_{t-1,\\mathcal{C}_{end}})',$      (6)\nwhere the ' notation indicates that the sample has been flipped along the time dimension, $\\lambda$ denotes interpolation ratio, $C_{start}$ and $C_{end}$ denotes the encoded latent condition of $I_{start}$ and $I_{end}$, respectively. However, as the authors in TRF (Feng et al., 2024) reported, the vanilla implementation of this fusion strategy suffers from random dynamics and unsmooth transitions. This occurs because linearly interpolating between two distinct sample points in the noisy manifold $M_t$ can cause the deviation from the original manifold, as illustrated in Fig. 3 (a).\nIn this work, we aim to leverage the image-to-video diffusion model (SVD) for keyframe interpolation tasks, eliminating the multiple noise injections or model fine-tuning. Notably, our key innovation lies in the sequential sampling of the temporally forward path of $x_t$ and the temporally backward path of $x'_t := flip(x_t)$ by integrating a single re-noising step between them:\n$x_{t-1,C_{start}} = x_{t-1}(x_t; \\sigma_t, C_{start}),$         (7)\n$x'_{t, C_{start}} = x_{t-1, C_{start}} + \\sqrt{\\sigma_t^2 - \\sigma_{t-1}^2}\\epsilon,$         (8)\n$x'_{t-1} = x_{t-1}(x'_{t,c_{start}}; \\sigma_t, C_{end}),$         (9)\n$x'_{t-1} = (x'_{t-1})'.$         (10)\nThis approach effectively constrains the sampling process for bounded generation between the start frame ($I_{start}$) and the end frame ($I_{end}$). As depicted in Fig. 3 (b), our method seamlessly connects the temporally forward and backward paths so that the sampling trajectory stays within the SVD manifold, resulting in smooth and coherent transitions throughout the interpolation process."}, {"title": "3.2 ADDITIONAL MANIFOLD GUIDANCES", "content": "We further employ recent advanced manifold guidance techniques to enhance the interpolation performance of the bidirectional sampling. First, we introduce additional frame guidance using DDS (Chung et al., 2023). Then, we replace traditional CFG (Ho & Salimans, 2021) with CFG++ (Chung et al., 2024) to mitigate the off-manifold issue of CFG in the original implementation of SVD (Blattmann et al., 2023a).\nLast frame guidance with DDS. DDS (Chung et al., 2023) synergistically combines the diffusion sampling and Krylov subspace methods (Liesen & Strakos, 2013) such as the conjugate gradient (CG) method, guaranteeing the on-manifold solution of the following optimization problem:\n$\\min_{x \\in \\mathcal{M}} l(x) := ||y \u2013 A(x)||^2,$      (11)\nwhere $A$ is the linear mapping, $y$ is the condition, and $\\mathcal{M}$ represents the clean manifold of the diffusion sampling path.\nHere, we leverage the DDS framework to guide a start-frame-conditioned sampling path of SVD to a start- and end-frame-conditioned sampling path. Specifically, for the temporally forward path, conditioned on the start frame ($I_{start}$), we take the DDS step on denoised estimate $x_{C_{start}}(x_t)$ to guide the last frame of $x_{C_{start}}(x_t)$ to align with $c_{end}$. For the temporally backward path, conditioned on the end frame ($I_{end}$), we take the DDS step on denoised estimate $x'_{C_{end}}(x'_t)$ to guide the last frame of $x'_{C_{end}}(x'_t)$ to align with $c_{start}$. In practice, we set $A(x) := x_{last}$ as last frame extractor, $y$ as matched condition which are $c_{end}$ for temporally forward path and $c_{start}$ for temporally backward path:\n$\\begin{aligned}\nx_{C_{start}} := & \\underset{x \\in x_{\\mathcal{E}_{C_{start}}} + K_l}{\\text{arg min}} ||C_{end} - A(x)||^2, x'_{C_{end}} := & \\underset{x \\in x'_{\\mathcal{E}'_{C_{end}}} + K_l}{\\text{arg min}} ||C_{start} - A(x)||^2,\n\\end{aligned}$      (12)\nwhere $K_l$ is the l-th order Krylov subspace, in which Krylov subspace methods seek an approximate solution. By leveraging this DDS framework, we effectively guide the sampling process toward a path conditioned by both the start and end frames, which is particularly effective for keyframe interpolation.\nBetter Image-Video alignment with CFG++. Recent advances of CFG++ (Chung et al., 2024) tackles the inherent off-manifold issue in CFG (Ho & Salimans, 2021). Specifically, CFG++ mitigates this undesirable off-manifold issue using the unconditional score instead of the conditional score in a re-noising process of CFG. By using the unconditional score, CFG++ can overcome the off-manifold phenomena in CFG-generated samples, resulting in better text-image alignment for text-to-image generation tasks.\nWhile SVD replaces text embedding with CLIP image embedding, we can still use CFG++ for image-to-video diffusion models to ensure better image-video alignment. Specifically, after applying CFG++ into SVD sampling framework, the Euler-step of SVD (3) now reads:\n$x_{t-1}(x_t; \\sigma_t, c) := c(x_t) + \\frac{\\sigma_{t-1}}{\\sigma_t}(x_t - x_{\\emptyset}(x_t)),$           (13)"}, {"title": "4 EXPERIMENTAL RESULTS", "content": "4.1 EXPERIMENTAL SETTING\nDataset. The high-resolution (1080p) video datasets used for evaluation are sourced from the DAVIS dataset (Pont-Tuset et al., 2017) and the Pexels dataset\u00b9. For the DAVIS dataset, we pre-processed 100 videos into 100 video-keyframe pairs, with each video consisting of 25 frames. This dataset includes a wide range of large and varied motions, such as surfing, dancing, driving, and airplane flying. For the Pexels dataset, we collected 45 videos, primarily featuring scene motions, natural movements, directional animal movements, and sports actions. We used the first and last frames from each video as keyframes for our evaluation.\nImplementation Details. For the sampling process, we used the Euler scheduler with 25 timesteps for both forward and backward sampling. The motion bucket ID was fixed at 127, and the decoding frame number was set to 4 due to memory limitations on an NVIDIA RTX 3090 GPU. All other parameters followed the default settings from SVD. Since micro-condition fps is sensitive to the data, we applied a lower fps for cases with large motion and a higher fps for cases with smaller motion. While both DDS and CFG++ generally improve the results, the choice between them depends on the specific use case. All evaluations were performed on a single NVIDIA RTX 3090.\n4.2 COMPARATIVE STUDIES\nWe conducted a comparative study with four different keyframe interpolation baselines, including FILM (Reda et al., 2019), a conventional flow-based frame interpolation method, and three frame interpolation methods based on video diffusion models: TRF (Feng et al., 2024), DynamiCrafter (Xing et al., 2023), and Generative Inbetweening (Wang et al., 2024). We conducted these studies using the official implementations with default values, except for TRF, which has not been open-sourced yet."}, {"title": "4.3 COMPUTATIONAL EFFICIENCY", "content": "We performed comparative studies on the computational cost of diffusion models, as presented in Table 2. In the training stage, DynamiCrafter undergoes additional training with a large-scale image-to-video model for the frame interpolation task, while Generative Inbetweening also necessitates SVD model fine-tuning, both of which demands significant computational resources. During the inference stage, both TRF and Generative Inbetweening generate videos in 25 ~ 50 steps for each forward and backward direction, with additional noise injection steps that further increase the number of function evaluations (NFE) and inference time. However, our method does not require additional training or fine-tuning and completes the process in just 25 steps per direction, without requiring multiple re-noising."}, {"title": "4.4 ABLATION STUDIES", "content": "Bidirectional sampling and conditional guidance. The effectiveness of bidirectional sampling can be validated in the vanilla version without any conditional guidance, such as CFG++ or DDS. As demonstrated in Table 1, our vanilla model outperforms TRF across all three metrics, supporting the claim that fusing time-reversed denoising paths leads to off-manifold issues, which our method addresses through bidirectional sampling. In addition, with conditional guidance from CFG++ and DDS, we could achieve even better results and outperform DynamicCrafter and Generative Inbetweening which further train the image-to-video models. This is consistent with Fig. 5, which illustrates that frames generated by TRF exhibit blurry shapes of the golfer and unnecessary camera movement. In contrast, the body shape of the golfer and the golf club are progressively better preserved as additional conditional guidance is incorporated.\nCFG++ guidance scale. As shown in Fig. 6, at higher CFG++ scales, the semantic information of the input frames is better preserved in the generated intermediate frames, resulting in improved"}, {"title": "4.5 IDENTICAL AND DYNAMIC BOUNDS", "content": "Our method is applicable not only to dynamic bounds, where the start and end frames are different, but also to static bounds, where the start and end frames are identical. As illustrated in Fig. 7, our method successfully generates temporally coherent videos with identical start and end images (a). For example, the wave line also consistently fluctuates with the progression of time. Furthermore, as seen in the fifth and sixth rows of Fig. 7, our method effectively generates intermediate frames based on varying end frames (b and c). Given that the end images of the two rows differ, the resulting intermediate frames are generated accordingly."}, {"title": "5 CONCLUSION", "content": "We present Video Interpolation using Bidirectional Diffusion Sampler (ViBiDSampler), a novel approach for keyframe interpolation that leverages bidirectional sampling and advanced manifold guidance techniques to address off-manifold issues inherent in time-reversal-fusion-based methods. By performing denoising sequentially in both forward and backward directions and incorporating CFG++ and DDS guidance, ViBiDSampler offers a reliable and efficient framework for generating high-quality, temporally coherent, and vivid video frames without requiring fine-tuning or repeated re-noising steps. Our method achieves state-of-the-art performance in keyframe interpolation, as evidenced by its ability to generate 25-frame video at high resolution in a short processing time."}, {"title": "A ALGORITHM", "content": "Algorithm 2 Bidirectional sampling (Vanilla)\n1: $C_{start}, C_{end}\u2190 encode(I_{start}, I_{end})$\n2: fort=T: 1 do\n3:\n4:\n5:\n7:\n8:\n$Xt-1, C_{start} C_{start} + -1(Xt-XCstart)$\n\u03c3\u03c4\nXt, Cstart Xt-1,Cstart + \u221a\u03c3\u03b5 -\u03c3\u03b5-1\u20ac\nX't, Cstart flip(xt, cstart)\nX't,c Cend\n(X't, Cstart - X'Cend)\n9:\nxt-1 flip(x-1)\n10: end for\n11: return xo"}]}