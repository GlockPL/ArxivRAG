{"title": "Causal Explanations for Image Classifiers", "authors": ["Hana Chockler", "David A. Kelly", "Daniel Kroening", "Youcheng Sun"], "abstract": "Existing algorithms for explaining the output of image classifiers use different definitions of explanations and a variety of techniques to extract them. However, none of the existing tools use a principled approach based on formal definitions of causes and explanations for the explanation extraction.\nIn this paper we present a novel black-box approach to computing explanations grounded in the theory of actual causality. We prove relevant theoretical results and present an algorithm for computing approximate explanations based on these definitions. We prove termination of our algorithm and discuss its complexity and the amount of approximation compared to the precise definition.\nWe implemented the framework in a tool Rex and we present experimental results and a comparison with state-of-the-art tools. We demonstrate that ReX is the most efficient tool and produces the smallest explanations, in addition to outperforming other black-box tools on standard quality measures.", "sections": [{"title": "1. Introduction", "content": "Neural networks (NNs) are now a primary building block of many computer vision systems. NNs are complex non-linear functions with algorithmically generated (and not engineered) coefficients. In contrast to traditionally engineered image processing pipelines it is difficult to retrace how the pixel data are interpreted by the layers of the network. Moreover, in many application areas, in particular healthcare, the networks are proprietary, making the analysis of the internal layers impossible. This \"black box\" nature of neural networks creates demand for techniques that explain why a particular input yields the output that is observed without understanding the model's parameters and their influence on the output.\nAn explanation of an output of an automated procedure is essential in many areas, including verification, planning, diagnosis and the like. A good explanation can increase a user's confidence in the result. Explanations are also useful for determining whether there is a fault in the automated procedure: if the explanation does not make sense, it may indicate that the procedure is faulty. It is less clear how to define what a good explanation is. There have been a number of definitions of explanations over the years in various domains of computer science (Chajewska & Halpern, 1997; G\u00e4rdenfors, 1988; Pearl, 1988), philosophy (Hempel, 1965) and statistics (Salmon, 1989).\nBlack-box explanations for the results of image classifiers are typically based on or are given in the form of a ranking of the pixels, which is a numerical measure of importance: the higher the score, the more important the pixel is for the NN's classification outcome. Often these rankings are presented in a form of a heat map, with higher scores corresponding to hotter areas. A user-friendly explanation can then be a subset of highest-ranked pixels that is sufficient for the original classification outcome.\nThis paper addresses the following research questions:\nRQ1 What is a formal rigorous definition of explanation, suitable for image classification?\nRQ2 Can we compute explanations based on the definition above without opening the black box? What is the complexity of such computation?\nRQ3 Is there an efficiently computable approximation to explanations?\nRQ4 What are suitable quality measures for explanations? A measure is suitable if it is meaningful with respect to quality and produces results that match our intuition.\nRQ5 What is the quality of explanations computed by our algorithms compared to other black-box methods?\nRQ6 Is there a trade-off between quality and compute cost of the explanations?\nRQ7 Can black-box methods achieve the same quality of explanations as white-box methods?\nOur algorithms are based on the formal definition of explanation in the theory of actual causality by Halpern (2019) and its adaptation to image classifiers by Chockler and Halpern (2024). Essentially, an explanation in the context of image classifiers is a minimal subset of pixels and their values in the original image that is sufficient for obtaining the same classification as the original image, given that the rest of the image has value taken from a"}, {"title": "2. Related Work", "content": "There is a large body of work on explaining the results of image classifiers. As our primary interest is in black-box approaches, we only give a brief overview of the white-box algorithms here. The existing approaches to post hoc explainability can be largely grouped into two categories: propagation and perturbation.\nPropagation-based explanation methods are often regarded as more efficient. They back-propagate a model's decision to the input layer to determine the weight of each input feature for the decision. Grad-cam by Selvaraju, Cogswell, Das, Vedantam, Parikh, and Batra (2017) only needs one backward pass and propagates the class-specific gradient into the final convolutional layer of a DNN to coarsely highlight important regions of an input image. Guided Back-Propagation (GBP) by Springenberg, Dosovitskiy, Brox, and Riedmiller (2015) computes the single and average partial derivatives of the output to attribute the prediction of a DNN. Integrated Gradients (IG) by Sundararajan, Taly, and Yan (2017) further uses two axioms called sensitivity and completeness for the problem of how to attribute the classification by a deep network to its input features. NoiseTunnel by Adebayo, Gilmer, Muelly, Goodfellow, Hardt, and Kim (2018) adds gaussian noise to the image before applying another XAI method. The noise serves to smooth any resultant saliency map, improving"}, {"title": "3. Background on Actual Causality", "content": "In this section, we briefly review the definitions of causality and causal models introduced by Halpern and Pearl (2005) and relevant definitions of causes and explanations in image classification by Chockler and Halpern (2024). The reader is referred to Halpern (2019) for further reading.\nWe assume that the world is described in terms of variables and their values. Some variables may have a causal influence on others. This influence is modeled by a set of structural equations. It is conceptually useful to split the variables into two sets: the exogenous variables, whose values are determined by factors outside the model, and the endogenous variables, whose values are ultimately determined by the exogenous variables. The structural equations describe how these values are determined.\nFormally, a causal model M is a pair (S, F), where S is a signature, which explicitly lists the endogenous and exogenous variables and characterizes their possible values, and F defines a set of (modifiable) structural equations, relating the values of the variables. A signature S is a tuple (U, V,R), where U is a set of exogenous variables, V is a set of endogenous variables, and R associates with every variable $Y \\in U \\cup V$ a nonempty set $R(Y)$ of possible values for Y (i.e., the set of values over which Y ranges). For simplicity, we assume here that V is finite, as is $R(Y)$ for every endogenous variable $Y \\in V$. The set F associates with each endogenous variable $X \\in V$ a function denoted $F_x$ (i.e., $F_x = F(X)$) such that $F_x:(\\times_{U\\in u}R(U)) \\times (\\times_{Y\\in v_{x}}R(Y)) \\rightarrow R(X)$.\nThe structural equations define what happens in the presence of external interventions. Setting the value of some variable X to x in a causal model M = (S, F) results in a new causal model, denoted $M_{X\\leftarrow x}$, which is identical to M, except that the equation for X in F is replaced by X = x.\nProbabilistic causal models are pairs (M, Pr), where M is a causal model and Pr is a probability on the contexts. A causal model M is recursive (or acyclic) if its causal graph is acyclic. If M is an acyclic causal model, then given a context, that is, a setting u for the exogenous variables in U, the values of all the other variables are determined. In this paper we restrict the discussion to recursive models.\nWe call a pair (M, \u0169) consisting of a causal model M and a context \u0169 a (causal) setting. A causal formula \u03c6 is true or false in a setting. We write $(M, \\tilde{u}) \\models \\varphi$ if the causal formula \u03c6 is true in the setting (M, \u0169). The |= relation is defined inductively. $(M, \\tilde{u}) \\models X = x$ if the variable X has value x in the unique solution to the equations in M in context u. Finally, $(M, \\tilde{u}) \\models [\\tilde{Y} \\leftarrow \\tilde{y}] \\varphi$ if $(M_{\\tilde{Y}=\\tilde{y}}, \\tilde{u}) \\models \\varphi$, where $M_{\\tilde{Y}\\leftarrow \\tilde{y}}$ is the causal model that is identical to M, except that the variables in Y are set to Y = y for each $Y \\in Y$ and its corresponding value y \u2208 \u1ef9.\nA standard use of causal models is to define actual causation: that is, what it means for some particular event that occurred to cause another particular event. There have been a number of definitions of actual causation given for acyclic models (e.g., Beckers (2021), Glymour and Wimberly (2007), Hall (2007), Halpern and Pearl (2005), Halpern (2019), Hitchcock (2001, 2007), Weslake (2015), Woodward (2003)). In this paper, we focus on what has become known as the modified Halpern-Pearl definition and some related definitions introduced by Halpern (2019). We briefly review the relevant definitions below.\nThe events that can be causes are arbitrary conjunctions of primitive events (formulas of the form X = x); the events that can be caused are primitive events, denoting the output of the model."}, {"title": "Definition 3.1 (Actual cause).", "content": "X = x is an actual cause of \u03c6 in (M, \u0169) if the following three conditions hold:\nAC1. $(M,\\tilde{u}) \\models (X = x)$ and $(M, u) \\models \\varphi$.\nAC2. There is a a setting $x'$ of the variables in X, a (possibly empty) set W of variables in V \u2013 X', and a setting w of the variables in W such that $(M,u) \\models W = \\tilde{w}$ and $(M, \\tilde{u}) \\models [X \\leftarrow x', W \\leftarrow w]\\neg \\varphi$, and moreover\nAC3. X is minimal; there is no strict subset X\u02b9 of X such that $X' = \\tilde{x''}$ can replace X = $x'$ in AC2, where $z''$ is the restriction of z to the variables in X'.\nIn the special case that W = 0, we get the but-for definition. A variable x in an actual cause X is called a part of a cause. In what follows, we adopt the convention of Halpern and state that part of a cause is a cause.\nThe notion of explanation taken from Halpern (2019) is relative to a set of contexts."}, {"title": "Definition 3.2 (Explanation).", "content": "X = 7 is an explanation of \u03c6 relative to a set K of contexts in a causal model M if the following conditions hold:\nEX1a. If \u0169 \u2208 K and $(M, \\tilde{u}) \\models (X = x) \\wedge \\varphi$, then there exists a conjunct X = x of X = 7 and a (possibly empty) conjunction \u1ef6 = \u1ef9 such that $X = x\\wedge\u1ef6 = \\tilde{y}$ is an actual cause of \u03c6 in (M, \u0169).\nEX1b. $(M, u') \\models [X = x]\\varphi$ for all contexts \u0169' \u2208 K.\nEX2. X is minimal; there is no strict subset X\u02b9 of X such that $X' = z'$ satisfies EX1, where $\\tilde{z}$\" is the restriction of z to the variables in X'. (This is SC4).\nEX3. $(M,u) \\models X = \\tilde{x} > \\varphi$ for some u \u2208 K."}, {"title": "4. Theoretical Foundations", "content": "We approach the first two research questions theoretically.\nRQ1 What is a formal rigorous definition of explanation, suitable for image classification?\nWe view an image classifier (e.g., a neural network) as a probabilistic causal model. Specifically, the endogenous variables are taken to be the set V of pixels that the image classifier gets as input, together with an output variable that we call O. The variable $V_i \\in V$ describes the color and intensity of pixel i; its value is determined by the exogenous variables. The equation for O determines the output of the model as a function of the pixel values. Thus, the causal network has depth 2, with the exogenous variables determining the feature variables, and the feature variables determining the output variable.\nIn this paper we also assume causal independence between the feature variables V. This is a non-trivial assumption and, in general, is not true in practice, where we expect the color and intensity of a pixel to be causally related to color and intensity of other pixels: if a group of pixels captures, say, a cat's ear, then a group of pixels below it should capture a cat's eye. That said, assuming independence greatly simplifies explanation extraction,"}, {"title": "Definition 4.1 (Explanation for image classification).", "content": "An explanation is a minimal subset of pixels of a given input image that is sufficient for the model N to classify the image, where \"sufficient\" is defined as containing only this subset of pixels from the original image, with the other pixels set to the masking color."}, {"title": "Definition 4.2 (Singleton cause for image classification).", "content": "For an image x classified by the DNN as f(x) = o, a pixel pi of x and its value in x is a cause of o iff there exists a subset Pj of pixels of x such that the following conditions hold:\nSC1. pi \u2209 Pj;\nSC2. changing the color of any subset $P_C \\subseteq P_j$ to the masking color does not change the classification;\nSC3. changing the color of Pj and the color of pi to the masking color changes the classification.\nWe call such Pj a witness to the fact that pi is a cause of x being classified as o."}, {"title": "Definition 4.3 (Original definition of actual cause).", "content": "X = x is an actual cause of \u03c6 in (M, \u0169) if the following three conditions hold:\nAC1. $(M,\\tilde{u}) \\models (X = x)$ and $(M, \\tilde{u}) \\models \\varphi$.\nAC2'. There exists a partition (Z, W) of V with X\u2286 Z and some setting (z\u201d, w\") of the variables in (Z, W) such that if $(M, \\tilde{u}) \\models Z = z*$ for $Z \\in Z$, then\n1. $(M,\\tilde{u}) \\models [X \\leftarrow z', W \\leftarrow w']\\neg\\varphi$. In other words, changing the values of (X, W) from (x, w) to (7', ') changes from true to false;\""}, {"title": "2. $(M, \\tilde{u}) \\models [X \\leftarrow \\tilde{x},\\tilde{W} \\leftarrow w', Z' \\leftarrow Z*]\\varphi", "content": "for all subsets $Z'$ of Z. In other words, setting W to w should have no effect on \u03c6 as long as X is kept at its current value, even if all the variables in an arbitrary subset of Z are set to their original values in the context u.\nAC3. X is minimal; there is no strict subset X' of X that satisfies AC1 and AC2'.\nThe equivalence is based on the following observations. The minimality is satisfied due to only considering singletons. The set Z does not play a role due to the independence between the pixels. Removing Z and the minimality condition from Definition 4.3 we get Definition 4.2.\nComputing causality in image classification is, therefore, NP-complete, based on the above equivalence and the result by Eiter and Lukasiewicz (2002), who proved NP-completeness of Definition 4.3 for binary causal models. Moreover, following the same reasoning, Definition 4.1 is equivalent to the definition of explanation by Halpern and Pearl (2005a) for binary causal models, which is DP-complete (Eiter & Lukasiewicz, 2004). Hence, there is little hope to find an efficient algorithm for computing exact explanations for image classification.\nFinally, the following definition of the degree of responsibility holds for the case of image classifiers and singleton causes as in Definition 4.2."}, {"title": "Definition 4.4 (Simplified responsibility).", "content": "The degree of responsibility r(pi, x, o) of pi for x being classified as o is defined as 1/(k + 1), where k is the size of the smallest witness set Pj for pi. If pi is not a cause, k is defined as \u221e, and hence r(pi, x, o) = 0. If changing the color of pi alone to the masking color results in a change in the classification, we have Pj = 0, and hence r(pi, x, 0) = 1.\nRQ2 Can we compute explanations based on the definition above without opening the black box? What is the complexity of such computation?\nThe complexity of actual causality implies the intractability of the degree of responsibility of pixels in image classification as well (more precisely, computing responsibility of pixels for the classification in our setting is $FPNP[logn]$-complete). The following observation describes a brute-force approach to computing responsibility, which is clearly exponential in the number of pixels of the image."}, {"title": "Observation 4.5.", "content": "Given an image x and its classification o, we can calculate the degree of responsibility of each pixel pi of x by directly applying Definition 4.2, that is, by checking the conditions SC1, SC2, and SC3 for all subsets Pj of pixels of x and then choosing a smallest witness subset. While there is an underlying Boolean formula that determines the classification o given the values of the pixels of x, we do not need to discover this formula in order to calculate the degree of responsibility of each pixel of x.\nIn the next section we introduce a compositional approach to computing approximate explanations. The main idea is first to rank pixels according to their approximate degree of responsibility, and then use a greedy approach to extract (approximate) explanations. The algorithm for computing an approximate degree of responsibility is based on the notion of a"}, {"title": "5. Compositional Explanations", "content": "We recall the third research question.\nRQ3 Is there an efficiently computable approximation to explanations?\nWe answer this question affirmatively, by presenting our greedy compositional explanation (CE) algorithm. The general idea is to calculate the responsibility of a superpixel and recursively distribute this responsibility to all pixels within this superpixel. The CE approach in this work consists of the following steps.\n1. Given a set of superpixels, compute the responsibility of each superpixel (Section 5.1).\n2. Following the responsibility result in Step 1, further refine the superpixel and calculate the responsibility for the refined superpixels (Section 5.2).\nWe note that the ranking depends on the selected partition. To ameliorate the effect of a particular partition, the algorithm is re-run a number of times with partitions selected independently at random, and the results are averaged across all the partitions (see Section 5.3). Section 5.4 gives a step-by-step example to illustrate the working of the algorithm."}, {"title": "5.1 Computing the responsibility of a superpixel", "content": "Given a set of pixels P, we use P\u00bf to denote a partition of P, that is, a set {$P_{i,j}: \\bigcup P_{i,j} = P$ and $\u2200j \u2260 k, P_{i,j} \u2229 P_{i,k} = 0$}. The number of elements in P\u00bf is a parameter, denoted by s; in this work, we consider s = 4. We refer to Pi,j as superpixels. It is insufficient to consider each superpixel in isolation: there is no reason why anyone super pixel should be the cause of a classification. Therefore we create combinations of superpixels from the power set of Pi, denoted $2^{P_i}$.\nFor an NN N, an input x, and a partition Pi, we can generalize Def. 4.2 to the set of superpixels defined by P\u017c. We denote by ri(Pi,j,x, N(x)) the degree of responsibility of a superpixel Pi,j for N's classification of \u00e6, given Pi.\nFor a partition Pi, we denote by $X_i$ the set of mutant images obtained from x by masking subsets of $2^{P_i}$, and by $X_i'$ the subset of $X_i$ that is classified as the original image x. Formally,\n$X_i' = \\{x_m: N(x_m) = N(x)\\}$.\nWe compute ri (Pi,j, x,N(x)), the responsibility of each superpixel Pi,j in the classification of x, in Algorithm 1. For a superpixel Pi,j, we define the set\n$X = \\{x_m : P_{i,j} \\text{ is not masked in } x_m \\} \u2229 X_i$.\nFor a mutant image xm, we define diffi(xm, x) as the number of superpixels in the partition Pi that are masked in xm (that is, the difference between x and xm with respect to Pi). For an image y, we denote by y(Pi,j) an image that is obtained by masking the superpixel Pi,j in y. The degree of responsibility of a superpixel Pij is calculated by Algorithm 1 as a minimum difference between a mutant image and the original image over all mutant images xm that do not mask Pi,j, are classified the same as the original image x, and masking Pij in xm changes the classification."}, {"title": "5.2 Iterative refinement of responsibility", "content": "Algorithm 1 calculates the responsibility of each superpixel, subject to a given partition. Then, it proceeds with only the high-responsibility superpixels. Note that in general, it is possible that all superpixels in a given partition have the same responsibility. Consider, for example, a situation where the explanation is right in the middle of the image, and our"}, {"title": "5.3 Explanation extraction", "content": "So far, we assume one particular partition Pi, which Algorithm 2 recursively refines and calculates the corresponding responsibilities of superpixels in each step by calling Algorithm 1. We note that the choice of the initial partition over the image can affect the values calculated by Algorithm 2, as this partition determines the set of possible mutants in Algorithm 1. We ameliorate the influence of the choice of any particular partition by iterating the algorithm over a set of partitions. Twenty iterations of the algorithm will therefore yield 20 starting partitions, chosen at random. Rex allows the user to choose from a number of different types of random to build these partitions, with uniform being the default.\nIn Algorithm 3, we consider N initial partitions and compute an average of the degrees of responsibility induced by each of these partitions. In the algorithm, P stands for a specific partition chosen randomly from the set of partitions, and rp denotes the degree of responsibility of a pixel p w.r.t. IPx.\nAlgorithm 3 has two parts: ranking all pixels (Lines 1-9) and constructing the explanation (Lines 10-17). The algorithm ranks the pixels of the image according to their responsibility for the model's output. Each time a partition is randomly selected (Line 3), the iterative responsibility refinement (Algorithm 2) is called to refine it into a set of fine-grained superpixels and calculate their responsibilities (Line 4). A superpixel's responsibility is evenly distributed to all its pixels, and the pixel-level responsibility is updated accordingly for each sampled partition (Lines 5-7). After N iterations, all pixels are ranked according to their responsibility rp."}, {"title": "5.5 Termination and complexity", "content": "It is clear that the algorithm terminates as soon as the parts can no longer be divided into superpixels. Moreover, the number of calls it performs to the model is linear in the size of the image, as proved in the following lemma."}, {"title": "Lemma 5.1.", "content": "The number of calls of Algorithm 3 to the model is $O(2^s nN)$, where s is the size of the partition in each step (in our setting s = 4), n is the number of pixels in the original image x, and N is the number of initial partitions.\nProof. The computation of responsibilities of superpixels in one partition is O(25), as the algorithm examines the effect of mutating each subset of the superpixels in the current partition. Note that s is a constant independent of the size of the image. The number of steps is determined by the termination condition on the size of a single superpixel, which in the worst case is the same as a single pixel. In our setting, the algorithm terminates when a single superpixel is smaller contains fewer than 10 pixels\u00b9. However, in general, the algorithm can continue down to the level of a single pixel, thus resulting in n pixels in the last step. The algorithm performs N iterations, and every iteration uses a different initial partition. The parameter N is independent of the size of the image."}, {"title": "6. Evaluation", "content": "6.1 Evaluation Criteria\nRQ4 What are suitable quality measures for explanations? A measure is suitable if it is meaningful with respect to quality and produces results that match our intuition.\nOur answer to this research question will largely remain empirical. There is no single best way to evaluate the quality of an explanation. We begin by discussing quality measures proposed in the literature.\nUtility and interpretation of deletion curves When insertion and deletion curves were introduced in RISE (Petsiuk et al., 2018), the authors asserted that a high insertion and a low deletion is preferable. This assertion relies on the assumption that there is only one explanation for the image's label, or that the XAI tool found only one explanation. Moreover, saliency map values outside of the single 'explanation' must be strictly uninformative: in order to keep deletion low, the saliency map cannot provide information about any substructures present in the image. Rex does not work like this: the responsibility map provides detail over the entire pixel space.\nIf we examine Figure 6a, we have an image with a normalized insertion curve of 0.97 but a normalized deletion curve of 0.32. There would appear to be tension between the two numbers, as a high insertion suggests a high quality explanation whereas a high deletion suggests a low quality explanation. The mystery is resolved, however, by noting that images can and do contain multiple explanations, as first shown in (Shitole, Li, Kahng, Tadepalli, & Fern, 2021). Indeed, there are four distinct peaks of responsibility in Figure 6a, each of which corresponds to an independent explanation for starfish (Figure 7).\nThe deletion score is high because the responsibility map is informative even in the lower rankings. In the multiple explanation scenario, one would expect a high deletion score. Uniquely among the tools examined here, Rex supports multiple explanations for an image natively (Chockler et al., 2023). If we remove all those images that have only one"}, {"title": "6.2 Experimental Setup", "content": "We answer research questions RQ5-RQ7 experimentally. We have implemented the proposed explanation approach in the publicly available tool Rex. For all other XAI tools, apart from RISE, we used the Captum PyTorch library (Kokhlikyan, Miglani, Martin, Wang, Alsallakh, Reynolds, Melnikov, Kliushkina, Araya, Yan, & Reblitz-Richardson, 2020)2. For RISE, we used the authors' implementation\u00b3, which we lightly altered to use PyTorch rather than tensorflow.\nIn the evaluation, we compare Rex with a wide range of explanation tools, specifically GradientSHAP (Lundberg & Lee, 2017b), RISE (Petsiuk et al., 2018), LIME (Ribeiro et al., 2016), IG (Sundararajan et al., 2017), and NoiseTunnel (Smilkov, Thorat, Kim, Vi\u00e9gas, & Wattenberg, 2017). While our main interest is in black-box explainability, we have included methods which require access to the model gradient, such as GradientSHAP and IG. We argue that these methods, while white-box, do not require active choices from the user, unlike e.g. Grad-Cam, where an activation layer must be specified.\nThis assumes architectural knowledge from the user, and the ability to access it, which is not assumed by either GradientSHAP or IG. Both of these methods are state-of-the-art for gradient-based explainability methods.\nWe use three data sets: Imagenet1k-mini validation, Pascal VOC2012 and a \"Photobombing\" dataset we created. Imagenetlk-mini comes with labels for ground truth. VOC2012 has labels and segmentation data. We created the \u201cPhotobombing\" dataset by inserting black occlusions into imagenet images, meaning we have the original label and also know the pixel coordinates of the occlusions. We use the torchvision implementation"}, {"title": "6.3 Results", "content": "RQ5 What is the quality of explanations computed by our algorithms compared to other black-box methods?\nRQ6 Is there a trade-off between quality and compute cost of the explanations?\nRQ7 Can black-box methods achieve the same quality of explanations as white-box methods?\nImagenet images We use the ImageNet validation dataset, containing approximately 4000 images in 1000 different classifications. On this data set we calculate the total average area of the minimal sufficient explanation for each of the six tools we investigate, and normalized insertion and deletion curves. Table 1 presents the results."}, {"title": "7. Conclusions", "content": "We describe an approach to explainability of image classifiers that is based on rigorous definitions from actual causality. Exact explanations are defined as minimal subsets of pixels of the image that are sufficient for the classification. As the exact computation is"}]}