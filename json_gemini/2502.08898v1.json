{"title": "Learning in Strategic Queuing Systems with Small Buffers", "authors": ["Ariana Abel", "Jer\u00f3nimo Mart\u00edn Duque", "Yoav Kolumbus", "\u00c9va Tardos"], "abstract": "Routers in networking use simple learning algorithms to find the best way to deliver packets to their desired destination. This simple, myopic and distributed decision system makes large queuing systems simple to operate, but at the same time, the system needs more capacity than would be required if all traffic were centrally coordinated. In a recent paper, Gaitonde and Tardos (EC 2020 and JACM 2023) initiate the study of such systems, modeling them as an infinitely repeated game in which routers compete for servers and the system maintains a state (number of packets held by each queue) that results from outcomes of previous rounds. Queues get to send a packet at each step to one of the servers, and servers attempt to process only one of the arriving packets, modeling routers. However, their model assumes that servers have no buffers at all, so queues have to resend all packets that were not served successfully. They show that, in their system, even with hugely increased server capacity relative to what is needed in the centrally-coordinated case, the use of timestamps and priority for older packets is required to ensure that the system is stable, i.e., that queue lengths do not diverge with time (see Section 2 for a formal definition).\nWe consider a system with two important changes, which make the model more realistic: first we add a very small buffer to each server, allowing the server to hold on to a single packet to be served later (even if it fails to serve it); and second, we do not require timestamps or priority for older packets. Our main result is to show that when queues are learning, a small constant factor increase in server capacity, compared to what would be needed if centrally coordinating, suffices to keep the system stable, even if servers select randomly among packets arriving simultaneously.\nThis work contributes to the growing literature on the impact of selfish learning in systems with carryover effects between rounds: when outcomes in the present round affect the game in the future.", "sections": [{"title": "Introduction", "content": "In this paper, we model routers in networking as agents using simple learning algorithms to find the best way to get their packets served. This simple, myopic and distributed multi-agent decision system makes large queuing systems simple to operate, but at the same time, the system needs more capacity than would be required if all traffic were centrally coordinated. In a recent paper, Gaitonde and Tardos [2020, 2023], the authors initiated the study of such systems, modeling them as an infinitely repeated game in which routers compete for servers and the system maintains a state (number of packets held by each queue) that results from outcomes of previous rounds. Routers get to send a packet at each step at which they received a packet or have one in their queue\u00b9 to one of the servers, and each server attempts to process only one of the packets arriving to it. However, the model of Gaitonde and Tardos [2020, 2023] assumes"}, {"title": "Model", "content": "We consider a modified version of the model used by Gaitonde and Tardos [2020, 2023]. There is a system of n queues and m servers. We divide time into discrete time steps $t = 0, 1, . . . $. In every step t, the following occurs.\n1. Packets arrive at each queue i according to a fixed probability $\\lambda_i$. Formally, we model this by defining $B_i \\sim Bern(\\lambda_i)$ as an independent random variable that indicates whether a packet arrives at queue i at time t. After arrival, each queue that contains at least one packet selects a server to which it attempts to send a single packet.\n2. Each server j processes incoming packets as follows: (i) If the server's buffer is full, all received packets are rejected and returned to their respective queues. (ii) Otherwise, the server accepts one of the received packets, chosen uniformly at random, and places it in its buffer, rejecting the re-maining packets. (iii) The server then attempts to process the packet in its buffer. With probability $\\mu_j$, the processing succeeds, and the packet is removed from the buffer. If the processing fails, the packet remains in the buffer.\n3. Any packets not accepted into a server's buffer are returned to their respective queues. Each queue receives bandit feedback based on the outcome of sending its packet: a reward of zero if the packet was not accepted by the server a reward of one if the packet was successfully placed in the server's buffer.\nDefinition 1. We say that a system with a schedule for sending packets is stable if the expected number of packets waiting to be sent or served in the system is bounded by a constant at all times. We allow the constant to depend on the number of queues or servers and the parameters of the system, but it may not grow with time \u0422."}, {"title": "Analysis", "content": "Our main result is the following theorem:\nTheorem 1. Assuming $\\sum_i \\lambda_i < \\frac{1}{3} \\sum_j \\mu_j$, $\\lambda_i < \\frac{1}{2}$ for all i, and all queues use a form of learning guaranteeing no-regret with high probability to identify servers they can use, the system remains stable with the expected number of packets in the system bounded by a (time-independent) constant at all times.\nThe key tool in our analysis is the following theorem of Pemantle and Rosenthal [1999]:\nTheorem 2. Let $X_1, X_2, . . .$ be a sequence of nonnegative random variables with the property that\n1. There exist constants $\\alpha, \\beta > 0$ such that $E[X_{t+1} \u2013 X_t|F_t & X_t > \\beta] < -\\alpha$, where $F_t$ denotes the history until period t.\n2. There exist $p > 2$ and a constant $\\theta > 0$ such that for any history, $E[|X_{t+1} \u2013 X_t|^p|F_t] \u2264 \\theta$.\nThen, for any $0 < r < p \u2212 1$, there exists an absolute constant $M = M(\\alpha,\\beta,\\theta,p,r)$ not depending on t such that $E[X_t^r] \u2264 M$ for all t."}, {"title": "Experiments", "content": "In the preceding section, we derived theoretical stability conditions and worst-case bounds on the buildup of queue lengths. Specifically, our proof showed that in a system with singleton buffers, when-ever a queue grows large, given that a condition of the total service capacity is satisfied, a no-regret property of the server-selection process guarantees that it will shrink at a linear rate, keeping the total lengths of queues bounded. In this section, we complement these theoretical guarantees with computa-tional experiments to observe the typical behavior of these systems beyond the worst case, and explore the empirical impact of buffers on the dynamics.\nWe conduct simulations using the EXP3 algorithm Auer et al. [2002b] (see also Slivkins [2019]), implemented in Python. For the simulation parameters, we use \u03b3 for exploration rate and T for the time horizon of the simulation."}, {"title": "Empirical Queue Buildup", "content": "We start by examining the relationship between the system's arrival rate and its service capacity. The service capacity must strictly exceed the arrival rate to prevent queue buildup. This observation is seen also in typical experimental scenarios.\nFigure 2 illustrates the total empirical buildup, which is the sum of all packets left in queues after T iterations, normalized by n \u00b7 T (vertical axis) as a function of the ratio $\\sum_i \\lambda_i/\\sum_j \\mu_j$ (horizontal axis).\nFigure 2a focuses on a symmetric system with three servers such that $\\mu_1 = 0.8$ and $\\mu_2 = \\mu_3 = 0.2$ and three queues with equal arrival rates $\\lambda_i$, which range between 0.02 and 0.4. The simulations run for $T = 50,000$ steps with $\\gamma = 1/\\sqrt{T}$. The solid line represents the average buildup over 200 independent simulations for each value of $\\lambda_i$, and the shaded region indicates the range between minimum and maxi-mum buildup values observed in the simulations. A clear transition emerges: when $\\sum_i \\lambda_i/\\sum_j \\mu_j > 0.9$, the buildup becomes proportional to T. Crucially, this transition occurs at a point above 0.5 and below 1 (where buildup is inevitable).\nFigure 2b presents the same analysis for an ensemble of randomized systems with 5 queues and 6 servers. To generate such instances, we proceed as follows: generate 200 base parameters by selecting 5 queue arrival rates, $\\lambda_i$, and 6 server capacity rates, $\\mu_j$, uniformly at random in (0, 1). For each value of the"}, {"title": "The Impact of Buffers", "content": "We now compare systems with vs. without buffers. The methodology follows the one described in the previous subsection. Figure 3a illustrates the effect of adding buffers to a system in the random-"}, {"title": "Dynamics and Convergence", "content": "Next, we look at the dynamics of the learning agents in our systems. We observe that in all parameter configurations we have tested, the dynamics converge in last iterate (up to a small noise level due to the finite time horizon and step size in our simulations) to Nash equilibria, as illustrated in Figure 4. This is surprising, as even without the carryover effects that our games have, no-regret dynamics need not converge to Nash equilibria. Figure 4a depicts the dynamics in systems with 2 queues and 3 servers. It can be seen that the probabilities of play of the algorithms approximately converge to a stationary distribution. Note that a stationary distribution of play and the no-regret condition imply that this distri-bution is a Nash equilibrium. Figure 4b shows a dynamic in a simpler system which has two pure Nash equilibria and one mixed equilibrium. As can be seen in the instance in the figure, which is a typical one, the dynamic in this system converges to a pure equilibrium. Figure 4c depicts the total variation distance from the closest Nash equilibrium as a function of the horizon T in 200 simulations of the same system for each T. The strategies quickly converge, and so the distance of the historical average of play shrinks as well, roughly as $1/\\sqrt{T}$."}]}