{"title": "UniDemoir\u00e9: Towards Universal Image Demoir\u00e9ing with Data Generation and Synthesis", "authors": ["Zemin Yang", "Yujing Sun", "Xidong Peng", "Siu Ming Yiu", "Yuexin Ma"], "abstract": "Image demoir\u00e9ing poses one of the most formidable chal-lenges in image restoration, primarily due to the unpre-dictable and anisotropic nature of moir\u00e9 patterns. Limited bythe quantity and diversity of training data, current methodstend to overfit to a single moir\u00e9 domain, resulting in perfor-mance degradation for new domains and restricting their ro-bustness in real-world applications. In this paper, we proposea universal image demoir\u00e9ing solution, UniDemoir\u00e9, whichhas superior generalization capability. Notably, we proposeinnovative and effective data generation and synthesis meth-ods that can automatically provide vast high-quality moir\u00e9images to train a universal demoir\u00e9ing model. Our exten-sive experiments demonstrate the cutting-edge performanceand broad potential of our approach for generalized imagedemoir\u00e9ing.", "sections": [{"title": "Introduction", "content": "Digital screens have become essential devices for display-ing information in our daily work and life. However, im-ages captured from screens frequently suffer from frustrat-ing moir\u00e9 patterns, significantly degrading image qualityand hindering content extraction. Therefore, it becomes cru-cial to effectively remove such moir\u00e9 artifacts to help usersobtain high-quality images from their digital imaging de-vices and to support industries in maintaining high-standardproduct visual presentation and digital archiving. However,moir\u00e9 patterns are characterized as anisotropic and multi-scale, as well as involving considerable shape variationsand color distortions (Amidror 2009). Such traits are sel-dom seen in other types of artifacts, like noise, rain streaks,fog, blurring, etc., posing a significant challenge for even themost advanced image restoration methods (Luo et al. 2023;Zhu et al. 2023; Fei et al. 2023).\nHence, many methods have been proposed to tackle theproblem of demoir\u00e9ing in recent years (Sun, Yu, and Wang2018; Liu et al. 2020; Luo et al. 2020; He et al. 2019, 2020;"}, {"title": "Related Work", "content": "Image Restoration and Demoir\u00e9ing\nThe inherent complexity of moir\u00e9 patterns presents a uniquechallenge compared to other artifacts such as noise (Xingand Egiazarian 2021), haze (Li et al. 2021), blur (Lee et al.2021), multiple artifacts in one go (Luo et al. 2023; Zhu et al.2023; Fei et al. 2023; Zhang et al. 2023), etc. Consequently,these methods may not effectively solve the moir\u00e9 issue.Current mainstream methods for image demoreing are learn-ing based (Sun, Yu, and Wang 2018; Liu et al. 2020; Luoet al. 2020; He et al. 2019, 2020; Niu et al. 2023; Wang et al."}, {"title": "Method", "content": "Overview\nThe generalization ability of SOTA demoir\u00e9ing models isgreatly limited by the scarcity of data. Therefore, we mainlyface two challenges to obtain a universal model with im-proved generalization capability: To obtain a vast amountof 1) diverse and 2) realistic-looking moir\u00e9 data. Notice that traditional moir\u00e9 image datasets contain real data, butcontinuously expanding their size to involve more diver-sity is extremely time-consuming and impractical. Whilecurrent synthesized datasets/methods struggle to synthesizeralistic-looking moir\u00e9 images. Hence, to tackle these chal-lenges, we introduce a universal solution, UniDemoir\u00e9 (Fig-ure 1). The data diversity challenge is solved by collect-ing a more diverse moir\u00e9 pattern dataset and presenting amoir\u00e9 pattern generator to increase further pattern varia-tions. Meanwhile, the data realistic-looking challenge is un-dertaken by a moir\u00e9 image synthesis module. Finally, oursolution can produce realistic-looking moir\u00e9 images of suf-ficient diversity, substantially enhancing the zero-shot andcross-domain performance of demoir\u00e9ing models.\nMoir\u00e9 Pattern Dataset\nThe traditional demoir\u00e9ing datasets (Sun, Yu, and Wang2018; He et al. 2020; Yu et al. 2022) typically exhibit a1-1 correspondence, 1 clean image corresponds to only 1moir\u00e9-contaminated image. However, in the real world, animage may be affected by various moir\u00e9 patterns. Mean-while, aligning moir\u00e9 images with clean images often intro-duces errors because of the non-linear distortions and moir\u00e9artifacts within cameras. Therefore, we propose to collect amoir\u00e9 pattern dataset rather than a moir\u00e9 image dataset, withno need for image alignment and can easily synthesize mul-tiple moir\u00e9 counterparts of a single natural image. The col-lection of such a dataset is inspired by MoireSpace, whichis designed to address the problem of detecting the presenceof moir\u00e9 rather than to eliminate moir\u00e9 artifacts.\nCapturing Process We capture videos of real-worldmoir\u00e9 patterns on a pure white screen with a mobile phone tominimize color distortion in the moir\u00e9 patterns. After record-"}, {"title": "Moir\u00e9 Pattern Generation", "content": "Although we have collected a large scale of diverse data,it cannot encompass all conceivable moir\u00e9 patterns. In-spired by recent diffusion models, which have been suc-cessfully trained towards diverse image generation in manytasks (Dhariwal and Nichol 2021), we propose to use dif-fusion models to further sample more diverse moir\u00e9 patternsby sufficiently learning the structural, textural, and color rep-resentations of real moir\u00e9 patterns. In this stage, we proposea multi-scale cropping strategy and a colorfulness-sharpnessselection strategy to filter high-quality real data. Then welearn the distribution of real moir\u00e9 patterns in the latentspace to generate diverse patterns (Figure 2-right).\nMulti-Scale Cropping Demoir\u00e9ing models typically employ image patches cropped from the entire image for train-ing. However, given the significant variation of image size indifferent demoir\u00e9ing datasets, the scale of content in croppedimage patches of the same size also varies greatly. Hence, to"}, {"title": "Moir\u00e9 Image Synthesis", "content": "Via data collection and generation, we obtain a vast num-ber of diverse moir\u00e9 patterns. Then, we need to compositemoir\u00e9 patterns with clean images In to form moir\u00e9 images.To make the synthesized images realistic-looking, We firstcreate handcraft rules to produce initial moir\u00e9 images in theMoir\u00e9 Image Blending (MIB) module, then design a ToneRefinement Network (TRN) to further faithfully replicatethe color and brightness variations observed in real scenesthat cannot be fully formulated in those handcraft rules. Theproposed synthesis process is illustrated in Figure 4.\nMoir\u00e9 Image Blending We blend the clean natural im-age $I_n$ (background layer) with the moir\u00e9 pattern $I_{mp}$ (fore-ground layer) to form our initial moir\u00e9 image $I_{mib}$. Noticethat MoireSpace (Yang et al. 2023) synthesized their moir\u00e9image $I'_{sm}$ via a Multiply Strategy $M(\u00b7,\u00b7)$,\n$I'_{sm} = M(I_{mp}, I_n) = I_{mp}  I_n$,\nwhere $`'`$ denotes element-wise multiplication. However,the result produced by MoireSpace (Yang et al. 2023) tendsto be dark and cannot replicate the desired contrast and colordistortion, as shown in Figure 5. Therefore, we design the"}, {"title": "Tone Refinement Network", "content": "Though the moir\u00e9 imageblending module creates a preliminary moir\u00e9 image $I_{mib}$,such a synthesized result based on handcraft rules still strug-gles to replicate accurate color and brightness changes.Comparatively, networks are more powerful in capturing such unknown changes and distortion by progressive learn-ing. Hence, we present a learnable refinement network tosynthesize more realistic results."}, {"title": "Loss Functions", "content": "The tone adjustment network aims to ad-just the overall color tone and contrast of $I_{trn}$ in a way thatit resembles $I_{rm}$ without affecting moir\u00e9 pattern $I_{mp}$.\nFirst, moir\u00e9 patterns can disrupt image structures by gen-erating strip-shaped artifacts (Yu et al. 2022). Therefore,comparing two moir\u00e9 images directly in pixel space isless effective. Thus, we adopt the perceptual loss (Johnson,Alahi, and Fei-Fei 2016) $L_{per}$ to optimize the $L_1$ distancebetween the extracted content features of $I_{mib}$ and $I_{trn}$:\n$L_{per} (I_{trn}, I_{mib}) = \\sum_{j=1}^{N_L} \\frac{||\\Phi_j(I_{trn}) \u2013 \\Phi_j(I_{mib})||_1}{C_jH_jW_j}$,\nwhere $\\Phi_j(I)$ is the activations of the j-th layer of the"}, {"title": "Image Demoir\u00e9ing", "content": "Our contributions mainly lie in the above three stages. Then,diverse and realistic-looking data synthesized by our solu-tion can be seamlessly integrated with demoir\u00e9ing modelsto improve their performance."}, {"title": "Experiments", "content": "Experimental Setups\nFor all compared methods, we used their released code.Thorough implementation details are in the appendix.\nDatasets and Metrics. 1) Moir\u00e9 Pattern Dataset is usedto train our moir\u00e9 pattern generator. 2) Real Moir\u00e9 ImageDataset, TIP (Sun, Yu, and Wang 2018), FHDMi (He et al.2020), and UHDM (Yu et al. 2022), are used to demonstrateour ability in restoring real moir\u00e9 images. 3) EvaluationMetrics. We evaluate demoir\u00e9ing performance on the Peak-Signal-to-Noise Ratio (PSNR), Structural Similarity Index(SSIM) (Wang et al. 2004), and LPIPS (Zhang et al. 2018).\nComparison Methods We compare UniDemoir\u00e9 to theSOTA synthesis methods in 3 current modalities: the sim-ulation method \"Shooting\" (Niu, Guo, and Wang 2021), the"}, {"title": "Technical Appendix", "content": "This document supplements the main body of our paper withadditional details, discussions, and results. In Section A, wepresent more details of the Moir\u00e9 Pattern Dataset collection,including a brief analysis of various previously overlookedfactors affecting moir\u00e9 pattern diversity. In Section B, wewill provide a detailed explanation of the two stages in-volved in implementing UniDemoir\u00e9: Moir\u00e9 Pattern Gener-ator and Moir\u00e9 Image Synthesis. In Section C, we providemore implementation details of experiments and show morequalitative results. Furthermore, as shown in Section C.4,we performed additional ablation experiments on the blend-ing strategy in the Moir\u00e9 Image Blending (MIB) module andthe design of the upsampling block and the loss function inthe Tone Refinement Network (TRN).\nA Dataset Capture and Analysis\nIn this section, we first present a brief introduction of variouspreviously overlooked factors of devices that affect moir\u00e9pattern diversity. Then, we provide more details about ourcapture settings."}, {"title": "The Impact of Device on Moir\u00e9 Pattern Diversity", "content": "Previous studies (Yu et al. 2022; Yang et al. 2023) have indi-cated that the geometric correlation between the screen andthe camera significantly influences the features of the moir\u00e9pattern. However, such studies have overlooked that someaspects of the camera and the screen can also impact themoir\u00e9 pattern.\nFor cameras, the two most critical factors affecting themoir\u00e9 pattern are the CMOS and the lens used. The pixeldensity of a CMOS sensor (i.e., the number of pixels perunit area) determines its maximum sampling frequency, also known as the Nyquist frequency. The higher the pixel den-sity, the higher the sampling frequency of the sensor andthe higher the frequency of the signal that can be sampled,resulting in a higher frequency of moir\u00e9 produced by thealiasing effect, which impacts the moir\u00e9 pattern. In addition,the lens's focal length also affects the formation of moir\u00e9.\nIn cell phone photography, lenses with shorter focal lengths(e.g., wide lenses/main camera lenses) usually have widerangles of view and can capture more of the scene content.Lenses with longer focal lengths (such as telephoto or tele-scopic lenses), on the other hand, offer a narrower angle ofview and greater magnification for capturing distant details.\nWhen the screen is photographed with lenses of different fo-cal lengths, the relative positional relationship between thepixels on the sensor and the pixels on the screen changes,which may cause the moir\u00e9 pattern to appear or disappear.\nFurthermore, the layout and the distance of pixels dots inthe panel used can also significantly impact the formationof moir\u00e9 on the display screen. The frequency of detail thata screen can display depends on how the pixel dots are ar-ranged. Various arrangements result in distinct frequenciesof detail, which impacts the formation of moir\u00e9 patterns. Thedistance between pixel dots on the screen then affects theshooting distance. Larger pixel dot spacing will make the"}, {"title": "More Details about Capture Settings", "content": "Based on the above analysis, we take screen images throughdifferent camera viewpoints to generate diverse moir\u00e9 pat-terns. Specifically, we apply six mobile phones and six digi-tal screens, as shown in Table 5 and 6 (6\u00d76 = 36 combina-tions). Figure 7 presents a scatter plot comparison betweenour 4K Moir\u00e9 Pattern Dataset and the MoireSpace datasetin terms of texture definition and color vibrancy, where eachdata point represents an individual sample. As illustrated inthe visualization, our captured moir\u00e9 patterns demonstrate amore comprehensive coverage in both the sharpness of tex-tural details and the chromatic saturation range. This com-parative analysis is further substantiated by the represen-tative samples juxtaposed in Figure 8, where side-by-sidecomparisons visually confirm that our dataset encompassesa broader diversity of morphological textures and a widergamut of color expressions.\nMobile Phones We chose six mobile phones with varyingcamera specifications to capture diverse moir\u00e9 patterns, asshown in Table 5. Our selection criteria included the cameratype, CMOS category, and number of megapixels. For theregular main camera with moderate resolution, we pickedthe iPhone 12 and iPhone 13. For electronic zooming at 2xand 3x, we selected the Honor 90 and Xiaomi 10s, whichhave high pixels. Additionally, we picked two iPhone 12 Proand iPhone 15 Pro models with different CMOS specifically"}, {"title": "Further details of our Method", "content": "This section will showcase the details of the implementa-tion of our UniDemoir\u00e9's Moir\u00e9 Pattern Generator stage andMoir\u00e9 Image Synthesis stage."}, {"title": "Moir\u00e9 Pattern Generator", "content": "The visualization of moir\u00e9 pattern patches generated usingthe Moir\u00e9 Pattern Generator(MPG) is shown in Figure 9.\nThe implementation details of data preprocessing Thedetails of our data preprocessing method in the Moir\u00e9Pattern Generator(MPG) are described in Algorithm 1.In Multi-Scale Cropping, \u201cRandom Crop(I, w, h)\" meansto randomly crop a patch of size $w \u00d7 h$ from $I$, and\u201cResize(I, w, h)\u201d means to resize the width and height of$I$ to $w$ and $h$ directly. In Sharpness-Colorfulness selection,\u201cRGB_to_Gray($I_{mp}$)\u201d refers to convert $I_{mp}$ to grayscale im-age $G_{mp}$, while \u201cRGB_to_LAB($I_{mp}$)\u201d refers to convert $I_{mp}$to LAB space and retrieve the corresponding channel matri-ces $L_{mp}, A_{mp}$, and $B_{mp}$ respectively. Moreover, \u201cF\u2217$G_{mp}$\u201ddenotes the convolution operation on the grayscale image$G_{mp}$ using the Laplace edge detection operator F. In theactual training process of MPG, we specify set n to 3 and(w, h) to (768,768), while \u03b4s and \u03b4c are set to 15 and 2, re-spectively.\nThe implementation details of Latent Diffusion ModelWe utilize the Latent Diffusion Model(LDM) (Rombachet al. 2022) as the network component of the Moir\u00e9 PatternGenerator.\nFirstly, we have trained our autoencoder model for moir\u00e9patterns according to the method described in (Rombachet al. 2022). Specifically, given a moir\u00e9 pattern patch $I_{mp} \u2208R^{wxhx3}$ that has gone through Multi-Scale Cropping andSharpness-Colorfulness selection, we utilize an Encoder Eto convert Imp to the latent space z = E(Imp) through mul-tiple downsampling blocks. Simultaneously, we expect thecorresponding Decoder D to reconstruct the moir\u00e9 patternfrom the latent space variable z : $I_{mp}$ = D(z) = D(E($I_{mp}$))by using the same upsampling factor. Note that the overalldownsampling factor is denoted as $f = \\frac{h}{h_o} = \\frac{w}{w_o}$,where $h_o$ and $w_o$ are hyperparameters chosen to ensure thatf is precisely $2^m$, with m \u2208 N. Our loss function is a combi-nation of a perceptual loss function $L_{rec}$ (Zhang et al. 2018)and patch-based adversarial targets $L_{adv}$ (Dosovitskiy andBrox 2016; Esser, Rombach, and Ommer 2021; Yu et al.2021), along with a KL-reg regularization term $L_{reg}$ wheret"}, {"title": "Moir\u00e9 Image Synthesis", "content": "In this section, we will demonstrate the implementation de-tails of the Moir\u00e9 Image Synthesis stage that were omittedin our main paper. Additionally, we include more visualiza-tions of the synthesis results in Figure 10.\nImplementations of the Moir\u00e9 Image Blending For theMIB module, $W_m$ in Eq. (5) is randomly selected from [0.65,0.75], while $W_g = 1-W_m$. The $o_{pm}$ and $o_{pg}$ in Eq. (6) are setto 1.0 and 0.8, respectively. Performance changes resultingfrom the use of both the Multiply and Grain Merge strategyare detailed in the additional ablation study in Section C.4.\nImplementations of the Tone Refinement Network Weimplement the backbone of our Tone Refinement Net-work(TRN) using Uformer-T(Tiny) (Wang et al. 2022),where the Transformer Block uses the Locally-enhancedWindow (LeWin) Transformer block proposed by Uformerand sets the window size to 8\u00d78. At the same time, wechange the encoder depth from {2,2,2,2} to {1,1,1,1}. Per-formance changes resulting from the use of the Uformer aredetailed in the additional ablation study in Section C.4.\nIn the context of TRN, utilizing the transposed convolu-tional upsampling block similar to Uformer may lead to the"}, {"title": "Limitations", "content": "In some cases, particularly when the moir\u00e9 artifacts in thetarget domain significantly differ from those in the sourcedomain, our solution may struggle to completely remove allartifacts, as Figure 13 shows. However, even in these chal-lenging scenarios, our method tends to perform better at ar-tifact removal compared to the baselines. Our performancecan be further refined by generating more diverse moir\u00e9 pat-terns and synthesized training data. In Figure 13, we showa failure case. When the moir\u00e9 artifacts in the target domainare too different from those in the source domain, our solu-tion still struggles to produce a completely moir\u00e9-free result.However, we still remove the artifacts comparatively betterthan baselines."}]}