{"title": "TRAINING-FREE ACTIVATION SPARSITY IN LARGE LANGUAGE MODELS", "authors": ["James Liu", "Pragaash Ponnusamy", "Tianle Cai", "Han Guo", "Yoon Kim", "Ben Athiwaratkun"], "abstract": "Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL (Training-Free Activation Sparsity in LLMs), a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53\u00d7 and 1.8\u00d7 at 40% and 50% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) demonstrate that scaling in both parameter count and training data leads to capabilities that are useful for addressing a variety of downstream tasks (Brown et al., 2020). However, the large number of parameters in modern LLMs can lead to substantial challenges during inference. In typical small-batch deployment settings, autoregressive inference is memory-bound, i.e., bottlenecked by the speed at which the parameters can be moved from off-chip to on-chip memory. This is in contrast to LLM training and prefill inference, which is generally compute-bound, i.e., bottlenecked by the speed at which computation can performed. A core strategy for overcoming this memory wall (Gholami et al., 2024) is through weight quantization (Frantar et al., 2022; Shao et al., 2023; Yuan et al., 2023; Lin et al., 2024; Dettmers et al., 2023c; Tseng et al., 2024; Egiazarian et al., 2024) and sparsification (Wang et al., 2019; Frantar & Alistarh, 2023; Xia et al., 2023; Ma et al., 2023), which can lead to practical speed-ups when coupled with specialized kernels that move the weights from off-chip to on-chip memory in quantized/sparse formats (Kim et al., 2023; Dettmers et al., 2023b; Frantar et al., 2024; Wang et al., 2024b; Xia et al., 2024; Guo et al., 2024).\nThe above methods directly compress a model's weights and apply the same (quantized/sparse) matrix to all inputs. Activation sparsity (Chen et al., 2023; Raihan & Aamodt, 2020; Kurtz et al., 2020) is an alternative method which enforces input-dependent structure on the weight matrices by leveraging (or inducing) sparsity in the hidden states. Since the weight channels corresponding to zero-valued activations are not used in computation, speed-up can be realized by selectively omitting these weights during memory transfer, which is possible due to the hardware-friendly channel-wise sparsity pattern. In older LLMs, activation sparsity is largely made possible by the high natural sparsity (around 95%) in the intermediate states of the MLP blocks in ReLU-based Transformer models (Li et al., 2023). Based on this, Liu et al. (2023) propose DejaVu, which learns a small auxiliary model that predicts the contextual activation sparsity patterns of future layers, and realize a 2\u00d7 wall-clock speed-up on OPT-175B (Zhang et al., 2022). Because the hidden state is extremely sparse, the less expressive auxiliary model can afford to overestimate non-zero activations while maintaining accuracy and efficiency (e.g., 20% predicted vs. 5% actual non-zero entries)."}, {"title": "RELATED WORK", "content": "Conditional computation (Bengio, 2013; Bengio et al., 2016) alleviates the burden of training and serving by selectively activating parts of a model. Shazeer et al. (2017) propose Mixture-of-Experts (MoE) in language models, applying conditional computation to the feed forward networks. Mixture-of-Experts models decouple parameter count with computational footprint (Fedus et al., 2022), and demonstrate superior scaling laws compared to dense baselines (Clark et al., 2022).\nActivation sparsity occurs when a significant portion of a model's hidden states contain zero-valued entries, and can be seen as an instance of conditional computaton. Activation sparsity is known to naturally emerge in the intermediate states of ReLU-based MLPs (Li et al., 2023). Liu et al. (2023) leverage activation sparsity to accelerate LLM inference by avoiding the transfer of weight channels associated with zero-valued entries to GPU registers. Song et al. (2023) and Alizadeh et al. (2024) extend activation sparsity to CPU offloading, reducing weight transfer from CPU to GPU memory. However, newer architectures typically make use of non-ReLU-based MLPs (e.g., SwiGLU, Shazeer, 2020), making these off-the-shelf methods difficult to use in practice.\nRecent work has thus focused on reintroducing activation sparsity in newer architectures. Mirzadeh et al. (2023) replace SiLU or GeLU activation functions with ReLU, followed by continued pretraining on hundreds of billions of tokens. Zhang et al. (2024b) experiment with different activations and find Squared ReLU (So et al., 2022) to be the most effective replacement. Song et al. (2024b) and Song et al. (2024a) introduce techniques such as activation regularization to push sparsity even higher in adapted models. Wang et al. (2024a) combine magnitude pruning with Squared ReLU and quantized activations, and establish scaling laws for sparsely activated LLMs during pretraining.\nLee et al. (2024) propose CATS, and realize training-free activation sparsity on SwiGLU based LLMs by applying magnitude pruning on the output of $W_{gate}$, with the intuition that in the training-free setting, ReLU-based methods suboptimally zero out nontrivial negative values but keep positive values with lower magnitude intact. They achieve up to 50% sparsity in $W_{up}$ and $W_{down}$ for Mistral and Llama-2-7B. However, other matrices including $W_{gate}$ and $W_{q,k,v,o}$ are computed densely, resulting in lower model-wide sparsity (roughly 25%), whereas we target every matrix in the model."}, {"title": "BACKGROUND: ACTIVATION SPARSITY IN NEURAL NETWORKS", "content": "The activation sparsity of a hidden state $x$ is defined as the proportion of zero-valued entries, which can interact with the model in two ways. The first is input sparsity: when computing $y = xW^T$ for"}, {"title": "TEAL: TRAINING-FREE ACTIVATION SPARSITY IN LLMS", "content": "This work describes TEAL (Training-Free Activation Sparsity in LLMs), a simple, training-free approach that applies activation sparsity based on magnitude pruning, where the threshold is layer-dependent. TEAL is based on the observation that distributional shapes in LLaMA-architecture LLMs are zero-mean unimodal. By pruning low-magnitude, non-salient activations, we achieve 40-50% model-wide (input-dependent) sparsity, in contrast to prior work which only achieves sparsity in portions of the model (Lee et al., 2024). We realize wall-clock speed-ups of up to 1.53\u00d7 and 1.8\u00d7 at 40% and 50% sparsity respectively through specialized kernels, and further demonstrate compatibility with weight quantization."}, {"title": "MOTIVATING STUDY: DISTRIBUTIONAL PROPERTIES OF ACTIVATIONS IN LLMS", "content": "We perform a preliminary study of the distributional properties of activations of LLMs. We collect activations of Llama-3-8B (Dubey et al., 2024) sampled from C4 (Dodge et al., 2021) at the four hidden states in a Transformer block,\u00b9 and visualize them in Figure 2. As indicated by prior work, some of the activations are heavy-tailed and contain outliers (Dettmers et al., 2022; Xiao et al., 2022; Wei et al., 2022; Nrusimha et al., 2024). The hidden states are moreover zero-mean unimodal, and qualitatively fall into two distinctly shaped distributions. The hidden states before the Attention and the MLP layers tend to be Gaussian-like, while the hidden states in the intermediate of such layers tend to be Laplacian-like. The concentration of the activations around zero motivates our magnitude-based activation pruning approach.\nRemark. We do not attempt to explain why these distributions are shaped the way they are, nor do we give the theoretical underpinnings of why activation sparsity works. However, we make a few general observations. LLM weights are typically Gaussian (Dettmers et al., 2023a), and multiplying an independent isotropic Gaussian vector with an independent Gaussian matrix follows a multi-variate generalized Laplace distribution Mattei (2017) (the weights and activations are clearly not independent in practice). Attention is a data-dependent linear operator (Poli et al., 2023) which may have similar properties. Distributions may be zero-mean due to layer normalization (Ba et al., 2016)."}, {"title": "TEAL", "content": "The above analysis motivates our simple approach for activation sparsity based on magnitude pruning. While small-magnitude activations could still have a large effect on the output if the norms of corresponding channels of the weight matrix are large, we find that magnitude-based pruning is empirically effective. We first define a sparsification function for an activation vector as follows:\nDefinition 1. For a random vector $x = (x_1,...,x_n)$ and sparsity level $p \\in [0,1]$, define the threshold $t_p$ as\n$\\frac{1}{n} \\sum_{i=1}^{n} P(x_i < t_p) = p$.\nThe sparsification function $s_{t_p}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ is defined as:\n$s_{t_p}(X) = (s_{t_p}(x_1),..., s_{t_p}(x_n))$\nwhere for each component:\n$s_{t_p}(x_i) = \\begin{cases}\n0 & \\text{if } x_i \\leq t_p \\\\\nx_i & \\text{otherwise}\n\\end{cases}$\nIn practice we estimate $t_p$ using an empirical distribution constructed offline using activations from generic text. The sparsity level $p$ is characterized entirely by threshold $t_p$, which is useful in both implementation and kernel design (Section 4.4).\nLet $W$ be the set of matrices in the MLP and Attention blocks of a model, and further let $N = |W|$. We define a model-level sparsification configuration as $p = (p_1,...,p_N)$, where each $p_i \\in [0, 1]$ represents the sparsity level for the corresponding matrix $W_i$. For each matrix $W_i \\in W$, we define its layer-level sparsified forward pass as:\n$\\hat{Y} = s_{t_p}(x)W$\nfor input $x$ and magnitude-based sparsification function $s_{t_p}(\\cdot)$ as defined in Definition 1. We apply this sparsified forward pass to all $N$ matrices to obtain the model-level sparsified forward pass."}, {"title": "BLOCK-WISE GREEDY OPTIMIZATION", "content": "How should we find the optimal $p$? We initially tried a gradient-based approach to learning the thresholds based on the straight through estimator (Bengio et al., 2013), but encountered optimization issues. We instead used a simple greedy approach illustrated in Algorithm 1, which was found to be effective.\nFor each Transformer block, we seek to minimize the block-wise $l_2$ activation error subject to a block-level sparsity constraint. Each Transformer block consists of seven matrices: $W_q, W_k, W_v, W_o, W_{gate}, W_{up}, W_{down}$. Algorithm 1 initializes the sparsity levels of all layers to zero, and attempts to increment the sparsity level of each layer by an amount inversely proportional to its memory footprint. The layer with the lowest $l_2$ activation error is incremented, and the block-level sparsity plus associated layer-level sparsities are recorded. We share the block-level sparsity levels across all Transformer blocks; therefore, all blocks have the same target sparsity level, but the individual layer-level sparsities could be different across different blocks."}, {"title": "HARDWARE AWARE ACCELERATION", "content": "We develop a specialized sparse GEMV kernel to achieve practical speed-ups, building on the Triton-based (Tillet et al., 2019) kernel introduced by DejaVu (Liu et al., 2023). This kernel takes in an input x, boolean sparsity mask s and matrix W, and returns $(xs)W^T$. Wall-clock speed-up is realized in three ways: (1) W is stored in column major format for optimal memory coalescing; (2) Columns $W_{:,i}$ are selectively loaded based on the truth value of $s_i$; (3) SplitK work decomposition is used, enabling finer-grained parallelism across thread blocks, combining partial results through atomic adds.\nOur kernel makes the following improvements on top of the original kernel: (1) We fuse the mask creation process, as $s = x[|x| > t_p]$ is entirely characterized by x and $t_p$ in TEAL; (2) We accumulate along the outer SplitK dimension in FP16 (keeping the inner in-register accumulation in FP32), as writing to global memory in FP32 results in significant traffic; (3) We specify an eviction policy in NVIDIA PTX, prioritizing cache retention for activations which are reused across multiple thread blocks, and deprioritizing weights which are block-specific. This guarantees that activations are persistent in L2 cache."}, {"title": "RESULTS", "content": "Models and Datasets. We evaluate TEAL on the Mistral (Jiang et al., 2023), Llama-2 (Touvron et al., 2023), and Llama-3 (Dubey et al., 2024) families. We measure the performance of sparsified models on language modeling using the WikiText (Merity et al., 2016) validation set, and on an aggregate of six downstream tasks using the EleutherAI LM Harness (Gao et al., 2023), including 5-shot MMLU, 25-shot ARC challenge, 10-shot HellaSwag, 5-shot GSM8K, zero-shot PiQA, and zero-shot Winogrande (Hendrycks et al., 2021; Clark et al., 2018; Zellers et al., 2019; Cobbe et al., 2021; Bisk et al., 2019; Sakaguchi et al., 2019). For language modeling, we evaluate all models on the same 128 random samples, using a 2048-token context and 512-token evaluation window.\nBaselines. We use the block-wise greedily optimized sparsities from Section 4.3 for TEAL, and primarily compare to CATS (Lee et al., 2024) and ReLUfication (Mirzadeh et al., 2023) in their training-free configurations with no finetuning. We report model-level sparsities for all methods."}, {"title": "ACCURACY", "content": "Main Results. TEAL is performant, as shown in Tables 1 and 2, showcasing near zero degradation at 25%, and minimal degradation at 40% sparsity. At 50% sparsity, Llama-3 variants show slightly more degradation compared to older Llama-2 and Mistral variants which are still fairly performant. This falls in line with prior work showing that quantization techniques are less effective on newer models trained on more tokens (Huang et al., 2024). Most models degrade significantly at 65% sparsity, with the exception of Llama-2-70B which is still reasonably performant. In terms of downstream task results, both of the 70B models are more sparsifiable than their smaller counterparts."}, {"title": "END-TO-END DECODING SPEED-UP", "content": "We benchmark TEAL's end-to-end single-batch decoding latency by integrating it with GPT-Fast (PyTorch, 2024). We enable CUDA graphs and torch.compile. Tests use Llama-2 (7B, 13B) and Llama-3 (8B, 70B) models at 0%, 25%, 40%, and 50% uniform sparsities. Our GPU power limit settings are 500W and 300W for A100 and A6000 respectively. As shown in Table 3, TEAL achieves significant speed-ups of up to 1.53\u00d7 and 1.8\u00d7 at 40% and 50% sparsity respectively. TEAL is slower than the baseline at 0% sparsity on A100 due to torch.compile strengthening the torch.matmul baseline. This suggests further room for optimization of our kernel. We find lower speedups for Llama-3-8B compared to Llama-2-7B partially due to its larger LM Head, which we do not currently sparsify."}, {"title": "COMPATIBILITY WITH QUANTIZATION", "content": "We demonstrate compatibility with quantization, which is another promising direction for efficient LLM inference. We consider 8-bit channel-wise RTN, 4-bit AWQ (Lin et al., 2024), and 2/3-bit QuIP# (Tseng et al., 2024), and plot the perplexity of Llama-2-7B on WikiText in Figure 4. The point of sharp perplexity degradation is similar across bitwidths, suggesting that errors from activation sparsity and weight quantization compound somewhat independently. Combining activation sparsity with weight quantization unlocks new regimes with respect to memory transferred to GPU registers, allowing for higher inference speed-up. This requires developing specialized sparse + quantized kernels, which we leave to future work."}, {"title": "ANALYSIS", "content": "The above analysis motivates our simple approach for activation sparsity based on magnitude pruning. While small-magnitude activations could still have a large effect on the output if the norms of corresponding channels of the weight matrix are large, we find that magnitude-based pruning is empirically effective. We first define a sparsification function for an activation vector as follows."}, {"title": "SHOULD WUP HAVE INPUT OR OUTPUT SPARSITY?", "content": "TEAL naturally differs from CATS in its treatment of $W_{up}$. TEAL uses input sparsity, whereas CATS uses output sparsity with output mask $s = s_{t_p}(SiLU(xW_{gate}))$, with the intuition that SiLU serves as a gating mechanism. We must choose one treatment over the other due to differing memory format constraints (see Section 3)."}, {"title": "BLOCK-WISE GREEDY SPARSITIES", "content": "We observe in Figure 6 that the block-level greedy method in Section 4.3 outperforms the uniform configuration across all sparsity levels. The resultant sparsities can be used to analyze the workings of modern LLMs. We make two interesting observations about Llama-3-70B, which tend to hold for the other models we analyze."}, {"title": "PREFILL SPARSIFICATION", "content": "We vary the proportion of prefill sparsified (along the sequence length dimension) in Figure 9. Sparsifying the second half of prefill is nearly identical to sparsifying 99% of prefill (all tokens besides the initial tokens). However, more severe degradation occurs when sparsifying the initial tokens. This is due to attention sinks (Xiao et al., 2024), a phenomenon in LLMs where initial tokens are allocated an outsized amount of attention due to the softmax operation. Degradation to keys and values of initial \u201cattention sink\u201d tokens results in more substantial model degradation due to their greater importance (Hooper et al., 2024).\nTEAL is a decoding solution so this is typically not an issue, but care must be taken when sparsifying prefill for evaluation on log-likelihood based tasks."}, {"title": "BATCHED SPARSIFICATION", "content": "We focus on the single-batch case, but it may be valuable to study activation sparsity in batched settings. The key challenge is that different inputs may prefer different sparsity patterns. We need to find a subset of weight columns associated with activations that are relatively low-magnitude for the entire batch.\nWe propose to sparsify based on the average magnitude of activations across the batch dimension, a natural extension from the single batch case. The resultant sparsification criterion is batch dependent, but is still entirely characterized by a threshold."}, {"title": "APPLICATIONS AND LIMITATIONS", "content": "Applications. The most immediate application of TEAL is accelerating inference in resource constrained edge settings. These settings are typically single-batch, which is where TEAL realizes the most salient speed-up. Furthermore, TEAL is compatible with quantization (Section 5.3), which is another essential axis of efficiency in this setting.\nLimitations. TEAL exhibits substantial sparsity in the low-batch setting (Section 5.4.4) but does not scale as well to higher batch sizes, which is a limitation of most activation sparsity work\u00b2. A way to alleviate this is to push sparsities higher through continued pretraining. While TEAL focuses on the training-free case, we provide many learnings that can aid future work in sparse aware adaptation.\nA setting where batched inference is less difficult is in the low-batch setting of Mixture of Experts (Shazeer et al., 2017) based models, as the baseline itself does not scale well due to having to activate more experts and lowering the arithmetic intensity."}, {"title": "CONCLUSION", "content": "We propose TEAL, a simple method that applies magnitude-based activation sparsity to modern LLMs without training, achieving 40-50% model-wide sparsity with minimal degradation. We additionally optimize per-layer sparsity levels, improve existing sparse kernels, and demonstrate compatibility with quantization. We achieve wall-clock speed-ups in single-batch decoding, which is crucial in resource-constrained edge settings. We hope TEAL has impact in real-world applications and enhances our understanding of activation sparsity in LLMs."}, {"title": "APPENDIX", "content": ""}, {"title": "DERIVATION OF SPARSIFICATION ERROR", "content": "We derive the error of magnitude-based activation sparsity for the case where $W$ and $X$ are independent Gaussian in Theorem A.1. Our error metric is $\\frac{E_X [||\\hat{Y} - Y||_2]}{E_X[||Y||_2]}$, where $X$ is the input, $\\hat{Y}$ is the predicted output and $Y$ is the ground truth output. We plot this error in Figure 10, along with empirical errors on $W_{up,down}$ in Block 16 of Llama-3-8B, and the theoretical error obtained from random sparsification."}]}