{"title": "Theoretical Analysis of Privacy Leakage in Trustworthy Federated Learning: A Perspective from Linear Algebra and Optimization Theory", "authors": ["Xiaojin Zhang", "Wei Chen"], "abstract": "Federated learning has emerged as a promising paradigm for collaborative model training while preserving data privacy. However, recent studies have shown that it is vulnerable to various privacy attacks, such as data reconstruction attacks. In this paper, we provide a theoretical analysis of privacy leakage in federated learning from two perspectives: linear algebra and optimization theory. From the linear algebra perspective, we prove that when the Jacobian matrix of the batch data is not full rank, there exist different batches of data that produce the same model update, thereby ensuring a level of privacy. We derive a sufficient condition on the batch size to prevent data reconstruction attacks. From the optimization theory perspective, we establish an upper bound on the privacy leakage in terms of the batch size, the distortion extent, and several other factors. Our analysis provides insights into the relationship between privacy leakage and various aspects of federated learning, offering a theoretical foundation for designing privacy-preserving federated learning algorithms.", "sections": [{"title": "1. Introduction", "content": "Federated learning (McMahan et al., 2017; Yang et al., 2019) has gained significant attention in recent years as a distributed machine learning paradigm that enables multiple parties to collaboratively train a model without sharing their raw data. By keeping the data locally and only exchanging model updates, federated learning mitigates privacy concerns and complies with data protection regulations. It has found applications in various domains, such as mobile computing (Hard et al., 2018), healthcare (Antunes et al., 2022), and finance (Long et al., 2020).\nDespite the promise of federated learning in protecting data privacy, recent studies have revealed that it is still susceptible to privacy attacks. Adversaries can exploit the shared model updates to infer sensitive information about the participants' private data. One notable class of attacks is data reconstruction attacks (Zhu et al., 2019; Geiping et al., 2020; Yin et al., 2021), which aim to recover the original training data from the gradients or model updates. These attacks pose a severe threat to the privacy of federated learning participants and undermine the trust in the system.\nTo better understand and mitigate privacy risks in federated learning, it is crucial to conduct a rigorous theoretical analysis of privacy leakage. Previous works have investigated the privacy guarantees of federated learning from different perspectives, such as differential privacy (Dwork et al., 2006; Abadi et al., 2016) and information theory (Wang et al., 2019). However, the theoretical understanding of privacy leakage in federated learning remains limited, especially in terms of the impact of the specific characteristics of federated learning in the local training process, such as the number of local data samples, the number of local epochs, and the batch size.\nIn this paper, we aim to fill this gap by providing a theoretical analysis of privacy leakage in federated learning from two complementary perspectives: linear algebra and optimization theory. From the linear algebra perspective, we formulate the local training process as an optimization problem and examine the uniqueness of its solution. We prove that when the Jacobian matrix of the batch data is not full rank, there exist different batches of data that produce the same model update, thereby ensuring a level of privacy. We further derive a sufficient condition on the batch size to prevent data reconstruction attacks. From the optimization theory perspective, we measure the privacy leakage using the discrepancy between the reconstructed data and the original data, and establish an upper bound on the privacy leakage in terms of the batch size, the distortion extent, and several other factors. Our analysis provides insights into the relationship between privacy leakage and various aspects of federated learning, such as the number of local data samples, the number of local epochs, and the batch size. Our main contributions are as follows:\n\u2022 We formulate the local training process in federated learning as an optimization problem and analyze the uniqueness of its solution from"}, {"title": "2. Related Work", "content": "In this section, we review the related work on federated learning, privacy attacks, and defense mechanisms."}, {"title": "2.1. Federated Learning", "content": "Federated learning has attracted significant attention in recent years as a promising approach for collaboratively training machine learning models while preserving data privacy. The concept of federated learning was first proposed by McMahan et al. (2017), who introduced the FedAvg algorithm for aggregating local models. Since then, various extensions and improvements have been proposed, such as FedProx (Li et al., 2020), and FedNova (Wang et al., 2020). Furthermore, the concept of trustworthy federated learning has emerged, focusing on ensuring high utility, fairness, robustness, and efficiency, while preserving privacy. Researchers have explored the trade-offs between these factors and proposed various techniques to achieve a balance Girgis et al. (2021); Zhang et al. (2022); Mitchell et al. (2022); Zhang et al. (2023a,d,e,b); He et al. (2024); Zhang et al. (2024a). Federated learning represents a promising direction for collaborative model training that can benefit a wide range of applications, from healthcare to finance, while addressing the growing concerns around data privacy and security."}, {"title": "2.2. Privacy Attacks in Federated Learning", "content": "Despite the privacy-preserving nature of federated learning, it has been shown that the shared model updates can still leak sensitive information about the participants' private data. Zhu et al. (2019) proposed the Deep Leakage from Gradients (DLG) attack, which reconstructs the training data from the gradients by solving an optimization problem. Geiping et al. (2020) developed a cosine similarity-based attack called Inverting Gradients (IG) that achieves better reconstruction quality. Yin et al. (2021) introduced the Recursive Gradient Inversion (RGI) attack, which recursively reconstructs the private data from the model updates in multiple rounds. Other notable privacy attacks in federated learning include membership inference attacks (Nasr et al., 2019), property inference attacks (Melis et al., 2019), and model inversion attacks (Fredrikson et al., 2015)."}, {"title": "2.3. Defense Mechanisms", "content": "To mitigate privacy risks in federated learning, various defense mechanisms have been proposed. Differential privacy (Dwork et al., 2006) is a well-established framework for protecting individual privacy by adding noise to the shared information. Abadi et al. (2016) and McMahan et al. (2017) applied differential privacy to federated learning by perturbing the gradients before aggregation. Secure aggregation (Bonawitz et al., 2016) is another approach that uses cryptographic techniques to ensure that the server can only see the aggregated model update without learning individual updates. Gradient compression (Haddadpour et al., 2021; Albasyoni et al., 2020) reduces the communication overhead and protects privacy by compressing the gradients before transmission. Other defense mechanisms include participant-level differential privacy (Geyer et al., 2017), model perturbation (Wu et al., 2020), and gradient sparsification (Han et al., 2020)."}, {"title": "3. Preliminaries", "content": "In this section, we introduce the basic concepts and notations used throughout the paper."}, {"title": "3.1. Federated Learning", "content": "Federated learning (McMahan et al., 2017; Yang et al., 2019) is a distributed machine learning paradigm that enables multiple parties to collaboratively train a model without sharing their raw data. In this paper, we focus on horizontal federated learning (HFL), where the participants have different data samples but share the same feature space and is the most widely applied federated learning setting in real-world applications.\nConsider a horizontal federated learning system with K clients, where each client k has a private dataset $D^k = \\{(x_i^k, y_i^k)\\} = \\{(x_1^k, y_1^k), \\dots, (x_{n_k}^k, y_{n_k}^k)\\}$, and $n_k = |D^k|$. In HFL, K participants collaboratively optimize a global model with parameter $\\theta$ by minimizing clients' local losses. The goal is to minimize the global objective function:\n$\\min \\ell(\\theta) \\triangleq \\frac{1}{K} \\sum_{k=1}^K \\ell^k(\\theta),$ (1)\nwhere $\\ell^k(\\theta) = \\frac{1}{n_k} \\sum_{i=1}^{n_k} \\ell(\\theta, x_i^k, y_i^k)$ is the local objective function of client k, and $\\ell(\\cdot)$ is the loss function.\nThe widely adopted algorithm for solving the above optimization problem is Federated Averaging (FedAvg) (McMahan et al., 2017). In each communication round t, the server sends the current global model $\\theta_t$ to all clients. Each client k then performs E local epochs of training on its local dataset $D^k$ to update the model parameters to $\\theta_t^k$. The local model update $\\Delta \\theta_t^k = \\theta_t^k - \\theta_t$ is sent back to the server, which aggregates the updates to obtain the new global model:\n$\\theta_{t+1} = \\theta_t + \\frac{1}{K} \\sum_{k=1}^K \\Delta \\theta_t^k.$ (2)"}, {"title": "3.2. Privacy Attacks in Federated Learning", "content": "Despite the promise of federated learning in protecting data privacy, recent studies have shown that it is vulnerable to various privacy attacks. In this paper, we focus on data reconstruction attacks (Zhu et al., 2019; Geiping et al., 2020; Yin et al., 2021), which aim to recover the original training data from the shared model updates. We consider a semi-honest adversary who follows the protocol faithfully but tries to infer sensitive information from the received messages. The adversary's goal is to reconstruct the private data of a target client k by solving the following optimization problem:\n$\\min_{\\hat{\\mathbf{x}}, \\hat{\\mathbf{y}}} \\operatorname{DIST}\\left(\\operatorname{GRAD}\\left(\\theta_{t}, \\hat{\\mathbf{x}}, \\hat{\\mathbf{y}}\\right), \\Delta \\theta_{t}^{k}\\right),$ (3)\nwhere $\\hat{\\mathbf{x}}$ and $\\hat{\\mathbf{y}}$ are the reconstructed data, $\\operatorname{DIST}(\\cdot)$ is a distance metric, and $\\operatorname{GRAD}(\\cdot)$ simulates the local training process to approximate the real model update $\\Delta \\theta_{t}^{k}$."}, {"title": "4. Theoretical Analysis from the Perspective of Linear Algebra", "content": "In this section, we analyze the privacy of federated learning from the perspective of linear algebra. By formulating the local training process as an optimization problem and examining the uniqueness of its solution, we establish a theoretical basis for understanding how the model update relates to the identifiability of private data. Central to our analysis is the Jacobian matrix, which captures the sensitivity of the model update to changes in the input data. We prove that when the Jacobian matrix is not full rank, there exist different batches of data that produce the same model update, thereby ensuring a level of privacy. Building upon this result, we derive a sufficient condition on the batch size to prevent data reconstruction attacks. This linear algebra based approach provides a rigorous and quantitative framework to reason about privacy in federated learning.\nTo solve the optimization problem of Eq.(1). federated learning (McMahan et al., 2017) involves T iterations (i.e., communication rounds) of training procedure between the server and K clients (illustrated in Algorithm 1). In each iteration t, the server sends the current version of the global model $\\theta_t$ to all clients. Each client k then computes multiple updates on its local model based on its private data and sends an updated version of the local model $\\theta_{t+1}^k$ back to the server (see Algorithm 2), which in turn aggregates the local models of all clients to form the next version of the global model. For simplicity of algorithm description, we assume the same learning rate $\\eta$, batch size B, and number of training epochs E are used for all clients, even though in practice these hyperparameters may be different across clients to account for their varying data distributions and computational capabilities."}, {"title": "4. Theoretical Analysis from the Perspective of Linear Algebra", "content": "model:\n$\\theta_{t+1} = \\theta_{t} + \\frac{1}{K} \\sum_{k=1}^{K} \\Delta \\theta_{t}^{k}.$\nThe local training process of client k can be formulated as:\n$\\begin{aligned} &\\min_{\\left\\{\\theta_{t, e}^{k}, \\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}} \\frac{1}{B} \\sum_{b=1}^{B} \\ell\\left(\\theta_{t, e}^{k}, \\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right) \\\\ &\\text { s.t. } \\sum_{e=0}^{E-1} \\nabla\\ell\\left(\\theta_{t, e}^{k}, \\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right) = \\Delta \\theta_{t}^{k}, \\\\ &\\theta_{t, e+1}^{k}=\\theta_{t, e}^{k}-\\eta \\frac{1}{B} \\sum_{b=1}^{B} \\nabla\\ell\\left(\\theta_{t, e}^{k}, \\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right), e=0, \\ldots, E-1 \\\\ &\\theta_{t, 0}^{k}=\\theta_{t} \\end{aligned}$  (4)\nwhere $\\theta_{t,e}^k$ represents the local model parameters after the e-th epoch in round t, and $\\eta$ is the learning rate.\nIn the above problem, the private data $(\\mathbf{x}^{k}, \\mathbf{y}^{k})$ is the optimization variable, and the constraint requires that the model update resulting from the data after E local epochs should be equal to the observed $\\Delta \\theta_{t}^{k}$. If there are multiple solutions to the constraint, i.e., there exist different $(\\mathbf{x}^{k}, \\mathbf{y}^{k})$ such that the constraint holds, then the private data cannot be uniquely determined from $\\Delta \\theta_{t}^{k}$. Then, we can get the following theorem.\nTheorem 4.1. Let d be the model parameter dimension and p be the dimension of a single data point. Consider a batch of data $\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\}_{b=1}^{B}$ with $\\mathbf{x} \\in \\mathbb{R}^{p}$ and $\\mathbf{y} \\in \\mathbb{R}$. Let $\\Delta \\theta(\\left\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B})$ represents the model update obtained after E local epochs on this batch of data. If the rank of the Jacobian matrix $J \\in \\mathbb{R}^{d \\times B p}$ of the batch data is $\\operatorname{rank}(J) < B p$, then for any $\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\}_{b=1}^{B} \\neq {\\bf{0}}$, there exists $\\{\\delta \\mathbf{x}_{b}^{k}\\}_{b=1}^{B} \\neq {\\bf{0}}$ such that,\n$\\Delta \\theta(\\left\\{\\mathbf{x}_{b}^{k}+\\delta \\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}) = \\Delta \\theta(\\left\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}).$\nThis means that the private batch data $\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\}_{b=1}^{B}$ cannot be uniquely determined from the model update $\\Delta \\theta$.\nProof. For any batch of data $\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\}_{b=1}^{B}$, consider its perturbation $\\{\\mathbf{x}_{b}^{k}+\\delta \\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\}_{b=1}^{B}$. For a multivariate function $f(x)$, its first-order Taylor expansion around $x_0$ can be written as:"}, {"title": "4. Theoretical Analysis from the Perspective of Linear Algebra", "content": "$f(x) \\approx f(x_0) + \\nabla f(x_0)^{\\top} (x - x_0)$,\nwhere $\\nabla f(x_0)$ is the gradient vector of function f at point $x_0$.\nThe model update $\\Delta \\theta^k$ can be seen as a function of the batch data $\\{\\mathbf{x}_b^k, \\mathbf{y}_b^k\\}_{b=1}^{B}$. We wish to estimate the impact of changes in batch data $\\{\\delta \\mathbf{x}_b^k\\}_{b=1}^{B}$ on the model update.\nWhen the training sample $\\mathbf{x}_b^k$ undergoes a small perturbation $\\delta \\mathbf{x}_b^k$, the change in the objective function can be approximated by a first-order Taylor expansion:\n$\\begin{aligned} \\Delta \\theta\\left(\\left\\{\\mathbf{x}_{b}^{k}+\\delta \\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}\\right) &\\approx \\Delta \\theta\\left(\\left\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}\\right) + \\left(\\nabla_{\\mathbf{x}}\\left(\\left\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}\\right)\\right)^{\\top} \\left\\{\\delta \\mathbf{x}_{b}^{k}\\right\\}_{b=1}^{B} \\\\ &=\\Delta \\theta\\left(\\left\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}\\right) + \\sum_{b=1}^{B} \\frac{\\partial \\Delta \\theta^{k}}{\\partial \\mathbf{x}_{b}^{k}} \\delta \\mathbf{x}_{b}^{k} \\\\ &=\\Delta \\theta\\left(\\left\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}\\right) + J \\delta \\mathbf{x}^{k}, \\end{aligned}$\nwhere $\\frac{\\partial \\Delta \\theta^{k}}{\\partial \\mathbf{x}_{b}^{k}} \\in \\mathbb{R}^{d \\times p}$ represents the Jacobian matrix block of the model update $\\Delta \\theta$ with respect to the b-th data point $\\mathbf{x}_b^k$, d is the dimension of the model parameters, and p is the dimension of the input, $J = \\left[\\frac{\\partial \\Delta \\theta^{k}}{\\partial \\mathbf{x}_{1}^{k}} \\ldots \\frac{\\partial \\Delta \\theta^{k}}{\\partial \\mathbf{x}_{B}^{k}}\\right] \\in \\mathbb{R}^{d \\times B p}$ $\\mathbb{R}^{d \\times B p}$ is the Jacobian matrix of $\\Delta \\theta$ with respect to $\\{\\mathbf{x}_{b}^{k}\\}_{b=1}^{B}$, with each row corresponding to an element in $\\Delta \\theta^k$ and each column corresponding to a\n$\\delta \\mathbf{x}_b^k$\nfeature in $\\{\\mathbf{x}_b^k\\}_{b=1}^{B}$, and $\\delta \\mathbf{x}^k =$$\\vdots$$\\delta \\mathbf{x}_B^k$$\\in \\mathbb{R}^{B p}$ is the perturbation vector of all training sample inputs.\nThe first-order Taylor expansion result tells us that when the training sample input undergoes a small perturbation, the change in the objective function can be approximated by a linear combination of the original objective function value and the Jacobian matrix J. Intuitively, the first-order Taylor approximation tells us that when the change in batch data $\\{\\delta \\mathbf{x}_b^k\\}_{b=1}^{B}$ is small, the change in the model update amount can be calculated by the product of the Jacobian matrix J and the batch data change vector $\\delta \\mathbf{x}^k$.\nSince the rank of the Jacobian matrix J is $\\operatorname{rank}(J) < B p$, where B is the number of training samples and p is the feature dimension of each sample, this means that the matrix J is not full rank. In other words, its column"}, {"title": "4. Theoretical Analysis from the Perspective of Linear Algebra", "content": "vectors are not linearly independent, and there are some linearly dependent parts. This result implies that the null space of matrix J, Ker(J), must contain non-zero vectors. The null space Ker(J) is defined as:\n$\\operatorname{Ker}(J)=\\{\\delta \\mathbf{x} \\in \\mathbb{R}^{B p}: J \\delta \\mathbf{x}=\\mathbf{0}\\}$\nThis means that the null space Ker(J) contains all non-zero vectors $\\delta \\mathbf{x}$ that satisfy $J \\delta \\mathbf{x}=\\mathbf{0}$. These vectors represent input perturbations $\\delta \\mathbf{x}$ that do not change the value of the objective function $\\Delta \\theta$.\nIntuitively, since J is not full rank, there are some input perturbations $\\delta \\mathbf{x}$ that do not affect the objective function. The null space of the Jacobian matrix corresponds to those directions of batch data perturbation that do not affect the model update.\nLet $\\delta \\mathbf{x}^{k^*}$ be any non-zero vector, then $J \\delta \\mathbf{x}^{k^*} = 0$. Therefore:\n$\\begin{aligned} \\Delta \\theta\\left(\\left\\{\\mathbf{x}_{b}^{k}+\\delta \\mathbf{x}_b^{k^*}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}\\right) &\\approx \\Delta \\theta\\left(\\left\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}\\right) + J \\delta \\mathbf{x}^{k^*} \\\\ & = \\Delta \\theta\\left(\\left\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}\\right) \\end{aligned}$\nThis indicates that the perturbed batch data $\\{\\mathbf{x}_{b}^{k}+\\delta \\mathbf{x}_b^{k^*}, \\mathbf{y}_{b}^{k}\\}_{b=1}^{B}$ and the original batch data $\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\}_{b=1}^{B}$ will yield almost the same model update after the same training process.\nThis means that there are non-zero $\\{\\delta \\mathbf{x}_b^{k^*}\\}_{b=1}^{B}$ such that the model update $\\Delta \\theta$ remains unchanged, i.e.:\n$\\Delta \\theta(\\left\\{\\mathbf{x}_{b}^{k}+\\delta \\mathbf{x}_b^{k^*}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B}) = \\Delta \\theta(\\left\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\right\\}_{b=1}^{B})$\nSince $\\delta \\mathbf{x}^{k^*} \\neq 0$, the perturbed batch data is different from the original batch data, but they correspond to the same model update. This means that the model update $\\Delta \\theta^k$ cannot uniquely determine the private batch data $\\{\\mathbf{x}_b^k, \\mathbf{y}_b^k\\}_{b=1}^{B}$, as there are different batches of data that can produce the same $\\Delta \\theta$.\nTherefore, under the condition $\\operatorname{rank}(J) < B p$, the private batch data has a certain level of non-identifiability, ensuring privacy protection.\nIntuitively, Theorem 4.1 reveals the relationship between the rank of the Jacobian matrix and the uniqueness of the solution to the batch data identification problem. When small changes in batch data cause redundant degrees"}, {"title": "4. Theoretical Analysis from the Perspective of Linear Algebra", "content": "of freedom in the changes of the model update, different batches of data may correspond to the same model update, making the batch data indeterminable. By fully utilizing this, batch data privacy can be ensured by limiting the rank of the Jacobian matrix.\nSince\n$\\Delta \\theta_{t}^{k}=-\\frac{\\eta}{B} \\sum_{e=0}^{E-1} \\sum_{b=1}^{B} \\nabla \\ell(\\theta_{t,e}^k, \\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}) = \\frac{\\eta E}{B} \\sum_{b=1}^{B} g_b^{k},$\nwhere $g_b^{k} = \\frac{1}{E} \\sum_{e=0}^{E-1} \\nabla \\ell(\\theta_{t,e}^k, \\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k})$ is the average gradient of data $(\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k})$, the Jacobian matrix can be written as:\n$J = \\frac{\\eta E}{B} \\left[\\nabla_{\\mathbf{x}_{1}^{k}} g_1^k \\quad \\cdots \\quad \\nabla_{\\mathbf{x}_{B}^{k}} g_B^k \\right] \\in \\mathbb{R}^{d \\times B p},$\nwhere $\\nabla_{\\mathbf{x}_{b}^{k}} g_b^{k} = \\left[\\frac{\\partial g_{b, 1}^{k}}{\\partial \\mathbf{x}_{b, 1}^{k}} \\ldots \\frac{\\partial g_{b, d}^{k}}{\\partial \\mathbf{x}_{b, p}^{k}}\\right] \\in \\mathbb{R}^{d \\times p}$.\nTherefore, $\\operatorname{rank}(J) \\leq \\min \\{d, B p\\}$.\nWhen the batch size B satisfies $d < B p$, $\\operatorname{rank}(J) < B p$ always holds, and the private batch data cannot be uniquely determined. This leads to the following theorem:\nTheorem 4.2. In horizontal federated learning, if the batch size B satisfies that $d < B p$, where p is the dimension of a single data point and d is the model parameter dimension, then the server cannot uniquely determine the private batch data $\\{\\mathbf{x}_{b}^{k}, \\mathbf{y}_{b}^{k}\\}_{b=1}^{B}$ from the model update $\\Delta \\theta$, thus ensuring privacy.\nProof. When $d < B p$, the rank of the Jacobian matrix $J \\in \\mathbb{R}^{d \\times B p}$ is $\\operatorname{rank}(J) < d < B p$. According to Theorem 4.1, the private batch data cannot be uniquely determined from $\\Delta \\theta$, and privacy is protected.\nThis theorem provides a sufficient condition for the batch size to prevent data reconstruction attacks. Intuitively, the product of the batch size and the data dimension should be much larger than the model parameter dimension, making the information about private data in $\\Delta \\theta$ incomplete. While increasing the batch size may improve model performance, it also brings greater privacy risks."}, {"title": "5. Theoretical Analysis from the Perspective of Optimization Theory", "content": "In this section, we provide a theoretical analysis of the relationship between privacy leakage and various factors in federated learning, including the number of local data samples, the number of local epochs, and the batch size. We measure the privacy leakage using the discrepancy between the reconstructed data and the original data, as defined in Definition 5.1. We also introduce the concept of distortion extent in Definition 5.2, which quantifies the difference between the gradients computed on the original parameter and the distorted parameter.\nFor the semi-honest attacker, the privacy leakage is measured using the discrepancy between the reconstructed data and the original data (Zhang et al., 2023f,c, 2024b). The privacy leakage is defined as follows.\nDefinition 5.1 (Privacy Leakage). Let $\\hat{x}_{i}^{k}$ represent the i-th data sample reconstructed by the attacker at round t for client k, and $x_i^k$ represent the i-th original data sample. The privacy leakage is measured as:\n$\\epsilon_{t}^{k}=1-\\mathbb{E}_{\\mathcal{D}^{k}}\\left[\\frac{1}{\\left|\\mathcal{D}^{k}\\right|} \\sum_{i=1}^{\\left|\\mathcal{D}^{k}\\right|} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\|x_{i}^{k}-\\hat{x}_{i}^{k}\\|}{D}\\right],$ (5)\nwhere D is a constant satisfying $\\|x_{i}^{k}-\\hat{x}_{i}^{k}\\| \\leq D$, and the expectation is taken over the randomness in the local dataset $\\mathcal{D}^{k}$.\nDefinition 5.2 (Distortion Extent). Let $g(x)$ and $\\hat{g}(\\hat{x})$ be the gradients computed on the original data x and the distorted data $\\hat{x}$, respectively. The distortion extent is defined as:\n$\\Delta=\\|g(x)-\\hat{g}(\\hat{x})\\|,$ (6)\nwhere $\\|\\cdot\\|$ is the Euclidean norm.\nRemark: Assuming general applicability, we posit that $\\Delta \\leq 1$.\nAssumption 5.1 (Bi-Lipschitz Condition of Gradients). Let $\\|x_{1}-x_{2}\\| \\leq D$ for any two data samples $x_{1}$ and $x_{2}$. We assume that their gradients satisfy the bi-Lipschitz condition with positive constants $c_{a}$ and $c_{b}$ (Royden and Fitzpatrick, 1968) as follows:\n$c_{a}\\|\\nabla \\ell(\\theta, x_{1}, y)-\\nabla \\ell(\\theta, x_{2}, y)\\| \\leq \\|x_{1}-x_{2}\\| \\leq c_{b}\\|\\nabla \\ell(\\theta, x_{1}, y)-\\nabla \\ell(\\theta, x_{2}, y)\\|.$ (7)"}, {"title": "5. Theoretical Analysis from the Perspective of Optimization Theory", "content": "Remark: In general, this assumption ensures the smoothness of gradients. For simplicity, we rewrite the bi-Lipschitz condition by placing $\\|x_{1}-x_{2}\\|$ in the middle of the inequality.\nAssumption 5.2 (Self-bounded Regret). Let T represent the total number of learning rounds for the semi-honest attacker. We assume that its regret bound $\\Theta(T^{1 / 2})$ satisfies $c_{o} \\cdot T^{1 / 2}<\\sum_{t=1}^{T}\\|\\nabla \\ell(\\theta, x_{t}, y)-\\nabla \\ell(\\theta, \\hat{x}, y)\\| \\leq \\odot(T^{1 / 2})<c_{2} T^{1 / 2}$, where $c_{o}$ and $c_{2}$ are positive constants, $x_{t}$ is the data reconstructed by the attacker at round t, and $\\hat{x}$ is the data satisfying $\\nabla \\ell(\\theta, x, y)=g$.\nRemark: This assumption reflects the realistic scenario where the attacker employs an optimization algorithm with a near-optimal regret bound. Many well-known gradient-based optimizers, such as AdaGrad (Duchi et al., 2011) and Adam (Kingma and Ba, 2014), achieve a regret bound of $\\Theta(T^{1 / 2})$, which matches the lower bound for online convex optimization. This indicates that the attacker can effectively minimize the gradient mismatch between the reconstructed data and the target data through an asymptotically optimal gradient-based learning process. The specific constants $c_{o}$ and $c_{2}$ in the assumption capture the dependence of the regret bound on the problem parameters, such as the data dimension and the smoothness of the loss function. This assumption allows us to analyze the performance of the overall defense mechanism against such a powerful attacker who can leverage state-of-the-art optimization techniques to accurately reconstruct the target data.\nLemma 5.1 (Chernoff-Hoeffding Inequality). Let $X_{1}, X_{2}, \\ldots, X_{T}$ be i.i.d. random variables supported on [0, 1]. For any positive number $\\epsilon$, we have:\n$\\operatorname{Pr}\\left(\\left|\\frac{1}{T} \\sum_{t=1}^{T} X_{t}-\\mathbb{E}\\left[\\frac{1}{T} \\sum_{t=1}^{T} X_{t}\\right]\\right| \\geq \\epsilon\\right) \\leq 2 \\exp \\left(-2 T \\epsilon^{2}\\right).$ (8)\nTo facilitate our analysis, we make two assumptions. Assumption 5.1 states that the gradients of any two data samples satisfy the bi-Lipschitz condition, which ensures the smoothness of gradients. Assumption 5.2 assumes that the semi-honest attacker's optimization algorithm has a self-bounded regret, which is reasonable in practice as many classical gradient-based optimizers satisfy this property.\nHere is a detailed theoretical analysis of the relationship between the upper bound of privacy leakage and the number of local data samples n,"}, {"title": "5. Theoretical Analysis from the Perspective of Optimization Theory", "content": "the number of local epochs E, and the batch size B in federated learning. Recent work by Zhang et al. (2023f,c, 2024b) has also provided upper bounds on the privacy leakage in federated learning. However, these prior analyses typically consider the scenario where the number of training epochs E = 1, and assume that the total number of training samples $n^k$ is equal to the batch size B. In contrast, the current setting considers a more general case where the number of training epochs E can be greater than 1, and the total number of training samples $n^k$ may not be equal to the batch size B. This generalization is important in practice, as it allows for more flexibility in the federated learning protocol and better captures real-world scenarios.\nTheorem 5.2 (Upper Bound for Privacy Leakage). Let Assumption 5.1 and Assumption 5.2 hold. Assume that $\\Delta^{k}>\\frac{2 c_{o}}{c_{a}} \\frac{20^{2} c_{b} E}{\\sqrt{T}}$, where $c_{a}, c_{2```json\n, and $c_{o}$ are introduced in Assumption 5.1 and Assumption 5.2. Let $n^{k}$ represent the number of local data samples of client k, E represent the number of local epochs, and B represent the batch size. Assume that the assumptions hold. With probability at least $1-\\exp (-\\operatorname{poly}(B))$, the privacy leakage of client k is bounded by:\n$\\epsilon_{t}^{k} \\leq 1+\\sqrt{\\frac{\\ln 2+\\operatorname{poly}(B)}{2 B}}+\\frac{c_{a}}{2 D} \\frac{c_{2}}{\\Delta^{k}},$ (9)\nwhere $\\operatorname{poly}(B)$ is a polynomial function of B, $c_{a}$ and D are constants defined in the assumptions, and $\\Delta^{k}$ is the distortion extent of client k.\nThis theorem provides insights into the factors that affect the privacy leakage in federated learning. It shows that increasing the batch size B or the distortion extent $\\Delta^{k}$ can reduce the upper bound of privacy leakage. The number of local data samples $n^{k}$ and the number of local epochs E do not directly appear in the bound, but they implicitly affect the privacy leakage since $n^{k}$ affects the batch size with the number of batches fixed, and E appears in the assumption on the attacker's optimization algorithm.\nThe significance of this theorem lies in its quantitative characterization of the relationship between privacy leakage and various factors in federated learning. It provides a theoretical foundation for understanding the impact of these factors on privacy and can guide the design of privacy-preserving federated learning algorithms. By carefully tuning the batch size, the number of local data samples, and the number of local epochs, one can potentially achieve a better trade-off between privacy and utility in federated learning."}, {"title": "5. Theoretical Analysis from the Perspective of Optimization Theory", "content": "Proof. We denote $\\mathcal{D}_{l o c a l}^{k}$ as the local dataset of client k and $\\mathcal{D}_{b a t c h}^{k}$ as a randomly sampled batch from $\\mathcal{D}_{l o c a l}^{k}$. The size of $\\mathcal{D}_{l o c a l}^{k}$ is $n^{k}$, and the size of $\\mathcal{D}_{b a t c h}^{k}$ is B. The total number of batches in one local epoch is $\\left[\\frac{n^{k}}{B}\\right]$. Let $\\hat{x}_{t, i}^{k}$ represent the i-th data sample of client k that is reconstructed by the attacker at the t-th round of the optimization algorithm, and $x_{t, i}^{k}$ represent the i-th original data sample of client k.\nWe begin by considering the term$\\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\|x_{t, i}^{k}-\\hat{x}_{t, i}^{k}\\|}{D}$, which represents the average Euclidean distance between the reconstructed data samples $\\hat{x}_{t, i}^{k}$ and the original data samples $x_{t, i}^{k}$, averaged over all data samples in a batch (size B) and all optimization rounds (from 1 to T). Then we have that\n$\\begin{aligned} &\\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\|x_{t, i}^{k}-\\hat{x}_{t, i}^{k}\\|}{D} \\\\ &\\geq \\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\left\\|x_{t, i}^{k}-\\hat{x}_{t, i}^{k}\\right\\|}{D} \\\\ &\\geq \\frac{c_{a}}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\|\\nabla \\ell(\\theta, x_{t, i}^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)\\|}{D} \\\\ &\\geq \\frac{c_{a}}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\|\\nabla \\ell(\\theta, x_{t, i}^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)\\|}{D} \\\\ &=\\frac{c_{a}}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\|\\nabla \\ell(\\theta, x_{t, i}^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)\\|}{D} \\\\ &=\\frac{c_{a}}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\|\\nabla \\ell(\\theta, x_{t, i}^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)\\|}{D} . \\end{aligned}$  (10)\n(11)\n(12)\n(13)\nIn Eq. (10), we apply the triangle inequality that $\\|a-c\\| > \\|a-b\\|-\\|b-c\\|$. In Eq. (11), we use the assumptions that $\\|x^{k}-\\hat{x}^{k}\\| \\geq c_{a}\\|\\nabla \\ell(\\theta, x^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}^{k}, y)\\|$, and $\\left\\|x_{i}^{k}-\\hat{x}_{i}^{k}\\right\\| \\leq c_{b}\\|\\nabla \\ell(\\theta, x^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}^{k}, y)\\|$. In Eq. (12), we apply Jensen's inequality: $\\frac{1}{B} \\sum\\left\\|a_{i}\\right\\| \\geq \\left\\|\\frac{1}{B} \\sum a_{i}\\right\\|$. In Eq. (13), we use the definition that $\\Delta^{k}=\\left\\|\\frac{1}{B} \\sum_{i=1}^{B}(\\nabla \\ell(\\theta, x_{i}^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}_{i}^{k}, y))\\right\\|$."}, {"title": "5. Theoretical Analysis from the Perspective of Optimization Theory", "content": "Therefore, we have that\n$\\begin{aligned} &\\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\|x_{t, i}^{k}-\\hat{x}_{t, i}^{k}\\|}{D} \\\\ &\\geq \\frac{c_{a}}{D} \\frac{1}{T} \\sum_{t=1}^{T} \\left\\|\\frac{1}{B} \\sum_{i=1}^{B}\\left(\\nabla \\ell(\\theta, x_{t, i}^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)\\right)\\right\\|\\\\ &\\geq \\frac{c_{a}}{D} \\Delta^{k}-\\frac{c_{b}}{T} \\sum_{t=1}^{T} \\left\\|\\frac{1}{B} \\sum_{i=1}^{B}\\left(\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)\\right)\\right\\|, \\end{aligned}$ (14)\nwhere $\\hat{x}_{i}^{k}$ is the i-th reconstructed data sample that generates the distorted gradient $\\hat{g}$, and $c_{a}, c_{b}$ are constants defined in the assumptions.\nNow we bound the second term on the right-hand side of the inequality. Since the attacker runs the optimization algorithm for T rounds and the model update in each round is computed based on E local epochs, we have that\n$\\begin{aligned} &\\frac{1}{T} \\sum_{t=1}^{T} \\left\\|\\frac{1}{B} \\sum_{i=1}^{B}\\left(\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)\\right)\\right\\| \\\\ &=\\frac{1}{T} \\sum_{t=1}^{T} \\left\\|\\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{E} \\sum_{e=1}^{E}\\left(\\nabla \\ell(\\theta_{t, e}, \\hat{x}_{t, i}^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)\\right)\\right\\| \\\\ &<\\frac{E}{T} \\sum_{t=1}^{T} \\frac{1}{B} \\sum_{i=1}^{B} \\max _{e \\in 1, \\ldots, E}\\left\\|\\nabla \\ell(\\theta_{t, e}, \\hat{x}_{t, i}^{k}, y)-\\nabla \\ell(\\theta_{t, e}, \\hat{x}_{t, i}^{k}, y)\\right\\|, \n\\end{aligned}$ (15)\nwhere $\\theta_{t, e}$ represents the local model parameters after the e-th local epoch in the t-th round. According to the assumptions, the attacker's optimization algorithm satisfies the self-bounded regret property, which means:\n$\\frac{1}{T} \\sum_{t=1}^{T} \\frac{1}{B} \\sum_{i=1}^{B} \\max _{e \\in 1, \\ldots, E}\\left\\|\\nabla \\ell(\\theta_{t, e}, \\hat{x}_{t, i}^{k}, y)-\\nabla \\ell(\\theta_{t, e}, \\hat{x}_{t, i}^{k}, y)\\right\\| \\leq c_{2} \\sqrt{T},$ (16)\nwhere $c_{2}$ is a constant.\nTherefore, we have that\n$\\frac{1}{T} \\sum_{t=1}^{T} \\left\\|\\frac{1}{B} \\sum_{i=1}^{B}\\left(\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)-\\nabla \\ell(\\theta, \\hat{x}_{t, i}^{k}, y)\\right)\\right\\| \\leq \\frac{E c_{2}}{\\sqrt{T}}$ (17)"}, {"title": "5. Theoretical Analysis from the Perspective of Optimization Theory", "content": "Plugging this back into the previous inequality, we get:\n$\\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\|x_{t, i}^{k}-\\hat{x}_{t, i}^{k}\\|}{D} \\geq \\frac{c_{a}}{D} \\Delta^{k}-\\frac{c_{b} E c_{2}}{D \\sqrt{T}}.$ (18)\nUsing Hoeffding's inequality (Lemma 5.1), we have that with probability at least $1-\\exp \\left(-2 B \\epsilon^{2}\\right)$,$\n$\\left\\|\\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{x_{t, i}^{k}-\\hat{x}_{t, i}^{k}}{D}-\\mathbb{E}\\left[\\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{x_{t, i}^{k}-\\hat{x}_{t, i}^{k}}{D}\\right]\\right\\| \\leq \\epsilon,$ (19)\nwhere D is a constant that bounds the distance between the reconstructed data and the original data. Let $\\epsilon=\\sqrt{\\frac{\\ln 2+\\operatorname{poly}(B)}{2 B}}$, where $\\operatorname{poly}(B)$ is a polynomial function of B. Then, with probability at least $1-\\exp (-\\operatorname{poly}(B))$, we have:\n$\\begin{aligned} &\\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\|x_{t, i}^{k}-\\hat{x}_{t, i}^{k}\\|}{D} \\\\ &<\\mathbb{E}\\left[\\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\|x_{t, i}^{k}-\\hat{x}_{t, i}^{k}\\|}{D}\\right]+\\sqrt{\\frac{\\ln 2+\\operatorname{poly}(B)}{2 B}} \\\\ &=1-\\epsilon_{t}^{k}+\\sqrt{\\frac{\\ln 2+\\operatorname{poly}(B)}{2 B}}, \\end{aligned}$ (20)\nwhere the last equality follows from the definition of privacy leakage $\\epsilon_{t}^{k}$. Combining the above results, we have that\n$1-\\epsilon_{t}^{k}+\\sqrt{\\frac{\\ln 2+\\operatorname{poly}(B)}{2 B}} \\geq \\frac{c_{a}}{D} \\Delta^{k}-\\frac{c_{b} E c_{2}}{D \\sqrt{T}}$ (21)\nRearranging the terms and using the assumption $\\Delta^{k}>\\frac{2 c_{o}}{c_{a}} \\frac{20^{2} c_{b} E}{\\sqrt{T}}$, we get that\n$\\epsilon_{t}^{k} \\leq 1+\\sqrt{\\frac{\\ln 2-\\operatorname{poly}(B)}{2 B}}-\\frac{c_{a}}{2 D} \\frac{c_{2}}{\\Delta^{k}}.$ (22)"}, {"title": "5. Theoretical Analysis from the Perspective of Optimization Theory", "content": "The upper bound of the privacy leakage of client k depends on the batch size B, the distortion extent $\\Delta^{k}$, and several constants. Note that the number of local data samples $n^{k}$ and the number of local epochs E do not directly appear in the bound. However, they implicitly affect the bound through the batch size B and the assumptions on the attacker's optimization algorithm. Intuitively, the theorem suggests that:\n\u2022 Increasing the batch size B can reduce the upper bound of privacy leakage, as the term $\\sqrt{\\frac{\\ln 2+\\operatorname{poly}(B)}{2 B}}$ decreases with larger B. When B goes to \u221e, the upper bound goes to 0.\n\u2022 Increasing the distortion extent $\\Delta^{k}$ can also reduce the upper bound of privacy leakage, as the term $\\frac{1}{\\Delta^{k}}$ increases with larger $\\Delta^{k}$.\n\u2022 The number of local data samples $n^{k}$ affects the privacy leakage through the batch size B. With a fixed B, a larger $n^{k}$ means more batches in each local epoch, which may provide more information to the attacker and potentially increase the privacy leakage.\n\u2022 The number of local epochs E affects the privacy leakage through the assumptions on the attacker's optimization algorithm. If the attacker's algorithm exploits the increased number of local updates caused by more local epochs, it may lead to higher privacy leakage.\nIt's worth noting that the actual impact of $n^{k}$ and E on privacy leakage may vary depending on the specific attack methods and the assumptions made. The provided theorem gives a general upper bound based on the stated assumptions, but the relationship between privacy leakage and these factors can be complex and requires further analysis in specific scenarios."}, {"title": "6. Conclusion", "content": "In this paper, we provided a theoretical analysis of privacy leakage in federated learning from two complementary perspectives: linear algebra and optimization theory. From the linear algebra perspective, we formulated the local training process as an optimization problem and examined the uniqueness of its solution. We proved that when the Jacobian matrix of the batch data is not full rank, there exist different batches of data that produce the same model update, thereby ensuring a level of privacy. We further derived a sufficient condition on the batch size to prevent data reconstruction attacks.\nFrom the optimization theory perspective, we measured the privacy leakage using the discrepancy between the reconstructed data and the original data, and established an upper bound on the privacy leakage in terms of the batch size, the distortion extent, and several other factors.\nOur analysis provided insights into the design of privacy-preserving federated learning algorithms and highlighted the impact of different factors on privacy leakage. First, increasing the batch size can reduce the upper bound of privacy leakage, as it increases the difficulty for the attacker to reconstruct the original data from the aggregated model updates. Second, increasing the distortion extent, which measures the difference between the gradients computed on the original parameter and the distorted parameter, can also reduce the upper bound of privacy leakage. Third, the number of local data samples and the number of local epochs have implicit effects on privacy leakage through their influence on the batch size and the assumptions on the attacker's optimization algorithm. By carefully tuning the batch size, the number of local data samples, and the number of local epochs, we can demonstrate the effectiveness of our proposed strategies for enhancing privacy protection in federated learning, and achieve a better trade-off between privacy and utility in federated learning.\nThere are still several open problems and challenges that require further investigation. The theoretical analysis in this paper is based on certain assumptions, such as the bi-Lipschitz condition of gradients and the self-bounded regret of the attacker's optimization algorithm. Relaxing these assumptions and extending the analysis to more general settings is an important direction for future work. Besides, the derived upper bound on privacy leakage is relatively loose and may not provide tight guarantees in practice. Developing sharper bounds and more precise characterizations of privacy leakage is an open challenge. Furthermore, our analysis can be extended to other types of privacy attacks beyond data reconstruction attacks and generalized to other variants of federated learning, such as vertical federated learning and federated transfer learning.\nIn conclusion, this work contributes to a better understanding of privacy risks in federated learning and provides a theoretical foundation for developing more secure and privacy-preserving federated learning algorithms. We hope that our findings will inspire future research in this important area and contribute to the development of trustworthy and privacy-preserving machine learning systems."}]}