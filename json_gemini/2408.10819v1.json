{"title": "EXPLOITING LARGE LANGUAGE MODELS CAPABILITIES FOR\nQUESTION ANSWER-DRIVEN KNOWLEDGE GRAPH COMPLETION\nACROSS STATIC AND TEMPORAL DOMAINS", "authors": ["Rui Yang", "Jiahao Zhu", "Jianping Man", "Hongze Liu", "Li Fang", "Yi Zhou"], "abstract": "Knowledge graph completion (KGC) aims to identify missing triples in a knowledge graph (KG). This\nis typically achieved through tasks such as link prediction and instance completion. However, these\nmethods often focus on either static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs),\naddressing only within-scope triples. This paper introduces a new generative completion framework\ncalled Generative Subgraph-based KGC (GS-KGC). GS-KGC employs a question-answering format\nto directly generate target entities, addressing the challenge of questions having multiple possible\nanswers. We propose a strategy that extracts subgraphs centered on entities and relationships within\nthe KG, from which negative samples and neighborhood information are separately obtained to\naddress the one-to-many problem. Our method generates negative samples using known facts to\nfacilitate the discovery of new information. Furthermore, we collect and refine neighborhood path\ndata of known entities, providing contextual information to enhance reasoning in large language\nmodels (LLMs). Our experiments evaluated the proposed method on four SKGs and two TKGs,\nachieving state-of-the-art Hits@1 metrics on five datasets. Analysis of the results shows that GS-KGC\ncan discover new triples within existing KGs and generate new facts beyond the closed KG, effectively\nbridging the gap between closed-world and open-world KGC.", "sections": [{"title": "Introduction", "content": "A knowledge graph (KG) is a structured semantic knowledge base that organizes entities, concepts, attributes, and\ntheir relationships in a graph format. In a KG, nodes represent entities or concepts, and edges depict the relationships\nbetween them. For instance, a KG in the film domain might include entities like \"movie,\" \"director,\" and \"actor,\"\nalong with relationships such as \"directed\" and \"acted in.\" This structure facilitates complex queries and reasoning\ntasks, proving valuable in areas like semantic search, recommendation systems, and natural language processing (NLP).\nHowever, due to limited annotation resources and technical constraints, existing KGs often have missing key entities or\nrelationships, limiting their functionality in downstream tasks. To address this, knowledge graph completion (KGC) has\nbeen developed to infer, predict, and fill in these missing entities and relationships by analyzing existing triples, thereby\nenhancing the value and effectiveness of KGs in practical applications.\nPrevious research has employed various methods to address the problem of KGC. Among these, KG embedding\nmethods like TransE[1] and ComplEx[2] map entities and relationships into a low-dimensional vector space and use\nscoring functions to evaluate triples for reasoning. These methods primarily focus on the structural information of\nKGs, often neglecting the rich semantic content of entities. To address this limitation, text-based KGC models like\nKG-BERT[3] and SimKGC[4] have emerged, utilizing pre-trained language (PLM) models to capture detailed semantic\ninformation of entities. While these text-based methods enhance semantic understanding, they often struggle to integrate"}, {"title": "2 Related Work", "content": "2.1 Knowledge Graph Completion\nKGC aims to fill in missing entities and links in a KG by learning and inferring stable relationships and attributes,\nthereby enhancing the graph's completeness. KGC is generally categorized into embedding-based and text-based\napproaches.\nEmbedding-based KGC: TransE[1] is a pioneering embedding model that addresses link prediction by viewing\nrelations as linear translations between entities. Subsequent models, TransH[7] and TransD[8], improve upon TransE by\nintroducing relation-specific hyperplanes and dynamic mapping matrices, which better handle complex relationships like\none-to-many, many-to-one, and many-to-many. DistMult[9] is a simple and efficient bilinear model that uses symmetric\nweight matrices to capture relationships between entities. However, it cannot handle the directionality of relationships.\nTo address this issue, ComplEx[2] introduces complex-valued representations, enabling the model to distinguish\nantisymmetric relationships. RotatE[10] models relationships by rotating entities in the complex space, capturing more\ncomplex relationship patterns. TuckER[11], based on tensor decomposition techniques, uses a core tensor and two\nembedding matrices to predict entities and relationships effectively, demonstrating superior generalization capabilities.\nWhile these embedding models have made significant progress in KGC, they often fail to capture the high-level semantic\nfeatures of entities and relationships, limiting their adaptability in more complex or dynamic environments.\nText-based KGC: Text-based KGC leverages PLMs to enhance the representation and understanding of entity relation-\nships using detailed textual descriptions. For instance, KG-BERT by Yao et al. [3] uses BERT[12] to encode triples\nand their textual descriptions into continuous text sequences, providing a foundational method for integrating deep\ncontextual information into KGC. StAR by Wang et al.[13] builds on KG-BERT by combining graph embedding tech-\nniques with text encoding, using a Siamese-style encoder to enhance structured knowledge and achieve more nuanced\nentity representations. Despite these advancements, text-based methods generally lagged behind embedding-based\napproaches in performance until the introduction of SimKGC[4]. SimKGC demonstrated, for the first time, superiority\nover embedding methods on certain datasets, proving the potential of integrating contrastive learning techniques with\nPLMs to significantly enhance performance.\n2.2 LLMs enhance KGC\nLLMs with Parameter-Efficient Fine-tuning: Recent LLMs like Qwen[14], Llama[15], and GPT-4[16] have gained\nattention for their exceptional emergent abilities[17, 18] and have significantly improved applications for enhancing\nKGs[19]. These models can generate new facts for KG construction using simple prompts[20, 21, 22, 23]. Additionally,\nthe introduction of chain-of-thought techniques has significantly enhanced the reasoning capabilities of LLMs. Wei\net al.[24] introduced the chain-of-thought prompting method, which greatly improved the model's performance on\narithmetic, common sense, and symbolic reasoning tasks by incorporating intermediate reasoning steps in the prompts.\nRecent research has introduced various parameter-efficient fine-tuning techniques (PEFT) to optimize the performance\nof PLMs. One method involves adding adapters - small trainable feed forward networks - into the architecture of PLMs\nto enhance their functionality[25, 26, 27, 28]. Additionally, the Low-Rank Adaptation (LoRA) technique[29] simplifies\nmodel parameter updates by using low-dimensional representations during the fine-tuning process. Furthermore, prompt\ntuning[30] and prefix tuning[31] enhance model input or activation functions by introducing learnable parameters,\nthus improving the model's adaptability and efficiency for specific tasks. These techniques improve the flexibility and\napplicability of models and enhance their ability to quickly adapt to new tasks while maintaining the stability of the\noriginal model structure.\nLLM-based KGC: Since the introduction of the KICGPT framework in KGC research[32], it has successfully guided\nLLMs to generate rich contextual paragraphs using a triple-based KGC retriever, supporting small-scale KGC models\nand significantly enhancing their performance. Research shows that LLMs have substantial potential in understanding\nand generating rich semantic information. The CP-KGC[5] framework further develops this approach by enhancing\ntext-based KGC methods through semantic enrichment and contextual constraints. This framework increases semantic\nrichness and effectively identifies polysemous entities in KGC datasets by designing prompts tailored to different\ndatasets. Additionally, the KG-LLM[33] method treats triples as text sequences and adjusts LLMs with instructions to\nevaluate the reasonableness of triples or candidate entities and relationships, showing potential for high performance\nin triple classification and relationship prediction tasks. The MPIKGC[34] framework enhances KGC from multiple\nangles by querying LLMs to expand entity descriptions, understand relationships, and extract structures. As research\nprogresses, the implementation of LLMs in temporal KGC (TKGC) has been extensively studied, requiring the use of\nestablished temporal structures to predict missing event links at future timestamps. The application of LLMs in this\nfield has improved reasoning ability on temporal structures through enhanced historical modeling and the introduction"}, {"title": "3 Methodology", "content": "In this section, we introduce a novel generative KGC method named GS-KGC. As shown in Figure 2, it comprises three\nmodules:\nSubgraph Partition: This module divides the graph G into several subgraphs, each containing a comparable number of\nentities. These subgraphs serve as efficient context for training.\nQA Template Mapping: This module transforms the traditional KGC task into a question-answering task, implementing\nKGC through a fine-tuned large model.\nEvaluation of Generative KGC: This module assesses generative KGC by discovering new triples within or outside\nthe KG, allowing LLMs to transcend the closed-world KGC paradigm.\n3.1 Subgraph partition\nA KG is defined as $\\mathcal{G} = {\\mathcal{E}, \\mathcal{R}, \\mathcal{T}}$. $\\mathcal{E}$ is the entity set, which includes individuals such as persons, locations, and\norganizations. $\\mathcal{R}$ is the relation set, encompassing relationships between entities such as $\\textit{hasFriend}$ and $\\textit{locatedIn}$.\n$\\mathcal{T} = \\{(h,r,t)|h\\in \\mathcal{E},r \\in \\mathcal{R},t \\in \\mathcal{E}\\}$ is the triple set, where h, r, and t represent the head entity, relation, and tail\nentity of the triple, respectively. Given $\\mathcal{G}$, the training set contains triples $T_{train}$, the validation set contains triples $T_{valida}$,\nand the test set contains triples $T_{test}$. Notably, the subgraph data in $T_{valida}$ and $T_{test}$ are derived from $T_{train}$. In the link\nprediction task, each triple must be predicted bidirectionally, considering forward prediction (h, r, ?) and backward\nprediction (?, r, t). We unify these two types of missing triples as (e,r), where e represents the existing head or tail\nentity, and r represents the relationship. $\\mathcal{G}_{part} = {\\mathcal{G}_1, \\mathcal{G}_2, \\ldots,\\mathcal{G}_m}$, $m = 2 \\times (|T_{train}| + |T_{valida}| + |T_{test}|)$."}, {"title": "3.1.1 Filtering $\\mathcal{G}_{negative}$", "content": "It is crucial to prevent information leakage between the training set and the test set. The test set must contain data that\nthe model has never seen to fairly evaluate its generalization ability. We describe how to generate negative samples for\npositive samples during training to enhance the learning effect of LLMs while preventing information leakage.\nWhen generating subgraphs for the missing triple $(e_i, r_j)$, the neighborhood subgraph $\\mathcal{G}_i$ is constructed with $e_i$ at\nthe center using neighborhood sampling techniques. This subgraph $\\mathcal{G}_i$ includes all entities and relationships directly\nconnected to $e_i$.\n$\\mathcal{G}_i = \\{(e_i, r, e_k) \\in T_{train} | r \\in \\mathcal{R}, e_i, e_k \\in \\mathcal{E}\\}$\nFrom $\\mathcal{G}_i$, we then filter out the set of triples $T_{e,r}$ that include $e_i$ and have the relationship $r_j$.\n$T_{e,r} = \\{(e_i, r_j, e_k) \\in \\mathcal{G}_i\\}$\nNext, we remove the true answer $T_{true}$ from $T_{e,r}$ to obtain the set of all other triples $\\tilde{T_{e,r}}$, excluding the correct triples.\n$\\tilde{T_{e,r}} = T_{e,r} \\setminus \\{T_{true}\\}$\nThis negative sampling strategy helps train the LLM to infer other potential entities from known answers, thereby\nsignificantly improving its generalization ability."}, {"title": "3.1.2 Pruning $\\mathcal{G}_{neighbors}$", "content": "We uniformly represent the missing forward and backward triples as (e, r). For each triple in $\\mathcal{G}$, starting from entity e,\nwe extract information directly related to e from $\\mathcal{G}$ using the neighborhood sampling method to form the subgraph $\\mathcal{G}_e$.\nTo improve efficiency and accuracy, we performed necessary pruning and compression on $\\mathcal{G}_e$.\nFirst, starting from entity e, we construct its connected subgraph $\\mathcal{G}_e$. To control the size of the subgraph, we prune\nentities on paths that are more than 5 hops away from e. Specifically, if the shortest path $d(e, x)$ from entity e to entity\nt/h is greater than 5[35], those entities are pruned.\n$\\mathcal{G}_e = \\{(h,r,t) \\in \\mathcal{G} : d(e, h) \\leq 5 \\lor d(e,t) \\leq 5\\}$\nNext, to ensure that $\\mathcal{G}_e$ does not contain negative samples, we remove the content in $\\mathcal{G}_{negative}$ from $\\mathcal{G}_e$ and generate\n$\\mathcal{G}_{neighbors}$ through set difference operations, thereby isolating the effects of positive and negative samples.\n$\\mathcal{G}_{neighbors} = \\mathcal{G}_e - \\mathcal{G}_{negative}$\nWhen dealing with large graphs $\\mathcal{G}$ even after pruning, the connected subgraphs generated for each entity may still be\ntoo large for LLMs. Therefore, we introduce $\\rho$ to limit the path depth in the subgraph. The setting of $\\rho$ directly affects\nthe path length starting from entity e, thereby determining the size of the subgraph. We define $C(e, \\rho)$ as the contextual\nneighborhood information set centered on entity e with a depth of $\\rho$.\n$C(e,\\rho) = \\begin{cases} \\emptyset & \\text{if } \\rho = 0 \\\\ \\{(e, r, t) \\in \\mathcal{G}_{neighbors}\\} & \\text{if } \\rho = 1 \\\\ \\bigcup_{(e_1,r_1,t_1)\\in C(e,1)} C(t_1, \\rho - 1) & \\text{if } 1 < \\rho < 5 \\end{cases}$\nWhen $\\rho = 0$, the contextual neighborhood set is empty; when $\\rho = 1$, only the first-order neighbors directly connected\nto e are considered.\nWhen $\\rho > 1$, we extend to deeper paths. For example, when $\\rho = 2$, paths in the form of $(e, r_1, t_1, r_2, t_2)$ are considered.\nUltimately, our goal is to accurately obtain and utilize the neighborhood context set $C(e, \\rho)$ of entity e based on the\nvalue of $\\rho$."}, {"title": "3.1.3 Information Merge", "content": "Through 3.1.1 and 3.1.2, we obtained the negative sample set $\\tilde{T_{e,r}}$ and the neighborhood information set $C(e, \\rho)$ from\nthe triples.\nWe set the number of required context information sets as M. For each triple's corresponding $\\tilde{T_{e,r}}$ and $C(e, \\rho)$, we\ndetermine the final information set according to the following rule: If $|\\tilde{T_{e,r}}| > M$, we will randomly sample M elements\nfrom $\\tilde{T_{e,r}}$ to ensure that the size of the final information set does not exceed M. If $|\\tilde{T_{e,r}}| < M$, to reach the set size, we\nwill randomly sample from $C(e, \\rho)$ to fill up to M elements. Specifically, we need to sample $M - |\\tilde{T_{e,r}}|$ number of\nelements from $C(e, \\rho)$ as shown in:\n$D(e, M) = \\begin{cases} RandomSample(\\tilde{T_{e,r}}, min(|\\tilde{T_{e,r}}, M)) & \\text{if } |\\tilde{T_{e,r}}| \\geq M \\\\ \\tilde{T_{e,r}} \\cup RandomSample(C, min(|C|, M - |\\tilde{T_{e,r}}|)) & \\text{if } | \\tilde{T_{e,r}}| < M \\end{cases}$\nThe final information set $D(e, M)$ consists of the adjusted negative sample set $\\tilde{T_{e,r}}$ and the supplemented neighborhood\ninformation set C. It should be noted that if $|\\tilde{T_{e,r}}| > M$, then the supplemented neighborhood information set C will\nbe empty. This method ensures that the number of information sets exactly meets the predetermined needs while\nmaintaining the diversity of the dataset and the balance of information."}, {"title": "3.2 QA Template Mapping", "content": "LLMs have demonstrated outstanding capabilities in NLP tasks such as question-answering. Based on this, we employ\na simple question-answer format to implement the KGC task. For a missing triple (h, r, ?), we designed a basic\nquestion-answer template $Prompt_{basic}$:\n$Prompt_{basic} = \\text{Please complete this triple: (h, r, ?). h means } Desc_h$\nHere, $Desc_h$ represents the textual description of entity h, using semantically enhanced descriptions from CP-KGC[5].\nAdding $Desc_h$ helps the LLM understand the entity's meaning and effectively distinguishes the polysemy of the\nentity in different contexts. For each triple in $\\mathcal{G}$, subgraphs are obtained through subgraph partitioning, resulting in\n$\\mathcal{G}_{part} = {\\mathcal{G}_1, \\mathcal{G}_2,..., \\mathcal{G}_m\\}$. For any pair (e, r), the corresponding subgraph is denoted as $\\mathcal{G}_{e,r}$, either $\\tilde{T_{e,r}}$ or $C(e, \\rho)$ may\nbe empty:\n$\\mathcal{G}_{e,r} = {\\tilde{T_{e,r}},C(e, \\rho)}$\nWhen $\\tilde{T_{e,r}}$ is not empty, e and r in $\\tilde{T_{e,r}}$ are the same, and $e'$ represents the negative sample set. $\\tilde{T_{e,r}}$ can be represented\nas $[e'_1, e'_2, ..., e'_n]$. At this point, $Prompt_{Negative}$ is:\n$Prompt_{Negative} = \\text{Please give an answer outside the list: } [e'_1, e'_2, ..., e'_n]$\nWhen $C(e, \\rho)$ is not empty, C includes triples or paths related to e. This neighborhood information provides LLMs with\nmore relevant knowledge for reasoning. At this point, $Prompt_{Neighbors}$ is:\n$Prompt_{Neighbors} = \\text{The neighbors of e are as follows: C(e, } \\rho)$\nFinally, the questions formed by integrating this information are input into the LLM for training:\n$Prompt_{e,r} = Prompt_{basic} + Prompt_{Negative} + Prompt_{Neighbors}$"}, {"title": "3.3 Model Training", "content": "Instruction-based fine-tuning is a training strategy for LLMs designed to optimize their performance on specific tasks by\nproviding instructions and examples. While full-parameter fine-tuning is effective, it encounters significant challenges\nwith large-scale data.\nLORA is an efficient fine-tuning technique that quickly adapts to tasks by adjusting a small number of model parameters,\nreducing computational resource demands while maintaining the model's generalization ability. During LoRA fine-\ntuning, the pre-trained weight matrix $W \\in \\mathbb{R}^{d\\times d}$ is updated by the product of two low-rank matrices, $W_A \\in \\mathbb{R}^{d\\times r}$,\n$W_B \\in \\mathbb{R}^{r\\times d}$, where $W_A, W_B$ are of rank r and $r < d$. Consequently, the updated weights can be represented as:\n$W' = W + W_A W_B$\nHere, $W_A W_B$ is a matrix of rank r, representing potential weight increments that may need updating. During fine-\ntuning, only $W_A$ and $W_B$ are updated. For example, the update rule for $W_A$ can be expressed as $W_A \\leftarrow W_A + l \\frac{\\partial L}{\\partial W_A}$,\nwhere l is the learning rate and L is the loss function."}, {"title": "3.4 Evaluation of Generative KGC", "content": "The main difference between generative KGC and traditional KGC is that generative KGC directly produces the answer\nit considers most probable. In the real world, many questions do not have a single correct answer. LLMs are trained on\nvast text datasets, enabling them to learn language nuances and a broad range of knowledge, thereby possessing the\npowerful ability to generate and understand human language. Therefore, even if the answer given by an LLM is not in\nthe standard answers of the test set, it may still be correct, as there can be multiple reasonable answers to the same\nquestion.\nAs illustrated in Figure 4, excluding parts X and Y, the closed-world assumption (CWA) implies that triples not present\nin the KG are considered false. Under the CWA, triples in the training set are known to be true, while test triples are\nof unknown truth value. The predicted set of triples, $T_{predict}$, can be divided into two subsets: $T^+_{predict}$, containing true\ntriples from the test set, and $T^{CWA-}_{predict}$ containing false triples not in Test. We can achieve more accurate outcomes, thus\nbridging the gap between CWA-based and open-world assumption (OWA)-based KGC.\n$T_{predict} = T^+_{predict} + T^-_{predict}$\n$T_{predict} = T^{CWA-}_{predict} + X + Y$\n$T^{true}_{predict} = T_{predict} - T^{CWA-}_{predict} - Y$\nWe reevaluate the generative KGC results produced by LLMs from the CWA perspective, utilizing X and Y. $T^{true}_{LLMs}$\nsignifies the true positive predictions by the LLM, and the final result should be the sum of $T^{true}_{predict}$ and X. For a detailed\nanalysis, refer to Section5.4."}, {"title": "4 Experiment", "content": "4.1 Datasets\nWe conducted experiments on several popular KGs, including four SKGs and two TKGs. The SKGs are WN18RR[36],\nUMLS[36], FB15k-237[37], and FB15k-237N[38]. WN18RR is a subset of WordNet[39] that consists of English"}, {"title": "4.2 Baselines and Experiment Setup", "content": "In our SKGC experiments, we used text-based KGC models such as KG-BERT, KG-S2S, and SimKGC for comparison.\nFor the LLM-based experimental setup, we selected KG-S2S-CD, SimKGC+MPIKGC-S, and SimKGC+CP-KGC. For\nTKGC, we compared traditional models like TiRGN, HGLS, GenTKG, and GPT-NeoX-20B-ICL with LLM-based\nmodels Llama2-7b-ICL and Llama2-7b-CoH.\nThe foundational LLM models used were llama3-8b-instruct for SKGC and glm4-9b-chat for TKGC. Specific fine-\ntuning parameters were: LORA rank set to 64, LoRA alpha set to 16, LoRA dropout rate at 0.05, learning rate at 0.0001,\nand a training period of 1.0 epoch. The training equipment included four A-800-80GB PCIe graphics cards. In addition,\nwe set $\\rho$ = 1,M = 100."}, {"title": "4.3 Main Results", "content": "We compared the performance of GS-KGC with baseline models. Experimental results on the UMLS, FB15k-237N,\nFB15k-237, and WN18RR datasets are presented in Table 2. Results on the ICEWS14 and ICEWS05-15 datasets are\nshown in Table 3. From these comparisons, we made the following three key observations:\nResults in GS \u2013 KGCQA-only and GS \u2013 KGC: In Tables 2 and 3, GS \u2013 KGCQA-only refers to using only the fine-tuned\nmodel, while GS-KGC employs both negative samples and neighborhood strategies. In the SKGC task, compared to\nGS - KGCQA-only, GS-KGC achieved performance improvements of 73.1%, 14%, 13%, and 6.4% on the UMLS,\nFB15k-237N, FB15k-237, and WN18RR datasets, respectively. In the TKGC task, GS-KGC improved performance\nby 8% and 3.4% on the ICEWS14 and ICEWS05-15 datasets, respectively, compared to the GS KGCQA-only.\nSpecifically, the improvement in Hits@1 is closely related to the dataset's size and type. For example, in the UMLS\ndataset, adding negative samples and neighborhood information significantly enhanced Hits@1 performance. This is\nprimarily because the UMLS dataset has a small sample size and mainly contains conceptual knowledge. Providing\nneighborhood information effectively assists LLMs in making more accurate inferences. These results indicate that the\nreasoning capabilities of LLMs are significantly enhanced when supported by external effective knowledge."}, {"title": "Results in SKGC:", "content": "In SKGC, GS-KGC outperforms text-based KGC methods and their LLM-enhanced variants\non the UMLS, FB15k-237N, and FB15k-237 datasets but shows slightly worse performance on WN18RR. Specifi-\ncally, GS-KGC outperformed SimKGC+CP-KGC by 20.8% on UMLS and by 7.4% on FB15k-237, and exceeded\nSimKGC+MPIKGC-S by 4.9% on FB15k-237N. The poorer performance on the WN18RR can be attributed to the\nunique characteristics of its entities, where the same word often has multiple meanings. For instance, in stool NN 2,\n\"stool\" is the entity, \"NN\" indicates that it is a noun, and \"2\" represents its second meaning. LLMs struggle to directly\nunderstand these complex meanings. While they can utilize known information during reasoning, they are prone to\nhallucinations, generating entities that do not belong to the WN18RR dataset. This highlights the model's limitations in\nhandling highly polysemous entities, indicating a need for further optimization to reduce errors and non-target entity\ngeneration. In the forward and backward test sets, the LLM generated 1,220 and 1,424 entities, respectively, that do not\nexist in WN18RR. These account for 38.9% and 45.3%, respectively. Entities like \"stool NN 2\" led to additional results\nthat were all incorrect.\nResults in TKGC: In TKGC, GS-KGC outperformed both LM-based and LLM-based KGC methods on the ICEWS14\nand ICEWS05-15 datasets. Compared to baseline models, GS-KGC showed improvements of 8.9% on ICEWS14\nand 1.5% on ICEWS05-15. Although GS-KGC shows smaller improvements over GS \u2212 KGCQA-only in SKGC,\nGS - KGCQA-only already achieves performance comparable to mainstream models. Unlike SKGC, the task of TKGC\nis to predict future entities based on historical observations. In this context, LLMs can effectively learn and adapt to\nthese tasks through LoRA. Since the data relationships in TKGC are relatively simpler and more straightforward than in\nSKGC, using the QA-only mode can achieve good results."}, {"title": "Analysis", "content": "These observations verify the effectiveness of GS-KGC. GS-KGC shows great potential in integrating negative samples\nand neighborhood information in LLMs for KGC. This feature allows GS-KGC to flexibly and continuously learn KGC\nfrom new entities and relations in a KG. Advances in LLMs can potentially further enhance GS-KGC's accuracy.\n5.1 Hyperparameter Analysis\nThe reasoning ability of LLMs is often limited by the amount of available context. In Tables 2 and 3, we set the total\nnumber of negative samples and neighbor information for GS-KGC to 100. To balance model reasoning performance\nand resource consumption, we conducted parameter analysis experiments on four datasets\u2014FB15k-237N, ICEWS14,\nUMLS, and WN18RR\u2014to explore the impact of different M values (0, 20, 40, 60, 80, 100) on model performance.\nWhen M=0, it corresponds to the QA-only mode; when M=100, the results are as shown in Tables 2 and 3, with specific\noutcomes illustrated in Table 4.\nThe performance variation of the model can be more clearly observed from Figure 5. Experimental results show that\nacross the four datasets, model performance significantly improves as the M value increases from 0 to 20, then stabilizes"}, {"title": "5.2 Ablation Studies", "content": "In Section 5.1 we analyzed the impact of contextual information, including negative samples and neighborhood\ninformation, on the performance of GS-KGC. To understand the specific contributions of negative samples and\nneighborhood information, we conducted a detailed ablation study on four datasets: FB15k-237N, ICEWS14, WN18RR,\nand UMLS. In Table 5, GS \u2013 KGCQA-only denotes the model without negative samples and neighborhood information,\nGS - KGCNegative includes only negative samples, GS \u2013 KGCNeighbors includes only neighborhood information, and\nGS-KGC includes both types of information.\nAccording to the results in Table 5, introducing negative samples significantly enhanced the performance of GS \u2212\nKGCNegative. However, adding only neighborhood information resulted in relatively unchanged performance and even\nperformance declines in three of the datasets, except for FB15k-237N. When both negative samples and neighborhood\ninformation were combined, the performance of GS-KGC not only surpassed that of the scenario with only negative\nsamples but also showed a synergistic effect, where the whole is greater than the sum of its parts. We believe this\nis because, in the real world, an incomplete triple often corresponds to a question with multiple potential answers.\nUsing known results as negative samples can motivate the model to explore more possibilities, thereby enhancing\nits generalization capability. On the other hand, when only neighborhood information is added, the model gains\nmore background information. However, this information highly overlaps with the background of the known answers,\nresulting in suboptimal performance in the test set. This is further illustrated in the results of GS-KGC."}, {"title": "5.3 How Model Size Affects Results", "content": "In this section, we explore how the scale of LLMs affects the performance of KGC. We compared two model scales,\nQwen2-1.5B-Instruct and LLaMA3-8b-Instruct/GLM4-9B-Chat, with the total contextual information M set to 100. All\nother conditions were kept constant. The specific comparison results are shown in Table 6.\nOn the FB15k-237N, ICEWS14, WN18RR, and UMLS datasets, the performance differences between the two models\nare 9.7%, 6.9%, 21.7%, and 38.1%, respectively. In the 1.5B parameter model, we observed certain differences"}, {"title": "5.4 Advantages of GS-KGC", "content": "In Section 3.4, we discussed the main differences between generative KGC and traditional KGC. Generative KGC\ncan identify new triple relationships in the KG and introduce entities outside the KG. This capability aligns more\nclosely with the original goal of KGC, which is to explore and complete unknown triple relationships rather than merely\nperform link prediction.\nUsing the test results of the FB15k-237N dataset as an example, the test set contains 8226 triples, of which 3774 were\nsuccessfully predicted, while 4452 were not. As shown in Figure 6, among the 4452 triples that were not successfully\npredicted, 794 were duplicates of triples in the training and validation sets, accounting for 17.8%. We believe this may\nbe due to the limitations of negative sample generation. Although negative samples can prompt the LLM to generate\nresults beyond known answers, input length limitations prevent all negative samples from being included in the context.\nFurther analysis shows that among the 3423 predicted entities, two LLMs simultaneously considered 880 completed\ntriples to be correct. Among the entities not present in the KG, the LLMs considered 87 completed triples to be correct;\nthese triples are referred to as X.\nTable 7 categorizes X into two types. The first type includes predicted entities that exist in the KG, where the composed\ntriple is not in the KG, but both LLMs agree on its validity. For instance, the predicted triple (Waylon Jennings, place of\ndeath, Phoenix) differs from the test set answer, Chandler. Since Chandler is in the Phoenix metropolitan area, both\ntriples are valid. The second type includes predicted entities that do not exist in the KG, and the composed triple is also\nmissing from the KG. These triples are newly introduced by the LLMs and are not present in the KG.\nThrough GS-KGC, generative KGC not only enhances and refines the internal structure of the KG but also expands\nits boundaries by introducing external entities, achieving a seamless transition from a closed world to an open world.\nThis cross-domain model significantly enhances the dynamic updating capability of the KG, greatly expanding its\npracticality and impact in various real-world applications."}, {"title": "6 Conclusion", "content": "This paper proposes a generative KGC method called GS"}]}