{"title": "EXPLOITING LARGE LANGUAGE MODELS CAPABILITIES FOR\nQUESTION ANSWER-DRIVEN KNOWLEDGE GRAPH COMPLETION\nACROSS STATIC AND TEMPORAL DOMAINS", "authors": ["Rui Yang", "Jiahao Zhu", "Jianping Man", "Hongze Liu", "Li Fang", "Yi Zhou"], "abstract": "Knowledge graph completion (KGC) aims to identify missing triples in a knowledge graph (KG). This\nis typically achieved through tasks such as link prediction and instance completion. However, these\nmethods often focus on either static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs),\naddressing only within-scope triples. This paper introduces a new generative completion framework\ncalled Generative Subgraph-based KGC (GS-KGC). GS-KGC employs a question-answering format\nto directly generate target entities, addressing the challenge of questions having multiple possible\nanswers. We propose a strategy that extracts subgraphs centered on entities and relationships within\nthe KG, from which negative samples and neighborhood information are separately obtained to\naddress the one-to-many problem. Our method generates negative samples using known facts to\nfacilitate the discovery of new information. Furthermore, we collect and refine neighborhood path\ndata of known entities, providing contextual information to enhance reasoning in large language\nmodels (LLMs). Our experiments evaluated the proposed method on four SKGs and two TKGs,\nachieving state-of-the-art Hits@1 metrics on five datasets. Analysis of the results shows that GS-KGC\ncan discover new triples within existing KGs and generate new facts beyond the closed KG, effectively\nbridging the gap between closed-world and open-world KGC.", "sections": [{"title": "1 Introduction", "content": "A knowledge graph (KG) is a structured semantic knowledge base that organizes entities, concepts, attributes, and\ntheir relationships in a graph format. In a KG, nodes represent entities or concepts, and edges depict the relationships\nbetween them. For instance, a KG in the film domain might include entities like \"movie,\" \"director,\" and \"actor,\"\nalong with relationships such as \"directed\" and \"acted in.\" This structure facilitates complex queries and reasoning\ntasks, proving valuable in areas like semantic search, recommendation systems, and natural language processing (NLP).\nHowever, due to limited annotation resources and technical constraints, existing KGs often have missing key entities or\nrelationships, limiting their functionality in downstream tasks. To address this, knowledge graph completion (KGC) has\nbeen developed to infer, predict, and fill in these missing entities and relationships by analyzing existing triples, thereby\nenhancing the value and effectiveness of KGs in practical applications.\nPrevious research has employed various methods to address the problem of KGC. Among these, KG embedding\nmethods like TransE[1] and ComplEx[2] map entities and relationships into a low-dimensional vector space and use\nscoring functions to evaluate triples for reasoning. These methods primarily focus on the structural information of\nKGs, often neglecting the rich semantic content of entities. To address this limitation, text-based KGC models like\nKG-BERT[3] and SimKGC[4] have emerged, utilizing pre-trained language (PLM) models to capture detailed semantic\ninformation of entities. While these text-based methods enhance semantic understanding, they often struggle to integrate"}, {"title": "2 Related Work", "content": "KGC aims to fill in missing entities and links in a KG by learning and inferring stable relationships and attributes,\nthereby enhancing the graph's completeness. KGC is generally categorized into embedding-based and text-based\napproaches.\nEmbedding-based KGC: TransE[1] is a pioneering embedding model that addresses link prediction by viewing\nrelations as linear translations between entities. Subsequent models, TransH[7] and TransD[8], improve upon TransE by\nintroducing relation-specific hyperplanes and dynamic mapping matrices, which better handle complex relationships like\none-to-many, many-to-one, and many-to-many. DistMult[9] is a simple and efficient bilinear model that uses symmetric\nweight matrices to capture relationships between entities. However, it cannot handle the directionality of relationships.\nTo address this issue, ComplEx[2] introduces complex-valued representations, enabling the model to distinguish\nantisymmetric relationships. RotatE[10] models relationships by rotating entities in the complex space, capturing more\ncomplex relationship patterns. TuckER[11], based on tensor decomposition techniques, uses a core tensor and two\nembedding matrices to predict entities and relationships effectively, demonstrating superior generalization capabilities.\nWhile these embedding models have made significant progress in KGC, they often fail to capture the high-level semantic\nfeatures of entities and relationships, limiting their adaptability in more complex or dynamic environments.\nText-based KGC: Text-based KGC leverages PLMs to enhance the representation and understanding of entity relation-\nships using detailed textual descriptions. For instance, KG-BERT by Yao et al. [3] uses BERT[12] to encode triples\nand their textual descriptions into continuous text sequences, providing a foundational method for integrating deep\ncontextual information into KGC. StAR by Wang et al.[13] builds on KG-BERT by combining graph embedding tech-\nniques with text encoding, using a Siamese-style encoder to enhance structured knowledge and achieve more nuanced\nentity representations. Despite these advancements, text-based methods generally lagged behind embedding-based\napproaches in performance until the introduction of SimKGC[4]. SimKGC demonstrated, for the first time, superiority\nover embedding methods on certain datasets, proving the potential of integrating contrastive learning techniques with\nPLMs to significantly enhance performance."}, {"title": "2.1 Knowledge Graph Completion", "content": "KGC aims to fill in missing entities and links in a KG by learning and inferring stable relationships and attributes,\nthereby enhancing the graph's completeness. KGC is generally categorized into embedding-based and text-based\napproaches."}, {"title": "2.2 LLMs enhance KGC", "content": "LLMs with Parameter-Efficient Fine-tuning: Recent LLMs like Qwen[14], Llama[15], and GPT-4[16] have gained\nattention for their exceptional emergent abilities[17, 18] and have significantly improved applications for enhancing\nKGs[19]. These models can generate new facts for KG construction using simple prompts[20, 21, 22, 23]. Additionally,\nthe introduction of chain-of-thought techniques has significantly enhanced the reasoning capabilities of LLMs. Wei\net al.[24] introduced the chain-of-thought prompting method, which greatly improved the model's performance on\narithmetic, common sense, and symbolic reasoning tasks by incorporating intermediate reasoning steps in the prompts.\nRecent research has introduced various parameter-efficient fine-tuning techniques (PEFT) to optimize the performance\nof PLMs. One method involves adding adapters - small trainable feed forward networks - into the architecture of PLMs\nto enhance their functionality[25, 26, 27, 28]. Additionally, the Low-Rank Adaptation (LoRA) technique[29] simplifies\nmodel parameter updates by using low-dimensional representations during the fine-tuning process. Furthermore, prompt\ntuning[30] and prefix tuning[31] enhance model input or activation functions by introducing learnable parameters,\nthus improving the model's adaptability and efficiency for specific tasks. These techniques improve the flexibility and\napplicability of models and enhance their ability to quickly adapt to new tasks while maintaining the stability of the\noriginal model structure.\nLLM-based KGC: Since the introduction of the KICGPT framework in KGC research[32], it has successfully guided\nLLMs to generate rich contextual paragraphs using a triple-based KGC retriever, supporting small-scale KGC models\nand significantly enhancing their performance. Research shows that LLMs have substantial potential in understanding\nand generating rich semantic information. The CP-KGC[5] framework further develops this approach by enhancing\ntext-based KGC methods through semantic enrichment and contextual constraints. This framework increases semantic\nrichness and effectively identifies polysemous entities in KGC datasets by designing prompts tailored to different\ndatasets. Additionally, the KG-LLM[33] method treats triples as text sequences and adjusts LLMs with instructions to\nevaluate the reasonableness of triples or candidate entities and relationships, showing potential for high performance\nin triple classification and relationship prediction tasks. The MPIKGC[34] framework enhances KGC from multiple\nangles by querying LLMs to expand entity descriptions, understand relationships, and extract structures. As research\nprogresses, the implementation of LLMs in temporal KGC (TKGC) has been extensively studied, requiring the use of\nestablished temporal structures to predict missing event links at future timestamps. The application of LLMs in this\nfield has improved reasoning ability on temporal structures through enhanced historical modeling and the introduction"}, {"title": "3 Methodology", "content": "In this section, we introduce a novel generative KGC method named GS-KGC. As shown in Figure 2, it comprises three\nmodules:"}, {"title": "Subgraph Partition:", "content": "This module divides the graph G into several subgraphs, each containing a comparable number of\nentities. These subgraphs serve as efficient context for training."}, {"title": "QA Template Mapping:", "content": "This module transforms the traditional KGC task into a question-answering task, implementing\nKGC through a fine-tuned large model."}, {"title": "Evaluation of Generative KGC:", "content": "This module assesses generative KGC by discovering new triples within or outside\nthe KG, allowing LLMs to transcend the closed-world KGC paradigm."}, {"title": "3.1 Subgraph partition", "content": "A KG is defined as G = {E,R,T}. E is the entity set, which includes individuals such as persons, locations, and\norganizations. R is the relation set, encompassing relationships between entities such as hasFriend and locatedIn.\nT = {(h,r,t)|h\u2208 E,r \u2208 R,t \u2208 E} is the triple set, where h, r, and t represent the head entity, relation, and tail\nentity of the triple, respectively. Given G, the training set contains triples Ttrain, the validation set contains triples Tvalida,\nand the test set contains triples Ttest. Notably, the subgraph data in Tvalida and Ttest are derived from Ttrain. In the link\nprediction task, each triple must be predicted bidirectionally, considering forward prediction (h, r, ?) and backward\nprediction (?, r, t). We unify these two types of missing triples as (e,r), where e represents the existing head or tail\nentity, and r represents the relationship. Gpart = {G1, G2, \u2026\u2026\u2026,Gm}, m = 2 \u00d7 (|Ttrain| + |Tvalida| + |Ttest|)."}, {"title": "3.1.1 Filtering $G_{negative}$", "content": "It is crucial to prevent information leakage between the training set and the test set. The test set must contain data that\nthe model has never seen to fairly evaluate its generalization ability. We describe how to generate negative samples for\npositive samples during training to enhance the learning effect of LLMs while preventing information leakage.\nWhen generating subgraphs for the missing triple (ei, rj), the neighborhood subgraph Gi is constructed with ei at\nthe center using neighborhood sampling techniques. This subgraph Gi includes all entities and relationships directly\nconnected to ej.\nGi = {(ei, r, ek) \u2208 Ttrain | r \u2208 R, ei, ek \u2208 E}\nFrom Gi, we then filter out the set of triples Te,r that include ei and have the relationship rj.\nTe,r = {(ei, rj, ek) \u2208 Gi}\nNext, we remove the true answer Ttrue from Te,r to obtain the set of all other triples $\\overline{T_{e,r}}$, excluding the correct triples.\n$\\overline{T_{e,r}}$ = Te,r \\ {Ttrue}\nThis negative sampling strategy helps train the LLM to infer other potential entities from known answers, thereby\nsignificantly improving its generalization ability."}, {"title": "3.1.2 Pruning $G_{neighbors}$", "content": "We uniformly represent the missing forward and backward triples as (e, r). For each triple in G, starting from entity e,\nwe extract information directly related to e from G using the neighborhood sampling method to form the subgraph Ge.\nTo improve efficiency and accuracy, we performed necessary pruning and compression on Ge.\nFirst, starting from entity e, we construct its connected subgraph Ge. To control the size of the subgraph, we prune\nentities on paths that are more than 5 hops away from e. Specifically, if the shortest path d(e, x) from entity e to entity\nt/h is greater than 5[35], those entities are pruned.\nGe = {(h,r,t) \u2208 G : d(e, h) \u2264 5 V d(e,t) \u2264 5}\nNext, to ensure that Ge does not contain negative samples, we remove the content in Gnegative from Ge and generate\nGneighbors through set difference operations, thereby isolating the effects of positive and negative samples.\nGneighbors = Ge - Gnegative\nWhen dealing with large graphs G even after pruning, the connected subgraphs generated for each entity may still be\ntoo large for LLMs. Therefore, we introduce p to limit the path depth in the subgraph. The setting of p directly affects\nthe path length starting from entity e, thereby determining the size of the subgraph. We define C(e, p) as the contextual\nneighborhood information set centered on entity e with a depth of p.\nC(e,p) =\n{\n{(e, r, t) \u2208 G G : : (e, r, t) \u2208 Gneighbors} s} if p = 0 if p = 1 U(e1,r1,t1)\u2208C(e,1) C(t1, p \u2212 1) if 1 <p< 5\nWhen p = 0, the contextual neighborhood set is empty; when p = 1, only the first-order neighbors directly connected\nto e are considered.\nWhen p > 1, we extend to deeper paths. For example, when p = 2, paths in the form of (e, r1, t1, r2, t2) are considered.\nUltimately, our goal is to accurately obtain and utilize the neighborhood context set C(e, p) of entity e based on the\nvalue of p."}, {"title": "3.1.3 Information Merge", "content": "Through 3.1.1 and 3.1.2, we obtained the negative sample set $\\overline{T_{e,r}}$ and the neighborhood information set C(e, p) from\nthe triples.\nWe set the number of required context information sets as M. For each triple's corresponding $\\overline{T_{e,r}}$ and C(e, p), we\ndetermine the final information set according to the following rule: If |$\\overline{T_{e,r}}$| > M, we will randomly sample M elements\nfrom $\\overline{T_{e,r}}$ to ensure that the size of the final information set does not exceed M. If |$\\overline{T_{e,r}}$| < M, to reach the set size, we\nwill randomly sample from C(e, p) to fill up to M elements. Specifically, we need to sample M - |$\\overline{T_{e,r}}$| number of\nelements from C(e, p) as shown in:\nD(e, M) =\n{\nRandom Sample ($\\overline{T_{e,r}}$, min(|$\\overline{T_{e,r}}$|, M)) if |$\\overline{T_{e,r}}$| \u2265 M ($\\overline{T_{e,r}}$ \u222a RandomSample(C, min(|C|, M \u2013 |$\\overline{T_{e,r}}$|)) if |$\\overline{T_{e,r}}$| < M\nThe final information set D(e, M) consists of the adjusted negative sample set $\\overline{T_{e,r}}$ and the supplemented neighborhood\ninformation set C. It should be noted that if |$\\overline{T_{e,r}}$| > M, then the supplemented neighborhood information set C will\nbe empty. This method ensures that the number of information sets exactly meets the predetermined needs while\nmaintaining the diversity of the dataset and the balance of information."}, {"title": "3.2 QA Template Mapping", "content": "LLMs have demonstrated outstanding capabilities in NLP tasks such as question-answering. Based on this, we employ\na simple question-answer format to implement the KGC task. For a missing triple (h, r, ?), we designed a basic\nquestion-answer template Promptbasic:\nPromptbasic = Please complete this triple: (h, r, ?). h means Desch\nHere, Desch represents the textual description of entity h, using semantically enhanced descriptions from CP-KGC[5].\nAdding Desch helps the LLM understand the entity's meaning and effectively distinguishes the polysemy of the\nentity in different contexts. For each triple in G, subgraphs are obtained through subgraph partitioning, resulting in\nGpart = {G1, G2,..., Gm}. For any pair (e, r), the corresponding subgraph is denoted as Ge,r, either $\\overline{T_{e,r}}$ or C(e, p) may\nbe empty:\nGe,r = {$\\overline{T_{e,r}}$,C(e, p)}\nWhen $\\overline{T_{e,r}}$ is not empty, e and r in $\\overline{T_{e,r}}$ are the same, and e' represents the negative sample set. $\\overline{T_{e,r}}$ can be represented\nas [e1, e2, ..., en]. At this point, PromptNegative is:\nPromptNegative Please give an answer outside the list: [e\u2081, e2, ..., en]\nWhen C(e, p) is not empty, C includes triples or paths related to e. This neighborhood information provides LLMs with\nmore relevant knowledge for reasoning. At this point, PromptNeighbors is:\nPromptNeighbors = The neighbors of e are as follows: C(e, p)\nFinally, the questions formed by integrating this information are input into the LLM for training:\nPrompte,r = Promptbasic + PromptNegative + PromptNeighbors"}, {"title": "3.3 Model Training", "content": "Instruction-based fine-tuning is a training strategy for LLMs designed to optimize their performance on specific tasks by\nproviding instructions and examples. While full-parameter fine-tuning is effective, it encounters significant challenges\nwith large-scale data.\nLORA is an efficient fine-tuning technique that quickly adapts to tasks by adjusting a small number of model parameters,\nreducing computational resource demands while maintaining the model's generalization ability. During LoRA fine-\ntuning, the pre-trained weight matrix W \u2208 Rd\u00d7d is updated by the product of two low-rank matrices, Wa \u2208 Rd\u00d7r,\nWB \u2208 Rr\u00d7d, where WA, WB are of rank r and r < d. Consequently, the updated weights can be represented as:\nW' = W + WAWB\nHere, WAWB is a matrix of rank r, representing potential weight increments that may need updating. During fine-\ntuning, only W A and W B are updated. For example, the update rule for W A can be expressed as Wa \u2190 Wa+l\u2202L/\u2202w, where l is the learning rate and L is the loss function."}, {"title": "3.4 Evaluation of Generative KGC", "content": "The main difference between generative KGC and traditional KGC is that generative KGC directly produces the answer\nit considers most probable. In the real world, many questions do not have a single correct answer. LLMs are trained on\nvast text datasets, enabling them to learn language nuances and a broad range of knowledge, thereby possessing the\npowerful ability to generate and understand human language. Therefore, even if the answer given by an LLM is not in\nthe standard answers of the test set, it may still be correct, as there can be multiple reasonable answers to the same\nquestion."}, {"title": "4 Experiment", "content": "We conducted experiments on several popular KGs, including four SKGs and two TKGs. The SKGs are WN18RR[36],\nUMLS[36], FB15k-237[37], and FB15k-237N[38]. WN18RR is a subset of WordNet[39] that consists of English"}, {"title": "4.1 Datasets", "content": "We conducted experiments on several popular KGs, including four SKGs and two TKGs. The SKGs are WN18RR[36],\nUMLS[36], FB15k-237[37], and FB15k-237N[38]. WN18RR is a subset of WordNet[39] that consists of English"}, {"title": "4.2 Baselines and Experiment Setup", "content": "In our SKGC experiments, we used text-based KGC models such as KG-BERT, KG-S2S, and SimKGC for comparison.\nFor the LLM-based experimental setup, we selected KG-S2S-CD, SimKGC+MPIKGC-S, and SimKGC+CP-KGC. For\nTKGC, we compared traditional models like TiRGN, HGLS, GenTKG, and GPT-NeoX-20B-ICL with LLM-based\nmodels Llama2-7b-ICL and Llama2-7b-CoH.\nThe foundational LLM models used were llama3-8b-instruct for SKGC and glm4-9b-chat for TKGC. Specific fine-\ntuning parameters were: LORA rank set to 64, LoRA alpha set to 16, LoRA dropout rate at 0.05, learning rate at 0.0001,\nand a training period of 1.0 epoch. The training equipment included four A-800-80GB PCIe graphics cards. In addition,\nwe set p = 1,M = 100."}, {"title": "4.3 Main Results", "content": "We compared the performance of GS-KGC with baseline models. Experimental results on the UMLS, FB15k-237N,\nFB15k-237, and WN18RR datasets are presented in Table 2. Results on the ICEWS14 and ICEWS05-15 datasets are\nshown in Table 3. From these comparisons, we made the following three key observations:"}, {"title": "5 Analysis", "content": null}, {"title": "5.1 Hyperparameter Analysis", "content": "The reasoning ability of LLMs is often limited by the amount of available context. In Tables 2 and 3, we set the total\nnumber of negative samples and neighbor information for GS-KGC to 100. To balance model reasoning performance\nand resource consumption, we conducted parameter analysis experiments on four datasets\u2014FB15k-237N, ICEWS14,\nUMLS, and WN18RR\u2014to explore the impact of different M values (0, 20, 40, 60, 80, 100) on model performance.\nWhen M=0, it corresponds to the QA-only mode; when M=100, the results are as shown in Tables 2 and 3, with specific\noutcomes illustrated in Table 4.\nThe performance variation of the model can be more clearly observed from Figure 5. Experimental results show that\nacross the four datasets, model performance significantly improves as the M value increases from 0 to 20, then stabilizes"}, {"title": "5.2 Ablation Studies", "content": "In Section 5.1 we analyzed the impact of contextual information, including negative samples and neighborhood\ninformation, on the performance of GS-KGC. To understand the specific contributions of negative samples and\nneighborhood information, we conducted a detailed ablation study on four datasets: FB15k-237N, ICEWS14, WN18RR,\nand UMLS. In Table 5, GS \u2013 KGCQA-only denotes the model without negative samples and neighborhood information,\nGS - KGCNegative includes only negative samples, GS \u2013 KGCNeighbors includes only neighborhood information, and\nGS-KGC includes both types of information."}, {"title": "5.3 How Model Size Affects Results", "content": "In this section, we explore how the scale of LLMs affects the performance of KGC. We compared two model scales,\nQwen2-1.5B-Instruct and LLaMA3-8b-Instruct/GLM4-9B-Chat, with the total contextual information M set to 100. All\nother conditions were kept constant. The specific comparison results are shown in Table 6."}, {"title": "5.4 Advantages of GS-KGC", "content": "In Section 3.4, we discussed the main differences between generative KGC and traditional KGC. Generative KGC\ncan identify new triple relationships in the KG and introduce entities outside the KG. This capability aligns more\nclosely with the original goal of KGC, which is to explore and complete unknown triple relationships rather than merely\nperform link prediction."}, {"title": "6 Conclusion", "content": "This paper proposes a generative KGC method called GS-KGC, a universal model capable of completing both SKGs\nand TKGs. To address the one-to-many problem in generative KGC, we introduced new negative sample generation\nmethods and neighborhood information path recall strategies to provide effective contextual information for LLM\nreasoning. Our exploration of GS-KGC's effectiveness revealed that GS can generate correct facts beyond the graph,\nindicating that GS-KGC can bridge closed-world KGC and open-world KGC, thereby enhancing KGC's potential\nin practical applications. In future research, we aim to explore higher-quality information recall strategies to enable\nbroader application of GS-KGC in vertical domains."}]}