{"title": "ADAGE: A generic two-layer framework for adaptive agent based modelling", "authors": ["Benjamin Patrick Evans", "Sumitra Ganesh", "Sihan Zeng", "Leo Ardon"], "abstract": "Agent-based models (ABMs) are valuable for modelling complex, potentially out-of-equilibria scenarios. However, ABMs have long suffered from the Lucas critique, stating that agent behaviour should adapt to environmental changes. Furthermore, the environment itself often adapts to these behavioural changes, creating a complex bi-level adaptation problem. Recent progress integrating multi-agent reinforcement learning into ABMs introduces adaptive agent behaviour, beginning to address the first part of this critique, however, the approaches are still relatively ad hoc, lacking a general formulation, and furthermore, do not tackle the second aspect of simultaneously adapting environmental level characteristics in addition to the agent behaviours. In this work, we develop a generic two-layer framework for ADaptive AGEnt based modelling (ADAGE) for addressing these problems. This framework formalises the bi-level problem as a Stackelberg game with conditional behavioural policies, providing a consolidated framework for adaptive agent-based modelling based on solving a coupled set of non-linear equations. We demonstrate how this generic approach encapsulates several common (previously viewed as distinct) ABM tasks, such as policy design, calibration, scenario generation, and robust behavioural learning under one unified framework. We provide example simulations on multiple complex economic and financial environments, showing the strength of the novel framework under these canonical settings, addressing long-standing critiques of traditional ABMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Agent-based models (ABMs) have shown great promise for modelling scenarios where outcomes may differ from traditional equilibrium analysis [5], for example, providing more realistic models of the dynamics in complex systems where deriving strict equilibria is unrealistic or impractical [4].\nHowever, traditional ABMs crucially rely on the behavioural rules of the agents, which are typically fixed and/or manually specified. This fixed behaviour opens ABMs to the famed Lucas Critique:\nGiven that the structure of a model consists of optimal decision rules of agents, and that optimal decision rules vary systematically with changes in the environment, it follows that any change in policy will systematically alter the structure of the models - paraphrased from [32]\nraising concerns about using fixed behaviour rules in ABMs [34]. Such a critique can be addressed with the introduction of adaptive agent behaviour [50], e.g. by using Artificial Intelligence (AI) techniques to introduce models of heterogeneous agents adapting their behaviour [3, 6] following the literature on adaptive learning in macroeconomics [26]. For example, [41] models adaptive expectations in response to economic environment changes, [10] lets firms forecast sales based on news (e.g. shocks), and [61] models household behaviour in response to changing macroeconomic policies. However, this is just one part of the adaptive problem. Crucially, the environment itself also often adapts to these behavioural changes (e.g., macroeconomic policy changing based on the agent behaviour), which has received comparatively little attention [61]. In this work, we develop a generic two-layer framework for ADaptive AGEnt-based modelling (ADAGE). ADAGE automatically learns behavioural rules that adapt to environmental changes, as well as learning these potential environmental changes based on the task of the designer (e.g., for policy design or calibration). The framework is built around a bi-level design, with an inner simulation layer, with agents learning behavioural rules conditioned on an observation of updateable characteristics 0, and an outer layer updating these characteristics 0. As agent behaviour is conditioned on the evolving characteristics, this helps mitigate the Lucas critique, adapting behaviour in response to environmental change, and, due to the bi-level design, simultaneously optimises the environment itself in response to the changing agent behaviours. We formulate the problem as a Stackelberg game, solving a coupled set of non-linear equations, and show how this subsumes several common ABM tasks, such as calibration, scenario analysis, robust behavioural learning, and policy design, under one unified framework.\nSpecifically, we:\n\u2022 Develop a generic framework for adaptive agent-based modelling based on Stackelberg games\n\u2022 Derive several common ABM tasks (such as calibration and policy design) as special instances of the framework\n\u2022 Evaluate the framework showing the flexibility and suitability in complex agent-based models.\nThe proposed approach is a generic framework for developing adaptive simulations - learning and adjusting agent behaviour and environment parameters in a tractable manner."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "The combination of machine learning techniques (such as Reinforcement learning) with agent-based simulation is a growing research area with much promise for improving upon hand-crafted models [2, 47], while maintaining some of the key benefits such as agent heterogeneity and relaxed notions of equilibria [16].\nHowever, most existing work only considers the learning of fixed behavioural rules for agent-based simulation [12, 29, 45, 53, 54], requiring retraining under novel conditions. For example, in a market setting, if the macroeconomic policies such as taxation rates changed, this would require retraining the model rather than automatically modelling the adaption in agent behaviour based on the updated tax rates. This issue was highlighted by Geanakoplos [21], where following regulatory policy changes, required redefining agent behaviours in their pioneering housing ABM [21]. A more advanced two-layer approach\u00b9 for not only learning agent behaviour but adapting this behaviour in response to changing macroeconomic conditions is introduced in the AI Economist [49, 61], providing an approach for AI-driven tax policy design where agent behaviours adapt automatically. The AI Economist features an inner layer (for simulation) and an outer layer (for policy design), serving as the key motivation for this work. While the results of [49] are extremely impressive, the framework is posed for the specific problem of AI-driven tax policy design and lacks a more general formulation for other common modelling tasks.\nIn this work, we develop a generic expansion based on the ideas of [61] for adaptive agent-based modelling, unifying several previously distinct ABM tasks, something yet to be considered. For example, in addition to the policy design task of [61], we show how other common tasks, such as model calibration and scenario generation, can be encapsulated under a more general framework, with the Al economist then serving as a specific instantiation of the generic framework developed. This generic expansion is based on formulating these two-layer frameworks as Stackelberg games, which are asymmetric Markov games between a leader and one or more followers [9, 23]. This formulation has been successfully investigated in the context of economic games [8], but has not yet been thoroughly investigated for complex agent-based models. Therefore, in the following section, we formulate adaptive agent-based model design as a Stackelberg game, with the leader as the outer layer, and the followers as the (inner) agent-based simulation layer."}, {"title": "3 PROPOSED APPROACH", "content": "We propose a generic two-layer framework, ADAGE, as visualised in fig. 1. This framework is represented as a Stackelberg game between a leader in the outer layer and n followers in the inner simulation layer, operating in a parameterised environment (see parametric games [20, 58]) representing the agent-based model."}, {"title": "3.1 Formulation", "content": "We model the problem as a Partially Observable Markov Game (POMG) with n + 1 agents [23] - agent 0 is reserved as the leader while the remaining n agents are the followers\u00b2. L = 0 indexes the leader agent, and F = {1, ..., n} denotes the set of follower agents, and we refer to them as the outer layer and inner simulation layer, respectively. The game can be characterised by the tuple:\n$(S, A, T, r, O, \\gamma)$\nwhere S is the state space, A = (A0, A1, ..., An) the action space of the agents, $T : S \\times A \\rightarrow S$ the transition function, $r : S \\times A \\rightarrow R^N$ the reward functions, and O = (00, 01, ..., ON) the observation spaces, and y the discount rate. The state s \u2208 S of the Markov game may not be generally known to any agent. Instead, agent i has a local observation of the state oi(s) where $o_i : S \\rightarrow O_i$.\nStackelberg games are essentially Markov games with inherent asymmetry, in which 1) the outer layer L usually exerts a stronger effect on the state transition/reward and has more informative observations, 2) the outer layer L and inner layer F may operate on separate decision-making timescales tL, tF (where typically tL > tF), and 3) the objectives of L and F exhibit hierarchical structure. The outer layer acts first, with the goal of maximising a global objective, and the inner layer reacts, with each follower aiming to maximise its local objective given the leader's behaviour.\nWe characterise the effect of the leader's behaviour on the follower's behaviour through a characteristics variable 0. The characteristics variable is an outcome of tL and parameterises the environment perceived by the followers. Each follower i \u2208 F has a local observation $o_i$ of the characteristics information, which drives their behaviour. We will discuss the meaning of 0 and $o_i$ in each specific task that we model.\nIn the rest of the paper, we denote the behavioural policy of agent i by $\u03c0_i(a \\vert o_i, \\hat{o}_i)$ to emphasise that the followers condition their behavioural policies on the (observed) environment characteristics. When the conditioning is not explicit, it is assumed that $o_i$ is part of the observation $o_i$."}, {"title": "3.1.1 Example tasks.", "content": "The framework's generality is in the outer layer's ability to serve multiple purposes based on the outcome of interest. By varying the leader's reward function and action space, we can capture many common ABM tasks. For example, if the desired outcome is policy or mechanism design [59], the outer layer creates new rules, e.g. taxation rules [61] through updating (e.g., Social Environment Design [59]). If the goal is calibrating a simulator, the outer layer fits latent parameters of agent characteristics to ensure the simulated outcomes most closely match the real-world dynamics [51]. If the desired outcome is adaptability to unseen scenarios, the outer layer encourages robust behavioural policies in the spirit of meta-learning [11].\nThe rewards for the outer layer are based on outputs from the agent-based simulator \u03a9. For example, for calibration, we can represent the leaders reward function as:\n$r_{L,t} = -|m(\\Omega_t) \u2013 \\varphi_\\tau|$\nwhere m is some metric function from the environment \u03a9 (e.g. resulting market prices), and \u03c6\u03c4 is the target metric (e.g. pricing from some ground truth data). The action aL,t is then to tune calibratable latent parameters to optimise eq. (3). Alternatively, we could think of the case of policy design where the goal of the leader is to improve social welfare for the follower agents:\n$r_{L,t} = E [\\sum_{i\\in F} r_{i,t}]$\nor other more advanced social welfare functions [59]. Likewise, an adversarial leader could be represented with:\n$r_{L,t} = -E[\\sum_{i\\in F} r_{i,t}]$\nin each case updating the parameters of the ABM in an effort to improve RL. Follower behaviour is then conditioned on observations of these characteristics $\u03c0_\u03c1(a \\vert \\theta_F, O_F)$, guiding the system behaviour based on optimising the rewards of interest.\nThe flexibility of ADAGE is in defining different reward structures and action spaces for the leader agent to achieve various tasks while maintaining the same representation (POMG) and solution concept (Stackelberg equilibria) for the leader-follower dynamics. In section 4 we provide concrete examples of this generality by subsuming several tasks under the framework."}, {"title": "3.2 Optimisation", "content": "The objective in optimising $\u03c0_L$ and $\u03c0_F$ is to find a Stackelberg equilibrium, i.e. a solution ($\u03c0_L$, $\u03c0_F$) at which no agent can improve its local objective function holding the behaviour of all other agents fixed. Due to the \"gradient domination\" condition satisfied by eq. (2), every stationary point is globally optimal [1]. This implies that to find ($\u03c0_L$, $\u03c0_F$), it suffices to solve the following coupled system of n+1 non-linear equations. Each equation i states that the behavioural policy $\u03c0_i$ is a first-order stationary point for agent i given that every other agent j \u2260 i follows $\u03c0_j$:\n$\\begin{cases}\n\\nabla_{\\pi_{i,t}} R_i = 0, & \\forall i\\in F, \\\\\n\\nabla_{\\pi_L}R_L = 0.\n\\end{cases}$\nNote that in cases of large n, in order to reduce the number of equations that must be solved, shared policy learning [52] can be used for agents of similar types, reducing the set of follower behavioural policies to a subset (<< n) of shared behavioural policies.\nThe standard method, and the one we generally use here, for solving such coupled equations is alternating gradient descent/ascent (A-GD), i.e. we maintain parameter $\u03c0_{i,t}$ as an estimate of $\u03c0_i^*$ and take turns to update $\u03c0_{i,t}$ for every agent i in the direction of $\\bar{\\mu}_{\u03c0_{i,t}} R_i$. $R_i$, where $\\bar{\\mu}_{\u03c0_{i,t}} R_i$ denotes an estimate of the exact gradient $\u2207_{\u03c0_{i,t}} R_i$ with the behaviour of agents j \u2260 i fixed to their latest iterates:\n$\u03c0_{L,t+1} = \u03c0_{L,t} + \u03b1_{L,t}\\bar{\\mu}_{\u03c0_{L,t}}R_L given {$\\eta_{j,t}$}$_{j\\in F}$\n$\u03b7_{i,t+1} = \u03b7_{i,t} + \u03b1_{i,t}\u2207_{\u03b7_{i,t}}R_i given {$\\eta_{j,t}$}$_{j\u2260i}, \\forall i \\in F$\nWhen Ri exhibits strong structure, the convergence of the A-GD algorithm to ($\u03c0_L$, $\u03c0_F$) is guaranteed under proper learning rates \u03b1L,t, \u03b1F,t [28, 57]. In general, we need to choose \u03b1L,t to be much larger than all \u03b1F,t to approximate a nested-loop algorithm that runs multiple inner layer updates per outer layer update."}, {"title": "3.2.1 Conditional behavioural policies.", "content": "A key component of the proposed approach is learning follower behavioural policies $\u03c0_F$ conditioned on observations OF of the environment characteristics 0. While in practice many learning algorithms could be used for finding $\u03c0_F(a \\vert o_F, \\hat{O}_F)$, and the framework is independent of any particular algorithm, here we use deep reinforcement learning (DRL) (specifically, PPO [44]), treating OF as part of the observation space. The benefit of using DRL is that it natively parameterises the decision function by the agents observation $o_i$, permitting any functional form for the behavioural rule and removing the requirement of manually determining the rule or specifying the form of the decision function a priori."}, {"title": "4 EXPERIMENTS", "content": "In order to demonstrate the flexibility of the framework, we provide four illustrative (but non-exhaustive) examples of different outer layer configurations spanning distinct modelling tasks, showing how each task can be viewed as a special instance of the proposed framework. For each configuration, we additionally use a separate agent-based model in the inner simulation layer to demonstrate that the framework is simulator-independent, as well as using a combination of individual and shared policy learning. We generally use reinforcement learning to learn the behaviour in both layers, but we show that the framework is learning algorithm-independent, as demonstrated with alternative configurations such as Bayesian layers and analytically derived behavioural rules. The chosen environments are well established, spanning several economic and financial domains. Additionally, the environments feature a mixture of continuous and discrete action and observation spaces, showing the generality of the approach with many different ABMs. The source code is provided in the supplementary material."}, {"title": "4.1 Policy Designer", "content": "Policy design is an important use case for modelling, for example, helping to design COVID containment strategies in epidemiology [40], optimal auction design [42], and assisting government and economic policy-making [59, 61]. As we demonstrate below, policy design naturally fits within the ADAGE framework."}, {"title": "4.1.1 Environment.", "content": "To demonstrate the use of ADAGE for policy design, we base an environment on TaxAI, \"the most realistic economic simulator for optimal tax policy\" [33], based on the canonical Bewley-Aiyagari model. Under this environment, there are multiple households, firms, banks, and a central government (visualised in fig. 2). This environment is similar to the one used in the AI-Economist [61], and ABIDES-Economist [13]. The government acts as the leader L. While there are several important tasks for the government, here we focus on the case where the government is acting to maximise the social welfare of the households. Households are the followers F, maximising their individual reward, subject to the income tax and asset tax rates from the government.\nEach time step t, households work ht for a firm to earn income based on their productivity level et and the wage rate Wt:\n$W_t = (1 - \u03b1) \\frac{K_t}{L_t}^\u03b1$"}, {"title": "4.1.2 Results.", "content": "where \u03b1 = 1/3 is the capital elasticity, $K_t = \\sum a_t$ is the capital, and $L_t = \\sum h_t * e_t$ is the aggregate labor. Households consume ct, save at, and make any required tax payments. The tax payments are set by the government using the HSV tax function [24, 33]:\n$T(x, \u03c4, \\xi) = \u03c4x^{1+\u03be}$\n$\u03c4 \\geq 0$ controls the average tax rate, and $\u03be \\geq 0$ the progressivity. Income taxes are $T^i = T(i_t, \u03c4^i, \u03be_i)$ and asset taxes $T^a = T(a_t, \u03c4^a, \u03be_a)$.\nReward Households aim to maximise their return, as given by a per timestep reward rft based on their consumption ct and working ht rates. The government aims to maximise the lifetime social welfare for households, receiving per timestep reward rL,t:\n$r_{f,t} = log c_t^{1+\u03b6}-\\frac{h_t^{1+\u03b6}}{1 + \u03b6}\\$ \\qquad $L_t = \\sum_{i\\in F} r_{i,t}$\nAction The action for the government aL,t is to set the taxation parameters for income and asset taxes. The action for households aft is to decide their savings ratio pt and working hours ht:\n$a_{L,t} = {\u03c4, \u03be, \u03c4^a, \u03be^a} = \\theta, a_{f,t} = {p_t, h_t}$\nwhere each value is in the unit interval [0, 1].\nObservation Households have a view of their current wealth at, income it, productivity ability et, the overall wage rate Wt, and the taxation rules set by the government:\n$O_{F,t} = {W_t, a_t, i_t, e_t, \\theta}$\nnote that here is fully observable, so \u03b8 = $\\hat{O_i}, \\forall i \\in F$.\nGovernments have a view of the averages of these values across the households, OL,t = {E[at], E[it], E[et], \u03b8t-1}\nResults. We investigate how successfully an adaptive outer layer can design taxation policies to maximise social welfare RL. For this, we compare to a free market (no tax) baseline. The proposed approach successfully maximises the social welfare in this environment (fig. 3), significantly improving upon the free market case, indicating that the adaptive framework can successfully learn policies to improve social welfare in a complex economic system.\nThis improvement in social welfare is achieved through households working less (fig. 4a), earning higher wages (fig. 4c), and consuming more, which comes at the expense of lower savings rates (fig. 4b). As households adjust their behaviour to the taxation rates, higher asset taxes encourage this additional consumption. Additionally, while not directly optimised for, the learnt taxation rules also improve equality in the system (fig. 5), reducing the right-tailed asset distribution, minimising the household asset inequality (fig. 5a) while achieving higher average income rates at no expense to the income equality (fig. 5b), resulting in a more equal population."}, {"title": "4.2 Calibrator", "content": "Another crucial modelling task is calibration [38]. Often, there are simulation parameters where the exact values are unknown, and must instead be calibrated to match some real-world dynamics to improve the simulator's realism. Examples include financial simulators [15], agent compositions [52], and models of human behaviour [16]. Again, this section demonstrates how calibration naturally fits within the ADAGE framework."}, {"title": "4.2.1 Environment.", "content": "To demonstrate how calibration can be performed with ADAGE, we use the Cobweb market game [27] to explore market price fluctuations, where with human participants, we see larger fluctuations than is to be expected by perfectly rational participants, so we must calibrate the information processing costs (modulating the bounded rationality) of the agents [16].\nIn a cobweb market, i \u2208 F producers must estimate the price pt of a good at the next timestep t. The price is a result of the estimates pi,t from each producer:\n$p_t = \\frac{a - E_{j\\in F} S(p_{j,t})}{b} + \\epsilon_t$\nwhere S(pi,t) = tanh(/(pi,t \u2013 n)) + 1 is the supply curve, and a = 13.8, b = 1.5, \u03b3 = 2 are market specific parameters (from [27]), and \u03b5 is a noise term (additional details in appendix B.2.1).\nProducers receive a reward rft based on the accuracy of their prediction pi,t (eq. (12)). However, each are boundedly rational, therefore they are subject to an information processing constraint when maximising RF:\nsubject to $\u0399(\u03c0_i, 0_{i,t}) < \\hat{I}$\ngiving learnt follower behavioural policies i \u2208 F of the form\n$\u03c0_{i}(a_{i}|o_i, \\lambda_i) = \\max_{\u03c0_i} E_\u03c0_i [\\sum_{t=0}^{\\infty} \\gamma^t r_{i,t} (S_t, a_{i,t}, a_{-i,t}) \u2013 \u03bb_iI(\u03c0_i, O_{i,t})]$\nencoding the soft information constraint based on some calibratable parameters \u03bbi, i \u2208 F, i.e., \u03b8i = \u03bb\u012f. While different information cost functions I can be used, here we use the KL penalty [17, 36]:\n$I(\u03c0, \u03bf) = \\sum_{a} \u03c0(a|o) log \\frac{\u03c0(\u03b1|o)}{\u03c0_0(\u03b1)}$\nwhere \u03c0\u03bf(a) is some prior belief, which we assume to be uniform.\nReward The calibrator L is attempting to find parameters \u03bb = 0 to minimise the difference in the distribution of the experimental market prices q and the simulated market prices:\n$r_{Lt} = -|q - p_t|$\nusing the absolute error per timestep."}, {"title": "4.2.2 Results.", "content": "Producers receive a non-linear reward based on the accuracy of their prediction compared to the resulting market price pt [27]:\n$r_{f,t} = max(0, 1300 \u2013 260(p_t \u2013 p_{i,t})^2)$\nAction The action for the calibrator is a vector action\n$a_{L,t} = {\u03bc, \u03c3} = \u03b8$\nto decide distributional parameters N(\u03bc, \u03c3), where individual processing resources \u03bb\u012f are sampled from this distribution \u03bb\u012f ~ N(\u03bc, \u03c3) (as in [16]). While here we demonstrate the case of learning distribution parameters, an alternative approach could also learn the individual \u03bb\u012f directly, as demonstrated in appendix B.2.2.\nObservation The observation for the calibrator at time t + 1 is the prediction vector of the producers p, and the past price pt:\n$O_{L,t+1} = {p_t, P_t}$\nThe observation for the followers at t + 1 is the realised market price at pt, the mean historical market price E[p\u2264t], their own mean predicted price E[pi,\u2264t], and their processing penalty \u03b8\u2081 = \u03bb\u1f76:\n$O_{F,t+1} = {E[p\u2264t], E[P_{i,\u2264t}], p_t, \u03bb_i}$\nnote that the producers only have a local view of the parameters 0, only knowing their individual \u03bb\u03af.\nResults. To investigate how successfully the adaptive outer layer calibrates a simulator to real-world data, we compare with uncalibrated baselines, including the analytically derived equilibrium and a base simulator with rational agents (F = 0) to show the importance of appropriate calibration. Additionally, while the task is not to compare specific calibration algorithms, we show how other algorithms can be natively integrated into ADAGE in the supplementary material (e.g. Bayesian optimisation in appendix B.2.2) to show compatibility with the framework.\nThe outer layer successfully calibrates the parameters to to the experimental data, well capturing the overall distribution (fig. 6) and producing the lowest error (table 1). These results demonstrate that ADAGE can also perform model calibration and that models of bounded rationality are still fully compatible with the learning-based framework, alleviating concerns about perfectly rational agents. We demonstrated three calibration approaches. One which optimised parameters to a distribution of information processing penalties with RL (fig. 6), one which optimised these penalties directly with RL (supplementary material, appendix B.2.2), and additionally, one which used a Bayesian outer layer to calibrate the parameters (supplementary material, appendix B.2.2) to show the framework is independent of the exact learning algorithm used.\nIn contrast to section 4.1, which maximised social welfare, here, we calibrated a simulator to real data. Despite the seemingly distinct goals, both tasks can be seen as specific instantiations of ADAGE by changing the reward and action space of the leader agent."}, {"title": "4.3 Scenario Generator", "content": "Another common modelling task is scenario generation [14], determining under what conditions specific scenarios emerge from a simulation. For example, when do crises form [39] or opinions split [14]. Despite being distinct from the previous tasks, we demonstrate the compatibility with the proposed framework."}, {"title": "4.3.1 Environment.", "content": "Here, we want to reduce (emergent) market volatility to promote more stable regimes. Therefore, the goal for the outer layer is to generate scenarios for stabilising a market. Specifically, we explore a market entrance game [37] and the impact of Tobins tax on controlling the volatility in this market [7, 43]."}, {"title": "4.3.2 Results.", "content": "Tobins tax was initially introduced [48] as an approach to penalising short-term currency conversions to stabilise the foreign exchange market. Here, we explore this idea in a generic market setting.\nThe outer layer is the scenario generator and sets the Tobins tax duty \u03b8 = \u03c4\u03c4. The inner layer is composed of n trading agents, making simulatenous decisions on whether or not to enter or stay out of a market, where the utility for the traders depends on the market entrance decisions of the other traders [17].\nReward The base per timestep utility for the traders is\n$r_{k,t} = \\begin{cases}\n\u03b2, & \\text{if } a_{k,t} = \\text{stay out} \\\\\n\u03b2 + v(C - D_t), & \\text{if } a_{k,t} = \\text{enter}\n\\end{cases}$\nwhere \u1e9e = 1 is the baseline reward (e.g., a risk-free rate if they decided not to enter a market), the demand $D_t = \\sum_{k\\in F} 1_{{a_{k,t} = enter}}$ is the total number of traders entering the market, 0 \u2264 C\u2264 n is the market capacity, and v = 2 is the strength of the reward/penalty for risking entering the market. Equation (13) rewards entrance with excess market capacity as there is more room to profit due to reduced competition, and penalises entry into overly saturated markets (D > C) where profit diminishes.\nThe reward for the outer layer is the (negative) standard deviation in historical demand, where larger fluctuations (i.e. more volatile markets) are implicitly discouraged:\n$R_{T,L} = -\u03c3(D_T) = -\\sqrt{\\frac{\\sum_{i=1}^{T}(D_i \u2013 E[D])^2}{T-1}}$\nAction The action for traders is a binary (discrete) decision on whether or not to enter the market:\nAS = {enter, stay out}\nThe action for the outer layer is to set the global tax rate 0 = 0\u2081 = 0 \u2264 t \u2264 0.1. The tax must be paid by the follower agents changing their market positions, giving the final reward r of:\n$r_{k,t}^{\\tau} = \\begin{cases}\nr_{k,t} * (\\frac{1-\u03c4_t}{\u03c4_t})^\u03b3 & \\text{if } a_{k,t} \u2260 a_{k,t-1} \\\\\nr_{k.t} & \\text{otherwise}\n\\end{cases}$\nObservation Trader k observes the previous demand, the mean historical demand, their previous position, and the tax rate:\n$Ok,t = {D_{t\u22121}, E[D_{<t}], a_{k,t\u22121}, \u03c4_t}$\nThe observation for the scenario generator (leader) agent is the running standard deviation of the demand (giving an estimate of the volatility) as well as the current tax rate:\n$OL,t = {\u03c3(Dt), \u03c4\u03c4}$"}, {"title": "4.4 Robust Behavioural Learning", "content": "Finally, another crucial modelling task is learning robust agent behaviour across environment configurations and/or agent preferences, i.e., meta-learning [22]. This section demonstrates how ADAGE can be used to learn this robust agent behaviour."}, {"title": "4.4.1 Environment.", "content": "We demonstrate robust behavioural learning under a financial environment with a market maker (MM) trading with various Liquidity Takers (LTs), where the MM may have different preferences in terms of maximising market share or profit and loss (PnL), and the goal is to learn generic MM behaviour across these preferences. For example, a MM trying to maximise market share may provide more desirable pricing through minimising the bid-ask spread they are offering, whereas a profit-focused MM will have a larger spread to only execute more profitable trades [51].\nWe follow the general formulation of [51] in a simplified environment inspired by [55]. There is one learning MM agent and n zero-intelligence LT agents [18, 30] interacting in a market. The key focus here is on the behaviour of the MM under various preferences.\nPrices in this market are exogenously determined and follow a mean reverting process around a reference (mean) price po [55]:\n$p_t = max(0, \u03bap_0 + (1 \u2212 \u03ba)p_{t\u22121} + N(0, \u03c3^2))$\nwhere \u043a = 0.01 is the mean reversion and \u03c3 the shock variance.\nReward The reward for the MM is a mixture of PnL and market share m, given by:\n$MM,t = w \\frac{PnL_t}{PnL_t} + (1 - 0) \\frac{m_t}{n}$\n$PnL_t = q_{bid} \u00d7 (p_{t\u22121} \u2013 p_{bid}) + q_{ask} \u00d7 (p_{ask} \u2013 p_{t\u22121})$,\n$m_t = q_{bid} + q_{ask}$\nwhere qbid (qask) is the quantity the MM bought (sold), and w is the MMs preference.\nThe reward for the outer layer (the meta learner) is to ensure that the MM is exposed to a variety of preferences w \u2208 \u03c9 in order to learn robust behaviour, which we encode as the entropy of the sampled preferences:\n$R_L = - \\sum_{\u0395\u03c9} \u03c0_i (\u03c9) log(\u03c0_i (\u03c9))$\nencouraging all regions of w to be explored. This formulation is based on the principle of indifference; however, if a priori preference information is known, this could be encoded directly in RL."}, {"title": "4.4.2 Results.", "content": "By optimising for eq. (18) with w sampled to maximise eq. (19)", "sell": "n$p_{bid"}, "p_t \u2013 \\frac{1}{2} \\text{half spread,, pask} = p_t + \\frac{1}{2} \\text{half spread,}$\nwith the resulting spread:\n$spread_t = p_{t}^{ask} - p_{t}^{bid}$\nThe LTs action spaces are:\n$A_{LT} = \\begin{cases} {buy, dont act}, & \\text{if buyer} \\\\ {sell, dont act}, & \\text{if seller} \\end{cases}$\nLT's are randomly assigned to be a buyer or seller (with equal probability) and follow fixed behavioural rules. LTs have a private valuation for the good v\nN(po, s), and will buy (sell) a unit quantity of the good iff v > pask (v < phid), and not act otherwise.\nThe action space for the outer layer is to sample w = \u03b8\u039c\u039c\u00b7 As the behavioural policy which optimises eq. (19) is analytically derivable (the maximum entropy probability distribution), we use this distribution directly instead of learning the behaviour. This shows an additional benefit of ADAGE: when specific behaviour can be computed analytically, this can be substituted directly for any of the behavioural policies, and the remaining behaviour will be optimised w.r.t this, again demonstrating independence on the learning algorithm.\nObservation The observation space for the MM includes a normalised view of the market price pt and its preference w:\n$O_t = {P_t/P_0, w}$\nincluding the preference w as part of the observations facilitates meta-learning across various preferences.\nResults. We investigate how the MM adapts their behaviour across distinct preferences w = {0, 0.25, 0.5, 0.75, 1}. The proposed approach is trained across w (as sampled by the outer layer) and then evaluated on a specific w \u2208 \u03c9, and compared to a baseline model which is trained and evaluated on a fixed w \u2208 w. If the adaptive model successfully learns to reproduce the fixed w behaviour, we can conclude that the MM successfully generalises across w.\nLooking at the resulting spread,"]}