{"title": "Evaluating the Performance of Large Language Models in Competitive Programming: A Multi-Year, Multi-Grade Analysis", "authors": ["Adrian Marius Dumitran", "Adrian C\u0103t\u0103lin Badea", "Stefan-Gabriel Muscalu"], "abstract": "This study explores the performance of large language models (LLMs) in solving competitive programming problems from the Romanian Informatics Olympiad at the county level. Romania, a leading nation in computer science competitions, provides an ideal environment for evaluating LLM capabilities due to its rich history and stringent competition standards. We collected and analyzed a dataset comprising 304 challenges from 2002 to 2023, focusing on solutions written by LLMs in C++ and Python for these problems.\nOur primary goal is to understand why LLMs perform well or poorly on different tasks. We evaluated various models, including closed-source models like GPT-4 and open-weight models such as CodeLlama and RoMistral, using a standardized process involving multiple attempts and feedback rounds. The analysis revealed significant variations in LLM performance across different grades and problem types. Notably, GPT-4 showed strong performance, indicating its potential use as an educational tool for middle school students. We also observed differences in code quality and style across various LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "This study evaluates the performance of various LLMS in solving competitive programming problems from the Romanian Informatics Olympiad at the county level (OJI \u201cOlimpiada Jude\u021bean\u0103 de Informatic\u0103\u201d). Our primary objective is to understand why LLMs excel or struggle with specific tasks. We analyze data spanning over two decades, focusing on challenges written in C++, the predominant language used in programming competitions, and Python, the language usually used in papers that test LLM code generation.\nWe collected and cleaned data from multiple sources, including historical archives and recent competition records. The dataset comprises 304 computer science challenges from 2002 to 2023, written in Romanian.\nOur findings reveal significant variations in LLM performance across different grades and problem types. GPT-4, for instance, demonstrated strong performance in lower grades but struggled with more complex problems typically encountered in higher grades. We also observed notable differences in code quality, with GPT-4 generating more verbose and production-ready code compared to other models, which produced more concise and straightforward solutions.\nThis research provides valuable insights into the capabilities and limitations of LLMs in competitive programming contexts. By understanding the factors that influence LLM performance, we can better design models and training datasets to enhance their problem-solving abilities. Furthermore, these findings have practical implications for educational settings, suggesting ways to leverage LLMs to support personalized learning and improve competitive programming training."}, {"title": "A. About OJI, county level Olympiads in Romanian", "content": "Romania is one of the top performers in computer science competitions for middle and high school students, placing 3rd in the all-time ranking of countries. With such a strong history, the national Olympiad generates a lot of interest, but out of tens of thousands of participants, only about 600 qualify for the national phase.\nThe final stage of qualifying is the OJI ('Olimpiada Jude\u021bean\u0103 de Informatic\u0103'), where participants compete within their county on common subjects.\nThese participants are organized into eight distinct classes. Approximately 80 students from each class advance to the nationals, with the top performer from each county guaranteed a spot by default.\nIn terms of problem-solving, middle school students tackle two problems, while high school students face between two and three problems, particularly after the year 2017 when this format was standardized.\nCompetition problems are exclusively designed in C++, the predominant language used by most contestants. Although Pascal is permitted within the rules of the competition, its usage among competitors is minimal.\nParticipants are allocated between three to four hours to solve these challenges during an in-person event.\nScoring in this competition allows for partial credit; some problems may include anywhere from ten to thirty tests"}, {"title": "II. RELATED WORK", "content": "The capabilities of large language models (LLMs) for code generation have been extensively explored in the literature, particularly in the context of competitive programming contests. Authors from [1] were pioneers in this area, creating a diverse dataset of 10,000 programming problems that span various difficulty levels. This dataset has been foundational for subsequent research in the field. Another notable contribution is the TACO dataset [2], which offers a broad array of programming contest problems and benchmarks the performance of various LLMs in code generation and program tagging. Additionally, AlphaCode [3] represents a significant development, providing an extensive dataset that has markedly enhanced the training of LLMs for competition-level code generation.\nAmong these resources, RoCode [4] stands out as it is the only dataset that includes problems in Romanian, offering a unique perspective on assessing code intelligence. Despite the availability of rich datasets and benchmarks, current research predominantly focuses on quantitative analyses and often overlooks qualitative aspects, such as the nuanced strategies employed in problem-solving within competitive programming.\nOur \"OJI\" dataset is unique in that it incorporates problems from the same competition, with a defined curriculum for each class, allowing for a more fine-grained analysis.\nA more focused dataset, HumanEval [5], encompasses a smaller yet insightful collection of problems used to evaluate the capabilities of code-centric LLMs. This dataset has been instrumental in refining evaluation techniques, which we have adopted and expanded upon in our study.\nSignificantly, existing literature does not adequately address code generation across multiple programming languages, nor does it specifically focus on C++, despite its prevalence in competitive programming contests. Our research addresses this gap by evaluating LLM performance in both Python and C++, with a particular emphasis on C++. This approach is supported by our newly developed evaluation system, designed to comprehensively assess the efficacy of LLMs in a multilingual, competitive programming context."}, {"title": "III. METHODOLOGY", "content": "Data was collected from Kilonova. The dataset comprises 304 computer science challenges from the 2002-2023 editions of the Romanian Computing Olympiad, at the county stage, written in Romanian.\nResults were gathered from multiple sources to ensure a comprehensive dataset. Historical data from 2002 to 2020 was obtained through the archived records available on olimpiada.info, despite some gaps and incomplete entries. For the period from 2021 onwards, we collected results from sepi.ro, which provided more complete and up-to-date records."}, {"title": "B. Data Cleaning", "content": "There were several challenges in normalizing the data collected from Kilonova into a format suitable for analysis. A parser was developed to extract the data into different sections such as metadata, year, grade, statement, input, output, constraints, and examples. The results gathered from olimpiada.info and sepi.ro were parsed and aggregated manually."}, {"title": "C. Choosing the LLMs to Evaluate", "content": "The LLMs selected for evaluation include both closed and open-source models. The models evaluated were of the chat/instruct type and are as follows:\nGPT-4 1106 Limited API access\nGemini 1.0 Pro - Provided freely by Google during beta testing\nCodestral (22B)\nDeepSeek Coder (6.7B, 33B)\nAutoCoder (6.7B, 33B)\nCodeLlama (7B, 13B)\nPhind-CodeLlama (34B)\nCodeQwen 1.5 (7B)\nMistral (7B)\nLlama 3 (8B)\nYi (9B)\nPhi3 (14B)\nStarCoder 2 (15B)\nRoLlama2 (7B)\nRoMistral (7B)\nThe closed-source models were chosen for their availability and ease of use, while the open-weight models were selected for their perceived strong performance in programming tasks and being below 33B."}, {"title": "D. Evaluation Methodology", "content": "All LLMs were given the same structure when prompting and providing feedback. Each model was given multiple attempts ($k = 5$ for closed-source, $k = 3$ for open-weights) to solve each problem. Each attempt allowed a maximum of feedback rounds ($f = 5$ for closed-source, $f = 3$ for open-weights) to complete the challenge. Closed-source models were tasked with solving the entire dataset, while open-weight models were only assigned grade 5 challenges. Unless specified, the best run out of all attempts per challenge is the only one being taken into account when creating statistics. Overall, around 22,700 attempts were computed on the full dataset using closed-source models, and around 3,280 attempts were computed on the open-weights models for the grade 5 subset of the dataset."}, {"title": "E. Conversation and Prompting", "content": "The initial prompt consisted of a brief description setting up the expectations from the model (programming language, input/output file names, or console interaction), followed by the problem statement, input, output, and one example.\nThe model was then expected to respond with a solution to the challenge. If the model failed to provide a solution (early stop), it was re-prompted to continue from where it left off.\nThe solution was compiled and evaluated in a Docker container using the gcc:11 image in the C++ tests, and evaluated using the Docker python:3.11 image for python tests. During this evaluation, all examples were tested and results were compared against the expected outcome.\nIf the solution was correct, the code was recorded and the process moved to step 5.\nIf the solution was incorrect, the model received feedback on the error (compilation error, runtime error, wrong answer) and was re-prompted to fix the mistake and rewrite the solution. Steps 2 through 4 were repeated until the feedback rounds were exhausted or the solution was correct.\nWhen the solution passed the examples or the feedback rounds were exhausted, the solution, if correct, was submitted to Kilonova for a comprehensive evaluation and grading."}, {"title": "F. Running the LLMs", "content": "All conversations were conducted using the model's conversational API. For closed-source models, the API was used to send and receive responses. For open-weight models, their GGUF variants were used in conjunction with an Ollama server to handle the conversations. Tests were conducted on an RTX 4090 GPU. Models below 10B parameters were run in FP16 format, those below 20B in quantized Q8_0, and those above 20B in quantized Q6_K / Q4_K formats, described as in [10] [11].\nThere were around 112,400 submissions which initially underwent a series of local evaluations. These are preliminary checks done at a smaller scale or earlier stage to ensure that each submission meets certain criteria or standards. After passing these initial evaluations, the submissions then proceeded to be evaluated by the Kilonova evaluator.\nThe average attempt took around 01m:56s, with duration as low as 00m:16s (at p05) and as high as 04m:28s (at p97), totaling all closed-source and open-weight models execution time to 31 days, 17h:05m:40s."}, {"title": "IV. RESULTS", "content": ""}, {"title": "A. LLM Comparison", "content": "1) Open-Weights Small Models: As illustrated in the figure 2, there is a noticeable trend where smaller models exhibit reduced performance in solving the challenges. Even models with parameters as large as 33B struggle to match the performance of Gemini 1.0 Pro, with the exception of one of the latest models, Codestral 22B. This performance discrepancy is somewhat expected given the nature of the challenges difficulty and the limited training most models have with Romanian text.\nWe also experimented with RoLlama2 7B and RoMistral 7B, which are fine-tuned versions of Llama 2 and Mistral models specifically on Romanian text. However, their performance was significantly degraded compared to their base models, as they failed to generate correct code syntax in English."}, {"title": "2) Code Quality and Particularities", "content": "Inspired by research in [4], we examined the length of the code generated by LLMs. While the difference between the length of correct and incorrect solutions was not significant (approximately 10%), we observed a notable disparity in the code length generated by GPT-4 compared to Gemini 1.0 PRO, as shown in Figure 3. As seen in the figure GPT-4 not only has more lines but also has longer lines. Variable names are one of the causes for this, for example Gemini uses h, m, s whereas GPT4.0 uses structures and uses startTime.h, starTime.m, startTime.s. This significant difference prompted further investigation. We found that Gemini tends to produce simpler code, similar to what might be expected from a novice programmer. In contrast, GPT-4 generates more comprehensive, production-ready code, which includes more comments, advanced features such as functions and structures, and sophisticated language constructs like 'std::find'. Such differences can be seen in figure4\nAlthough such detailed and well-structured code is advantageous in a production environment, competitive programming often favors more concise and efficient code.\nWe have also investigated why ChatGPT-4 sometimes fails to solve certain problems and discovered that it struggles with tasks that involve multiple components, such as this problem 7. It can easily solve case 1 but struggles with cases 2 and 3. Even when provided with assistance, it may solve case 2 but then forget to address case 1, and so on.\nThis raises the question of whether writing extensive code might impede GPT-4's ability to solve tasks with more complex structures, even if the tasks themselves are not inherently difficult."}, {"title": "B. GPT4 contestant and teacher", "content": "Table I shows GPT-4 C++ performance compared to pupils in county-level Olympiads. Despite GDPR challenges, we collected 63 data points, with at least 7 per grade. GPT-4 qualified in 100% of 5th grade cases and over 50% for 6th, 7th, 8th, and 10th grades but struggled with 9th, 11th, and 12th grades due to problem complexity for 11th and 12th grades and higher competition in 9th grade.\nGiven the impressive results of GPT-4, there are numerous opportunities for its use in educational settings:\nPersonalized Learning:\nStrength Identification: GPT-4 can pinpoint students' strengths and weaknesses through detailed feedback.\nTargeted Practice: Generates specific practice problems to address weak areas.\nImmediate Feedback:\nReal-time Assistance: Provides instant help with problem-solving and concept understanding.\nSolution Explanation: Offers step-by-step explanations to enhance comprehension.\nFurthermore, LLMs have the potential to generate programming contests. However, this introduces several challenges:\nFairness and Integrity: Given that LLMs can solve many problems at this level, there is a risk of unfair use by students. Strict monitoring and innovative problem design are necessary to address this.\nProblem Complexity: Creating problems that LLMs cannot yet solve or modifying tasks to require human-LLM collaboration could be effective strategies.\nEthical Considerations: The use of LLMs in contests should be managed to ensure that they are used to enhance learning rather than undermine the competition's integrity.\nBalancing the benefits of GPT-4 in educational settings with the need to maintain fair and challenging contests is crucial for fostering a healthy and effective learning environment."}, {"title": "C. Year to year comparison", "content": "Our dataset includes OJI problems spanning over more than 20 years. We focused our analysis on ChatGPT (though Gemini and CodeStral for 5th grade results appear in the f6), which was the only LLM to achieve competitive results. Through this analysis, we identified several inflection points where the difficulty of problems increased, even though these increases were not continuous. Surprisingly, while the curriculum has not changed significantly over the years, we observed that the difficulty of the problems increased over time until 2020, punctuated by several notable inflection points::"}, {"title": "D. Grade by Grade", "content": "The graph provided (Figure 6) shows the LLM's performance in solving CS problems for grades 5 through 12, coded in C++ and Python. We analyze the curriculum content to understand why certain performance patterns emerge.\nGrade 5:\nContent: Basic algorithms, simple data types, control structures, number processing.\nPerformance: High (74%). The curriculum involves fundamental concepts, making it easier for the LLM.\nGrade 6:\nContent: Real numbers, character types, modular arithmetic, prime factorization.\nPerformance: Drop to 61%. Increased complexity with abstract concepts. LLM's struggle with solving mathematical problems.\nGrade 7:\nContent: Functions, advanced array techniques, greedy methods.\nPerformance: Increase to 62%. Systematic approach and modular problems align with LLM's capabilities.\nGrade 8:\nContent: Character strings, combinatorial algorithms, basic queue operations.\nPerformance: High (59%). Proficiency in handling string operations and combinatorial problems.\nGrade 9:\nContent: Binary search, prefix sums, greedy methods.\nPerformance: Drop to 52%. Complexity of algorithms and data structures increases significantly.\nGrade 10:\nContent: Stack, queue, deque operations, advanced data structures.\nPerformance: Small drop to 50%. Focus on data structures introduces intricate problems.\nGrade 11-12:\nContent: Dynamic programming, complex graph algorithms, advanced tree structures.\nPerformance: Significant drop to 26%. Challenges with deep abstraction and sophisticated problem-solving.\nThe LLM's performance aligns with the increasing complexity of the curriculum. Higher proficiency is observed with simpler, structured problems, while significant challenges arise with advanced topics requiring deep abstraction and intricate algorithms. Consistent performance from grades 7 to 10 indicates the LLM's ability to manage complex but pattern-recognizable problems, whereas a sharp decline at grade 11 highlights its difficulty with dynamic programming and graph theory."}, {"title": "E. Comparison of C++ and Python Results", "content": "This subsection is dedicated to contrasting the efficiency of two programming languages, C++ and Python, in the context of the GPT code generation test. The outcomes revealed substantial disparities in the efficiency of the two languages. The analysis focuses on languages that were accessible for both models: GPT-4-1106-Preview and Gemini Pro 1.0."}, {"title": "C++ advantages over Python", "content": "The committee's preference for C++ as the official source language might one of the reasons for this. This preference is not arbitrary but is based on several factors, including better management of soft and hard resources.\nAn additional factor could be the limited language options available to competitors, which are confined to C/C++ and Pascal.\nFurthermore, C++ has a robust standard library that includes a rich set of functions, algorithms and data structures, making it a versatile language for various programming tasks. Data structures also exist in Python, but they are not so popular and probably do not appear in the majority of training code.\nAs shown in Figure 6, the performance of C++ was better than that of Python for GPT-4-1106-Preview."}, {"title": "Python has an advantage in solving problems related to strings and to big integers", "content": "For example, in the following problem, Python obtains 100 points, whereas C++ only 35 as it tries to transform a 2000-digit string to an integer (stoll (num_combined);... which is possible in Python but not in C++).\nAnother example is this problem 10, a classical strings problem. For this problem, Python obtains 100 points whereas C++ obtains 0.\nIn our code generation example, Python encountered issues with memory exhaustion and time constraints, as shown in figure 7\nIt's evident that Python's primary application is not competitive programming, as it is typically utilized in environments without strict boundaries, such as training models and artificial intelligence applications."}, {"title": "F. Temperatures", "content": "Comparable to findings in previous studies [9], problem-solving performance starts to notably deteriorate when the temperature surpasses 1.0."}, {"title": "Sample Size Impact", "content": "Single sample scores are consistently lower across all temperatures.\nA significant improvement is seen when increasing the sample size from 1 to 3 samples, particularly at mid to high temperatures (0.4 to 0.8).\nThe largest gains from increasing sample size are observed at temperatures 0.6 and 0.8, where scores almost double from 1 sample to 5 samples.\na) Score and Completion Percentage:\nThe score and completion percentages are annotated on the bars. Higher scores generally correlate with higher completion percentages.\nCompletion percentage tends to be higher at higher sample sizes and higher temperatures, indicating more reliable performance under these conditions."}, {"title": "V. CONCLUSIONS", "content": "1) Our study demonstrates that LLMs are more efficient in solving Olympiad problems when generating C++ code. Most existing studies have focused on generating Python code, the most frequently used by LLMs. However, our findings show that C++ performs better for these tasks."}, {"title": "B. Limitations", "content": "Open-weight models were constrained by their maximum context size, which was set according to the model's specifications. This limit was often reached by smaller models, causing evaluation to stop prematurely.\nOpen-weight models were constrained by our testing environment. All models above 10 billion parameters were quantized to Q8, Q6 or Q4 [10] [11] in order to be evaluated in a feasible time-frame."}, {"title": "VI. FURTHER WORK", "content": "Ongoing efforts are focused on tagging each challenge based on the type of optimal solution required to achieve the maximum score, as well as identifying any possible partial solutions tags. [12]"}, {"title": "Difficulty Assessment", "content": "The current study assumes that difficulty increases with grade level. While this is generally accurate, a detailed expert assessment of all challenges would be beneficial for determining their true difficulty levels."}, {"title": "Translation to English", "content": "Further work includes translating the dataset into English to compare open-weight and closed-source models. This aims to identify score losses due to misunderstandings of the challenge statements. However, naive translations may lose important nuances."}, {"title": "Human + LLM", "content": "While LLMs can solve many problems independently, a human-LLM collaboration is expected to be even more effective. We aim to experiment with various settings to explore how humans and LLMs can work together to solve problems more efficiently."}, {"title": "ChatGPT Augmented Challenges", "content": "We are exploring the potential of combining GPT models with human experts in the creation of new challenges for future contests. This approach aims to enhance the quality and diversity of the challenges."}]}