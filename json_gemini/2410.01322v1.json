{"title": "FORTE : FINDING OUTLIERS WITH REPRESENTATION TYPICALITY ESTIMATION", "authors": ["Debargha Ganguly", "Warren Morningstar", "Andrew Yu", "Vipin Chaudhary"], "abstract": "Generative models can now produce photorealistic synthetic data which is virtually indistinguishable from the real data used to train it. This is a significant evolution over previous models which could produce reasonable facsimiles of the training data, but ones which could be visually distinguished from the training data by human evaluation. Recent work on OOD detection has raised doubts that generative model likelihoods are optimal OOD detectors due to issues involving likelihood misestimation, entropy in the generative process, and typicality. We speculate that generative OOD detectors also failed because their models focused on the pixels rather than the semantic content of the data, leading to failures in near-OOD cases where the pixels may be similar but the information content is significantly different. We hypothesize that estimating typical sets using self-supervised learners leads to better OOD detectors. We introduce a novel approach that leverages representation learning, and informative summary statistics based on manifold estimation, to address all of the aforementioned issues. Our method outperforms other unsupervised approaches and achieves state-of-the art performance on well-established challenging benchmarks, and new synthetic data detection tasks.", "sections": [{"title": "INTRODUCTION", "content": "In the past decade, deep learning has made significant strides, primarily due to the availability of large-scale annotated datasets (Deng et al., 2009) used in supervised learning and the emergence of self-supervised learning utilizing vast web-scale crawled open data (Schuhmann et al., 2022; Gao et al., 2020; Sharma et al., 2018; Radford et al., 2021b). The transition from large-scale annotated datasets to self-supervised learning was driven by the expensive and labor-intensive nature of creating these datasets and yet the concerns surrounding data usage rights persist (He et al., 2022b; Carlini et al., 2021; Huang et al., 2022). Recent advancements have led to the development of generative models that excel in generating highly realistic and detailed synthetic images (Stein et al., 2024). In this paper, we investigate identifying out-of-distribution (OOD) data and generated synthetic data created using large-scale pre-trained generative models, commonly referred to as \"foundation models\"(Bommasani et al., 2021).\nBroadly speaking, data encountered during deployment that was not sampled from the distribution used to generate training data is considered OOD. OOD data represents a challenge to safe deployment of predictive models because they can make confident incorrect predictions, leading to actions which could have negative outcomes. Foundation models complicate the traditional definition of OOD slightly. These models are trained on extensive and diverse datasets, making the data generating process, and thus possible OOD inputs difficult to specify cleanly. Despite its diversity, the data generating process only samples a small portion of the input space, leaving many potential subspaces open for OOD contamination. This contamination can erode the calibration of predictive models trained using the foundation model as a base, representing a significant hazard for safe model deployment.\nPredictive model failures due to OOD inputs can be understood through the lens of typicality. The concept of typicality arises from information theory and codifies the difference between likelihood from a generative process, and the probability of generating a sample from that process with a"}, {"title": "RELATED WORKS", "content": "In discriminative machine learning, the assumption that inference data mirrors the training data distribution is foundational, yet often flawed. The occurrence of out-of-distribution (OOD) inputs can lead to misleadingly confident but incorrect predictions by models, posing significant reliability and safety concerns. Large neural networks are vulnerable to adversarial perturbations (Szegedy et al., 2013) and poor calibration (Guo et al., 2017), necessitating OOD detection. OOD detection methods are either supervised, using labels and examples to calibrate models or train them to identify OOD data (Liang et al., 2018; Hendrycks et al., 2019b; Meinke & Hein, 2020; Dhamija et al., 2018), or unsupervised, employing generative models to approximate the training data density $q(X)$ and determine prediction reliability via $q(Y|X)$, assuming OOD inputs have lower probability (Bishop, 1994).\nUnsupervised OOD detection methods typically use generative models to measure the likelihood of the data. (Bishop, 1994) assumed low likelihood would be observed on OOD data, but this may fail in high dimensionality (Choi et al., 2018; Nalisnick et al., 2019; Hendrycks et al., 2019b; Serr\u00e0 et al., 2019). Fixes include using WAIC (Choi et al., 2018), likelihood ratios (Ren et al., 2019), or typicality tests (Nalisnick et al., 2019), but these have limitations associated with high dimensional likelihoods. Morningstar et al. (2021) proposed the Density of States Estimator (DoSE). Inspired by principles from statistical physics, it leverages multiple summary statistics from generative models to distinguish between in-distribution and OOD data."}, {"title": "METHODOLOGY", "content": "We start with data $X_{i=1}^n = {x_i}$ that comes from a true but unknown distribution $p$, where each $X_i$ is an independent and identically distributed random variable from $p$ and each $x_i$ is an observation. In the context of out-of-distribution detection, we consider that the unseen data might come from a mix of the true distribution $p$ and an unknown confounding distribution $p'$ (which can be from an OOD benchmark or synthetic data generated using a model such as Stable Diffusion). The mixed distribution is denoted by $X \\sim \\alpha p(X) + (1 - \\alpha)p'(\\Upsilon)$, where $\\alpha$ is an unknown mixing parameter. Since both $\\alpha$ and $p'$ are unknown, we cannot directly obtain samples from $p$ or make any assumptions"}, {"title": "CREATING GENERALIZED NOVEL SUMMARY STATISTICS", "content": "We propose the following per-point metrics, based on previous work in manifold estimation inside representation spaces by Naeem et al. (2020b) and Kynk\u00e4\u00e4nniemi et al. (2019a) for capturing different facets of the generated samples. Let $\\mathbb{1}(\\cdot)$ be the indicator function, $S(x^\\prime; x_j^r = 1^m) = \\cup_{j=1}^mB(x^\\prime, NND_k(x_j))$, where $B(x, r)$ is a Euclidean ball centered at $x$ with radius $r$, and $NND_k(x_j)$ is the distance between $x_j$ and its k-th nearest neighbor in $x_j^r = 1^m$, excluding itself.\nPrecision per point is a binary statistic that indicates whether each test point is within the nearest neighbor distance of any reference point:\n$precision_{pp} = \\mathbb{1}(x^\\prime \\in S(x_j^r|j = 1^m)).$ \nA high overall precision indicates that the test samples are closely aligned, and similar to the reference data distribution.\nRecall per point is computed for each test point by counting the number of reference points within its nearest neighbor distance and dividing by the total number of reference points:\n$recall_{pp} = \\frac{1}{m} \\sum_{j=1}^m \\mathbb{1}(x^\\prime \\in B(x_j, NND_k(x_j))).$\nA high recall implies that the test distribution collectively covers a significant portion of the reference data distribution, i.e. that the test distribution contains diverse samples that represent the different regions of the reference data distribution.\nDensity per point is computed for each test point by counting the number of reference points within its nearest neighbor distance and dividing by the product of nearest and the total number of reference points:\n$density_{pp} = \\frac{1}{km}\\sum_{j=1}^m \\mathbb{1}(x^\\prime_i \\in B(x_i^r, NND_k(x_i))).$"}, {"title": "DEVELOP DECISION RULE", "content": "We use the summary statistics to develop an non-parametric density estimator as an anomaly detection model. First, we generate feature vectors for the reference data using self-supervised learning methods. We split the data into three parts: one-third for held-out testing, one-third as the reference distribution, and one-third as a test distribution that is drawn from the reference distribution. We calculate summary statistics for the second and third to understand what these statistics look like when the test data matches the real data (i.e. $P = Q$). We train One-Class SVM (Sch\u00f6lkopf et al., 2001), Gaussian Kernel Density Estimation (Parzen, 1962), and Gaussian Mixture Model (Reynolds et al., 2009) on the reference summary statistics. These models learn a decision boundary enclosing the typical set of the reference data distribution. We then test the models' ability to distinguish between a mix of held-out test and reference features by comparing their atypicality i.e. summary statistics to the reference distribution. By evaluating the models' performance on the test set, we assess their effectiveness in detecting OOD samples and distinguishing real from synthetic data distributions.\nTo assess the performance of the anomaly detection models, using an unseen part of the training real distribution and the generated data distribution, we use Area Under the Receiver Operating Characteristic Curve (AUROC), which measures the ability of the model to discriminate between normal and anomalous points across different decision thresholds, and False Positive Rate at 95% True Positive Rate (FPR@95) which indicates the proportion of normal points incorrectly identified as anomalies when the model correctly identifies 95% of the true anomalies.\nUnder certain theoretical assumptions, we can justify the effectiveness of our per-point metrics in distinguishing between in-distribution (ID) and out-of-distribution (OOD) data. Specifically, if the reference data ${x_i}_{i=1}^m$ and the test data ${x_j}_{j=1}^n$ are drawn from Gaussian distributions with the same covariance but different means (with a significant mean difference), the expected values of these metrics differ markedly between ID and OOD samples. For ID data, the expected per-point precision and coverage approximate $1 - e^{-k}$, the expected per-point recall is roughly $k/m$, and the expected per-point density is close to 1. In contrast, for OOD data, these expected values are near zero due to the large mean difference, which causes the test samples to fall outside the typical regions of the reference distribution. This substantial disparity provides a strong theoretical foundation for using these metrics as effective summary statistics for OOD detection. By computing these metrics for test samples and comparing them to the expected ID values, we can effectively identify anomalous data points."}, {"title": "BASELINE PERFORMANCE FOR OOD", "content": "Unsupervised Baselines : We train an ensemble of deep generative models on in-distribution data, validate on a heldout set to ensure no memorization, and compute DoSE scores on in-distribution and OOD test sets. We measure OOD identification performance and compare against several unsupervised baselines: single-sided threshold (Bishop, 1994), single-sample typicality test (TT) (Nalisnick et al., 2019), Watanabe-Akaike Information Criterion (WAIC) (Choi et al., 2018), and likelihood ratio method (LLR) (Ren et al., 2019). Given the best demonstrated unsupervised OOD detection results were in (Morningstar et al., 2021), we stick to using Glow models (Kingma & Dhariwal, 2018), where we use summary statistics: log-likelihood, log-probability of the latent variable, and log-determinant of the Jacobian between input and transformed spaces.\nSupervised Baselines: We also compare our approach against state-of-the-art methods in out-of-distribution (OOD) detection literature, selecting the best reported results on established benchmarks, regardless of the model architecture or techniques employed. Specifically, we consider results from NNGuide (Park et al., 2023), Virtual Logit Matching (Wang et al., 2022), and OpenOOD v1.5 (Zhang et al., 2024a). For OpenOOD v1.5, we select the top-performing entries from the leaderboard, which utilize a Vision Transformer (ViT-B) trained with cross-entropy loss, along with Maximum Logit Score (MLS) (Hendrycks et al., 2019a) and Relative Mahalanobis Distance Score (RMDS) (Ren et al., 2021) for OOD detection. To explore the potential of foundation models in OOD detection, OpenOOD also employs a linear probing of Dinov2 (Oquab et al., 2023b) in conjunction with MLS, however performance is demonstrated to be poor."}, {"title": "BASELINE PERFORMANCE FOR SYNTHETIC DATA DISTRIBUTIONS", "content": "Distribution Divergence: We compute pairwise distances and divergences between real and gen-erated feature distributions. First we start with KL Divergence which measures information loss when approximating P with Q Kullback & Leibler (1951): $D_{KL}(P || Q) = \\sum_{x \\in X} P(x) log(\\frac{P(x)}{Q(x)})$, and JS Divergence which is the symmetric KL divergence variant, comparing distributions with disjoint support Lin (1991): $D_{JS}(P || Q) = \\frac{1}{2}(D_{KL}(P || M) + D_{KL}(Q || M))$, $M = \\frac{1}{2}(P + Q)$.\nThe Wasserstein Distance on the other hand gives the minimum cost to transform one distribution into the other Rubner et al. (2000): $W(P, Q) = inf_{\\gamma \\in \\Gamma(P,Q)} E_{(x, y) \\sim \\gamma}[|x \u2212 y|]$, $\\Gamma(P, Q)$: joint distri-butions with marginals P, Q, while the Bhattacharyya Distance gives the similarity between P and Q, 0 (identical) to \u221e (separated) Bhattacharyya (1943): $D_B(P,Q) = \u2212 ln (\\sum_{x \\in X}\\sqrt{P(x)Q(x)})$\nCLIP-based Zero Shot Strong OOD Baseline: To establish baseline performance for detecting anomalous generated images directly from representations, we split the in-distribution reference features X extracted from any of the feature extraction model (e.g. CLIP) into training $X_{train}$ and testing $X_{test}$ sets and train all the same non-parametric density estimation models with the same hyperparameter tuning, including OCCSVM, KDE and GMM. We evaluate each model on a test set consisting of representations from held-out reference images $X_{test}$ and anomalous images $X_g$. This provides an strong initial assessment of how well these models can distinguish between reference and anomalous data based on, for e.g. the CLIP features."}, {"title": "RESULTS", "content": "Table 1 & 2 present the performance comparison between our proposed methods (Forte+SVM, Forte+KDE, and Forte+GMM) and various state of the art supervised and unsupervised techniques for out-of-distribution (OOD) detection techniques. Our methods consistently outperform techniques across all challenging dataset pairings, demonstrating their effectiveness in detecting OOD samples without relying on labeled data.\nTable 3 presents an ablation study investigating the impact of incorporating multiple representation learning techniques into the Forte framework, when detecting arbitrary superclasses from the Im-"}, {"title": "PERFORMANCE ON SYNTHETIC DATA DETECTION", "content": "To generate synthetic data for assessing distributional robustness, we first employ the Stable Diffusion Img2Img setting (Rombach et al., 2022), where a diffusion-based model can generate new images conditioned on an input image and a text prompt. We use the Stable Diffusion 2.0 base model and generate images with varying strength parameters (0.3, 0.5, 0.7, 0.9, 1.0) to control the influence of the input image on the generated output, essentially allowing our real distribution to be prior of controllable strength for the generated distribution. We also use the Stable Diffusion 2.0 text-to-image model to generate new images conditioned on the captions generated by BLIP(Li et al., 2022) for each real image from the reference distribution. This allows us to create images that are semantically similar to the real images but with novel compositions and variations. Finally, we also generate images directly from the class name associated with each real image (e.g., \"a photo of a {monarch butterfly}, in a natural setting\"). This provides a baseline for generating images that capture the essential characteristics of the class without relying on specific input images or captions. The image generation pipeline is implemented using the Hugging Face Transformers (Wolf et al., 2020) and Diffusers (von Platen et al., 2022) libraries, which provide high-level APIs for working with pre-trained models. Examples can be found in Figure 2 Figure 3 Figure 4\nFr\u00e9chet Distance (FD), $FD_\\infty$, and CMMD scores provide insights into the distribution shift between real and synthetic images, with generated images moving further away from the real distribution as diffusion strength increases. However, these distribution-level statistics do not provide information about individual images within the distribution, necessitating an OOD detection strategy. Tables 4 & 7 compare the performance of Forte+GMM against a strong CLIP-based baseline on the Golden Retriever and Volleyball classes from ImageNet. For the well-represented Golden Retriever class, Forte+GMM consistently outperforms the baseline across all image generation settings, achieving near-perfect AUROC scores and low FPR95 values for Img2Img with strength parameters 0.9 and 1.0, Caption-based, and Class-based image generation. Lower performance at lower strengths is due to images being too similar to the real distribution. The Volleyball class, part of Hard ImageNet (Moayeri et al., 2022), focuses on classes with strong spurious cues. Volleyballs rarely occur alone in ImageNet images, and generating images without appropriate priors results in mode collapse (see Figure 4). Forte+GMM surpasses the CLIP-based baseline in all image generation"}, {"title": "PERFORMANCE ON MEDICAL IMAGE DATASETS", "content": "Magnetic resonance imaging (MRI) datasets and models present a unique challenge in a high stakes scenario, making robust out-of-distribution (OOD) detection paramount. These datasets, typically acquired under specific study protocols, suffer from batch effects that hinder model generalization even when changes in protocols are minute (Horng, 2023). The problem is exacerbated by dataset homogeneity within studies and limited dataset sizes in clinical applications, making it impractical to train separate models for each batch. A single model cannot be robust to all MRI datasets of the same subject matter, even if they have similar acquisition parameters. Such datasets still carry enough differences to impact model performance.\nTo simulate this scenario, two public datasets are used for the experiments in Section 4.2: coronal knee MRI from FastMRI Zbontar et al. (2018); Knoll et al. (2020) with two subsets and Osteoarthritis Initiative (OAI) Nevitt et al. (2006) with three subsets. The acquisition parameters, including sequence and fat suppression are detailed in Table 8 and samples are shown in Figure 14. Treating the FastMRI dataset as in-distribution and assuming that models have been trained on them, Forte is used to determine the next course of action when confronted with the OAI dataset: 1) which subsets of the new dataset can be aligned with the existing subsets / models? 2) To what degree do these subsets diverge from each other? Using the FastMRI FS subset as in-distribution, the two OAI subsets (OAI T1 and OAI MPR) are tested for OOD detection. Similarly, the FastMRI NoFS subset is used as in-distribution and the OAI TSE subset is tested for OOD detection."}, {"title": "DISCUSSION", "content": "While just using precision and density as summary statistics would lead Forte to be considered single-sample tests, recall and coverage make our framework a two-sample test, as they take into account the relationship between each synthetic image and the entire set of real images. This also allows us to better understand the behavior of the generated images in relation to the real data distribution. This is usually not a problem in production, and benchmarks, since when you check whether a sample is out of distribution, you usually sample from the mixture probability distribution of in and out of distribution, which is what we really use here. Prior work has also operated in this paradigm, i.e. two sample OOD test such as the typicality test (Nalisnick et al., 2019).\nWhen selecting non-parametric density estimators to model typicality, it is important to consider the manifold properties. We observe that GMM excels with clustered data, especially with a bigger number of gaussians when operating with data from multiple possible classes, while KDE struggles with sharp density variations. OCSVM is robust to outliers and performs well in high-dimensional spaces, making it suitable for cohesive normal data."}, {"title": "CONCLUSION", "content": "Our work underscores the importance of developing robust methods for detecting out-of-distribution (OOD) data and synthetic images generated by foundation models. The proposed Forte framework combines diverse representation learning techniques, non-parametric density estimators, and novel per-point summary statistics, demonstrating far superior performance compared to state-of-the-art baselines across various OOD detection tasks and synthetic image detection tasks. The experimental results not only showcase the effectiveness of Forte but also reveal the limitations of relying solely on statistical tests and distribution-level metrics for assessing the similarity between real and synthetic data. We hope that as generative models continue to advance, strong test frameworks like Forte will play a crucial role in maintaining the reliability of ML systems by detecting deviating data, unsafe distributions, distribution shifts, and anomalous samples, ultimately contributing to the development of more robust and trustworthy AI applications in the era of foundation models."}, {"title": "APPENDIX : THEORETICAL PROPERTIES OF PER-POINT PRDC", "content": "Theorem C.1. Under the following assumptions:\nFeature Space: $X = \\mathbb{R}^D$, where D is the dimensionality.\nIn-Distribution (ID) Data:\nTraining Set: $X^{train} = {x_1, ..., x_{N_{train}}}$ with $x_i \\sim \\mathcal{N}(\\mu_{ID}, \\sigma^2 I)$.\nTest Set: $X^{test} = {x_1^\\prime, ..., x_{N_{test}}^\\prime}$ with $x^\\prime_i \\sim \\mathcal{N}(\\mu_{ID}, \\sigma^2 I)$.\nOut-of-Distribution (OOD) Data:\nTest Set: $X^{OOD} = {x_1^\\prime, ..., x_{N_{OOD}}^\\prime}$ with $x_i^\\prime \\sim \\mathcal{N}(\\mu_{OOD}, \\sigma^2 I)$, where $\\Delta = ||\\mu_{ID} - \\mu_{OOD}|| \\gg 0$.\nDistance Function: Euclidean distance $d(x, y) = ||x - y||_2$.\nk-Nearest Neighbors: For a point x, $NN_k(x)$ denotes its k-nearest neighbors in $X^{train}$.\nUsing previous definitions of Per-Point PRDC Metrics:\nPrecision per point (P(x')):\n$P(x^\\prime) = I[\\min_{x \\in X^{train}} d(x^\\prime, x) \\leq r_k(x)]$\nwhere $r_k(x)$ is the distance from x to its k-th nearest neighbor in $X^{train}$.\nRecall per point (R(x')):\n$R(x^\\prime) = \\frac{1}{N_{train}} \\sum_{x \\in X^{train}} I[d(x^\\prime, x) \\leq r_k(x)]$.\nDensity per point (D(x')):\n$D(x^\\prime) = \\frac{1}{k} \\sum_{x \\in X^{train}} I[d(x^\\prime, x) \\leq r_k(x)].$"}, {"title": "APPEND IX : BASELINES STATISTICAL TESTS", "content": ""}, {"title": "PRDC AS LOCAL TRANSFORMATIONS", "content": "A local transformation is a function applied to a vector in a space which depends not only on the vector itself but also on its neighboring vectors. This allows for contextual dependence, where the transformation depends on the local environment of the vector, therefore making it sensitive to local variations and patterns. This also allows the local transformations to be non-linear, allowing for complex manipulations that are not possible with global linear transformations. This means they can adapt to regions of the data space, capturing variations and features that might be missed by global transformations.\nThese are obviously very powerful, as evidenced by convolution kernels (depending on neighbour pixels), Graph convolutions, Laplacian smoothing, and wavelet transforms. However, they can be sensitive to parameters like kernel size, weights, and tuning is important to ensure robustness. PRDC metrics might look very different at different k values, for example."}, {"title": "KEY PROPERTIES AND CHARACTERISTICS OF THE FORTE ALGORITHM", "content": "At its core, Forte leverages locality, basing its per-point PRDC (Precision, Recall, Density, Coverage) metrics on local neighborhood structures. This approach makes the algorithm sensitive to local variations in data distribution. Simultaneously, Forte performs dimensionality reduction, compressing high-dimensional data (D \u226b 4) into a more manageable R4 space while preserving essential information for OOD detection. This reduction not only aids in mitigating overfitting but also improves generalization and computational efficiency.\nA key strength of Forte lies in its model-agnostic nature, allowing it to work with any feature extractor that provides meaningful representations in RD. Self-supervised models like CLIP, ViTMSN, and DINOv2 are particularly effective in this context due to their rich feature representations. The algorithm's non-parametric approach, which avoids assumptions about the data distribution's form, contributes to its flexibility and robustness. This is further enhanced by the tunable parameter k (number of nearest neighbors), which allows for balancing sensitivity to local structures with noise robustness.\nForte's effectiveness in OOD detection stems from its ability to capture both density (proximity of neighbors) and coverage (sample's position within the support of PID). By focusing on local neighborhood information, the algorithm can detect subtle discrepancies between in-distribution and OOD samples, particularly effective when OOD samples reside in low-density regions. The use of multiple metrics (precision, recall, density, and coverage) provides a holistic view of how each sample relates to the training data Xtrain, enhancing the algorithm's discriminative power. The local transform employed by Forte also confers robustness to feature space variability, an important consideration when working with different self-supervised learning models. By normalizing differences through a focus on relative distances within the feature space, Forte maintains consistency across varied feature representations. Empirical results presented in this paper have comprehensively demonstrated Forte's superior performance compared to traditional methods, showcasing how the combination of powerful feature extractors and the local transform leads to effective OOD detection."}, {"title": "APPENDIX: MEDICAL IMAGE DATASETS", "content": "In medical imaging research, studies are often done using one in-house dataset. Conclusions and models drawn from these studies are then applied to new data, with poor results. In particular, MRI datasets exhibit strong batch effects that prevent them from being in-distribution relative to each other"}]}