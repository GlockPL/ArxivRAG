{"title": "PSEUDO-PROBABILITY UNLEARNING: TOWARDS EFFICIENT AND PRIVACY-PRESERVING MACHINE UN-LEARNING", "authors": ["Zihao Zhao", "Yijiang Li", "Yuchen Yang", "Wenqing Zhang", "Nuno Vasconcelos", "Yinzhi Cao"], "abstract": "Machine unlearning\u2014enabling a trained model to forget specific data\u2014is crucial for addressing biased data and adhering to privacy regulations like the General Data Protection Regulation (GDPR)'s \u201cright to be forgotten.\u201d Recent works have paid little attention to privacy concerns, leaving the data intended for forgetting vulnerable to membership inference attacks. Moreover, they often come with high computational overhead. In this work, we propose Pseudo-Probability Unlearning (PPU), a novel method that enables models to forget data efficiently and in a privacy-preserving manner. Our method replaces the final-layer output probabilities of the neural network with pseudo-probabilities for the data to be forgotten. These pseudo-probabilities follow either a uniform distribution or align with the model's overall distribution, enhancing privacy and reducing risk of membership inference attacks. Our optimization strategy further refines the predictive probability distributions and updates the model's weights accordingly, ensuring effective forgetting with minimal impact on the model's overall performance. Through comprehensive experiments on multiple benchmarks, our method achieves over 20% improvements in forgetting error compared to the state-of-the-art. Additionally, our method enhances privacy by preventing the forgotten set from being inferred to around random guesses.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine unlearning, which focuses on eliminating the negative impact of specific data subsets\u2014such as biased, erroneous, or privacy-leaking instances -used in model training has emerged as a critical area of research. Its significance is increasing due to growing concerns about data privacy, legal requirements for data deletion, and the necessity for models to adapt to new information without complete retraining. Though the most straightforward approach is to retrain the model with a new dataset that excludes the data needing removal, this approach is computationally expensive and needs continuous access to the training set.\nThere are two main challenges in existing machine unlearning methods. On the one hand, they still face high computational time without retraining the model if they aim to maintain decent unlearning performance in two aspects: the efficiency of forgetting the specified data subset, and the need to maintain performance on the remaining data. For example, an existing machine unlearning work requires 4.30 seconds to forget 25 data samples, while retraining takes 4.21 seconds. The retraining process is even faster due to the unlearning method's complexity of the loss computation.\nOn the other hand, existing methods are vulnerable to privacy leakage attacks, where an attacker can infer which data is within the forgetting set from the post-unlearning models. This still violates the right to be forgotten, even though the model has been updated to remove the data. This vulnerability arises because existing methods typically require the model to perform poorly (i.e., have a high loss) on the forgetting set, making it easier to distinguish from the retraining set. We denote this vulnerability as privacy leakage, measured by Membership Inference Attacks (MIA) . There is a lack of existing work addressing privacy leakage in machine unlearning; to the best of our knowledge, Kurmanji et al. has considered this issue, but it is computationally costly as mentioned earlier."}, {"title": "2 RELATED WORK", "content": "The discussion on unlearning has been broadened to include two principle paradigms: exact unlearning and approximate unlearning. Exact unlearning mandates that the performance of a model, post-unlearning, should be indistinguishable from that of a model retrained in the absence of the forgotten data. In this vein, Brophy and Lowd, along with Schelter et al., applied exact unlearning methods specifically to random forest models. Similarly, Ginart et al. developed an exact unlearning technique for k-means clustering. Despite the efficacy and precision of exact unlearning approaches in diminishing the influence of specific data, they face significant constraints related to underlying assumptions and scalability issues, as highlighted by Xu et al.. In particular, these methods are unsuitable for models such as Convolutional Neural Networks (CNN) and Residual Networks (ResNet). To address this, Golatkar et al. introduced the concept of selective unlearning, aiming to achieve forgetting by adjusting model weights. Furthermore, Golatkar et al. proposed approximating the weights that would result from unlearning by using a linearization inspired by Neural Tangent Kernel (NTK) theory."}, {"title": "3 NOTATIONS AND PROBLEM DEFINITION", "content": "Consider a dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$, composed of $N$ data points, where each instance consists of an input feature vector $x_i$ and its corresponding label $y_i$. Let $f(\\cdot; w)$ represent a function implemented by a deep neural network, parameterized by the weights $w$. In this context, we are provided with a \u201cforget set\u201d $\\mathcal{D}_{fog} = \\{(x_f, y_f)\\}_{f=1}^{N_f} \\subset \\mathcal{D}$, consisting of $N_f$ instances extracted from $\\mathcal{D}$, as well as a \u201cretain set\u201d $\\mathcal{D}_{ret} = \\{(x_r, y_r)\\}_{r=1}^{N_r} \\subset \\mathcal{D}$ containing $N_r$ training samples. For simplicity, we assume that $\\mathcal{D}_{ret}$ is the complement of $\\mathcal{D}_{fog}$, satisfying the condition $\\mathcal{D}_{fog} \\cup \\mathcal{D}_{ret} = \\mathcal{D}$ and $N_f + N_r = N$, thereby covering the entire original dataset.\nThis formulation sets the foundation for exploring methods capable of effectively \u201cunlearning\u201d the specified $\\mathcal{D}_{fog}$ from the original model, ensuring that the resulting model's performance is primarily influenced by the data in $\\mathcal{D}_{ret}$. The goal of deep machine unlearning is to derive a new set of weights, $w_u$, such that the updated model, $f(\\cdot; w_u)$, effectively \u201cerases\u201d the information related to $\\mathcal{D}_{fog}$. This process should be carried out without compromising the model's utility, as demonstrated by its performance on $\\mathcal{D}_{ret}$ and its ability to generalize to unseen data. We propose an optimization technique that refines the model's last layer output predictive probability distribution to efficiently achieve unlearning objectives."}, {"title": "4 METHODS", "content": "Building on the foundational framework, we propose an approach for deep machine unlearning that leverages pseudo-label optimization. In this method, we construct a matrix where each column represents a class, and each row corresponds to the probability of a data point belonging to each class. If the training dataset contains N data points and k classes, the resulting matrix has dimensions Nxk. For instance, in a neural network trained on the CIFAR-10 dataset with 50,000 training images, the matrix would have 50,000 rows and 10 columns. This matrix serves as the foundation for refining the model's predictions and optimizing the unlearning process.\nWe define the output probabilities as p, where each p vector has a length of k for each data point. Specifically, we assign the probabilities for the forgotten data as $p_f$ and for the retained data as $p_r$. Thus, each cell in the matrix represents the probability of a data point belonging to a particular class, consisting of values from both $p_f$ and $p_r$. For a data point $x_1$ in the forget set, the probability of $x_1$ belonging to class k is denoted as $p_{f1}(k)$. Here, we define $f(x; w)$ as the output probability generated by the input $x$ when passed through the model with weights $w$. Additionally, $f_k(x_f; w)$ represents the probability of the forgotten data point $x_f$ belonging to class k.\nThe core of our method lies in the formulation of an optimization objective tailored to adjust the model's output distribution in such a manner that it effectively \u201cforgets\u201d the information related to the forget set $\\mathcal{D}_{f}$, while maintaining or even enhancing its performance on the retain set $\\mathcal{D}_{r}$. To this end, we introduce an optimization objective designed to measure the discrepancy between the desired output distribution and the one currently produced by the model with original weights $w$."}, {"title": "4.1 PSEUDO-PROBABILITY REFINEMENT FOR DEEP MACHINE UNLEARNING", "content": "In the proposed formulation, $\\{p_{fi}\\}_{i=1}^{N_f}$ represents the set of pseudo-probabilities for the forget set. To ensure the model forgets this data, we can either set the pseudo-probabilities to a uniform distribution or generate them randomly. These strategies help to \u201cmask\u201d or obscure the model's previous knowledge about the forget set, making it harder for the model to retain those associations with the forget set, thereby improves the precision and effectiveness of the unlearning process. While the pseudo-probabilities for the forget set $p_f$ are adjusted to obscure the model's learned associations with $\\mathcal{D}_{fog}$, the probabilities for the retain set $\\{p_r\\}$, are $f (x_r; w)$ to ensure consistency with the model's knowledge of $\\mathcal{D}_{ret}$.\nDuring optimization, $\\hat{p}_r$ evolves from the initial model output $f (x_r, w)$ into a distribution that better reflects the knowledge the model should retain. The objective function incorporates a KL-divergence term for both the forget set and the retain set, weighted by the parameter $\\lambda$. This ensures a balanced approach to managing knowledge across both sets. The aim is to reach a state of neutrality or ignorance for the forget set, while ensuring the output distribution of the retain set aligns closely with the target distribution. We assume that the total probability for class k across all data points is $M_k$, and we strive to maintain this constant, regardless of any changes in the probabilities. This constraint prevents the unlearning process from excessively distorting the model's ability to forget the specified data while retaining knowledge about the retained set.\n$\\min \\limits_{\\{f(x_f;w)\\}_{f=1}^{N_f}, \\{f(x_r;w)\\}_{r=1}^{N_r}} \\sum_{f=1}^{N_f} D_{KL}(f(x_f;w)||p_f) + \\lambda \\sum_{r=1}^{N_r} D_{KL}(f(x_r; w)||p_r)$ (1)\nsubject to $\\sum_{f=1}^{N_f} f_k (x_f; w) + \\sum_{r=1}^{N_r} f_k (x_r; w) = M_k, \\forall k$, (2)\n$\\sum_{k=1}^{K} f_k(x_f; w) = 1, \\forall f, \\sum_{k=1}^{K} f_k (x_r; w) = 1, \\forall r$, (3)\n$f_k(x_f; w) \\in [0, 1], \\forall f,k, f_k(x_r; w) \\in [0,1], \\forall r,k$. (4)\nAdditionally, within the constraints, we must ensure that the sum of the probabilities across all classes for each data point equals 1. Furthermore, the probabilities should be bounded between 0 and 1, meaning they must be greater than or equal to 0 and less than or equal to 1.\nThe constraints guarantee that the pseudo-labels for both sets adhere to predefined distributions and form valid probability distributions over class labels. This optimization strategy thus offers a comprehensive framework for managing the objectives of unlearning and adaptive retention within a machine learning model."}, {"title": "4.1.1 CONVERGENCE TO THE UNIQUE OPTIMAL SOLUTION", "content": "To address computational efficiency, particularly for large datasets, we adopt an iterative solution reminiscent of coordinate ascent algorithms applied to the Lagrangian dual of our problem.\nTheorem 1 The proposed iterative procedure for the optimization problem described in ( 1) converges to the unique optimal solution, provided that feasible initial conditions are used and the total KL divergence remains finite for all feasible pseudo-labels."}, {"title": "The Kullback-Leibler (KL) divergence, $D_{KL}(p||q)$, is a well-known convex function in p when q is fixed. The optimization objective function is a sum of convex KL divergence terms. Consequently, the entire objective function is convex. Since the optimization problem consists of minimizing a convex function subject to linear constraints, the problem is a convex optimization problem. Convexity ensures that there is a unique global minimum.", "content": "The iterative algorithm begins with feasible initial conditions, where the $f(x_f; w)$ and $f(x_r; w)$ satisfy the constraints (2) (3) (4). These feasible initial conditions guarantee that the optimization process starts in the valid region and remains within this region during the optimization. Because the objective function is convex and the constraints are linear, the iterative procedure will converge to the global optimal solution. Strong duality ensures that the primal and dual solutions will converge to a common point, satisfying both the objective function and the constraints.\nThe uniqueness of the solution follows from the strict convexity of the KL divergence and the linear constraints. Therefore, the iterative procedure converges to the unique global minimum, as guaranteed by the structure of the problem.\nGiven this property, we can choose an initialization that is close to the pseudo-probabilities. Starting from a point near the optimal solution significantly reduces the number of iterations required for convergence. This improves computational efficiency by reducing the overall cost of optimization, while still guaranteeing that the solution is optimal."}, {"title": "4.1.2 INTEGRATION INTO DEEP MACHINE UNLEARNING", "content": "Moreover, our approach can be combined with other unlearning methods. After an initial unlearning phase conducted using existing techniques, our post-processing step can further refine the model's output distribution, ensuring that the unlearning is both comprehensive and efficient."}, {"title": "4.2 PROOF FOR OPTIMIZATION STRATEGY", "content": "We now provide a detailed mathematical proof to establish the connection between the optimization strategy for model unlearning and the adaptive post-learning method, using Lagrangian duality and iterative coordinate ascent."}, {"title": "4.2.1 LAGRANGIAN DUAL FORMULATION", "content": "Consider the optimization problem where the goal is to find the refined probabilities $f_k(x_f; w)$ and $f_k (x_r; w)$ for the forget and retain sets, respectively, to minimize the objective function (1).\nThe objective is to adjust the model outputs $f (x_f; w)$ and $f(x_r; w)$ such that the pseudo-labels for the forget set $f (x_f; w)$ obscure the model's learned associations while ensuring that the retain set pseudo-labels $f(x_r; w)$ are aligned with the model's original predictions.\nTo handle the class distribution constraints, we introduce dual variables $\\alpha_k$ associated with the class distribution constraint for each class k and define the Lagrangian as follows:\n$\\mathcal{L}(f(x_f; w), f(x_r; w), \\alpha) = \\sum_{f=1}^{N_f} D_{KL} (p_f || f(x_f; w)) + \\lambda \\sum_{r=1}^{N_r} D_{KL}(p_r || f(x_r; w))$ \n$+ \\sum_k \\alpha_k (\\sum_{f=1}^{N_f} f_k (x_f; w) + \\sum_{r=1}^{N_r} f_k (x_r; w) - M_k )$ (5)\nThe Lagrangian formulation allows us to handle the constraints directly by incorporating them into the objective function using the dual variables $\\alpha_k$. Given the convexity of the objective function, strong duality holds, meaning that the optimal solution can be found by solving the Lagrangian dual problem."}, {"title": "4.2.2 SOLUTION VIA COORDINATE ASCENT", "content": "The coordinate ascent method can now be applied to solve the optimization problem. The dual variables $\\lambda_k$ are updated iteratively to ensure that the class distribution constraints are satisfied. For each iteration, the primal variables $f (x_f; w)$ and $f (x_r; w)$ are updated to minimize the Lagrangian, followed by updates to the dual variables $\\lambda_k$ to satisfy the class constraints.\nThe primal and dual updates can be written as:\n$f_k(x_f; w) = A_{f,k}e^{-\\frac{\\alpha_k}{\\omega}}$ , $f_k(x_r; w) = A_{r,k}e^{-\\frac{\\lambda\\alpha_k}{\\omega}}$ (6)\nwhere $A_{f,k}$ and $A_{r,k}$ are the initial probabilities.\nThe dual variable update follows:\n$\\alpha_k^{(t+1)} = \\alpha_k^{(t)} + \\eta (\\sum_{f=1}^{N_f} f_k(x_f; w) + \\sum_{r=1}^{N_r} f_k(x_r; w) - M_k)$ (7)\nwhere $\\eta$ is the step size."}, {"title": "4.3 CHANGE THE WEIGHTS", "content": "After updating the probabilities, we adjust the model's weights accordingly by using the KL divergence as the loss function to calculate the loss."}, {"title": "5 EXPERIMENT", "content": "In this section we conduct several experiments to demonstrate the superior performance of our PPU method."}, {"title": "5.1 DATASETS AND METRICS", "content": "In this study, we employ two distinct datasets that were also used in prior research: CIFAR-10 and Lacuna-10. Lacuna-10 is a curated dataset formed by selecting data from 10 distinct classes, randomly chosen from the extensive VGG-Face2 dataset. These selected classes each have a minimum of 500 samples, with the data further segmented into 400 training and 100 testing images per class. Lacuna-100 expands on this concept by selecting 100 classes with the same criteria. Our evaluation metric focuses on the model's accuracy, specifically assessing its performance on both the forget set and the retain set to evaluate memory retention. Additionally, we measure the model's resistance to membership inference attacks for the privacy task."}, {"title": "5.2 IMPLEMENTATION DETAILS", "content": "To facilitate a comprehensive comparison with the performance of other models, we follow the setup in . We establish two experimental conditions: small-scale and large-scale. The small-scale setting, referred to as CIFAR-5/Lacuna-5, involves a subset of 5 classes from each dataset, comprising 100 training, 25 validation, and 100 testing samples per class. Notably, the forget set includes 25 samples from the initial class, accounting for 5% of the dataset. Conversely, the large-scale setting encompasses all classes from both CIFAR-10 and Lacuna-10, providing a broader spectrum for analysis. In the large-scale scenario, we will explore both class unlearning and selective unlearning. For class unlearning, we define the forget set as the entirety of the training set for class 5, which constitutes 10% of the data. In the selective unlearning scenario, we aim to forget 100 examples from class 5, representing 0.25% of CIFAR-10 and 2% of Lacuna-10.\nTo align with precedents in the field, our experiments will be conducted using two established architectures: ResNet-18 and ALL-CNN . The baseline model will be pretrained on CIFAR-100 and Lacuna-100 datasets for initial weight setting. Additionally, $\\Lambda$ will be set to a default value of 1 in the following experiments."}, {"title": "5.3 BASELINE", "content": "Our approach is benchmarked against the latest state-of-the-art methods and established baselines to highlight its efficacy: Retrain: This involves retraining the original model solely on the retain set $\\mathcal{D}_{r}$, considered the gold standard. However, this method is typically deemed impractical for real-world applications. Original: TThe baseline model trained on the complete dataset $\\mathcal{D}$, without any modifications for data forgetting. Finetuning: The original model is fine-tuned on the retain set $\\mathcal{D}_{r}$, incorporating no specific forgetting mechanism. NegGrad+ : An innovative method that applies gradient ascent to the forget set and gradient descent to the retain set over 500 iterations. Fisher Forgetting : Adjusts the model's weights to effectively \"unlearn\" the data meant to be forgotten, simulating a scenario where the model was never exposed to this data. NTK Forgetting Employs novel techniques like PCA-OGD to minimize forgetting by orthogonally projecting onto principal directions, preserving data structure integrity. CF-k, EU-k : These methods focus on the model's last k layers. \"Exact-unlearning\" (EU-k) re-trains these layers from scratch, while \"Catastrophic Forgetting\" (CF-k) fine-tunes them on the retain set $\\mathcal{D}_{r}$. SCRUB : Introduces a novel training objective and has demonstrated superior performance in prior metrics."}, {"title": "5.4 REMOVE BIAS", "content": "In addressing bias removal, our goal is to maximize the forget set error. Thus, instead of performing optimizations, we can directly modify the forget set probabilities to reflect pseudo probabilities. Additionally, we experimented with various distributions for the pseudo-probabilities, including uniform and random distributions.Specifically, the random distribution will be generated by applying the softmax function to randomly generated numbers. In general, the uniform distribution tends to perform better in terms of forget error, but it often leads to worse results in test error and retain error.\nAs shown in Table 1, our method PPU comes with a much higher forget error (60-80 percent higher), which is the desired outcome in unlearning scenarios. Table 2 also demonstrates that PPU achieves better performance in forget error for selective unlearning in larger models. Based on the forget error metric, our method appears to be the most successful at unlearning, achieving the desired outcome of complete forgetfulness without severely compromising the performance of the data that should be retained. Additionally, our method exhibits the lowest test error, demonstrating that the model's performance and generalizability are well-preserved even after applying our unlearning technique. The results demonstrate that using pseudo-probabilities is effective for bias removal."}, {"title": "5.5 PROTECT PRIVACY", "content": "To protect privacy, our goal is to ensure that the forget error remains close to that of retraining. For membership inference attacks, we adopt the approach outlined by Kurmanji et al.. Specifically, we train a binary classifier (the \"attacker\") using the losses of the unlearned model on both the forget and test examples, with the objective of classifying instances as either \"in\" (forget) or \"out\" (test). The attacker then predicts labels for held-out losses-losses that were not used during training-balanced between the forget and test sets. A successful defense is indicated by an attacker accuracy of 50%, signifying that the attacker is unable to distinguish between the two sets, demonstrating the effectiveness of the unlearning method.\nTo preserve privacy, we monitor both the training and retain accuracy at each epoch. As shown in Figure 2, the experiment on selective unlearning with ALL-CNN on CIFAR-10 reveals that the forget error gradually increases during training. Therefore, checkpoints are saved at each epoch, and the model closest to the original is selected. In this setup, pseudo-probabilities are initialized using a uniform distribution. According to Figure 3, PPU's forget error is very close to that of retraining, particularly in the Lacuna-10 experiment, where it is the closest match. In the membership inference attack experiment, shown in Figure 4, PPU consistently achieves nearly 50% accuracy, indicating strong privacy preservation. This demonstrates that, with the refinement of pseudo-probabilities, the model can maintain the original distribution while effectively forgetting the designated forget set."}, {"title": "5.6 COMPUTATIONAL EFFICIENCY", "content": "We compare the time required for SCRUB (Kurmanji et al., 2024), retraining, and our method, with all experiments conducted on an NVIDIA RTX-4090. Time is recorded over 5 runs, and we report both the mean and the standard error. In Figure 3, we present the time required for the bias removal tasks using the ResNet-18 model and selective unlearning using ALL-CNN. Compared to other methods, PPU significantly reduces computation time, cutting it to less than half of what is required by SCRUB. The results further emphasize the high effectiveness of the optimization approach and the use of pseudo-probabilities to fine-tune the model weights."}, {"title": "5.7 ADAPTIVE UNLEARNING", "content": "Our method can also be applied as post-processing after unlearning methods to enhance their results. PPU can be considered a plug-in that is compatible with nearly all existing methods. In our experiments, we built on SCRUB  and applied our method afterward. For the bias removal task, this approach improves forget error by more than 50%, with less than a 0.5% decrease in retain error. Detailed results can be found in Appendix A.1. In addition to SCRUB, we applied our method after fine-tuning on CIFAR-10 with a pretrained ResNet, achieving a 2.5% retain error and a 60% forget error. In comparison, the original fine-tuning method achieved only a 2% retain error and a 16% forget error."}, {"title": "6 ABLATION STUDY", "content": "In the optimization objective function (1), the value of $\\lambda$was set to 1 in all previous experiments. Here, we explore the impact of varying $\\lambda$ on the retain and forget errors in a small-scale unlearning experiment on CIFAR-5 with ResNet. As $\\lambda$ increases, more weight is assigned to the retain set, resulting in a decrease in retain error from 0.21% to 0%. However, this reduction comes at a significant cost to the forget error.\nFor investigation our method in a larger setting, we also conducted an additional experiment on the CIFAR-100 dataset with one class unlearning. Our method demonstrated very good performance. Using the ResNet architecture, SCRUB achieved a forget error of 5.19 and a retrain error of 0.00015. In contrast, our method achieved a retrain 031 error of 0.00 and a forget error of 98.25."}, {"title": "7 CONCLUSION", "content": "This research introduces a novel approach to machine unlearning, presenting an optimization framework that refines predictive probability distributions within deep learning models. Our method excels in striking an optimal balance between forgetting effectiveness and preserving model performance on retained data. Additionally, it demonstrates superior resilience against membership inference attacks. Empirical results across diverse datasets and model architectures, including CIFAR-10 and Lacuna-10 with ResNet and ALL-CNN, highlight the superiority of our approach over existing state-of-the-art methods.\nFurthermore, the operational flexibility, theoretical insights, and high computational efficiency of our approach provide a solid foundation for further developments. However, we acknowledge certain limitations. Our current method is limited to addressing unlearning in classification tasks and may encounter convergence issues during the optimization process. Additionally, the approach is restricted to supervised learning settings and does not extend to unsupervised tasks at this stage. Future work will focus on extending the method to various models, including large language models, and broadening its applicability beyond classification tasks."}, {"title": "A APPENDIX", "content": "Here there are more experiments and result tables to support our claims, especially in the bias removal task."}, {"title": "A.1 MORE RESULTS FOR ADAPTIVE UNLEARNING", "content": "Here, we present additional results for our method, applied as a post-processing step after SCRUB. For the bias removal task, our method significantly improves the forget error while having minimal impact on the model's original performance. The results are incorporated in Table 6, Table 7, Table 8, Table 9, and Table 10."}]}