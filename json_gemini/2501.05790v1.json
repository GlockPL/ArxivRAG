{"title": "UNDERSTANDING IMPACT OF HUMAN FEEDBACK VIA INFLUENCE FUNCTIONS", "authors": ["Taywon Min", "Haeone Lee", "Hanho Ryu", "Yongchan Kwon", "Kimin Lee"], "abstract": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn suitable reward models from human feedback to align large language models (LLMs) with human intentions. However, human feedback can often be noisy, in- consistent, or biased, especially when evaluating complex responses. Such feed- back can lead to misaligned reward signals, potentially causing unintended side effects during the RLHF process. To address these challenges, we explore the use of influence functions to measure the impact of human feedback on the perfor- mance of reward models. We propose a compute-efficient approximation method that enables the application of influence functions to LLM-based reward mod- els and large-scale preference datasets. In our experiments, we demonstrate two key applications of influence functions: (1) detecting common forms of labeler bias in human feedback datasets and (2) guiding labelers to refine their strategies to align more closely with expert feedback. By quantifying the impact of hu- man feedback on reward models, we believe that influence functions can enhance feedback interpretability and contribute to scalable oversight in RLHF, helping la- belers provide more accurate and consistent feedback.", "sections": [{"title": "1 INTRODUCTION", "content": "As large language models (LLMs) demonstrate remarkable capabilities across various domains, en- suring their behaviors align with human intentions becomes increasingly important. To this end, reinforcement learning from human feedback (RLHF) has emerged as a powerful solution for fine- tuning LLMs (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022). In RLHF, human feedback is collected to train reward models that capture important human values, such as help- fulness and harmlessness (Bai et al., 2022a; Ji et al., 2024). LLMs are then fine-tuned to produce outputs that closely align with these reward models.\nHowever, human feedback can often be noisy, inconsistent, or biased, especially when evaluating complex responses (Casper et al., 2023). This variability can lead to misaligned reward signals, potentially causing unintended side effects during the RLHF process. For example, feedback that favors supportive and enthusiastic responses might inadvertently lead the reward model to prioritize overly agreeable responses, which could result in sycophantic behavior (Sharma et al., 2023; Perez et al., 2022). This issue highlights the need for robust methods that precisely evaluate the impact of feedback on reward models, enabling humans to detect biased feedback and refine their feedback strategies more effectively.\nIn this work, we assess the impact of human feedback on reward models by utilizing influence func- tions (Hampel, 1974; Koh & Liang, 2017). However, a significant challenge arises when applying influence functions to reward models, especially large-parameter models like LLMs and those in- volving extensive preference datasets, due to the high computational costs involved. To address this, we introduce a compute-efficient method that utilizes vector compression techniques (Li & Li, 2023) alongside the influence estimation method (Kwon et al., 2024), achieving a 2.5-fold speed acceler- ation compared to previous methods in computing influence functions. This approach significantly reduces the computational costs required to compute influence functions, facilitating more practical applications in large-scale settings."}, {"title": "2 RELATED WORK", "content": "Influence functions Influence functions measure the impact of individual training data points on the resulting model and have been applied to various tasks, such as identifying influential data, detecting label errors, and interpreting model behavior (Koh & Liang, 2017; Guo et al., 2021; Kwon et al., 2024; Lin et al., 2024). Given their broad applicability to diverse tasks, we extend the use of influence functions to reward modeling, to measure the impact feedback has on reward models. A key challenge in this approach is the high computational cost of estimating influence. Building on recent advancements in efficient influence computation methods, which enables the estimation of influence functions even for LLMs (Kwon et al., 2024; Lin et al., 2024; Grosse et al., 2023), we apply influence functions to LLM-based reward models.\nScalable oversight As AI models become more powerful, reliably providing feedback on their behavior becomes increasingly challenging (Burns et al., 2024). For instance, humans struggle to accurately evaluate LLM-generated summaries of long passages as they cannot review entire source texts (Saunders et al., 2022). This challenge highlights the need for scalable oversight (Amodei et al., 2016; Bowman et al., 2022), where non-expert humans are required to provide feedback on complex outputs produced by advanced AI systems. A common approach to scalable oversight involves using capable AI models during the feedback process, either to assist humans (Saunders et al., 2022) or to replace them (Bai et al., 2022b; Cui et al., 2023). However, AI-assisted feedback processes can"}, {"title": "3 PRELIMINARIES", "content": "3.1 INFLUENCE FUNCTIONS\nThe influence function quantifies the impact of individual training data points on model param- eters by measuring the change in parameters in response to an infinitesimal adjustment in the weight of a specific data point (Hampel, 1974; Koh & Liang, 2017). To be more specific, we denote a parameter by $\\theta$, an associated parameter space by $\\Theta$, a loss function by $l$, a parame- terized model by $f_{\\theta}$, and a training dataset by $D$. The empirical risk minimizer $\\theta^*$ is defined as $\\theta^* := \\text{arg\\,min}_{\\theta\\in\\Theta} \\frac{1}{|D|}\\sum_{x\\in D}l(f_{\\theta}(x))$, and the $\\varepsilon$-weighted risk minimizer for a single training data point $x_i \\in D$ is defined as follows:\n$\\theta^{(\\varepsilon)}(i) := \\text{arg\\,min}_{\\theta\\in\\Theta} \\frac{1}{|D|}\\sum_{x\\in D}l(f_{\\theta}(x)) + \\varepsilon l(f_{\\theta}(x_i)).$ (1)\nThe influence function is defined as the derivative of $\\theta^{(i)}(\\varepsilon)$ at $\\varepsilon = 0$, capturing how fast the pa- rameter would change when the weight on $x_i$ is slightly changed. With the standard assumptions (e.g., twice-differentiability and strong convexity of a loss function $l$), the influence at training data point $x_i$ is expressed with the Hessian matrix of the empirical loss and the first-order gradient as follows (Cook & Weisberg, 1980):\n$I_{\\theta^*}(x_i) := \\frac{d\\theta^{(i)}(\\varepsilon)}{d\\varepsilon}|_{\\varepsilon=0} = -H(D;\\theta^*)^{-1}\\nabla_{\\theta} l_i|_{\\theta=\\theta^*},$ (2)\nwhere $H(D;\\theta) := (\\frac{1}{|D|}\\sum_{x\\in D} l(f_{\\theta}(x)))$ and $\\nabla_{\\theta} l_i = \\nabla_{\\theta} l(f_{\\theta}(x_i))$. In many recent machine learning applications, the focus has been extended beyond the model parameter to any univariate quantity of interest $f(\\theta)$, such as validation loss or a model prediction, leading to the following influence function via the chain rule of derivatives (Koh & Liang, 2017):\n$I_f(x_i) = -\\nabla_{\\theta} f(\\theta)|_{\\theta=\\theta^*}H(D;\\theta^*)^{-1}\\nabla_{\\theta} l_i|_{\\theta=\\theta^*}.$ (3)\nThe influence function $I_f(x_i)$ quantifies the impact of a training data point $x_i$ on $f(\\theta)$. Based on this derivation, it has been utilized in various downstream tasks such as detecting noisy labels (Koh & Liang, 2017; Pruthi et al., 2020; Guo et al., 2021) and interpreting model predictions (Han et al., 2020; Grosse et al., 2023).\n3.2 REINFORCEMENT LEARNING FROM HUMAN FEEDBACK\nRLHF is an effective technique for aligning LLMs with human preferences by incorporating human evaluations into the learning process. It has become increasingly standard due to its powerful capa- bility to generate human-like, helpful, and safe model outcomes (Bai et al., 2022a; Ouyang et al., 2022; Dai et al., 2024). Preference data in RLHF are often represented as a tuple of a prompt $x$, a pair of LLM responses $(y^{(0)}, y^{(1)})$, and a binary label $z \\in \\{0, 1\\}$ assigned by a human labeler to indicate the preferred response. For clarity, we introduce the notation $d := (x, y^{(0)}, y^{(1)}, z)$ to represent feedback data points. Such preference data are learned by minimizing the following cross-entropy loss based on the Bradley-Terry model (Bradley & Terry, 1952):\n$l_{pref}(d; \\theta) = - \\text{log}\\sigma(r_{\\theta}(x, y^{(z)}) - r_{\\theta}(x, y^{(1-z)})),$ (4)\nwhere $\\sigma(t) = 1/(1 + e^{-t})$ is the sigmoid function and $r_{\\theta}$ is a reward model parametrized by $\\theta$. Here, the reward model $r_{\\theta}(x, y)$ represents how well the LLM response $y$ aligns with human values"}, {"title": "4 METHOD", "content": "We describe our approach to applying influence functions in reward modeling. Section 4.1 intro- duces the formulation of influence functions for preference data. This provides rigorous insights into how human feedback influences a reward model's outcomes. Section 4.2 introduces a compute-efficient estimation method that enables the scaling of influence functions for large-scale datasets.\n4.1 INFLUENCE FUNCTIONS IN PREFERENCE-BASED REWARD LEARNING\nIn the standard RLHF framework, a reward function $r_{\\theta}$ is trained using a human-labeled dataset $D_{tr} = \\{d_i\\}_{i=1}^n$ to enhance the performance of LLMs (see Section 3.2 for more details about RLHF). We utilize influence functions to analyze the impact of this feedback on the behavior of the reward model. Formally, we assume the availability of a small validation set $D_{val}$ to evaluate the quality of reward functions. Using Equation 3, we compute the influence function for each training data point $d_i \\in D_{tr}$ to determine its contribution to the validation loss as follows:\n$I_{val}(d_i) := -\\nabla_{\\theta}L(D_{val}; \\theta) H_{pref}(D_{tr}; \\theta)^{-1}\\nabla_{\\theta}l_{pref} (d_i; \\theta),$ (5)\nwhere $l_{pref}(d_i; \\theta)$ is the preference loss defined in Equation 4, and $L(D_{val}; \\theta)$ is the aggregated loss on the validation set: $L(D_{val}; \\theta) = \\sum_{d_j\\in D_{val}} l_{pref}(d_j; \\theta)$. The terms $H_{pref}(D_{tr}; \\theta)$ and $\\nabla_{\\theta}l_{pref} (d_i; \\theta)$ are derived from Equation 2 by plugging-in the preference loss $l_{pref}$ to the gen- eral form. When the influence function $I_{val}(d_i)$ exhibits positive or negative values, it indicates an impact on increasing or decreasing the total validation loss $L(D_{val}; \\theta)$. We refer to $d_i$ with posi- tive values of $I_{val}(d_i)$, which harms the performance of $r_{\\theta}$, as negatively-contributing. Conversely, $d_i$ with negative values of $I_{val}(d_i)$, which improves the performance of $r_{\\theta}$, are called positively- contributing.\n4.2 EFFICIENT COMPUTATION\nComputing influence functions $I_{val}(d_i)$ is computationally expensive, primarily due to the calcula- tion of the inverse Hessian $H_{pref} (D_{tr}; \\theta)^{-1}$. The dimension of the Hessian matrix, which is deter- mined by the size of the model parameters $\\theta$, makes this computation infeasible for reward models based on LLMs. To address this issue, we utilize DataInf (Kwon et al., 2024), which approximates the inverse Hessian $H_{pref}(D_{tr}; \\theta)^{-1}$ as follows:\n$H_{pref} (D_{tr}; \\theta)^{-1} \\approx \\frac{1}{\\eta\\lambda} \\sum_{d\\in D_{tr}} (I - \\frac{\\nabla_{\\theta}l_{pref}(d; \\theta) \\nabla_{\\theta}l_{pref}(d; \\theta)^T}{\\lambda + \\nabla_{\\theta}l_{pref}(d; \\theta)^T \\nabla_{\\theta}l_{pref}(d; \\theta)});$ (6)"}, {"title": "5 EXPERIMENT", "content": "We design our experiments to investigate the following:\n\u2022 Can influence functions effectively detect length and sycophancy labeler bias in human feedback datasets? (Section 5.1)\n\u2022 Can influence functions guide labelers to refine and improve their labeling strategies? (Sec- tion 5.2)\n5.1 BIAS DETECTION USING INFLUENCE FUNCTIONS\nIn this experiment, we assess the effectiveness of the influence function in detecting biases within preference data. Specifically, we focus on two prevalent types of labeler bias: length (Saito et al., 2023) and sycophancy (Sharma et al., 2023). Length bias refers to the tendency of labelers to prefer longer responses under the belief that they are more informative or helpful, simply due to their verbosity, regardless of the actual content quality. Sycophancy bias is the tendency to favor responses that agree with the user or contain flattery, even when these responses are not accurate or helpful.\n5.1.1 EXPERIMENTAL SETUP\nDatasets We construct our training and validation sets using the helpful split of Anthropic's Helpfulness-Harmlessness (Anthropic-HH) dataset (Bai et al., 2022a), which was annotated by hu- mans who evaluated responses based on helpfulness, providing binary preference labels for conver- sations between a human and an assistant. To test the ability of influence functions to detect biased feedback, we synthetically generate biased samples in the training set by flipping preference labels. Specifically, we flip the labels in a subset of the training set to favor responses that are either lengthy, measured by token count, or sycophantic, assessed using scores evaluated by LLMs. This manipu- lation affects 6.56% of the labels for the length bias experiments and 4.17% for the sycophancy bias experiments. Each training set comprises 15,000 samples.\nAs noted in Remark 4.1, constructing a specific validation set is crucial for effectively utilizing influence functions. Therefore, we carefully design validation sets that contain unbiased samples for detecting biased feedback. Specifically, for the length bias experiments, we create a validation set with 2,629 samples, where the chosen responses are concise (i.e., both helpful and of short length), denoted as the Concise set. For the sycophancy bias experiments, we construct a validation set with 171 samples, consisting of chosen responses that are helpful and objective, without sycophantic behavior, denoted as the Less Sycophantic set. Details about both the training and validation sets are provided in Appendix B."}, {"title": "5.1.2 RESULTS AND ANALYSIS", "content": "Main results The ROC curves in Figure 2 demonstrate that our method, utilizing influence func- tions, significantly outperforms all baselines in detecting length and sycophancy biases. It achieves AUC values of 0.8 for length bias and 0.711 for sycophancy bias, compared to 0.6 for other threshold-based detectors. Our method also achieves a higher TPR than LLM-based detectors at equivalent FPR. Specifically, in length bias experiments, our detector outperforms GPT-40 by 5.3% and Gemini-1.5-Pro by 25.6%. For sycophancy bias, it exceeds GPT-40 by 14.8% and Gemini- 1.5-Pro by 11.9%. On average, our method identifies 14.4% more biased samples at a fixed FPR compared to LLMs, underscoring the effectiveness of influence functions.\nFurthermore, we note that length bias is easier to detect than sycophancy bias across all meth- ods. Detecting sycophancy bias poses greater challenges as it requires an understanding of context- dependent agreement with user opinions or notions of flattery, which is more complex than length bias. Despite these complexities, influence functions still prove highly effective in identifying sycophancy-biased samples, demonstrating their robust capability to detect complex labeler biases."}, {"title": "Qualitative analysis", "content": "In Figure 3, we present a qualitative analysis of the most positively- contributing and negatively-contributing samples for both length and sycophancy bias experiments. A clear difference in response verbosity is observed in the length bias experiment, with positively- contributing samples typically featuring brief chosen responses, compared to the lengthy and often less accurate chosen responses of negatively-contributing samples. In the sycophancy bias experi- ment, we notice a pattern where the chosen responses of positively-contributing samples are neutral or even disagree with human opinions, while the chosen responses of negatively-contributing sam- ples tend to overly sympathize or naively agree with humans. These qualitative examples underscore the efficacy of using influence functions to identify biased samples within the training set, offering valuable insights to labelers. For a more detailed analysis of these influential samples, please refer to Appendix G."}, {"title": "5.2 LABELING STRATEGY OVERSIGHT USING INFLUENCE FUNCTIONS", "content": "We also investigate whether influence functions can reliably guide non-expert labelers using expert feedback. We present a proof-of-concept experiment where the labeling strategies of non-experts and experts are differentiated by their priorities across multiple sub-objectives.\n5.2.1 EXPERIMENTAL SETUP\nWe provide an overview of our labeler strategy oversight experiment in Figure 6, which illustrates a scenario designed to model simulated labelers and their labeling strategies. In this experiment, each response is evaluated based on multiple fine-grained sub-objectives, such as correctness and verbosity. Labelers evaluate the overall score of a response using a weighted sum of sub-objectives, formulated as $r = w^T (r_1, r_2, r_3, r_4)$, where each $r_i \\in \\mathbb{R}$ represents a sub-objective score of a response. We assume that the sub-objective scores are consistent across labelers, but the weight vector $w \\in \\mathbb{R}^4$, which represents a labeler's strategy for prioritizing these sub-objectives, varies among them. To generate feedback, labelers determine the preference label $z$ by comparing the scores of two responses, $z = I(w^T r^{(0)} < w^T r^{(1)})$, where $r^{(0)}$ and $r^{(1)}$ are the sub-objective score"}, {"title": "5.2.2 RESULTS AND ANALYSIS", "content": "Main results As shown in Figure 7, using influence functions to update weights (blue bar) re- sults in significant improvements: label accuracy increases by 15.8%, reward model accuracy by 2.2%, and cosine similarity by 0.45, compared to the initial weights (gray bar). In contrast, the Mahalanobis and KNN baselines fail to identify discrepancies between Alice and Bob's labeling strategies, resulting in worsened performance across all metrics. This demonstrates that influence functions can effectively guide the non-expert, Bob, toward adopting Alice's expert labeling strategy, even with only a small validation set. Such results underscore the potential of influence functions in addressing the challenges of scalable oversight. By transferring Alice's expertise to Bob, we circumvent the need for large-scale, expert-level data collection, which is often challenging.\nTo further examine the impact of using a small validation set, we present performance metrics across different validation set sizes starting from 10 samples. Figure 8 shows that influence functions can accurately update Bob's weights even with just 50 samples, almost matching the label accuracy achieved with 400 samples. This can be particularly advantageous for complex labeling tasks, where collecting large amounts of expert-level data is challenging.\nLimitations We highlight several constraints in our experimental setup that may not extend to real- world settings. First, we use specific sub-objective scores to define labeler strategies, but assume that these scores are same across both labelers. In real-world scenarios, however, the sub-objective scores between experts and non-experts might differ, as they could assess identical sub-objectives differently. Also, our weight update strategy involves using all training samples and employing a support vector machine to determine new weights. In practical situations, non-expert labelers are unlikely to update their strategies based on all scores estimated by influence functions. More realistically, they might focus on refining their strategies using only a subset of the most and least influential samples. Despite these limitations, we believe that our proof-of-concept experiments provide meaningful insights into using influence functions to help labelers provide accurate feedback to reward models for complex tasks, contributing to scalable oversight."}, {"title": "6 CONCLUSION", "content": "In this work, we demonstrate the effectiveness of influence functions to measure the impact of human feedback on the performance of reward models. Our experiments verify that influence functions can detect complex labeler biases existing in preference datasets and can guide non-expert labelers toward experts. Given that feedback can be noisy or biased for complex tasks, addressing these biases is a critical problem. We believe that developing methods to identify and mitigate them is essential for advancing reliable AI systems. We hope our work contributes to the broader goal of scalable oversight (Amodei et al., 2016; Bowman et al., 2022), by improving our understanding of how feedback samples impact our models during RLHF."}, {"title": "A VECTOR COMPRESSION DETAILS", "content": "A.1 VECTOR COMPRESSION METHOD\nIn this section, we describe the vector compression method employed in our work: the one- permutation, one-random-projection (OPORP) technique (Li & Li, 2023). OPORP allows the com- pression of high-dimensional vectors to a predefined smaller size. By applying this method, we reduce the size of a single gradient vector from 160MB (corresponding to 42 million dimensions) to 256KB (equivalent to 65 thousand dimensions), facilitating the efficient storage of complete gra- dients even for large-scale preference datasets. The original gradient vector in our setup consists of 42 million dimensions, as we utilize Low-Rank Adaptation (Hu et al., 2022) to train our reward models.\nOPORP is a straightforward two-step method consisting of (1) permutation and (2) projection. In the first step, the gradient vector is permuted using a permutation matrix. Specifically, we implement the efficient permutation technique proposed in Lin et al. (2024), where the vector is permuted using multiple sub-permutations. In the second step, the permuted gradient vector undergoes element-wise multiplication with a projection vector, denoted as $p$, where each element $p_i$ is randomly sampled from $\\{-1, +1\\}$ with equal probability.\nAfter projection, the resulting vector is divided into equal-sized bins (with $2^{16}$ bins in our case), and the values within each bin are summed to form the final compressed vector. This permutation and projection procedure is applied uniformly across all vectors, ensuring that dot product values are preserved even after compression.\nOPORP allows us to efficiently store compressed gradient vectors for entire preference datasets using a manageable amount of storage.\nA.2 PERFORMANCE COMPARISON WITH DATAINF\nIn Table 2, we present a performance comparison between our proposed method and DataInf (Kwon et al., 2024). While our approach achieves a 2.5-fold decrease in time consumption compared to DataInf, it delivers comparable performance. This evaluation is conducted using the experimental setup detailed in Section 5.1, with performance assessed by measuring the AUC metric, as defined in Section 5.1. Additionally, we compute the Pearson correlation between the influence function values generated by DataInf and our method to evaluate their similarity in influence estimation further. DataInf and our method perform very similarly to each other both in influence function value and AUC, showing that our OPORP compression method preserves the gradient dot product values efficiently."}, {"title": "B DATASETS DETAIL", "content": "In this section, we describe the details of datasets used in our experiments including their sources and sizes.\nB.1 BIAS DETECTION\nWe use Anthropic's Helpfulness-Harmlessness dataset(Anthropic-HH) (Bai et al., 2022a) for bias detection experiments. This dataset was constructed by human labelers who evaluated responses based on helpfulness and provided binary preference labels z for conversations between a human and an assistant. Table 3 summarizes dataset information in this experiment.\nLength bias We randomly sampled 15k samples from Anthropic-HH-helpful dataset, the helpful split of Anthropic-HH dataset, where responses were evaluated regarding helpfulness. To inject the length bias, we inverted the preference label to always prefer the verbose response for 20% of the dataset by inverting the label when the chosen response had a shorter token length than the rejected response, which inverts 6.56% of the dataset. For a validation set, we use the validation split of the Anthropic-HH-helpful dataset consisting of 6121 validation samples. From this validation set, we construct a Concise subset by selecting validation samples where the chosen response is shorter in token length than the rejected response and conversely constructed the Verbose subset. The size of Concise and Verbose datasets are 2629 and 3492 respectively.\nSycophancy bias We randomly sampled 15,000 examples from the helpful-online split of the Anthropic-HH dataset, referred to as Anthropic-HH-helpful-online. We focused on this subset be- cause sycophantic behavior is more prevalent in LLMs that have undergone extensive RLHF train- ing. To introduce a sycophancy bias into the dataset, we measured the degree of sycophancy in each response. Using prompts, we asked Gemini-1.5-Pro (Reid et al., 2024) and GPT-40 (OpenAI, 2024) to generate sycophancy scores on a Likert scale from 1 to 5, then averaged the scores across the two models.\nIn cases where the chosen response was less sycophantic than the rejected one by a score difference of less than 1.5, we inverted the preference label, corrupting 4.17% of the dataset. For the validation set, we used the validation split of the Anthropic-HH-helpful-online dataset and created Less Syco- phantic and More Sycophantic subsets, where the chosen response was less or more sycophantic than the rejected one, based on reference sycophancy scores. The sizes of the Less Sycophantic and More Sycophantic datasets are 171 and 150 samples, respectively."}, {"title": "B.2 LABELING STRATEGY OVERSIGHT", "content": "We use the Helpsteer2 (Wang et al., 2024) dataset for the labeling strategy oversight experiment, which provides four different fine-grained objectives, correctness, coherence, complexity, and ver- bosity, measuring the score of LLM responses. We exclude the helpfulness score that Helpsteer2 provides and only consider the remaining 4 objectives. This is because this score rates the overall helpfulness of the response, compared to the other 4 criteria which measure specific sub-aspects of the helpfulness of the response (Wang et al., 2024). This makes the helpfulness score unnec- essary for our experiment motivation, as we want labelers to decide preferences based on fine- grained objectives. We use the training split of Helpsteer2 to construct Bob's training set DB, and the validation split of HelpSteer2 to construct Alice's validation set DA. Alice's optimal weight, $W_A$ = [1.04, 0.46, 0.47, -0.33], is adopted from the optimal weight of HelpSteer2 used on Reward- Bench evaluations (Lambert et al., 2024). For Bob's weight $W_B$, we construct five different weights for each subcriteria as $w^1$ = [1.1,1,3.1,3], $w^2$ = [2.1, 0.5, 4.9, 5.1], $w^3$ = [0.9,5.9, 2.1, 3.1], $w^4$ = [0.9, 6.1,5.8, 4.1], and $w^5$ = [0.2, 0.9, 0.9, 1.1]. The weights were randomly selected to en- sure Bob's initial labeling accuracy consistently ranged between 70-80%. To maintain diversity, overly similar weights were discarded, resulting in a well-distributed set that captures varied aspects of the sub-criteria. The size of DB and DA are 8218 and 423, respectively."}, {"title": "C BASELINES", "content": "C.1 LLM BASELINES\nIn this study, we employed large language model (LLM) baselines to evaluate performance across two specific bias-oriented tasks: length bias and sycophancy bias. The models used for these base- lines were GPT-40 and Gemini-1.5-Pro, both of which were queried using 3-shot learning examples. These baselines provided critical reference points to assess model bias and response quality in vari- ous conversation settings.\nFor each task, we designed few-shot prompts that include examples of conversations between a human and an AI assistant, followed by a comparison of two responses. One response was selected as the chosen answer based on helpfulness and honesty, while the other was rejected. The task for the models was to select the most helpful and honest response, along with a one-sentence explanation. The following process was implemented across both length bias and sycophancy bias experiments:\nIn the length bias experiment, the 3-shot examples consisted of conversations where the chosen response was deliberately more concise, depending on the prompt structure. The model was tasked with evaluating both the brevity and the quality of the content. Additionally, in the sycophancy bias experiment, the 3-shot examples included scenarios where the chosen response was factually accurate but less aligned with the user's opinion, while the rejected response exhibited sycophantic tendencies."}, {"title": "D SYCOPHANCY BIAS LABELING PROMPT AND DETAILS", "content": "Obtaining a reference sycophancy score A sycophancy score of responses is measured to con- struct the datasets used in our sycophancy bias experiment. We measure the sycophancy score of each response using GPT-40 (OpenAI, 2024) and Gemini-1.5-Pro (Reid et al., 2024), employing the assessment prompt from Prometheus2 (Kim et al., 2024). Through few-shot prompting, each response is assigned a sycophancy score ranging from 1 to 5. The scores from both LLMs are aver- aged to obtain a reference sycophancy score. This reference score is used to invert the binary labels, creating the sycophancy-biased dataset and to define the validation set Less Sycophantic."}, {"title": "E ADDITIONAL METRICS FOR BIAS DETECTION", "content": "In this section we report the area under the precision-recall curve (AP) and the TNR value at a fixed TPR of 0.8 (TNR80), along with precision-recall curves for both length and sycophancy bias. Ta- ble 8 and Figure 11 show that influence functions significantly outperform threshold-based baselines and LLM-based detectors in detecting labeler biases."}, {"title": "F ALICE AND BOB EXPERIMENT WEIGHT UPDATE METHOD", "content": "In this section, we describe how we leveraged influence values to improve the alignment between Alice's and Bob's labeling strategies. This process is detailed in Algorithm 1.\nInfluence-based partitioning Alice and Bob each label their respective datasets, Da and DB, using their weight vectors, wa and WB. For a given input xi, Bob evaluates two responses, y(0) and y(1), and computes scores wer(0) and wer(1). Bob's preference label zi is determined by whether wer(1) > wer(0), assigning zi = 1 if true, and zi = 0 otherwise.\nTo assess the alignment between Alice's and Bob's labels, we compute influence values Ival(di) using Alice's dataset Da as a reference. We set the threshold \u03b7 to the median of influence values {Ival(di) | di \u2208 DB}, ensuring that Bob's dataset DB is evenly split into two groups, where 50% of the data points with the highest influence values are considered likely to be mislabeled.\nTraining the SVM classifier For each sample in Bob's dataset DB, we compute the score dif- ferences ri = r(zi) \u2014 r(1-zi). These score differences represent how much better one response is compared to the other based on Bob's preferences. Samples are then partitioned according to the influence values: data points with Ival(di) > \u03b7 (likely mislabeled) are assigned label ti = 0, and those with Ival(di) \u2264 \u03b7 (correctly labeled) are labeled as ti = 1.\nWe then apply a linear Support Vector Machine (SVM) to the score differences and their corre- sponding labels. The SVM learns a new weight vector wsvm, which is designed to maximize the separation between high-influence (mislabeled) and low-influence (correctly labeled) data points, aiming to reduce Bob's mislabeling.\nCosine similarity and accuracy evaluation After training the SVM, we evaluate the alignment between Alice's and Bob's updated weight vectors. The cosine similarity between wa and WB is computed, as well as the cosine similarity between wA and wsvm (the SVM-derived weight vector). This helps us understand how closely Bob's updated labeling strategy aligns with Alice's after the influence-based update.\nWe further assess the accuracy of the labeling strategies before and after the update. Accuracy before the update is computed by checking how often Alice and Bob's original preferences agree on the same response. After applying the SVM classifier, we compute the accuracy again using the classifier's new weights wsvm. The improvement in accuracy shows how effectively the SVM has adjusted Bob's labeling strategy to be more aligned with Alice's."}, {"title": "G QUALITATIVE ANALYSIS", "content": "We analyze samples contributing both positively and negatively to length and sycophancy biases. The most positively-contributing and most negatively-contributing samples for each bias are sum- marized, with visual details provided in Figures Figure 12 and Figure 13.\nLength Bias Analysis To investigate length bias, we used the Concise dataset, focusing on cases where response length may affect outcomes. As shown in Figure 12, the most positively-contributing samples, which negatively impact bias"}]}