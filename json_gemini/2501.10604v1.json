{"title": "WHEN LANGUAGE AND VISION MEET ROAD SAFETY: LEVERAGING MULTIMODAL LARGE LANGUAGE MODELS FOR VIDEO-BASED TRAFFIC ACCIDENT ANALYSIS", "authors": ["Ruixuan Zhang", "Beichen Wang", "Juexiao Zhang", "Zilin Bian", "Chen Feng", "Kaan Ozbay"], "abstract": "The increasing availability of traffic videos functioning on a 24/7/365 time scale has the great potential of increasing the spatio-temporal coverage of traffic accidents, which will help improve traffic safety. However, analyzing footage from hundreds, if not thousands, of traffic cameras in a 24/7/365 working protocol remains an extremely challenging task, as current vision-based approaches primarily focus on extracting raw information, such as vehicle trajectories or individual object detection, but require laborious post-processing to derive actionable insights. We propose SeeUnsafe, a new framework that integrates Multimodal Large Language Model (MLLM) agents to transform video-based traffic accident analysis from a traditional extraction-then-explanation workflow to a more interactive, conversational approach. This shift significantly enhances processing throughput by automating complex tasks like video classification and visual grounding, while improving adaptability by enabling seamless adjustments to diverse traffic scenarios and user-defined queries. Our framework employs a severity-based aggregation strategy to handle videos of various lengths and a novel multimodal prompt to generate structured responses for review and evaluation and enable fine-grained visual grounding. We introduce IMS (Information Matching Score), a new MLLM-based metric for aligning structured responses with ground truth. We conduct extensive experiments on the Toyota Woven Traffic Safety dataset, demonstrating that SeeUnsafe effectively performs accident-aware video classification and visual grounding by leveraging off-the-shelf MLLMs.", "sections": [{"title": "1 Introduction", "content": "The extensive deployment of traffic cameras results in the daily accumulation of vast amounts of video footage. These recordings contain crucial information regarding traffic safety, such as vehicle conflicts, pedestrian-vehicle interactions, and other risk-related events. With advancements in computer vision technologies, it is now possible to extract this valuable safety-related data, offering transportation managers insights to enhance safety and mobility performance across various neighborhoods within expansive road networks. Prior works have explored utilizing publicly available video streams from over 900 traffic cameras throughout New York City for diverse urban management studies, including social distancing analysis Zuo et al. [2021], illegal parking detection Gao et al. [2022], work zone identification Zuo et al. [2023] and mobility monitoring Li et al. [2024a]. These initial efforts underscore the potential for more informative and effective urban traffic safety management through video and image analysis.\nWhile extensive camera coverage yields valuable traffic scene videos, thoroughly understanding these videos across spatial and temporal dimensions demands significant manual effort and traffic safety expertise. Even with automated image processing techniques like object detection and tracking, identifying recordings relevant to traffic safety analysis remains challenging due to the lack of comprehensive scene-level analysis that considers multiple contributing factors. Beyond merely extracting information from raw visual inputs, leveraging cameras within a large-scale road network for comprehensive accident analysis necessitates the integration of perception results from each camera and deriving safety insights from these distributed sources. The increasing availability of traffic video footage presents a significant opportunity for advancing traffic safety analysis. However, the rarity of safety-critical events, such as collisions and near-misses, buried within the vast volume of routine recordings poses a substantial challenge for transportation managers and researchers to work with. This disparity between the abundance of video footage and the rarity of safety-critical events highlights the need for an automated framework capable of efficiently extracting meaningful safety-related insights from massive amounts of data at scale. Without such a solution, fully utilizing traffic camera footage to enhance traffic safety remains an overwhelming and impractical task. Therefore, there is a pressing need to free experts from complex image processing tasks, allowing them to focus on minimizing life and property loss, which motivates us to find a solution that eases information extraction and supports the primary goal of improving road safety.\nThe emergence of Multimodal Large Language Models (MLLMs) has opened up exciting directions due to their ability to process complex data across multiple modalities (vision, sound, and more) and produce coherent, text-based outputs in a human-like logic. Before MLLMs, the potential of Large Language Models for transportation has been explored in motion planning Yang et al. [2023a], traffic rule understanding Zheng et al. [2023], and data management Zhang et al. [2024a]. However, these Large Language Models can only take text inputs and thus are incapable of tasks requiring visual inputs. Visual input is crucial for transportation tasks as it provides rich, real-time contextual information that is much more available than textual data. For example, there is no textual equivalent of a traffic camera feed or a driver's facial expression during a critical moment. Recently, researchers have used MLLMs with visual input processing to apply image captioning and video question-answering to transportation problems. Zhang et al. Zhang et al. [2024b] employ MLLMs to classify driving behaviors and provide risk assessment using images from in-cabin cameras. MLLMs also show remarkable performance in scene understanding and causal reasoning for autonomous driving [Wen et al., 2023, Cui et al., 2024] and traffic event analysis [Wang et al., 2024a, Abu Tami et al., 2024, Zhou and Knoll, 2024]. Although early efforts show the potential of multi-layered learning models (MLLMs) in understanding traffic scenes, they primarily concentrate on vehicle-centric reasoning tasks. These tasks are designed to enhance \"surrounding awareness\" for subsequent activities, such as motion planning, which are not directly applicable to traffic monitoring. In contrast, we leverage MLLMs to primarily assist traffic managers and researchers in efficiently processing large-scale video data with enhanced accessibility and reduced effort. Our motivation is straightforward: the more traffic accidents we find, the more we can analyze and the better we can prevent them in the future."}, {"title": "2 Related works", "content": "Large language models (LLMs), based on the Transformer architecture Vaswani et al. [2017], have demonstrated advanced reasoning capabilities and have been applied to autonomous driving Yang et al. [2023b], traffic safety question answering Zheng et al. [2023], and traffic data management Zhang et al. [2024a]. Multimodal Large Language Models (MMLMs) extend these capabilities to image and video data, focusing on tasks like captioning and question-answering. For example, DDLM Zhang et al. [2024b] uses a fine-tuned MLLM, LLaVA Liu et al. [2023a], to detect driver behaviors from in-vehicle images, while de Zarz\u00e0 et al. [2023] combine MLLMs for reasoning over time series and image data to enhance autonomous driving systems. Recent works Wang et al. [2023a], Zhou and Knoll [2024] investigate using ChatGPT-4V traffic accidents using sequential image inputs. However, existing studies largely focus on vehicle-centric surrounding awareness, and few explorations have been made for traffic accident analysis from a city management perspective. To address this gap, we propose SeeUnsafe, which leverages MLLM agents to enable user-friendly interactions for accident-aware video classification and identifying objects involved in critical events, empowering traffic managers with cutting-edge AI tools."}, {"title": "2.2 Vision-based traffic event analysis", "content": "The rapid advancement of computer vision technologies renovates the data collection strategies in the era of Intelligent Transportation Systems (ITS). Compared to traditional data sources of loop detectors Xu et al. [2013], Dia and Thomas"}, {"title": "3 Methodology", "content": "In this section, we introduce SeeUnsafe, an MLLM-integrated framework, to assist accident-aware video classification, as shown in Fig. 2. We first give the formal definition of our task in Section 3.1. Then, we illustrate two plug-and-play modules: severity-based aggregation in Section 3.2 and task-specific multimodal prompt in Section 3.3, respectively. Lastly, we show the limitations of the existing natural language metrics for structured responses in accident scene understanding and propose the MLLM-based Information Matching Score (IMS) in Section 3.4."}, {"title": "3.1 Problem statement", "content": "We formally define the accident-aware video classification problem discussed in this work. Denote an input traffic video of N frames as V = {I_n}_{n=1}^{N} and assume that the video contains exactly one type of event: a collision, a near-miss, or normal activity. We refer to collisions and near-miss as critical events and are constrained to occur exclusively between two road users. The goal is to classify each video into one of three categories: normal activity, near-miss, or collision, following the standard definition of video classification [Karpathy et al., 2014]. For videos classified as near-miss or collision, the secondary task is to identify the two road users involved, known as visual grounding [Peng et al., 2023]. The target outputs include a single video class label and the identities of the two involved road users. Labels 0, 1, 2 represent normal activity, near-miss, and collision, respectively. Multimodal Large Language Model (MLLM) agents are denoted as A, with prompts represented by P. When querying a video, the MLLM agents produce outputs such as the predicted class label \u0177, textual responses d, and object identities \u00f4. Superscripts are used to distinguish instances of the same symbol where needed. For clarity, we treat video and event as equivalents in this work and use the terms interchangeably in the following sections."}, {"title": "3.2 Severity-based aggregation", "content": "Capturing temporal dependencies in videos remains a significant challenge for MLLM agents, primarily due to the curse of dimensionality inherent in visual inputs, such as resolution and temporal range. This limitation is particularly pronounced in traffic monitoring applications, where accidents and near-misses are rare events embedded within vast amounts of daily footage. The natural idea is to split long videos into shorter clips to analyze them individually within the MLLM agents' capability. With possibly many shorter clips, some of them may be classified as critical events (collision or near-miss), while others may contain only normal activities. A simple majority voting approach [Hasan et al., 2024] may suppress rare critical events, and one should consider classifying videos by the most severe event across all clips, prioritizing collisions and near-misses. Based on this, we propose a severity-based aggregation method to fuse classification results from multiple clips, as illustrated in blue in Fig. 2. Without losing generality, we assume the existence of a splitting function g that divides the total N frames chronologically into K non-overlapping clips Ck such that:\n{C_k}_{k=1}^{K} = g({I_n}_{n=1}^{N})\n{I_n}_{n=1}^{N} = \\bigcup_{k=1}^{K} C_k\nC_m \\bigcap C_n = \\emptyset, \\forall m \\neq n\nC_{1,1} = I_1\nC_{K,|C_K|} = I_N\nwhere C_{k,j} indicate the j-th frame in the k-th clip. By sending K clips parallel into the MLLM agent A with prompt P, the set of output tuples is:\n{(\\hat{y}_k, d_k)}_{k=1}^{K} = {A(P, C_k)}_{k=1}^{K}\nWe use the label value as a proxy for event severity, with collisions (2) being the most severe and normal activity (0) being the least severe. The video V is classified based on the highest severity class among all clips. Descriptions from clips with the highest severity are aggregated using a fusion function h. The final classification label \u0176 and responses D for video V are then obtained as:\n\\hat{Y} = \\arg \\max({y_k}_{k=1}^{K})\nD = h({d_{k-:1(y_k = \\hat{Y})}}_{k=1}^{K})\nThis aggregation method integrates seamlessly with any user-defined splitting function g and fusion function h. For example, when K = 1, the entire video is processed by the MLLM agent A in a single pass, assuming token capacity permits. When K = N, the method aggregates N image-wise reasoning responses, degenerating to many existing image-based traffic scene understanding works. In our experiments, we use a uniform splitting function g and a heuristic fusion function h. The choice of g and h is a design art and is beyond the scope of this work. We provide further discussion in Section 5. Visual grounding for object identification involved in critical events occurs after the severity-based aggregation and will be detailed in the following Section 3.3."}, {"title": "3.3 Task-specifc multimodal prompt", "content": "Prompts are crucial in MLLM applications as they provide a prior that conditions the MLLM to generate posterior responses. A more informative prior leads to better posterior outputs. In this work, we aim not only to predict the video class but also to generate responses that justify the prediction\u2014an advantage MLLMs have over traditional video classification models. Like multimodal inputs enrich the information sources, prompts can also be in the format of multiple modalities that bring various priors [Khattak et al., 2023, Jiang et al., 2022]. The prompt used in this work consists of both textual and visual components. For the textual component, the design is guided by the question: \"What attributes can describe a traffic event?\" Traffic event descriptions can include several attributes: scene context (e.g., weather, road topology, lighting conditions), object information (e.g., types of objects in the video and their appearance), and justification (e.g., the explanation for classifying the event into a specific category). These attributes define the desired output responses from MLLM agents. Specifically, we ask the MLLM agent to generate structured responses across four predefined attributes: event class, scene context, object description, and justification. Structuring responses in this way leads to more precise descriptions and can slightly improve classification accuracy, as we will show in Section 4. Moreover, these structured responses facilitate attribute-based retrieval, as discussed in the use case in Section 1."}, {"title": "3.4 Information matching score", "content": "In addition to evaluating classification performance, it is crucial to assess the correctness of the output responses, as these responses may be used for decision-making and retrieval indexing. In the natural language processing (NLP) domain, metrics such as Bilingual Evaluation Understudy (BLEU) and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) are widely used to evaluate text quality. However, these metrics primarily focus on the similarity between predicted sentences and ground truth, often overlooking nuanced yet critical mismatches. This limitation makes them unsuitable for traffic accident analysis, where even a single word can drastically alter the justification or interpretation of an event.\nGiven the limitations of traditional NLP metrics in accident analysis, we propose a new metric, the Information Matching Score (IMS). Humans can easily identify critical errors while ignoring minor mismatches, understanding that some information (e.g., road topology, lighting conditions, object classes, and dynamics) is more important than others. However, due to the inherent flexibility of language, evaluating information matching across varied narratives is challenging. IMS leverages the information understanding and extraction capabilities of MLLMs to mimic human behavior when comparing texts in the context of traffic accident analysis. As shown in Prompt 1, we design an attribute-matching prompt to instruct the MLLM agent to compare the information-matching level between generated responses and ground truth for each attribute of scene_conetext, object_description, and justification. The MLLM then generates component-wise matching scores in a 100-scale to evaluate VLM output quality. To avoid the \"modal collapse\" phenomenon and enhance evaluation robustness, we perform T trials with a non-zero temperature \u03b7, which controls output randomness and creativity [Peeperkorn et al., 2024]: higher temperature values result in more random and creative outputs.\nLet the s\u0142 represent t-th trial of the l-th attribute and l represents l-th response attribute. IMS is obtained following:\nIMS = \\frac{1}{LT} \\sum_{l=1}^{L} \\sum_{t=1}^{T} \\alpha_l s_l^t\ns_l = A^{IMS} (\\hat{D}_l^t, D_l^t, \\eta, P^{IMS})\nwhere \u03b1 represents the weight of the l-th attribute,  A^{IMS} is the MLLM agent for generating IMS,  \\hat{D}_l is the l-th attribute in the generated response."}, {"title": "4 Experiments", "content": "We conduct experiments on the Toyota Woven Traffic Safety (WTS) dataset [Kong et al., 2025], a real-world collection focused on pedestrian-vehicle interactions, originally designed for video captioning tasks. The dataset includes 249 scenarios (167 in training and 82 in validation). In this work, results are reported on all 249 scenarios to explore the feasibility of accident-aware video classification and visual grounding. Each scenario provides one to four videos from overhead surveillance cameras and one video in-vehicle dash camera. During the data preprocessing, we manually select the best PoV from the overhead videos in one scenario. The ground truth consists of captions describing pedestrian and vehicle behaviors, which we summarize into four predefined attributes, event class, scene context, object description, and justification using GPT-40, followed by manual verification for accuracy.\nFollowing the mainstream setting for video understanding tasks [Goyal et al., 2017, Lin et al., 2019], we use 9 frames uniformly sampled from the videos and split them into 3 clips of 3 frames each. If multiple clips belong to the highest severity class, we select the response from the earliest clip. We compare three MLLM agents, LLaVA-NeXT [Li et al., 2024b], GPT-40 mini [OpenAI, 2024a], and GPT-40 [OpenAI, 2024b], and one non-MLLM model, VideoCLIP [Xu et al., 2021]. For reproducibility, the temperature is set to 0 for GPT-40 and GPT-40 mini, and 0.1 for LLaVA. We use Segment Anything and GroundingDINO with default hyperparameters, and modify the visual prompt generation process from [Wang et al., 2024c]. IMS is computed using a GPT-40 agent with a temperature of 0.5, repeated three times for average. Visual grounding is successful only if both road users are correctly indexed and any errors are considered a failure. \"VP\" stands for \"visual prompt.\" in this section."}, {"title": "4.2 Quantitative analysis", "content": "We compare SeeUnsafe, powered by a GPT-40 agent, with state-of-the-art MLLMs, including LLaVA, GPT-40, and a lighter version, GPT-40 mini. Models labeled with \"vanilla\" (e.g., GPT-40 (vanilla), GPT-40 mini (vanilla)) process all nine frames simultaneously without added visual prompts or splitting and use a simplified version of the SeeUnsafe prompt (excluding instructions for structured output). As shown in Table 2, the traditional VideoCLIP model achieves the lowest scores for video classification and cannot generate responses due to its design, highlighting the limitations of such vision-language pre-trained models for this task. During the experiment, we observed that while LLAVA-NeXTvideo could generate some responses, it consistently predicted the same video class regardless of the actual class. As a result, we excluded it from the accident-aware classification evaluation. Among the remaining models, GPT-40 (vanilla) performed second-best across most metrics, with its lighter version, GPT-40 mini (vanilla), performing slightly worse but still outperforming the other models. This trend highlights that overall performance improves with the reasoning capabilities of MLLMs. Notably, SeeUnsafe is the only one capable of performing visual grounding credit to the added visual prompt, while all other models fail at this task."}, {"title": "4.2.1 Alignment evaluation of IMS metric", "content": "To verify the alignment capabilities of IMS, we visualize the response evaluation results of SeeUnsafe for three attributes: scene context, object description, and justification. The scatter plots for BLEU and ROUGE-L show spatial clustering with uniformly distributed colors, whereas IMS values form a linear pattern from the bottom left to the top right, with brighter colors as points move upward. This indicates that the three attributes in the responses are positively correlated and collectively contribute to the final classification: better alignment in scene_context and object_description often leads to better alignment in justification, and vice versa. IMS captures this intuition more effectively, while BLEU and ROUGE-L provide less informative and sometimes misleading evaluations. We observe similar patterns when comparing IMS results for SeeUnsafe using MLLM agents of GPT-40 mini, LLaVA-NeXTinterleave and LLaVA-NeXTvideo, and note that stronger MLLM scorers (e.g., GPT-40 compared to GPT-4) produce stricter evaluations, while weaker scorers provide overconfident values. Additionally, visual prompts do not significantly affect the spatial or color distributions of the scatter points. A quantitative comparison in Table 3 highlights class-wise changes with and without adding visual prompts, corresponding to the centroids of each class. Across the three event classes, evaluation results remain consistent, and we discuss how visual prompts influence classification accuracy and response alignment in the next subsection."}, {"title": "4.2.2 Performance using different MLLM agents", "content": "For our SeeUnsafe framework, MLLM agents are the core components responsible for video understanding and response generation. Thus, comparing the framework's performance with different MLLMs is essential. As shown in Table 4, we evaluate performance with and without visual prompts. The results show that as the reasoning capability of the MLLM increases, overall performance improves, indicating that more powerful MLLM agents are preferred if resources permit. We also observe that adding visual prompts introduces a marginal performance drop with GPT-40 and a more significant drop with GPT-40 mini and LLaVA-NeXTinterleave. As mentioned in Section 3.3, visual prompts introduce \"noise\" by adding object boundaries, which is a type of noise hardly seen by these MLLM agents during the training process. Among these MLLM agents, GPT-40 is a larger and more robust model, and it can effectively handle the added boundaries and even utilize them for visual grounding tasks. Conversely, GPT-40 mini, with fewer parameters, is more sensitive to object boundaries, resulting in classification degradation, although its response evaluations remain largely unaffected. For LLaVA-NeXTinterleave, the larger performance drop suggests greater sensitivity to visual prompts, likely due to less training data compared to the GPT-40 series. The confusion matrices confirm these observations. The confusion matrix of the GPT-40 agent shows minimal changes before and after adding visual prompts. While the GPT-40 mini agent experiences classification degradation, it maintains a similar pattern, excelling in collision classification but frequently misclassifying normal events as near-miss events. In contrast, LLaVA-NeXTinterleave shows a more pronounced impact, with visual prompts causing most classifications to skew toward near-miss, regardless of the ground truth labels. Finally, the last column highlights the failure of LLAVA-NeXTvideo that uses all nine frames at a time, which produces meaningless results, classifying all collisions incorrectly."}, {"title": "4.2.3 Performance of visual grounding", "content": "To make a meaningful evaluation of visual grounding performance, we only consider those of collision and near-miss classes with at least three objects present to avoid trivial cases, totaling 136 videos. During manual validation, we find only 88 videos have valid masks for both involved objects and the remaining 48 have at least one involved object not correctly tracked due to failures in the detection and tracking models. However, we still report the success rate based on the total 136 videos containing critical events and note that the success rates can be easily boosted by using more robust detection and tracking models. As shown in Table 4, using the GPT-40 agent, SeeUnsafe successfully identifies the correct road users in 70 of the 136 videos, achieving a success rate of 51.47%, which can go up to 87.5% when considering only the 88 videos with valid masks. With using GPT-40 mini agent, the success rate shows a decline from 51.47% to 25.74% on 136 videos and from 87.5% to 39.78% on 88 correctly tracked videos, reflecting its reduced capacity for handling complex object interactions compared to GPT-40. Visualization of visual grounding outputs can be found in Section 4.3. These results highlight the importance of robust object tracking and GPT-40's effectiveness in leveraging visual prompts, enabling applications like extracting trajectories for objects involved in accidents without relying on handcrafted filtering criteria."}, {"title": "4.2.4 Correlation between classification accuracy and response alignment", "content": "We use the GPT-40 series within SeeUnsafe to explore the correlation between video classification accuracy and attribute-wise response alignment (against the ground truth). In Fig. 9, we compare IMS values of each attribute (scene context, object description, and justification) obtained using GPT-40 and GPT-40 mini agents. Each radar plot includes four contours: correct classification, correct classification with visual prompts, incorrect classification, and incorrect classification with visual prompts. For GPT-40, responses show better alignment when videos are correctly classified, and adding visual prompts minimally impacts the contour shape. In cases of incorrect classification, significant shrinkage is observed in the justification attribute for all classes, while scene_context remains largely unaffected. The object_description attribute shows inconsistent changes, decreasing for collision and near-miss videos but increasing for normal videos. For GPT-40 mini, the radar plot reveals greater shrinkage for correct classifications, especially when visual prompts are added, with the most noticeable decline in the normal class. Interestingly, IMS values for scene_context and object_description increase in some cases of incorrect classification. These findings highlight the nuanced interplay between video classification and response alignment and imply that generating the response in a structured format can be a useful attempt to improve the interpretability of MLLM outputs."}, {"title": "4.2.5 Performance under different lighting conditions", "content": "Input quality plays a critical role in vision-based tasks, with factors such as image resolution, noise, lighting, and lens distortion affecting performance. This applies to our problem as well. In the context of traffic videos mostly sourced from surveillance cameras, resolution and lens distortion are typically fixed for a given camera, but lighting conditions vary with time and weather. To evaluate the robustness of existing MLLM agents under different lighting conditions, we analyze performance during daytime and nighttime scenarios. In detail, we find all 19 nighttime scenarios (6 collisions and 13 near-misses) in the WTS dataset and compare them against 19 randomly sampled daytime scenarios (from the remaining 126) over 200 repeats. As shown in Table 5, classification accuracy and response alignment are generally better during daytime than nighttime, as expected. Adding visual prompts improves daytime performance for collisions and near-misses by clarifying object boundaries and enhancing spatial relationships. However, in nighttime scenarios, visual prompts degrade performance, likely due to added noise. One explanation is that nighttime inputs already contain white noise, and the additional prompts further obscure the images, making object identification more challenging."}, {"title": "4.3 Qualitative results", "content": "We provide qualitative examples for three video classes, one for each respectively. We start with a near-miss event in which a white sedan drives through an intersection and stops in time to avoid colliding with a male pedestrian. The first clip is classified as normal because the vehicle and pedestrian are not present in the FoV. In the second clip, while the objects (vehicle: 0 and person: 1) appear in the FoV, they are far apart, so it is also classified as normal. In the final clip, the vehicle approaches the pedestrian but stops at a safe distance, resulting in a near-miss classification. Using the severity-based aggregation method (Sec. 3.2), the video is classified as near-miss overall. The visual grounding results confirm that the involved objects are successfully highlighted with segmentation masks. Following the near-miss example, we present a collision scenario in Table 7. In this case, a white sedan approaches from behind and hits a pedestrian. Lastly, we present an example of a normal event, which constitutes the majority of daily traffic camera footage. These results indicate that SeeUnsafe can classify traffic events and identify road users involved in critical events."}, {"title": "5 Discussion", "content": "Although this work focuses on helping traffic managers process large volumes of traffic camera footage, the framework can also be applied in vehicles equipped with dash cameras."}, {"title": "5.1 From traffic camera to dash camera", "content": "Leveraging the Toyota WTS dataset, which includes in-vehicle footage, we present a case study to demonstrate the potential for using dash cameras to provide real-time surrogate safety measurements from the driver's perspective. This example suggests the potential for collaborative video understanding in multi-view settings, useful when object interactions are occluded in surveillance but visible from in-vehicle views."}, {"title": "5.2 Limitations", "content": "Our work is among the first to explore using MLLMs for risk-aware video classification and visual grounding to identify road users in critical events. However, as MLLMs are still in their early stages of solving complex vision tasks, this work has suffered from some limitations. Firstly, our work defines critical traffic events as collisions and near-misses, but identifying near-miss events is challenging. As shown in Fig. 6 and Fig. 8, near-misses are often confused with collisions, especially under occlusion or limited visibility from a single camera view. This limitation is common in vision-based tasks, where a single perspective provides only partial observations. Multi-view video footage can mitigate these issues by offering complementary perspectives. From a framework standpoint, the current splitting and aggregation functions are intuitive and could benefit from advanced techniques. For example, key frame selection methods could filter irrelevant footage, while robust aggregation methods could address information uncertainty. Addressing these limitations would significantly improve the robustness and reliability of the SeeUnsafe framework in real-world applications."}, {"title": "5.3 Broader impacts", "content": "The SeeUnsafe framework leverages Multimodal Large Language Models (MLLMs) to tackle the challenge of processing vast amounts of unprocessed traffic video data, which often goes overlooked due to limited processing capabilities. By enabling efficient analysis, SeeUnsafe helps uncover previously unidentified accidents and near-misses, enhancing situational awareness and contributing to improved road safety. This framework's adaptability allows deployment in vehicles as a real-time surrogate safety measure, providing instant insights into critical traffic scenarios. For vehicles equipped with dash cameras, SeeUnsafe offers a deeper understanding of driver behavior in high-risk situations, generating valuable data to improve driver training, refine traffic policies, and design safer road systems. By streamlining video analysis and situational awareness, SeeUnsafe has the potential to revolutionize traffic management and safety research, paving the way for a safer, more efficient, intelligent transportation system."}, {"title": "6 Conclusions", "content": "We present SeeUnsafe, a novel framework for accident-aware video classification and visual grounding that leverages Multimodal Large Language Models as the central agent for orchestrating these tasks. Empirical evaluations show that SeeUnsafe excels in traffic video understanding, offering a user-friendly, zero-shot, and scalable solution to process vast amounts of traffic footage without the need for extensive technical adaptation. SeeUnsafe is not only able to classify videos with 76.31% accuracy but is also capable of performing visual grounding at a success rate of 51.47%, which achieves the best performance compared to vanilla GPT-40 and other state-of-the-art models. This work marks an important step in leveraging MLLMs to transform unprocessed video data into actionable, safety-related insights for a range of applications. Looking ahead, several directions for improvement remain."}, {"title": "A Appendix", "content": "The complete prompts used for the experiments are provided as follows."}, {"title": "A.1 Complete textual prompts", "content": null}]}