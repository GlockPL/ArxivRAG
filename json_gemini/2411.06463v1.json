{"title": "RL-PRUNER: STRUCTURED PRUNING USING REINFORCEMENT LEARNING FOR CNN COMPRESSION AND ACCELERATION", "authors": ["Boyao Wang", "Volodymyr Kindratenko"], "abstract": "Convolutional Neural Networks (CNNs) have demonstrated exceptional performance in recent years. Compressing these models not only reduces storage requirements, making deployment to edge devices feasible, but also accelerates inference, thereby reducing latency and computational costs. Structured pruning, which removes filters at the layer level, directly modifies the model architecture. This approach achieves a more compact architecture while maintaining target accuracy, ensuring that the compressed model retains good compatibility and hardware efficiency. Our method is based on a key observation: filters in different layers of a neural network have varying importance to the model's performance. When the number of filters to prune is fixed, the optimal pruning distribution across different layers is uneven to minimize performance loss. Layers that are more sensitive to pruning should account for a smaller proportion of the pruning distribution. To leverage this insight, we propose RL-Pruner, which uses reinforcement learning to learn the optimal pruning distribution. RL-Pruner can automatically extract dependencies between filters in the input model and perform pruning, without requiring model-specific pruning implementations. We conducted experiments on models such as GoogleNet, ResNet, and MobileNet, comparing our approach to other structured pruning methods to validate its effectiveness.", "sections": [{"title": "1 Introduction", "content": "Convolutional neural networks (CNNs) have demonstrated outstanding performance across a range of computer vision tasks, including image classification, detection, and segmentation [Krichen, 2023, Bharadiya, 2023, O'Shea and Nash, 2015, Gu et al., 2018, Li et al., 2022]. As these architectures become wider and deeper, they gain an enhanced ability to extract complex features from input data. However, this increase in parameters leads to substantial computational costs, making inference both resource-intensive and slow. Moreover, there is a growing need to deploy CNNs on edge devices, which are often limited in computational power and memory [Rashid et al., 2022, Stahl et al., 2021]. Consequently, effective methods for compressing CNNs are increasingly in demand, aiming to reduce parameter counts and accelerate inference while maintaining the original model's performance. Such compression techniques are crucial for creating efficient and deployable CNNs that can meet the challenges of real-world applications.\nSeveral techniques have been proposed to compress CNNs, most of which fall into one of four categories: structured and unstructured pruning, quantization, low-rank factorization, and knowledge distillation [Cheng et al., 2020, Deng et al., 2020]. Structured pruning [Fang et al., 2023, He and Xiao, 2024, Anwar et al., 2017] removes entire filters from neural networks and directly modifies the model's architecture, enabling both compression and realistic acceleration on standard hardware. Unstructured pruning [Zhang et al., 2018, Ma et al., 2022], also known as weight pruning, removes specific unimportant elements from the weight matrix, which requires hardware or libraries that support sparse computation to achieve practical speedup. Low-rank factorization [Swaminathan et al., 2020, Idelbayev and Carreira-Perpinan, 2020] approximates the model's weight matrix by decomposing it into a product of low-rank matrices. Quantization [Wu et al., 2016, Gong et al., 2014, Zhou et al., 2017] reduces the bitwidth of the weight data in a model, achieving significant compression, but also requires hardware support to realize theoretical speedups for low-bit quantization, such as binary or ternary quantization. Knowledge distillation [Gou et al., 2021, Cho and Hariharan, 2019] transfers knowledge from a larger, more\nIn structured pruning, while it is crucial to identify the least important filters to prune from weight matrices with minimal performance degradation, it is equally important to assess the importance of different layers to the model's performance and determine which layers should be pruned more or less. This involves learning the sparsity distribution across layers. As shown in Figure 1, the importance of different convolutional and linear layers to the overall model performance varies significantly. Moreover, after a certain extent of pruning, the relative importance among different layers also changes. To leverage this insight, we can assign higher filter sparsity to less important layers and lower sparsity to more important layers, adjusting the sparsity distribution dynamically throughout the pruning process to minimize the performance drop.\nIn this paper, we introduce a novel approach called RL-Pruner. Unlike other structured pruning methods that learn the sparsity distribution among layers during sparse training [Liu et al., 2017a, Ding et al., 2021, Fang et al., 2023], RL-Pruner is a post-training structured pruning method that utilizes reinforcement learning with sampling to learn the optimal sparsity distribution across layers for several pruning steps. At each step, the current model architecture is defined as the state, while the pruning sparsity distribution acts as the policy. RL-Pruner adds Gaussian noise to the policy distribution to generate the real pruning sparsity distribution as an action, producing the corresponding compressed architecture as the next step's state. Each generation is treated as a sampling process. For each step, RL-Pruner maintains a replay buffer that stores the pruning sparsity distribution actions and their corresponding Q values, computed by the reward function. The reward function can be defined flexibly, such as based on the compressed model's test error, a combination of test error and parameter reduction ratio, or other criteria depending on the specific use case. After each pruning step, RL-Pruner updates the sparsity distribution policy to learn the optimal sparsity distribution. When computational resources allow, post-training stages are periodically applied after several pruning steps to recover any performance loss caused by pruning. We employ knowledge distillation in these post-training stages, with the original model acting as the teacher and the compressed model as the student.\nRL-Pruner can automatically extract dependencies between filters in different layers of the input model by tracking tensor computations. Currently, our approach supports several popular CNN architectures, including residual connections, concatenation connections and skip connections. As a result, our method does not require model-specific pruning implementations and can perform pruning autonomously, enhancing the method's generality.\nTo validate the effectiveness of RL-Pruner, we apply the proposed method to several popular CNNs used for image classification, including VGGNet [Simonyan and Zisserman, 2014], ResNet [He et al., 2016], GoogLeNet [Szegedy et al., 2015], and MobileNet [Howard, 2017], using the CIFAR-10 and CIFAR-100 datasets [Krizhevsky et al., 2009]. According to the experimental results, RL-Pruner achieves 60% channel sparsity for VGG-19 on CIFAR-100 and 40% channel sparsity for GoogLeNet and MobileNetV3-Large on CIFAR-100, all with a performance drop of less than 1%."}, {"title": "2 Related Work", "content": "Pruning Pruning is one of the mainstream neural network compression methods and has made impressive progress [Fang et al., 2023, He and Xiao, 2024, Anwar et al., 2017, Zhang et al., 2018, Ma et al., 2022]. Formally, given a neural network containing $L_c$ convolutional layers and $L_f$ fully connected layers, that is, $N = \\{W_l \\in \\mathbb{R}^{N_{out} \\times N_{in} \\times K_l \\times K_l}, 1 < l \\leq L_c\\} \\cup \\{W_l \\in \\mathbb{R}^{N_{out} \\times N_{in}}, 1 < l \\leq L_f\\}$, where $N_{out}, N_{in}$ denotes the output and input channel number of the $l$th layer respectively. Each layer contains $N_{l+1}$ filters, with each filter having a shape of $N_l \\times K_l \\times K_l$ for convolutional layers and a shape of $N_l$ for fully connected layers. Unstructured pruning removes individual weights, resulting in sparsified filter matrices, which do not lead to inference speedups unless specialized hardware is available. In contrast, structured pruning operates at a larger granularity, removing entire filters and achieving both compression and realistic acceleration on standard hardware. Several pruning criteria\nhave been proposed, including norm-based methods that prune weights or filters with the least norms [Li et al., 2017, He et al., 2018a] and activation-based methods that use the activation map to identify unimportant weights or filters [He et al., 2017, Lin et al., 2020a, Luo et al., 2017], and regularization-based methods that learn structured sparse networks by adding different sparsity regularizers [Hoefler et al., 2021, Liu et al., 2017b]. In this paper, we adapt the Taylor criterion, which combines the gradient of a weight, calculated from a calibration dataset, with its norm to determine the importance of weight filters.\nNeural Architecture Search Structured pruning removes filters and generates compressed subnetworks, which can be viewed as a form of neural architecture search (NAS). Since different layers in a model have varying importance to its overall performance, several works have focused on automatically assigning layer-wise sparsity. Among these, [He et al., 2018b, Yu et al., 2021, 2022] deploy reinforcement learning agents to automatically select the appropriate layer-wise pruning ratio by searching over the action space, i.e., the pruning ratio, without requiring manual sensitivity analysis. Gradient-based methods [Guo et al., 2020, Ning et al., 2020] modify the gradient update rule to make the optimization problem with sparsity constraints differentiable with respect to weights. Evolutionary methods [Liu et al., 2019, Lin et al., 2020b] use evolutionary algorithms to explore and search for sparse subnetworks. Our method primarily uses reinforcement learning to determine the optimal sparsity distribution across layers.\nKnowledge Distillation Pruning typically reduces a network's performance, so post-training is often employed to recover this loss. Knowledge distillation [Hinton, 2015] transfers knowledge from a larger teacher model to a smaller student model. Response-based knowledge distillation allows the student model to mimic the predictions from the teacher model's final output layer by adding a distillation loss between their predictions to the optimization function. Feature-based knowledge distillation, on the other hand, enables the student model to learn intermediate layer representations from the teacher model. In our method, we adopt response-based knowledge distillation during the recovery post-training stage, as pruning alters the dimensions of intermediate layers, making knowledge transfer between these layers less straightforward."}, {"title": "3 Method", "content": "RL-Pruner first builds a dependency graph between layers in the model, then performs pruning in several steps. In each step: 1) A new pruning sparsity distribution is generated as an action based on the base distribution, which serves as the policy; 2) Each layer is pruned using the Taylor criterion according to the corresponding sparsity; 3) The compressed model is evaluated to obtain a reward, and the action and reward are stored in a replay buffer. After each step, the base distribution is updated according to the replay buffer, and if computational resources are sufficient, a post-training stage is applied to the compressed model using knowledge distillation, where the original model acts as the teacher. Figure 2 illustrates our method.\n3.1 Build Dependency Graph\nDefinition Given a convolutional neural network $N = \\{W_l \\in \\mathbb{R}^{N_{out} \\times N_{in} \\times K_l \\times K_l}, 1 < l \\leq L_c\\} \\cup \\{W_l \\in \\mathbb{R}^{N_{out} \\times N_{in}}, 1 < l \\leq L_f\\}$, we define convolutional and linear layers as prunable layers $\\{L_i\\}, 1 \\leq i \\leq L_c + L_f$, while activation layers, and pooling layers are considered non-prunable layers. Batch normalization layers are also considered non-prunable, as they are only pruned if their preceding convolutional or linear layers are pruned. Formally, we establish the dependency between any two layers $L_i, L_j, i < j$ by,\n$DG_{(i,j)}(k_{out}) = [k_{in}],\\forall 1 < k_{out} \\leq N_{out} \\quad(1)$\nif a tensor flows from $L_i$ to $L_j$ with no intermediate layer between them. This means that if we prune a specific output channel $k_{out}$ in $L_i$, the corresponding input channel $k_{in}$ in $L_j$ must also be pruned. If there is no dependency between $L_i$ and $L_j$, we have:\n$DG_{(i,j)}(k_{out}) = [-1],\\forall 1 < k_{out} < N_{out} \\quad(2)$\nOur goal is to build a dependency graph $DG$ among all layers, and record the relationships among their channel index mappings.\nBuild Dependency Graph To achieve this, we fed the model an example input image and extract all layers $\\{L_i, 1 \\leq i \\leq L\\}$ along with their input tensor $\\{T_l^{in}, 1 < l < L\\}$ and output tensor $\\{T_l^{out}, 1 < l < L\\}$, where $L$ is the number of all layers. We then build the dependency graph by matching the input and output tensors of each layer. A basic dependency occurs when the output tensor of one layer $L_i$ is directly fed into layer $L_j$, which implies for basic dependency\n$DG_{(i,j)} (k_{out}) = [k_{out}],\\forall 1 < k_{out} < N_{out} \\quad(3)$\nWe first iterate over $\\{T_l^{in}\\}$ and $\\{T_l^{out}\\}$ to check whether any two tensors match, in order to detect all the basic dependencies. Next, we check for any unused input tensor, e.g. $T_b^{in}$, which may be caused by tensor modifications (e.g., flattening) or special connections between layers (e.g., residual or concatenation connections). Flattening is commonly used between convolutional and linear layers. To detect the use of tensor flattening, we also flatten all output tensors and compare them to the unused input tensors to build the dependency. Let us denote the detected output tensor as $T_b^{out}$, which represents the tensor before flattening. To determine the mapping relationship after flattening, we need to check the output area $A_{out}$ of the preceding layer that produces $T_b^{out}$. For flattening, we have\n$DG_{(i,j)} (k_{out}) = [k_{out} * A_{out}, (k_{out} + 1) * A_{out}]\\quad(4)$\nwhich indicates the output channel $k_{out}$ of $L_i$ and the input channel from $k_{out} * A_{out}$ to $(k_{out} + 1) * A_{out}$ must be\npruned simultaneously. To detect concatenation connections, we iterate over $\\{T_b^{out}\\}$ to check whether there exists $T_c^{out}, T_d^{out}$ such that\n$T_a^{in} = concat(T_c^{out}, T_d^{out}), 1 \\leq j \\leq L, j \\neq b \\quad(5)$\nThis can be detected by slicing $T_c^{out}, T_d^{out}$ over $T_a^{in}$ such that $T_a^{in}[i_b, i_b + N_{out}] = T_c^{out}$. We can then build the dependency as\n$DG_{(b,a)} (k_{out}) = [i_b + k_{out}] \\quad(6)$\nTo detect residual connections, we iterate over $\\{T_b^{out}\\}$ to check whether there exists $T_b^{out}, T_c^{out}$ such that\n$T_a^{in} = T_b^{out} + T_c^{out}, a \\neq b, a \\neq c, b \\neq c \\quad(7)$\nWe can then build the dependency as\n$DG_{(b,a)} (k_{out}) = [k_{out}] \\quad(8)$\n$DG_{(c,a)} (k_{out}) = [k_{out}] \\quad(9)$\nThis also applies to the Squeeze-and-Excitation Module, where\n$T_a^{in} = T_b^{out} * T_c^{out}, a \\neq b, a \\neq c, b \\neq c \\quad(10)$\nFor residual connections and the Squeeze-and-Excitation Module, tensors from layers $L_b$ and $L_c$ are either added or multiplied and then passed to $L_a$, meaning that pruning output channel $k_{out}$ of $L_b$ also requires pruning output channel $k_{out}$ of $L_c$ and vice versa. As a result, layers $L_b$ and $L_c$ must be pruned simultaneously. After constructing the dependency graph, we divide the layers that need to be pruned simultaneously into disjoint sets, ensuring that layers within the same set are pruned together.\n3.2 Assigning Layer-wise Sparsity\nAfter automatically extracting the layer dependencies and corresponding channel index mappings, we still need to determine how to distribute sparsity among layers for each pruning steps to minimize the performance drop caused by pruning while achieving the desired overall model sparsity. Let the pruning distribution $PD = \\{p_i \\in [0,1]^Z | \\sum_{i=1}^{L_c + L_f} p_i = 1\\}$ represent the sparsity distribution among prunable layers, where we assign sparsity $p_i * S$ to the $i$th prunable layer, with $S$ being the target overall model sparsity. Since we can only prune output channels in integer quantities, the optimization problem of finding the optimal $PD$ is not differentiable.\nTo address this, we adopt a Monte Carlo sampling strategy [Rubinstein and Kroese, 2016] combined with the Q-learning algorithm [Watkins and Dayan, 1992] from reinforcement learning. Specifically, Monte Carlo sampling is used to explore different pruning distributions, while Q-learning updates the policy, represented by the base pruning distribution $PD$. Thus, each pruning step consists of multiple sampling stages followed by a pruning stage.\nSampling We define each compressed model's architecture as the state and $PD$ as the policy. In each sampling, we add a Gaussian noise vector $z$ to $PD$ to generate the real pruning action $a$:\n$z = [z_1, z_2, ..., z_L], z_i \\sim \\mathcal{N}(0, v), \\forall i = 1, 2, . . ., L \\quad(11)$\n$a = PD + z \\quad(12)$\nwhere $v$ is the variance of Gaussian noise that controls the exploration volume around $PD$. A larger $v$ indicates greater exploration, allowing the pruning policy to explore a wider range of potential architectures. Let $s$ denote the current model architecture, then, inspired by Bellman equation [Bellman, 1966], we evaluate the Q value of each $a_i$ and the corresponding compressed model's architecture $s_i$:\n$Q(s, a_i) = \\mathbb{E} \\{r + \\gamma max_{a'_j} Q(s_i, a'_j) | s, a_i, 1 \\leq i, j \\leq N_s\\} \\quad(13)$\n$r = R(s) \\quad(14)$\nwhere $\\gamma$ is the discount factor and $N_s$ is the sampling number at each time step. To achieve the sampling efficiency, we sample $T$ time steps in each sampling stage to approximate the expectation in equation 13:\n$Q(s, a_i) \\approx \\frac{1}{T} \\sum_{t=1}^T r_t + \\gamma max_{a'(s_{t,i})} Q(s_{t,i}, a'_j), 1 \\leq i, j \\leq \\mathcal{N} \\quad(15)$\n$r_t = R(S(t,i)) \\quad(16)$\nThe reward function is determined by the compressed model's architecture, modeled by\n$R(s) = T_e(s) + \\alpha * C_F(s) + \\beta * C_p(s) \\quad(17)$\nwhere $T_e, C_F, C_p$ represent the test accuracy, FLOPs compression ratio, and parameter count compression ratio of the input model, respectively. The hyperparameters $\\alpha, \\beta$ can be set to prioritize either accelerating inference or reducing parameter numbers, depending on the specific objective.\nUpdate Policy After each sampling stage, we store each action $a_i$ along with its Q value $Q(s, a_i)$ in a replay buffer $RB$, replacing the entry with the lowest Q value in $RB$. If the lowest Q value exceeds $Q(s, a_i)$, the corresponding sampling data is discarded. We then select an action $a^*$ from the replay buffer to update the policy $PD$. To balance exploration and exploitation, we use an $\\epsilon$-greedy strategy: with probability $\\epsilon$, we select $a^*$ randomly from $RB$. Otherwise, we select the action with the highest Q value:\n$a^* = arg \\underset{a_i}{\\text{max}} RB(s, a_i) \\quad(18)$\nAs the model architecture is initially more redundant and becomes more compact through pruning, we use a high exploration $\\epsilon$ value during the initial pruning steps, gradually reducing $\\epsilon$ in the later steps. Several $\\epsilon$ decay strategies are considered, including constant, linear, and cosine decay. We then adjust the policy distribution toward the selected action distribution by a step size $\\lambda$. To ensure stability in the update process, we adopt the idea of proximal policy optimization (PPO) [Schulman et al., 2017], which constrains the change in the policy distribution:\n$PD_{new} = PD + \\lambda * a^* \\quad(19)$\n$PD_{new} = clip(\\frac{PD_{new}}{PD}, 1 - \\delta, 1 + \\delta) * PD \\quad(20)$\nwhere each element's change ratio is limited within $[1 - \\delta, 1 + \\delta]$. As a result, after several sampling stages, the policy $PD$ will converge toward an optimal pruning distribution across the layers.\n3.3 Layer Pruning\nNow that we have the dependency graph and the pruning sparsity for each layer, the next question is how to select which output channel within each layer to prune in order to minimize performance drop. For each convolutional layer $W_i \\in \\mathbb{R}^{N_{out} \\times N_{in} \\times K_l \\times K_l}$ and linear layer $\\mathbb{R}^{N_{out} \\times N_{in}}$ that containing a total of $N_{out}$ output channels, each corresponds to a weight matrix, we use a calibration dataset extracted as a subset from the training dataset to help determine which weight matrix is less important. According to [LeCun et al., 1989], the importance of the $i$th weight matrix $W_i$ in layer $L$ is given by:\n$I_{W_i} = |\\Delta L(D)| \\quad(21)$\n$= |L_{W_i}(D) - L_{W_i=0}(D)| \\quad(22)$\n$\\approx |\\frac{\\Delta L^2 (D)}{\\Delta W_i^2}| = |\\frac{1}{2} W_i^T H W_i + O(||W_i||^3)| \\quad(23)$\nwhere $H = \\frac{\\Delta L^2}{\\Delta W^2}$ is the Hessian matrix. Since computing the Hessian matrix requires $O(N^2)$ computational resources, we omit it to accelerate the pruning process. We also disregard $O (||W_i||^3)$, as its value is typically small compared to the first term. Thus, the estimated weight structure importance becomes:\n$\\hat{I}_{W_i} = \\frac{|\\Delta L(D)|}{\\Delta W_i} * W_i \\quad(24)$\nAfter evaluating the importance of each output channel, we sort them and prune the least important ones based on the specified sparsity.\n3.4 Post-train with Knowledge Distillation\nGiven that the model is pruned multiple times to achieve the desired overall sparsity, and that pruning inevitably reduces performance, it is beneficial to periodically apply post-training after each pruning step to recover lost performance. Knowledge distillation [Hinton, 2015] facilitates the transfer of knowledge from a more advanced teacher model to a smaller or compressed student model. To preserve the performance of the compressed models, we use the original model as the teacher and the corresponding compressed model as the student. Specifically, during post-training, we introduce a penalty term to the loss function, which measures the distance between the probability distributions of the teacher model, $p_t$, and the student model, $p_s$, based on the input image $x$. The modified loss function is:\n$Loss = \\tau * Loss(p_t(x), p_s(x)) \\quad(25)$\n$+ (1 - \\tau) * Loss(label, p_s(x)) \\quad(26)$\nwhere $\\tau$ is a hyperparameter that controls the extent to which the student model learns from the teacher. As a result, the compressed model benefits from the knowledge of the original model, helping to maintain more of its performance."}, {"title": "4 Experiment", "content": "4.1 Settings\nSetup We build RL-Pruner from the scratch using Pytorch [Paszke et al., 2019]. Our experiments utilize an NVIDIA 4090D GPU with 24GB of memory and Intel(R) Xeon(R) Platinum 8481C CPU.\nFor implementing our methods, we conduct experiments using default hyperparameter values: noise variance $z = 0.04$, step size for policy update $\\lambda = 0.1$, discount factor $\\gamma = 0.9$, sample steps $T = 1$ and sample number $N_s = 10$ for each sampling stage, with each pruning step containing 10 sampling stages. Proximal policy optimization is employed with $\\delta = 0.2$. We utilize three reward strategies in our experiments: accuracy-based, FLOPs-based, and parameter-based, corresponding to $(\\alpha = 0, \\beta = 0), (\\alpha = 0.25, \\beta = 0)$ and $(\\alpha = 0, \\beta = 0.25)$ in equation 17 respectively. For pruning each layer using the Taylor method, we extract 100 samples from the training dataset as the calibration dataset. A quick post-training is performed with a coefficient $\\tau = 0.75$ for the loss between the teacher model and the student model. Furthermore, we switch the teacher model if a compressed model with higher accuracy is found during pruning. For the exploration strategy, we initialize the exploration parameter $\\epsilon = 0.4$, which decreases in a cosine fashion over the first 10% of pruning steps. Detailed hyperparameter analysis is provided 4.3.\nModels and Datasets In this study, we focus on classification tasks, although our proposed methods are readily extendable to other tasks, such as segmentation. To demonstrate the general applicability of RL-Pruner for CNNs, we evaluate it on several widely used classification models, including VGG-19 [Simonyan and Zisserman, 2014], ResNet-56 [He et al., 2016], GoogLeNet [Szegedy et al., 2015], DenseNet121 [Huang et al., 2017], and MobileNetV3-Large [Howard, 2017]. These models encompass diverse architectural features such as residual connections, concatenation connections, and Squeeze-and-Excitation modules. For datasets, we utilize CIFAR-10 and CIFAR-100 [Krizhevsky et al., 2009] to assess performance. Floating point operations (FLOPs) determine computational complexity and inference speed, while the parameter count reflects memory usage during inference.\nIn our experiments, we primarily use three criteria to evaluate the compressed model: test Top-1 accuracy, FLOPs compression ratio, and parameter count compression ratio. These ratios are computed as $1 - \\frac{FLOPs(pruned \\ model)}{FLOPs (base \\ model)}$ and $1 - \\frac{Para. Num. (pruned \\ model)}{Para. Num. (base \\ model)}$ respectively.\nBaseline Setup To validate the effectiveness of our method, we compare it against other structured pruning techniques, including DepGraph [Fang et al., 2023] and GReg [Wang et al., 2020]. Additionally, we benchmark our method against other neural architecture search approaches, such as GNN-RL [Yu et al., 2022]. We evaluate all methods at three channel sparsity ratios\u201425%, 50%, and 75% to assess their performance under various levels of sparsity.\n4.2 Results\nPerformance We begin by evaluating our methods on popular CNN architectures using CIFAR-100. As shown in Table 1, we can compress VGG-19 to 60% channel sparsity, and GoogleNet and MobileNetV3-Large to 40% channel sparsity, with a performance drop of less than 1%. This indicates that there is significant redundancy in these architectures that can be pruned without substantial performance loss, given the considerable reduction in FLOPs and parameter counts at these sparsity levels. We also note that ResNet-56's performance declines rapidly as pruning progresses. This may be due to ResNet-56's many layers with relatively few parameters, resulting in a low number of channels per layer. Consequently, the optimal sparsity distribution learned by our algorithm is easily rounded to zero during pruning, which limits its ability to represent each layer's relative importance and leads to a performance drop.\n4.3 More Analysis\nIn this section, we explore the effects of various methods employed during the pruning process, such as reward strategies and exploration ($\\epsilon$). To minimize errors introduced by complex architectures, we conduct our experiments on the simpler VGG-19 architecture to evaluate these methods.\nEffects of Reward Strategy We first evaluate how different reward strategies influence pruning results. We track the sparsity changes of specific layers in VGG-19 over 75 pruning steps. As shown in Figure 3, the parameter-based reward strategy tends to prune more filters in the convolutional layers, while the accuracy-based reward strategy favors pruning more neurons in the linear layers. It is challenging to establish a definitive principle linking the reward strategy to which layers are pruned more. Instead, our method identifies the optimal architecture based on the given reward strategy.\nExploration $\\epsilon$ Choice We evaluate different initial values of exploration $\\epsilon$ and reduce it to 0 over the first 10% of pruning steps using various decay strategies for VGG-19 on CIFAR-100. As shown in Table 3, adopting an exploration strategy can lead to higher performance by exploring a broader range of architectures. However, it may also occasionally converge to a suboptimal architecture, resulting in reduced performance.\nCompared to Scratch Training We then validate the distillation effects of our method. Specifically, we train a compressed version of the VGG-19 model, where the architecture is initialized to match the compressed model, from scratch, and compare its performance to that of the"}, {"title": "5 Conclusion", "content": "In this paper, we present RL-Pruner, a structured pruning method that learns the optimal sparsity distribution across layers and supports general pruning without model-specific modifications. We hope our approach, which recognizes that each layer has a different relative importance to the model's performance, will influence future work in neural network compression, including unstructured pruning and quantization."}]}