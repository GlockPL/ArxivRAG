{"title": "ReMOVE: A Reference-free Metric for Object Erasure", "authors": ["Aditya Chandrasekar", "Goirik Chakrabarty", "Jai Bardhan", "Ramya Hebbalaguppe", "Prathosh AP"], "abstract": "We introduce REMOVE, a novel reference-free metric\nfor assessing object erasure efficacy in diffusion-based im-\nage editing models post-generation. Unlike existing mea-\nsures such as LPIPS and CLIPScore, REMOVE addresses\nthe challenge of evaluating inpainting without a reference\nimage, common in practical scenarios. It effectively distin-\nguishes between object removal and replacement. This is a\nkey issue in diffusion models due to stochastic nature of im-\nage generation. Traditional metrics fail to align with the in-\ntuitive definition of inpainting, which aims for (1) seamless\nobject removal within masked regions (2) while preserving\nthe background continuity. REMOVE not only correlates\nwith state-of-the-art metrics and aligns with human percep-\ntion but also captures the nuanced aspects of the inpainting\nprocess, providing a finer-grained evaluation of the gener-\nated outputs.", "sections": [{"title": "1. Introduction", "content": "In the contemporary creative landscape, diffusion models\nhave surged in popularity, driving innovation in visual con-\ntent generation [9, 30, 31]. One of the primary applica-\ntions of diffusion models lies in their role in image editing,\nachieved through prompt-based user inputs, which stream-\nline access to complex editing functionalities in the cre-\native process [6, 8, 10, 12]. These models provide a robust\nmethodology for various tasks, including object replace-\nment, position switching, and object erasure [7, 20]. Among\nthese categories, object erasure is the task of inpainting a\nmasked region with neighbouring pixels to seamlessly re-\nmove undesired objects from an image.\nImage inpainting, a technique in computer vision akin\nto a digital paintbrush, serves to restore or complete im-\nages by intelligently reconstructing missing or damaged ar-\neas [13, 34]. This process involves filling in the gaps with\ndetails that seamlessly blend with the surrounding context.\nMoreover, inpainting extends to removing unwanted objects\nfrom a photo, such as a power line disrupting a scenic land-\nscape. By analyzing the surroundings, the technique can\nreconstruct the missing pixels, creating an image as if the\npower line was never there.\nOver the years, there has been significant advancements\nin image inpainting [37] and it's evaluation metrics. These\nmetrics typically fall into three categories: structure-based,\nsaliency-based, and machine learning-based. However, the\nmajority of these metrics are reference-dependent, requir-\ning a ground truth inpainting, which is often very hard to\nobtain on a large scale for real images [29]. This reliance on\nreference-based evaluation can pose challenges, highlight-\ning the need for developing alternative approaches to assess\nthe quality and efficacy of inpainting methods.\nZhang et al. [40] show that traditional per-pixel met-\nrics like Mean Squared Error (MSE) and Peak Signal-to-\nNoise Ratio (PSNR) often employed in image enhance-\nment, super-resolution [11, 22] are inadequate for evaluat-\ning structured outputs such as images. This is because these\nmetrics assume independence among pixels, failing to cap-\nture perceptual changes accurately. A notable illustration is\nthe discrepancy between perceptual differences and minor\nchanges in MSE induced by blurring or noising [36]. Thus,\nthe authors proposed Learned Perceptual Image Patch Sim-\nilarity (LPIPS) [40], obtained using feature distances in a\nneural network, outperforming traditional metrics while ac-"}, {"title": "2. Related Work", "content": "Image Editing with Diffusion Models: Diffusion models\nexcel in image editing using textual prompts (and additional\ninputs such as masks). Notably, InstructPix2Pix [8] fine-\ntunes a diffusion model using instruction prompts generated\nwith LLMs for better textual alignment, but struggles with\ncomposability and spatial reasoning. Edit masks can allevi-\nate this issue as shown in Refs. [5, 14, 28], which utilize edit\nmasks to perform iterative editing of objects within the im-\nage. Ref. [10] improves upon this by performing zero-shot\nmultiple object edits in a single pass. We refer the reader to\nRef. [19] for a more comprehensive review of the domain.\nObject Deletion: Viewed as inpainting, deletion aims to\nremove an object from the image and reconstruct the back-\nground faithfully. Generally, deletion is a challenging prob-\nlem - naive inpainting models usually fail to understand\nthe prompt or replace the object with another object. This\nmotivated Ref. [39] to solve the problem by constructing\na deletion-specific dataset from the GraphVQA dataset to\nfinetune inpainting models to deletion prompts. Ref. [38]\nimproves the performance by attaching guidance to the dif-\nfusion process.\nMetrics for Image Quality Assessment: Editing, inpaint-\ning, and deletion methods have been generally evaluated\nthrough image perceptual quality metrics. Most of these\nmetrics are either fully reference-based or distribution-\nbased, i.e., they require a reference image or a reference dis-\ntribution along with the output image to measure the qual-\nity of the output. Fr\u00e9chet Inception Distance [17], assess\nthe quality of generated images by comparing its distri-\nbution to ImageNet. Structural Similarity Index Measure\n(SSIM) [36] measures image degradation as a perceived\nchange in structural information. Inspired by SSIM, [35]"}, {"title": "3. Proposed Metric", "content": "REMOVE leverages feature extraction to assess inpainting\nquality - assessing visual saliency at a patch level rather\nthan pixel level. Specifically, REMOVE employs a ViT [15]\ntrained on an image segmentation task [21] to extract fea-\nture embeddings for the image patches. Average embed-\ndings are calculated for the masked regions and the un-\nmasked regions, which are then compared to score \"how\nconsistent the inpainted region is with the background re-\ngions\". Ideally, for deletion we would want no noticeable\nchange within the masked region compared to the back-\nground region, i.e, the similarity score is high.\nGiven an image x from image-space $\\mathcal{I} \\subset \\mathbb{R}^{w\\times h\\times 3}$ (x $\\in$ $\\mathcal{I}$), a\nforeground mask M, a ViT feature extractor $\\mathcal{E}$ and\na similarity measure S, the working of the metric can be\nsummarized as follows:"}, {"title": "1. Image Preprocessing", "content": "The image x undergoes initial\npre-processing steps including resizing and normaliza-\ntion according to the ViT's preprocessor [15] to obtain\nx0. In our case, x0 $\\in$ $\\mathbb{R}^{1024\\times1024\\times3}$\n2. Feature Extraction: The ViT is used to extract features\nz from x0 using patches of size p$\\times$p from the image, i.e.,\nz$\\in$$\\mathbb{R}^{\\frac{w}{p}\\times\\frac{h}{p}\\times d_f}$ where $d_f$ is the embedding size for each\npatch. These features capture the color patterns, textures,\nand spatial relationships within each region [15]. Using\nx0 and patch size 16 $\\times$ 16, z $\\in$ $\\mathbb{R}^{64\\times64\\times d_f}$:\nz = $\\mathcal{E}(x_0)$"}, {"title": "3. Masking", "content": "Given inpainting mask M$\\in$ {0,1}$^{w\\times h}$,\nwhere 1's denote the region targeted for inpainting, we\nresize it to match the patch-level output, i.e, M $\\in$\n{0,1}$^{64\\times64}$.\n4. Feature Segregation: The features z are split into two\ndisjoint sets using the mask M: masked features zm and\nunmasked features zu.\n$Z_m = \\{Z_{ij} \\text{ where } M_{ij} = 1\\}_{i\\in\\{0,...,\\frac{w}{p}\\}, j\\in\\{0,...,\\frac{h}{p}\\} }$\n$Z_u = \\{Z_{ij} \\text{ where } M_{ij} = 0\\}_{i\\in\\{0,...,\\frac{w}{p}\\}, j\\in\\{0,...,\\frac{h}{p}\\} }$"}, {"title": "5. Mean Features", "content": "For both the sets, we then calculate the\nmean feature vector as the average ($\\bar{.}$) of features within\nthe set, obtaining vectors $\\bar{Z_m}$ and $\\bar{Z_u}$.\n6. REMOVE Calculation: The quality of inpainting is given\nby the similarity measure S between the mean masked\nand unmasked feature vectors.\n$REMOVE = S (\\bar{Z_m}, \\bar{Z_u})$\nIn this work, we define S as the cosine similarity, i.e.,\nfor REMOVE, higher values indicate better results. Addi-\ntionally, we incorporate cropping during the preprocess-\ning stage to ensure comparability between the number of\npatches belonging to the masked and unmasked regions.\nFurther elaboration on this aspect can be found in Sec. 4.2."}, {"title": "4. Experimental Setup", "content": "In this section, we outline the experiments conducted to val-\nidate the efficacy of REMOVE as a reference-free metric for\nassessing inpainting quality. We first demonstrate its reli-\nability through a simple experiment wherein we observe a\nconsistent trend between REMOVE and perceptual similar-\nity measured by LPIPS using the ground truth inpaintings\nas a baseline. Thereafter, we test the metric on a real-world\ninpainting scenario on the DEFACTO dataset [27]."}, {"title": "4.1. Toy Experiment", "content": "Traditional evaluation methods often rely on reference im-\nages, which may not be readily available. Thus, this toy ex-\nperiment aims to empirically validate REMOVE for assess-\ning image inpainting quality on a large synthetically gener-\nated dataset to check REMOVE's correlation with LPIPS."}, {"title": "4.1.1 Dataset Generation", "content": "We start by creating a comprehensive dataset for evaluating\nthe effectiveness of REMOVE. Firstly, we use a collection of\n4300 background images [32] from Flikr comprising vari-\nous landscapes that can roughly be categorized as mountain,\nsea, desert, beach, and island. Next, we randomly select 20\nmasks from the PIE-Bench image editing dataset [20]. The\nmasks are categorized as either coarse or fine, depending\non whether they exhibit a general blob shape or represent a\nspecific identifiable object, respectively. This distinction is\nillustrated in Figure 5a.\nTo generate inpainted images, we employ Stable Diffu-\nsion Inpaint (SD-Inpaint) [31] using randomly selected im-\nages and masks from the dataset defined above. These in-\npainted images utilize the input background images as their\nground truth inpainting. Additionally, the inpainted images\nare generated with various seeds and an empty prompt (\"\")\nto introduce variability in the inpaintings.\nThe quality of inpainting varied across instances, with\nSD-Inpaint occasionally producing satisfactory results, as\nshown in Figure 5b, while at other times exhibiting sub-\noptimal performance. This variability in inpainting qual-\nity results in a diverse photo-realistic dataset comprising\n~200,000 images with varying degrees of inpainting qual-\nity. Consequently, this dataset provides a robust foundation\nfor evaluating our framework."}, {"title": "4.1.2 Empirical Validation", "content": "To empirically validate REMOVE, we leverage LPIPS, a\nmetric used to quantify perceptual similarity between im-\nages. A lower LPIPS score indicates a higher degree of\nsimilarity between the generated image and its ground truth,\nsuggesting accurate inpainting. Furthermore, LPIPS has\nbeen empirically validated to closely align with human per-\nception, making it a suitable metric for evaluating inpaint-\ning quality [40].\nFor every generated image and its corresponding ground\ntruth counterpart, we compute both REMOVE and the LPIPS\nscore. Subsequently, we arrange the dataset in ascending\norder according to the calculated LPIPS scores, partition-\ning it into N = 20 equally spaced subsets. We then deter-\nmine the mean value of REMOVE within each partition. This\nhelps facilitate a systematic analysis of inpainting quality\nassessment across various levels of similarity to the ground\ntruth. Based on the findings depicted in Fig. 6, the de-\ncreasing trend across partitions infers a close correlation\nbetween REMOVE and the quality of inpainting, consistent\nwith LPIPS.\nIn summary, this validation protocol aims to provide in-\nsights into the effectiveness of REMOVE to reliably serve as\na reference-free measure for assessing inpainting quality."}, {"title": "4.2. Real World Experiment", "content": "To assess REMOVE's performance in a real-world scenario,\nwhere object masks are relatively small, we utilize the\nobject removal category of the DEFACTO dataset [27],\nconstructed over MSCOCO [24]. Additionally, alongside\nLPIPS, we include comparisons with CLIPScore (CS) [16],\na widely used metric for validating diffusion-based image\nediting performance."}, {"title": "4.2.1 DEFACTO Dataset", "content": "The DEFACTO dataset comprises ~25,000 input image, in-\npainting mask, and ground truth tuples. Samples from the\ndataset are illustrated in Fig. 7. As CS necessitates a text\nprompt, we utilize BLIP [23] to generate prompts using\nboth the input images (resulting in CS-NR) and the ground\ntruth inpaintings (resulting in CS-FR), where NR and FR\nrefer to no-reference and full-reference, respectively [29].\nThe DEFACTO dataset differs from the toy dataset\n(Sec. 4.1.1) primarily in two aspects: (1) it exhibits a wide\nvariation in the types of masks present, and (2) the images\nwithin the dataset are considerably more complex compared\nto those in the toy experiment dataset."}, {"title": "4.2.2 Empirical Validation", "content": "Due to multiple mask sizes in the DEFACTO dataset, we\npartitioned it into three parts based on size: small, medium,"}, {"title": "5. Discussions", "content": "Why does CLIPScore fail in measuring inpainting per-\nformance? Text captions in intricate scenes (see Fig. 7)\noften lack comprehensive detail to encapsulate every ob-\nject [33]. Thus, relying solely on CLIPScore to evaluate in-\npainting performance may yield unreliable results. In many\nscenarios, ground truth data is unavailable, rendering CS-\nFR impractical. Consequently, CS-NR is commonly em-\nployed in practice. However, CS-NR predominantly de-\ntects the absence of the object to be inpainted without thor-\noughly evaluating the semantic coherence of the inpainted\nbackground. This can lead to ambiguity between object\nremoval and replacement. The limitation stems from CS-\nNR's design, which prioritizes the identification of miss-\ning elements over assessing the coherence and fidelity of\nthe inpainted region within the broader context of the entire\nscene. Thus, while CLIPScore offers insights, its effective-\nness is constrained in complex scenes where text captions\nor source images may not provide sufficient information for\naccurate evaluation.\nWhy is cropping necessary for accurately estimating in-\npainting performance? During our preliminary toy exper-\niment, we observed promising performance from REMOVE\nwithout cropping, demonstrating a strong alignment with\nthe expected trend with respect to LPIPS. However, as we\nconducted experiments with real data, we encountered a\nchallenge posed by the varying sizes of the masks. This\nvariability introduced complexities, especially concerning\nsmaller masks. In such cases, we posit that evaluating the\neffectiveness of background inpainting may primarily focus\non a localized area surrounding the mask rather than con-\nsidering the entire image. This issue can be seen in Table 1"}, {"title": "5.1. User Study", "content": "This study evaluates the effectiveness of an inpainting met-\nric in determining user preferences for inpainted images.\nA. Protocol: Participants ranked a set of two inpainted im-\nages based on personal preference and rated the visual qual-\nity of inpainting. The metrics (LPIPS and ReMOVE) will\nindependently rank the same images. Then, we measure the\naccuracy of the metrics in predicting the user preference.\nB. Expected Outcome: We hypothesize an alignment be-\ntween participant and metric's preference for each inpaint-\ning pair, indicating the metric's ability to capture user pref-\nerences. Consistent performance across both metrics and\nuser preferences may identify effective inpainting methods.\nC. Results: We conducted a user study with a set of 20\nparticipants in the age range of 23-30. They contributed a\ntotal of 1000 data points. we observe that REMOVE agrees\n74.7% times with the preference of the users while LPIPS\nagrees only 71.9% times. It is to be noted that LPIPS is a\nreference-based metric and thus has more information while\nevaluating the quality of inpainting than REMOVE. Thus,\nwe conclude that in our experimental setup, REMOVE aligns\nmore closely with the user preference than LPIPS.\nD. Conclusion: This study validates the proposed inpaint-"}, {"title": "6. Conclusion", "content": "We propose ReMOVE, a novel reference-free metric tailored\nto assess the effectiveness of object erasure post-generation\nin generative models like Stable Diffusion. This metric ad-\ndresses the limitations of existing evaluation measures such\nas LPIPS and CLIPScore, especially in scenarios where ref-\nerence images are unavailable, which is common in practi-\ncal applications. By overcoming the challenge of distin-\nguishing between object removal and object replacement in-\nherent in stable diffusion models, REMOVE provides a com-\nprehensive evaluation framework. Through empirical eval-\nuations, we have demonstrated that REMOVE not only cor-\nrelates with established metrics reflecting human perception\nbut also captures the nuances of the inpainting process, of-\nfering a better assessment of generated outputs. We believe\nREMOVE will serve as a valuable tool for researchers and\npractitioners in evaluating and advancing image inpainting\ntechniques, ultimately enhancing their applicability in the\nreal-world."}]}