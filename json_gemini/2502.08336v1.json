{"title": "Salience-Invariant Consistent Policy Learning for Generalization in Visual Reinforcement Learning", "authors": ["Jingbo Sun", "Songjun Tu", "Qichao Zhang", "Ke Chen", "Dongbin Zhao"], "abstract": "Generalizing policies to unseen scenarios remains a critical challenge in visual reinforcement learning, where agents often overfit to the specific visual observations of the training environment. In unseen environments, distracting pixels may lead agents to extract representations containing task-irrelevant information. As a result, agents may deviate from the optimal behaviors learned during training, thereby hindering visual generalization. To address this issue, we propose the Salience-Invariant Consistent Policy Learning (SCPL) algorithm, an efficient framework for zero-shot generalization. Our approach introduces a novel value consistency module alongside a dynamics module to effectively capture task-relevant representations. The value consistency module, guided by saliency, ensures the agent focuses on task-relevant pixels in both original and perturbed observations, while the dynamics module uses augmented data to help the encoder capture dynamic- and reward-relevant representations. Additionally, our theoretical analysis highlights the importance of policy consistency for generalization. To strengthen this, we introduce a policy consistency module with a KL divergence constraint to maintain consistent policies across original and perturbed observations. Extensive experiments on the DMC-GB, Robotic Manipulation, and CARLA benchmarks demonstrate that SCPL significantly outperforms state-of-the-art methods in terms of generalization. Notably, SCPL achieves average performance improvements of 14%, 39%, and 69% in the challenging DMC video hard setting, the Robotic hard setting, and the CARLA benchmark, respectively. Project Page: https://sites.google.com/view/scpl-rl.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, visual reinforcement learning (RL) [30] has achieved remarkable success across various domains, including video games [26, 31], robot control [16, 17, 33], and autonomous driving [29, 34, 35]. However, generalizing policies to novel scenarios remains a significant challenge. Small visual perturbations in observations can distract RL agents [7, 20], leading to representations containing task-irrelevant information and decisions that deviate from their training behavior, ultimately hindering visual generalization. In this paper, we aim to develop generalizable RL agents that can generate effective task-relevant representations and make consistent decisions across both original and perturbed observations.\nData augmentation (DA)-based methods [13, 15, 38] are widely used to enhance the representational ability of visual RL agents. Recent advances, such as SVEA [11] and SGQN [2], leverage augmented data for implicit regularization to improve generalization. Unfortunately, as illustrated in Fig.1 (left), these methods fail to maintain consistent task-relevant attention regions in perturbed observations, impeding the learning of task-relevant representations. Other studies [27, 42] employ dynamic models as auxiliary tasks to capture task-relevant representations. However, the encoder's primary design for original observations prevents it from generating task-relevant representations for perturbed observations. Furthermore, it is difficult to generate task-relevant representations with uncritical attention to task-relevant regions."}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 Data augmentation for visual RL", "content": "Data augmentation is widely used to enhance the generalization of visual reinforcement learning (RL) [8, 15, 37]. DrQ [38] employs image transformation strategies to augment observations through implicit regularization. SVEA [11] enhances generalization by updating the value function with both original and augmented data. CG2A [21] improves generalization by combining various data augmentations and alleviating the gradient conflict bias caused by these augmentations. CNSN [18] employs normalization techniques to improve visual generalization. SGQN [2] utilizes saliency guidance to focus agents' attention on task-relevant areas in original observations while aligning their attention across original and augmented data using a trainable network. MaDi [7] improves generalization by incorporating a mask network before the value function to filter out task-irrelevant regions in observations. While these methods can identify effective task-relevant regions in original observations, they often struggle with perturbed observations. In this paper, SCPL focuses on maintaining consistent task-relevant regions in both original and perturbed data with a value consistency module."}, {"title": "2.2 Representation learning in visual RL", "content": "Numerous methods [5, 23] improve generalization by learning task-relevant representations through auxiliary tasks. Some approaches [19] improve representation effectiveness by employing observation reconstruction as auxiliary tasks. DBC [42] minimizes the bisimulation metric in latent spaces to learn invariant representations without task-irrelevant information. PAD [10] utilizes an inverse dynamic model to predict actions based on current and next states. Dr.G [8] trains the encoder and world model using contrastive learning and introduces an inverse dynamics model to capture temporal structure. These methods learn task-relevant information through the guidance of rewards and dynamic consistency. However, they struggle to extract task-relevant information for perturbed observations due to their exclusive training on original data and uncritical task-relevant attention. SCPL achieves task-relevant representations for both original and perturbed observations by training a dynamics module using both original and augmented data while focusing on task-relevant regions."}, {"title": "2.3 Policy learning for RL", "content": "Some studies explore the decoupling of value functions and policy networks to learn effective policies or obtain invariant representations [1, 39]. PPG [3] mitigates the interference between policy and value optimization by distilling the value function while constraining the policy. IDAAC [25] models both the policy and value function while introducing an auxiliary loss to obtain representations that remain invariant to task-irrelevant properties. DCPG [22] implicitly penalizes value estimates by optimizing the value network less frequently, using more training data than the policy network. However, prior studies that focus on learning invariant representations often overlook the consistency of policies across both original and perturbed observations. In contrast, SCPL learns task-relevant representations while maintaining a consistent superior policy for perturbed observations, similar to that for original observations. To the best of our knowledge, we are the first to highlight the importance of policy consistency between original and perturbed observations for generalization ability."}, {"title": "3 PROBLEM FORMULATION", "content": "Visual reinforcement learning (RL) is considered a partially observable Markov decision process (POMDP) because only partial states are observed from images. A POMDP is defined as a tuple M = (S, O, A, P, r, y), where S is the state space, O is the observation space, A is the action space, P : S\u00d7A\u00d7S \u2192 R is the transition probability distribution, r : S \u2192 R is the reward function, and y \u2208 (0, 1) is the discount factor. Let \u03c0 denote a stochastic policy and \u03b7(\u03c0) denote its expected cumulative reward: \\(\\eta(\\pi) = E_{s_0,a_0,...} [\\Sigma_{t=0}^{\\infty} \\gamma^t r(s_t)]\\), where \u03c0 is the policy, a is the action, and st is the state in the t step. The purpose of visual RL is to find a policy \u03c0* to maximize the expected cumulative reward. The state-action value function $Q_\\pi$, the value function $V_\\pi$, and the advantage function $A_\\pi$ are defined as: $Q_\\pi(s_t, a_t) = E_{s_{t+1},a_{t+1}...} [\\Sigma_{i=0}^{\\infty} \\gamma^i r(s_{t+1})]$, \\(V_\\pi (s_t) = E_{a_t,s_{t+1}...} [\\Sigma_{t=0}^{\\infty} \\gamma^i r(s_{t+1})]\\), and \\(A_\\pi (s, a) = Q_\\pi (s, a) - V_\\pi (s)\\)."}, {"title": "4 METHODOLOGY", "content": "We propose a salience-invariant consistent policy learning (SCPL) framework to improve the zero-shot generalization of visual RL. As shown in Fig.2, SCPL mainly consists of three modules: the value consistency module, the policy consistency module, and the dynamics module. In this paper, \u03b8, \u03b6, \u03c6, and \u03c8 represent the parameters of the encoder, the value function, the policy network, and the dynamic model, respectively."}, {"title": "4.1 Value Consistency Module", "content": "To extract task-relevant representations from both original and perturbed observations, it is essential for the encoder and value function to consistently focus on task-relevant regions. We introduce a value consistency module with a novel loss function for the encoder and value function, leveraging augmented data and their saliency maps. To improve the consistency of attention regions for original and perturbed observations, we update the value function with both original and augmented observations. The value loss for the original data \\(L_{Q1}\\) is:\n\\(L_{Q1} (\\theta, \\zeta) = E_{s,a}[(Q_\\zeta (f_\\theta(s), a) - y_t)^2].\\)\nThe value loss for the augmented data \\(L_{Q2}\\) is:\n\\(L_{Q2}(\\theta, \\zeta) = E_{s_a,a}[(Q_\\zeta (f_\\theta(s_a), a) - y_t)^2],\\)\nwhere yt represents the target of Q-value. These losses encourage the value function to estimate the same values for both original and augmented observations, thereby promoting consistency in the attention regions. However, maintaining consistency in value estimation alone may be insufficient to ensure that agents focus on task-relevant regions amidst increasing perturbations. Therefore, additional guidance is necessary to help agents remain attentive to task-relevant regions in the observations.\nSCPL utilizes saliency attribute masked maps to guide the encoder and the value function to focus on task-relevant regions for observations. As shown in Fig.3, we generate the saliency attribute masked maps (\u015d and \u015da) for original and augmented observations (s and sa) using the vanilla gradient method [28]. We use guided backpropagation to compute the gradient map M(Q, s, a) of the Q-network, represented as M(Q, s, a) = dQ(s, a)/ds. Let \\(M_p (Q, s, a)\\) be the binarized p-quantile saliency attribute map. Specifically, if the gradient pixel \\(M(Q, s, a)_j\\) belongs to the top 1-p quantile of gradient values, then \\(M_p (Q, s, a)_j\\) will be set to 1, otherwise 0. The p-quantile saliency attribute map \u015d and \u015da represents the attention of the value function towards input observations, where white areas indicate regions of high attention and less attended regions are masked out. Then, the saliency attribute masked maps are generated by multiplying the observations with their saliency attribute maps to show the attended pixels. \u25c9 denotes the Hadamard product. To guide agents to focus on task-relevant pixels, we introduce a saliency consistency term between original observations and their respective saliency attribute maps. The saliency consistency loss for the original data is:\n\\(L_{QC1}(\\theta, \\zeta) = E_{s,\\hat{s},a}[(Q_\\zeta (f_\\theta(\\hat{s}), a) - Q_\\zeta (f_\\theta(s), a))^2].\\)\nTo ensure that agents focus on task-relevant regions in both original and perturbed observations, we extend saliency guidance to augmented data while updating the value function with this augmented data. The saliency consistency loss for the augmented data is:\n\\(L_{QC2}(\\theta, \\zeta) = E_{s_a,\\hat{s_a},a}[(Q_\\zeta (f_\\theta(\\hat{s_a}), a) - Q_\\zeta (f_\\theta(s_a), a))^2].\\)\nWith the saliency guidance from Eq.(3) and Eq.(4), agents are able to focus on task-relevant regions in both original and perturbed observations.\nWe combine the value loss with the saliency consistency loss to form the training objective. The value function's objective is:\n\\(L_Q(\\theta, \\zeta) = L_{Q1} (\\theta, \\zeta) + L_{Q2}(\\theta, \\zeta) + \\lambda(L_{QC1}(\\theta, \\zeta) + L_{QC2}(\\theta, \\zeta)),\\)\nwhere \u03bb is the value consistency coefficient. \\(L_{Q1} (\\theta, \\zeta)\\) and \\(L_{Q2} (\\theta, \\zeta)\\) ensure the encoder and value function attend to consistent regions in both the training and test environments, while \\(L_{QC1}(\\theta,\\zeta)\\) and \\(L_{QC2} (\\theta, \\zeta)\\) ensure agent focus on task-relevant regions within observations. This value loss enables the encoder and value function to capture consistent task-relevant pixels from both original and perturbed observations."}, {"title": "4.2 Dynamics Module", "content": "To enable the encoder to effectively provide task-relevant representations, we develop a dynamic model to ensure representations meet the conditions of rewards and dynamics. Specifically, we construct this dynamic model by predicting rewards and next-state representations for both the original and augmented observations. The loss of the dynamics module for embedding e is:\n\\(L_{Te}(\\theta,\\psi) = E_{s,a}[(e' - P_\\psi(f_\\theta(s), a))^2 + (r - R_\\psi (f_\\theta (s), a))^2],\\)\nwhere e and e' are the latent representations of the current observation and the next observation. Dynamics module T consists of dynamic head P and reward head R. The training objective of the dynamics module of the embedding for augmented data ea, is:\n\\(L_{Tea} (\\theta, \\psi) = E_{s_a,a}[(e'_a - P_\\psi(f_\\theta(s_a), a))^2 + (r - R_\\psi (f_\\theta(s_a), a))^2].\\)\nThe training objective for the dynamics module is:\n\\(L_T (\\theta,\\psi) = L_{Te} (\\theta, \\psi) + L_{Tea} (\\theta, \\psi).\\)\nIn SCPL, the value consistency module ensures attention to task-relevant regions, while the dynamics module guides representations that align with reward and dynamics conditions. With the combination of the value consistency module and the dynamics module, the encoder can generate task-relevant representations."}, {"title": "4.3 Policy Consistency Module", "content": "As illustrated in Fig.1 (middle), previous RL agents frequently exhibit poor policy consistency, reflected in the substantial KL divergence, between training and test environments. In this section, our theoretical analysis reveals that the policy consistency of agents contributes to enhanced generalization capability. Furthermore, we propose a policy consistency module that improves generalization by enhancing agents' policy consistency across both original and perturbed observations.\nRelationship between policy consistency and generalization ability. We utilize the KL divergence of action distributions between training and test environments to assess the policy consistency of agents. Additionally, the divergence in cumulative rewards between these environments reflects the agents' generalization capabilities. In the context of visual generalization, the training and test environments are identical except for visual observations. Consequently, the agent's policies in both environments can be regarded as two equivalent policies within the training environment. We prove that there is a positive correlation between the upper bound of the divergence in cumulative rewards of the two policies and their KL divergence.\nInitially, we utilize the total variation divergence to measure the distance between two policies' distributions. The divergence is defined as : \\(D_{TV} (p||q) = \\frac{1}{2} \\sum_i |p_i - q_i|\\) for probability distributions p and q. Define \\(D^{max}_{TV} (\\pi_0, \\pi_\\rho)\\) as :\n\\(D^{max}_{TV} (\\pi_0, \\pi_\\rho) = max_s D_{TV} (\\pi_0(\\cdot | s)||\\pi_\\rho(\\cdot | s)),\\)"}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct experiments to investigate the following questions: (1) Does SCPL exhibit superior visual generalization capability compared to current state-of-the-art methods? (2) Can SCPL focus on consistent task-relevant pixels in both original and perturbed observations? (3) Does SCPL possess consistent representations and policies? (4) What is the contribution of various modules to generalization performance? (5) Can SCPL demonstrate advanced generalization in challenging robotic and autonomous driving environments?"}, {"title": "5.1 Experimental Settings", "content": "We evaluate the zero-shot generalization performance of our method in DeepMind Control Suite (DMC) [12, 32], Robotic Manipulation tasks [14], and CARLA [4]. All methods are trained in the default environment and evaluated with visual perturbations. In the DMC experiment, we compare the generalization ability of SCPL with SOTA methods including SAC [9], SVEA [11], SIM [36], TLDA [40], PIE-G [41], SGQN [2], CG2A [21], MaDi [7], and CNSN [18]."}, {"title": "5.2 Evaluation on the DeepMind Control Suite", "content": "We evaluate the agent's generalization ability on five tasks in DMC-GB [12]. The agent is trained with default backgrounds and evaluated on test environments: Color hard, Video easy, and Video hard. Does SCPL exhibit superior visual generalization capability? We evaluate the visual generalization performance of SCPL across 15 visual perturbed control tasks in the DMC. As shown in Table 1, we report the mean and standard deviation of episode returns over three seeds. SCPL agents are trained using two data augmentation techniques from [12]: random convolution and random overlay. The SCPL results in Table 1 are based on random convolution for the Color hard task, and random overlay for both the Video easy and Video hard tasks. Table 1 shows that SCPL outperforms other baselines in 13 out of 15 tasks within unseen test environments. Notably, SCPL achieves performance improvements of 12% in walker stand, 11% in walker walk, 28% in cartpole swing-up, 18% in ball in cup, and 9% in finger spin tasks in the challenging video hard setting. Overall, SCPL achieves an average performance improvement of 14% across all tasks in the video hard environments. Fig.4 presents the test curves for SCPL, SGQN, SVEA, and SAC in these environments, where SCPL demonstrates faster convergence due to its effective task-relevant representations and consistent policies. The experimental results demonstrate that SCPL exhibits superior visual generalization ability in various perturbed environments. Can SCPL focus on consistent task-relevant pixels? To evaluate the SCPL agent's attention to task-relevant regions, we visualized the saliency maps of agents in both original and perturbed video hard observations across five DMC tasks. Fig.5 presents a"}, {"title": "5.3 Ablation Study", "content": "What is the contribution of various modules? SCPL leverages the value consistency module, policy consistency module, and dynamics module to enhance generalization. To assess the contribution of each component, we evaluate the generalization performance of SAC with the different modules and analyze their respective and combined effects. The results are demonstrated in Table 3. SAC + dynamics module and SAC + value consistency refer to the application of the dynamics module and the value consistency module to SAC, respectively. SAC + value + policy consistency represents applying both the value consistency module and the policy consistency module to SAC. The percentages denote the enhanced performance of modules within SCPL compared to the performance of vanilla SAC. Specifically, the dynamics module yields improvements of 101% in color hard environments, 166% in video easy, and 178% in video hard environments. The value consistency module achieves impressive gains of 97% in color hard, 191% in video easy, and 405% in video hard. When combining both the value and policy consistency modules, generalization improves further, resulting in performance gains of 112%, 206%, and 465% across the three environments, respectively. In SCPL, the integration of these modules significantly boosts performance across all modes. The ablation"}, {"title": "5.4 Generalization in robotic and autonomous driving environments?", "content": "Evaluation on Vision-based Robotic Manipulation. To further evaluate the generalization ability of the proposed SCPL, we consider three robot manipulation tasks based on third-person visual input introduced in [14]: Reach, Push, and Peg in Box. All agents are trained using the default settings and evaluated in two modes. The easy mode substitutes the default environment with five different background colors and desktop textures, while the hard mode further replaces the desktop textures with complex images. We compare SCPL with baseline algorithms SAC, SVEA, and SGQN. The results, presented in Table 4, demonstrate that SCPL outperforms the best prior methods in terms of generalization, achieving an average improvement of +7% on the training set, +52% on the easy set, and +39% on the hard set. The experimental results indicate that SCPL outperforms previous methods in robotic environments.\nEvaluation on CARLA autonomous driving environments. CARLA [4] is a widely used simulator for autonomous driving. In our generalization experiments [6], the agents aim to navigate along the road in the Highway Town04 map, striving to travel as far as possible without colliding within 1000 time steps. The agent is trained under clear noon weather conditions and evaluated in five different weather scenarios, which include varying lighting conditions, realistic rain, and slippery surfaces. We adapted the reward function to align with the settings used in prior work [42]. In Table 5, we present the average driven distance without collisions for vehicles across different weather conditions. Averaged over 10 episodes per weather condition and three training runs, SCPL is able to drive, on average, 69% farther than previous baselines during tests. Notably, in the sunset weather scenario, where all other methods struggle, SCPL demonstrates exceptional generalization capabilities. These experimental results indicate that SCPL achieves superior visual generalization performance in CARLA's autonomous driving environments."}, {"title": "6 CONCLUSION", "content": "This paper proposes a Salience-Invariant Consistent Policy Learning (SCPL) algorithm for generalization in visual RL. SCPL improves visual generalization by promoting task-relevant representations through its value consistency module, which ensures consistent focus on critical regions in both original and perturbed observations, and its dynamics module, which learns dynamics-relevant features. Additionally, our theoretical analysis reveals that maintaining policy consistency between original and perturbed observations is crucial for visual generalization. Therefore, we propose a policy consistency module to enhance generalization performance by reinforcing policy consistency. Through the extensive experiment results, SCPL demonstrates superior zero-shot generalization performance compared to prior SOTA methods. In this study, SCPL employs fixed saliency quantiles during training. Exploring adaptive quantiles for saliency maps to enhance task-relevant attention regions presents a promising direction for future research."}, {"title": "APPENDIX", "content": ""}, {"title": "A ARCHITECTURE OVERVIEW", "content": "We implement the SCPL within the SAC framework. The network architecture of SCPL is depicted in Fig. 7. SCPL comprises three main components: the value module, the policy module, and the dynamics module. For simplicity, we use e to represent the parameters of all the networks.\nThe value module can be divided into two primary components: an encoder \\(f_\\theta\\) and a value function \\(Q_\\zeta\\). The encoder \\(f_\\theta\\) is composed of a convolutional neural network with 11 convolutional layers and 32 convolutional kernels. On the other hand, the value function \\(Q_\\zeta(s)\\) is constructed with three fully connected layers, each comprising 1024 neurons. For a given input observation s, the state-action value can be decomposed as \\(Q = Q_\\zeta (f_\\theta(s), a)\\). During the training of the value network, both the encoder \\(f_\\theta\\) and the value function \\(Q_\\zeta\\) are updated. The policy network also consists of an encoder \\(f_\\theta\\) and a policy function \\(\\pi_\\theta\\). The encoders in the value module, policy module, and dynamics module share identical parameters. The action performed by the policy network for a given input observation s can be expressed as \\(a = \\pi_\\theta(f_\\theta(s))\\). Due to the mutual influence during the training of the policy and value networks [3], when updating the policy network, only the policy function \\(\\pi_\\theta\\) is modified, while the encoder \\(f_\\theta\\) remains unchanged. The policy function \\(\\pi_\\theta\\) is also composed of a 3-layer fully connected network. The dynamics module consists of an encoder \\(f_\\theta\\) and a dynamics network \\(T_\\theta\\), with the encoder sharing parameters with the policy and value networks. The dynamics module generates the estimated representation \\(\\bar{e}_s\\) and estimates reward \\(\\bar{r}\\) based on the current state \\(e_s\\) and the action a. It can be represented as \\(\\bar{e}_s', \\bar{r} = T_\\theta (f_\\theta(s), a)\\). The dynamic model is employed as an auxiliary task to encourage encoders to provide robust latent representations for the value function and the policy network. Thus, both the dynamics network and the encoder are updated during training. The dynamics network comprises three fully connected layers.\nIn order to capture temporal information, we employ a sequence of three consecutive frames as input. This stacking of images is a common approach in reinforcement learning. Lastly, the architecture of the agent in the SCPL framework is visualized in Fig.7."}, {"title": "B PROOF OF THE RELATIONSHIP BETWEEN CUMULATIVE REWARDS DIFFERENCE AND POLICY DISTANCE", "content": "Theorem 1 demonstrates a positive relationship between the upper bound of the difference in cumulative rewards for two policies and the distance between their action distributions. The following is the proof for Theorem 1. Our proof relies on the concept of coupling, where we define two policies, \\(\\pi_0\\) and \\(\\pi_\\rho\\), which take the same actions with a high probability of 1 \u2013 a. As a result, the disparity in expected cumulative rewards between the two policies depends on the varying choices made by these policies.\nTo prove Theorem 1, we begin by proving Lemma 1, which demonstrates that the difference of two policies' expected discounted reward \\(\\eta(\\pi_0) - \\eta(\\pi_\\rho)\\) can be decomposed as the sum of the per-timestep advantage over the episode.\nLemma 1. [?] Given two policies \\(\\pi_0\\) and \\(\\pi_\\rho\\), the difference of two policies' performance is:\n\\(\\eta(\\pi_0) - \\eta(\\pi_\\rho) = E_{\\tau~\\pi_0} [\\Sigma_{t=0}^\\infty \\gamma^t A_{\\pi_\\rho} (s_t, a_t)]\\)\nThis expectation is taken over trajectories \\(\\tau := (s_0, a_0, s_1, a_1, \u00b7 \u00b7 \u00b7 )\\), and \\(\\tau ~ \\pi\\) indicates that actions are taken from \u03c0 to generate \u03c4. Proof. Note that the definition of the advantage function \\(A_{\\pi_\\rho} (s_t, a_t) = V_{\\pi_\\rho} (s_{t+1}) - V_{\\pi_\\rho} (s_t)\\), hence the following equation holds:\n\\(\\eta(\\pi_0) - \\eta(\\pi_\\rho) = E_{\\tau~\\pi_0} [\\Sigma_{t=0}^\\infty \\gamma^t r(s_t)] - E_{s_0~\\pi_\\rho} [V_{\\pi_\\rho} (s_0)]\\)\n\\(= E_{\\tau~\\pi_0} [\\Sigma_{t=0}^\\infty \\gamma^t r(s_t)] - E_{\\tau~\\pi_0} [\\gamma^t V_{\\pi_\\rho} (s_{t+1}) + V_{\\pi_\\rho} (s_t)]\\)\n\\(= E_{\\tau~\\pi_0} [\\Sigma_{t=0}^\\infty \\gamma^t r(s_t) + \\gamma^t V_{\\pi_\\rho} (s_{t+1}) - V_{\\pi_\\rho} (s_t) ]\\)\n\\(= E_{\\tau~\\pi_0} [\\Sigma_{t=0}^\\infty \\gamma^t A_{\\pi_\\rho} (s_t, a_t)]\\)\nTo prove the relationship between the disparity in cumulative rewards and the distance between two policies, it is essential to select a suitable measurement approach. We measure the distance between two policies by the probability that the two policies take identical actions at each time step. The polices \\(\\pi_0\\) and \\(\\pi_\\rho\\) denote the marginal distributions of ao and ap, respectively.\nDefinition 1. A policy pair (\\(\\pi_0\\), \\(\\pi_\\rho\\)) is a-coupled if for each action pair (ao, ap) sampled from the policy pair (\\(\\pi_0\\), \\(\\pi_\\rho\\)), it holds that \\(P(a_o \\neq a_p) \\leq a\\) for all states s.\nThe concept of a-coupling implies that, for the same seed and input, the sampled actions of policies \\(\\pi_0\\) and \\(\\pi_\\rho\\) agree with the probability of at least 1 \u2013 \u03b1.\nLemma 2.[?] If policies \\(\\pi_0\\) and \\(\\pi_\\rho\\) are a-coupled, then for all states s:\n\\(|E_{a~\\pi_0 (\\cdot|s)} [A_{\\pi_\\rho} (s, a)]| \\leq 2a max |A_{\\pi_\\rho} (s, a)|.\\)\nProof. Since \\(E_{a~\\pi} [A_{\\pi}(s, a)] = E_{a~\\pi} [Q_{\\pi}(s, a) - V_{\\pi}(s)] = 0\\), then\n\\(E_{a~\\pi_0 (\\cdot|s)} [A_{\\pi_\\rho} (s, a)] = E_{(a_p,a_o) ~ (\\pi_\\rho,\\pi_0)} [A_{\\pi_\\rho} (s, a_o) - A_{\\pi_\\rho} (s, a_p)]\\)\n\\(= P(a_p \\neq a_o|s)E_{(a_p,a_o) ~ (\\pi_\\rho,\\pi_0)} [A_{\\pi_\\rho} (s, a_o) - A_{\\pi_\\rho} (s, a_p)]\\)\n\\(\\leq 2aB_{(a_p,a_o) ~ (\\pi_\\rho,\\pi_0)} [A_{\\pi_\\rho} (s, a_o) - A_{\\pi_\\rho} (s, a_p)].\\)\nTaking absolute values on both sides, (17) holds.\nTheorem 1. Let \\(a = D^{max}_{TV} (\\pi_0, \\pi_\\rho)\\), the following bound holds:\n\\(|\\eta (\\pi_0) - \\eta (\\pi_\\rho)| \\leq \\frac{2a^2 \\gamma \\epsilon}{(1-\\gamma)^2}\\)\nwhere e = maxs,a \\(|A_{\\pi} (s, a)|\\).\nProof. Let nt denote the number of times that \\(a_i \\neq a_o\\) for i < t, then the expected discount cumulative rewards can be divided into two parts with \\(P(n_t = 0)\\) and \\(P(n_t > 0)\\).\n\\(\\eta(\\pi_0) = P(n_t = 0) E_{s_t ~ \\pi_0 | n_t=0} [\\Sigma_{t=0}^\\infty \\gamma^t r(s_t)] + P(n_t > 0) E_{s_t ~ \\pi_0 | n_t>0} [V_{\\pi_\\rho} (s_0)].\\)"}, {"title": "C BASELINE METHODS", "content": "In the visual generalization task, we compared SCPL with (i) a pure reinforcement learning algorithm, SAC [9], and (ii) reinforcement learning algorithms designed for generalization with data augmentation, including SVEA [11], SIM [36], TLDA [40], SGQN [2], CG2A [21], CNSN[18], and MaDi [7]. The detailed baseline methods are presented below:\n\u2022 SAC [9]: SAC presents a soft actor-critic algorithm without augmentation.\n\u2022 SVEA [11]: SVEA improves generalization by updating the value function with both original and augmented data. It advocates updating the value function of the augmented data with the target value of the original data to improve the stability of the algorithm.\n\u2022 SIM [36]: SIM designs a cross-correlation matrix for self-supervised representation learning and utilizes the redundancy reduction-based self-supervised loss as an intrinsic reward.\n\u2022 TLDA [40]: TLDA identifies task-correlated pixels with large Lipschitz constants and selectively augments task-irrelevant pixels to guide agents in learning relevant information.\n\u2022 PIE-G [41]: PIE-G learns the representations with a pre-trained image encoder to improve visual generalization;\n\u2022 SGQN [2]: SGQN employs saliency guidance to direct agents' attention to task-relevant areas in original observations. It further aligns agents' attention across original and augmented data with a trainable network. However, as depicted in Fig.1, the network results in ambiguous attention regions for augmented data, containing more task-irrelevant information compared to the original data. SGQN captures task-relevant regions in original observations, but the alignment network limits its attention to perturbed observations.\n\u2022 CG2A [21]: CG2A proposes a general policy gradient optimization framework to adaptively balance the varying gradient magnitudes and alleviate the gradient conflict bias caused by data augmentation.\n\u2022 CNSN [18]: CNSN employs normalization techniques to improve visual generalization in reinforcement learning.\n\u2022 MaDi[7]:MaDi proposes that identifying task-relevant areas helps improve the generalization ability of agents. It incorporates a mask network before the value function to mask out task-irrelevant regions in the input images. The mask network is updated using value loss, which enhances the agent's focus on task-relevant areas.\nThe aforementioned data augmentation methods focus on task-relevant regions in original observations but struggle to capture task-relevant pixels for perturbed observations with a lack of guidance. In our work, agents focus on task-relevant regions in both the original and perturbed data by updating the value function with augmented data and their corresponding saliency maps."}, {"title": "D DATA AUGMENTATION", "content": "We consider two data augmentation methods in this paper", "40": "and [36", "12": "Combine the original observation s with another image \u015d using linear interpolation. Specifically", "as": "sa = \u03b1s + (1 \u2212 \u03b1)\u015d. In practice, \u015d is chosen from Places Dataset [?"}]}