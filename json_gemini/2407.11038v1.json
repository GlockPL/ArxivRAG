{"title": "FUZZY RECURRENT STOCHASTIC CONFIGURATION NETWORKS FOR INDUSTRIAL DATA ANALYTICS", "authors": ["Dianhui Wang", "Gang Dang"], "abstract": "This paper presents a novel neuro-fuzzy model, termed fuzzy recurrent stochastic configuration networks (F-RSCNs), for industrial data analytics. Unlike the original recurrent stochastic configuration network (RSCN), the proposed F-RSCN is constructed by multiple sub-reservoirs, and each sub-reservoir is associated with a Takagi-Sugeno-Kang (TSK) fuzzy rule. Through this hybrid framework, first, the interpretability of the model is enhanced by incorporating fuzzy reasoning to embed the prior knowledge into the network. Then, the parameters of the neuro-fuzzy model are determined by the recurrent stochastic configuration (RSC) algorithm. This scheme not only ensures the universal approximation property and fast learning speed of the built model but also overcomes uncertain problems, such as unknown dynamic orders, arbitrary structure determination, and the sensitivity of learning parameters in modelling nonlinear dynamics. Finally, an online update of the output weights is performed using the projection algorithm, and the convergence analysis of the learning parameters is given. By integrating TSK fuzzy inference systems into RSCNs, F-RSCNS have strong fuzzy inference capability and can achieve sound performance for both learning and generalization. Comprehensive experiments show that the proposed F-RSCNs outperform other classical neuro-fuzzy and non-fuzzy models, demonstrating great potential for modelling complex industrial systems.", "sections": [{"title": "1 Introduction", "content": "Recently, complex patterns and dynamics of industrial processes such as electricity, chemical industry, metallurgy, and machinery manufacturing pose significant challenges in developing precise physical models. Complex systems identification and modelling have emerged as a key focus to address these issues [1, 2]. By leveraging the observed input-output data, this approach aims to construct mathematical models that can accurately represent the intricate dynamics of nonlinear systems.\nFuzzy rule-based modelling techniques offer an alternative approach to approximate nonlinear systems through a set of fuzzy rules, where the Takagi-Sugeno\u2013Kang (TSK) fuzzy inference system has been the most widely used framework in nonlinear system identification [3, 4]. Utilizing the existing expert knowledge and rule-based experience allows TSK fuzzy inference systems to capture the qualitative aspects of human decision-making. Therefore, they have good interpretability and are well-suited for dealing with uncertain dynamic systems. Nevertheless, constructing a comprehensive set of rules can be complex and time-consuming, especially for large-scale tasks [5]. Furthermore, TSK fuzzy inference systems cannot automatically update their rules or parameters based on changing data patterns, indicating a lack of adaptive learning capability. Neural networks (NNs) have demonstrated potential in modelling complex dynamics due to their powerful approximation ability for nonlinear maps, high fault tolerance, and adaptive learning capability [6, 7]. In light of this, neuro-fuzzy models have been successfully applied to nonlinear systems identification, which integrate the strengths of both NNs and fuzzy inference systems. They have strong nonlinear processing capability and adaptive learning ability, and maintain the logical reasoning capability and interpretability of the fuzzy inference systems [8, 9, 10]. However, in real-world industrial applications, fluctuations in the controlled plant and external disturbances can introduce order uncertainty, presenting significant obstacles in developing precise forecasting models.\nRecurrent neural networks (RNNs) have feedback connections between neurons, which can handle the uncertainty caused by the selected input variables that are fed into a learner model. Unfortunately, training RNNs by the back-propagation (BP) algorithm suffers from the sensitivity of learning rate, slow convergence, and local minima. With such a background, randomized learner models have been developed over the past years [11, 12]. Among them, echo state networks (ESNs) [11] use large-scale sparsely connected reservoirs to capture and process temporal information, which can effectively avoid the drawbacks of the BP-based method and have been widely applied for modelling complex dynamics. However, ESNs adopt the fixed scope setting of the random parameters, which cannot be adaptively adjusted based on the input data. In addition, both RNNs and ESNs have the problem of arbitrary structure determination. To address these uncertainty issues, we proposed a novel randomized learner model, termed recurrent stochastic configuration networks (RSCNs) [13], which were constructed incrementally by assigning the random parameters in the light of a supervisory mechanism. RSCNs have advantages of strong learning capability, computational simplicity, and compact reservoir topology, but they have limitations in dealing with complex fuzzy systems and lack interpretability.\nThis paper presents a fuzzy version of RSCNS (F-RSCNs) to enhance the modelling capabilities of RSCNs for uncertain and fuzzy nonlinear systems, while also addressing the drawbacks of traditional neuro-fuzzy models in terms of modelling efficiency and structure design. F-RSCNs can integrate prior knowledge into the learning process, thereby improving the learning ability of the original RSCNs. By incorporating fuzzy rules, the model's transparency in decision-making is enhanced, making it easier for experts to understand, validate, and modify the system's behavior, thus increasing the interpretability of the model. Moreover, fuzzy logic rules can effectively represent intricate relationships with fewer parameters, helping to construct a more compact network structure. Specifically, F-RSCNs employ the TSK fuzzy inference systems to reason the fuzzy information, and the network parameters and reservoir topology are adaptively learned by using the recurrent stochastic configuration (RSC) algorithm. Therefore, the proposed approach has following advantages.\n1) A hybrid framework with multiple sub-reservoirs is presented in which the historical order information can be stored and employed to calculate the current output, overcoming the uncertainty of dynamic orders.\n2) Each sub-reservoir is constructed by the RSC algorithm, ensuring its universal approximation property for any nonlinear maps.\n3) The prior knowledge is embedded into the learning process through the TSK fuzzy inference module, which makes the proposed scheme with strong fuzzy inference capability and good interpretability.\n4) The projection algorithm is used to update the readout weights to respond to the dynamic changes of the system and guarantee the convergence of the learning parameters.\nThe remainder of this paper is organized as follows. Section II reviews the relevant background of TSK fuzzy inference systems, SCNs, and ESNs. Section III details the proposed F-RSCNs with both algorithm description and theoretical analysis. Section IV reports our experimental results with comparisons and discussions. Finally, Section V concludes this paper and provides further research direction."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 TSK fuzzy inference systems", "content": "TSK fuzzy inference system is a classical fuzzy framework proposed by Takagi, Sugeno, and Kang in [14], which is based on the fuzzy logic theory and is usually used for handling fuzzy rule-based modelling tasks. Unlike the traditional Mamdani fuzzy inference systems, TSK-based approaches represent the outputs by combining linear functions rather than fuzzy sets. Moreover, they can easily explain some fuzzy and qualitative knowledge, making them suitable for applications in nonlinear system identification, process control, and modelling [3, 4, 5, 10, 15, 16]."}, {"title": "2.2 Stochastic configuration networks", "content": "Stochastic configuration networks (SCNs) [17] are a class of randomized learner models, where the random weights and biases are incrementally generated in the light of a supervisory mechanism, ensuring the universal approximation property of the built model. Additionally, the stochastic configuration (SC) algorithm can be easily implemented, and solve the problems of network parameter selection and arbitrary structure determination. Due to their advantages of high learning efficiency, low human intervention, and strong approximation ability, SCNs have attracted considerable interest in modelling uncertain nonlinear systems, and many promising results have been reported [18, 19, 20]. For more details about SCNs, one can refer to [17].\nGiven an objective function $f : \\mathbb{R}^K \\to \\mathbb{R}^L$, $n_{\\text{max}}$ groups training samples $\\{u(n),t(n)\\}$, $u(n) = [u_1(n), ..., u_K(n)]^\\top$, $t(n) = [t_1(n),..., t_L(n)]^\\top$, $U = [u(1), ..., u(N_{\\text{max}})] \\in \\mathbb{R}^{K\\times n_{\\text{max}}}$ is the input data, and the corresponding output is $T = [t(1), . . ., t(N_{\\text{max}})] \\in \\mathbb{R}^{L\\times n_{\\text{max}}}$, $K$ and $L$ are the dimensions of the input and output, respectively. Suppose we have built a single-layer feedforward network with $N - 1$ hidden nodes,\n\n$\\begin{equation}  f_{N-1} = \\sum_{j=1}^{N-1} \\beta_j g_j (w_j^\\top U + b_j) (f_0 = 0, N = 1,2,3...), \\tag{5} \\end{equation}$\n\nwhere $w_j$, $b_j$, and $\\beta_j$ are the input weight, bias, and output weight of the $j$-th hidden node, $g$ is the activation function. The residual error between the current model output and the expected output $f_{\\text{exp}}$ is defined as\n\n$\\begin{equation}  e_{N-1} = f_{\\text{exp}} - f_{N-1} = [e_{N-1,1}, e_{N-1,2}, ...e_{N-1,L}] . \\tag{6} \\end{equation}$\n\nIf $e_{N-1}$ fails to reach the preset error tolerance, it is necessary to configure a new random basis function $g_N$ based on the supervisory mechanism. Then, we evaluate the output weight $\\beta_N$ and update the model $f_N = f_{N-1} + \\beta_N g_N$. Repeat the configuration until satisfy the terminal conditions.\nIn [17], three SC algorithms are presented to learn the network parameters, and numerous simulation results have indicated that the SC-III algorithm outperforms others with better learning ability and sound generalization. Therefore, the SC-based networks mentioned in this paper are all trained using the SC-III algorithm. The basic theoretical description of SCNs is shown in Theorem 1."}, {"title": "Theorem 1", "content": "Theorem 1 [17]. Let span(\u0393) be dense in $L_2$ space and $\\forall g \\in \\Gamma, 0 < ||g|| < b_g$, $b_g \\in \\mathbb{R}^+$. Given $0 < r < 1$ and a non-negative real number sequence $\\{\\mu_N\\}$, satisfy $\\lim_{N\\to\\infty} \\mu_N = 0$ and $\\mu_N \\leq (1 - r)$. For N=1, 2, ..., define\n\n$\\begin{equation}  \\delta_N = \\sum_{q=1}^{L} \\delta_{N,q}, \\delta_{N,q} = (1 - r - \\mu_N)||e_{N-1,q}||^2, q = 1, 2, ..., L. \\tag{7} \\end{equation}$\n\nIf the generated random basis function $g_N$ satisfies the following inequality constraint\n\n$\\begin{equation}  \\frac{(e_{N-1,q}, g_N)^2}{||g_N||^2} \\geq \\delta_{N,q}, q = 1,2,..., L, \\tag{8} \\end{equation}$\n\nand the output weight is determined by the global least square method\n\n$\\begin{equation}  [\\beta_1^*, \\beta_2^*, ..., \\beta_N^*] = \\arg \\min_\\beta \\left\\|f_{\\text{exp}} - \\sum_{j=1}^{N} \\beta_j g_j \\right\\|_2, \\tag{9} \\end{equation}$\n\nwe have $\\lim_{N\\to\\infty} || f_{\\text{exp}} - f_N || = 0$."}, {"title": "2.3 Echo state networks", "content": "ESNs randomly assign the input and reservoir weights in a fixed uniform distribution, with only the output weights being calculated [11]. The readout can be obtained using a linear combination of the reservoir states. ESNs simplify the training process of RNNs, solving the problems of complex parameter updates, excessive computational burden, and local minima. Moreover, ESNs have the echo state property [21], that is, the reservoir state $x (n)$ is the echo of the input and $x (n)$ should asymptotically depend on the driving input signal. This distinctive property makes them highly promising for handling temporal data.\nConsider an ESN model\n\n$\\begin{equation}  x(n) = g(W_{in}u(n) + W_r x(n - 1) + b), \\tag{10} \\end{equation}$\n\n$\\begin{equation}  y(n) = W_{out} (x(n), u(n)), \\tag{11} \\end{equation}$\n\nwhere $u(n) \\in \\mathbb{R}^K$ is the input signal at time step $n$; $x(n) \\in \\mathbb{R}^N$ is the internal state of the reservoir; $W_{in} \\in \\mathbb{R}^{N\\times K}$, $W_r\\in\\mathbb{R}^{N\\times N}$ represent the input and reservoir weights, respectively; $b$ is the bias; $W_{out} \\in \\mathbb{R}^{L\\times (N+K)}$ is the output weight; and $g$ is the activation function selected from the hyperbolic tangent function or sigmoid function. $x(0)$ is started with a zero matrix, and a few samples are usually washed out for minimizing the influence of the initial zero states. $W_{in}$, $W_r$, and $b$ are generated from the uniform distribution $[-1, \\lambda]$. Notably, the value of $\\lambda$ has a significant impact on model performance. The original ESNs adopt a fixed $\\lambda$, which may lead to poor performance. Scholars have focused on optimizing the weight scope, and some promising results have been reported in [21, 22]. However, the optimization process inevitably increases the complexity of the algorithm. Therefore, selecting a data-dependent and adjustable $\\lambda$ is crucial.\nDefine $X$= $[(x(1), u(1)), ..., (x(n_{\\text{max}}), u(n_{\\text{max}}))]$, where $n_{\\text{max}}$ is the number of training samples and the output is\n\n$\\begin{equation}  Y = [y (1), y (2), ...y (N_{\\text{max}})] = W_{out} X. \\tag{12} \\end{equation}$\n\nThe output weight $W_{out}$ can be calculated by the least squares method\n\n$\\begin{equation}  \\min_{W_{out}} \\left\\| W_{out}X - T\\right\\|^2, \\tag{13} \\end{equation}$\n\n$\\begin{equation}  W_{out}^\\top = (XX^\\top)^{-1}XT^\\top, \\tag{14} \\end{equation}$\n\nwhere $T = [t (1), ...t (N_{\\text{max}})]$ is the desired output."}, {"title": "3 Fuzzy recurrent stochastic configuration networks", "content": "This section details the proposed fuzzy RSC frameworks, which integrate TSK fuzzy inference systems with RSCNs. This hybrid structure is constructed on the basic RSCN with multiple sub-reservoirs and establishes the fuzzy inference module between the input data and sub-reservoirs. Each sub-reservoir is contributed with a TSK fuzzy rule. The network structure is determined by the RSC algorithm, which ensures the built model can infinitely approximate any nonlinear maps. The architecture of the fuzzy RSCN is shown in Figure 1, and the training algorithm is summarized in"}, {"title": "Algorithm 1", "content": "Algorithm 1. Additionally, the echo state property, parameters learning and convergence analysis of fuzzy RSCNs are given in this section.\nGiven the dataset $\\{U, T\\}$, $U = [u (1) ,..., u (n_{\\text{max}})] \\in \\mathbb{R}^{K\\times n_{\\text{max}}}$, $T = [t (1),...,t (N_{\\text{max}})] \\in \\mathbb{R}^{L\\times n_{\\text{max}}}$, $u (n) = [u_1(n), ..., u_K(n)]^\\top$, $t (n) = [t_1 (n), . . ., t_L (n)]^\\top$. The fuzzy module with Q fuzzy rules is formulated as\n\n$\\begin{cases}  \\text{Rule: If } u_1 (n) \\text{ is } A_1^i, u_2 (n) \\text{ is } A_2^i, ..., \\text{and} u_k (n) \\text{ is } A_K^i, \\\\   \\begin{aligned}  &\\text{Then:} \\\\  &\\begin{cases}   x^i (n) = g (W_{in}^i u(n) + W_r^i x^i(n - 1) + b^i) \\\\   y^i(n) = W_{out, N}^i (x^i (n), u(n))   \\end{cases}   \\end{aligned} \\\\   i = 1, 2, ..., Q,  \\tag{15} \\end{cases}$\n\nwhere $W_{in}^i$, $W_{r}^i$, $b^i$, and $W_{out, N}^i$ are the input weight matrix, reservoir weight matrix, input bias, and output weight matrix of the i-th sub-reservoir, respectively. $x^i (n)$ and $y^i (n)$ denote the reservoir state and output of the i-th sub-reservoir."}, {"title": "3.1 Construction of fuzzy RSCNS", "content": "As shown in Figure 1, fuzzy RSCNs consist of five layers, namely the input layer, rule firing layer, normalization layer, enhancement layer, and output layer. The functions of each layer are described in this subsection.\n\u2022 Input layer\nThe input layer is the first layer in the network, which is responsible for receiving and processing the raw input data so that the subsequent layers can better learn the input information. The number of nodes in this layer is the dimension of the input data. Assume the n-th sample is employed to train the network, then, the input is $u (n) = [u_1 (n), ..., u_K (n)]^\\top \\in \\mathbb{R}^K$.\n\u2022 Rule firing layer\nThe second layer is the rule firing layer and each node in this layer corresponds to a fuzzy rule. The fire strength reflects the degree to which associated rule conditions are satisfied by the input information. The fuzzy membership function can be used for calculating the fire strength $\\psi_i$ of the i-th fuzzy rule, represented by a Gaussian function as\n\n$\\begin{equation}  \\psi_i = \\prod_{k=1}^{K} \\exp \\left( -\\frac{(u_k - c_k^i)^2}{2(\\sigma_k^i)^2} \\right), i = 1, 2, ..., Q, \\tag{16} \\end{equation}$\n\nwhere $c_k^i$ and $\\sigma_k^i$ are the center and width of the k-th input for the i-th fuzzy rule, respectively."}, {"title": "Normalization layer", "content": "\u2022 Normalization layer\nThe third layer is the normalization layer, whose function is to adjust the distribution of $\\{\\psi_1, \\psi_2, ..., \\psi_Q \\}$, and improve the stability and training speed of the model. The fire strength of each fuzzy rule given in the last layer is normalized by\n\n$\\begin{equation}  \\varphi_i = \\frac{\\psi_i}{\\sum_{i=1}^{Q} \\psi_i} \\tag{17} \\end{equation}"}, {"title": "Enhancement layer", "content": "\u2022 Enhancement layer\nThe fourth layer is the enhancement layer, where multiple sub-reservoirs are used to process the dynamic nonlinear data. Each sub-reservoir is constructed by using the RSC algorithm, which guarantees the universal approximation capability of the network. The state and output of the i-th sub-reservoir with N reservoir nodes can be calculated by\n\n$\\begin{equation}  x^i_N (n) = g (W_{inN}^i u(n) + W_{rN}^i x^i_N(n - 1) + b^i_N), \\tag{18} \\end{equation}$\n\n$\\begin{equation}  y^i(n) = W_{out, N}^i (x^i (n), u(n)), \\tag{19} \\end{equation}$\n\nwhere $W_{in, N}^i$, $W_{r, N}^i$, $b^i_N$, $W_{out, N}^i$, and $x^i (n)$ are the input weight matrix, reservoir weight matrix, input bias, output weight matrix, and reservoir state of the i-th sub-reservoir with N reservoir nodes. The residual error is denoted as $e_N = Y^i - T$, where $Y^i = [y^i (1), y^i (2), ..., y^i (n_{\\text{max}})]$ and $T = [t (1), t (2), ...t (N_{\\text{max}})]$ are the model output of the i-th sub-reservoir and the desired output, respectively. Once $e_N$ cannot satisfy the preset error threshold, it is necessary to add nodes under the supervisory mechanism. The configuration process can be summarized as follows.\nStep 1: Generate $[w^i_{N+1,1}, w^i_{N+1,2}, ..., w^i_{N+1,K}]$, $[w^r_{N+1,1}, w^r_{N+1,2}, ..., w^r_{N+1,N+1}]$, and $b_{N+1}^i$ stochastically in $G_{\\text{max}}$ times from the adjustable uniform distribution $[-1, \\lambda]$, $\\lambda \\in \\{\\lambda_{\\text{min}}, \\lambda_{\\text{min}} + \\Delta\\lambda, ..., \\lambda_{\\text{max}}\\}$. Construct the input weight matrix, bias matrix, and the reservoir weight matrix with a special structure,\n\n$\\begin{equation}  \\begin{aligned}  &W_{inN+1}^i = \\begin{bmatrix}   w_{1,1}^i & w_{1,2}^i & ... & w_{1,K}^i\\\\   w_{2,1}^i & w_{2,2}^i & ... & w_{2,K}^i\\\\   : & : & & :\\\\   w_{N,1}^i & w_{N,2}^i & ... & w_{N,K}^i\\\\   w_{N+1,1}^i & w_{N+1,2}^i & ... & w_{N+1,K}^i  \\end{bmatrix} , \\quad b^i_{N+1} = [b_1^i, b_2^i,...,b_{N+1}^i]^\\top,  \\\\  &W_{rN+1}^i = \\begin{bmatrix}   w_{1,1}^r & 0 & 0 & 0 \\\\   w_{2,1}^r & w_{2,2}^r & 0 & 0 \\\\   : & : &  & :\\\\   w_{N,1}^r & w_{N,2}^r & ... & w_{N,N}^r & 0 \\\\   w_{N+1,1}^r & w_{N+1,2}^r & ... & w_{N+1,N}^r & w_{N+1,N+1}^r   \\end{bmatrix},  \\end{aligned} \\tag{20} \\end{equation}$\n\nRemark 1 To ensure the echo state property of the network, $W_{rN+1}^i$ should be reset as\n\n$\\begin{equation}  W_{rN+1}^i = \\alpha \\frac{W_{rN+1}^i}{\\rho(W_{rN+1}^i)}, \\tag{21} \\end{equation}$\n\nwhere $0 < \\alpha < 1$ is the scaling factor, and $\\rho (W_{rN+1}^i)$ is the maximum eigenvalue value of $W_{rN+1}^i$.\nStep 2: Seek the random basis function $g_{N+1}$ satisfying the following inequality constraint with an increasing factor $r_m$, $m = 1,2,..., t$, $0 1, $\n\n$\\begin{equation}  \\xi_{N+1,q} = \\frac{(e_{N,q}, g_{N+1})^2}{(1 - \\mu_{N+1} - r) \\|e_{N,q}\\|^2  \\\\frac{<g_{N+1},g_{N+1}>}{e_{N,q}^\\top e_{N,q}}}  \\tag{24} \\end{equation}"}, {"title": "3.2 Echo state property of fuzzy RSCNS", "content": "The echo state property enhances the model's ability to handle intricate scenarios such as long-term dependencies, high-dimensional inputs, real-time data, and dynamic environments. This unique property allows the network to retain and echo previous input information in future time steps, thereby facilitating the analysis and processing of temporal data.\nTheorem 2. Define two reservoir states $x^1_{N+1} (n)$ and $x^2_{N+1} (n)$ for the i-th sub-reservoir, which are driven by the same input sequence but start from different initial conditions. The proposed fuzzy RSCN holds the echo state property if the scaling factor is chosen as\n\n$\\begin{equation}  0 < \\alpha < \\frac{\\rho (W_{r,N+1})}{\\sigma_{\\text{max}} (W_{r,N+1})}, \\tag{27} \\end{equation}$\n\nwhere $\\sigma_{\\text{max}} (W_{r,N+1})$ is the the maximal singular value of $W_{r,N+1}$.\nProof. The error between $x^1_{N+1} (n)$ and $x^2_{N+1} (n)$ can be expressed as\n\n$\\begin{equation}  \\begin{aligned}  \\left\\|x^1_{N+1} (n) -x^2_{N+1} (n)\\right\\| &= \\left\\|g \\left(W_{in,N+1}u(n) + W_{r,N+1}x^1_{N+1} (n - 1) + b^1_{N+1}\\right) \\\\  &-g \\left(W_{in,N+1}u(n) + W_{r,N+1}x^2_{N+1} (n - 1) + b^1_{N+1}\\right)\\right\\|,  \\end{aligned} \\tag{28} \\end{equation}$\n\nwhere $g$ is selected from the sigmoid or tanh function. According to the Lipschitz condition, yields\n\n$\\begin{equation}  \\begin{aligned}  \\left\\|x^1_{N+1} (n) -x^2_{N+1} (n)\\right\\| &\\leq \\text{max} (g') \\left\\| W_{r,N+1}x^1_{N+1} (n - 1) - W_{r,N+1}x^2_{N+1} (n - 1)\\right\\|  \\\\  &\\leq \\left\\|W_{r,N+1}\\right\\| \\left\\|x^1_{N+1} (n - 1) -x^2_{N+1} (n-1)\\right\\|^2  \\\\  &\\leq...\\leq \\left\\|W_{r,N+1}\\right\\|^n \\left\\|x^1_{N+1} (0) -x^2_{N+1} (0)\\right\\|^2.  \\end{aligned} \\tag{29} \\end{equation}"}, {"title": "3.3 Universal approximation property of fuzzy RSCNs", "content": "Fuzzy RSCNs have the ability to approximate any complex functions, offering a significant advantage in modelling un-certain complex industrial systems. This subsection provides a detailed theoretical proof of the universal approximation property for the proposed approach.\nTheorem 3. Suppose that span (\u0393) is dense in $L_2$ space. For $b \\in \\mathbb{R}^+, \\forall g \\in \\Gamma, 0 < ||g|| < b$. Given $0 < r < 1$ and a nonnegative real sequence $\\{\\mu_{N+1}\\}$ satisfying $\\lim_{N\\to\\infty} \\mu_{N+1} = 0$, and $\\mu_{N+1} \\leq (1 - r)$. For N = 1,2..., q = 1, 2, ..., L, and i = 1, 2, ..., Q, define\n\n$\\begin{equation}  \\delta_{N+1} = \\sum_{q=1}^{L} \\delta_{N+1,q}, \\delta_{N+1,q} = (1 - r - \\mu_{N+1}) \\|e_{N,q}\\|^2 . \\tag{31} \\end{equation}$\n\nIf $g_{N+1}$ satisfies the following inequality constraints\n\n$\\begin{equation}  \\frac{(e_{N,q}, g_{N+1})^2}{\\|g_{N+1}\\|^2} \\geq \\delta^2\\delta_{N+1,q}, q = 1, 2, ... L, \\tag{32} \\end{equation}$\n\nand the output weight is constructively evaluated by\n\n$\\begin{equation}  W_{out, N+1,q}^* = \\frac{(e_{N,q}, g_{N+1})}{\\|g_{N+1}\\|^2}. \\tag{33} \\end{equation}$\n\nThen, we have $\\lim_{N\\to\\infty} \\|e_{N+1}\\| = 0$.\nProof. Note that\n\n$\\begin{equation}  \\begin{aligned}  \\|e_{N+1}\\|^2 &= \\|e_{N}\\|^2 - (r + \\mu_{N+1}) \\|e_{N}\\|^2  \\\\  &=\\sum_{q=1}^{L}e_{N,q} - \\sum_{q=1}^{L} \\left( W_{out, N+1,q}g_{N+1}\\right) - \\sum_{q=1}^{L} (r + \\mu_{N+1}) \\left(e_{N,q}, e_{N,q}\\right)  \\\\  &= (1 - r - \\mu_{N+1}) \\|e_{N}\\|^2 - (2\\sum_{q=1}^{L} \\frac{(e_{N,q}, g_{N+1})}{\\|g_{N+1}\\|^2} g_{N+1}, e_{N,q}) - (\\sum_{q=1}^{L} \\frac{(e_{N,q}, g_{N+1})}{\\|g_{N+1}\\|^2} g_{N+1}, \\sum_{q=1}^{L} \\frac{(e_{N,q}, g_{N+1})}{\\|g_{N+1}\\|^2} g_{N+1})  \\\\  &= (1 - r - \\mu_{N+1}) \\|e_{N}\\|^2 - \\sum_{q=1}^{L} \\frac{(e_{N,q}, g_{N+1})^2}{\\|g_{N+1}\\|^2}  \\\\  &\\leq \\sum_{q=1}^{L} \\delta_{N+1} \\sum_{q=1}^{L} -\\sum_{q=1}^{L} \\frac{(e_{N,q}, g_{N+1})^2}{\\|g_{N+1}\\|^2}  \\\\  &= \\sum_{q=1}^{L} \\delta_{N+1} \\sum_{q=1}^{L} -\\sum_{q=1}^{L} \\delta_{N+1}  \\\\  &\\leq 0.  \\end{aligned} \\tag{34} \\end{equation}$\n\nThus, we have $\\|e_{N+1}\\|^2 - (r + \\mu_{N+1}) \\|e_{N}\\|^2 \\leq 0$. Notably, the global least squares method is used to calculate the output weight in our proposed algorithm. We can obtain the optimum output weight $W_{out, N+1} = [W_{out, 1}, W_{out, 2}, ..., W_{out, N+1+K}]$ and residual error $e_{N+1}$ based on Eq. (24). It is easily inferred that $\\|e_{N+1}\\|^2 \\leq \\|e_{N+1}^\\dagger\\|^2$. Therefore, the following inequalities can be established\n\n$\\begin{equation}  \\begin{aligned}  \\|e_{N+1}\\|^2 \\leq r \\|e_{N}\\|^2 + \\gamma_{N+1}, (\\gamma_{N+1} = \\mu_{N+1} \\|e_{N}\\|^2 \\geq 0) .  \\end{aligned} \\tag{35} \\end{equation}$\n\nNote that $\\lim_{N\\to\\infty} \\gamma_{N+1} = 0$. Obviously, $\\lim_{N\\to\\infty} \\|e_{N+1}\\|^2 = 0$, which implies $\\lim_{N\\to\\infty} \\|e_{N+1}\\| = 0$. This completes the proof."}, {"title": "3.4 Parameter learning of fuzzy RSCNs", "content": "Due to the dynamic changes in the process environment, we need to update learning parameters to cope with various complex uncertainties. In this subsection, the projection algorithm [23] is used to update the output weights online. Combining Eq. (19) and Eq. (26), the model output can be written as\n\n$\\begin{equation}  y(n) = W_{out,1}\\varphi_1g^1 (n) + .... + W_{out,Q}\\varphi_Qg^Q (n), \\tag{36} \\end{equation}$\n\nwhere $W_{out}$ is the output weight of the i-th sub-reservoir and $\\varphi_ig^i (n) = (x^i (n), u(n))$. Define $\\Theta = [W_{out,1}, ..., W_{out,Q}]$\nand $G (n) = [\\varphi_1g^1 (n), ..., \\varphi_Qg^Q (n)]$, we have\n\n$\\begin{equation}  y(n) = \\Theta G (n). \\tag{37} \\end{equation}"}, {}]}