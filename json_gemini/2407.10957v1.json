{"title": "Ref-AVS: Refer and Segment Objects in\nAudio-Visual Scenes", "authors": ["Yaoting Wang", "Peiwen Sun", "Dongzhan Zhou", "Guangyao Li", "Honggang Zhang", "Di Hu"], "abstract": "Traditional reference segmentation tasks have predominantly\nfocused on silent visual scenes, neglecting the integral role of multimodal\nperception and interaction in human experiences. In this work, we in-\ntroduce a novel task called Reference Audio-Visual Segmentation (Ref-\nAVS), which seeks to segment objects within the visual domain based on\nexpressions containing multimodal cues. Such expressions are articulated\nin natural language forms but are enriched with multimodal cues, includ-\ning audio and visual descriptions. To facilitate this research, we construct\nthe first Ref-AVS benchmark, which provides pixel-level annotations for\nobjects described in corresponding multimodal-cue expressions. To tackle\nthe Ref-AVS task, we propose a new method that adequately utilizes mul-\ntimodal cues to offer precise segmentation guidance. Finally, we conduct\nquantitative and qualitative experiments on three test subsets to com-\npare our approach with existing methods from related tasks. The results\ndemonstrate the effectiveness of our method, highlighting its capability\nto precisely segment objects using multimodal-cue expressions.", "sections": [{"title": "1 Introduction", "content": "In the real world, visual scenes are often accompanied by diverse multimodal\ninformation\u00b9, including audio and text modalities. This additional multimodal"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Referring Video Object Segmentation (R-VOS)", "content": "The main objective of R-VOS is to perform object segmentation in stream-\ning videos based on given natural language expressions. Initially proposed in\n2018, A2D-Sentence [14] aims to segment actors in video content based on"}, {"title": "2.2 Audio-Visual Segmentation (AVS)", "content": "The multimodal segmentation in audio-visual scenes, which is of great concern\nto us, has received some attention in AVS [13,38,47]. AVS aims to obtain finer\npixel-level masks corresponding to sound-emitting objects in the visual space.\nIn contrast, previous studies on audio-visual localization [17,37] use heat maps\nfor coarse localization in an unsupervised learning manner. The existing works\non AVS can be broadly categorized into fusion-based methods [13, 18, 23, 25,\n26,47] and prompt-based methods [27, 29, 30, 38]. The former primarily focuses\non localizing sounding objects by fusing audio and visual features, while the\nlatter emphasizes constructing effective audio prompts for the visual foundation\nmodel. However, AVS simply segments all sound-emitting objects in the visual\nspace, without the flexibility to combine the multimodal cues to segment specific\nobjects of interest."}, {"title": "2.3 Language-aided Audio-Visual Scene Understanding", "content": "Currently, there is a limited number of public tasks offering datasets for audio-\nvisual scene comprehension accompanied by language assistance. Two notable\nexamples of such datasets are Music-AVQA [22] and AVQA [43], both encom-\npassing audio, visual, and language information. These datasets contain rich\naudio-visual components and focus on annotations for textual questions and an-\nswers, emphasizing temporal (Before/After), spatial cues (Left/Right), etc.\nResearchers [4,19,21] acknowledge the significance of the provided question,\nas it guides the feature extraction process for both audio and visual signals. Con-\nsequently, they have made efforts to identify pertinent segments by evaluating\nthe semantic similarity between the question and temporal segments. Further-\nmore, [21] has been dedicated to locating crucial areas by leveraging semantic\nsimilarity between questions and visual tokens. Clearly, existing explorations\nhave significantly advanced the research on language-aided audio-visual scene\nunderstanding. Nevertheless, the works that rely on the aforementioned datasets"}, {"title": "3 Refer-AVS Dataset", "content": ""}, {"title": "3.1 Object Category", "content": "To ensure a diverse range of referred objects, we have carefully selected a wide\nvariety of audible objects in 48 categories as well as a smaller selection of static,\nunsoundable objects in 3 categories. For the objects that can produce sound, we\nhave chosen 20 categories of musical instruments, 8 of animals, 15 of machines,\nand 5 of humans. In the special case of humans, considering their diverse ap-\npearances, voices, and actions, we have employed the concept of morphology and\nclassified them into 5 categories, based on age and gender."}, {"title": "3.2 Video Selection", "content": "During the process of video collection, we employed the techniques presented\nin [3,47] to ensure the alignment of audio and visual snippets with the intended\nsemantics. All videos were sourced from YouTube under the Creative Commons\nlicense, and each video was trimmed to a duration of 10 seconds. Throughout\nthe manual collection process, we deliberately avoided videos falling into several\ncategories (detailed in the appendix): 1) Videos with a large number of instances\nof the same semantics; 2) Videos characterized by extensive editing and camera\nswitching; 3) Non-realistic videos containing synthetic artifacts.\nTo raise the alignment with real-world distributions, we carefully select videos\nthat contribute to the diversification of scenes within our dataset. Specifically, we\nfocus on selecting videos that involve interactions between multiple objects, such\nas musical instruments, people, vehicles, etc. The rich combination of categories\nindicates that our dataset is not limited to a narrow set of scenarios but rather\nencompasses a broad spectrum of real-life scenes where such objects are likely\nto naturally appear together. Refer to the supplementary materials for details.\nIn addition to diversity, we also carefully filter the videos to ensure that the\ndataset includes scenes with greater complexity and a larger number of objects.\nSpecifically, 56% of the total videos contain two or more objects, while 13% of\nthe total videos contain three or more objects."}, {"title": "3.3 Expressions", "content": "The diversity of expressions is one of the core factors in the assembly of the Ref-\nAVS dataset. The expressions consist of three dimensions of information, namely\naudio, vision, and time. The auditory dimension incorporates characteristics such\nas volume and rhythm while the visual dimension encompasses attributes like\nthe appearance and spatial configuration of objects. We also leverage temporal"}, {"title": "3.4 Segmentation Masks", "content": "We divide each 10-second video into ten equal 1-second snippets and the objec-\ntive of annotation is to acquire the first frame's mask for each snippet. For these\nsampled frames, the ground truth labels are binary masks indicating the target\nobject, according to expressions and multimodal information.\nTo obtain such a mask, initially, we manually select the pivotal frames for\neach 10-second video in which the target object is present. These pivotal frames"}, {"title": "3.5 Dataset Statistics", "content": "In addition, we compare Refer-AVS with other popular audio-visual benchmarks\nin the Tab. 1. The Flickr-SoundNet [35] and VGG-SS [2] benchmarks are labeled\nat a patch level through bounding boxes and possess around 5,000 frame-level an-\nnotations. Compared with our dataset with pixel-level annotations, these bench-\nmarks have a significantly lower quantity of annotations. Regarding the AVS\ndataset [45-47], the videos in our dataset contain a higher average number of\nobjects about 1.72 objects per video, implying that our scenarios are more chal-\nlenging with multiple sound sources and multiple semantics. Within such sce-\nnarios, our Ref-AVS benchmark is particularly valuable as it demonstrates the\nability to effectively focus on objects of real interest. Our dataset also has more\nuniform video durations and a more refined video selection process than AVS.\nWhen it comes to datasets for R-VOS tasks, we maintain a consistent advantage\nin the number of videos compared to other datasets [9,12,14,20,36]. Moreover, we\npossess a considerable volume of data encompassing a large number of objects,\nexpressions, and complex scenes.\nOverall, our Ref-AVS dataset encompasses a substantial collection of 20,000\nexpressions and pixel-level annotations spread across 4,000 videos, totaling more\nthan 11 hours. As a result, we believe that this benchmark adequately fulfills the\nrequirements for facilitating research on the Ref-AVS task, while also presenting\na significant level of challenge in this domain. We aim to continuously expand\nthe dataset for broader community needs, like training larger foundation models."}, {"title": "3.6 Dataset Split", "content": "As shown in Tab. 2, the complete dataset is divided into three sets, i.e., a training\nset of 2908 videos, a validation set of 276 videos, and a test set of 818 videos. The\nvideos in the test set and their corresponding annotations undergo a meticulous\nreview and re-annotation process conducted by experienced workers. In order to\nthoroughly evaluate the models' performance in the Ref-AVS task, the test set\nis further divided into three distinct subsets, each serving a specific purpose.\nSeen: The first test subset, referred to as the \"seen test set\", comprises ob-\nject categories that have already appeared in the training set. The purpose of\nestablishing this subset is to evaluate the model's fundamental performance and\nassess how well it generalizes to familiar object categories.\nUnseen: To address the growing demand for the generalization capabilities\nof various models in an open-world scenario, an additional test set was created\nspecifically to assess the model's ability to generalize to unseen audio-visual\nscenes. This \"unseen test set\" consists of object categories that did not appear\nin the training set, although their super-categories (e.g., animal, vehicle) may\nhave been present in the training set. The purpose of this test set is to evaluate\nthe model's capacity to generalize to novel object categories while leveraging its\nunderstanding of broader super-categories.\nNull: A \"null reference problem\" arises when an expression refers to an object\nthat either does not exist or is not visible in the given context. A model that\naccurately understands expression guidance should not segment any object in\nscenarios of a null reference [39]. Based on this consideration, we have developed\na \"null subset\" to test the model's robustness against such challenges. This subset\ncomprises object categories that were present during training but are paired with\nexpressions that do not correlate, ensuring all objects in the video frames are\nirrelevant to the given reference expression. Therefore, the true masks for this\nsubset are empty, and the model should refrain from segmenting any object."}, {"title": "4 Expression Enhancing with Multimodal Cues", "content": ""}, {"title": "4.1 Overall Architecture", "content": "Ref-AVS is designed to locate objects of interest in dynamic audio-visual scenes\nby utilizing multimodal cues. To solve this challenging task, we propose the"}, {"title": "4.2 Multimodal Representations", "content": "Audio: Similar to the video processing, we divide the audio input into clips at\n1-second intervals. Audio representations $F_a \\in \\mathbb{R}^{t\\times d_a}$ is encoded from VGGish\n[15,16], where t represents the duration of the audio in seconds and matches the\nnumber of video frames. The audio representations are extracted offline and the\naudio encoder is not fine-tuned.\nVisual: We sample t frames from the video input at 1-second intervals and\nextract visual representations $F_v \\in \\mathbb{R}^{t\\times d_v\\times H\\times W}$ using a pre-trained Swin-base\nmodel [11]. The visual encoder is not fine-tuned.\nExpression: We use RoBERTa [8,28] as our text encoder to extract textual\nexpression features $F_T \\in \\mathbb{R}^{L\\times d_r}$, where L denotes the length of the expression.\nThe expression representations are obtained off-the-shelf without fine-tuning."}, {"title": "4.3 Temporal Bi-modal Transformer", "content": "Temporal A-T & V-T Fusion: This module is introduced to retrieve the\nexpression-related information of each modality. Firstly, for the convenience of\nthe following multimodal fusion, we prepare the projected audio feature $F_A \\in$\n$\\mathbb{R}^{t\\times 1\\times d_v}$, downsampled and flattened visual feature $F_v \\in \\mathbb{R}^{t\\times(\\frac{H}{8} \\times \\frac{W}{8})\\times d_v}$, and\nprojected text feature $F_T \\in \\mathbb{R}^{t\\times L\\times d_v}$ which is expanded along the temporal di-\nmension. Subsequently, the features are combined to yield the expression-related\nmultimodal features $F_A$, $F_v$ and modality-aware expression features $F_{TA}$, $F_{TV}$,\nwith the same dimension as $F_A'$, $F_v'$, $F_T'$, respectively:\n$F_M, F_{TM} = MF(Concat(F_M'; F_T')), M \\in \\{A, V\\}$  (1)\nwhere $Concat(\\cdot)$ denotes the concatenate operation, and $MF(\\cdot)$ denotes the\nmodality fusion function, which is employed as self-attention.\nCached Memory: Although the aforementioned fusion method can combine\ncross-modal information, it does not give much emphasis to strong temporal\ndynamics. To address this issue, we introduce a straightforward cached memory\n$CA \\in \\mathbb{R}^{t\\times 2\\times d_r}$, $Cv \\in \\mathbb{R}^{t\\times(\\frac{W}{8}\\times\\frac{H}{8}+1)\\times d_r}$ to explicitly capture the temporal muta-\ntions as they occur. The cache memory is required to store the mean modality\nfeatures in the temporal domain from the beginning to the current moment.\nThen, the multimodal cues feature $Q_A$ and $Q_V$ output from temporal-aware\ntransformers can be calculated as\n$Q_M = (\\beta + 1)F_M \u2013 \\beta C_M, M \\in \\{A, V\\}$, (2)\n$\\begin{equation}C_M = \\begin{cases}0, & \\text{if } \\tau = 1,\\\\frac{1}{\\tau - 1} \\sum_{t=0}^{\\tau - 1} F_M, & \\text{if } \\tau > 1,\\end{cases} \\end{equation}$ (3)\nwhile \u03c4 is the time stamp of the specific feature at the temporal dimension and\n$\\beta$ is a hyper-parameter to control the sensitivity to temporal dynamics. When\nthe current audio or visual features do not change significantly compared to the\nmean of past features (i.e., $F_M \\approx C_M$), QM remains nearly unchanged. However,\nin cases where the current change is drastic (i.e., $F_M \\neq C_M$), the cached memory\ncan amplify the difference in the current feature, leading to an output with salient\nfeatures. It is important to note that text cue feature QT inherently possesses a\nhighly abstracted semantic nature, which means manipulation may not be neces-\nsary at this stage. Therefore, at this stage, we combine and average the original\ntextual features with the text features enhanced by multimodal information, re-\nsulting in a comprehensive enhanced text feature $Q_T = mean(F_T' +F_{TA}+F_{TV})$.\nFinally, we obtain the processed cue feature of each modality QA, Qv and QT."}, {"title": "Modality Encoding", "content": "The modality encoding approach involves incorporating\nmodality identify tokens $[aud] \\in \\mathbb{R}^{1\\times d_v}$ for audio modality and $[vis] \\in \\mathbb{R}^{1\\times d_v}$\nfor visual modality into the following multimodal cues integration process. Since\nonly two tokens are required to split the sequence into three segments, we only\nneed to manipulate the audio and visual cue features."}, {"title": "4.4 Prompting with Multimodal Cues", "content": "In this Prompting with Multimodal Cues (PMC) phrase, when we obtain the\nfinal multimodal cues features, we can concatenate the multimodal cues together\nand then apply self-attention to obtain comprehensive multimodal cues $Q_m \\in$\n$\\mathbb{R}^{t\\times(\\frac{H}{8}\\times\\frac{W}{8}+L+3)\\times d_v}$ through the Multimodal Integration Transformer:\n$Q_m = MF(Concat([Q_A; [aud]; Q_V; [vis]; Q_T])),$ (4)\nwhere MF is the multimodal fusion function serving as the self-attention for mul-\ntimodal cues interaction. Then we utilize these comprehensive multinational cues\nto prompt the learnable mask queries $q \\in \\mathbb{R}^{N\\times d_v}$ in the transformer-based seg-\nmentation decoder with cross-attention, where N is the number of mask queries:\n$q_Q = CATF(query = q, key = Q_m, value = Q_m),$ (5)\nwhere $q_Q \\in \\mathbb{R}^{N\\times d_v}$ is the updated mask queries. $CATF(\\cdot)$ is the cross-attention\ntransformer with q and Qm as the information target and source, respectively."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Implementation Details", "content": "Mask2Former [5] serves as our visual foundation model to provide the com-\nmonly used transformer-based segmentation decoder. By default, in this work,\nall input video frames are scaled to 384 \u00d7 384. The shape of the visual fea-\ntures is $[H = 64, W = 64,d_v = 256]$, and we employ 8-fold downsampling\nto reduce the computation costs. The audio features with $d_a = 128$ are ex-\ntracted from the mono-channel waveform. The text features have a shape of\n$[L = 25,d_r = 768]$. For convenience, we map all modality feature dimensions\nto $d_v$. Hyper-parameter \u03b2 is set to 1 by default. Transformer layers $N_L$ of the\nTemporal Bi-modal Transformer, Multimodal Integration Transformer and the\ncross-attention transformer CATF are set to 4 by default. The number of mask\nqueries $N_q$ is fixed to 100."}, {"title": "5.2 Evaluation Metrics", "content": "To conduct a comprehensive evaluation of our Ref-AVS method, we employ the\nJaccard index (J) and F-score (F) as performance metrics. Additionally, we\nintroduce S in the null reference test set to assess the efficacy of expression\nguidance in the model. S denotes the square root of the ratio between the pre-\ndicted mask area and the background area, a higher value indicates a larger\nproportion of the mask relative to the background area, suggesting a lack of\nprecise expression guidance provided to the model."}, {"title": "5.3 Quantitative Results", "content": "We compare our Ref-AVS method with existing approaches in the relevant field\nusing our Ref-AVS benchmark. The results from the seen test set demonstrate\nthe superior performance of our Ref-AVS method on this dataset, outperforming\nother methods by a significant margin. To ensure fairness, we equip the other\nmethods with the same inputs from both the audio and visual modalities as\nours. However, even with this additional information, these methods still fail to\nachieve satisfactory results. The result indicates that simple modal fusion is in-\nadequate for addressing the challenges of understanding multimodal cues within\nthe Ref-AVS task. Instead of directly incorporating audiovisual information, our\napproach selects textual representations as carriers of multimodal information\nbecause they contain rich semantics and cues that are contextually relevant to\nthe current audiovisual environment.\nWe conduct tests on the unseen test set and the null reference test set to\nexplore the generalization and multimodal cues-following ability. Our method\nstill maintains a leading position in the unseen test set because we leverage text\nwith highly abstract semantic capabilities as a multimodal information carrier,\ninstead of directly fusing audio and visual information. Therefore, the obtained\nmultimodal cues can provide more robust semantic guidance. On the null test set,\nwe obtain leading results among all other methods, indicating that our method\ncan perceive multimodal cues to a reasonably accurate degree. Compared to\nother transformer-based and query-based frameworks, AVSBench, as a classic\nAVS method, obtains inferior results. This may be attributed to the model simply\nfuses multimodal cues and visual features in a progressive fashion, resulting in\nrelatively weak guidance over the visual space."}, {"title": "5.4 Qualitative Results", "content": "We visualize the segmentation masks on the test set of the Ref-AVS Bench and\ncompare them with AVSegFormer and ReferFormer, as shown in Fig. 4. From\nthese qualitative results, we observe that both ReferFormer in the Ref-VOS\ntask and AVSegFormer in the AVS task fail to accurately segment the object\ndescribed in the expression. Specifically, AVSegformer struggles to fully compre-\nhend the expression and tends to directly generate the sound source. This issue is\nexemplified in the bottom-left sample, where AVSegformer erroneously segments\nthe vacuum cleaner instead of the boy. On the other hand, Ref-VOS may not\nadequately understand the audio-visual scene and thus mistakenly identify the\ntoddler as the piano player, as shown in the top-right sample. In contrast, our\nRef-AVS method demonstrates a superior capability to simultaneously process\nmulti-modal expressions and scenes, enabling it to accurately interpret the user\ninstruction and segment the intended object of interest."}, {"title": "5.5 Ablation Study", "content": "We conduct ablation studies to investigate the impact of two modalities (audio\nand text) information on the Ref-AVS task, as well as the effectiveness of the\nproposed method. As shown in the Tab. 4, we observe that in setting, removing\nthe text information results in a significant performance degradation of 11.44% in"}, {"title": "6 Conclusion", "content": "In this work, we introduce a novel task, Ref-AVS, which requires the machine to\nsegment objects of interest in dynamic audio-visual scenes based on expressions\nthat incorporate multimodal cues and temporal information. To support this\nfield, we develop the first benchmark, Ref-AVS Bench, for performance train-\ning and evaluation. Our method, EEMC, establishes a robust baseline with an\nadvanced multimodal cues fusion module. We compare our method with sev-\neral existing approaches on the Ref-AVS Bench dataset and demonstrate its\npromising performance in accurately locating objects using multimodal refer-\nence expressions. Essentially, Ref-AVS leverages audio, visual, and textual data\nto enrich real-world scene comprehension. Our task has potential in applications\nsuch as attention focusing and object editing in immersive audio-visual scenar-\nios like extended reality, thereby enhancing user interaction experiences. In the\nfuture, we plan to further expand the scale of the current dataset to meet the\ngrowing data demands in the era of large language models."}]}