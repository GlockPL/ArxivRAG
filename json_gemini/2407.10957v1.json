{"title": "Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes", "authors": ["Yaoting Wang", "Peiwen Sun", "Dongzhan Zhou", "Guangyao Li", "Honggang Zhang", "Di Hu"], "abstract": "Traditional reference segmentation tasks have predominantly focused on silent visual scenes, neglecting the integral role of multimodal perception and interaction in human experiences. In this work, we introduce a novel task called Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment objects within the visual domain based on expressions containing multimodal cues. Such expressions are articulated in natural language forms but are enriched with multimodal cues, including audio and visual descriptions. To facilitate this research, we construct the first Ref-AVS benchmark, which provides pixel-level annotations for objects described in corresponding multimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method that adequately utilizes multimodal cues to offer precise segmentation guidance. Finally, we conduct quantitative and qualitative experiments on three test subsets to compare our approach with existing methods from related tasks. The results demonstrate the effectiveness of our method, highlighting its capability to precisely segment objects using multimodal-cue expressions. Dataset is available at https://gewu-lab.github.io/Ref-AVS.", "sections": [{"title": "1 Introduction", "content": "In the real world, visual scenes are often accompanied by diverse multimodal information\u00b9, including audio and text modalities. This additional multimodal"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Referring Video Object Segmentation (R-VOS)", "content": "The main objective of R-VOS is to perform object segmentation in streaming videos based on given natural language expressions. Initially proposed in 2018, A2D-Sentence [14] aims to segment actors in video content based on"}, {"title": "2.2 Audio-Visual Segmentation (AVS)", "content": "The multimodal segmentation in audio-visual scenes, which is of great concern to us, has received some attention in AVS [13,38,47]. AVS aims to obtain finer pixel-level masks corresponding to sound-emitting objects in the visual space. In contrast, previous studies on audio-visual localization [17,37] use heat maps for coarse localization in an unsupervised learning manner. The existing works on AVS can be broadly categorized into fusion-based methods [13, 18, 23, 25, 26,47] and prompt-based methods [27, 29, 30, 38]. The former primarily focuses on localizing sounding objects by fusing audio and visual features, while the latter emphasizes constructing effective audio prompts for the visual foundation model. However, AVS simply segments all sound-emitting objects in the visual space, without the flexibility to combine the multimodal cues to segment specific objects of interest."}, {"title": "2.3 Language-aided Audio-Visual Scene Understanding", "content": "Currently, there is a limited number of public tasks offering datasets for audio-visual scene comprehension accompanied by language assistance. Two notable examples of such datasets are Music-AVQA [22] and AVQA [43], both encompassing audio, visual, and language information. These datasets contain rich audio-visual components and focus on annotations for textual questions and answers, emphasizing temporal (Before/After), spatial cues (Left/Right), etc. Researchers [4,19,21] acknowledge the significance of the provided question, as it guides the feature extraction process for both audio and visual signals. Consequently, they have made efforts to identify pertinent segments by evaluating the semantic similarity between the question and temporal segments. Furthermore, [21] has been dedicated to locating crucial areas by leveraging semantic similarity between questions and visual tokens. Clearly, existing explorations have significantly advanced the research on language-aided audio-visual scene understanding. Nevertheless, the works that rely on the aforementioned datasets"}, {"title": "3 Refer-AVS Dataset", "content": ""}, {"title": "3.1 Object Category", "content": "To ensure a diverse range of referred objects, we have carefully selected a wide variety of audible objects in 48 categories as well as a smaller selection of static, unsoundable objects in 3 categories. For the objects that can produce sound, we have chosen 20 categories of musical instruments, 8 of animals, 15 of machines, and 5 of humans. In the special case of humans, considering their diverse appearances, voices, and actions, we have employed the concept of morphology and classified them into 5 categories, based on age and gender."}, {"title": "3.2 Video Selection", "content": "During the process of video collection, we employed the techniques presented in [3,47] to ensure the alignment of audio and visual snippets with the intended semantics. All videos were sourced from YouTube under the Creative Commons license, and each video was trimmed to a duration of 10 seconds. Throughout the manual collection process, we deliberately avoided videos falling into several categories (detailed in the appendix): 1) Videos with a large number of instances of the same semantics; 2) Videos characterized by extensive editing and camera switching; 3) Non-realistic videos containing synthetic artifacts.\nTo raise the alignment with real-world distributions, we carefully select videos that contribute to the diversification of scenes within our dataset. Specifically, we focus on selecting videos that involve interactions between multiple objects, such as musical instruments, people, vehicles, etc. The rich combination of categories indicates that our dataset is not limited to a narrow set of scenarios but rather encompasses a broad spectrum of real-life scenes where such objects are likely to naturally appear together. Refer to the supplementary materials for details.\nIn addition to diversity, we also carefully filter the videos to ensure that the dataset includes scenes with greater complexity and a larger number of objects. Specifically, 56% of the total videos contain two or more objects, while 13% of the total videos contain three or more objects."}, {"title": "3.3 Expressions", "content": "The diversity of expressions is one of the core factors in the assembly of the Ref-AVS dataset. The expressions consist of three dimensions of information, namely audio, vision, and time. The auditory dimension incorporates characteristics such as volume and rhythm while the visual dimension encompasses attributes like the appearance and spatial configuration of objects. We also leverage temporal"}, {"title": "3.4 Segmentation Masks", "content": "We divide each 10-second video into ten equal 1-second snippets and the objective of annotation is to acquire the first frame's mask for each snippet. For these sampled frames, the ground truth labels are binary masks indicating the target object, according to expressions and multimodal information.\nTo obtain such a mask, initially, we manually select the pivotal frames for each 10-second video in which the target object is present. These pivotal frames"}, {"title": "3.5 Dataset Statistics", "content": "In addition, we compare Refer-AVS with other popular audio-visual benchmarks in the Tab. 1. The Flickr-SoundNet [35] and VGG-SS [2] benchmarks are labeled at a patch level through bounding boxes and possess around 5,000 frame-level annotations. Compared with our dataset with pixel-level annotations, these benchmarks have a significantly lower quantity of annotations. Regarding the AVS dataset [45-47], the videos in our dataset contain a higher average number of objects about 1.72 objects per video, implying that our scenarios are more challenging with multiple sound sources and multiple semantics. Within such scenarios, our Ref-AVS benchmark is particularly valuable as it demonstrates the ability to effectively focus on objects of real interest. Our dataset also has more uniform video durations and a more refined video selection process than AVS. When it comes to datasets for R-VOS tasks, we maintain a consistent advantage in the number of videos compared to other datasets [9,12,14,20,36]. Moreover, we possess a considerable volume of data encompassing a large number of objects, expressions, and complex scenes.\nOverall, our Ref-AVS dataset encompasses a substantial collection of 20,000 expressions and pixel-level annotations spread across 4,000 videos, totaling more than 11 hours. As a result, we believe that this benchmark adequately fulfills the requirements for facilitating research on the Ref-AVS task, while also presenting a significant level of challenge in this domain. We aim to continuously expand the dataset for broader community needs, like training larger foundation models."}, {"title": "3.6 Dataset Split", "content": "As shown in Tab. 2, the complete dataset is divided into three sets, i.e., a training set of 2908 videos, a validation set of 276 videos, and a test set of 818 videos. The videos in the test set and their corresponding annotations undergo a meticulous review and re-annotation process conducted by experienced workers. In order to thoroughly evaluate the models' performance in the Ref-AVS task, the test set is further divided into three distinct subsets, each serving a specific purpose.\nSeen: The first test subset, referred to as the \"seen test set\", comprises object categories that have already appeared in the training set. The purpose of establishing this subset is to evaluate the model's fundamental performance and assess how well it generalizes to familiar object categories.\nUnseen: To address the growing demand for the generalization capabilities of various models in an open-world scenario, an additional test set was created specifically to assess the model's ability to generalize to unseen audio-visual scenes. This \"unseen test set\" consists of object categories that did not appear in the training set, although their super-categories (e.g., animal, vehicle) may have been present in the training set. The purpose of this test set is to evaluate the model's capacity to generalize to novel object categories while leveraging its understanding of broader super-categories.\nNull: A \"null reference problem\" arises when an expression refers to an object that either does not exist or is not visible in the given context. A model that accurately understands expression guidance should not segment any object in scenarios of a null reference [39]. Based on this consideration, we have developed a \"null subset\" to test the model's robustness against such challenges. This subset comprises object categories that were present during training but are paired with expressions that do not correlate, ensuring all objects in the video frames are irrelevant to the given reference expression. Therefore, the true masks for this subset are empty, and the model should refrain from segmenting any object."}, {"title": "4 Expression Enhancing with Multimodal Cues", "content": ""}, {"title": "4.1 Overall Architecture", "content": "Ref-AVS is designed to locate objects of interest in dynamic audio-visual scenes by utilizing multimodal cues. To solve this challenging task, we propose the"}, {"title": "4.2 Multimodal Representations", "content": "Audio: Similar to the video processing, we divide the audio input into clips at 1-second intervals. Audio representations \\(F_a \\in \\mathbb{R}^{t \\times d_a}\\) is encoded from VGGish [15,16], where t represents the duration of the audio in seconds and matches the number of video frames. The audio representations are extracted offline and the audio encoder is not fine-tuned.\nVisual: We sample t frames from the video input at 1-second intervals and extract visual representations \\(F_v \\in \\mathbb{R}^{t \\times d_v \\times H \\times W}\\) using a pre-trained Swin-base model [11]. The visual encoder is not fine-tuned.\nExpression: We use RoBERTa [8,28] as our text encoder to extract textual expression features \\(F_T \\in \\mathbb{R}^{L \\times d_r}\\), where L denotes the length of the expression. The expression representations are obtained off-the-shelf without fine-tuning."}, {"title": "4.3 Temporal Bi-modal Transformer", "content": "Temporal A-T & V-T Fusion: This module is introduced to retrieve the expression-related information of each modality. Firstly, for the convenience of the following multimodal fusion, we prepare the projected audio feature \\(F_A \\in \\mathbb{R}^{t \\times 1 \\times d_v}\\), downsampled and flattened visual feature \\(F_v, \\in \\mathbb{R}^{t \\times (\\frac{H}{8} \\times \\frac{W}{8}) \\times d_v}\\), and projected text feature \\(F_T, \\in \\mathbb{R}^{t \\times L \\times d_v}\\) which is expanded along the temporal dimension. Subsequently, the features are combined to yield the expression-related multimodal features \\(F_A, F_v\\) and modality-aware expression features \\(F_{TA}, F_{TV}\\), with the same dimension as \\(F_A', F_v', F_T'\\), respectively:\n\\[F_M, F_{TM} = MF(Concat(F_M'; F_T')), M \\in \\{A, V\\} \\quad(1)\\]\nwhere \\(Concat(\\cdot)\\) denotes the concatenate operation, and \\(MF(\\cdot)\\) denotes the modality fusion function, which is employed as self-attention.\nCached Memory: Although the aforementioned fusion method can combine cross-modal information, it does not give much emphasis to strong temporal dynamics. To address this issue, we introduce a straightforward cached memory \\(C_A \\in \\mathbb{R}^{t \\times 2 \\times d_r}, C_v \\in \\mathbb{R}^{t \\times (W \\times H + 1) \\times d_r}\\) to explicitly capture the temporal mutations as they occur. The cache memory is required to store the mean modality features in the temporal domain from the beginning to the current moment. Then, the multimodal cues feature \\(Q_A\\) and \\(Q_v\\) output from temporal-aware transformers can be calculated as\n\\[Q_M = (\\beta + 1) F_M - \\beta C_M, M \\in \\{A, V\\}, \\quad(2)\\]\n\\[C_M = \\begin{cases} 0, & \\text{if } \\tau = 1, \\\\ \\frac{1}{(\\tau - 1)} \\sum_{\\tau'=1}^{\\tau - 1} F_{\\tau'}, & \\text{if } \\tau > 1, \\end{cases} \\quad(3)\\]\nwhile \\(\\tau\\) is the time stamp of the specific feature at the temporal dimension and \\(\\beta\\) is a hyper-parameter to control the sensitivity to temporal dynamics. When the current audio or visual features do not change significantly compared to the mean of past features (i.e., \\(F_M \\approx C_M\\)), \\(Q_M\\) remains nearly unchanged. However, in cases where the current change is drastic (i.e., \\(F_M \\neq C_M\\)), the cached memory can amplify the difference in the current feature, leading to an output with salient features. It is important to note that text cue feature \\(Q_T\\) inherently possesses a highly abstracted semantic nature, which means manipulation may not be necessary at this stage. Therefore, at this stage, we combine and average the original textual features with the text features enhanced by multimodal information, resulting in a comprehensive enhanced text feature \\(Q_T = mean(F_T' + F_{TA} + F_{TV})\\). Finally, we obtain the processed cue feature of each modality \\(Q_A, Q_v\\) and \\(Q_T\\).\nModality Encoding: The modality encoding approach involves incorporating modality identify tokens \\[aud\\] \\(\\in \\mathbb{R}^{1 \\times d_v}\\) for audio modality and \\[vis\\] \\(\\in \\mathbb{R}^{1 \\times d_v}\\) for visual modality into the following multimodal cues integration process. Since only two tokens are required to split the sequence into three segments, we only need to manipulate the audio and visual cue features."}, {"title": "4.4 Prompting with Multimodal Cues", "content": "In this Prompting with Multimodal Cues (PMC) phrase, when we obtain the final multimodal cues features, we can concatenate the multimodal cues together and then apply self-attention to obtain comprehensive multimodal cues \\(Q_m \\in [][\\mathbb{R}^{t \\times (H \\times W + L + 3) \\times d_v}\\) through the Multimodal Integration Transformer:\n\\[Q_m = MF(Concat([Q_A; \\text{[aud]}; Q_v; \\text{[vis]}; Q_T])), \\quad(4)\\]\nwhere \\(MF\\) is the multimodal fusion function serving as the self-attention for multimodal cues interaction. Then we utilize these comprehensive multinational cues to prompt the learnable mask queries \\(q \\in \\mathbb{R}^{N \\times d_v}\\) in the transformer-based segmentation decoder with cross-attention, where N is the number of mask queries:\n\\[q_Q = CATF(query = q, key = Q_m, value = Q_m), \\quad(5)\\]\nwhere \\(q_Q \\in \\mathbb{R}^{N \\times d_v}\\) is the updated mask queries. \\(CATF(\\cdot)\\) is the cross-attention transformer with q and \\(Q_m\\) as the information target and source, respectively."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Implementation Details", "content": "Mask2Former [5] serves as our visual foundation model to provide the commonly used transformer-based segmentation decoder. By default, in this work, all input video frames are scaled to 384 \u00d7 384. The shape of the visual features is \\([H = 64, W = 64, d_v = 256]\\), and we employ 8-fold downsampling to reduce the computation costs. The audio features with \\(d_a = 128\\) are extracted from the mono-channel waveform. The text features have a shape of \\[L = 25, d_r = 768\\]. For convenience, we map all modality feature dimensions to \\(d_v\\). Hyper-parameter \\(\\beta\\) is set to 1 by default. Transformer layers \\(N_L\\) of the Temporal Bi-modal Transformer, Multimodal Integration Transformer and the cross-attention transformer CATF are set to 4 by default. The number of mask queries \\(N_q\\) is fixed to 100."}, {"title": "5.2 Evaluation Metrics", "content": "To conduct a comprehensive evaluation of our Ref-AVS method, we employ the Jaccard index (\\(J\\)) and F-score (\\(F\\)) as performance metrics. Additionally, we introduce \\(S\\) in the null reference test set to assess the efficacy of expression guidance in the model. \\(S\\) denotes the square root of the ratio between the predicted mask area and the background area, a higher value indicates a larger proportion of the mask relative to the background area, suggesting a lack of precise expression guidance provided to the model."}, {"title": "5.3 Quantitative Results", "content": "We compare our Ref-AVS method with existing approaches in the relevant field using our Ref-AVS benchmark. The results from the seen test set demonstrate the superior performance of our Ref-AVS method on this dataset, outperforming other methods by a significant margin. To ensure fairness, we equip the other methods with the same inputs from both the audio and visual modalities as ours. However, even with this additional information, these methods still fail to achieve satisfactory results. The result indicates that simple modal fusion is inadequate for addressing the challenges of understanding multimodal cues within the Ref-AVS task. Instead of directly incorporating audiovisual information, our approach selects textual representations as carriers of multimodal information because they contain rich semantics and cues that are contextually relevant to the current audiovisual environment.\nWe conduct tests on the unseen test set and the null reference test set to explore the generalization and multimodal cues-following ability. Our method still maintains a leading position in the unseen test set because we leverage text with highly abstract semantic capabilities as a multimodal information carrier, instead of directly fusing audio and visual information. Therefore, the obtained multimodal cues can provide more robust semantic guidance. On the null test set, we obtain leading results among all other methods, indicating that our method can perceive multimodal cues to a reasonably accurate degree. Compared to other transformer-based and query-based frameworks, AVSBench, as a classic AVS method, obtains inferior results. This may be attributed to the model simply fuses multimodal cues and visual features in a progressive fashion, resulting in relatively weak guidance over the visual space."}, {"title": "5.4 Qualitative Results", "content": "We visualize the segmentation masks on the test set of the Ref-AVS Bench and compare them with AVSegFormer and ReferFormer, as shown in Fig. 4. From these qualitative results, we observe that both ReferFormer in the Ref-VOS task and AVSegFormer in the AVS task fail to accurately segment the object described in the expression. Specifically, AVSegFormer struggles to fully comprehend the expression and tends to directly generate the sound source. This issue is exemplified in the bottom-left sample, where AVSegFormer erroneously segments the vacuum cleaner instead of the boy. On the other hand, Ref-VOS may not adequately understand the audio-visual scene and thus mistakenly identify the toddler as the piano player, as shown in the top-right sample. In contrast, our Ref-AVS method demonstrates a superior capability to simultaneously process multi-modal expressions and scenes, enabling it to accurately interpret the user instruction and segment the intended object of interest."}, {"title": "5.5 Ablation Study", "content": "We conduct ablation studies to investigate the impact of two modalities (audio and text) information on the Ref-AVS task, as well as the effectiveness of the proposed method. As shown in the Tab. 4, we observe that in setting \u2461, removing the text information results in a significant performance degradation of 11.44% in"}, {"title": "6 Conclusion", "content": "In this work, we introduce a novel task, Ref-AVS, which requires the machine to segment objects of interest in dynamic audio-visual scenes based on expressions that incorporate multimodal cues and temporal information. To support this field, we develop the first benchmark, Ref-AVS Bench, for performance training and evaluation. Our method, EEMC, establishes a robust baseline with an advanced multimodal cues fusion module. We compare our method with several existing approaches on the Ref-AVS Bench dataset and demonstrate its promising performance in accurately locating objects using multimodal reference expressions. Essentially, Ref-AVS leverages audio, visual, and textual data to enrich real-world scene comprehension. Our task has potential in applications such as attention focusing and object editing in immersive audio-visual scenarios like extended reality, thereby enhancing user interaction experiences. In the future, we plan to further expand the scale of the current dataset to meet the growing data demands in the era of large language models."}]}