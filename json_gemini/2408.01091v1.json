{"title": "Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions", "authors": ["Jin Gao", "Lei Gan", "Yuankai Li", "Yixin Ye", "Dequan Wang"], "abstract": "Large multimodal models (LMMs) excel in adhering to human instructions. However, self-contradictory instructions may arise due to the increasing trend of multimodal interaction and context length, which is challenging for language beginners and vulnerable populations. We introduce the Self-Contradictory Instructions benchmark to evaluate the capability of LMMs in recognizing conflicting commands. It comprises 20,000 conflicts, evenly distributed between language and vision paradigms. It is constructed by a novel automatic dataset creation framework, which expedites the process and enables us to encompass a wide range of instruction forms. Our comprehensive evaluation reveals current LMMs consistently struggle to identify multimodal instruction discordance due to a lack of self-awareness. Hence, we propose the Cognitive Awakening Prompting to inject cognition from external, largely enhancing dissonance detection. Here are our website, dataset, and code.", "sections": [{"title": "1 Introduction", "content": "Large multimodal models (LMMs) have become prominent for their exceptional ability to follow human instructions [1,4,12,26,29,31,32,38]. Designed to process various data types, LMMs can generate and understand content in a human-like way, aligning closely with human cognition through extensive research and development [3,14,44,45]. This focus on following human instructions has led to high compliance, sometimes verging on sycophancy [9,36,40].\nLMMs are also rapidly developing to expand context windows and strengthen multimodal interaction. The Claude 3 family of models [1] offers a 200K token context window. Gemini 1.5 Pro [12] comes with a standard context window size of 128K (even up to 1M tokens in a private preview phase). Both models have sophisticated vision capabilities and can process a wide range of visual formats, including photos, figures, graphs, and technical diagrams. New multimodal models are emerging at a fantastic speed, demonstrating unprecedented performance in tackling long-context and multimodal instructions [11,12, 20, 22, 25, 32].\nHowever, self-contradictory instructions may arise due to the increasing trend of multimodal interaction and context window expansion, which is particularly challenging for language beginners and vulnerable populations. As shown in Fig. 1, children or language beginners may not realize the potential multimodal conflicts when LMMs are used in translation and education. It is also difficult for users to remember all details in multi-round conversations to avoid in-"}, {"title": "Self-Contradictory Instructions (SCI)", "content": "struction contradiction, especially when the context window size grows to 1M tokens and beyond. Moreover, conflicts between modalities may occur as the number of modalities gradually increases. Such conflicts may compromise the performance of LMMs once they fail to own meta-awareness [2] and to recognize the dissonance. Such self-awareness raises attention from researchers who attempt to enhance from the model level, while instruction-level studies are overlooked [7, 23, 43, 47].\nHence, we propose a multimodal benchmark, Self-Contradictory Instructions (SCI), to evaluate the ability of LMMs to detect conflicted instructions. It encompasses 20K conflicting instructions and 8 tasks, evenly distributed between language-language and vision-language paradigms (Fig. 2). SCI is constructed using our novel automatic dataset creation framework, AUTOCREATE (Fig. 3), which builds a multimodal cycle based on programs and large language models. We have rigorously guaranteed the quality of SCI and manually provide three levels of splits according to the occurring frequency of conflict types, SCI-CORE (1%), SCI-BASE (10%), and SCI-ALL (100%), to facilitate qualitative evaluation. AUTOCREATE expedites the dataset creation process and enables the inclusion of a wide array of instruction forms, complexities, and scopes.\nBased on SCI, we assess the capability to decipher self-contradictory instructions for current LMMs, including 5 language and 6 vision-language models. Experiments reveal that LMMs consistently fall short of accurately identifying conflicts despite remarkable performance in following instructions. Besides, we observe that such deficiency persists owing to a lack of self-awareness. Although the training process enables LMMs to handle information and knowledge but not to assess the reasonableness of user instructions and context, a capability we term cognition. Hence, we propose a plug-and-play prompting approach, Cognitive Awakening Prompting (CAP), to inject cognition from the external world, thereby largely enhancing dissonance detection even compared with advanced in-context learning techniques [5,42,46]. CAP is demonstrated to improve performance on both language-language and vision-language instruction conflicts.\nOur contributions:\nWe propose the SCI benchmark, a multimodal dataset designed to evaluate the capability of LMMs to comprehend conflicting instructions effectively.\nWe design a novel LLM-based cyclic framework, AUTOCREATE, for automatic dataset creation, substantially accelerating the process and allowing for the integration of extensive knowledge.\nWe present CAP, a prompting approach to enhance instruction conflict awareness of LMMs, significantly improving dissonance detection compared to advanced in-context learning techniques."}, {"title": "2 Related Work", "content": "Instruction Following is a remarkable ability showcased by large language models [13, 28, 33], highlighting their proficiency in comprehending and executing a given set of directives. This capability has been further amplified in the domain of large multimodal models (LMMs), where the alignment between the model and multimodal human instruction is particularly noteworthy [12,25-27,32]. Researchers have actively focused on leveraging human instruction and feedback to enhance the aptitude of these models for instruction-following [3,8,14,41,44,45]. Consequently, LMMs strive to emulate human instructions to an extraordinary degree, bordering on what can be described as sycophantic [9,36,40]. This trend underscores the deep integration of human-like understanding and execution within LMMs, positioning them as powerful tools for various tasks requiring nuanced interpretation and execution of instructions. As LMMs continue to advance, exploring the boundaries and implications of their instruction-following capabilities becomes increasingly pertinent.\nInformation Inconsistency is an inherent challenge faced by LMMs in certain scenarios, despite their advantage in handling vast amounts of information [19, 34, 35]. Researchers have dedicated efforts to address the issue of knowledge conflicts within language models, where textual disparities emerge between the parametric knowledge embedded within LLMs and the non-parametric information presented in prompts [7, 21, 43, 47]. Furthermore, information contradictions can manifest in both textual and visual domains. For instance, some studies [23, 24, 37] investigate language hallucination and visual illusion. Nevertheless, the aforementioned research has not systematically explored one of the most prevalent forms of inconsistency-the contradiction within input instructions. In contrast, our SCI benchmark tackles this challenge by constructing and studying 20,000 multimodal conflicts, offering a comprehensive examination of this vital aspect of information inconsistency in the context of LMMs.\nAutomatic Dataset Curation has emerged as a transformative paradigm within the domain of large language models (LLMs), offering several advantages such as enhancing model performance and reliability, saving time and resources, and mitigating the risk of human errors. This paradigm is particularly pivotal within the domain of LLMs. Wang et al. propose the SELF-INSTRUCT framework [44], which leverages LLMs' own generated content to create instructions, input data, and output samples autonomously. Besides, Saparov et al. introduce PRONTOQA [39], a highly programmable question-answering dataset generated from a synthetic world model. The advent of AUTOHALL [6] has furthered the field by offering a method to construct LLM-specific hallucination datasets automatically. Additionally, TIFA [16] automatically generates several question-answer pairs using LLMs to measure the faithfulness of generated images to their textual inputs via visual question-answering. In this paper, we systematically discuss automatic dataset automation leveraging LLMs and introduce eight specific tasks to exemplify the potential of this approach."}, {"title": "3 Dataset", "content": "In this section, we first discuss the novel automatic dataset creation framework, AUTOCREATE, in Section 3.1. Moreover, leveraging AUTOCREATE, we construct the multimodal Self-Contradictory Instructions benchmark, SCI, which is elaborated in Section 3.2. More details of AUTOCREATE and SCI are in the Appendix.\n3.1 AUTOCREATE\nLeveraging the power of large language models (LMMs), datasets can be created rapidly with higher quality and wider coverage than pure human handcrafts. Previous works have made initial attempts to construct datasets automatically in the domain of LLM [6,16,39,44], but do not systematically build an automatic framework. Here we introduce a novel automatic dataset creation, AUTOCREATE, shown in Fig. 3.\nAUTOCREATE requires a small batch of manually input seeds to automatically generate a large quantity of high-quality, diverse data by Large Language Models (LLMs). Specifically, in a single iteration, the generation process comprises two loops: the Language Loop (left) and the Visual Loop (right). Each loop originates from the Seed Pool and is sequentially processed by a fully automated Generator, Decorator, and Cleaner, culminating in a high-quality dataset"}, {"title": "3.2 SCI", "content": "Based on AUTOCREATE, we build the Self-Contradictory Instructions (SCI) multimodal benchmark which consists of two paradigms, language-language (L-L) and vision-language (V-L) as illustrated in Fig. 2. While the generation prompts vary across tasks, the generation process is unified in AUTOCREATE: generator-decorator-cleaner. For V-L conflicts, the image caption is modified to introduce a conflict. SCI comprises 20,000 self-contradictory instructions that span a wide range of instruction forms, complexities, and scopes. Besides that whole dataset, SCI-ALL, we also introduce two subsets, SCI-BASE and SCI-CORE, to cater to different needs. The latter subsets are selected manually with a size of 10% (1%) of SCI-ALL. Within 8 types of conflicts, only SemanticConflict involves external data, ImageNet. More details of SCI are in the Appendix.\nLanguage-Language (L-L) Conflict refers to the contradiction within text inputs. The L-L paradigm consists of 4 tasks, each with 2,500 texts. Based on the inherent nature of user prompts, we describe the tasks as RuleConflict, AttributeConflict, ExclusionConflict, and ForbbidenConflict.\nRuleConflict involves contradictory textual instructions where a rule is stated, but an example violating the rule is provided (see Fig. 2a). RuleConflict is generated in two steps: first, establish a strict rule in the context; second, craft a"}, {"title": "Self-Contradictory Instructions (SCI)", "content": "sentence that intentionally violates this rule. This process forms the RuleConflict by pairing the rule context with its violation. At test time, a single unanswerable question is created due to the rule violation. The prompt consists of the context, violating sentence, and unanswerable question concatenated sequentially.\nAttribute Conflict involves a scenario where a text provides two contradictory descriptions for an attribute of an object (see Fig. 2b). The generation of AttributeConflict includes three steps: first, create a descriptive text for a fictitious object with various attributes; second, extract a description for each attribute from the text; third, generate an opposite description to contradict the original for each attribute. By concatenating any opposite description with the original text, an AttributeConflict is formed. At test time, the task is to describe the specific attribute of the object based on the text.\nExclusion Conflict pertains to a situation where the user's prompt provides two instructions, each involving mutually exclusive operations, as demonstrated in Fig. 2c. The core of a ExclusionConflict is a pair of conflicting instructions. (e.g., \"Translate the text to Chinese\" versus \"Translate the text to French\"). Specifically, our dataset focuses on instructions for mutually exclusive operations on the same text passage. By combining a pair of exclusive instructions and a text, an ExclusionConflict prompt in the following format is generated.\n{{instruction1}{text}{instruction2}}\nForbbiden Conflict deals with conflicting instructions in conversational contexts. Here, users initially tell the LLM not to mention a particular topic and then later prompt it to discuss that same topic, as shown in Fig. 2d. To generate a ForbbidenConflict in our dataset, we first select a word from a seed pool as the forbidden word. Then, we create a question that ensures the respondent will inevitably talk about the forbidden word. At test time, a prompt with a ForbbidenConflict combines an instruction forbidding discussion of a certain word and a question that prompts the LLM to engage with that word.\nVision-Language (V-L) Conflict refers to conflicts between the multimodal components of vision and language. Below will elaborate on 4 subclasses of conflicts: OCRConflict, FigureConflict, GeometricConflict, and SemanticConflict.\nOCRConflict consists of two conflicting instructions respectively in vision and language form, as presented in Fig. 2e. The generation of OCRConflict can be"}, {"title": "J. Gao et al.", "content": "summarized in two steps. First, a list of short sentences is generated to provide the context for the conflicts. Second, utilizing instructions pairs from Section 3.2, an image of the concatenation of an instruction and a sentence is crafted. The image varies in font, size, and color to augment diversity. At test time, presenting the image and the conflicting instruction concurrently yields a conflict.\nFigure Conflict involves a simple chart with an incorrect text description, as shown in Fig. 2f. It is created through four steps. First, a list of commonly used words and entities with related numerical data is generated to decide the conflict's topic. Second, a narrative description and question are crafted for each entity and its data. Third, the numerical data is manipulated by changing the maximum value to the minimum value. Finally, a chart is plotted based on the altered data, with random choices for font, size, color, and other style options. At test time, combining the question and the figure creates a FigureConflict.\nGeometric Conflict involves an image of geometric shapes with an incorrect description, as shown in Fig. 2g. The generation process has four main steps. First, an image of two geometric objects with different attributes (shape, size, color, and position) is created. Second, a phrase is crafted to describe an object using two attributes (e.g., \"the smaller gray object\"). Third, this phrase is modified to refer to a non-existent object (e.g., \"the larger gray object\"). Finally, a question is generated about a third attribute of the non-existent object (e.g., \"What is the shape of the larger gray object?\"). At test time, presenting the image and question together creates a Geometric Conflict.\nSemantic Conflict involves an erroneously classified image, as shown in Fig. 2h. To be specific, a question about the wrong class (e.g., \"kiwi\") should be answered according to the given image (e.g., \u201costrich\"). The generation process of Semantic Conflict is based on the ImageNet dataset [10]. First, we generate some questions about a label and retrieve images according to that label in the ImageNet dataset. Second, we substitute the correct label in the questions with some similar but different objects. At test time, combining the image and the substituted question will create a conflict."}, {"title": "4 Approach", "content": "In this section, we delve into our exploration using in-context learning techniques, detailed in Section 4.1. Through experiments across various Large Multimodal Models (LMMs), we've pinpointed a crucial challenge where LMMs struggle to detect instruction conflicts. Additionally, we introduce our proposed Cognitive Awakening Prompting (CAP) approach, outlined in Section 4.2.\n4.1 In-Context Learning\nWe study three in-context learning techniques in SCI, including few-shot prompting [5], zero-shot chain-of-thoughts prompting [18], and self-consistency prompting [42]. Although few-shot prompting has been widely used in Large Language Models, its application in Large Multimodal Models (LMMs) remains limited. Recent research highlights challenges such as LMMs' inability to support multiple image inputs or comprehend sophisticated few-shot prompts [17,50]. Consequently, few-shot prompting is primarily employed within the language-language paradigm. Here, we detail the application of these prompting techniques in our SCI.\nZero-shot Prompting refers to the ability of the model to perform a task without providing examples of how to perform a task correctly. We task the model with generating responses in SCI solely based on its general knowledge and understanding of language and vision. This capability underscores the model's innate capacity to detect self-contradictory conflicts.\nZero-shot Chain-of-thoughts Prompting [46] (CoT) involves appending text like \"Please think step by step\" to user prompts, proven to enhance LMMs' inference ability. In our experiment, we incorporate this text into the prompt.\nSelf-consistency Prompting [42] (SC) involves sampling multiple reasoning paths and selecting the most consistent answers. In this paper, we generate three replies for each instruction (3-SC) and determine the final result through majority voting.\n4.2 Cognitive Awakening Prompting\nOur initial exploration reveals an intriguing phenomenon: the performance order in vision-language tasks is 0-Shot, CoT, and 3-SC across diverse LMMs, shown in the Appendix. While 3-SC provides additional experience through more attempts, CoT offers extra knowledge by stimulating reasoning capabilities through a chain of thought. However, neither surpasses the simplicity of zero-shot prompting, suggesting that both additional experience and extra knowledge derived from the model itself may be counterproductive. We hypothesize that the LMMs may not fully grasp restricted cognition in the self-contradictory instruction scenarios.\nTherefore, we propose a plug-and-play prompting approach to infuse cognition from the external world: Cognitive Awakening Prompting (CAP). The externally added cognition prompt reminds LMMs of potential inconsistencies"}, {"title": "J. Gao et al.", "content": "hidden in their cognition, e.g., adding \"Please be careful as there may be inconsistency in user input. Feel free to point it out.\" at the end of the prompt. The injected cognition does not impair the basic functioning of LMMs but fosters self-awareness of internal information and knowledge defects. Detailed experiments are presented in Section 5.3.\nWhile CAP stems from observation and analysis in vision-language tasks, it also demonstrates promise in language-language tasks, outperforming 3-Shot in over half of LMMs. Generally, the 3-Shot provides extra information since more question-answer pairs are provided. This underscores that cognition represents a higher level of existence than experience, information, and knowledge. CAP embodies a prompting technique standing on the cognition dimension, enabling the identification of LMMs' shortcomings and exploration of profound issues. Detailed experiments are outlined in Section 5.2."}, {"title": "5 Experiments", "content": "In this section, we begin with the experimental settings and introduce the Large Multimodal Models (LMMs), metric, and evaluation in Section 5.1. Furthermore, we assess the capacity of various large multimodal models (LMMs) to detect self-contradictory instructions in SCI for language-language (L-L) and vision-language (V-L) tasks, in Section 5.2 and Section 5.3 respectively.\n5.1 Experimental Settings\nLarge Multimodal Models including 11 types are experimented on SCI to assess how well LMMs can detect self-contradictory instructions. To elaborate, L-L conflicts are experimented on ChatGLM [51], ChatGPT [30], GPT-4 [32], Llama 2 [28], and GLM-4 [52]. V-L conflicts are experimented on GPT-4V [32], LLaVA-1.5 [25], Gemini [12], LLaMA-Adapter V2 [11], BLIP-2 [20], and SPHINX-v2 [22]."}, {"title": "5.2 Language-Language Conflict", "content": "We experiment with ChatGPT, ChatGLM, GLM-4, and Llama2-7b-chat on the SCI, while GPT-4 is tested on SCI-BASE, as introduced in Section 3.2. For"}, {"title": "5.3 Vision-Language Conflict", "content": "We experiment with GPT-4V, LLaVA-1.5 (with 8-bit approximation), and Gemini5, LLaMA-Adapter V2 (BIAS-7B), BLIP-2 (FlanT5XXL), and SPHINX-v2 on SCI-CORE using basic zero-shot prompting. As evident from Table 4, GPT-4V outperforms other LMMs greatly in all 4 tasks. Even SPHINX performs miserably, a rather large open-source model. Gemini shows a slightly better result"}, {"title": "J. Gao et al.", "content": "but the overall performance is still poor. This proves current LMMs' inability to detect self-contradictory instructions. Considering the unparalleled advantage of GPT-4V, we reckon the simple design of current open-source LMMs cannot handle self-contradictory instructions correctly even with LLMs, and more advanced architecture is a must to handle such a challenge.\nIt is also noteworthy that OCRConflict and SemanticConflict are relatively easy for GPT-4V and Gemini to perform, while FigureConflict and GeometricConflict exhibit the greatest difficulty. This demonstrates that current LMMs still struggle with interpreting figures and performing spatial reasoning tasks.\nWe further the experiment to explore whether in-context learning can improve performance. Due to the current limitations of vision-language models, it is typically not recommended to apply few-shot learning in this setting as we've discussed in Section 4.1. We simply apply plain zero-shot prompting, zero-shot chain-of-thoughts prompting [18], self-consistency prompting [42], and cognitive awakening prompting."}, {"title": "J. Gao et al.", "content": "improved version of chain-of-thoughts prompting, shows a similar but slightly more satisfying result than CoT prompting.\nIt is also worth mentioning that, in our self-contradictory setting, these in-context learning skills sometimes fail to achieve the originally expected result, which could be the reason why they fail to improve performance on SCI. For example, \"Please think step by step\" is meant to elicit a chain of LMM thoughts but is sometimes deemed as a normal context to be translated, paraphrased, and summarized in OCRConflict.\nFinally, CAP is harmless since it serves as an additional module for conflict detection. If it detects conflicts, it can ask the user to check input. Otherwise, the original task will proceed as usual. We conduct experiments on two non-conflict datasets to prove that SCI will not lead to misjudgment in normal cases, MMMU [49] by LLaMa-Adapter-V2 [11] and MMLU [15] by GPT-4 [32]. We find that only 1.38% replies mistakenly mentioned a conflict on the MMMU benchmark (1.11% on the MMLU)."}, {"title": "6 Conclusion", "content": "We introduce the Self-Contradictory Instructions (SCI) benchmark, comprising 20,000 conflicts distributed between language and vision domains. This benchmark aims to evaluate Large Multimodal Models (LMMs) regarding their ability to detect conflicting commands. Our innovative automatic dataset creation framework, AUTOCREATE, facilitates this process and encompasses a wide range of instruction complexities. Our evaluation reveals current LMMs' consistent struggle to identify instruction conflicts. Hence, we propose a novel approach, Cognitive Awakening Prompting (CAP), to inject cognition from the external world, leading to a substantial improvement in dissonance detection.\nSocial Impact Our work on the SCI benchmark, along with the AUTOCREATE framework and CAP approach, has significant social implications. It provides researchers and practitioners with a standardized platform to assess and enhance LMMs' ability to navigate conflicting instructions, advancing human-computer interaction and communication technologies. The AUTOCREATE framework facilitates the creation of diverse instruction datasets, promoting inclusivity in AI research. Additionally, the CAP approach integrates external cognition into multimodal models, enhancing context-aware understanding. By improving dissonance detection, our approach boosts LMM performance and fosters trust and reliability in AI systems, essential for societal integration.\nLimitations To begin with, we only include language-language and vision-language paradigms. More modalities will be included in our SCI benchmark based on our automatic framework, AUTOCREATE. Besides, no fine-grained control is introduced to determine the conflict degree. Finally, we do not provide a detailed study on the attention mechanism of large multimodal models when confronted with conflict instructions."}, {"title": "A SCI Construction", "content": "This section provides a detailed description of SCI automatic generation, with individual explanations for the eight tasks in SCI.\nA.1 Language-Language Conflict\nLanguage-language conflicts are categorized into four distinct tasks: RuleConflict, AttributeConflict, ExclusionConflict, and Forbbiden Conflict. Their generation processes will be detailed separately in the following sections.\nRuleConflict RuleConflict generation involves a systematic process that can be divided into three key steps:\n1. Develop a Context: Start by crafting a contextual setting that establishes a strict rule and provides background information. This context serves as the foundation for the subsequent conflict generation.\n2. Generate a Violating Sentence: Create a sentence that intentionally violates the established rule, as if it is acceptable to break the rule within the given context. This violating sentence should effectively challenge the rule's integrity.\n3. Pose an Unanswerable Question: Formulate a single question that becomes unanswerable when posed to the model due to the paradox created by the rule violation. The question should be designed to make it impossible for the model to provide a coherent or correct response while confronting the conflict introduced by the rule violation."}, {"title": "J. Gao et al.", "content": "RuleConflict AttributeConflict introduces a distinct type of L-L conflict related to the attributes of fictitious objects. Its generation comprises three key steps:\n1. Generate Object Description: Prompt LLM to create a descriptive text about a fictitious object, including various attributes that the virtual object supposedly possesses. This text should describe the object in detail, even though it does not exist in the real world.\n2. Attribute Description Extraction: Prompt LLM to extract descriptions for each attribute mentioned in the generated text. Each attribute description will be used to generate a conflict in the next step.\n3. Generate Contradictory Sentence: For each attribute extracted in the previous step, we instruct LLM to generate a single sentence that conveys the exact opposite of the original attribute description. This sentence will contradict the original description.\nFollowing the procedures outlined above, we generate a virtual object with a descriptive text that enumerates its various attributes. Each attribute has a contradictory sentence that disputes the original text. By simply concatenating each contradictory sentence with the original text, we can generate an object attribute conflict.\nExclusion Conflict In the case of ExclusionConflict, users provide two conflicting tasks within the same prompt. The generation process is detailed below.\n1. Generate Exclusive Instruction pairs: Handcraft a list of exclusive instructions and prompt LLM to generate more (e.g., \"Translate the text to Chinese\" versus \"Translate the text to French\"). Then, choose desired pairs and add them back to the list and thus iteratively generate more. The list can be sampled to make conflicts."}, {"title": "Self-Contradictory Instructions (SCI)", "content": "2. Generate Task Text: Start with prompting LLM to generate a pool of common elements in a story. Then randomly sample 3 elements from the pool and instruct LLM to create a short storyline consisting of these elements.\nBy combining the exclusive instructions and the text, the final conflict is generated. For example, a prompt may be formatted as follows:\n{{instruction1}{text}{instruction2}}\nForbbidenConflict ForbbidenConflict initially provide LLM with an instruction to avoid using a specific word, referred to as the \"forbidden word\" (e.g., word A). Subsequently, users ask LLM a question or request information that inherently requires the use of the forbidden word to answer accurately. We apply the following approach to synthesize the conflict.\n1. Sample from categories: First, instruct LLM to generate a list of different categories(e.g., \"history\", \"chemistry\"). For each different category, let LLM sample 50 or more different entities.\n2. Pose Unique Question: For each entity sampled, we prompt LLM to generate a question that can only be correctly answered by the entity.\n3. Select the questions: Use an LLM agent to judge whether the answers to these generated questions are unique. This is necessary because LLM tends to return questions with ambiguous answers even with explicit emphasis on uniqueness.\nTo create conflicts, simply pose one question while asking LLM not to mention the entity corresponding to it."}, {"title": "J. Gao et al.", "content": "A.2 Vision-Language Conflict\nVision-Lanuage conflict refers to conflicts between multimodal. Below will elaborate on 4 subclasses of conflicts: OCRConflict, FigureConflict, GeometricConflict, and Semantic Conflict.\nOCRConflict OCRConflict mostly inherits from the ExclusionConflict in L-L conflict but presents in a language-vision form. The generation process can be summarized in 2 steps below.\n1. Generate short sentence: Instruct LLM to generate a list of meaningful sentences instead of long stories.\n2. Create Image: Directly use instructions pair from ExclusionConflict, concatenating one of the instructions with the sentence. Generate an image of solely the text, varying in font, size, and color.\nTo generate a conflict, simply simultaneously give an image and a conflicting instruction of the image to LMM.\nFigure Conflict FigureConflict examines LMM's ability to read figures and relate them to text information. To elaborate, users provide LMM with both a figure and some text description contrasting with it when asking a related question. The generation process is detailed below.\n1. Generate Data Dictionary: First get a list of 500 commonly used words and entities in English. For each word, instruct LLM to return a JSON format of a dictionary that can be used to plot a bar graph, pie graph, or line chart.\n2. Describe Data: Feed the data dictionary to an LLM and ask it to describe the data and make some conclusions. Then, prompt LLM to ask a question regarding the largest value in the dictionary.\n3. Modify and Plot: Tamper with the data dictionary, i.e. change the largest value in it to the smallest value. Plot corresponding figures based on the tampered data, randomly choosing font, size, color and other stylish options.\nSimply concatenating the question and the figure yields a conflict."}, {"title": "Self-Contradictory Instructions (SCI)", "content": "Geometric Conflict GeometricConflict challenges LMM's ability to detect dissonance between geometric objects and related text descriptions. In this setting, LMM is given an image of 2 geometric objects with certain colors and shapes. The generation process contains the following steps.\n1. Generate Shape: Draw 2 random geometric objects, each with four attributes shape, size, color, and position.\n2. Construct Question: Query about one attribute while introducing confusion in two of the remaining attributes, i.e. exchanging the description of 2 attributes in the text description.\nConflict can be introduced by simply giving the image and the question to LMM."}, {"title": "J. Gao et al.", "content": "Semantic Conflict SemanticConflict refers to situations where the text description classifies the input image into the wrong class. To be specific, users may ask for information about object A(e.g. an ostrich) in an image of object B (e.g. a rooster). The ImageNet dataset is used to create the conflict. The detailed process is below:\n1. Generate Similar Object: For each imagenet-1k class name(e.g. \"mop\"), prompt LLM to generate several similar objects(e.g. \"duster\"). These similar objects will later be used to substitute the class name.\n2. Pose Related Question: For each class name, prompt LLM to ask several potential questions as if an image of that class is given. The questions must contain the class name.\n3. Substitue Object: Substitute the class name in the questions with a random similar object.\n4. Sample Image: Use the class name to retrieve an image from imagenet-1k validation set.\nThe final conflict is generated by combining the sampled image and the substituted question."}, {"title": "Self-Contradictory Instructions (SCI)", "content": "A.3 Dataset Overview\nThe SCI framework comprises 20,000 conflicts, evenly split between language-language conflicts and vision-language conflicts, each comprising 4 subsets. The dataset is split into 3 different levels: SCI-CORE, SCI-BASE, and SCI-ALL to cater to different needs.\nRegarding the splitting of the dataset, subsets are selected manually with a size of 10% (1%) of SCI-ALL. Notably, ExclusionConflict, ForbbidenConflict, and OCRConflict require extra effort to guarantee diversity when used in SCI-CORE.\nB Experiment details\nV-L tasks Besides experiments on SCI-CORE in the main text, massive experiments are also conducted on SCI-CORE, SCI-BASE, and SCI-ALL with LLaVA-1.5. Table 5 detailed the results. Despite some numerical disparities, the consistent trend persists across SCI-CORE, SCI-BASE, and SCI-ALL. Specifically, chain-of-thoughts prompting enhances performance in FigureConflict and Geometric Conflict, potentially impacting negatively on OCRConflict and SemanticConflict. CAP significantly improves SemanticConflict while also greatly enhancing performance across other tasks.\nB.1 Evaluation by different Agents\nFor the evaluation of replies from LMMs, human evaluation is the most accurate and expensive approach for evaluation, while LLM evaluation is a less accurate but efficient approach. The main text has presented correlation coefficients from GPT-4, while this section will demonstrate more detailed results from more evaluation agents. For L-L tasks, experiments are conducted on SCI-ALL and SCI-BASE, and evaluation is conducted by ChatGPT and ChatGLM. For V-L tasks, experiments conducted on SCI-CORE are evaluated by both human experts and three LLMs."}, {"title": "J. Gao et al.", "content": "L-L tasks All results of L-L tasks in the main text are from the evaluation by ChatGPT. Table 6 presents the results from ChatGLM evaluation. Although the evaluation results of ChatGLM exhibit a slight numerical discrepancy compared to ChatGPT, they demonstrate a similar trend when compared across different methods.\nV-L tasks Human experts conduct all the evaluations of V-L tasks in the main text. Below will elaborate on evaluations conducted by GPT-4.\nAs can be seen in Table 7, GPT-4 yields almost identical outcomes (overall discrepancy within 5%) to those of human experts, underscoring its unparalleled capability to consistently perform such evaluations.\nC LMM Responses\nThis section showcases some examples illustrating how LMMs respond to SCI tasks. Each example box contains a user prompt and several replies by various LMMs. In the user prompt, parts referencing a conflict are highlighted in brown font. In LMM replies, sentences that acknowledge the presence of a conflict are in dark green font, and sentences that neglect the conflict are in dark red font."}, {"title": "J. Gao et al.", "content": "Four example boxes are demonstrated for RuleConflict, AttributeConflict, Exclusion Conflict, and Forbbiden Conflict respectively."}, {"title": "Self-Contradictory Instructions (SCI)", "content": "C.2 V-L tasks\nFour example boxes are demonstrated for OCRConflict, FigureConflict, Geometric Conflict, and Semantic Conflict respectively."}, {"title": "Acknowledgments", "content": "This research is supported by the Key R&D Program of Shandong Province, China (2023CXGC010112). We express our gratitude to the funding agency for their support."}]}