{"title": "Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions", "authors": ["Jin Gao", "Lei Gan", "Yuankai Li", "Yixin Ye", "Dequan Wang"], "abstract": "Large multimodal models (LMMs) excel in adhering to human instructions. However, self-contradictory instructions may arise due to the increasing trend of multimodal interaction and context length, which is challenging for language beginners and vulnerable populations. We introduce the Self-Contradictory Instructions benchmark to evaluate the capability of LMMs in recognizing conflicting commands. It comprises 20,000 conflicts, evenly distributed between language and vision paradigms. It is constructed by a novel automatic dataset creation framework, which expedites the process and enables us to encompass a wide range of instruction forms. Our comprehensive evaluation reveals current LMMs consistently struggle to identify multimodal instruction discordance due to a lack of self-awareness. Hence, we propose the Cognitive Awakening Prompting to inject cognition from external, largely enhancing dissonance detection. Here are our website, dataset, and code.", "sections": [{"title": "1 Introduction", "content": "Large multimodal models (LMMs) have become prominent for their exceptional ability to follow human instructions [1,4,12,26,29,31,32,38]. Designed to process various data types, LMMs can generate and understand content in a human-like way, aligning closely with human cognition through extensive research and development [3,14,44,45]. This focus on following human instructions has led to high compliance, sometimes verging on sycophancy [9,36,40]. LMMs are also rapidly developing to expand context windows and strengthen multimodal interaction. The Claude 3 family of models [1] offers a 200K token context window. Gemini 1.5 Pro [12] comes with a standard context window size of 128K (even up to 1M tokens in a private preview phase). Both models have sophisticated vision capabilities and can process a wide range of visual formats, including photos, figures, graphs, and technical diagrams. New multimodal models are emerging at a fantastic speed, demonstrating unprecedented performance in tackling long-context and multimodal instructions [11,12, 20, 22, 25, 32]. However, self-contradictory instructions may arise due to the increasing trend of multimodal interaction and context window expansion, which is particularly challenging for language beginners and vulnerable populations. As shown in Fig. 1, children or language beginners may not realize the potential multimodal conflicts when LMMs are used in translation and education. It is also difficult for users to remember all details in multi-round conversations to avoid in-"}, {"title": "2 Related Work", "content": "Instruction Following is a remarkable ability showcased by large language models [13, 28, 33], highlighting their proficiency in comprehending and executing a given set of directives. This capability has been further amplified in the domain of large multimodal models (LMMs), where the alignment between the model and multimodal human instruction is particularly noteworthy [12,25-27,32]. Researchers have actively focused on leveraging human instruction and feedback to enhance the aptitude of these models for instruction-following [3,8,14,41,44,45]. Consequently, LMMs strive to emulate human instructions to an extraordinary degree, bordering on what can be described as sycophantic [9,36,40]. This trend underscores the deep integration of human-like understanding and execution within LMMs, positioning them as powerful tools for various tasks requiring nuanced interpretation and execution of instructions. As LMMs continue to advance, exploring the boundaries and implications of their instruction-following capabilities becomes increasingly pertinent.\nInformation Inconsistency is an inherent challenge faced by LMMs in certain scenarios, despite their advantage in handling vast amounts of information [19, 34, 35]. Researchers have dedicated efforts to address the issue of knowledge conflicts within language models, where textual disparities emerge between the parametric knowledge embedded within LLMs and the non-parametric information presented in prompts [7, 21, 43, 47]. Furthermore, information contradictions can manifest in both textual and visual domains. For instance, some studies [23, 24, 37] investigate language hallucination and visual illusion. Nevertheless, the aforementioned research has not systematically explored one of the most prevalent forms of inconsistency-the contradiction within input instructions. In contrast, our SCI benchmark tackles this challenge by constructing and studying 20,000 multimodal conflicts, offering a comprehensive examination of this vital aspect of information inconsistency in the context of LMMs.\nAutomatic Dataset Curation has emerged as a transformative paradigm within the domain of large language models (LLMs), offering several advantages such as enhancing model performance and reliability, saving time and resources, and mitigating the risk of human errors. This paradigm is particularly pivotal within the domain of LLMs. Wang et al. propose the SELF-INSTRUCT framework [44], which leverages LLMs' own generated content to create instructions, input data, and output samples autonomously. Besides, Saparov et al. introduce PRONTOQA [39], a highly programmable question-answering dataset generated from a synthetic world model. The advent of AUTOHALL [6] has furthered the field by offering a method to construct LLM-specific hallucination datasets automatically. Additionally, TIFA [16] automatically generates several question-answer pairs using LLMs to measure the faithfulness of generated images to their textual inputs via visual question-answering. In this paper, we systematically discuss automatic dataset automation leveraging LLMs and introduce eight specific tasks to exemplify the potential of this approach."}, {"title": "3 Dataset", "content": "In this section, we first discuss the novel automatic dataset creation framework, AUTOCREATE, in Section 3.1. Moreover, leveraging AUTOCREATE, we construct the multimodal Self-Contradictory Instructions benchmark, SCI, which is elaborated in Section 3.2. More details of AUTOCREATE and SCI are in the Appendix.\n3.1 AUTOCREATE\nLeveraging the power of large language models (LMMs), datasets can be created rapidly with higher quality and wider coverage than pure human handcrafts. Previous works have made initial attempts to construct datasets automatically in the domain of LLM [6,16,39,44], but do not systematically build an automatic framework. Here we introduce a novel automatic dataset creation, AUTOCREATE, shown in Fig. 3.\nAUTOCREATE requires a small batch of manually input seeds to automatically generate a large quantity of high-quality, diverse data by Large Language Models (LLMs). Specifically, in a single iteration, the generation process comprises two loops: the Language Loop (left) and the Visual Loop (right). Each loop originates from the Seed Pool and is sequentially processed by a fully automated Generator, Decorator, and Cleaner, culminating in a high-quality dataset"}, {"title": "3.2 SCI", "content": "Based on AUTOCREATE, we build the Self-Contradictory Instructions (SCI) multimodal benchmark which consists of two paradigms, language-language (L-L) and vision-language (V-L) as illustrated in Fig. 2. While the generation prompts vary across tasks, the generation process is unified in AUTOCREATE: generator-decorator-cleaner. For V-L conflicts, the image caption is modified to introduce a conflict. SCI comprises 20,000 self-contradictory instructions that span a wide range of instruction forms, complexities, and scopes. Besides that whole dataset, SCI-ALL, we also introduce two subsets, SCI-BASE and SCI-CORE, to cater to different needs. The latter subsets are selected manually with a size of 10% (1%) of SCI-ALL. Within 8 types of conflicts, only SemanticConflict involves external data, ImageNet. More details of SCI are in the Appendix.\nLanguage-Language (L-L) Conflict refers to the contradiction within text inputs. The L-L paradigm consists of 4 tasks, each with 2,500 texts. Based on the inherent nature of user prompts, we describe the tasks as RuleConflict, AttributeConflict, ExclusionConflict, and ForbbidenConflict.\nRuleConflict involves contradictory textual instructions where a rule is stated, but an example violating the rule is provided (see Fig. 2a). RuleConflict is generated in two steps: first, establish a strict rule in the context; second, craft a"}, {"title": "4 Approach", "content": "In this section, we delve into our exploration using in-context learning techniques, detailed in Section 4.1. Through experiments across various Large Multimodal Models (LMMs), we've pinpointed a crucial challenge where LMMs struggle to detect instruction conflicts. Additionally, we introduce our proposed Cognitive Awakening Prompting (CAP) approach, outlined in Section 4.2.\n4.1 In-Context Learning\nWe study three in-context learning techniques in SCI, including few-shot prompting [5], zero-shot chain-of-thoughts prompting [18], and self-consistency prompting [42]. Although few-shot prompting has been widely used in Large Language Models, its application in Large Multimodal Models (LMMs) remains limited. Recent research highlights challenges such as LMMs' inability to support multiple image inputs or comprehend sophisticated few-shot prompts [17,50]. Consequently, few-shot prompting is primarily employed within the language-language paradigm. Here, we detail the application of these prompting techniques in our SCI.\nZero-shot Prompting refers to the ability of the model to perform a task without providing examples of how to perform a task correctly. We task the model with generating responses in SCI solely based on its general knowledge and understanding of language and vision. This capability underscores the model's innate capacity to detect self-contradictory conflicts.\nZero-shot Chain-of-thoughts Prompting [46] (CoT) involves appending text like \"Please think step by step\" to user prompts, proven to enhance LMMs' inference ability. In our experiment, we incorporate this text into the prompt.\nSelf-consistency Prompting [42] (SC) involves sampling multiple reasoning paths and selecting the most consistent answers. In this paper, we generate three replies for each instruction (3-SC) and determine the final result through majority voting.\n4.2 Cognitive Awakening Prompting\nOur initial exploration reveals an intriguing phenomenon: the performance order in vision-language tasks is 0-Shot, CoT, and 3-SC across diverse LMMs, shown in the Appendix. While 3-SC provides additional experience through more attempts, CoT offers extra knowledge by stimulating reasoning capabilities through a chain of thought. However, neither surpasses the simplicity of zero-shot prompting, suggesting that both additional experience and extra knowledge derived from the model itself may be counterproductive. We hypothesize that the LMMs may not fully grasp restricted cognition in the self-contradictory instruction scenarios.\nTherefore, we propose a plug-and-play prompting approach to infuse cognition from the external world: Cognitive Awakening Prompting (CAP). The externally added cognition prompt reminds LMMs of potential inconsistencies"}, {"title": "5 Experiments", "content": "In this section, we begin with the experimental settings and introduce the Large Multimodal Models (LMMs), metric, and evaluation in Section 5.1. Furthermore, we assess the capacity of various large multimodal models (LMMs) to detect self-contradictory instructions in SCI for language-language (L-L) and vision-language (V-L) tasks, in Section 5.2 and Section 5.3 respectively.\n5.1 Experimental Settings\nLarge Multimodal Models including 11 types are experimented on SCI to assess how well LMMs can detect self-contradictory instructions. To elaborate, L-L conflicts are experimented on ChatGLM [51], ChatGPT [30], GPT-4 [32], Llama 2 [28], and GLM-4 [52]. V-L conflicts are experimented on GPT-4V [32], LLaVA-1.5 [25], Gemini [12], LLaMA-Adapter V2 [11], BLIP-2 [20], and SPHINX-v2 [22].\nMetric in our experiment is the hit ratio, which is defined as the proportion of the conflict-aware replies with the total replies. To calculate the hit ratio, each reply generated by LMM will be evaluated to determine whether it successfully identifies the conflict hidden in the user's input.\n5.2 Language-Language Conflict\nWe experiment with ChatGPT, ChatGLM, GLM-4, and Llama2-7b-chat on the SCI, while GPT-4 is tested on SCI-BASE, as introduced in Section 3.2. For"}, {"title": "5.3 Vision-Language Conflict", "content": "We experiment with GPT-4V, LLaVA-1.5 (with 8-bit approximation), and Gemini5, LLaMA-Adapter V2 (BIAS-7B), BLIP-2 (FlanT5XXL), and SPHINX-v2 on SCI-CORE using basic zero-shot prompting. As evident from Table 4, GPT-4V outperforms other LMMs greatly in all 4 tasks. Even SPHINX performs miserably, a rather large open-source model. Gemini shows a slightly better result\n6 Conclusion\nWe introduce the Self-Contradictory Instructions (SCI) benchmark, comprising 20,000 conflicts distributed between language and vision domains. This benchmark aims to evaluate Large Multimodal Models (LMMs) regarding their ability to detect conflicting commands. Our innovative automatic dataset creation framework, AUTOCREATE, facilitates this process and encompasses a wide range of instruction complexities. Our evaluation reveals current LMMs' consistent struggle to identify instruction conflicts. Hence, we propose a novel approach, Cognitive Awakening Prompting (CAP), to inject cognition from the external world, leading to a substantial improvement in dissonance detection.\nSocial Impact Our work on the SCI benchmark, along with the AUTOCREATE framework and CAP approach, has significant social implications. It provides researchers and practitioners with a standardized platform to assess and enhance LMMs' ability to navigate conflicting instructions, advancing human-computer interaction and communication technologies. The AUTOCREATE framework facilitates the creation of diverse instruction datasets, promoting inclusivity in AI research. Additionally, the CAP approach integrates external cognition into multimodal models, enhancing context-aware understanding. By improving dissonance detection, our approach boosts LMM performance and fosters trust and reliability in AI systems, essential for societal integration.\nLimitations To begin with, we only include language-language and vision-language paradigms. More modalities will be included in our SCI benchmark based on our automatic framework, AUTOCREATE. Besides, no fine-grained control is introduced to determine the conflict degree. Finally, we do not provide a detailed study on the attention mechanism of large multimodal models when confronted with conflict instructions."}]}