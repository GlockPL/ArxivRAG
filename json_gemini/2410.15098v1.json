{"title": "Incorporating Group Prior into Variational Inference for Tail-User Behavior Modeling in CTR Prediction", "authors": ["Han Xu", "Taoxing Pan", "Zhiqiang Liu", "Xiaoxiao Xu", "Lantao Hu"], "abstract": "User behavior modeling-which aims to extract user interests from behavioral data-has shown great power in Click-through rate (CTR) prediction, a key component in recommendation systems. Recently, attention-based algorithms have become a promising direction, as attention mechanisms emphasize the relevant interactions from rich behaviors. However, the methods struggle to capture the preferences of tail users with sparse interaction histories. To address the problem, we propose a novel variational inference approach, namely Group Prior Sampler Variational Inference (GPSVI), which introduces group preferences as priors to refine latent user interests for tail users. In GPSVI, the extent of adjustments depends on the estimated uncertainty of individual preference modeling. In addition, We further enhance the expressive power of variational inference by a volume-preserving flow. An appealing property of the GPSVI method is its ability to revert to traditional attention for head users with rich behavioral data while consistently enhancing performance for long-tail users with sparse behaviors. Rigorous analysis and extensive experiments demonstrate that GPSVI consistently improves the performance of tail users. Moreover, online A/B testing on a large-scale real-world recommender system further confirms the effectiveness of our proposed approach.", "sections": [{"title": "1 INTRODUCTION", "content": "Click-through rate (CTR) prediction is a fundamental task for many large-scale applications, such as e-commerce and short-video sharing platforms [4, 14]. Regarding each user's behaviors as a sequence in chronological order, extracting user preferences behind given historical behaviors has achieved great improvement [10, 19, 27, 28]. Consequently, modeling users' complex sequential behaviors is a key component in CTR prediction.\nInitially, researchers focus on learning better representation of user historical behaviors [9, 27, 28]. For example, DIN [28] introduces target attention, which emphasizes target-relevant behaviors and suppresses irrelevant ones. In [27], DIEN proposes an interest-evolving layer to capture the dynamic evolution of user interests about the target items. Recent works endeavor to leverage rich user behaviors under the constraints of simple model deployment and system latency, such as MIMN [18] and SIM [19]. These popular approaches achieve promising performance when most users' historical behavioral data is sufficient enough because they assume that user interests are included within their behaviors. Consequently, the previous user behavior modeling methods are sensitive to the scale and quality of historical behavior data. However, in practical applications, the number of user behaviors inherently follows a long-tail distribution [26]. Sequence lengths for head users can range from hundreds to thousands, while tail users have only a few or even none. Notably, due to the limited opportunities for feedback from long-tail users, their historical behaviors alone are insufficient for representing their interests. Existing deterministic methods primarily focus on the historical behavior of tail users, leading to inaccurate interest modeling and thereby resulting in misguidance in the CTR prediction system. Our experiments further reveal that while attention-based models yield promising results with behavior-rich head users, they often overlook many behavior-poor tail users and degrade the performance of CTR tasks.\nIn recent years, a number of studies aim to enhance tail users' experiences by improving embedding generalization ability, such as content-based methods [16, 22, 24], meta-learning involved methods [8, 17], and variational inference methods [25]. Although these works make some progress, capturing genuine tail users' interests remains an unresolved challenge. To address the problem, we propose a novel Group Prior Sampler Variational Inference (GPSVI) approach based on uncertainty estimation. Regarding group preferences as an appropriate prior for long-tail users, the proposed method integrates behavior modeling with this prior for correction. Additionally, the method adaptively adjusts the extent of correction based on the confidence level in the user interest modeling. We further enhance the expressive power of variation inference by adding a normalizing flow. An attractive property of the GPSVI method is its ability to revert to traditional attention mechanisms for head users while consistently enhancing performance for long-tail users. We conduct extensive experiments and rigorous analysis to verify the effectiveness and generality of our proposed method. Our experiments are conducted in a large-scale industry dataset collected from a real-world application and a famous public Amazon dataset [15]. The experiments indicate that GPSVI consistently and significantly outperforms the state-of-the-art user behavior modeling methods Trans [5]. Rigorous ablation studies demonstrate the necessity of our proposed modules, including the group prior based sampler and the volume-preserving flow. Furthermore, we test our proposed algorithm through online experiments and observe a great improvement. Notice that unlike head users, who already present satisfaction with little scope for further enhancement, improving the performance of tail users offers a greater opportunity for significant gains. Compared to the baseline model, our method results in a 0.306% increase in overall CTR, with a specific improvement of 0.659% among tail users. Our methods have been successfully deployed to serve the main traffic for two months."}, {"title": "2 METHODS", "content": "CTR prediction is a critical task in industry applications, such as e-commerce search engines and recommendation systems. Given a user, a candidate item, and the contexts in an impression scenario, CTR prediction is used to infer the probability of a click event. The mainstream methods for click-through rate (CTR) prediction mostly adopt the Embedding&Network paradigm, which takes several critical fields of features as input, such as user profile, item profile, context and user behaviors. Denote the features mentioned above as $u, i, c, s$. Then, the output of the CTR model is\n$\\hat{y} = f (u, i, c, s)$,\nwhere $\\hat{y}$ is the prediction.\nAmong them, user behaviors are records of users' interactions on specific items, faithfully reflecting users' immediate and evolving interests. Consequently, recent works consider these features as a key for user interest modeling, and propose extensive deterministic attention-based user behavior modeling approaches. They aim to extract the user's preference $\\hat{u}_q$, when given the specific item $q \\in R^d$ and the historical behavior sequence with length $L$. We denote the key vector of the $l$-th behavior as $k_l \\in R^d$, and the corresponding value vector as $v_l \\in R^d$. The keys and values are packed together into matrices $K$ and $V$, i.e.,\n$K = (k_1,k_2,..., k_L)$,\n$V = (v_1, v_2, \\ldots, v_L)$.\nThe commonly seen attention mechanism for user behavior modeling is as follows, i.e.,\n$\\alpha_l = \\frac{\\exp (q \\cdot k_l)}{\\sum_{l'=1}^L \\exp (q \\cdot k_{l'})}$\n$\\hat{q} = \\sum_{l=1}^L \\alpha_l v_l$.\nHowever, existing user behaviors modeling methods struggle to precisely capture long-tail users preferences from their scarce behaviors. Since short behavior sequences indicates a lack of user interaction records, user interest extracted from these sequences may be incomplete and unreliable. In such cases, existing deterministic behaviour modelling modules will mislead the CTR prediction. To tackle this problem, we propose a novel Group Prior Sampler Variational Inference (GPSVI) approach based on uncertainty estimation. Considering the group preference as a proper prior for long-tail users, the proposed method incorporates the behavior modeling with the prior for correction, and the method adaptively controls the magnitude of correction by the confidence of the user interest modeling. Consequently, an appealing property of the GPSVI method is that it can degenerate to traditional attention for highly active users and achieve consistent improvement for long-tail users."}, {"title": "2.1 Variational Framework with Group Prior\nSampler", "content": "Given the output of the attention-based behavior modeling module as input, the GPSVI method learns the posterior distribution $q_\\phi (z|v)$ of latent variable $z$, where $z \\in R^d$ and $v \\in R^d$. Similar to general variational inference methods, GPSVI defines the posterior distribution as an isotropic Gaussians with diagonal covariance, i.e., $z \\sim N(\\mu, diag(\\sigma))$, where $\\mu\\in R^d$, $\\sigma\\in R^d$, and $diag(\\sigma$ is the diagonal matrix with diagonal entries $\\sigma$. Then, the decoder, parameterized by $\\theta$, in GPSVI predicts the probability of click events conditional on the latent variable, that is, $\\hat{y} = p_\\theta (\\hat{y}|z)$. The training objective is to maximize the Evidence Lower Bound (ELBO):\n$L(\\phi, \\theta) = ELBO(\\phi, \\theta)$\n$= E_{z\\sim q_\\phi (z|v)} [\\log p_\\theta (y|z)] + D_{KL} (q_\\phi (z|v)||p(v)).$ (1)\nThe choice of prior and posterior distributions has a significant impact on the quality of inferences. In GPSVI, the prior is the standard normal, $p(v) = N(0, I)$. The prior is both appropriate and computationally convenient, before obtaining the information about the user interest [3, 12]. As for the posterior $q_\\phi (z|v)$, learnable parameters $\\mu$ and $\\sigma$ are derived by a neural network [21]. For the mean $\\mu$, GPSVI applies an identity transformation, i.e., $\\mu = v$. The identify transformation makes much sense as it maintains the concept of \"attention\". GPSVI computes the $\\sigma$ by a neural layer with exponent activation, to ensure positive values. Since the estimation variance decreases as the number of behaviors increase, GPSVI adds a monotonic regularizer, that is\n$R_m = \\sum_{(u_i,u_j)} \\sum_{m=0}^d (\\max (0, \\sigma^m_{u_j} - \\sigma^m_{u_i}) I (l_{u_i} > l_{u_j})$\n$+ \\max (0, \\sigma^m_{u_i} - \\sigma^m_{u_j}) I (l_{u_j} > l_{u_i})),$ (2)\nwhere $l_{u_i}, l_{u_j}$ are the lengths of behavior sequences of user $u_i$ and $u_j$ respectively and $\\sigma^m$ is the element in $m$-th entry of $\\sigma\\in R^d$.\nThe core component of GPSVI is the group-prior-based sampler, which shifts the latent representation of the user interest by adding the group prior. Coarse features of a user $c(u)$, such as the age, the gender and the location, categorize the individual into a specific group. GPSVI models the group interest $g \\in R^d$ for a specific item $i$ by an mlp, parameterized by $\\varphi$, that is,\n$g = f_\\varphi (c(u), i) \\in R^d.$ (3)\nIn general, the diagonal entry vector $\\sigma$ is considered as the uncertainty of the latent user interest. Regardless of uncertainty, deterministic methods place excessive trust in the user behavior modeling, while probabilistic methods introduce random vectors with covariance matrix $diag(\\sigma)$. Both of these methods struggle to capture genuine user interests from scarce behaviors. An interesting insight is that the group interest is an appropriate prior for the individual interest [25]. Based on the insight, the sampler in GPSVI shifts the latent variable by adding the projection of a random vector onto the plane spanned by group interest vectors. Incorporating the reparameterization technique, the sample of the latent variable $z$ is\n$z = \\mu + Proj_g (diag(\\sigma)\\xi), \\xi \\sim N(0, I),$ (4)\n$Proj_g(y) = \\frac{<y, g>}{\\|g\\|^2}g, y\\in R^d.$ (5)\nIn fact, $z$ is still a Gaussian distribution, i.e., $z \\in N(\\mu, P \\sigma diag(\\sigma))$, where $P$ is the projection matrix, with learnable parameters $\\varphi$. Thus, the posterior is parameterized by $\\theta$ and $\\varphi$, i.e., $q_{\\phi,\\varphi}$. Combining with the equation (2), the training objective is\n$L(\\phi, \\theta, \\varphi) =E_{z\\sim q_{\\phi,\\varphi} (z|v)} [\\log p_\\theta (y|z)]$\n$+D_{KL} (q_{\\phi,\\varphi}(z|v)||p(z)) + R_m,$ (6)\nwhere $q_{\\phi,\\varphi}(z|v) = N(\\mu, P_\\varphi diag(\\sigma))$. As the latent variable is restricted in space spanned by the vector $g$, we employ the standard normal distribution on the group interest space as the prior [1]. In this case, the closed form of Kullback-Leibler divergence is\n$\\sum_{i=1}^d (exp(\\sigma_i) - (1 + \\mu'^2 + (\\sigma'_i)^2)),$ (7)\nwhere $\\mu' = (\\mu'_1, \\mu'_2, ..., \\mu'_d) = Proj_g(\\mu)$ and $\\sigma' = (\\sigma'_1, ..., \\sigma'_d) = Proj_g (\\sigma)$, respectively"}, {"title": "2.2 Volume-preserving Flow", "content": "The choice of approximate posterior distribution is one of the core problems in variational inference [21]. Most applications of variational inference employ isotropic Gaussians with diagonal covariance for posterior approximations. This restriction has a significant impact on the quality of inferences [21, 23]. However, such a simple distribution may not be expressive enough to approximate the true posterior distribution. Specifically, the distribution of the user preference can deviate from the Gaussian form, indicating that employing simple distribution families of posterior results in a loose gap between the Evidence Lower Bound (ELBO) and the true marginal likelihood. Inspired by prior works [7, 11, 13], normalizing flow is a promising method to tackle this problem. The basic idea of normalizing flow is to apply $K$ invertible parametric transformation functions $f_k$ called flows to transform the sample $z_0 \\in R^d$ of the latent variable. The process is as follows:\n$z_K = f_K \\circ f_{K-1}\\circ\\cdots \\circ f_1 (z_0),$ (8)\nwhere $f_k : R^d \\rightarrow R^d$ is an invertible function with Jacobian, $\\frac{\\partial f_k}{\\partial z_{k-1}}$, for $k \\in \\{1, 2, ..., K\\}$. With change-of-variables theorem, the probabilistic density function (PDF) of the random vector $F_1$ is given as,\n$p_{z_1} = p_{z_0} \\big( f^{-1}(z_0) \\big) \\big|\\frac{\\partial f_1}{\\partial z_0} \\big|,$ (9)\nFollowing the sequential procedure, we have\n$\\log p_{z_K} = \\log p_{z_0} - \\sum_{k=1}^K \\log \\big|\\frac{\\partial f_k}{\\partial z_{k-1}} \\big|.$ (10)\nAccording to the discussion in section 2.1, the variance of the latent variance $z_k$, regarded as the uncertainty of user interest modeling, the chosen normalizing flow needs to preserve variance of the latent variable sequence $\\{z_1, z_2, ..., z_K\\}$. After the invertible transform, the variance of a random variable approximately remains unchanged when the determinant of the Jacobian matrix is equal to 1. The work [2] has shown the following theorem.\nTHEOREM 2.1. Given a random vector x and a transform f(.), the approximate variance of f (x) is given by\n$Var [f(x)] \\sim (f' \\big|_{x=E[x]})^2 Var[x].$ (11)\nThe previous work [20] proves the similar theorme for random vectors. The theorem guides the choice of the normalization flows. As the determinant of the Jacobian matrix of a volume-preserving flow always equals 1, the determinant of the covariance of the latent variable $z$ approximately remains unchanged after going through a sequence of volume-preserving flows. In this paper, we employ the Non-linear Independent Components Estimation (NICE) developed by [7], which is an instance of a volume-preserving flow. The transformations used are neural networks f(.) with easy to compute inverse g() of the form:\n$f(z) = (z_A, z_B + h_\\lambda (z_A))$ (12)\n$g(z') = (z'_A, z_B + h_\\lambda (z'_A)),$ (13)\nwhere $z = (z_A, z_B)$ is a partitioning of the vector $z$ and $h_\\lambda$ is a neural network with parameters $\\lambda$. Obviously, the Jacobian matrix of the function $f$ is a lower triangular matrix, resulting in a determinant of 1. Finally, our methods use the output $z_K$ of flows as the result of user behavior model."}, {"title": "3 EXPERIMENTS", "content": "Datasets. 1). The industrial dataset is constructed from the traffic logs of a large-scale recommendation system. We sample a subset of the online logs from 2023-10-05 to 2023-10-16, a total of 12 days, for training and the instances on the next day for testing. This dataset includes substantial details, such as user clicks, user behaviors, user profiles, item profiles, and context features. Besides, we limit the"}, {"title": "3.2 Online A/B tests", "content": "To conduct the online A/B test, we deployed the proposed GPSVI model within a large-scale recommender system. The online A/B test spanned two months. Compared to the baseline model (Transformer), our method resulted in a 0.306% increase in overall CTR, with a specific improvement of 0.659% in CTR among tail users."}, {"title": "4 CONCLUSION", "content": "Attention-based methods have achieved impressive improvement in user behavior modeling, as they emphasize the relevant interactions from rich behaviors. However, the methods encounter challenges capture the preferences of tail users with limited interaction histories. To address the problem, we propose a novel variational inference approach, namely GPSVI, which introduces group preferences as priors to refine latent user interests. Extensive offline experiments and online A/B tests demonstrate that GPSVI consistently improves the performance of tail users."}]}