{"title": "ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration", "authors": ["Chaojun Ni", "Guosheng Zhao", "Xiaofeng Wang", "Zheng Zhu", "Wenkang Qin", "Guan Huang", "Chen Liu", "Yuyin Chen", "Yida Wang", "Xueyang Zhang", "Yifei Zhan", "Kun Zhan", "Peng Jia", "Xianpeng Lang", "Xingang Wang", "Wenjun Mei"], "abstract": "Closed-loop simulation is crucial for end-to-end autonomous driving. Existing sensor simulation methods (e.g., NeRF and 3DGS) reconstruct driving scenes based on conditions that closely mirror training data distributions. However, these methods struggle with rendering novel trajectories, such as lane changes. Recent works have demonstrated that integrating world model knowledge alleviates these issues. Despite their efficiency, these approaches still encounter difficulties in the accurate representation of more complex maneuvers, with multi-lane shifts being a notable example. Therefore, we introduce ReconDreamer, which enhances driving scene reconstruction through incremental integration of world model knowledge. Specifically, DriveRestorer is proposed to mitigate artifacts via online restoration. This is complemented by a progressive data update strategy designed to ensure high-quality ren-", "sections": [{"title": "1. Introduction", "content": "Open-loop simulation techniques have made significant advancements in the field of autonomous driving [22, 23, 26]. However, current open-loop evaluation methods fall short in providing an accurate assessment of end-to-end planning algorithms, underscoring the need for more robust evaluation frameworks [32, 34, 67]. A promising approach to address this issue involves using closed-loop evaluations conducted in real-world scenarios, which require retrieving sensor data from novel trajectory views. This demands the driving scene representation capable of reconstructing the intricate and dynamic nature of driving environments.\nClosed-loop simulation predominantly hinges on scene reconstruction approaches like Neural Radiance Fields (NeRF) [15, 39, 59, 61] and 3D Gaussian Splatting (3DGS) [9, 24, 28, 58]. Despite their contributions, these techniques are fundamentally restricted by the density and diversity of training data, often limiting their rendering capabilities to scenarios that closely mimic the original training data. Consequently, they underperform in complex, high-variation driving maneuvers. Current developments in autonomous driving world models [13, 21, 49, 51, 52, 69] have introduced the capability to generate diverse videos aligned with specific driving commands, renewing the potential for more robust closed-loop simulation. The recent DriveDreamer4D [68] has further evidenced that leveraging pretrained world models as data machines can substantially improve the quality of dynamic driving scene reconstruction. However, while this training-free integration of world model knowledge is efficient, its current design still encounters challenges in executing larger maneuvers (e.g., multi-lane shifts).\nIn this paper, we introduce ReconDreamer, which enhances driving scene reconstruction via incrementally integrating knowledge from autonomous driving world models. Unlike [68] which leverages pretrained world models to directly expand novel trajectory views, ReconDreamer trains the world model to progressively mitigate ghosting artifacts in complex maneuver renderings. Specifically, we generate a video restoration dataset by sampling rendering outputs at various training stages. Based on the dataset,\nwe propose the DriveRestorer, which is fine-tuned upon the world model to mitigate ghosting artifacts via online restoration. During the training, the masking strategy is introduced to emphasize restoration of challenging areas (e.g., sky and distant regions). Furthermore, we propose the Progressive Data Update Strategy (PDUS) to gradually restore artifacts, which ensures high-quality rendering for larger maneuvers. The proposed PDUS, by incrementally integrating world model knowledge, reduces the complexity of video restoration, making ReconDreamer the first approach capable of handling large viewpoint shifts in rendering (e.g., across multiple lanes, spanning up to 6 meters). As illustrated in Fig. 1, experimental results confirm that ReconDreamer substantially improves Street Gaussians [58] during novel trajectory rendering, achieving a relative improvement in the average NTA-I0U, NTL-IoU, and FID by 24.87%, 6.72%, and 29.97%. Additionally, ReconDreamer strengthens spatiotemporal coherence in larger maneuvers, outperforming DriveDreamer4D [68] with a win rate 96.88% in the user study, and a relative improvement of 195.87% in the NTA-IoU metric.\nThe primary contributions of this work are as follows: (1) We present ReconDreamer, which enhances dynamic driving scene reconstruction via incremental integration of world model knowledge. Notably, to our knowledge, ReconDreamer is the first method to effectively render in large maneuvers (e.g., spanning up to 6 meters). (2) The DriveRestorer is proposed to mitigate ghosting artifacts via online restoration. Besides, we introduce the progressive data update strategy to maintain high-quality rendering for larger maneuvers. (3) We perform comprehensive experiments to validate that ReconDreamer can enhance rendering quality during large maneuvers, as well as the spatiotemporal coherence of driving scene elements."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Driving Scene Reconstruction Methods", "content": "NeRF and 3DGS have become prominent techniques in scene reconstruction. NeRF models [2, 3, 39, 40] use multilayer perceptron (MLP) networks to represent continuous volumetric scenes, achieving exceptional rendering quality. Recently, 3DGS [28, 64] introduced a novel approach by defining anisotropic Gaussians in 3D space and employing adaptive density control, which allows for high-quality renderings even from sparse point cloud data. Various studies have adapted NeRF [10, 15, 25, 37, 43, 47, 59, 61] and 3DGS [8, 9, 24, 35, 58, 65, 70] for driving scene reconstructions. To accommodate the dynamic nature of driving environments, some methods incorporate time as an additional parameter to capture temporal variations in dynamic scenes [1, 11, 24, 33, 36, 42, 45], while others treat the scene as a combination of moving objects overlaid on a static back-"}, {"title": "2.2. World Models", "content": "World models predict possible future world states as a function of imagined action sequences proposed by the actor [31, 71]. Based on world models, recent methods [4, 5, 14, 16, 18-20, 29, 38, 50, 51, 55, 57, 63, 66] have advanced the simulation of environments by generating videos that are guided by free-text actions. Leading this development is Sora [6], which employs cutting-edge generative methods to create complex visual sequences that adhere to the physical laws governing the environment. This capability not only enhances the fidelity of generated video content but also holds significant potential for applications in real-world driving scenarios. In autonomous driving, world models [13, 21, 49, 52, 60, 69] utilize predictive techniques to interpret driving environments. These methods generate realistic driving scenarios while extracting driving policies from video data, renewing the potential for more robust closed-loop simulations. The recent DriveDreamer4D [68] has further evidenced that leveraging pretrained world models as data machines improves dynamic driving scene reconstruction. Nonetheless, it still encounters challenges in executing larger maneuvers (e.g., multi-lane shifts)."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Overall Framework of ReconDreamer", "content": "Traditional scene reconstruction methods [8, 24, 28, 39, 53, 58, 62] face challenges due to the sparsity of training data. Recent approaches [17, 65, 68] alleviate this issue by leveraging generative priors to increase the data density. However, a gap remains between the generated data and the real data. In contrast, the proposed ReconDreamer expands the training data through an online restoration process. Notably, ReconDreamer progressively restores rendered data, effec-"}, {"title": "3.2. Training and Inference of DriveRestorer", "content": "Traditional scene reconstruction methods often suffer from artifacts when rendering novel trajectory views. To address this issue, we introduce DriveRestorer to restore these degraded renderings. In the next, we elaborate on training and inference details of the DriveRestorer.\nTraining. The major challenge of training DriveRestorer lies in the absence of rendering restoration datasets. Therefore, we propose a novel method for constructing restoration pairs. As illustrated in Fig. 3, we leverage undertrained reconstruction models [8, 24, 58, 62] to render videos $V_{ori} = G(T_{ori})$ along the original trajectory, naturally producing ghosting artifacts due to model underfitting. These degraded frames are then paired with their corresponding ground truth video $V_{ori}$, forming a rendering restoration dataset. To further enhance dataset diversity, we sample rendered videos from different training stages. Consequently, the constructed rendering restoration pairs are represented as {$V_k, V_{ori}$}, where $V_k$ denotes the degraded video frame sampled at training stage k. Fig. 4 provides a visualization of these data pairs. Based on the constructed dataset, we train DriveRestorer to restore artifacts in the rendered videos. The DriveRestorer is finetuned upon the world model [69]. Specifically, we introduce degraded video frames $V_{ori}$ as a control condition to provide appearance priors. To further emphasize the restoration of challenging regions, we apply masks to the degraded video frames $V_{ori}$ during training. Since video quality degrades in distant areas (further from the camera center) and at the sky-scene boundary, our masks $M$ focus primarily on these problematic regions (see the supplement for more details). During the training of DriveRestorer, we first feed the masked video frame $V_{mask} = V_{ori} \\odot M$ into the encoder $E$ to obtain the low-dimensional latent feature $z = E(V_{mask})$. The fine-tuning process of the world model is optimized us-"}, {"title": "3.3. Progressive Data Update Strategy", "content": "Based on DriveRestorer's capability to restore novel trajectory videos, we propose the Progressive Data Update Strategy (PDUS) to enhance driving scene reconstruction under large maneuver conditions. The PDUS first constructs a mixed dataset $D = 0.5D_{ori} \\cup 0.5D_{novel}$, where $D_{ori}$ is the original trajectory video dataset and $D_{novel}$ refers to the restored novel trajectory video dataset, which can be updated throughout the training process. The update strategy, detailed in Algo. 1, uses an update distance of $\\gamma = k\\Delta y$ meters at k-th update step to progressively update the novel trajectory $T_{novel}$. Then the reconstruction model G renders novel trajectory video $V_{novel}$, which are then processed by"}, {"title": "4. Experiments", "content": "In this section, we present our experimental setup, which encompasses the datasets, implementation details, and evaluation metrics. Subsequently, both quantitative and qualitative results are provided to demonstrate that the proposed ReconDreamer can effectively render large maneuvers while also enhancing spatiotemporal coherence. Finally, we perform experiments to explore the different stride settings in progressive data update strategy and evaluate different DriveRestorer backbone choices."}, {"title": "4.1. Experiment Setup", "content": "Dataset. We conduct experiments in eight highly interactive scenes from the Waymo dataset [46]. These scenes are characterized by numerous vehicles in various positions, following complex driving trajectories, with multiple lanes that increase the complexity of foreground and background reconstruction (see supplementary materials for specific scene IDs and frame numbers).\nImplementation Details. To showcase the capability of ReconDreamer in rendering large maneuvers, we conduct comprehensive comparisons against several state-of-the-art methods for dynamic driving scene reconstruction. These methods include Deformable-GS [62], $S^3$Gaussian [24], PVG [8], Street Gaussians [58], and DreamerDriver4D [68]. For the training phase, we divide the scenes into multiple segments, each containing 40 frames, and exclusively use data from the front-facing camera. Furthermore, we set up the training strategies and hyperparameters for each baseline method to match their original configurations, ensuring consistent training for 50,000 iterations. After the training, we assess the model's performance across three distinct"}, {"title": "4.2. Main Results", "content": "Comparison with Scene Recosntruction Baselines. In Tab. 1, we compare ReconDreamer with different dynamic driving scene reconstruction methods [8, 24, 58, 62]. The experimental results demonstrate that the current state-of-the-art dynamic driving scene reconstruction method, Street Gaussians [58], outperforms other traditional approaches (i.e., PVG [8], $S^3$Gaussian and Deformable-GS [62]) across various novel trajectory renderings. Therefore, we investigate how much ReconDreamer can improve performance based on [58]. Specifically, ReconDreamer with Street Gaussians outperforms Street Gaussians across all metrics for different novel trajectory renderings. The average scores (NTA-IoU, NTL-IoU, FID) are relatively improved by 13.38%, 3.35%, and 26.40%, respectively. Notably, ReconDreamer with Street Gaussians effectively renders large maneuvers (e.g., Lane Shift @ 6m), relatively surpassing Street Gaussians by 24.87%, 6.72%, and 29.97% on the NTA-IOU, NTL-IoU, and FID metrics, respectively.\nComparison with DriveDreamer4D. We compare the pro-"}, {"title": "4.3. Ablation Study", "content": "DriveRestorer Backbone. As shown in Tab. 4, we compare the performance of DriveRestorer using different backbones and fine-tuning strategies, including Stable Diffusion [44], Stable Video Diffusion [4], DriveDreamer-2 [69], and DriveDreamer-2 with mask. All these methods utilize Street Gaussians [58] for reconstruction. The DriveRestorer based on the Stable Diffusion [44] demonstrates strong performance, achieving relative improvements over the baseline by 2.41%, 2.88%, and 13.71% in NTA-IoU, NTL-IoU, and FID metrics, respectively. However, image-based restoration models perform moderately on metrics such as NTAIoU, primarily due to their lack of spatiotemporal consistency, which leads to positional deviation of vehicles after restoration. Meanwhile, the video-based method Stable Video Diffusion [4] provides better spatial continuity but faces challenges with high fine-tuning difficulty and lack of controllability, leading to color discrepancies and difficulties in restoring details such as lane lines. DriveDreamer-2 [69], built upon Stable Video Diffusion [4], introduces additional control conditions, such as 3D boxes and HDMaps, significantly enhancing the NTA-IoU and NTL-IoU. Compared to Stable Diffusion [44], DriveDreamer-2 improves the NTA-IoU by 9.42% and the NTL-IoU by 0.13%. Compared to Stable Video Diffusion [4], the improvements are"}, {"title": "5. Discussion and Conclusion", "content": "In closed-loop simulation, a crucial aspect is the ability to retrieve sensor data from any specified viewpoint, which can be achieved through accurate scene reconstruction. Methods like NeRF and 3DGS struggle with novel trajectory renderings. DriveDreamer4D alleviates the challenge by using a pre-trained world model but still struggles with larger maneuvers. To address these limitations, we propose ReconDreamer, a system designed to enhance driving scene reconstruction via online restoration. ReconDreamer incorporates DriveRestorer to effectively reduce artifacts and leverages a progressive data update strategy, ensuring high-quality rendering even for large-scale maneuvers spanning up to 6 meters. Experiments show ReconDreamer surpasses Street Gaussians in NTA-IoU, NTL-IoU, and FID with improvements of 24.87%, 6.72%, and 29.97%, respectively. It also outperforms DriveDreamer4D with PVG in large maneuver rendering, with a comprehensive user study and 195.87% improvement in NTA-IoU."}, {"title": "6. Implementation Details", "content": "Training for DriveRestorer. As shown in Fig. 7,the frames rendered by the reconstruction model often exhibit significant degradation at the boundary between the sky and the background and the areas far from the camera in the image center. To address these issues, we introduce a masking strategy, applying random masks to these degraded regions to guide the model in repairing them.\nMetrics. As mentioned in the main text, we utilize Novel Trajectory Agent Intersection over Union (NTA-IoU) and Novel Trajectory Lane Intersection over Union (NTL-IoU) to assess the quality of the rendered video, both metrics proposed in DriveDreamer4D [68]. These metrics are specifically designed to evaluate the spatiotemporal coherence of foreground agents and background lanes, respectively.\nThe NTA-IoU processes images rendered under new trajectories using the YOLO11 [27] detector to extract 2D bounding boxes of vehicles. Meanwhile, by applying geometric transformations to the 3D bounding boxes from the original trajectories, they can be accurately projected onto the new trajectory perspective, thus obtaining the ground truth 2D bounding boxes in the new trajectory view. Each projected 2D bounding box will find the nearest 2D bounding box generated by the detector and compute their Intersection over Union (IoU).\nSimilarly, the NTL-IoU employs the TwinLiteNet [7] model to detect lane in the images rendered under the new trajectories, and the lane from the original trajectories will also be projected onto the new trajectory through corresponding geometric transformations. Finally, the mean Intersection over Union (IoU) between the projected and detected lane lines is calculated.\nScene Selection. We select eight scenes from the validation set of the Waymo dataset [46]. These scenes feature high levels of interactive activity, with numerous vehicles in varied positions and exhibiting complex driving trajectories. Additionally, these scenes include multiple lanes, which increases the complexity of foreground and background reconstruction. As shown in Table. 6, we provide a detailed list of the segment IDs.\nUser Study. In the user study, we compare our results with two baseline models: DriveDreamer4D with PVG [68] and Street Gaussians [58]. This comparison is conducted across the eight scenarios we selected, with an emphasis on the overall quality of the videos, including the consistency and clarity of the background, as well as the positional accuracy of foreground objects. In each comparison, our method and the baseline methods are randomly assigned to the top or bottom of the video, and participants are asked to choose the option they find most satisfactory."}, {"title": "7. Baseline", "content": "PVG [8] introduces a novel unified representation model designed to capture dynamic scenes through the use of timevarying Gaussian distributions. These Gaussians are characterized by adjustable properties such as vibration direction, duration, and peak intensity. The approach distinguishes between static and dynamic elements by sorting the Gaussians according to their durations.\nDeformable-GS [62] establishes a canonical space where scenes are represented using Gaussian distributions. For capturing dynamics, it employs a deformation network to forecast the offsets of Gaussian attributes, which subse-"}]}