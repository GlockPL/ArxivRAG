{"title": "Exploring and Learning Structure:\nActive Inference Approach in Navigational Agents", "authors": ["Daria de Tinguy", "Tim Verbelen", "bart Dhoedt"], "abstract": "Drawing inspiration from animal navigation strategies, we\nintroduce a novel computational model for navigation and mapping,\nrooted in biologically inspired principles. Animals exhibit remarkable\nnavigation abilities by efficiently using memory, imagination, and strate-\ngic decision-making to navigate complex and aliased environments. Build-\ning on these insights, we integrate traditional cognitive mapping ap-\nproaches with an Active Inference Framework (AIF) to learn an environ-\nment structure in a few steps. Through the incorporation of topological\nmapping for long-term memory and AIF for navigation planning and\nstructure learning, our model can dynamically apprehend environmen-\ntal structures and expand its internal map with predicted beliefs during\nexploration. Comparative experiments with the Clone-Structured Graph\n(CSCG) model highlight our model's ability to rapidly learn environmen-\ntal structures in a single episode, with minimal navigation overlap. this is\nachieved without prior knowledge of the dimensions of the environment\nor the type of observations, showcasing its robustness and effectiveness\nin navigating ambiguous environments.", "sections": [{"title": "Introduction", "content": "A functional navigation system must seamlessly fulfil three key functions: self-\nlocalisation, mapping, and path planning. This requires both a sensing compo-\nnent for spatial perception and a storage capability to extend these perceptions\ntemporally and spatially [32]. Animals exhibit a remarkable capacity for rapidly\nlearning the structure of their environment, often in just one or a few visits,\nrelying on memory, imagination, and strategic decision-making [31,25].\nThe hippocampus and neocortex play crucial roles in episodic memory, spa-\ntial representation, and relational inference. Mammals rely on mental representa-\ntions of spatial structures, traditionally viewed as either cognitive maps or cogni-\ntive graphs, conceptualising environmental space as a network of nodes [32,24,6,1].\nRecent research suggests an integrated approach combining these concepts is\nmore effective [22]."}, {"title": "Related work", "content": "While navigating their environment, animals often encounter ambiguous sensory\ninputs due to aliasing, resulting in repetitive observations, such as encountering\ntwo overly similar corridors. They must rapidly disambiguate the structure of\ntheir environment to navigate successfully."}, {"title": "Method", "content": "In our study, our agent initiates exploration of the environment without any prior\nknowledge regarding the observations and dimensions of the map it is about to\nnavigate. Subsequently, we will clarify how, at each step, the agent engages in\ninferring the current state, a process that integrates both the notion of obser-\nvation and proprioception (position perception given a motion). This inference\ntask involves updating past beliefs based on the latest observation and motion,\nfollowing the principles of a Partially Observable Markov Model (POMDP).\nHenceforth, the agent strategically envisions sequences of actions to explore,\ntermed policies, while concurrently expanding its internal map to accommodate\npotential unexplored areas with uncertain priors. Although the agent may know\nthe relative positions of these areas, it does not foresee observations. This iter-"}, {"title": "Inference and spatial abstraction", "content": "In the context of Active Inference (AIF), the process of inferring the agent's cur-\nrent state involves integrating sensory inputs and prior beliefs within a Partially\nObservable Markov Decision Process (POMDP). We consider that our inference\nmechanism operates at the highest level of abstraction within a hierarchical spa-\ntial framework [30], where lower layers handle observation transformation and\nthe concept of blocked paths, akin to how visual observations are processed\nin the visual cortex and motion limitations are perceived by border cells [29].\nFigure 1 illustrates the agent navigating through an environment, where doors\nsignify transitions to different states while walls correspond to obstacles. We\ngive our agent the notion that doors lead to another location, while walls lead\nto the same observation and the pose stays static. At the centre of this Figure,\nwe see a path taken by the agent depicted along with the observations perceived\nby our model, demonstrating how observations are simplified and generalised at\nthe highest abstraction level into a single colour per room (floor colour). The\ninternal topological map generated by the agent based on its exploration path is\npresented in the final frame of Figure 1. The underlying POMDP model guiding\nthis inference process is depicted in Figure 2, where the current state $s_t$ (defin-\ning a room) and position $p_t$ (the location of that room) are inferred based on\nthe previous state $s_{t-1}$, $p_{t-1}$ and action $a_{t-1}$ leading to the current observa-\ntion $o_t$ (the colour of that room). The generative model capturing this process\nis described by Equation 1, where the joint probability distribution over time\nsequences of states, observations, and actions is formulated. Tildes are used to\ndenote sequences over time.\n$P(\\tilde{o}, \\tilde{s}, \\tilde{p}, \\tilde{a}) = P(o_0|s_0)P(s_0)P(p_0)P(a_0) \\prod_{t=1}^{T} P(o_t|s_t)P(s_t, p_t|s_{t-1}, p_{t-1}, a_{t-1})$\nDue to the posterior distribution over a state becoming intractable in large\nstate spaces, we use variational inference instead. This approach introduces an\napproximate posterior denoted as $Q(\\tilde{s}, \\tilde{p}|\\tilde{o}, \\tilde{a})$ and is presented in equation 2 [27].\n$Q(\\tilde{s}, \\tilde{p}|\\tilde{o}, \\tilde{a}) = Q(s_0, p_0|o_0) \\prod_{t=1}^{T} Q(s_t, p_t|s_{t-1}, p_{t-1}, a_{t-1}, o_t)$\nThe classical inference scheme heavily relies on past and current experiences\nto localise the agent within its environment, using observation alone, the agent\nwould be weak to aliased observations at different locations. By combining ob-\nservation with the agent's proprioception the model is much more robust in dif-\nferentiating ambiguous environments. The internal positioning $p_0$ is initialised at\nthe start of exploration in the absence of prior information, and is updated as the"}, {"title": "Parameter Learning", "content": "Learning within the Active Inference framework encompasses the adaptation of\nbeliefs concerning model parameters, such as transition probabilities $P(s_t|s_{t-1})$\n(e.g. how rooms are connected) and likelihood probabilities $P(o_t|s_t)$ (e.g. what\na room looks like). These parameters reflect the structural connectivity of the\nenvironment and the expected sensory outcomes given particular states.\nGenerative models in Active Inference rely on prior beliefs regarding param-\neter distributions, with updates driven by the active inference framework [21].\nUnlike traditional discrete-time POMDP, where either transitions or likelihoods\nprobabilities are fixed and updating parameters implies reasoning over a fixed\nspatial dimension [20,12,18], our model learns the probabilities of all its Markov\nmatrices and extends their dimensions dynamically. The state transitions $B ="}, {"title": "Incorporating spatial dynamics in model parameters", "content": "To extend the internal map (our state space), we propose a novel approach where\nthe agent predicts one-step policy outcomes in all directions. $B'$ can expand its\nposition dimension given a motion and $A'$ considers the probability of being at\na given state given the position. This results in the following alternative joint\nprobability:\n$A' * B' = P(s_{t+1}|p_{t+1})P(p_{t+1}|p_t, a_t)$\nAs can be foreseen, if the expected motion leads to an un-visited location,\n$s_{t+1}$ does not exist in the model. The state is undefined while the position $p_{t+1}$ is\ncertain, therefore all Markov matrices are expected to grow in their state dimen-\nsion to match this new prediction. This process enables the dynamic expansion\nof the dimensions of both the observation and position likelihoods (A, A') and\nstate transition (B) to consider the novel state $s_{t+1}$ in a process equivalent to [8].\nSubsequently, the state transition probability (B) and position likelihood (A')\ncan be updated with the imagined beliefs, forming new connections enriching\nthe agent's ability to reason about potential unexplored areas within the envi-\nronment, assisting in decision-making processes. While $s_{t+1}$ can be predicted, it\nlacks information regarding the specific observation $o_{t+1}$ expected in that loca-\ntion. Such areas exhibit high uncertainty in their observation likelihood model.\nUsing those prior, the agent can leverage the Active Inference scheme to deter-\nmine where to direct itself to maximise its objective (e.g. forming a comprehen-\nsive map of the environment). While previous models such as [8,28] adjust their"}, {"title": "Policy Selection in Active Inference", "content": "Policy selection plays a crucial role in exploring those expected states generated\nby the model. The AIF guides the agent's decision-making process based on the\nminimisation of expected surprise and uncertainty. Policy selection, informed\nby the AIF, determines the agent's actions and map extension in response to\nsensory inputs and internal beliefs.\nTypically, agents are assumed to desire to minimise their variational free\nenergy (F), which can serve as a metric to quantify the discrepancy between the\njoint distribution P and the approximate posterior Qas presented in Equation 5.\n$F = E_{Q(\\tilde{s},\\tilde{p}|\\tilde{a},\\tilde{o})} [log[Q(\\tilde{s}, \\tilde{p}|\\tilde{a}, \\tilde{o})] \u2013 log[P(\\tilde{s}, \\tilde{p}, \\tilde{a}, \\tilde{o})]$\n$= D_{KL}[Q(\\tilde{s}, \\tilde{p}|\\tilde{a}, \\tilde{o}))||P(\\tilde{s}, \\tilde{p}|\\tilde{a}, \\tilde{o})] \u2013 log[P(\\tilde{o})]$\nposterior approximationlog evidence\n$= D_{KL}[Q(\\tilde{s}, \\tilde{p}|\\tilde{a}, \\tilde{o}))||P(\\tilde{s}, \\tilde{p}, \\tilde{a})] \u2013 E_{Q(\\tilde{s},\\tilde{p}|\\tilde{a},\\tilde{o})} [log[P(\\tilde{o}|\\tilde{s})]$\ncomplexityaccuracy\nActive inference agents aim to minimise their free energy by engaging in three\nmain processes: learning, perception, and planning. Learning involves optimis-\ning the model parameters, perception entails estimating the most likely state,\nand planning involves selecting the policy or action sequence that leads to the\nlowest expected free energy. Essentially, this means that the process involves\nforming beliefs about hidden states that offer a precise and concise explanation\nof observed outcomes while minimising complexity.\nWhile planning, however, we use the expected free energy (G), indicating\nthe agent's anticipated variational free energy following the implementation of\na policy $\\pi$. Unlike the variational free energy, which focuses on current and past\nobservations, the expected free energy incorporates future expected observations\ngenerated by the selected policy.\n$G(\\pi,\\tau) = E_{Q(o_\\tau, s_\\tau|\\pi)}[log(Q(s_\\tau|\\pi) \u2013 log(Q(s_\\tau|o_\\tau, \\pi))]$\ninformation gain term\n$\u2013 E_{Q(o_\\tau, s_\\tau|\\pi))} [log(P(o_\\tau))]$\nutility term\nThe expected information gain quantifies the anticipated shift in the agent's\nbelief over the state from the prior $Q(s_\\tau|\\pi)$ to the posterior $Q(s_{\\tau}|o_{\\tau}, \\pi)$ when\npursuing a particular policy. On the other hand, the utility term assesses the\nexpected log probability of observing the preferred outcome under the chosen\npolicy. This value intuitively measures the likelihood that the policy will guide"}, {"title": "Results", "content": "We explore experimental scenarios where an agent navigates within a grid en-\nvironment with cardinal motions and still motion. The agents have no direct\naccess to a map of the environment and visual observations are considered to\nundergo hierarchical processing, transforming them from a vector to a single\ndescriptor corresponding to one colour per room. They receive localised sensory\ninputs, corresponding to the current room they are in. Sensory inputs which are\npossibly repeated at different locations (aliased observations). Given a series of\ndiscretised egocentric observations and actions, the agent must deduce the la-\ntent topology of its environment to assess various navigation options. Learning\nthis latent graph from aliased observations presents a challenge for most artifi-\ncial agents [17]. We contrast our model with CSCG [9], a specialised variant of\nHidden Markov Models (HMM). CSCG employs a probabilistic approach, us-\ning sequences of action-observation pairs without assuming Euclidean geometry.\nEach observation corresponds to a subset of hidden states known as clones. Al-\nthough these states share the same observation likelihood, they differ in their\nimplied dynamics encoded in the transition model. By analysing the sequence of\naction-observation pairs, specific clones with higher likelihoods can disambiguate\nthe aliased observations. Initially, CSCG gathers a dataset through maze explo-"}, {"title": "Discussion", "content": "This study proposes a novel high-level abstraction model informed by biologically\nplausible principles mimicking key points of animal navigation strategies [32,1].\nBy integrating a dynamic cognitive graph with internal positioning and an Active\nInference Framework, our model successfully explores the environment and learns\nits structure in a few steps, as expected from animals [31,25], facilitating adaptive\nlearning and efficient exploration. Moreover, allowing the internal map to grow\nwith expected beliefs not only creates a map adapted to any environment di-\nmension, shape or observations but also enhances exploration by creating highly\nuncertain states where the whereabouts are predictable but the corresponding\nobservations aren't. Comparative experiments with the Clone-Structured Graph\n(CSCG) model [9] underscore the effectiveness of our approach in learning envi-\nronment structures with minimal data and without prior knowledge of specific\nobservation dimensions. This is mainly due to our agent's capacity to imagine\nactions' consequences and integrate them into its beliefs. Moving forward, it\nwould be interesting to increase the prediction range of new states to integrate\ninto the model and determine the impact on navigation. Moreover, studying the\nimpact of a perfect memory on future policies and exploration efficiency, as well\nas seeing how the agent fares when trying to reach a defined objective it has\nprior upon in a familiar or novel environment would enhance the research. Fi-\nnally deploying this model in real-world scenarios such as StreetLearn [19], based\non Google map observations, would approach further this mechanism to animal\nbehaviour and provide more conclusive evidence."}]}