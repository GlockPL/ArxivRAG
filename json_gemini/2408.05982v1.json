{"title": "Exploring and Learning Structure: Active Inference Approach in Navigational Agents", "authors": ["Daria de Tinguy", "Tim Verbelen", "bart Dhoedt"], "abstract": "Drawing inspiration from animal navigation strategies, we introduce a novel computational model for navigation and mapping, rooted in biologically inspired principles. Animals exhibit remarkable navigation abilities by efficiently using memory, imagination, and strategic decision-making to navigate complex and aliased environments. Building on these insights, we integrate traditional cognitive mapping approaches with an Active Inference Framework (AIF) to learn an environment structure in a few steps. Through the incorporation of topological mapping for long-term memory and AIF for navigation planning and structure learning, our model can dynamically apprehend environmental structures and expand its internal map with predicted beliefs during exploration. Comparative experiments with the Clone-Structured Graph (CSCG) model highlight our model's ability to rapidly learn environmental structures in a single episode, with minimal navigation overlap. this is achieved without prior knowledge of the dimensions of the environment or the type of observations, showcasing its robustness and effectiveness in navigating ambiguous environments.", "sections": [{"title": "1 Introduction", "content": "A functional navigation system must seamlessly fulfil three key functions: self-localisation, mapping, and path planning. This requires both a sensing component for spatial perception and a storage capability to extend these perceptions temporally and spatially [32]. Animals exhibit a remarkable capacity for rapidly learning the structure of their environment, often in just one or a few visits, relying on memory, imagination, and strategic decision-making [31,25].\nThe hippocampus and neocortex play crucial roles in episodic memory, spatial representation, and relational inference. Mammals rely on mental representations of spatial structures, traditionally viewed as either cognitive maps or cognitive graphs, conceptualising environmental space as a network of nodes [32,24,6,1]. Recent research suggests an integrated approach combining these concepts is more effective [22]."}, {"title": "2 Related work", "content": "While navigating their environment, animals often encounter ambiguous sensory inputs due to aliasing, resulting in repetitive observations, such as encountering two overly similar corridors. They must rapidly disambiguate the structure of their environment to navigate successfully."}, {"title": "3 Method", "content": "In our study, our agent initiates exploration of the environment without any prior knowledge regarding the observations and dimensions of the map it is about to navigate. Subsequently, we will clarify how, at each step, the agent engages in inferring the current state, a process that integrates both the notion of observation and proprioception (position perception given a motion). This inference task involves updating past beliefs based on the latest observation and motion, following the principles of a Partially Observable Markov Model (POMDP). Henceforth, the agent strategically envisions sequences of actions to explore, termed policies, while concurrently expanding its internal map to accommodate potential unexplored areas with uncertain priors. Although the agent may know the relative positions of these areas, it does not foresee observations. This iterative and multi-step process serves as the cornerstone for the agent's adaptive learning and navigation strategies within the environment."}, {"title": "3.1 Inference and spatial abstraction", "content": "In the context of Active Inference (AIF), the process of inferring the agent's current state involves integrating sensory inputs and prior beliefs within a Partially Observable Markov Decision Process (POMDP). We consider that our inference mechanism operates at the highest level of abstraction within a hierarchical spatial framework [30], where lower layers handle observation transformation and the concept of blocked paths, akin to how visual observations are processed in the visual cortex and motion limitations are perceived by border cells [29].\nFigure 1 illustrates the agent navigating through an environment, where doors signify transitions to different states while walls correspond to obstacles. We give our agent the notion that doors lead to another location, while walls lead to the same observation and the pose stays static. At the centre of this Figure, we see a path taken by the agent depicted along with the observations perceived by our model, demonstrating how observations are simplified and generalised at the highest abstraction level into a single colour per room (floor colour). The internal topological map generated by the agent based on its exploration path is presented in the final frame of Figure 1. The underlying POMDP model guiding this inference process is depicted in Figure 2, where the current state \\(s_t\\) (defining a room) and position \\(p_t\\) (the location of that room) are inferred based on the previous state \\(s_{t-1}, p_{t-1}\\) and action \\(a_{t-1}\\) leading to the current observation \\(o_t\\) (the colour of that room). The generative model capturing this process is described by Equation 1, where the joint probability distribution over time sequences of states, observations, and actions is formulated. Tildes are used to denote sequences over time.\n\n\\(P(\\tilde{o}, \\tilde{s}, \\tilde{p}, \\tilde{a}) = P(o_0|s_0)P(s_0)P(p_0)P(a_0) \\prod_{t=1}^{T} P(o_t|s_t)P(s_t, p_t | s_{t-1}, p_{t-1}, a_{t-1})\\)\n\nDue to the posterior distribution over a state becoming intractable in large state spaces, we use variational inference instead. This approach introduces an approximate posterior denoted as \\(Q(\\tilde{s}, \\tilde{p}|\\tilde{o}, \\tilde{a})\\) and is presented in equation 2 [27].\n\n\\(Q(\\tilde{s}, \\tilde{p}|\\tilde{o},\\tilde{a}) = Q(s_0, p_0|o_0) \\prod_{t=1}^{T} Q(s_t, p_t | s_{t-1}, p_{t-1}, a_{t-1}, o_t)\\)\n\nThe classical inference scheme heavily relies on past and current experiences to localise the agent within its environment, using observation alone, the agent would be weak to aliased observations at different locations. By combining observation with the agent's proprioception the model is much more robust in differentiating ambiguous environments. The internal positioning \\(p_0\\) is initialised at the start of exploration in the absence of prior information, and is updated as the agent transitions between rooms (e.i., by passing through a door), as illustrated in Figure 2, thus as long as the agent is confident in its current state.\nIf the agent were to be kidnapped and re-localised elsewhere, the observation o and inferred position p would not match expectations and the confidence in the state would decrease. If the confidence in the state goes below a given threshold, the agent stops updating its internal model given new information and focuses on re-gaining confidence over its state/location.\nHowever, inferring the position p has much more to offer than localisation robustness, it is key to extending the internal map over unexplored areas yet to be integrated into the model through parameter learning."}, {"title": "3.2 Parameter Learning", "content": "Learning within the Active Inference framework encompasses the adaptation of beliefs concerning model parameters, such as transition probabilities \\(P(s_t|s_{t-1})\\) (e.g. how rooms are connected) and likelihood probabilities \\(P(o_t|s_t)\\) (e.g. what a room looks like). These parameters reflect the structural connectivity of the environment and the expected sensory outcomes given particular states.\nGenerative models in Active Inference rely on prior beliefs regarding parameter distributions, with updates driven by the active inference framework [21]. Unlike traditional discrete-time POMDP, where either transitions or likelihoods probabilities are fixed and updating parameters implies reasoning over a fixed spatial dimension [20,12,18], our model learns the probabilities of all its Markov matrices and extends their dimensions dynamically. The state transitions B = \\(P(s_t | s_{t-1}, a_{t-1})\\) and the observation A = \\(P(o_t|s_t)\\), position likelihood A' = \\(P(p_t|s_t)\\) probabilities are optimised over transitions. The position transition B' = \\(P(p_t|p_{t-1}, a_{t-1})\\), however, is not a Markov matrix and entails an incremental process based on consecutive motions (experimented or predicted), without any parameters to be learned by belief optimisation.\nThe optimisation of beliefs of the generative model parameters occur after state inference and involves minimising the free energy \\(F_e\\) while considering prior beliefs and uncertainties associated with both parameters and policies, as defined in[21]:\n\n\\(\\theta =(A, A', B)\\)\n\\(F_e =E_{Q(\\pi,\\theta)} [F(\\pi,\\theta)] + D_{KL}[Q(\\theta)||P(\\theta)] + D_{KL}[Q(\\pi)||P(\\pi)]\\)\n\nWith P and Q being respectively the joint distribution and the approximate posterior of the model. The model updates its parameters based on observed data and transitions, expanding the observation dimension of A upon encountering new information as can be seen in [28]. At initialisation, high certainty is assigned to the likelihood probabilities integrating the first observation. After realising the parameter update based on priors, the model edits its internal map dimensions and parameters based on predicted transitions, expanding all parameters in their state dimensions, thus improving exploration in unexplored environments of any unknown size."}, {"title": "3.3 Incorporating spatial dynamics in model parameters", "content": "To extend the internal map (our state space), we propose a novel approach where the agent predicts one-step policy outcomes in all directions. B' can expand its position dimension given a motion and A' considers the probability of being at a given state given the position. This results in the following alternative joint probability:\n\n\\(A' * B' = P(s_{t+1}|p_{t+1})P(p_{t+1}|p_t, a_t)\\)\n\nAs can be foreseen, if the expected motion leads to an un-visited location, \\(s_{t+1}\\) does not exist in the model. The state is undefined while the position \\(p_{t+1}\\) is certain, therefore all Markov matrices are expected to grow in their state dimension to match this new prediction. This process enables the dynamic expansion of the dimensions of both the observation and position likelihoods (A, A') and state transition (B) to consider the novel state \\(s_{t+1}\\) in a process equivalent to [8].\nSubsequently, the state transition probability (B) and position likelihood (A') can be updated with the imagined beliefs, forming new connections enriching the agent's ability to reason about potential unexplored areas within the environment, assisting in decision-making processes. While \\(s_{t+1}\\) can be predicted, it lacks information regarding the specific observation \\(o_{t+1}\\) expected in that location. Such areas exhibit high uncertainty in their observation likelihood model.\nUsing those prior, the agent can leverage the Active Inference scheme to determine where to direct itself to maximise its objective (e.g. forming a comprehensive map of the environment). While previous models such as [8,28] adjust their internal model growth to accommodate new patterns of observations, we extend the concept to predicted, un-visisted, areas and generate new states holding no observation. Those unknown states are therefore highly attractive when seeking information gain and largely improve exploration strategy."}, {"title": "3.4 Policy Selection in Active Inference", "content": "Policy selection plays a crucial role in exploring those expected states generated by the model. The AIF guides the agent's decision-making process based on the minimisation of expected surprise and uncertainty. Policy selection, informed by the AIF, determines the agent's actions and map extension in response to sensory inputs and internal beliefs.\nTypically, agents are assumed to desire to minimise their variational free energy (F), which can serve as a metric to quantify the discrepancy between the joint distribution P and the approximate posterior Qas presented in Equation 5.\n\n\\(F = E_{Q(\\tilde{s},\\tilde{p}|\\tilde{a},\\tilde{o})} [log[Q(\\tilde{s}, \\tilde{p}|\\tilde{a}, \\tilde{o})] \u2013 log[P(\\tilde{s}, \\tilde{p}, \\tilde{a}, \\tilde{o})]\\)\n\\(= D_{KL}[Q(\\tilde{s}, \\tilde{p}|\\tilde{a}, \\tilde{o}))||P(\\tilde{s}, \\tilde{p}|\\tilde{a}, \\tilde{o})] \u2013 log[P(\\tilde{o})]\\)\n\nposterior approximation\n\nlog evidence\n\\(= D_{KL}[Q(\\tilde{s}, \\tilde{p}|\\tilde{a}, \\tilde{o}))||P(\\tilde{s}, \\tilde{p}, \\tilde{a})] \u2013 E_{Q(\\tilde{s},\\tilde{p}|\\tilde{a},\\tilde{o})} [log[P(\\tilde{o}|\\tilde{s})]\\)\n\ncomplexity\n\naccuracy\nActive inference agents aim to minimise their free energy by engaging in three main processes: learning, perception, and planning. Learning involves optimising the model parameters, perception entails estimating the most likely state, and planning involves selecting the policy or action sequence that leads to the lowest expected free energy. Essentially, this means that the process involves forming beliefs about hidden states that offer a precise and concise explanation of observed outcomes while minimising complexity.\nWhile planning, however, we use the expected free energy (G), indicating the agent's anticipated variational free energy following the implementation of a policy \\(\\pi\\). Unlike the variational free energy, which focuses on current and past observations, the expected free energy incorporates future expected observations generated by the selected policy.\n\n\\(G(\\pi,\\tau) = E_{Q(o_{\\tau},s_{\\tau}|\\pi)}[log(Q(s_{\\tau}|\\pi) \u2013 log(Q(s_{\\tau}|o_{\\tau}, \\pi))]\\)\n\ninformation gain term\n\\(- E_{Q(o_{\\tau},s_{\\tau}|\\pi))} [log(P(o_{\\tau}))]\\)\n\nutility term\n\nThe expected information gain quantifies the anticipated shift in the agent's belief over the state from the prior \\(Q(s_{\\tau}|\\pi)\\) to the posterior \\(Q(s_{\\tau}|o_{\\tau}, \\pi)\\) when pursuing a particular policy. On the other hand, the utility term assesses the expected log probability of observing the preferred outcome under the chosen policy. This value intuitively measures the likelihood that the policy will guide the agent toward its prior preferences. In this study, we give no prior preference to the agent, as it does not know the environment (unknown observations and map size).\nTo calculate this expected free energy \\(G(\\pi)\\) over each step \\(\\tau\\) of a policy we sum the expected free energy of each time-step.\n\n\\(G(\\pi) = \\sum_{\\tau} G(\\pi, \\tau)\\)\n\nTo consider the best policy, we recall that active inference achieves goal-directed behaviour by selecting policies minimising this expected free energy, thereby aiming to produce observations closer to preferred outcomes or prior preferences. This is achieved by setting the approximate posterior over policies as in Equation 8 [18]:\n\n\\(P(\\pi) = \\sigma(-\\gamma G(\\pi))\\)\n\nWhere \\(\\sigma\\), the softmax function is tempered with a temperature parameter \\(\\gamma\\), given as a hyper-parameter, converting the expected free energy of policies into a categorical distribution over policies. Actions are then sampled based on this posterior distribution, with lower temperatures resulting in more deterministic behaviour.\nBy navigating without a clear preference, we desire the highest information gain, effectively pushing the agent toward states it anticipates but doesn't know what to expect from."}, {"title": "4 Results", "content": "We explore experimental scenarios where an agent navigates within a grid environment with cardinal motions and still motion. The agents have no direct access to a map of the environment and visual observations are considered to undergo hierarchical processing, transforming them from a vector to a single descriptor corresponding to one colour per room. They receive localised sensory inputs, corresponding to the current room they are in. Sensory inputs which are possibly repeated at different locations (aliased observations). Given a series of discretised egocentric observations and actions, the agent must deduce the latent topology of its environment to assess various navigation options. Learning this latent graph from aliased observations presents a challenge for most artificial agents [17]. We contrast our model with CSCG [9], a specialised variant of Hidden Markov Models (HMM). CSCG employs a probabilistic approach, using sequences of action-observation pairs without assuming Euclidean geometry.\nEach observation corresponds to a subset of hidden states known as clones. Although these states share the same observation likelihood, they differ in their implied dynamics encoded in the transition model. By analysing the sequence of action-observation pairs, specific clones with higher likelihoods can disambiguate the aliased observations. Initially, CSCG gathers a dataset through maze exploration to learn the spatial structure [9]."}, {"title": "5 Discussion", "content": "This study proposes a novel high-level abstraction model informed by biologically plausible principles mimicking key points of animal navigation strategies [32,1]. By integrating a dynamic cognitive graph with internal positioning and an Active Inference Framework, our model successfully explores the environment and learns its structure in a few steps, as expected from animals [31,25], facilitating adaptive learning and efficient exploration. Moreover, allowing the internal map to grow with expected beliefs not only creates a map adapted to any environment dimension, shape or observations but also enhances exploration by creating highly uncertain states where the whereabouts are predictable but the corresponding observations aren't. Comparative experiments with the Clone-Structured Graph (CSCG) model [9] underscore the effectiveness of our approach in learning environment structures with minimal data and without prior knowledge of specific observation dimensions. This is mainly due to our agent's capacity to imagine actions' consequences and integrate them into its beliefs. Moving forward, it would be interesting to increase the prediction range of new states to integrate into the model and determine the impact on navigation. Moreover, studying the impact of a perfect memory on future policies and exploration efficiency, as well as seeing how the agent fares when trying to reach a defined objective it has prior upon in a familiar or novel environment would enhance the research. Finally deploying this model in real-world scenarios such as StreetLearn [19], based on Google map observations, would approach further this mechanism to animal behaviour and provide more conclusive evidence."}]}