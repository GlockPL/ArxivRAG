{"title": "Reconsidering the energy efficiency of spiking neural networks", "authors": ["Zhanglu YAN", "Zhenyu Bai", "Weng-Fai Wong"], "abstract": "Spiking neural networks (SNNs) are generally regarded as more energy-efficient because they do not use multiplications. However, most SNN works only consider the counting of additions to evaluate energy consumption, neglecting other overheads such as memory accesses and data movement operations. This oversight can lead to a misleading perception of efficiency, especially when state-of-the-art SNN accelerators operate with very small time window sizes. In this paper, we present a detailed comparison of the energy consumption of artificial neural networks (ANNs) and SNNs from a hardware perspective. We provide accurate formulas for energy consumption based on classical multi-level memory hierarchy architectures, commonly used neuromorphic dataflow architectures, and our proposed improved spatial-dataflow architecture. Our research demonstrates that to achieve comparable accuracy and greater energy efficiency than ANNs, SNNs require strict limitations on both time window size T and sparsity s. For instance, with the VGG16 model and a fixed T of 6, the neuron sparsity rate must exceed 93% to ensure energy efficiency across most architectures. Inspired by our findings, we explore strategies to enhance energy efficiency by increasing sparsity. We introduce two regularization terms during training that constrain weights and activations, effectively boosting the sparsity rate. Our experiments on the CIFAR-10 dataset, using T of 6, show that our SNNs consume 69% of the energy used by optimized ANNs on spatial-dataflow architectures, while maintaining an SNN accuracy of 94.18%. This framework, developed using PyTorch, is publicly available for use and further research.", "sections": [{"title": "Introduction", "content": "Edge computing has gained prominence by processing data locally instead of relying on cloud servers, thus conserving battery life and enabling applications in environments where signal transmission is constrained (Chen et al. 2021; Jiang et al. 2020; Rashid et al. 2022). By replacing multiplications with additions to enhance local computation, spiking neural networks (SNNs) have acquired a reputation for energy efficiency (Yan et al. 2024). However, recently there are some researchers have questioned the energy efficiency of SNNs by highlighting the computational overheads, such as memory accesses and data movements. This contrast poses critical questions that we aim to address in this paper: are SNNs truly more energy-efficient than ANNs, and what are the factors that determine the energy efficiency of SNNs?\nIn fact, the comparison of energy consumption between ANNs and SNNs is very complex and varies greatly depending on the specific architectures, mapping algorithms, and network structures used. Moreover, ANNs can also be optimized to leverage parameter reuse and exploit sparsity. A fair comparison should involve an ideally optimized ANN. Only if SNNs surpass these optimized ANNs in performance, taking into account all hardware overheads, can we confidently declare SNNs are energy-efficient.\nIn this paper, we conduct a detailed analysis of the energy consumption of ANNs and SNNs across various architectures. On classical multi-level memory hierarchy architectures like GPUs, shown in Figure 1a, we assume that ANNs have optimal sparsity and weight reuse to minimize the DRAM energy, serving as the benchmark for their most efficient energy use. Then, we assess SNNs under specific conditions of time window size and sparsity to determine when they surpass this benchmark on the same system. For dataflow architecture, neuromorphic dataflow architectures (Figure 1b I) reduce SNN energy by substituting activation read and storage with network-on-chip(NOC) data movement. However, direct comparisons with ANNs are challenging because these architectures are designed for SNNs to exploit bit-level sparse spike patterns. Thus, we keep exploring the spatial-dataflow architecture(Figure 1b II), which utilizes a mesh NOC design that prioritizes connections between neighboring processing elements to eliminate long-distance data transfers. We use a sparsity-aware mapping strategy that minimizes router impact, enabling both ANNs and SNNs to operate on this architecture. We then make a fair comparison of energy consumption on the dataflow architecture.\nFor the widely used VGG16 model, our findings indicate that stringent requirements for time window size (T) and sparsity s are necessary for SNNs to outperform ANNs in energy efficiency on both classical and spatial-dataflow Architectures. Specifically, a sparsity rate exceeding 97% is required when T > 16, typically at the cost of reduced accuracy for SNNs. Additionally, with a smaller T of 6, sparsity"}, {"title": "Background", "content": "SNNs use binary spike trains over a time window size of T, consisting of only 0s and 1s, to transmit information among layers. Rate-encoded SNNs encode membrane potential information through the firing rates of neurons, providing robust data representation (Stein, Gossen, and Jones 2005; Brette 2015). Commonly, integrate-and-fire (IF) model is used to generate spikes (Eshraghian et al. 2023; Ganguly and Chakrabarti 2020; Rueckauer et al. 2017; Wu et al. 2023). In the IF model, each neuron at timestep t calculates the weighted sum of incoming spikes and biases, adds this to the membrane potential V from the previous timestep t - 1, and checks against a threshold \u03b8. If exceeded, the neuron emits a spike (s'(t) = 1 and resets the potential; otherwise, it outputs '0'.\nUnlike traditional neural networks, SNNs are more hardware-friendly due to their binary nature (Yan, Zhou, and Wong 2023). However, the binary spike-based data transmission complicates the use of traditional backpropagation algorithms, which depend on partial derivatives and the chain rule. To overcome this, two primary approaches are adopted: SNN-direct training algorithms (Wu et al. 2019; Taherkhani et al. 2019) and ANN-to-SNN conversion (Yan, Zhou, and Wong 2023). The former, often relying on spike-timing-dependent plasticity (Vigneron and Martinet 2020), is hard to train and prone to overfitting. The latter approach, which we use in this study, involves training an ANN and transferring its weights to an SNN. However, direct weight transfer can lead to an accuracy drop espeacially when T is small. To address this, we replace ReLU function with clamp C(x) and quantize Q(x) functions during ANN training as described in Equation 1 and Equation 2, aligning ANN activations more closely with SNN operations.\n$C(x) = \\begin{cases}0, & x < 0 \\\\ x, & 0 \\\\ 1, & x > 1.\\end{cases}$  (1)\n$Q_T(x) = \\frac{\\lfloor x \\cdot T \\rfloor}{T}$  (2)\nThen, we can run the SNN inference and minimize the ANN-to-SNN conversion accuracy loss."}, {"title": "Energy Comparison and Analysis between ANNs and SNNS", "content": "In our study, we analyze two primary contributors to energy consumption in spiking neural networks: the energy required by computing units and the energy used for memory operations, which include both input data delivery to computing units and the movement of intermediate results. Given the highly parallel nature of current spatial architectures, developing a unified model is challenging due to variables such as the computation's mapping to compute units and the size of each unit's local memory. Effective mapping can enhance data reuse within a compute unit's local memory, leading to greater efficiency. In this section, we assess the models using a 'reuse factor' (RF), defined as the number of times a variable is reused once it is stored in local memory (SRAM).\nFurther, this paper discusses the architectures of SNNs in the digital domain. While the analog domain remains promising for SNNs, it is less mature in terms of product development compared to the digital domain."}, {"title": "Classical Architectures", "content": "Classical accelerators like GPUs, TPUs, and NPUs can be abstractly modelled as computing units situated above a multi-level memory hierarchy. The hierarchical memory system exchanges inputs, outputs, and intermediate data (Chen et al. 2020).\nIn the ANN model, energy consumption is outlined in Equation 3. For each neuron, weights are initially transferred from DRAM to SRAM; however, the cost associated with this transfer (EDRAM + ESRAM) is mitigated by reusing the weights. We specifically consider scenarios where activations and outputs for each layer are small enough to be cached within local SRAM, eliminating DRAM operations for them. This is typically feasible in CNN architectures like VGG, where the average size of layer activations is usually smaller than 18KB and can be efficiently stored in SRAM (Mascarenhas and Agarwal 2021). Then both activations and weights are accessed from local SRAM for computations.\n$E_{ANN} = N_{src} * (\\frac{E_{DRAM} + E_{SRAM}}{R_{weight}} + \\frac{E_{SRAM}}{W_{weight}})\\frac{1}{RF_w} + (1 - \\gamma) * (E_{MAC} + \\frac{E_{SRAM}}{R_{input}/weights} + E_{SRAM} + E_{SRAM} + \\frac{E_{SRAM}}{W_{output}}))$ (3)\n, where Nsrc represents the number of source neurons that have a connection to the target neuron, and 1 \u03b3 indicates the sparsity rate that the ANN can exploit.\nThe reuse factor for a convolutional layer can be expressed as:\n$RF_w = (\\lfloor \\frac{H_{in} - K_h + 2P_h}{S_h} +1 \\rfloor) * (\\lfloor \\frac{W_{in} - K_w + 2P_w}{S_w} +1 \\rfloor)$ (4)\n, where $H_{in}$ and $W_{in}$ denote the height and width of the input, respectively; $K_h$ and $K_w$ represent the kernel height and width; $P_h$ and $P_w$ specify the padding along the height and width; and $S_h$ and $S_w$ refer to the stride lengths in the height and width dimensions.\nFor SNNs, the dependency across timesteps introduces additional overhead on these accelerators. SNNs use the same weights across different timesteps but require only one bit of the input spike train per timestep. Consequently, the energy consumption per timestep in an SNN is comparable to producing a single output in an ANN. The key differences include:\nThus, we can present energy of SNNs as follows:\n$E_{SNN} = N_{src} * T* (-\\frac{E_{SRAM}}{R_{input}} + \\frac{k (E_{DRAM} + E_{SRAM})}{R_{weight}} )\\frac{1}{RF_w} weight +(1-\\delta) * (E_{ADD compute} \\frac{E_{SRAM}}{R_{input}/weights} + E_{SRAM} \\frac{E_{SRAM}}{W_{output}}))+T*(E {SRAM} + E_{ADD} + E_{CMP}+ \\frac{E_{SRAM}}{storing output}\\frac{k}{W_{spike} SRAM}+(1-\\delta) \\frac{E_{SUB firing} + E {SRAM} update potential}) $ (5)\nThe reuse factor for SNNs, RFw, can be formulated as ranging between RFw and T \u00d7 RFw. The worst-case scenario occurs when no weight reuse between time window size is possible, requiring each time to process weights in a manner similar to ANNs, but on a per-timestep basis. In the best case, weights are effectively reused across all timesteps."}, {"title": "Neuromorphic Dataflow Architecture", "content": "Unlike GPUs, dataflow architectures emphasize distributed memory and on-chip data transfers among smaller compute cores, known as Processing Elements (PEs). These architectures mainly manage on-chip data transfers at a finer granularity, reducing the overhead associated with memory operations in traditional memory-centralized architectures. This design makes dataflow architectures particularly well-suited for handling SNNs, as it aligns with the network's distributed processing needs. Examples of such processors include digital neuromorphic chips, such as Intel's Loihi seris and IBM's TrueNorth (Cheng et al. 2017; Lines et al. 2018). This design is characterized by the local memories of each PE and the connectivity among each pair of PEs through a NOC\nTypically, multiple neurons are statically mapped to each PE. The weights of the inputs of the neuron are stored in the PE's local memory. PE is event-triggered and works only upon the new arrival of spikes. As illustrated in Figure 2, each PE should minimally be capable of several key functions: receiving a spike from the router, retrieving the corresponding weight from local memory for the spike generator and the receiving neuron (to maximize PE utilization, several neurons are typically assigned to one PE), accumulating the voltage of the receiving neuron, updating the neuron's state, and, if the neuron spikes post-accumulation, generating an output spike packet to send back to the router. Most neuromorphic chips are optimized specifically for SNNs due to their efficient handling of sparse spike data, making the running of ANNs significantly less efficient on these platforms. Given the platform's specific design optimizations, we focus solely on running SNNs and present their energy consumption as follows:\n$E_{SNN} = N_{src} * T* (1 - s) * (\\frac{(E_{SRAM} + E_{ADD})}{number of actual spikes read weight and add operations} +T*(\\frac{E_{SRAM} + E_{ADD} + E_{CMP} + (1 - s)E_{SUB} + E_{SRAM}}{R_{state}})) + N_{src} * T * (1 \u2013 s) * N_{hop} * E_{TPHop}$ (6)\nHere, $N_{hop}$ represents the average number of hops or the number of routers a spike passes through to reach its destination neuron. $E_{TPHop}$ denotes the energy expended per hop."}, {"title": "Spatial-Dataflow Architecture", "content": "In neuromorphic architecture, computation is handled efficiently, yet network costs pose significant challenges. The NOC must ensure robust connectivity, allowing each neuron from one layer to connect to any neuron in the subsequent layer. When layers have high dimensionality, data transfers between distantly located neurons become costly. Additionally, this can lead to excessive network traffic, causing congestion and necessitating increased network capacity.\nHowever, other dataflow architectures like SambaNova, AMD AI Engine, and Tenstorrent offer alternative solutions to network cost issues. These architectures feature larger Processing Elements (PEs) equipped with local memory and specialized Instruction-Set Architectures, such as RISC-V-based or Very-Long-Instruction-Word (VLIW)-based systems. Typically, their NOC design prioritizes connections between neighboring PEs. While any-to-any data transfer is technically possible, it is restricted, with most data transfers occurring between adjacent PEs. The key distinction between these and neuromorphic architectures lies in the enhanced capabilities of PEs-larger memory and greater computational power and reduced capacity for long-distance data transfers.\nInspired by Gustavson's data flow (Gustavson 1978), we have developed an improved mapping design. This design efficiently maps both convolution-based and MLP-based SNNs onto these accelerators, ensuring minimal data transfers across distantly located PEs. The details of such mappings are illustrated as follows:\nFigure 4 depicts the mapping process for the FC layer. During inference, constant weights, indexed by the input (x) and output (y) neurons they connect and represented as Wx,y, are preloaded into the local memory of each PE. Each PE manages a subset of these connections, with the number of input neurons it handles determined by the connectivity bandwidth among PEs.\nIn our spiking neural network model, input spikes are streamed horizontally in 128-spike packets to the PEs. Each PE decodes the positions of non-zero values, fetches the corresponding weights, and locally accumulates the weighted sums.\nThis mapping strategy can extend to convolutional layers, where the computation of weight sums is based on kernel size. Rows share input channels across different output channels, while columns accumulate these input channels into a single output.\nConsequently, the energy can be computed by:\n$E_{SNN} = N_{src}* T* (1-s) * (\\frac{(E_{SRAM} + E_{ADD})}{number of actual spikes read weight and add operations} + (E_{SRAM} + E_{ADD} + E_{CMP} + (1 \u2212 s)E_{SUB} + E_{SRAM}))$  (7)\n$E_{ANN} = N_{src} * \\gamma * ((E_{SRAM}) + E_{MAC})$  (8)"}, {"title": "Energy comparison on varies architectures", "content": "In this section, we derive the energy consumption for each operation, as detailed in Table 1. This analysis enables us to identify conditions, such as sparsity rate and time window size, under which SNNs achieve optimal energy efficiency.\nThus, within classical architectures, for an SNN to surpass the energy efficiency of an ANN, the sparsity parameter s must meet the following condition:\n$s > 1 - \\frac{(80.23 \\frac{E_{DRAM} + E_{SRAM}}{RF_W} + \\frac{0.2j}{N_{src}})}{(64.32 \\frac{E_{DRAM} + E_{SRAM}}{RF_W} + \\frac{2020}{N_{src}})T}$ (9)\nSimilarly, within spatial-dataflow architecture, the required sparsity s must satisfy this criterion:\n$s > 1 - \\frac{(20.23 \\frac{E_{SRAM}}{RF_W} + \\frac{40.06}{N_{src}}T)}{(20.03 + \\frac{2020}{N_{src}})T}$  (10)\nFor the neuromorphic dataflow architecture, since ANNs are not well-suited for this architecture due to its bit-level granularity of data transfer, we compare the performance of SNNs on this architecture against ANNs on two other architectures. Acknowledging that this comparison may not be entirely fair, we only detail the results in the appendix.\nUsing the widely implemented VGG16 network architecture as an example, we illustrate the energy efficiency breakpoint between ANNs and SNNs in Figure 5. Spe-cially, Nsrc and RFw is set as the average number of source neurons and the reuse factor across all layers, respectively. For the purpose of illustrating the average-case scenario, RFw is equated to $(1 + T)RF/2$. In alignment with related works (Yan et al. 2024; Dampfhoffer et al. 2023), the parameter k is set at 4.66 and \u03b3 is set at 0.45. Specifically, for a time window size of T = 6, the energy consumption of ANNs matches that of SNNs at a sparsity rate of 0.92 and 0.93 on classical architectures and spatial-dataflow architectures. If the sparsity rate s falls below these thresholds, SNNs demonstrate greater energy efficiency on both types of architectures.\nAdditionally, for comparison, if we disregard hardware overhead, the energy consumption for ANNs can be modeled as Nsrc \u00d7 EMAC, and for SNNs as Nsrc \u00d7 T \u00d7 (1 \u2212 s) \u00d7 EADD, ignoring the smaller spike energy for simplicity. Under these conditions, the sparsity s for SNNs only needs to exceed 1 \u2013 0.23/(0.03 \u00d7 T) to be deemed energy-efficient. For instance, at T = 6, even with s = 0\u2014indicating no sparsity-SNNs would still appear more energy-efficient than ANNs. This presents a contradiction, as reality requires s to exceed 92% for actual energy savings in classical architectures. This highlights a potential misconception where SNNs may seem more energy-efficient than they truly are."}, {"title": "Low-sparsity and high-efficiency SNNs", "content": "To enhance the energy efficiency of SNNs relative to ANNs, our findings suggest that increasing the sparsity rate s when T is fixed. In this section, we introduce two strategies to achieve this: incorporating regularization terms specifically designed to reduce the weights and the activations, shown in Figure 6. ANN-to-SNN conversion training methods which introduced in background are used in this section."}, {"title": "Regularization term for minimizing weight", "content": "To achieve lower spike rates, we leverage the principle that smaller synaptic weights result in reduced membrane potentials at each timestep, subsequently leading to decreased spiking activity. As detailed in Equations 11 and 12, we augment the original cross-entropy loss function, Lce, with an L2 regularization term, Lw. This term is scaled by a factor, \u03bb\u2081, modulating the synaptic weights to optimize energy efficiency.\n$L_w = \\frac{1}{2N} \\sum_{i=1}^{N} ||W^l||^2$  (11)\n$L = L_{ce} + \\lambda_1 L_w$  (12)\nIn exploring the trade-off between SNN accuracy and efficiency, we use the VGG16 architecture applied to the CIFAR-10 dataset as a case study. We train the network for 100 epochs with a learning rate of 1e-2 and investigate the relationship between SNN accuracy and sparsity across various time window sizes."}, {"title": "Regularization term for minimizing activations", "content": "Furthermore, by lowering activation levels during training, we can reduce spike count and enhance sparsity when converting the model to an SNN. We control the activations AL of the convolutional layers by clamping them (see Eq 1) between 0 and 1, followed by quantization (see Eq 2), and record the modified activations. We introduce a regularization term La, which sums all activations post-clamp and quantization, and integrate it into the original cross-entropy loss function, Lce, to further decrease activation levels. Building on previous results, we continue to train the network using varying levels of \u03bb\u2082 applied to La.\n$A^l = C(Q_T(A^l))$  (13)\n$L = L_{ce} + \\lambda_2 \\sum A^l$  (14)\nAs demonstrated in Table 3, after training for 100 epochs, we achieved an SNN accuracy of 92.76% with a sparsity rate of 94.19% using a scaling factor of \u03bb\u2082 = 1 \u00d7 10-6. Using either a larger or smaller scaling factor results in significant accuracy drops to as low as 86.41% or maintains a low sparsity rate. Consequently, we have set \u03bb\u2082 at 1 \u00d7 10-6."}, {"title": "SNN performances", "content": "The comprehensive experimental results are presented in Table 5. We employed a range of VGG network structures, from the smaller scale VGG* to VGG19, each trained with a learning rate of 5e-3 and hyper-parameters optimized as discussed in the previous section. Testing on the CIFAR-10 dataset with models VGG* and VGG16, we achieved SNN accuracies of 94.18% and 92.76%, respectively, and corresponding sparsity rates of 94.85% and 94.19%, all at a time window size of T = 6. Comparable outcomes are observed on the CIFAR-100 dataset, where the VGG16 model achieved an SNN accuracy of 69.44% and a sparsity rate of 93.98%. Similarly, the VGG* model reached an accuracy of 76.63% and a sparsity rate of 93.41%."}, {"title": "Conclusion", "content": "In this study, we analyze and quantify the energy consumption of ANNs and SNNs across classical, neuromorphic-dataflow, and spatial-dataflow architectures. We find that SNNs often present misleading energy savings due to overlooked hardware overhead. Energy efficiency in SNNs is only achieved under stringent conditions. For example, taking VGG16 as a case study, with a time window of T = 6, the sparsity rate must be at least 93% to make SNN energy efficient than ANN in most architectures, and a higher time window size makes this condition even more stringent. We hope our research can provide SNN peoples with a benchmark to show under what conditions and in which architectures SNNs are truly energy-efficient and worth pursuing. Additionally, by integrating specific regularization techniques, we achieve SNNs with a sparsity of 94.19% on the CIFAR-10 dataset with VGG16 network structure, reducing energy use to 0.85 and 0.78 times that of the best-case ANN on classical GPU-like and spatial-dataflow architectures while maintaining 92.76% accuracy. We believe our results will prompt the SNN community to prioritize true energy efficiency and foster further advancements in the field."}, {"title": "Experiment Setup", "content": "In this study, training was conducted using CUDA-accelerated PyTorch version 1.12.1+cu116. on a system with an AMD EPYC 7763 64-Core Processor, 1000GB of DRAM, and dual NVIDIA A100 GPUs. The system operated on Linux 5.15.0-86-generic x86_64."}, {"title": "Comparison on neuromorphic dataflow architecture", "content": "Due to the limitations of the neuromorphic dataflow architecture, where routers are designed to transmit binary signals, running ANNs is not feasible, making an apple-to-apple comparison with this architecture impossible. Consequently, most related works recommend comparisons with ANNs on classical architectures. However, in our paper, we present the energy efficiency breakpoints between ANNs and SNNs for both classical and spatial-dataflow architectures in Figure 9. Furthermore, the actual number of hops required for communication heavily depends on the neuron mapping across the network. For a chip typically sized 3x8, this can vary between 0 to 23 hops. For our analysis, we use an empirical average of 6 hops. We detail this relationship in the equation provided and illustrate it in the accompanying figure.\n$s > 1 - \\frac{80.23 + \\frac{2020}{N_{src}} + \\frac{40.06 T}{RF_W}}{(20.03 + \\frac{0.03}{N_{src}} + \\frac{10 N_{hop}}{N_{src}})T}$  (15)\nSimilarly,SNN with neuromorphic dataflow and ANN with spatial-dataflow architecture:\n$s > 1 - \\frac{20.23 - \\frac{40}{N_{src}} + \\frac{40.06 T}{RF_W}}{(20.03 + \\frac{0.03}{N_{src}} + \\frac{10 N_{hop}}{N_{src}})T}$  (16)"}]}