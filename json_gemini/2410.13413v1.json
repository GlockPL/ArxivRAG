{"title": "THINK THRICE BEFORE YOU ACT: PROGRESSIVE THOUGHT REFINEMENT IN LARGE LANGUAGE MODELS", "authors": ["Chengyu Du", "Jinyi Han", "Yizhou Ying", "Aili Chen", "Qianyu He", "Haokun Zhao", "Sirui Xia", "Haoran Guo", "Jiaqing Liang", "Zulong Chen", "Liangyue Li", "Yanghua Xiao"], "abstract": "Recent advancements in large language models (LLMs) have demonstrated that progressive refinement, rather than providing a single answer, results in more accurate and thoughtful outputs. However, existing methods often rely heavily on supervision signals to evaluate previous responses, making it difficult to effectively assess output quality in more open-ended scenarios. Additionally, these methods are typically designed for specific tasks, which limits their generalization to new domains. To address these limitations, we propose Progressive Thought Refinement (PTR), a framework that enables LLMs to progressively refine their responses. PTR operates in two phases: (1) Thought data construction stage: We propose a weak and strong model collaborative selection strategy to build a high-quality progressive refinement dataset to ensure logical consistency from thought to answers, and the answers are gradually refined in each round. (2) Thought-Mask Fine-Tuning Phase: We design a training structure to mask the \"thought\" and adjust loss weights to encourage LLMs to refine prior thought, teaching them to implicitly understand \"how to improve\" rather than \"what is correct.\" Experimental results show that PTR significantly enhances LLM performance across ten diverse tasks (avg. from 49.6% to 53.5%) without task-specific fine-tuning. Notably, in more open-ended tasks, LLMs also demonstrate substantial improvements in the quality of responses beyond mere accuracy, suggesting that PTR truly teaches LLMs to self-improve over time.", "sections": [{"title": "INTRODUCTION", "content": "Recent advancements in large language models (LLMs) have highlighted that progressive refinement is more important than simply providing a single answer (Yang et al., 2023b; Madaan et al., 2023b). Humans often rely on a combination of two thinking systems to solve problems, known as System 1 and System 2 (Kahneman, 2011). System 1 facilitates quick, intuitive responses but often lacks the depth required to handle complex reasoning tasks. In contrast, System 2 engages in progressive refinement, gradually improving a solution by starting with a rough approximate thought and iteratively adding detail and accuracy. Recent work, such as GPT-01 (OpenAI, 2024), demonstrates that LLMs perform better by adopting progressive thought refinement. This approach leads to more accurate and thoughtfully considered outcomes, similar to how the human brain processes complex tasks."}, {"title": "RELATED WORK", "content": "Progressive Refinement with External Feedback Existing work often relies on external tools or stronger LLMs to provide feedback for refinement. For example, external tools are used to critique and provide feedback on the primary model's responses (Yang et al., 2023a; Chen et al., 2023; Charalambous et al., 2023; Nijkamp et al., 2023; Yao et al., 2022; Gou et al., 2023). Models have improved their code generation capabilities by leveraging error messages from the Python interpreter (Wang et al., 2023b) and by teaching large language models to debug and explain their own code, allowing them to identify and fix errors without human feedback (Chen et al., 2023). Similarly, compiler feedback has been utilized in code generation (Chen et al., 2024; Olausson et al., 2023). Additionally, some approaches utilize criticisms or constraints generated by stronger models (Pan et al., 2023; Du et al., 2023; Bai et al., 2022; Huang et al., 2023a), such as using a strong model to verify the correctness of another model's math solutions (Wang et al., 2023a), thereby relying on external information sources to guide improvements. Although models can self-correct through external feedback (Pan et al., 2023), this approach does not fully tap into their intrinsic progressive refinement capabilities. Moreover, it requires task-specific feedback models or tools, increasing the cost of adapting to a broader range of tasks. Furthermore, current LLMs struggle to self-correct reasoning errors without external feedback (Huang et al., 2023b). Our work aims to unlock the model's inherent Progressive Refinement ability, enabling it to perform progressive refinement across all domains without relying on external tools.\nPrompting for Progressive Refinement Various Prompting methods have been introduced to enhance Progressive Refinement, such as prompting LLMs to generate explanations and self-correct code (Li et al., 2023a), or encouraging them to generate alternative solutions and revision suggestions (Zhang et al., 2024). Some methods iteratively improve outputs by generating feedback through task-specific prompts (Madaan et al., 2023a), or guide models to generate fine-grained feedback in mathematical problem-solving, further enhancing solution accuracy and quality (Xue et al., 2023). The Reflexion method enables language models to operate effectively in specific environments by allowing them to reflect and adjust their actions when encountering errors (Shinn et al., 2023b). However, these approaches often require carefully designed, task-specific prompts or even oracle ground-truth answers (Shinn et al., 2023a), making LLMs highly sensitive to evaluating response and achieving optimal performance (Wu et al., 2024a). Without external tools, LLMs have limited self-correction capabilities when relying solely on prompting (Huang et al., 2023b; Zheng et al., 2024).\nFine-Tuning for Progressive Refinement In current progressive refinement work, fine-tuning typically relies on reward models or verifiers to assess the accuracy of model outputs based on predefined criteria (Wang et al., 2023a; Lightman et al., 2023; Uesato et al., 2022a). For instance, some research focuses on improving the model's ability to identify and correct mistakes (Han et al., 2024), while others progressively validate solutions, such as in solving math problems (Uesato et al., 2022b). Additionally, reinforcement learning (RL) (Chen et al., 2024; Yuan et al., 2024; Rosset et al., 2024a; Akyurek et al., 2023) has been applied to align model outputs with correct responses. For example, researchers create preference-based datasets to align outputs with human values and reduce harmful content (Wang et al., 2024; Rosset et al., 2024b). Similarly, ROUGE has been used as a reward function in text summarizing tasks to optimize generated summaries (Akyurek et al., 2023). While these methods effectively train models, they focus on building task-specific datasets and reward functions tailored to particular objectives. In contrast, our approach redefines the fine-tuning objective to bolster the model's capacity for progressive refinement. Rather than relying on domain-specific datasets, our model is trained to iteratively enhance its responses-starting from initial thoughts and evolving toward increasingly refined answers."}, {"title": "PROGRESSIVE THOUGHT REFINEMENT FRAMEWORK", "content": "Our proposed framework, Progressive Thought Refinement (PTR), comprises two stages, as illustrated in Figure 1: (1) Progressive Thought Refinement Dataset Construction and (2) Progressive Weighted Thought-Mask Fine-tuning. The primary objective of this framework is to enhance models' progressive refinement abilities, enabling them to handle diverse and unfamiliar tasks without relying on task-specific fine-tuning. Since fine-tuning models for every task is impractical, our approach utilizes general queries, thoughts, and answers to help models comprehend progressive refinement. This strategy gradually improves their capacity to tackle complex tasks through progressive refinement."}, {"title": "PROGRESSIVE THOUGHT REFINEMENT DATASET CONSTRUCTION", "content": "In the first stage, we construct a progressive refinement dataset that includes Queries, Thoughts, and Answers. The Thoughts capture a sequence of different reasoning attempts, which may be varied, incomplete, or even incorrect, reflecting the model's initial exploration of the problem. In contrast, the Answers provide more confident and well-reasoned responses. This structured approach helps the model implicitly understand the difference between initial thoughts and improved answers, enabling it to generate more thoughtful and in-depth responses over time."}, {"title": "QUERY PREPARATION", "content": "To enhance the model's generalization, we avoid creating domain-specific datasets. Instead, we use queries from open-domain general datasets (details in Appendix B.3), ensuring the model develops general refinement abilities rather than specializing in specific areas. Our data preprocessing involves three key steps. First, we perform data cleaning to remove noise and irrelevant content, such as images or URLs. Second, to prevent data leakage, we exclude domain-specific testing queries during training. Finally, we incorporate traditional SFT data (queries and answers) into our dataset to mitigate the risk of catastrophic forgetting."}, {"title": "THOUGHT-ANSWER PREPARATION", "content": "We strategically select weak and strong models to generate sequences of thoughts and improved answers from an initial query. The objective is to ensure that the final answer is progressively improved through multiple iterations rather than relying on a single-step response. We also employ In-Context Learning (ICL) (Dong et al., 2024) and consistency filtering to ensure logical coherence between thoughts and answers.\nWeak-Strong Model Collaborative Selection Criteria To ensure the final answer shows significant improvement over the initial thought sequence, we adopt a weak-strong model collaborative selection strategy. Let $\\theta_w$ and $\\theta$ represent the abilities of the weak and strong models, respectively, with the goal of ensuring $\\theta \\gg \\theta_w$. We employ three key strategies: Model Parameter Strength, Model Version (New vs. Old), and Domain-Specific Fine-Tuning. These selection strategies ensure the quality of the final answer surpasses that of the previous thoughts. Additionally, we validate that the strong model performs significantly better than the weak model through Wilcoxon significance tests, as shown in Appendix \u0412.2."}, {"title": "Thought Generation by the Weak Model", "content": "The weak model generates a sequence of thoughts based on the input query $q_i$, with $y_{i,w}^t$ representing the initial thought at the t-th attempt. We denote the strong model as $\\pi_{strong,\\theta_s}$ and the weak model as $\\pi_{weak,\\theta_w}$. These initial thoughts may contain errors but provide a foundation for further refinement:\n$S_{i,thought} = [y_{i,w}^1, y_{i,w}^2, ..., y_{i,w}^t] = \\pi_{weak,\\theta_w}(\\cdot | q_i)$."}, {"title": "Answer Refinement by the Strong Model", "content": "To achieve progressive refinement, we leverage the strong model to produce increasingly improved answers. We use ICL to ensure logical coherence between the outputs of the strong and weak models and to avoid randomness. This guides the strong model to generate better answers based on prior thoughts. Specifically, the strong model takes the sequence of thoughts $S_{i,thought}$ and query $q_i$ as input and generates the final answer $\\hat{y}_{i,s,icl}$:\n$y_{i,s,icl} = \\pi_{strong,\\theta_s}(\\cdot | S_{i,thought}, q_i)$."}, {"title": "Thoughts-Answer Consistency Filtering", "content": "To further ensure that the thought process exhibits logical coherence, we apply consistency filtering to remove inconsistent outputs. If the consistency score is below a certain threshold, the pair is considered inconsistent and removed, ensuring that only coherent thought sequences are used for the final output (see Appendix A.1)."}, {"title": "PROGRESSIVE WEIGHTED THOUGHT-MASK FINE-TUNING", "content": "In the second stage, we perform weighted thought-mask fine-tuning using the datasets constructed previously, consisting of the input query $q_i$, the initial thought sequence $S_{i, thought}$, and the final answer $y_{i,s,icl}$. Formally, the dataset is represented as:\n$D = \\{(q_i, S_{i, thought}, y_{i,s,icl})\\}_{i=1}^{N}$"}, {"title": "Thought Mask Mechanism", "content": "To help the model understand the improvement between the thought process and the answer-rather than focusing solely on the final answer-we introduce a thought mask mechanism. This mechanism selectively hides parts of the thought process during training, as shown in Figure 1 (B). It calculates the loss based only on the accuracy of the refined final answer, ensuring the model focuses on enhancing the quality of its ultimate response. Additionally, we provide refinement instructions (e.g., \"Please continue thinking and refine your answer\") after each thought process to prompt better refinement in subsequent iterations."}, {"title": "Weighted Supervised Learning", "content": "We adopt a weighted supervised learning approach that enables the model to focus on refining its answers by progressively improving its thought process. Specifically, we perform weighted supervised learning that emphasizes both the accuracy of the final answers and the logical consistency of the thought process. The loss function optimizes the model in three key areas: generating accurate final answers, maintaining consistency in reasoning and ensuring that the model's confidence increases progressively throughout the thought process.\n$L_{PTR}(\\theta) = \\sum_{(q_i, S_{i,thought}, y_n) \\in D}  -\\lambda_1 \\log P_\\theta(y_n | q_i, S_{i,thought}; \\theta) \\+\\ \\lambda_2 \\sum_{t=2}^{n} F_{cons}(y_t, y_{t-1}) + \\lambda_3 \\sum_{t=1}^{n} \\beta_t (1 - P_{\\theta}(q_i, S_{i, thought}; \\theta))$"}, {"title": null, "content": "Unlike standard supervised fine-tuning, which trains the model to produce a single response y given x, Equation 3.4 focuses exclusively on the accuracy of the final response generated after the thought refinement process. It also ensures that the final response remains logically consistent with the previous thought sequence. We encourage the model to maintain higher confidence in its predictions during subsequent reasoning steps. Here, $\\lambda_1$, $\\lambda_2$, and $\\lambda_3$ are dynamically adjusted according to the model's needs, with their sum constrained to 1."}, {"title": "EXPERIMENTS", "content": "The goal of our experiments is to demonstrate the effectiveness of PTR in enabling language models to progressively enhance their responses. Specifically, we aim to answer the following questions: (1) Can the PTR method activate the model's progressive refinement ability? (2) Does our method demonstrate generalization? (3) Does progressive refinement ability emerge during training? (4) Is our method robust across different LLMs and instructions? (5) How many iterations are required for our method to achieve optimal performance?\nDatasets Sources Our model has trained on our PTR dataset, derived from the WizardLM dataset (Xu et al., 2023). After thorough cleaning, we reduced the original dataset from approximately 50k QA pairs to 40k high-quality QA pairs.\nEvaluation Tasks In our experiments, we perform generalization over ten datasets across different tasks. For general tasks, we use MMLU (Hendrycks et al., 2020), and for coding tasks, we use HumanEval (Chen et al., 2021) (abbreviated as H-Eval). DROP (Dua et al., 2019) is used for comprehension tasks (abbreviated as Comp), and XSum (Narayan et al., 2018) is applied for summary tasks. We use GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) for math-related tasks. For complex reasoning tasks, we use ARC and GPQA (Rein et al., 2023). For knowledge reasoning, we utilize Winogrande (Sakaguchi et al., 2019) (abbreviated as Wino) and CommonsenseQA (Talmor et al., 2019) (abbreviated as Comm).\nEvaluation Settings We use greedy decoding (with temperature set to 0) for final generation, as lower temperature yields better performance shown in Appendix A.2. We utilize zero-shot prompting (Kojima et al., 2023) for both answer sampling and evaluations, observing that zero-shot prompting outperforms few-shot prompting for LLMs fine-tuned on specific tasks. All of our experiments are conducted on workstations equipped with eight NVIDIA A800 PCIe GPUs with 80GB memory, running Ubuntu 20.04.6 LTS and PyTorch 2.0.1.\nBaselines We compare our model with base models and prior approaches: (1) Prompt: Directly prompting the model to refine its answer (Huang et al., 2023b). (2) IFT: Instruction Fine-Tuning by directly fine-tuning the input-output pairs from strong models on the PRD dataset to show that improvements are not due to knowledge distillation. (3) RL: Perform one reinforcement learning training (Wu et al., 2024b) iteration on the PRD dataset to compare with our method. Specifically, we use the thoughts and answers of the PRD dataset to construct preference data, and prefer model to produce stronger answers through DPO (Rafailov et al., 2024). We compare these methods on the PRD dataset under the same settings as in the previous section. Detailed settings are in Appendix B.3."}, {"title": "CAN THE PTR METHOD ACTIVATE THE MODEL'S PROGRESSIVE REFINEMENT ABILITY?", "content": "PTR Activates Progressive Refinement Ability As shown in Table 1, to emphasize the progressive refinement ability, we conduct tests on a broad range of tasks. The result demonstrates that our PTR activate models substantially refine their responses across multiple iterations in the majority of tasks. For instance, in the Qwen2-7B model, accuracy on MMLU increased by 7.0%, from 57.1% (Base model Prompt Iteration 1) to 64.1% (PTR Iteration 3). On several additional tasks, PTR also showed improvements, with the average score across all tasks increasing by 3.9%-rising from 49.6% to 53.5%. However, the Prompting method results show that both two base models degrade in performance when asked to refine, producing worse answers compared to initial responses. These results indicate that PTR effectively enables base models to improve based on previous thoughts.\nPTR vs. Knowledge Distillation (IFT) We also compare our PTR with Knowledge Distillation (IFT). We find that PTR is not equivalent to knowledge distillation. At the first iteration, we observe that when models are trained on general datasets rather than domain-specific tasks, its initial performance tends to decline at first. We found that this performance drop largely stems from supervised fine-tuning amplifying the initial biases of the base model. When trained on general datasets, the base models tend to accumulate biases that may not apply to specific domains, leading to poorer performance on domain-specific tasks. For example, in ARC tasks, accuracy drops from 60.6% (Base model Iteration 1) to 54.9% (IFT Iteration 1). This demonstrates that our datasets are clean and that knowledge distillation alone cannot improve accuracy in specific domains. Instead of correcting these biases at first response, we focus on correcting them through iterative refinement. However, the IFT approach fails to activate the model's progressive refinement ability and does not significantly increase the performance after the first attempts. On some reasoning benchmarks, such as CommonsenceQA, The IFT approach does not perform a better response at the second iteration (40.3%) compared to their first attempt (46.1%). In contrast, PTR approach improves through iterative attempts without an approach on domain-specific knowledge. This suggests that our method is not simply distilling knowledge but effectively activating the model to refine outputs and enhance performance through self-driven iterative improvement.\nRefinement beyond Correction Deeper analysis reveals that in open-ended tasks without clear ground truth, LLMs refine responses to be more thoughtful and comprehensive, regardless of correctness. For example, in the code task shown in Figure 2, the LLM iteratively improves its response over three iterations, considering additional aspects of the problem. This highlights PTR's ability to enhance not just correctness but also the quality and usability of outputs (Shown in Appendix D)."}, {"title": "DOES OUR METHOD DEMONSTRATE GENERALIZATION?", "content": "PTR vs. Other Progressive Refinement methods Unlike previous approaches, our method activates the model's inherent progressive refinement ability rather than merely boosting accuracy in specific domains. To validate PTR's generalization capability, we use datasets with general queries and evaluate whether the model can iteratively refine responses across various tasks. As seen in Table 1, our model refines responses across multiple iterations, significantly improving accuracy across tasks, and demonstrating effective generalization. We also compare PTR with other progressive refinement methods like RL to assess generalization. Our results show that methods like RL, when fine-tuned only on general-domain tasks, fails to activate iterative refinement in specialized tasks, often showing decreased accuracy. This suggests that our method is more robust in diverse environments, as it enables the model to iteratively refine its responses without being limited to domain-specific fine-tuning. By leveraging the model's inherent progressive refinement capabilities, PTR achieves consistent improvements across a wide range of tasks."}, {"title": "IS OUR METHOD ROBUST ACROSS DIFFERENT LLMS AND INSTRUCTIONS?", "content": "Prompt Robustness We also evaluated PTR robustness with different prompts and LLMs. Table 2 shows the model's performance using three different prompts across various tasks, refined over four iterations. Across all prompts, we find that PTR achieves iterative improvement across different prompts. Specifically, In the math (GSM8K) tasks, PTR is well-performed(78.1%) compared with initial responses (75.1%). On reasoning tasks (ARC), PTR see substantial improvements, especially with Prompts 1 (62.8%) and Prompts 3 (63.2%). DROP tasks also improve steadily, with accuracy increasing to 22.5% by Iteration 3 in Prompt 2. Our approach enables the model to learn from previous thoughts, rather than relying on the instruction used during training. This PTR enables the model to consistently improve its performance on different prompts, demonstrating the robustness of the PTR mechanism.\nLLMS Robustness The table 1 also demonstrates that both Llama3-8B and Qwen2-7B exhibit robustness across different prompts and tasks. While Llama3-8B often outperforms Qwen2-7B, both models show consistent improvements with iterative refinement. This robustness ensures that PTR can be applied effectively to a wide variety of open-source LLMs."}, {"title": "DOES PROGRESSIVE REFINEMENT ABILITY EXHIBIT EMERGENCE DURING TRAINING?", "content": "Overall Performance Figures (A) and (B) show a clear upward trend in performance, as shown in Figure 3 shown. Notably, after 24,000 training steps (equivalent to 93 million tokens), significant improvements indicate the emergence of inference capabilities. As training continues, we observe that the average performance of PTR increases from 40.1% to 55.6%, showing an overall improvement across different tasks.\nTask Complexity and Learning Curve We also find that tasks of varying difficulty exhibit different emergence timings and improvement rates. Plots (C) and (D) reveal that simpler tasks such as MMLU and DROP show early and steady improvements around 22,000 steps. More complex inference tasks such as ARC and GPQA exhibit delayed emergence, with ARC improving from 36.3% to 65.2% and GPQA from 23.2% to 25.6% after 24,000 steps. This shows that as training continues, the model's ability to handle complex reasoning and other tasks significantly improves, showing clear emergent behavior in different task types."}, {"title": "HOW MANY THINKING STEPS ARE REQUIRED TO ACHIEVE OPTIMAL PERFORMANCE?", "content": "We investigate how iterative thinking steps influence performance across tasks by conducting experiments over ten iterations using the Qwen2-8B model. Figure 4 illustrates performance trends.\nImprovements in the First Three Iterations In the first three iterations, we saw significant improvements in model performance. In the mathematical reasoning task GSM8K, the accuracy improved from 75.0% in the first iteration to 79.9% in the second iteration. Similarly, the ARC dataset improves from 58.6% to 65.2% in the third iteration. This shows that PTR quickly refines its problem-solving through progressive refinement.\nAfter the third iteration, the performance improvements for most tasks stabilize. In GSM8K, the accuracy fluctuates slightly between the third and tenth iterations, ranging from 79.9% to 80.1%. In MATH, the accuracy remains around 50.2% to 50.6% after reaching a peak in the second iteration. This indicates that the marginal gains decrease over time, indicating that the performance ceiling of the model is converging.\nSustained Performance Without Overfitting PTR consistently improves across iterations without signs of overfitting. Performance remains stable or improves slightly, with no notable declines. For instance, in DROP and XSum, accuracy increases from 19.0% and 45.9% to 21.6% and 49.7%, respectively, over ten iterations.\nMore Computation for Hard Tasks Complex tasks benefit more from iterative thinking and may require additional iterations for optimal performance. Accuracy in CommonsenseQA improves from 47.9% to 58.6% by the eighth iteration, suggesting that tasks with higher cognitive demands allow PTR to leverage iterative refinement more effectively. While GSM8K reaches near-optimal performance within a few iterations, tasks like MATH require more computation to achieve substantial gains, likely due to the challenging nature of logical reasoning involved."}, {"title": "CONCLUSION", "content": "We propose PTR, an approach designed to stimulate the progressive thought refinement capabilities inherent in LLMs, allowing them to improve their responses through multiple rounds of iterations. PTR adopts an annotation-free strategy to gradually build refined thoughts and answers through a weak and strong models collaborative selection process, and combines thought-answer consistency filtering to ensure logical coherence. Our weighted thought mask fine-tuning further activates the model's internal refinement ability by learning the improvement from initial thoughts to refined answers. Experimental results show that PTR simply trained with general open-domain datasets, but significantly improves the model's progressive refinement capabilities in ten different tasks, including knowledge reasoning, code generation, and mathematical reasoning, achieving a generalization level not observed by previous methods."}, {"title": "METHOD", "content": "A.1 SELF-CONSISTENCY FILTERING\nIn each iteration of thought generation, we apply multiple sampling techniques to generate several candidate thoughts. These candidate thoughts undergo a consistency check against the final answer to ensure logical coherence throughout the thought process."}, {"title": "N-SAMPLING FOR THOUGHT GENERATION", "content": "For each query $q_i$, we perform n-sampling to generate $N$ candidate thoughts at each step of the thought generation process. These thoughts denoted as $\\hat{y}_{i,w}^t$, represent the m-th sample at the t-th attempt, and they collectively form the set of potential thought sequences."}, {"title": "CONSISTENCY FILTERING", "content": "To evaluate the consistency between the thought sequences and the final answer, we vectorize the thoughts and the answer using two complementary techniques:\n\u2022 Distinct N-grams: This method captures surface-level similarities by extracting unique N-grams from the thought sequences and the final answer. Higher distinct N-gram overlap between two texts suggests greater structural similarity.\n\u2022 Sentence-BERT embeddings: This technique captures semantic similarities by embedding both the thought sequences and the final answer into a shared vector space using Sentence-BERT (Reimers & Gurevych, 2019). This enables a deeper comparison of the meaning conveyed by the thoughts and the answer.\nThe similarity between each thought y; and the final answer $\\hat{y}_{i}$ is computed as a weighted combination of the N-gram similarity and the Sentence-BERT embedding similarity.\nTo refine the thought sequences, we apply a consistency score between each generated thought and the final answer using the Consistency Function $F_{cons} (Yj, \\hat{y}_{i})$, which measures the similarity between each thought and the final answer:\n$F_{cons}(Yj, \\hat{y}_{i}) = \\alpha_{1}\\cdot N-gram(yj, \\hat{y}_{i}) + \\alpha_{2} \\cdot BERT(yj, \\hat{y}_{i})$\nHere, $\\alpha_1$ and $\\alpha_2$ are weighting factors that balance the importance of N-gram similarity and sentence-BERT embeddings. This function calculates the overall similarity between a thought y; and the final answer $\\hat{y}_{i}$, with a higher value indicating better consistency.\nFormally, the consistency score for the entire thought sequence $S_{thought} = {y_{i,w}^{1}, y_{i,w}^{2}, ..., y_{i,w}^{t}}$ with respect to the final answer $\\hat{y}_{i}$ is computed as:\n$C(S_{thought}, \\hat{y}_{i}) = \\frac{1}{|S_{thought}|}\\sum_{j=1}^{|S_{thought}|} I (F_{cons}(yj, \\hat{y}_{i}) \\geq \\delta)$\nHere, $I(\\cdot)$ is an indicator function that returns 1 if the consistency score $F_{cons}(yj, \\hat{y}_{i}) \\geq \\delta$, and 0 otherwise. The overall score $C'(S_{thought}, \\hat{y}_{i})$ represents the average consistency between the thought sequence and the final answer."}, {"title": "TEMPERATURE ADJUSTMENT", "content": "This graph illustrates the performance of a model under different temperature settings during inference, measured over three iterations in terms of accuracy. The comparison includes four scenarios: fixed temperature at 0, the fixed temperature at 0.4, fixed temperature at 0.8, and gradually decaying temperature. The main findings are as follows: The graph clearly shows that setting the temperature to 0 yields the best performance. A temperature of 0 ensures that the model generates deterministic outputs at every step, leading to more reliable and stable results. Higher temperatures (such as 0.4 and 0.8) introduce randomness into the process, reducing overall accuracy. The decaying temperature approach improves accuracy over time but does not surpass the performance of a fixed temperature of 0."}, {"title": "EXPERIMENT", "content": "ITERATION RESULT"}, {"title": "WILCOXON SIGNED-RANK TEST", "content": "In this experiment, we analyzed samples across three dimensions: model parameter strength and model version (new vs. old), and domain-specific fine-tuning. Using the Wilcoxon signed-rank test, we assessed the differences in inference quality between the strong and weak models across these dimensions to verify whether the strong model provides significant improvements. We use human experts and Auto-j Li et al. (2023b) to judge the quality of the generated responses.\nTo visually present the score differences across the three dimensions, we plotted a distribution of inference score differences (see Figure 6). The box plot displays score differences in the following three dimensions:\n\u2022 Model Parameter Strength: Differences in inference quality between models with strong parameters and weak parameters."}, {"title": "SETTING DETAILS", "content": "Open-domain Datasets\n\u2022 WizardLM (Xu et al., 2023) is an instruction dataset built with the EVOL-INSTRUCT method. EVOL-INSTRUCT utilizes CHATGPT to augment the complexity of the same queries in Alpaca and ShareGPT. We denote these two subsets as WizardLM(Alpaca) and WizardLM(ShareGPT) for clarification."}, {"title": null, "content": "Data Filtering In this section, we provide details about the open-domain datasets used for query preparation. These datasets were chosen for their generalizability and diversity of content, ensuring the model is exposed to a wide range of topics and query types. Our selection process was guided by the following criteria:\n\u2022 Data Cleaning Pipeline: The cleaning process involved removing noise such as HTML tags, non-alphanumeric characters, and duplicate entries. We applied frequency-based filtering to exclude long-tail queries and low-frequency phrases that are unlikely to contribute to the model's refinement abilities.\n\u2022 Final Dataset Size: After applying all filtering and cleaning steps, the final dataset consisted of approximately 40k high-quality, open-domain query-answer pairs.\nEval Tasks Choice We deploy a benchmark to evaluate whether our approach can activate the model's progressive refinement capabilities, enabling it to think and iterate across various dimensions."}, {"title": "CASE STUDY", "content": "In this section, we present a comprehensive case study analysis of the output generated by various language models on several real-world tasks. Each case is evaluated based on the quality of responses, relevance to the question, and overall utility for the end-user by GPT-4. These cases were randomly selected from our evaluation results.\nWe have divided the analysis into six tasks: flight time estimation, web browser usage statistics, language comparison, REST API implementation, JavaScript list item addition, and calculating averages in Python. These real-world tasks are often challenging for humans to label accurately. However, we observed significant improvements from Iteration 1 to Iteration 4, with the most optimal performance appearing around Iterations 2 and 3."}, {"title": "TASK 1: FLIGHT TIME ESTIMATION", "content": "Question: How long does it take from Hangzhou to New York by air?\nThis task evaluates the quality of responses related to estimating flight times from Hangzhou to New York. The different outputs showcase varying levels of accuracy and relevance:"}, {"title": "Outputs", "content": "\u2022 Iteration 1 It depends on various factors such as the type of airplane and weather conditions. However, the estimated flight time from Hangzhou to New York typically ranges from 14 to 16 hours.\n\u2022 Iteration 2 The flight time from Hangzhou to New York is approximately 15 hours and 30 minutes. Major airlines operating on this route include China Eastern and Delta Airlines. Keep in mind that additional time for security checks, boarding, and customs at both airports should also be considered.\n\u2022 Iteration 3 The direct flight time from Hangzhou to New York is approximately 15 hours and 30 minutes. Airlines like China Eastern and Delta operate this route. Keep in mind that the total travel time may vary depending on factors such as layovers, flight delays, and weather conditions. Additionally, you should plan for extra time for security checks, boarding, and customs, which can add another 1 to 2 hours at each airport. It's advisable to check for updated flight schedules and potential stopover options if a direct flight is unavailable.\n\u2022 Iteration 4 The flight on December 2, 2021, from Hangzhou to New York took about 15 hours with a stopover in Tokyo. Depending on the route and the specific date, direct flights can be faster, but you should plan for various connection options."}, {"title": "Analysis", "content": "\u2022 Iteration 1 provides a general estimate but introduces unnecessary ambiguity by discussing different airplane types without focusing on direct flights.\n\u2022 Iteration 2 is the most accurate, offering a specific duration of 15 hours and 30 minutes, while also considering practical factors such as airport procedures and layovers.\n\u2022 Iteration 3 The improved answer not only provides a specific flight duration but also mentions the airlines that operate the route. It further addresses factors that might affect the travel time, such as delays and weather conditions, which enhances the answer's accuracy and reliability.\n\u2022 Iteration 4 includes complex and irrelevant details regarding connecting flights, which are not pertinent to the user's query."}]}