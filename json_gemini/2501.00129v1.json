{"title": "A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection", "authors": ["Julia Ive", "Paulina Bondaronek", "Vishal Yadav", "Daniel Santel", "Tracy Glauser", "Tina Cheng", "Jeffrey R. Strawn", "Greeshma Agasthya", "Jordan Tschida", "Sanghyun Choo", "Mayanka Chandrashekar", "Anuj J. Kapadia", "John Pestian"], "abstract": "Introduction Healthcare analytics and Artificial Intelligence (AI) hold transformative potential, yet Al models often inherit biases from their training data, which can exacerbate healthcare disparities, particularly among minority groups. While efforts have primarily targeted bias in structured data, mental health heavily depends on unstructured data like clinical notes, where bias and data sparsity introduce unique challenges. This study aims to detect and mitigate linguistic differences related to non-biological differences in the training data of Al models designed to assist in pediatric mental health screening.\nOur objectives are: (1) to assess the presence of bias by evaluating outcome parity across sex subgroups, (2) to identify bias sources through textual distribution analysis, and (3) to develop and evaluate a de-biasing method for mental health text data. Methods We examined classification parity across demographic groups, identifying biases through analysis of linguistic patterns in clinical notes. Using interpretability techniques, we assessed how gendered language influences model predictions. We then applied a data-centric de-biasing method focused on neutralizing biased terms and retaining only the salient clinical information. This methodology was tested on a model for automatic anxiety detection in pediatric patients a crucial application given the rise in youth anxiety post-COVID-19.\nResults Our findings show a systematic under-diagnosis of female adolescent patients, with a 4% lower accuracy and a 9% higher False Negative Rate (FNR) compared to male patients, likely due to disparities in information density and linguistic differences in patient notes. Notes for male patients were on average 500 words longer, and linguistic similarity metrics indicated distinct word distributions between genders. Implementing our de-biasing approach reduced this diagnostic bias by up to 27%, demonstrating the approach's effectiveness in enhancing equity across demographic groups\nDiscussion We developed and evaluated a data-centric de-biasing framework to address gender-based content disparities within clinical text, specifically in pediatric anxiety detection. By neutralizing biased language and enhancing focus on clinically essential information, our approach highlights an effective strategy for mitigating bias in Al healthcare models trained on unstructured data. This work emphasizes the importance of developing bias mitigation techniques tailored for healthcare text, advancing equitable Al-driven solutions in mental health.", "sections": [{"title": "Introduction", "content": "The global pandemic has acted as a catalyst and highlighted the changes required in health and social care systems to ensure the ongoing well-being of the population, with an emphasis on mental health. This is especially true for children and adolescents with the prevalence of anxiety and depression symptoms doubled during the pandemic\u00b9. These increases, particularly in older and female adolescents, highlight an urgent need for mental health help and equitable early detection efforts to mitigate the long-term impact\u00b2. Even without the additional load, comprehensive screening for pediatric mental health concerns is challenging\u00b3. AI constitutes a promising solution for expanding mental health diagnostics\u2074.\nHowever, in spite of the promise of AI to assist in mental health, the development and deployment of machine learning models into real clinical environments remains limited\u2074,\u2075. One of the potential reasons is the risk of propagating harmful biases. Additionally, clinicians often have concerns about the interpretability of predictions from \u201cblack box\" models obscuring model suggestions.\nOne crucial aspect that influences the development and implementation of building trustworthy AI models in clinical setting is the availability of high-quality data in sufficient volumes. This is particularly challenging in mental health care the primary source of information in mental health care is free text (clinical notes) containing highly sensitive information. However, the available data are often sparse and biased because their selection process reflects an underlying unfairness within the healthcare system (known as selection bias) (see Figure 1). Unfortunately, AI models are prone to exaggerating this bias in a self-reinforcing cycle: when trained on data that reflect historical inequalities, these models tend to perform worse for marginalized groups, and leading to inequitable in treatment outcomes, and amplifying existing inequalities\u00b2,\u2077. A recent study by Yates Coley et al.\u2078 found poor performance in predicting the risk of suicide for underrepresented demographic groups, including Black, Native American, and Alaskan patients.\nAnnotation bias is caused by limitations in annotation, i.e., the difficulty of hiring a sufficient number of annotators or those with relevant expertise and the different views and perspectives that caregivers may have. When dealing with text, bias can also be introduced through the language itself (linguistic or textual bias). This bias stems from differences in word usage in texts describing different population subgroups. For example, the word \u201cfootball\u201d may occur more frequently in texts describing males, and the word \u201ccheerleading\u201d may be more often used in association with females. Finally, bias can occur due to over-amplification when AI models learn a spurious correlation between the patterns (words) found in the data and the labels instead of relying on relevant information. For example, in the medical domain, chat bots have exhibited subtle sex and race bias when prescribing pain medications\u00b9\u2070 or associating women with cooking in semantic role labeling\u00b9\u00b9.\nThere has been extensive research in the assessment of bias in AI algorithms in machine learning (ML)\u00b9\u00b2\u207b\u00b9\u2075 and natural language processing (NLP)\u00b9\u2076\u207b\u00b9\u2078 as part of broader research on fairness (equitable and non-discriminatory outcomes). However, these general techniques are only beginning to be applied in fields like medicine, and potential remains largely unexplored in the area of mental health\u00b2.\nTraditionally, bias mitigation techniques in ML are broadly divided into: pre-processing techniques, learning algorithm modifications, and post-processing techniques. Pre-processing techniques includes approaches like down-weighting the biased instances during training to discourage the model from exaggerating related effects\u00b9\u2079,\u00b2\u2070, , as well as resampling methods such as under-sampling or oversampling to balance classes in the data, and data augmentation, which synthesizes samples for underrepresented groups\u00b2\u00b9,\u00b2\u00b2. There are also some de-biasing techniques specifically developed in the Natural Language Processing (NLP) domain, such as attribute swapping, which creates new text examples by swapping words indicating sensitive attributes in sentences\u00b2\u00b3.\nLearning algorithm modifications include approaches like adversarial de-biasing which uses adversarial networks to penalize predictions influenced by protected attributes\u00b2\u2074,\u00b2\u2075. Other approaches also include modifying objective functions to promote fairness during learning (e.g., by penalizing performance differences across demographic groups\u00b2\u2076,\u00b2\u2077). Recently, self-supervised pre-trained AI models have shown promise in reducing performance disparities across demographic groups\u00b2\u2078.\nPost-processing techniques include approaches like calibration, which aligns predicted probabilities with actual observed distributions\u00b2\u2079. Popular post-processing approaches transform linguistic embeddings by removing their projections onto the semantic subspace of the demographic aspect\u00b9\u2076.\nAs these methods are applied in the healthcare field, including mental health, several important factors must be taken into account. In healthcare, observed differences between demographic groups can generally be characterized as follows\u00b2: (a) genuine differences based on biological influences on disease risk (e.g., presence of hormones contributing to breast or prostate cancer), which should be preserved; (b) disparities shaped by non-biological differences (e.g., variations in clinical visit frequency and the writing style of clinical notes resulting from visiting different experts), which should be when possible reduced; and (c) false differences caused by flawed measurement, such as over-diagnosis in certain groups due to misperceptions (e.g., female depression\u00b3\u2070). These errors are primarily annotation flaws that need correction.\nThis hierarchy of disparities, combined with data sparsity, makes many existing de-biasing methods unsuitable for healthcare. For instance, techniques like swapping gender words between sentences may introduce unrealistic symptom profiles, while removing gender components from word embeddings can distort the meaning of medical terms. Similarly, modifying learning algorithms can lead to overly complex models and poor results when training data is limited.\nAdditionally, healthcare data is highly heterogeneous, as clinical records often come from various care sites. This heterogeneity remains largely unaddressed by current de-biasing techniques, which typically focus on data from a single source.\nIn this work, we focus on identifying and mitigating textual bias in demographic subgroups (focusing on sex) found in the text of electronic healthcare records (EHR) in the context of pediatric anxiety\u2014an important issue given the post-pandemic rise in anxiety symptoms among underrepresented groups, particularly females\u00b9.\nIn the challenging pediatric primary care setting (multiple informants, variety in interpretation of risk factors and symptoms, overlapping symptoms)\u00b3,\u00b3\u00b9,\u00b3\u00b2, AI has the potential to support the healthcare in this challenging setting and under growing pressures. But only if it can offer consistent support across demographic subgroups.\nThis study aims to detect and mitigate disparities in the textual training data of AI models intended to assist in pediatric mental health screening. These disparities are caused by biological differences and differences in social circumstances (male and female patients with different symptoms were treated in different healthcare sites with different reporting practices) across sex groups and lead to biased predictions.\nTo address this, we first assess the presence of bias by evaluating outcome parity across sex subgroups in an Al model trained to predict pediatric anxiety. Observing disparities in model performance indicates bias, which may disproportionately impact underrepresented groups.\nNext, we identify sources of bias by examining how linguistic and statistical properties in clinical text contribute to unequal outcomes. Using interpretability techniques, we analyze the impact of gender-related language (e.g., first and last names, gender pronouns) on model predictions to trace the influence of potentially biased terms.\nFinally, we develop and evaluate a data-centric de-biasing method specifically adapted for mental health text. This method includes normalizing information density to reduce bias and replacing biased words with neutral alternatives. Our approach complements general de-biasing techniques and contributes to best practices in the field, particularly as Large Language Models (LLMs) continue to grow in prominence."}, {"title": "Methods", "content": "Our data come from one of the highest-ranked pediatric institutions in US. We apply a range of state-of-the-art NLP methods to these data."}, {"title": "Datasets", "content": "As part of this study, we created a foundational database consisting of EHR data from the Cincinnati Children's Hospital Medical Center Epic Link. This database has approximately 1.3 million unique patients seen at CCHMC between January 1, 2009, and March 31, 2022, with 63 million clinical notes.\nWe define anxiety patients as any patients who have ever received any of the diagnostic codes listed in Appendix Table 7. Additional selection criteria are that the patient must have had at least one encounter in the EHR in the 18 months prior to the anxiety diagnosis. As a result, there were 1,383,145 total patients in the CCHMC EHR, 84,426 total anxiety cases that passed our selection criteria, 77,187 total anxiety cases with at least 1 note, 73,288 total anxiety cases with at least 1 note >30 days before their first anxiety diagnosis, and 7,810,849 notes for these 73k patients.\nThis cohort contains demographic data (sex and race) and the timeline of clinical textual notes provided by different care providers (progress notes, telephone encounters, plan of care notes, patient instructions, etc.).\nWhen we apply our additional selection criteria on note types (we select Progress Notes and Telephone Encounters as deemed the most informative by internal experts), this is reduced further to 4.3 million notes."}, {"title": "Age binning", "content": "The patients were grouped into age ranges by age (in years) at the time of their anxiety diagnosis. For example, the 5-year-old age group consists of patients who received an anxiety diagnosis between their fifth and sixth birthdays. The controls were matched one-to-one with cases by age and sex. Matched controls 1) were born within 30 days of the case and 2) were of the same sex. Additional criteria for controls were that they had never received one of the anxiety diagnoses at the time of the matched case's anxiety diagnosis and that they had had at least one encounter in the EHR within the 18-month window prior to the case's anxiety diagnosis. Only data prior to the first anxiety diagnosis are used for our analysis.\nWe used Bins of age 5, 8, 10, 12 and 15 (see the Age Binning subsection below). This selection of bins is not common for pediatrics but gives us a comprehensive selection of datasets with diverse percentages of female patients varying from 36% to 69% (see section Descriptive Analysis in Results)"}, {"title": "Data Cleaning", "content": "To remove duplicates in progress notes and telephone encounters, we first tokenize our notes by removing punctuation and stopwords using the NLTK toolkit\u00b3\u00b3. We then vectorized the notes using the CountVectoriser from the Scikit-learn toolkit\u00b3\u2074. Finally, duplicate notes with cosine similarity \u2265 0.8 were removed. Finally, we selected the 25 most recent notes in each patient's history (average minimum count of notes per patient across the bins). We kept the timelines with at least one record and maintained the 1:1 proportion of cases and controls.\nEach final bin contained the training set of ~3,700-5,064 cases and controls, and the testing set included ~852\u20131,278 cases and controls. This study was approved by the Institutional Review Board of Cincinnati Children's Hospital as STUDY2020-0942."}, {"title": "Anxiety Prediction Models", "content": "We built our anxiety prediction models by fine-tuning the state-of-the-art Transformer-based Clinical-BigBird model\u00b3\u2075 as imported from HuggingFace\u00b3\u2076. This model is not only pre-trained on clinical text but can also handle long input sequences (up to 4,096 tokens).\nWe followed the best practices in the domain and fine-tuned the model for 2 epochs. We limited the input length of notes to 1,000 tokens (since considering longer inputs did not result in further improvement). We used AdamW optimizer\u00b3\u2077 with a learning rate of le-5 and a batch size of 8 (these were the best-performing set of parameters out of the sets suggested for fine-tuning by the model authors\u00b3\u2075)."}, {"title": "Explainability", "content": "In addition to measuring the performance of our models with the standard accuracy measure, we perform qualitative analysis of words our models rely on while making predictions. The Local Interpretable Model-Agnostic Explanations (LIME) technique \u00b3\u2078 enables this qualitative analysis by offering local explanations and pinpointing specific words that significantly influenced the model decisions. These explanations are \u201clocal\u201d because they relate to the model's behavior for each specific incoming note. Note that globally important features (for example, weights that a model assigns to words in its vocabulary) might not be precise enough for the local prediction context.\nIn our study, we used the LIME methodology to highlight influential words in order to verify if our prediction models are functioning correctly. LIME is designed to uncover undesirable behaviors in AI models that might seem efficient based on standard metrics. For example, Ribeiro et al.\u00b3\u2078 showed that a model, despite a 94% accuracy in differentiating documents on Atheism versus Christianity, relied on irrelevant words such as \"posting\u201d, \u201chost\u201d and \u201cre\u201d. These words were wrongly associated with Atheism due to their frequent appearance in the training data."}, {"title": "Text De-biasing Methods", "content": "Motivated by our observations over the textual distributions across demographic subgroups, we propose two following bias mitigation methods:\n1. Information density filtering (tf-idf_filt): we perform normalization of content which involves filtering sentences from concatenated notes using their importance scores. Those importance scores are computed as averaged sum of word-level TF-IDF scores per sentence. TF-IDF scores help identify most salient words in a document by multiplying the frequency of the word in that document by its rarity across all documents (the more rare is the word the higher is the score) so that document keywords (e.g., term \"myopia\u201d) that may appear multiple times in one document but not in the others receive higher score than auxiliary words such as articles (\"the\", \"a\") or verbs (\u201cam\", \"have\").\n2. Gender-word debiasing (gen_sub): Following the best de-biasing practices from the NLP domain\u00b3\u2079, we focus on names and pronouns as gender-biased attributes in text.\nWe automatically detect those biased words and replace them with relevant neutral versions. In particular, we detect proper nouns (first and last names) using the off-shelf Stanza tool\u2074\u2070. We extract unique names and group them based on the character similarity, ensuring that variations of names are considered equivalent (for example, \u201cJonathan\u201d, \u201cJohnathan\" and \"Johnatan\"). This grouping facilitates the creation of a mapping system where each name group is replaced by a generic identifier, such as \u201cperson1\u201d, \u201cperson2\", etc. (enumeration is maintained per note).\nFollowing the name replacement, the text undergoes pronoun substitution, where gender-specific pronouns from a dictionary (for example, \"she\u201d and \u201che\u201d) are replaced with their gender-neutral counterparts using a predefined pronoun mapping (for example, \"she\u201d and \u201che\u201d are replaced with \"they\"). Note that this approach maintains the integrity and coherence of the original text while achieving certain gender neutrality.\nThose methods could be used separately or combined. For example, substitution of biased words could be applied to the original text or after the removal of the least informative sentences.\nAll models used in this study were downloaded and installed locally. All experiments were performed on NVIDIA A100-SXM4-40GB GPUs.\""}, {"title": "Results", "content": "We elaborate on our findings per each research objective below."}, {"title": "Objective 1: Assessing the Presence of Bias by Evaluating Outcome Parity Across Sex Subgroups", "content": "Best practices in Machine Learning\u00b9\u2074,\u00b9\u2075 measure model bias in terms of classification parity (equality of classification error between groups), anti-classification (effect of protected attributes on predictions) and calibration (difference between predicted risk and factual risk).\nIn this work with predictive models, we focus on classification parity. Several metrics are commonly used in the fairness literature to evaluate classification parity. These are accuracy equality (equal accuracy between groups), equal opportunity (equal false negative rate (FNR), orientation towards recall), or predictive equality (equal false positive rate (FPR) between groups, orientation towards precision)\u2074\u00b9,\u2074\u00b2. Following Feldman et al., we also use the balanced error rate (BER), which is the unweighted average of FPR (precision) and FNR (recall):\n$\\mathrm{BER} = \\frac{\\frac{FP}{FP+TN}+\\frac{FN}{FN+TP}}{2}$"}, {"title": "Objective 2: Identifying Bias Sources Through Textual Distribution Analysis", "content": "To understand the reasons for performance differences, we analyzed primarily the sex subgroups of the dataset and the properties of the relevant texts. In particular, we considered the following characteristics of textual distributions commonly applied in the NLP domain:\n1. Average length of a patient note in words (without tokenization, i.e., separation of punctuation). We measured the concatenation of all the valid notes per patient.\n2. Percentage of medical terms, in each note on average. We extract biomedical named entities using the state-of-the-art Stanza tool\u2074\u2074. Those in-domain entities are extracted for ten standard categories such as Observation, Treatment, Anatomy, Procedure, etc.\n3. Percentage of gender-biased words, in each patient note on average. Using the best practices from NLP\u00b3\u2079, we extract all the proper nouns (first and last names) using the off-shelf tool\u2074\u2070, as well as gender pronouns (he, she, his, her, him, hers). We focus on this semantic group as the most relevant to our cohort design involving matching by age and sex.\n4. Jaccard distance measures the similarity between two vocabularies by estimating the portion of common words. It is computed by dividing the number of words shared by both vocabularies by the total number of words in both vocabularies combined. We mainly compare vocabularies for male/female.\n5. Familiarity score assesses the ratio of common words to unique words. It is computed dividing the proportion of words which occur in both sets of words by the uniqueness ratio, the ratio of words that occur only in one of the sets. Again we mainly compare vocabularies male/female."}, {"title": "Objective 3: Developing and Evaluating a De-biasing Method for Mental Health Text Data", "content": "We have already seen that there are considerable differences in information density and textual distributions of notes for sex subgroups, in particular, those differences naturally involve gender words which our models can erroneously rely on. Our core hypothesis is that, by eliminating less relevant sentences (tf-idf_filt) we can balance density in the notes and reduce bias in our models. Regarding gender words, we have already seen that male and female notes contain similar percentages of such words. Hence we have developed an approach to neutralize those words via substitution with their gender-neural versions rather than their removal (gen_sub, see subsection Text De-biasing Methods in Methodology).\nWe applied our de-biasing methods as described in the Methodology section above to modify our training data. In particular, we applied the rnd_filt method and removing 20% of sentences at random as our baseline. We compared it to the performance of our approach tf-idf_filt which removes 20% sentences according to their informativeness score (threshold defined empirically). We applied the gen_sub method on the original notes, as well as on the notes filtered with tf-idf_filt to trace the effect of both models combined. We compare our approach to the baseline approach where we remove 20% of sentences chosen at random (rnd_filt).\nEach time we modified the training data, we re-trained our models to obtain new models, which were then tested on the original test data.\nOur results show that the de-biased models in general maintain the performance of the original models (Table 6): both gen_sub and rnd_filt slightly increase the performance by +0.5 accuracy, while tf-idf_filt decreases the perfor-mance on average by -1 accuracy. The mixed method tf-idf_filt+gen_sub maintains the original performance with negligible changes. While maintaining performance, our de-biasing methods exhibit positive influence on the performance by re-ducing the percentage of uncertain predictions by -8% on average with the highest reduction for the tf-idf_filt+gen_sub of -12%.\nIn terms of the reduction of bias as measured by FNR, tf-idf_filt is a clear winner (see Table 1). It outperforms the random sentence removal baseline rnd_filt, reducing the FNR gap by -0.024 (27%, initial average gap 0.09) point on average with the highest reduction by -0.11 point for bin 5 (from 0.13 to 0.02) and -0.06 point for bin 15 (from 0.13 to 0.07). rnd_filt baseline does not exhibit any consistent behavior and does not influence the gap across bins. Our gen_sub approach when applied alone is not efficient to reduce the FNR gap as well. On average we even observe a small increase in this gap rather than a decrease (+0.008). tf-idf_filt+gen_sub has roughly the same performance as tf-idf_filt with the average decrease of -0.022 point. Both tf-idf_filt and tf-idf_filt+gen_sub approaches manage to maintain the BER ratios within the acceptable level, and even decrease the BER ratio for Bin 10 from 1.33 to the acceptable values of 1.24 and 0.98 for tf-idf_filt and tf-idf_filt+gen_sub respectively. tf-idf_filt reduces the increase in uncertain predictions for females by 50%, while tf-idf_filt+gen_sub fully eliminates this dis-balance.\ntf-idf_filt+gen_sub has also capacity to mitigate bias for race subgroups reducing the FNR gap by -0.034 point on average. For bin 10, we also reduce the bias towards the non-privileged class (from BER ratio 0.74 to the acceptable level of 0.95).\nWe also observe a positive effect of our de-biasing methods in terms of the words our classifiers rely on (anti-classification, see Table 3)). Our first observation is that both tf-idf_filt and tf-idf_filt+gen_sub improve the generalizability of our models. We observe less terms appear in the statistics signifying more reliance on context (e.g., words \"complaint\", \"presents\", \"no", "anxiety\", \"depression\", etc.). Furthermore, our de-biasing techniques reduce the percentage of biased words our models rely on": "for example, tf-idf_filt reduces this percentage from 10% to 3% for cases, from 4% to 2% for controls. tf-idf_filt+gen_sub further reduces twice the frequency of biased words tf-idf_filt relies on. This is a positive effect since those residual biased associations become less systematic as compared to tf-idf_filt."}, {"title": "Discussion", "content": "This study aimed to detect and mitigate linguistic bias in training data of AI models intended to assist in pediatric mental health screening, focusing on sex bias. First, we found measurable disparities in AI model performance across sex subgroups, highlighting predictive bias that disproportionately affects females. This was evident in the lower classification parity (4% lower accuracy for females than for males across age groups) and higher false negative rates for female patients (9% higher on average across age groups), suggesting that the model was less accurate in diagnosing anxiety in this subgroup. Second, we identified intrinsic differences in textual properties between male and female patient notes, such as variations in note length (notes for males are 500 words longer), word distribution (low similarities for male/female word distributions of 0.54 Jaccard index, whereas values above 0.7 are considered indicative), and information density (low similarities for male/female term distributions of 0.34 Jaccard index). These differences are likely linked to reporting practices and medical documentation styles, which contribute to biased outcomes in AI predictions. Third, our data-centric approach to mitigate this bias using information density filtering and gender-neutral word substitutions improved classification parity by up to 27%, particularly benefiting the non-privileged subgroup (females).\nThis study supports previous findings that AI models trained on clinical data can perpetuate biases present in the original data, disproportionately affecting underrepresented groups, e.g., the study of Obermeyer et al. has found that commercial prediction algorithms used in healthcare to identify patients with complex needs exhibit significant racial bias, as they predict healthcare costs instead of illness severity, resulting in Black patients being under-identified for additional care despite having more severe health conditions\u2074\u2075. Similarly, our findings align with prior research indicating that linguistic patterns, such as gendered language, contribute to bias in natural language processing models used in healthcare\u00b9\u2078,\u2074\u2076.\nWhile previous studies have shown that AI models can produce biased outcomes across demographic groups, addressing bias in healthcare data presents unique challenges. In healthcare, it is important to retain biological differences in the training data that reflect actual patient needs, while mitigating biases that arise from non-biological factors, such as cultural or provider-based documentation differences. Traditional NLP methods, such as swapping gendered words\u00b2\u00b3 or removing gendered meanings from word representations\u00b9\u2076, ,are not suitable for this purpose in healthcare, as they could lead to inaccurate data representations. Moreover, healthcare notes vary widely across providers, introducing additional complexity. Our method addresses these challenges by selectively de-biasing data: it maintains information relevant to clinical care while reducing the influence of biased language and normalizing information density across records.\nThis study has several strengths. First, this study focuses on a data-centric approach, emphasizing the quality and relevance of data rather than improving algorithms (model-centric AI)\u2074\u2077,\u2074\u2078. Second, the de-biasing methodology developed here is specifically adapted for heterogeneous healthcare text data from different clinical sites. This tailored approach is particularly effective in pediatric mental health, where reliable and equitable early detection is critical. This approach not only clarifies how specific language influences model predictions but also demonstrates practical effectiveness: bias mitigation techniques, such as word substitution and information density balancing, reduced diagnostic bias by up to one-third for systematically under-diagnosed female patients. Third, this study creates a pathway to further exploration of complimentary de-biasing techniques specific to AI in the mental health domain.\nThe study has some limitations. The focus of the study relies on the quality and consistency of the EHR text. Variability in note quality between providers may influence the model's ability to generalize across different clinical settings, which is a common challenge in EHR research. Also, the study focuses on only one type of demographic bias within a male/female pediatric population."}, {"title": "Conclusion", "content": "Anxiety disorders are a leading cause of disability in children and adolescents worldwide, with rising rates among minority groups. AI can play a transformative role in early mental health detection, but its success depends on reliable, unbiased data.\nThis study presents a data-centric de-biasing approach designed to address disparities in AI model performance in clinical text, especially among under-diagnosed groups like female patients. By balancing information density and neutralizing biased terms, our approach reduced diagnostic bias by up to one-third. These findings underscore the importance of bias-aware data processing to create fair and effective AI tools in mental health."}, {"title": "Code and Data Availability", "content": "The code and data could not be publicly shared due to their confidentiality requirement. The code was implemented in Python 3.10."}]}