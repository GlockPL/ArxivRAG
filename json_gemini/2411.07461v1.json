{"title": "BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions", "authors": ["Anas Awadalla", "Le Xue", "Manli Shu", "An Yan", "Jun Wang", "Senthil Purushwalkam", "Sheng Shen", "Hannah Lee", "Oscar Lo", "Jae Sung Park", "Etash Guha", "Silvio Savarese", "Ludwig Schmidt", "Yejin Choi", "Caiming Xiong", "Ran Xu"], "abstract": "We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that bridges the gap\nbetween descriptive synthetic captions and factual web-scale alt-text. KALE augments synthetic\ndense image captions with web-scale alt-text to generate factually grounded image captions.\nOur two-stage approach leverages large vision-language models and language models to create\nknowledge-augmented captions, which are then used to train a specialized VLM for scaling up\nthe dataset. We train vision-language models on KALE and demonstrate improvements on\nvision-language tasks. Our experiments show the utility of KALE for training more capable and\nknowledgeable multimodal models. We release the KALE dataset at https://huggingface.co/\ndatasets/Salesforce/blip3-kale.", "sections": [{"title": "1 Introduction", "content": "We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that advances the state of\nknowledge-augmented image captioning. KALE builds upon recent work in this area, particularly\nCapsFusion [28], which pioneered the use of large language models to fuse synthetically generated\ncaptions with alt-text to incorporate real-world knowledge. KALE makes two key contributions\nbeyond CapsFusion:"}, {"title": "2 Approach", "content": ""}, {"title": "2.1 Stage 1: Generating initial knowledge-augmented captions", "content": "The first stage of our approach focuses on creating an initial pool of knowledge-augmented dense\ncaptions. We begin by leveraging CogVLM-17B [25] to generate dense captions for images from the\nDatacomp-1B dataset. These captions serve as a foundation for our knowledge augmentation process.\nTo enhance these captions with real-world knowledge, we employ Mistral, a powerful language\nmodel. We prompt Mistral using the CogVLM-generated captions, instructing it to augment the\ndescriptions with relevant factual information. This step aims to incorporate broader contextual\nknowledge into the image descriptions, following CapsFusions' prompting method. Through this\nprocess, we create an initial pool of 100 million knowledge-augmented captions."}, {"title": "2.2 Stage 2: Scaling up", "content": "The second stage of our approach focuses on scaling up the dataset to achieve our target of 218 million\nimage-text pairs. We accomplish this by training a specialized VLM using the knowledge-augmented\ncaptions generated in Stage 1. We construct our VLM similar to the LLaVA [16] model; using\nQwen1.5-1.5B [27] for the language model and DFN ViT-H [6] for the vision encoder. We resize the\npositional embeddings for the vision encoder to handle 490x490 images matching the resolution used\nby CogVLM. Our VLM takes two inputs: image patch embeddings and the original Datacomp-1B\ncaptions. The model is trained to output the knowledge-augmented captions produced in Stage 1.\nWe use this VLM to caption an additional 118 million images from the Datacomp-1B dataset. This\ntwo-stage approach enables us to efficiently scale up KALE to 218 million image-text pairs."}, {"title": "2.3 Removing pipeline artifacts", "content": "Artifacts from the prompts passed in to LLM-\ns/VLMs to generate KALE occasionally leak\ninto the generated captions. We present an ex-\nample of these artifacts in Figure 3 where the\nsystem prompt to the LLM used in the rewriting\nstage has leaked into the generated caption. To\nremove these artifacts we create a set of words\nthat commonly appear in these artifacts such as\n'real-world' or 'sentence structure' and remove\nsentences that contain these keywords."}, {"title": "3 Experiments", "content": "We validate the effectiveness of KALE by train-\ning VLMs. In this section we outline our training and evaluation setup and present results when\ntraining on KALE."}, {"title": "3.1 Training setup", "content": "We follow the Llava architecture in using a linear layer to project the image patch embeddings from\na vision encoder into the text embeddings space. We use Qwen2.5-1.5B [27] for the language model,\nand SigLIP ViT-L 384 [29] for the vision encoder. We use a batch size of 80 image-text pairs and\na peak learning rate of 5e\u00af5. We train all of our models on two million samples from image-text\ndata. We then fine-tune the model, using a peak learning rate of 3e\u00af5, on one million multimodal\ninstruction tuning samples from the Cauldron [11] dataset. We remove multi-image samples from\nthe Cauldron data and sample different subsets according to the ratios used in Idefics2."}, {"title": "3.2 Evaluation setup", "content": "We evaluate the instruction-tuned model on various vision-language benchmarks including TextVQA\n(val set) [24], VQAv2 (val lite) [1], ScienceQA [19], AI2D [8], MMBench [17], ChartQA [20],\nInfoVQA [21], OCRBench [18], RealWorldQA\u00b9, and MMStar [4] using the Imms-eval framework [30].\nThis comprehensive evaluation suite covers a wide range of capabilities from general visual question\nanswering to specialized tasks involving scientific reasoning and OCR-based comprehension."}, {"title": "3.3 Results", "content": "We find that pre-training on KALE captions improve downstream model performance on most\nVLM benchmarks, achieving the highest average performance at 51.96%. In particular, KALE\nshows strong performance on TextVQA (59.92%), VQAv2 (70.10%), and ScienceQA (72.68%).\nCogVLM's synthetic captions also demonstrate robust performance. Both KALE and CogVLM\nsignificantly outperform Datacomp-1B's noisier alt-text captions, which achieves lower scores across\nmost benchmarks (49.86% average). Earlier attempts at knowledge integration, such as CapsFusion\n(50.88% average), while showing improvements over the Datacomp baseline, didn't achieve the same\nlevel of performance as our approach. The LAION-COCO dataset, constrained by both vocabulary"}, {"title": "4 Related Works", "content": "KALE builds on many large-scale image-text datasets such as\nLAION-5B [23], Datacomp-1B [7], COYO-700M [3], and many more.\nThese datasets were sourced from large amounts of images paired\nwith alt-text captions found in the HTML image tags associated\nwith these images. As LLM/VLMs have become more capable, many\nworks have explored generating synthetic multimodal training data.\nThere is a line of work that seeks to improve image-text datasets by\nusing VLMs to generate synthetic captions [22, 13, 15, 12]. Works\nsuch as LaCLIP [5] took an alternative approach of rewriting the\nexisting alt-text caption using an LLM to improve caption quality.\nMoreover, works such as LLaVA [16] and LLaVAR [31] have synthet-\nically generated visual question-answer pairs in the context of instruction tuning data. Additionally\nthe xGen-mm [26] model leverages our KALE dataset to improve the quality of their caption data.\nPrevious work has pointed out that synthetic captions lack real-world knowledge, limiting their\napplicability in many domains. CapsFusion [28] addresses this issue by augmenting LAION-COCO\nsynthetic captions with alt-text from the LAION dataset. VeCLIP [9] also addresses this issue but\ninstead of using existing captions, it generates synthetic captions using a LLaVA model.\nAn adjacent line of work improves the text quality of multimodal data by instead sourcing web-scale\ninterleaved image/text samples from web documents as opposed to HTML alt-text captions. Works\nsuch as MMC4 [32], OBELISC [10], MINT-1T [2], and OmniCorpus [14] all build multimodal\ninterleaved datasets, which is a promising direction for attaining high-quality and knowledge rich\nmultimodal data."}, {"title": "5 Limitations and conclusion", "content": "KALE represents a step forward in bridging the gap between descriptive synthetic captions and\nfactual web-scale alt-text. Our experiments demonstrate that models trained on KALE consistently\noutperform baseline models across various benchmarks. While KALE performs favorably compared\nto other open-source image-text datasets, the data still suffers from hallucination, particularly in\ntext-dense images. Future work should scale KALE to billions of image-text pairs, explore more\nsophisticated knowledge augmentation techniques, and investigate its impact on a broader range of\nmultimodal tasks."}]}