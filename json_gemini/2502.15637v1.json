{"title": "Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time Series Classification", "authors": ["Vasilii Feofanov", "Songkang Wen", "Marius Alonso", "Romain Ilbert", "Hongbo Guo", "Malik Tiomoko", "Lujia Pan", "Jianfeng Zhang", "Ievgen Redko"], "abstract": "In recent years, there has been increasing interest in developing foundation models for time series data that can generalize across diverse downstream tasks. While numerous forecasting-oriented foundation models have been introduced, there is a notable scarcity of models tailored for time series classification. To address this gap, we present Mantis, a new open-source foundation model for time series classification based on the Vision Transformer (ViT) architecture that has been pre-trained using a contrastive learning approach. Our experimental results show that Mantis outperforms existing foundation models both when the backbone is frozen and when fine-tuned, while achieving the lowest calibration error. In addition, we propose several adapters to handle the multivariate setting, reducing memory requirements and modeling channel interdependence.", "sections": [{"title": "Introduction", "content": "The advent of large foundation models (Bommasani et al., 2021) in computer vision (He et al., 2015; Dosovitskiy et al., 2021) and natural language processing (Achiam et al., 2023; Touvron et al., 2023) has significantly transformed research and applications. These models are pre-trained on extensive, diverse datasets to generalize across a wide range of downstream tasks. This approach not only simplifies model architecture selection but also reduces the need for large amounts of labeled data for new tasks, as the foundation model leverages its previously acquired knowledge.\nOver the past two years, the development of time series foundation models (TSFMs) has emerged as a prominent research area. For time series forecasting, numerous models have been proposed, either pre-trained from scratch on large volumes of time series data (Rasul et al., 2023; Das et al., 2023; Woo et al., 2024; Wang et al., 2024), including synthetic time series (Ansari et al., 2024; Bhethanabhotla et al., 2024), or adapted from pre-trained large language models (LLMs, Jin et al., 2023; Chang et al., 2023; Cao et al., 2023; Xue and Salim, 2023; Gruver et al., 2024).\nSome foundation models are designed to handle multiple tasks simultaneously, such as forecasting, classification, imputation, and anomaly detection (Zhou et al., 2023; Goswami et al., 2024). However, these general-purpose models may exhibit suboptimal performance because certain pre-training schemes are inherently more suitable for specific tasks (e.g., masked reconstruction loss may be better suited for imputation, while contrastive loss aligns more naturally with classification). Surprisingly, despite the widespread use of time series classification (Bagnall et al., 2018; Dau et al., 2019; Dempster et al., 2020), relatively few models focus specifically on this task. To address this gap, we developed Mantis, a new time series classification foundation model:\nOur goal is to provide an open-source, lightweight, and high-performance model that can be\nreadily used by practitioners and researchers across diverse domains.\nThe design of Mantis draws from advancements in time series representation learning (Yue et al., 2022; Eldele et al., 2021; Zhang et al., 2022), transformer-based models for time series (Nie et al., 2023; Ilbert et al., 2024; Lin et al., 2024).\nThe architecture of our model (Figure 1) features a token generator that transforms time series data into meaningful"}, {"title": "Methodology", "content": "In this section, we present the main technical details behind Mantis: we mathematically introduce the problem setup, discuss the data pre-processing, and present the architecture and the pre-training process."}, {"title": "Problem Setup", "content": "Mathematically speaking, our time series classification foundation model is an encoder $F : \\mathbb{R}^t \\rightarrow \\mathbb{R}^q$ that projects any time series $x \\in \\mathbb{R}^t$ with a fixed sequence length $t$ to a discriminative hidden space $\\mathbb{R}^q$. During the pre-training phase, we observe an unlabeled pre-training set $\\mathcal{X}_0$ that is sufficiently large in order to learn rich embeddings that generalize well across different tasks. During the fine-tuning phase, we observe a supervised downstream task with observations $\\mathcal{X}$ and labels $\\mathcal{Y}$. The two options are possible: 1) we use $F$ to extract deep embeddings $Z = \\{F(x), x \\in \\mathcal{X}\\}$ and then learn any classifier $h : \\mathbb{R}^q \\rightarrow \\{1, . . ., K\\}$ using $Z$ as features and $\\mathcal{Y}$ as corresponding labels, 2) append a classification head $h : \\mathbb{R}^q \\rightarrow \\mathbb{R}^K$ and fine-tune $h \\circ F$ by minimizing a loss function evaluated on the downstream dataset.\nWhen time series with multiple channels $x = [x_1,...,x_d] \\in \\mathbb{R}^{d\\times t}, d > 1$, are considered, we send each channel $x_i, i \\in [1, d]$, to the TSFM independently, i.e., the embedding of $x$ is defined as $z = \\text{concat} [ (F(x_i))_{1 \\le i \\le d}]$, where $\\text{concat}$ denotes the vector concatenation operator, and the input dimension of the classifier (head) is $\\mathbb{R}^{d\\times q}$. As we will discuss in Section 2.5, when $d$ is large, we can employ an adapter $a : \\mathbb{R}^{d\\times t} \\rightarrow \\mathbb{R}^{d_{new}\\times t}$ that compresses the original channels into new $d_{new}$ channels. In this case, the embedding $z$ is calculated as $z = \\text{concat} [ (F(a(x)_i))_{1 \\le i \\le d_{new}}]$, where $a(x)_i$ denotes the i-th new channel."}, {"title": "Data Pre-processing", "content": "Since time series data may vary in sequence lengths and units of measurement, pre-processing is required for both pre-training and inference. Similar to practices in computer vision, we pre-train the model with fixed input dimensions and resize inputs during inference and fine-tuning. Specifically, we set the input sequence length of Mantis to 512 and resize inputs using PyTorch's interpolation function (Paszke et al., 2019). To address the issue of varying units of measurement, we apply instance-level standard scaling. For each time series observation (and, in the case of multiple channels, for each individual channel), we subtract the mean and divide by the standard deviation calculated across the time steps. This scaling process is implemented directly within the model architecture, ensuring that inputs are transformed during the forward pass."}, {"title": "Architecture", "content": "Below, we give details on the architecture of Mantis, which is an adaptation of the Vision Transformer (ViT, Dosovitskiy et al., 2021) to the time series framework. The entire architecture is depicted in Figure 1.\nToken Generator Unit. The first step is to encode a time series into meaningful tokens, which are then passed to a transformer. Our token generation process consists of the following steps:\na. Before encoding a time series, we perform instance-level normalization, as explained in Section 2.2. We then split the time series into patches. While Lin et al. (2024) and Nie et al. (2023) directly split the time series into non-overlapping and overlapping patches, respectively, we achieve this by applying a single convolution layer followed by mean pooling to obtain 32 patches. We set the number of output channels in the convolution to 256, which means each patch represents 256 convolutional features.\nb. Similarly, we generate patches for the time series differential, which is computed by taking the difference between two adjacent timestamps. The main idea behind the differential is to make the time series stationary, thus reducing the influence of trends. In Section 3.4, we show that incorporating this feature improves the overall performance.\nc. To preserve information about the original unit measurements, we split the raw time series observation (i.e., before instance normalization) into 32 non-overlapping patches, compute the mean and standard deviation for each patch, and encode them using the Multi-Scaled Scalar Encoder (Lin et al., 2024).\nd. Finally, we concatenate all features (from the time series, its differential, and the statistics) and pass them through a linear projector followed by a layer normalization step (Ba et al., 2016), generating the final 32 tokens with a dimension of 256."}, {"title": "ViT Unit", "content": "We then feed the generated tokens into a ViT unit, with the main steps summarized below:\na. First, we append a class token to the 32 tokens generated in the previous step. As a learnable vector, the class token is introduced to attend to all the other tokens, aggregating information from the entire input into the embedding associated with it.\nb. To incorporate information about the positions of the tokens, we use the classical sinusoidal positional encoding (Vaswani et al., 2017). The position embeddings are summed with the input tokens and fed into a series of transformer layers.\nc. The transformer layer is identical to the one used by Dosovitskiy et al. (2021). We apply 6 transformer layers, each with multi-head attention consisting of 8 heads. During pre-training, we use a dropout rate of 10%.\nd. Finally, the class token's representation generated by the transformer serves as the final output of the foundation model."}, {"title": "Projector and Prediction Head", "content": "To use the output of the foundation model, we append different layers depending on whether the model is in the pre-training or fine-tuning stage:\nPre-training: At this stage, we add a layer normalization step followed by a linear layer, which projects the embeddings for calculating their similarities.\nFine-tuning: At this stage, we add a classification head that maps the embeddings to class logits."}, {"title": "Pre-training", "content": "We pre-train Mantis in a self-supervised way using a contrastive learning approach that aims to train an encoder that outputs similar representations for two random augmentations of the same sample (positive pair) and dissimilar representations for augmentations of two different samples (negative pair). More formally, let $\\mathcal{T}$ be a considered space of transformations (augmentations) such that $\\forall \\varphi \\in \\mathcal{T}, x \\in \\mathcal{X}$ we have $\\varphi(x) \\in \\mathcal{X}$. To measure the similarity of two embeddings, we first project the output of the foundation model $F(x)$ to a new dimension using a projector $g : \\mathbb{R}^q \\rightarrow \\mathbb{R}^{q'}$ and then compute the cosine similarity between the two vectors defined as follows:\n$S_{\\text{cos}}(a, b) := \\frac{a^T b}{\\|a\\| \\|b\\|}, \\quad \\forall (a, b) \\in \\mathbb{R}^{2q'}.$ \nGiven a batch $B = \\{x_i\\}_{i=1}^b$, for each example $x_i$, we sample two augmentation functions $\\varphi$ and $\\psi$ uniformly from $\\mathcal{T}$, i.e., $\\varphi, \\psi \\sim \\mathcal{U}(\\mathcal{T})$, compute the pairwise similarities between all the examples in the following way:\n$S_i(\\varphi, \\psi) = [S_{\\text{cos}} (g \\circ F \\circ \\varphi(x_i), g \\circ F \\circ \\psi(x_j))]_{j=1}^b \\in \\mathbb{R}^b$.\nFollowing Oord et al. (2018) as well as He et al. (2020) and denoting the cross-entropy error function by $l_{ce} : \\mathbb{R}^b \\times \\{1, ...,b\\} \\rightarrow \\mathbb{R}$, we update the weights of $F$ and $g$ by minimizing the contrastive loss which we define as\n$\\sum_{i=1}^b l_{ce} \\left( \\frac{S_i(\\varphi, \\psi)}{T}, i \\right),$ where $T \\in (0, +\\infty)$ is a temperature that we fixed to 0.1.\nRegarding the choice of augmentation methods, we empirically tested several augmentation functions and found that their effectiveness is highly dataset-dependent, as they may distort a time series and cause the loss of important information. For our pre-training, we chose the RandomCropResize augmentation (Figure 2), which involves randomly cropping $c\\%$ of a time series (with the condition that the remaining (1-c)\\% forms a contiguous window) and then resizing the cropped portion to the original sequence length. We applied relatively moderate distortions, varying the crop rate from 0\\% to 20\\%, preserving the main structure of a time series.\nAs a pre-training dataset, we use a union of various public datasets including UCR (Dau et al., 2019), UEA (Bagnall et al., 2018, all except EigenWorms and InsectWingbeat), ECG (Clifford et al., 2017), EMG (Goldberger et al., 2000), Epilepsy (Andrzejak et al., 2001), FD-A and FD-B (Lessmeier et al., 2016), Gesture (Liu et al., 2009), HAR (Anguita et al., 2013), SleepEEG (Kemp et al., 2000). We ensure that test sets used for evaluation in Section 3 are not part of the pre-training dataset. The total pre-training consists of 7 million time series examples. We pre-trained the model for 100 epochs with a batch size equal to 2048 on 4 NVIDIA Tesla V100-32GB GPUs."}, {"title": "Adapters", "content": "Nowadays, the multivariate setting represents one of the main challenges for time series foundation models. In practice, different time series classification tasks vary in the number of channels, which raises non-trivial questions on how the model should be pre-trained and applied for a new task. Similarly to other foundation models (Goswami et al., 2024; Lin et al., 2024), Mantis is pre-trained in a univariate way and can be applied to the multivariate setting by treating each channel independently. However, this strategy has several limitations. Firstly, the foundation models are heavy by design, so treating all channels independently may lead to excessive resource consumption and failure to fine-tune them. Secondly, at the feature extraction step, the channel correlations are ignored, which may lead to the loss of some important information.\nTo address these concerns, we study a simple approach of using a channel-level adapter $a : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d_{new}}$ that precedes the foundation model and transforms the original $d$ channels into new $d_{new}$ ones. While there are other ways to adapt foundation models to the multivariate setting (Zhang and Yan, 2023; Zhou et al., 2023), we consider this definition of an adapter due to its high flexibility: (a) not only Mantis but any foundation model can be plugged in, (b) channel-level transformation prevents from disrupting the temporal structure, (c) adaptation to the computation budget by determining the number of encoded channels.\nIn our experiments, we considered five adapters that we present further. Our idea was to test classical dimension reduction approaches like Principal Component Analysis (PCA) that are computationally efficient and are trained in an unsupervised way. However, their application is limited to 2D data matrices, making them not straightforward to use in our case. To overcome this problem, we train the dimension reduction method on the data reshaped to $(n \\times t, d)$, where $n$ is the number of training examples. Thus, with this trick, methods like PCA will be focused on correlations between channels over all time steps, capturing spatial correlations and learning the rotation matrix $W \\in \\mathbb{R}^{d_{new}\\times d}$ that linearly combines the original channels into new ones. Overall, these four adapters rely on this strategy:\nPrincipal Component Analysis (PCA) seeks to find an orthogonal basis of principal components where few components capture most of the data variance.\nTruncated Singular Value Decomposition (SVD, Halko et al., 2009) is similar to PCA, but it applies SVD decomposition directly on the non-centered data matrix.\nRandom Projection (Rand Proj) is a very simple baseline that projects data by randomly generating the rotation matrix $W \\in \\mathbb{R}^{d_{new}\\times d}$.\nVariance-Based Channel Selection (Var Selector) is a feature selection method that selects channels with the highest variance. This approach can be useful when channels with low variance are not very informative, so they can be discarded without affecting the overall performance.\nAlthough these adapters are computationally very efficient, they are not supervised meaning that they may lead to suboptimal classification performance. Following this reasoning, we introduce one more adapter:\nDifferentiable Linear Combiner (LComb) performs a linear channel transformation $W \\in \\mathbb{R}^{D' \\times D}$, which, unlike to previous adapters, is learned via backpropagation together with the encoder and classification head."}, {"title": "Experimental Results", "content": "In this section, we conduct a set of experiments to validate the high performance of our model. All our experiments have been performed on a single NVIDIA Tesla V100-32GB GPU."}, {"title": "Setup", "content": "Baselines. In our experiments, we compare Mantis with the State-of-the-Art (SOTA) time series classification foundation modeling. More specifically, these are the four following foundation models, whose implementation details can be found in Appendix A:\nUniTS (Gao et al., 2024) is a multi-task time series model that can be also considered as a foundation model with 1 million parameters. We use their checkpoint pre-trained in a supervised fashion on 18 datasets.\nGPT4TS (Zhou et al., 2023) is a method that partially fine-tunes a pre-trained GPT2 with an additional tokenization layer to perform different time series tasks including classification, anomaly detection, and forecasting. The total number of parameters is approximately 80 millions (10x larger than Mantis)."}, {"title": "Comparison with SOTA: Zero-shot Feature Extraction", "content": "In this section, we use a time series foundation model as a feature extractor without fine-tuning it to a given downstream task. First, we generate embeddings for time series observations by passing them through the frozen encoder. Then, we use embeddings as new features to train a classifier and report its performance on test data. As a classifier, we have chosen the Random Forest algorithm (Breiman, 2001) due to its versatility and high performance without the need to perform hyperparameter searching. We fixed the number of trees to 200 while not restricting the maximum tree depth. In this section, we compare Mantis with NuTime and MOMENT while removing GPT4TS and UniTS from consideration as they have not provided a methodology for zero-shot extraction."}, {"title": "Comparison with SOTA: Fine-tuning", "content": "In this experiment, we perform a comparison by fine-tuning foundation models to a downstream task. More specifically, we append a prediction head after an encoder and fine-tune all layers or a subset of them on the training data. For all models under consideration, we fix a fine-tuning scheme: we minimize the cross-entropy loss for 100 epochs with a fixed batch size equal to 256, using an AdamW optimizer (Loshchilov et al., 2017) with a weight decay of 0.05. For each model and each dataset, We choose a learning rate over the grid: $\\{10^{-4}, 2\\cdot10^{-4}, 10^{-3}\\}$ based on the validation set. For GPT4TS, we follow their paper and fine-tune the layers of the language backbone specified by the authors. For Mantis, NuTime, MOMENT, and UniTS, we perform fine-tuning of all the layers."}, {"title": "Ablation Study", "content": "In this section, we perform several ablation experiments for a more thorough analysis of Mantis. First, we validate some of our architecture choices. More specifically, we test whether the incorporation of features extracted from the differential of a time series, proposed in Section 2.3, improves the quality of embeddings. For this, we have pre-trained another version of our foundation model where the differential feature extraction part was removed from the Token Generator Unit. One can see that the incorporation of the differential block noticeably improves the accuracy score. When comparing Figure 3 and Figure 6, it is interesting to notice that Mantis without the differential block still slightly outperforms NuTime and MOMENT in the case of zero-shot feature extraction.\nIn the second part of our ablation study, we raise the following questions: (a) is pre-training really useful, or it is sufficient to train Mantis from scratch on each dataset?, (b) should we fine-tune the head or the whole model?, (c) can Mantis be fine-tuned with default hyperparameters, and how much we can gain from the model selection? To answer these questions, we empirically compare the following fine-tuning regimes:\nThe setup which we further call RF, and which we used for the zero-shot feature extraction experiment: the encoder of Mantis is frozen, and embeddings are used as features to train a Random Forest classifier.\nThe encoder of Mantis is frozen, and the task is solved by fine-tuning a classification head, which, in our case, is layer normalization step + linear layer. The purpose of comparing this strategy, further called Head, with RF is to see the impact of the choice of a classifier (linear vs non-linear, differentiable vs non-differentiable) on the overall performance.\nThe encoder is randomly initialized and fine-tuned together with a classification head. This baseline, which we further call Scratch, is introduced to see whether or not pre-training of Mantis is useful.\nFinally, the strategy further called Full consists in fine-tuning the pre-trained encoder together with a classification head.\nIn order to answer question (c), for every method, we report two scores: the test accuracy of the model at the last epoch (i.e., at the end of fine-tuning), and the best test accuracy of the model across all fine-tuning epochs. While the last epoch score gives a pessimistic evaluation of the performance as no model selection is performed, the best epoch score is an optimistic evaluation as it indicates the best achievable performance. In this experiment, we fix the fine-tuning scheme: the number of epochs is 100, the batch size is 256, the optimizer is AdamW with the weight decay of 0.05 and the learning rate set to $2\\cdot10^{-4}$ and adjusted following the cosine annealing decay strategy (Loshchilov and Hutter, 2016) with 10 warm-up epochs.\nFigure 7 depicts the average performance over the 151-D benchmark both for the last and best epoch, while the complete results can be found in Appendix, Table 6 and Table 7. First, we can see that the pre-training of Mantis is visibly useful as Full outperforms Scratch by 5.08\\% and 3.92\\% in the last and best epoch score, respectively. On the other hand, using a frozen pre-trained encoder is rather suboptimal in terms of performance, and full fine-tuning is preferred. This observation points out an important direction of future work, namely, to advance time series classification foundation models in terms of zero-shot performance."}, {"title": "Adapters", "content": "In this section, we study the performance of Mantis when it is combined with one of the adapters we introduced in Section 2.5. For PCA, SVD, Rand Proj and Var Selector, the new number of channels is equal to D' = min{D, 10}, while for LComb is always fixed to D' = 10. In addition, Figure 8 and Table 8 of the Appendix compare the no-fine-tuning case when Random Forest is used directly after the feature extraction step and the full fine-tuning with the best adapter chosen per dataset. Below, we discuss the experimental results.\nDimension reduction: When the number of channels is too large, the full fine-tuning of the foundation model is limited as treating all channels independently is computationally costly. Nevertheless, applying one of the five adapters solves the issue, allowing the full fine-tuning and increasing the performance as can be seen in Table 8.\nChannel interactions: Another advantage of adapters is that they mix the original channels allowing channels to interact before they are sent to the encoder. The utility of adapters from this perspective was shown by Benechehab et al. (2024) for zero-shot multivariate forecasting. Our experiments reveal that, in the classification context, conclusions are rather dataset-dependent:\nDifferentiable adapter: Although on some datasets, LComb yields high performance, overall, its performance is lower than those of PCA, SVD and Var Selector. Since LComb is fine-tuned together with the encoder and the head, we suggest that the optimization of the adapter is intricate, so searching for a better optimization scheme can be a good direction for future work."}, {"title": "Calibration", "content": "Uncertainty quantification is crucial when a learning model is part of a safety-critical system (Wang, 2023) or when its output is used to analyze the performance on test data (Xie et al., 2024). Consequently, in this section, we study the calibration properties of Mantis and other time series classification foundation models. We say a model is calibrated if, for a given confidence level $\\alpha$, the probability that the model predicts the true class is equal to $\\alpha$ (Guo et al., 2017):\n$P(Y = \\hat{Y} | \\text{conf}(X) = \\alpha) = \\alpha, \\quad \\forall \\alpha \\in [0, 1],$ where $\\hat{Y}$ and $\\text{conf}(X)$ denote the predicted label and model's confidence of a random variable $X$, respectively, which were formally defined in Section 2.1. We consider the Expected Calibration Error (ECE, Naeini et al., 2015) as a metric to evaluate the calibration on a test set $\\{x_i, Y_i\\}_{i=1}^n$. We discretize the interval [0, 1] into $m$ disjoint bins: $B_j \\cap B_k = \\emptyset, \\forall j \\neq k, 1 \\le j, k < m$ and $\\bigcup_{j=1}^m B_j = [0, 1]$. As commonly done in the literature, we split the interval into 10 equally spaced bins. Then, denoting the predicted label for $x_i$ by $\\hat{y}_i$, the ECE is defined as follows:\n$\\text{ECE} = \\frac{1}{n} \\sum_{j=1}^m \\left| \\sum_{\\substack{1 \\le i \\le n \\\\ \\text{conf}(x_i) \\in B_j}} \\mathbb{I}(y_i = \\hat{y}_i) - \\text{conf}(x_i) \\right|,$ Following the setup in Section 3.3, we evaluate the ECE before and after applying a post-hoc calibration for the five fine-tuned foundation models on 131 datasets from 131-D. We use two calibration techniques: (a) Temperature Scaling (Guo et al., 2017) that regulates the amplitude of logits before applying softmax, and (b) Isotonic Regression (Zadrozny and Elkan, 2002), a non-parametric method that adjusts predicted probabilities by fitting a piecewise constant function. For the second approach, instead of the classical algorithm, we consider the multi-class isotonic regression (IRM, Zhang et al., 2020) as it is shown to be less prone to overfitting. For each dataset, both approaches are fitted using the corresponding validation set (20\\% of training data as described in Section 3.1)."}, {"title": "Conclusion and Future Work", "content": "In this paper, we presented a technical report on Mantis, an open-source, lightweight foundation model for time series classification. Our experimental results show that Mantis outperforms other foundation models while demonstrating superior calibration. We also introduced an adapter approach that offers a user-friendly solution for practitioners with limited computational resources. However, the results suggest that there is still much work to be done in advancing foundation models. First, we believe that designing foundation models for time series classification is a quite challenging task, as can be seen in Figure 11, where there is no clear correlation between model size and overall performance. Second, our results reveal a significant performance gap between the zero-shot feature extraction and full fine-tuning regimes, which indicates that there is still a big room for improvement. Finally, we believe that improving the interpretability of the model output is also an important direction for future work. Specifically, ensuring strong calibration properties of the foundation model could"}, {"title": "Experimental Setup", "content": "Below we give more details how datasets were chosen for each benchmark."}, {"title": "Datasets and Benchmarks", "content": "UCR: For some datasets, the sequences are very long, so fine-tuning GPT4TS does not fit the memory.\nUEA: We have excluded AtrialFibrillation and StandWalkJump datasets due to their very small test size and PenDigits due to its very small sequence length. For InsectWingbeat dataset, we subsampled 1000 examples from the original training set (which contains 30,000 examples) and 1000 from the original test set (of 20,000 examples) to reduce computational overhead while maintaining sufficient variety in the data for robust model evaluation. The following 7 datasets have a very large number of channels, creating issues with full fine-tuning:\nOther datasets: EMOPain has a lot of channels making the full fine-tuning costly, while MotionSenseHAR has too long sequences for GPT4TS.\nTo sum up, for Section 3.2 and the first experiment of Section 3.4, we use all 159 datasets without exceptions; for Section 3.3 and Section 3.6, we exclude all long and high-dimensional datasets, i.e., 15 datasets from UCR, 7+4 datasets from UEA, 2 datasets from the others; for the second experiment of Section 3.4, we exclude only high-dimensional datasets, i.e., 7 datasets from UEA + EMOPain dataset; finally, for Section 3.5, we use all 27 UEA datasets under consideration."}, {"title": "Implementation Details", "content": "MOMENT: We use the MOMENT-large model (d_model=1024), which pre-trained weights can be found on the corresponding HuggingFace repository. To handle the multi-channel setup, we process every channel independently and concatenate all the embeddings before passing them to the classification head. In the paper, they have considered datasets with a sequence length < 512 and use zero-padding to fix the input size to 512. At the same time, we have also tried to interpolate sequences to 512 instead, and it did not affect the performance of MOMENT. Thus, we have decided to stick to the latter option as it allows us to evaluate MOMENT for any sequence length.\nGPT4TS: We use the code provided by the authors. As a backbone, the model uses 6 layers from the pre-trained GPT2, which checkpoint can be found on HuggingFace. Channels are processed simultaneously as a part of their tokenizer.\nNuTime. We use the pre-trained weights provided by the authors in their GitHub repository, while fixing the hyperparameters of the architecture according to this configuration file. In contrast to the original implementation, we do not use their adapter (described in Section 3.4 of their paper) but process all channels independently as for Mantis and MOMENT. This allows us to use NuTime in the zero-shot feature extraction setting as their adapter has to be fine-tuned.\nUniTS. We use the implementation and the checkpoint provided the authors at their GitHub repository. Similarly to Mantis, MOMENT and NuTime, channels are processed independently. Stride and patch size are set to 16. Dimension of the model is 64."}, {"title": "Complete Results", "content": "Below we provide full tables with experimental results."}]}