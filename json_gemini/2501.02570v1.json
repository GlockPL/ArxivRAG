{"title": "Decoding fMRI Data into Captions using Prefix Language Modeling", "authors": ["Vyacheslav Shen", "Kassymzhomart Kunanbayev", "Dae-Shik Kim"], "abstract": "With the advancements in Large Language and Latent Diffusion models, brain decoding has achieved remarkable results in recent years. The works on the NSD dataset, with stimuli images from the COCO dataset, leverage the embeddings from the CLIP model for image reconstruction and GIT for captioning. However, the current captioning approach introduces the challenge of potential data contamination given that the GIT model was trained on the COCO dataset. In this work, we present an alternative method for decoding brain signals into image captions by predicting a DINOv2 model's embedding of an image from the corresponding fMRI signal and then providing its [CLS] token as the prefix to the GPT-2 language model which decreases computational requirements considerably. Additionally, instead of commonly used Linear Regression, we explore 3D Convolutional Neural Network mapping of fMRI signals to image embedding space for better accounting positional information of voxels.", "sections": [{"title": "Introduction", "content": "It has been a common practice for brain decoding studies on the Natural Scenes Dataset (NSD) (Allen et al., 2021) to predict the embeddings of multimodal vision-language models like CLIP (Radford et al., 2021), GIT (Wang et al., 2022) from brain activations to use these predictions for further image and caption generation. These approaches typically involve mapping high-dimensional fMRI data (e.g., 15,724 voxels for subject 1) to even higher-dimensional model embeddings. Current state-of-the-art brain captioning methods (Ferrante, Ozcelik, Boccato, VanRullen, & Toschi, 2023) (Scotti et al., 2024), which primarily leverage the GIT model (with an embedding dimension of 257 \u00d7 1024) for caption generation from fMRI signals, require considerable computational resources due to these high-dimensional transformations. Also, in the conventional NSD train/test split (Takagi & Nishimoto, 2023) (Ozcelik & VanRullen, 2023), all images from the test set appear in the training set of GIT, raising the concern of possible data contamination.\nAdditionally, in other brain decoding works including (Mai & Zhang, 2023) (Ozcelik & VanRullen, 2023) (Takagi & Nishimoto, 2023), fMRI voxels for a particular image are linearized using an ROI mask, followed by the application of Ridge Regression to map the fMRI voxels to the model embeddings. However, activations from regions not present in the ROI mask could be overlooked, and positional information of these voxels, which might help in brain decoding, is neglected.\nTo address the mentioned challenges, our study tests a new method utilizing a single DINOv2 (Oquab et al., 2023) [CLS] vector (of size 1536) as a representation of an image and the GPT-2 language model (Radford et al., 2019) for text generation. The main idea is to predict the DINOv2 embedding directly from the corresponding fMRI signals using 3D-Convolutional ResNets (He, Zhang, Ren, & Sun, 2016), and pass them as a prefix through the language model to generate captions as proposed in (Mokady, Hertz, & Bermano, 2021)."}, {"title": "Methodology", "content": "We follow the traditional NSD train/test split and data preprocessing of 4 subjects (sub1, sub2, sub5, sub7) obtained from GLM (betas_fithrf_GLMdenoise_RR) as it was utilized in Ozcelik and VanRullen (2023).\nGiven that, fMRI data represents a 4D array (time \u00d7 W \u00d7 D x H), for Ridge Regression mapping from brain activity to the DINOv2 embedding space. We applied z-normalization for the linearized fMRI voxels\u00b9 extracted from the NSDGeneral ROI mask, whereas for CNNs, a 3D input\u00b2 at each time point was scaled between -1 and 1."}, {"title": "Brain Captioning", "content": "The scheme of our approach is presented in Figure 1. It has two parts: a brain and a captioning module. Both modules are trained separately.\nThe brain module is used to map fMRI activations into a DINOv2 embedding. DINOv2 was chosen due to its rich and robust visual features achieved by self-supervised learning compared to CLIP. It is trained using Mean Squared Error (MSE) loss where the ground truth label is the image embedding from the DINOv2-g model. We tried 3 mapping networks: Ridge Regression and two variations of 3D ResNet with 18 layers referred to as Shallow and Wide CNNs. Wide CNN has more feature planes than Shallow CNN.\nThe captioning module consists of a light transformer (Vaswani et al., 2017) and a language model. During training, the transformer converts the DINOv2 image embedding into prefix tokens that have the same dimensions as a word embedding. These prefix tokens are then used as inputs for the language model. The training objective is to predict caption tokens conditioned on the prefix autoregressively (Mokady et al., 2021).\nAt inference time, the brain module predicts the embedding of the seen image from the fMRI signal, which is then passed to the captioning module. While decoding from the language model, the beam search was employed to select the next token."}, {"title": "Results", "content": "Table 1 presents our Wide CNN results in comparison with existing works. Following the evaluation protocol used by Scotti et al. (2024), we assess captions predicted by our model with two different ground truth captions: the original COCO captions and captions generated directly by our captioning module.\nWhen evaluated against the original COCO captions, our approach demonstrates superior performance in the METEOR metric and achieves comparable ROUGE scores to MindEye-2, while requiring only 1/171th of the parameter space.\nTable 2 shows the results of different mapping networks used in the brain module. The advantages of CNN architectures compared to Ridge Regression, particularly our Wide CNN implementation, are evident across all metrics."}, {"title": "Conclusion & Future Work", "content": "This work demonstrates competitive performance in brain captioning while significantly reducing computational requirements compared to previous approaches. Our approach minimizes the issue of data contamination to zero by using the DINOv2 model which has not been trained on the COCO dataset, and introduces a new method to fMRI signals to the image embedding space by passing complete fMRI volume through the convolutional neural networks. This improvement implies that information outside of the ROI mask and positional information can enhance brain decoding.\nAlso, providing fMRI data as the prefix for the language models (Ye et al., 2023) can be promising in creating brain decoding frameworks designed for complex tasks like visual-question answering."}]}