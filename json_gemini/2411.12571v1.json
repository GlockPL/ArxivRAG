{"title": "LARGE LANGUAGE MODELS FOR COMBINATORIAL\nOPTIMIZATION OF DESIGN STRUCTURE MATRIX", "authors": ["Shuo Jiang", "Min Xie", "Jianxi Luo"], "abstract": "Combinatorial optimization (CO) is essential for improving efficiency and performance in engineering\napplications. As complexity increases with larger problem sizes and more intricate dependencies,\nidentifying the optimal solution become challenging. When it comes to real-world engineering\nproblems, algorithms based on pure mathematical reasoning are limited and incapable to capture the\ncontextual nuances necessary for optimization. This study explores the potential of Large Language\nModels (LLMs) in solving engineering CO problems by leveraging their reasoning power and\ncontextual knowledge. We propose a novel LLM-based framework that integrates network topology\nand domain knowledge to optimize the sequencing of Design Structure Matrix (DSM)\u2014a common\nCO problem. Our experiments on various DSM cases demonstrate that the proposed method achieves\nfaster convergence and higher solution quality than benchmark methods. Moreover, results show\nthat incorporating contextual domain knowledge significantly improves performance despite the\nchoice of LLMs. These findings highlight the potential of LLMs to address complex CO problems by\ncombining semantic and mathematical reasoning. This approach paves the way for a new paradigm\nin real-world engineering combinatorial optimization.", "sections": [{"title": "1 Introduction", "content": "Combinatorial optimization (CO) problems are ubiquitous across fields, where finding an optimal solution from a\nfinite set often drives improvements in efficiency, cost, and performance [1]. For instance, applications such as DNA\nbarcoding and DNA assembly in synthetic biology [2], as well as job scheduling in manufacturing [3], rely heavily on\neffective CO solutions. However, due to their NP-hard nature, these problems present substantial challenges, especially\nas complexity increases with larger problem sizes and more intricate dependencies. Traditionally, CO problems in\nengineering are usually approached through the following process: the problem is first modelled mathematically, then\nsolved using specific algorithms or heuristics, and finally interpreted within the context of practical engineering [4]."}, {"title": "2 Methodology", "content": "In this section, we introduce our proposed LLM-based framework for solving CO problems. The framework is\ndesigned to harness the generative and reasoning powers of LLMs in combination with domain knowledge and objective\nevaluation. The framework begins with the initialization of a solution randomly sampled in the total solution space.\nEach solution is evaluated based on predefined criteria by an evaluator, which quantifies the quality of a solution.\nUsing this evaluation, the framework iteratively updates the solution base through few-shot learning and suggesting\nnew candidates, guided by crafted prompts that include both network information in mathematical form and domain\nknowledge in natural language description. The newly generated solutions are appended into the solution base, together\nwith their evaluation results. When the iteration time is reached, the solution base returns the best one as the final output.\nIn following, we focus on DSM sequencing as a common CO problem to illustrate the pipeline. The framework is\ndepicted in Figure 2."}, {"title": "Initialization and solution sampling", "content": "The Solution Base serves as an essential module to (1) enable storage of explored solutions and their evaluation results,\n(2) provide historical solutions to the backend LLM for few-shot learning, and (3) return the top-performing solution\nwhen iteration ends.\nThe initialization involves generating an initial solution that is randomly sampled from the entire solution space. In\nthe DSM sequencing task, a solution represents a complete and non-repetitive sequence of nodes (see Figure 2). This\ninitial solution is then evaluated and added to the Solution Base for future use. In subsequent iterations, we design\na sampling rule that selects $K_p$ top-performance solutions and randomly samples $K_q$ solutions from the remaining\n$K_n - K_p$ solutions to form a solution set, where $K_n$ is the total number of solutions in the Solution Base. $K_p$ and $K_q$\nare adjustable parameters. The obtained solution set is further refined and crafted into prompts."}, {"title": "LLM-driven optimization using network information and domain knowledge", "content": "In each iteration of the optimization process, we prompt backend LLM with the following information: (i) Typology\nInformation: These two elements complete the mathematical description of a DSM. It is noteworthy that there\nare multiple equivalent representations for describing a network mathematically, such as an edge list, a dependency\nrelationship list according to node sequence, or an adjacency matrix. In this research, we choose the edge list as the\nrepresentation of the network's topology and shuffle all edges to avoid any possible bias. (ii) Contextual Domain\nKnowledge: This includes the name of each node and an overall description of the network, which conveys the domain\nknowledge underlying the DSM's mathematical structure to the LLM. For instance, in an activity DSM, each node\nrepresents the name of an activity in the entire design process. (iii) Meta-instructions: We adopt some frequently used\nprompt engineering strategies [20], including role-playing, task specification, and output format specification. These\nstrategies allow the LLM to follow the guidance to perform reasoning and generate solutions in a specific format. (iv)\nSelected historical solutions: As described in the last section, we obtain a set of up to $K_p + K_q$ solutions through\nsampling from the Solution Base for the LLM to use in few-shot learning.\nOnce receiving inputs described above, the backend LLM combines the network topology information with domain\nknowledge to infer and suggest new solutions. The generated solution must pass the checker, which verifies that all\nnodes are present exactly once in the sequence. Once validated, the solution is then evaluated and appended to the\nSolution Base. The detailed input prompts are included in Appendix 1."}, {"title": "Evaluation of DSM sequencing solutions", "content": "The evaluator is used to quantify each newly generated solution. For the DSM sequencing task, the goal is to reorder\nthe rows and columns of the DSM to minimize feedback loops. To achieve this objective, the evaluator calculates the\nnumber of backward dependencies in the corresponding sequence."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Data", "content": "We collected four DSM cases for our experiments, which can be categorized into two types [21]: (1) Activity-based\nDSMs, which represent the input-output relationships between different tasks or activities within a project; and (2)\nParameter-based DSMs, which illustrate the relationships among design parameters of a product.\nThe DSM of the Unmanned Combat Aerial Vehicle (UCAV) includes 12 conceptual design activities conducted at\nBoeing [22]. The DSM of the Microfilm Cartridge was derived from Kodak's Cheetah project including 13 major\ntasks [23]. These two activity-based DSMs were constructed based on interviews with relevant engineers, followed\nby review, verification, and calibration to ensure accuracy. For the parameter-based DSMs, the Heat Exchanger [19]\nand the Automobile Brake System [24], researchers first identified the key components from the product and then\ninterviewed the corresponding designers to define design parameters and establish precedence relationships. The Heat\nExchanger DSM contains 17 components related to core thermal exchange elements, while the Brake System DSM\nincludes 14 main parameters covering braking mechanisms and their dependencies.\nFrom the referential documents, we extracted: (1) the name of each node, (2) the edge list obtained from the adjacency\nmatrix, and (3) the overall description of each network. All data were kept consistent with the original references. The\nspecific data formats are shown in Appendix 1. The characteristics of four DSMs are summarized in Table 1. In general,\nthe node count (N) of DSMs ranges from 12 to 17, and the edge count (E) varies between 32 and 47. The network\ndiameter, representing the longest shortest path between any two nodes, spans from 2 to 7. We also present measures\nsuch as network density and clustering coefficient to highlight the complexity. For instance, the UCAV DSM has a high\nnetwork density of 0.712 and a clustering coefficient of 0.773, suggesting strong interconnections, while measures of\nthe Heat Exchanger DSM indicate a sparser network with more distinct relationships."}, {"title": "3.2 Experiment Setup", "content": ""}, {"title": "Experimental setting", "content": "In all our experiments, we ran each method 10 times with different random seeds to ensure robustness. For solution\nsampling, we set $K_p$ = 5 and $K_q$ = 5. For the index of each node, we use a string composed of 5 randomly generated\ncharacters (a mix of numbers and letters) to represent the node uniquely. The maximum number of iterations was set to\n20. We selected Claude-3.5-Sonnet-20241022 as the backbone LLM and kept all other settings of the LLM as their\ndefault values [25]."}, {"title": "4 Results and Discussion", "content": "We first compared the convergence speed among our LLM-based methods and the three variants of GA, as shown in\nFigure 3. In GA, due to different parameter settings (e.g., population size, crossover, and mutation rates), each iteration\nexplores multiple unique solutions. In contrast, our methods suggest only one unique solution per iteration. Therefore,"}, {"title": "Comparative methods for benchmarking", "content": "We consider two types of approaches for benchmarking.\nThe first type is stochastic methods, which rely on probabilistic rules to explore the solution space. We chose the classic\nGenetic Algorithm (GA) for comparison, as it has been successfully applied to various optimization problems [26]. We\nimplemented GA using the DEAP library [27]. We considered three different settings for GA: exploration-focused,\nexploitation-focused, and balanced setting. Specific parameter settings of three variants are detailed in Appendix 2.\nThe second type is deterministic methods, which rank nodes based on a specific measure and then use this ranking for\nreordering. Since the calculation of each measure is deterministic, the resulting solution is consistent. We considered\nfive deterministic methods for comparison, each reordering nodes based on different criterion: (i) Out-In Degree [28]:\nReordering based on the difference between out-degree and in-degree of each node. (ii) Eigenvector [29]: Calculating\nthe values of components in the Perron vector of the adjacency matrix and sorting nodes accordingly. (iii) Walk-based\n(Exponential) [30]: Considering in-depth connectivity patterns by involving the power of the adjacency matrix A. It is\ncalculated by: $F(A) = exp(A)$. (iv) Walk-based (Resolvent) [31]: Calculated by: $F(A) = (I \u2212 \u03b4A)^{-1}$, where \u03b4\nrepresents the probability that a message will successfully traverse an edge. We set d = 0.025 in experiments following\nthe original reference. (v) Visibility [32]: Involving the visibility matrix, showing the dependencies between all system\nelements for all possible path lengths up to the matrix size. It is calculated by: $F(A) = \\sum_{i=0}^{n} A^{k}$, where n represents\nthe number of nodes. The resulting visibility matrix is then binarized. For methods (iii), (iv), and (v), once the F(A)\nis obtained, nodes are reordered by the sum of rows of F(A). If nodes share the same value, they are then reordered\nbased on the sum of columns of F(A).\nTo further investigate the effectiveness of incorporating domain knowledge, we also consider a variant of our proposed\nmethod for comparison. In this variant, all settings are kept the same as in the main method, except that the contextual\nknowledge about nodes and the entire network is removed. Therein, only the topological information of the network\nis engineered into the input prompt, guiding the LLM to search for an optimal solution solely through mathematical\nreasoning."}, {"title": "5 Concluding Remarks", "content": "Experimental results demonstrate that our proposed method, particularly when incorporating contextual domain\nknowledge, offer significant advantages in both convergence speed and solution quality compared to benchmarking\nstochastic and deterministic approaches. Across all experiments and backbone LLMs, the inclusion of domain\nknowledge consistently improved performance. While Claude-3.5-Sonnet is the most effective backbone LLM, open-\nsource models such as Llama3-70B also shows strong performance. Overall, these findings illustrate the capability\nof our proposed method to effectively address DSM sequencing tasks, leveraging both semantic and mathematical\nreasoning of LLMs for real-world combinatorial optimization problems.\nDespite the promising results, there are some limitations. First, the scope of the current experiments could be expanded\nby collecting more diverse DSM cases, including larger and more complex networks, to achieve more robust statistical\nresults. Second, while we focused on DSM sequencing tasks, the effectiveness of our proposed method could be\nexplored across a wider range of real-world combinatorial optimization tasks to assess its generalizability. Third, we\nplan to delve into the intermediate changes in LLM outputs during optimization in future work, which may provide\ngreater interpretability and insights into the LLMs' reasoning processes.\nIn conclusion, our study highlights the potential of LLMs for combinatorial optimization of design structure matrix,\nshowing that including domain knowledge can significantly enhance the performance of our methods. Future research\ncan build on these findings to further refine LLM-driven methods and extend applications to more complex and diverse\ncombinatorial optimization problems."}, {"title": "Appendix 1. Full prompts", "content": "Prompt for our proposed method (the main method):\nYou are an expert in the domain of combinational optimization.\nPlease assist me to find an optimal sequential order that minimizes feedback cycles in the dependency\nnetwork described below. Your task is to propose a new order that differs from previous attempts and has fewer\nfeedback cycles than any listed.\n<Description of the entire Network> {network_description} </Description of the entire\nNetwork>\n<Nodes with Descriptions> {node_list_with_description}</Nodes with Descriptions>\n<Edges> {edge_list}</Edges>\nBelow are some previous sequential orders arranged in descending order of feedback cycles (lower is\nbetter): {selected_historical_solutions}\nPlease suggest a new order that:\n- Is different from all prior orders.\nHas fewer feedback cycles than any previous order.\nCovers all nodes exactly once.\nStarts with <order> and ends with </order>.\nYou can use the descriptions of nodes and networks to support your suggestion.\nOutput Format:\n<order>\n</order>\nPlease provide only the order and nothing else.\nPrompt for our proposed method (removing contextual domain knowledge):\nYou are an expert in the domain of combinational optimization.\nPlease assist me to find an optimal sequential order that minimizes feedback cycles in the dependency\nnetwork described below. Your task is to propose a new order that differs from previous attempts and has fewer\nfeedback cycles than any listed.\n<Nodes> {node_list} </Nodes>\n<Edges> {edge_list} </Edges>\nBelow are some previous sequential orders arranged in descending order of feedback cycles (lower is\nbetter): {selected_historical_solutions}\nPlease suggest a new order that:\n- Is different from all prior orders.\nHas fewer feedback cycles than any previous order.\nCovers all nodes exactly once.\nStarts with <order> and ends with </order>.\nOutput Format:\n<order>\n</order>\nPlease provide only the order and nothing else."}, {"title": "Appendix 2. Parameter settings for three variants of GA", "content": "In all three variants of the GA used for DSM sequencing tasks, we employed the following shared settings: the number\nof generations was set to 2,000, with a selection mechanism using tournament selection and mutation using shuffled\nindexes. The GA was implemented using the DEAP library with standard configurations for initial population generation\n[27]. The different configurations for each GA setting are shown in the table below. The meaning of each parameter\nalso refers to [27]."}]}