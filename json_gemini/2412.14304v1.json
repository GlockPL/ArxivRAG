{"title": "Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICS", "authors": ["David Restrepo", "Chenwei Wu", "Zhengxu Tang", "Zitao Shuai", "Thao Nguyen Minh Phan", "Jun-En Ding", "Cong-Tinh Dao", "Jack Gallifant", "Robyn Gayle Dychiao", "Jose Carlo Artiaga", "Andr\u00e9 Hiroshi Bando", "Carolina Pelegrini Barbosa Gracitelli", "Vincenz Ferrer", "Leo Anthony Celi", "Danielle Bitterman", "Michael G Morley", "Luis Filipe Nakayama"], "abstract": "Current ophthalmology clinical workflows are plagued by over-referrals, long waits, and complex and heterogeneous medical records. Large language models (LLMs) present a promising solution to automate various procedures such as triaging, preliminary tests like visual acuity assessment, and report summaries. However, LLMs have demonstrated significantly varied performance across different languages in natural language question-answering tasks, potentially exacerbating healthcare disparities in Low and Middle-Income Countries (LMICs). This study introduces the first multilingual ophthalmological question-answering benchmark with manually curated questions parallel across languages, allowing for direct cross-lingual comparisons. Our evaluation of 6 popular LLMs across 7 different languages reveals substantial bias across different languages, highlighting risks for clinical deployment of LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought or Retrieval-augmented generation (RAG) by themselves fall short of closing this performance gap, often failing to improve performance across all languages and lacking specificity for the medical domain. To address this issue, We propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time de-biasing method leveraging retrieval augmented generation and self-verification. Our approach not only improves performance across all languages but also significantly reduces the multilingual bias gap, facilitating equitable LLM application across the globe.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) such as Llama and GPT families have emerged as a transformative force in the field of artificial intelligence, offering new tools and solutions in healthcare, with ophthalmology presenting a particularly promising field for their applications (Rojas-Carabali et al. 2024). Characterized by high patient volumes and substantial documentation burdens (Nath et al. 2022; Lin et al. 2020), ophthalmology could benefit significantly from LLM integration, mainly in regions such as LMICs where specialized personnel are scarce resources (Malerbi and Andrade 2022). LLMs could potentially automate remote triage (Tan et al. 2020), clinical decision support (Nath et al. 2022), patient education (Yang et al. 2024b), and administrative workflows (Antaki et al. 2023).\nWhile LLMs have shown remarkable potential in healthcare, their performance is not uniform across all languages (Marchisio et al. 2024; Plaza et al. 2024; Wang et al. 2024). These models typically perform best in English, benefiting from abundant training data, but often struggle with languages common in Low and Middle-Income Countries (LMICs) such as Portuguese, Spanish, Hindi, and Filipino (Marchisio et al. 2024; Zhao et al. 2024c; Dychiao et al. 2024), where the amount of ophthalmological data available is considerably limited (Restrepo et al. 2024a; Han et al. 2023). This disparity poses a significant challenge to the equitable deployment of LLMs in global healthcare (Lai, Mesgar, and Fraser 2024; Wang, Minervini, and Ponti 2024; Huang et al. 2023). Ironically, LMICs may stand to benefit the most from validated LLM applications like remote diagnostics (Henkel et al. 2023) in the future due to their limited trained personnel (Doshi et al. 2023; Restrepo et al. 2024b) compared to wealthier nations. Also, effective healthcare delivery in LMICs often requires communication in local languages(Mensah, Quao, and Group 2024). To fully realize the potential of LLMs in improving global health outcomes (Poulain, Fayyaz, and Beheshti 2024), it's crucial to evaluate and mitigate this linguistic bias (Wei et al. 2024) and ensure these advanced technologies can effectively serve diverse populations (Li et al. 2024c), regardless of their primary language (Kaur et al. 2024)."}, {"title": "Related Work", "content": "Existing Work on LLM Multilingual Dataset and Biases in Medicine LLMs' unequal performance across languages in tasks such as question answering and numerical reasoning has raised concerns, as training corpora are primarily from resource-rich languages (Xu et al. 2024). Studies have revealed that LLMs in multilingual tasks suffer from language bias, demographic bias, and evaluation bias (Xu et al. 2024), significantly favoring Western continents in factual information and reasoning (Shafayat et al. 2024; Zhao et al. 2024b).\nThere is also a lack of comprehensive multilingual evaluation efforts for medical question-answering with LLMS, particularly in specialized domains like ophthalmology. As shown in Table 1, existing datasets are predominantly in English or Mandarin, with minimal representation of languages from LMICs. None of the existing datasets provide paired data across multiple languages, which is crucial for consistent evaluation of LLMs' multilingual capabilities. While some datasets are substantial in size (e.g., PubMedQA with"}, {"title": "Existing LLM Debiasing Methods for Multilingual QA", "content": "Recent efforts have introduced innovative methodologies to enhance the multilingual capabilities of LLMs, primarily in non-medical domains. Notable approaches include Cross-Lingual Template Prompting (XLT) (Huang et al. 2023; Li et al. 2023b), Pre-Translate COT (Intrator et al. 2024; Chai et al. 2024), and EN-COT (Ruder et al. 2021). XLT (Huang et al. 2023) employs a generic template prompt to engage LLMs' cross-lingual and logical reasoning capabilities. While effective for simple questions, its applicability to complex medical queries may be limited. EN-COT (Ruder et al. 2021) generates a chain of thought directly in English, regardless of the original problem language, leveraging English for cross-lingual knowledge transfer. Pre-Translate COT (Intrator et al. 2024; Chai et al. 2024) involves translating problems into English before applying the English Chain of Thought (CoT) technique. However, it may not fully address the specific linguistic needs of clinicians due to inconsistent translation qualities. Frameworks like causality-guided debiasing(Li et al. 2024b)also provide a theoretical grounding for understanding and addressing biases in LLMs. Challenges persist in ensuring these techniques' effectiveness across diverse medical terminologies and contexts."}, {"title": "Retrieval Augmented Generation and LLM Self-Verification", "content": "While methods like EN-COT(Ruder et al. 2021) attempt to overcome language limitations, LLMs may still lack domain knowledge and not fully capture the nuances of medical terminology in different languages. Retrieval Augmented Generation (RAG) has emerged as a powerful technique to address LLMs' lack of medical knowledge without requiring extra fine-tuning on the model (Ren et al. 2024; Grandinetti and McBeth 2024). However, RAG faces challenges in content relevance and quality of matches, especially for target LMIC languages with limited resources (Shi et al. 2024; Salemi and Zamani 2024). Self-verification techniques have shown potential in enhancing the reliability of AI-generated responses. Frameworks like Self-RAG (Self-Reflective Retrieval-Augmented Generation) (Asai et al. 2023) use retrieval and self-reflection mechanisms to critique and improve responses iteratively, enhancing the quality and factuality of generated text."}, {"title": "Benchmark Construction, Results and Analysis", "content": "This section details the processes of data collection, preparation, and technical implementation of our evaluation framework to assess LLM multilingual language understanding and medical knowledge. The proposed dataset aims to address the following key research questions (RQs):\nRQ1: Are current LLMs equipped with domain knowledge to be useful for ophthalmologist assistance and safe for patients (e.g. understand clinical terms)?\nRQ2: How and how much does LLM performance vary between languages from HICs and those from LMICs?\nRQ3: What factors may contribute to any observed language disparities?\nRQ4: Are current debiasing methods adequate to address language disparities?"}, {"title": "Benchmark Construction", "content": "To construct our dataset, 7 board-certified ophthalmologists with diverse language backgrounds over the globe curated a set of ophthalmological test questions, creating a multilingual dataset of 1184 questions.\nThe questions were manually designed and inspired by the Brazilian Ophthalmological Board exams, with clinical experts ensuring there would be neutrality in answers across different regions of the world. The benchmark consists of two high-level categories: basic sciences and clinical-surgical ophthalmology, with subgroups on anatomy, pharmacology, clinical optics, strabismus, cataract, uveitis, oncology, refractive surgery, contact lens, and genetics. The distribution of the questions is in Figure 2.\nOriginally written in Portuguese, these questions were manually translated into English, Spanish, Filipino, Mandarin, French, and Hindi to ensure broad linguistic, population, and geographical coverage. We select English as the most HIC-representative language, French, Spanish, and"}, {"title": "Benchmark Results and LLM-Failure Analysis", "content": "The performance of LLMs differs significantly between languages from HICs and those from LMICs. As shown in Table 2, our quantitative evaluation results highlight significant disparities in the LLMs' performance across languages, with the most notable underperformance in Filipino and Hindi. GPT-4 achieved only 51.8% accuracy in Filipino and 50.6% in Hindi, compared to 63.4% in English, a substantial 11.6% and 12.8 difference respectively. This gap is even more pronounced in Llama-3 70B with a difference of 14% in Filipino compared to English, and Mixtral, with a difference of 17.6% for Filipino, and 20.1% for Mandarin compared to English. As can be seen in table 1, the tendency of LMIC languages, such as Filipino (FIL), Hindi (HI) and Mandarin (ZH), consistently exhibit similar patterns of decreased accuracy across all models. One exception stands for Mandarin on Qwen-2, potentially due to the larger Mandarin corpus involved in its pretraining and its tokenizer being also trained on Chinese characters (Yang et al. 2024a).\nLLMs are not equipped with sufficient ophthalmology domain-specific knowledge. We observed that more advanced LLMs demonstrate smaller performance gaps between languages. GPT-4, for instance, shows a narrower margin between its highest and lowest-performing languages compared to GPT-3.5. This trend is also evident in open-source models: Qwen-2 72B, which exhibits a smaller gap between English and Filipino performance compared to less sophisticated models like Llama-2 70B or Mixtral-8x7B. However, it is noteworthy that while these models show improvement in reducing language disparities, significant gaps persist. While the concrete composition of every model's pretraining datasets is unknown, it is widely acknowledged that they are largely composed of more common languages, such as English, in online corpora. These findings pinpoint a concerning bias in these models, which tend to favor languages with extensive representation in their training datasets.\nPerformance degradation results from difficulties of LLMs in understanding clinic terms, linguistic nuances and cultural context. Our analysis also reveals a concerning trend of performance degradation when LLMs encounter more complex, clinically oriented questions. GPT-4's average performance on clinical/surgical questions experiences a 3.79% drop from its basic science questions, while for Llama-3 and Qwen-2 this number goes up to 4.46% and 6.84%. This trend suggests a lack of specialized clinical knowledge in LLMs, which becomes more pronounced when prompting using LMIC languages.\nQualitative analysis by ophthalmologists of the CoT histories revealed striking examples of how LLMs struggle with the nuanced interpretation of medical terminology across different languages, potentially leading to critical misunderstandings in clinical contexts. Our complete qualitative analysis is included in the Appendix. For instance, when presented with the Filipino term \"namamaga,\u201d which can mean both \"swollen\u201d and \u201cinflamed\u201d, GPT-3.5 consistently interpreted it as \"swollen\u201d in all contexts, missing the subtle distinction that could be crucial in diagnosing conditions like uveitis. Similarly, in Portuguese, the term \u201cmancha\u201d can refer to both a \"spot\" and a \"stain,\" leading to ambiguous interpretations in descriptions of retinal abnormalities.\nWe observed that cultural context significantly influenced the models' responses, particularly in post-operative-related queries. When asked about post-operative care in Spanish, GPT-4 emphasized the role of family support, reflecting cultural norms in many Spanish-speaking countries. However, this emphasis was notably absent in English responses, which focused more on clinical follow-up procedures. This discrepancy highlights the models' inconsistent incorporation of cultural nuances in medicine across languages. In Filipino CoT histories, we noticed a tendency for all models to use more colloquial language when discussing symptoms, often employing idiomatic expressions that lacked the precision required in medical communication. For example, it used \"parang may lumutang na langaw\" (like there's a floating fly) to describe a visual disturbance, an expression that doesn't directly translate to the medical term \"floaters\" used in English.\nThese qualitative observations highlight not just linguistic barriers but also the need for models to understand and appropriately navigate cultural contexts, idiomatic expressions, and region-specific health concerns across different languages. Such nuanced understanding is needed for the responsible and effective deployment of AI in global healthcare settings."}, {"title": "Debiasing LLMs for Multilingual Ophthalmology QA", "content": "Our evaluation of LLMs in multilingual ophthalmology question-answering reveals several existing challenges. Primarily, LLMs exhibit a lack of language proficiency in LMIC languages, compounded by insufficient medical domain knowledge. This knowledge gap often causes models to generate plausible but incorrect reasoning. Another contributor to LLM's poor performance is that LLMs often struggle with understanding nuanced linguistic differences across languages, which can lead to misinterpretations in medical contexts (Zhu et al. 2024)."}, {"title": "Baselines", "content": "Several baseline approaches have been proposed to address these challenges, each with its own limitations. To tackle language differences, methods such as direct inference and pre-translation have been explored. Direct inference attempts to prompt the LLM to process and respond in the target language directly, but this approach may be ineffective for complex medical queries in diverse languages where the model's proficiency is limited. Pre-translation, which"}, {"title": "CLARA: Cross-LinguAl Reflective Agent", "content": "Inspired by the qualitative and quantitative findings, we propose CLARA (Cross-Lingual Reflective Agentic system), a novel method to address bias in multilingual ophthalmology question-answering. CLARA combines pre-translation, evaluation, corrective RAG, web search, and query transformation within an agentic framework to enhance the robustness and accuracy of responses across diverse languages. Inspired by the concept of agentic AI(Talebirad and Nadiri 2023; Zhao et al. 2024a), CLARA employs multiple specialized agents that collaborate to process and respond to complex queries. Figure 3 illustrates our pipeline.\nTranslation and Initial Evaluation Agent The process begins with the Translation Agent converting the input query to English, standardizing inputs across languages. An Evaluation Agent, functioning as a meta-cognitive component, then performs a dual assessment: first, it evaluates the translation quality and certainty; second, it gauges the LLM's overall certainty about the medical question, identifying potential issues with jargon or contextual understanding. This comprehensive evaluation determines the subsequent actions in the pipeline.\nKnowledge Augmentation Agent Based on the Evaluation Agent's assessment, CLARA's Knowledge Agent triggers one of the two following RAG approaches. This involves querying a vector embedding database, with weights assigned based on the uncertainty level of different query parts. For parts of the query where translation uncertainty is expressed, a weighted RAG approach is used. We add an additional relevance score to the full-original-query score, here the add-on relevance score $R_i$ for each retrieved document $i$ is calculated as:\n$R_i = \\sum(w_j * sim(q_j, d_i))$\nwhere $w_j$ is the weight of query part $j$, and $sim(q_j, d_i)$ is the cosine similarity between query part $j$ and document $i$."}, {"title": null, "content": "For parts requiring additional context or containing domain-specific jargon, a reweighted RAG approach is applied as well. We perform jargon handling by our Knowledge Augmentation agent, where domain-specific terms are identified and expanded using a specialized ophthalmology dictionary. Key ophthalmological concepts are enriched with relevant contextual information and formatted next to the concept in the prompt. The reweighted relevance score $RR$ for each document i is then calculated as:\n$RR_i = \\sum(w_j * sim(q_j, d_i)) + \\sum(w_k * sim(j_k, d_i))$\nwhere $w_k$ is the weight assigned to jargon term k, and $j_k$ is the expanded definition or context for that term.\nWe curated a diverse set of RAG corpora from three distinct sources, each contributing unique aspects of medical and general knowledge. PubMed provides a comprehensive repository of biomedical abstracts, widely recognized in the field(Canese and Weis 2013). To ensure in-depth, domain-specific knowledge, we incorporated medical textbooks. Additionally, we included Wikipedia to cover general knowledge aspects to broaden the width of our knowledge base(Vrande\u010di\u0107 and Kr\u00f6tzsch 2014). Full details of our RAG sources can be found in Table 4. We use MedCPT-Query-Encoder(Jin et al. 2023) as our RAG retriever in our model, which is specifically designed for medical domain queries and helps ensure more accurate and relevant retrievals for ophthalmological content.\nIterative Evaluation (of Retrieval) and Refinement Agent CLARA employs an iterative evaluation process where a second Evaluation Agent, acting as a critic, assesses the relevance and utility of the retrieved documents. This agent determines whether the retrieved and augmented information is useful for answering the query. If the information is deemed insufficient or irrelevant, it is discarded, and the system's Web Search Tool (using Tavily API) initiates a search for additional information. This iterative process continues until the Evaluation Agent determines that sufficient relevant information has been gathered or a maximum iteration limit = 5 is reached. Then all collected information along with the translated question is passed for final question answering.\nQuestion Rewriting Agent When the maximum rounds are reached, and RAG and web searches fail to yield useful information, CLARA triggers its Rewriting Agent. This situation often arises due to complex, nested information in ophthalmological queries. For instance, consider an ophthalmological examination case that states: \"Bilateral papilledema noted, more pronounced in the right eye with flame-shaped hemorrhages, the left eye showing early cotton wool spots and an area of possible choroidal neovascularization temporal to the macula.\" Here, the right eye's information is omitted and may cause confusion. Our Rewriting Agent recomposes the translated question and passes the rewritten question back to the first evaluation agent, addressing the limitation in complex information extraction from the query. This structured rewriting helps the LLM to process intricate ophthalmological queries and allows for more targeted information retrieval in subsequent RAG iterations.\nThrough this multi-agent, reflective process, CLARA aims to mitigate biases and enhance accuracy in multilingual ophthalmology QA, addressing the challenges of language diversity and domain-specific knowledge in medical AI."}, {"title": "Debias Results Analysis and Ablation Studies.", "content": "Current de-bias methods are not sufficient to address the performance disparity between HICs and LMICs languages. The results in Table 3 demonstrate the varied effectiveness of debiasing techniques for multilingual ophthalmology question-answering using GPT-3.5 and GPT-4, revealing important insights when analyzing both performance gaps (relative to English) and absolute accuracy. Direct Inference, the baseline approach, exhibits the largest performance gaps between English and LMIC languages, particularly for Filipino (11.6% for GPT-4, 11.4% for GPT-3.5), Mandarin (11.0% for GPT-4, 13.9% for GPT-3.5), and Hindi (12.8% for GPT-4, 9.0% for GPT-3.5). This method's overall accuracy is also the lowest across languages, leaving lower-resource languages at a significant disadvantage.\nWeb-ToolCall demonstrates inconsistent results, both in terms of fairness gap reduction and absolute performance. For GPT-3.5, it hurts the performance of French and Mandarin by 0.6% and 1.2% and offers no value for Hindi. For GPT-4, while it improves absolute performance for most languages, we observe a concerning larger performance gap for Spanish, Portuguese, and French. This inconsistency highlights the unreliability of Web-ToolCall, suggesting that web search processes reliant on scarce LMIC medical text may introduce noisy information, particularly for non-English queries. Translate-COT (Shi et al. 2022; Ruder et al. 2021) shows more consistent improvements in both gap reduction and absolute performance across languages for both models. Notably, for GPT-4, it helps Mandarin, Hindi and Filipino pass the benchmark (60%). However, it still falls short of achieving parity for Hindi and Filipino, especially for less advanced LLMs like GPT-3.5.\nOur proposed method demonstrates superior performance in both aspects. It consistently reduces performance gaps across all languages while simultaneously improving absolute accuracy, even for English. Compared to Translate-COT, our method not only narrows the Filipino-English gap from 7.9% to 5.1% but also boosts Filipino performance from 61.6% to 67.1%. Similar improvements are seen in Mandarin and Hindi.\nThe ablation study for GPT-4 reveals the incremental benefits of each component in CLARA. Details of how each component is implemented is included in Appendix. Translation alone yields substantial improvements, particularly for lower-resource languages, with Filipino, Mandarin, and Hindi seeing increases of 9.8%, 13.8%, and 14.7%, respectively. Web search capabilities add modest but consistent gains across languages when coupled with Translation, indicating that English-based web retrieval is more reliable than LMIC language-based. Basic RAG further enhances performance, with Filipino, Chinese, and Hindi improving by an additional 0.7%, 1.8%, and 1.9%. The corrective-RAG component shows language-dependent improvements, most notably for Filipino (+2.4%). The full CLARA system achieves the best results, with Filipino, Mandarin, and Hindi reaching 67.1%, 70.2%, and 68.4%. Importantly, CLARA also improves English performance from 66.4% to 72.2%, indicating enhanced overall model capability."}, {"title": "Conclusion and Broader Impacts", "content": "Our work contributes to more equitable access to medical information across diverse linguistic communities, potentially enhancing healthcare delivery in resource-limited settings. While our method improves performance across languages, the persistent limitations in less advanced models like GPT-3.5 raise ethical concerns about deploying medical AI systems in the real world. This underscores the need for continued research and careful consideration of AI's role in healthcare."}]}