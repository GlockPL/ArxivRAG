{"title": "WaikVLM: Aid Visually Impaired People Walking by Vision Language Model", "authors": ["Zhiqiang Yuan", "Ting Zhang", "Jiapei Zhang", "Jie Zhou", "Jinchao Zhang"], "abstract": "Approximately 200 million individuals around the world suffer from varying degrees of visual impairment, making it crucial to leverage AI technology to offer walking assistance for these people. With the recent progress of vision-language models (VLMs), employing VLMs to improve this field has emerged as a popular research topic. However, most existing methods are studied on self-built question-answering datasets, lacking a unified training and testing benchmark for walk guidance. Moreover, in blind walking task, it is necessary to perform real-time streaming video parsing and generate concise yet informative reminders, which poses a great challenge for VLMs that suffer from redundant responses and low inference efficiency. In this paper, we firstly release a diverse, extensive, and unbiased walking awareness dataset, containing 12k video-manual annotation pairs from Europe and Asia to provide a fair training and testing benchmark for blind walking task. Furthermore, a WalkVLM model is proposed, which employs chain of thought for hierarchical planning to generate concise but informative reminders and utilizes temporal-aware adaptive prediction to reduce the temporal redundancy of reminders. Finally, we have established a solid benchmark for blind walking task and verified the advantages of WalkVLM in stream video processing for this task compared to other VLMs. Our dataset and code will be released at anonymous link https://walkvlm2024.github.io.", "sections": [{"title": "1. Introduction", "content": "Approximately 200 million people worldwide suffer from varying degrees of visual impairment, with 36 million completely blind [1, 2]. These visually impaired people (VIPs) are facing severe challenges in daily activities such as walking, which may be alleviated by contemporary artificial intelligence technologies [3, 4].\nThe current walking assistance works primarily concentrate on electronic assistive devices, sensory substitution devices, and computer vision-based assistive systems [5\u20137]. Among them, vision-based assistive systems can be roughly divided into detection-based methods and semantic-based methods [8\u201310]. Detection-based methods"}, {"title": "2. Related Work", "content": "Vision Datasets for Blind Walking. Existing datasets for blind walking can be roughly divided into two types: detection-based [8, 22-24] and semantic-based [9, 13]. Detection-based datasets have been extensively studied in the blind walking, where researchers utilize these datasets to train the obstacle detection model, thereby reducing the accident rate of VIPs in this task. For example, Zhang et al. [22] recently developed a TP-Dataset for detecting visual tactile paving surfaces and offered guidance for the visually impaired through provide walking routes. Islam et al. [23] introduced a dataset for improving real-time object recognition systems to aid VIPs in navigation tasks, which contains 90 object annotations from 31 video clips. Compared with detection-based datasets, semantic-based datasets are relatively rare, which contain question-answering properties and provide an enhanced human-computer interaction experience. Gurari et al. [9] constructed a VQA dataset for VIPs, which contains 31k visual questions, each with 10 crowdsourced answers. In addition, some researchers have constructed several self-built question-answer datasets with specific attributes during their studies [3, 13], however, these self-built datasets are not open-sourced and are relatively small in scale, making them unsuitable for large-scale and unified benchmarking.\nVision-based Methods for Blind Walking. Similar to the division of datasets, the vision-based methods that help VIPs walking can also be divided into detection-"}, {"title": "3. Walking Awareness Dataset", "content": "In this section, we have constructed a walking awareness dataset to provide open data support for blind walking task."}, {"title": "3.1. Data Collection", "content": "The WAD dataset has a wide range of geographical sources, which originate from 10 different locations in Europe and Asia. 20% of the original data in the WAD dataset comes from the annotators' recordings, and the rest comes from YouTube\u00b2. During the recording, six recorders positioned the camera at a height corresponding to chest level, employing focal lengths of 13mm, 20mm, and 26mm, as well as resolutions ranging from 1080p to 4k at 60fps, to enhance the variability of the data. Lastly, we have amassed approximately 13 hours of walking video, and see Appendix A for the duration of data gathered from various regions."}, {"title": "3.2. Annotation Strategy", "content": "Figure 2 shows the overall annotation pipeline of walking awareness dataset. Next, we will elaborate from two aspects: scene annotation and response annotation.\nScene annotation. Scene annotation aims to label the inherent attributes of the current scene. We requested nine annotators to label the video scene in terms of weather conditions, location type, traffic flow rating, danger level, and scene description. When outdoors, weather conditions are divided into six categories such as sunny and rainy, while the status is empty when indoors. The location type is divided into eight categories, such as corridors and pedestrian walkway. The traffic flow rating is divided into three levels, which are defined based on the person number in the video stream. The danger level is defined as the walking hazard in the current scene, which is qualitatively divided by the traffic flow rating and road smoothness. The scene description is an overview of the current environment, including an expansion on factors such as pedestrian flow, vehicle traffic, road conditions, and the surrounding environment. Subsequently, we employed the open-world detection model [39] for the preliminary detection of targets, and carried out a corresponding human review to uphold the result accuracy."}, {"title": "3.3. Dataset Analysis", "content": "Figure 5 shows a sample of the WAD dataset, and we divide the annotations into three parts following lower to higher levels: perception, comprehension, and decision. The perception label reflects the basic attributes of the video, such as obstacle location, weather conditions, etc., while the comprehension label reflects the model's understanding of the entire scene. The decision label contains reminder and QA, reflecting the model's decision on the user's walking based on its understanding of the current scenario."}, {"title": "3.4. Possible Sources of Bias", "content": "Although the WAD dataset is collected from a wide range of geographical sources, we are aware of a few biases in our dataset. The regions are still limited, which is still a long way from complete coverage of the globe. The position of the camera and the divergence of focal length are also concerns for us, which need to obtain more general data to compensate for this. In addition, the linguistic preferences of the annotators can introduce specific biases into the generated reminder, which implies that during the walking process, the model might provide information that are more appropriate for the area where the annotation was made."}, {"title": "4. WalkVLM", "content": "This section proposes WalkVLM, attempting to empower the blind walking task using a vision-language model based on the WAD dataset. The overall architecture of WalkVLM is shown in Figure 7. We will start with problem formulation and proceed with hierarchical planning and temporal-aware adaptive prediction to generate concise and opportune walking reminders."}, {"title": "4.1. Problem Formulation", "content": "We aim to steer a VLM to process video streams, enabling it to provide walking reminders that include temporal attributes, and to enable the model to answer specific questions in human-machine interactions. Specifically, at time \\( t_0 \\), given the newly appeared frames \\( [I_{t-N}, ..., I_{t-1}, I_{t_0}] \\), categlory and obstacle position in the image \\( [O_{t-N}, ..., O_{t-1}, O_{t_0}] \\), VLM is hoped to generate a concise and informative reminder \\( T_{t_0} \\) based on visual information. During walking, VIPs can also raise a question \\( Q_{t_0} \\) to communicate with the VLM at any time, so as to inquire about information such as the current scene and route. Additionally, since generating reminders at every frame may lead to a poor walking guidance experience and impose significant real-time processing pressure on hardware,\nWalkVLM needs to be able to predict the current VLM trigger state \\( s_{t_0} \\) based on historical states \\( [S_{t-n}\u2026\u2026, S_{t-1},S_{t_0}] \\) and the previous N frames, so as to choose specific moments to output reminders."}, {"title": "4.2. CoT-Based Hierarchical Planning", "content": "We attempt to make VLM conduct step-by-step derivation by a Chain of Thought (CoT) [41], enabling it to summarize from comprehensive information such as the static attributes and the summary of the scene, thereby refining out concise and informative reminders. The model architecture integrates a vision transformer encoder and a large language model (LLM). The vision encoder generates image tokens, while an attention-based extractor aligns these tokens with the LLM, enabling comprehensive understanding and information processing. WalkVLM combines multi-frame information to make reminders, ensuring that the model has a comprehensive perception of the environment.\nWe divide the process of reminder generation into three levels: perception, comprehension, and decision. At the perception level, the model extracts static visual attributes from the current frame, such as location type, weather conditions, and traffic flow rating. To enhance the VLM model's focus on significant elements and improve visual perception accuracy, we incorporate a priori-object location module (POLM). The POLM initially uses a generic object detector [39] to identify and locate objects in the scene, then filters them based on size and confidence scores to highlight crucial items that reflect road conditions and potential danger. The filtered information and basic environmental attributes provide the necessary input for the model to perceive the external world. At the comprehension level, the model integrates all outputs from the perception layer, merging local detection results and fragmented scene information into a comprehensive global summary. Relying on the capabilities of the VLM and the detailed attributes from the perception stage, this stage ensures that the model has a clear understanding of the current environment. At the decision level, we focus on training the WalkVLM model to achieve visual QA and reminder. At this stage, the model already possesses an understanding of the static attributes and overall situation of the environment. Therefore, with appropriate guidance, the model is expected to briefly analyze potential hazards in the scene.\nDuring training, we adopted a CoT approach to gradually feed information from three levels into the VLM, and during testing, we let the model predict the aforementioned attributes and generate the corresponding responses."}, {"title": "4.3. Temporal-Aware Adaptive Prediction", "content": "Although VLMs are capable of scene parsing across multiple frames and generating the required output, directly applying them to video streaming will lead to unavoidable issues. For instance, when utilizing VLM to generate walking"}, {"title": "5. Experiments", "content": "5.1. Settings\nModels & Details. WalkVLM is implemented with the MiniCPM-V2.6 model [36], which is an 8B multimodal model built upon Qwen2-7B [46]. We add LoRA to all the linear layers of MiniCPM-V2.6 with a rank of 64, while maintaining the video stream sampling rate of 2 FPS. The number of historical frames N is set to 3, and the visual extraction backbone in the TAP module is ConvNext3D [47]. We compared WalkVLM with multiple popular multimodal models, including GPT-40 [44], Qwen2-VL(7B) [45], MiniCPM-V2.6(8B) [36], DeepSeek(1.3B&7B) [42], Yi-VL(6B) [43]. All the prompts of the large models used in this paper can be found in Appendix B.\nMetrics. We use the following metrics to evaluate the models: (a) ROUGE. This metric measures the similarity between the generated text and the reference text by comparing overlapping words or phrases, including ROUGE-1, ROUGE-2, and ROUGE-L [48]. (b) TF-IDF Similarity (TF-IDF). Combine term frequency and inverse document frequency to evaluate the weight of words, represent the text as a TF-IDF vector, and then measure the semantic similarity between texts [49]. (c) GPT Score. GPT4 is used to evaluate the superiority ratio between the generation results of different multimodal models and the ground truth (GT) [50, 51]. (d) Temporal Redundancy F1-Score (TRF). Given the historical model state and historical frames, let the model predict the danger level of the current moment,"}, {"title": "5.2. Quantitative Results", "content": "Table 2 presents the quantitative metrics of different models on the reminder and QA task. On the ROUGE metric, WalkVLM has achieved the best results in both tasks, verifying that the model's output is closest to the GT at the word granularity. On the TF-IDF metric for measuring semantic similarity, WalkVLM performs the best in reminder tasks, indicating that the model can generate more concise and accurate results like GT. While in QA tasks, WalkVLM's performance on TF-IDF scores does not stand out significantly. This could be attributed to the fact that during training, the model is encouraged to generate concise answers, which may inadvertently diminish its capacity to offer elaborate explanations of the questions. The GPT score represents the overall evaluation of the LLM on the generated results and the GT. WalkVLM outperforms other models such as GPT-40 in terms of GPT scores for reminder and QA tasks, validating that the model's output has the most consistent distribution with the GT."}, {"title": "5.3. Qualitative Results", "content": "Figure 9 presents the visual comparison in reminder task between different VLM models. Compared to other methods such as GPT-40, WalkVLM can generate more concise and informative responses, thus providing a better experience for users. In the left case, whereas other models offer highly detailed responses, WalkVLM simply provides a concise prompt to the user, effectively highlighting the crucial aspect. As in the right case, WalkVLM perceives the car coming from the one o'clock direction and conveys the focus to the user, which other models have not accomplished. Figure 8 shows a qualitative comparison of GPT-40 and WalkVLM in terms of temporal redundancy. Our model triggers VLM with lower temporal redundancy and can provide information to users in a more timely manner. Appendix C presents more qualitative results, including the comparison with other VLMs on actual video streams."}, {"title": "5.4. Subjective Results", "content": "As illustrated in Table 4, we requested nine annotators to perform a subjective evaluation of various VLM models with respect to language conciseness and semantic similarity to the GT. Participants are required to rank the results individually, and we use the top-1 superiority ratio to eval-"}, {"title": "5.5. Ablative Study", "content": "The ablation study of WalkVLM is shown in Table 5 to verify the effectiveness of CoT-based hierarchical planning (CHP) and POLM prior. We conducted three sets of ablation experiments: (a) w/o CHP. Remove the CHP mechanism and generate reminder directly based on the input visual information. (b) w/o Pos Prior. Remove the approximate position of significant obstacles in POLM. (c) w/o POLM Prior. Remove the input filtered target exact location and category. In these experiments, when the CHP mechanism was removed, the model's degradation was significant, which may be due to the model's inability to fully"}, {"title": "6. Conclusion", "content": "To fulfill the mission of technology for good, this paper presents WalkVLM, a vision-language model for blind walking task, which employs chain of thought for hierarchical planning to generate concise but focused reminders, and utilizes temporal-aware adaptive prediction to reduce the redundancy of reminders in the time series. Additionally, we have constructed a diverse, extensive, and unbiased walking awareness dataset, aimed at providing a more robust data foundation for this field. Comprehensive experiments show that, compared to other VLM models, WalkVLM can generate more concise reminder and better temporal adaptability when handling video streaming in blind walking task."}, {"title": "7. Limitations", "content": "This paper proposes a WAD dataset and systematically establishes the blind walking task based on the vision-language model, thereby setting up an extensive benchmark and offering valuable data support to this field. Although the WAD dataset covers dozens of cities, its generalization capability is still relatively limited in practical applications, making the collection of additional data an essential endeavor. Moreover, we devised the WalkVLM to make the reminders concise and opportune, but still leave considerable room in inference efficiency."}]}