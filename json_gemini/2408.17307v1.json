{"title": "Hybridizing Base-Line 2D-CNN Model with Cat Swarm Optimization for Enhanced Advanced Persistent Threat Detection", "authors": ["Ali M. Bakhiet", "Salah A. Aly"], "abstract": "In the realm of cyber-security, detecting Advanced Persistent Threats (APTs) remains a formidable challenge due to their stealthy and sophisticated nature. This research paper presents an innovative approach that leverages Convolutional Neural Networks (CNNs) with a 2D baseline model, enhanced by the cutting-edge Cat Swarm Optimization (CSO) algorithm, to significantly improve APT detection accuracy. By seamlessly integrating the 2D-CNN baseline model with CSO, we unlock the potential for unprecedented accuracy and efficiency in APT detection. The results unveil an impressive accuracy score of 98.4%, marking a significant enhancement in APT detection across various attack stages, illuminating a path forward in combating these relentless and sophisticated threats.\nIndex Terms-Advanced Persistent Threats (APTs), Cat Swarm Optimization (CSO), Optimized Convolution model, Hybridization.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's digitally connected world, the protection of sensitive information and critical infrastructure is paramount. As technology advances, so do the threats from those seeking to exploit vulnerabilities. Among these threats, Advanced Persistent Threats (APTs) pose a significant challenge in cyber security and information protection.\nMachine learning is increasingly adopted by information security researchers and enterprises due to its potential for detecting assaults, particularly those that evade conventional signature-based intrusion detection systems. Detecting APTs is akin to finding a needle in a haystack; their activities closely mimic legitimate network traffic, rendering traditional detection methods inadequate. In this challenging landscape, the integration of cutting-edge technologies is imperative. Deep learning, with its prowess in processing vast volumes of data, emerges as a beacon of hope. However, harnessing the full potential of deep learning for APT detection requires more than just advanced neural networks; it demands the synergy of modern optimization algorithms.\nUnlike conventional attacks that are easily detected by signature-based intrusion detection systems, APTs are characterized by their sophistication, persistence, and insidious nature. These stealthy adversaries operate with a singular objective to infiltrate networks, remain undetected, and quietly exfiltrate sensitive data over an extended period. In doing so, they challenge the very foundations of cybersecurity and pose a substantial risk to the confidentiality, integrity, and availability of data.\nThis research introduces a pioneering approach that combines Convolutional Neural Networks (CNNs) with Cat Swarm Optimization (CSO)\u2014an optimization algorithm inspired by the collective behavior of cats. This fusion aims to revolutionize APT detection by enhancing accuracy, reducing false positives, and improving the efficiency of cybersecurity operations. We utilize the \"DAPT 2020\" dataset, a meticulously constructed benchmark reflecting real-world APT scenarios. The cornerstone of our endeavor is the 2D-CNN baseline model, further enhanced through hybridization with CSO. The paper is structured as follows: Section II presents related work. Section III describes the DAPT 2020 dataset. Section IV details our model and algorithm procedures. Section V discusses simulation studies for the proposed algorithms. Finally, Section VI concludes the paper."}, {"title": "II. RELATED WORK", "content": "Chu et al. [4] introduced the first Cat Swarm Optimization (CSO) met-heuristic optimization technique in 2006. CSO, inspired by the collective behavior of cats, has shown remarkable capability in solving complex optimization problems and forms the backbone of our approach to enhancing APT detection.\nA comprehensive review by the authors in [5] evaluated CSO's developments and applications across various fields. CSO operates in two independent modes: seeking mode and tracing mode. The authors tested CSO on 23 classical benchmark functions and 10 modern benchmark functions, comparing its performance against three powerful optimization algorithms: Dragonfly Algorithm (DA), Butterfly Optimization Algorithm (BOA), and Fitness Dependent Optimizer (FDO).\nRecently, the authors in [6] created a dataset encompassing the entire life cycle of an APT attack, focusing on detection during different phases such as reconnaissance, initial compromise, lateral movement, and data exfiltration. They"}, {"title": "III. MODEL DATASETS", "content": "Datasets are essential for creating machine learning models that can identify sophisticated and complicated dangers like Advanced Persistent dangers (APT). APT-datasets, however, that may be utilized for modeling and detecting APT assaults are not yet available.\nSeveral datasets have been proposed for network intrusion detection systems and cybersecurity, such as CICIDS 2018 [25], CICIDS 2017 [26], UNB 2015 [27], and others [1].\nOur research centers on the development of a 2D-CNN baseline model trained on the DAPT 2020 dataset. Recognizing the capability of CNNs to capture spatial features within network traffic data, we aim to optimize this baseline model. The DAPT 2020 dataset was created using network traffic collected over five days, with each day simulating three months of real-world traffic. This dataset is designed to help researchers understand anomalies, the relationships between different attack vectors, and hidden correlations that aid in early APT detection.\nGeneric intrusion datasets have three major limitations:\n1) They only include attack traffic at external endpoints, limiting their applicability for APTs, which also involve internal network attack vectors.\n2) They make it difficult to distinguish between normal and anomalous behavior, making them unrepresentative of the sophisticated nature of APT attacks.\n3) They lack the data balance characteristic of real-world scenarios, making them suitable for supervised models but inadequate for semi-supervised learning.\nThe DAPT 2020 dataset addresses these issues by including assaults categorized as Advanced Persistent Threats (APTs). These assaults are difficult to differentiate from legitimate traffic flows when looking at the raw feature space and include both internal (private network) and public-to-private interface traffic. We benchmark the DAPT 2020 dataset on semi-supervised models, demonstrating that they perform poorly in detecting attack traffic at various stages of an APT due to substantial class imbalance."}, {"title": "IV. MODEL AND ALGORITHM PROCEDURE", "content": "In this section, we will demonstrate the methodology of our model. In our methodology, we initiate by initializing N cats representing potential solutions within the optimization space. These cats undergo evaluation based on a fitness function, and the top-performing cats are stored in memory. Our model functions in two primary modes: Seeking Mode, where individual cats explore the solution space, and Tracing Mode, involving collaborative refinement of solutions among cats. To validate the efficacy of anomaly detection, we employ the CSO-2D-CNN model, as illustrated in Figure 1.\nThe anomaly detection approach entails training the CSO-2D-CNN model on normal network traffic data and evaluating incoming traffic packets during testing. In the training phase, the model learns baseline patterns from normal network traffic data. Subsequently, during the testing phase, incoming traffic packets traverse through the CSO-2D-CNN model, and their normalized reconstruction error is computed. If the error surpasses a predefined threshold, the packet is deemed anomalous; otherwise, it is classified as normal network traffic.\nOur model development workflow encompasses several crucial steps. Initially, we perform data preprocessing tasks to prepare the dataset for model training. Next, we engineer features to capture pertinent information from the network traffic data. Following this, the dataset undergoes splitting into training and validation sets, with the CSO-2D-CNN model being trained on the training data. The trained model is then validated on the validation set to ensure generalization. Finally, we evaluate the performance of the CSO-2D-CNN model using precision scores to gauge its effectiveness in anomaly detection.\nThis methodology delineates our approach to harnessing Cat Swarm Optimization alongside 2D Convolutional Neural Networks for robust and efficient anomaly detection in network traffic as shown in Alg.1.\nLearning Rate Reduction: Learning Rate Reduction"}, {"title": "Algorithm 1 CSO-2D-CNN Model Training and Evaluation", "content": "Require: DAPT2020 dataset on semi-supervised models\nEnsure: Accuracy, Loss and saved model (CSO-2D-CNN)\n1: Step 1: Model Initialization\n2: Initialize parameters for 2D-CNN model.\n3: Initialize parameters for CSO alg., including fitness func. and main optimiz. func.\n4: Hybridize CSO alg. with 2D-CNN to create CSO-2D-CNN.\n5: Step 2: Data Preprocessing and Analysis\n6: Perform data preprocessing on DAPT2020 dataset.\n7: Apply feature engineering and selection techniques tailored for CSO-2D-CNN model.\n8: Encode data and scale it appropriately for CSO-2D-CNN model training.\n9: Step 3: Model Training and Evaluation\n10: Train CSO-2D-CNN model on the preproc. dataset.\n11: - Validate performance of the CSO-2D-CNN model using appropriate validation techniques.\n12: Evaluate performance of the CSO-2D-CNN model.\n13: Step 4: Model Saving\n14: - Save CSO-2D-CNN model with its trained parameters.\n15: Step 5: Documentation\n16: Document the alg. used for the CSO-2D-CNN model.\ndynamically adjusts the learning rate during training to enhance model performance. The technique operates based on the validation accuracy, where adjustments occur when improvements plateau. A patience of 2 epochs is set to prevent premature reductions, allowing the model to explore different weight configurations. Verbosity is configured at 1 to provide feedback on learning rate adjustments during training. Additionally, the learning rate is reduced by a factor of 0.5, facilitating more reliable convergence by halving the rate, enabling the optimizer to take smaller steps during gradient descent.\nMinimum Learning Rate: We have specified a minimum learning rate of 0.00001. This ensures that the learning rate won't drop below this value, preventing it from becoming too small.\nCallbacks: Callbacks are essential tools for monitoring and optimizing the training process. I've configured several callbacks to enhance the training of my baseline deep learning model:\nReduce LR On Plateau: This callback dynamically reduces the learning rate as needed to achieve better model convergence. It's especially useful when training reaches a plateau in terms of accuracy.\nEarly Stopping: Early stopping is set with a patience of 2 epochs. If the validation accuracy doesn't improve for two consecutive epochs, training will be halted early. This helps prevent overfitting and saves time by stopping training when further improvements are unlikely.\nModel Checkpoint: This callback saves the model's weights to a file during training. It uses a specific naming format that includes the epoch number and validation loss. The model with the best validation accuracy will be saved as \"Ann_model_Dense_lab.h5,\u201d allowing you to retrieve the best-performing model after training.\nLoss Function: sparse_categorical_crossentropy serves as a pivotal component in deep learning model training. You've selected \"sparse_categorical_crossentropy\" as the loss function for our baseline model, a common choice for multi-class classification tasks involving categorical (discrete) target variables. In our scenario, this loss function aptly addresses the classification of transactions into fraudulent or non-fraudulent categories (binary classification) based on our model's output.\nOptimizer: Adam is a pivotal component in updating model parameters during training. Adam, short for Adaptive Moment Estimation, is the chosen optimizer for our model. It amalgamates the advantages of AdaGrad and RMSprop, adapting learning rates for each parameter individually. Renowned for its efficiency and rapid convergence, Adam proves advantageous in training deep neural networks, aligning with a diverse array of deep learning tasks.\nEpochs: 5 signifies the frequency with which our model traverses the entire training dataset during training. Our configuration sets the number of epochs to 5, indicating that our model iterates over the dataset 5 times, refining its parameters to minimize the chosen loss function. The selection of epochs hinges on the problem's complexity and our model's convergence behavior. Opting for a relatively low number of epochs can be advantageous if our model converges swiftly without succumbing to overfitting.\nBatch Size: 640 dictates the quantity of data samples processed in each forward and backward pass during a singular training iteration. With a batch size set at 640, our model processes 640 data samples before parameter updates. The chosen batch size significantly influences training speed, memory demands, and the quality of model updates. Opting for a batch size of 640 strikes a balance between computational efficiency and model convergence, representing a common choice in the field.\nThese configurations synergistically enhance the efficiency and efficacy of model training. Learning rate reduction facilitates convergence to an optimal solution, while callbacks mitigate overfitting and facilitate the preservation of the best-performing model for future utilization. Employing these"}, {"title": "V. RESULTS OF THE APT ATTACKS DATASET", "content": "In this section, we will demonstrate the results and performance of our model. We utilize the Base-Line model, a 2D-CNN model, on the numeric dataset, and optimize it with the Cat Swarm Optimization (CSO) Algorithm, resulting in the proposed CSO-2D-CNN model. Following data processing and splitting, pertinent details about the dataset are as follows: it comprises 75 features and is divided into 32 parts. These parts include a training set with a shape of (55262, 75) accounting for 70% of the dataset, a validation set with a shape of (6141, 75) comprising 10%, and a testing set with a shape of (15351, 75) encompassing 20%.\nFor evaluating the CSO-2D-CNN model's performance, we employ four key evaluation metrics: accuracy, precision, recall, and F1 score. These metrics are derived by assessing the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). Specifically, Accuracy represents the likelihood of correct predictions made by the CSO-2D-CNN model across all sample data, providing an overarching measure of model prediction accuracy. The Accuracy measurement can be evaluated as:\n$Accuracy = \\frac{TP+TN}{TP+FP+TN + FN}$\nPrecision Recall (PR) represents the proportion of true attacks among all the samples detected as APT attacks, and is computed by # (Anomalous Traffic) /#(Dataset), where #(\u2022) denotes the cardinality, see [1]. The Precision measurement can be evaluated as:\n$Precision = \\frac{TP}{TP+FP}$\nRecall represents the proportion of detected attacks among all the true attacks. The Recall measurement can be evaluated as:\n$Recall = \\frac{TP}{TP+FN}$\nThe F1-score is calculated using Precision and Recall and represents their harmonic mean. It can assess the overall performance of the model.\n$F1 \\ score = \\frac{2* Precision * Recall}{Precision + Recall}$\nThe Sensitivity measurement can be evaluated as:\n$Sensitivity = \\frac{TP}{TP+FN}$\nThe Specificity measurement can be evaluated as:\n$Specificity = \\frac{TN}{TN+FP}$\nThe formula for Cohen's kappa is calculated as:\n$Kappa = \\frac{Po - Pe}{1- Pe}$"}, {"title": "VI. CONCLUSION", "content": "In conclusion, this paper presents a pioneering framework demonstrating the symbiotic relationship between modern optimization algorithms and deep learning models for APT detection. The integration of BaseLine 2D-CNN models with Cat Swarm Optimization yields promising results, with our"}]}