{"title": "Regret-Free Reinforcement Learning for LTL Specifications", "authors": ["Rupak Majumdar", "Mahmoud Salamati", "Sadegh Soudjani"], "abstract": "Reinforcement learning (RL) is a promising method to learn optimal control policies for systems with unknown dynamics. In particular, synthesizing controllers for safety-critical systems based on high-level specifications, such as those expressed in temporal languages like linear temporal logic (LTL), presents a significant challenge in control systems research. Current RL-based methods designed for LTL tasks typically offer only asymptotic guarantees, which provide no insight into the transient performance during the learning phase. While running an RL algorithm, it is crucial to assess how close we are to achieving optimal behavior if we stop learning.\nIn this paper, we present the first regret-free online algorithm for learning a controller that addresses the general class of LTL specifications over Markov decision processes (MDPs) with a finite set of states and actions. We begin by proposing a regret-free learning algorithm to solve infinite-horizon reach-avoid problems. For general LTL specifications, we show that the synthesis problem can be reduced to a reach-avoid problem when the graph structure is known. Additionally, we provide an algorithm for learning the graph structure, assuming knowledge of a minimum transition probability, which operates independently of the main regret-free algorithm.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL) is becoming more prevalent in computing efficient policies for systems with unknown dynamical models. Although experimental evidence highlights the effectiveness of RL in numerous applications, ensuring the safety and reliability of algorithms is imperative for certain safety-critical systems. In such scenarios, the development of algorithms that provide explicit performance guarantees becomes essential.\nAmong the existing performance guarantees for RL, regret minimization has widely been studied recently (Auer et al., 2008; Agarwal et al., 2014; Srinivas et al., 2012; Dann et al., 2017). For an online learning algorithm, intuitively, regret is defined as the difference between the accumulated (expected) rewards collected by an optimal policy and the the algorithm during learning. Existing regret minimization algorithms assume that optimal policies are positional, meaning that optimal policies (deterministically) map every state into a corresponding action. While this suffices for most of basic reward structures, optimal policies may not be positional for more general reward structures. In particular, when the control objective is set by an LTL formula, optimal policy is, in general, not positional.\nRL for LTL specifications has recently become popular. Majority of the existing methods provide no performance guarantees (Icarte et al., 2018; Camacho et al., 2019; Hasanbeig et al., 2019; Kazemi et al., 2022; Hahn et al., 2019; Oura et al., 2020; Cai et al., 2020; Bozkurt et al., 2021; Sickert et al., 2016). Controller synthesis for finite-horizon and infinite-horizon LTL with probably approximately correct (PAC) guarantee has recently been studied (Fu & Topcu, 2014; Voloshin et al., 2022). It is shown that providing PAC guarantees for learning algorithms that target controller synthesis against infinite-horizon specifications requires additional knowledge, such as minimum transition probability (Alur et al., 2022). However, there exists no regret-free RL-based controller synthesis method for LTL tasks.\nIn this paper, we propose an online learning algorithm for control synthesis problems against LTL objectives, which provides sublinearly growing regret bounds. Specifically, we want to measure the performance of the learned control policy during learning. For that, we compare the satisfaction probabilities of an optimal policy and sequence of policies generated by the algorithm during learning.\nWe consider the class of systems, whose dynamics can be captured by a finite MDP with unknown transition probabilities and fixed initial state $s_{init}$. The control objective is to synthesize a control policy that maximizes probability of satisfying a given LTL specification $\\varphi$. Let $\\pi^*$ denote an optimal policy, meaning that applying $\\pi^*$ maximizes"}, {"title": "2. Related Work", "content": "We discuss existing results in four related domains.\nNon-Guaranteed reinforcement learning techniques for LTL specifications. During the course of the past years, lots of efforts have been dedicated into solving the controller synthesis problem for systems modeled as finite MDPs with unknown transition probabilities and have to fulfill certain tasks encoded as formula in LTL. Early results focused only on the finite-horizon subset of LTL specifications. Icarte et al. introduced reward machines, which use finite state automata to encode finite-horizon specifications, along with specialized (deep) Q-learning algorithms to support their approach (Icarte et al., 2018). Camacho et al. later formalized the automatic derivation of reward machines for finite-horizon subsets of LTL (Camacho et al., 2019). The development of efficient and compact automata, such as limit deterministic B\u00fcchi automata (LDBA), for representing LTL formulas has led to significant advances in reinforcement learning for the full class of LTL specifications, including infinite-horizon cases (Sickert et al., 2016). Typically, one has to first translate the given LTL formula into an appropriate automaton, such as a Rabin or limit-deterministic B\u00fcchi automaton, and then compute the product of this automaton with the actual MDP to formulate the final (discounted) learning problem. This formulation ensures that, with a sufficiently large discount factor (dependent on system dynamics and goal specifications), applying standard RL techniques will lead the policy to converge asymptotically to the optimal one (Kazemi et al., 2022; Bozkurt et al., 2021; Oura et al., 2020). However, these methods fail to provide a finite-time performance, and the critical lower bound for the discount factor cannot be known in advance.\nGuaranteed reinforcement learning for LTL specification The two most popular metrics for evaluating the performance of learning algorithms are probably approximately correct (PAC) and regret bounds. In (Fu & Topcu, 2014), the problem of synthesizing controllers for finite-horizon LTL specifications over finite MDPs with unknown transition probabilities was addressed, and a PAC learning algorithm was proposed. However, its sample complexity explicitly depends on the horizon length, making it unsuitable for full"}, {"title": "3. Preliminaries", "content": "3.1. Notation\nFor a matrix $X \\in \\mathbb{R}^{m \\times n}$, we define the $\\infty$-matrix-norm $||X||_{\\infty} := \\max_{1 \\leq i \\leq m} \\sum_{j=1} || X_{ij} ||$. Given two integer numbers $a, b$ s.t. $a \\leq b$, we denote the set of integer numbers $a \\leq 1 \\leq b$ by $[a; b]$.\n3.2. Linear Temporal Logic\nWe consider specifications in the form of formulas in Linear Temporal Logic (LTL). Here, we give a brief introduction to LTL. For detailed syntax and semantics of LTL, we refer to the book by Baier and Katoen (Baier & Katoen, 2008) and references therein. We consider LTL specifications with syntax\n$\\psi := p | \\neg \\psi | \\psi_1 \\land \\psi_2 | \\bigcirc \\psi | \\psi_1 \\mathcal{U} \\psi_2$,\nwhere $p \\subseteq S$ is an element of the set of atomic propositions AP. Let $\\rho = \\rho_0, \\rho_1,...$ be an infinite sequence of elements from $2^{AP}$ and denote $\\rho_i = \\rho_{i}, \\rho_{i+1},...$ for any $i \\in \\mathbb{N}$.\nThen the satisfaction relation between $\\rho$ and a property $\\psi$, expressed in LTL, is denoted by $\\rho \\models \\psi$. We denote $\\rho \\models p$ if $\\rho_0 \\in p$. Furthermore, $\\rho \\models \\neg \\psi$ if $\\rho \\nvDash \\psi$ and $\\rho \\models \\psi_1 \\land \\psi_2$ if $\\rho \\models \\psi_1$ and $\\rho \\models \\psi_2$. For next operator, $\\rho \\models \\bigcirc \\psi$ holds if $\\rho_1 \\models \\psi$. The temporal until operator $\\rho \\models \\psi_1 \\mathcal{U} \\psi_2$ holds if $\\exists i \\in \\mathbb{N}: \\rho_i \\models \\psi_2$, and $\\forall j\\in\\mathbb{N}:0 \\leq j < i, \\rho_j \\models \\psi_1$. Disjunction ($\\lor$) can be defined by $\\rho \\models \\psi_1 \\lor \\psi_2 \\triangleq \\neg (\\neg \\psi_1 \\land \\neg \\psi_2)$. The operator $\\rho \\models \\lozenge \\psi$ is used to denote that the property will eventually happen at some point in the future. The operator $\\rho \\models \\square \\psi$ signifies that $\\psi$ must always be true at all times in the future. We also define $\\psi_1 \\to \\psi_2$ with $\\neg \\psi_1 \\lor \\psi_2$. For a given LTL specification $\\psi$, we can monitor satisfaction of $\\psi$ by running an appropriate automaton $A = (Q, \\Sigma, \\delta, q_{init}, Acc)$, which consists of a finite set of states $Q$, a finite alphabet $\\Sigma = 2^{AP}$, a transition function $\\delta: Q \\times \\Sigma \\to 2^{Q}$, an initial state $q_{init}$, and an accepting condition Acc. For example, the accepting condition in deterministic Rabin automaton (DRA) is in the form of tuples $(J_i, K_i) | i = 1, . . ., m$, consisting of subsets $J_i$ and $K_i$ of $Q$. An infinite sequence $\\rho$ is accepted by the DRA $A$ if there exists at least one pair $(J_i, K_i) \\in Acc$ such that $inf(\\rho) \\cap J_i = \\emptyset$ and $inf(\\rho) \\cap K_i \\neq 0$, where $inf(\\rho)$ is the set of states that appear infinitely often in $\\rho$.\n3.3. MDPs\nLet $\\Delta(X)$ be the set of probability distributions over the set $X$ and AP be a set of atomic propositions. We consider MDPS $M = (S, A, T, s_{init}, AP)$, where $S$ and $A$ denote the finite set of states and actions, respectively, $T: S \\times A \\times S \\to \\Delta(S)$ denotes an unknown transition function,"}, {"title": "3.4. Regret Analysis", "content": "We are interested in synthesizing policies that maximize satisfaction probability of LTL specifications over MDPs with finite set of states and actions and fixed initial state $s_{init}$. Let $v^* (s_{init})$ denote the probability with which the optimal policy $\\pi^*$ satisfies the target specification $\\varphi$, when started at $s_{init}$. Learning takes place over consecutive episodes. For the episode $k$ we define $v_k(s_{init})$ denote the probability of satisfying the target specification $\\varphi$ under the policy $\\pi_k$ in the MDP $M_\\varphi$, when started at $s_{init}$. We measure the success of the learning algorithm through its corresponding regret that is defined as\n$R(K) := \\sum_{k=1}^{K-1} (v^* (s_{init}) \u2013 v_k (s_{init})).$\nIn practice, one is interested to terminate the online algorithm based on a reasonable stopping criterion. Let us define the normalized regret, as follows:\n$R_a(K) = \\frac{R(K)}{K}$\nAn online algorithm is called regret-free if its corresponding regret grows sublinearly with respect to the number of episodes $K$. Running a regret-free online algorithm enables achieving arbitrary small values of normalized regret. Therefore, one could fix a threshold $\\varepsilon \\in (0, 1)$ and terminate the algorithm once the corresponding normalized regret goes below $\\varepsilon$. Therefore, one can consider the smallest number of episodes $k_{reg} \\in \\mathbb{N}$ after which $R_a(k_{reg}) < \\varepsilon$ with confidence $1 - \\delta$ as a complexity metric for the proposed online learning algorithm, with respect to parameters $\\delta, \\varepsilon \\in (0,1)$. Intuitively, after $k > k_{reg}$ many learning episodes, with confidence $1 \u2013 \\delta$ the average satisfaction probability for the policy computed in the $k$th episode, will be $\\varepsilon$-optimal."}, {"title": "4. Regret-Free Controller Synthesis for Reach-Avoid Specifications", "content": "In this section, we study the controller synthesis problem for MDPs with unknown (but fixed) transition function against reach-avoid specifications. Let $G$ and $B$ be two distinct atomic propositions. We are interested in finding policies which can maximize the satisfaction probability for the (unbounded) reach-avoid specification $\\varphi = \\neg B \\mathcal{U} G$. We let $M = (S, A, T, s_{init}, \\{ G, B \\})$ be the MDP that is resulted by making states within $G$ and $B$ absorbing. Now, we are able to define the problem that is the main subject of study in this section.\nProblem 1. Given an MDP M with unknown transition function, minimum transition probability $p_{min} \\in (0,1)$, a reach-avoid specification $\\varphi = \\neg B \\mathcal{U} G$, and a confidence parameter $0 < \\delta < 1$, find an online learning algorithm such that with confidence greater than $1 - \\delta$ the resulted regret defined by Eq. (1) grows sublinearly with respect to the number of episodes $K$."}, {"title": "4.1. Methodology", "content": "Our proposed algorithm is demonstrated in Alg. 1. We propose our algorithm in the known paradigm of optimism in the face of uncertainty. Learning takes place over consecutive environmental episodes. Each episode is a finite sequence $s_1, a_1, s_2, ... s_L$ that starts from the initial state of $M_\\varphi$, i.e., $s_1 = s_{init}$, and ends if either (1) one of the MECs in $G$ are reached meaning that $s_L \\in G$, or (2) an episode-specific deadline is reached.\nComputing confidence intervals. Let $\\delta \\in (0, 1)$ be a given confidence threshold, $t_k$ be the time point at which $k$th episode begins and $N_k(s, a)$ denote the number of times the state-action pair $(s, a)$ has been visited before the start of the $k$th episode. Let $\\hat{T}_k$ and $M_k$ denote the empirical transition function and the set of statistically plausible MDPs, both computed using the observations before the start of the $k$th episode. In particular, we define $M_k$ as the interval MDP with interval transition function $T_k$ for which all transition functions $T_k \\in T_k$ satisfy\n$||T_k(\\cdot | s, a)-\\hat{T}_k(\\cdot | s, a)||_1 \\leq \\beta_k(s, a) =: \\sqrt{\\frac{8|S|\\log(2|A|k/3\\delta)}{\\max (1, N_k (s, a))}}$\nIntuitively, we pick the confidence bound on the right hand side of Eq. (4), such that the corresponding inequality holds with high confidence. More concretely, we have the following result.\nLemma 4.1. (Tarbouriech et al., 2020) Let $E = \\bigcup_{k=1}^{K} \\{ M_\\varphi \\in M_k \\}$. Then $P(E) \\geq 1 \u2013 \\delta$.\nExtended value iteration. In every episode $k \\in \\mathbb{N}$, we"}, {"title": "4.2. Proof Sketch of Thm. 4.4", "content": "In order to bound the total accumulated regret $R(K)$, we define $R(K) = \\sum_{k=1}^{K-1} \\Delta_k$, where $\\Delta_k = v^*(s_{init}) - v_k (s_{init})$. As mentioned before, our analysis categorizes episodes into slow or fast, corresponding to episodic regrets $\\Delta^{(s)}_k$ and $\\Delta^{(f)}_k$, respectively. Note that for a fast episode, we have $\\Delta^{(s)}_k = 0$; similarly, for a slow episode, we have $\\Delta^{(f)}_k = 0$.\nFor the slow episodes, we use the obvious upper bound\n$\\Delta^{(s)}_k < 1.$\nFor fast episodes, since it is possible that a run ends in one of MECs in $B$ before reaching $G$, we need to define a reset transition which takes the states in $B$ to $s_{init}$. Therefore, every episode $k$ can be broken to $I_k \\in \\mathbb{N}$ intervals such that the first $I_k - 1$ intervals start from $s_{init}$ and end at $B$, and the $I_k$th interval starts from $s_{init}$ and end at $G$. We denote the $i$th interval of the $k$th episode\u2014in which the policy $\\pi_k$ is taken\u2014by $\\rho_{k,i}$, and the corresponding value is defined as"}, {"title": "5. Regret-Free Controller Synthesis for LTL Specifications", "content": "In this section, we study the controller synthesis problem for MDPs with unknown (but fixed) transition function against LTL specifications. In the following, we state the main problem of study in this section."}, {"title": "6. Experimental Evaluation", "content": "In this section, we evaluate an implementation of our algorithm. All of the experiments were performed on a laptop with core i7 CPU at 3.10GHz, with 8GB of RAM. We considered a reach-avoid controller synthesis problem in the gridworld example described in Fig. 1. The world is characterized by the number of cells per column and row, which is denoted by $l \\in \\mathbb{N}_{\\geq 4}$. The agent can move using the cardinal directions, i.e., $A = \\{ right, left, up, down \\}$. Movement along an intended direction succeeds with probability 0.9 and fails with probability 0.1. In case of failure, the agent does not move. Walls are considered to be absorbing, i.e., the agent will not be able to move after hitting a wall. We have conducted experiments to (1) evaluate the empirical performance of our algorithcm, (2) observe how episode length vary throughout the run of our algorithm, and (3) assess the sample complexity of our method.\nEmpirical performance. Fig. 2 illustrates the variations of empirical mean for the normalized regret $R(K)/K$ for our regret-free algorithm which is run for the gridworld example with $l = 6$. We set $\\delta = 0.1$ over 10 runs. Furthermore, we group all of the cells associated with the wall into an absorbing state $B$, such that we have $|S| = 17$ and $|A| = 4$. The target specification is $\\varphi = \\neg B \\mathcal{U} G$. It can be observed"}, {"title": "7. Discussion and Conclusions", "content": "In this paper, we proposed a regret-free algorithm for the control synthesis problem over MDPs against infinite-horizon LTL specifications. The defined regret quantifies the accumulated deviation over the probability of satisfying the given LTL specification. Below, we have discussed several aspects of the proposed scheme.\nPossibility of applying the regret-free algorithms that are proposed for SSP. The assumptions that are needed for solving SSP in a regret free way, are inherently non-applicable to our target setting as we discussed in the related work section. In particular, relaxing the requirement for existence of a proper policy (which is equivalent to the existence of a policy which satisfies the given LTL specification with probability one) requires attributing a large enough cost to the policies which may end up visiting the non-accepting MECs. Such an assumption (even in the case finding such an upper bound is feasible) would automatically require defining a cost structure over the product MDP which would in turn change the function of regret so that it can only be defined with respect to the accumulated cost and not the probability of satisfying the given specification. However, we desire knowing the value of regret with respect to the satisfaction"}, {"title": "A. Graph Identification", "content": "In order to propose a regret-free controller synthesis method, we need to know the underlying MECs. In this section, we show how to use the knowledge of minimum transition probability $p_{min} \\in \\mathbb{R}_{>0}$ of a given MDP M in order to identify the underlying graph of the MDP which is valid with a desired confidence. By the following lemma, we can determine the number of samples needed in order to ensure whether a transition exists or not.\nLemma A.1. (Voloshin et al., 2022) For any state-action pair $(s,a) \\in S \\times A$, let $\\hat{T}_n (s' | s,a)$ denote the empirical transition probability estimation for the transition probability $T(s' | s, a)$ after sampling $(s, a)$ for $n$ times. Given a positive lower bound over the minimum transition probability $p_{min} \\in (0,1)$ and a confidence parameter $\\delta \\in (0,1)$, we have $(s, a, s') \\notin E$ with confidence $1 - \\delta$ if $\\hat{T}_{n^*} (s' | s, a) = 0$, for\n$n^* > \\psi^{-1}(p_{min})$,\nwhere\n$\\psi(n) = \\sqrt{\\frac{\\psi(n)}{n}} + \\frac{7\\psi(n)}{3(n - 1)}$,\nand $\\psi(n) = \\log(\\frac{4n^2}{|S|^2|A|p_{min}})$ if $n > 1$.\nRemark A.2. To compute policies $\\pi^{(s)}$ for $s \\in S$, one needs to use undiscounted RL formulations which explicitly can handle the exploration-exploitation trade-off. E3 (Kearns & Singh, 2002) and $w$-PAC (Perez et al., 2023) are two such approaches which use $\\epsilon$-return mixing time and $\\epsilon$-recurrence time, respectively, in order to avoid unbounded explorations. In our case, we could define an $\\epsilon$-covering time for a policy $\\pi^{(s)}$, as the number of time steps required to visit $s$ for $n^*$ times with probability at least $1 - \\epsilon$ for $\\epsilon \\in (0, 1)$. Similar to the theoretical guranatees of methods like E3, we could easilly see that our graph learning algorithm has a sample complexity that is polynomial in the size of state and action spaces and maximum $\\epsilon$-covering time among all policies.\nAlg. 5 outlines our proposed algorithm to learn the graph of a given MDP. There are two main steps: (1) for every state $s \\in S$, we utilize an RL algorithm to get a policy $\\pi^{(s)}$ under which, $s$ is reachable from $s_{init}$ (with positive probability); (2) we run $\\pi^{(s)}$ for every $a \\in A$ until $(s, a)$ is visited for $n^*$ times; upon reaching $(s, a)$, we collect the resulted outgoing transition by running the MDP. Within the rest of this paper, we are going to use the MDP graph $X_M$ which is correct with confidence $1 \u2013 \\delta$."}, {"title": "B. Proofs", "content": "B.1. Proof of Lem. 4.2\nProof. Assuming that $\\Lambda^*$ is finite implies that there exists at least one policy under which G is reachable from $s_{init}$. Under such a policy, in the worst-case scenario, each state in the Markov chain must be visited at least once. Visiting every state requires following a path of length $|S|$, which occurs with probability $p_{min}^{|S|}$. After $l$ attempts of traversing this path, the probability of success at least once is given by $1-(1-p_{min}^{|S|})^l > 1-(1-p_{min}^{|S|})$. If $l > \\frac{\\log(\\delta)}{\\log(1-p_{min}^{|S|})}$, then $1-(1-p_{min}^{|S|})^l \\geq 1\u2013\\delta$. Finally, since each of the $l$ attempts takes $|S|$ steps in the worst case, the total number of steps is bounded by $\\Lambda \\leq |S|l = |S| \\frac{\\log(\\delta)}{\\log(1-p_{min}^{|S|})}$.\nB.2. Proof of Lem. 4.5\nProof. First, using the Markov's inequality (since $x \\to \\sqrt{x}$ is a non-decreasing mapping for non-negative reals), we have\n$P(\\lambda_k(s) \\geq H_k - 1) \\leq \\frac{\\mathbb{E}[\\lambda_k(s)]}{(H_k - 1)}$.\nNow, we note that by Lem. 15 in (Tarbouriech et al., 2020), we have if $\\lambda_k(s) \\leq \\Lambda$ for every $s \\in S \\setminus (G \\cup B)$ and $\\lambda \\geq 2$, then\n$\\mathbb{E}(\\lambda_k(s)^r) \\leq 2(\\Gamma \\Lambda)^r$,\nfor any $r \\geq 1$. Therefore, substituting $\\Lambda$ with $\\Lambda$, we will have\n$P(\\lambda_k(s) \\geq H_k - 1) \\leq \\frac{2(\\Gamma \\Lambda)^r}{(H_k - 1)}$.\nNote that there exists $y \\in S$ such that\n$||Q_k^{H_k-2}||_{\\infty} = 1, Q_k^{H_k-2} \\mathbf{1} = P(\\lambda_k(y) > H_k - 2)$\n$= P(\\lambda_k(y) \\geq H_k \u2013 2)$.\nBy definition of $H_k$ we have $||Q_k^{H_k-2}||_{\\infty} > 1/\\sqrt{k}$. Combining this with Eqs. (22), (23) yields\n$\\frac{2(\\Gamma \\Lambda)^r}{(H_k - 1)} > 1/\\sqrt{k}$,\nwhich implies that\n$H_k - 1 < \\Gamma \\Lambda(2\\sqrt{k})^{1/r}$.\nBy selecting $r = [\\log(2\\sqrt{k})]$, we get\n$H_k-1 < [\\log(2\\sqrt{k})]\\Gamma \\Lambda(2\\sqrt{k})^{\\frac{1}{[\\log(2\\sqrt{k})]}}$\n$\\leq [3 \\Gamma \\Lambda \\log(2\\sqrt{k})]$.\nHence\n$\\alpha_k \\leq [3 \\Lambda \\log(2\\sqrt{K})]$.\nIn order to prove the sub linear regret bound, we make use of the Azuma-Hoeffding inequality.\nLemma B.1 (Azuma-Hoeffding inequality, Hoeffding 1963). Let $X_1, X_2,...$ be a martingale difference sequence with $|X_1| \\leq c$ for all $l$. Then for all $\\gamma > 0$ and $n \\in \\mathbb{N}$,\n$P[\\sum_{l=1}^{n} X_1 \\geq \\gamma] \\leq \\exp(-\\frac{\\gamma^2}{2nc^2})$.\nNow, we proceed by showing why $\\sum_{k=1}^{K-1} \\Delta_k^{(1)}$ grows sub linearly with K."}, {"title": "B.3. Proof of Lem. 4.6", "content": "Proof. In order to reformulate the regret, we define the following reward function $r: S \\to \\{0,1\\}$\n$r(s) =\n0 \\qquad s \\notin G\n1 \\qquad s \\in G$\nFurther, for the time step $h$ within episode $k$ we define\n$\\Theta_{k,h}(s_{k,h}) := \\hat{v}_k(s_{k,h}) - \\sum_{t=h}^{H_{k, I_k(h)-1}} r(s_{k,t})$,\nwhere $I_k: [1; H_k] \\to [1; I_k]$ maps the time points in episode $k$ into the corresponding interval, and $H_{k,i}$ denotes the length of the $i^{th}$ interval within the $k^{th}$ episode for $1 \\leq i \\leq I_k$ and $H_{k,0} = 1$. Therefore, we have\n$\\sum_{k=1}^{K} \\Delta = \\sum_{k=1}^{K} \\sum_{i=1}^{I_k} \\Theta_{k, H_{k,i-1}} (s_{k, H_{k,i-1}})$.\nLet us further define\n$\\Phi_{k,h} := \\hat{v}_k(s_{k,h+1}) - \\sum_{y \\in S} p(y | s_{k,h}, \\pi_k(s_{k,h}))\\hat{v}_k(y)$,\nwhere $p(\\cdot | \\cdot, \\cdot)$ corresponds to the transition probability in the true MDP $M$. Similarly, we denote by $p_k(\\cdot | \\cdot, \\cdot)$ for the transition probability in the optimistic MDP $M_k$. Note that for $s_{k,h} \\in G \\cup B$, we have $\\Theta_{k,h}(s_{k,h}) = 0$. For $s_{k,h} \\notin G \\cup B$, we have\n$\\Theta_{k,h}(s_{k,h}) = \\hat{v}_k(s_{k,h}) - \\sum_{t=h}^{H_{k, I_k(h)-1}} r(s_{k,t})$\n$\\leq \\hat{v}_k(s_{k,h}) + \\epsilon_k - \\sum_{t=h}^{H_{k, I_k(h)-1}} r(s_{k,t})$\n$= \\sum_{y \\in S} p_k(y | s_{k,h}, \\pi_k(s_{k,h}))\\hat{v}_k(y) + \\epsilon_k - \\sum_{t=h}^{H_{k, I_k(h)-1}} r(s_{k,t})$\n$= \\sum_{y \\in S} (p_k(y | s_{k,h}, \\pi_k(s_{k,h})) - p(y | s_{k,h}, \\pi_k(s_{k,h}))) \\hat{v}_k(y)$\n$+ \\sum_{y \\in S} p(y | s_{k,h}, \\pi_k(s_{k,h})) \\hat{v}_k(y) + \\epsilon_k - \\sum_{t=h}^{H_{k, I_k(h)-1}} r(s_{k,t})$\n$\\leq 2\\beta_k(s_{k,h}, \\pi_k(s_{k,h})) \\times 1$\n$+ (\\sum_{y \\in S} p(y | s_{k,h}, \\pi_k(s_{k,h}))\\hat{v}_k(y) - \\hat{v}_k(s_{k,h+1}))$\n$+ \\epsilon_k + (\\hat{v}_k(s_{k,h+1}) - \\sum_{t=h+1}^{H_{k, I_k(h)-1}} r(s_{k,t}))$\n$\\leq 2\\beta_k(s_{k,h}, \\pi_k(s_{k,h})) + \\Phi_{k,h} + \\epsilon_k + \\Theta_{k,h+1}(s_{k,h+1})$,\nwhere we used Eq. (6) for the first inequality, the fact that $r(s_{k,t}) = 0$ for every $s_{k,t} \\notin G \\cup B$ for the second equality, $\\hat{v}_k(y) \\leq 1$ for every $y \\in S$ for the third equality, definition of $\\beta_k$ (Eq. (4)) for the second inequality, and definition of $\\Phi_{k,h}$"}, {"title": "B.5. Proof of Lem. 4.8", "content": "Proof. Let $\\tau_k$ and $\\Lambda_k$ denote the hitting times of policy $\\hat{\\pi}_k$ in the true and optimistic models, respectively. We define\n$\\Gamma_{k,h}(s_{k,h}) = 1_{\\tau_k(s_{k,h})>H_k-h} - P(\\Lambda_k((s_{k,h})) > H_k - h)$.\nNote that we have\n$\\mathcal{F}_K = \\sum_{k=1}^{K} 1_{\\tau_k (s_{k,1})>H_k-1}$\n$= \\sum_{k=1}^{K} \\Gamma_{k,1} (s_{k,1}) + P(\\Lambda_k (s_{init}) > H_k \u2013 1)$.\nLet\nwhere $\\mathfrak{Q}_k$ denote the transition probability matrix of the DTMC created by connecting B to $s_{init}$. Let $\\mathfrak{p}'(\\cdot | \\cdot, \\cdot)$ denote the transition probability in the optimistic model $M_\\mathfrak{p}'$ (that is the optimistic MDP constructed from $M_k$ by connecting states in B into $s_{init}$). Similarly, let $\\mathfrak{p}'(\\cdot | \\cdot, \\cdot)$ denote the transition probability in the MDP $M_\\mathfrak{p}'$ (that is the MDP constructed from $M_p$ by connecting states in B into $s_{init}$). Since for $1 \\leq h < H_k - 1, 1_{\\tau_k(s_{k,h})>H_k-h} = 1_{\\tau_k(s_{k,h+1})>H_k-h-1}$ we have\n$\\Gamma_{k,h}(s_{k,h}) = 1_{\\tau_k(s_{k,h+1})>H_k-h-1}$\n$= \\sum_{y \\in S} \\mathfrak{p}_k (y | s_{k,h}, \\pi_k(s_{k,h}))P(\\Lambda_k(y) > H_k - h - 1)$\n$\\leq 1_{\\tau_k (s_{k,h+1})>H_k-h-1}$\n$-"}]}