{"title": "DIFFUSION CURRICULUM: SYNTHETIC-TO-REAL GENERATIVE\nCURRICULUM LEARNING VIA IMAGE-GUIDED DIFFUSION", "authors": ["Yijun Liang", "Shweta Bhardwaj", "Tianyi Zhou"], "abstract": "Low-quality or scarce data has posed significant challenges for training deep neural networks in\npractice. While classical data augmentation cannot contribute very different new data, diffusion\nmodels opens up a new door to build self-evolving AI by generating high-quality and diverse\nsynthetic data through text-guided prompts. However, text-only guidance cannot control synthetic\nimages' proximity to the original images, resulting in out-of-distribution data detrimental to the\nmodel performance. To overcome the limitation, we study image guidance to achieve a spectrum of\ninterpolations between synthetic and real images. With stronger image guidance, the generated images\nare similar to the training data but hard to learn. While with weaker image guidance, the synthetic\nimages will be easier for model but contribute to a larger distribution gap with the original data. The\ngenerated full spectrum of data enables us to build a novel \u201cDiffusion CurricuLum (DisCL)\u201d. DisCL\nadjusts the image guidance level of image synthesis for each training stage: It identifies and focuses\non hard samples for the model and assesses the most effective guidance level of synthetic images\nto improve hard data learning. We apply DisCL to two challenging tasks: long-tail (LT) classification\nand learning from low-quality data. It focuses on lower-guidance images of high-quality to learn\nprototypical features as a warm-up of learning higher-guidance images that might be weak on diversity\nor quality. Extensive experiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy\nwhen applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base model's\ntail-class accuracy from 4.4% to 23.64% and leads to a 4.02% improvement in all-class accuracy.", "sections": [{"title": "1 Introduction", "content": "While existing machine learning approaches can train representation or discriminative models with promising\ngeneralization performance, their success highly relies on the quality and quantity of the training data. However, in\nenormous practical scenarios, the data are collected from real environments so neither the quality nor the quantity can\nalways be guaranteed. For example, it is difficult to control the light conditions, weather, motion blur, or the position\nof objects in the scenes captured by trail/animal cameras, traffic cameras, motion cameras, or robot cameras. Likewise,\nit is also difficult to keep different classes in the collected data balanced so the model may perform much poorer on tail\nclasses with scarce data. On the other hand, the low-quality/quantity of data also makes the model more prone to the gap\nbetween the test and training distributions, thereby posing an out-of-distribution challenge. In many cases, such \"hard\"\ntraining data hinders effective learning, introduces biases or outliers, and may even impact the learning of other data.\nData augmentation and synthesis have been studied to address the challenges of hard real data. By applying pre-defined\ntransformations (Ahn et al., 2023) to data in scarce classes or modifying their backgrounds (Beery et al., 2020; Gao\net al., 2022), data augmentation helps learn representations robust to these task-irrelevant variations. While the\naugmented data may lack sufficient diversity or non-trivial difference to the original data, the recent text-to-image\ngenerative models such as GAN or Stable Diffusion enable more sophisticated data synthesis (Dunlap et al., 2024)"}, {"title": "2 Related Work", "content": "Diffusion models for Synthetic Data Recently, a diverse array of generative diffusion models have been proposed,\nincluding GLIDE (Halgren et al., 2004), Imagen (Saharia et al., 2022), Stable Diffusion (Rombach et al., 2022), Dall-E\n(Ramesh et al., 2022), and Muse (Chang et al., 2023). These models can generate realistic, high-resolution images\nwhen conditioned on text prompts, and therefore, are used off-the-shelf to augment the datasets for enhancing data\ndiversity. For instance, He et al. (2022) demonstrates that synthetic data created with GLIDE can significantly improve\nboth zero-shot and few-shot performance on image classification. Wu et al. (2023) has explored Stable Diffusion to\ngenerate perception data for downstream dense prediction tasks such as Human Pose Estimation, Depth Estimation,\nand Segmentation. Recent works like Bansal & Grover (2023) and Sariyildiz et al. (2022) have shown that real data\ncombined with synthetic data generated by Stable Diffusion models, boosts the robustness of standard ImageNet\nclassifiers. Other works like Azizi et al. (2023) have finetuned the Imagen model using ImageNet data to enhance the\nalignment of synthetic data with their classes, while improving the sample diversity. In this work, we utilize off-the-shelf\nStable Diffusion models without further finetuning. Unlike previous works, we harness different image guidance\nlevels to generate training images for each stage of model training, thereby progressively learning a full spectrum of\ninterpolations from synthetic to real data."}, {"title": "3 Methodology", "content": "We propose diffusion curriculum (DisCL) to \"close the distribution gap between original data and the target data\ndistribution\". DisCL comprises two phases: (Phase 1) Synthetic-to-Real Data Generation that generates a syn-to-real\nspectrum of interpolated data for hard samples, and (Phase 2) Generative Curriculum learning based on the synthetic\ndata from Phase 1. The two phases are illustrated in Fig. 1."}, {"title": "3.1 Synthetic-to-Real Data Generation", "content": "Hard Sample Identification We first identify the difficult samples where the model struggles to extract helpful\nfeatures for target classification. The difficulty estimation can be task-specific. For instance, in long-tail classification\nwith scarce data, the difficulty of each sample depends on whether it belongs to tail classes. For tasks with low-quality\ndata, we can utilize the loss or confidence on the ground-truth class to measure the difficulty. These samples are marked\nas \"hard samples\" within the training set (see Fig. 1), to highlight their role in the model's learning process.\nSynthetic Data Generation with Image Guidance Classifier-free guidance was initially introduced by Ho &\nSalimans (2022), to integrate conditional information into the image denoising process of diffusion models without\nthe need for a classifier. It has been adopted by several Text-to-Image generation models such as Stable Diffusion\n(SD) (Rombach et al., 2022). Given the original image's latent representation $z_{real}$, the denoising (backward diffusion)\nprocess can start from any step t with initial $z_t$ defined as\n$z_t = \\sqrt{\\bar{\\alpha}_t} z_{real} + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\epsilon \\sim N(0, I)$."}, {"title": "3.2 Generative Curriculum Learning with Synthetic Data", "content": "With the full spectrum of syn-to-real generated data, we achieve a smooth transition from images of prototypical features\nand high diversity to task-specific features with high resemblance to real images. This enables us to design a curriculum\nselecting data with according to their diversity and feature types for different training stages. With a curriculum of rich\nsynthetic data, we can enhance the model's performance in challenging and diverse cases which are otherwise difficult\nto using only the real data. On the other hand, it also allow us to control the distribution gap to the original data.\nWe apply our method to two challenging applications in the following sections: long-tail classification and learning from\nlow-quality data. In long-tail classification, the scarcity of data in minority/tail classes makes it difficult for models to\nextract useful features for these classes, leading to poor generalization on balanced test set. To address this, we develop\na curriculum strategy that initially exposes the model to diverse synthetic samples of tail classes, and then progressively\nfocuses on task-specific features. This helps mitigate distribution differences between synthetic and real data. In learning\nfrom low-quality data, the poor quality of data limits the model's ability to detect and extract critical visual features. By\nemploying an adaptive curriculum of synthetic data, we can warm up the model training by learning from varying levels\nof prototypical features, gradually aiding the model in extracting features useful for out-of-domain generalization."}, {"title": "4 Applications", "content": "We apply our method to two challenging applications in the following sections: long-tail classification and learning from\nlow-quality data. In long-tail classification, the scarcity of data in minority/tail classes makes it difficult for models to\nextract useful features for these classes, leading to poor generalization on balanced test set. To address this, we develop\na curriculum strategy that initially exposes the model to diverse synthetic samples of tail classes, and then progressively\nfocuses on task-specific features. This helps mitigate distribution differences between synthetic and real data. In learning\nfrom low-quality data, the poor quality of data limits the model's ability to detect and extract critical visual features. By\nemploying an adaptive curriculum of synthetic data, we can warm up the model training by learning from varying levels\nof prototypical features, gradually aiding the model in extracting features useful for out-of-domain generalization."}, {"title": "4.1 Long-Tail (LT) Classification", "content": "Synthetic Data Generation For synthetic data generation, we follow a standard split of tail classes in the studied\ndataset. Given the real tail-class samples and the associated text prompts, we generate a full spectrum of synthetic data\nby techniques in \u00a73.1. To mitigate the imbalance among classes, the key is to increase data diversity and quantity for\ntail classes. We employ a diverse set of textual prompts to achieve the goal2.\nGenerative Curriculum The generated spectrum of synthetic data provides varying degrees of data diversity: the\nimages generated with text-only guidance display the highest diversity but may suffer from visual discrepancies\nto the original images, resulting in a distribution gap that may undermine model performance. To bridge the gap,\nwe progressively shift the synthetic data to a task-specific distribution closer to the original images. This yields a\nnon-adaptive \"Diverse-to-Specific\" curriculum that starts with synthetic data with a lower guidance scale ($\\lambda \\rightarrow 0$) and\ngradually moves toward data of a higher guidance scale ($\\lambda \\rightarrow 1$)."}, {"title": "4.2 Learning from Low-quality Data", "content": "The data collected in real-world scenarios may suffer from low qualities, such as obscurity in images from traffic,\nmotion, or wildlife observation cameras. We investigate wildlife observation as an example application of DisCL to\nenable effective learning under such challenging scenarios.\nSynthetic Data Generation For low-quality images from camera traps, we aim to generate simpler images containing\nmore prototypical features of the animals that can warm up the training and generalize to more challenging cases. We\nfirst identify hard samples based on the ground-truth class probability by a pretrained classifier: a lower probability\nindicates more difficulty. We vary the image guidance scale to generate a full spectrum of synthetic data for these hard"}, {"title": "5 Experiments", "content": "We propose diffusion curriculum (DisCL) to \"close the distribution gap between original data and the target data\ndistribution\". DisCL comprises two phases: (Phase 1) Synthetic-to-Real Data Generation that generates a syn-to-real\nspectrum of interpolated data for hard samples, and (Phase 2) Generative Curriculum learning based on the synthetic\ndata from Phase 1. The two phases are illustrated in Fig. 1."}, {"title": "5.1 Long-Tail Classification", "content": "Setup To validate the efficacy of DisCL method on long-tail classification, we conduct main experiments with\nImageNet-LT (IN-LT) dataset (Liu et al., 2019). This dataset includes 1000 classes, with class cardinality ranging from 5\nto 1,280. To assess the robustness of DisCL more comprehensively, we conduct experiments on two additional datasets:\na synthetically imbalanced dataset, CIFAR100-LT (Cao et al., 2019), and a real-world benchmark, iNaturalist2018\n(Van Horn et al., 2018). CIFAR100-LT is provided with imbalanced classes by synthetically sampling the training\ndata with multiple imbalance ratios {100,50}. iNaturalist2018 dataset represents a naturally occurring long-tailed\ndistribution with class cardinality ranging from 2 to 1000. We evaluate overall accuracy and the accuracy across three\ncategories of classes: many (most frequent), medium, and few (least frequent, tail) classes on the standard balanced\ntest sets of three datasets. For synthetic data generation, we use DDIM (Song et al., 2020) as our noise scheduler. For\ntraining, following Ahn et al. (2023); Han et al. (2024), we use ResNet-10 as the visual backbone. We average results\nover 3 runs, with training details in Appendix A.3.1 and hyperparameters in Appendix A.4.\nBaselines We compare the effect of DisCL with comparable baseline of CUDA (Ahn et al., 2023) and LDMLR (Han\net al., 2024), mainly using Cross-Entropy (CE) loss function. To further illustrate the robustness of DisCL, we try\nBalanced Softmax (BS) loss (Ren et al., 2020), known for its competitive performance on long-tail learning.\n\u2022 CUDA: Engineered data augmentation + curriculum learning on IN-LT.\n\u2022 LDMLR: A three-stage training using diffusion model to improve LT.\n\u2022 BS loss: Balanced softmax to address class-distribution shift between training and test sets.\nWe also conduct ablation study to analyze the effect of DisCL under different hyperparameter settings. We note that, real\ndata for hard samples (~1) is included by default; however, this doesn't apply to the Fixed Guidance and Text-only\nGuidance ablation:\n\u2022 Text-only Guidance: Using data at image guidance scale $\\lambda$ = 0 without curriculum strategy.\n\u2022 Fixed Guidance 4: uses data generated from a single guidance scale $\\lambda_i \\in [0, 1)$. We report results for the guidance\nwith the highest few-class accuracy.\n\u2022 DisCL: employs multiple levels of guidance scales alongside a range of curriculum strategies. These strategies and\nthe guidance intervals used for training, are defined below:\nSpecific to Diverse: Non-adaptive strategy with guidance changing from largest (task-specific augmentation) to\nsmallest (diverse augmentation).\nDiverse to Specific: Non-adaptive strategy with guidance changing from smallest to largest.\nAdaptive: Curriculum strategy to adaptively select guidance during training."}, {"title": "5.2 Learning from Low-quality Data", "content": "Setup We also conduct DisCL experiments with iWildCam dataset (Beery et al., 2021) to evaluate its efficacy in\nclassifying low-quality data. The task is to classify 182 different animal species from images captured by camera traps.\nWe evaluate model performance on standard out-of-domain (OOD) and in-domain (ID) test sets in terms of macro\nF1 score. We choose the CLIP ViT model as our base model and finetune CLIP ViT-B/16 and CLIP ViT-L/146 models\nwith DisCL. The reported accuracy is averaged over 3 random seeds. More training details and hyperparameters are\nprovided in Appendix A.3.2 and Appendix A.4.\nBaselines We compare the effect of our method with three benchmark algorithms, LP-FT (Kumar et al., 2022), FLYP\n(Goyal et al., 2023), and ALIA (Dunlap et al., 2024). To further analyze the gain of our model, we try Weighted\nEnsembling (WE) method (Wortsman et al., 2022), which can further improve model performance by integrating prior\nknowledge from pretrained model:"}, {"title": "6 Ablation Study and Analysis", "content": "We also conduct ablation study to analyze the effect of DisCL with different hyper-parameters introduced in \u00a75.1, and the\nnewly introduced ablation hyper-parameters:\n\u2022 DisCL: employs multiple levels of guidance scale and a range of curriculum strategies:\nEasy to Hard: Non-adaptive strategy with guidance changing from smallest (easiest and most prototypical features,\n$\\lambda \\sim 0$) to largest (hardest and most task-specific features, $\\lambda \\sim 1$).\nRandom: Randomly selecting guidance at each training stage."}, {"title": "6.1 Effect of Syn-to-Real Interpolation Data", "content": "We examine the effectiveness of using a spectrum of data generated with our DisCL method, by comparing All-Level\nGuidance and Text-only Guidance rows in both the task tables (IN-LT and iWildCam). For IN-LT results in Table 1,\nAll-Level Guidance brings ~1.27% gain in few-class accuracy, alongwith significant gains across other class-categories.\nLikewise, All-Level Guidance shows a superior ID and OOD performance as compared to Text-only Guidance for the\niWildCam as well, see Table 5. These findings corroborate that utilizing a spectrum of data with multiple guidance\nlevels helps mitigate the negative effects of the distribution gap."}, {"title": "6.2 Effect of Curriculum Learning Strategy", "content": "Long Tail Classification We compare the impact of our Diverse to Specific curriculum strategy tailored for IN-LT\ntask against other strategies, notably All-Level Guidance which employ no curriculum and uses all synthetic data. The\nDiverse to Specific demonstrate a higher few-class accuracy with a margin of 4.47%, see Fig. 3b. We then compare it"}, {"title": "6.3 Effect of CLIPScore Threshold", "content": "Long Tail Classification Our analysis of CLIPScore distribution on IN-LT generated data leads us to infer that the best\nCLIPScore threshold for filtering is 0.3 (detailed explained in the Appendix A.1.2). We then assess different CLIPScore\nthresholds with the Diverse to Specific curriculum strategy, by experimenting with different values: lower (0.28), and\nhigher (0.32), shown in Fig. 3a. However, we find that changing the CLIPScore threshold does not significantly affect\nthe performance. As shown in Figure 4b, the CLIPScore of synthetic data is concentrated, as Stable Diffusion model\nperforms well on generating high-quality images for ImageNet classes. Changes in the CLIPScore threshold will not\nsignificantly affect the quality of synthetic images and corresponding effects in downstream classification tasks.\nLearning from Low Quality Data In the iWildCam task, we identify the optimal threshold as 0.25. To further\nvalidate this choice, we experiment with nearby thresholds (0.23 and 0.27) with the chosen Adaptive Curriculum\nstrategy suited for low-quality image classification. As depicted in Fig. 3c, the 0.25 threshold markedly improves\nOOD performance compared to other CLIPScore thresholds. Unlike the ImageNet dataset, the iWildCam images are\ncharacterized by significant difficulty and poor quality, leading to high variance in CLIPScores of synthetic data (as\nshown in Fig. 5b). In this scenario, adjusting the CLIPScore threshold can impact model performance. When a higher\nthreshold is used, the selected synthetic images include more prototypical visual features but they are less similar to\nthe original images. Hence, they improve OOD performance but lead to a drop of ID F1 score."}, {"title": "7 Conclusion", "content": "In this paper, we introduce DisCL, a novel paradigm designed to enhance model performance when dealing with\nlow-quality or scarce data. DisCL effectively bridges the distribution gap between original and target data using a\nspectrum of synthetic data, particularly for challenging samples. Our method utilizes image guidance in diffusion\nmodels to generate a comprehensive range of interpolated data from synthetic to real. Additionally, we design specific\ncurricula to maximize the benefits of synthetic data for learning hard samples and closing the gap between synthetic\nand real data. The efficacy of DisCL is demonstrated through its significant and robust performance improvements in\nlong-tail classification and learning from low-quality data, across various base model settings. Our analyses reveal that\nthe interpolation of synthetic-to-real data, the selection of guidance intervals, and the proposed curriculum strategy are\nall essential components contributing to these gains.\nDespite the promising results, the performance of DisCL is influenced by certain limitations. The quality of the\ngenerated data spectrum is dependent on the capabilities of the diffusion model and the visual-text alignment ability of\nfiltering models. These dependencies constrain the overall performance of DisCL. Additionally, the current approach to\ngenerate text prompts for long-tail classification relies solely on category names derived from large language models\n(LLMs). To better align with the real data distribution and to reduce the gap between synthetic and real data, future\nworks could focus on generating text prompts from image captions. Lastly, discrepancies in the position and size of\nclass objects between real and synthetic images can widen the distribution gap. Addressing this issue may involve\ndetecting objects and performing crop operations on real images or using detailed prompts to control these properties in\nsynthetic data. These areas present opportunities for further research and improvement."}, {"title": "A.1 Synthetic Data Generation with Image Guidance", "content": "In this section, we visualize more generated images in (Phase 1) of our method with various levels of image guidance,\nfor two different classification tasks."}, {"title": "A.1.1 Generation Settings and Statistics", "content": "We provide the statistics for the synthetic data generation within our paradigm on ImageNet-LT, CIFAR100-LT,\niNaturalist2018, and iWildCam, as shown in Table 6."}, {"title": "A.1.2 ImageNet-LT Synthetic Generation", "content": "Selection of Text prompts To improve model performance on the minority classes, high-quality and diverse synthetic\nsamples are required. To achieve so, we follow the approach in Fu et al. (2024), and utilize publicly available GPT-3.5-\nturbo to generate diverse prompts for these 1000 IN-LT classes. We use the following prompt to query GPT-3.5-turbo\nfor generating descriptions for class X:\n\"Please provide 10 language descriptions for random scenes that contain only the class X from the ImageNet-LT dataset.\nEach description should be different and contain a minimum of 15 words. These descriptions will serve as a guide for\nStable Diffusion in generating images.\"\nSelection of Images Guidance Levels We first analyze the cosine similarity between synthetic images and real\nimages, as well as between synthetic images and text prompts. The similarity score between synthetic images and real\nimages can be used to quantify the diversity introduced in the synthetic images. As depicted in Fig. 4a, the similarity\nbetween synthetic images and real images decrease as the guidance level reduces, demonstrating the trend of increased\ndiversity in the data spectrum. However, the changes in the scores are relatively small across varying guidance levels."}, {"title": "A.1.3 iWildCam Synthetic Generation", "content": "Selection of Text prompts Following previous work (Clark & Jaini, 2024; Trabucco et al., 2023), we first define\nprompts for each class using the template \"a photo of <class>\". However, the classnames in iWildCam comprises of\nscientific names, which are usually unseen/unknown concepts to the diffusion text encoder. For example, \"canis lupus\"\nis the class name for \"wolf\" animal. To address this, we replace the scientific names with their common names and add\na postfix \"in the wild\" in the prompt to drive the generation of wild images. The final text prompt we use is \"a photo of\n<common name of class> in the wild\".\nSelection of Images Guidance Levels Based on the generated data with multiple image guidance scales, we search for\neffective image guidance scales for this task using CLIP cosine similarity scores between synthetic image embeddings\nand real image embeddings. As shown in Fig. 5a, as the difference between real images and synthetic images increases,\nthe cosine similarity between image embeddings decreases from X = 1 to X = 0.3. However, when the image guidance\ncontinues to decrease to x = 0, the cosine similarity score increases slightly. With low image guidance scales, the\ndiffusion model tends to generate images that heavily rely on text information, maintaining only global information\n(such as the color of the image background) in the synthetic data for some images. This creates a distribution gap\nbetween these synthetic data and real data that is too large for the model to accurately compare the differences between\nthe two images using embedding representation. Additionally, based on the analysis of the quality of synthetic images\nand to leverage the difficulty of the features and the distribution gap between synthetic and real data, we set the image\nguidance scales to {0.5, 0.7, 0.9} for this task."}, {"title": "A.1.4 Visualization", "content": "Visual Cases We provide additional visual examples of synthetic data generated with multiple guidance levels and\ntext prompts for the ImageNet-LT and iWildCam datasets. The results are visualized in Fig. 6 and Fig. 7. These\nexamples demonstrate that the model can generate synthetic data with various postures, backgrounds, and actions as\nthe image guidance level decreases. Particularly for ImageNet-LT generation results, diverse prompts introduce more\nvaried features into low-guidance data. These diverse features enable the model to achieve better generalization on the\ntarget distribution.\nFailure Cases During generation, despite designing text prompts and applying CLIPScore to filter to remove low-\nquality data, some failure cases still occur in the synthetic dataset. In this section, we discuss these failure cases\nencountered during the generation process. As shown in Fig. 8 and Fig. 9, the first failure case is caused due to the\ninability to recognize objects in the original images. If these objects are clearly obscured or hard-to-identify (e.g. second\ncase in Fig. 9 and first case in Fig. 8), diffusion models cannot accurately identify the object or modify details for\ngenerating diverse and useful data. For these seed images, only synthetic data generated with a low-guidance scale can\nachieve a CLIPScore higher than the threshold. However, this approach compromises the smooth transition of data\nfrom synthetic to real distribution. Even though the diffusion model can generate images with a smooth transition for\nmost-of-the-cases, our quality-check on synthetic data can constrain the feature extraction and alignment ability of the\nCLIP model. For example, in second case of Fig. 8, CLIPScore filters out the slightly modified but perceptually useful\nimages, containing prototypical class features."}, {"title": "A.2 Applications on Other Datasets", "content": "To further evaluate the robustness of DisCL, we extended our experiments to two additional widely used imbalanced\ndatasets: CIFAR-100-LT (Cao et al., 2019) and iNaturalist2018 (Van Horn et al., 2018). For iNaturalist2018, We\ngenerated synthetic data for these datasets following the same approach and settings used for the long-tail classification\ntask on ImageNet-LT. For CIFAR-100-LT dataset, due to the low resolution of the original images, we adjust the image\nguidance scale to 0.5, 0.7, 0.9 to ensure generation quality for the synthetic data. Visual examples of the generated data\nare shown in Fig. 10 and Fig. 11. For CIFAR-100-LT, we assessed the performance of DisCL across different imbalance\nratios (50 and 100). The results, along with those of the baseline methods, are presented in Table 2 and Table 3. Our\nexperimental findings demonstrate that DisCL achieved significant improvements in Top-1 accuracy for both overall\nand few-shot classes across these datasets."}, {"title": "A.3 Training with Curriculum Learning", "content": "To further evaluate the robustness of DisCL, we extended our experiments to two additional widely used imbalanced\ndatasets: CIFAR-100-LT (Cao et al., 2019) and iNaturalist2018 (Van Horn et al., 2018). For iNaturalist2018, We\ngenerated synthetic data for these datasets following the same approach and settings used for the long-tail classification\ntask on ImageNet-LT. For CIFAR-100-LT dataset, due to the low resolution of the original images, we adjust the image\nguidance scale to 0.5, 0.7, 0.9 to ensure generation quality for the synthetic data. Visual examples of the generated data\nare shown in Fig. 10 and Fig. 11. For CIFAR-100-LT, we assessed the performance of DisCL across different imbalance\nratios (50 and 100). The results, along with those of the baseline methods, are presented in Table 2 and Table 3. Our\nexperimental findings demonstrate that DisCL achieved significant improvements in Top-1 accuracy for both overall\nand few-shot classes across these datasets."}, {"title": "A.3.1 Long-Tail Classification with Non-Adaptive Strategy", "content": "For long-tail classification, we propose a non-adaptive curriculum learning strategy that starts with the lowest guidance\nand progressively increases to the highest guidance within the defined interval A. We employ a linear scheduler to\nadjust the guidance levels during training, allowing the model to train with data from various guidance levels for equal\ndurations. Furthermore, the test set of ImageNet-LT is in-distribution to its training data; unlike the training data, it is a\nclass-balanced set. To mitigate the potential negative effects of the distribution gap between synthetic and real data,\nall the hard tail samples from original data are involved into training at all times. Furthermore, with DisCL, number\nof samples for tail classes increases along with the introduction of synthetic data at each stage, however the ratio of\ntail-to-nontail samples is still very skewed. To preserve a constant imbalance-ratio throughout all training stages and\nexperiments, we undersample the non-tail samples at \"each stage\" so that ratio of tail-samples to non-tail samples\nmatches the proportion of tail classes to non-tail classes present in the original data (13.6%).\nAll experiments are conducted based on this proportion setting. Complete strategy details are covered in Algorithm 1."}, {"title": "A.3.2 Learning from Low-Quality Data with \"Adaptive Curriculum\" Strategy", "content": "An approximation method to assess the effectiveness of samples in helping model achieve greatest progress on and\nfastest learning face is introduced by DoCL (Zhou et al., 2021a) as shown in Eq 4.\n$E_{x \\in D, x \\sim D} (y - f(x), \\frac{df(x)}{dt} \\nu) \\approx \\frac{1}{|D|} \\sum_{x_i \\in D} (y_i - f(x_i), \\frac{df(x_i)}{dt} \\nu)$,\nwhere D is the training distribution and x \u2208 D is a set of finite samples randomly sampled from the original distribution\nD. V denotes the subset of samples. Here, y and f (x) denotes the target-class and sample prediction. $(y- f(x), \\frac{df(x)}{dt} \\nu)$\nrepresents the project of residual $y - f(x)$ on the model dynamics $\\frac{df(x)}{dt} |\\nu$. This equation indicates that when trained\nwith subset V, the expected progress E of samples in the original training dataset can be approximated by the progress\nof samples on subset V achieved via training on the set D.\nFor learning from low-quality data, we adopt DoCL and implement an adaptive curriculum strategy to select the\nsynthetic data with best guidance level for each training stage. Before the training process, we randomly select samples\nfrom the spectrum for each guidance level in A and mark it as guidance validation set V for progress evaluation. This set\nhas zero overlap with the training data Dall. At each training stage, we randomly sample a set D (termed as random-real\nset) from the training dataset Dall. Before selecting the guidance level, we train the model on dataset D and evaluate\nthe progress (in terms of classifier's prediction score) achieved on samples of each subset V corresponing to a given\nguidance Ai. We then select the \\lambda\u2081 with the highest progress to gather synthetic data and combine it with other non-hard\nsamples from the original training data for the current training stage. This technique encourages the model to adaptively\nselect the most informative guidance for the current training stage. At the end of the curriculum-training, to alleviate the\nnegative effect of the distribution gap between synthetic data and real data for this task, we keep finetuning the model\nwith real data for a short period. The steps of algorithm are detailed in Algorithm 2."}, {"title": "A.4 Hyperparameters for Synthetic Generation and Model Training", "content": "The values of all hyperparameters used for synthetic data generation with diffusion model and curriculum learning\nstrategy are listed in Table 8.\nFor ImageNet-LT, we implement baselines based on the codebase and the pretrained model from LDMLR. We also\nre-implement CUDA baseline from this codebase, containing some missing models. We use the same hyper-parameter\nsettings as listed in the CUDA paper. For FLYP, we implement baseline models with FLYP codebase and leverage the\navailable pretrained model from Open CLIP."}, {"title": "A.5 Computational Requirements for Synthetic Generation", "content": "For computational requirements of offline generation, 1 RTX A5000 GPU is used to generate synthetic images. For\ntime efficiency, It took 10 seconds to generate a full spectrum (6 image guidance levels) of synthetic images for each\nreal image with resolution=480 \u00d7 270."}, {"title": "A.6 Further Discussion on Experiment Results", "content": "In this section, we analyze the results of each guidance level under Fixed Guidance experiment to observe the effect of\ndifferent image guidance levels on the classifier's performance. During the training process, synthetic data generated\nfrom only a specific guidance level combined with original real data is presented to the model. The ablation numbers\nare shown in Fig. 12.\nFor the iWildCam dataset, data generated with text-only guidance (X = 0) has the largest distribution gap between\nsynthetic and real data, and it also showcases lowest Out-of-Distribution (OOD) performance. As the guidance scale\nincreases, this distribution gap diminishes, and the OOD F1 score consistently improves. This outcome aligns with the\nvisually observed reduction in distribution differences between generated and real images.\\"}]}