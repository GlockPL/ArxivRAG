{"title": "Pre-Trained Video Generative Models as World Simulators", "authors": ["Haoran He", "Yang Zhang", "Liang Lin", "Zhongwen Xu", "Ling Pan"], "abstract": "Video generative models pre-trained on large-scale internet datasets have achieved remarkable success, excelling at producing realistic synthetic videos. However, they often generate clips based on static prompts (e.g., text or images), limiting their ability to model interactive and dynamic scenarios. In this paper, we propose Dynamic World Simulation (DWS), a novel approach to transform pre-trained video generative models into controllable world simulators capable of executing specified action trajectories. To achieve precise alignment between conditioned actions and generated visual changes, we introduce a lightweight, universal action-conditioned module that seamlessly integrates into any existing model. Instead of focusing on complex visual details, we demonstrate that consistent dynamic transition modeling is the key to building powerful world simulators. Building upon this insight, we further introduce a motion-reinforced loss that enhances action controllability by compelling the model to capture dynamic changes more effectively. Experiments demonstrate that DWS can be versatilely applied to both diffusion and autoregressive transformer models, achieving significant improvements in generating action-controllable, dynamically consistent videos across games and robotics domains. Moreover, to facilitate the applications of the learned world simulator in downstream tasks such as model-based reinforcement learning, we propose prioritized imagination to improve sample efficiency, demonstrating competitive performance compared with state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "The field of video generation has experienced remarkable progress in recent years, with models such as Brooks et al. (2024), Polyak et al. (2024), Sharma et al. (2024), Yang et al. (2024c), Zheng et al. (2024) demonstrating an exceptional ability to generate high-fidelity and temporally consistent videos conditioned on various inputs, most notably text and initial frames. However, these models are limited to support interactive simulation scenarios, as they are trained for one-shot generation with static prompts, lacking frame-level interactivity and frame-to-frame dynamic modeling. To fill this gap, the community is increasingly focusing on building action-conditioned video models Bruce et al. (2024), Che et al. (2025), Decart et al. (2024), Valevski et al. (2024), Wu et al. (2024), Xiang et al. (2024), Yang et al. (2023, 2024a).\nThese action-conditioned models effectively act as interactive environment simulators (\u201cworld models\u201d or \u201cworld simulators\u201d), which leverage advanced transformers or diffusion model architectures to predict future visual outcomes based on the agent's actions. Their goal is to encapsulate an understanding of the underlying dynamic transitions and commonsense knowledge about how the world works, enabling action-driven imagination analogous to the human cognition process."}, {"title": "2 Related Work", "content": "Video World Models. With the development of internet-scale datasets (Bain et al., 2021, Chen et al., 2024) and advanced model architecture (Brooks et al., 2024, Peebles and Xie, 2023), significant progress has been made in realistic video generation conditioned on text descriptions and initial frames (Blattmann et al., 2023, Lin et al., 2024, Ma et al., 2024, Yang et al., 2024c, Zheng et al., 2024). Building upon these foundations, current research has increasingly focused on action-controllable video generation, aiming to develop generalist world simulators (Bruce et al., 2024, Che et al., 2025, Feng et al., 2024, Parker-Holder et al., 2024, Valevski et al., 2024, Xiang et al., 2024, Yang et al., 2023, Zhu et al., 2024) that can effectively model both physical dynamics and action consequences. However, these models typically require training from scratch on large-scale datasets and involve millions (or billions) of parameters, resulting in substantial computational overhead and slow inference speed. In contrast, we propose to adapt publicly available pre-trained video generative models (Wu et al., 2024, Zheng et al., 2024) into action-driven world simulators. Our proposed fine-tuning approach, DWS, achieves efficient model adaptation while requiring minimal computational overhead, significantly reducing both training costs and inference latency. Concurrent works (Rigter et al., 2024, Yu et al., 2025) have also explored leveraging pre-trained models for action-conditioned video generation. However, their investigations are limited to diffusion-based models, and neither work validates the effectiveness of their approaches in facilitating downstream tasks such as model-based RL.\nModel-Based RL Model-based RL aims to build world models in which the trial-and-errors can take place without real cost. With a sufficiently accurate world model, agents can develop imagination abilities, allowing them to simulate interactions and generate synthetic experience data. This simulated data can then be leveraged to learn optimal policies for diverse decision-making tasks, effectively reducing the need for real-world interactions. Sutton (1991) introduce the first general framework for model-based RL, highlighting the utility of an estimated dynamics model in facilitating the training of value functions and policies (Sutton and Barto, 1998). Recent years have witnessed remarkable progress in model-based methods for learning complex environmental"}, {"title": "3 Preliminaries", "content": "3.1 Problem Formulation\nThe conditional video generation framework can be adaptable to instantiate a world simulator (or a world model) (Yang et al., 2023). The world model takes in some action as input and produces the visual consequence of the action as output, which aims to simulate the environment. This environment can be represented as a Partially Observable Markov Decision Process (POMDP), encapsulated within the tuple (S, O, \u03c6, A, p, r, \u03b3). Here, S is the state space, and O is the observation space which only provides incomplete information of S. At each timestep t, the agent chooses an action at by following a policy \u03c0 : O \u2192 \u2206a, the environment updates the state following the dynamics, st+1 ~ p(st+1|st, at), the next observation ot+1 = \u00a2(st+1) is received and a scalar reward rt is computed as R(st, at, st+1). The goal of the agent is to learn a policy \u03c0* = arg max E_{\\tau~\u03c0} [\u03a3_{t=To}^{yT} \u03b3^t rt] by maximizing the \u03b3-discounted cumulative rewards.\nA well-trained world model can replace the environment to interact with the agent, and thus benefit downstream policy learning by providing infinite experiences. Concretely, given a history observation oT, at each timestep t = To,\u2026\u2026,T\u22121, the agent takes an action at based on its policy and previous imagined observations, and then the world model predicts the transition p(ot+1, rt+1|Ot, at) to feedback the agent.\n3.2 Pre-Trained Video Generative Models\nBy formulating learning world models for visual control as an interactive video generation problem, we can harness the widely available video data, which embeds broad knowledge that is generalizable across different domains (Yang et al., 2024b). Video data not only contains semantic visual details but also includes motion movements that capture the dynamic rules in the physical world. However, training such video world models on internet-scale video datasets from scratch is expensive and time-consuming. We propose to fine-tune pre-trained advanced video models to enable them to simulate interactions. Specifically, we adopt two different pre-trained video generative models as our base models, which are diffusion models, i.e., Open-Sora (Zheng et al., 2024), and autoregressive transformer models, i.e., iVideoGPT (Wu et al., 2024). Open-Sora is a kind of rectified flow-based diffusion model that is fully open-sourced and pre-trained on millions of internet videos. We consider using it because it requires only a few sampling steps, benefiting from flow-matching training. A"}, {"title": "4 Method", "content": "In this section, we first introduce our proposed action-conditioned module in \u00a74.1, and the motion-reinforced loss for enhancing dynamic modeling in \u00a74.2. After presenting methods for fine-tuning general video generative models into world simulators, we introduce the prioritized imagination technique for improving model-based reinforcement learning performance in \u00a74.3.\n4.1 Action-Conditioned Module\nRecent video generative models have achieved significant success in generating realistic videos correlated with conditioned text prompts or initial frames. These text-to-video tasks operate with static prompts that globally describe the entire video without specifying what the next frames should be. This design paradigm, while effective for general video generation, presents fundamental challenges when adapting them as world models that aim to simulate action-rich interactions, where the conditions are frame-level, fine-grained action trajectories. To address this requirement and ensure each generated frame matches its corresponding action in the trajectory, we leverage an action-conditioned module that conditions the generation of each frame by its corresponding action individually. Unlike previous text-to-video models that compress the entire action trajectory into a single embedding, our approach, similar to IRASim (Zhu et al., 2024), implements a more granular action encoding mechanism. We introduce a lightweight add-on module, consisting of two linear layers within each transformer block, to encode individual actions separately. This design ensures that each frame's content is directly modulated by its corresponding action, rather than being guided by a global description, and leads to a direct correspondence between actions and generated frames.\nAction Representation. A key challenge in adapting video generative models for action-based control lies in the representation of actions. In discretized action spaces, actions are typically represented as integer values, which lack the rich semantic context present in text prompts used in traditional text-to-video models. This semantic gap can limit the model's ability to interpret and respond to different actions effectively. To bridge this gap, we propose to represent the actions using language templates that depict the meanings of the actions. Specifically, given an action trajectory y = {at, at+1,\u2026, at+H\u22121}, where H is the horizon of the trajectory, we develop a mapping function \u03a8 to translate abstract action integers into meaningful languages, i.e., \u03c8 : A \u2192 L, where L is the language space. This mapping enables us to leverage the text encoder in pre-trained video generative models to obtain rich feature embeddings c\u2208 R^{nH\u00d7nd}, where \u043f\u043d and na represent the horizon and the dimension of each token respectively. For continuous action spaces, following Wu et al. (2024), Zhu et al. (2024), we use a trainable linear action embedder to directly generate feature embeddings c without language translation.\nFrame-Level Condition. In the context of video generative models serving as world simulators, precise temporal control is important as each action should directly modulate the visual content of its subsequent frame. To explicitly model and enhance this action-frame correspondence, we incorporate a frame-level action-conditioning module within each transformer block, drawing inspiration from IRASim (Zhu et al., 2024). While IRASim's implementation was limited to specific diffusion models with temporal-spatial transformer architectures, we significantly extend this concept by developing a versatile add-on module that generalizes across different model architectures, including both diffusion-based and transformer-based frameworks. Therefore, our design offers en-"}, {"title": "4.2 Motion-Reinforced Loss", "content": "Traditional video generative models commonly employ the squared l2 distance (for rectified flow (Liu et al., 2022, Zheng et al., 2024) in the continuous space) or cross-entropy loss (for next-token-prediction transformers (Vaswani et al., 2017, Wu et al., 2024) in discrete space) as their training objectives. While these training objectives have demonstrated effectiveness in general video generation tasks (Brooks et al., 2024, Lin et al., 2024, Tian et al., 2024, Yan et al., 2021, Zheng et al., 2024), they typically consider each pixel equally, which may compromise the model's ability to capture action-dependent state changes. Therefore, it is inefficient for them to function effectively as world simulators for RL agents, where accurate modeling of dynamic transitions is more crucial for learning than maintaining high-fidelity background details. This limitation arises because RL agents predominantly learn from action-induced state changes rather than static visual elements.\nTo tackle this problem and enable more precise dynamic modeling, we introduce a new motion-reinforced loss to improve the action-following ability of video models. At each training step, we sample a random batch of ground-truth video embeddings x = {x0,...,x_k,...,x_{H-1}}, where H denotes the horizon of the video clips. We then compute the differences w = {w_1, ..., w_H} between consecutive frames, denoted as\nwi+1 = softmax(x_{i+1} - x_i),\nwhere w_i \u2192 [1,c], and we set w_0 = 1 for the initial frame x^0 since it serves as a conditioned frame. Here, c denotes a hyperparameter that modulates the motion-reinforced strength. After obtaining w, we integrate it as pixel-wise weights into the supervised training loss. The resulting motion-reinforced loss function can be formulated as:\nL_{motion} = L_{prev} * \u03c9,\nwhere L_{prev} represents either the original MSE loss used in diffusion models, or the cross-entropy loss function in transformer-based architectures. We include more implementation details in Appendix B.2. Through this formulation, pixels that change across frames will have a greater impact"}, {"title": "4.3 Model-Based Reinforcement Learning", "content": "Given the video-based world simulators fine-tuned from pre-trained video generative models, one of the most promising applications is to utilize them as world models for policy learning in model-based reinforcement learning (MBRL). In MBRL, the agent optimizes a policy to maximize the cumulative rewards by interacting with the trained world models which improves sample efficiency.\nTo enable effective policy training in MBRL, the world model should predict both transition dynamics and rewards. We now complete our world model with a reward prediction model. Since estimating the reward is a scalar prediction problem, we introduce a separate model Ry consisting of linear layers, self-attention blocks, and cross-attention blocks to estimate the reward given past"}, {"title": "5 Experiments", "content": "In this section, we evaluate the video world model fine-tuned by our proposed method from two critical perspectives: (1) action simulation capability, assessed through the quality of action-conditioned video prediction, and (2) the effectiveness in both online and offline model-based reinforcement learning, quantified by the cumulative return across tasks.\n5.1 Action-Conditioned Simulation\nTo evaluate the effectiveness of DWS in enhancing action-conditioned video prediction for world simulation, we conduct experiments using two architecturally distinct pre-trained video generative models: Open-Sora (Zheng et al., 2024), which employs a diffusion-based architecture, and iVideoGPT (Wu et al., 2024), which utilizes an autoregressive architecture. For computationally efficiency, we utilize a compressed version of Open-Sora with 12 layers (approximately 280M parameters) instead of the original 1.1B model. The details and our setup of these base models can be found in Appendix B.1. The evaluation is performed across three diverse datasets: the BAIR dataset (Ebert et al., 2017) featuring continuous action spaces, and both Procgen (Cobbe et al., 2020) and Atari (Bellemare et al., 2013, Machado et al., 2018) datasets incorporating discrete action spaces.\nExperiment Setup. The BAIR robot pushing dataset which is about a robotic arm manipulating various objects consists of 43k training and 256 test videos, Following previous works (Gupta et al., 2022, Yan et al., 2021), we predict 15 frames from a single initial frame. In this benchmark, we compare against a variety of video prediction models, including diffusion (Voleti et al., 2022, Zheng et al., 2024), masked (Gupta et al., 2022, Yu et al., 2023), and autoregressive models (Wu et al., 2024, Yan et al., 2021). For the Procgen dataset, we evaluate DWS on two platformer games, i.e., namely Coinrun and Ninja. For the Atari dataset, we assess performance on two Atari games: Breakout and"}, {"title": "5.2 Model-Based Reinforcement Learning", "content": "To validate the practical utility of DWS-fine-tuned models for world simulators, we evaluate their performance by utilizing them as world models in MBRL policy learning.\nBenchmarks and Baselines. We conduct experiments on coinrun and ninja platformer games from the Procgen benchmark, and Breakout and Battle Zone games from the Atari benchmark. These environments take RBG images as observations and discrete actions for control. Procgen employs a 15-dimensional action space, Breakout operates with a 4-dimensional space, while Battle Zone utilizes an 18-dimensional action space. We compare our MBRL method with prioritized imagination with the following baselines: 1) PPO (Schulman et al., 2017) is a model-free RL method that is widely used. Our method is built on PPO. 2) Dreamerv3 (Hafner et al., 2023) is a SOTA model-based RL method that employs a recurrent network for dynamic prediction and actor-critic RL for policy learning, which is effective in handling tasks with discrete action spaces. For Procgen environments, we additionally include PPG (Cobbe et al., 2021) for comparison, as it represents a competitive algorithm on Procgen.\nResults Analysis. The experimental results presented in Fig. 6 demonstrate that our DWS-trained world model, when combined with a simple PPO algorithm, significantly outperforms both vanilla PPO and state-of-the-art model-based reinforcement learning methods, i.e., Dreamerv3. In Procgen environments, DWS exhibits substantial performance improvements over model-free approaches such as PPO and PPG. Although the DWS-trained world model requires fine-tuning during MBRL policy training\u2014due to the episodic variations in background and object details inherent to Procgen, it maintains competitive performance compared to existing model-based methods. In Atari environments, DWS demonstrates substantial performance improvements over existing methods, attributed to its world models having acquired comprehensive dynamics knowledge for action simulation. Specifically, in Breakout, DWS achieves a remarkable 7\u00d7 performance gain compared to SOTA methods. This superior performance, particularly in terms of sample efficiency, can be"}, {"title": "5.3 Offline Model-Based Reinforcement Learning", "content": "We further explore the potential of leveraging trained video world models to augment offline datasets for policy enhancement. Using CoinRun and Ninja environments as case studies, we first establish baseline datasets by collecting 1M expert trajectories for each environment using a well-trained PPO agent, following Mediratta et al. (2024). For each evaluated environment, we employ Open-Sora+DWS to synthesize an additional 1M state-action transitions during training, effectively doubling the size of the original datasets. To validate the effectiveness of this data augmentation approach, we evaluate the performance improvements using two different offline RL algorithms: Conservative Q-Learning (CQL) (Kumar et al., 2020) and Implicit Q-Learning (IQL) (Kostrikov et al., 2022). As shown in Table 3, augmenting offline RL with world model-generated data during training significantly enhances performance across different algorithms and environments. These results demonstrate that the DWS-trained world simulator can generate meaningful state-action-reward transitions that effectively supplement the offline dataset for policy learning. The success of this approach highlights the potential of using pre-trained video models as world simulators for reinforcement learning applications. Furthermore, these results validate that DWS-trained models can generate accurate dynamic transitions, making them valuable tools for policy learning."}, {"title": "6 Conclusion and Limitation", "content": "In this paper, we present DWS, a novel approach that efficiently adapts pre-trained video generative models as world simulators by leveraging their rich prior knowledge learned from large-scale datasets for downstream action-conditioned simulation. Our framework introduces a lightweight action-conditioned module that enables action-frame alignment and can be seamlessly integrated into various model architectures, and a motion-reinforced loss specifically designed to model inter-frame pixel dynamics crucial for accurate world simulation. Extensive experimental results demonstrate that DWS significantly improves both video prediction quality and MBRL performance, leading to meaningful applications. However, DWS currently is limited in modeling videos with extended temporal horizons or high spatial resolutions. We leave it as future work."}, {"title": "A Details of Dataset Collection", "content": "A.1 Atari\nIn our experiments on two Atari games - Breakout and Battle Zone, we utilized a well-established baseline for Atari, Deep Q-Learning (DQN) agent (Mnih et al., 2013), to collect the offline dataset for tuning. Specifically, we adopted an open-source DQN implementation provided at https://github.com/vwxyzjn/cleanrl, trained the agent for 1 million environment steps, and stored the replay buffer as the offline dataset. Since our work focuses on developing a more accurate world model, we intentionally initialized these two game environments without enabling random sticky actions. This ensures that the collected trajectories accurately capture the ground-truth state transitions of the environment. Further, we processed the initialized environments by applying a FrameSkipping wrapper with a frame skip of 4 and a ResizingObservation wrapper, which resizes the output RGB observations to a resolution of (84, 84) for DQN agent training. In terms of the hyperparameter of DQN, we followed the default setting provided at https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py.\nA.2 Procgen\nFollowing a similar approach to our Atari collection experiments, we collected replay datasets from two Procgen games: Coinrun and Ninja. The data collection process utilized a publicly available Proximal Policy Optimization (PPO) implementation (Huang et al., 2022), accessed through https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_procgen.py. For each game, we collected 1 million state transitions at the default Procgen resolution of 64 \u00d7 64 pixels for DWS training."}, {"title": "B Implementation Details", "content": "B.1 Details of Base Models\nOpen-Sora. Open-Sora, based on rectified-flow (Liu et al., 2022) and spatial-temporal transformer (Chen et al., 2023, Ma et al., 2024) architecture, is a text-and-frame-conditioned video generation model. For our implementation of DWS, we utilized Open-Sora version 1.2 as our base model. To maintain consistency with the original architecture, we employed the same Variational Autoencoder (VAE) provided in the Open-Sora library. However, to optimize computational efficiency, we adopted T5-small for text encoding instead of the original text encoder. Given that the original Open-Sora model comprises 1.1 billion parameters, which poses significant computational constraints for inference and downstream applications, we strategically initialized our model using only the first 12 layers of the Open-Sora architecture. This modification substantially reduced the model's computational footprint while preserving essential generative capabilities. Our proposed action-conditioned module is integrated into each layer of Open-Sora to improve action-frame alignment.\niVideoGPT. iVideoGPT is an autoregressive transformer-based architecture that builds upon the LLaMA architecture (Touvron et al., 2023). The model extends the base architecture by incorporating specialized reward prediction head-layers, enabling it to perform both video generation and reward estimation tasks. A distinctive feature of iVideoGPT is its context-aware tokenizer, which implements compressive tokenization for efficient video prediction. We initialize our model with weights from https://huggingface.co/thuml/ivideogpt-oxe-64-act-free, including a tokenizer of"}, {"title": "B.2 Implementation of Motion-Reinforced Loss", "content": "For diffusion-based models, which use a squared l2 distance for computing loss function and conducting gradient updates, their original loss functions can be represented as follows:\nL_{prev} = E[||g - p_\u03b8(x_t, t, y)||_2],\nwhere y is the condition, and g represents the target signal to be estimated, which takes different forms depending on the model architecture: For DDPM-style diffusion models (Ho et al., 2020), it corresponds to the ground-truth noise, while for flow-matching-based models (Liu et al., 2022), it represents the velocity. Given that the output of p\u03b8 and the video embedding x\u0142 share identical dimensionality, thus we can incorporate the motion-based weights w into Lprev as follows:\nL_{motion} = E[\u03c9||g \u2013 p_\u03b8(x_t, t, y)||_2].\nFor autoregressive transformer-based models, which use a cross-entropy loss for training, their original loss functions can be represented as follows:\nL_{prev} = - \u03a3_{i=t}^L log p(x_i | x_{<i}),\nwhere xi denotes discrete tokens embedded in the transformer, and L is the sequence length. In this case, we compute w as follows:\n\u03c9_{i+1} = cI(x_{i+1}=x_i),\nwhere c = e and I denotes an indicator function. Thus, we obtain the final Lmotion for transformer-based models:\nL_{motion} = \u03a3_{i=t}^L \u03c9 log p(x_i | x_{<i}),"}, {"title": "B.3 Implementation of the MBRL Algorithm", "content": "We develop a simple model-based RL algorithm using a DWS-trained world model within the MBPO framework (Janner et al., 2019a, Wu et al., 2024), with PPO (Schulman et al., 2017) as the base actor-critic RL algorithm. We provide the pseudo-code in Alg. 1. We build our codes upon the implementation in https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_procgen.py for Procgen environments and https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_atari.py for Atari environments, using the same hyperparameters and architecture for actor-critic learning. Hyperparameters specific to model-based RL are listed in Table 4. Following Dreamerv3 (Hafner et al., 2023), we use a symlog transformation for reward prediction."}]}