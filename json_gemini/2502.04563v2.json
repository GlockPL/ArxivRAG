{"title": "WaferLLM: A Wafer-Scale LLM Inference System", "authors": ["Congjie He", "Yeqi Huang", "Pei Mu", "Ziming Miao", "Jilong Xue", "Lingxiao Ma", "Fan Yang", "Luo Mai"], "abstract": "Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh-based architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to fully exploit these accelerators.\nWe introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as \"Plummer\") that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators.\nEvaluations show that WaferLLM achieves 200\u00d7 better wafer-scale accelerator utilization than state-of-the-art systems. On a commodity wafer-scale accelerator, WaferLLM delivers 606\u00d7 faster and 22\u00d7 more energy-efficient GEMV compared to an advanced GPU. For LLMs, based on 16-bit data type, WaferLLM achieves 2700 toks/sec/req decode speed on Llama3-8B model and 840 toks/sec/req decode speed on Qwen2-72B model, which enables 39\u00d7 faster decoding with 1.7\u00d7 better energy efficiency. We anticipate these numbers will grow significantly as wafer-scale AI models, software, and hardware continue to mature.", "sections": [{"title": "1 Introduction", "content": "Large Language Model (LLM) inference is a rapidly growing workload. It has two phases [14]: (i) the prefill phase, which processes input tokens (the prompt) and spends most of its cycles on General Matrix Multiply (GEMM); and (ii) the decode phase, which generates tokens one by one in an autoregressive manner, primarily performing General Matrix-Vector Product (GEMV). Decode requires repeatedly loading the entire LLM model into on-chip memory, with GEMV dominating its cycles. Since LLMs generate many tokens, inference is constrained by GEMV latency, making it inherently memory-bandwidth-bound.\nTo address memory bandwidth bottlenecks, AI accelerators are increasingly adopting system-on-wafer integration [20]. This approach scales chip area to a full wafer, up to 100\u00d7 larger than a typical GPU die, enabling significantly more on-chip cores, memory and bandwidth. Examples include Cerebras WSE [23] and upcoming Tesla Dojo [38]. The Cerebras WSE-2, for instance, integrates 850,000 cores with 40GB of on-chip memory\u20141000\u00d7 more than GPUs\u2014and provides 22PB/s memory bandwidth, 7000\u00d7 higher than GPUs. TSMC predicts widespread adoption of system-on-wafer integration due to its performance advantages, energy efficiency in connecting dies, and lowering cost, with IEEE forecasting a wave of wafer-scale computers by 2027 [20].\nUnlocking the potential of wafer-scale accelerators is challenging because current LLM systems rely on shared memory architectures typical of GPUs and TPUs. Wafer-scale accelerators, however, adopt network-on-chip (NoC) designs that interconnect millions of cores with local memory in a massive-scale, mesh-based memory architecture. This architecture far exceeds the scale of on-chip crossbars (e.g., one-hop NUMA such as GraphCore IPU), multi-socket NUMA [2], and high-density AI clusters (hundreds of GPUs per pod) [15]. Without fully addressing this fundamental shift in memory architecture, directly applying designs from state-of-the-art systems like T10 [24] and Ladder [41] to wafer-scale devices often results in extremely poor performance.\nTo address these challenges, we propose a device model that captures the critical hardware properties of wafer-scale accelerators, highlighting key differences from shared-memory devices. This model enables us to evaluate current LLM inference design principles, identify non-compliant areas, and pinpoint where new approaches are required. Guided by this model, we can achieve an ambitious system design: running complete LLM inference on a single chip, minimizing costly off-chip communication and maximizing on-chip memory bandwidth utilization.\nThe above idea drives the design of WaferLLM, the first wafer-scale LLM inference system, yielding several contributions:\n(1) Device model for wafer-scale accelerators. We propose the PLMR model\u00b9, which captures the following hardware properties of wafer-scale accelerators: (i) Massive Parallel cores (P): Millions of cores can be integrated on a large wafer, requiring systems to effectively partition LLMs and their operations. (ii) Highly non-uniform memory access Latency (L): Inter-core data access exhibits significant variation, with"}, {"title": "2 Background and Motivation", "content": "An LLM inference system typically performs auto-regressive token-by-token generation, as illustrated in Figure 1. The model comprises multiple transformer layers, dominated by self-attention and feedforward blocks. Inference operates in two phases: prefill and decode. The total cycles of the prefill phase are dominated by GEMM operations (shown by 1). Similarly, the total cycles of the decode phase are dominated by GEMV operations (shown by 2).\nLLM inference is memory bandwidth-bound. Model weights (10-100 GB) must be repeatedly fetched from exter-"}, {"title": "2.2 Reasons for wafer-scale accelerators", "content": "To increase memory bandwidth, accelerator designers are increasingly adopting system-on-wafer integration [20] for several reasons:\nPerformance advantages. System-on-wafer technology allows trillions of transistors to be integrated into a single wafer-scale chip\u2014100\u00d7 more than a typical GPU die. This enables millions of AI-optimized cores, providing tens of GBs of on-chip memory and up to tens of PB/s memory bandwidth\u20141,000\u00d7 higher than a standard GPU\u2019s several TB/s. Future wafer-scale chips can also attach 40-80\u00d7 more HBM chips to their edge compared to a standard die [20].\nIntegration efficiency. System-on-wafer excels at integrating massive parallel cores, with wafer-based die-to-die connections offering up to 10\u00d7 more bandwidth per unit area and nearly 100\u00d7 better power efficiency per bit than conventional PCB-based I/O (e.g., NVIDIA NVLink).\nLower cost. Wafer-scale integration can lower the manufacturing cost, since the significant fraction of the cost of fabrication (typically 30-50%) is related to testing and packaging the individual chips [44]. Additionally, wafer-scale integration has made notable progress in yield improvement. Companies such as TSMC are also developing techniques to integrate fully tested dies on a single wafer, further enhancing yield."}, {"title": "2.3 Challenges for wafer-scale LLM inference", "content": "The key challenge in leveraging wafer-scale accelerators for LLM inference is their shift to a distributed, non-uniform memory architecture on a single chip. Current LLM systems are optimized for shared memory (single chip) or fully connected architectures (e.g., GPU pods). However, as on-chip memory size grows, these architectures face exponential manufacturing costs and performance degradation, driving the need for a distributed on-chip architecture.\nAI accelerator designers predominantly use a mesh-like network-on-chip (NoC) to connect massive cores (ranging from hundreds of thousands to millions). The mesh topology is favored for its efficiency in core arrangement, enabling effective cooling [28], power delivery [18], and cost-efficient wiring [37, 42], with each core communicating only with nearby neighbors. Alternative topologies, such as 3D torus or tree structures, are impractical due to high on-chip wiring costs. Therefore, wafer-scale chip makers such as Cerebras WSE [23] and Tesla Dojo [38] adopt massive-scale mesh architectures. Even non-wafer-scale accelerators such as Meta MTIA [29], Tenstorrent [17], and others [4, 30] use mesh to scale cores on a chip.\nThe massive-scale mesh architecture presents challenges for several LLM operations due to their high data movement demands: (i) managing LLM models and KV cache, (ii) GEMM operations during the prefill phase, and (iii) GEMV operations during decoding. Other operations, such as element-wise computations such as dot-product and activation functions, require no data movement and naturally benefit from parallelism. Operations needing allreduce, such as RMSNorm and Softmax, can leverage GEMV solutions."}, {"title": "3 Device Model for Wafer-Scale Accelerators", "content": "We develop the PLMR model to capture the unique hardware properties of wafer-scale accelerators and to motivate system requirements needed for utilizing this emerging hardware.\n(1) Massive Parallelism (P): A wafer-scale accelerator can easily be equipped with millions of parallel cores, compared to thousands in GPUs. Each core features a local hardware pipeline that overlaps data ingress, egress, computation, and memory access at the cycle level. This requires the computation to be partitioned at a massive scale and a fine-grained schedule to overlap computation, memory access, and NoC communication."}, {"title": "3.1 The PLMR model", "content": "(2) Highly non-uniform memory access Latency (L): Accessing memory on other cores in a mesh exhibits highly non-uniform latency. In a mesh with $N_w \\times N_h$ cores, the maximum NoC hops to a remote core is max($N_w, N_h$). For a million-core mesh, this can reach 1000 hops, causing a 1000\u00d7 latency difference between local and remote memory access. Therefore, it is crucial for the computation to minimize long-range communication whenever possible.\n(3) Constrained local Memory (M): Each core has a small local memory (tens of KBs to several MBs), as performance and energy efficiency decline with larger capacities [43]. As a result, computation data must be explicitly partitioned into fine-grained chunks to fully fit within the constraints of each core\u2019s local memory.\n(4) Constrained Routing resources (R): The message size in the NoC of a wafer-scale accelerator is extremely limited (e.g., a few bytes). This constraint requires message headers (e.g., address encoding) to be restricted to just a few bits, maximizing the capacity for actual data transfer. Consequently, only limited routing paths can be used, and the software system must carefully plan these paths.\nWe expect these properties to remain relevant, as they are rooted in the fundamental characteristics of hardware and its manufacturing process. The PLMR model applies to both current (Cerebras WSE) and future (Tesla Dojo) wafer-scale devices. Even some non-wafer-scale devices with mesh-based NoC architectures, such as Tenstorrent Blackhole [17], can be represented by PLMR with adjusted parameters for parallelism (P), the size of the mesh (L), or relaxed constraints on local memory (M) and routing resources (R)."}, {"title": "3.2 Limitations of state-of-the-art approaches", "content": "Leveraging the PLMR model, we analyze why existing AI systems fail to fully utilize wafer-scale accelerators. To run an LLM model on a wafer-scale accelerator, we generally have two choices: (i) abstract the distributed local memory in each core as a shared memory and directly access data placed in a remote core through NoC; and (ii) explicitly partition computation into distributed cores and use message passing to exchange necessary data. We analyze two types of representative systems: LLM runtime or DNN compilers for shared memory architecture such as GPUs, e.g., Ladder [41]; and the SOTA compiler for distributed on-chip memory architectures, e.g., T10 [24] for GraphCore IPU.\nShared-memory system. A shared-memory-based DNN compiler such as Ladder usually assumes a uniform memory access pattern within the underlying memory hierarchy, which cannot tolerate the 1000\u00d7 latency variance in wafer-scale accelerators when accessing data from remote memory (failing in L). Moreover, these compilers [10, 27, 36, 41, 47, 49, 52] often focus primarily on partitioning computation, with less emphasis on optimizing data partitioning. This approach can easily lead to significant data duplication and violate the memory constraint requirements (failing in M). Finally, these compilers are unaware of the communication distance of each core, poorly addressing the constraint of routing resources.\nDistributed-memory system. The T10 system [24] is designed for AI accelerators with an on-chip crossbar which ensures a constant hop of memory access to other cores on the same chip. T10 handles small local memory and balances communication loads, addressing memory constraints (M) and routing resource limits (R). However, on a PLMR device, it fails to account for varying hop distances (failing in L) and scales to thousands, not millions, of cores (failing in P)."}, {"title": "4 Wafer-Scale LLM Parallelism", "content": "We present wafer-scale LLM parallelism, featuring new designs across prefill, decode and KV cache management."}, {"title": "4.1 Prefill parallelism", "content": "The parallelism for LLM prefill must ensure compliance with the PLMR model. Key challenges include: (i) Handling multiple large matrices during prefill, requiring effective dimension partitioning to achieve million-core parallelism (P); (ii) Optimizing GEMM operations, which involve further partitioning and overlapping computation and communication, to minimize long-range communication overhead (L), respect local memory constraints (M), and account for limited routing resources (R); and (iii) Handling matrix transposes, which are costly on a NoC (L) but often required for sequential GEMM operations.\nDesigning fine-grained partitioning for million-core parallelism. To achieve high chip utilization, we propose partitioning two dimensions of the input activation and weight matrices along both the X- and Y-axes of cores. This approach enables finer-grained, million-scale parallelism compared to existing methods [12, 14, 31, 34], which typically partition only the embedding dimension, resulting in insufficient parallelism on PLMR devices.\nWe illustrate this partitioning using self-attention and feedforward, as shown in Figure 3. For this discussion, we define the following annotations: the input activation A and weight W are multi-dimensional tensors during the prefill process. B represents the batch size, L the sequence dimension, E the embedding dimension, H the head dimension, and F the hidden dimension in the feedforward block. As shown by 1, the partitioning layout of A is represented as $BL_{yE_x}$, where the L dimension is partitioned along the Y-axis of cores, and the E dimension along the X-axis of cores. Similarly, all weight matrices ($W_o$, $W_K$, $W_v$, $W_{in}$, and $W_{out}$) are partitioned across both dimensions.\nDesigning PLMR-compliant distributed GEMM. We propose replacing conventional GEMM operators, designed for shared memory architectures, with a newly designed PLMR-compliant distributed GEMM during the prefill phase (as shown in of Figure 3). Unlike TPU and GPU systems that primarily rely on allgather operations for GEMM, PLMR-compliant distributed GEMM algorithms achieve high NoC bandwidth utilization while respecting local memory and routing constraints, ensuring compliance with the L, M, and R properties. This PLMR-compliant distributed GEMM is fully described in Section 5.\nUsing transposed distributed GEMM to avoid matrix transpose. We propose a transpose-free parallelism plan for prefill to avoid matrix transpose, a common operation in LLM systems designed for shared memory architectures. The L property in PLMR highlights that matrix transposition is particularly costly on a wafer-scale device. It requires a core on one corner of the mesh to send data to the opposite diagonal corner, creating a long-range communication path.\nOur transpose-free parallelism plan leverages transposed distributed GEMM (denoted as dist-GEMM-T) [11, 39] to compute $Q@K^T$ during LLM prefill, as shown by $\\rightarrow$ in Figure 3. Specifically, the intermediate Q and K tensors, generated by multiplying X with $W_o$ and $W_k$, require transposing K before proceeding with dist-GEMM operations due to the on-chip partition shape."}, {"title": "4.2 Decode parallelism", "content": "The parallelism strategy for LLM decode must address its memory-bandwidth-intensive nature, presenting several challenges: (i) Decode uses smaller matrices than prefill due to limited input sequences and batch sizes, requiring careful parallelism when dimensions are insufficient for partitioning; (ii) The phase heavily relies on GEMV operations, which are less compute-intensive than GEMM, resulting in short computation phases with limited overlap with communication, making GEMV vulnerable to long-range communication overhead on a NoC (L) and requiring adherence to local memory and routing constraints (M and R); and (iii) Sequential GEMV operations introduce costly matrix transpose on a NoC, risking violation of the L property.\nDesigning fine-grained replication to enable parallelism at minimal communication cost. When tensor dimensions are insufficient to achieve the high parallelism required for decode, we propose fine-grained replication of tensors in LLMs, specifically replicating the sequence dimension, where the sequence length equals the prompt length during prefill phase and equals 1 during the decode phase. This approach offers two key advantages: (i) It improves parallelism and ensures balanced loads across all cores, and (ii) It avoids additional communication operations such as allreduce. As shown by 1 in Figure 4, the E dimension is partitioned along the y-axis, and the L dimension is replicated along the x-axis, represented as $BE_{yL^*}$. Weight matrices W are partitioned across both dimensions, consistent with the prefill phase.\nOur fine-grained replication differs from recent work on long-context/sequence inference systems [45, 50], which selectively replicate certain dimensions during the prefill phase rather than the decode phase.\nDesigning PLMR-compliant distributed GEMV. We found that existing GEMV implementations fail to fully comply with PLMR requirements due to long-range communication and excessive routing resource consumption at each core. To address this, we propose a PLMR-compliant distributed GEMV, utilizing this new implementation throughout the decode phase (as detailed in of Figure 4). A comprehensive description of this GEMV design is provided in Section 6.\nPre-optimizing model weight placement to avoid matrix transpose. To avoid matrix transpose during decode, we propose pre-optimizing the model weight layout for decode, particularly for the distributed GEMV operation, to eliminate matrix transpose. While this introduces re-placement overhead between prefill and decode phases, the overhead is far smaller than that of sequential matrix transpose during token generation.\nFigure 4 illustrates this proposal, detailed in . Specifically, we optimize the placement of weights such as $W_o$ and $W_{out}$ for distributed GEMV in decode, differing from their layout in the prefill phase. This approach also removes the need for transpose operations in calculating $Q@K^T$ during decode"}, {"title": "4.3 Shift-based KV cache management", "content": "KV cache management on PLMR devices is challenging as it requires storing large data across distributed cores while adhering to local memory constraints (M) and distributing KV cache computations to achieve high parallelism (P). To address these, we have the following insights:\nExisting concatenate-based management causes skewed core utilization. Current KV cache management methods primarily concatenate newly generated KV vectors to the existing cache. While efficient in shared memory architectures, this concatenate operation leads to highly skewed core utilization on PLMR devices, as shown in 1 of Figure 5, where only core in a row is responsible for storing and computing over the newly generated KV vector. After several token generation steps, this only core quickly becomes the bottleneck, as depicted in of Figure 5, causing skewed memory usage and violating the M in PLMR. Moreover, the imbalanced KV cache distribution across cores results in inefficient parallelism, violating the P property.\nProposing shift-based management for balanced core utilization. We propose a shift-based KV cache management strategy that evenly distributes cache data across all cores. Instead of concatenating new KV cache vectors at the end, this method performs a balancing shift operation, where each row transfers the oldest KV cache data to the row above, as shown in of Figure 5. When new KV data arrives, each core checks its local capacity against its neighbors. If equal, upward shifts are triggered, with each row receiving data from below and passing some to the row above. As illustrated in 4, this ensures even KV cache distribution across all cores.\nThe upward shifts utilize all NoC links in parallel, maintaining high performance and satisfying the P property. The physical placement of KV cache aligns with logical continuity, adhering to the L property. This method also fully resolves the M violation issue observed in the last row of cores with the concatenate-based approach."}, {"title": "4.4 Implementation details", "content": "We outline several implementation details below:\nPrefill and decode transition. Prefill and decode require distinct strategies. To handle the transition efficiently, we reshuffle KV cache and weights through the fast NoC which often provides 100s Pbits/s aggregated bandwidth, completing instantly without relying on slower off-chip memory.\nParallelism configuration. We empirically determine the scalable parallelism for LLM operators. Automatic parallelism configuration is left for future work.\nVariations of self-attention. WaferLLM supports variations of Self-Attention, including Grouped Query Attention [3], Multihead Attention [40], and Multi-query Attention [6]. These differ by performing dist-GEMM, dist-GEMV and dist-GEMM-T locally after grouping by head dimensions."}, {"title": "5 Wafer-Scale GEMM", "content": "In this section, we introduce MeshGEMM, a scalable distributed GEMM for massive-scale, mesh architectures."}, {"title": "5.1 PLMR compliance in distributed GEMM", "content": "To identify an scalable distributed GEMM for PLMR devices, we define the following metrics: (i) Paths per core: The number of routing paths per core, with fewer paths ensuring compliance with the R property. (ii) critical path: The longest communication path in each step to transmit submatrix (as the red lines in Figure 6), with fewer hops adhering to the L property. (iii) Memory per core: The memory required per core, with lower usage ensuring the M property.\nWe analyze current distributed GEMM methods and show how MeshGEMM meets these metrics:\n(1) GEMM via Allgather is commonly used in GPU and TPU pods for distributed GEMM [31, 34, 51]. Its longest communication path in each step is one core gathering data from the farthest cores, shown as the red line in Figure 61, and N steps to complete the allgather. Each core creates N communication paths to neighbors in its row and column (violating R). The gather in each step spans the critical path with $O(N)$ hops (violating L), and each core uses O(1/N) memory due to inflated working buffers, far exceeding the $O(1/N^2)$ for local submatrices (violating M).\n(2) SUMMA is Cerebras\u2019 default choice for distributed GEMM on its wafer-scale engine [8]. Its longest communication path in each step is where one core broadcasts data to the farthest core along the column or row, shown by the red line in of Figure 6. Every core creates N communication paths (violating R) and spans the critical path with $O(N)$ hops (violating L) in the longest path. While SUMMA improves memory usage compared to AllGather, requiring only a working set equal to the size of locally partitioned submatrices, it still doubles memory usage.\n(3) Cannon is mesh-optimized choice for distributed GEMM [7], popular in supercomputers. Its longest communication path in each step is the head cores send data to the tail cores. As shown in of Figure 6, each core communicates with two neighbours in a 2D torus, and only needs O(1) communication paths and optimal memory usage of $O(1/N^2)$. However, it incurs the critical path with $O(N)$ hops as the red line, violating L.\n(4) MeshGEMM (Ours) is a distributed GEMM which complies with the PLMR model. Its longest communication path in each step is shown as the red line in of Figure 6. Each core communicates with two neighbors, two hops away (proven in later sections to be scalable for mesh architectures). This design achieves O(1) communication paths per core needed and optimal memory usage of $O(1/N^2)$, similar to Cannon. Crucially, it bounds the critical path to 2 hops with $O(1)$ complexity, making it uniquely capable of addressing the L property."}, {"title": "5.2 Design intuitions and scalability analysis", "content": "Our design involves two steps: (i) We ensure algorithm correctness using a cyclic shifting process for GEMM, and (ii) We prove that two-hop communication on this cycle is the minimal distance required to satisfy the L property.\nCyclic shifting. Cyclic shifting enables MeshGEMM to satisfy the M and R properties by limiting communication to two neighbors and minimizing memory usage. It ensures correct GEMM results, following reasoning similar to Cannon [7]. As illustrated in of Figure 6, a logical circle of 5 cores is flattened into the physical communication mapping, with a critical path from head core to tail core.\nInterleaving. For the flatten communication plan, we would like to minimize the length of the critical path further, thus satisfying the L property. Our key intuition here is to introduce an INTERLEAVE operation to find the mapping relationship from logical to physical, defined in Algorithm 1. As shown by of Figure 7, MeshGEMM first insert core 1 in between core 0 and 4 and core 2 in between core 4 and 3 to form a logical mapping, and then call the INTERLEAVE operation to get the send to and receive from neighbours\u2019 index, resulting in a permutated, equivalent communication plan as shown by in Figure 7. For example, there are 5 cores total (N=5), so physical core 2 (index=2) sends data to physical core 4 (send_index=4) and receives from physical core 0 (recv_index=0).\nScalability analysis. We can prove that the two-hop distance created by INTERLEAVE cannot be further reduced. The proof relies on the fundamental properties of sequential arrangements: if we attempt to create a circular sequence where each number differs from its neighbors by exactly one hop, we encounter a mathematical impossibility. This can be understood by visualizing the numbers as points on a line: while adjacent numbers can be connected, the endpoints of the sequence cannot simultaneously maintain single-hop differences with their neighbors while forming a circle.\nNote that our discussion, based on a 1D array, naturally extends to a 2D mesh, as the 1D array corresponds to the mesh\u2019s X-axis and Y-axis due to their symmetry."}, {"title": "5.3 The MeshGEMM algorithm", "content": "We outline the key steps of MeshGEMM below:\n(1) Initialization: Consider C = A \u00d7 B. MeshGEMM will partition A and B into tiles $A_{sub}$ and $B_{sub}$ along two dimensions, forming N \u00d7 N tiles, which are distributed across the cores. Each core receives one tile of $A_{sub}$ and one of $B_{sub}$. MeshGEMM will then use INTERLEAVE to initialize the neighbor\u2019s positions for each core.\n(2) Alignment: Each core will then align with neighbors to align the input submatrices in a way that ensures every core in the distributed system begins with the appropriate operands for the matrix multiplication process.\n(3) Compute-shift loop: Each core operates with a compute-shift loop involving N steps of communication and computation. In each step, every core computes the partial sum of its corresponding $C_{sub} = A_{sub} \\times B_{sub} + C_{sub}$. Meanwhile, shift $A_{sub}$ along the X-axis and $B_{sub}$ along the Y-axis to get new A'sub and B' sub for the next step computation as we shown in Figure 7. After N steps, the accumulated $C_{sub}$ is returned."}, {"title": "5.4 Implementation details", "content": "Handling non-square mesh. For a non-square mesh $N_h \\times N_w$ ($N_h\\neq N_w$), the A and B matrices can be logically partitioned into $N_{lcm} \\times N_{lcm}$ cores, where $N_{lcm}$ is the least common multiple of $N_h$ and $N_w$.\nTransposed distributed GEMM. The above algorithm key steps can be applied to the computation of C = A \u00d7 $B^T$, the dist-GEMM-T in Figure 3 to avoid transposing B on mesh. It does not require alignment before computation and only necessitates N steps two-hop compute-shift for the right matrix B along the Y-axis. After each shift step, each core computes $C_{sub} = A_{sub} \\times B_{sub}$, followed by a ReduceAdd of $C_{sub}$ along the X-axis. After N steps, the final matrix C is obtained."}, {"title": "6 Wafer-Scale GEMV", "content": "In this section, we describe MeshGEMV, a scalable GEMV algorithm for PLMR devices."}, {"title": "6.1 PLMR compliance in distributed GEMV", "content": "The completion time of a distributed GEMV is primarily determined by an allreduce operation that aggregates partial results from all selected cores and broadcasts the aggregated results back to all cores. So, we define the number of add-operations (hops) in the longest aggregation path as the critical path in GEMV. Below, we analyze common distributed GEMV implementations in LLM systems and demonstrate that MeshGEMV is the only approach fully compliant with the PLMR model.\n(1) GEMV with pipeline allreduce is commonly used in TPU pod systems [34] and as the default in Cerebras demo [9]. As shown by 1 in Figure 8, it bounds routing resource usage to O(1) per core (meeting R in PLMR). However, its longest aggregation path is from tail to head cores, as shown in the red line, and spans the critical path at O(N), violating the L property.\n(2) GEMV with ring allreduce is commonly used in GPU pod systems, where it is the default configuration. As shown by in Figure 8, it bounds routing resource usage to O(1) (meeting R in PLMR). However, it spans O(N) hops in the critical path, violating the L property.\n(3) GEMV with two-way K-tree allreduce (Ours). As shown by in Figure 8, we build a balanced K-tree to reduce from two-way; its longest aggregation path is from the head or tail core to the tree root core. The critical path is O($NK$) which can address the L. The max number of communication paths at each root core is O(K), and can meet the R limitation by adjusting the K."}, {"title": "6.2 The MeshGEMV algorithm", "content": "We will outline the key steps of MeshGEMM below:\n(1) Initialization: Consider C = A \u00d7 B and A is a vector. MeshGEMV will partition B into tiles $B_{sub}$ along two dimensions, forming N \u00d7 N tiles and distributed across the cores. For A, MeshGEMV will partition it along the vector length, forming N tiles distributed on one axis and replica A on another axis. Each core receives one tile of $A_{sub}$ and one of $B_{sub}$. Then we determine which cores form a group to obtain aggregated results in each phase based on the K-tree.\n(2) Parallel computation: In this stage, each core performs a local GEMV $A_{sub} \\times B_{sub}$ to obtain $C_{sub}$ partial sum.\n(3) Aggregation: The aggregation step primarily involves using the two-way K-tree allreduce we design. The key steps as follows: (i) In the 1st-phase, each group performs group reduction and obtains the partial sum of $C_{sub}$ at the root core of each group. (ii) In the kth-phase, the results from the (k-1) th-phase are reduced to the root cores of each group in the kth-phase. After K times repeating, C can be obtained by concatenating the $C_{sub}$ from all K-tree root cores. (iii) Optionally, a broadcast operation from the root core of the K-tree may follow, depending on whether continuous GEMV is required.\nScalability Analysis. As shown in of Figure 8, this method scales efficiently with parallelism and meets the L property by selecting an appropriate K. It requires K + 1 paths at the tree root core but allows flexible adjustment of K to address R based on hardware limitations."}, {"title": "7 Evaluation", "content": "We extensively evaluated WaferLLM against various state-of-the-art methods and systems. Our results show that:\n(1) WaferLLM achieves orders of magnitude speedup over T10 and Ladder in LLM inference (\u00a77.1);\n(2) WaferLLM's MeshGEMM and MeshGEMV achieve strong performance and scalability over state-of-the-arts (\u00a77.2);\n(3) WaferLLM's shift-based KV cache management enables over 360\u00d7 more token capacity (\u00a77.4);\n(4) WaferLLM on Cerebras WSE-2 achieves up to 38.6\u00d7 throughput and 1.7\u00d7 energy efficiency compared to VLLM on A100 in LLM inference (\u00a77.5).\nExperiment setup. We evaluate WaferLLM on a server with Cerebras WSE-2. WSE-2 has 850,000 Cores, each with a Compute Engine (CE) operating at a maximum 1.1 GHz. Each clock cycle can fetch two 32-bit operands from SRAM, perform a multiply-accumulate operation, and then write back to SRAM. Each core also has a fabric router that can send or receive 32-bit messages from neighbouring cores with a single clock cycle. Additionally, each core contains 48KB of SRAM, with the chip totalling 40GB of aggregated SRAM [23].\nWe compare WaferLLM with two DNN compilers: (i)T10 [24], the state-of-the-art compiler for AI accelerators with inter-core connections and distributed on-chip memory, and (ii)Ladder [41], the state-of-the-art compiler for shared memory architectures. For T10, we implemented it on WSE-2, treating each core as part of a distributed memory system interconnected by a crossbar, despite the actual mesh topology. T10 maps data to core IDs and fetches data from local SRAM as required. For Ladder, we treated the distributed memory architecture of the chip, interconnected by mesh, as unified memory, requiring collective communication over the NoC to access data.\nLLM models. Our evaluation includes various representative LLMs of different sizes and architectures. Specifically, LLaMA3-8B and LLaMA2-13B are widely used open-source LLMs, with LLaMA3 using group-query attention instead of multi-head attention to reduce KV cache usage. CodeLLaMA-34B is a specialized LLM for coding tasks, while QWen2-72B, another popular LLM, is renowned for its high model quality."}, {"title": "7.1 LLM inference", "content": "We first report the end-to-end performance of WaferLLM compared to T10 and Ladder. To provide deeper insights, we further analyze the performance by breaking down the execution into prefill and decode phases.\nEnd-to-end throughput. shows the inference throughput (i.e., tokens per second) of LLaMA3-8B and L"}]}