{"title": "Optimizing Few-step Sampler for Diffusion Probabilistic Model", "authors": ["Jen-Yuan, Huang"], "abstract": "Diffusion Probabilistic Models (DPMs) have demonstrated exceptional capability of generating high-quality and diverse images, but their practical application is hindered by the intensive computational cost during inference. The DPM generation process requires solving a Probability-Flow Ordinary Differential Equation (PF-ODE), which involves discretizing the integration domain into intervals for numerical approximation. This corresponds to the sampling schedule of a diffusion ODE solver, and we notice the solution from a first-order solver can be expressed as a convex combination of model outputs at all scheduled time-steps. We derive an upper bound for the discretization error of the sampling schedule, which can be efficiently optimized with Monte-Carlo estimation. Building on these theoretical results, we purpose a two-phase alternating optimization algorithm. In Phase-1, the sampling schedule is optimized for the pre-trained DPM; in Phase-2, the DPM further tuned on the selected time-steps. Experiments on a pre-trained DPM for ImageNet64 dataset demonstrate the purposed method consistently improves the baseline across various number of sampling steps.", "sections": [{"title": "1. Introduction", "content": "The great advancement in deep generative model has opened up new possibilities to both Machine Learning community and our living-world. In Computer Vision, this trend is led by Diffusion Probabilistic Model (DPM) [6, 17, 21] which presents impressive creativity that originally believed to be only possessed by mankind. DPM does not rely on a surrogate loss neither an adversarial training, making it efficient to train and scale easily with data magnitude and model size. Breaking records left by other generative models on various tasks including unconditional [6] and conditional image generation, likelihood estimation [10, 19], inpainting [6, 14] and super-resolution [23].\nDPM was initially inspired by molecular dynamic system [17] and introduced two stochastic processes to describe the gradual changes in image over time. In the forward process, images are corrupted by progressively added Gaussian noises. Whereas in the reverse process, information is reconstructed from noise. DPM is trained to denoise images corrupted with different amount of noise called noise levels. The reverse generation process is then a step-by-step inference on less noisy inputs. Song et al. [21] proposed a generalized class of DPM in continuous cases. They revealed the generation process is solving a Stochastic Differential Equation. Though in practice we still inference on the discrete basis, one can regard the specification of noise levels as step-sizes of the numerical solver in continuous DPM. Simulating this process is extremely expensive as thousands of iterations with model called are required to achieve lower global error,i.e. higher fidelity. This property of DPM imposes a natural trade-off between generation quality and speed, preventing a wider application where computational resources are often restricted.\nPrevious work [8, 12, 18] found an evenly distributed noise levels sub-optimal. It is beneficial to take smaller step-sizes when the noise level is low, and steps with relatively high noise level can be skipped without risks of losing too much quality. This empirical discovery has long been adopted by practitioners and intrigued explorations towards sampling algorithms for DPM [18]. Higher-order solvers [8] or adaptive solvers [7] from the literature of numerical solving differential equations have been attempted. However, these methods are proposed without awareness of DPM and do not fully utilize its properties. Some other methods [6, 24] manipulate the underlying probabilistic model of DPM and build a deterministic generation process that shares the same marginal distribution as their stochastic counterparts. In this way, they reduce the number of iterations required by diffusion sampling to a large extent, but the noise level used at each step, i.e., step-size of the solver is chosen empirically. Besides, both of these methods say nothing about truncation error, the optimization of DPM generation process still lacks of theoretical basis.\nIn this study, we focus on the optimization of DPM as a numerical differential equation solver. We analyze the truncation error of common deterministic DPM sampler [8], and discover the deep connection between sampling schedule"}, {"title": "2. Preliminaries", "content": "and DPM training objective. Finally, we derive a two-stage algorithm based on these analysis for optimizing DPM from the practical perspective. Our contributions are as follows:\n1.  We present theoretical analysis in the truncation error of common first-order DPM sampling algorithm;\n2.  We propose an efficient method to optimize diffusion generation process with some per-sample loss;\n3.  Based on these analysis, we derive a novel two-stage method to finetune pretrained models as a numerical differential equation solver for a given sampling steps."}, {"title": "2.1. Diffusion Probabilistic Models", "content": "Ho et al. [6] defined DPM with two stochastic processes named forward and reverse process. For an image data x, the forward process progressively adds small amount of i.i.d. Gaussian noise to it. Due to the Markovian property, the distribution of such a noisy version of image x is:\n$x_i \\sim \\mathcal{N} (\\sqrt{1 - \\sigma_i}x, \\sqrt{\\sigma_i}I)$,\nwhere $\\sigma_i$ is called the noise level at step i. As our primary object is to analyze the reverse process, we denote the minimum noise level as $\\sigma_T$ and the maximum as $\\sigma_0$. One can use re-parameterization of Eq. 1 and easily sample x with:\n$x_i = x + \\sigma_i\\epsilon$,\nwhere $\\epsilon$ is a standard Gaussian random variable with the same shape as x. Information from x eventually vanish as i goes to zero, and $x_0$ becomes in-distinguishable with pure noise. In the reverse process a neural network is applied to reconstruct images from noise. The magical thing in DPM is that this difficult procedure only requires a model learned to denoise a noisy version of x with different noise level $\\sigma_i$, and the training objective of DPM can be written as:\n$\\mathcal{L}_{diff} = E_{x,\\epsilon,i} [\\lambda_i||D_{\\theta} (x_i, \\sigma_i) - x||_2^2]$,\nwhere the denoising function $D_{\\theta}$ explicitly conditions on noise level $\\sigma_i$. $\\lambda_i$ is a weighting factor that is often adopted to enhance training on specific noise levels."}, {"title": "2.2. Diffusion Differential Equation", "content": "Song et al. [21] generalizes the above Markov-based DPM into continuous cases where the two processes are expressed by Stochastic Differential Equation (SDE). Specifically, for the forward process we have an SDE:\n$dx = f(x,t)dt + g(t)dw$,\nwhere $f(x, t)$ is the drift coefficient and $g(t)$ the diffusion coefficient. The noise level is now a continuous function"}, {"title": null, "content": "$\\sigma(t)$ for $t \\in [0,1]$. The marginal distribution $p(x;t)$ induced by this SDE is still a Gaussian $\\mathcal{N} (s(t)x, s^2(t)\\sigma^2(t))$ where $s(t)$ is derived from drift coefficient $f (x, t)$ [8]. Correspondingly, we have another SDE:\n$dx = (f(x, t) - \\frac{1}{2}g^2(t)\\nabla_x \\log p(x;t)) dt + g(t)d\\bar{w}$,\nfor the reverse process. $w$ and $\\bar{w}$ are standard Weiner process and its reverse. Song et al. also introduced Probability-Flow Ordinary Differential Equation (PF-ODE) which is a deterministic process once the initial point is set, but shares the same marginal distribution $p(x; t)$ as Eq. 5. Comparing to its stochastic counterpart, PF-ODE is much more stable in simulating and provides possibility in large-step integration.\nThe choice of $f(x, t)$ and $g(t)$ depend on parameterizations of the underlying diffusion process. Variance Preserving (VP) and Variance Exploding (VE) are two most widely used instances [21] with $\\sigma_0 = 1$ and $\\sigma_{max}$ accordingly. Karras et al. [8] demonstrate both VP and VE models are variations to a canonical one with PF-ODE:\n$dx = -\\sigma(t) \\nabla_x \\log p(x; \\sigma(t))d\\sigma(t)$.\nOur analysis will be conducted on this canonical ODE, and we will later show in \u00a74.1 that our method is valid for both different modeling with consistent improvements."}, {"title": "2.3. Sampling Schedule", "content": "The SDE formulation in \u00a72.2 is in the context of continuous diffusion process and is only for theoretical analysis purpose. In practice $\\sigma(t)$ must be discretized into a strictly decreasing sequence of noise levels $\\sigma_i$, $i \\in \\{0, ..., T\\}$ called sampling schedule. The nabla term in Eq. 6 is modeled by denoising function that $\\nabla_x \\log p(x; \\sigma(t)) = \\frac{D_{\\theta}(x_i; \\sigma_i) - x_i}{\\sigma_i}$.\nMost previous works treat sampling schedule as a hyperparameter[]. However numerical solving Eq. 6 will inevitably induce truncation error, the empirically chosen step-sizes are by no means optimal and may vary across different models. Given the major drawback of DPM, identify the optimal sampling schedule for numerically solving Eq. 6, and the denoising model $D_{\\theta}$ optimized for this sampling schedule can jointly reduce the truncation error of DPM as a numerical differential equations solver, allowing for larger step-sizes and fewer iterations to alleviate the limitations of DPM."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Discretization Loss", "content": "To analyze the truncation error of DPM reverse process, consider the most commonly adopted Euler method [8, 21]."}, {"title": null, "content": "An Euler-step of numerically solving Eq. 6 at step i is:\n$\\hat{x}_{t-1} = x_t + (\\sigma_{t-1} - \\sigma_t) \\frac{\\sigma_{t-1}}{\\sigma_t} \\frac{x_t - D_{\\theta}(x_t, \\sigma_t)}{\\sigma_t} \\\\= \\frac{\\sigma_{t-1}}{\\sigma_t} \\hat{x}_t + (1-\\frac{\\sigma_{t-1}}{\\sigma_t})D_{\\theta} (x_t, \\sigma_t)$,\nwhere the iteration sequence is denoted as {$\\hat{x}_i, i = 0, ...T$}. In the second equation of Eq. 7 we rewrite the forward Euler-step as a linear combination of current iteration point $\\hat{x}_t$ and its denoise output $D_{\\theta} (x_i, \\sigma_i)$ with factor $\\sigma_{t+1}/\\sigma_t$. For a strictly decreasing noise level these factors lie within interval (0,1), and Eq. 7 elucidates the reconstruction of DPM reverse process is by continuously adjusting generated contents through the incorporation of new denoise predictions at each step. Eq. 7 can be recursively applied to sequence {$\\hat{x}_i$} and $\\hat{x}_0$ can then be expanded into a weighted summation of denoise outputs at each iteration:\n$\\hat{x}_0 = \\sum_{t=T}^{1} \\lambda_tD_{\\theta}(x_t, \\sigma_t) + \\lambda_Tx, t = \\frac{\\sigma_0}{\\sigma_T} - \\frac{\\sigma_0}{\\sigma_{T-1}}\\\\= \\frac{\\sigma_0}{\\sigma_t} - \\frac{\\sigma_0}{\\sigma_{t-1}}$,\nwhere $\\lambda_t = \\frac{\\sigma_{i+1}}{\\sigma_{i}} \\sum_t$. Weights in Eq. 8 capture the rate of change in the reciprocal of noise levels that denoise outputs with lower input noise levels are granted higher weights in $\\hat{x}_0$. This interpretation aligns with observations of previous works [1, 3]; they visualized diffusion generation process and found little perceptual contents are generated when it is too noisy for the network to reconstruct any information.\nOur second key observation is the final output $\\hat{x}$ of iterating Eq. 7 is solely given by $D_{\\theta} (\\hat{x}_T, \\sigma_T)$ due to $\\sigma_{T+1} = 0$."}, {"title": null, "content": "From this perspective, sample quality only depends on the capability of denoise model $D_{\\theta} (\\cdot, \\sigma_T)$ and input $\\hat{x}_T$. The former will not be the bottleneck because the noise level is quite small at this point, leaving room for improvements to the latter. An iteration point that is well converged to the data manifold can lead to potential boost in sample quality. Consider the L2-error of $\\hat{x}$ at iteration step T:\n$E_T = ||\\hat{x}_1 - x_1||^2$,\nwhere $x_1$ is sampled with $\\epsilon$ and $x_0$ using Eq. 2. Given a fixed denoising model $D_{\\theta}$, Eq. 9 is then a direct consequence of the sampling schedule.By bringing in Eq. 8 we propose the discretization loss w.r.t. a sampling schedule:\n$\\mathcal{L}_{disc} (\\sigma) = E_{x,\\epsilon,T} [\\sum_{t=T}^{1} \\lambda_tD_{\\theta}(x_t, \\sigma_t) + \\frac{\\sigma_0}{\\sigma_T}x_0 - x_1 ]^2$,\nwhere we denote {$\\sigma_i, i = 0, ...,T$} as $\\sigma$. We next demonstrate sampling according to an optimal schedule $\\sigma^*$ minimizing Eq. 10 provides a lower global-error, i.e., $||x - \\hat{x}||^2$.\nConsider the sequence of denoise outputs {$D_{\\theta} (\\hat{x}_i, \\sigma_i)$}. Given the initial point $x = x + \\sigma_0\\epsilon_0$, {$\\hat{x}_i$} is then a deterministic process and so do their denoise outputs. We denote such a sequence of {$D_{\\theta} (\\hat{x}_i, \\sigma_i)$} as {$T_{\\sigma_i} (x_0)$}, and function $\\tau (\\sigma (t); x_0) = D_{\\theta} (x (t) ; \\sigma (t))$ where x (t) is an integral of Eq. 6 from 0 to t for t \u2208 [0,1]. As \u03c3(t) is a monotonic function, we can consider this ground-truth trajectory in 1/\u03c3 (t) space with change-of-variable. By definition we have \u03c4 (1/\u03c3 (1); $x_0$) = x, and the remaining"}, {"title": null, "content": "demonstration can be done by showing the end point of trajectory {$T_{\\sigma_i} (x_0)$} converges to \u03c4 (1/\u03c3 (1) ; $x_0$).\nTheorem 1 For \u03c3* = arg min, $L_{disc} (\\sigma)$, the backward difference of the i-th element in sequence {$T_{\\sigma_i} (x_0)$} equals to the first-order increment in \u315c (; $x_0$) with step-size $\\frac{\\sigma_{i+1}}{\\sigma_i}$ for i = 1, ...,T \u2013 1.\nTheo. 1 is obtained by setting the derivative of Eq. 10 w.r.t. each $\\sigma_i$ to zero. Detailed proof can be found in Appendix. Theo. 1 shows that in the optimal sampling schedule, the sequence of denoise outputs {$D_{\\theta} (\\hat{x}_i, \\sigma_i)$} approximates function $D_{\\theta} (x(t) ; \\sigma (t))$ with backward Euler-steps. As its end point $D_{\\theta} (\\hat{x}_T, \\sigma_T)$ is also the sample output $\\hat{x}$, the optimal sampling schedule thus guarantees a first-order global error in both {$D_{\\theta} (\\hat{x}_i, \\sigma_i)$} and {$\\hat{x}_i$}, which could not be found in an arbitrarily chosen sampling schedule. We provide a intuitive illustration in Fig. 1."}, {"title": "3.2. Two-stage Finetune", "content": "In Eq. 10 we represent the discretization loss as a function of weighted denoise outputs. By extracting $D_{\\theta} (x_0, \\sigma_0)$"}, {"title": null, "content": "from the summation Eq. 10 can be further simplified as:\n$\\mathcal{L}_{disc} (\\sigma) = [\\sum_{t=T}^{1} \\lambda_t (D_{\\theta} (x_t, \\sigma_t) - \\frac{\\sigma_0}{\\sigma_T}x_0) - x_1 ]^2 + \\sigma_1^2 (\\epsilon_T - \\epsilon_1)^2\\\\= [\\sum_{t=T}^{1} \\lambda_t (D_{\\theta} (x_t, \\sigma_t) - \\frac{\\sigma_0}{\\sigma_T}x_0)]^2 + \\sigma_1^2 ||\\epsilon_T - \\epsilon_1 ||^2$,\nwhere we re-parameterize $x_T$ and $x_0$, and define $\\lambda_0 = \\frac{\\sigma_0}{\\sigma_T}$ for notation simplicity. Note that $\\epsilon$ are i.i.d. Gaussian noise by definition so we have $\\sigma_1^2 ||\\epsilon_0 - \\epsilon_T||^2 = 2\\sigma_1^2$. As for the first term, it is easy to verify $\\sum_{t=T}^{1} \\lambda_t = 1$. Recall $\\lambda_i$ are in the range of (0, 1), the first term in Eq. 11 is actually a square of convex combination and is upper-bounded by:\n$\\mathcal{L}_{disc} (\\sigma) < \\sum_{t=T}^{1} \\lambda_t ||D_{\\theta} (x_t, \\sigma_t) - x||^2 + 2\\sigma_1^2$,\naccording to Jensen Inequality. Since the minimum noise level $\\sigma_T$ is set as hyper-parameter, we simply ignore it here. Eq. 12 shows the discretization loss in Eq. 10 is upper-bounded by a diffusion loss with weighting factors $\\lambda_t = \\frac{\\sigma_i}{\\sigma_{i+1}} \\sum_T$. It can be efficiently optimized w.r.t. model $D_{\\theta}$ in the same way as Eq. 3. Put it all together, we propose a two-stage finetuning method for pretrained diffusion models given T steps sampling budget:\n1.  In stage-one, learn the optimal sampling schedule \u03c3 for current denoising model $D_{\\theta}$ by minimizing Eq. 10;\n2.  In stage-two, finetune the denoising model $D_{\\theta}$ to current \u03c3 by minimizing the upper-bound Eq. 12."}, {"title": "3.3. Implementation", "content": "For a T-step sampling schedule, we set the maximum and minimum noise levels to those supported by the pretrained models, and use a learning variable with T - 2 degrees of freedom to model the increments following [24]. This is to ensure the sampling schedule is a valid monotonic function. Specifically, for noise level $\\sigma_t$ it is parameterized as:\n$\\sigma_t = \\sigma_{min} + \\sum_{j=T}^{t} Softmax(v_j)$,\nwhere v = Concat (v; 1) and v is the learning tensor.\nDirectly optimizing Eq. 10 needs to back-propagate loss through all steps, memory consumption grows linearly with number of steps T as the denoising model is called each iteration. Watson et al. [24] applies gradient rematerialization for back-propagating per-sample loss across diffusion generation process. They store all the {$\\hat{x}_i+$} sequence in memory and perform an extra forward pass of the corresponding inputs to re-build the computational graph at each"}, {"title": null, "content": "step. In this case time consumption grows linearly with T. In our structure however, the discretization loss Eq. 10 can be efficiently optimized without extra time or memory cost. Recall in the first equation of Eq. 11 discretization loss is represented as a function of ($D_{\\theta} (x_i, \\sigma_i) - x$). Gradient of Eq. 10 w.r.t. each $\\sigma_t$ can be written as:\n$\\frac{\\delta\\mathcal{L}_{disc} (\\sigma)}{\\delta\\sigma_t} = 2\\lambda_t (D_{\\theta} (x_t, \\sigma_t) - x), \\frac{d (D_{\\theta} (x_t, \\sigma_t) - x)}{\\delta\\sigma_t}$,\nwhere the derivative term is available as a by-product of back-propagting diffusion loss $||D_{\\theta} (x_t, \\sigma_t) - x||^2$. In practice, we jointly optimize Eq. 10 and the diffusion loss at each step, with a loss scaler \u03b3 as hyper-parameter. The overall framework of our method is listed in Algo. 1."}, {"title": "4. Experiment", "content": "In this section, we conduct several finetuning experiments using our two-stage method specified in Algo. 1. We first investigate the finetuning improvements of our method across pretrained models with different parameterization to various sampling steps in \u00a7. 4.1. We then conduct ablation experiments on the challenging ImageNet dataset [2] to examine the effects of individual components in \u00a7. 4.2. Finally, based on the experiment results and our theoretical analysis in \u00a7. 3, we delve into a detailed discussion of the deep connection between diffusion generation process and weighting scheme of training objective in \u00a7. 4.3. We employ Fr\u00e9chet inception distance (FID) [5] in terms of sample quality assessment, as it is the most widely applied indicator in the literature and the results of different models are available for direct comparison. The FID results reported in this section is computed between 50K generated images and the training set following [8]."}, {"title": "4.1. Finetune to Sampling Budgets", "content": "We finetune the pretrained Elucidated Diffusion Model (EDM) [8] to given sampling steps, swapping from T = {5, 8, 10, 12, 15, 20}. We adopt a model trained on downsampled FFHQ dataset at resolution 64\u00d764, and a model trained on CIFAR-10 dataset at resolution 32\u00d732. The models are finetuned for 50K parameters update using Algo. 1, and the finetuning improvements is presented in Fig. 2. On both datasets, our method yield approximate an improvement of 1 in FID score across different sampling budgets. The maximal improvement is observed at 20-steps finetuning on FFHQ model, while on CIFAR-10 model the maximum is observed at 12-steps finetuning. We suggest that this is because of the diversity of FFHQ dataset, which implies a more complicate ODE function for learning."}, {"title": "4.2. Ablation Study", "content": "We conduct the ablative experiments on the challenging ImageNet dataset [2] to examine the effectiveness of individual components in Algo. 1. The dataset are also preprocessed to 64\u00d764 resolution following [8]. We also perform 50K model parameters update on each of the configuration and present the result in Table. 1. The experiment results show that both learning the sampling schedule in stage-1 and the model finetuning in stage-2 is sufficient for improvements. We also finetune the pretrained model with original weighting scheme [8] to testify our training objective induced by the learned sampling schedule. The result show finetuning following the original weights leads to a deterioration in model performance. This is because the original objective is derived in the continuous diffusion context which cannot be directly transfered to the discrete diffusion training. We provide further discussion on the connection between discrete sampling schedule and training objective in next section."}, {"title": "4.3. Analysis", "content": "Learned sampling schedule In the baseline pretrianed models of Karras et al. [8], they employ a convex function to provide the sampling schedule with a hyperparameter \u03c1 controlling the curvature. They found in experiments that using a sampling schedule with moderate curvature that smoothly decreases the noise levels leads to favorable results, which is illustrated by the orange line in Fig. 3. Our two-stage finetuning method can lead to a sampling schedule with larger curvature, and we denote it by the blue line in Fig. 3. The learned sampling schedule aggressively skips the early sampling steps with large noise levels, where previous work [12, 18] found little contribution to the generation quality. However, these methods empirically choose sampling schedules according to the observation, while our method can learn this trend by directly optimizing the diffusion generation process. In EDM, although the curvature of the sampling schedule can directly controlled by the hyperparameter \u03c1, they found a too large \u03c1,i.e., skipping too much early steps is detrimental to generation quality. We suggest that this is attributed to the finetuning of the pretrained models in stage-2 of our method, results in smoother denoise"}, {"title": null, "content": "outputs trajectory {$75$} w.r.t. the learned sampling schedule as proposed in Theo. 1. This phenomenon is more pronounced when the sampling budget is further limited, where the sampling schedule drastically reduce noise levels, leaving as much computational resources to lower noise level as possible.\nLearned weighting scheme The weighting scheme of diffusion training objective influences model performance to a large extent. In our structure, the weighting scheme is determined by learned sampling scheduled, while in previous work it is either derived from different theoretical perspective [8, 10, 17, 20] or chosen empirically [1, 3, 6]. We compare the weighting scheme corresponds to our learned sampling schedule, to that employed by training EDM [8] in Fig 4. We scale the weights of EDM training objective by 10\u22127 to directly compare two weighting schemes in one graph. Note that this corresponds to applying a loss-scaler during training. Our weighting scheme exhibits a sharp increase at the second last step, while the weighting function of EDM appears smoother. This is because EDM training objective is derived from the theoretical continuous diffusion model, the weighting scheme is also a continuous increasing function over time. However, the noise level where a continuous diffusion model is trained on follows a continuous probability distribution, which corresponds to a likelihood term in the Monte Carlo estimating loss function. EDM employted a Gaussian distributed noise level whose mean and variance are controlled by hyperparemeters. In our method as we finetune the models to a a given number of sampling steps, we sample noise levels with equal probabilities following [10]. By explicitly including the likelihood terms, we plot the active weighting scheme in Fig. 4 (Right). The active weights of EDM continuously increase before reaching a threshold near the minimum noise level. They argue that models benefits little from training on noise levels that are either too large or too small. In a too large noise level with substantial information loss, the reconstruction result will approach dataset mean; in a too small noise level, the reconstruction loss can hardly provide any signal for updating model parameters. This is actually aligned with previous work observation [3]. The weighting scheme induced by our learned sampling schedule exhibits the same"}, {"title": "5. Related Work", "content": ""}, {"title": "5.1. Diffusion Generative Modelsx+x", "content": "Diffusion model is a class of generative models first proposed by Sohl-Dickstein et al. [17] in the spirit of modeling the gradual changes in the distribution of a molecular system. Diffusion model generates images from noise by iteratively sampling from conditional distributions. Ho et al. [6] significantly improved the performance by introducing a large U-Net model [15] to model this denoise process, demonstrating the potential of such models. In a concurrent work, Song et al. [20] proposed Score-Based Generative Models which employs Langevin Dynamics to model the continuous changes in the data distribution. Although these two models are proposed from different theoretical frameworks, they bear a strong resemblance in both the inference process and training objectives, and were later unified into a generalized model by Song et al. [21]. Dhariwal et al. [3] reported for the first time diffusion-based models outperform generative adversarial models [4, 9, 16] on various tasks by further increasing model size. Besides, they applied an additional classifier to provide category information for effective conditional generation. Building upon these works, the influential work of Rombach et al. [14] proposed latent diffusion model. They applied a variational auto-encoder [11,22] to project image data into latent space"}, {"title": "5.2. Diffusion Training Objective", "content": "The original diffusion training objective was derived from the variational lower-bound (VLB) of data sample negative log-likelihood (NLL) [17]. This objective can be simplified as a weighted summation of denoising loss at each step [21]. However, it was originally observed in experiments [6, 20] that a simple unweighted objective can somehow lead to better results. As the VLB objective theoretically guarantee better NLL, a combination of the simple loss and the VLB loss was also attempted [3]. Essentially, this hybrid objective adopted a linear combination of two weighting schemes and empirically found improvements in both samples density estimation and FID score. Recent work [10, 19] proposed some new weighting schemes for maximum likelihood training. Kingma et al. [10] rewrite the objective as a function of signal-to-noise ratio (SNR), and found the original weights in VLB objective are actually the changing rate in SNR. By directly learning the SNR function, they achieved state-of-the-art density estimation."}, {"title": "5.3. Diffusion Sampling", "content": "Diffusion sampling process was constraint to the noise levels used during training [6]. The generalized class of continuous diffusion models can perform sampling steps on"}, {"title": "6. Conclusion", "content": "arbitrary noise level within range supported by the models [21], providing possibilities for exploring diffusion sampling algorithms. Previous work [3, 12] empirically found that the majority of perceptual content is generated when the noise level is low, and faster sampling can be achieved by reducing sampling steps with large noise levels. However, these methods still requires multiple iterations at lower noise levels. Song et al. [18] proposed a deterministic sampler named DDIM which can yield comparable results with 10 times faster generation speed. They manipulated the underlying probabilistic graphical and skipped a substantial number of iteration steps. From the theoretical perspective [21], this method corresponds to taking large step in numerical simulating as we analyzed in the main body of this paper.\nIn this study, we explore the possibility of finetune pretrained diffusion models to optimal ODE solver for a given iteration steps. We provide a detailed analysis on the truncation error made by common first-order diffusion sampler, find a simple formulation of the truncation error that represented by a function of denoising loss at each step. We propose a efficient method to directly optimize diffusion generation process without extra time or memory cost. The theoretical analysis of truncation error shows that the optimal sampling schedule can lead to a first order global error. In this light we propose a two-stage finetuning method that learn the optimal sampling schedule in stage-1 and finetune the pretrained model according to the learned schedule in stage-2. We hope this novel methodology can inspire future explorations towards better sampling algorithms for diffusion models."}]}