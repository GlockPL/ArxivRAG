{"title": "MADE: Graph Backdoor Defense with Masked Unlearning", "authors": ["Xiao Lin", "Mingjie Li", "Yisen Wang"], "abstract": "Graph Neural Networks (GNNs) have garnered significant attention from researchers due to their outstanding performance in handling graph-related tasks, such as social network analysis, protein design, and so on. Despite their widespread application, recent research has demonstrated that GNNs are vulnerable to backdoor attacks, implemented by injecting triggers into the training datasets. Trained on the poisoned data, GNNs will predict target labels when attaching trigger patterns to inputs. This vulnerability poses significant security risks for GNNs' applications in sensitive domains, such as drug discovery. While there has been extensive research into backdoor defenses for images, strategies to safeguard GNNs against such attacks remain underdeveloped. Furthermore, we point out that conventional backdoor defense methods designed for images cannot work well when directly implemented on graph data. In this paper, we first analyze the key difference between image backdoor and graph backdoor attacks. Then we tackle the graph defense problem by presenting a novel approach called MADE, which devises an adversarial mask generation mechanism that selectively preserves clean sub-graphs and further leverages masks on edge weights to eliminate the influence of triggers effectively. Extensive experiments across various graph classification tasks demonstrate the effectiveness of MADE in significantly reducing the attack success rate (ASR) while maintaining a high classification accuracy.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph data has recently gained significant attention for their ubiquity, ranging from social networks [1; 2] to protein structures [3]. The distinctive feature of graph data lies in their topology structure, which significantly influences final predictions. Notably, a node's class often exhibits strong connections with the labels of its neighbors [4; 5], and causal relationships may exist between a graph's specific structure and its properties, especially in molecular analysis and other domains. Traditional deep neural networks like convolution neural networks or multilayer perceptrons often fall short in making accurate predictions, particularly when dealing with smaller graph datasets, as they struggle to leverage the rich information embedded in the graph structure for analysis.\nTo address these challenges, Graph Neural Networks (GNNs) have emerged as a widely adopted learning frame-work for graph data [6; 7; 8]. GNNs employ a message-passing mechanism to learn the structural relationships, aggregating information from the local neighborhood of each node. This allows GNNs to capture crucial topological and attributive features inherent in graph data. As a result, GNNs have proven effective across diverse application do-mains related to graph structures, including drug discovery [9; 10; 11], traffic forecasting [12], 3D object detection [13], recommender systems [1; 14], and webpage ranking [15; 16].\nHowever, recent works [17; 18; 19] show that GNNs are susceptible to backdoor attacks. Attackers can easily ma-nipulate the model's predictions by inserting specific graph patterns for harmful purposes, known as triggers, into the original graphs. For example, attackers can surreptitiously append trigger subgraphs or features to training graphs, leading GNNs employed in social network analysis or molecular studies to produce predictions that align with the attackers' goals. This vulnerability significantly undermines the reliability of GNNs, especially in high-stakes applications such as protein structure prediction and drug discovery. Therefore, interest in effective methods to mitigate these problems increased sharply these days.\nUnfortunately, due to some unique features of graph tasks compared with image and textual tasks, the commonly used backdoor defense methods, like adversarial neuron pruning [20] (ANP) and Anti-backdoor Learning [21] (ABL), demonstrate unsatisfied performance on graph data. The first unique feature of the graph task is that its graph structure contains more information compared with image and textual data. For example, in graph classification, different nodes can be connected with an edge due to some specific reasons like social communication, chemical properties, and others, while different components in image or textual data can only be connected by spatial or time series. Such a unique feature gives large flexibilities for attacks and enforces the defending procedure to be more careful like specifying the harness of every edge and node in the graph. The second feature is that graph datasets are much harder to collect and thus usually significantly smaller compared with image"}, {"title": "2 PRELIMINARY", "content": "In this section, we first briefly introduce some typical GNN models and basic concepts of backdoor attacks. Then, we further discuss the problems we try to conquer in the following.\n2.1 Graph Neural Networks\nWe first give a detailed definition of the main notation used in this paper, and then introduce some basic GNNs.\nNotation Convention. We use bold uppercase and lower-case letters to denote matrices (e.g., A) and vectors (e.g., v), respectively. We also use italic letters for scalars (e.g., d), and calligraphic letters for sets (e.g., N). In terms of indexing, the i-th row of a matrix is denoted as the corresponding bold lowercase letter with the subscript i (e.g., the i-row of X is Xi).\nNetwork Frameworks. Graph neural networks have demonstrated their expressive power in learning represen-tations of graphs. The key technique for the strong power of GNNs is message passing, which iteratively updates the node features by aggregating information from neighbors for every node in graphs. Specifically, let us have a graph G = {V, A, X} with V, A and X representing the set of nodes, the adjacency matrix of the graph G, and the node feature matrix, respectively. Then during k message passing, the update of node features can be expressed as:\n$$a_i^{(k)} = AGGREGATE (\\{h_j^{(k)} : v_j \\in N(v_i)\\}),$$\n$$h_i^{(k+1)} = COMBINE (\\{h_i^{(k)}, a_i^{(k)}\\}),$$\nwhere AGGREGATE and COMBINE represent the aggregation function and the transform operation, $h_i^{(k+1)}$ is the feature of the node $v_i$ after k message passing ($h_i^{(1)} = x_i$ for initialization), $a_i^{(k)}$ is the intermediate variables of the node $v_i$, and $N(v_i)$ represents the set of the 1-hop neighbors. For example, Then by iteratively aggregating graph information, GNNs can effectively process the topological information contained in graph datasets and achieve satisfying results on graph tasks. As one can see, the key architecture of GNNs is different from neural networks for image domains like convolutional networks [23; 24] and transformers [25]. Therefore, the properties for backdoor-attacked samples are also different from former models.\n2.2 Backdoor Attacks\nIn this subsection, we introduce the backdoor attack and its variation in the graph domain.\nBackdoor Attacks [26; 27; 28; 29]. Given a dataset $D = \\{x,y\\}$ with X and Y representing the set of samples and corresponding labels, respectively, attackers generate"}, {"title": "2.3 Problem Definition", "content": "Based on the definition of Problem 1, our objective is to create a robust defense mechanism ensuring that the model trained under the setting of Problem 1 remains insensitive to triggers. Hence, we define the problem of backdoor defense on graphs as follows:\nProblem 2. Backdoor defense on graph classification tasks.\nGiven: A poisoned dataset $D_{poison}$ is given which has been attached by a user-invisible trigger $(X_{tri}, A_{tri})$ with the target label $Y_{tar}$ \nCapability of Defenders: We can only control the training process.\nGoal: We aim to ensure the GNNs model free from data poisoning attacks. After training on a poisoned dataset, get a clean model $f(\u00b7)$ free from the backdoor threat, which means the model performs well on clean graphs (X, A, y), \u0456.\u0435.,\n$$f(X, A) = y.$$\nIn the meanwhile, the model cannot be manipulated by triggers, i.e.,\n$$f(X \\oplus X_{tri}, A \\odot A_{tri}) \\neq Y_{tar}.$$"}, {"title": "2.4 Difference between Image and Graph Backdoor Defense", "content": "Firstly, as illustrated in Table 2, a critical difference between image and graph datasets is the number of samples. Image datasets generally have sample numbers and feature sizes that are magnitudes larger than those in graph datasets. This abundance allows backdoor defense mechanisms greater flexibility to remove a relatively higher number of potentially poisoned samples. In contrast, graph datasets often have limited training samples, and further removing a portion of these samples could significantly degrade model perfor-mance. This scarcity of training data in graph tasks makes backdoor defense more challenging, necessitating highly accurate detection of poisoned samples.\nSecondly, unlike backdoor attacks for images that modify features, graph-based backdoor attacks can rewire the edges between nodes for attack. Therefore, even if all node features are clean, an attacker can still successfully attack the model with the modified graph's topology. Consequently, existing backdoor detection methods may be ineffective for graphs, as they primarily focus on feature manipulation and lack designs on the graph structure."}, {"title": "3 MADE ALGORITHM", "content": "Since our training-time defense setting is the same as ABL's scenario [21], we first analyze ABL's limitations on graphs from the data and training perspective. Based on that, we propose our MADE with the help of unique graph properties.\n3.1 Data Isolation\nTo understand ABL's limitations on graphs, we evaluate the data isolation part in ABL when applying it to graph neural networks. Unfortunately, we find that their isolation accuracy varies significantly across different datasets, as shown in Table 3. From the above results, one can see that the original ABL's isolation method cannot successfully select suspicious backdoor samples and create an enriched poisoned dataset. Therefore, the performance of ABL is not satisfactory on some graph backdoor datasets. Next, we explore why former isolation methods cannot work and propose effective isolation methods for graphs.\nTo be specific, we analyze the difference of backdoor samples on the image and graph domains from the training perspective, as shown in Figure 1. From the figure, one can see that, unlike image backdoor samples, the difference between the convergence rate for backdoor graphs (orange) and clean graphs (blue) is not significant. Therefore, solely relying on loss scores as isolation metrics, as done by ABL, is inadequate in the graph domain and leads to the failure of ABL's application on graph datasets. Therefore, we need to explore new metrics for the backdoor sample's isolation.\nRecently, some researchers [36] have proved that some graph adversarial attacks may inevitably affect the graph's homophily, a widely used metric for evaluating the general relationships between the neighboring nodes. For the graph G, the graph's homophily score homo(G) [4; 37] can be formulated as:\n$$homo(G) = \\frac{1}{|E|} \\sum_{(i,j) \\in E} sim(x_i, x_k),$$\nwhere & denotes the edge set for the graph G, and sim(\u00b7, \u00b7) represents the cosine similarity.\nInspired by the strong connections between the attacked graphs and their homophily changes, we explore the ho-mophily behaviors of clean graphs and backdoor graphs generated by GTA [17], a typical method of graph backdoor attacks. The results presented in Table 4 demonstrate a notable difference in homophily scores between backdoor and clean graphs, which can be leveraged to distinguish backdoor graphs from clean ones. Intuitively, since backdoor graphs constitute only a small fraction of the training set, the overall distribution of homophily scores is predominantly influenced by the clean graphs. Consequently, the homophily scores of backdoor graphs exhibit a significant deviation from the average homophily score of the entire training set. Therefore, we can reliably identify the potentially attacked graphs as those whose homophily scores fall within the following deviant range:\n$$(0, \\mu_{homo} - \\sigma_{homo}) \\cup (\\mu_{homo} + \\sigma_{homo}, 1),$$\nwhere $\\mu_{homo}$ and $\\sigma_{homo}$ represents the mean and standard variance of the homophily scores for all the graphs within the training set, respectively. As a result, we acquire a subset of graphs with a high possibility of containing triggers, denoted as $D_h$.\nHowever, there are usually not enough outlier graphs to form the detected backdoor subset through the above methods. Therefore, additional techniques are required to isolate the remaining graphs. Considering that the sign operator in ABL's warmup loss can impede the convergence of graph models, we opt to discard the sign function in ABL. Instead, we employ the original Cross Entropy loss to train and select the remaining graphs. In this context, a lower classification loss typically signifies a higher likelihood of being backdoor-attacked. Specifically, following an initial warm-up training phase, we compute the per-sample loss function and select the top samples with the lowest loss as the enriched harmful sample set, denoted as $D_I$. Combined with $D_h$, we can finally get the enriched backdoor subset denoted as $D_{bad} = D_h \\cup D_I$ with $a_1$ isolation rate. Moreover, we select $a_2$ samples with the highest loss as the enriched clean subset, denoted as $D_{clean}$. To assess the efficacy of our proposed data isolation, we conduct experiments using the GTA backdoor attack with an injection rate of 10.0% on four distinct datasets. we evaluate the proportion of truly injected trigger samples in the enriched backdoor subsets. The experimental results"}, {"title": "3.2 Forward Propagation with Masks", "content": "After data isolation, we aim to leverage the enriched clean subset $D_{clean}$ to ensure the model's natural classification ability while utilizing the enriched backdoor subset $D_{bad}$ to remove the influences of malicious triggers. For graph classification tasks, as attackers usually modify a small portion of nodes within the poisoned graphs to ensure the stealthiness, the isolated poisoned graphs also contain useful structures. Thus, we propose to generate masks for selectively removing the trigger subgraphs while preserving the model's natural performance.\n3.2.1 Masked Aggregation\nTo purify the backdoor samples, one widely used way in the image domain is to calculate the masks through gradient scores [38] as the model tends to exhibit more straightforward and localized attention to the trigger patterns embedded in the image domain. As shown in Figure 3(a), the model's gradient heatmap mainly focuses on the patterns, i.e., the right bottom corner. Unfortunately, such a finding does not apply to the graph domain. As shown in Figure 3(b), the distribution of gradient magnitudes for both malicious (backdoor) and clean (benign) nodes across all poisoned graphs are almost the same. The y-axis represents the magni-tude values of the gradients, while the x-axis distinguishes between malicious and clean nodes. We can observe that there is a slightly higher proportion of nodes with larger gradient magnitudes in the malicious nodes compared to the clean nodes. However, the overall gradient magnitude distributions for both types of nodes appear relatively similar, without any significant visual difference in Figure 3 (a). This similarity may be attributed to the more rapid information aggregation facilitated by graph structures compared to image convolutions. Consequently, using gradient scores to generate masks is ineffective in the graph domain.\nFortunately, we manage to find the difference between backdoor samples and natural samples from the spectral perspective, inspired by the former works [26; 39]. Firstly, we draw the features' spectrum of clean nodes and backdoor nodes. Specifically, we stacked the node features of clean and poisoned graphs, respectively, and then computed the singular values of the stacked node features. The sorted singular values are displayed in Figure 4. From the figure, it is obvious that the singular values for backdoor nodes decrease rapidly while the singular values for clean nodes distribute more uniformly. The above results indicate that we can effectively separate backdoor subgraphs and natural subgraphs via a linear layer especially if the projection matrix can project node features into a subspace spanned by singular vectors corresponding to small singular values. Then we can generate masks to purify the original graph samples. Thereby, we introduce a learnable projection head that projects node features onto the subspace w.r.t. small singlur values:\n$$p_i^{(k)} = HEAD (h_i^{(k)}),$$\nwhere HEAD denotes the learnable projection head. From our analysis, the above projection head can be used as a soft indicator of clean nodes or trigger nodes as the projection of the trigger nodes will be close to zero if its weight relates to the eigenvectors of the tail eigenvalues. We leave its training strategy in the following section. However, the scale of projections output may also hinder the node's classification. For instance, a trivial solution to remove backdoors involves setting all nodes' projections to be small during training. While this approach effectively eliminates backdoor nodes, it also significantly diminishes the model's learning ability on clean nodes, thereby degrading overall classification performance. To solve the above problem, we integrate the graph node's neighbors, and hence we define the natural score as below:\n$$score^{(k)} (v_i) = AVG (\\{sim(p_i^{(k)}, p_j^{(k)}) : v_j \\in N(v_i)\\}),$$\nwhere AVG denote an average function and $sim(p_i^{(k)}, p_j^{(k)})$ represents the cosine similarity between the node features $h_i^{(k)}$ and $h_j^{(k)}$. The neighborhood information can help us"}, {"title": "3.2.2 Loss Definition", "content": "Once we have obtained the mask for the graph, we proceed to update the gradients based on whether the graph belongs to the enriched poisoned dataset Dbad or the enriched clean dataset Dclean.\nIn the case where the graph G = {A,X} belongs to enriched backdoor subset Dbad, we employ an adversarial loss function Ladv to deteriorate the model's performance on the backdoor samples, effectively unlearning triggers. Mathematically, this process can be represented as follows:\n$$L_{adv} := softmax (f (A, X, m)) [y]$$\nwhere f(\u00b7) represents the model to be trained, m denotes the graph mask mentioned above in Section 3.2, and y is the label. In this case, y is highly likely to be the target label. Therefore, the intuition of Eq. (10) is to defend backdoor attack by reducing the probability of successfully predicting backdoor samples.\nFor the graph G = {A, X} belonging to the enriched clean dataset Dclean, we utilize the natural loss Lnat to ensure the natural classification ability of the model on clean graphs. Mathematically, the natural loss could be expressed as:\n$$L_{nat} := CE (GNN (A, X, m), y),$$\nwhere CE represents the cross-entropy function, a widely used classification loss.\nBesides, to guarantee that the performance of our model on clean samples remains unaltered when masks are added, a supplementary smooth loss Lsmh is incorporated into our training process. This loss serves to harmonize the output of clean samples when processed both with and without masks:\n$$L_{smh} := || f (A, X, m) \u2013 f (A, X) ||^2.$$\nFinally, combining the clean loss Lcln and the smooth loss Lsmh, we arrive at the clean loss Lcln tailored for clean graphs:\n$$L_{cln} := L_{nat} + \\lambda L_{smh},$$\nwhere A is a hyperparameter.\n3.3 Method Summarization\nWe summarize the above procedures in Figure 5. From the figure, one can see that training with our method can be roughly divided into two stages: \u201cData Isolation\" and \u201cForward Propagation with Masks\u201d. Firstly, we try to locate the poisoned subset in the training sets with our homophily and loss detection method. Instead of only finding the poisoned instances like the poisoned graphs or nodes. We try to use a learnable mask generator to remove the poisoned sub-graphs in the second stage and then train the GNN with the purified graphs. After training, our mask generator can also mask the trigger subgraphs of the input graphs and further mitigate the trigger's influence on GNN's predictions.\nGiven that MADE offers instance-level detection (data isolation), it can be seamlessly extended from graph classifi-cation tasks to node classification tasks. Please note that, for node classification, the instance-level detection of MADE can already accurately identify potentially attacked nodes, thus constructing precise masks to filter out triggers. Therefore, for node classification, MADE can effectively defend backdoor attacks, even without the adaptive mask generation process described in Section 3.2.1. Detailed description of MADE on node classification is provided in Appendix A. Algorithm 1 summarizes the inference phase of MADE for both graph classification and node classification tasks, while Algorithm 2 summarizes the training phase."}, {"title": "4 EXPERIMENTS", "content": "In this section, we utilize MADE to defend the state-of-the-art graph backdoor attack method to demonstrate that MADE effectively defend against backdoor attacks while maintaining relatively high prediction accuracy.\n4.1 Experiment Setup\nDatasets. For graph classification, we conducted experiments on four real-world datasets: AIDS [40], PROTEINS [41], PRO-TEINS_full [41], and ENZYMES [41]. For node classification, we evaluate the effectiveness of MADE on four real-world datasets: Cora [42], PubMed [43], OGBN-Arxiv [44] and Flickr [45]. The detailed description of these datasets are provided in Appendix E.\nBaselines. Due to limited prior work on backdoor de-fense for graphs, we compare MADE with state-of-the-art backdoor defense methods in the image domain and some typical graph defense methods. Backdoor defense methods on images include ABL [21], ANP [20] and fine-tune [22]. Typical graph defense methods include edge dropout and GCNJaccard [46]. Details are in Appendix F.\nParameter Settings. For graph classification, we em-ployed GTA [17] as the backdoor attack method with GCN (Graph Convoluation Network) being the surrogate GNN. For node classification, we employed UGBA [32] instead of GTA [17], since UGBA exhibits a stronger attack on node classification. The attack settings followed the default configurations, with a 10% injection rate. For MADE, we set A to 5, and adopted a 2-layer GCN as the model architecture, with a hidden dimension of 128 and \u00df of 0.9. The Adam optimizer was used with a weight decay of 5e-4 and betas set to 0.5 and 0.999. We set the learning rate as 0.01 and decayed by 0.1 every 40 epochs, with 200 training epochs in total. Each experiment setting is repeated for 5 times under different random seeds, and the mean results are recorded.\nEvaluation Metrics. The accuracy (ACC) is to assess the classification performance. A higher accuracy indicates better utility of models. The attack success rate (ASR) is to evaluate the defense effectiveness. A lower ASR indicates a smaller impact of the backdoor attack, thereby signifying a more successful backdoor defense.\n4.2 Experiment Results\n4.2.1 Comprehensive Experimental Results\nWe conduct comprehensive experiments on four real-world graph (node) classification datasets, and the results are detailed in Table 5 (Table 6). For both these two tasks, our proposed method exhibits the lowest Adversarial Success Rate (ASR) while maintaining satisfactory natural accuracy across all datasets. This suggests its effectiveness in defend-ing against backdoor attacks, especially when compared to typical backdoor defense methods like ABL and ANP, which struggle to perform well on graphs. Moreover, the natural accuracy drop of our MADE, when compared with natural training (vanilla), is minimal-less than 5% in most cases and even better in certain scenarios, such as PROTEINS and ENZYMES. This improved natural accuracy may be attributed to the masks, which encourage original GNNs to focus more on key parts of graphs.\nBeyond the commonly used GCNs, we extended our experiments to Graph Attention Networks (GAT) [47] and GraphSage [7], as detailed in Tables 5 and 6. For both tasks, our MADE consistently defends against backdoor attack across different graph neural networks. These findings affirm the versatility of our MADE as a universal approach for graph backdoor defense.\n4.2.2 Defense against Backdoor Attack with Various Strengths\nIn this section, we conduct a comprehensive assessment of our proposed defense method's efficacy in countering backdoor attacks of varying strengths. Specifically, we deploy MADE on AIDS dataset against GTA backdoor attacks with injection rates of 0.05,0.10,0.15, and 0.20. We gauge the effec-tiveness of our approach across GCN, GAT, and GraphSAGE, all configured with default parameters mentioned in Section 4.1. The experiment results are shown in Figure 6. Notably, we observe successful removal of backdoor triggers across all injection rates, with all the attack success rates less than 0.08 and most of them less than 0.02. Impressively, even when confronted with the backdoor attack with an injection rate of 0.20, our method maintains a competitive natural accuracy, with less than 3% decrease in accuracy. This remarkable performance underscores the superiority and effectiveness of MADE.\n4.2.3 Ablation Study\nTo investigate the sensitivity of MADE to hyperparameters of GNNs, we conduct a comparative analysis with GCN on both graph classification and node classification, varying the number of layers and hidden dimensions. The detailed results of node classification are provided in Appendix D. Here, we mainly focus on graph classification, whose results are shown in Figure 7. Specifically, to investigate the impact of the number of layers on backdoor defense, experiments are performed using 2-layer GNNs with a hidden dimension of 128, 3-layer GNNs with hidden dimensions of 128 and 32, and 4-layer GNNs with hidden dimensions of 128, 32, and 32. The corresponding results are illustrated in Figure 7(a) and 7(b). Regarding the model's hidden layer dimensions, given a 2-layer GNN, we conducted extensive experiments with hidden dimensions set to 64, 128, and 256, whose results are depicted in Figure 7(c) and 7(d). The results show that MADE reduces the attack success rate to near-zero while maintaining high accuracy across all the configurations, demonstrating the insensitivity of MADE on hyperparameters. Notably, under specific hyperparameter configurations, such as 2 or 4 layers or a hidden dimension of 64, MADE consistently demonstrates improved performance in both backdoor de-fense and utility, highlighting its superiority.\n4.2.4 Evaluations on Unlearning with Masks\nIn this subsection, we aim to further validate the conclusion in this paper that a well-designed mask generation module can help overcome the over-modification problem of original unlearning, thus ensuring better performance. Therefore, we compare the performance of ABL and MADE when trained on the same purified training samples which are obtained through data isolation in this paper.\nFrom Figure 8, it is evident that ABL can still eliminate the influence of backdoor attacks when employing better data isolation methods. However, this comes at the expense of a significant decrease in accuracy, supporting the conclusion that original unlearning may excessively modify neurons in networks. In contrast, MADE consistently achieves near-zero ASR while maintaining high classification accuracy across all four datasets. Notably, under some datasets (PROTEINS_full and ENZYMES for data isolation, and PROTEINS, PRO-TEINS_full and ENZYMES for ground truth), the accuracy of MADE even surpasses that of the vanilla GCN. It clearly demonstrates the superiority of MADE. This superior perfor-mance may be attributed to the fact that, with edge masking, only the poisoned nodes in an attacked graph are eliminated, preserving the topological structure between the remaining clean nodes."}, {"title": "5 RELATED WORKS", "content": "5.1 Graph Neural Networks.\nGraph neural network (GNN) has demonstrated state-of-the-art performance in various learning tasks, such as drug discovery [9; 10; 11], traffic forecasting [12], 3D object detec-tion [13], recommender systems [1; 14], and webpage ranking [15; 16]. Graph Convolutional Network (GCN) [6] utilizes localized first-order spectral graph convolution to learn node representations. GraphSAGE[7] uses neighborhood sampling and aggregation to employ graph neural networks in induc-tive learning. GAT[47] leverages self-attention mechanisms to learn attention weights for neighborhood aggregation.\n5.2 Backdoor Attack and Defenses\nBackdoor Attacks. Existing backdoor attacks aim to optimize three objectives: 1) making the trigger more invisible, 2) reducing the trigger injection rate, and 3) increasing the attack success rate. Many works focus on designing special patterns of triggers to make them more invisible. Triggers can be designed as simple patterns, such as a single pixel [26] or a black-and-white checkerboard [28]. Triggers can also be more complex patterns, such as mixed backgrounds [27], natural reflections [29], invisible noise [48], and adversarial patterns [49]. As for backdoor attacks on graphs, GTA [17] dynamically adapts triggers to individual graphs by optimizing both attack effectiveness and evasiveness on a downstream classifier. TRAP [19] generates perturbation triggers via a gradient-based score matrix from a surrogate model. UGBA [32] deploys a trigger generator to inject unnoticeable triggers into the nodes deliberately selected for stealthiness and effectiveness.\nBackdoor Defense. In the field of backdoor defense, [14; 30] tries to reverse potential triggers through imitation and reverse engineering of backdoor attacks, then they try to remove the generated triggers to enhance defense effec-tiveness. Apart from these reverse-based methods, [20; 31] try to identify malicious neurons and purify backdoored models by exploiting the sensitivity of backdoored neurons to adversarial perturbations. Besides the above methods, [21; 26; 50] try to detect backdoor samples and then remove the poison behavior by retraining or unlearning. However, those existing methods are not suitable for the graph scenario. Apart from these image-based methods, [51] is designed for graph datasets. However, such a method only focuses on detecting the injected edge trigger and can not deal with"}, {"title": "6 CONCLUSION", "content": "In this paper, we research on the usefulness of graph topology on backdoor defenses. We empirically demonstrate the drawback of traditional backdoor defense methods on graphs, and further give insightful explanations. Guided by our analyses, we point out that mask learning serves as an excellent graph defense framework to remain the clean part of graphs while mitigating malicious effect of triggers. Hence, we propose MADE, which utilizes graph topology to generate defensive masks for the purpose of preserving untainted subgraphs and hence training trigger-insensitive robust GNNs. Extensive evaluations on real-world datasets have demonstrated the effectiveness of MADE on defending backdoor attack while maintaining competitive utility."}, {"title": "APPENDIX A MADE ON NODE CLASSIFICATION", "content": "The MADE algorithm for node classification is highly similar to that for graph classification. However, due to the differences between these tasks, we have made slight modifications to MADE to enhance its defensive efficacy. These modifications primarily focus on (1) data isolation and (2) masked aggregation.\nData Isolation. To achieve node-level data isolation, we need to adapt the graph-level homophily in Eq. (3) to a node-specific homophily metric. First, we leverage ego graphs to determine node homophily. For each node v, we extract its 2-hop ego graph $G_v$ and utilize the homophily of the ego graph as the node homophily. Second, we aim to use the uniqueness of node classification to improve data isolation. Specifically, for node classification, since backdoor attacks significantly alter the predictions for attacked nodes, the pseudo-labels of attacked nodes may markedly differ from those of neighboring nodes. Consequently, calculating homophily based on the similarity of pseudo-labels can yield more effective data isolation. Hence, for the node v, Eq. (3) could be modified as:\n$$homo(v) = \\frac{1}{|N_v|} \\sum_{j \\in N_v} sim(\\hat{y}_v, \\hat{y}_j)$$\nwhere $N_v$ is the neighboring nodes within the ego graph $G_v$, $\\hat{y}_j$ is the pseudo-label of the node j. Once the node homophily is clearly defined, we are able to calculate $\\mu_{homo}$ and $\\sigma_{homo}$, and then implement data isolation strictly following the rest part of Section 3.1.\nMasked Aggregation. After detecting the trigger nodes using our data isolation method, we then try to remove the impacts of these poisoned triggers. To achieve such a goal, we design masks on the graph adjacency matrix to remove the trigger nodes or edges while maintaining the original graph structure. The masks are defined as follows:\n$$m_{ij}^{(k)} = \\begin{cases} A_{ij} & \\text{ if } v_i \\in V_{clean} \\text{ and } v_j \\in V_{clean} \\\\ 0, & \\text{ else,} \\end{cases}$$\nwhere $V_{clean}$ denoted the vertex set containing all the benign nodes split in our data isolation part. After obtaining m, we treat the graph as a weighted graph for message passing. Mathematically, the message passing is modified as follows:\n$$a^{(k)}_{i} = AGGREGATE (\\{m \\cdot h_j^{(k)}: V_j \\in N(v_i)\\} )$$\n$$h^{(k+1)} = COMBINE (\\{m \\cdot h_i^{(k)}, a^{(k)} \\} )$$\nOnce the masked aggregation is defined, we will update the model for node classification based on the losses in Section 3.2.2."}, {"title": "APPENDIX B EVALUATIONS ON DATA ISOLATION UNDER DIFFERENT ISOLATION RATES.", "content": "To validate the efficacy of data isolation, we compute the precision and recall of isolated backdoor samples among all the backdoor samples across various isolation rates. Specifically, we maintain a fixed injection rate of 10% on"}, {"title": "APPENDIX D ABLATION STUDY ON HYPERPARAMETERS TO GNNS FOR NODE CLASSIFICATION", "content": "We conduct comprehensive experiments to assess the hy-perparameter sensitivity of MADE in the context of node classification. The findings are illustrated in Figure 10, using the same experimental setup as described in Section 4.2.3 for graph classification. The results allow us to draw a parallel conclusion to that of Section 4.2.3: MADE consis-tently provides effective backdoor defense across neural networks with different parameter sizes, while maintaining competitive accuracies. These findings robustly demonstrate that MADE exhibits insensitivity to hyperparameters, thereby underscoring its superior performance and reliability."}, {"title": "APPENDIX E DATASET DESCRIPTIONS", "content": "Here we provide a detailed descriptions for the datasets in this paper. For graph classification, we choose four widely-used dataset, i.e., AIDS [40", "40": "PROTEINS_full [41", "41": "."}, {"40": "comprises molecular structure graphs repre-senting active and inactive compounds, totaling 2000 graphs. These compounds are derived from the AIDS Antiviral Screen Database of Active Compounds, which includes 4395 chemical compounds. Among these, 423 belong to class CA, 1081 to CM, and the remaining compounds to CI.\nPROTEINS and PROTEINS_full [41", "41": "consists of 600 protein tertiary struc-tures sourced from the BRENDA enzyme database, featuring a total of 6 enzymes.\nFor node classification, we choose four real-world datasets, i.e., Cora [42", "43": "OGBN-Arxiv [44", "45": ".", "42": "consists of 2708 scientific publications clas-sified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\nPubMed[43"}]}