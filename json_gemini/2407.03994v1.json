{"title": "Unlocking the Potential of Model Merging for Low-Resource Languages", "authors": ["Mingxu Tao", "Chen Zhang", "Quzhe Huang", "Tianyao Ma", "Songfang Huang", "Dongyan Zhao", "Yansong Feng"], "abstract": "Adapting large language models (LLMs) to new languages typically involves continual pre-training (CT) followed by supervised fine-tuning (SFT). However, this CT-then-SFT approach struggles with limited data in the context of low-resource languages, failing to balance language modeling and task-solving capabilities. We thus propose model merging as an alternative for low-resource languages, combining models with distinct capabilities into a single model without additional training. We use model merging to develop task-solving LLMs for low-resource languages without SFT data in the target languages. Our experiments based on Llama-2-7B demonstrate that model merging effectively endows LLMs for low-resource languages with task-solving abilities, outperforming CT-then-SFT in scenarios with extremely scarce data. Observing performance saturation in model merging with more training tokens, we further analyze the merging process and introduce a slack variable to the model merging algorithm to mitigate the loss of important parameters, thereby enhancing performance. We hope that model merging can benefit more human languages suffering from data scarcity with its higher data efficiency.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) demonstrate remarkable capabilities across various NLP tasks, owing to the vast amounts of high-quality training data (Touvron et al., 2023; Bai et al., 2023). However, developing models with task-solving abilities for low-resource languages remains challenging due to limited data availability.\nA common practice for constructing task-solving LLMs for a low-resource language involves continual pertaining (CT) and supervised fine-tuning (SFT) for the target language (Yong et al., 2023; Nguyen et al., 2023), known as CT-then-SFT. The scarcity of CT data impedes LLMs' ability to learn effective language modeling for these target languages. Additionally, it is difficult to acquire sufficient SFT data in low-resource languages to enhance downstream task performance. To address this issue, previous works attempt to transfer capabilities from high-resource languages to low-resource languages by training on English SFT data (Chirkova and Nikoulina, 2024; Shaham et al., 2024). However, this approach can lead to catastrophic forgetting (Thrun, 1998; Chen and Liu, 2018) of language modeling for the target languages (Mehta et al., 2021; Kotha et al., 2024), resulting in LLMs still failing to solve tasks due to the loss of language abilities.\nTo better integrate the language modeling and task-solving capabilities, we introduce model merging for low-resource languages, which can combine multiple models with distinct abilities into a single model without additional training. Previous work (Akiba et al., 2024) has shown that an LLM for high-resource languages can be merged with task-specific models, such as Japanese language models and math models. In this work, we explore whether model merging can effectively construct task-solving LLMs for low-resource languages. Specifically, we investigate the following research questions: RQ1: What is the viability of constructing task-solving LLMs in low-resource languages through model merging? RQ2: Is model merging always a better choice than CT-SFT? RQ3: What factors may affect LLMs in obtaining task-solving capabilities through model merging?\nTo answer these questions, we study the adaptation of Llama-2-7B (Touvron et al., 2023), an English-centric LLM, into seven distinct low-resource languages. We first continually pre-train Llama-2-7B on monolingual texts in each language. Next, we explore two approaches to inject task-solving capabilities into this continually pre-trained model: (1) training the LLM with English SFT data"}, {"title": "Preliminary: Model Merging", "content": "Model merging is a technique for combining multiple models possessing different capabilities into a single versatile model without additional training. For example, we can merge a model specialized for Japanese and a model specialized for math to obtain a model that excels at solving mathematical problems in Japanese (Akiba et al., 2024). In this work, we investigate two commonly-used methods of model merging: weighted averaging (Choshen et al., 2022; Wortsman et al., 2022) and TIES (Yadav et al., 2023). Here we provide a brief overview of these methods.\nWeighted averaging is simply averaging the parameters of two models with a weight tuned on the validation set.\nTIES aims to handle the parameter conflicts across multiple models more meticulously. Suppose we have two models specialized for distinct tasks, denoted as \\(\\theta_1\\) and \\(\\theta_2\\), both trained from the same initial model \\(\\theta_{init}\\). Task vectors for these models are calculated as follows: \\(\\tau_1 = \\theta_1 - \\theta_{init}\\) and \\(\\tau_2 = \\theta_2 - \\theta_{init}\\). The objective is to merge these task vectors and reintegrate them into the initial model.\nThe merging process of TIES consists of three steps: (1) Trim: For \\(\\tau_1\\) and \\(\\tau_2\\), we trim the redundant parameters by keeping the top-\\(k_1\\)\\% and top-\\(k_2\\)\\% values, respectively, creating \\(\\hat{\\tau}_1\\) and \\(\\hat{\\tau}_2\\). (2) Elect Signs: For each parameter p in \\(\\hat{\\tau}_1\\) and \\(\\hat{\\tau}_2\\), we select the sign (+1 or -1) with the higher magnitude, denoted as \\(y^p = sgn(\\hat{\\tau}_1^p + \\hat{\\tau}_2^p)\\). (3) Disjoint Merge: For each parameter p, we only keep the parameter values from \\(\\hat{\\tau}_1\\) and \\(\\hat{\\tau}_2\\) whose signs are the same as the aggregated elected sign and calculate their mean. Specifically, for each parameter p, its disjoint mean is calculated as \\(t_m^p = avg(S^p)\\), where \\(S^p = {\\hat{\\tau}_i^p | sgn(\\hat{\\tau}_i^p) = y^p, i = 1, 2}\\).\nGiven the final merged task vector \\(\\tau_m\\), we scale it and add it to the initial model \\(\\theta_{init}\\) to obtain the merged model"}, {"title": "Roadmap Towards LLMs for Low-Resource Languages", "content": "Given a base model only pre-trained on an English-centric corpus, e.g., Llama-2-7B (Touvron et al., 2023) in our study, we want to construct a model capable of solving tasks in a low-resource language. For the target language, there are very limited pre-training texts, ranging from 1B to 20B tokens, and almost no data for supervised finetuning (SFT). In this scenario, we investigate two representative paradigms of constructing such a model: CT-then-SFT and model merging. We illustrate the roadmap in Figure 1, which demonstrates the relations between the models.\nConventional Practice: CT-then-SFT The common practice is (1) first continual pre-training (CT) on the monolingual texts in the target language X to learn the language modeling and (2) then learning task-solving abilities through SFT (Yong et al., 2023; Nguyen et al., 2023). This approach is referred to as CT-then-SFT. In this approach, we have the following models:\nBASE: We employ the original Llama-2-7B without SFT as the base LLM.\nCT-X: We continually pre-train BASE on the corpus in the target language X. Following previous works (d'Autume et al., 2019; Tao et al., 2023), we add 1/4 English corpus for memory replay, to avoid catastrophic forgetting English language modeling.\nCTSFT-X: We train CT-X with SFT data to enhance its task solving ability. There are two variants"}, {"title": "Model Merging for Low-Resource Languages", "content": "The conventional CT-then-SFT paradigm struggles to balance language modeling and task-solving abilities in the context of low-resource languages. We propose model merging as an alternative, which can construct task-solving LLMs for low-resource languages without requiring SFT data in the target languages."}, {"title": "Experimental Setup", "content": "Languages We use 7 low-resource languages from five distinct language families for experiments: Tamil, Telugu, Odia, Bengali, Tibetan, Uyghur, and Mongolian (in the traditional Mongolian script). See their basic information in Table 1.\nWe select these languages because they are underrepresented in currently popular LLMs despite"}, {"title": "Results and Analysis", "content": "In this work, we mainly study two categories of common roadmaps, CT-then-SFT and model merging, to transfer the task-solving ability from a high-resource language to a low-resource one. We aim to investigate following research questions. RQ1: What is the viability of constructing task-solving LLMs in low-resource languages via model merging? RQ2: Is model merging always a better choice than CT-then-SFT?\nWe take the settings of BASE, SFT-flan, and CT-X as the baselines. For CT-then-SFT, we investigate two common methods to build task-solving models, CTSFT-X-flan and CTSFT-X-mt. For model merging, we study two effective algorithms to combine the abilities of language modeling and task solving: weighted averaging and TIES.\nFor all the studied languages except Bengali, the models constructed by merging outperform those built by CT-then-SFT. For example, the merged models based on TIES (TIES-X-flan) achieve an average score of 46.80% across languages, surpassing CT-then-SFT models (CTSFT-X-flan) by +4.69%. Although using a na\u00efve merging algorithm, model merging with WAVG (WAVG-X-flan) still achieves better performance than CT-then-SFT (CTSFT-X-flan) in four languages.\nIn the conventional CT-then-SFT approach, LLMs often fail to acquire sufficient comprehension of the target language from a small-scale CT corpus, and this ability may be further diminished by supervised fine-tuning. In contrast, model merging can preserve the language modeling acquired during CT while incorporating task-solving capabilities by resolving parameter conflicts carefully. In conclusion, model merging can be an effective pathway to obtain task-solving LLMs for low-resource languages."}, {"title": "Performance Plateau of Model Merging", "content": "Although model merging proves more effective than CT-then-SFT in most languages of our study, this is not the case for Bengali, which utilizes the largest corpus in the experiments: CTSFT-ben-flan outperforms TIES-ben-flan. We guess that the applicability of model merging may be related to the amount of corpus used in CT. Thus, we examine the performance changes of the merged models and the CT-then-SFT models under different amounts of CT tokens.\nWe collect the intermediate checkpoints of CT-ben and CT-tel, which are the two languages with the largest size of pre-training corpora. Then, we derive CTSFT-X-flan and TIES-X-flan models based on these checkpoints.\nFigure 2 illustrates the performance of CT-only (CT-X), CT-then-SFT (CTSFT-X-flan), and model merging (TIES-X-flan) based on every checkpoint. For Bengali and Telugu, TIES-X-flan outperforms CT-X in each checkpoint, indicating model merging can robustly enhance LLM's task-solving ability, while CT-then-SFT demonstrates greater variability. And in both languages, for all checkpoints where the amount of pre-trained tokens is less than 10.4B tokens, TIES-X-flan can achieve better results than CTSFT-X-flan.\nModel merging can integrate language modeling and task-solving capabilities more effectively than CT-then-SFT, in scenarios with limited language resources (<10B tokens). Four out of the seven languages in our study utilize corpora with fewer than 10B tokens. This data scarcity is characteristic of low-resource languages. For instance, 81% (135 out of 166) of the languages in the multilingual corpus CulturaX (Nguyen et al., 2024) have fewer than 10B tokens. Consequently, our findings on the effectiveness of model merging are broadly applicable and have the potential to benefit a wide range of human languages.\nAs the amount of CT tokens increases, the CTSFT-X-flan models show more rapid improvement in task-solving capabilities compared to TIES-X-flan. Specifically, when pre-trained with more than 14.4B tokens, CTSFT-ben-flan demonstrates superior performance over TIES-ben-flan. Similarly, in Telugu, the performance gap between the two models diminishes with additional CT tokens. However, due to the smaller size of the Telugu corpus compared to Bengali's, we have not observed CTSFT-tel-flan overtaking TIES-tel-flan. We note that CT-then-SFT may be a better method to construct task-solving LLMs in languages with sufficient resources, for example, Bengali, Vietnamese, Indonesian, etc."}, {"title": "Understanding the Dynamics of Model Merging", "content": "As shown in Figure 2, we find that the performance of TIES-X-flan on downstream tasks may no longer improve as we use more tokens for CT. In this section, we want to investigate RQ3: What factors may affect LLMs in obtaining task-solving capabilities through model merging?"}, {"title": "Quantification of Parameter Conflict", "content": "Revisiting the mechanism of TIES, we find that during the merging stage, parameters from one model may be discarded due to differences in the parameter signs between the two models. The discarding of model parameters usually leads to a decline in the corresponding capabilities. Considering that the CT model's language modeling ability continuously improves with the increase of pre-training data, we suspect that it is more likely that the SFT model discards too much information during the merging process, resulting in the merged model's inability to further enhance its task-solving ability in the target language.\nTo verify this hypothesis, we take Bengali, the language using the largest amount of CT corpus in our study, magnitude We explore the changes in the number of discarded parameters in the SFT model when merging it with CT models trained with different quantities of data.\nAs explained in Section 3.1, we first calculate the task vectors of SFT-flan and each checkpoint of CT-ben. In the trimming stage, we find the optimal hyperparameters are \\(k_{sft} = 0.2\\) and \\(k_{ct} = 1.0\\) for most scenarios. Thus, to provide a fair comparison, we freeze (\\(k_{sft}\\), \\(k_{ct}\\)) as (0.2, 1.0) for all checkpoints. Then, we examine the signs and magnitudes of each parameter of the trimmed task vectors of SFT-flan and CT-ben. A parameter that has contrary signs in two models can be regarded as a parameter with sign conflict. And if its magnitude in SFT-flan is smaller than that in CT-ben, it will be removed at the stage of sign election.\nFigure 4 illustrates the proportion of parameters that are discarded from SFT-flan during the merging stage. We can find as the LLM is pre-trained with more tokens, 4% more parameters are removed in trimmed SFT-flan. We believe that when using more tokens for CT, TIES discards"}, {"title": "Model Merging with a Slack Variable", "content": "To mitigate the information loss in SFT-flan during the merging stage, we propose TIES-SV, enhancing TIES with a Slack Variable to reduce the number of discarded parameters in the model with higher information density, i.e., SFT-flan in this situation.\nAccording to the process of TIES, for each parameter to be discarded from SFT-flan in the Disjoint Merge step, its magnitude is smaller than the magnitude of its counterpart parameter in CT-X. To retain the parameters of SFT-flan while minimizing the information loss of CT-X, we first rank these pairs of parameters between SFT-flan and CT-X according to their differences in the magnitude. Next, we select a subset of parameters with the smallest magnitude differences to reserve.\nWe evaluate the effectiveness of TIES-SV on the model merging process of SFT-flan and the last checkpoint of CT-ben. Based on the results in Figure 4, we reserve 4% parameters in SFT-flan that were to be discarded in the original algorithm of TIES. In all three tasks, our TIES-SV outperforms vanilla TIES by 0.52% on average.\nIn conclusion, we propose a simple strategy to improve model merging. It suggests that during the process of model merging, the importance of different models' parameters is not the same. When parameter conflicts occur, we cannot simply rely on the magnitude of the vectors to decide which model's parameters to reserve. Instead, we should use prior information obtained through pilot studies or other means to reserve the parameters of the more important model. We hope our TIES-SV can shed light on the study of new model-merging algorithms in the future."}, {"title": "Discussions", "content": "In this section, we further explore two important questions related to model merging. First, we investigate the potential of merging more than one low-resource language into a task-solving model, which could offer a new avenue for constructing multilingual models. Second, we examine why commonly-used machine-translated data often fails in the context of low-resource languages. This failure underscores the advantage of model merging, as it does not require SFT data in the target language."}, {"title": "Can We Merge Multiple Languages?", "content": "Previous work (Akiba et al., 2024) shows that model merging algorithms can be used to combine multiple LLMs with different capabilities, which may enhance the model to solve complex problems. We wonder whether multiple LLMs adapted to different low-resource languages can be merged with the same SFT model, to construct a a task-solving LLM supporting these languages simultaneously.\nAs a pilot study, we attempt to merge Mongolian and Uyghur LLMs with the English task-solving model SFT-flan. Due to the limited budget for computational resources, we cannot conduct a grid search across the hyperparameter space of the three models. Therefore, we employ the optimal hyperparameters derived from merging each of the two CT models with SFT-flan.\nThis indicates that model merging has great potential for constructing multilingual task-solving LLMs. We hope this approach can assist multilingual speakers, particularly those using underrepresented languages, by combining multiple existing LLMs in distinct languages without the need for expensive pre-training."}, {"title": "Why do Machine Translated Data Fail?", "content": "Collecting synthetic SFT data through MT is an intuitive method for constructing task-solving LLM in non-English languages (Muennighoff et al., 2023; Li et al., 2023a). However, the experiment results in Table 3 show that MT-translated SFT data may not work when it comes to low-resource languages. The models trained with MT-translated data (CTSFT-X-mt) models exhibit inferior performance across all languages compared to those trained on English SFT data (CTSFT-X-flan), with a gap of -3.38%~-24.61%.\nThe decline in model performance can be attributed to the low-quality MT results. Current open-source MT systems have limited abilities of low-resource languages. For example, we ask Uyghur and Tibetan native speakers to evaluate sampled translation results by NLLB. They find that there are often irrelevant contents and undesired code-switching in the translation results, as shown in Appendix C.2. This kind of noise not only hinders the model's ability to learn task-solving strategies but also interferes with language modeling, which itself is inadequately learned.\nIn contrast, model merging eliminates the need to collect SFT data in the target languages. This approach also eliminates the reliance on unreliable machine translation systems."}, {"title": "Conclusion", "content": "In this paper, we investigate the potential of using model merging to construct task-solving LLMs for low-resource languages. Our findings demonstrate that model merging outperforms the conventional CT-then-SFT paradigm, achieving higher data efficiency. We further analyze the mechanism behind the performance saturation of model merging with an increased number of CT tokens, which inspires a simple yet effective improvement to the model merging algorithm. We hope that model merging can reduce the costs associated with data collection and model training, benefiting a greater number of languages suffering from data scarcity."}, {"title": "Limitations", "content": "Studied Languages Due to the high computational cost of continually pretraining LLMs, we cannot cover a wider range of low-resource languages, only focusing on seven underrepresented languages in India and China. However, we make efforts to improve the diversity of selected languages by including different language families and writing systems.\nEvaluation Tasks Most of our evaluated tasks focus on natural language understanding, with less emphasis on natural language generation. This limitation arises from the insufficient CT corpus available for the studied low-resource languages, which is insufficient for the models to learn to perform complex generation in the target language."}]}