{"title": "Unlocking the Potential of Model Merging for Low-Resource Languages", "authors": ["Mingxu Tao", "Chen Zhang", "Quzhe Huang", "Tianyao Ma", "Songfang Huang", "Dongyan Zhao", "Yansong Feng"], "abstract": "Adapting large language models (LLMs) to new languages typically involves continual pre-training (CT) followed by supervised fine-tuning (SFT). However, this CT-then-SFT approach struggles with limited data in the context of low-resource languages, failing to balance language modeling and task-solving capabilities. We thus propose model merging as an alternative for low-resource languages, combining models with distinct capabilities into a single model without additional training. We use model merging to develop task-solving LLMs for low-resource languages without SFT data in the target languages. Our experiments based on Llama-2-7B demonstrate that model merging effectively endows LLMs for low-resource languages with task-solving abilities, outperforming CT-then-SFT in scenarios with extremely scarce data. Observing performance saturation in model merging with more training tokens, we further analyze the merging process and introduce a slack variable to the model merging algorithm to mitigate the loss of important parameters, thereby enhancing performance. We hope that model merging can benefit more human languages suffering from data scarcity with its higher data efficiency.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) demonstrate remarkable capabilities across various NLP tasks, owing to the vast amounts of high-quality training data (Touvron et al., 2023; Bai et al., 2023). However, developing models with task-solving abilities for low-resource languages remains challenging due to limited data availability.\nA common practice for constructing task-solving LLMs for a low-resource language involves continual pertaining (CT) and supervised fine-tuning (SFT) for the target language (Yong et al., 2023; Nguyen et al., 2023), known as CT-then-SFT. The scarcity of CT data impedes LLMs' ability to learn effective language modeling for these target languages. Additionally, it is difficult to acquire sufficient SFT data in low-resource languages to enhance downstream task performance. To address this issue, previous works attempt to transfer capabilities from high-resource languages to low-resource languages by training on English SFT data (Chirkova and Nikoulina, 2024; Shaham et al., 2024). However, this approach can lead to catastrophic forgetting (Thrun, 1998; Chen and Liu, 2018) of language modeling for the target languages (Mehta et al., 2021; Kotha et al., 2024), resulting in LLMs still failing to solve tasks due to the loss of language abilities.\nTo better integrate the language modeling and task-solving capabilities, we introduce model merging for low-resource languages, which can combine multiple models with distinct abilities into a single model without additional training. Previous work (Akiba et al., 2024) has shown that an LLM for high-resource languages can be merged with task-specific models, such as Japanese language models and math models. In this work, we explore whether model merging can effectively construct task-solving LLMs for low-resource languages. Specifically, we investigate the following research questions: RQ1: What is the viability of constructing task-solving LLMs in low-resource languages through model merging? RQ2: Is model merging always a better choice than CT-SFT? RQ3: What factors may affect LLMs in obtaining task-solving capabilities through model merging?\nTo answer these questions, we study the adaptation of Llama-2-7B (Touvron et al., 2023), an English-centric LLM, into seven distinct low-resource languages. We first continually pre-train Llama-2-7B on monolingual texts in each language. Next, we explore two approaches to inject task-solving capabilities into this continually pre-trained model: (1) training the LLM with English SFT data or the data translated to the target low-resource language; (2) merging the model with an English task-solving LLM. Experiments show that model merging can effectively equip the CT models with task-solving capabilities. Notably, when pre-training corpora in the target language are extremely scarce (<10B tokens), model merging outperforms CT-then-SFT. As an LLM is pre-trained with more tokens, the improvements brought by model merging gradually saturate. Then, model merging can no longer significantly surpass the SFT method.\nTo further investigate the factors impeding the continuous improvement of model merging, we conduct a detailed analysis of the process of merging two LLMs. We find that when an English SFT model is merged with an LLM continually pre-trained with more tokens in the target language, more parameters from the SFT model are discarded during merging. The loss of these parameters may lead to a decline in task-solving capabilities, preventing the merged model from improving performance on downstream tasks. To mitigate the loss of important parameters from the SFT model, we propose model merging with slack variables. This strategy allows for more flexible control over the merging process to retain the important parameters.\nOur contributions are as follows: (1) We are the first to introduce model merging to construct task-solving LLMs for low-resource languages; (2) We reveal that model merging is more effective than SFT in the scenarios of extremely low-resource languages; (3) Through a quantitative study of the merging process, we explain the performance plateau of model merging with a larger CT corpus and propose a simple yet effective enhancement to popular model merging algorithms."}, {"title": "Related Works", "content": "Model Merging Model merging is a promising way to combine the abilities of multiple models. Pioneering works explore strategies to find the best weights for averaging (Choshen et al., 2022; Wortsman et al., 2022; Matena and Raffel, 2022; Jin et al., 2022). Task Arithmetic (Ilharco et al., 2022) employs task vectors, enabling control through arithmetic operations to steer the merged model's behavior. TIES (Yadav et al., 2023) further addresses the problem of information loss by handling parameter conflict more carefully. DARE (Yu et al., 2023) zeros out redundant parameters and amplifies the remaining ones. Evolutionary Model Merge (Akiba et al., 2024) automatically discovers optimal model combinations through evolutionary algorithms.\nThere is little discussion of model merging in the context of multilinguality. Instead, previous works attempt to introduce language-specific and task-specific modular adapters (Pfeiffer et al., 2020; Parovic et al., 2023; Parovi\u0107 et al., 2024; Zhao et al., 2024), which require additional training. These works focus on high-resource languages and specific tasks. In contrast, model merging can utilize existing models without additional training, making it a versatile approach for building more general task-solving LLMs. Besides, we are the first to study model merging for low-resource languages.\nLLMs for Low-Resource Languages There is a line of works aiming to adapt LLMs to under-represented human languages. A common practice is continually pre-training existing LLMs on the corpus in the target languages (Yong et al., 2023; Nguyen et al., 2023; Zhang et al., 2024b). To improve the efficiency of training, previous works adopt techniques such as adapters (Pfeiffer et al., 2020), script conversion (Micallef et al., 2024), integration of similar languages (Senel et al., 2024).\nFollowing pre-training, LLMs typically undergo supervised fine-tuning to acquire task-solving capabilities (Muennighoff et al., 2023; Nguyen et al., 2023). To address the data scarcity in this step, researchers have employed various methods to collect SFT data, including crowd-sourcing (Singh et al., 2024), machine translation (Muennighoff et al., 2023; Li et al., 2023a), LLM distillation (Li et al., 2024),rule-based conversion (Cahyawijaya et al., 2023), et al. However, these methods are not without limitations, particularly in terms of cost, data quality, and generalizability. In contrast, the model merging paradigm studied in our work eliminates the need for expensive and potentially error-prone SFT data collection by leveraging pre-trained task-solving models from high-resource languages."}, {"title": "Model Merging for Low-Resource Languages", "content": "The conventional CT-then-SFT paradigm struggles to balance language modeling and task-solving abilities in the context of low-resource languages. We propose model merging as an alternative, which can construct task-solving LLMs for low-resource languages without requiring SFT data in the target languages."}, {"title": "Preliminary: Model Merging", "content": "Model merging is a technique for combining multiple models possessing different capabilities into a single versatile model without additional training. For example, we can merge a model specialized for Japanese and a model specialized for math to obtain a model that excels at solving mathematical problems in Japanese (Akiba et al., 2024). In this work, we investigate two commonly-used methods of model merging: weighted averaging (Choshen et al., 2022; Wortsman et al., 2022) and TIES (Yadav et al., 2023). Here we provide a brief overview of these methods.\nWeighted averaging is simply averaging the parameters of two models with a weight tuned on the validation set.\nTIES aims to handle the parameter conflicts across multiple models more meticulously. Suppose we have two models specialized for distinct tasks, denoted as \\(\\theta_1\\) and \\(\\theta_2\\), both trained from the same initial model \\(\\theta_{\\text{init}}\\). Task vectors for these models are calculated as follows: \\(\\tau_1 = \\theta_1 - \\theta_{\\text{init}}\\) and \\(\\tau_2 = \\theta_2 - \\theta_{\\text{init}}\\). The objective is to merge these task vectors and reintegrate them into the initial model.\nThe merging process of TIES consists of three steps: (1) Trim: For \\(\\tau_1\\) and \\(\\tau_2\\), we trim the redundant parameters by keeping the top-\\(k_1\\)% and top-\\(k_2\\)% values, respectively, creating \\(\\hat{\\tau}_1\\) and \\(\\hat{\\tau}_2\\). (2) Elect Signs: For each parameter p in \\(\\hat{\\tau}_1\\) and \\(\\hat{\\tau}_2\\), we select the sign (+1 or -1) with the higher magnitude, denoted as \\(y^p = \\text{sgn}(\\hat{\\tau}_1^p + \\hat{\\tau}_2^p)\\). (3) Disjoint Merge: For each parameter p, we only keep the parameter values from \\(\\hat{\\tau}_1\\) and \\(\\hat{\\tau}_2\\) whose signs are the same as the aggregated elected sign and calculate their mean. Specifically, for each parameter p, its disjoint mean is calculated as \\(\\tau_m^p = \\text{avg}(\\mathcal{S}^p)\\), where \\(\\mathcal{S}^p = {\\{\\hat{\\tau}_i^p | \\text{sgn}(\\hat{\\tau}_i^p) = y^p, i = 1,2\\}}\\).\nGiven the final merged task vector \\(\\tau_m\\), we scale it and add it to the initial model \\(\\theta_{\\text{init}}\\) to obtain the merged model \\(\\theta_m\\) as \\(\\theta_m = \\theta_{\\text{init}} + \\lambda \\cdot \\tau_m\\), where \\(\\lambda\\) is a scaling hyperparameter.\nFor TIES, we tune three hyperparameters in total on the validation set: two sparsity rates \\(k_1, k_2\\) and a scaling factor \\(\\lambda\\).\nPlease refer to the original paper of TIES (Yadav et al., 2023) for more details."}, {"title": "Roadmap Towards LLMs for Low-Resource Languages", "content": "Given a base model only pre-trained on an English-centric corpus, e.g., Llama-2-7B (Touvron et al., 2023) in our study, we want to construct a model capable of solving tasks in a low-resource language. For the target language, there are very limited pre-training texts, ranging from 1B to 20B tokens, and almost no data for supervised finetuning (SFT). In this scenario, we investigate two representative paradigms of constructing such a model: CT-then-SFT and model merging.\nConventional Practice: CT-then-SFT The common practice is (1) first continual pre-training (CT) on the monolingual texts in the target language X to learn the language modeling and (2) then learning task-solving abilities through SFT (Yong et al., 2023; Nguyen et al., 2023). This approach is referred to as CT-then-SFT. In this approach, we have the following models:\nBASE: We employ the original Llama-2-7B without SFT as the base LLM.\nCT-X: We continually pre-train BASE on the corpus in the target language X. Following previous works (d'Autume et al., 2019; Tao et al., 2023), we add 1/4 English corpus for memory replay, to avoid catastrophic forgetting English language modeling.\nCTSFT-X: We train CT-X with SFT data to enhance its task solving ability. There are two variants using different SFT data:\n(1) CTSFT-X-flan: We finetune CT-X with English SFT data, which includes the original FLAN datasets and the training set of GSM8K. This approach is based on the assumption that task-solving abilities in English can be transferred to the target language (Chirkova and Nikoulina, 2024; Shaham et al., 2024).\n(2) CTSFT-X-mt: We translate FLAN and the training set of GSM8K into the target language X with machine translation (MT) systems, which is a common practice to obtain SFT data for non-English languages (Muennighoff et al., 2023; Li et al., 2023a,b). We then finetune CT-X with the translated data.\nNew Paradigm: Model Merging By model merging, we can integrate distinct LLMs with various capabilities into one LLM. To obtain a model capable of solving task in the target language X, we can merge the following two models:\nCT-X: As discussed in the CT-then-SFT procedure, this model learned a certain amount of language modeling in the language X after CT. However, its task-solving ability is limited.\nSFT-flan: We directly finetune BASE with the SFT data used by CTSFT-X-flan. The resulting model has sufficiently learned task solving, but the target language X is still foreign to it.\nWe merge the two models above to unlock the dual benefits of proficient language modeling and effective task-solving capabilities. Specifically, we investigate two methods of model merging: weighted averaging (WAVG, Choshen et al., 2022; Wortsman et al., 2022) and TIES (Yadav et al., 2023). We derive two variants of merged models, namely WAVG-X-flan and TIES-X-flan."}, {"title": "Experimental Setup", "content": "Languages We use 7 low-resource languages from five distinct language families for experiments: Tamil, Telugu, Odia, Bengali, Tibetan, Uyghur, and Mongolian (in the traditional Mongolian script). We select these languages because they are underrepresented in currently popular LLMs despite their large population (over 475M) worldwide. As shown in Table 3, the performance of Llama-2-7B in these languages is close to or even worse than random guessing. Notably, the vocabulary of Llama-2 does not even contain tokens for Odia and traditional Mongolian, which indicates that the model has hardly seen these languages during pre-training. Moreover, limited resources are available for these languages on the internet. Among those languages, we can only collect fewer than 1B tokens of monolingual texts for Odia, Tibetan, Uyghur, and traditional Mongolian. The problem of data scarcity becomes more severe in terms of high-quality data for supervised fine-tuning.\nPre-training Corpus During continual pre-training, we use the largest available corpus for each language from CulturaX (Nguyen et al., 2024), IndicCorp-v2 (Doddapaneni et al., 2023), and MC\u00b2 (Zhang et al., 2024b). To maximize language coverage with constraint computational resources, we sample 8B tokens for continual pre-training of Tamil and Telugu, and 16B tokens for Bengali. The corpus sizes are shown in Table 2.\nFollowing Llama models (Touvron et al., 2023), we employ RedPajama (Computer, 2023) with the same sampling proportion for memory replay to reduce forgetting of English language modeling.\nSFT Data We mainly use FLAN (Longpre et al., 2023) for SFT, which consists 155K training instances for 1,411 distinct tasks. Since there are limited math reasoning tasks in FLAN, we additionally incorporate 7,473 instances from GSM8K (Cobbe et al., 2021) into the supervised training sets.\nWe translate FLAN into the languages of our study using NLLB-200-Distilled-1.3B (NLLB Team et al., 2022). Note that this model does not support traditional Mongolian and there are no open-source MT models available for this language currently. We thus adopt a roundabout way: translating the instructions into Cyrillic Mongolian, which NLLB supports, and converting them into traditional Mongolian with an open-source transliteration tool."}, {"title": "Results and Analysis", "content": "In this work, we mainly study two categories of common roadmaps, CT-then-SFT and model merging, to transfer the task-solving ability from a high-resource language to a low-resource one. We aim to investigate following research questions. RQ1: What is the viability of constructing task-solving LLMs in low-resource languages via model merging? RQ2: Is model merging always a better choice than CT-then-SFT?\nWe take the settings of BASE, SFT-flan, and CT-X as the baselines. For CT-then-SFT, we investigate two common methods to build task-solving models, CTSFT-X-flan and CTSFT-X-mt. For model merging, we study two effective algorithms to combine the abilities of language modeling and task solving: weighted averaging and TIES.\n5.1 Effectiveness of Model Merging\nFor all the studied languages except Bengali, the models constructed by merging outperform those built by CT-then-SFT. For example, the merged models based on TIES (TIES-X-flan) achieve an average score of 46.80% across languages, surpassing CT-then-SFT models (CTSFT-X-flan) by +4.69%. Although using a na\u00efve merging algorithm, model merging with WAVG (WAVG-X-flan) still achieves better performance than CT-then-SFT (CTSFT-X-flan) in four languages.\nIn the conventional CT-then-SFT approach, LLMs often fail to acquire sufficient comprehension of the target language from a small-scale CT corpus, and this ability may be further diminished by supervised fine-tuning. In contrast, model merging can preserve the language modeling acquired during CT while incorporating task-solving capabilities by resolving parameter conflicts carefully. In conclusion, model merging can be an effective pathway to obtain task-solving LLMs for low-resource languages.\n5.2 Performance Plateau of Model Merging\nAlthough model merging proves more effective than CT-then-SFT in most languages of our study, this is not the case for Bengali: CTSFT-ben-flan outperforms TIES-ben-flan. We guess that the applicability of model merging may be related to the amount of corpus used in CT. Thus, we examine the performance changes of the merged models and the CT-then-SFT models under different amounts of CT tokens.\nWe collect the intermediate checkpoints of CT-ben and CT-tel, which are the two languages with the largest size of pre-training corpora. Then, we derive CTSFT-X-flan and TIES-X-flan models based on these checkpoints.\nFigure 2 illustrates the performance of CT-only (CT-X), CT-then-SFT (CTSFT-X-flan), and model merging (TIES-X-flan) based on every checkpoint. For Bengali and Telugu, TIES-X-flan outperforms CT-X in each checkpoint, indicating model merging can robustly enhance LLM's task-solving ability, while CT-then-SFT demonstrates greater variability. And in both languages, for all checkpoints where the amount of pre-trained tokens is less than 10.4B tokens, TIES-X-flan can achieve better results than CTSFT-X-flan.\nModel merging can integrate language modeling and task-solving capabilities more effectively than CT-then-SFT, in scenarios with limited language resources (<10B tokens). Four out of the seven languages in our study utilize corpora with fewer than 10B tokens. This data scarcity is characteristic of low-resource languages. For instance, 81% (135 out of 166) of the languages in the multilingual corpus CulturaX (Nguyen et al., 2024) have fewer than 10B tokens. Consequently, our findings on the effectiveness of model merging are broadly applicable and have the potential to benefit a wide range of human languages.\nAs the amount of CT tokens increases, the CTSFT-X-flan models show more rapid improvement in task-solving capabilities compared to TIES-X-flan. Specifically, when pre-trained with more than 14.4B tokens, CTSFT-ben-flan demonstrates superior performance over TIES-ben-flan. Similarly, in Telugu, the performance gap between the two models diminishes with additional CT tokens. However, due to the smaller size of the Telugu corpus compared to Bengali's, we have not observed CTSFT-tel-flan overtaking TIES-tel-flan. We note that CT-then-SFT may be a better method to construct task-solving LLMs in languages with sufficient resources, for example, Bengali, Vietnamese, Indonesian, etc."}, {"title": "Understanding the Dynamics of Model Merging", "content": "As shown in Figure 2, we find that the performance of TIES-X-flan on downstream tasks may no longer improve as we use more tokens for CT. In this section, we want to investigate RQ3: What factors may affect LLMs in obtaining task-solving capabilities through model merging?\n6.1 Quantification of Parameter Conflict\nRevisiting the mechanism of TIES, we find that during the merging stage, parameters from one model may be discarded due to differences in the parameter signs between the two models. The discarding of model parameters usually leads to a decline in the corresponding capabilities. Considering that the CT model's language modeling ability continuously improves with the increase of pre-training data, we suspect that it is more likely that the SFT model discards too much information during the merging process, resulting in the merged model's inability to further enhance its task-solving ability in the target language.\nTo verify this hypothesis, we take Bengali, the language using the largest amount of CT corpus in our study, magnitude We explore the changes in the number of discarded parameters in the SFT model when merging it with CT models trained with different quantities of data.\nAs explained in Section 3.1, we first calculate the task vectors of SFT-flan and each checkpoint of CT-ben. In the trimming stage, we find the optimal hyperparameters are \\(k_{\\text{SFT}} = 0.2\\) and \\(k_{\\text{CT}} = 1.0\\) for most scenarios. Thus, to provide a fair comparison, we freeze (\\(k_{\\text{SFT}}, k_{\\text{CT}}\\)) as (0.2, 1.0) for all checkpoints. Then, we examine the signs and magnitudes of each parameter of the trimmed task vectors of SFT-flan and CT-ben. A parameter that has contrary signs in two models can be regarded as a parameter with sign conflict. And if its magnitude in SFT-flan is smaller than that in CT-ben, it will be removed at the stage of sign election.\nFigure 4 illustrates the proportion of parameters that are discarded from SFT-flan during the merging stage. We can find as the LLM is pre-trained with more tokens, 4% more parameters are removed in trimmed SFT-flan. We believe that when using more tokens for CT, TIES discards a larger proportion of parameters from the SFT model, which may continuously undermine the task-solving capabilities.\n6.2 Model Merging with a Slack Variable\nTo mitigate the information loss in SFT-flan during the merging stage, we propose TIES-SV, enhancing TIES with a Slack Variable to reduce the number of discarded parameters in the model with higher information density, i.e., SFT-flan in this situation.\nAccording to the process of TIES, for each parameter to be discarded from SFT-flan in the Disjoint Merge step, its magnitude is smaller than the magnitude of its counterpart parameter in CT-X. To retain the parameters of SFT-flan while minimizing the information loss of CT-X, we first rank these pairs of parameters between SFT-flan and CT-X according to their differences in the magnitude. Next, we select a subset of parameters with the smallest magnitude differences to reserve.\nWe evaluate the effectiveness of TIES-SV on the model merging process of SFT-flan and the last checkpoint of CT-ben. According to the results in Figure 4, we reserve 4% parameters in SFT-flan that were to be discarded in the original algorithm of TIES. Table 4 shows the results of vanilla TIES and our TIES-SV on three Bengali tasks. In all three tasks, our TIES-SV outperforms vanilla TIES by 0.52% on average.\nIn conclusion, we propose a simple strategy to improve model merging. It suggests that during the process of model merging, the importance of different models' parameters is not the same. When parameter conflicts occur, we cannot simply rely on the magnitude of the vectors to decide which model's parameters to reserve. Instead, we should use prior information obtained through pilot studies or other means to reserve the parameters of the more important model. We hope our TIES-SV can shed light on the study of new model-merging algorithms in the future."}, {"title": "Discussions", "content": "In this section, we further explore two important questions related to model merging. First, we investigate the potential of merging more than one low-resource language into a task-solving model, which could offer a new avenue for constructing multilingual models. Second, we examine why commonly-used machine-translated data often fails in the context of low-resource languages. This failure underscores the advantage of model merging, as it does not require SFT data in the target language.\n7.1 Can We Merge Multiple Languages?\nPrevious work (Akiba et al., 2024) shows that model merging algorithms can be used to combine multiple LLMs with different capabilities, which may enhance the model to solve complex problems. We wonder whether multiple LLMs adapted to different low-resource languages can be merged with the same SFT model, to construct a a task-solving LLM supporting these languages simultaneously.\nAs a pilot study, we attempt to merge Mongolian and Uyghur LLMs with the English task-solving model SFT-flan. Due to the limited budget for computational resources, we cannot conduct a grid search across the hyperparameter space of the three models. Therefore, we employ the optimal hyperparameters derived from merging each of the two CT models with SFT-flan.\nTable 5 illustrates the average scores of tasks in the two languages. The merged model serving two low-resource languages (TIES-mvf&uig-flan) performs comparably to the two single-language merged models (TIES-mvf-flan and TIES-uig-flan) on tasks in the respective languages.\nThis indicates that model merging has great potential for constructing multilingual task-solving LLMs. We hope this approach can assist multilingual speakers, particularly those using underrepresented languages, by combining multiple existing LLMs in distinct languages without the need for expensive pre-training.\n7.2 Why do Machine Translated Data Fail?\nCollecting synthetic SFT data through MT is an intuitive method for constructing task-solving LLM in non-English languages (Muennighoff et al., 2023; Li et al., 2023a). However, the experiment results in Table 3 show that MT-translated SFT data may not work when it comes to low-resource languages. The models trained with MT-translated data (CTSFT-X-mt) models exhibit inferior performance across all languages compared to those trained on English SFT data (CTSFT-X-flan), with a gap of -3.38%~-24.61%.\nThe decline in model performance can be attributed to the low-quality MT results. Current open-source MT systems have limited abilities of low-resource languages. For example, we ask Uyghur and Tibetan native speakers to evaluate sampled translation results by NLLB. They find that there are often irrelevant contents and undesired code-switching in the translation results, as shown in Appendix C.2. This kind of noise not only hinders the model's ability to learn task-solving strategies but also interferes with language modeling, which itself is inadequately learned.\nIn contrast, model merging eliminates the need to collect SFT data in the target languages. This approach also eliminates the reliance on unreliable machine translation systems."}, {"title": "Conclusion", "content": "In this paper, we investigate the potential of using model merging to construct task-solving LLMs for low-resource languages. Our findings demonstrate that model merging outperforms the conventional CT-then-SFT paradigm, achieving higher data efficiency. We further analyze the mechanism behind the performance saturation of model merging with an increased number of CT tokens, which inspires a simple yet effective improvement to the model merging algorithm. We hope that model merging can reduce the costs associated with data collection and model training, benefiting a greater number of languages suffering from data scarcity."}, {"title": "Limitations", "content": "Studied Languages Due to the high computational cost of continually pretraining LLMs, we cannot cover a wider range of low-resource languages, only focusing on seven underrepresented languages in India and China. However, we make efforts to improve the diversity of selected languages by including different language families and writing systems.\nEvaluation Tasks Most of our evaluated tasks focus on natural language understanding, with less emphasis on natural language generation. This limitation arises from the insufficient CT corpus available for the studied low-resource languages, which is insufficient for the models to learn to perform complex generation in the target language."}]}