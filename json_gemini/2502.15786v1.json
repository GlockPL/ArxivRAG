{"title": "MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding", "authors": ["Weikang Qiu", "Zheng Huang", "Haoyu Hu", "Aosong Feng", "Yujun Yan", "Rex Ying"], "abstract": "Decoding functional magnetic resonance imaging (fMRI) signals into text has been a key challenge in the neuroscience community, with the potential to advance brain-computer interfaces and uncover deeper insights into brain mechanisms. However, existing approaches often struggle with suboptimal predictive performance, limited task variety, and poor generalization across subjects. In response to this, we propose MindLLM, a model designed for subject-agnostic and versatile fMRI-to-text decoding. MindLLM consists of an fMRI encoder and an off-the-shelf LLM. The fMRI encoder employs a neuroscience-informed attention mechanism, which is capable of accommodating subjects with varying input shapes and thus achieves high-performance subject-agnostic decoding. Moreover, we introduce Brain Instruction Tuning (BIT), a novel approach that enhances the model's ability to capture diverse semantic representations from fMRI signals, facilitating more versatile decoding. We evaluate MindLLM on comprehensive fMRI-to-text benchmarks. Results demonstrate that our model outperforms the baselines, improving downstream tasks by 12.0%, unseen subject generalization by 16.4%, and novel task adaptation by 25.0%. Furthermore, the attention patterns in MindLLM provide interpretable insights into its decision-making process.", "sections": [{"title": "1. Introduction", "content": "Decoding human brain activity (fMRI) to text has sparked significant interest within the neuroscience community (Xia et al., 2024; Chen et al., 2023a; Luo et al., 2023; Hmamouche et al., 2024). The ability to translate brain activity patterns into natural language carries both academic and societal importance. For neuroscientists, it provides deeper and novel insights into cognition, behavior, and consciousness (Qiu et al., 2023; Luo et al., 2023). On a societal level, it presents opportunities for medical applications and improves human-computer interaction (HCI) (Bernal et al., 2022; Du et al., 2022). For example, for individuals with speech impairments, this technology could restore communication capabilities, enabling them to express their thoughts effortlessly (Card et al., 2024). Moreover, as shown in Figure 1, it benefits healthy individuals by allowing neural control of digital devices, such as embodied AIs or prosthetic limbs, allowing for more intuitive and precise movements.\nDespite its potential, decoding brain activity to language still faces significant challenges. One major obstacle is the need for versatile decoding tailored to specific applications. For example, decoding may aim to translate a subject's movement intention to control a prosthesis, or to interpret abstract thoughts or memories. Traditional models fail to accommodate such diverse requirements. To address this, UMBRAE (Xia et al., 2024) integrates a Visual Language Model (VLM) (Chen et al., 2023b) and learns to map from fMRI data to corresponding stimulus images. While this approach achieves versatility to some extent, it remains constrained to tasks directly tied to the current stimulus image and cannot address broader tasks, such as retrieving memories of past visual experiences.\nAnother critical challenge lies in designing a unified and subject-agnostic architecture. Current methods of brain multimodal decoding mostly rely on a preprocessing step: selecting responsive voxels by comparing the task-based fMRI to the resting-state fMRI. The selection typically results in higher performance compared to using whole-brain data. However, the varying number and irregular spatial distribution of selected voxels across subjects pose significant challenges for developing a unified architecture. To this end, recent studies (Wang et al., 2024a;b) have proposed pooling or sampling voxels to standardize input dimensions. However, as illustrated in Figure 3, these methods still suffer from the loss of spatial information and uneven representations of certain areas, ultimately comprising performance.\nPresent Work Here we present MindLLM, a subject-agnostic and versatile model for fMRI-to-text decoding. Our approach consists of a subject-agnostic fMRI encoder and an off-the-shelf LLM. The subject-agnostic fMRI encoder incorporates a neuroscience-informed attention layer with learnable queries, enabling dynamic feature extraction by leveraging both spatial information and neuroscientific priors of voxels, thereby significantly enhancing prediction accuracy. The design of values and keys separates the voxel's functional information-which is largely consistent across individuals-from its fMRI value, allowing the model to benefit from shared priors across subjects and enhancing generalization to novel subjects. Moreover, to address the challenge of versatile decoding, we propose Brain Instruction Tuning (BIT). BIT trains the model using a diverse dataset that employs images as intermediaries, encompassing tasks designed to capture diverse aspects of semantic information encoded in fMRI data, including perception & scene understanding, memory & knowledge retrieval, language & symbolic processing, and complex reasoning. Figure 1 illustrates the corresponding components.\nWe evaluate our model on comprehensive benchmarks. Results reveal it outperforms baselines with 12.0% average improvement in various downstream tasks and 16.4% improvement in generalization on unseen subjects. Additionally, we show that our model adapts effectively to novel tasks, demonstrating high customizability and flexibility in real-world applications. Furthermore, our analysis of attention weights offers valuable insights into the working mechanism of our fMRI encoder."}, {"title": "2. Related Works", "content": "Brain-Conditioned Text Generation This line of research mostly focuses on decoding perceived visual stimuli into natural language from fMRI signals. MindGPT (Chen et al., 2023a), UniBrain(Mai & Zhang, 2023) and BrainCap (Ferrante et al., 2023) employ an fMRI encoder guided by CLIP (Radford et al., 2021) and use a language model (Radford et al., 2019; Wang et al., 2022) to decode natural language from the encoded representations. BrainChat (Huang, 2024) utilizes multiple pretraining strategies (Devlin, 2018; He et al., 2022; Yu et al., 2022) to align fMRI with image and text embeddings. These methods fall short in performance and versatility. UMBRAE (Xia et al., 2024) proposes to learn a mapping from fMRI to stimulus images, which later serves as a proxy input for an off-the-shelf visual language model (VLM). Although they achieve performance improvements, the strategy prevents the model from performing tasks that are not directly related to the stimulus images (e.g. answering memory-related questions). In contrast, our end-to-end Brain Instruction Tuning (BIT) ensures seamless and versatile fMRI-to-text decoding, offering the potential to tackle tasks beyond vision-related ones.\nCross-subjects Decoding In voxel-level machine learning for brain decoding, the number of voxels varies between subjects (Allen et al., 2022). Most prior works (Scotti et al., 2024a;b) use an MLP for each subject individually. However, due to the fixed input size required by MLP architectures, these models cannot handle varying input shapes. As illustrated in Figure 2, MindBridge (Wang et al., 2024a) proposed to use an adaptive max pooling layer to standardize the input shapes. However, unlike images, which are considered translation invariance, positions in fMRI carry specific bio-informative significance that pooling operations may overlook. UniBrain (Wang et al., 2024b) proposed to sample groups of voxels. Such a sampling strategy, on the one hand, may lead to information loss if some voxels are not included in any group. On the other hand, the irregular spatial distribution of 3D voxels with varying density and curvature may result in underrepresentation or overrepresentation of certain areas. Different from these methods, our model employs a neuroscience-informed attention mechanism that accounts for every single voxel while preserving their bio-informative positional information, ensuring a more comprehensive and precise representation.\nMulti-Modal Large Language Model Aiming to augment the perceptual capacities of Large Language Models (LLMs), there has been a growing interest in extending them to handle multiple modalities within a unified model. Numerous studies have attempted to incorporate modalities such as images (Alayrac et al., 2022; Zhang et al., 2023b; Wang et al., 2023b), videos (Cheng et al., 2024; Kondratyuk et al., 2023; Zhang et al., 2023a), and point clouds (Xu et al., 2023; Qi et al., 2025). OneLLM (Han et al., 2024a) stands out by aligning eight different modalities, including fMRI, with language. However, their approach employs an individual convolution network for each subject instead of a unified architecture for fMRI encoding across subjects, which restricts its applicability to new subjects in real-world scenarios. Furthermore, the approach solely relies on captions as textual annotations, which limits the model's capability for versatile fMRI decoding."}, {"title": "3. Method", "content": "In this section, we propose a neuroscience-informed fMRI encoder designed to achieve high-performance, subject-agnostic decoding. To further enable versatile decoding, we introduce the construction of a brain instruction tuning dataset, which captures diverse semantic representations encoded in fMRI data."}, {"title": "3.1. Method Overview", "content": "As illustrated in Figure 3, our model consists of an fMRI encoder fe and an off-the-shelf LLM. In practice, we use Vicuna-7b (Zheng et al., 2023) as our LLM to maintain consistency with our baseline (Xia et al., 2024). For each sample, let v = [v\u2081, v\u2082,\u2026, v\u2099] \u2208 \u211d\u1d3a be the fMRI signals of input voxels, where N is the number of voxels. Note that N varies between different subjects, ranging from 12, 682 to 17, 907 in the dataset we use (Allen et al., 2022).\nThe fMRI encoder fe, featuring a neuroscience-informed attention layer, encodes v to fMRI tokens X=[X\u1d65,\u2081, X\u1d65,\u2082 \u00b7\u00b7\u00b7, X\u1d65,\u2097] \u2208 \u211d\u1d48\u02e3\u1d38, where L is the number of tokens and d is the dimension of token embeddings. We then prepend these learned fMRI tokens to the language tokens in the BIT dataset we propose."}, {"title": "3.2. fMRI Encoder", "content": "As mentioned before, currently most models for fMRI decoding can not handle varying input shapes and are not subject-agnostic, with only a few exceptions (Mai & Zhang, 2023). However, these exceptions still suffer from information loss and uneven representations of certain brain areas. To this end, we propose a novel neuroscience-informed attention mechanism to accommodate varying voxel numbers across subjects, enabling a subject-agnostic encoding strategy. Below we talk about the design of queries {q}, keys {ki} and values {vi} in the attention layer. For values, we directly use the fMRI signal of each voxel, which means v\u1d62 = v\u1d62 \u2208 \u211d. Making each voxel a value token maximally prevents information loss compared to pooling- (Wang et al., 2024a) or sampling-based (Mai & Zhang, 2023) methods. The queries are randomly initialized and learnable. We expect each query to represent a certain pattern of the brain (refer to visualizations in Section 4.7). The design of keys will be discussed below.\nExclude fMRI values from keys The vanilla cross attention (Zhu et al., 2020; Vaswani, 2017) derives both keys and values from the same input source. However, we found this would lead to poor performance in fMRI. We argue the reason: different from images or text, which are usually considered translation-invariant, the positions of voxels carry specific brain functional information, as voxels in different areas are associated with distinct brain functions. Consequently, a voxel's position alone can theoretically serve as effective keys for attention weight computation. Including fMRI values into keys, however, introduces additional noise instead of valuable information, thus resulting in poorer performance. Moreover, since brain regions tend to serve similar functions across individuals, decoupling voxel positions from fMRI signals can facilitate the sharing of priors across subjects, potentially improving generalization to unseen subjects.\nIn light of this, instead of the vanilla cross attention, which derives the keys and values from the same inputs, we exclude the fMRI value of each voxel and use its positional information alone as its key embedding. The positional information is encoded from the coordinates of each voxel, i.e. k\u209a\u2092\u209b = PE(c\u1d62) for the i-th voxel, where c\u1d62 \u2208 \u211d\u00b3 denotes the coordinates of the voxel. In practice, we use the Fourier positional encoding proposed in (Tancik et al., 2020) due to its superiority in encoding coordinate information.\nIncorporation of Brain Parcellations While positional encoding alone improves performance, it lacks inherent neuroscientific grounding, potentially making it challenging for the model to efficiently learn representations aligned with established principles of brain function. To overcome this, we incorporate existing brain region parcellations (Glasser et al., 2016; Rolls et al., 2020) into the key embeddings. Formally, given a parcellation P, with regions indexed by 1,..., Np. Let P(i) \u2208 [1,2,..., Np] be the region that the i-th voxel belongs to, and E[P(i)] \u2208 \u211d\u1d48 be the corresponding learnable embedding of the region, which will be incorporated in the key embeddings as k\u1d63\u2091\u2089, \u209a(\u1d62) \u2208 \u211d\u1d48.\nCombining Multiple Parcellations It is crucial to choose an appropriate brain region parcellation. Previous region-based methods (Qiu et al., 2023; Li et al., 2021; Kan et al., 2022) can usually only choose one arbitrarily. In contrast, our model design allows us to combine multiple parcellations P\u2081, P\u2082, ... by concatenating their respective region encodings to the key embeddings. In conclusion, the final key embeddings are the concatenation by the positional encoding and multiple region encodings,\nki = k\u209a\u2092\u209b || k\u1d63\u2091\u2089, \u209a\u00b9 || k\u1d63\u2091\u2089, \u209a\u00b2 || ...   (1)\nwhere || denotes the concatenation operation. This process is illustrated in Figure 3's lower right part.\nThe positional and region encodings complement each other: The region encodings serve as coarse-scale features, providing a neuroscientific-grounded basis, while the fine-scale positional encoding allows our model to learn finer-grained information directly from the data.\nThis attention design separates a voxel's functional information-which is largely consistent across individuals-from its fMRI value, thereby enhancing generalization. Instead of relying on pooling or sampling, the attention mechanism employs learnable aggregation, while the integration of positional encoding and neuroscientifically informed region encodings further ensures high performance.\nAfter the attention layer, we obtain the hidden representations zq \u2208 \u211d\u1d3aq where Nq is the number of query embeddings. We then employ an MLP and a reshape operation to map the hidden representations to L fMRI tokens, i.e., X\u1d65 = reshape (MLP({zq})) \u2208 \u211d\u1d38\u02e3\u1d48.\nThe process of the fMRI encoder is illustrated in Figure 3. The obtained fMRI tokens are then prepended to the language tokens in conversations."}, {"title": "3.3. Brain Instruction Tuning (BIT)", "content": "To enable versatile fMRI-to-text decoding, an appropriate BIT dataset is required, yet no such dataset currently exists. To bridge this gap, we construct one based on the fact: MSCOCO images (Chen et al., 2015) serve as stimuli for fMRI recordings in the fMRI study (Allen et al., 2022), and an abundance of datasets provide text annotations (e.g., VQA) for MSCOCO images. Using the images as intermediaries, we select those relevant to brain functions and pair the fMRI data with corresponding text annotations. For example, given an image of a billboard with annotated textual content, we can reasonably infer that when a subject perceives textual information (e.g., contents on the billboard), corresponding representations are encoded in the brain. This suggests the possibility of extracting such information from fMRI signals. We select datasets to fulfill various purposes, enabling the model to capture diverse aspects of semantic information embedded in fMRI signals, including visual perception & scene understanding, language & symbolic processing, memory & knowledge retrieval and complex reasoning, which are considered among most fundamental and essential properties of human brains (Robertson, 2002; Stenning & Van Lambalgen, 2012; Wade & Swanston, 2013; Friederici, 2017).\nAs illustrated in Figure 4, we begin by using caption tasks at both coarse and fine-grained levels to train the model's ability to understand and summarize what the subject perceives visually (Chen et al., 2015; Krause et al., 2017). Additionally, we incorporate QA tasks (Ren et al., 2015; Krishna et al., 2017; Acharya et al., 2019) to enhance the model's ability to retrieve and reason about visually perceived content.\nTo go beyond tasks directly related to present visual perception, we construct the previous captioning task, a memory-oriented task that challenges the model to caption images that the subject previously viewed, simulating memory recall processes. Furthermore, we aim to encode knowledge structures in human brains. The OK-VQA (Marino et al., 2019) and A-OKVQA (Schwenk et al., 2022) datasets include questions requiring external knowledge that is not present in the image but resides in human brains. For example, A photo of a hydrant may prompt the answer \"firetruck,\u201d even though the firetruck is absent in the image. This association also reflects the way human cognition operates through a network of interconnected meanings, where one concept unconsciously triggers another. Such a process, which is called \"slippage of the signifier\" (Lacan, 2001; 1988; Miller & Lacan, 2018), highlights the symbolic processes through which the brain constructs and retrieves meaning.\nIn addition to the aforementioned OK-VQA and A-OKVQA datasets, which are also related to symbolic process, we further combine datasets of text recognition (Biten et al., 2019) and numerical reasoning (Acharya et al., 2019) to facilitate this aspect.\nFinally, we try to approximate the reasoning process that happens in human brains with datasets (Liu et al., 2023; Wang et al., 2023a; Li et al., 2018) that require intricate logical and inferential processes. We expect these datasets to challenge the model to extract the reasoning process, drawing upon both visual understanding and abstract problem-solving, thus bridging perception, memory, and knowledge into a cohesive cognitive framework.\nWe ended up with a brain instruction tuning dataset consisting of 980, 610 conversations associated with fMRI recordings from 15 datasets. Appendix A lists the instructions and other details for each dataset. The instruction tuning enables versatile fMRI-to-text decoding. In particular, the introduction of tasks like previous caption empowers the model to perform a broader range of tasks beyond vision-related ones, which the previous model (Xia et al., 2024) fails.\nTo train the model with the BIT dataset, for each sample v, we sample a multi-run conversation X\u209c = (X\u1d58, X\u2090, ..., X\u1d58, X\u2090) from all conversations associated with it, where T > 1 represents the number of turns. a indicates the message from the assistant and u indicates the message is from the user. The training objective is to maximize the probability of the assistant's response only\narg max p(X\u2090 | X\u1d65, X\u1d62\u2099\u209b\u209c) = \u220f[t=1 to T] P(X\u2090\u1d57 | X\u1d65, X\u1d62\u2099\u209b\u209c, X\u1d58)   (1)\nWe freeze the weights of the LLM and only train the fMRI encoder since we want to preserve the LLM's language modeling prior and ensure a fair comparison with baselines such as Xia et al. (2024).\nAccording to the analysis in Appendix C, our model does not introduce additional complexity compared to previous methods (Scotti et al., 2024b; Wang et al., 2024a)."}, {"title": "4. Experiments", "content": "In this section, we first evaluate our model on various downstream tasks, demonstrating its versatile decoding capabilities. Next, we assess its generalizability to novel subjects and its adaptability to real-world applications. Finally, we analyze the functions of queries in our neuroscience-informed attention mechanism."}, {"title": "4.1. Settings", "content": "We use the widely used Natural Scenes Dataset (NSD) (Allen et al., 2022), a large-scale dataset consisting of fMRI measurements of 8 healthy adult subjects. During data collection, subjects viewed images from the MS-COCO dataset (Lin et al., 2014) and were instructed to press buttons to indicate whether they had previously seen each image.\nThe downstream dataset will be discussed within each experiment section. See examples and a short description for all dataset we will use in Appendex A. Implementation details could be found in Appendix B."}, {"title": "4.2. Brain Captioning", "content": "To evaluate the model's performance on downstream tasks, we start with the widely used brain captioning benchmark (Xia et al., 2024). The task, built upon COCO Caption (Chen et al., 2015) requires the model to predict captions of given images as fMRI stimuli.\nThe following baselines are considered in this experiment: SDRecon (Takagi & Nishimoto, 2023), UniBrain (Mai & Zhang, 2023), and BrainCap (Ferrante et al., 2023) employs a linear regression, mapping the fMRI to the inputs of an image caption model (Li et al., 2023). OneLLM (Han et al., 2024a) is a multimodal large language models that align 8 modalities (including fMRI) with language all in one model. For fair and efficient comparison, we only finetune the encoder, given that we freeze the LLM in our method as well. UMBRAE learns an encoder that maps fMRIs to images through an encoder similar to the MLP mixer (Tolstikhin et al., 2021). BrainChat (Huang, 2024) segments the flattened voxels into 16 patches and employs a transformer to decode text conditioned on the patches. It is worth noting that all of these baselines require subject-specific layers or parameters. In contrast, our model is subject-agnostic, thus with the potential to generalize on novel subjects.\nFollowing previous works, we use five standard metrics for text generation: BLEU-k (Papineni et al., 2002), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016), METEOR (Banerjee & Lavie, 2005).\nTable 1 shows that our model outperforms baselines in terms of most metrics, with an average improvement of 3.32%, even if our model does not have any subject-specific layers. We argue that this is attributed to both the novel architecture and the introduction of BIT, which will be evident in the next experiment."}, {"title": "4.3. Versatile Decoding", "content": "The purpose of experiments in this section is two-fold: 1) To investigate the impact of our model design and the introduction of BIT on performance improvement. 2) To evaluate the versatility of the model, i.e., its performance on various downstream tasks.\nBesides baselines that could be adapted to this experiment from the previous one, we further consider the following subject-agnostic models as baselines. 1) MindBridge (Wang et al., 2024a) flatten the voxels and adaptively adjust the padding and stride to pool the voxels into a fixed dimension. The original implementation of MindBridge has subject-specific parameters. However, since those parameters are of the same size, we make them shared across subjects and thus make the model subject-agnostic. 2) UniBrain (Wang et al., 2024b) samples voxels into a fixed number of groups and employs a transformer where groups are treated as tokens. This UniBrain is unrelated to the UniBrain in the previous section; they just share the same name.\nWe use the test split of all QA & caption datasets in the BIT dataset. We strictly adhere to the official metrics on all datasets. In summary, for sentence generation, we use BLEU-k (Papineni et al., 2002), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016), METEOR (Banerjee & Lavie, 2005). For QA-related tasks, we use VQA accuracy (Antol et al., 2015) as well as special metrics proposed in the original paper (e.g. ANLS for ST-VQA (Biten et al., 2019)).\nThe results are shown in Table 2. Our model outperforms baselines, with an average improvement of 12.0%. Further, by comparing instruction tuning and from-scratch models, we find that instruction tuning has a significant positive effect, with an average improvement of 28.0%. The results remain stable across different random seeds; for instance, according to our observations, the BLEU-1 score for paragraph captioning exhibits a maximum of \u00b10.3 variance."}, {"title": "4.4. Unseen Subject Generalization", "content": "Our neuroscience-informed, subject-agnostic design enhances generalization to novel subjects, a crucial factor in real-world applications where training a model for each individual is impractical. To evaluate it, we perform instruction tuning on 7 out of the 8 subjects in the natural scene dataset (Allen et al., 2022), and evaluate generalization on the held-out subject. Table 3 shows our model outperforms two other subject-agnostic baselines in most cases, with an average improvement of 16.4% compared to the second-best model."}, {"title": "4.5. Adapting to New Tasks", "content": "It is common that users want to adapt the MindLLM to their own specific use cases. To this end, we aim to assess our model's adaptability to new tasks.\nWe use TDIUC (Kafle & Kanan, 2017), a QA dataset consisting of 12 types of questions, as a benchmark to evaluate the model's various capabilities comprehensively. Additionally, we further select 2 task types-sentiment understanding and utility/affordance tasks, that are particularly relevant to BCI applications as sub-datasets. The utility/affordance task, for instance, enables the model to identify useful objects in a given scene and autonomously decide whether to utilize them. Following their paper, we compute the accuracy of each type and report the arithmetic mean-per-type (A-MPT) and the harmonic mean-per-type (H-MPT). For the 2 selected types, we report the accuracy respectively.\nTable 4 shows our model achieves balanced (high harmonic mean) and consistently improved performances with an average of 13.5%. We could also observe the performance benefits from BIT, with 25.0% absolute improvement."}, {"title": "4.6. Ablation Study", "content": "We conduct ablation studies on the design of key embeddings in the neuroscience-informed attention module in Figure 7. The results strongly validate our design. The vanilla cross attention (Pos Enc.+fMRI) leads to poor performance while removing fMRI values from the key embeddings (Pos Enc.) yields a significant improvement. Replacing positional encoding with region encodings (Reg. Enc.) accelerates convergence in the early stages since it is grounded by neuroscientific principles. However, it is eventually outperformed by Pos Enc. due to the lack of finer-grained information. Combining the positional encoding and region encodings (Pos Enc.+Reg Enc.) achieves the best model design. In addition, replacing positional encoding with an MLP that maps coordinates to embeddings results in poor performance ((x,y,z)+MLP), which indicates the amount of high-frequency spatial information in fMRI signals."}, {"title": "4.7. Visualizations and Interpretations", "content": "Unlike previous deep learning models (Scotti et al., 2024b; Mai & Zhang, 2023), our model allows interpretations by investigating how queries work in the neuroscience-informed attention layer. We inspect the attention weights between queries and voxels in Figure 6.\nWe found that some queries primarily focus on processing single brain regions, such as Parahippocampal Place Area (PPA) (Figure 6a) and Fusiform Face Area (FFA) (6b). As previous research has shown, PPA is related to conceptual association, semantic processing and environmental memory (Epstein et al., 1999; K\u00f6hler et al., 2002; Bar et al., 2008; Epstein & Ward, 2010) and FFA is known for its critical role in expertise recognition, social cognition and identity memory (Schultz et al., 2003; Tsantani et al., 2021; Xu, 2005). Both are important brain regions for the conceptualization of visual information and are responsible for the interaction between real-time stimulus and past memory (Brewer et al., 1998; Ranganath et al., 2004; Golarai et al., 2007).\nMoreover, there are some queries that attend to multiple brain regions, revealing the information transmission between low- and high-level brain regions. For instance, interactions between early visual areas and higher-level regions like PPA and IntraParietal Sulcus (IPS) (Figure 6e), revealing a potential pattern for human attention-guided actions (Tunik et al., 2007; Connolly et al., 2016). Additionally, queries are also found responsible for communications between high-level brain regions (Figure 6c,6f). Together, these findings indicate that the learnable queries may reflect the dynamics of human brain activities in the visual task, from seeing and thinking about the image to pressing the button for the visual recall task in NSD (Allen et al., 2022).\nWe also provide qualitative analysis of model responses in Appendix D."}, {"title": "5. Conclusion", "content": "In this work, we propose MindLLM, a subject-agnostic and versatile fMRI-to-text decoding model. Our neuroscience-informed attention mechanism in the fMRI encoder ensures subject generalization, while brain instruction tuning enhances versatility. Comprehensive benchmarks demonstrate our model's state-of-the-art performance, and visualizations offer insights into the patterns it leverages. We envision that these advancements will contribute to medical treatments, neuroscience research, and brain-computer interfaces in the future. Limitations: This work focuses on static fMRI, without incorporating temporal dynamics. Future research could explore integrating temporal information and investigating its relationship with other modalities such as videos."}, {"title": "C. Computation Complexity", "content": "In the neuroscience-informed attention, the complexity of the dot product between queries and keys is O(dN Nq). The complexity of the aggregation of values is O(N Nq), which is neglectable. The MLP maps the hidden representation of dimension Nq to L \u00d7 d, therefore its complexity is dLNq. Therefore, the complexity of the fMRI encoder is O(dN Nq + dLNq) = O(dNq(L + N)) = O(dNqN) given that L \u00ab N."}, {"title": "D. Qualitative Analysis", "content": "In this section, we present a qualitative analysis of our model on COCO Captioning, COCO-QA, and OK-VQA, comparing its performance against MindBridge (Wang et al., 2024a) and UniBrain (Wang et al., 2024b). As shown in Figure 8 and 9, our results demonstrate significant improvements in visual understanding across multiple tasks. The model shows strength in the following areas: Static Object Recognition. The model demonstrates superior accuracy in identifying stationary objects. In comparison with baseline models (MindBridge and UniBrain), our approach shows improvement in spatial context understanding. For example, when analyzing aircraft imagery (e.g., (a) of Figure 8), our model correctly identifies \"airplane sitting on the runway\u201d while baselines incorrectly interpret the scene as \"flying in the sky\u201d or \"flying over a city\u201d, demonstrating better state-space recognition. Action Recognition. Our proposed model exhibits enhanced capability in distinguishing between similar actions. In sports scenarios (e.g., (f) of Figure 9), our model correctly identifies \"catch ball\u201d while both baselines incorrectly predict \"serve\u201d, indicating improved action-state discrimination. Potential of neuroscience application. The demonstrated improvements in object understanding and action recognition suggest the potential for advancing brain-computer interface technology and neural processing research. The model's enhanced capabilities in distinguishing object states and actions could lead to more effective neural prosthetics and improved assistive technologies for individuals with visual or motor impairments."}]}