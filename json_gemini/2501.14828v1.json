{"title": "An Ensemble Model with Attention Based Mechanism for Image Captioning", "authors": ["Israa Al Badarneh", "Bassam H. Hammo", "Omar Al-Kadi"], "abstract": "Image captioning creates informative text from an input image by creating a relationship between the words and the actual content of an image. Recently, deep learning models that utilize transformers have been the most successful in automatically generating image captions. The capabilities of transformer networks have led to notable progress in several activities related to vision. In this paper, we thoroughly examine transformer models, emphasizing the critical role that attention mechanisms play. The proposed model uses a transformer encoder-decoder architecture to create textual captions and a deep learning convolutional neural network to extract features from the images. To create the captions, we present a novel ensemble learning framework that improves the richness of the generated captions by utilizing several deep neural network architectures based on a voting mechanism that chooses the caption with the highest bilingual evaluation understudy (BLEU) score. The proposed model was evaluated using publicly available datasets. Using the Flickr8K dataset, the proposed model achieved the highest BLEU-[1-3] scores with rates of 0.728, 0.495, and 0.323, respectively. The suggested model outperformed the latest methods in Flickr30k datasets, determined by BLEU-[1-4] scores with rates of 0.798, 0.561, 0.387, and 0.269, respectively. The model efficacy was also obtained by the Semantic propositional image caption evaluation (SPICE) metric with a scoring rate of 0.164 for the Flicker8k dataset and 0.387 for the Flicker30k. Finally, ensemble learning significantly advances the process of image captioning and, hence, can be leveraged in various applications across different domains.", "sections": [{"title": "1 Introduction", "content": "Identifying key components in an image, understanding their relationships, and creating syntactically and semantically consistent descriptions of the visual content are all necessary to create an image caption. This is one of the hardest tasks in artificial intelligence because it requires the integration of two very different research communities: natural language processing and computer vision [1]. An overview of the standard architecture of the image captioning model is given in Fig.1. The general architecture of an image captioning system typically consists of several key components. It begins with an image input processed by a Convolutional Neural Network (CNN) for feature extraction, utilizing pre-trained models like ResNet or Inception to capture the essential visual elements. These extracted features are fed into a generation caption model such as Long-Short-Term Memory (LSTM) units or transformers. An optional attention mechanism can enhance this process by allowing the model to focus on specific image areas while forming each caption word. Finally, the system produces an output caption that represents the generated description of the image.\nRecent developments in deep learning models, made possible by cutting-edge computational capabilities, have significantly advanced this discipline [2, 3]. Image captioning is challenging in artificial intelligence since it combines computer vision and natural language processing research. A captioning model aims to represent the text and scene, as this is essentially what the human brain does. Humans can automatically describe much information about any given image with a glance. One of the many difficulties and unsolved problems inherent in image captioning is the parallax error. It may be difficult for the human eye to identify an object, even at certain angles, where its appearance varies to the point of being undetected. An object class may include several objects of various forms and angles. Additionally, the visual assistant could have difficulty correctly identifying objects hidden by other objects. Object recognition is negatively affected by scene clutter [4]. There are many industries in which image captioning research can find practical applications. Examples include medical imaging for analysis and diagnostics [5-8], improving education for students [9, 10], supporting visually impaired people [11], helping virtual assistants [12], facilitating information retrieval [13], aiding video surveillance [14], improving social media content [15], and even assisting automated self-driving cars [16]. Additionally, it is essential to improve the quality of image search [17]. Template-based, retrieval-based, and deep learning-based approaches are the three primary categories of image captioning techniques. Template-based approaches create captions using predefined templates with"}, {"title": "2 Related works", "content": "Several papers have recently employed deep-learning techniques to generate captions for images. This section provides an overview and discussion of related works."}, {"title": "2.1 Neural Network based model", "content": "In the automatic generation of image captions[24], the encoder-decoder architecture was used in the suggested model. The encoder processed the input image to extract the relevant data, while the decoder used these features to generate the caption. The image was encoded into a feature vector with a specified length. Long Short-Term Memory (LSTM) cells were used to implement the decoder. Utilizing pre-trained models and deep learning approaches, the suggested method showed encouraging results. The work in [25] The suggested model performed better when creating informative captions for images. Authors of [26] provided a model that creates natural language descriptions of images for generating image descriptions. The method consisted of bidirectional RNNs over phrases, CNNs over image areas, and a structured objective using multimodal embedding to align the two modalities. The alignment model obtained state-of-the-art results."}, {"title": "2.2 Attention based model", "content": "The challenging task of automatically producing meaningful captions for images was discussed in [27], and suggested a collaborative model known as AICRL (Automatic Image Captioning based on ResNet50 and LSTM with Soft Attention). The encoder used a CNN called ResNet50 to represent the input image comprehensively. The model gathered the quality of generated captions by focusing on relevant locations. The experiment results showed that the AICRL model is useful for producing image captions. It offers a promising means of bridging the gap between natural language descriptions and visual content, making it applicable to various computer vision applications and beyond. It is remarkable that the aligned attention method is model-independent and may be quickly added to current innovative image captioning models to enhance their captioning capabilities. [28] presented a transformer-based model for image captioning; their strategy used a mask operation to automatically assess the influence of image region features and use the results as supervised information to direct attention alignment. This work provided a useful reference for self-supervised learning. The transformer-based framework LATGeO was proposed in [29] to caption images, and it includes multi-level geometrically coherent and visual recommendations to relate objects based on their localized ratios. LATGeO used object proposals to find coherence and connected its embeddings with less significant surrounds. A brand-new label-attention module (LAM), an extension of the traditional transformer, was developed to bridge the gap between the visual and linguistic worlds. Although normalization has traditionally only been used outside of self-attention, the work of [30] provided a unique normalization method and showed that doing so in hidden activation within self-attention is feasible and advantageous. They provide a class of geometry-aware self-attention (GSA) that extends self-attention to explicitly and efficiently consider the relative geometry relations between the objects in the image to model the geometry structure of the input objects for feature extraction. Faster-RCNN was used. The inputs to the transformer encoder are region-based visuals, and the transformer decoder predicts the subsequent word recursively using the attended senses and the embedding of the preceding words."}, {"title": "2.3 Ensemble based model", "content": "Ensemble learning aims to increase generalizability and robustness over a single model by combining the predictions of various base models. Modern techniques for detecting hate speech in multimodal memes [32] applied the majority voting technique, also known as the hard voting or voting classifier, which combines many classifiers and voting classifiers. As a result, it performs better than any individual model utilized in the ensemble. Textual and visual hybrid methods are combined using the max voting technique to classify a fake or real news instance. [33], in this work, the maximum voting method was used. The proposed system consists of four independent parallel streams capable of detecting specific forgeries. All four streams handled each input instance. These independent predictions are finally combined using the maximum voting ensemble method.\nIn [34], an image captioning method was presented using a set of weighted multi-channel fusion optimization enhancements to optimize the encoder and decoder. In the model that is being described, a multichannel encoder was suggested that can combine different models and algorithms to extract different information from the same image, researchers suggested combining separate decoders of the same type using the voting weight technique for decoder fusion to improve the description produced by the decoder. For the concept detection task, [35] considered an image retrieval approach using an ensemble of five different CNNs, where the top N photos most similar to the training set and their related CUIs were used to assign a set of CUIs to each query image. The top N images that look the most like a query image, determined by the cosine similarity between image embeddings, were extracted using CNN as the image encoder; then, an aggregation step was carried out to choose the set of CUIs to link to each query image. This involved soft majority voting. A recent work [36] proposes a soft voting-based ensemble model that benefits from the efficient operation of various classifiers on various modalities. Deep feature extraction from multimodal datasets was performed for the proposed model using deep learning methods (BiLSTM, CNN). The final feature sets were classified using the soft voting-based ensemble learning model after completing the feature selection process for the features that combine text and image features. In [37], an effective deep-set medical image captioning network (DCNet) was suggested to give doctors and patients explanations. Three well-known"}, {"title": "2.4 Insights from previous research and our solution", "content": "The discussion above revealed that contemporary captioning models rely on RNN and LSTM as language models. However, one key issue with these approaches is the occurrence of vanishing gradients, limiting their effectiveness. Moreover, the RNN and LSTM models are not hardware-friendly and require additional computational resources. An alternative approach explored in the literature is using Generative Adversarial Networks (GAN) for image captioning. However, GANs come with challenges due to their discrete nature, making training such systems a difficult task [39, 40]. Using a hybrid approach, combining LSTM with transformer models introduces specific limitations and drawbacks. For example, it can increase the complexity of the model, attributed to architectural differences, resulting in a higher demand for resources and extended training times [41]. Consequently, this complexity can affect the interpretation of model decisions, hindering a clear understanding of the underlying reasoning. Image captioning is an attractive task that involves understanding visual and textual information. The need for image captioning arises from the need to make visual content accessible to individuals. Therefore, developing and implementing dedicated image captioning systems is essential to address this need. Therefore, this research aims to bridge this gap by introducing a hybrid approach that combines a transformer with an attention mechanism to help the model capture complex details in images and generate more contextually relevant captions. The rationale behind this combination is that transformers are great for capturing long-range dependencies in data, while attention mechanisms help them focus on relevant parts. Ensemble learning, on the other hand, can boost overall performance by combining multiple models. The subsequent section will explore the details of this approach."}, {"title": "3 Methodology", "content": "This work followed a methodology incorporating four stages: data description and data preprocessing, model development, experimentation, and performance evaluation. The following subsections discuss each stage in more detail."}, {"title": "3.1 Dataset", "content": "This section will introduce the commonly used datasets in image captioning. details of these datasets.\nFlickr8K[42]: it was published for public use in 2013. The photographs in the dataset, which total 8000, are all from the photo and image-sharing website Flickr. The image content is mostly human and animal. The description for the label was"}, {"title": "3.2 Data preprocessing", "content": "Because of raw textual data challenges, cleaning and preprocessing datasets before they are used in ML models have become essential. The approach we applied to text preprocessing was comprehensive and systematic. Several procedures were employed in the data preprocessing, including the following:\n(a) Text normalization: Typically, actions are taken to reduce the number of extracted terms. They include eliminating special and non-letter characters ($, &, %,..).\n(b) Text tokenization: In this step, a linguistic analysis of the text is performed. Separates words, character strings, and punctuation marks into tokens during indexing. This process aims to divide the text into a stream of discrete tokens, or words, by identifying the sentences' borders and eliminating any unnecessary punctuation.\n(c) Adding start and end tokens: Finally, distinctive start and end tokens were appended to determine the beginning and end of each caption, adding a layer of structural clarity to the dataset. A unique padding token was introduced to address the variability and standardize the length of captions."}, {"title": "3.3 The proposed model for image captioning", "content": "The following are the steps applied through the model, and the following subsections discuss each stage in more detail. Algorithm 1 provides the pseudo-code outlining the operations of the model."}, {"title": "3.3.1 Image feature extraction", "content": "a) Convolutional neural network (CNN): Popular deep learning models include recurrent neural networks (RNNs), convolutional neural networks (CNNs), deep belief networks (DBNs), and deep Boltzmann machines (DBMs). Using shared weight filters and hierarchical learning, CNNs are highly effective in understanding visual data [45, 46]. CNN-based encoders on ImageNet that have been pre-trained are frequently used in image captioning to convert images into visual vectors. Selective focus during generation is made possible by preserving fine-grained correspondence using sets from lower convolution layers [47, 48].\nCNNs provide the following benefits over conventional neural networks when used in computer vision applications: 1) The main reason to consider CNN is its weight-sharing feature, which reduces the number of trainable network parameters, allowing the network to increase generalization and preventing overfitting. 2) Learning both the classification layer and the feature extraction layers simultaneously produces a well-structured model output that depends on the features that were extracted. 3) CNN facilitates large-scale network installation more easily than other neural networks [49].\nb) Transfer learning: Applying a previously learned model to a modified environment is known as transfer learning. Due to its ability to train deep neural networks on tiny datasets, it is particularly well preferred in the deep learning field. This is particularly helpful in data science because most real-world scenarios do not require millions of labeled data sets to train complicated models. To apply transfer learning to image captioning, the model was first trained on a standard dataset under supervision, and then its knowledge was transferred to a new dataset consisting of unpaired"}, {"title": "3.3.2 Text generation with a transformer", "content": "One kind of neural network architecture is a transformer. Transformer was first introduced in the publication \"Attention is all you need\" [59]. Text data is well handled by the Transformer architecture, which is sequential by design. After receiving one text sequence as input, they create another one with a stack of encoder and decoder layers. The encoder and decoder stacks contain matching embedding layers for their respective inputs. There is an output layer at the end to create the final result. The encoder and a feedforward layer contain the crucial self-attention layer, which determines the connections between the words in the sequence. The decoder consists of the feedforward layer, the self-attention layer, and a second encoder-decoder attention"}, {"title": "3.3.3 The attention mechanism", "content": "Attention mechanisms focus on the most relevant features extracted by CNNs, which is crucial for tasks such as image captioning, where context is key. Attention in image processing mimics human attention patterns. Its strength lies in establishing meaningful connections between features and enhancing the models' ability to prioritize important features while filtering out noise. This aligns with the attention mechanisms that guide the focus of the model during training [4]. Despite the richness of the image data, not all features require explicit attention in captioning. When attention is integrated into the encoder-decoder picture captioning framework, sentence creation becomes contingent on hidden states computed using the attention method. The attention mechanism is a fundamental component of the encoder-decoder architecture within this framework. Using various types of input image patterns to guide the decoding process, ensuring that attention is focused on specific features of the input image at each time step. This composed focus on attention facilitates the generation of a descriptive caption for the input image [62].\nAttention guides computations on significant regions to improve caption quality in image annotation. This is achieved by using soft and hard attention mechanisms to estimate the focus of attention. Soft attention, trainable via standard backpropagation,"}, {"title": "3.3.4 The Beam search algorithm", "content": "The greedy decoding technique outputs the word with the highest probability. However, it quickly accumulates potential errors. To solve this problem, the beam search"}, {"title": "3.3.5 Ensemble learning", "content": "Typical learning techniques may not produce sufficient results because various features and the underlying structure of data are difficult for these methods to capture. So, building an effective model becomes a crucial problem in the data mining industry. An area of research that is gaining interest is ensemble learning, which tries to combine data fusion, data modeling, and data mining into a single framework. In which a set of features is first extracted using multiple learning algorithms to provide predictions based on these learned properties. Then, ensemble learning combines useful information to improve prediction accuracy across a variety of voting processes to outperform the results of any individual algorithm. Through the use of multiple machine learning algorithms, ensemble learning techniques generate weak predictions based on features extracted from a variety of data projections. The results are then fused with different voting mechanisms to produce performances that are better than those of any one of the constituent algorithms alone. Ensemble learning is used to improve architecture performance [23]. Several ensemble models exist [69], including bagging, boosting, stacking, voting:\na) Bagging: Breiman [70] created bootstrap aggregation, often known as bagging, to improve the classification performance of machine learning models by aggregating the predictions from randomly generated training sets. It was argued that bagging can increase accuracy because varying the learning set can result in appreciable changes to the predictor that is produced. In addition, diversity is achieved in bagging through the creation of bootstrapped copies of the input data, in which a number of randomly selected subsets are selected with replacements from the initial training set. As a result, the different training sets are considered distinct and are used to train different base learners for the same machine-learning algorithm.\nb) Boosting: A machine learning method called \"boosting\" can turn a weak classifier into a powerful one. It is a kind of ensemble meta-algorithm that lowers variance and bias. A classifier that performs marginally better than random guessing is considered weak, whereas classifiers that achieve considerable accuracy are considered strong, and it is upon these classifiers that the boosting ensemble methods are based. [71] addressed the boosting algorithm regarding the possibility of producing a single strong learner from a group of weak learners.\nc) Stacking: An ensemble learning framework called stacked generalization, or stacking, trains a different machine learning algorithm to aggregate the predictions of two or more ensemble members. Wolpert [72] first proposed an effort to reduce the generalization error in machine learning issues. When many machine learning models are particularly skilled at a specific position, stacking can be helpful. In this case,"}, {"title": "4 Experimental results", "content": "This section presents the results obtained from the proposed model and compares them with the latest models."}, {"title": "4.1 Environment setup", "content": "To assess the performance of the proposed model, a set of experiments was conducted using the Google Colab Pro+ framework, equipped with 52 GB of RAM and 1 TB of"}, {"title": "4.2 Evaluation metrics", "content": "While direct human judgment is the simplest way to evaluate text generated for images, scalability is challenging due to nonreusable human effort and subjective nature. To overcome these challenges, various evaluation metrics assess the performance of image captioning systems. These metrics measure the systems' ability to generate linguistically acceptable and semantically valid phrases. However, the choice of the most significant metric depends on the specific objectives of the image captioning task. BLEU and ROUGE are often considered standard. However, recent research has shown the value of incorporating diverse metrics such as METEOR, CIDEr, and SPICE to"}, {"title": "4.2.1 Bilingual evaluation understudy (BLEU)", "content": "BLEU is a metric that evaluates the quality of machine-generated text by comparing individual segments to a set of reference texts [76]. Its approach varies with the number of references and the length of the text. BLEU scores are higher for short autogenerated text and range from 0 to 1. The comparisons of the gram and the bigram determine BLEU-1 and BLEU-2, with an empirically determined maximum order of four for optimal correlation with human judgments. BLEU assesses adequacy through unigram scores and fluency through higher n-gram scores. Although widely used and language-independent, BLEU has drawbacks. It favors brief output texts, and a high score does not guarantee higher quality, making it imperfect for certain evaluations [4]."}, {"title": "4.2.2 Recall-oriented understudy for gisting evaluation (ROUGE)", "content": "ROUGE is a set of measures that evaluate text summaries by comparing word sequences and pairs to a database of human-written reference summaries [81]. Originally designed for machine translation accuracy and fluency assessment, it quantifies sentence-level similarity using the longest common subsequence between candidate and reference sentences. Similarly to BLEU, ROUGE is also computed by varying the n-gram count. However, unlike BLEU, which is based on precision, ROUGE is based on recall values. It captures sentence-level structure with in-sequence word matches, allowing non-sequential matching. ROUGE-L is the version that is used in the evaluation of image and video captioning. It calculates the recall and precision scores of the longest common subsequences (LCS) between each generated sentence and its corresponding reference sentence."}, {"title": "4.2.3 Metric for explicit ordering translation evaluation (METEOR)", "content": "METEOR is designed for machine translation evaluation and is considered more valuable than BLEU, with a stronger link to human evaluations [78]. It calculates scores based on generalized unigram matches between a candidate sentence and human-written reference sentences. The precision, recall, and alignment of the matched words"}, {"title": "4.2.4 Consensus-based image description evaluation (CIDEr)", "content": "CIDEr is an image caption quality assessment paradigm that relies on human consensus [79]. Assesses the similarity of a generated sentence to a set of human-written ground-truth sentences. Using the TF-IDF weighting for each n-gram in the candidate phrase, CIDEr encodes their frequency in reference sentences. CIDEr evaluates the grammar, significance, and accuracy of image captions and descriptions. Unlike metrics that work with a limited number of captions per image, CIDEr employs consensus utilization, making it suitable for analyzing the agreement between generated captions and human assessments [4]."}, {"title": "4.2.5 Semantic propositional image caption evaluation (SPICE)", "content": "SPICE is a semantic concept-based image caption evaluation metric based on semantic scene graphs [80]. It uses a graph-based semantic representation extracted from image descriptions [1]. Generated and ground-truth captions are converted into an intermediate scene graph representation through semantic parsing to calculate the SPICE score. The F1 score derived from precision and recall measures the similarity between the generated and ground-truth caption scene graphs."}, {"title": "4.3 Quantitative analysis", "content": "Table 2 compares the results obtained by the proposed model with the latest methods and Fig. 7. As will be discussed soon, the proposed model exhibits superior performance, with the highest scores highlighted in bold. The result includes the research with models based on the Flickr8k dataset. The proposed model achieved the highest scores in BLEU-1, BLEU-2, and BLEU-3: 0.728, 0.495, and 0.323, respectively. These scores indicate how well our system's predictions align with reference captions on n-gram overlap. Our model obtained the highest result of the METEOR score, 0.604, which evaluates the semantic similarity between the generated and reference captions. The SPICE score, which focuses on semantic content overlap, was used to assess the quality of image captions. Our model achieved the highest SPICE value of 0.164, indicating strong semantic alignment. We also get competitive results for ROUGE L (0.432) and CIDEr (0.604). ROUGE L measures the longest common subsequence between generated and reference captions, CIDEr considers word frequency and diversity.\nTo demonstrate the effectiveness of the suggested model, we contrasted its results with those of the most advanced models on Flickr30k datasets, as indicated in Table 3 and Fig. 8. The suggested model outperformed the latest methods in Flickr30k datasets, as demonstrated by the table, as determined by the BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores. ROUGE L METEOR CIDEr showed that the model"}, {"title": "4.4 Qualitative analysis", "content": "As demonstrated in Fig. 7 and Fig. 8, we have provided several sentences produced by our caption method to validate the effectiveness of our model. In general, our model demonstrates proficiency in generating captions that are not only relevant but also accurate in describing the image content. Fig. 7 presents samples of nearly correct captions from the Flicker8K dataset. Green text is used to identify the generated captions. As noted in Fig. 7.b, where the man is not riding the bike in the position in the picture, it is revealed that he is performing a trick by \"do the trick\". In Fig. 7.c, despite the terms \"forest\" and \"wood\" appearing in the references, the model was able to accurately depict the appearance of wood in the image accurately; however,"}, {"title": "4.5 Ablation study", "content": "An ablation study was conducted to assess the contributions of individual components within the proposed ensemble model for image captioning. The Flicker8K dataset was utilized for this purpose. In this study, evaluation metrics such as BLEU, ROUGE, METEOR, CIDEr, and SPICE were applied. This section outlines the methodology used and presents the findings, highlighting the importance of each model within the architecture. The following ensemble configurations were analyzed:\n1. Baseline 1: MobileNet V2, VGG16, VGG19, and ResNet50. These CNN models were used for feature extraction, capturing essential visual elements from the images before the transformer processes the extracted features for text generation."}, {"title": "2. Baseline 2:", "content": "MobileNetV2, VGG16, VGG19, ResNet50, and ResNet101. Like Baseline 1, this configuration incorporates additional CNNs to enhance feature extraction, providing a richer representation for the transformer during caption generation."}, {"title": "3. Baseline 3:", "content": "MobileNet V2, VGG16, VGG19, ResNet50, ResNet101, RegNetX120 and Efficient NetB4. This ensemble combines multiple CNN models to maximize feature extraction capabilities, allowing the transformer to generate more accurate and contextually relevant captions."}, {"title": "4. Full Ensemble Model:", "content": "In the full model, we evaluated the performance of the complete ensemble model, which integrates the outputs of all CNNs and a transformer language model to generate captions.\nThe results are summarized and compared in Table 4."}, {"title": "4.6 Discussion", "content": "The results demonstrated how our model differs from other methods in feature text extraction by focusing on salient image regions and characteristics through attention mechanisms. Table 2 and Table 3 highlight the important distinctions between our suggested model and the other models and emphasize the research contributions in the following areas:(1) enhanced prediction robustness: In contrast to previous methods, our model uses an ensemble learning strategy, which effectively combines eight CNN models via a voting process, to fine-tune the ideal caption for every image. This increases the architecture's robustness and generalizability, while greatly improving its efficiency. Our model efficiently reduces overfitting by combining predictions from many base models, resulting in a more robust and flexible solution. (2) comprehensive evaluation metrics: to gain a deeper understanding of the model's capabilities, we used a methodology in this research work that took a variety of indicators into account. A more realistic description of the overall performance of a recently suggested model in this growing field will come from a comprehensive evaluation that considers multiple factors."}, {"title": "4.7 Real-World applications of the proposed image captioning model", "content": "Our proposed image captioning model could be applied in real-world scenarios in enhanced search engines, a search engine integrates an image captioning model to improve the search experience by delivering more informative and relevant image results. When a user enters a search query for images, the model analyzes each image in the database and generates detailed captions that accurately describe the content. These captions are indexed alongside the images, enabling the search engine to retrieve results that align more closely with the user's query. Another compelling use case is assistive technology for the visually impaired. In this scenario, visually impaired individuals use a mobile application to capture photos of their environment. The image captioning system analyzes these images and generates descriptive audio captions that"}, {"title": "5 Limitation and future work", "content": "The proposed Attention-Based Transformer Model for Image Captioning encounters several challenges. First, the model sometimes misinterprets the color of certain areas as corresponding to different areas or clothing items, highlighting the difficulty of recognizing multiple attributes associated with a single factor in computer vision. Second, there are instances where the model does not accurately count the number of elements in the target image; this task involves a higher level of artificial intelligence than simple object recognition. In addition, the model may struggle to understand complex settings, leading to incorrect interpretations. Lastly, a key limitation that affects model performance in image captioning is the presence of noisy or ambiguous images. Although this issue falls outside the scope of the current research, it is important to note that such images, characterized by distracting elements, low resolution, or unclear subjects, can hinder the model's ability to accurately interpret visual content, resulting in incorrect or irrelevant captions.\nFuture advances in image captioning can effectively address existing limitations through focused research initiatives. First, improving object recognition algorithms will enhance the model's ability to detect and distinguish items within images accurately. Second, improving the robustness of the model against noisy or ambiguous input to improve the quality of the caption and the overall performance in various real-world scenarios. Developing more comprehensive evaluation metrics can also offer a deeper assessment of caption quality. Furthermore, exploring how to handle diverse datasets effectively, ensuring that the model can generalize well across different scenarios. Finally, enhancing the visualization of image captioning models will allow insight into how the model focuses on specific areas of an image when generating captions."}, {"title": "6 Conclusions", "content": "Converting an input image into an explanation in words is known as image captioning. It can be used in various situations, including social networking, smart travel, assisting the blind, medical image captioning for healthcare, education, and training of children. Competition among researchers is leading to an increase in the number of unique models. In this research, we thoroughly investigated the transformer model and provided many helpful ways to attract attention. We have shown the potential for significant advancement in this field by implementing an attention mechanism in a transformer-based design. We introduce a novel ensemble learning framework to generate captions based on a voting mechanism that selects the caption with the highest bilingual evaluation understudy (BLEU) score. This framework enhances the richness of the captions generated by utilizing multiple deep neural network architectures. The robustness and efficacy of the proposed approach have been demonstrated by a comprehensive analysis of the Flickr8K and Flickr30K datasets combined with common"}, {"title": "7 Declaration", "content": "Data Availability: The datasets used in this study are publicly available and can be downloaded by the following links: Flickr8k (https://www.kaggle.com/datasets/adityajn105/flickr8k), Flickr30K https://www.kaggle.com/datasets/eeshawn/flickr30k)"}]}