{"title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey", "authors": ["Bo Ni", "Zheyuan Liu", "Leyao Wang", "Yongjia Lei", "Yuying Zhao", "Xueqi Cheng", "Qingkai Zeng", "Luna Dong", "Yinglong Xia", "Krishnaram Kenthapadi", "Ryan Rossi", "Franck Dernoncourt", "Md Mehrab Tanjim", "Nesreen Ahmed", "Xiaorui Liu", "Wenqi Fan", "Erik Blasch", "Yu Wang", "Meng Jiang", "Tyler Derr"], "abstract": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact. For more information about the survey, please check our GitHub repository*.", "sections": [{"title": "1 Introduction", "content": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to address the challenges faced by Large Language Models (LLMs), such as hallucinations, reliance on outdated knowledge, and the lack of explainability [55, 223]. By incorporating external information into the generation context, RAG improves the accuracy and reliability of the generated content. The recency of information also enables the model to stay current with minimal training costs by reducing the need for extensive retraining of the entire system to update its parameters. These benefits have profound implications for real-world applications. For example, RAG has been effectively applied in medical question answering [195, 216, 162], legal document drafting [190, 132], educational chatbots [172], and financial report summarization [208] due to their adaptability in various domains.\nThe definition of trustworthiness often depends on the context of discussion [102, 191, 95, 58, 36, 107, 219, 105]. In the context of machine learning and artificial intelligence, trustworthy AI must exhibit characteristics that make the system worthy of trust. In 2022, the National Institute of Standards and Technology (NIST) published guidelines for trustworthy AI, defining trustworthiness from several perspectives [169]: Reliability, Privacy, Explainability, Fairness, Accountability, and Safety.\nReliability ensures that the system consistently performs as expected and produces accurate results under various conditions. It includes addressing challenges such as uncertainty quantification and robust generalization, which are critical for enhancing system dependability. For instance, in a legal case analysis system, reliability involves balancing uncertainty quantification (e.g., confidence scores for retrieved legal citations and the number of retrieved legal citations) and robust generalization (e.g., applying precedents to new cases) to ensure lawyers are not misled during case preparation.\nPrivacy focuses on safeguarding user data, ensuring control over personal information. Since RAG has been applied to sensitive domains like the medical field, protecting patient information is important. For example, when a healthcare assistant retrieves medical records or generates treatment suggestions, the system must prevent data breaches and ensure sensitive patient details embedded in the language model remain secure.\nExplainability emphasizes the need for transparent decision-making processes, enabling users to understand how outputs are generated. For example, a university admissions assistant powered by RAG should offer clear explanations of how student profiles are matched with program requirements, providing insights that users can readily understand and verify.\nFairness focuses on minimizing biases introduced during both retrieval and generation stages, as these biases can significantly affect outcomes in high-stakes domains. Recent advancements include the use of re-ranking methods to mitigate societal biases in retrieval and fine-tuning techniques to balance demographic fairness with system performance. For example, the admissions assistant must ensure fair treatment of applicants by addressing potential biases, such as those related to gender, race, or socioeconomic status.\nAccountability pertains to the governance of AI, including policymaking and law enactment, but also extends to technical aspects such as tracing the origins and processes behind AI-generated content. For example, ensuring that a news-generating system can trace its retrieved sources to improve content accountability and reduce misinformation is critical. Techniques like content watermarking help identify the provenance of retrieved information and the generation process, providing a clear audit trail for future verification.\nSafety addresses the system's capacity to prevent and mitigate harm, with a particular focus on defending against adversarial attacks and reducing risks from malicious actors. Current chatbot systems often interact with high-risk users, such as teenagers, who may unknowingly be exposed to harmful or inappropriate content. Adversarial attacks and jailbreaking attempts that alter the chatbot's behavior could lead to misinformation, inappropriate responses, or even dangerous suggestions. Thus, building robust safeguards, such as adversarial training and ethical guardrails, is crucial for ensuring safety and preventing harm in such interactions.\nDespite their recent success, concerns about the trustworthiness of RAG-based systems have become an increasing subject of debate. First, RAG systems are susceptible to reliability issues since developers must ensure the output is accurately grounded on the retrieved content [98, 55]. Second, the reliance on an external database introduces a new attack surface, exposing the systems to a range of adversarial threats [198, 45, 194, 214, 215, 227]. As a result, robustness improvements are needed"}, {"title": "2 Preliminaries", "content": "This section provides the preliminaries of the RAG framework for LLMs. We will introduce the concept of RAG and the common downstream tasks. We acknowledge the wide range of applications of RAG in domains other than LLMs (e.g., Image Generation), but this survey limits the scope to the applications of RAG in LLMs, sometimes referred to as Retrieval Augmented Language Models (RA-LLMs) [44]. As a simplification of the terminology, in the rest of this survey, we use RAG, RA-LLM, and RAG-based systems interchangeably."}, {"title": "2.1 Retrieval Augmented Generation", "content": "As illustrated in Figure ??, a typical RAG framework consists of three stages: information retrieval, knowledge augmentation, and content generation. Given a query, the retrieval process aims to provide relevant information and context to facilitate the reasoning of the query. Following the classification of previous work [55], the retrieval process contains two stages - indexing and retrieving. The indexing stage takes inputs from a diverse range of formats (PDFs, HTML, words, Markdowns, etc.) and converts them into chunks of data. Subsequently, the chunks of data are converted into vector representations and stored in a vector database for access during inference. However, it is worth noting that for some retrieval-augmented tasks, such as those involving knowledge graphs"}, {"title": "2.2 Tasks and Evaluations", "content": "The RAG paradigm has enabled multiple applications in the natural language processing (NLP) domains. The following briefly introduces some of the common tasks including question answering and chatbots, while also introducing some of the associated and commonly used datasets."}, {"title": "2.2.1 Tasks", "content": "One of the primary downstream tasks for RAG-based language models is question answering (QA), which includes various sub-tasks such as long-form question answering, multi-hop question answering, domain-specific question answering, and open-domain question answering. In these tasks, the system is given a user query and aims to generate the most relevant and accurate answer. The RAG paradigm assists these tasks by integrating the relevant external context into the generation.\nDifferent QA sub-tasks might require different evaluation paradigms. For example, for multi-hop and multiple choice question answering, since the answer is relatively structured or limited to a pre-defined set of answers, the performance is often evaluated through metrics such as hits@n, where n represents the rank of the correct answer in the list of retrieved candidates, and F1, where precision and recall are balanced to assess both the correctness of the generated answers and the system's ability to retrieve all relevant answers [114, 166]. On the other hand, for open domain question answering where answers are more unstructured, Exact Match (EM) and Lexical Match are commonly used [220]. These metrics both provide a good measure of quality of the generated answers. It is also worth mentioning that for some works that emphasize the retrieval process also report results that measures the quality of the retrieval. Commonly used QA datasets include the following:\n\u2022 MMLU [68]: A commonly used dataset for multiple-choice question answering (MCQA). It contains MCQA questions from 57 domains, including STEM (science, technology, engineering, and math), humanities, and medicine. In their experiments, existing uncertainty quantification research chooses to use a subset of the dataset for evaluation [207, 88].\n\u2022 TriviaQA [79]: A widely-used large-scale dataset for open-domain question answering, designed to test models on questions from Wikipedia and web search engines. TriviaQA includes question-answer pairs along with evidence documents for context, making it suitable for testing reading comprehension and retrieval-based models. It is used by most open-domain question answering models [163, 137, 98].\n\u2022 WebQSP [209]: A popular dataset for multi-hop knowledge base question answering, focusing on the task of answering questions by traversing multiple entities and relations within a knowledge graph. WebQSP provides questions labeled with their corresponding semantic parses, enabling models to learn complex query structures for effective knowledge graph traversal and reasoning [124, 114, 166]."}, {"title": "Chatbots.", "content": "Another common application for RAG-based language models is Chatbots. Chatbots are designed to handle an array of dialogue types, ranging from task-oriented interactions to open-domain conversations. The RAG paradigm can enhance Chatbots performance by integrating external knowledge into the conversation, allowing the system to access up-to-date information that would otherwise be out of its parameters trained from outdated text corpus [6, 165, 46]. Contemporary knowledge will be especially important for those Chatbots operating in dynamic or domain-specific environments, such as medical and financial dialogue systems.\nWhen evaluating the performance of a chatbot, two primary goals are considered. The first goal is to assess the quality and coherence of the dialogue. To achieve this, various metrics have been proposed, including those that measure utility [24], response understanding [211], and overall aesthetics [188]. Additionally, some metrics evaluate the similarity between the generated responses and human responses, as seen in works such as [3, 197]. While these metrics are generally effective for open-domain dialogues, they often fail to account for specific use cases. Thus, the second goal focuses on evaluating the chatbot's effectiveness in meeting users' needs, particularly in real-world, business-oriented use cases. In these scenarios, beyond the aforementioned dialogue quality metrics, practical effectiveness is a key consideration. General natural language generation (NLG) metrics, such as BLEU [127] and ROUGE [100], are frequently employed to measure this aspect.\nThere is currently no standardized holistic evaluation of chatbots. For large commercial models, the performance of the chatbots is evaluated on the subtasks such as code generation, problem solving, and complex reasoning. On the other hand, for the business-oriented, task specific models, they are evaluated on the corresponding task-specific or industry-specific datasets. We introduce some of the commonly used datasets below.\n\u2022 HellaSwag [217]: A commonly used dataset for commonsense reasoning and story completion tasks. HellaSwag presents models with scenarios requiring contextually appropriate completions, testing their ability to reason beyond surface-level semantics. It has been widely adopted for benchmarking commonsense reasoning capabilities in large language models [42, 23].\n\u2022 HumanEval [26]: A widely used dataset for evaluating code generation capabilities of language models. HumanEval includes programming problems of varying difficulty levels, along with unit tests to validate the correctness of generated code. It is a standard benchmark for assessing the coding performance of generative models [42].\n\u2022 MedicationQA [17]: A popular dataset for question answering in the medical domain, focusing on patient-generated questions about medication. It includes complex medical queries paired with evidence-based answers, making it a crucial benchmark for evaluating the applicability of language models in healthcare and patient communication [42, 92].\nBeyond language-based tasks, RAG-based language models can be applied to a diverse range of downstream tasks, including recommendation systems [97], software engineering [72], and Al for scientific discovery [5]. However, these applications are often overlooked in discussions of trustworthy RAG frameworks. Recognizing their importance, we highlight the need for further exploration of trustworthiness in these domains and propose to address them in future directions.\nTo comprehensively assess trustworthiness, additional metrics are necessary, including those that evaluate bias, fairness, and reliability in the generated answers. As AI trust evaluation has well been debated across cognitive, communication, information, and social dimensions, the focus was on the use of the result. To maintain trust, a trust worthy evaluation is also needed to determine the efficacy under changing conditions. These metrics will be designed to ensure that the system aligns well with trustworthy aspects. Due to their heterogeneous nature, we will delve into these specific metrics in detail in the corresponding sections throughout the rest of the survey."}, {"title": "2.3 Motivation", "content": "Although trustworthiness in deep learning and LLMs has been well-explored in the general AI community, it is rather critical to consider the trustworthiness for RAG-based LLMs because (a) their growing usage in real-world applications often involves high-stakes decision-making, where errors or biases can lead to significant consequences; (b) RAG models are vulnerable to trustworthiness"}, {"title": "2.3.1 Related Surveys and Differences", "content": "As shown in Table ??, recently, several surveys have been conducted on RAG [55, 44] and trustworthy LLMs [105, 74]. On one hand, although the RAG surveys provide a comprehensive overview of the state-of-the-art models and architectures, detailing some of the methods' efforts to address challenges in the realm of trustworthiness (e.g., robustness), few of them provide a comprehensive and focused discussion on the specific challenges and solutions related to trustworthiness across the entire RAG pipeline, particularly in areas such as reliability, fairness, privacy, and safety [55, 44]. On the other hand, the surveys on LLM trustworthiness focus mostly on the generation aspects of LLMs, such as mitigating hallucinations or enhancing explainability, and cannot be directly applied to RAG models due to the added complexity introduced by the retrieval and augmentation stages [105].\nMost recently, there is a survey dedicated to trustworthiness in RAG systems [230]. Although the survey provides reviews on existing works, most of its content focuses on experiments related to trustworthy generation across different language models within a basic retrieval-augmented generation setup. We acknowledge the importance of such empirical studies in advancing the field but would like to emphasize the different focus of our survey. Rather than empirical studies, our survey centers on providing a more detailed and comprehensive literature review. We systematically categorize trustworthiness challenges and solutions across the RAG applications. By focusing on these broader aspects, our work aims to establish a unifying framework to guide future research and development in trustworthy RAG systems.\nNonetheless, all of the existing surveys have acknowledged the importance of trustworthiness and included it as a critical area for future research directions [55, 44], demonstrating the aligned interests in the community."}, {"title": "2.4 Paper Collection", "content": "To construct a comprehensive survey of trustworthy RAG systems in LLMs, we follow a systematic literature review methodology. We began by identifying relevant papers through keyword searches across major academic databases, including Google Scholar, ACM Digital Library, and arXiv, using general terms such as \"Retrieval-Augmented Generation,\" \"trustworthiness,\" as well as specific terms for the different aspects. Papers were included if they directly discussed the trustworthiness aspects of RAG systems, such as reliability, privacy, safety, fairness, and accountability. Because of the modular nature of RAG-based systems where external databases are coupled with a language model, we also take works that discuss each into consideration. After an initial screening, we categorize the papers into each aspect of trustworthiness, situating them into corresponding taxonomies. Our final collection of papers, as discussed in this survey, reflects a diverse range of perspectives on RAG and trustworthiness. The cut off date for the papers to be included in this survey is October 2024."}, {"title": "2.5 Notes on the Organization of the Survey", "content": "As outlined in the previous sections, the remainder of this survey is structured around the six key aspects of trustworthiness: Reliability, Privacy, Safety, Fairness, Explainability, and Accountability. Given the distinct nature of each aspect, every section will follow its own taxonomy, introducing relevant works accordingly. To further emphasize these differences, we include specific future directions and evaluation protocols within each section. A general discussion of the future directions for trustworthy Retrieval-Augmented Generation (RAG) systems will follow at the conclusion, providing a broader discussion that encompasses and integrates the section-specific insights."}, {"title": "3 Reliability of Retrieval Augmented Generation", "content": "While RAG improves factual consistency and adaptability, it also introduces unique reliability challenges. Unlike standalone generative models, RAG reliability depends not only on the underlying LLM but also the alignment of retrieved information. Ensuring reliability in RAG therefore requires evaluating both the the retrieval process and the generation conditioned on the retrieved content."}, {"title": "3.1 Taxonomy of RAG Reliability", "content": "At a high level, reliability requires the system to perform as expected under various conditions. Just as other deep learning models that train on the big data, RAG models are susceptible to common pitfalls of reliability. Previous work [176] defines reliability from three granular aspects: the ability to express uncertainty in predictions, the capability of robust generalization under various conditions, and the extent to which the model can adapt to new tasks. However, since RAG is inherently adaptable because of the retrieved context, we will only consider uncertainty and robust generalization in our following discussion."}, {"title": "3.2 Uncertainty", "content": "Uncertainty is a crucial factor for model reliability. Uncertainty quantification (UQ) helps quantify the confidence in the model's predictions, which is essential in high-stakes scenarios. Consider a medical question answering chatbot where a patient inquires about their condition. If the model can express uncertainty in its responses, it significantly reduces the risk associated with its predictions. The patient can then make more informed judgments based on the confidence level of the information provided. Thus, due to the imperativeness of accurately conveying uncertainty, we need to ensure that robust uncertainty quantification methods are integrated into the system.\nFor Retrieval-Augmented Generation (RAG) systems, uncertainty quantification presents two primary challenges: First, during the generation phase, uncertainty stems from the inherent limitations of large language models (LLMs). Standard techniques for quantifying uncertainty in LLMs, such as conformal prediction, can be applied here with few adaptations [207]. Second, uncertainty arises during the retrieval phase and its interaction with the LLM, introducing a more complex dynamics. The combination of retrieval and generation processes creates unique challenges for UQ, necessitating advanced methods to address the overall system complexity. The following section outlines ongoing efforts to tackle these challenges, with a summary of the relevant literature presented in Table 2."}, {"title": "3.2.1 Uncertainty Quantification in Generation", "content": "The generation phase in Retrieval-Augmented Generation (RAG) systems is critically influenced by the UQ of LLMs. Recent studies have explored various approaches in this area, with a focus on techniques like conformal prediction (CP) a model-agnostic, distribution-free method that uses a calibration set to estimate prediction confidence [154]. To apply conformal prediction, a non-conformity score is first defined to measure the confidence of a given prediction. Using a calibration set, the $1 - \\alpha$ quantile of the non-conformity score is then calculated, where $\\alpha$ represents the user-defined error rate. Finally, the prediction set is constructed by selecting valid predictions based on the quantile score, ensuring that the set satisfies the $1 - \\alpha$ confidence level, assuming the calibration and test sets are exchangeable.\nThe cornerstone of CP lies in defining the non-conformity score. In traditional multi-class classification, a common approach is to use the softmax score of the from the class prediction. Extending the logit-based non-conformity score to LLMs, methods have been further developed. Typically, they assume white-box access to the model, making them unsuitable for commercial LLMs such as ChatGPT. For instance, Kumar et al. [88] applied standard CP to the Llama model [175] by leveraging softmax scores of token logits in multiple-choice tasks. Similarly, Ye et al. [207] extended logit-based approaches to multiple baselines and language models.\nTo compensate the lack of application on black models, another promising direction is proposed for sampling-based techniques, where model confidence is estimated by repeatedly prompting the LLM. Quach et al. [137] adapted the learn-then-test risk-control framework [11] for LLMs, approximating the non-conformity score through sampling, which allows uncertainty quantification in black-box models without direct logit access. Su et al. [163] further advanced these methods by introducing non-conformity measures that integrate both coarse-grained and fine-grained notions of uncertainty, leading to smaller, more refined prediction sets."}, {"title": "3.2.2 Uncertainty Quantification in Retrieval and Generation", "content": "As shown in Figure ??, a traditional RAG system includes multiple components from retrieval to generation. Due to its complex, multi-component nature, directly applying LLM-based UQ methods will produce less accurate, sub-optimal results [98, 146]. This necessitates the development of specialized techniques tailored to the unique structure and requirements of RAG models.\nRecently, researchers proposed a multi-step calibration framework to enhance the retrieval process of RAG [146]. Specifically, this framework uses conformal prediction to quantify retrieval uncertainty, ensuring trustworthiness in RAG systems. The framework involves constructing a calibration set of questions answerable from the knowledge base and comparing their embeddings against document embeddings to identify the most relevant chunks containing the answers. By analyzing similarity scores and determining a cutoff threshold based on a user-specified error rate ($\\alpha$), the system retrieves all chunks exceeding this threshold during inference. This multi-step calibration ensures the true answer is captured in the context with a (1 - $\\alpha$) confidence level.\nMoreover, TRAQ [98] expanded the conformal prediction framework to include a Bayesian optimization module that minimizes the prediction set during the multi-step calibration. Because of the complexity of RAG, the constructed prediction set will be very large after aggregating the error rates of multiple components. By leveraging Bayesian optimization, the framework efficiently searches for the optimal parameters that reduce the size of the prediction set while maintaining the desired confidence level. TRAQ ensures that the retrieval process remains both accurate and computationally feasible, enhancing the overall reliability and performance of RAG systems."}, {"title": "3.3 Uncertainty Evaluation", "content": "Traditionally, uncertainty quantification is evaluated from two key perspectives: coverage and efficiency [67]. Recall that the goal of uncertainty quantification is to ensure that the returned answer set satisfies a user-defined error tolerance of 1 $\\alpha$. Thus, the coverage rate measures how effectively the model meets this requirement.\nGiven a returned set of answers, $A_{ret}$, and the correct answer set, $A_{true}$, the coverage rate, $C$, is calculated as the proportion of instances where the correct answer is included in the returned set. Formally, it is defined as:\n$C = \\frac{N_{correct}}{N_{total}}$,\nwhere $N_{correct}$ represents the number of times the correct answer $A_{true}$ is contained in the returned set $A_{true} \\cap A_{ret}$, and $N_{total}$ is the total number of instances.\nFor the model to be considered reliable, $C$ should be at least 1 - $\\alpha$. However, simply exceeding this threshold does not necessarily indicate optimal performance. Overestimating the returned set size while still satisfying the desired error rate implies inefficiency, as a smaller set could suffice for the same error rate.\nAlongside coverage, efficiency, denoted as $E$, is another critical metric, often evaluated by the size of the returned answer set (i.e. the number of returned answers per question):\n$E = A_{ret}$.\nEfficiency reflects the utility of the model's output, as larger sets may contain more irrelevant information, reducing their usefulness to the user. Thus, an efficient uncertainty quantification process minimizes $E$ while maintaining the desired coverage rate, $C$."}, {"title": "3.4 Robust Generalization", "content": "Previous work [176] defines robustness as the ability to make accurate estimates or forecasts about unseen events caused by out-of-distribution data, covariate shift, domain change, concept change, or population shift, etc. In the context of RAG, the most significant challenge is the shift in the distribution of the database. Realistically, the database will always be evolving, introducing new knowledge into the system. Without dedicated robustness measures, this can cause the model to underperform in various situations. Consequently, it is essential to develop approaches that allow the model to continually learn from new data and adjust its retrieval and generation processes accordingly such as in concept drifts. Specifically, we will consider two aspects of robustness for RAG: resilience against irrelevant context and resilience against corrupted or misinformation contexts. It is worth mentioning that there is another type of context that we define as adversarially constructed corrupted context. Sometimes they are closely related to corrupted context, but due to their adversarial nature, we will consider them in Section 5 for Adversarial Robustness. This section will focus on the context that occurs organically over time."}, {"title": "3.4.1 Irrelevant Context", "content": "Fang et al. [45] considers the noise robustness of RAG with adaptive adversarial training. The paper explores three types of retrieval noises: (i) contexts that appear to be related to the query but do not contain the correct answer, (ii) contexts that are entirely unrelated to the query, and (iii) contexts that are thematically related to the query but include incorrect information. With the conclusion that type"}, {"title": "3.4.2 Corrupted Context", "content": "Recently, Xu et al. [198] proposed a theoretical framework to explore the benefits and detriments of the RAG, in the situation where there's a discrepancy between the retrieved knowledge and the LLM knowledge. Specifically, they observed that the similarity between the RAG representation and the retrieved representation is bounded by the benefits and detriments, and the similarity is positively correlated with the value of benefits minus detriments. These results suggest that the similarities can be used as a proxy for the benefits and detriments of the RAG. Building upon the theoretical results, the author further proposed an interactive inference framework X-RAG that leverages the benefit of both worlds of retrieved knowledge and LLM knowledge."}, {"title": "3.5 Robustness Evaluation", "content": "The evaluation of the model's robustness focuses on assessing its performance when noise is present in the data. Thus, the setup of the noisy data, which will be detailed in the dataset section, plays a key role in this evaluation. Exsting metrics outlined in 2.2.1 will be applied to assess the model's performance. It is important to note that there are different reporting styles for these metrics in the context of model robustness. Some authors present standard tables comparing the proposed model's performance against baselines [45, 198], while others report only the performance delta between the proposed fine-tuned model and the corresponding baseline for better visualization [214].\nCurrently, there is no widely-used benchmark for RAG robustness. To simulate real-world conditions and evaluate robustness, existing works create customized datasets that incorporate generated noise. Typically, a common QA benchmark (e.g., TriviaQA) is used, and during the retrieval process, noises are injected into the retrieved content. Depending on the problem setting (irrelevant context vs. corrupted context), the noise is either randomly selected or filtered using heuristic techniques [198, 214]. These datasets attempt to replicate the type of challenges encountered in realistic environments where the retrieved information may not perfectly align with the query.\nRecently, Fang et al. [45] proposed a benchmark for noise-robust RAG. For each QA instance, the proposed dataset includes three types of augmented retrieval noise: relevant retrieval noise, irrelevant retrieval noise, and counterfactual retrieval noise where the answer entity is intentionally incorrect, as well as the golden retrieval data. We recognize this as one of the first publicly available datasets for RAG robustness evaluation, and future works could benefit from using this for benchmarking."}, {"title": "3.6 Future Directions of RAG Reliability", "content": "Reliability remains an important challenge in the development of trustworthy RAG systems. While our current sections are structured independently for uncertainty and robustness, future research should aim for a more integrated approach that captures the intricate interactions between these aspects. These concepts are not necessarily exclusive; uncertainty quantification can help the model produce more trustworthy results when faced with less accurate or noisy contexts, while better robustness can reduce the model's overall uncertainty."}, {"title": "4 Privacy of Retrieval Augmented Generation", "content": "Although privacy risks in LLMs have been extensively studied, RAG systems introduce additional complexities by leveraging external data. This integration poses new challenges in maintaining privacy, ensuring data integrity, and managing the overall trustworthiness of the RAG system. In this section, we will introduce the threat model concerning privacy leaks in RAG systems. We will then discuss the current efforts to address these challenges. Towards the end, we will explore potential future directions for enhancing the trustworthiness and privacy of RAG systems."}, {"title": "4.1 Taxonomy of RAG Privacy", "content": "In Table 3, we outline the existing efforts in addressing the privacy issues present in the RAG systems. We will briefly introduce the relevant taxonomy in the following."}, {"title": "4.1.1 Training", "content": "Training refers to whether the attack or defense requires prior training on the data. Models that require training typically assume a distinct threat model compared to those that do not. When a model requires training, it often presumes white-box access to either the retriever or the language model, allowing attackers or defenders to fine-tune or adjust components of the RAG system to exploit or mitigate vulnerabilities. On the other hand, models that do not require training typically rely on prompt-based methods or zero-shot techniques. This distinction has significant implications for the feasibility of privacy attacks and defenses in RAG systems."}, {"title": "4.1.2 Tasks", "content": "Admittedly, research on RAG privacy is still in its infancy. Current literature focuses on three main tasks: Document Extraction, Training Data Extraction, and Membership Inference Attack. Document Extraction seeks to extract confidential information from the retrieval database, such as Personally Identifiable Information (PII). Membership Inference Attack aims to determine whether specific passages are present in the retrieval database. While not a direct privacy attack, it introduces risks by exposing sensitive associations between queries and database contents, potentially enabling adversaries to infer private information. Lastly, Training Data Extraction examines the leakage of LLM training data in the context of retrieval-augmented generation, highlighting vulnerabilities that could lead to the unauthorized exposure of proprietary or sensitive datasets."}, {"title": "4.1.3 Leakage", "content": "We consider two sources of leakages. First, the leakage of the external retrieval database involves leaking targeted/untargeted information from external knowledge sources, such as sensitive data in proprietary databases or publicly available but privacy-relevant information inadvertently retrieved during query processing. Second, the leakage of the internal training data focuses on the exposure of the LLM training data in the context of retrieval-augmented generation. This occurs when the language model unintentionally reproduces sensitive or proprietary information from its training dataset during response generation, raising concerns about policy violations and privacy breaches. We organize the rest of the section from the above two aspects."}, {"title": "4.2 Data Leakage From the External Retrieval Database", "content": "The goal of the attacker is to exploit privacy vulnerabilities within the retrieval dataset, targeting two main objectives: (1) eliciting specific information from the retrieval system with high accuracy, and (2) outputting the retrieved private data. Zeng et al.[158] introduced a composite structured prompt, formulated as q = information + command, which leverages the context retriever's propensity for similarity-based matching. However, a significant limitation of this approach is its reliance on fixed queries, which cannot dynamically adapt to varying contexts. To address this limitation, Jiang et al.[77] proposed a learning-based method. Their framework begins with an initial adversarial query and iteratively refines it based on the model's responses, progressively generating queries to extract as many documents as possible from the retrieval database.\nWhen considering white-box access to the model, Peng et al. [129] focused on data extraction through backdoor attacks. Their method trains a model to associate specific triggers with desired outputs. Beyond directly extracting documents, their approach also explores generating stealthy outputs by employing a language model to paraphrase the retrieved content, thereby increasing the difficulty of detecting the attack. Furthermore, Cohen et al. [31] demonstrated that these attacks can escalate beyond isolated cases. By crafting an adversarial self-replicating prompt, attackers can initiate a chain reaction that propagates through the entire Retrieval-Augmented Generation (RAG) system.\nAlthough distinct from direct extraction methods and based on a different threat model, membership inference attacks have also proven effective for data extraction. These attacks allow malicious users to infer whether specific content is present in the retrieval database. Liu et al. [103] introduced a mask-based attack that obscures portions of documents, compelling the language model to predict the masked words. This technique not only reveals sensitive information but also highlights vulnerabilities in the retrieval system's training data."}, {"title": "4.3 Data Leakage From the LLM Training Data", "content": "The goal of the attacker is to extract data from the LLM's training and fine-tuning data that are encoded in the model parameters. In their paper, Zeng et al. [158] compared the effect of RAG in preventing data leakage from the LLM training data. The result shows that incorporating retrieved passages greatly reduces LLM's propensity to reproduce content memorized during its training/fine-tuning process. To isolate the effect of retrieval data integration, the author also attached 50 tokens of random noise injection as prefix. Although the random noise could also mitigate the data leakage, it is far less effective than integrating the retrieved content."}, {"title": "4.4 Defense on Privacy Attacks", "content": "Although still a relatively under-explored area, some works have proposed defenses to mitigate privacy vulnerabilities in RAG systems. In their foundational work, Zeng et al. [158] observed that using a separate model to summarize the retrieved documents effectively reduces privacy leakage by abstracting sensitive information into generalized content. Additionally, they proposed implementing a distance threshold in the retrieval database, ensuring that only documents with certain relevance requirements are returned. However, this approach introduces a trade-off between system performance and privacy protection, as stricter thresholds can limit retrieval accuracy.\nBuilding on these mitigation strategies, the authors further suggested the use of purely synthetic data as a way to entirely avoid potential leakage of real data [218]. This method involves identifying importing attributes of the data through few-shot samples, extracting key information associated with these attributes, and generating synthetic data that mirrors the original data without exposing sensitive information. This approach has shown promise in effectively mitigating privacy leakage while maintaining the performance of the RAG system."}, {"title": "4"}]}