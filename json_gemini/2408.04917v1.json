{"title": "Avoid Wasted Annotation Costs in Open-set Active Learning with Pre-trained Vision-Language Model", "authors": ["Jaehyuk Heo", "Pilsung Kang"], "abstract": "Active learning (AL) aims to enhance model performance by selectively collecting highly informative data, thereby minimizing annotation costs. However, in practical scenarios, unlabeled data may contain out-of-distribution (OOD) samples, leading to wasted annotation costs if data is incorrectly selected. Recent research has explored methods to apply AL to open-set data, but these methods often require or incur unavoidable cost losses to minimize them. To address these challenges, we propose a novel selection strategy, CLIPN for AL (CLIPNAL), which minimizes cost losses without requiring OOD samples. CLIPNAL sequentially evaluates the purity and informativeness of data. First, it utilizes a pre-trained vision-language model to detect and exclude OOD data by leveraging linguistic and visual information of in-distribution (ID) data without additional training. Second, it selects highly informative data from the remaining ID data, and then the selected samples are annotated by human experts. Experimental results on datasets with various open-set conditions demonstrate that CLIPNAL achieves the lowest cost loss and highest performance across all scenarios. Code is available at https://github.com/DSBA-Lab/OpenAL.", "sections": [{"title": "1. Introduction", "content": "Deep learning has succeeded in various tasks such as classification and regression [7]. However, this success relies heavily on well-refined large datasets and the associated annotation costs. In practice, spending significant resources on annotating a large volume of data is difficult. To address this issue, research on active learning (AL) has been continuously studied, which aims to enhance model performance by selectively annotating informative data from unlabeled data [23].\nAL uses a query strategy to decide whether to annotate an unlabeled sample for model training. Traditional AL research, termed standard AL [15, 18, 20, 21], assumes that the unlabeled data is collected from the same domain as the training data called in-distribution data (ID). As shown in Fig. 2 (a), the query strategy of standard AL selects an unlabeled sample based on its informativeness, which refers to the degree to which the sample can help improve model performance. However, it is difficult to assume that unlabeled data only contains ID samples. For instance, if the classification task involves distinguishing between dogs and cats, it is assumed that the unlabeled data only contains images of dogs and cats. However, diverse data collected online might include other animals like guinea pigs or zebras (Fig. 1). If the unlabeled data includes OOD samples and only informativeness is considered, the selected data might include OOD samples, which cannot be used for training, leading to wasted annotation costs\nRecent research has focused on open-set AL, which assumes that unlabeled data might contain out-of-distribution (OOD) data as well [4, 10, 11, 13]. Open-set AL considers not only the informativeness but also the purity, indicating whether the data is relevant to the domain of the training data. As illustrated in Fig. 2 (b), data selection should be based on both purity and informativeness to avoid selecting irrelevant OOD data in open-set AL assumption.\nRecent methods proposed for open-set AL consider both purity and informativeness. These methods can be categorized into those that perform contrastive learning on unlabeled data [4,11] and those that train an OOD detector using collected OOD data [10,13]. However, both approaches face cost inefficiency issues. Contrastive learning methods often"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Standard Active Learning", "content": "Standard AL aims to reduce annotation costs by selecting only the most informative data from a large pool of unlabeled data to enhance model performance. The query strategies proposed to select data are primarily based on the informativeness of the data.\nThe criteria for determining informativeness in query strategies can be broadly divided into two classes. The first is uncertainty-based query strategy [18, 21]. In a classification task, the uncertainty-based strategy assesses the degree of confidence in classifying unlabeled data based on the decision boundary of the trained model. If the model's certainty about the data is low, it implies that the model finds it difficult to make an accurate judgment. Therefore, data with high uncertainty is selected for model training. The second is diversity-based query strategy [15, 20]. Despite the large data size, much of it contains redundant information. Hence, a small subset of data selected based on diversity can provide information similar to that of a larger dataset. Therefore, strategies that consider diversity select data that differ from the current data based on a comparison with labeled data.\nHowever, if the unlabeled data contains OOD data, both criteria can lead to selecting data that cannot be used for training, causing cost losses. It is difficult to determine ID data based solely on high uncertainty. This is because OOD samples, which were not seen during model training, also have high uncertainty. OOD samples are selected based solely on the diversity of representation because the representation of OOD samples is located in a different space compared to ID samples."}, {"title": "2.2. Open-set Active Learning", "content": "Unlike the assumptions in standard AL, open-set AL considers the presence of OOD data in the unlabeled pool. Various methods have been proposed for this purpose. Firstly, CCAL [4] and MQNet [11] perform contrastive learning on unlabeled data to determine data purity using the trained model. CCAL uses SimCLR [1] and CSI [17] to reflect purity and informativeness, respectively, and proposes a joint query strategy that combines these scores for selecting data. However, contrastive learning alone focuses on purity, making it difficult to select highly informative data. To resolve this trade-off, MQNet adjusts the purity obtained from CSI and the informativeness from standard AL using meta-learning, allowing data collection to prioritize purity or informativeness as needed.\nSecondly, research has proposed training OOD detection models to assess data purity. LfOSA [10] trains a classifier to distinguish \\(K + 1\\) classes, including \\(K\\) ID classes and one additional OOD class, using labeled ID and OOD data. The classifier predicts the class for each sample of unlabeled data, and based on the maximum activation value of the predicted class, a Gaussian Mixture Model (GMM) is trained with \\(K\\) components. Each GMM assumes two Gaussian distributions. The distribution with the higher mean is assumed to represent the ID data distribution. Therefore, the higher the probability of a sample belonging to the Gaussian distribution with the higher mean, the higher its purity. Conversely, a lower probability indicates that the sample does not belong to any ID classes, meaning it is OOD class. However, LfOSA does not consider informativeness, limiting model performance. EOAL [13] addresses this issue by proposing a method to select data using a combined score based on two entropy measures representing purity and informativeness. Firstly, to assess informativeness, \\(K\\) binary classifiers are trained, and the entropy scores for the unlabeled data are calculated using these \\(K\\) binary classifiers corresponding to the ID classes. Secondly, to assess purity, a classifier is trained to classify \\(K + 1\\) classes, and the representations from this trained classifier for the labeled OOD data are used to perform clustering. The probability of belonging to each cluster is calculated based on the distances between clusters, and the entropy for data purity is computed for the unlabeled data. Combining these two scores, EOAL selects data with high purity and informativeness.\nWhile previous open-set AL methods show improved performance over standard AL by considering OOD data, they still incur annotation cost losses. MQNet, for instance, tends to select a large amount of OOD data because it prioritizes informativeness over purity when selecting high-informativeness data. Methods like LfOSA and EOAL require additional OOD data for OOD detector training, inherently leading to cost losses. Furthermore, both approaches necessitate extra training costs for collecting OOD data. However, our proposed method sequentially assesses purity and informativeness, avoiding the trade-off between these criteria. We use the pre-trained CLIP model for OOD detection, requiring no additional training (Fig. 4)."}, {"title": "3. Proposed Method", "content": "The proposed CLIPNAL comprises two sequential processes: (1) measuring data purity based on a pre-trained CLIP and (2) computing informativeness of query strategies used in standard AL. In this section, we describe how our CLIP-based method assesses the purity of unlabeled data. As illustrated in Fig. 5, we use ID class names as linguistic information to calculate the ID probability of unlabeled images. Furthermore, we integrate visual information from labeled ID data to apply weighting to the ID probability. CLIPNAL process and an example of assessing purity can be found in Appendix. A."}, {"title": "3.1. Problem Statement", "content": "The open-set AL problem defined in this study aims to classify images into \\(K\\) classes, with the goal of improving the performance of an image classifier \\(f_{clf}\\) based on labeled data. We define the unlabeled data targeted for collection as open-set data \\(D_u = \\{x_i \\in \\mathcal{X}_{ID} \\cup \\mathcal{X}_{OOD}\\}_{i=1}^{N_U}\\), assuming the presence of both ID and OOD data. In AL, a fixed budget \\(C\\) is used for annotation costs in each round of data collection, repeated \\(R\\) times. If annotating a single image costs one unit, setting \\(C\\) to 10 means 10 images can be annotated per round. The selected queries from \\(D_u\\) are denoted as \\(Q\\). These queries are annotated by human experts, adding ID data \\(x_{ID}\\) to \\(S_{ID}\\) and OOD data to \\(S_{OOD}\\). The labeled ID data \\(S_{ID}\\) is used to train the classifier \\(f_{clf}\\). Thus, open-set AL aims to maximize model performance by minimizing the proportion of OOD data and selecting highly informative data with minimal cost."}, {"title": "3.2. CLIPN", "content": "Recently, large-scale datasets like MS COCO [8] and LAION-2B [14] have enabled CLIP [12] to be used not only for ID classification but also for OOD detection [5, 9, 19]. CLIPN [19], the latest method, enhances CLIP by adding a text encoder for the concept of \"no\". Unlike previous CLIP-based zero-shot OOD detection methods [5,9], which only consider proximity to class-specific text, CLIPN incorporates the notion of dissimilarity to achieve high performance in OOD detection. Therefore, our proposed method uses CLIPN to detect OOD data in the unlabeled pool.\nCLIPN assesses whether an image is OOD by considering the similarity between the image and class-specific textual information and the probability that the image does not belong to the class. First, the visual representation of an image \\(x_{i}^{img}\\) is obtained by inputting it into the image encoder. Next, pre-defined textual templates with class names are input into the text encoder for each target class to calculate the average textual representation. Similarly, the \"no\" text encoder in CLIPN calculates the average representation for each class.\n\n\n\\begin{equation}\nz_{j}^{text} = \\frac{1}{T} \\sum_{t=1}^{T} f_{text}(t_{prompt}(\\text{class name}_{j}))\n\\end{equation}\n\n\\begin{equation}\nz_{j}^{no \\; text} = \\frac{1}{T} \\sum_{t=1}^{T} f_{no \\; text}(t_{prompt}(\\text{class name}_{j})).\n\\end{equation}\nNext, the probability that \\(x^{img}\\) belongs to each class is calculated by computing the cosine similarity between the visual and textual representations and applying softmax.\n\n\n\\begin{equation}\nP_{ij}^{yes} = \\frac{\\exp (z_{i}^{img} \\cdot z_{j}^{text} / \\tau)}{\\sum_{k}^{K} \\exp (z_{i}^{img} \\cdot z_{k}^{text} / \\tau)},\n\\end{equation}"}, {"title": "3.3. Visual Similarity Weighting", "content": "Although CLIPN shows high performance in detecting OOD data using textual information, it encounters challenges when the number of classes increases or when the OOD data domain is similar to that of ID data. We use visual information from ID data as additional weights to address these challenges. First, the average representation \\(\\phi_j\\) for each class is calculated by applying the image encoder \\(f_{img}\\) to the labeled ID data \\(x_{i}^{img, ID}\\).\n\n\n\\begin{equation}\n\\phi_{j} = \\frac{1}{N_{ID}} \\sum_{i=1}^{N_{ID}} f_{img} (x_{i}^{img, ID}).\n\\end{equation}\nNext, the visual similarity weight \\(s_{ij}\\) for each class is calculated by computing the cosine similarity between the visual representation \\(z_{i}^{img}\\) and the average representation \\(\\phi_{j}\\), and converting it to probabilities using the softmax function.\n\n\n\\begin{equation}\ns_{ij} = \\frac{\\exp (z_{i}^{img} \\cdot \\phi_{j}/\\tau)}{\\sum_{k}^{K} \\exp (z_{i}^{img} \\cdot \\phi_{k}/\\tau)}\n\\end{equation}\nThe visual similarity weight \\(s_{ij}\\) is used to adjust the probabilities of ID data. The adjusted probabilities \\(p_{ij}^{*, ID}\\) and \\(p_{i}^{*, OOD}\\) are calculated by applying the weights as follows.\n\n\n\\begin{equation}\n\\begin{aligned}\np_{ij}^{*, ID} &= s_{ij} \\cdot p_{ij}^{ID}\\\\\np_{i}^{*, OOD} &= p_{i}^{OOD} / K \\\\\nI^{*}(x_i) &= \\begin{cases}\n1, & \\text{if } p_{i}^{*, OOD} > \\max_j p_{ij}^{*, ID}, \\\\\n0, & \\text{else}.\n\\end{cases}\n\\end{aligned}\n\\end{equation}"}, {"title": "3.4. Self-Temperature Tuning", "content": "The temperature parameter significantly influences model confidence in predictions. The optimal temperature varies depending on the domain of the data used for OOD detection. In open-set AL, labeled ID and OOD data allow for exploring the optimal temperature parameter.\n\n\\begin{equation}\n\\begin{aligned}\n\\hat{y}_{i}(\\tau) &= I^{*}(x_i, \\tau),\\\\\nAcc(\\tau) &= \\frac{1}{N_{ID}} \\sum_{i=1}^{N_{ID}} I(\\hat{y}_{i}^{OOD} = y_{i}^{OOD}),\\\\\n\\tau^{*} &= \\arg \\max_{0<\\tau} Acc(\\tau).\n\\end{aligned}\n\\end{equation}\nAccording to Eq. (8), we apply a self-temperature tuning to explore the optimal temperature parameter by comparing the accuracy of the predicted results \\(\\hat{y}^{OOD}(\\tau)\\) against the ground truth \\(y^{OOD}\\) for ID and OOD data."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Baselines. To demonstrate the efficiency of CLIPNAL, we select query strategies from standard AL and open-set AL as comparison methods. For standard AL, the following strategies are selected: (1) uncertainty-based strategy, including CONF [18], LL [21], and (2) diversity-based strategy, including CORESET [15]. For open-set AL, the chosen strategies are (1) contrastive learning-based strategy MQNet [11], (2) OOD detector-based strategy, including LfOSA [10] and EOAL [13]. To evaluate the effectiveness of query strategies, we include RANDOM, which selects data randomly without specific criteria, in the experiments. The detail of the baselines is described in Appendix. D.\nOpen-set Data. To construct various open-set scenarios, we consider mismatch ratios, which vary the proportion of classes in the data, and OOD ratios, which vary the proportion of OOD data in the unlabeled pool [4, 10]. The mismatch ratios are defined as 20%, 30%, and 40%. The OOD ratios are defined as 10%, 20%, 40%, and 60%. For instance, in CIFAR10 with a mismatch ratio of 20%, two classes are ID, and the remaining eight are OOD. The composition of the open-set data used in the experiments is detailed in Appendix. B. We evaluate the robustness and effectiveness of the query strategies under various scenarios. The annotation cost C for AL is 500 for both datasets. The initial data for training the classifier is randomly selected from the unlabeled pool, equivalent to the defined cost. In subsequent iterations, data is selected based on the query strategy with a cost of C.\nImplementation Details. The image classifier is ResNet18 [6] in the experiments. The number of rounds R for AL experiments is set to 10, with results reported for each round. Each query strategy experiment is repeated five times, with average results reported. The hyper-parameters and pre-trained models are described in Appendix. C.\nEvaluation Metrics. Precision, the proportion of selected ID queries, is used to evaluate annotation cost loss in open-set AL [10]. Higher precision indicates a higher proportion of ID data in the selected queries.\n\n\n\\begin{equation}\nPrecision(\\text{%}) = \\frac{\\sum_{i=1}^{C} I(x_i = ID)}{C} \\times 100.\n\\end{equation}"}, {"title": "4.2. Open-set Robustness", "content": "Comparison of Annotation Cost Loss. We examine annotation cost loss through precision changes during data collection in Fig. 6. Each round's precision indicates the proportion of ID data among the selected data at the round. RANDOM, shown in black, selects data randomly, so the proportion of OOD data in the selected queries matches the OOD ratio in the unlabeled pool. A higher precision than RANDOM indicates a higher selection of ID data than the OOD ratio.\nStandard AL strategies, except CORESET, show lower precision than RANDOM in all cases, particularly as the OOD ratio increases. This indicates significant cost loss when OOD data is in the unlabeled pool. In contrast, most open-set AL strategies show higher precision than RANDOM. However, MQNet, LfOSA, and EOAL do not show consistent precision across rounds. MQNet initially focuses on purity, selecting more ID data, but later prioritizes infor-"}, {"title": "4.3. Ablation Studies", "content": "Effect of Visual Similarity Weights. To improve the accuracy of OOD detection, CLIPNAL uses visual similarity weights from labeled ID data. Fig. 10 shows (a) accuracy and (b) precision improvements with and without visual similarity weights. (a) Accuracy improvement is significant with increasing mismatch ratios and OOD ratios, except for a 20% OOD ratio in CIFAR10. (b) Precision results indicate that visual similarity weights significantly increase the amount of ID data selected, especially in CIFAR100, demonstrating the effectiveness of visual similarity weights in preventing cost loss.\nNecessity of Self-Temperature Tuning. Fig. 11 shows the accuracy of purity assessment with varying temperature parameters \\(\\tau\\). The accuracy of purity assessment changes significantly with small variations in \\(\\tau\\), indicating the im-"}, {"title": "4.4. Visualization of Query Strategy Results", "content": "Fig. 12 visualizes the ID data and informativeness selected by each query strategy. In the case of RANDOM, the selected data shows no specific pattern and is chosen randomly. In contrast, CORESET selects data with diversity, resulting in less redundancy and more varied data than RANDOM. LL and CONF select data based on uncertainty and choose data near the decision boundaries between classes. However, the query strategies in standard AL result in fewer usable training data due to the selection of OOD data. On the other hand, open-set AL methods select more usable training data than standard AL. However, LfOSA and EOAL, focusing on purity rather than decision boundaries, tend to select less informative data. MQNet selects relatively more ID data than LL, but only in certain decision boundary areas. Conversely, CLIPNAL selects data primarily from high informativeness areas near decision boundaries, similar to CONF, and achieves the highest selection of ID data among the query strategies."}, {"title": "5. Conclusion", "content": "This study introduces CLIPNAL, a two-stage query strategy designed to minimize cost losses in open-set active learning by improving the accuracy of purity assessments and selecting highly informative data from ID data only. CLIPNAL utilizes a pre-trained CLIPN model without additional training to detect OOD data based on linguistic information and visual similarity weights derived from collected ID data. To maximize the accuracy of the pre-trained CLIPN's ID data assessment, CLIPNAL optimizes the temperature parameter through self-temperature tuning based on the labeled ID and OOD data. By accurately selecting ID data from unlabeled data, CLIPNAL can apply a standard AL query strategy to select only highly informative data, reducing cost losses and approximating the conditions assumed in standard AL.\nTo validate the efficiency of the proposed method, various open-set datasets with different mismatch ratios and OOD ratios were constructed using CIFAR10 and CIFAR100. Experimental results demonstrate that CLIPNAL consistently achieves the lowest cost losses and highest accuracy across all scenarios. As CLIPNAL conducts purity assessment and data informativeness evaluation sequentially, it can incorporate various query strategies proposed in standard AL after assessing purity. This flexibility indicates that future research can extend the application of CLIPNAL to not only classification but also regression, object detection, and object segmentation in open-set scenarios."}, {"title": "A. CLIPNAL Process", "content": "Algorithm 1 summarizes the entire CLIPNAL process. AL involves data collection and model training. Initially, since no labeled data exists, data is randomly selected from the unlabeled pool for model training. Given that the unlabeled data contains OOD data, the proportion of OOD data in the selected data reflects the overall distribution. Only ID data is used for model training, excluding OOD data. Once initial training is complete, data is selected using the proposed CLIPNAL method. CLIPNAL assesses the purity of data from the unlabeled pool. First, it explores the optimal temperature parameter using labeled ID and OOD data, then applies CLIPN's purity assessment process and visual weighting to calculate the purity of the unlabeled data. Based on the calculated purity, OOD data is excluded, and the remaining data is assessed for informativeness using a standard AL query strategy. The selected data is annotated according to predefined costs and added to the labeled data. After data collection, the classifier is trained with the newly labeled data, and the model is reinitialized and retrained upon further data collection. Once the user-defined number of rounds is complete, the final classifier trained with the last labeled data is used as the final model."}, {"title": "B. Open-set Data Description", "content": "We utilize CIFAR10 and CIFAR100 for our image classification tasks. CIFAR10 and CIFAR100 consist of 10 and 100 classes, each comprising 50,000 training and 10,000 test images. We reconstruct the datasets by employing mismatch and OOD ratios for open-set data. The mismatch ratio represents the proportion of ID classes in the dataset [10, 13], while the OOD ratio denotes the proportion of OOD data in the unlabeled pool [4, 11]. Previous experiments by LfOSA and EOAL focused on open-set data constructed with the mismatch ratio but did not consider the OOD ratio. We incorporate both ratios to create more realistic scenarios when constructing open-set data."}, {"title": "C. Implementation Details", "content": "The image classifier used in the experiments is ResNet18 [6]. The model is trained for 20,000 steps with a batch size of 64 for all datasets. The learning rate was set to 0.1, weight decay to 0.0005, and momentum to 0.9, with the model parameters updated using Stochastic Gradient Descent (SGD). The temperature parameter for purity assessment in CLIPNAL is tuned within the range of 0 to 1 based on the labeled data.\nThe image encoder and text encoder for CLIPN were pre-trained on LAION-2B [2], and the \"no\" text encoder was pre-trained on CC3M [16]. The image encoder uses a ViT-Base model [3] with a patch size of 16 \u00d7 16. For data informativeness, CLIPNAL uses CONF [18], one of the query strategies proposed in standard AL. Hyper-parameters for LL, MQNet, LfOSA, and EOAL are set as per the original studies [10, 11, 21]. Experiments are conducted using Python 3.8.10 and PyTorch 1.14 on an Intel Xeon CPU E5-2698 v4 with a Tesla V100 GPU."}, {"title": "D. Baselines Description", "content": "We select query strategies for standard AL based on uncertainty and diversity. Uncertainty-based query strategies include CONF [18], which considers the lowest class prediction confidence of the model for a given data point as uncertainty. If the minimum prediction confidence is high, it implies that the model lacks certainty, thus selecting data with high uncertainty for training. LL [21] adds a Loss Prediction Module to the trained model to predict the loss during training, selecting data points with higher predicted losses as those with higher uncertainty. Diversity-based query strategy: CORESET [15] selects data that encapsulates diverse information to cover the entire dataset, thereby selecting less redundant data containing new information.\nWe chose query strategies for open-set AL using contrastive learning and OOD detector. Contrastive learning-based query strategy: MQNet [11] uses CSI for OOD detection. It calculates purity using CSI and informativeness using LL, combining the two through meta-learning to define data scores. OOD detector-based query strategy: LfOSA [10] trains a classifier on labeled ID and OOD data to classify K+1 classes, then uses a Gaussian Mixture Model to determine data purity. EOAL [13] uses two entropy scores representing purity and informativeness to select data based on these combined scores."}, {"title": "E. Overall Performance on Open-set Data", "content": "AL results are assessed using Area Under Budget Curve (AUBC) [22]. AUBC measures the efficiency of AL by calculating the area under the curve of accuracy improvement over rounds. A larger area indicates greater accuracy improvement after data collection.\nTable 2 summarizes the AUBC results for all open-set scenarios. MQNet shows high AUBC in CIFAR10 with high OOD and mismatch ratios but does not outperform standard AL in all cases. In CIFAR100, RANDOM often outperforms other strategies. However, CLIPNAL consistently shows higher AUBC than existing methods, indicating significant accuracy improvement from the initial round."}]}