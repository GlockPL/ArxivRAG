{"title": "Avoid Wasted Annotation Costs in Open-set Active Learning with Pre-trained Vision-Language Model", "authors": ["Jaehyuk Heo", "Pilsung Kang"], "abstract": "Active learning (AL) aims to enhance model performance by selectively collecting highly informative data, thereby minimizing annotation costs. However, in practical scenarios, unlabeled data may contain out-of-distribution (OOD) samples, leading to wasted annotation costs if data is incorrectly selected. Recent research has explored methods to apply AL to open-set data, but these methods often require or incur unavoidable cost losses to minimize them. To address these challenges, we propose a novel selection strategy, CLIPN for AL (CLIPNAL), which minimizes cost losses without requiring OOD samples. CLIPNAL sequentially evaluates the purity and informativeness of data. First, it utilizes a pre-trained vision-language model to detect and exclude OOD data by leveraging linguistic and visual information of in-distribution (ID) data without additional training. Second, it selects highly informative data from the remaining ID data, and then the selected samples are annotated by human experts. Experimental results on datasets with various open-set conditions demonstrate that CLIPNAL achieves the lowest cost loss and highest performance across all scenarios. Code is available at https://github.com/DSBA-Lab/OpenAL.", "sections": [{"title": "1. Introduction", "content": "Deep learning has succeeded in various tasks such as classification and regression [7]. However, this success relies heavily on well-refined large datasets and the associated annotation costs. In practice, spending significant resources on annotating a large volume of data is difficult. To address this issue, research on active learning (AL) has been continuously studied, which aims to enhance model performance by selectively annotating informative data from unlabeled data [23].\nAL uses a query strategy to decide whether to annotate an unlabeled sample for model training. Traditional AL research, termed standard AL [15, 18, 20, 21], assumes that the unlabeled data is collected from the same domain as the training data called in-distribution data (ID). As shown in Fig. 2 (a), the query strategy of standard AL selects an unlabeled sample based on its informativeness, which refers to the degree to which the sample can help improve model performance. However, it is difficult to assume that unlabeled data only contains ID samples. For instance, if the classification task involves distinguishing between dogs and cats, it is assumed that the unlabeled data only contains images of dogs and cats. However, diverse data collected online might include other animals like guinea pigs or zebras (Fig. 1). If the unlabeled data includes OOD samples and only informativeness is considered, the selected data might include OOD samples, which cannot be used for training, leading to wasted annotation costs\nRecent research has focused on open-set AL, which assumes that unlabeled data might contain out-of-distribution (OOD) data as well [4, 10, 11, 13]. Open-set AL considers not only the informativeness but also the purity, indicating whether the data is relevant to the domain of the training data. As illustrated in Fig. 2 (b), data selection should be based on both purity and informativeness to avoid selecting irrelevant OOD data in open-set AL assumption.\nRecent methods proposed for open-set AL consider both purity and informativeness. These methods can be categorized into those that perform contrastive learning on unlabeled data [4, 11] and those that train an OOD detector using collected OOD data [10, 13]. However, both approaches face cost inefficiency issues. Contrastive learning methods often fail to satisfy both purity and informativeness in all cases, leading to annotation cost losses. OOD detector methods require collecting OOD data to ensure high performance, inherently causing cost losses. Thus, while these methods perform better than standard AL, they still result in significant annotation cost losses. Fig. 3 shows the annotation costs wasted when using different active learning strategies.\nRecent studies have explored using CLIP for zero-shot OOD detection [5, 9, 19]. To address these cost-inefficiency issues, we suggest a two-step query strategy using CLIP named CLIPN for Active Learning (CLIPNAL) for open-set data, as shown in Fig. 2 (Ours). In the first step, we use a pre-trained CLIP [12] to perform zero-shot OOD detection to evaluate unlabeled data's purity. We utilize a pre-trained CLIPN [19] with an additional text encoder to incorporate the concept of \u201cno\u201d to evaluate the purity of unlabeled data. However, as textual information alone cannot fully represent all image classes, we enhance purity performance using visual similarity weights based on labeled ID data. Secondly, informativeness is assessed using the query strategy proposed in the Standard AL, which is only for the predicted ID data. Detecting OOD data with high accuracy makes it possible to convert open-set data into closed-set data. In other words, it leads to the same assumptions as Standard AL and helps solve the trade-off between purity and informativeness, a current issue in open-set AL.\nThe advantages of the proposed methodology can be summarized as follows: (1) It prevents annotation cost losses through high-performance OOD detection using pre-trained CLIPN and visual similarity weights reflecting ID data. (2) It does not require additional training for OOD detection because it uses a pre-trained model. (3) The OOD detection performance is not dependent on training, thus maintaining model robustness regardless of the proportion of OOD data in the unlabeled pool. Experiments assuming various open-set conditions demonstrate that our method outperforms both standard and existing open-set AL methods."}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Standard Active Learning", "content": "Standard AL aims to reduce annotation costs by selecting only the most informative data from a large pool of unlabeled data to enhance model performance. The query strategies proposed to select data are primarily based on the informativeness of the data.\nThe criteria for determining informativeness in query strategies can be broadly divided into two classes. The first is uncertainty-based query strategy [18, 21]. In a classification task, the uncertainty-based strategy assesses the degree of confidence in classifying unlabeled data based on the decision boundary of the trained model. If the model's certainty about the data is low, it implies that the model finds it difficult to make an accurate judgment. Therefore, data with high uncertainty is selected for model training. The second is diversity-based query strategy [15, 20]. Despite the large data size, much of it contains redundant information. Hence, a small subset of data selected based on diversity can provide information similar to that of a larger dataset. Therefore, strategies that consider diversity select data that differ from the current data based on a comparison with labeled data.\nHowever, if the unlabeled data contains OOD data, both criteria can lead to selecting data that cannot be used for training, causing cost losses. It is difficult to determine ID data based solely on high uncertainty. This is because OOD samples, which were not seen during model training, also have high uncertainty. OOD samples are selected based solely on the diversity of representation because the representation of OOD samples is located in a different space compared to ID samples."}, {"title": "2.2. Open-set Active Learning", "content": "Unlike the assumptions in standard AL, open-set AL considers the presence of OOD data in the unlabeled pool. Various methods have been proposed for this purpose. Firstly, CCAL [4] and MQNet [11] perform contrastive learning on unlabeled data to determine data purity using the trained model. CCAL uses SimCLR [1] and CSI [17] to reflect purity and informativeness, respectively, and proposes a joint query strategy that combines these scores for selecting data. However, contrastive learning alone focuses on purity, making it difficult to select highly informative data. To resolve this trade-off, MQNet adjusts the purity obtained from CSI and the informativeness from standard AL using meta-learning, allowing data collection to prioritize purity or informativeness as needed.\nSecondly, research has proposed training OOD detection models to assess data purity. LfOSA [10] trains a classifier to distinguish K + 1 classes, including K ID classes and one additional OOD class, using labeled ID and OOD data. The classifier predicts the class for each sample of unlabeled data, and based on the maximum activation value of the predicted class, a Gaussian Mixture Model (GMM) is trained with K components. Each GMM assumes two Gaussian distributions. The distribution with the higher mean is assumed to represent the ID data distribution. Therefore, the higher the probability of a sample belonging to the Gaussian distribution with the higher mean, the higher its purity. Conversely, a lower probability indicates that the sample does not belong to any ID classes, meaning it is OOD class. However, LfOSA does not consider informativeness, limiting model performance. EOAL [13] addresses this issue by proposing a method to select data using a combined score based on two entropy measures representing purity and informativeness. Firstly, to assess informativeness, K binary classifiers are trained, and the entropy scores for the unlabeled data are calculated using these K binary classifiers corresponding to the ID classes. Secondly, to assess purity, a classifier is trained to classify K + 1 classes, and the representations from this trained classifier for the labeled OOD data are used to perform clustering. The probability of belonging to each cluster is calculated based on the distances between clusters, and the entropy for data purity is computed for the unlabeled data. Combining these two scores, EOAL selects data with high purity and informativeness.\nWhile previous open-set AL methods show improved performance over standard AL by considering OOD data, they still incur annotation cost losses. MQNet, for instance, tends to select a large amount of OOD data because it prioritizes informativeness over purity when selecting high-informativeness data. Methods like LfOSA and EOAL require additional OOD data for OOD detector training, inherently leading to cost losses. Furthermore, both approaches necessitate extra training costs for collecting OOD data. However, our proposed method sequentially assesses purity and informativeness, avoiding the trade-off between these criteria. We use the pre-trained CLIP model for OOD detection, requiring no additional training (Fig. 4)."}, {"title": "3. Proposed Method", "content": "The proposed CLIPNAL comprises two sequential processes: (1) measuring data purity based on a pre-trained CLIP and (2) computing informativeness of query strategies used in standard AL. In this section, we describe how our CLIP-based method assesses the purity of unlabeled data. As illustrated in Fig. 5, we use ID class names as linguistic information to calculate the ID probability of unlabeled images. Furthermore, we integrate visual information from labeled ID data to apply weighting to the ID probability. CLIPNAL process and an example of assessing purity can be found in Appendix. A."}, {"title": "3.1. Problem Statement", "content": "The open-set AL problem defined in this study aims to classify images into K classes, with the goal of improving the performance of an image classifier \\(f_{clf}\\) based on labeled data. We define the unlabeled data targeted for collection as open-set data \\(D_u = \\{x_i \\in \\mathcal{X}_{ID} \\cup \\mathcal{X}_{OOD}\\}_{i=1}^{NU}\\), assuming the presence of both ID and OOD data. In AL, a fixed budget C is used for annotation costs in each round of data collection, repeated R times. If annotating a single image costs one unit, setting C to 10 means 10 images can be annotated per round. The selected queries from \\(D_u\\) are denoted as Q. These queries are annotated by human experts, adding ID data \\(\\mathcal{X}_{ID}\\) to \\(S_{ID}\\) and OOD data to \\(S_{OOD}\\). The labeled ID data \\(S_{ID}\\) is used to train the classifier \\(f_{clf}\\). Thus, open-set AL aims to maximize model performance by minimizing the proportion of OOD data and selecting highly informative data with minimal cost."}, {"title": "3.2. CLIPN", "content": "Recently, large-scale datasets like MS COCO [8] and LAION-2B [14] have enabled CLIP [12] to be used not only for ID classification but also for OOD detection [5, 9, 19]. CLIPN [19], the latest method, enhances CLIP by adding a text encoder for the concept of \"no\". Unlike previous CLIP-based zero-shot OOD detection methods [5, 9], which only consider proximity to class-specific text, CLIPN incorporates the notion of dissimilarity to achieve high performance in OOD detection. Therefore, our proposed method uses CLIPN to detect OOD data in the unlabeled pool.\nCLIPN assesses whether an image is OOD by considering the similarity between the image and class-specific textual information and the probability that the image does not belong to the class. First, the visual representation of an image \\(x^{img}_i\\) is obtained by inputting it into the image encoder. Next, pre-defined textual templates with class names are input into the text encoder for each target class to calculate the average textual representation. Similarly, the \"no\" text encoder in CLIPN calculates the average representation for each class.\n\\[\nz^{img}_i = f_{img}(x^{img}_i)\n\\]\n\\[\nz^{text}_j = \\frac{1}{T} \\sum_{t=1}^{T} f_{text}(t_{prompt}(class \\ name_j))\n\\]\n\\[\nz^{no \\ text}_j = \\frac{1}{T} \\sum_{t=1}^{T} f_{no \\ text}(t_{prompt}(class \\ name_j)).\n\\]\nNext, the probability that \\(x^{img}\\) belongs to each class is calculated by computing the cosine similarity between the visual and textual representations and applying softmax.\n\\[\np_{ij}^{yes} = \\frac{exp(z^{img}_i \\cdot z^{text}_j / \\tau)}{\\sum_{k=1}^{K} exp(z^{img}_i \\cdot z^{text}_k / \\tau)}\n\\]\nwhere \\(\\tau\\) is a temperature parameter that adjusts model confidence. The probability that \\(x^{img}\\) does not belong to each class \\(p^{no}_{ij}\\) is defined as follows.\n\\[\np_{ij}^{no} = \\frac{exp(z^{img}_i \\cdot z^{no \\ text}_j / \\tau)}{exp(z^{img}_i \\cdot z^{text}_j / \\tau) + exp(z^{img}_i \\cdot z^{no \\ text}_j / \\tau)}\n\\]\nFinally, the probabilities \\(p^{ID}_{ij}\\) for ID data and \\(p^{OOD}_{i}\\) for OOD data are calculated using the agreeing-to-differ (ATD) method proposed in CLIPN. ATD defines \\(P^{OOD}\\) based on \\(p^{no}\\) without requiring a user-defined threshold.\n\\[\np_{ij}^{ID} = p_{ij}^{yes}(1 - p_{ij}^{no})\n\\]\n\\[\nP_i^{OOD} = 1 - \\sum_{j=1}^K p_{ij}^{ID}\n\\]\n\\[\nI(x_i) = \\begin{cases}\n1, & P_i^{OOD} > max_j(p_{ij}^{ID}), \\\\\n0, & else,\n\\end{cases}\n\\]\nwhere \\(I(x_i)\\) indicates whether the image \\(x^{img}_i\\) is OOD. If \\(P_i^{OOD}\\) is greater than the maximum \\(p_{ij}^{ID}\\), then \\(I(x_i)\\) is 1, indicating that \\(x_i\\) is considered OOD."}, {"title": "3.3. Visual Similarity Weighting", "content": "Although CLIPN shows high performance in detecting OOD data using textual information, it encounters challenges when the number of classes increases or when the OOD data domain is similar to that of ID data. We use visual information from ID data as additional weights to address these challenges. First, the average representation \\(\\phi_j\\) for each class is calculated by applying the image encoder \\(f_{img}\\) to the labeled ID data \\(x^{img,ID}_i\\).\n\\[\n\\phi_j = \\frac{1}{N^{ID}} \\sum_{i=1}^{N^{ID}} f_{img}(x^{img,ID}_i).\n\\]\nNext, the visual similarity weight \\(s_{ij}\\) for each class is calculated by computing the cosine similarity between the visual representation \\(z^{img}_i\\) and the average representation \\(\\phi_j\\), and converting it to probabilities using the softmax function.\n\\[\ns_{ij} = \\frac{exp(z^{img}_i \\cdot \\phi_j / \\tau)}{\\sum_{k=1}^{K} exp(z^{img}_i \\cdot \\phi_k / \\tau)}\n\\]\nThe visual similarity weight \\(s_{ij}\\) is used to adjust the probabilities of ID data. The adjusted probabilities \\(p^{*,ID}_{ij}\\) and \\(P^{*,OOD}_{i}\\) are calculated by applying the weights as follows.\n\\[\np_{ij}^{*,ID} = s_{ij} \\cdot p_{ij}^{ID}\n\\]\n\\[\nP^{*,OOD}_i = P^{OOD}_i / K\n\\]\n\\[\nI^*(x_i) = \\begin{cases}\n1, & p^{*,OOD}_i > max_j(p^{*,ID}_{ij}), \\\\\n0, & else,\n\\end{cases}\n\\]"}, {"title": "3.4. Self-Temperature Tuning", "content": "The temperature parameter significantly influences model confidence in predictions. The optimal temperature varies depending on the domain of the data used for OOD detection. In open-set AL, labeled ID and OOD data allow for exploring the optimal temperature parameter.\n\\[\n\\hat{y}_i(\\tau) = I^*(x_i, \\tau),\n\\]\n\\[\nAcc(\\tau) = \\frac{1}{N^{ID}} \\sum_{i=1}^{N^{ID}} I^{(y^{OOD}_i = \\hat{y}_i(\\tau))},\n\\]\n\\[\n\\tau^* = arg \\underset{0<\\tau}{max} \\ Acc(\\tau).\n\\]\nAccording to Eq. (8), we apply a self-temperature tuning to explore the optimal temperature parameter by comparing the accuracy of the predicted results \\(\\hat{y}^{OOD}(\\tau)\\) against the ground truth \\(y^{OOD}\\) for ID and OOD data."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Baselines. To demonstrate the efficiency of CLIPNAL, we select query strategies from standard AL and open-set AL as comparison methods. For standard AL, the following strategies are selected: (1) uncertainty-based strategy, including CONF [18], LL [21], and (2) diversity-based strategy, including CORESET [15]. For open-set AL, the chosen strategies are (1) contrastive learning-based strategy MQNet [11], (2) OOD detector-based strategy, including LfOSA [10] and EOAL [13]. To evaluate the effectiveness of query strategies, we include RANDOM, which selects data randomly without specific criteria, in the experiments. The detail of the baselines is described in Appendix. D.\nOpen-set Data. To construct various open-set scenarios, we consider mismatch ratios, which vary the proportion of classes in the data, and OOD ratios, which vary the proportion of OOD data in the unlabeled pool [4, 10]. The mismatch ratios are defined as 20%, 30%, and 40%. The OOD ratios are defined as 10%, 20%, 40%, and 60%. For instance, in CIFAR10 with a mismatch ratio of 20%, two classes are ID, and the remaining eight are OOD. The composition of the open-set data used in the experiments is detailed in Appendix. B. We evaluate the robustness and effectiveness of the query strategies under various scenarios. The annotation cost C for AL is 500 for both datasets. The initial data for training the classifier is randomly selected from the unlabeled pool, equivalent to the defined cost. In subsequent iterations, data is selected based on the query strategy with a cost of C.\nImplementation Details. The image classifier is ResNet18 [6] in the experiments. The number of rounds R for AL experiments is set to 10, with results reported for each round. Each query strategy experiment is repeated five times, with average results reported. The hyper-parameters and pre-trained models are described in Appendix. C.\nEvaluation Metrics. Precision, the proportion of selected ID queries, is used to evaluate annotation cost loss in open-set AL [10]. Higher precision indicates a higher proportion of ID data in the selected queries.\n\\[\nPrecision(%) = \\frac{\\sum_{i=1}^{C} I(x_i = ID)}{C} \\times 100.\n\\]"}, {"title": "4.2. Open-set Robustness", "content": "Comparison of Annotation Cost Loss. We examine annotation cost loss through precision changes during data collection in Fig. 6. Each round's precision indicates the proportion of ID data among the selected data at the round. RANDOM, shown in black, selects data randomly, so the proportion of OOD data in the selected queries matches the OOD ratio in the unlabeled pool. A higher precision than RANDOM indicates a higher selection of ID data than the OOD ratio.\nStandard AL strategies, except CORESET, show lower precision than RANDOM in all cases, particularly as the OOD ratio increases. This indicates significant cost loss when OOD data is in the unlabeled pool. In contrast, most open-set AL strategies show higher precision than RANDOM. However, MQNet, LfOSA, and EOAL do not show consistent precision across rounds. MQNet initially focuses on purity, selecting more ID data, but later prioritizes informativeness, causing precision to drop. LfOSA and EOAL initially had low precision due to the limited OOD data for training the detection model, but they improved as more OOD data was collected. Unlike existing strategies, CLIPNAL maintains high precision throughout data collection, regardless of the OOD ratio, indicating minimal annotation cost loss.\nRobustness to OOD Ratio. Fig. 7 shows the results for (a) CIFAR10 and (b) CIFAR100 with varying OOD ratios. The closer the model performance is to the black dashed line, the more informative the labeled ID data. CLIPNAL demonstrates robust performance, with minimal accuracy drop at the final round regardless of the OOD ratio. In CIFAR10, the accuracy of the classifier trained with ten rounds approximates that of a classifier trained on the entire dataset. This indicates that CLIPNAL minimizes cost loss and selects highly informative data.\nStandard and open-set AL methods show varying results depending on the OOD ratio. For OOD ratios of 10% and 20%, standard AL methods perform better than open-set methods on CIFAR10, suggesting that standard AL methods select more informative data when the proportion of OOD data is low. However, as the OOD ratio increases, open-set AL methods outperform standard AL methods, as they can collect more ID data, enhancing classifier accuracy. LfOSA performs poorly across all cases, indicating its inability to select highly informative data despite high purity.\nRobustness to Mismatch Ratio. Fig. 8 shows the results for (a) CIFAR10 and (b) CIFAR100 with varying mismatch ratios. CLIPNAL shows significant accuracy improvement in the initial data collection iterations compared to other query strategies. In CIFAR10, its performance remains close to the classifier trained on the entire dataset regardless of the mismatch ratio. In CIFAR100, despite a larger number of classes, CLIPNAL maintains high accuracy compared to other strategies. Overall AL performance per the mismatch and OOD ratio can be found in Appendix. E.\nComparison with Standard AL Scenarios. Fig. 9 demonstrates the robustness of CLIPNAL. The black dashed line represents the accuracy change during data collection assuming no OOD data, applying CONF to only ID data. Without OOD data, there is no cost loss. CLIPNAL, despite the presence of OOD data, shows accuracy convergence to the black dashed line in CIFAR10 and even surpasses it in CIFAR100. This indicates that CLIPNAL selects more informative data than standard AL assuming only ID data."}, {"title": "4.3. Ablation Studies", "content": "Effect of Visual Similarity Weights. To improve the accuracy of OOD detection, CLIPNAL uses visual similarity weights from labeled ID data. Fig. 10 shows (a) accuracy and (b) precision improvements with and without visual similarity weights. (a) Accuracy improvement is significant with increasing mismatch ratios and OOD ratios, except for a 20% OOD ratio in CIFAR10. (b) Precision results indicate that visual similarity weights significantly increase the amount of ID data selected, especially in CIFAR100, demonstrating the effectiveness of visual similarity weights in preventing cost loss.\nNecessity of Self-Temperature Tuning. Fig. 11 shows the accuracy of purity assessment with varying temperature parameters \\(\\tau\\). The accuracy of purity assessment changes significantly with small variations in \\(\\tau\\), indicating the importance of optimal temperature tuning. The optimal \\(\\tau\\) values for 0.108 on CIFAR10 and 0.123 on CIFAR100 differ, emphasizing the need for self-temperature tuning based on labeled data to select high-purity data correctly."}, {"title": "4.4. Visualization of Query Strategy Results", "content": "Fig. 12 visualizes the ID data and informativeness selected by each query strategy. In the case of RANDOM, the selected data shows no specific pattern and is chosen randomly. In contrast, CORESET selects data with diversity, resulting in less redundancy and more varied data than RANDOM. LL and CONF select data based on uncertainty and choose data near the decision boundaries between classes. However, the query strategies in standard AL result in fewer usable training data due to the selection of OOD data. On the other hand, open-set AL methods select more usable training data than standard AL. However, LfOSA and EOAL, focusing on purity rather than decision boundaries, tend to select less informative data. MQNet selects relatively more ID data than LL, but only in certain decision boundary areas. Conversely, CLIPNAL selects data primarily from high informativeness areas near decision boundaries, similar to CONF, and achieves the highest selection of ID data among the query strategies."}, {"title": "5. Conclusion", "content": "This study introduces CLIPNAL, a two-stage query strategy designed to minimize cost losses in open-set active learning by improving the accuracy of purity assessments and selecting highly informative data from ID data only. CLIPNAL utilizes a pre-trained CLIPN model without additional training to detect OOD data based on linguistic information and visual similarity weights derived from collected ID data. To maximize the accuracy of the pre-trained CLIPN's ID data assessment, CLIPNAL optimizes the temperature parameter through self-temperature tuning based on the labeled ID and OOD data. By accurately selecting ID data from unlabeled data, CLIPNAL can apply a standard AL query strategy to select only highly informative data, reducing cost losses and approximating the conditions assumed in standard AL.\nTo validate the efficiency of the proposed method, various open-set datasets with different mismatch ratios and OOD ratios were constructed using CIFAR10 and CIFAR100. Experimental results demonstrate that CLIPNAL consistently achieves the lowest cost losses and highest accuracy across all scenarios. As CLIPNAL conducts purity assessment and data informativeness evaluation sequentially, it can incorporate various query strategies proposed in standard AL after assessing purity. This flexibility indicates that future research can extend the application of CLIPNAL to not only classification but also regression, object detection, and object segmentation in open-set scenarios."}, {"title": "D. Baselines Description", "content": "We select query strategies for standard AL based on uncertainty and diversity. Uncertainty-based query strategies include CONF [18], which considers the lowest class prediction confidence of the model for a given data point as uncertainty. If the minimum prediction confidence is high, it implies that the model lacks certainty, thus selecting data with high uncertainty for training. LL [21] adds a Loss Prediction Module to the trained model to predict the loss during training, selecting data points with higher predicted losses as those with higher uncertainty. Diversity-based query strategy: CORESET [15] selects data that encapsulates diverse information to cover the entire dataset, thereby selecting less redundant data containing new information.\nWe chose query strategies for open-set AL using contrastive learning and OOD detector. Contrastive learning-based query strategy: MQNet [11] uses CSI for OOD detection. It calculates purity using CSI and informativeness using LL, combining the two through meta-learning to define data scores. OOD detector-based query strategy: LfOSA [10] trains a classifier on labeled ID and OOD data to classify \\(K\\)+1 classes, then uses a Gaussian Mixture Model to determine data purity. EOAL [13] uses two entropy scores representing purity and informativeness to select data based on these combined scores."}, {"title": "E. Overall Performance on Open-set Data", "content": "AL results are assessed using Area Under Budget Curve (AUBC) [22]. AUBC measures the efficiency of AL by calculating the area under the curve of accuracy improvement over rounds. A larger area indicates greater accuracy improvement after data collection.\nTable 2 summarizes the AUBC results for all open-set scenarios. MQNet shows high AUBC in CIFAR10 with high OOD and mismatch ratios but does not outperform standard AL in all cases. In CIFAR100, RANDOM often outperforms other strategies. However, CLIPNAL consistently shows higher AUBC than existing methods, indicating significant accuracy improvement from the initial round."}]}