{"title": "Future of Information Retrieval Research in the Age of Generative AI", "authors": ["James Allan", "Eunsol Choi", "Daniel P. Lopresti", "Hamed Zamani"], "abstract": "In the fast-evolving field of information retrieval (IR), the integration of generative AI technologies such as large language models (LLMs) is transforming how users search for and interact with information. Recognizing this paradigm shift at the intersection of IR and generative AI (IR-GenAI), a visioning workshop supported by the Computing Community Consortium (CCC) was held in July 2024 to discuss the future of IR in the age of generative AI. This workshop convened 44 experts in information retrieval, natural language processing, human-computer interaction, and artificial intelligence from academia, industry, and government to explore how generative AI can enhance IR and vice versa, and to identify the major challenges and opportunities in this rapidly advancing field.", "sections": [{"title": "1. INTRODUCTION", "content": "In today's rapidly evolving digital landscape, the field of information retrieval (IR) is at the intersection of traditional search methodologies and cutting-edge machine learning and artificial intelligence (AI) technologies. As we witness the proliferation of generative AI-driven models, such as diffusion and large language models (LLMs), it has become increasingly evident that the boundaries of IR research are expanding. The ways users search for information and the ways systems recommend information to users are already being impacted by generative AI technologies. Conversely, information retrieval technologies can have substantial impact on generative AI applications in terms of efficiency, effectiveness, robustness, and trustworthiness. Retrieval-augmented language models are just one example of direct impact that has recently attracted considerable attention in both academia and industry. A few of these areas have been discussed in two visioning perspective papers on \u201cRetrieval-Enhanced Machine Learning\u201d (Zamani et al., 2022) and \u201cLarge Language Models and Future of Information Retrieval\u201d (Zhai, 2024) and multiple workshops including \u201cSearch Futures\u201d (Azzopardi et al, 2024) and \u201cTask Focused IR in the Era of Generative AI\u201d (Shah & White, 2024). These areas are becoming increasingly important every day. This growth of activity is why we believe this is the right time to discuss major challenges and opportunities in shaping the Future of Information Retrieval Research in the Age of Generative AI. This has motivated us to organize a visioning workshop on this topic with the support of the Computing Community Consortium (CCC).\nTo explore and navigate this dynamic landscape, we gathered a group of 44 experts across academia, industry, and government in the fields of information retrieval, natural language processing, human-computer interaction, machine learning, and broadly artificial intelligence to Washington, D.C. for this visioning workshop. The group came together to outline the future of IR research, with generative AI playing a central role in reshaping how we discover and interact with information. This report is a reflection of the themes, recommendations, and ideas that emerged during the visioning workshop. The workshop used the Chatham House rule, so the ideas are not attributed to specific participants. This report is a synthesis of the discussions during the workshop and does not include citations as in a traditional journal article.\nFrom the visioning workshop discussions, two major themes emerged: (1) enhancing generative AI models and applications using information retrieval techniques, and (2) enhancing information retrieval models and applications using generative AI techniques. We acknowledge that it is sometimes difficult to draw the boundaries between IR and generative AI systems and these two systems are intertwined from many aspects. The ill-defined boundary between the two areas was brought up and thoroughly discussed during the workshop and the participants acknowledge and stress that information retrieval is not (and never has been) limited to current search engine systems, but broadly addresses how users or machines look for, find, present, access, discover, and interact with information. This creates a large number of challenges that information retrieval and other AI communities urgently need to tackle in the next five to ten years, and corresponding research opportunities that are available. Important real-world applications that can benefit from research in this area include, but are not limited to, search engines, recommender systems, question answering systems, dialogue systems, and intelligent assistants.\nWe want to stress that this workshop focuses on challenges and opportunities that arise at the intersection of IR and generative AI fields and is not intended to cover the union of the two fields. There are additional issues that arise within generative AI independent of IR and within IR that are unrelated to generative AI. This report makes no attempt to discuss those issues."}, {"title": "2. HOW THIS DOCUMENT CAME ABOUT", "content": "Given the importance of the topic, the UMass Amherst Center for Intelligent Information Retrieval (CIIR) organized a relatively small, regional, invitation-only Brainstorming Session on Information Retrieval Research in the Age of Generative AI on December 1-2, 2023, in Amherst, Massachusetts. The brainstorming session included seven faculty members from University of Massachusetts Amherst (UMass), six faculty members or senior researchers from five other institutions, and 15 doctoral students from UMass and Carnegie Mellon University. In this two-day event, the future challenges in the intersection of IR and generative AI research were discussed and there was a unanimous consensus that the organization of a follow-up workshop was needed.\nThe organizers thus reached out to the CCC with a proposal to run this visioning workshop. Upon CCC's approval, the organizers worked together to expand the organization team and compile a list of potential participants from diverse research communities, backgrounds, demographics, and institutions. A questionnaire was sent to the potential participants to collect feedback on their interest in joining the visioning workshop and preference on timing and location. Given the feedback from over 30 potential participants, the organizers and the CCC decided to hold the visioning workshop in Washington DC on July 19-20, 2024, immediately following the ACM SIGIR International Conference on Research and Development in Information Retrieval, the premier conference for information retrieval research.\nA few weeks before the workshop, the participants were asked via a second survey to think about the core topics of the workshop and provide their individual ideas and indicate which of those particularly interested them. The responses were used by the organizers to frame initial ideas for the workshop.\nIn all, 44 experts from various computing disciplines, including 40 invited researchers and 4 organizers, in addition to 5 members of the CRA staff and 1 member of the CIIR staff joined the visioning workshop in person at the Planet Word Museum in Washington, D.C. The participating experts comprised 30 academics, 5 government employees, and 9 industrial researchers; 34 US-based attendees and 10 from outside the US; 24 primarily affiliated with information retrieval, 12 with NLP, and 8 with other AI topics.\nThe workshop was structured as follows:\n\u2022 Opening: The organizers kicked off the workshop by describing the visioning process, defining the scope of the IR-GenAI workshop, highlighting participation rules for breakout sessions, and presenting the workshop agenda. There also was a quick introduction of each participant.\n\u2022 Kicking off the discussions: The organizers presented an overview of the discussions that happened during the CIIR Brainstorming Session on IR Research in the Age of Generative AI and the MSR Workshop on Task Focused IR in the Era of Generate AI. Seven participants were selected by the organizers based on their responses to the pre-workshop questionnaire, aiming for a diverse set of topics of broad interest. They each presented a two-minute visioning idea (a \"flash prompt\") to catalyze discussions on those and related topics. These talks were followed by substantial open-floor discussions suggesting alternative topics, expanding on those previously offered, and offering combinations and divisions. Workshop organizers, staff, and participants synchronously took notes on the discussions in a shared document and added additional possible topics or thoughts within the document.\n\u2022 Forming breakout sessions: The participants then compiled a list of potential breakout sessions. (We also used existing commercial generative AI technologies to produce a list of breakout sessions from the notes; unfortunately without useful output.) The list of potential breakout sessions was iteratively refined based on the feedback from the participants in an open-floor discussion and periodic votes of interest, resulting in eight breakout sessions within IR-GenAI:\n    a. Evaluation: This breakout group focused on (1) developing datasets, methods, and platforms for evaluating the next generation of (LLM-powered) information access systems, and (2) using generative AI technologies to develop more reliable and cheaper (personalized) evaluation methodologies for existing information access tasks.\n    b. Training, Feedback, and Reasoning: This breakout group discussed (1) effective methods to collect and use explicit or implicit feedback from users for training the next generation of retrieval-enhanced generative AI systems, and (2) the obstacles and potential solutions in unlocking higher-level reasoning capabilities in the current generative AI technologies.\n    c. Understanding and Modeling Users: This group discussed major research questions related to what users need and expect from new generative AI-powered information access systems, how to build effective user models for these new technologies, and how to mitigate issues related to ethics and privacy.\n    d. Social Ramifications: This breakout group focused on socio-technical challenges in developing the new generation of information access systems and potential solutions to mitigate or address societal and ethical issues these technologies may create.\n    e. Personalization: This breakout group explored challenges, opportunities, and potential solutions in developing \u201cdigital twins\u201d or \u201cdigital shadows\u201d for users to develop effective personal assistants for information discovery and access.\n    f. Scaling Across Compute, Data, and Human Efforts: This group built on the idea that current success in deep learning and generative AI research is largely due to various scaling efforts, to discuss potential challenges and opportunities in scaling efforts and how to continue this progress in the scope of IR and generative AI research efficiently.\n    g. AI Agents and Information Retrieval: This group discussed the challenges and opportunities in developing intelligent agents that are ubiquitous, effective, inexpensive and able to provide information, or accomplish tasks on behalf of users. This group also explored how different agents can interactively communicate and solve complex problems, such as planning and decision making.\n    h. Foundation Models for Information Access and Discovery: This breakout group focused on the needs for and potential next steps in developing foundation models, specifically designed for information access and discovery.\n\u2022 Breakout sessions: Each participant chose one breakout session and participated in detailed discussions throughout Friday afternoon and Saturday. Each breakout group had chosen a leader and a scribe, though writing was typically distributed throughout the group. Each breakout session presented a brief oral summary of their work-so-far to all participants twice, with responses by the full set of participants entered directly into that group's notes. This process ensured that all voices were heard and feedback was received. The last part of the workshop was devoted to writing up descriptions of sessions and compiling the challenges and opportunities discussed in the breakout session (and presented below).\n\u2022 Closing: Each breakout session presented two things to the entire set of participants: (1) a major and compelling recommendation that arose in their discussions and (2) a burning question whose answer could improve the group's report. The workshop ended with an open-floor discussion of the questions, challenges, and any other topic that someone felt needed to be raised.\nThe workshop organizers gathered immediately after the workshop, read the reports from each breakout session, and discussed the outline and potential content of the report. The first draft of the report was produced solely by the organizers using all the material produced by each breakout session (i.e., notes, summaries, comments, challenges, questions, recommendations) and extending or modifying them as needed. The draft of the report was shared with CCC and workshop participants for further feedback and it was revised to shape the report at hand. This report has been reviewed internally by a member of the CCC council and externally by 2 members of the IR/Al research community."}, {"title": "3. SUMMARY OF THE DISCUSSED RESEARCH TOPICS FOR FUTURE EXPLORATION", "content": "Here we present a brief summary of the key observations, challenges, and opportunities discussed in each breakout session. More details on each topic are provided in the following sections.\nExploring the limits of using LLMs to label material as relevant or not, including automatically generated explanations that indicate why something is relevant (or not).\nExpanding evaluation approaches to include the entire search and discovery process: handling multi-step processes such as conversations, measuring the accuracy of generated (rather than simply retrieved) responses, and designing approaches that measure the interactions between IR and GenAl.\nDeveloping and employing \u201cdigital twin\u201d technology to enable strong and reliable simulated user evaluations. That includes using variations of the same \u201ctwin\u201d to more broadly understand the range of human responses.\nDeveloping interactive generative AI and information retrieval systems that cooperatively learn when to submit queries and what information to retrieve that augments the knowledge encoded in generative AI systems.\nExploring various implicit and explicit feedback mechanisms to iteratively improve retrieval-enhanced generative AI systems.\nRetrieving, organizing, and synthesizing vast amounts of information using generative AI systems that can manage complex reasoning tasks.\nUnderstanding user needs and their multimodal interactions with generative AI systems for information discovery and access.\nLearning an effective model of users by representing their cognitive state, including their state of knowledge, while using generative AI systems for information discovery and access.\nExploring privacy-preserving solutions in modeling users in generative AI systems.\nThinking about the impact on all aspects of society continually: before, during, and after development.\nIdentifying risks, challenges, and opportunities of generative AI for information retrieval requires an interdisciplinary approach informed by socio-technical perspectives and co-developed with social science scholars, legal scholars, civil society representatives, and policy makers among others.\nDeveloping efficient, personalized generative AI systems that act as digital twins to learn personal behavior and preferences of the user for personalized retrieval, recommendation, and synthesis of information.\nDeveloping persuasive recommender systems that not only produce accurate recommendations but also provide sufficient explanation, transparency, and justification to persuade their users.\nDeveloping efficient personalized generative AI systems for on-device intelligent assistance and information access.\nExploring the capabilities of numerous small LLMs rather than a single monolithic approach.\nInvestigating the tradeoffs between different sized LLMs and different sized IR systems.\nExploring ways to accomplish the same (or improved) capabilities with a fixed hardware footprint rather than one that is assumed to grow continuously.\nConsidering the impact of new computational paradigms (e.g., quantum, bio, or neuromorphic computing) to understand their impact on GenAl and IR.\nUnderstanding how mixed-initiative systems leverage GenAl and IR to gather information proactively for users without direct initiation.\nExploring how agents can communicate with each other reliably, particularly in the face of current hallucination challenges.\nDeveloping approaches that allow GenAl to make novel and perhaps complex plans for gathering information in response to challenging questions.\nCreating evaluation frameworks for groups of agents, some of which address multimodal inputs, collaborating with each other and users to accomplish a task.\nDeveloping a task-agnostic foundation model for information access, extending human intelligence with personalized, contextual, and multimodal information and beyond.\nDeveloping techniques for better incorporating user behavior, supporting multiple modalities, and generating accessible output."}, {"title": "4. SHORT- AND LONG-TERM RESEARCH TOPICS AND RECOMMENDATIONS", "content": "In this section we provide more context, additional details, and extra challenges and opportunities from each breakout group, building on the summaries provided above. Each includes summary information followed by a list of short-term (within five years) and long-term (within ten years) research challenges.\nWe identified two broad classes of evaluation challenges: (1) the application of genAl to IR evaluation and (2) the evaluation challenges of generative IR.\nGenerative Al offers two key opportunities to the evaluation of classic document or passage retrieval systems. First, a classic approach to offline evaluation in IR is the use of a test collection. The construction of these collections is resource intensive, in particular the creation of the lists of the documents that are relevant or not relevant to queries. There is growing evidence that LLMs may be useful in labeling documents. Research to explore and refine this labeling process holds great promise.\nA second key aspect of offline evaluation is the so-called user evaluation: the observation of people in how they initiate or react to the actions of a search engine. There is the prospect of exploiting GenAl to simulate the actions of people. One might even speculate that human \u201cdigital twins\u201d could be created for the purposes of testing an information retrieval system.\nFor the purposes of evaluating a generative IR system, there is much to consider. Retrieval Augmented Generation represents a coalition of IR and generative systems to accomplish a task. It is necessary to consider evaluation of these component systems in concert as well as individually. It is also necessary to establish the domain of competency of generative IR systems. A challenge with the output of a generative system is that it always expresses an answer with apparent confidence. There is great value in knowing the knowledge domain that a system is competent in. How do we establish and communicate that competency? In a related topic, we don't know how an LLM is achieving its answers. A research challenge is to establish a methodology for building confidence in the output of LLMs. Finally the reproducibility of systems needs to be considered.\nTo support the evaluation of classic search, researchers rely on a fixed set of information needs and the identification of relevant documents. There is currently an explosion of interest in having LLMs do the labeling of documents to control costs. There is a spectrum of ways to use LLMs to that end, from replacing human judgments with LLM labels to utilizing them as an assistant. Using LLMs as labelers directly suffers from various issues. For instance, LLMs make mistakes, create misinformation through hallucination, and suffer from various types of biases. Alternatively one could use an LLM to assist an assessor who is the one responsible for providing the labels. In particular an LLM could help assessors be more consistent by having LLMs partner with assessors to identify documents that may need to be rejudged because relevance judgments are inconsistent. Here continual fine-tuning of a personal, topic-specific LLM could support label suggestion.\nFor many evaluation data sets, relevance labels are applied at the document level, but there is no recording of why a document was labeled as relevant or \u2013 when not all of the document is relevant to the information need \u2013 which part of the document contains the relevant information. A research direction could investigate whether LLMs could be used to generate written descriptions of why a particular document is relevant. Alternatively, LLMs could be used to identify which particular parts of a document provide relevant information.\nThe current evaluation methodologies for Retrieval-Augmented Generation (RAG) pipelines assess retrieval and generation separately, assuming that the better the retrieval (evaluated on its own), the better will the generation (evaluated on its own) based on the top-ranked documents. However, there is not an a priori guarantee that the text of the most relevant document used as a prompt will lead to the generation of the best output; it could be that the second or the third relevant document would provide a better priming of the LLM, inducing it to produce a better response. Current evaluation is not designed to investigate these aspects and this kind of interaction and there is an opportunity to develop more comprehensive approaches able to assess the whole RAG pipeline and clarify the effects of this interaction. The benefit of these new evaluation methods would be to not only lead to improving RAG itself but also to shed some light on the inner workings of LLMs.\nIt is currently thought that LLMs embed knowledge distilled through their learning process, but it is not fully clear what this knowledge actually is and what domains it covers. There is a broad understanding that general purpose models need to be specialized or fine-tuned to specific domains and this led to a proliferation of models for specific languages or areas. There is also the idea that bigger and bigger models should be able to perform better and better in more and more domains, even if it could still be the case that they lack enough training data in a given domain or that the relative size of the training data for a domain becomes smaller and smaller as the other domains grow bigger, making the model less effective in that domain. In general, there is a lack of an evaluation methodology able to investigate and determine which is the actual domain of competence of an LLM and/or how an LLM could perform in a given domain (or outside it). This kind of evaluation would be needed for understanding the extent to which an LLM is suitable for one or more domains. Moreover, it would open the possibility for a new area of investigation, i.e. Prompt Performance Prediction (PPP), aimed at estimating whether a given prompt could be effectively dealt with by an LLM, given its domain(s) of competency.\nReproducibility is a primary concern in almost any area of science and it has been discussed in IR for many years as well. LLMs, and particularly proprietary LLMs, bring the reproducibility concerns to a completely new and different level. There is an urgent need for much more \u201copen source\u201d and shared models and datasets, in terms of code, training data, training regimes, intermediate snapshots, and so on. This is needed just to be able to conduct experiments on a common ground and to strive for reproducibility. Another angle of reproducibility concerns the extreme variability in the functioning of LLMs themselves where even small changes in the prompt may lead to substantially different answers. Therefore, there is a need for a more systematic investigation of the behavior of such systems in order to understand how to control the effect of such small variations or how to account for them when assessing and comparing systems, e.g. by suitably computed \u201cconfidence intervals\u201d. The ultimate goal is to improve the validity and generalizability of the conclusions drawn from the experiments.\nSimulation has been pursued for a long time in IR, and LLMs provide a concrete opportunity to push its boundaries and open up to the evaluating system by credibly simulating the user behavior or even by creating whole synthetic experimental collections (e.g., topics, documents, relevance judgements, clicks). Ultimately this kind of simulation would correspond to the creation of digital twins of people/users (or better of groups of people/users) which can be exploited to assess systems. Such digital twins could be useful when testing new systems for which there are no existing interactions and one wants to be proactive. Such a simulation would clearly offer the possibility to scale-up and make evaluation much more fine-grained, allowing for more systematic testing before moving to the user-side. Simulation naturally fits in the continuum of evaluation which ranges from lab-based evaluation based on experimental collections, i.e. a kind of static simulation of user information needs and answers to them, to interactive evaluation, e.g. A/B testing with real users in an operational setting. One open question is whether current LLMs are enough to create such digital twins or whether we should imagine more advanced models which, besides the language generation capability, include behavioral models of users, browsing or interaction components, and more. In some sense, these enhanced models could parallel what multimedia models are today, instead of being constituted by multiple media, they would be constituted by multiple components of such a digital twin. The perspective of such models raises several questions: how do we validate them? How stable would they be? Would their training data be rich and diverse enough to (even partially) capture what a real person is? It is important to recall that this will also be an approximation of users and thus in the end cannot substitute for actual user studies.\nLLMs are systems initially designed for a specific task, i.e. language generation, that are now used for many different purposes and downstream tasks. The current evaluation methodology is designed to assess systems that were specifically designed for one single purpose, e.g., sentiment classification. As a consequence, it is based on verifying that for a given input, the expected output is produced, e.g., positive or negative sentiment. However, LLMs are not designed for a specific purpose and there is not an a priori guarantee that the correct answer is generated because of the correct process/inference/reasoning. There is a need and an opportunity for developing deeper evaluation methodologies to investigate and provide evidence about how and why a given answer has been generated. One can consider this linked to some sort of \u201cexplainability\u201d, or can view this as a kind of \"debugging\", or if one imagines an LLM as a big matrix or an extremely complex circuit, this kind of evaluation tries to understand which parts of the matrix (or the circuit) deliver classification, which ones ranking, etc. In this sense, this is different from Al explainability which is instead targeted at providing an explanation for a specific answer, e.g. which features mattered most.\nResponses to information needs that are knowledge requests where a generative response is appropriate will need new approaches to evaluation. This is in contrast to item-based retrieval where a ranked list is an appropriate response. The overarching evaluation of generated output will need to verify that the output is responsive to the information need. Different evaluation approaches will be needed depending on whether the generated output includes citations to source documents or not. With source documents, existing approaches that utilize document labels could be integrated into the evaluation; however, whether or not there are document citations, there will need to be approaches to evaluating whether or not the content that appears in the generated text is responsive to the request. LLMs have a role to play in answering such questions. It is likely that in addition to new frameworks for evaluation, new metrics for evaluation will be needed. The reproducibility of the evaluation is an important consideration and how to perform the evaluation in a way that enables consistent comparison of systems (ie. techniques) is also critical.\nAt the heart of retrieval-augmented generation (RAG) systems, and more broadly retrieval-enhanced generative AI (RE-GenAl) systems, is the interaction between the information retrieval / access model and the language model (LM), and eventually the user. Current training methods focus on specific and narrow definition of RE-GenAl and mainly focus on training RAG components separately, but we see promise in tightly coupling the training of the information access and language model, because such training would allow components to adapt to one another for the purpose of doing well on the downstream task. Some near challenges in joint training are to learn when to retrieve (e.g., based on measures of model uncertainty), when to rely on parametric knowledge, and how to deal with conflicting information.  Moreover, the granularity and the representation space of retrieval are also unclear, i.e., retrieving sentences, passages, entire documents, or even latent representations from a memory. When it comes to new sources of human feedback, it is unclear what constitutes meaningful feedback when click data (user clicks on relevant documents) is no longer available. Some sources of implicit feedback include absence of follow-up questions or question reformulation, which provide explicit scalar or natural language feedback, and how to perform credit assignment from target answer to source documents. We also envision that components can self-improve \u2013 the information access and LM components can provide training signals to one another. Nevertheless, we realize the importance of human annotations, and advocate for a mix of human and machine annotations and argue research on principled integration of human and machine labeling is required. We also envision some paradigm changes where retrieval is possible even when it requires complex reasoning over document collections, e.g., when queries are not similar to the underlying knowledge but are more abstract and/or require aggregation and synthesis at different granularity."}, {"title": "5. ADDITIONAL RECOMMENDATIONS FOR FUNDING AGENCIES AND THE RESEARCH COMMUNITIES", "content": "Funding evaluation pays off in three distinct ways: (1) there are evaluation artifacts such as test collections that can be used to measure systems; (2) those artifacts are built on a task concept, an understanding of what the user is trying to do in the large, that itself can be studied; and (3) the artifacts persist over a long period of time, supporting research well beyond the term of the funded program. \nOne area which information retrieval in particular holds in high regard (and in which the information retrieval community has made important advances) is evaluation. Pushing the state-of-the-art on shared reproducible benchmarks and test collections has always been important for advancing IR \u2013 and all other research areas touched on by this report. Much of the progress toward shifting the state-of-the-art across the information access community has been enabled by large-scale evaluation campaigns that challenge the community to work on a common problem and provide broadly agreed upon measures for reliably comparing system effectiveness.\nThese benchmarks are now common in several styles:\n1. Challenges often affiliated with a conference or workshop. For example, the RecSysChallenge at the ACM RecSys conference, and the prototype LLMJudge Challenge run by the LLM4Eval workshop at SIGIR 2024.\n2. Challenges sponsored by commercial organizations. For example, the Netflix Prize and the Alexa Prize competitions.\n3. Campaigns run by organizations such as NIST's TREC (in the US), CLEF (EU), NTCIR (Japan), and FIRE (India), all of which have provided venues for numerous community-wide evaluations. We note that those campaigns are well-known and trusted in the IR community, though it is not clear that they are similarly viewed outside of that community.\nAs information access moves more toward interactive, GenAl-based approaches, we anticipate the need for new evaluation campaigns and new demands upon the evaluations. We discuss some overarching changes that we believe should impact most of these campaigns.\nEvaluation campaigns within the information access community predominantly focus on measuring how well systems can match \u201cgold standard\u201d output. Some campaigns are more user-focused, attempting to measure how well systems support a task, either directly or through side-by-side preference comparisons or what is commonly called A/B testing. Those sorts of evaluations have been incredibly successful in understanding and improving systems. However, in the context of societal challenges posed by GenAl, which impact a broad range of people across various professions, backgrounds, and abilities, a narrow perspective on evaluation is no longer adequate.\nExperimental evaluation frameworks that do not directly leverage human input \u2013 e.g., leveraging LLMs to determine whether an answer is biased or using digital twins as synthetic users  \u2013 continue to be valuable and will likely continue to provide some gains. However, they ultimately risk a type of confirmation bias, where the LLMs agree with LLMs, ignoring human input. Unfortunately, the cost of providing human assessments is prohibitive at the scale needed for training and measuring the impacts of the mechanisms we list. For that reason, we strongly encourage efforts to support the type of human-based evaluations necessary to reduce those risks.\nEvaluation \u2013 indeed underlying research \u2013 must engage with all feasible stakeholders to understand the impact of GenAl and IR (at least) across those groups and the individuals within them We thus strongly encourage a rethinking of evaluation campaigns to be stakeholder-engaged. It is critical that specific task-based evaluations be designed and carried out \u2013 from start to finish and even beyond \u2013 in consultation with the people who are involved with that task and with anyone who is impacted by the results.\nTo better understand the social issues underlying a challenge and the societal impact of potential solutions, it is necessary to involve research communities outside of computing (in addition to the stakeholders). For example, social and behavioral scientists are skilled in studying human motivations and reactions and are often much better attuned to the impact of technology on marginalized groups. As outlined in Societal Ramifications, we recommend multi-pronged campaigns where studies of users surface challenges, technology innovations are developed, their use by people is explored to identify new challenges and opportunities, technology advances, more exploration, and so on.\nWe strongly encourage community-wide evaluation campaigns where resources can be made available broadly, amortizing the cost of creating judgments across the entire research community.\nEvaluation practices should be geared towards modeling a heterogeneous set of controlled but realistic hardware environments that capture different operating points and constraints (e.g., environment with no GPU resources, embedded systems with specialized GPUs, small-scale, mid-scale and large-scale GPU environments). We envision these taking the form of shared tasks where such hardware environments are standardized and controlled \u2013 mimicking the familiar concept of a test-collection in TREC, but from a hardware environment standpoint. We recognize initial inroads towards addressing this goal have been recently made. For example the ReNeulR shared task at SIGIR 2024 featured a fixed hardware environment and a common set of efficiency measurements - but it still lacked a holistic evaluation approach, and the modeling of a diversity of constrained hardware environments.\nThere is a need for greater national and international computing facilities specifically that support research on the development and use of generative Al in information retrieval systems. The funding agencies should invest in infrastructure suitable for IR-GenAI. Traditional supercomputers are often not well suited for these tasks. It may require significant storage and GPU capacity at scale to process large-scale multimodal datasets. This requires careful engineering and collocation of compute with storage (petabyte scale), high memory (nodes with multi-terabyte of memory for in-memory dense retrieval), and GPUs optimized for a mixture of both training and inference.\nWe call for significant investment in open-source tooling to maintain and evolve both core agentic tools as well as evaluation methods.\nWe recommend computing research infrastructure grants in this area. Funding agencies should support creating such shared resources to ensure everyone can run experiments on a common ground and improve reproducibility.\nRelated to this, we recommend the creation of a non-profit organization for developing open models, open code, and open data for foundation models with applications to information access.\nIR-GenAl is an interdisciplinary research topic that involves researchers from various communities, such as information retrieval, natural language processing, human-computer interaction, machine learning, and broadly artificial intelligence. We recommend funding agencies to provide support for collaborative research as major progress will not happen without the cooperation of various communities within and outside the computing field.\nIn addition to Al-related areas, we recommend programs that address the joint development of IR-GenAl and specialized hardware to support it. GenAl and IR systems' efficiency is constrained by the hardware capabilities upon which they are executed. Direct interaction between researchers developing new hardware and those optimizing GenAl and IR systems would ensure that future Al & IR technologies can fully leverage the next generation of hardware, ensuring short-circuiting the path from hardware ideation to usage."}]}