{"title": "Accurate Prediction of Ligand-Protein Interaction\nAffinities with Fine-Tuned Small Language Models", "authors": ["Ben Fauber"], "abstract": "We describe the accurate prediction of ligand-protein interaction (LPI) affinities, also known as drug-\ntarget interactions (DTI), with instruction fine-tuned pretrained generative small language models (SLMs).\nWe achieved accurate predictions for a range of affinity values associated with ligand-protein interactions\non out-of-sample data in a zero-shot setting. Only the SMILES string of the ligand and the amino acid\nsequence of the protein were used as the model inputs. Our results demonstrate a clear improvement over\nmachine learning (ML) and free-energy perturbation (FEP+) based methods in accurately predicting a\nrange of ligand-protein interaction affinities, which can be leveraged to further accelerate drug discovery\ncampaigns against challenging therapeutic targets.", "sections": [{"title": "Introduction", "content": "Significant advances have been made in the in silico prediction of molecular and pharmacokinetic prop-\nerties associated with successful drug-like molecules (Leeson et al., 2021; Lombardo et al., 2017). These\ncheminformatics advances have laid the foundation for further enhancements in drug candidate screening,\nprioritization for advancement into in vivo studies, and clinical candidate selection (Maurer et al., 2021).\nDespite these impressive improvements in molecular property predictions, a considerable challenge remains\nin accurately predicting the affinity/potency of a ligand-protein interaction (LPI), also known as a drug-target\ninteraction (DTI) (Yamanishi et al., 2008).\nDrugs convey their phenotypic effects through interactions with a variety of biological targets with varying\naffinities (Swinney & Anthony, 2011). Some interactions produce desirable outcomes and phenotypes, while\nothers can create undesired side effects and/or safety risks (Waring et al., 2015). Accurately predicting the\naffinities of ligand-protein interactions would enable drug discovery teams to better design and prioritize the\nsynthesis of molecules that interact with intended protein targets, while minimizing undesired interactions\nwith off-targets like hERG and liver enzymes, ultimately increasing the chances of preclinical success.\n(Sadybekov & Katritch, 2023)."}, {"title": "Our Contribution", "content": "In our work we used pretrained foundational small language models (SLMs), which were generative models\nwith millions of parameters, as starting points. These small foundational models were instruction fine-tuned\non domain-specific data for a few epochs. We evaluated the performance of our instruction fine-tuned\nlanguage models against ground truth data (i.e., out-of-sample \"test\" data) in a zero-shot setting within a\nrigorous and reproducible evaluation framework.\nHerein, we demonstrate accurate prediction for a range of ordinal affinity values associated with ligand-\nprotein interactions on out-of-sample data in a zero-shot setting. Only the SMILES (Simplified Molecular-\nInput Line-Entry System) string (Swanson, 2004; Weininger, 1988) of the ligand and the amino acid sequence\nof the target protein were used as model inputs (Figure 1). Our results demonstrate a clear improvement\nover machine learning (ML) and free-energy perturbation (FEP) based methods in accurately predicting a\nrange of ligand-protein interaction affinities, which can be further leveraged to accelerate drug discovery\ncampaigns against challenging therapeutic targets."}, {"title": "Related Work", "content": "Drug discovery is a challenging multivariate optimization process (Hughes et al., 2011). Ligand-protein,\nor drug-target, interaction affinities are typically assessed by biochemical assays (Macarron et al., 2011),\nbiophysical assays such as nuclear magnetic resonance (NMR) or surface plasmon resonance (SPR) (Renaud\net al., 2016), and in some instances assumed via phenotypic assays (Moffat et al., 2017). These assay results\nare the gold standard by which ligands/drugs are assessed and prioritized for progression in preclinical drug\ndiscovery campaigns."}, {"title": "Machine Learning and Deep Learning", "content": "Several research groups have explored the in silico prediction of ligand-protein interaction affinities with\nstatistical machine learning (ML) algorithms (Oliveira et al., 2024; Kimber et al., 2021; Martin et al., 2019;\nMayr et al., 2018; Martin et al., 2011; Yamanishi et al., 2008; Faulon et al., 2007). Many other groups have\nexplored deep learning (DL) methods (Huang et al., 2020a;b; Li et al., 2020; \u00d6zt\u00fcrk et al., 2019; Whitehead\net al., 2019; Lee et al., 2018; Lenselink et al., 2017; Wen et al., 2017).\nPublished results have typically relied upon small data sets of approximately 10,000 or fewer examples\nwhere ligand-protein interaction affinities were represented as binary values with a \"binder\" represented as a\n1, and \"non-binder\" represented as a 0 in the data sets. Examples of these binary ligand-protein interaction\ndata sets include BioSNAP (Zitnik et al., 2018), DrugBank (Wishart et al., 2007), and the Yamanishi data set\n(Yamanishi et al., 2008).\nBinary data sets and logistic regression methods offer a fruitful landscape for impressive receiver\noperating characteristic (ROC) curves and accuracy values, as predicting logits is a well-formulated machine\nlearning task (James et al., 2013). Yet, in practice ligand-protein binding affinities are a continuum and\nnot binary values. Further, binder/non-binder binary classification is of limited practical value when rank\nordering virtual screening molecules for purchase and in vitro testing, as virtual screening campaigns can\ninclude more than $10^{10}$ compounds (Sadybekov et al., 2021).\nThe commonality of machine learning methods across the LPI prior art has led research groups to focus\non various LPI data representations. Groups have explored vector embeddings of both the ligand (Kimber\net al., 2021) and protein (Kalakoti et al., 2022), including dense and sparse embedding techniques. Recent\nstudies have revealed that the data embedding method plays a minimal role in the accurate prediction of\nligand-protein interaction affinities (Gorantla et al., 2024).\nGraph representations of the ligands and proteins data sets have also been explored. In this setting, nodes\nthat meet user-defined thresholds via inner products and other similarity assessments are connected by edges\n(Svensson et al., 2024; Chatterjee et al., 2023; Thafar et al., 2022; Kimber et al., 2021). Despite these efforts,\nthe accurate in silico prediction and quantification of ligand-protein interactions with machine learning\nand/or deep learning methods remains and unsolved challenge."}, {"title": "Physics-based Methods", "content": "Free-energy perturbation (FEP+) calculations are computationally intensive and low throughput physics-\nbased approaches for rank ordering ligand-protein interactions (Wang et al., 2015). Despite these limitations,\nFEP+ methods are often used to supplement drug discovery campaigns as they allow practitioners to rank\norder LPI affinities of proposed ligands relative to a known benchmark ligand(s)."}, {"title": "Challenges in Biological Data Representations", "content": "Small molecules/ligands in drug discovery interact with their protein targets in a variety of manners (Hughes\net al., 2011). The most common interaction is the ligand burying itself within the core of the protein's\nligand binding pocket, where the protein's native substrate usually resides (Carpenter & Altman, 2024). A\nligand residing in the protein's ligand binding pocket either accelerates or decelerates the protein's standard\nfunction, thereby eliciting the desired biological phenotype. Alternatively, ligands which reside on the\nsurface of proteins, sometimes referred to as allosteric interactions, disrupt protein-protein interactions (PPI)\nand impact the usual function of a protein (Carpenter & Altman, 2024).\nMeaningful data representations of ligand binding pocket and allosteric binding interactions of small\nmolecules with proteins remain a challenge for machine learning practitioners (Figure 3). Namely, the\n3-dimensional topology and plasticity of ligand-protein interactions can be difficult to capture with traditional\nmachine learning data structures such as lists, hash tables, and graphs. Further, it has been shown that a\nsingle atom change on a ligand can not only erode potency, but also lead to completely different mechanisms\nof action on the same protein (Ren\u00e9 et al., 2015).\nModeling complicated multivariate phenomena, such as language or human behavior, via elegant, closed"}, {"title": "Methods", "content": "Several publicly available data sets describe binary ligand-protein interactions where ligands are either\n\u201cbinders\" represented as a 1, or \u201cnon-binders\u201d represented as a 0, of a target protein. Examples of binary\ndata sets include BioSNAP (Zitnik et al., 2018), DrugBank (Wishart et al., 2007), and the Yamanishi data set\n(Yamanishi et al., 2008).\nConversely, the Davis data set describes a range of ligand affinities for proteins with their corresponding\npIC50 affinity values (Davis et al., 2011). The Davis data set contains 6,557 examples with 42 ligands and\n272 protein kinases. The Davis data set has been utilized in LPI prior art, yet this small kinase-focused data\nset offers limited opportunity for generalization.\nA recent release of BindingDB (April 2024 version) contains 2M unique ligand-protein interactions and\ntheir corresponding Ka, Ki, IC50, and/or EC50 affinity value(s) (Gilson et al., 2015). Specifically, this data\nset contains 2,020,737 unique examples with 1,203,453 ligands, and 6,480 proteins. We refer to this data set\nas BindingDB-2M."}, {"title": "Our Data Sets", "content": "We created an additional 1.5M examples of protein-ligand interactions and their corresponding affinity values\nto further expand on the Davis and BindingDB data sets. Our expanded data set was created from all entries\nin the United States National Institutes of Health (NIH) PubChem database (Kim et al., 2015). Our data set\ncuration process is described in the Appendix section.\nWe chose to gather additional data from PubChem as a majority of the entries in the BindingDB data set\noriginated from the ChEMBL database (Zdrazil et al., 2023), and only 4% originated from the PubChem\ndatabase (Figure 4). This difference in data sources offered an opportunity to complement the existing data\nwithin BindingDB with additional data from PubChem.\nOur mining of PubChem to create a new ligand-protein interaction affinity data set resulted in 1,478,702\nunique examples with 927,688 ligands and 4,771 proteins. We refer to our new data set as LPI-1.5M (Table\n1). Our LPI-1.5M data set was also merged with the BindingDB and Davis data sets, then all duplicate\nentries were removed, resulting in a final data set of 3,503,932 examples with 2,130,550 ligands and 6,732\nproteins. We refer to our larger data set as LPI-3.5M. The LPI-1.5M and LPI-3.5M data sets both contained\nthe ligand SMILES string, UNIPROT ID of the protein (Consortium, 2022), amino acid sequence of the\nprotein, and PIC50 affinity value of the each ligand-protein interaction."}, {"title": "Data Formatting", "content": "All pIC50 values, regardless of the data set, were binned into five discrete ordinal affinity values corresponding\na letter of the alphabet: A through E (Figure 5). The ordinal values included: A (pIC50\u2265 8), B (8 > PIC50\n\u2265 7), C (7 > pIC50 \u2265 6), D (6 > pIC50 \u2265 5), and E (5 > pIC50). Our machine learning studies used the\nalphabetical ordinal values, while instruction fine-tuning of pretrained generative small language models\n(SLMs) utilized these same alphabetical values and assigned them onomatopoeia consistent with the language\nof Dr. Seuss (Geisel, 1970)."}, {"title": "Data Sampling", "content": "Following best practices in machine learning, we randomly divided parent data sets into training/fine-tuning\ndata and test data by sampling without replacement. The training/fine-tuning and testing data sets mirrored\ntheir parent data set ordinal affinity value distributions (Figure 5), only differing by \u00b12% at most."}, {"title": "Pretrained Foundational Small Language Models", "content": "We selected the OPT (open pretrained transformer) family of pretrained foundational generative language\nmodels as the starting point for our studies (Zhang et al., 2022). We also explored the GPT-Neo (Black et al.,\n2021) and TinyStories (Eldan & Li, 2023) families of language models. Both OPT-125m and GPT-Neo-125m\ncontained 125M model parameters, whereas TinyStories-28M contained 28M model parameters. All models\nprovided up to 2,048 positional embeddings for their inputs, permitting context for long SMILES strings\nand/or amino acid sequences which can be present in our method.\nIn our work, we defined model fine-tuning as initialization of a pretrained foundational language model\nfollowed by updates to the model weights and biases. In our fine-tuning setting, all language model\nparameters could undergo gradient updates \u2013 there were no frozen layers nor adapters. In our prior work\n(Fauber, 2024), we found the full fine-tuning approach was superior to adapter-based methods like LORA\n(Low-Rank Adaptation) (Hu et al., 2021). Other research groups have since confirmed our initial findings\n(Biderman et al., 2024).\nThe prompt for the language models was consistent throughout our evaluation and across all models.\nThe language model prompt was general and agnostic to the data set instructions. The prompt used for our\nevaluation was: \"Below is an instruction that describes a task. Write a response that appropriately completes\nthe request. ### Instruction: {instruction} ### Response:\"."}, {"title": "Evaluation of Our Method", "content": "We evaluated the performance of our fine-tuned language models on their ability to correctly provide the\nordinal value prediction that exactly matched the ground truth ordinal affinity value in our test data set in a\nzero-shot setting. We also evaluated the ability of our fine-tuned models to correctly predict the exact ordinal\nvalue or \u00b11 ordinal value relative to the ground truth value (e.g., prediction of B when the ground truth was"}, {"title": "Results", "content": "We explored the performance of statistical machine learning (ML) models on our LPI affinity prediction task.\nA training set of 100,000 LPI examples, and their corresponding ordinal affinity values, were drawn from the\nLPI-1.5M data set.\nThe ligand SMILES strings were converted into both MACCS (Molecular ACCess System) fingerprint\nsparse embeddings (Durant et al., 2002) and extended-connectivity \"circular\" fingerprint (ECFP) sparse\nembeddings (Rogers & Hahn, 2010). The protein amino acid sequences were converted into dense em-\nbeddings with the ESM2-3B (Evolutionary Scale Modeling 2) model (Lin et al., 2023). These ligand and\nprotein embedding techniques were selected due to their prevalence and performance in LPI binary affinity\nclassification prior art (Kimber et al., 2021). The ligand and protein embeddings were concatenated, then\nl2-normalized. The same process was applied to a 10,000-example test set from the LPI-1.5M data set. The\ntrain and test data sets were unique with no overlap.\nA support vector machines (SVM) machine learning model was selected for this analysis given its strong\nperformance on imbalanced data sets (Chakrabarti & Fauber, 2022), which are often present in multinomial\nclassification tasks such as ours (Figure 5). A one-versus-rest (OvR) instance of a linear kernel SVM was\nemployed, thus enabling our multinomial classification task. Additional details for our data embedding and\nML methods are described in the Appendix."}, {"title": "Language Model Baseline Performance", "content": "We initially established a baseline for the pretrained foundational small language models on our LPI affinity\nprediction task. All models were incapable of performing our task with any detectable proficiency (Table 2).\nWe recognize that fine-tuning language models over multiple epochs may obliterate some portion of\ninformation that resides within the pretrained foundational language model. This potential change did not\nconcern us as our objective was to create specialized language models from pretrained foundational language\nmodels, with the objective of effectively executing a highly specialized task that the original pretrained\nfoundational models were incapable of performing."}, {"title": "Performance of Our Method", "content": "The OPT-125M pretrained small language model was instruction fine-tuned on 100,000 training examples\ndrawn from the LPI-1.5M data set. We observed a significant improvement in the performance of our\nfine-tuned SLM on our LPI affinity prediction task versus the baseline model on a test set of 10,000 examples\nfrom the LPI-1.5M data set. Our fine-tuned SLM achieved 37% overall accuracy and 37% overall exact\nmatches on our task. Notably, our fine-tuned SLM achieved 14%, 36%, 64%, and 22% exact matches for the\nordinal affinity values B, C, D, and E, respectively (Figure 6). These results were significantly better than the\nML results (Table 2) and baseline language model results (Table 3) on the same train/test data sets."}, {"title": "Influence of Data Set Size on Our Method", "content": "Increasing the number of training examples during instruction fine-tuning of pretrained foundational small\nlanguage models resulted in monotonically increasing performance, as assessed by overall % accuracy and\noverall % exact matches (Table 4). The OPT models were of comparable performance to the GPT-Neo\nmodels on our LPI affinity prediction task. The performance of the TinyStories models were below those of"}, {"title": "Ablation Studies", "content": "We conducted training data ablation studies to assess the importance of both the ligand SMILES string\ninputs and the protein amino acid sequence inputs in the instruction fine-tuning of pretrained small language\nmodels. We utilized 100,000 training examples from our LPI-1.5M data set for this ablation study. The\ntraining examples contained either: 1) ligand and protein inputs; 2) only ligand inputs (i.e., SMILES strings);\nor 3) only protein inputs (i.e., amino acid sequences).\nThree distinct models were created by instruction fine-tuning an OPT-125M pretrained language model\non one of the three training data sets. The resulting three fine-tuned SLMs were then evaluated on our LPI\naffinity prediction task with 10,000 test examples drawn from the LPI-1.5M data set.\nOur analysis revealed that only ligand inputs (34% overall accuracy) and only protein inputs (32% overall\naccuracy) were unable to achieve similar LPI affinity prediction performance to the ligand and protein inputs\n(37% overall accuracy). This led us to conclude that both the ligand and protein inputs were valuable in\neffectively predicting LPI affinities."}, {"title": "Discussion", "content": "Our results demonstrated a clear improvement over ML and FEP+ based approaches in accurately predicting\na range of ligand-protein interaction affinities. Our method also illustrated that powerful specialized models\ncan be created with the instruction fine-tuning of pretrained foundational language models. Our method is\npractical and useful for the in silico evaluation and prioritization of ligands for drug discovery campaigns\nwhether a practitioner chooses to use the % exact match or % near match evaluation criteria.\nWe note that the performance of our method on the A and B ligand-protein interaction ordinal affinity\nvalue predictions was below that of the other ordinal affinity values: C, D, and E (Figures 6 and 8). Yet,\nthese results were anticipated as the ordinal affinity values A and B had low representation in the parent\ndata sets (Figure 5). This low representation of molecules with potent LPI affinity values (e.g., pIC50 \u2265 8)\nrelative to less potent analogs (e.g., pIC50 < 5) likely mirrors the distribution of LPI affinity results found\nin biotechnology and pharmaceutical company databases, as inactive ligands or weak binders are far more\ncommon than potent binders of a target protein. Models learn from their training data, thus inclusion of\nadditional potent LPI pairs might improve the prediction performance on the LPI ordinal affinity classes A\nand B.\nWe explored the performance of our method on test (i.e., out-of-sample or hold-out) data to avoid\ntrain/test data contamination. We view this as a reasonable evaluation framework for our and other LPI\naffinity prediction methods.\nOther research groups have proposed holding out distinct clusters of ligands and classes of proteins from\nthe training/fine-tuning data sets. Thereby, allowing rigorous testing of LPI affinity prediction methods,"}, {"title": "Conclusion", "content": "We have demonstrated that instruction fine-tuned pretrained language models can accurately predict a range\nof ligand-protein affinities. Our results further demonstrated that pretrained foundational language models,\nand their architectures, can serve as general learning frameworks for a novel task of which the base model\nwas incapable of performing.\nOur results illustrated a clear improvement over ML and FEP+ based approaches in accurately predicting\na range of ligand-protein interaction affinities. Our method is practical and useful for the in silico evaluation\nand prioritization of ligands for drug discovery campaigns. Our method can prove valuable whether a\npractitioner chooses to use the % exact match or % near match evaluation criteria. Additionally, our method\nis simple to implement as it only requires the SMILES string of the ligand and amino acid sequence of the\ntarget protein. We demonstrated that our approach can be generalized across many different open-source\npretrained foundational language models.\nSpecifically, we demonstrated that instruction fine-tuning pretrained SLMs with 10,000 to 3.5M examples\nresulted in the accurate prediction of a range of ligand-protein interaction affinities. Increasing the instruction\nfine-tuning examples can impart additional performance improvements in predicting LPI interaction affinities.\nIt is likely that the prediction performance of language model based LPI affinity prediction methods like ours\nwill continue to scale as instruction fine-tuning data sets grow larger."}, {"title": "Acknowledgements", "content": "The author would like to thank Anas Bricha and Neil Cameron for supporting this project. The author would\nalso like to thank Deepayan Chakrabarti, Hans Purkey, and Dan Sutherlin for their insights, and Guy Laporte\nfor providing access to the computational infrastructure to conduct these studies. The author declares no\nfinancial interests nor conflicts."}, {"title": "Appendix", "content": "The results described in this article were carried out using a Dell Technologies PowerEdge C4140 server\nwith 4 x V100 NVIDIA\u00ae SXM GPU cards with 32 GB VRAM each and NVLinkTM connectivity. There\nwere 2 x Intel\u00ae Xeon\u00ae processors on the server with 1.5 TB of CPU RAM.\nThe server was configured with the Ubuntu v22.04 Linux operating system, Anaconda v23.1.0, NVIDIA\u00ae\nCUDA v12.2, and NVIDIA\u00ae drivers v535.54.03. Additional python dependencies included: accelerate\nv0.25.0, biopython v1.83, deepchem v2.7.1, scikit-learn v1.3.0, rdkit v2023.3.3, torch\nv2.1.1, and transformers v4.36.2.\nThe Stanford ALPACA language model code was git cloned directly from https://github.com/\ntatsu-lab/stanford_alpaca (accessed 30Dec2023). The train. py file in the GitHub repo, along with\nour corresponding instruction fine-tuning data set, was used to instruction fine-tune the language models in\nour study. The language model fine-tuning code was executed via the command line interface (CLI).\nAs an example, the following CLI command was used to instruction fine-tune a pretrained foundational\nlanguage model on 4 GPUs:\ntorchrun --nproc_per_node=4 [TRAINING_PY_FILE] \\\n--model_name_or_path [HUGGINGFACE_MODEL_NAME] \\\n--data_path [DATA_PATH_TO_FORMATTED_JSON_FILE] \\\n--bf16 False \\\n--output_dir [OUTPUT_DIRECTORY] \\\n--overwrite_output_dir True \\\n--num_train_epochs 3 \\\n--per_device_train_batch_size 4 \\\n-per_device_eval_batch_size 4 \\\n-gradient_accumulation_steps 8 \\\n-save_strategy \"steps\" \\\n--save_steps 5000 \\\n--save_total_limit 1 \\\n--learning_rate 2e-4 \\\n--weight_decay 0. \\\n--warmup_ratio 0.03 \\\n--lr_scheduler_type \"cosine\" \\\n-seed 41 \\\n--logging_steps 1 \\\n--tf32 False"}, {"title": "Data Set Curation for LPI-1.5M and LPI-3.5M", "content": "We created a data set of 1.5M examples of protein-ligand interactions and their corresponding affinity values\nto further expand on the Davis and BindingDB data sets. Our expanded LPI data set was created from all entries\nin the United States National Institutes of Health (NIH) PubChem database as of 08Feb2024 (Kim et al., 2015).\nAll available assay data was collected from the PubChem site for all compound identification values\n(CIDs), then filtered to those entries which contained either an IC50, EC50, AC50, Ki, or Kd value. If an\nassay did not demonstrate a range of affinity values for different compounds (e.g., all compounds were\ninactive), the assay was omitted from the data set. If a CID contained multiple affinity values for the same\nassay, the mean affinity value was carried forward.\nThe PubChem assay results for LPI affinities were not in uniform units, as some were reported in molar\n(M) concentrations, while others were reported in millimolar (mM) concentrations. If we detected the range\nof LPI affinities were outside of the nanomolar (nM) to micromolar (\u00b5M) range of affinities for a single assay,\nthen those affinity results were normalized to the nM-to-\u00b5M range of affinities for that individual assay.\nThe amino acid sequences associated with the PubChem assay results were mined from the NIH Entrez\nMolecular Sequence Database System using the UNIPROT ID of the assay target protein as the retrieval key\n(Consortium, 2022). Our mining of PubChem to create a new ligand-protein interaction affinity data set\nresulted in 1,478,702 unique examples with 927,688 ligands and 4,771 proteins. We refer to this data set as\nLPI-1.5M. In this data set, the average length of ligand SMILES strings was 68 characters, and the average\nlength of protein amino acid sequences was 667 characters.\nOur LPI-1.5M data set was also merged with the BindingDB (April 2024 version) and Davis data sets,\nthen all duplicate entries were removed, resulting in a final data set of 3,503,932 examples with 2,130,550\nligands and 6,732 proteins. We refer to this larger data set as LPI-3.5M. The LPI-1.5M and LPI-3.5M data\nsets both contained the ligand SMILES string, UNIPROT ID of the protein, amino acid sequence of the\nprotein, and PIC50 affinity value of the each ligand-protein interaction.\nAll pIC50 values, regardless of the data set, were binned into five discrete ordinal values corresponding a\nletter of the alphabet: A through E (Figure 5). The ordinal values included: A (pIC50\u2265 8), B (8 > PIC50 \u2265 7),\nC (7 > pIC50 \u2265 6), D (6 > pIC50 \u2265 5), E (5 > pIC50). Our machine learning studies used the alphabetical\nordinal values, while the instruction fine-tuned small foundational pretrained generative language models\n(SLMs) utilized these same alphabetical values and assigned them onomatopoeia consistent with the language\nof Dr. Seuss (Geisel, 1970).\nThe A through E onomatopoeia utilized for the instruction fine-tuning of the pretrained foundational\nsmall language models were A \u2192 \"achoo,\" B \u2192 \"blurpblurp,\" C \u2192 \"choochoo,\" D \u2192 \"dibbledopp,\" E\n\u2192 \"eekeek.\" We chose to encode the target predictions as onomatopoeia given that generative language\nmodels predict the next most likely token in a sequence, and we wished for the target predictions to be\nsemantically distinct from the inputs. We verified their semantic differences by computing and comparing\nthe inner products of the mean-pooled penultimate OPT-125M model layer outputs for the various tokenized\nonomatopoeia."}, {"title": "Machine Learning Data Representations, Models, and Metrics", "content": "We explored the performance of statistical machine learning (ML) models on our LPI affinity prediction task.\nA training set of 100,000 LPI examples, and their corresponding ordinal affinity values, were drawn from the\nLPI-1.5M data set.\nThe ligand SMILES strings were converted into both MACCS (Molecular ACCess System) fingerprint\nsparse embeddings (Durant et al., 2002) and extended-connectivity \"circular\" fingerprint (ECFP) sparse\nembeddings (Rogers & Hahn, 2010). The RDKit python toolkit with default settings was used to generate\nboth embeddings. The 167-dimensional MACCS embeddings of the ligands were 67% sparse on average,\nand the 2,048-dimensional ECFP embeddings of the ligands were 97% sparse on average with this training\ndata set.\nThe protein amino acid sequences were converted into dense embeddings with the ESM2-3B model\n(Lin et al., 2023). The \"esm2_t36_3B_UR50D\" instance of the ESM2-3B models was used with the default\nparameters. To generate dense embeddings of the protein amino acid sequences, the amino acid sequence\nwas tokenized with the ESM2-3B model's tokenizer. The tokenzied sequence was then sent to the ESM2-3B\nmodel and the penultimate layer of the model output, but for the first and final columns of the output, was\nsubjected to a column-wise mean-pooling operation to provide a 2,560-dimensional dense embedding of the\ninitial protein amino acid sequence input.\nThe ligand and protein embeddings were concatenated along the same axis in that order, then 12-\nnormalized. The same process was applied to a 10,000-example test set from the LPI-1.5M data set. As\nnoted earlier, the train and test data sets were unique with no overlap.\nA support vector machines (SVM) machine learning model was selected for this analysis given its strong\nperformance on imbalanced data sets (Chakrabarti & Fauber, 2022), which are often present in multinomial\nclassification tasks such as ours (Figure 5). A one-versus-rest (OvR) instance of a linear kernel SVM was\nemployed, thus enabling our multinomial classification task. Both the linear SVM and OvR instances were\nused with their default hyperparameters/settings.\nModel performance was assessed using the python scikit-learn \"accuracy_score()\" and \"classifica-\ntion_report()\" modules. The percentage of correctly classified instances per class (i.e., % exact matches for\neach ordinal affinity value) were scored via the per-class recall metric."}, {"title": "Language Model Text Generation Configuration", "content": "The same language model fine-tuning and generation configurations were utilized throughout our studies,\nand only single-parameter changes were permitted, as annotated in the tables, when comparing methods.\nLanguage model text generation was conducted via the HuggingFace transformers library. Transformers\nGenerationConfig() was set to the default parameters, along with:\n\u2022 num_beams = 2,\n\u2022\n\u2022 repetition_penalty = 1.3,\n\u2022 do_sample = False (for consistent output generation),\n\u2022 early_stopping = True,\n\u2022 max_time = 10, and\n\u2022 length_penalty = 0.4.\nIn prior studies, we found the above configuration parameters provided stable and reproducible text\ngeneration (Fauber, 2024). The text generation prompt and the general prompt used in the language model\nfine-tuning process were identical:\n\"Below is an instruction that describes a task. Write a response that appropriately\ncompletes the request. ### Instruction: {instruction} ### Response: {output}\".\nAlthough not always necessary, we enforced truncation of the output text for all models to ensure\nconsistency in outcomes. Truncation of the OPT model text output returned all text following the \"###\nResponse:\" string. Similarly, the GPT-Neo and TinyStories families of models truncated the output to the\ntext following the \"<|endoftext|>\" string."}, {"title": "LPI Affinity Predictions for BindingDB-2M data set", "content": "A pretrained OPT-125M language model was instruction fine-tuned on our LPI affinity prediction task\nwith either 10,000, 100,000, or 1,000,000 training examples from the BindingDB-2M data set to create\nthree distinct models. The prediction performance of the three fine-tuned models were assessed with a\n10,000-example test set drawn from the BindingDB-2M data set. The model outputs were compared to their\nground truth for scoring.\nWe noted that increasing the number of training/fine-tuning examples increased the % exact matches for\nmost LPI ordinal affinity values (Figure A1). Yet, we also noted that the performance of the three different\ninstruction fine-tuned SLMs did not mirror the distribution of the parent BindingDB-2M data set (Figure\n5). Rather, the affinity prediction results for each LPI ordinal affinity value resulted in an overall bimodal\ndistribution (Figure A1). The reasons for the observed bimodal distribution in LPI affinity predictions were\nunclear, but they were consistent outcomes with all three models that were fine-tuned on the BindingDB-2M\ndata set."}]}