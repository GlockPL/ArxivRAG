{"title": "Beyond Models! Explainable Data Valuation and Metric Adaption for Recommendation", "authors": ["Renqi Jia", "Xiaokun Zhang", "Bowei He", "Qiannan Zhu", "Weitao Xu", "Jiehao Chen", "Chen Ma"], "abstract": "User behavior records serve as the foundation for recommender systems. While the behavior data exhibits ease of acquisition, it often suffers from varying quality. Current methods employ data valuation to discern high-quality data from low-quality data. However, they tend to employ black-box design, lacking transparency and interpretability. Besides, they are typically tailored to specific evaluation metrics, leading to limited generality across various tasks. To overcome these issues, we propose an explainable and versatile framework DVR which can enhance the efficiency of data utilization tailored to any requirements of the model architectures and evaluation metrics. For explainable data valuation, a data valuator is presented to evaluate the data quality via calculating its Shapley value from the game-theoretic perspective, ensuring robust mathematical properties and reliability. In order to accommodate various evaluation metrics, including differentiable and non-differentiable ones, a metric adapter is devised based on reinforcement learning, where a metric is treated as the reinforcement reward that guides model optimization. Extensive experiments conducted on various benchmarks verify that our framework can improve the performance of current recommendation algorithms on various metrics including ranking accuracy, diversity, and fairness. Specifically, our framework achieves up to 34.7% improvements over existing methods in terms of representative NDCG metric. The code is available at https://github.com/renqii/DVR.", "sections": [{"title": "1 Introduction", "content": "With the vast amount of online information, Internet users are constantly exposed to an ever-growing number of online products or services. This abundance makes it challenging for individuals to find items that truly align with their interests. To solve the problem of information overload, personalized recommendation systems have emerged, playing a pivotal role in helping users navigate through the vast choices in modern society [19, 20, 32, 31].\nCurrent recommender systems usually prioritize the development of intricate model structures like RNN [14], Attention [30], Transformer [23], and GNN [12]. With delicate model designs, these methods typically resort to Bayesian Personalized Ranking (BPR) to train their models on users' behavior data. BPR generates training samples by negative sampling [22], constructing triplets {user, positive item, negative item} to distinguish the positive items from the negative ones. The quality of these training samples plays a crucial role in the BPR training procedure, directly impacting the effectiveness of the recommendation model. However, most methods overlook data quality as they use the vanilla BPR training procedure, which penalizes all samples equally without accounting for distinguishing the distinct effects of various training instances.\nA common case of fluctuating data quality arises from the negative sampling process, where the sampled items may include both true negative items and false negatives. For instance, in a movie recommendation system where a user prefers action movies, sampling true negative items like art films leads to higher-quality data over false ones sampled as war movies. Consequently, treating all training samples equally in this context can lead to sub-optimal representation learning, ultimately undermining the model's performance.\nTo differentiate high-quality data from low-quality data, some data-valuation methods for recommendation have been proposed [25, 21, 8, 18, 28]. These methods typically resort to some pre-defined heuristic rules to identify the data quality, such as the prediction scores, popularity levels, or loss values. While these approaches have demonstrated promising performance, we identify two main limitations:\n\u2022 Lacking interpretability. Current methods em-"}, {"title": "2 Related Work", "content": "2.1 Data valuation Originating from game theory, many data valuation methods based on Shapley value have been proposed. Data Shapley [9] is commonly used for feature attribution tasks, where the prediction performance of all possible subsets is considered to compute the marginal improvement in performance as the data value. However, this method decouples data valuation from predictor model training, limiting their overall performance due to the lack of joint optimization. Different from the prior works, DVRL [29] directly models the data values using learnable neural networks. For training the data value estimator, DVRL utilizes a reinforcement learning approach coupled with a sampling process. DVRL demonstrates model-agnostic behavior and can be applied even to non-differentiable target objectives. Notably, the learning process is conducted jointly for the data value estimator and the associated predictor model, leading to exceptional outcomes across all considered use cases. To efficiently compute the Shapley value of input variables in a deep learning model, HarsanyiNet utilizes the intermediate neurons the network to capture Harsanyi Interaction [4], which allows"}, {"title": "3 Problem Formulation", "content": "In this paper, the recommendation task takes the user behavior data as input. Let U and V be sets of users and items respectively, where |U| = m, and |V| = n. We use the index u \u2208 U to denote a user and i \u2208 V to denote an item. The user-item rating matrix is denoted as R = $[r_{ui}]_{m \\times n} \\in R^{m \\times n}$ to indicate whether user u has interacted with item i, where $r_{ui}$ = 1 represents user u has interacted with item i, whereas $r_{ui}$ = 0 represents user u has not interacted with item i. We use $V^+_u = \\{i \\in V | r_{ui} = 1\\}$ to represent a set of items that user u has interacted with. $V_u$ can be splited into a training set $S_u^t$ and a testing set $T_u$, requiring that $S_u^t \\cup T_u = V$ and $S_u^t \\cap T_u = \\emptyset$. It worth noted that $S_u^- = \\{i | r_{ui} = 0, i \\in I\\}$, which means $S_u^-$ consists of the negative items that user u have not interacted with. The training set is denoted as $D = \\{(u,i,j) | u \\in U, i \\in S_u^+, j \\in S_u^-\\}$. The testing set is denoted as $D_t = \\{(u, i) | u \\in U, i \\in T_u\\}$.\nIn the recommendation task, the model aims to recommend a list of k items $X_u$ for the user u, which matches the condition $X_u \\cap S_u^t = \\emptyset$. By comparing the recommendation list $X_u$ with the testing set $T_u$, we evaluate the recommendation quality from various perspectives, including accuracy, diversity, and fairness."}, {"title": "4 Preliminary", "content": "For optimization of the recommendation model, the Bayesian Personalized Ranking (BPR) is used to learn the user preference from behavior data. The central idea of BPR is to maximize the ranking of positive items compared with the randomly sampled negative ones, achieved by the following loss function:\n(4.1) $L_{BPR}(u, i, j; \\Theta) = -log(\\sigma(f(u, i; \\Theta) - f(u, j; \\Theta)))$,\nwhere (u,i,j) is the training sample with a positive item i and a negative item j for user u. $f(\\Theta)$ refers to the recommendation model. The learnable parameter $\\Theta$ includes the user embedding $p_u \\in R^d$ and the item embedding $q_i \\in R^d$, where d is the embedding dimension. $f(u, i; \\Theta)$ is used to compute the relevance score between user u and item i."}, {"title": "5 Methodology", "content": "In this section, we first introduce the data valuator for explainable data valuation that evaluates the data quality by calculating the Shapley value from the game-theoretic perspective. Then, we present the metric adapter that achieves metric adaption for both differentiable and non-differentiable metrics by reinforcement learning in an end-to-end manner."}, {"title": "5.1 Data Valuator", "content": "Our intuition for explainable data valuation is that a sample's data quality is related to its impact on model performance. That is, samples that contribute more to model performance are considered high-quality data, whereas those with less impact are deemed low-quality data. Therefore, the sample's contribution to the model performance can be viewed as the explanation of the data valuation.\nTo incorporate the above intuition, we propose to measure the contribution of each training sample (u, i, j) to the model performance as the data value $W_{uij}$. As samples work together to impact the model's performance collectively, we contend that considering the training sample collectively is better than measuring them in isolation. Based on game theory, the Shapley value [27] is a popular technique that computes the players' contribution to the outcome by the average marginal contribution considering all possible coalitions. Therefore, we treat each sample as the player cooperating with others and contributing to model performance.\nGiven the large volume of training samples in the recommender system, the computational complexity associated with computing all potential sample coalitions with the original Shapley value is prohibitively high. To reduce computational complexity, we focus on computing the Shapley value of samples within the training batch data, which is represented as B = $\\{(u_1, i_1, j_1), (u_2, i_2, j_2), ..., (u_{n_b}, i_{n_b}, j_{n_b})\\}$. The Shapley value for sample $h_m = (u_m, i_m, j_m) \\in B$ is formulated as:"}, {"title": "5.1 Data Valuator", "content": "Our intuition for explainable data valuation is that a sample's data quality is related to its impact on model performance. That is, samples that contribute more to model performance are considered high-quality data, whereas those with less impact are deemed low-quality data. Therefore, the sample's contribution to the model performance can be viewed as the explanation of the data valuation.\nTo incorporate the above intuition, we propose to measure the contribution of each training sample (u, i, j) to the model performance as the data value $W_{uij}$. As samples work together to impact the model's performance collectively, we contend that considering the training sample collectively is better than measuring them in isolation. Based on game theory, the Shapley value [27] is a popular technique that computes the players' contribution to the outcome by the average marginal contribution considering all possible coalitions. Therefore, we treat each sample as the player cooperating with others and contributing to model performance.\nGiven the large volume of training samples in the recommender system, the computational complexity associated with computing all potential sample coalitions with the original Shapley value is prohibitively high. To reduce computational complexity, we focus on computing the Shapley value of samples within the training batch data, which is represented as B = $\\{(u_1, i_1, j_1), (u_2, i_2, j_2), ..., (u_{n_b}, i_{n_b}, j_{n_b})\\}$. The Shapley value for sample $h_m = (u_m, i_m, j_m) \\in B$ is formulated as:\n(5.2)\n$\\phi(h_m) = \\sum_{S \\subseteq B \\setminus \\{h_m\\}} \\frac{|S|! (n_b - |S| - 1)!}{n_b!} [v(S \\cup \\{h_m\\}) - v(S)]$,\nwhere v(S) is the value function that computes the model performance with the sample set S as the input. Many data valuation methods [9] utilize retraining techniques to calculate v(S). However, due to the large computational consumption, it is challenging to apply the retraining-based methods to the scenario of the recommender system.\nFor the scenario of the recommender system, we employ a customized DNN consisting of Harsanyi Interaction to compute the Shapley value effectively. Specifically, Harsanyi interaction S represents the sample coalition contributing to the model performance collectively. Each Harsanyi interaction S makes a specific numerical contribution, denoted by I(S), to the model performance. Based on prior research [10], the Shapley value is proved to be calculated as:\n(5.3)\n$\\phi(h_m) = \\sum_{S \\subseteq B: S \\ni h_m} I(S)$.\nThe customized DNN is specified as $\\omega_B(\\Lambda) = G(B; \\Lambda)$, where $\\Lambda$ is a set of learnable parameters."}, {"title": "5.1.1 Sample Encoder", "content": "We first employ the sample encoder to encode each sample within the training batch to obtain the sample representation. For the training sample $h_m \\in B$, we use the following network get the representation of $h_m$:\n(5.4)\n$\\bar{e}_m = ReLU(W_1 e_m + b_1)$,\n$h_m = Tanh(W_2 \\bar{e}_m + b_2)$,\nwhere the input $e_m = [p_{u_m};q_{i_m};q_{j_m}] \\in R^{3d}$ is the concatenation of the representation of user $u_m$, item $i_m$, and item $j_m$. $W_1 \\in R^{d \\times 3d}$, $W_2 \\in R^d$, $b_1 \\in R^d$, and $b_2 \\in R$. We take $h_m$ as the sample representation."}, {"title": "5.1.2 Batch Valuator", "content": "To compute the contribution of each training sample to the model performance, the batch valuator is achieved by a customized DNN, which takes batch samples as input to predict the model performance.\nReferring to HarsanyiNet [4], the customized DNN is designed by considering the AND relationship between the children nodes of the neuron. Since the customized DNN contains L stacked Harsanyi blocks, we focus on the m-th neuron in the l-th block. Given the children set $C_m^{(l)}$, the neural activation $z_m^{(l)}$ of the neuron (l, m) is computed by applying the AND operation on"}, {"title": "the Linear network:", "content": "(5.5)\n$z_m^{(l)} = A_m^{(l)} \\cdot (\\sum_{(l,m') \\in C_m^{(l)}} v_{lm'}^{(l-1)})$,\\\n$v_{lm}^{(l)} = ReLU(z_m^{(l)} \\cdot \\prod I(z_{m'}^{(l)} \\neq 0))$.\n$(l, m') \\in C_m^{(l)}$\nThe input of the l-th block are neurons from the last block, denoted as $z^{(l-1)} = [z_1^{(l-1)}, ..., z_m^{(l-1)}, ..., z_{M^{(l-1)}}^{(l-1)}]$, where $M^{(l-1)}$ is the number of neurons in the (l \u2013 1)-th block. Especially, for the first block, the input neurons are $z^{(0)} = [h_1, ..., h_m, ...h_{n_B}]$. The children set $C_m^{(l)}$ is implemented as a trainable binary diagonal matrix $\\Sigma^{(l)} \\in \\{0,1\\}^{M^{(l-1)} \\times M^{(l-1)}}$, which selects children nodes of the neuron (l, m) from the neurons in the last blocks. $A_m^{(l)} \\in R^{M^{(l-1)}}$ denotes the weight vector."}, {"title": "5.1.3 Prediction and Optimization", "content": "Neurons in multiple layers of blocks are used to predict the model performance:\n(5.6)\n$\\hat{y} = \\sum_{l=1}^{L} (v^{(l)})^T z^{(l)}$,\nwhere $v^{(l)} = [v_1^{(l)}, ..., v_{M^{(l)}}^{(l)}] \\in R^{M^{(l)}}$ denotes the weight vector. $z^{(l)} = [z_1^{(l)}, ..., z_{M^{(l)}}^{(l)}]$ denotes neurons in the l-th block. $\\hat{y}$ is the prediction value of model performance. We use batch loss to represent the model performance. Then, the model is trained by the mean squared error of predicted value $\\hat{y}$ and the batch BPR loss. The loss function is denoted as:\n(5.7) $L_{MSE}(B; \\Lambda) = (y - \\sum_{(u,i,j) \\in B} L_{BPR}(u, i, j; \\Theta))^2$."}, {"title": "5.1.4 Calculation of Shapley Value", "content": "As proved in the prior method [4], through the Harsanyi Interaction represented by neurons in the customized DNN, we implement the formula 5.3 to compute Shapley value $\\phi(h_m)$ as:\n(5.8)\n$\\hat{C}_m^{(l)} := \\cup_{U^{(l,m)} \\in c_m^{(l)}} U^{(l,m)}, s.t. c_m^{(l)} := C_m^{(l)}$,\n$\\phi(h_m) = \\sum_{l=1}^{L} \\sum_{m=1}^{M^{(l)}} \\frac{1}{m}v_m^{(l)} \\sum_{U^{(l,m)} \\in \\hat{C}_m^{(l)}} I(h_m)$."}, {"title": "5.2 Metric Adapter", "content": "5.2.1 The Bilevel Optimization We utilize the assigned Shapley value for training sample selection. For the training batch B, the probability of sampling is denoted as $\\pi(B, s; \\Lambda) = \\prod_{B} [\\omega_{uij}(\\Lambda)^{s_{uij}} \\cdot (1 - \\omega_{uij}(\\Lambda))^{1-s_{uij}}]$. In order to calculate the sampling probability, we normalize the Shapley value as $\\bar{\\omega}_{uij}$.\nThe selection vector is dentoed as s = $[s_{uij}]_{|B|}$ where $s_{uij}$ is the parameterized Bernoulli variable with the probability $\\bar{\\omega}_{uij}$ to be 1 and 1 - $\\bar{\\omega}_{uij}$ to be 0. If $s_{uij}$ = 1/0, the sample (u, i, j) is selected/not selected for training the recommendation model.\nAfter data selection, we use the selected data to train the recommendation model and data valuator in a bilevel manner. The bi-level optimization is formulated as:\n(5.9)\n$\\min_{\\Lambda} E_{s \\sim \\pi(B, \\cdot; \\Lambda)} R(D, f(\\Theta^*(\\Lambda))) + L_{MSE}(B; \\Lambda)$\ns.t.${\\Theta}^*(\\Lambda) = arg\\min_{\\Theta} E_{s \\sim \\pi(B, \\cdot; \\Lambda)} L_{BPR}(u, i, j; \\Theta)$.\nThe recommendation evaluator R(D, f(${\\Theta}^*(\\Lambda)$)) means that R(.) takes the test data D and recommendation model f(${\\Theta}^*(\\Lambda)$) as input to measure the metric R. In the outer optimization, the data valuator G(B; $\\Lambda$) is optimized by the performance metric and the MSE loss. In the inner optimization, the recommendation model is optimized by the BPR loss of selected training samples."}, {"title": "5.2.2 Reinforced Metric Adaption", "content": "The sampled BPR loss and MSE loss can be optimized with backprop gradients [1] because they are differentiable. The challenge arises from the fact that many metrics are non-differentiable, which poses difficulties for optimization of the metric in the outer optimization of Eq. (5.9):\n(5.10)\n$\\bar{L}(\\Lambda) = E_{s \\sim \\pi(B, \\cdot; \\Lambda)}R(D, f(\\Theta^*(\\Lambda)))$.\nTo handle this situation, we propose to use REIN-FORCE algorithm to optimize non-differentiable metrics. The optimization gradient $\\nabla_{\\Lambda}\\bar{L}(\\Lambda)$ can be computed directly as:\n(5.11)\n$\\nabla_{\\Lambda}\\bar{L}(\\Lambda) = E_{s \\sim \\pi(B, \\cdot; \\Lambda)}[R(D, f(\\Theta^*(\\Lambda))) \\cdot \\nabla_{\\Lambda}log(\\pi(B, s; \\Lambda))]$,\nwhere $R(D, f(\\Theta^*(\\Lambda))) \\nabla_{\\Lambda}log(\\pi(B, s; \\Lambda))$ is the policy gradient of $\\nabla_{\\Lambda}\\bar{L}(\\Lambda)$, which can gudide the data valuator to identify samples that are beneficial for the metric. In the Eq. 5.11, $\\nabla_{\\Lambda}log(\\pi(B, s; \\Lambda))$ can be further computed as:\n(5.12)\n$\\nabla_{\\Lambda}log(\\pi(B, s; \\Lambda))$\n= $\\nabla_{\\Lambda} \\sum_{(u,i,j) \\in B} log(\\omega_{uij}(\\Lambda)^{s_{uij}} \\cdot (1 - \\omega_{uij}(\\Lambda))^{1-s_{uij}})$\n$\\nabla_{\\Lambda} \\sum_{(u,i,j) \\in B} s_{uij}\\nabla_{\\Lambda}log (\\omega_{uij}(\\Lambda)) + (1 - s_{uij})\\nabla_{\\Lambda}log(1 - \\omega_{uij}(\\Lambda))$.\nTherefore, given the optimum ${\\Theta}^*(\\Lambda)$ of the recommendation model, we can update the parameter $\\Lambda$ of the data valuator as:\n(5.13)\n$\\Lambda \\leftarrow \\Lambda - \\eta[R(D, f(\\Theta^*(\\Lambda))) \\nabla_{\\Lambda}log(\\pi(B, s; \\Lambda))]$."}, {"title": "6 Experiment", "content": "We compare our framework on four public datasets with various base models to sate-of-the-art baselines from the aspects of accuracy, diversity, and fairness."}, {"title": "6.1 Datasets", "content": "We use four real-world datasets in our experiments. Table 1 provides an overview of the data statistics. Beauty and CD are both product recommendation datasets adopted from the Amazon Review Dataset [11]. It covers users' purchases over the category of All Beauty and CDs and Vinyl with rating score. LastFM was collected from the Last.fm music social platform [3], covering a large amount of users' music listening activities and metadata from the period between 2005 and 2009. Gowalla was collected worldwide from the Gowalla website [6], which is a location-based social networking website over the period from February 2009 to October 2010.\nWhen dealing with datasets originally containing explicit ratings, we consider any ratings equal to or greater than four (on a scale of five) as positive feedback and all other ratings as missing entries to maintain consistency within the implicit feedback setting. Furthermore, to ensure data quality and reliability, we follow the common practice used in prior works to filter out users and items with fewer than ten ratings."}, {"title": "6.2 Evaluation Protocols", "content": "We utilize cross-validation to evaluate our proposed model. The user-item interactions are divided into three sets: training set, validation set, and testing set, with a ratio of 8:1:1, respectively. We evaluate our model from three aspects: accuracy, diversity, and fairness. With respect to accuracy, Recall and Normalized Dis-"}, {"title": "6.3 Methods Studied", "content": "To show the compatibility of our method, we apply the DVR framework on four recommendation backbones, i.e., BRPMF [16], NeuMF [13], MGCF [24], and LightGCN [12]. Based on these backbones, our DVR framework can optimize diverse metrics, where ranking accuracy is denoted as DVR-Loss, DVR-Recall, and DVR-NDCG, diversity and fairness are labeled as DVR-CC, DVR-ILD, and DVR-Gini. Besides, we compare our framework with various data valuation methods for recommendations including BPR [22], AOBPR [21], WBPR [8], PRIS [18], TCE-BPR, RCE-BPR[25], TIL-UI and TIL-MI [28]."}, {"title": "6.4 Performance Comparison", "content": "6.4.1 Comparison of Accuracy As shown in Table 2, we compare the accuracy performance of DVR-Loss, DVR-Recall, and DVR-NDCG with several baseline approaches. The following is observations about the results: (i) Our framework outperforms most baseline methods across diverse evaluation metrics and consistently maintains this superiority across multiple datasets. This shoes the ability of our data valuator to accurately assess the quality of user behavior data, enhancing the efficacy of the recommendation model. (ii) Compared to TCE-BPR and RCE-BPR, our framework consistently outperforms them. These approaches assume that noise only comes from positive samples and treat all negative items equally, which is suboptimal. Moreover, relying solely on loss values for weighting positive samples can reinforce errors. (iii) Our framework outperforms AOBPR, WBPR, and PRIS. One factor is that these methods rely solely on negative samples to assess data quality. They select or adjust the weight of"}, {"title": "6.4.2 Comparison of Diversity", "content": "We compared DVR-CC and DVR-ILD with the BPR model on the Beauty dataset based on BPRMF and MGCF back-bones. The performance results are presented in Table 3. Key observations from the experiments include: (i) DVR-CC and DVR-ILD achieved the best performance for CC@20 and ILD@20 metrics, respectively, as they are directly optimized for diversity metrics. This allows the data valuator to adjust data values for improved training of the recommendation model towards higher diversity. (ii) An interesting finding is that the Recall@20 and NDCG@20 of DVR-CC and DVR-ILD show no significant decrease compared to the BPR model, demonstrating the robustness of our reinforcement learning framework. This is attributed to our approach of enhancing recommendation model optimization by adjusting data values instead of directly"}, {"title": "6.4.3 Comparison of Fairness", "content": "The performance comparison of item fairness is displayed in Table 4. We compare DVR-Gini with the BPR model based on BPRMF and LightGCN backbones. From the experimental findings, we draw the following conclusions: (i) DVR-Gini consistently outperforms the BPR model in terms of the Gini metric, as DVR-Gini utilizes Gini as the reward in its reinforcement learning-based optimization approach. By leveraging policy gradients learned from the Gini metric, the data valuator improves the fairness performance. (ii) The Recall@20 and NDCG@20 of DVR-Gini exhibit strong performance compared to the BPR model, showcasing the robustness of our reinforcement learning framework. This resilience"}, {"title": "6.4.4 Case Study of Shapley value", "content": "To verify the interpretability of the assigned Shapley value, we conduct a case study of the BPR-Loss model on the Beauty and LastFM datasets. Figure 2 shows the average cosine similarity between the Shapley value and the negative BPR loss of the batch data in every two epochs. Our analysis revealed a progressive increase in cosine similarity throughout the training process. Notably, the Beauty and LastFM datasets exhibited distinct initial values, peak values, and growth rates. These variations could be attributed to differences in dataset size and density, resulting in varying convergence speeds."}, {"title": "7 Conclusion", "content": "We introduce an interpretable and adaptable recommendation framework to enhance data utilization and meet the specific requirements of model architectures and evaluation metrics. Our approach includes a data valuator to assess data quality through the calculation of Shapley value, ensuring robust mathematical properties. Additionally, we develop a metric adapter based on reinforcement learning to accommodate various evaluation metrics, including both differentiable and non-differentiable. Our extensive experiments on real-world datasets show that our framework outperforms state-of-the-art methods, achieving notable improvements in accuracy, diversity, and fairness metrics."}]}