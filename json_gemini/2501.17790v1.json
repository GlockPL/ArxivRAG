{"title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced\nPolyphone Disambiguation \u2013 Challenges and Insights", "authors": ["Chan-Jan Hsu", "Yi-Cheng Lin", "Chia-Chun Lin", "Wei-Chih Chen", "Ho Lam Chung", "Chen-An Li", "Yi-Chang Chen", "Chien-Yu Yu", "Ming-Ji Lee", "Chien-Cheng Chen", "Ru-Heng Huang", "Hung-yi Lee", "Da-Shan Shiu"], "abstract": "We present BreezyVoice, a Text-to-Speech\n(TTS) system specifically adapted for Tai-\nwanese Mandarin, highlighting phonetic con-\ntrol abilities to address the unique challenges\nof polyphone disambiguation in the language.\nBuilding upon CosyVoice, we incorporate a\nS\u00b3 tokenizer, a large language model (LLM),\nan optimal-transport conditional flow matching\nmodel (OT-CFM), and a grapheme to phoneme\nprediction model, to generate realistic speech\nthat closely mimics human utterances. Our\nevaluation demonstrates Breezy Voice's supe-\nrior performance in both general and code-\nswitching contexts, highlighting its robustness\nand effectiveness in generating high-fidelity\nspeech. Additionally, we address the chal-\nlenges of generalizability in modeling long-tail\nspeakers and polyphone disambiguation. Our\napproach significantly enhances performance\nand offers valuable insights into the workings\nof neural codec TTS systems.", "sections": [{"title": "1 Introduction", "content": "Text-to-Speech (TTS) technology has been widely\nadopted in customer service and accessible tools.\nRecently, the significant advancements in virtual as-\nsistants (Hurst et al., 2024) increased the frequency\nof breadth of TTS utilization, which has prompted\na shift away from the mechanical voice outputs to-\nwards speech that closely mimics the nuances of\nhuman conversation. Aside from immediate practi-\ncal scenarios, we also highlight the emerging use\ncase of synthetic data generation, as it offers exten-\nsive coverage for spontaneous speech in control-\nlable written domains. The benefits of using these\notherwise inaccessible synthetic data for training\npurposes have been well-documented in previous\nliterature (Lin et al., 2022; Yang et al., 2024).\nIn response to the increasing expectations for\nhigh-quality outcomes, it is particularly advan-\ntageous to address localized TTS needs, from\nadaptation of an existing system, leveraging the\nknowledge transfer potential from pretrained large-\nscale corpora. Recent realistic TTS systems have\nreported training data of approximately 170,000\nhours (Du et al., 2024a), which is comparable to\nthe scale of training large ASR systems (Radford\net al., 2023), where adaptation efforts for special-\nization are common (Tseng et al., 2024).\nIn this work, we build upon Cosy Voice to de-\nvelop a Taiwanese Mandarin TTS system, which\nwe name as BreezyVoice. Breezy Voice demon-\nstrates superior performance compared to com-\nmercial TTS systems, both in general and code-\nswitching contexts. Beyond the training process,\nwe tackle generalizability challenges in the adapted\nsystem from lower-resourced training, primarily\ninvolving the correct modeling of long-tail speaker\ncharacteristics and the polyphonic characters in-\nherent in the Taiwanese Mandarin language. Our\ndetailed analysis provide valuable insights into the\nworkings of neural codec TTS systems."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Unit-TTS Systems", "content": "Advanced speech compression codec systems (D\u00e9-\nfossez et al., 2022; Du et al., 2024b) have success-\nfully encapsulated verbose speech information into\nquantized speech units. These units are seamlessly\nadopted for pursuing Text-to-Speech (TTS) sys-\ntems (Ju et al., 2024), via incorporating a module\nthat predicts the compressed units from text along\nwith an optional speaker embedding. The design\nof the text-to-unit model is highly dependent on\nthe granularity of the units to predict. For example,\nmulti-layered codes are often predicted recursively\nusing a non-autoregressive model (Wang et al.,\n2023; Chen et al., 2024), whereas single-layered\ncodes are structured as a sequence-to-sequence gen-\neration task (Du et al., 2024a). From this perspec-\ntive, we identified the benefits of single-layered su-"}, {"title": "2.2 The Necessity of Taiwanese Mandarin\nConversion", "content": "The default language of the Cosyvoice model is\nMandarin, which is mutually intelligible with Tai-\nwanese Mandarin, but also features various lexi-\ncal and phonological differences (Bradley, 1992).\nFurther, Taiwanese Mandarin employs Traditional\nChinese characters as the surface form, which sig-\nnificantly overlap with Cantonese in written form\nbut differ greatly in pronunciation, and there have\nbeen scholars that regard them as separate lan-\nguages (Mair, 1991). As the CosyVoice model\npredominantly associates Traditional Chinese char-\nacters with Cantonese, it is crucial to ensure that\nthe model is adequately trained with Taiwanese\nMandarin data to guarantee performance on this\ndomain."}, {"title": "2.3 Polyphone Disambiguation and Phonetic\nControl for Boosting TTS Accuracy", "content": "Polyphone disambiguation is crucial for Mandarin\ngrapheme-to-phoneme (G2P) conversion, essential\nfor accurate text-to-speech (TTS) systems. Man-\ndarin characters can have multiple pronunciations\nbased on context\u00b9, making disambiguation vital, as\nincorrect pronunciations make phrases unintelligi-\nble. To alleviate potential errors in a speech synthe-\nsis system, the phonetic controllability of system\nis highly desired, as it allows us to preamptively\naugment the accurate Mandarin Phonetic Symbols\npredicted from a Neural Model g2pW (Chen et al.,\n2022). We perform phonteic augmenetations using\ng2pW in both training and inference to boost TTS\naccuracy."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Breezy Voice Architecture", "content": "The Breezy Voice Framework consists of four com-\nponents: A Supervised Semantic Speech (S3) Tok-\nenizer, a large language model (LLM), an optimal-\ntransport conditional flow matching model (OT-\nCFM) (Mehta et al., 2024), and a grapheme to\nphoneme prediction model g2pW. The following\nsections describe the components in detail."}, {"title": "3.1.1 Supervised Semantic Speech Tokenizer", "content": "The speech tokenizer is a Speech Encoder with a\nvector quantization layer (Zeghidour et al., 2021)\nto output discrete units. The semantic supervision\nis derived from training these units on a supervised\nAutomatic Speech Recognition (ASR) task. In the\none-shot speech cloning setting, the speech tok-\nenizer is used to generate units of the conditioning\nspeaker sample."}, {"title": "3.1.2 Large Language Model for TTS", "content": "In our framework, the text-to-unit generation task\nis fulfilled with a large language model (LLM).\nThe model takes tokenized text data and a speaker\nembedding as inputs, and outputs speech units.\nDuring training, the LLM uses teacher forcing\ntechnique, taking advantage of the decoding struc-\nture to minimizing the cross-entropy losses of the\npredicted speech sequences. At inference, the LLM\ngenerates speech tokens in an auto-regressive man-\nner, providing a foundation for high-fidelity speech\nsynthesis."}, {"title": "3.1.3 Optimal-transport Conditional Flow\nMatching", "content": "In our framework, the transformation of speech to-\nkens into a Mel spectrogram is achieved using an\noptimal-transport conditional flow matching (OT-\nCFM) model. OT-CFM models leverage optimal\ntransport (OT) to learn a dynamic gradual transfor-\nmation, iteratively refining a prior into the mean-\ningful data distribution. In particular, our OT-CFM\nmodel generates continuous spectrograms that ac-\ncurately reflect the time-frequency structure of an\nutterance, from conditioning on speaker charac-\nteristics. The input to this stage includes LLM-\ngenerated speech tokens, a speaker embedding, and\na masked Mel spectrogram where the masked part\nis to be reconstructed."}, {"title": "3.1.4 Phoneme Prediction Model", "content": "In this work, we utilize g2pW as our phoneme\nprediction model. g2pW is an open-source model\nthat currently holds the highest performance on\nthe CPP benchmark (Park and Lee, 2020). This\nmodel employs learnable softmax weights to condi-\ntion the outputs of BERT (Kenton and Toutanova,\n2019). The authors demonstrate that using a condi-\ntional weighted softmax, which is conditioned on\nthe polyphonic character of interest and its part-of-\nspeech (POS) tagging, enhances the performance\nof polyphone disambiguation."}, {"title": "3.2 Implementation Details", "content": "We now describe implementation details of our\napproach, covering data preparation and training."}, {"title": "3.2.1 Data Preparation", "content": "We use a diverse collection of publicly available\nspeech data and utilize its paired transcriptions.\nWhen transcriptions are unavailable, we use Gen-\nerative Fusion Decoding (Hsu et al., 2024a), an\nLLM-assisted automatic speech recognition sys-\ntem, to conduct pseudo-labeling.\nTo ensure that punctuations are present in the ap-\npropriate places, we use the Breeze-7B LLM (Hsu\net al., 2024b) to augment the punctuations in the\nraw text. This aligns the text with the downstream\nuse, which is likely to contain punctuations. Ad-\nditionally, to make the model understand phonetic\nrepresentations, we randomly augment text with\nmandarin phonetic symbols. The augmentation and\nnoising logic is inspired from BERT (Devlin et al.,\n2019) and shown in Figure 1. The emphasis of the\naugmentation technique is to maximize the model's\nefficiency in learning the pronunciation of Chinese\ncharacters, while developing the model's ability\nto prioritize attending to supplementary symbols\nwhen they occur."}, {"title": "3.2.2\nTraining Details", "content": "We train only the LLM, which has been reported to\nmostly capture semantic and prosody information.\nWe train the model for three epochs, with a flat\nlearning rate of le-4. We use dynamic batching\nwith a length of 8000 to accommodate training\nsamples that have significantly varied lengths."}, {"title": "4 Experimental Settings", "content": "Our experiments are two composed of two parts:\ncomparisons with other systems, and evaluations\non voice cloning.\nOur comprehensive survey reveals that the lead-\ning Taiwanese Mandarin TTS systems are all pro-\nprietary services. Since many online services use\none or more preset voices and lack cloning features,\nit is impossible to compare them on that aspect. As\na result, we primarily focus on comparing audio\nquality, by selecting an utterance from a generic fe-\nmale speaker that closely matches the preset voice\nof multiple TTS systems to serve as the speaker\nconditioner for our model. The evaluation targets\nare both in Chinese and bilingual, considering that\ncontemporary language usage frequently incorpo-\nrates foreign terms, especially in contexts such as\nentity names, technical jargon, and cultural refer-\nences.\nFor voice cloning evaluations, we evaluate on\nPER and speaker similarity using samples gener-\nated from a diverse range of speakers to gauge the\nrobustness of our system."}, {"title": "4.1 Evaluation Dataset", "content": "The dataset consists of a multi-speaker corpus for\nspeech conditioning, and a textual corpus for gen-\neration. For the multi-speaker corpus, we have\ncollected 15 utterances sampled from the For-\nmosaSpeech (Liao et al., 2020) corpus, and 100\nutterances of spontaneous recordings in an environ-\nment with limited background noise. The speech\nsamples come from a diverse group of Taiwanese\nMandarin speakers varying in age, gender, pitch,\nand other characteristics. Each utterance ranges\nfrom 5 to 15 seconds in duration, with their spoken\nword targets manually annotated and verified.\nThe Traditional Chinese Monologue Dataset\n(TCMD) is derived from a Traditional Chinese con-\nversational dataset and includes exclamations, ques-\ntions, and regular sentences, which naturally diver-\nsify the intended expressions without relying on ex-\nplicit expression annotations. The Traditional Chi-\nnese Code-switching Dataset (TCCSD) is a code-\nswitching text corpus, and is generated and catego-\nrized into five categories from scouring read-world\ndata for natural context-switching scenarios. The"}, {"title": "4.2 Inference Strategies", "content": "The standard inference pipeline of BreezyVoice\nis derived from Cosy Voice (Du et al., 2024a).\nHighlighting the input and output data types, the\npipeline is described as such:\n$U_{output} = \\text{LLM}(v, Y_{cond}, Y_{output}, U_{cond})$ (1)\n$X_{output} = \\text{CFM}(v, U_{cond}, U_{output}, X_{cond})$ (2)\nwhere v denotes the speech representation, CFM\ndenotes the OT-CFM model, Y denotes the byte-\npair encoded text, U denotes the derived speech\nunit, and X denotes the mel-spectrogram. The sub-\nscripts \u201ccond\" and \"output\u201d denote the sources of\nthese elements, implying from conditioning speech\nor from speech to be generated, respectively. The\nconversion from mel-spectrogram to audio is com-\npleted with an static algorithmic approach and\nexcluded in the discussion. Additionally, there is the\noption to augment Mandarin phonetic symbols.\n$Y_{augmented} \\leftarrow g2pW(Y)$ (3)\nThe multitude of conditioners, coupled with\ndropout training of previous work (Du et al.,\n2024a), prompts an exploration of alternative in-\nference possibilities. What is the optimal infer-\nence strategy for various scenarios, and when are\naugmentation necessary? These insights into the\neffects of conditioning and auxiliary inputs are cru-\ncial for making informed design decisions in the\nindustrial deployment of the system, which we ex-\namine in Section 6."}, {"title": "5 Experimental Results", "content": "The following section describes our experimental\nresults on BreezyVoice, using the standard infer-\nence pipeline to generate speech audio from a given\nspeaker and specified text."}, {"title": "5.1 Comperlative Evaluations with\nCompeting Systems", "content": "To allow for a head-to-head comparison, an iconic\nspeaker is isolated from FormosaSpeech to serve\nas the reference anchor. This selection is made"}, {"title": "5.1.1 Comparing Code-Switching Abilities", "content": "We compare the performance of various systems on\ncode-switched terms of the TCCSD dataset across\nfive prominently occurring categories to evaluate"}, {"title": "5.2 Evaluations in Voice Cloning", "content": "We evaluate the model's voice cloning performance\nusing our two curated speaker corpora comprising\na total of 115 speakers. The performance across\nthis diverse speaker profile provides a reasonable\nindication of how comprehensively the model rep-\nresents the Taiwanese Mandarin-speaking popula-\ntion. From Figure 3, it can be seen that performance\non FormosaSpeech is generally better than Sponta-\nneous Speech. Despite this, it is worth noting that\nover half of the spontaneous samples samples ex-\nhibit an error rate of less than 3%. Excluding ASR\ninaccuracies, these samples are nearly error-free\nupon human inspection. Additionally, the speaker\nsimilarity averages 92.29%, highlighting the high\nfidelity of the cloned voices. Generally speaking,\nvoice cloning capabilities is robust across speakers."}, {"title": "6 Analysis", "content": "At the extreme end of the spectrum of Figure 3,\nsome samples, likely from the long tail of the data\ndistribution, exhibited catastrophic error rates of\nover 10 percent. In this section, we look deeper\ninto the failure modes of both speaker-cloning and\nword-pronunciation issues, aiming to uncover the\nunderlying causes and identify potential mitiga-\ntions."}, {"title": "6.1 Model Robustness in Voice Cloning", "content": "A manual review of these samples confirms that\nthe audios are genuinely prone to errors. The\nmost common issues include the random insertion\nof speech stopwords, followed by stuttering, and\ninsertion of unintelligible hallucinated utterances.\nTo address these errors, we carefully analyze the\npipeline and evaluate the impact of each component\nin search of a potential resolution."}, {"title": "6.1.1 Effects on CFM Conditions", "content": "Working backwards, we begin by examining the\nCFM model. A brief recap of the original CFM\nformula is as follows:\n$X_{output} = \\text{CFM}(v, U_{cond}, U_{output}, X_{cond})$ (4)\nIn the case of an erroneous $X_{output}$, one or more\nelements from the set {v, {$U_{cond}, X_{cond}$}, $U_{output}$}\ncontribute to the error. To assess the impact of these\nfactors, we designed two additional experiments,\nusing $Z_{iconic}$ to denote an equivalent latent type of\nZ generated from an top-line iconic speaker instead\nof the noisy speaker:\n$X_{output}^{I} = \\text{CFM}(v_{Iconic}, U_{cond}, U_{output}, X_{cond})$ (5)\n$X_{recon} = \\text{CFM}(v, U_{conds}, U_{cond}, X_{conds})$ (6)\nEquation 5 isolates the effect of the speaker embed-\nding comparing with the standard process of Equa-\ntion 4, while Equation 6 unveils whether $U_{cond}$ is\nre-constructable from $X_{cond}$. Our results show that\n$X_{output} \\approx X_{output}$ and $X_{recond} \\approx X_{conds}$, which\ndiscounts the effect of v and {$U_{cond}, X_{cond}$} caus-\ning the failure. These two results together point to\n$U_{output}$ predicted with the LLM being the primary\nsource of error, backing claims made by Du et al.\n(2024a) that the speech units emcompass semantic\ncontent and prosody."}, {"title": "6.1.2\nEffects on LLM Conditions", "content": "We now delve deeper into the causes of the noisy\n$U_{output}$. Recall the formula for generation of units\nwith the LLM:\n$U_{output} = \\text{LLM}(v, Y_{cond}, Y_{output}, U_{cond})$ (7)\nWe performed experiments under the following\ndropout conditions:\n$U_{output} = \\text{LLM} (v, Y_{output})$ (8)\nOur thorough benchmarking across all spontaneous\nspeakers show that the zero-shot inference setting\nof Equation 8 yield meaningfully different speech\nsamples (units) than that of Equation 7. However,\nthe evaluation metrics indicate that omitting the\nconditions does not result in more stable speech\noutputs: 61% of speakers show deterioration while\nonly 39% of speakers improved, leading to an over-\nall increase in PER. These results highlight the\nimportance of an in-distribution LLM speaker em-\nbedding in generating coherent speech outputs."}, {"title": "6.1.3 Iconic Units for Robust Speech Cloning", "content": "Given the importance of LLM speaker embed-\ndings, we aim to explore the feasibility of a two-\nstage approach: first, consolidating content infor-\nmation through unit generations from an iconic\nspeaker, and subsequently applying voice conver-\nsion to another (noisy) speaker using the CFM\nmodel. We dub this as Iconic Unit Augmented\nSpeech Cloning. Our benchmark results show that\nthis approach drastically reduces phoneme error\nrate from 3.4% to 1.3%, which is an overall reduc-\ntion of 61.2%, accompanied by only a slight trade-\noff in speaker similarity (-2.51%). The reduction\nis evenly spread among speakers, where samples\nfrom 86 out of 100 speakers showed improvements.\nFigure 4 reveals a weak to moderate positive corre-\nlation of r = 0.29 between these changes in error\nrate and speaker similarity, suggesting that voice\ncloning performance remains more constrained for\nsamples prone to generating noisy units, despite\nthe mitigative effects of the proposed approach."}, {"title": "6.2 Model Robustness in Phonetic\nIdentification", "content": "While most pronunciations of most Chinese char-\nacters can be inherently derived from the LLM, as\ndemonstrated by general benchmarking results, er-\nrors occur more frequently in challenging scenarios\nsuch as polyphone disambiguation and rare word\nidentification. Our testing results on 23 hard in-\nstances requiring polyphone disambiguation show\nthat the original pipeline failed in 8 cases, while\ng2pW augmentation successfully corrected all but 1\nof the errors. Similar improvements were observed\nfor rare words with fewer than 500 occurrences in\nthe training corpus, where the error-inducing char-"}, {"title": "7 Conclusion", "content": "In this paper, we present BreezyVoice, a TTS\nsystem designed for Taiwanese Mandarin with\nenhanced polyphone disambiguation capabilities,\nwhich outperforms existing commercial TTS sys-\ntems in both general and code-switching contexts.\nExperimental results show human parity cloning\nquality of BreezyVoice, which is further enhanced\nby our innovative iconic unit augmented speech\ncloning pipeline and phonetic augmentation tech-\nniques."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Subjective annotation instructions", "content": "In subjective evaluations, the annotators were\ntasked to evaluate the audio sample based on au-\nthenticity and audio quality\n\u2022 Authenticity: Determine how authentic the\nvoice sounds. Assess them based on prosody\nand intonation closeness to natural human\nspeech.\n\u2022 Audio Quality: Determine the audio quality of\nthe utterance. Assess it based on similarity to\nhuman articulation, and distinctiveness from\na synthetic speech sample."}, {"title": "B Ethics Statement", "content": "As Breezy Voice enables few-shot voice cloning;\nthere is the inherent risk of potential misuse, such\nas voice spoofing. To ensure ethical use in real-\nworld applications involving unknown speakers, it\nis essential to implement protocols that secure the\nspeaker's consent before utilizing their voice. Ad-\nditionally, we are committed to developing anti-\nspoofing solutions to detect machine-generated\nspeech, including but not limited to BreezyVoice,\nto alleviate these risks."}]}