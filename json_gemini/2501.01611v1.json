{"title": "Google is all you need: Semi-Supervised Transfer Learning Strategy For Light Multimodal Multi-Task Classification Model", "authors": ["Haixu Liu", "Zerui Tao", "Penghao Jiang"], "abstract": "As the volume of digital image data increases, the effectiveness of image classification intensifies. This study introduces a robust multi-label classification system designed to assign multiple labels to a single image, addressing the complexity of images that may be associated with multiple categories (ranging from 1 to 19, excluding 12). We propose a multi-modal classifier that merges advanced image recognition algorithms with Natural Language Processing (NLP) models, incorporating a fusion module to integrate these distinct modalities. The purpose of integrating textual data is to enhance the accuracy of label prediction by providing contextual understanding that visual analysis alone cannot fully capture.\nOur proposed classification model combines Convolutional Neural Networks (CNN) for image processing with NLP techniques for analyzing textual description (i.e., captions). This approach includes rigorous training and validation phases, with each model component verified and analyzed through ablation experiments. Preliminary results demonstrate the classifier's accuracy and efficiency, highlighting its potential as an automatic image-labeling system. The link of our code is code link.", "sections": [{"title": "Introduction", "content": ""}, {"title": "Aim of Study", "content": "This research aims to propose an effective and efficient multi-label image classification model that\ncan assign one or more labels to each image by utilizing multi-modal features, ensuring high ac-\ncuracy and feasible run time. More specifically, this work seeks to advance the field of image\nclassification by investigating the integration of visual and textual information within a multi-modal\nframework. This approach is designed to harness the complementary strengths of visual and textual\nanalysis, overcoming the limitations presented by employing either modality in isolation.\nTo develop a classifier with high accuracy and efficiency, this study focuses on exploring various\nfeature extraction algorithms from the perspectives of image and text separately. It then investigates\nhow to effectively fuse the features extracted from different modalities for the downstream classifica-\ntion task. Once the overall framework is established, the hyper-parameter tuning will be conducted\nto optimize the model's performance, followed by an ablation study to evaluate the contribution of\neach component to the model."}, {"title": "Importance of Study", "content": "Accurate automated image classification reduces the reliance on manually labeled images, saving\nsignificant time and resources. For instance, medical images can be categorized using image clas-\nsification models to aid medical diagnoses. More specifically, multi-label classification task is ad-\ndressing a more practical issue in real world, as a single instance is more often associated with\nmultiple labels. For example, a photo can contain multiple objects, assigning it multiple tags (e.g.,\n'tree', 'car', 'bridge'). Such that, multi-label classification enables more accurate and comprehen-\nsive tagging by learning label correlations. Furthermore, when the model is capable of recognising\nthe relationship among labels, it demonstrates its ability to capture more information and context\nabout the data, resulting in improved performance. The enhanced understanding can help the model\nbe more robust and reliable in various application.\nFeatures learned from different modalities will complement each other in terms of the information\nthey provide, allowing for a more comprehensive data representation when combined. The enriched\nmulti-modal features will be more robust to noise and incomplete data, as compensation from other\nmodalities can occur if one modality information is missing, enabling more reliable outputs.\nTo finalize the final best model, it is crucial to identify the most effective image and textual fea-\nture extractors by comparing various state-of-the-art (SOTA) algorithms before the multi-modal\nfeature fusion step. Once the overall model structure with the feature fusion module is established,\nhyper-parameter tuning is necessary to figure out key parameters of the final model for optimal per-\nformance. In the end, it is also important to perform an ablation study accessing the necessity and\nimpact of each component within the framework on the classification performance."}, {"title": "General Introduction and Motivation", "content": "We propose a multi-modal approach to construct an accurate and efficient multi-label image clas-\nsification model. The method integrates three principal modules: the vision module, the natural\nlanguage processing (NLP) module, and the feature fusion module.\nVision Module: This module extracts salient features from image data. It employs advanced image\nprocessing techniques and other methods based on convolutional neural networks (CNNs) to analyze\nand identify key visual patterns and structures within the images.\nNatural Language Processing Module: The role of this module is to process text data, mining the\nstructural and semantic nuances of the language to extract textual features. This module uses various\nNLP techniques such as tokenization, semantic analysis, and embedding generation to understand\nand quantify the textual context that complements the visual data.\nFeature Fusion Module: The core function of this module is to integrate the outputs from both the\nvision and NLP modules. By fusing the extracted visual and textual features, this module enhances\nthe model's capacity to assimilate information from diverse sources, thereby improving the accuracy\nand reliability of decision-making. This integration augments the classification process's precision\nand significantly boosts the model's generalization ability across unseen samples.\nMotivation: Given image and textual information, we hope that our model can leverage the comple-\nmentary strengths of both visual and textual information, contributing to a more robust and versatile\nclassification system. The approach utilizing multi-modal data is pivotal in tackling the inherent\ncomplexities of multi-label image classification and sets a foundation for further exploration and\nenhancement in multi-modal machine learning applications."}, {"title": "Related Works", "content": ""}, {"title": "Multi-Label Learning", "content": "Deep learning-based methods designed for multi-label image classification tasks can be grouped into\ntwo: feature representation learning and label dependency learning. The former one aims to extract\nappropriate features from input data and then effectively connect them to related labels, thereby\nenhancing model performance. Recent successful feature-learning methods for the task are mostly\nbased on convolutional neural networks (CNN)[15-17]. The latter, on the other hand, focuses on\nlearning label relations by treating them as a sequence[18-20] or a graph[21-23].\nWith the availability of cross-modal data, multi-modal learning techniques have recently been in-\ntegrated into either category to fuse the semantic information learned from texts (e.g., captions)\nwith the spatial information extracted from images. Pre-trained language models are suggested as\nvaluable semantic resources for multi-label image classification when combined with appropriate\nmulti-modal learning techniques.\nIn this study, given inputs of images and captions, we focus on feature representation learning by\nintegrating multi-modal learning techniques. Therefore, section 3.2 and 3.3 will demonstrate related\nworks on visual and textual feature learning respectively."}, {"title": "Visual Feature Learning", "content": "With the development of deep learning, CNN architectures have become the basic of computer\nvision, introducing advanced models like AlexNet[1] and Inception[2] capturing essential spatial\nfeatures for classifying images. These networks use multiple transformations, including convolu-\ntions and pooling, to detect and synthesize low-level and high-level features, although increasing\ncomplexity often comes at the cost of efficiency. Lightweight CNN architectures aim to streamline\nthese models by reducing parameters and accelerating learning, maintaining accuracy while ensur-\ning fast processing and low memory use, making them suitable for real-world applications with\nlimited computational resources.\nSqueezeNet[3], one of the first lightweight CNNs, matched the accuracy of AlexNet on Ima-\ngeNet with 50 times fewer parameters, illustrating its suitability for resource-constrained environ-\nments. MobileNet[4], introduced by Google in 2016, is specifically designed for mobile devices\nand employs depth-separable convolutions, which reduce computational cost without compromising\nmodel effectiveness. This approach was further evolved in MobileNetV2[5] and MobileNetV3[6],\nwhich incorporated innovations such as the Inverted Residuals module from ResNet and the SENet\n(Squeeze and Excitation) mechanism, enhancing the model's attention capabilities.\nShuffleNet, developed by the Face++ team, includes iterations such as ShuffleNetV1[7] and Shuf-\nfleNetV2[8]. The first version introduces pointwise group convolution and channel shuffling, op-\ntimizing parameter efficiency and computational speed. The subsequent version, ShuffleNetV2,\nfurther refines these techniques, redesigning the basic unit architecture to optimize performance.\nEfficientNet[9], another innovation by Google Research introduced in 2019, is based on a system-\natic study of model scaling. This model uses a novel composite scaling method to coordinate depth,\nwidth, and resolution scaling with fixed coefficients, enhancing model efficiency. The latest itera-\ntion, EfficientNetV2[10], introduced in 2021, offers improvements that make the model smaller and\nfaster to train than its predecessors."}, {"title": "MoblieNet", "content": "MobileNet is a trendy lightweight neural network architecture developed by a Google team in 2017.\nIt is designed to operate on resource-constrained devices such as mobile phones and other embedded\nsystems. While its accuracy slightly trails that of VGG16\u2014by approximately 0.9%\u2014its model size\nis a mere 1/32 of VGG16's. This impressive efficiency is primarily due to its use of Depthwise\nSeparable Convolution (DW Convolution).\nTraditional convolution operations involve each filter engaging with all channels of the input feature\nmatrix, a process that requires significant computational resources. In contrast, depthwise separable\nconvolution drastically reduces computational demands by splitting the traditional convolution into"}, {"title": "MobileNet V2", "content": "Inverted Residuals and Linear Bottlenecks: MobileNetV2 introduces a novel architectural ele-\nment inverted residual blocks, which employ lightweight depthwise separable convolutions to filter\nfeatures. A significant improvement in this architecture is the implementation of linear bottlenecks\nwithin the residual blocks. Unlike MobileNetV1, which utilized traditional residuals, MobileNetV2\nincorporates linear activations in the terminal layer of each block, thereby preserving the representa-\ntional power by maintaining the integrity of nonlinear activations. Furthermore, the inverted residual\nmodule is organized into three distinct stages: the extension, deep convolutional, and projection lay-\ners.\nExtension Layer: This layer increases the number of channels in the input feature map, creating a\nhigh-dimensional space conducive to processing the input data. The expansion factor, denoted by\nt, determines the extent of this increase. If the input possesses M channels, the expanded dimen-\nsionality becomes tM. Formulas 4 and 5 mathematically articulate the process of this layer. Assume\nx \u2208 [][RH\u00d7W\u00d7C is the input tensor. Fexpand uses a set of filters Wexpand \u2208 R1\u00d71\u00d7C\u00d7(ts) and each filter's\ndimension is 1 \u00d7 1 to mapping C input channels into tC output channels. And hand w index the\nspatial dimensions. c indexes the input channels, k indexes the output channels, bexpand is the bias\nterm.\nXexpanded [h, w,k] =ReLU (\u03a3 (\u00a3x[h, w,c] \u00b7 Wexpand [1,1,c,k] + bexpand[k])\nExpandpimension = t \u00d7 M"}, {"title": "MobileNetV3", "content": "MobileNetV3 inherits the features of MobileNetV2 and makes significant architectural and effi-\nciency improvements. mobileNetV3 further improves the performance and efficiency of the model\nby incorporating the latest Network Architecture Search (NAS) technology and hardware-based op-\ntimizations.\nNetwork Architecture Search (NAS):\nNetwork Architecture Search (NAS) is a methodology that employs machine learning techniques to"}, {"title": "Efficient Net", "content": "EfficientNet is an efficient convolutional neural network architecture proposed by Google in 2019,\ndesigned to optimize the model's performance and efficiency through compound scaling. Its pri-\nmary feature is the simultaneous adjustment of the input image's depth, width, and resolution to\nachieve optimal performance under various computational resource constraints. The core idea of\nEfficientNet is to scale neural networks using compound scaling strategies systematically. Tradi-\ntional approaches typically extend the network's depth, width, or resolution independently, which\ncan lead to imbalanced models. EfficientNet's compound scaling method adjusts all three dimen-\nsions simultaneously to achieve a more balanced and efficient network expansion.\nIncreasing the depth in a deep learning network allows the model to capture more features, thereby\nenhancing its expressive ability. However, as the depth increases by d times, the computational\ncomplexity increases by d times. Increasing the width improves the feature representation ability of\neach layer, with the computational complexity increasing by w times when the width is scaled by\nw\u00b2 times. Enhancing the resolution boosts the model's ability to capture detailed features, and when\nthe resolution increases by r times, the computational complexity also rises by r\u00b2 times.\nThe basic principle of the compound scaling method is to balance the three key dimensions of the\nnetwork (depth, width, resolution) by adjusting a scaling factor o, rather than expanding one of\nthe dimensions alone. This approach ensures that the network scales evenly across all dimensions,\nthereby improving model performance while maintaining computational efficiency. The mathemat-\nical expressions of the compound scaling method are detailed in formulas 16,17 and 18. \u03b1, \u03b2, and \u03b3\nare fixed constants representing the basic scaling ratios of depth, width, and resolution, respectively.\nThe scaling factor o is used to control the expansion ratio of the model.\ndepth = \u03b1\nwidth = \u03b2\nresolution = y\nGoal Function: The goal is to find the optimal values for \u03b1, \u03b2, and y such that, given a fixed\ncomputational budget (e.g., fixed FLOPs), the model accuracy is maximized. We first define an"}, {"title": "Efficientnet V2", "content": "EfficientNet represents a highly efficient Convolutional Neural Network (CNN) architecture that\noptimizes network performance through a systematic approach to scaling, which involves adjusting\nthe depth, width, and resolution simultaneously. EfficientNetV2 builds upon the foundations of\nEfficientNet by incorporating three innovative techniques and strategies to augment efficiency and\neffectiveness further.\nUse of Lighter Weight Convolutional Operations:\nEfficientNetV2 introduces the Fused-MBConv convolution technique, simplifying the traditional\nMBConv block. The critical innovation of Fused-MBConv lies in its ability to 'fuse' specific convo-\nlution operations together, thereby reducing the number of layers and computational complexity.\nTypically, Fused-MBConv combines a 1x1 expansion convolution with a subsequent deep con-\nvolution into a single regular convolution operation, such as a 3x3 convolution. This integration\neliminates the need for separate expansion layers and deep convolutions, streamlining the module,\nsubstantially lowering computational complexity, and enhancing computational efficiency.\nUsing a Progressive Training Strategy:\nEfficientNetV2 employs a progressive training strategy, beginning the training process at a low res-\nolution and incrementally increasing the resolution. This approach accelerates training speed and\nreduces memory consumption during training. The gradual transition from low to high resolution\nenables the model to enhance its learning from general to specific features. In the initial phases, the\nmodel captures more abstract and fundamental image features; as the resolution increases, it refines\nthese features and adjusts to more intricate details, thereby improving the model's generalization\nability and overall performance.\nImproved Scaling Method:\nThe scaling strategy of EfficientNet involves simultaneous adjustments to the depth, width, and\ninput resolution of the network, guided by compounding coefficients denoted as $. This method\nfacilitates a systematic enhancement of the network's capacity and performance. The mathematical\nexpressions for this initial scaling are detailed in Formula 22, 23, and 24. In these formulas, do\nrepresents the number of layers in the basic network. wo represents the width of the network and ro\nrepresents the resolution.a represents the depth scaling factor, which controls the growth of network\nlayers.\u1e9e represents the width scaling factor, which controls the growth of the number of channels in\neach network layer. y represents the resolution scaling factor, which controls the growth of the input\nimage size, and & represents the compound scaling coefficient, allowing users to adjust this value to\nscale the entire network as needed.\nd = do xa\nw = wo x B\nr = ro x y\nEfficientNetV2 enhances the original scaling methodology to adapt more efficiently to diverse train-\ning conditions and application demands. It revises and optimizes the foundational scaling parameters\nof EfficientNet, achieving a more refined balance between computational efficiency and model per-\nformance. This optimization allows EfficientNetV2 to manage varying types and sizes of datasets\nefficiently.\nSpecifically, EfficientNetV2 excels in processing images of different resolutions and complexities.\nThis capability stems from its advanced scaling strategy, which not only adjusts the depth, width,\nand resolution based on the application's specific needs but also optimizes these parameters to han-\ndle the diverse characteristics of datasets effectively. It makes EfficientNetV2 particularly adept at"}, {"title": "Textual Feature Learning", "content": "Word embedding offers a technique for representing words in a continuous vector space, encapsu-\nlating semantic and syntactic relationships. One of the seminal models, the Word2Vec [11], was de-\nveloped by Google in 2013 and employs neural networks to discern word associations. This model\nfeatures two primary components: Continuous Bag of Words (CBOW) and Skip-Gram. CBOW\npredicts a target word based on contextual words, whereas Skip-Gram uses a word to predict its\nsurrounding context. These methods are particularly effective in capturing word relationships and\nenhancing the processing efficiency for large datasets.\nAnother notable approach, GloVe (Global Vectors for Word Representation)[12], developed by Stan-\nford University, merges the benefits of matrix factorization methods (as seen in latent semantic\nanalysis) with the contextual learning strategies employed by Word2Vec. GloVe constructs a co-\noccurrence matrix, recording the frequency of word pair occurrences within a specified context\nwindow. It then employs matrix decomposition to produce more condensed word vectors.\nIn a significant advancement in 2018, Google introduced Bert (Bidirectional et al. from Trans-\nformers)[13]. Utilizing the Transformer architecture, Bert has significantly impacted the Natural\nLanguage Processing (NLP) field. Differing from earlier models, Bert generates context-sensitive\nembeddings by considering both left and right context across all layers, which helps the model to\ngrasp more complex linguistic structures effectively.\nHowever, as these models grow in complexity and size, they face increasing challenges, such as\nGPU/TPU memory constraints and prolonged training periods. To mitigate these issues, Google\ndeveloped ALBERT (A Lite BERT)[14] in 2020. ALBERT refines Bert's architecture by sharing\nparameters across different layers and reducing vocabulary size, substantially lowering memory\ndemands and boosting training efficiency. Despite its streamlined design, ALBERT achieves per-\nformance comparable to its predecessor, Bert, making it more suitable for practical training and\ndeployment scenarios."}, {"title": "Bert(Tiny)", "content": "BERT (Tiny) is a streamlined version of the BERT (Bidirectional et al. from Transformers) model,\nengineered to minimize computational resource demands while achieving relatively high perfor-\nmance. Unlike the standard BERT models, where BERT Base comprises 12 transformer layers, and\nBERT Large contains 24, BERT (Tiny) features only 2-4 layers of transformers. Moreover, each\ntransformer layer in BERT (Tiny) includes fewer hidden units and self-attention heads, typically\nranging from 2-4 heads, compared to the 12 heads found in BERT Base.\nThe underlying architecture of BERT (Tiny) is based on the transformer framework, which utilizes\nthe self-attention mechanism to simultaneously process all elements of a sequence. This approach\nsignificantly enhances processing speed and efficiency. Central to the BERT model is its ability to\ncomprehend and represent the contextual relationships within textual data.\nSelf-attention mechanism: The core function of the self-attention mechanism is to enable the\nmodel to process a sequence while recognizing the interdependencies among the elements within\nthat sequence. Each word in the input sequence is initially converted into a fixed-size vector rep-\nresentation. This vector can be sourced from pre-trained word embeddings or generated internally\nby the model. These vectors undergo various linear transformations to produce three distinct sets of\nvectors: query (Q), key (K), and value (V). The attention scores are computed using these vectors,\nwhere self-attention is achieved by calculating the dot product between each query and all keys, pro-\nviding a measure of each element's influence over others in the sequence. The scores are typically\nscaled down following the dot product computation to mitigate the risk of gradient vanishing during\ntraining. The process for calculating attention is delineated in Formula 25. \u221adris a constant used\nto scale the result of the dot product to prevent gradient vanishing or explosion during training. The\nAttention(Q, K,V) = softmax (OKT/\u221adkV)"}, {"title": "ALBERT", "content": "ALBERT (A Lite BERT) is an optimized version of the BERT model, specifically designed to en-\nhance memory efficiency and training speed by significantly reducing the number of model parame-\nters. Like BERT, ALBERT is built upon the Transformer architecture and utilizes the self-attention\nmechanism for text processing. However, ALBERT introduces two major innovations to decrease\nparameter count and improve memory usage: the parameter sharing mechanism and factorized em-\nbedding parameterization.\nParameter Sharing Mechanism:\nUnlike traditional BERT, where each layer maintains independent parameters necessitating separate\nupdates and storage during training, ALBERT employs a parameter-sharing strategy. This approach\ninvolves reusing the same parameters across all Transformer layers, including those of the self-\nattention layers and the feed-forward network layers. Consequently, weights and biases learned\nin one layer are applied directly to subsequent layers. This mechanism ensures that the model's\nmemory footprint is significantly reduced and acts as a form of regularization. By limiting the\ndegrees of freedom, parameter sharing helps prevent overfitting, which is particularly beneficial\nwhen training data are limited..\nFactorized Embedding Parameterization:\nIn conventional BERT models, the word embedding layer, which transforms words into dense vector\nrepresentations to capture semantic relationships, typically involves many parameters due to large\nvocabularies. ALBERT addresses this issue with factorized embedding parameterization, where the\ntraditional high-dimensional word embedding matrix is decomposed into two smaller matrices. This\ndecomposition reduces the parameter count while maintaining model performance. In ALBERT,\nthe embedding process involves two matrices: E and R. Matrix E maps words from the vocabulary\nindex to a lower-dimensional embedding space, and matrix R maps these embeddings to the model's\nhidden layer size. The resulting embedding for a word w can be mathematically represented as\nshown in Formula 26, where E[w] is the embedding of the word w in E, and R is the transformation\nmatrix applied on E[w] to project it to the hidden layer dimension.\nh = R \u00d7 E[w]\nHowever, the parameter compression operation in ALBERT may lead to a slight performance trade-\noff compared to models without such compression. Additionally, using a parameter-sharing method\nin ALBERT necessitates more meticulous hyperparameter tuning to optimize model performance.\nThis detailed tuning process can be time-consuming and labor-intensive, requiring careful explo-\nration and validation of hyperparameter settings. As a result, while ALBERT provides significant\nefficiency and memory usage benefits, it may also demand more significant effort during the tuning\nphase to achieve the best possible outcomes."}, {"title": "Technology", "content": "The overview of our proposed method is shown in Figure 2. Detailed explanation and justification\non each component of our proposed model will be explained and discussed in the following subsec-\ntions (i.e., section 4.1 introduces the overall framework and section 4.2 introduces the novel fusion\nmodule)."}, {"title": "Interpretation of Our Method", "content": "To fully leverage the dataset's rich information for this multiclassification task, we developed a\nmulti-modal model composed of specialized modules for visual processing, natural language pro-\ncessing (NLP), and a fusion module that integrates outputs from the first two. Considering the\nconstraints and requirements of the task, we selected three lightweight visual models for our visual\nprocessing module -EfficientNet, MobileNet, and ShuffleNet-due to their efficiency in environ-\nments with limited computational resources. For the natural language processing component, we\nopted for Tiny-BERT and ALBERT, which are similarly optimized for performance in resource-\nconstrained settings. These models are characterized by their reduced parameter count and notably\nfaster training speeds compared to traditional deep-learning models\nFor the Fusion Module of our multi-modal model, we employ a strategy of direct logit fusion to\nintegrate outputs from the vision and natural language processing (NLP) modules. Principles of\ntransfer learning underpin this process. Specifically, we freeze the weights of both the vision and\nNLP modules to utilize pre-trained efficiencies and minimize overfitting during the fusion process.\nThe extracted feature vectors from each module are combined using a fully connected layer, which\nserves as the fusion point. The combined features are subsequently transformed into logits, which\nare the final outputs representing the integrated data from both modalities.\nAdditionally, we have designed a cross-attention module that incorporates a residual layer. This\nmodule inputs the combined feature vectors from the vision and NLP modules. The cross-attention\nmechanism focuses on enhancing feature integration by dynamically adjusting the attention to more\nrelevant features from each module based on the task at hand. The output from this cross-attention\nmodule is also processed through fully connected layers to produce logits.\nThe inclusion of a residual layer within the cross-attention module provides significant advantages.\nIt helps to mitigate the vanishing gradient problem by allowing gradients to flow through the network\nmore effectively during backpropagation. This operation benefits training stability and can lead to\nmore robust learning outcomes. Moreover, the residual layer supports deeper network architectures\nwithout the risk of performance degradation, thereby enhancing the model's overall performance.\nThis fusion approach, combining direct logit fusion with advanced attention mechanisms and resid-\nual learning, aims to maximize the synergistic effects of visual and textual data, thus optimizing the\nmodel's performance on multiclassification tasks."}, {"title": "Data Augumentation Method", "content": "To enhance the performance of our multi-modal model, we implemented data augmentation tech-\nniques on both textual and visual components of the dataset. Our primary concern was that several\nimages were associated with identical captions, potentially limiting the model's ability to learn dis-"}, {"title": "Weighted Loss Fuction", "content": "To address the issue of label imbalance in multi-class classification tasks, we developed a novel\nweighting method based on logarithmic operations. We calculate the logarithm of the total number\nof labels, denoted as log(T), and the logarithm of the number of labels for each category, denoted\nas log(ni). Two weight vectors are generated: the first vector assigns weights as log(ni)/log(T), favoring cat-\negories with a more significant number of labels, while the second vector uses log(T)/log(ni), which gives\nhigher weights to categories with fewer labels. By averaging these two vectors, a new weight vector\nis formed where both minority and majority classes receive substantial weights, whereas categories\nwith moderate numbers of labels receive lesser weights. This approach enhances the prediction per-\nformance of minority classes without significantly sacrificing the accuracy of the majority classes."}, {"title": "Label Assignment Strategy", "content": "During the model evaluation and testing phase, we encountered an issue where some model outputs\ndisplayed empty labels despite no test images explicitly assigned empty labels in the dataset. This\nissue stemmed from the model's classification mechanism, where each category's output probability\nwas determined via a sigmoid function, with a standard threshold of 0.5 set for positive classification.\nConsequently, if no category probabilities exceeded this threshold, the sample would receive no\nlabels, resulting in \"empty labels.\"\nTo address this, we revised our label assignment method. For samples that would have previously\nresulted in empty labels, we now assign the label of the category with the highest probability, re-\ngardless of whether this probability surpasses the 0.5 threshold. This adjustment ensures that each\nsample is assigned a label, thereby eliminating instances of empty labels.\nThis new strategy not only resolves the issue of high classification thresholds leading to unassigned\nlabels but also enhances the model's robustness and accuracy in practical applications. By ensuring\nthat every sample is labeled, we can maintain consistency and reliability in the model's output,\nwhich is crucial for downstream tasks and real-world applicability. This approach also prevents the\nmodel from disregarding weaker signals that may still provide valuable predictive insights despite\nnot meeting the stringent threshold."}, {"title": "Special Train Strategy", "content": "We find that a significant improvement in F1 scores is obtained by averaging the logits obtained for\nthe output of the visual and natural language processing modalities. This situation suggests that the\nmodels of the two modalities label the wrong images differently, while the output obtained by aver-\naging the logits contains information that is difficult to learn for the models of the two modalities,"}, {"title": "Fusion Module", "content": "In our fusion module design, we combined the logits from the visual and the natural language pro-\ncessing (NLP) modules, employing the EfficientNet_b4 model for visual data and the BERT (Tiny)\nmodel for NLP tasks. The initial strategy involves freezing the weights of these two models during\nthe training phase. Freezing weights is a common practice used to harness pre-trained models' robust\nfeature extraction capabilities without altering their learned behaviors. It is particularly beneficial\nwhen training data is limited and the risk of overfitting is high.\nFollowing the freezing of weights, the feature vectors extracted by the visual and NLP modules are\nconcatenated. This method of splicing together the outputs from two distinct data types-visual and\ntextual aims to capitalize on the unique strengths of each modality. By integrating these diverse\nfeatures, we enhance the model's ability to capture a broader spectrum of information, potentially\nleading to superior predictive performance in complex multi-classification tasks. Assuming the text\ninput is T and the image input is I. The output feature of EfficientNet_b4 isF\u2081 which could be detailed\nby Formula 27, and the output feature of BERT is Fr, which could be detailed by Formula 28.\nF\u2081 = BERT (T)\nF\u2081 = Ef ficientNet4(I)\nAfter extracting the feature vectors from the EfficientNet_b4 model for the visual module and the\nBERT (Tiny) model for the natural language processing module, we combine these vectors through\na feature splicing operation. This operation is crucial as it merges the distinct modalities, leveraging\ntheir respective strengths to enhance the overall model performance. The mathematical expression\nfor the splicing operation is shown in Equation 29, and the symbol ; represents the splicing operation.\nF = [Fr; F1]\nThe combined feature vector fed into a fully connected (FC) layer. This layer serves as a dense layer\nin the neural network architecture, where each input node is connected to each output node and\nis primarily used for mapping the spliced features to the desired output space. The mathematical\nexpression for the operation performed by this fully connected layer can be described as formula 30,\nwhere W is the weight of the fully connected layer and b is bias.\nL=WF+b\nIncorporating cross-attention mechanisms with residual layers into the feature fusion module is a\nstrategic enhancement to refine the interplay between our multi-modal model's visual and natural\nlanguage processing (NLP) modalities. This integration addresses the challenges of correlating\ndistinct data types, which is crucial for tasks involving complex data interactions."}, {"title": "Logits Fusion", "content": "Logits fusion is an effective technique in multimodal learning, particularly when combining the\nstrengths of models trained on different types of data or modalities. The logits, which are the out-\nputs of models before applying a softmax or similar activation function, effectively represent raw,\nunnormalized scores that can be interpreted as the model's confidence in each class of a classifica-\ntion task. The model can leverage a richer information set to make more accurate predictions by\nfusing these logits from multiple sources.\nAssuming our multimodal model operates with two different modalities, visual and textual, the\noutput logits from the visual modality can be represented as logits\u2081 = [Z11,712,...Z1k and from the\ntextual modality as logits2 = [Z21, Z22,...Z2k. We then fuse these logits using simple averaging. The\nmathematical expression for this fusion process is detailed in ForFormula 31.\nlogits fusion = 1/2(logits1+logits2)\nFormula 31 indicates that each element of the fused logits vector is the average of the corresponding\nelements from both the visual and textual output logits. This simple averaging method effectively\ncombines the distinct strengths of each modality, potentially enhancing the model's overall predic-\ntive performance.\nWhen the logits from the visual and textual modalities are fused using simple averaging, an acti-\nvation function such as softmax is applied to convert these combined logits into a final predicted\nprobability distribution. This conversion is crucial for interpreting the logits as probabilities, which\nindicate the likelihood of each category being the true class for a given input. The mathematical\nexpression for applying activation function is detailed in Formula 31 where zj is the value of class j\nin the fused logits.\nP(y = jx)= ezj/\u03a3kiezi"}, {"title": "Cross-Attention Mechanism", "content": "Cross-attention facilitates a dynamic focus on the most pertinent aspects of one data sequence about\nanother. In the context of our model, cross-attention allows the system to allocate attention selec-\ntively across the visual and text features, emphasizing the elements most relevant to the task at hand.\nBy computing the attention distribution from the visual to the textual sequence (or vice versa), the\nmodel can enhance its interpretative accuracy, focusing computational resources on analyzing and\nintegrating the most informative features from both modalities.\nWe observed that many images with different labels correspond to the same caption. We also no-\nticed that the encoding vector for text is 128 dimensions, while that for images is 1792 dimensions.\nFrom this, we deduce that the amount of information carried by text is smaller than that carried\nby images. Based on this observation, we designed a Cross-Attention module. In this module, the\ntext encoding serves as the Query through a fully connected layer. In contrast, the image encoding\nserves as the Key and Value through another fully connected layer arranged in parallel. To avoid the\ncomputational complexity of Cross-Attention, which can lead to unstable convergence, we concate-\nnate the image encoding vector and the text encoding vector via the fully connected layer's Cutoff.\nAdditionally, we add the output of the Cross-Attention module to construct a residual layer. This\nresidual layer aggregates the information obtained from Cross-Attention and the feature information\nextracted by the original encoder, thus enhancing the overall representation."}, {"title": "Integration with Residual Layers", "content": "The introduction of residual layers within the cross-attention framework serves multiple purposes:\nPrevention of Gradient Vanishing As models deepen, the risk of vanishing gradients increases,\nwhich can stall the learning process during backpropagation, especially in deep neural networks.\nResidual layers mitigate this by introducing skip connections that allow gradients to flow through\nthe network more directly and efficiently.\nEnhanced Convergence Skip connections not only solve the vanishing gradient problem but also\nfacilitate faster network convergence by smoothing the optimization landscape."}, {"title": "Operational Dynamics in the Model", "content": "In practical terms"}]}