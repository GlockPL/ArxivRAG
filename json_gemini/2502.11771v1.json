{"title": "The Validation Gap: A Mechanistic Analysis of How Language Models Compute Arithmetic but Fail to Validate It", "authors": ["Leonardo Bertolazzi", "Philipp Mondorf", "Barbara Plank", "Raffaella Bernardi"], "abstract": "The ability of large language models (LLMS) to validate their output and identify potential errors is crucial for ensuring robustness and reliability. However, current research indicates that LLMs struggle with self-correction, encountering significant challenges in detecting errors. While studies have explored methods to enhance self-correction in LLMs, relatively little attention has been given to understanding the models' internal mechanisms underlying error detection. In this paper, we present a mechanistic analysis of error detection in LLMs, focusing on simple arithmetic problems. Through circuit analysis, we identify the computational subgraphs responsible for detecting arithmetic errors across four smaller-sized LLMs. Our findings reveal that all models heavily rely on consistency heads-attention heads that assess surface-level alignment of numerical values in arithmetic solutions. Moreover, we observe that the models' internal arithmetic computation primarily occurs in higher layers, whereas validation takes place in middle layers, before the final arithmetic results are fully encoded. This structural dissociation between arithmetic computation and validation seems to explain why current LLMs struggle to detect even simple arithmetic errors.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models have demonstrated notable performance across a variety of reasoning tasks, including arithmetic problem-solving (Sawada et al., 2023; Phan et al., 2025; Liu et al., 2024a; Achiam et al., 2023). However, a gap appears to exist between the models' ability to generate solutions and their capacity to validate them effectively (Huang et al., 2024; Hong et al., 2024; Jiang et al., 2024). Specifically, while LLMs are often able to correct mistakes once they have been identified, they struggle to detect errors in the first place (Tyen et al., 2024; Kamoi et al., 2024a,b). Several studies have proposed methods to enhance LLMs' ability to detect errors and correct their own output, such as additional fine-tuning (Welleck et al., 2023; Ye et al., 2023) and the integration of external tools (Gou et al., 2024; Chern et al., 2024). However, comparatively little attention has been given to understanding why current models inherently struggle with error detection (Hong et al., 2024; Kamoi et al., 2024a; Li et al., 2024). In particular, few studies have examined the internal mechanisms responsible for error detection in LLMs (Liu et al., 2024b). In this paper, we seek to bridge this gap by presenting a mechanistic analysis of error detection in LLMs, focusing on simple arithmetic problems. We examine four LLMs\u2014Qwen-2.5-(Math)-1.5B-Instruct (Yang et al., 2024a,b), Llama-3.2-3B-Instruct (Dubey et al., 2024), and Phi-3-Mini-4k-"}, {"title": "2 Background", "content": "A common goal in interpretability research is to gain a deeper understanding of the internal mechanisms that drive the behavior of language models for a given task (Ferrando et al., 2024; Mueller et al., 2024; Bereska and Gavves, 2024). The circuit framework seeks to achieve this by identifying model components that causally influence the model's task output (Elhage et al., 2021; Wang et al., 2023; Hanna et al., 2023). In essence, a circuit refers to the computational subgraph $C \\subseteq G = (V, E)$ that represents the task-relevant flow of information across the model's layers (Conmy et al., 2023; Bhaskar et al., 2024; Hanna et al., 2024). A node $v \\in V$ in this graph can represent different components depending on the desired level of granularity-ranging from entire attention or multi-layer perceptron (MLP) layers to individual attention heads or even single neurons (Mueller et al., 2024). An edge $e_{ij} = (v_i, v_j) \\in E$ denotes a connection between two nodes, where the output of the source node $v_i$ serves as the input to the destination node $v_j$. The total input received by a node $v_j$ can be expressed as $\\sum_{e_{ij} = (v_i,v_j) \\in E_{v_j}} z_i$ where $z_i$ represents the activation of node $v_i$ and $E_{v_j}$ denotes the set of incoming edges to $v_j$.\nCircuit Identification. A common method for identifying circuits in language models is activation patching (Vig et al., 2020; Geiger et al., 2021). The key idea is to intervene on the latent activations of components in the computation graph $G$ to measure their indirect effect (Pearl, 2001) on the model's output. Adopting the terminology of Zhang and Nanda (2024), activation patching typically requires three forward passes to determine a component's indirect effect for a given task input:\n1. Clean run: run the model on a clean prompt $X_{clean}$, for which the model generates the desired task-specific output $Y_{clean}$. Cache the component's latent activations, denoted as $z_i$.\n2. Corrupted run: run the model on a corrupt prompt $X_{corrupt}$, for which the model generates a related but altered output $Y_{corrupt}$.\n3. Patched run: run the model on $X_{corrupt}$, but this time, replace the component's activations associated with $X_{corrupt}$ with the cached activations $z_i$ from the clean run.\nFinally, the indirect effect is calculated by comparing the output of the patched run to that of the corrupted run using a predefined metric $P$. If the component under consideration causally influences the model's task output, the patched activations should shift the prediction $Y_{corrupt}$ toward $Y_{clean}$. As performing these steps for every model component and sample can become computationally"}, {"title": "3 Circuits for Arithmetic Error Detection", "content": "In this section, we present the dataset used to study arithmetic error detection in LLMs. Additionally, we outline our use of edge attribution patching (described in Section 2) to identify circuits responsible for arithmetic error detection in LLMs.\nDataset. In this study, we focus on simple arithmetic problems. As illustrated in Figure 2, we employ templates to systematically generate data (Wang et al., 2023; Hanna et al., 2023). Each sample consists of a basic arithmetic problem, its corresponding solution, and a final statement that evaluates the solution's validity. Samples derived from the same template maintain a consistent sentence structure but incorporate variable components such as names or numerical values (left box in Figure 2). To analyze the models' error detection mechanisms, we introduce simple arithmetic errors into the sample's solution statement. Specifically, we consider two types of errors separately: i) a miscalculation of the arithmetic result, and ii) an incorrect final numeric answer. The perturbed sample forms our clean prompt, for which models can successfully detect the arithmetic error (see upper-right box in Figure 2, showing an error for the arithmetic result). Additionally, we construct a corrupt prompt without errors, for which models predict the solution to be \"valid\u201d (lower-right box). Note that we use single-digit numerical values that sum to a two-digit arithmetic result across all templates and samples. The introduced errors always correspond to a different, incorrect two-digit number ranging from 10 to 19.\nFor each template, we generate 6,000 pairs of (clean, corrupt) prompts. We use eight different"}, {"title": "4 Experiments", "content": "Before identifying circuits, we ensure that all models are capable of detecting the types of arithmetic errors described in Section 3. Specifically, we randomly generate 5,000 (clean, corrupt) prompt pairs for each template and evaluate whether the models predict tokens that correctly indicate the validity of the presented solutions (we expect predictions such as \"invalid\u201d, \u201cincorrect\u201d, or \u201cwrong\u201d for clean prompts, and \"valid\u201d, \u201ccorrect\u201d, or \u201cright\u201d for corrupt prompts). Table 1 summarizes the models' average accuracy along with the standard deviation"}, {"title": "4.1 Identified Circuits", "content": "As described in Section 3, we employ edge attribution patching to identify circuits $C_i = E_i \\subseteq E$ that achieve faithfulness scores between 99% and 101% for each template $T_i \\in \\{T_1, ..., T_8\\}$. We find that for all models, only 100 to 900 edges (less than 0.1% of total edges) are sufficient to achieve around 100% faithfulness for the task. Due to space constraints, we present the faithfulness scores and the corresponding number of edges for each circuit in Table 9 in the Appendix-categorized by model, template, and error type. Once a circuit is identified for each template, we compute the soft intersection circuit $C^{(\\tau)}_{8}$ as outlined in Section 3. Figure 3 illustrates the faithfulness scores and associated edge counts of the soft intersection circuits for Qwen-2.5-1.5B across different threshold values $\\tau \\in \\{\\frac{1}{8}, ..., \\frac{8}{8}\\}$. For errors at the position of the arithmetic result (e.g., \"5 + 8 = 16\"), the soft intersection circuit $C^{(5/8)}_{result}$ achieves an average faithfulness of around 100%, effectively generalizing across templates, while retaining 245 edges (see\""}, {"title": "4.2 Decoding the Error Detection Process", "content": "Once we obtain a soft intersection circuit $C^{(\\tau)}_8$, we can analyze its components to gain deeper insights into the model's error detection mechanisms. For instance, $C^{(8/8)}_{answer}$ contains several edges that connect attention heads in the model's middle layers at the position of the error, as illustrated in Fig-"}, {"title": "4.3 Dissociation of Arithmetic Validation and Computation", "content": "Our findings presented in Section 4.2 suggest that the considered models tend to rely on surface-level consistency checks rather than re-evaluating the given arithmetic equation and comparing the result with the final numeric answer. Notably, we find that all models achieve 100% accuracy in predicting the correct result of the arithmetic equation provided in each prompt. However, we hypothesize that this correctly predicted result is not used for validation. To better understand the relationship between the models' arithmetic computation and validation procedures, we identify another set of circuits responsible for computing the correct arithmetic result at the position of the arithmetic equation (e.g., 5 + 8 = 13), following a similar procedure as described in Section 3. For further details on the identification process, please refer to Appendix D.3. Interestingly, we find that for"}, {"title": "5 Related Work", "content": "Self-correction in LLMs. Self-correction in LLMs refers to the ability of models to correct their own generated output (Kamoi et al., 2024b; Huang et al., 2024; Madaan et al., 2023). Recent studies (Tyen et al., 2024; Kamoi et al., 2024a) suggest that LLMs tend to struggle with intrinsic self-correction, especially with detecting errors in their own output (Huang et al., 2024; Tyen et al., 2024; Kamoi et al., 2024b). While most studies focus on improving the models' ability to self-correct (Kamoi et al., 2024b; Madaan et al., 2023; Chen et al., 2024; Zhao et al., 2023; Welleck et al., 2023), our study analyzes the capacity of LLMs to detect errors from a mechanistic point of view.\nArithmetic and Error Detection in LLMs. To the best of our knowledge, the underlying processes of arithmetic reasoning and error detection have been studied independently in LLMs so far. Several studies (Stolfo et al., 2023; Zhang et al., 2024; Nikankin et al., 2024) use causal mediation analysis (Pearl, 2001) to identify circuits that account for how LLMs process arithmetic operations. As of now, only few studies have analyzed self-correction in LLMs beyond the models' generated output (Li et al., 2024; Liu et al., 2024b)."}, {"title": "6 Conclusion", "content": "This paper presents a mechanistic analysis of arithmetic error detection in LLMs. Our findings reveal that smaller-sized LLMs heavily rely on consistency heads-attention heads that evaluate surface-level alignment of numerical values in an arithmetic solution. Moreover, we highlight a structural dissociation between the models' arithmetic computation and validation processes. Finally, we show that bridging this gap can significantly improve the models' arithmetic error detection capacity."}, {"title": "7 Limitations", "content": "While our study provides new insights into the mechanisms underlying arithmetic error detection in LLMs, several limitations exist that can be addressed by future work.\nTask Design. This study focuses on examining the error detection behavior of LLMs in the context of simple arithmetic tasks. Specifically, we analyze math word problems involving the addition of two single-digit numbers that yield a two-digit result, as described in Section 3. Future research could extend these findings to other arithmetic operations, such as subtraction, multiplication, and division, or explore their applicability to more complex mathematical problems. It would also be valuable to investigate how these insights generalize to other domains, such as logical or causal reasoning tasks.\nModel Selection. As discussed in Section 3, our analysis is limited to four smaller-sized LLMs. Although we observe consistent patterns across various model architectures, sizes, and fine-tuning procedures (particularly within the mathematical domain), future research could investigate how these findings extend to larger models with more advanced arithmetic capabilities.\nCircuit Identification Method. As highlighted in Section 2, edge attribution patching (Syed et al., 2024) serves as a linear approximation of activation patching (Vig et al., 2020). It involves a trade-off between accuracy and computational efficiency. Notably, circuits identified using EAP are not guaranteed to be complete (Wang et al., 2023). Although the circuits identified in this study are highly sparse (comprising less than 0.1% of the total edges) and achieve near-perfect task faithfulness (see Section 4), future research should explore how these circuits compare to those identified through more exact methods."}, {"title": "A Reproducibility Statement", "content": "To ensure the reproducibility of our experiments, we make all code publicly available at: https://github.com/mainlp/validation-gap. Details of our circuit identification process are described in Section 3, Appendix B.3 and D.3. Documentation of our computational budget and the software we use can be found in Appendix F. Furthermore, a detailed account of the data used in this work is provided in Section 3 and Appendix C."}, {"title": "B Circuit Discovery Details", "content": "B.1 Edge Attribution Patching\nAttribution patching (Nanda, 2024), and specifically edge attribution patching (EAP) (Syed et al., 2024), is a computationally efficient linear approximation of activation patching to estimate the effect of interventions on latent activations. Following the activation patching terminology proposed by Zhang and Nanda (2024), consider a clean prompt $X_{clean}$ and a corrupted prompt $X_{corr}$. EAP approximates the change in a predefined metric $P$ on the model's output when a specific activation $z$ is patched from its corrupted value $z(X_{corr})$ to its clean value $z(X_{clean})$. This approximation is formulated using a first-order Taylor expansion around the corrupted input $X_{corr}$. Specifically, EAP approximates $f_z(X_{corr}; z(X_{clean})) \\fz(X_{corr}; z(X_{corr}))$ as:"}, {"title": "B.2 Faithfulness Metric", "content": "Let $(X_{clean}, X_{corr})_i$ represent a pair of clean and corrupt prompts within a dataset of size $N$. For each input, let $C(X_{i,clean})$ and $C(X_{i,corr})$ denote the logits of the clean and corrupt answer tokens in the circuit's output, and let $M(X_{i,clean})$ and $M(X_{i,corr})$ be the corresponding logits for the full model. In this study, we define the faithfulness as the logit difference recovered:"}, {"title": "B.3 Circuit Identification Process", "content": "To identify a minimal set of edges whose circuit achieves a faithfulness score between 99% and 101%, we employ an iterative search process. Starting from the sorted absolute attribution scores, denoted by $|\\nabla_zP|$, we initially select the top-k edges and evaluate the corresponding circuit. In each iteration, we then add the next n edges from this sorted list and re-evaluate the faithfulness of the resulting circuit. The search stops as soon as a circuit with a faithfulness score (see Equation 2) within the desired interval (99% to 101%) is found. In our experiments, we set k = 100 and n = 20."}, {"title": "C Dataset", "content": "C.1 Templates\nWe generate our dataset of clean and corrupt prompts based on the templates in Table 7. These templates have a set of variables, namely [instruction], [person], [object], [pronoun], [num1], [num2], [num3], each of which can be assigned different values. Table 8 lists all possible values. For the numerical variables, we use single-digit numerical values ([num1] and [num2]) that add to a two-digit arithmetic result ([num3]). To ensure that each variable assignment occupies the same position in the token sequence within a template, we retain only variables that are tokenized as a single token for each model. For the [instruction] variable, we include only instructions that share the same number of tokens. Finally, for the [correct_pair] variable, we select assignments where labels are tokenized as a single token across models.\nC.2 Aligning Token Positions Across Templates\nSince we employ token-specific EAP to identify relevant edges at specific token positions, the same element (e.g., the arithmetic result or the final numeric answer token) might appear at different token positions depending on the specific template (see Table 7). This variation in token positions makes it difficult to determine whether edges from two different template-specific circuits appear at semantically similar tokens (e.g., the arithmetic result in template 1 at token position 13 and the arithmetic result in template 2 at token position 16). To address this challenge, we assign shared abstract labels to corresponding elements across templates. Examples of such labels include [op1-in-eq], [op2-in-eq], [equals], [result-first], [result-second], [answer-first], and [answer-second], which represent the two operands of the addition, the equal sign, and the digits of the arithmetic result and the numeric answer, respectively. Mapping tokens to a shared set of labels enables us to compute the soft intersection circuits between templates -allowing for the comparison of circuits associated with semantically equivalent elements without being confounded by their varying positions within the sequence."}, {"title": "D Experiment Details", "content": "D.1 Models\nDetails of the models used in this study are presented in Table 2. Specifically, we include information on the number of parameters, the number of layers, the size of the hidden model dimension, the number of attention heads per attention block, and the respective model licenses. All models are instruction-tuned and expect a series of special tokens (e.g., to indicate the beginning of the user prompt or the end of a turn). Therefore, we wrap all prompts in the respective chat templates of the models. When applicable, we use the models' default system prompts.\nD.2 Edge Overlap\nTo quantify the proportion of shared edges between two circuits, $C_1$ and $C_2$, we compute both the Intersection over Union (IoU) and the Intersection over Minimum (IoM).\nAs discussed in Section 3, we employ token-level EAP to identify relevant edges for each token position t in the prompt. Therefore, the set of edges $e_{ij}^{t} \\in E = C$ is determined by the specific token position t. When computing the IoU and IoM between two circuits, the intersection and union of edge sets are computed separately for each token position. Specifically, we define the two metrics as:"}, {"title": "D.3 Circuits for Arithmetic Computation", "content": "To gain a deeper understanding of the relationship between models' mechanisms for arithmetic computation and validation, we identify an additional set of circuits responsible for correctly computing the arithmetic result at the position of the equation (e.g., \"5 + 8 = 13\"). This process involves generating a new set of (clean, corrupt) prompt pairs for each template $T_i \\in \\{T_1, ..., T_8\\}$.\nWe construct these datasets based on the data samples used for identifying circuits for arithmetic error detection (as described in Section 3). Specifically, we first truncate both clean and corrupt prompts at the position of the equation sign (e.g., \"\"...5 + 8 =\"). Next, we modify the corrupt prompt by replacing the numbers with a different set of numbers that produces an alternative result (e.g., \"...3 + 9 =\"). This process results in two prompts-neither containing any errors yet-where the next token is expected to be the correct outcome of the arithmetic equation (e.g., \"13\" for the clean prompt and \u201c12\u201d for the corrupt prompt). The corresponding labels represent the correct results for each prompt.\nUsing the new sets of clean and corrupt prompt pairs, we aim to identify the model components involved in computing the correct arithmetic result. We follow the same steps described in Section 3 and Appendix B.3 to identify circuits for each template. Finally, we compute the corresponding soft intersection circuits, which are responsible for generating the correct arithmetic result across templates."}, {"title": "D.4 Patching Consistency Heads", "content": "To evaluate the influence of individual consistency heads on the models' error detection behavior, we run models on prompts $X_{both}$, containing a consistent error at both the position of the arithmetic result and the final numeric answer, and patch the latent activations of the consistency heads listed in Table 3 with activations associated with prompts $X_{result}$, where the error appears only at the arithmetic result.\nSpecifically, for an attention head h, let $A_h (X)$ denote its attention matrix for a given prompt X. The patching operation we perform is defined as $A_h(X_{both}) = \\alpha \\cdot A_h(X_{result})$, where $\\alpha$ is a scaling factor that controls the influence of the patched activation. We set $\\alpha$ to 3.1 for Qwen-2.5-1.5B-Instruct and Llama-3.2-3B-Instruct, to 3.3 and 3.4 for Qwen-2.5-Math-1.5B-Instruct and Phi-3-Mini-4k-Instruct, respectively. We perform the patching over 1,000 prompt pairs. As a control setup for this experiment, we compare the result to patching randomly selected attention heads that are not labeled as consistency heads (see the full list in Table 3)."}, {"title": "D.5 Linear Probes", "content": "We train linear probes on the hidden states of the models' residual stream to test whether a specific layer linearly encodes information about the correct result of the arithmetic equation within the prompts described in Section 3. The probes are trained separately for each layer and a set of selected token positions. For each layer and token position, we use a training set of 500 hidden states per template (i.e., 4,000 samples in total per layer and token"}, {"title": "E Additional Results", "content": "In this section, we present the results of additional experiments we conducted.\nE.1 Error Detection Circuits\nAs described in Section 4.1, we identify a circuit with faithfulness score between 99% and 101% for each template $T_i \\in \\{T_1, ..., T_8\\}$. Table 9 provides a comparison between the size of the identified circuits, their exact faithfulness scores, and the total number of edges in the full computational graph for all models and templates.\nOnce a circuit is identified for each template, we compute the soft intersection circuit $C^{(\\tau)}_{8}$ to derive a final circuit that generalizes across templates, as described in Section 3. For each model, we analyze the faithfulness scores and edge counts for different threshold values in the soft intersection circuit $C^{(\\tau)}.$"}, {"title": "E.2 Edge Overlap of Error Detection Circuits", "content": "Table 4 shows the the Intersection over Union (IoU) and Intersection over Minimum (IoM) between the error detection circuits $C_{result}$ and $C_{answer}$ for each model. Notably, the IoM between the two circuits remains consistently $\\geq$ 0.75 across all models. Meanwhile, IoU values exhibit greater variability, ranging from 0.20 for Qwen-2.5-1.5B-Instruct to 0.80 for Phi-3-Mini-4k-Instruct. This variability is primarily attributable to the size differences between circuits. Specifically, the circuits responsible for detecting errors at the position of the final numeric answer are generally smaller, with particularly pronounced size reductions for the Qwen family of models (refer to Table 9)."}, {"title": "E.3 Detection of Consistent Errors", "content": "As outlined in Section 4.2, we expect models to struggle with differentiating between error-free samples and those containing a consistent error in both the arithmetic result and the final numeric answer. We evaluate models on a dataset of 1,000 (clean, corrupt) prompt pairs for each template $T_i \\in \\{T_1,...,T_8\\}$, where the clean prompts contain a consistent error at both specified positions. As mentioned in Section 4, a prompt pair is considered correctly classified if the model predicts the clean prompt as erroneous ($Y_{clean} \\in \\{invalid, incorrect, wrong\\}$) and the corrupt prompt as error-free ($Y_{corrupt} \\in \\{valid, correct, right\\}$). The respective accuracy of all models is summarized in Table 6. Overall, the results indicate that the models perform poorly on this task. For instance, Qwen-2.5-Math-1.5B-Instruct achieves an average accuracy of only 3.37% $\\pm$ 2.06%. Among the evaluated models, Phi-3-Mini-4k-Instruct demonstrates the best performance, with an average accuracy of 40.98% $\\pm$ 18.41%."}]}