{"title": "Partial Identifiability in Inverse Reinforcement Learning\nFor Agents With Non-Exponential Discounting", "authors": ["Joar Skalse", "Alessandro Abate"], "abstract": "The aim of inverse reinforcement learning (IRL) is to in-\nfer an agent's preferences from observing their behaviour.\nUsually, preferences are modelled as a reward function, R,\nand behaviour is modelled as a policy, \u03c0. One of the cen-\ntral difficulties in IRL is that multiple preferences may lead\nto the same observed behaviour. That is, R is typically un-\nderdetermined by \u3160, which means that R is only partially\nidentifiable. Recent work has characterised the extent of this\npartial identifiability for different types of agents, including\noptimal and Boltzmann-rational agents. However, work so\nfar has only considered agents that discount future reward\nexponentially: this is a serious limitation, especially given\nthat extensive work in the behavioural sciences suggests that\nhumans are better modelled as discounting hyperbolically.\nIn this work, we newly characterise partial identifiability in\nIRL for agents with non-exponential discounting: our results\nare in particular relevant for hyperbolical discounting, but\nthey also more generally apply to agents that use other types\nof (non-exponential) discounting. We significantly show that\ngenerally IRL is unable to infer enough information about R\nto identify the correct optimal policy, which entails that IRL\nalone can be insufficient to adequately characterise the pref-\nerences of such agents.", "sections": [{"title": "Introduction", "content": "Inverse reinforcement learning (IRL) is a subfield of ma-\nchine learning that aims to develop techniques for infer-\nring an agent's preferences based on their actions. Prefer-\nences are typically modelled as a reward function, R, and\nbehaviour is typically modelled as a policy, \u03c0. An IRL algo-\nrithm must additionally employ a behavioural model that de-\nscribes how \u03c0 is computed from R: by inverting this model,\nan IRL algorithm can then deduce R from \u03c0.\nThere are many motivations and applications underpin-\nning IRL. For example, it can be used in imitation learning\n(e.g. Hussein et al. 2017) or as a tool for preference elicita-\ntion (e.g. Hadfield-Menell et al. 2016). In the former case it\nis not fundamentally important that the learnt reward func-\ntion corresponds to the actual preferences of the observed\nagent, as long as it aids the imitation learning process. How-\never, in the latter, it is instead fundamental that the learnt\nreward function captures the preferences of the observed\nagent as closely as possible. In this paper, we are primarily\nconcerned with IRL in the context of preference elicitation,\nnamely in settings where IRL is used to learn a representa-\ntion of the preferences of an actual human subject, based on\ninformation about how that human behaves in some environ-\nment, and where we wish for the learnt reward function to\ncapture these preferences as faithfully as possible.\nOne of the central challenges in IRL is that a given se-\nquence of actions typically can be explained by many dif-\nferent goals. That is, there may be multiple reward func-\ntions that would produce the same policy under a given\nbehavioural model. This means that the goals of an agent\nare ambiguous, or partially identifiable, even in the limit of\ninfinite data. To clearly understand the impact of this par-\ntial identifiability, it is important that this ambiguity can be\nquantified and characterised. The ambiguity of the reward\nfunction in turn depends on the behavioural model. For some\nbehavioural models, the partial identifiability has been stud-\nied (Ng and Russell 2000; Dvijotham and Todorov 2010;\nCao, Cohen, and Szpruch 2021; Kim et al. 2021; Skalse\net al. 2022; Schlaginhaufen and Kamgarpour 2023; Metelli,\nLazzati, and Restelli 2023). However, this existing work has\nfocused on a small number of behavioural models that are\nprevalent in the current IRL literature, whereas for other\nplausible or more general behavioural models, the issue of\npartial identifiability has largely not been studied.\nOne of the most important parts of a behavioural model\nis the choice of the discount function. In a sequential de-\ncision problem, different actions may lead the agent to re-\nceive more or less reward at different points in time. In these\ncases, it is common to let the agent discount future reward,\nso that reward which will be received sooner is given greater\nweight than reward which will be received later. Discount-\ning can be done in many ways, but the two most prominent\nforms of discounting are exponential discounting, according\nto which reward received at time t is given weight $\\gamma^t$; and\nhyperbolic discounting, according to which reward received\nat time t is given weight $1/(1 + kt)$. Here $\\gamma \\in (0,1]$ and\n$k\\in (0,\\infty)$ are two parameters."}, {"title": "Reinforcement Learning", "content": "In this paper, we take a Markov decision processes (MDP)\nto be a tuple $(S, A, \\{s_T\\}, \\tau, \\mu_0, R, \\gamma)$ where S is a set of\nstates, A is a set of actions, $\\{s_T\\}$ is a terminal state, $\\tau :$\n$S \\times A \\rightarrow \\Delta(S \\cup \\{s_T\\})$ is a transition function, $\\mu_0 \\in \\Delta(S)$\nis an initial state distribution, $R : S \\times A \\times (S \\cup \\{s_T\\}) \\rightarrow$\n$\\mathbb{R}$ is a reward function, and $\\gamma \\in (0,1]$ is a discount rate.\nWe will also assume that S and A are finite. A policy is a\nfunction $\\pi: (S\\times A)^* \\times S \\rightarrow \\Delta(A)$. If a policy \u03c0 can be\nexpressed as a simpler function $S \\rightarrow \\Delta(A)$, then we say\nthat it is stationary. We use $\\mathcal{R}$ to denote the set of all reward\nfunctions definable over S and A, i.e. $\\mathcal{R}^{S\\times A\\times S}$, and $\\Pi$ to\ndenote the set of all (stationary and non-stationary) policies\nthat can be defined over S and A, i.e. $\\Delta(A)^{(S\\times A)^*\\times S}$.\nA trajectory $\\xi = (s_0, a_0, s_1...)$ is a (finite or in-\nfinite) sequence of states and actions that form a path\nin an MDP. If $s_T \\in \\xi$, then we assume that $\\xi$ is fi-\nnite, and that $s_T$ is the last state in $\\xi$. The return func-\ntion G gives the cumulative discounted reward of a tra-\njectory, $G(\\xi) = \\sum_{t=0}^\\infty R(s_t, a_t, s_{t+1})$. The value func-\ntion $V^\\pi: S \\rightarrow \\mathbb{R}$ of a (stationary) policy \u03c0 encodes\nthe expected cumulative discounted reward from each state\nunder policy \u03c0, and its related Q-function is $Q^\\pi(s,a) =$\n$E_{S'\\sim \\tau(s,a)} [R(s, a, S') + \\gamma V^\\pi(S')]$. If a policy w satisfies\nthat $V^\\pi(s) \\geq V^{\\pi'}(s)$ for all states s and all policies \u03c0',\nthen we say that \u03c0 is an optimal policy. $Q^*$ denotes the Q-\nfunction of optimal policies. This function is unique, even\nwhen there are multiple optimal policies.\nWe say that an MDP is episodic if there is some $H \\in \\mathbb{N}$\nsuch that any policy \u03c0 with probability 1 will enter the terminal\nstate $s_T$ after at most H steps, starting from any state. Note\nthat $H \\leq |S|$ in any episodic MDP. We say that an MDP\nis non-episodic if $s_T$ is unreachable from any $s \\in S$. Note\nthat an MDP may be neither episodic or non-episodic. Since\nthe transition function $\\tau$ alone determines whether or not an\nMDP is episodic, non-episodic, or otherwise, we will also\nrefer to episodic and non-episodic transition functions. In\nepisodic MDPs, we refer to a trajectory that starts in some\nstate $s_0 \\in supp(\\mu_0)$ and ends in $s_T$ as an episode.\nWhen constructing examples of MDPs, it will sometimes\nbe convenient to let the set of actions A vary between differ-\nent states. In these cases, we may assume that each state has\na \"default action\" that is chosen from the actions available\nin that state, and that all actions that are unavailable in that\nstate simply are equivalent to the default action."}, {"title": "Inverse Reinforcement Learning", "content": "In IRL we wish to infer a reward function R based on a pol-\nicy $\\pi$ that has been computed from R. To do this, we need\na behavioural model that describes how \u03c0 relates to R. One\nof the most common models is known as Boltzmann Ratio-\nnality (e.g. Ramachandran and Amir 2007), and is given by\n$P(\\pi(s) = a) \\propto \\exp{\\beta Q^*(s, a)}$, where \u03b2 is a temperature\nparameter, and $Q^*$ is the optimal Q-function for exponential\ndiscounting of R with fixed discount parameter \u03b3. In other\nwords, a Boltzmann-rational policy is given by applying a\nsoftmax function to $Q^*$. An IRL algorithm infers R from\n$\\pi$ by inverting a behavioural model. There are many algo-\nrithms for doing this (e.g. Ng and Russell 2000; Ramachan-\ndran and Amir 2007; Haarnoja et al. 2017, and many others),\nbut for the purposes of this paper, it will not be important to\nbe familiar with the details of these algorithms."}, {"title": "Partial Identifiability", "content": "Following Skalse et al. (2022), we will characterise partial\nidentifiability in terms of transformations and equivalence\nrelations on $\\mathcal{R}$. Let us first introduce a number of definitions:\nDefinition 1. A behavioural model is a function $\\mathcal{R} \\rightarrow \\Pi$.\nFor example, we can consider the function $b_{\\beta,\\tau,\\gamma}$ that,\ngiven a reward R, returns the Boltzmann-rational policy\nwith temperature \u03b2 in the MDP $(S, A, \\{s_t\\}, \\tau, \\mu_0, R, \\gamma)$.\nNote that we consider the environment dynamics (i.e. the\ntransition function, \u03c4) to be part of the behavioural model.\nThis makes it easier to reason about if and to what extent the\nidentifiability of R depends on \u03c4.\nDefinition 2. A reward transformation is a function $t :$\n$\\mathcal{R} \\rightarrow \\mathcal{R}$. Given a behavioural model $f : \\mathcal{R} \\rightarrow \\Pi$ and a\nset $\\mathcal{T}$ of reward transformations, we say that f determines R\nup to $\\mathcal{T}$ if $f(R_1) = f(R_2)$ if and only if $R_2 = t(R_1)$ for\nsome $t \\in \\mathcal{T}$.\nDefinition 2 states that the partial identifiability of the re-\nward under a particular behavioural model can be fully char-\nacterised in terms of reward transformations. To see this, let\nus first build an abstract model of an IRL algorithm. Let $R^*$\nbe the true reward function. We model the data source as\na function $f: \\mathcal{R} \\rightarrow \\Pi$, so that the learning algorithm ob-\nserves the policy $f(R^*)$. A reasonable learning algorithm\nshould learn (or converge to) a reward function $R_H$ that is\ncompatible with the observed policy, i.e. a reward such that\n$f(R_H) = f(R^*)$. This means that if f determines R up to\n$\\mathcal{T}$, then an IRL algorithm based on f is unable to distinguish\nbetween two reward functions $R_1, R_2$ exactly when $R_1$ and\n$R_2$ are related by some transformation in $\\mathcal{T}$. Hence, the par-\ntial identifiability of R under f can be characterised (Skalse\net al. 2022)."}, {"title": "The Non-Exponential Setting", "content": "In order to study partial identifiability in IRL with non-\nexponential discounting, we must first develop behavioural\nmodels for this setting. Since the Boltzmann-rational model\nis the most prominent behavioural model in the standard\n(exponentially discounted) setting, we will generalise the\nBoltzmann-rational behavioural model to work for general\ndiscount functions. However, before we can do this, we must\nfirst generalise the basic RL setting. We will allow a dis-\ncount function to be any function $d : \\mathbb{N} \\rightarrow [0, 1]$ such that\n$d(0) = 1$. Some noteworthy examples of discount functions\ninclude exponential discounting, where $d(t) = \\gamma^t$, hyper-\nbolic discounting, where $d(t) = 1/(1 + k \\cdot t)$, and bounded\nplanning, where $d(t) = 1$ if $t < n$, else 0. Here \u03b3, k, and n\nare parameters. In this paper, we are especially interested in\nhyperbolic discounting, since it is argued to be a good match\nto human behaviour. However, most of our results apply to\narbitrary discount functions.\nMany of the basic definitions in RL can straightforwardly\nbe extended to general discount functions. We consider an\nMDP to be a tuple $(S, A, \\{s_T\\}, \\tau, \\mu_0, R, d)$, where d may\nbe any discount function. As usual, we define the trajectory\nreturn function as $G(\\xi) = \\sum_{t=0}^\\infty d(t) R(s_t)$. We say that\n$V^\\pi(\\xi)$ is the expected future discounted reward if you start\nat the (finite) trajectory $\\xi$ and sample actions from \u03c0, and\nthat $Q^\\pi(\\xi, a)$ is the expected future discounted reward if you\nstart at trajectory $\\xi$, take action a, and then sample all subse-\nquent actions from \u03c0. As usual, if \u03c0 is stationary, then we\nlet $V^\\pi$ and $Q^\\pi$ be parameterised by the current state, instead\nof the past trajectory.\nIt will be convenient to also use value- and Q-functions\nthat start discounting from a different time than zero\nwe will indicate this with a superscript. Specifically, we let\n$V^{\\pi,n}(\\xi) = E[\\sum_{t=0}^\\infty d(t + n) \\cdot R(s_t)]$, where the expecta-\ntion is over a trajectory $\\xi$ (given by starting with the (finite)\ntrajectory $\\xi$, and then sampling all subsequent actions from\n\u03c0. We also define $Q^{\\pi,n}$ analogously. Note that $V^\\pi = V^{\\pi,0}$\nand $Q = Q^{\\pi,0}$. Intuitively speaking, $V^{\\pi,n}(\\xi)$ is the ex-\npected future discounted reward if you start at the (finite)\ntrajectory $\\xi$ and then sample all subsequent actions from \u03c0,\nbut discount as though you are starting at time n. Note that\nwith exponential discounting, we have that $V^\\pi \\& V^{\\pi,n}$ for\nall n, but not with non-exponential discounting.\nFor exponential discounting where $\\gamma < 1$, we have that\n$\\sum_{t=0}^\\infty \\gamma^t < \\infty$. This ensures that $V^\\pi$ always is strictly finite\nfor any choice of R and T. However, if $\\sum_{t=0}^\\infty d(t)$ diverges,\nthen $V^\\pi$ will also diverge for some R and T, which of course\nis problematic for policy selection. Therefore, it could be\nreasonable to impose the requirement that $\\sum_{t=0}^\\infty d(t) < \\infty$\nas a condition on d. Unfortunately, this would rule out the\nhyperbolic discount function. Since this discount function is\nof particular interest to us, we will instead impose conditions\non the environment. In particular, if the MDP is episodic\nthen $V^\\pi$ is always finite, regardless of which discount func-\ntion is chosen. For this reason, most of our results will as-\nsume that the environment is episodic.\nAn important property of general discount functions is\nthat they can lead to preferences that are inconsistent over\ntime. To understand this, consider the following example:\nExample 1. Let Gym be the MDP where $S = \\{s_0, s_1, s_2\\}$,\n$A = \\{buy, exercise, enjoy, go home\\}$, $\\mu_0 = s_0$, and the\ntransition function \u03c4 is the deterministic function given by\nthe following labelled graph:"}, {"title": "New Behavioural Models for General Discounting", "content": "We wish to construct behavioural models that are analo-\ngous to Boltzmann-rationality for non-exponential discount-\ning. Recall that in the exponentially discounted setting, the\nBolzmann-rational policy is given by applying a softmax\nfunction to the optimal Q-function. We must therefore first\ndecide what it means for a policy to be \"optimal\" in this\nsetting. Because of temporal inconsistency, the ordinary no-\ntion of optimality does not automatically apply, and there\nare multiple ways to extend the concept. Accordingly, we\nintroduce three new definitions:\nDefinition 4. A policy \u03c0is is resolute if there is no \u03c0' or \u03be\nsuch that $V^{\\pi',|\\xi|}(\\xi) < V^{\\pi,|\\xi|}(\\xi)$.\nA resolute policy maximises expected reward as calcu-\nlated from the initial state. In other words, it effectively ig-\nnores the fact that its preferences might be changing over\ntime, and instead always sticks to the preferences that it had\nat the start. In Example 1, a resolute policy would buy a gym\nmembership, and then exercise.\nDefinition 5. A policy \u03c0 is na\u00efve if for each trajectory \u03be,\nif $a \\in supp(\\pi(\\xi))$, then there is a policy \u03c0* such that \u03c0*\nmaximises $V^{\\pi^*,0}(\\xi)$ and $a \\in supp(\\pi^*(\\xi))$.\nA na\u00efve policy ignores the fact that its preferences may\nnot be temporally consistent. Rather, in each state, it com-\nputes a policy that is resolute from that state, and then takes\nan action that this policy would have taken, without taking\ninto account that it may not actually follow this policy later.\nIn Example 1, a na\u00efve policy would buy a gym membership,\nbut then go home without exercising.\nDefinition 6. A policy \u03c0is sophisticated if $supp(\\pi(\\xi)) \\subseteq$\n$argmax Q^{\\pi, |\\xi|}(\\xi, a)$ for all trajectories \u03be.\nA sophisticated policy is aware that its preferences are\ntemporally inconsistent, and acts accordingly. Specifically,\n\u03c0is sophisticated if it only takes actions that are optimal\ngiven that all subsequent actions are sampled from \u03c0. In Ex-\nample 1, a sophisticated policy would choose to not exercise\nin state $s_1$. Hence, in state $s_0$, it would realise that in $s_1$ it\nwould go home, instead of exercising. Thus, in $s_0$ it prefers\nto go home over buying a gym membership and then going\nhome, and it chooses to go home without buying a member-\nship.\nFor consistency, if $d(t) = \\gamma^t$ for some $\\gamma \\in (0,1]$, then\nDefinitions 4-6 reduce to optimality. Formally:"}, {"title": "Partial Identifiability for General Discounting", "content": "Having laid the groundwork necessary to generalise the\nBoltzmann-rational behavioural model to non-exponential\ndiscounting, we are now able to characterise how ambigu-\nous the reward function is for these new behavioural models.\nWe will first provide an exact characterisation of this ambi-\nguity, expressed in terms of necessary and sufficient condi-\ntions. Moreover, in order to interpret intuitively these results,\nwe will also provide a number of results with more intuitive\ntakeaways.\nExact Characterisation\nIf two reward functions R1, R2 have the property that\n$Q_1(s, a) = Q_2(s, a) - \\Phi(s)$ for some function $\\Phi : S \\rightarrow \\mathbb{R}$,\nwhere $Q_1$ and $Q_2$ are the optimal Q-functions for R1 and\nR2, then those reward functions are said to differ by po-\ntential shaping (Ng, Harada, and Russell 1999). It can be\nshown that Boltzmann-rational policies (with exponential\ndiscounting) determine R up to potential shaping (Skalse\net al. 2022). We will show that this result generalises to the\nsetting with non-exponential discounting, if the definition of\npotential shaping is adjusted appropriately. To state this re-\nsult properly, we must first introduce two new definitions:\nDefinition 13. Given an episodic MDP, we say that two\nreward functions R1, R2 differ by sophisticated potential\nshaping if there is a potential function $\\Phi : S \\rightarrow \\mathbb{R}$ such that\n$Q_2(s, a) = Q_1(s, a) - \\Phi(s)$\nfor all $s \\in S$ and $a \\in A$, where $Q_1^S$ and $Q_2^S$ are computed\nrelative to \u03c4 and d (for R1 and R2 respectively).\nDefinition 14. Given an episodic MDP, we say that two re-\nward functions R1, R2 differ by na\u00efve potential shaping if\nthere is a potential function $\\Phi : S \\rightarrow \\mathbb{R}$ such that\n$Q_2^N(s, a) = Q_1^N(s, a) - \\Phi(s)$\nfor all $s \\in S$ and $a \\in A$, where $Q_1^N$ and $Q_2^N$ are computed\nrelative to \u03c4 and d (for R1 and R2 respectively).\nNote that these two definitions do not state if such reward\nfunctions actually exist, nor do they state how to compute\nthem. Our next result therefore shows that we always can\nfind a reward R2 that differs from R1 by na\u00efve or sophisti-\ncated potential shaping with \u03a6, for any R1 and any \u03a6. The\nkey insight is that this R2 can be computed from R1 and\n\u03a6 via backwards induction, provided that the MDP is episodic.\nTheorem 5. For any episodic MDP with reward R1 and for\nany potential function $\\Phi : S \\rightarrow \\mathbb{R}$, there exists a reward\nfunction R2 that differs from R1 by na\u00efve potential shaping\nwith \u03a6, and a reward function R3 that differs from R1 by\nsophisticated potential shaping with \u03a6.\nUsing this, we can now provide an exact characterisa-\ntion of the ambiguity of the Boltzmann-sophisticated and the\nBoltzmann-na\u00efve behavioural model, which is applicable for\nany discount function. This is a core result:\nTheorem 6. In any episodic MDP, the Boltzmann-\nsophisticated policy determines R up to sophisticated po-\ntential shaping.\nTheorem 7. In any episodic MDP, the Boltzmann-na\u00efve pol-\nicy determines R up to na\u00efve potential shaping.\nIt would be desirable to generalise these results to the\nBoltzmann-resolute behavioural model next. However, this\npresents a number of challenges, primarily stemming from\nthe fact that the Boltzmann-resolute policy may be time-\ndependent. We detail and discuss these issues in the ap-\npendix.\nIt is important to note that whether or not two reward func-\ntions R1 and R2 differ by na\u00efve or sophisticated potential\nshaping is relative to a given transition function, and that\nthere is no simple closed-form expression for computing R2\nbased on R1 and \u03a6 (unlike what is the case for potential\nshaping for exponential discounting, as introduced by Ng,\nHarada, and Russell 1999). The reason for this is that the\noptimal Q-function $Q^*$ in the exponentially discounted case\ncan be expressed by a local recursive equation (namely the\nBellman optimality equation), but this is in general not pos-\nsible for $Q^N$ and $Q^S$ with non-exponential discounting."}, {"title": "Qualitative Characterisation", "content": "We have provided an exact characterisation of the ambiguity\nof the underlying reward R given both na\u00efve and sophisti-\ncated policies. However, these necessary and sufficient con-\nditions can appear to be technically sophisticated. For this\nreason, we next provide a result that is easier to interpret\nqualitatively:\nTheorem 8. Let $f_{\\tau,d,\\beta}, g_{\\tau,d,\\beta} \\in \\{r_{\\tau,d,\\beta}, n_{\\tau,d,\\beta}, s_{\\tau,d,\\beta}\\}$ be\ntwo behavioural models. Let $d_1$ and $d_2$ be any two discount\nfunctions, and let $\\beta_1, \\beta_2 \\in (0,\\infty)$ be any two temperature\nparameters. Then unless $d_1(t) = d_2(t)$ for all $t \\leq |S| - 1$,\nthere exists an episodic transition function \u03c4 such that for\nany reward $R_1$ there exists a reward $R_2$ such that\n$f_{\\tau,d_1,\\beta_1}(R_1) = f_{\\tau,d_1,\\beta_1}(R_2)$,\nbut such that\n$g_{\\tau,d_2,\\beta_2}(R_1) \\neq g_{\\tau,d_2,\\beta_2}(R_2)$.\nMoreover, unless $d(t) = \\alpha \\cdot \\gamma^t$ for some $\\alpha, \\gamma \\in [0, 1]$ and all\n$t < |S|-1, we also have that, for any reward $R_1$ there exists\na reward $R_2$, such that $f_{\\tau,d_1,\\beta_1}(R_1) = f_{\\tau,d_1,\\beta_1}(R_2)$, but\nsuch that $R_1$ and $R_2$ have different optimal policies under\nexponential discounting with \u03b3.\nLet us briefly unpack this result. Suppose $R_1$ is the true\nreward function, and that the training data for a given IRL\nalgorithm is generated via $f_{\\tau,d_1,\\beta_1}$. Suppose also that we\nwant to use the learnt reward function to compute the output\nof a different behavioural model $g_{\\tau,d_2,\\beta_2}$. If $f_{\\tau,d_1,\\beta_1}(R_1) =$\n$f_{\\tau,d_1,\\beta_1}(R_2)$, then the IRL algorithm may converge to $R_2$\ninstead of $R_1$, since they have identical $f_{\\tau,d_1,\\beta_1}$-policies.\nHowever, if $g_{\\tau,d_2,\\beta_2}(R_1) \\neq g_{\\tau,d_2,\\beta_2}(R_2)$, then $R_1$ and $R_2$\nhave different policies under $g_{\\tau,d_2,\\beta_2}$. In other words, un-\nless $d_1$ and $d_2$ are exactly equal over all time horizons that\nare possible in a given state space, then the reward function\nis too ambiguous under $f_{\\tau,d_1,\\beta_1}$ to infer the correct value\nof $g_{\\tau,d_2,\\beta_1}$. For example, this means that the Boltzmann-\nsophisticated policy for the hyperbolic discount function, or\nthe Boltzmann-na\u00efve policy for the bounded planning dis-\ncount function, both leave the underlying reward too am-\nbiguous to infer the Boltzmann-rational policy under expo-\nnential discounting, and so on. This suggests that the ambi-\nguity of the reward can be problematic if we want to use the\nlearnt reward to compute a policy using a discount function\nthat is different from that used by the observed agent.\nNote that Theorem 8 says that there exists some transi-\ntion function for which this issue can occur. This does,\nby itself, not rule out the possibility that the ambiguity of\nR may be more modest for \"typical\u201d transition functions.\nTherefore, our next result applies to a very wide range of\ntransition functions. We say that a non-terminal state s' is\ncontrollable if there is a state s and actions $a_1, a_2$ such that\n$P(\\tau(s, a_1) = s') \\neq P(\\tau(s,a_2) = s')$, and that \u03c4 is non-trivial if it has at\nleast one controllable state.\nTheorem 9. Let $f_{\\tau,d,\\beta}, g_{\\tau,d,\\beta} \\in \\{r_{\\tau,d,\\beta}, n_{\\tau,d,\\beta}, s_{\\tau,d,\\beta}\\}$ be\ntwo behavioural models. Let $d_1$ and $d_2$ be any two discount\nfunctions, and let $\\beta_1, \\beta_2 \\in (0,\\infty)$ be any two tempera-\nture parameters. Let \u03c4 be any non-trivial episodic transition\nfunction. Then unless $d_1(1)/d_1(0) = d_2(1)/d_2(0)$, we have\nthat for any reward $R_1$ there exists a reward $R_2$ such that\n$f_{\\tau,d_1,\\beta_1}(R_1) = f_{\\tau,d_1,\\beta_1}(R_2)$,\nbut such that\n$g_{\\tau,d_2,\\beta_2}(R_1) \\neq g_{\\tau,d_2,\\beta_2}(R_2)$.\nNote that in an episodic MDP, any episode has length at most\n$|S| - 1$. In other words, the horizon cannot exceed $|S| - 1$.\nMoreover, unless $d_1(1)/d_1(0) = \\gamma$, we also have that\nthere for any reward $R_1$ exists a reward $R_2$ such that\n$f_{\\tau,d_1,\\beta_1}(R_1) = f_{\\tau,d_1,\\beta_1}(R_2)$, but such that $R_1$ and $R_2$\nhave different optimal policies under exponential discount-\ning with \u03b3.\nNearly all transition functions are non-trivial, so Theo-\nrem 9 applies very broadly. Note that Theorem 8 makes\nweaker assumptions about the discount function but stronger\nassumptions about the transition function, whereas Theo-\nrem 9 makes stronger assumptions about the discount func-\ntion but weaker assumptions about the transition function."}, {"title": "Discussion and Further Work", "content": "We have analysed partial identifiability in IRL with non-\nexponential discounting, including (but not limited to) hy-\nperbolic discounting. To this end, we have introduced three\ntypes of policies (resolute policies, na\u00efve policies, and so-\nphisticated policies) that generalise the standard notion\nof optimality to non-exponential discount functions, and\nshown that these policies always exist in any episodic MDP.\nWe have used these policies to generalise the Boltzmann-\nrational model to non-exponential discounting in three ways,\nand analysed the identifiability of the reward function under\nthese models. We have demonstrated that the Boltzmann-\nna\u00efve and the Boltzmann-sophisticated policies let us iden-\ntify the true reward function up to na\u00efve and sophisticated\npotential shaping, and shown that each of the three models in\ngeneral is too ambiguous (even in the limit of infinite data)\nto compute the correct policy for a different form of dis-\ncounting. We have thus made an important contribution to\nthe study of partial identifiability in IRL, by extending exist-\ning results to the setting with non-exponential discounting.\nThis is of particular importance, since hyperbolic discount-\ning is considered to be a good fit to human behaviour.\nThere are several ways that our work can be extended.\nImproving our understanding of identifiability in IRL is of\ncrucial importance, if we want to use IRL (and similar tech-\nniques) as a tool for preference elicitation. This analysis\nshould consider behavioural models that are actually realis-\ntic. We have considered hyperbolic discounting, since this is\nwidespread in the behavioural sciences, but there are many\nother ways to make our models more psychologically plausi-\nble. For example, it would be interesting to incorporate mod-\nels of human risk-aversion, such as prospect theory (Kahne-\nman and Tversky 1979). Moreover, our analysis is primarily\nrestricted to episodic environments with a bounded horizon\n\u2014 it would be interesting to generalise it to broader classes\nof environments. This issue is further discussed in the ap-\npendix. It would also be interesting to exactly characterise\nthe partial identifiability of the Boltzmann-resolute model\n\u2014 this issue is also discussed in the appendix. Finally, it\nwould be interesting to study the case where the discount\nfunction is misspecified, i.e., where the IRL algorithm as-\nsumes that the observed agent discounts using some function\n$d_1$, but where it in fact discounts using some other function\n$d_2$ (see, e.g., Skalse and Abate 2023, 2024)."}, {"title": "Using More General Environments", "content": "Our results in this paper make use of episodic MDPs. More-\nover, we only focus on episodic MDPs with a bounded hori-\nzon that is, MDPs for which there is a number H such that\nany policy with probability 1 will enter the terminal state\nafter at most H steps, starting from any state. It is possi-\nble to give a more general definition of episodic MDPs, by\nonly requiring that any policy with probability 1 eventually\nenters a terminal state, starting from any state. The first def-\ninition requires that there is"}]}