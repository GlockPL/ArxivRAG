{"title": "POKERBENCH: Training Large Language Models to become Professional Poker Players", "authors": ["Richard Zhuang", "Akshat Gupta", "Richard Yang", "Aniket Rahane", "Zhengyu Li", "Gopala Anumanchipalli"], "abstract": "We introduce POKERBENCH a benchmark for evaluating\nthe poker-playing abilities of large language models (LLMs).\nAs LLMs excel in traditional NLP tasks, their application to\ncomplex, strategic games like poker poses a new challenge.\nPoker, an incomplete information game, demands a multitude\nof skills such as mathematics, reasoning, planning, strategy,\nand a deep understanding of game theory and human psy-\nchology. This makes Poker the ideal next frontier for large\nlanguage models. POKERBENCH consists of a comprehen-\nsive compilation of 11,000 most important scenarios, split\nbetween pre-flop and post-flop play, developed in collabora-\ntion with trained poker players. We evaluate prominent mod-\nels including GPT-4, ChatGPT 3.5, and various Llama and\nGemma series models, finding that all state-of-the-art LLMS\nunderperform in playing optimal poker. However, after fine-\ntuning, these models show marked improvements. We vali-\ndate POKERBENCH by having models with different scores\ncompete with each other, demonstrating that higher scores\non POKERBENCH lead to higher win rates in actual poker\ngames. Through gameplay between our fine-tuned model and\nGPT-4, we also identify limitations of simple supervised fine-\ntuning for learning optimal playing strategy, suggesting the\nneed for more advanced methodologies for effectively train-\ning language models to excel in games. POKERBENCH thus\npresents a unique benchmark for a quick and reliable evalua-\ntion of the poker-playing ability of LLMs as well as a compre-\nhensive benchmark to study the progress of LLMs in complex\ngame-playing scenarios. The dataset and code will be made\navailable at: https://github.com/pokerllm/pokerbench.", "sections": [{"title": "Introduction", "content": "As large language models (LLMs) become exceedingly\nbetter at performing traditional natural language process-\ning tasks (Wang et al. 2018, 2019; Radford et al. 2019;\nBrown et al. 2020), they are now evaluated on more com-\nplicated tasks like recalling world knowledge (Hendrycks\net al. 2020), reasoning (Talmor et al. 2018) and the ability to\nmathematics (Cobbe et al. 2021). A natural next evaluation\nsetting for the ever-growing capabilities of these models is\n\"game-playing\" - a setting that requires not just the ability to\ndo math and reasoning, but also planning, decision-making,\nand a deeper understanding of opponent behavior and state\nof mind (Hu et al. 2024; Xu et al. 2024; Kosinski 2023).\nPoker is one such game that requires the above-described\ncomplex skill set.\nPoker is an example of an incomplete information game\n(Harsanyi 1995) where a player has complete information\nabout their own holdings, but incomplete information about\nthe holdings of their opponents. The game requires a player\nto make strategic decisions based on their estimation of the\nopponent's holdings by accounting for their actions, style of\nplay, game situation, and possible future outcomes (Gupta\n2023). This requires a complex combination of skills in-\ncluding math, reasoning, memory, long-term and short-term\nplanning, and strategy, as well as a deep understanding of\ngame theory and player behavior and psychology. Thus, the\ndevelopment of LLMs in game-playing settings like poker\ncan potentially unlock higher cognitive capabilities in these\nmodels.\nExisting AI systems for poker, commonly called poker\n\"solvers\", play \"game theory optimal\" poker and have been\nshown to have superhuman performance (Brown et al. 2019;\nBrown and Sandholm 2019, 2018). Yet, these solvers have\nseveral limitations. (i) Firstly, poker solvers can take a long\ntime to produce solutions for a spot, thus making them un-\nusable for real-time use. (ii) Secondly, poker solvers are only\nable to calculate solutions for a limited and discrete set of\nscenarios, since the game tree in poker can become unman-\nageably large. The game tree explosion also limits the use of\nsolvers in multi-player settings. (iii) Thirdly, poker solvers\ndo not necessarily provide the most profitable strategies in\npoker. Solvers are trained to play game theory optimal poker,\nwhich means they are trained to be unexploitable. However,\nbeing game theory optimal also means that these solvers do\nnot take further advantage when their opponents are playing\nimperfectly. (iv) Finally, the solutions or strategies presented\nby solvers are abstract and not always interpretable which\nmakes it hard to group and study these strategies.\nWith these limitations in mind and the growing cognitive\ncapabilities of large language models, we explore the use\nof LLMs as poker solvers. Using LLMs for poker immedi-"}, {"title": "Related Work", "content": "Creating machines that are able to beat humans in gameplay\nsettings has a long history. The first big success for an AI\nsystem at beating humans happened in 1997 when IBM's\nDeep Blue (Campbell, Hoane Jr, and Hsu 2002) beat Garry\nKasparov, the world number 1 chess player and an all-time\ngreat of the game. While this was a significant step, Garry\nKasparov recovered from a single-game loss and ended up\nbeating Deep Blue 4-2. More recently, AlphaGo (Silver et al.\n2016) beat a Go world champion in 2016 in a comprehensive\nvictory of 4-1. Different from Chess and Go, Poker repre-\nsents a unique challenge by being an incomplete information\ngame (Harsanyi 1995). While in Chess and Go, all pieces of\nthe game are visible to all players; in poker, all informa-\ntion about the opponent's holdings is not available to play-ers. Significant progress was made in creating superhuman\nAI systems at poker between 2017-2019 with various algo-\nrithmic advances (Brown et al. 2019; Brown and Sandholm\n2018, 2019).\nWith the increasing cognitive capabilities of LLMs, recent\nwork has started exploring the use of LLMs in gameplay\nsettings. Ruoss et al. (2024) recently explored the possibil-\nity of using LLMs to create non-search-based systems that\nplay \"grandmaster\" level chess using LLMs. Gupta (2023)\nevaluated the poker-playing capability of GPT-4 and Chat-\nGPT, whichrepresents the first work exploring the possibil-\nity of playing poker using LLMs. While they concluded that\nLLMs were not good poker players, their analysis was lim-\nited to the first betting action in poker, also called \"raise first\nin\" or RFI spots. Huang et al. (2024) recently fine-tuned\nLLMs on data collected from PokerStars, a popular poker\nsite, and showed that this improved the poker-playing abili-\nties of LLMs. Yet, most of this development was done in the\nblind and models could only be evaluated at the end of the\nfine-tuning process by having them play a large number of\nhands. POKERBENCH allows for the constant development\nof LLMs as poker-playing agents and provides a universal\nbenchmark for creating such systems."}, {"title": "Poker Preliminaries", "content": "In this paper, we study the most popular version of poker,\ncalled Texas No-Limit Hold'em (NLH), which has also been\nthe focus of prior work (Gupta 2023; Brown and Sand-\nholm 2019, 2018). The number of players in a Texas NLH\ngame varies from 2-10, with the 6-player game being the\nmost popular. Texas NLH poker is the epitome of decision-\nmaking under uncertainty and incomplete information with\na near-infinite decision tree. In Texas NLH, each player is\ngiven two private cards, also called hole cards, that are only\nknown to them, and five community cards that are visible to\neveryone, peeled in three rounds accompanied by four bet-\nting rounds. Each player thus possesses seven cards, out of\nwhich two cards are private and only known to the player,"}, {"title": "Actions in Poker", "content": "There are four basic actions that a player takes while playing\npoker :\n\u2022 Check: This action means that a player wants to continue\nplaying in a betting round without wagering any chips.\nThis can only be done if no wagers have yet been made\npreviously in that betting round.\n\u2022 Bet: The action of placing a wager in a betting round is\ncalled a bet. In poker, \u201cbet\u201d is specifically referred to a\nsituation when no wager has yet been made in a betting\nround, then the first wager is called a \"bet\".\n\u2022 Call: The action of matching a wager made by a player\npreviously is called a \"call\".\n\u2022 Raise: The action of wagering a larger amount of chips\nthan the previous wager is called \u201craise\u201d.\n\u2022 Fold: The action of choosing not to match a previous wa-\nger (bet or raise) and thus giving up claim on the pot is\ncalled a \"fold\". When a player folds, they are no longer\npart of the current hand being played."}, {"title": "The Unit of Measurement in Poker", "content": "Poker is a very popular game played at different stakes.\nWhile the lowest stakes in most casinos require a buy-in\nof 100$, the buy-in can go as high as millions of dollars\nin high-stakes games around the world. Yet, every person in\nthe game usually starts with the same amount of effective\nstack size\u00b3 according to a normalized unit of measurement.\nThe normalized unit of measurement in poker is the min-\nimum amount of money a player is allowed to bet, and is\ncalled a \"big blind\" (BB). If the minimum bet amount in a\ngame is 2$, then 1BB = 2$. If the minimum bet amount in\na game is 200$, then 1BB = 200$. Poker games are usually"}, {"title": "Game Theory Optimal Poker", "content": "\"Game Theory Optimal\u201d (GTO) strategy of playing poker\nrefers to the optimal way of playing poker such that a player\ncannot be exploited by their opponents. A GTO strategy is\nusually a balanced strategy where the opponent is unable to\ncorrelate the actions of a player with their holdings. For a\nsimplistic pedagogical example to illustrate this, if Player\nA's strategy is to go all-in with only pocket aces (AA), which\nare the best starting hands in poker, then Player B can ex-\nploit Player A by folding against Player A's all-in with any\ntwo cards that are not pocket aces (AA). As a result, Player\nA is unable to extract value from their strongest hand. Thus,\nin GTO play, Player A should go all-in with a wider selec-\ntion of hands, and also not go all-in with pocket Aces every\nsingle time. This is a more optimal strategy in the long run\nagainst all kinds of players since it becomes challenging for\nour opponent to narrow down player A's exact holding. A\nwidely used technical term for the philosophy behind such a\nstrategy is called playing a \"balanced\" game."}, {"title": "The POKERBENCH Benchmark", "content": "We carefully design the POKERBENCH benchmark to thor-\noughly evaluate the poker-playing abilities of LLMs with\nan exhaustive coverage of many types of poker spots. The\naim of creating POKERBENCH is to evaluate LLMs at play-\ning game theory optimal poker in a quick and reliable way.\nPOKERBENCH is designed such that the higher a model\nscores on our benchmark, the better it is at playing optimal\npoker.\nWe define a \"spot\" to be a combination of hole cards\nthat a player has, the board, and the actions taken in the\ndifferent betting rounds. Similar to chess, the search space\nfor Texas NLH poker is extremely large. Thus, we build\nPOKERBENCH with two main balancing principles: diver-\nsity and simplicity. We want to be able to evaluate an LLM\nthoroughly by having them play a wide category of scenar-\nios while keeping a reasonable total inference time to en-\nable quick development. The following subsections explain\nour design choices in condensing the enormous search space\nby careful filtering and pruning using principles of optimal\npoker play.\nThe POKERBENCH benchmark is created based on 6-max\nplayer Texas NLH poker. It consists of two separate sets\na pre-flop and a post-flop evaluation dataset. Pre-flop games\nare usually very different from post-flop play and have a very\ndifferent distribution of decisions, which is why we decided"}, {"title": "Pre-flop Action Selection", "content": "After each player is dealt with two hole cards, the first bet-\nting round can have an exponentially large number of deci-\nsions being made. For example, a player can decide to bet\na certain amount, and a following player can decide to bet\nan even larger amount (called a \"raise\" in poker), and this\nraising and re-raising can happen with an exploding num-\nber of permutations, each with a different bet and raise size.\nExisting pre-flop GTO strategies exist for all of these sce-\nnarios. In POKERBENCH, we only consider scenarios where\na maximum of one raise has happened in the pre-flop betting\nround. This includes scenarios where (i) all players fold, (ii)\nonly one player bets chips and other players either call or\nfold, and (ii) one player bets chips, a second player raises\nthat bet by a higher wager followed by only calls or folds\n(also called \"3-bet pots\"). This covers the majority of the\npossible pre-flop scenarios that are considered a viable GTO\nplay as most pots do not go beyond a single raise."}, {"title": "Board Selection", "content": "A \"board\" is the list of community cards that show up during\na poker game. There are in total 5 community cards, where\neach card can be any of the 52 cards in the deck. Thus, we\nhave a total of 52C5 \u2248 311 million possible boards that can\nshow up. It is impossible to evaluate our model on that many\nboards, which is why we group the boards into 11 classes,\ncalled textures. These board textures cover the most com-\nmon situations on the flop.\nOne of the most commonly studied board textures on the\nflop is what we call \"single-broadway-dry\". As a reminder,\n\"flop\" is a term used to describe the first three community\ncards. The term \"broadway cards\" is used to refer to the\ncards in the set {A,K,Q,J,10}, that is, it refers to the\nfive strongest cards in a suit. The term \"single-broadway\"\nrefers to the opening of exactly one broadway card on the\nflop. The term \"dry\" is used to describe flops in poker which\ndo not have a lot of possibilities to make different winning\nhands. For example, a board {Kh, 7d, 2s}, where the sub-\nscripts show the suit of the cards, is a typical dry board\nin poker. The board does not have any repetition of suit,\nwhich means it is less likely to form flushes, and does not\nhave numbers close to each other, which means it is less\nlikely to form straights. The strategy a player should use to\nplay on a {Kh, 7d, 2s} board is going to be very similar to\nstrategy on similar boards like {Kh, 7d, 4s}, {Ah, 8d, 3s},\n{Qh, 8d, 2s}, and hence all such boards are group together"}, {"title": "Selecting Hole Cards", "content": "GTO play dictates to have nondeterministic strategies since\nif a deterministic strategy exists in a player's game, it be-\ncomes exploitable. For example, if a player always goes all-\nin when they have aces (AA), then all other players are likely\nto fold, ending up in an unprofitable play. Thus, a balanced,\ngame theory optimal way of playing any spot in poker is to\nhave at least two actions chosen with certain probabilities at\nthe time of play. For selecting hole cards, we choose spots\nwhere there is a clear dominant strategy as the best action.\nFor example with a board like {Ah, Kh, 8d, 3s, 10c}, fac-\ning a small bet from the opponent, private cards that have\na clear dominant strategy are hands like Qd, Js of raising,\nwhereas hands like 10d, 9d have a more mixed strategy of\ncalling or folding. We decided to filter hole cards by select-\ning action lines that choose one dominant action with greater\nthan 50% probability."}, {"title": "Dataset Summary", "content": "The POKERBENCH benchmark consists of 1k evaluation\nspots for pre-flop and 10k evaluation spots for post-flop play.\nalong with the evaluation benchmark,\nwe also release a training set containing 60k pre-flop spots\nand 500k post-flop spots. The distribution of actions for the\nPOKERBENCH benchmark and the training set can be seen\nin Figure 1.\nFor the pre-flop training set, we preserve the original dis-\ntribution of actions that a player can take because the search\nspace there is relatively small, and so an exhaustive strat-\nergy that covers most possible scenarios is learnable. For the\npost-flop training set, we experiment with a few different\nsampling strategies and find that resampling the action dis-\ntribution using a balanced strategy results in the best fine-\ntuning performance. Specifically, a balanced sample helps"}, {"title": "Experiments", "content": "After carefully curating POKERBENCH, we move on to eval-\nuating the poker-playing ability of LLMs.\nEvaluation Metrics We use the following two metrics to\nevaluate the poker skills of LLMs by evaluating their re-\nsponses on POKERBENCH:\n\u2022 Action Accuracy (AA): Action accuracy measures if\nLLMs can take game theory optimal actions (fold, raise,\nbet, etc.) for a given spot.\n\u2022 Exact Match Accuracy (EM): Actions like bet and\nraise are followed by the amount of the bet and raise.\nHence, exact match accuracy also considers if the wager\namount is game theory optimal."}, {"title": "Are Modern LLMs Good at Playing Optimal Poker?", "content": "We evaluate GPT-4 (OpenAI 2023), ChatGPT 3.5 (Ouyang\net al. 2022), Llama-3 models (8B, 70B) (Meta 2024) and\nLlama-2 70B (Touvron et al. 2023) on POKERBENCH. We\ntake inspiration from the evaluation protocol for the popular\nMMLU dataset (Hendrycks et al. 2020) and evaluate these\nmodels in a few-shot setting. While MMLU is a multiple-choice question-answering dataset, POKERBENCH is not.\nThus we require the model to generate the action and the ex-\nact bet amount for a given spot. For few-shot examples, we\nselect one example randomly for each possible action from\nthe training dataset and add it to the context. Thus our few-shot setting contains 5 examples in context, one for each ac-\ntion as shown in Figure 1. An example prompt can be found\nin Table 7 (appendix). For generating text, we set tempera-\nture = 0.1 and top-p = 0.95 to generate the most probable\nanswer to get statistically stable results. We use the OpenAI\nAPI for evaluating OpenAI models and TogetherAI API\nfor evaluating models from the Llama series.\nThe results for evaluation on POKERBENCH can be found\nin Table 2. We use chat or instruct models for this evalu-\nation as applicable. GPT-4 outperforms all other models\nboth in pre-flop and post-flop play. The second best per-\nforming model is Llama-3-70B with its performance on the\nbenchmark being significantly lower than GPT-4. A surpris-\ning thing to note is that ChatGPT 3.5 performs comparably\nto a significantly smaller Llama-3-8B model. Llama-3 out-\nperforms Llama-2 in generating correct bet/raise amounts\n(higher EM score) while this lead diminishes when generat-\ning optimal actions (similar AA score). While we also tried\nto evaluate the smaller models from the Llama-2 series, they\nare unable to follow poker instructions, which is why we\ndo not report their accuracy scores. The benchmark results\nshow that all modern LLMs significantly lack in their ability\nto play game theory optimal poker and that there is a lot of\nroom for improvement."}, {"title": "Fine-Tuning LLMs into Better Poker Players", "content": "In the previous section, we saw that state-of-the-art LLMs\nare not good at playing poker and significantly underper-\nform on POKERBENCH compared to other tasks they are\nevaluated on (Hendrycks et al. 2020; Talmor et al. 2018;\nWang et al. 2018, 2019). A possible reason for this could\nbe the complexity of the game which requires a multitude of\nskills coming together. To improve the poker-playing ability\nof LLMs, we fine-tune the model on a subset of the accom-\npanying training set released with this paper. The training set\nconsists of 30k pre-flop spots and 80k post-flop spots. We\nhave a much larger amount of post-flop spots in the training"}, {"title": "Heads-Up Games between Fine-tuned Checkpoints", "content": "Finally, we test if the scores on POKERBENCH translate\ninto actual performance. To do so, have different models\nwith different scores play an actual poker game against\neach other. To do so, we pick three different Llama-3-8B model checkpoints with different accuracy scores on\nPOKERBENCH. We simulate a heads-up game between the\nthree players. The details of the selected players are shown\nTable 3. The checkpoint name refers to the number of\ngradient updates that have happened in the fine-tuning pro-\ncess at the time of selection. For example, the Llama-3-8B\n1600 has gone through 1600 gradient update steps on the\ntraining dataset. Going forward, we will refer to the players\nby their checkpoint names as player 800, player 1600, and\nplayer 5000.\nThe heads-up games are played with two modifications\nthe stacks of the players are always reset to 100BB to match\nthe GTO strategies, and the seating positions of the players\nare randomly assigned before each hand. To create statisti-\ncally significant results, we have each player play against the\nother for 50k hands. The win rate is calculated with a met-ric called big blinds per hundred hands, or bb/100, defined\nas the number of big blinds won per hundred hands played.\nAs a reminder, a big blind is the basic unit of measurement\nin poker and represents the minimum bet that can be made."}, {"title": "Fine-tuned Model vs. GPT-4", "content": "Next, we perform a heads-\nup game between the best fine-tuned model checkpoint and\nthe best pre-trained model, namely GPT-4, using a same\nsetting. Due to inference cost constraints, the two models\nplayed for 1,000 hands only. The result for this tournament\ncan be seen in Table 5.\nWe notice an unorthodox phenomena, that despite achiev-ing higher test accuracy, our fine-tuned Llama model was\noutplayed by GPT-4. To investigate this, we separate the\nwin-rate by stage of the game (pre-flop, flop, turn, and river),\nand conduct an in-depth analysis of the game log. Overall,\nwe identify spots where Llama is effectively learning from\nthe training set and winning by playing more GTO than\nGPT-4. For instance, as shown in Table 6, Llama is win-\nning in the pre-flop stage by raising a wider range of hands\n(which is more optimal) and squeezing GPT-4 out of the pot.\nHowever, one reason of GPT-4's winning is that it adopts\na sub-optimal strategy called \"donking\u201d. In a nutshell, it is\na losing move in the long run, hence not frequently seen\nin GTO strategies (more details can be found in the Ap-\npendix). But our dataset assumes GTO play from both play-\ners. Consequently, this move is unfamiliar to the fine-tuned\nmodel, constituting a big part of Llama's losses as GPT-4\nwins 100% of the hands where it donks.\nAlso, neither player is playing at an optimal action fre-\nquency: GPT-4 is being over-aggressive while Llama is over-"}, {"title": "Discussion", "content": "There is a fine line between winning against an\nimperfect strategy versus minimizing losses against a GTO\nstrategy. The two methods of evaluation used in this paper\nare evaluating two different things. The gameplay is eval-\nuating how well a strategy performs against an imperfect\nstrategy, while our dataset is evaluating the how well a strat-\nergy perform against a GTO strategy. One of the advantage\nabout training the model to approach GTO strategy is that\nbeing GTO is a sufficient condition for winning against any\nimperfect strategy. But when an non-perfect GTO strategy\nis learned (for instance in the case of our fine-tuned model\nsince it is not perfectly GTO), the relationship between the\ntwo could become complex. Yet, we argue that succeeding\non our dataset is a necessary condition for minimizing loss\nagainst a GTO strategy because our dataset is a compila-\ntion of a diverse set of samples that any near GTO strategy\nmust agree on (and non-GTO strategy is guaranteed to lose\nin the long run). We also believe that this finding is also a\nlimitation of the simple supervised fine-tuning in training\nLLMs to effectively succeed in poker. We would like to use\nthis example to encourage further research in improving the\nadaptability of language models in game environments."}, {"title": "Conclusion", "content": "In this paper, we present POKERBENCH - a comprehensive\nbenchmark that evaluates optimal poker-playing ability of\nLLMs. We evaluate multiple state-of-the-art language mod-\nels and show that current LLMs fail significantly in playing\noptimal poker. We also fine-tune Llama-3-8B among other\nmodels on the accompanying training dataset and show that\nresultant models can outperform much larger models. We\nalso show that the scores on POKERBENCH actually trans-\nlate to superior poker-playing skills by evaluating models of\ndifferent scores through game simulations over 50k hands.\nThus, POKERBENCH represents a quick and reliable mea-\nsure of the optimal poker-playing ability of large language\nmodels as well as a comprehensive benchmark to study the\nprogress of LLMs in this domain. This study not only ex-\nplores the potential of LLMs in strategic game-playing but\nalso presents a benchmarks to evaluate higher-level cogni-\ntive capabilities of LLMs in complex game-playing situa-"}, {"title": "Appendix", "content": "Explanation of Board Textures\nThe different board textures are shown in Figure 3. With\nthese textures, we present a comprehensive group of scenar-\nios on the flop. Hands that fall in the above categories are\nusually played in very similar ways. The term \u201cwet\u201d is used\nto describe flops that have a likelihood of making the hands\nof some player. A \u201cdynamic\u201d board is one where the best\nhand may not be made yet but is likely to show up as the next\ntwo cards are dealt. A \u201cmid\u201d board represents medium-rank\ncards, whereas a \u201clow\u201d board represents a board with low-\nrank cards. A \u201cmonotone\u201d board represents a board where\nall cards have the same suit.\nGPT-4 vs. Fine-tuned Model Gameplay Analysis\nPre-flop: As shown in Table 5, our fine-tuned Llama model\nperforms better pre-flop. Through aggregating actions taken\nby both player from the game log, we find that Llama played\nremarkably more aggressive than GPT-4. Specifically, out of\nthe 752 hands that end pre-flop Llama open-raises (raises as\nthe first player that moves) 27.3% of the times and GPT-\n4 open-raises 15.3% of the times. Compared to GTO pre-\nflop strategy, GPT-4 is playing too tightly, only raising the\npremium hands and thus losing overall by giving up their\nblinds most of the time. We see a similar pattern for sit-\nuations with multiple raises pre-flop, where Llama is bal-\nancing between raising strong hands like KK and AKs with\nbluffing hands like KTo, while GPT-4 only reraises with\nAA,KK, AKS. GPT-4's lack of bluff in their raises presents\nan exploitable leak: if opponents know GPT-4 is never rais-\ning with weak hands, they will easily fold to GPT-4's raise,\nleading to GPT-4 missing out value when holding strong\nhands.\nFlop/Turn: Most of the winning of GPT-4 comes from\nhands that end flop or turn. After inspecting the specific\ngame history, we find that one primary source of GPT-4 win-\nning is adopting a \"donking\" strategy, in which the out-of-position player who is not the pre-flop aggressor decides\nto bet as the flop/turn cards come out. In theory, donking\ngives negative expected value because pre-flop aggressor\nholds a stronger range of hands, including premium hands\nlike AA,KK,QQ, while the non-aggressor holds a weaker\nrange of hands. Betting a weaker range against a stronger\nrange automatically leads to losing bigger pots with the same\nwinning probabilities. Also, as mentioned previously, a key\nconcept in GTO play is balance. It is generally very hard to\nbalance donking with very strong hand with bluffs as now\nplayer needs to separate their range of hands into a more\ncomplicated game tree with donking available. Under spe-\ncific board textures, this move can sometimes be adopted by\nGTO strategy, but we find GPT-4 was donking on spots that\nare clearly losing plays (not adopted by GTO strategy), if\ncountered properly. However, as donking usually constitute\na very small portion of a GTO strategy, and our dataset as-sumes GTO play by both players, it is not well-represented"}]}