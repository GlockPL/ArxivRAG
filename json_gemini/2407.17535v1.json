{"title": "LAMBDA: A Large Model Based Data Agent", "authors": ["Maojun Sun", "Ruijian Han", "Binyan Jiang", "Houduo Qi", "Defeng Sun", "Yancheng Yuan*", "Jian Huang*"], "abstract": "We introduce \u201cLAMBDA,\" a novel open-source, code-free multi-agent data analysis system that that harnesses the power of large models. LAMBDA is designed to address data analysis challenges in complex data-driven applications through the use of innovatively designed data agents that operate iteratively and generatively using natural language. At the core of LAMBDA are two key agent roles: the programmer and the inspector, which are engineered to work together seamlessly. Specifically, the programmer generates code based on the user's instructions and domain-specific knowledge, enhanced by advanced models. Meanwhile, the inspector debugs the code when necessary. To ensure robustness and handle adverse scenarios, LAMBDA features a user interface that allows direct user intervention in the operational loop. Additionally, LAMBDA can flexibly integrate external models and algorithms through our knowledge integration mechanism, catering to the needs of customized data analysis. LAMBDA has demonstrated strong performance on various machine learning datasets. It has the potential to enhance data science practice and analysis paradigm by seamlessly integrating human and artificial intelligence, making it more accessible, effective, and efficient for individuals from diverse backgrounds. The strong performance of LAMBDA in solving data science problems is demonstrated in several case studies, which are presented at https://www.polyu.edu.hk/ama/cmfai/lambda.html.", "sections": [{"title": "1 Introduction", "content": "Over the past decade, the data-driven approach utilizing deep neural networks has motivated the success of artificial intelligence across an extensive of challenging applications in various fields [23]. Despite these advancements, the current paradigm encounters challenges and limitations in data science applications, particularly in domains that demand extensive expertise and advanced coding knowledge, such as biology [40], healthcare [28], and business [39]. A notable barrier is the lack of effective communication channels between domain experts and sophisticated AI models [30]. To address this issue, we introduce \u201cLAMBDA,\" a new open-source, code-free multi-agent data analysis system designed to overcome this dilemma. LAMBDA aims to facilitate the creation of a much-needed medium, fostering seamless interaction between domain knowledge and the capabilities of AI in data science. Our main objectives in developing LAMBDA are as follows.\n(a) Crossing coding barrier: Coding has long been recognized as a significant barrier for domain experts without a computer science background to leverage powerful AI tools effectively [28]. LAMBDA addresses this challenge by enabling users to interact with data agents through natural language instructions, thereby offering a coding-free experience. This approach significantly lowers the"}, {"title": "2 Backgrounds and Foundations", "content": "In recent years, the rapid progress in Large Language Models (LLMs) has brought boundless possibilities to the field of artificial intelligence. Notable examples of LLMs include GPT-3 [6], GPT-4 [29], PaLM [8], LLaMA [37] and Qwen [2]. LLMs demonstrate an outstanding ability to understand, generate, and apply natural language. Benefiting from this revolution, LLM-powered agents (LLM agents) are developed to automatically solve problems in various domains like the search engine, software engineering, gaming, recommender systems, and scientific experiments [14, 42, 49]. LLM agents are usually guided by a chain of thought (CoT) such as ReAct [43], and hold capabilities of tool use such as APIs, code interpreters, and retrievers. These works motivated us in the design of the LAMBDA in two ways: function calling and code interpreter."}, {"title": "2.1 Enhancing LLMs with Function Calling", "content": "The integration of external APIs or tools into LLMs is known as function calling, which signifies LLMs are able to use tools to handle tasks by their functional capabilities [7, 21]. This process can be summarized as follows: First, LLMs classify the user's instruction to functions based on the function annotations. Then, the program executes the selected functions, and the LLM makes a final answer based on the result of the execution [21, 26]. [35] investigates the current paradigm of tool learning with LLMs and shows its advancements in diverse applications such as programming, calculators, and weather inquiry. However, In the data science scenario, we conjecture the function calling method can not perform well because of the following drawbacks:\n\u2022 Conjecture A In the data science scenario, numerous APIs have complex interrelationships and extensive annotations. We hypothesize that these lengthy API annotations may result in sequences that exceed the maximum processing capacity of LLMs, leading to the truncation of both API details and user messages.\n\u2022 Conjecture B We speculated that the model's capacity to select APIs accurately diminishes with an increasing number of APIs. This is attributed to the enhanced complexity introduced by an increasing number of APIs that LLMs need to select. The wrong choice of tools can lead directly to the wrong result and answer."}, {"title": "2.2 Powering LLMs by Code Interpreter", "content": "Equipping LLMs with code interpreters can enable them to execute code and obtain the execution result [12, 26]. [45, 10, 46] demonstrate the code interpreter capabilities in ChatGLM, making it accessible for programming like complex calculation and drawing figures. [48] ingeniously utilized the code interpreter to solve mathematics problems, achieving significant progress. However, the code in data science problems is more complex and challenging than in the aforementioned domains. If the correctness of the code cannot be guaranteed, the reliability of the code interpreter approach will be compromised. To address this issue, we propose a self-correction mechanism that enhances reliability by enabling our agent to learn from its failures."}, {"title": "2.3 Multi-agent Collaboration", "content": "A multi-agent system consists of numerous autonomous agents that collaboratively engage in planning, discussions, and decision-making, mirroring the cooperative nature of human group work in problem-solving tasks [14]. Each agent plays a role that possesses unique capabilities, objectives, and"}, {"title": "2.4 Retrieval Augmented Generation", "content": "Retrieval Augmented Generation (RAG) is a technique that retrieval external sources to improve the accuracy and reliability of LLM's responses [24, 17, 4, 26]. Resources will be split and embedded into sub-fragment vectors and stored in the vector base. RAG first queries a vector database, matching document fragments relevant to the query from the user by computing similarity. These fragments are then used to enhance the accuracy of the responses generated by LLMs [24]. This approach is often used for infusing external knowledge and reducing hallucinations [13]. However, the general RAG workflow can not work well in some data science scenarios. First, the user's instructions and the relevant code fragments may not exhibit significant similarity in the representation space, leading to inaccurate searches. Second, the varying lengths of code fragments can also impact the final search results. For example, the code of customized optimization algorithms is quite long, which leads to the challenge of matching whole correct code fragments. In LAMBDA, we designed a KV knowledge base to solve this issue in our scenario and integrate human intelligence into AI."}, {"title": "3 Methodology", "content": "Our proposed multi-agent data analysis system (LAMBDA) consisted of two agents that cooperate seamlessly to solve data analysis tasks by natural language, as shown in Figure 2. The macro workflow involves initially writing code based on user instructions and subsequently executing that code. Besides, another idea of a function calling-based agent system is presented in Section 7.1. Through comparison study, we will discuss the dilemma of the function calling method and the merits of the multi-agent collaboration system in the section Result 4.1."}, {"title": "3.1 Overview", "content": "LAMBDA is structured around two core agent roles: the \"programmer\" and the \"inspector\", who are tasked with code generation and evaluation respectively. When a user submits an instruction, the programmer agent writes code based on the provided instructions and dataset. This code is then executed within the kernel environment of the host system. Should any errors arise during execution, the inspector intervenes, offering suggestions for code refinement. The programmer takes these suggestions into account, revises the code, and resubmits it for re-evaluation. This iterative cycle continues until the code runs error-free or a preset maximum number of attempts is reached. In order to cope with unfavorable situations and enhance its reliability and flexibility, a human intervention mechanism is integrated into the workflow. This feature allows users to directly modify the code and intervene when deemed necessary. The collaboration algorithm is demonstrated in Algorithm 1."}, {"title": "3.2 Programmer", "content": "The primary responsibility of the programmer is to write code and respond to the user. Upon the user's dataset upload, the programmer receives a tailored system prompt that outlines the programmer's role, environmental context, and the I/O formats. This prompt is augmented with examples to facilitate few-shot learning for the programmer. Specifically, the system prompt encompasses the user's working directory, the storage path of the dataset, the dimensions of the dataset, the name of each column, the type of each column, information on missing values, and statistical description.\nThe programmer's workflow can be summarized as follows: initially, the programmer writes code based on instructions from the user or the inspector; subsequently, the program extracts code blocks from the programmer's output and executes them in the kernel. Finally, the programmer generates a final response based on the execution results and communicates it to the user. This final response consisted of a summary and suggestions for the next steps."}, {"title": "3.3 Inspector and Self-correcting Mechanism", "content": "The inspector's primary role is to provide reasonable modification suggestions when errors occur in code execution. The prompt of the inspector includes the code written by the programmer during the"}, {"title": "3.4 Integrating Human Intelligence and AI", "content": "Beyond leveraging the inherent knowledge of LLMS, LAMBDA is further enhanced to integrate human intelligence through external resources such as customized algorithms and models from users. Due to the challenges faced by general RAG methods in data science scenarios, which stem from the potential lack of clear correlation between user instructions and code fragments in the representation space, as well as the impact of varying code fragment lengths. We design a KV knowledge base to store the code resources.\nWhen the user issues an instruction ins, an embedding model (denoted as $\\mathcal{F}$) encodes all descriptions in the knowledge base and the ins. The embedding tensors for descriptions and instruction are represented by $e_{d_i}$ and $e_{ins}$ respectively. The cosine similarity between them is calculated to select the knowledge with a similarity score greater than a threshold @ and the highest match as the knowledge.\nLet the embedding function be $\\mathcal{F}$, the $e_{d_i}$ and $e_{ins}$ are formulated as follows\n$e_{d_i} = \\mathcal{F}(d_i) \\forall i \\in \\{1,2,..., n\\}, e_{ins} = \\mathcal{F}(ins)$\nThe similarity $S_i$ between description and instruction can be computed by cosine similarity as\n$S_i(e_{d_i}, e_{ins}) = \\frac{e_{d_i} \\cdot e_{ins}}{||e_{d_i}|||| e_{ins} ||} \\forall i \\in \\{1,2,..., n\\}$\nWe let the matching threshold 0 = 0.5. The matched knowledge k with the highest $S_i$ is selected while it satisfies $S_i > 0$, computed as\n$k = c_{i^*}, i^* = arg \\underset{i}{\\text{max}} (S_i(e_{d_i}, e_{ins}). 1\\{S_i(e_{d_i,e_{ins})>\\theta}\\}) \\forall i \\in \\{1,2,..., n\\}$\nThe knowledge k will be embedded in in-context learning (ICL) for the LLM to generate answer A. Formally, given a query q, matched knowledge k, a set of demonstrations D $\\{(q_1, k_1, a_1), (q_2, k_2, a_2), ..., (q_n, k_n, a_n)\\}$, and the LLM M, the model estimates the probability $P(a|q, k, D)$ and outputs the answer \u00c2 that maximizes this probability. The final response A is generated by the model M as follows:\n$\\hat{A} \\leftarrow M(q, D)$"}, {"title": "3.5 Report Generation", "content": "LAMBDA can generate analysis reports based on the dialogue history. The report usually includes data processing, data visualizations, model descriptions, and evaluation results. We offer various report templates for users to select. LAMBDA produces reports in the desired format by ICL. This feature enables users to concentrate on high-value tasks, rather than spending time and resources on report writing and formatting. A sample case can be found in the Figure 21.\nOverall, the programmer agent, inspector agent, self-correcting mechanism, and human-in-the-loop provide LAMBDA with theoretical reliability. Knowledge integration makes LAMBDA scalable and flexible. Besides, to bring portability to LAMBDA, we provide the OpenAI style interface. This implies that most LLMs, once deployed via open-source frameworks such as vLLM [22] and LLaMAF-Factory [47], can be compatible with our system. Some prompts and cases are provided in Section 7.3."}, {"title": "4 Experiments and Results", "content": "We first verify the conjectures of the function calling method in Section 2.1. Additionally, we conducted an ablation study on the proposed LAMBDA to show the impact and performance of each agent. Lastly, to evaluate the proposed LAMBDA, we observed the performance of LAMBDA on several machine learning (ML) datasets and whether human intervention is required in the process."}, {"title": "4.1 Challenges of the Function Calling Method", "content": "We estimate the maximum number of APIs that some open-source LLMs can handle in the data science scenario by using the average length of the APIs we pre-defined. Figure 4 illustrates the results. Qwen1.5 and Mistral-v0.1 [19] were specifically designed to naturally handle lengthy sequences, capable of managing 400 and 372 APIs respectively. However, general-purpose LLMs such as LLaMA2, LLaMA3, Mistral-V0.2, Qwen1, ChatGLM2, and ChatGLM3 can process fewer than 100 APIs, posing a challenge for applications requiring a larger number of APIs, such as data science tasks.\nTo investigate the impact of the number of APIs on the accuracy of LLMs selecting these APIs, we made a dataset comprising 100 commonly used APIs in the data science scenario. Through few-shot learning, we generated 880 testing instructions aligned with these APIs by Qwen1.5-110B. Subsequently, we segmented both the APIs and testing instructions into intervals of 10 functions for analysis."}, {"title": "4.2 Ablation Study on LAMBDA", "content": "To assess the reliability and performance of each agent in LAMBDA, We designed an ablation study based on the heart disease dataset [18]. This dataset containing missing values will bring challenges naturally. We utilized Qwen1.5-110B to generate instructions for related tasks. There are 454 instructions in the experiment after filtering. We evaluated the execution pass rate on a single programmer agent and multiple agents (programmer and inspector) respectively."}, {"title": "4.3 Experiment on Machine Learning Dataset", "content": "In order to acquire practical experience and performance in real-world data science tasks, we evaluated LAMBDA on several ML datasets 4 by recording its performance on these datasets and noted if human involvement was needed. Accuracy was utilized as the evaluation metric for classification tasks, while mean squared error was employed for regression tasks.\n\u2022 AIDS Clinical Trials Group Study 175 provides healthcare statistics and categorical data on AIDS patients. Released in 1996, it includes 2139 instances with 23 features, primarily used for predicting patient mortality within a specified time frame [15].\n\u2022 National Health and Nutrition Health Survey 2013-2014 Age Prediction Subset (NHANES) is derived from the CDC's comprehensive health and nutrition survey. This subset, with 6287 instances and 7 features, focuses on predicting respondents' age groups (senior or non-senior) using physiological, lifestyle, and biochemical data [20].\n\u2022 Breast Cancer Wisconsin (Diagnostic) comprises 569 instances and 30 features. It classifies patients into categories of malignant or benign [41].\n\u2022 Wine contains the results of a chemical analysis of wines from a specific region in Italy, derived from three different cultivars. It includes 178 instances and 13 features, focusing on the quantities of various constituents found in the wines. [1].\n\u2022 Concrete Compressive Strength contains 1030 instances with 8 features, examining the highly nonlinear relationship between concrete compressive strength, age, and ingredients [44].\n\u2022 Combined Cycle Power Plant features 9568 data points collected over six years, with 4 features, to analyze the performance of a power plant under full load conditions [36].\n\u2022 Abalone predicts the age of abalone from physical measurements. Comprising 4177 instances with 8 features [27].\n\u2022 Airfoil Self-Noise aims to predict scaled sound pressure, includes 1503 instances with 5 features derived from aerodynamic and acoustic tests of airfoil blade sections in an anechoic wind tunnel [5]."}, {"title": "5 Examples", "content": "We provide three examples to demonstrate the ability of LAMBDA in data analysis, integrating human intelligence and education respectively.\nData Analysis: We simulate the scenarios where the user asks LAMBDA to conduct different tasks, including data preprocessing, data visualization, and model training, on the provided dataset Iris"}, {"title": "6 Conclusion", "content": "In this work, we introduce LAMBDA, an open-source multi-agent data analysis system that integrates human intelligence with AI. Experimental results demonstrate that LAMBDA achieves satisfactory"}, {"title": "7 Appendix", "content": ""}, {"title": "7.1 Initial Idea of Function Calling Based Agent System", "content": "The first idea that came to our mind was function calling. We developed extensive APIs that encompass a wide range of data processing and machine learning functionalities, including statistical descriptions (e.g., mean, median, standard deviation), encoding schemes (e.g., one-hot encoding, ordinal encoding), data partitioning, and model training (e.g., logistic regression, decision tree). We utilized five function libraries to build these APIs, each tailored for different purposes: the Data Description Library, Data Visualization Library, Data Processing Library, Modeling Library, and Evaluation Library. Each library caches variables such as processed data and models throughout the program's lifecycle."}, {"title": "7.2 Datasets and Metrics in the Study", "content": "Accuracy is defined as the ratio of the number of correctly classified instances to the total number of instances. It is given by the formula:\n$Accuracy = \\frac{TP+TN}{TP+TN+FP + FN}$\nwhere TP is True Positives, TN is True Negatives, FP is False Positives, FN is False Negatives\nMean Squared Error (MSE) is defined as the average of the squared differences between the predicted values and the actual values. It is given by the formula:\n$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\nwhere: n is number of data points, $y_i$ is actual value, $\\hat{y}_i$ is predicted value."}, {"title": "7.3 Prompt in LAMBDA", "content": "Prompt for Programmer\nInitial Prompt for Programmer\nYou are a data scientist, your mission is to help humans do tasks related to data science and analytics.\nSystem Prompt for Programmer with DataSets\nYou are a data scientist, your mission is to help humans do tasks related to data science and analytics. You are\nconnecting to a computer, but there is no internet connection. You should write Python code to complete the user's\ninstructions. Since the computer will execute your code in Jupyter Notebook, you should directly use defined\nvariables instead of rewriting repeated code. And your code should be started with markdown format like:\\n\n```python\nWrite your code here, you should write all the code in one block.\n\nImportantly, You can not generate the executed result by yourself because you should run the code on the\ncomputer and get the results. So, you must stop generating text immediately after writing the code (end with ```).\nIf the execute results have errors, you need to revise it and improve the code as much as possible.\nRemember 2 points:\n1.  You can work with data uploaded to your computer by users, the working path of the user is {working_path}. You\nmust read or save files in this path.\n2.  In the code, you must show some results of the code, you can follow these situations:\n(1). For data processing, use 'data.head()' after processing.\n(2). For data visualization, use 'plt.savefig({working_path})' after plotting instead of 'plt.show()'. Then the figure will\nbe displayed in the dialogue.\n(3). For model training, use 'joblib.dump(model, working_path})' after training. Then the model will be displayed in\nthe dialogue.\nHere is an example:\n{example}\nSystem Prompt for Datasets (Example of iris dataset)\nNow, the user uploads the datasets, and here is the general information of the dataset:\n{'num_rows': 150,\n'num_features': 5,\n'features': Index(['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width',\n 'Species'], \n dtype='object'),\n'col_type': Sepal.Length float64\nSepal.Width float64\nPetal.Length float64\nPetal.Width\nfloat64\nSpecies object\ndtype: object,\n'missing_val': Sepal.Length 0\nSepal.Width 0\nPetal.Length 0\nPetal.Width 0\nSpecies 0\ndtype:\nint64,\n'describe': Sepal.Length Sepal.Width Petal. Length Petal.Width\ncount 150.000000 150.000000\n150.000000 150.000000\nmean 5.843333 3.057333 3.758000 1.199333\nstd 0.828066\n0.435866 1.765298 0.762238\nmin 4.300000 2.000000 1.000000 0.100000\n25%\n5.800000 3.000000 4.350000\n5.100000 2.800000 1.600000 0.300000\n50%\n1.300000\n75% 6.400000 3.300000 5.100000\n6.900000 2.500000}\nmax\n7.900000 4.400000\n1.800000\nPrompt for Formatting the Tabular Result to Markdown\nRewrite {table} in markdown format without any elaboration:\nPrompt for Execution Results\nThis is the executing result by computer (If nothing is printed, it may be plotting figures or saving files):\n{Executing_result}.\nYou should use 1-3 sentences to explain or give suggestions for next steps:"}, {"title": "Prompt for Self-correcting and Human Intervention", "content": "Prompt for Inspector\nYou are an experienced and insightful inspector, and you need to identify the bugs in the given code based on\nthe error messages and give modification suggestions.\n- bug code:\n{bug_code}\nWhen executing the above code, errors occurred: {error_message}.\nPlease check the implementation of the function and provide a method for modification based on the error\nmessage. No need to provide the modified code.\nModification method:\nPrompt for Programmer to fix the bug\nYou should attempt to fix the bugs in the bellow code based on the provided error information and the method\nfor modification. Please make sure to carefully check every potentially problematic area and make appropriate\nadjustments and corrections.\nIf the error is due to missing packages, you can install packages in the environment by \"!pip install\npackage_name\".\n- bug code:\n{bug_code}\nWhen executing the above code, errors occurred: {error_message}.\nPlease check and fix the code based on the modification method.\nmodification method:\n{fix_method}\nThe code you modified (should be wrapped in ```python```):\nPrompt for Human intervention\nI write or repair the code for you.\n```python\n{code}"}, {"title": "Prompt for Report Generation", "content": "Prompt for Academic Report\nYou are a report writer. You need to write a academic report in markdown format based on what is within the\ndialog history. The report needs to contain the following (if present):\n1.  Title: The title of the report.\n2.  Abstract: Includes the background of the task, what datasets were used, data processing methods, what\nmodels were used, what conclusions were drawn, etc. It should be around 200 words.\n3.  Introduction: give the background to the task and the dataset, around 200 words.\n4.  Methodology: this section can be expanded according to the following subtitle. There is no limit to the number\nof words.\n(4.1) Dataset: introduce the dataset, including statistical description, characteristics and features of the\ndataset, the target, variable types, missing values, and so on.\n(4.2) Data Processing: Includes all the steps taken by the user to process the dataset, what methods were\nused to process the dataset, and you can show 5 rows of data after processing.\nNote: If any figure is saved, you should include them in the document as well, use the link in the chat\nhistory, for example:\n![confusion_martix.png](http://url/of/the/path).\n(4.3) Modeling: Includes all the models trained by the user, you can add some introduction to the algorithm of\nthe model.\n5.  Results: This part is presented in tables as much as possible, containing all model evaluation metrics\nsummarized in one table for comparison. There is no limit to the number of words.\n6.  conclusion: summarize this report, around 200 words.\nHere is an example for you:\n{example}\nPrompt for Experimental Report\nYou are a report writer. You need to write an experimental report in markdown format based on what is within the\ndialog history. The report needs to contain the following (if present):\n1.  Title: The title of the report.\n2.  Experiment Process: Includes all the useful processes of the task, You should give the following information\nfor every step:\n(1) The purpose of the process\n(2) The code of the process (only correct code.), wrapped with ```python```.\n# Example of code snippet\n```python\nimport pandas as pd\ndf = pd.read_csv('data.csv')\ndf.head()\n(3) The result of the process (if present).\nTo show a figure or model, use ![confusion_matrix.png](http://url/of/the/path)\n3.  Summary: Summarize all the above evaluation results in tabular format.\n4.  Conclusion: Summarize this report, around 200 words.\nHere is an example for you:\n{example}"}, {"title": "Prompt for Knowledge Integration", "content": "System Prompt for Retrieval\nYou can retrieve codes from the knowledge base. The retrieved code will be formatted as:\nRetrieval: The retriever finds the following pieces of code cloud address the problem: \\n```python\\n[retrieval_code]\\n```\nFor example:\n{example}\nPrompt for Retrieval\nRetrieval: The retriever finds the following pieces of code cloud address the problem. You should refer to this\ncode, and if there is a test case, you can make changes directly to the test case instead of re-writing all functions:\nRetrival code:\n{code}"}]}