{"title": "Towards identifying possible fault-tolerant advantage of quantum linear system algorithms in terms of space, time and energy", "authors": ["Yue Tu", "Mark Dubynskyi", "Mohammadhossein Mohammadisiahroudi", "Ekaterina Riashchentceva", "Jinglei Cheng", "Dmitry Ryashchentsev", "Tam\u00e1s Terlaky", "Junyu Liu"], "abstract": "Quantum computing, a prominent non-Von Neumann paradigm beyond Moore's law, can offer superpolynomial speedups for certain problems. Yet its advantages in efficiency for tasks like machine learning remain under investigation, and quantum noise complicates resource estimations and classical comparisons. We provide a detailed estimation of space, time, and energy resources for fault-tolerant superconducting devices running the Harrow-Hassidim-Lloyd (HHL) algorithm, a quantum linear system solver relevant to linear algebra and machine learning. Excluding memory and data transfer, possible quantum advantages over the classical conjugate gradient method could emerge at $N \\approx 2^{33} \\sim 2^{48}$ or even lower, requiring $O(10^5)$ physical qubits, $O(10^{12} \\sim 10^{13})$ Joules, and $O(10^6)$ seconds under surface code fault-tolerance with three types of magic state distillation (15-1, 116-12, 225-1). Key parameters include condition number, sparsity, and precision $K, s \\approx O(10 \\sim 100)$, $\\epsilon \\sim 0.01$, and physical error $10^{-5}$. Our resource estimator adjusts $N, K, s, \\epsilon$, providing a map of quantum-classical boundaries and revealing where a practical quantum advantage may arise. Our work quantitatively determine how advanced a fault-tolerant quantum computer should be to achieve possible, significant benefits on problems related to real-world.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum computing [1] stands as the leading paradigm of next-generation computing technology. Unlike traditional computing, quantum computers can access and manipulate quantum states, which serve as carriers of information for computational purposes. One of the primary motivations for developing quantum computing is its potential to achieve a possible quantum advantage over its classical counterparts in solving certain problems. This potential quantum advantage, studied primarily in terms of time cost within computational complexity theory, has been theoretically demonstrated in problems such as factoring [2], searching [3], and simulation [4]. These advancements offer hope for sustaining or surpassing Moore's law in the semiconductor industry.\nHowever, beyond the time complexity estimated in the gate models from theoretical computer science, it is challenging to estimate and justify the possible quantum advantage in practice. First, practical cost estimation of quantum computing requires state-of-the-art knowledge, from detailed theory covering prefactors in front of the big-O notation of complexity [5, 6], to explicit designs of quantum hardware, and it includes more comprehensive measurement such as time costs (measured in seconds), space costs (number of physical qubits), and in particular energy costs. Especially, the complex nature of quantitative energy efficiency estimation is highly uninvestigated, although the possible energy advantage of quantum computing algorithm is discussed mostly in the qualitative argument [7\u20139]. Second, although the existence of potential quantum advantage for some algorithms is solidly justified in theory, it is challenging to prove that those algorithms can turn into real-world, significant benefits especially for commercial applications [10]. Finally, quantum states are extremely fragile and current quantum processors are noisy, making quantum error correction the only possible way to make large-scale, fault-tolerant quantum computing. Fault-tolerance, although sustainable in theory, requires lots of additional resources and experimental challenges, making precise resource estimation much more challenging.\nIn this work, we address those challenges by conducting a full-stack, energy-aware resource estimation for the so-called Harrow-Hassidim-Lloyd (HHL) algorithm [11]. The HHL algorithm provides a Quantum Linear System Algorithm (QLSA) that can be used for solving linear algebra problems. Given a linear equation $A|x\\rangle = |b\\rangle$, the algorithm returns a quantum state $|x\\rangle = A^{-1}|b\\rangle$ as a solution. For certain classes of matrices, it has been theoretically shown that the algorithm runs in poly(log N) time for an $N \\times N$ matrix, making it exponentially faster than any known classical counterpart. Complexity-theoretic argument also suggests that the algorithm under certain settings are BQP-complete [11]. Since lin-"}, {"title": "II. RESULTS", "content": "In this chapter, we present the main theoretical and numerical results of our study. For the HHL resource estimation, we primarily use the following statement,\nStatement 1. Consider a system of linear equations $Ax = b$, where $A$ has been scaled such that its eigenvalues lie in the interval $[-1,1]$. Denote by $|b\\rangle$ the normalized quantum state proportional to $b$. Assume access to an oracle that provides access to the elements of a sparse submatrix of A. Then, there exists a quantum algorithm that takes the input state $|b\\rangle$ and outputs the normalized solution state $|\\tilde{x}\\rangle$ with additive error $||x - \\tilde{x}||$ less than $\\epsilon$."}, {"title": "III. METHODS", "content": "The quantum resource estimation primarly contain logical resource analysis and physical resource analysis.\nAt the logical level, we first implemented the quantum circuit for the Quantum Linear Systems Algorithm (QLSA) using Q#, a Microsoft programming language designed for circuit-based quantum computing [14]. This implementation allowed us to estimate the logical qubit and T-gate requirements for each submodule. Among all submodules in the QLSA algorithm, the primary bottleneck lies in one specific subroutine\u2014the one-sparse Hamiltonian simulation (HS1). This subroutine is responsible for constructing a unitary operation that implements $e^{iAt}$, where $A$ is the input matrix. The high resource demand of HS1 stems from the Trotterization process, which requires applying HS1 repeatedly to suppress the error to a desired level. Additionally, the unitary operation itself must be executed multiple times as part of the Quantum Phase Estimation (QPE) procedure, further increasing the total number of HS1 invocations. To precisely determine the scaling behavior of HS1, it is essential to analyze the algorithm's error. There are two primary sources of error in the QLSA algorithm: (1)"}, {"title": "IV. CONCLUSION AND OUTLOOK", "content": "In this work, we perform an end-to-end resource estimation of the HHL algorithm in terms of time, space, and energy. Unlike existing resource estimations, including [5], our work provides a detailed analysis of fault tolerance based on surface codes. Moreover, we precisely address energy efficiency by employing physical models of superconducting quantum devices. Our findings confirm that quantum computing will indeed have an energy advantage at a large scale based on current superconducting technologies, resolving a long-standing concern about the energy efficiency of quantum computing in practical applications [7, 8]. Other innovative aspects of our work include a detailed comparison between quantum and classical approaches to solving linear systems, and an analysis of theoretical upper bounds versus practical average cases, the details of which are summarized in the Appendix.\nAlthough we confirm that potential quantum advantages might appear at the scale of $2^{33} \\sim 2^{48}$ matrix sizes and $10^5$ physical qubits, the practical costs could be lower due to the overestimation of resource counts in the Trotter simulation. Moreover, smarter designs for quantum error correction and energy-sustainable innovations in quantum hardware might further enhance performance. On the other hand, compared to other resource estimations of quantum algorithms in different domains (such as cryptography used for digital signatures and blockchains [28]), the HHL algorithm might achieve potential quantum advantages with a smaller number of physical qubits [28]. However, better classical and quantum designs in-"}, {"title": "Appendix A: QLSA algorithm implementation", "content": "1. A brief introduction to QLSA\nThe Quantum Linear Systems Algorithm (QLSA) is a quantum algorithm designed to solve systems of linear equations efficiently. It relies on principles of quantum mechanics to provide a possible superpolynomial advantage for specific cases compared to classical methods.\nThe core idea is to represent the solution to a linear system $Ax = b$ in the quantum state $|x\\rangle$, where A is a sparse and well-conditioned matrix. The algorithm involves several key steps:\n\u2022 Preparation of the Input State: The algorithm starts with the quantum encoding of the input vector $\\langle b|$ into a quantum state.\n\u2022 Quantum Phase Estimation (QPE): QPE approximate the eigenvalues of the matrix A by estimating the phase of the unitary $e^{iAt}$. Such unitary is implemented using the Trotter decomposition method.\n\u2022 Matrix Inversion: The eigenvalues of A are inverted conditionally, enabling the construction of the quantum state corresponding to the solution.\n\u2022 Measurement: Finally, measurements are performed to extract information about the solution vector.\nQLSA's efficiency is highly dependent on the sparsity and condition number of the matrix A, making it particularly useful for problems where these constraints are satisfied\n2. Resource bottleneck in QLSA\nA significant computational bottleneck in the QLSA arises from the repetitive application of one-sparse Hamiltonian simulation, which is a fundamental component of the algorithm. This challenge emerges due to the following reasons:\n\u2022 Quantum Phase Estimation (QPE): The QPE step requires the Hamiltonian simulation of A, which is an s-sparse matrix, to be applied multiple times to achieve sufficient precision. The repetition ensures the accurate estimation of eigenvalues, which directly affects the correctness of the algorithm.\n\u2022 Simulation of s-Sparse Hamiltonians: To implement the simulation of s-sparse Hamiltonians, the Trotter decomposition method is often employed. However, this approach introduces further overhead, as it decomposes the s-sparse Hamiltonian into a series of one-sparse Hamiltonians.\n\u2022 One-Sparse Hamiltonian Simulation: Each s-sparse Hamiltonian simulation requires the repetitive application of one-sparse Hamiltonian simulations. This nested structure compounds the computational cost, making one-sparse Hamiltonian simulation a critical bottleneck for resource efficiency.\nIn essence, the hierarchical nature of the QLSA\u2014where QPE requires multiple iterations of s-sparse simulations, and s-sparse Hamiltonian simulations depend on repeatedly applying a series of one-sparse simulations\u2014highlights the algorithm's computational resource bottleneck.\nTherefore, our resource estimation focuses solely on the one-sparse Hamiltonian simulations, reducing the overall resource estimation to (1) the number of one-sparse Hamiltonian simulations. (2) the resource cost of each one-sparse Hamiltonian simulation. Next we will discuss the resource cost of each one-sparse Hamiltonian simulation by giving a concrete code implementation of such step. We left the analysis for number of one-sparse Hamiltonian simulation in the error analysis section.\n3. One-sparse Hamiltonian simulation implementation\nIn general, directly implementing the unitary $e^{iHt}$ is challenging because H is an extremely large matrix, and computing its exponential is highly complex. Here we use the methods in [41]. The core idea for implementing"}, {"title": "Appendix B: Scaling analysis", "content": "In this section, we derive the exact scaling of the QLSA algorithm. As mentioned in A 2, the bottleneck of the QLSA algorithm lies in the number of one-sparse Hamiltonian simulations (nHS1) we have to perform. This is determined by two factors:\n1. The number of one-sparse Hamiltonian simulations required to implement the s-sparse Hamiltonian simulation (the unitary).\n2. The number of unitaries (the number of clock qubits, nC) in the quantum phase estimation.\nIt is crucial to recognize that the QLSA algorithm is essentially an iterative method. To suppress the error to a certain level, we need to perform the subroutine for a specific number of iterations. Thus, analyzing the scaling of the QLSA algorithm is equivalent to bounding the error.\nThe error of the QLSA algorithm is mainly introduced by two parts: the error introduced by the Trotterization and the error introduced by the Quantum Fourier Transform (QFT). Previous error analyses mainly focused on the latter, and a comprehensive error analysis of both parts has not been provided. Here, we will provide a detailed error analysis of the QLSA algorithm.\nThe main idea to bound the error of both parts simultaneously is to use an exponential-type error to express the error introduced by Trotterization. We define the Exponential Type Error (ETE) as\n$\\text{ETE} := ||\\tilde{A} \u2013 A||$.  (B1)\nThis error measure quantifies the deviation in the exponent matrix of the Hamiltonian evolution, which can be viewed as the error of the input. Thus, we can separate the error analysis of Trotterization and QFT, and then add the errors together.\nNext, we first derive the exponential-type error of Trotterization, followed by the error of QFT. The derivation is heavily inspired by [13]. Although the paper does not provide an exact error bound for exponential-type error, it provides the main idea of how to derive the error bound. Therefore, it is relatively straightforward to derive it ourselves. We start with:\n$f(t) = e^{iH_1t}e^{iH_2t} ... e^{iH_nt}$ (B2)\nSince f(t) is a unitary operator, we have:\n$f(t) = e^{iH't}$ (B3)\nand we want to bound:\n$\\frac{||H' \u2013 H||}{||H||}$ (B4)\nGiven in the paper, we have:\n$f(t) = \\text{expr} (\\int_0^t d\\tau \\mathbb{E}(\\tau))$  (B5)"}, {"title": "Appendix C: Surface code protocol", "content": "1. Introduction to surface code\nTo reliably store and process quantum information over extended periods, active error correction is essential. This is achieved by encoding multiple physical qubits into logical qubits using quantum error-correcting codes [44\u201346]. Among these, the surface code is not only the most widely used but also one of the most crucial due to its compatibility with the locality constraints of practical quantum hardware, such as superconducting qubits, which only support two-dimensional local operations [47, 48].\nHowever, despite its importance and widespread adoption, the surface code introduces significant computational overhead. The replacement of physical qubits with logical qubits drastically increases space requirements, while the restriction to two-dimensional local operations imposes additional time costs. Arbitrary quantum gates may require multiple time steps instead of executing in a single step, making the choice of surface code schemes\u2014which define parameters such as code distance, logical gate protocols, and resource allocation\u2014critical in determining the actual overhead.\nTo accurately assess the resource demands of a quantum algorithm, it is essential to analyze the specific surface code scheme under which it is executed. By optimizing these schemes, we can minimize space-time overhead and better evaluate the feasibility of quantum computations within a surface-code-based architecture.\n2. Key concepts\n\u03b1. Patch\nA patch is a two-dimensional regular lattice of entangled physical qubits that serves as the substrate on which logical qubits are defined. Physical qubits within a patch are categorized into two types:\n\u2022 Data qubits: These qubits store quantum information and are measured less frequently, primarily during computational operations.\n\u2022 Syndrome qubits: These qubits interact repeatedly with neighboring data qubits and are frequently measured to detect the presence of errors.\nLogical qubits within a patch can perform logical operations (or gates) using a technique known as lattice surgery, which is beyond the scope of this discussion. The code distance, denoted as d, determines the error-correcting capability of the patch. A patch of code distance d consists of $d^2$ physical qubits, with larger values of d yielding higher fidelity computations.\nb. Blocks\nBlocks are functional units that organize multiple patches according to specific rules. Blocks are categorized as follows:\n\u2022 Data blocks: These accommodate logical data qubits and execute logical operations, including logical gates.\n\u2022 Distillation blocks: These are responsible for generating magic states, which are necessary for executing certain non-Clifford gates.\nFor Clifford gates, computations can be efficiently performed within the data block. However, executing a T-gate requires magic state resources, making it significantly more challenging. The distillation block produces magic states, which are then consumed by the data block to enable T-gate operations. Since both processes rely on complex and time-consuming protocols, T-gates become the primary bottleneck of quantum computation.\nC. Protocols\nProtocols define how patches within data blocks and distillation blocks are organized and how magic states are produced and consumed. Protocol selection is crucial in optimizing the space-time trade-off of quantum computations.\n\u2022 Distillation protocols: By allocating more patches to a distillation block, magic states can be generated more quickly or with higher fidelity.\n\u2022 Data block protocols: Increasing the number of patches in a data block enables faster consumption of magic states, enhancing computational speed."}, {"title": "Appendix D: The classical counterparts", "content": "1. Conjugate Gradient Method\nIn this section we introduce the runtime resource estimation of the Conjugate Gradient method.\nDefinition 1 (LSP). The Linear System Problem: Find a vector $x \\in \\mathbb{R}^n$ such that it satisfies equation $Ax = b$ with coefficient matrix $A \\in \\mathbb{R}^{m \\times n}$ and right-hand side (RHS) vector $b \\in \\mathbb{R}^n$.\nA basic approach for solving an LSP is Gaussian elimination, or LU factorization, with $O(N^3)$ arithmetic operations. If $A$ is a square symmetric positive semi definite (PSD) matrix, we can also apply Cholesky factorization with $O(N^3)$ arithmetic operations. The best complexity for an iterative algorithm with respect to N is $O(Ns\\sqrt{\\kappa}\\log(1/\\epsilon))$ arithmetic operations for the Conjugate Gradient (CG) method solving systems with symmetric PSD matrices, where s is the maximum number of non-zero elements in any row or column of A, $\\kappa$ is the condition number of A, and $\\epsilon$ is the error allowed. If matrix A is just symmetric, one can use the Lanczos algorithm with higher complexity. For an LSP with a general square matrix A, the best iterative method is the GMRES algorithm, which has $O(n^3)$ worst-case complexity [49]. For problems in the form of $E^TE x = E^T\\psi$, known as normal equations, one can use a version of CG methods with complexity $O(nd\\kappa_E \\log(1/\\epsilon))$, where $\\kappa_E$ is the condition number of matrix $E$ [49]. For a linear system in general form with a non-PSD non-symmetric matrix, one can use the reformulation $A^T A x = A^T b$ and use a CG method to solve it. Although CG methods for this reformulation have better worst-case complexity than GMRES"}, {"title": "Appendix E: Quantum energy analysis", "content": "The energy consumption of a quantum computer can be estimated as the product of three factors: the scale of the quantum computer (i.e., the number of physical qubits), the energy efficiency of the quantum computer (i.e., power consumption per qubit), and the operating time. This relationship is expressed as:\n$E = n_q \\times P_q \\times T,$ (E1)\nwhere:\n\u2022 E is the total energy consumption,\n\u2022 $n_q$ is the number of physical qubits,\n\u2022 $P_q$ is the power consumption per physical qubit, and\n\u2022 T is the total operating time."}, {"title": "Appendix F: Classical energy analysis", "content": "The estimation of classical energy consumption follows the formula:\n$E_c = t P_c = \\frac{o_p}{f_c}P_c = o_p \\frac{P_c}{f_c}$, (F1)\nwhere $E_c$ represents the total energy consumption, $t$ denotes the runtime, $o_p$ is the total number of clock cycles, $f_c$ stands for the clock frequency, and $P_c$ is the power consumption. Consequently, determining the energy efficiency of CPUs, expressed as $\\frac{P_c}{f_c}$, is crucial for performance evaluation.\nMost mainstream desktop CPUs exhibit power consumption ranging from 50 Watts to 400 Watts, with clock frequencies spanning from 1 GHz to 5 GHz. Given that power and frequency are positively correlated [19], the power efficiency typically falls within the range of 50 Watts per GHz to 80 Watts per GHz. In this analysis, we assume a representative value of 50 Watts per GHz to estimate the CPU energy consumption of the algorithm."}]}