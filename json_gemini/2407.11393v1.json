{"title": "CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation", "authors": ["Kalliopi Basioti", "Mohamed A. Abdelsalam", "Federico Fancellu", "Vladimir Pavlovic", "Afsaneh Fazly"], "abstract": "Controllable Image Captioning (CIC) aims at generating natural language descriptions for an image, conditioned on information provided by end users, e.g., regions, entities or events of interest. However, available image-language datasets mainly contain captions that describe the entirety of an image, making them ineffective for training CIC models that can potentially attend to any subset of regions or relationships. To tackle this challenge, we propose a novel, fully automatic method to sample additional focused and visually grounded captions using a unified structured semantic representation built on top of the existing set of captions associated with an image. We leverage Abstract Meaning Representation (AMR), a cross-lingual graph-based semantic formalism, to encode all possible spatio-semantic relations between entities, beyond the typical spatial-relations-only focus of current methods. We use this Structured Semantic Augmentation (SSA) framework to augment existing image-caption datasets with the grounded controlled captions, increasing their spatial and semantic diversity and focal coverage. We then develop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that sources its control signals from SSA-diversified datasets. We empirically show that, compared to SOTA CIC models, CIC-BART-SSA generates captions that are superior in diversity and text quality, are competitive in controllability, and, importantly, minimize the gap between broad and highly focused controlled captioning performance by efficiently generalizing to the challenging highly focused scenarios.", "sections": [{"title": "1 Introduction", "content": "Image captioning refers to the task of providing an AI system with an input image, and asking the system to describe the visual content in natural language. This process requires the captioning system to understand what objects are present, in what context (e.g., event or scene), and how they relate. Recent deep learning approaches to this task [15,27,32,34, 41, 43, 44, 54, 55,62] surpass human performance in standard image captioning metrics. However, these models tend to generate general captions that describe the entirety of an image, and are often of limited diversity; see Original Captions in Fig. 1.\nControllable image captioning (CIC) overcomes these challenges by generating different descriptions for the same image in a user-controlled fashion. That is, a CIC model receives as input an image paired with a user-specified control signal (e.g., entities or regions of interest), and generates a caption conditioned on the control signal. CIC models are thus capable of generating a diverse set of captions by varying the control signal for the same image; see CIC generated captions 1-15 in Fig. 1.\nIn realistic applications, the easiest way for the user to control the generation of captions is to limit the focus of the desired captions by selecting different entities (objects) using their bounding boxes, as shown in Figs. 1 and 2. Most previous work focuses on such spatial control signals [19, 24, 30, 52, 60, 61]. To improve performance, more recent studies supplement this spatial signal with additional information on the desired length, style, or syntactic and semantic structure of the generated text [13,14], increasing the richness and complexity of control signals. However, for the CIC approach to succeed, the CIC models need to be trained on equally rich datasets that incorporate, explicitly or implicitly, those control signals. Unfortunately, most image captioning datasets today, such as Flickr30k [40] or MS-COCO [19], lack this necessary diversity of controls and corresponding captions.\nOur goal is to achieve SOTA performance in CIC without the need for new, increasingly rich, yet also costly, and impractical-to-collect datasets, where human workers would face the burden of having to provide multitudes of control signals and corresponding descriptive captions. To achieve this goal, we propose a novel Structured Semantic Augmentation (SSA) method, which automatically"}, {"title": "3 Model", "content": "We propose CIC-BART, specifically designed to generate controlled image captions. Specifically, it can generate descriptions of particular areas within a scene with a desired level of detail. Our model, based on VLBART [15], utilizes a transformerbased encoder-decoder architecture, as shown in Fig. 3. CIC-BART extends VL-BART encoder to the CIC task by modifying the encoder input to include: a) a global image embedding that provides the context of the full image to the model; b) the visual control signal, including the visual embedding of the regions that contain the entities of interest; c) the text control signal, containing length control (indicating the desired length range of the output caption) and an optional verb signal that indicates the action we want the generated caption to concentrate on.\nThe visual embeddings of the regions are position-aware embeddings from a Faster R-CNN model [42] trained for visual object and attribute classification [3] on Visual Genome. The global image feature vector is extracted as well from Faster R-CNN. For the length control signal, we add to our vocabulary L tokens for the L different caption length levels; for instance, level one represents sentences between one and nine words, and level two, ten to nineteen. These tokens describe our coarse levels, for a finer sentence size accuracy, we accompany the tokens with the desired number of words. This choice gives our model the capacity to generate diverse captions for a particular length level. Finally, the output of the decoder generates the desired, controlled image caption."}, {"title": "4 Structured Semantic Augmentation (SSA)", "content": "The goal of our SSA method is to augment existing image captioning datasets with new focused captions along with their control signals (i.e., regions corresponding to entities). We rely on datasets where visual entities in the captions are annotated with their corresponding regions (see Sect. 5 for details on the datasets). The SSA process consists of four main steps, as described below. For more details, refer to Appendix B, which includes a step-by-step example of our SSA methodology.\nStep 1: Image-level AMR graph generation. Our objective in this stage is to enclose all the information available from the visually grounded captions into a single representation. To accomplish this, we create a visually grounded AMR graph (vgAMR) for each caption of an image and then merge them into a single image-level graph, the meta-vgAMR. To create the vgAMRs of an image, we first convert each of its N captions to their AMR representation, using the Neural transition-based Text-to-AMR parser [5] which also aligns words in a caption with their respective nodes in the AMR graph. We utilize the alignment information of 1) caption words and AMR nodes (from Text-to-AMR parser) and 2) caption words to image bounding boxes (from existing dataset annotations) to visually ground the AMR nodes. After this step, we get the collection of nodes referring to visual entities, where each grounded meta-AMR node is linked with a non-empty set of bounding boxes. This extended representation, 'AMR + visual grounded nodes', is our vgAMR.\nOur next step is to combine the N vgAMRs to form a single meta-vgAMR. To achieve this, we employ a pairwise strategy to merge the most similar vgAMRs first (we measure similarity with Smatch score [11]). We use the UPGMA hierarchical clustering algorithm [33,37] to find the optimal merge ordering starting from the most similar graphs. UPGMA creates a hierarchy where the bottom level consists of the N individual vgAMRs. By merging all vgAMRs using the UPGMA ordering, we obtain a single structure called meta-vgAMR.\nWhen merging two vgAMRs, the main challenge is identifying which nodes correspond to the same concepts, such as entities, attributes, actions, and relations. We use three node properties to accomplish this: a) visual grounding information, b) semantic similarity of node labels, and c) node neighborhood semantic similarity. We derive two node-merging criteria from there: 1) visually grounded entity nodes are merged if they point to the same image-bounding boxes. When 1) does not hold, we check the second criterion: 2) for the remaining non-grounded nodes, including amr-specific, predicates, adjectives, and adverbs, we use a combination of node label semantic similarity (cosine similarity of the labels using their GloVe embeddings) and neighborhood similarity. Neighborhood similarity examines the similarity of parents for adjectives/adverbs nodes and children for predicate nodes, along with the similarity of connecting edge roles. When two nodes satisfy criterion 1) or 2), we merge them into a single node. Moreover, if they have different labels, we maintain both names by keeping a list of synonyms to increase representation diversity. In Appendix B (Fig. 7) we have included the flow diagram depicting the process of merging two nodes corresponding to the same concepts.\nIn the special case when the two vgAMRs describe two totally different concepts, and hence they have no common nodes, we add an amr-specific node called 'multi-sentence' as the root with the two independent vgAMRs as its children.\nThe final graph, meta-vgAMR, includes all non-redundant elements of the original N captions while preserving the visual grounding between the meta nodes and their respective image regions.\nRemark: Meta-vgAMR efficiently compresses all available image information into a single structure. Following our approach, we can easily scale when new scene information becomes available by applying our pairwise merge procedure.\nStep 2: Event-based graph sampling from image-level AMRs. We start from the predicate nodes, which mainly correspond to verbs, to sample subgraphs in meta-vgAMR graphs. Predicate nodes are identified by their label and the edges connected to them. The label of a predicate node typically follows the format 'predicate_name-xx,' where \u2018xx' represents the different senses a word can have regarding the concept it is used for. Predicate nodes have outward ARGy edges, where 'y' can take values from 0 to 5, connecting them to their arguments. We sample subgraphs from these nodes by following the outgoing argument edges, which are labeled as ARGn in an AMR graph, each defining a particular semantic role (e.g., ARGO points to the agent, ARG1 to the patient, etc.). Finally, we add one more subgraph containing the remaining children branches of other non-ARG optional predicate edges (e.g., 'location', 'time'). We repeat this process until the leaves of the graphs are reached. During sampling, we randomly select one of the synonyms if a node is a list of synonym labels, as mentioned in the previous step. The output of this step is our more focused event-focused subgraphs.\nStep 3: New caption generation from sampled AMRs We use the SPRING AMR-to-Text model [9] to generate new event-focused captions from the sampled vgAMR subgraphs. Because both vgAMR merging and sampling steps introduce noise, the output captions are not always of good quality. We automatically filter low-quality captions by using a linguistic well-formedness measure, GRUEN [63], which is a reference-free metric based on BERT contextual embeddings.\nStep 4: Control signal generation. The last step is to create the control signal for the generated captions. The spatial control signal for a specific caption is extracted from the corresponding sampled vgAMR, by pulling the bounding boxes of the visual entity linked AMR nodes."}, {"title": "4.1 Mixing Strategies of Original and SSA Data", "content": "To analyze the impact of our SSA data, we explore various mixing strategies with the original training set. Assume D represents the training control-caption pairs in the original dataset, containing ND samples, and SSA represents our SSA samples, containing NSSA instances. The augmented dataset DSSA is defined by combining D and SSA: DSSA = samD(TD, \u03c1D) UsamSSA(TSSA,\u03c1SSA), where the functions samp samples a subset of the original dataset, and samSSA a subset of our SSA data. Since we are interested in the effect of our SSA, we assume that samD(TD, \u03c1D) = D, with TD = \u2018Random Sampling Strategy' and \u03c1D = 100%, meaning that all original data are included in the mixed dataset. Depending on the samSSA parameter TSSA we have the cases:\nRandom Sampling Strategy. In this case, we randomly select a pre-specified number of examples from SSA. The parameter \u03c1SSA expresses the percentage of SSA samples included in DSSA. With boundary cases \u03c1SSA = 100% (all NSSA samples are included), and \u03c1SSA = 0% (no SSA data are added).\nUniform-Coverage Sampling Strategy. To mitigate the original dataset's bias (having mainly samples describing the entire image), we aim to create a new focus-unbiased dataset. By modeling the control signal focus as the image area percentage covered by the bounding boxes of the control signal, we split the original data into B coverage bins. Then, we will randomly add in each bin SSA samples, aiming to create a new uniform, coverage-unbiased DSSA dataset. Here, \u03c1ssa contains the range of each bin for the coverage histogram. For example, in the case where we choose ten uniform coverage bins, we have \u03c1ssA = {[0%, 10%), [10%, 20%), . . ., [90%, 100%]}.\nDue to space limitations, we present results from the Random Sampling Strategy for \u03c1SSA = 0% and \u03c1ssA = 100% in the main paper. Results from other scenarios can be found in Appendix D.2."}, {"title": "5 Experimental Setup", "content": "5.1 Data\nWe use Flickr30k Entities (Flickr-Ent) [40] and MS-COCO Entities (COCO-Ent) [19] for training and evaluation. Flickr-Ent augments the original captions of Flickr30k [56] with manually-annotated region-phrase groundings. Flickr-Ent contains the original 31K images annotated with five captions each. COCO-Ent augments the original MS-COCO [29] (120K images each annotated with around five captions) with semi-automatically collected grounding annotations; see [19] for details on the annotation process. For both datasets, we follow previous work and use the training and test splits by Karpathy et al. [26].\nWe apply our SSA algorithm on the aforementioned datasets to create their augmented variations, COCO-Ent-SSA and Flickr-Ent-SSA, containing about"}, {"title": "5.2 Models and Evaluation Metrics", "content": "We compare two variations of our model (with and without SSA augmentations) with SOTA models as our baselines: Show Control & Tell (SCT) [19] that uses region-based control (bounding boxes of visual entities of interest); ASG2Caption (ASG) [14] that draws on visually grounded abstract scene graphs as control signal; and VSR [13] that uses overly descriptive control signals that express verb(s) and fine-grained verb-specific semantic roles of the desired captions; ComPro [53] that learns a mapping from the bounding boxes of the entities of"}, {"title": "6 Results", "content": "6.1 Overall Performance\nWe first compare the overall performance of our models with the SCT, ASG, and VSR baselines with respect to controllable captioning metrics. We do not include ComPro in this comparison due to the unavailability of the codebase. We also exclude LaBERT since this model solely focuses on length controllability.\nTab. 1 presents results for content and length controllability (IoU and L), text quality (G), and diversity (sC, D-1, D-2), as well as the harmonic mean (H). For both datasets, CIC-BART-SSA has the best performance in all metrics, except IoU, where it is the second best. Specifically, CIC-BART-SSA is superior to all other models with respect to diversity (sC, D-1, D-2) and text quality (G), but comparable to VSR in terms of content controllability (IoU). The length controllability (L) scores show that our SSA augmentation helps the model learn to generate high-quality output at the desirable length (compare CIC-BART and CIC-BART-SSA). This is due to the increased diversity in caption length provided by our SSA augmentations.\nImportantly, we can see that model performance can vary depending on the metric. E.g., whereas VSR has the highest IoU, it falls behind in text quality and diversity. In our qualitative analysis, we observe the poor quality of the captions generated by VSR. The best-performing model should be identified based on the H score that summarizes content controllability, text quality, and diversity into"}, {"title": "6.2 Effect of SSA on Content Controllability", "content": "To analyze the impact of our SSA augmentations, we measure the content controllability (IoU) performance of CIC-BART at different levels of focus of the control signals and report it in Fig. 4. We use coverage, defined as the area of the image enclosed by the bounding boxes of the entities of interest in the control signal, to quantify that focus. For example, highly focused control signals cover a small area, yielding low coverage, while broader signals cover a larger area and have high coverage. We 'break down' the IoU performance into 10 coverage bands and report the average IoU over control signals in those bands. In addition,"}, {"title": "6.3 Qualitative Analysis", "content": "In Fig. 5, we present qualitative examples from the original test sets. examples from our SSA (only) test set control signals. In these examples, the control signals are extracted from the ground-truth captions. In the two figures, each highlighted word found in the generated controlled captions corresponds to the control entity of the same color. This shows the match between the captions produced and the control signal. We also strike through the parts where the model hallucinates or generates redundant references to the entities of interest. Our models have been observed to outperform the previous state-of-the-art models by substantially enhancing the quality of the generated controlled captions. This behavior was expected from our quantitative analysis, which showed that our models have significantly higher text quality (G). More importantly, our CIC-BART-SSA model is capable of generating captions that are faithful to the control signal and better understand the relationships that connect the entities of interest."}, {"title": "7 Conclusions", "content": "We address two main challenges faced by the controllable image captioning (CIC) models. First, standard image-caption datasets lack the controllability and diversity needed for proper training and evaluation of CIC. Second, most recent SOTA models require complex and overly descriptive control signals as input (including, e.g., the main action/verb to appear in the generated caption). To address the first challenge, we propose a novel technique that draws on a structured semantic augmentation (SSA) formalism to generate focused captions and"}]}