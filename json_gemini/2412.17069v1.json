{"title": "Optimizing Data Curation through Spectral Analysis and Joint Batch Selection (SALN)", "authors": ["Mohammadreza Sharifi"], "abstract": "In modern deep learning models, long training times and large datasets present significant challenges to both efficiency and scalability. Effective data curation and sample selection are crucial for optimizing the training process of deep neural networks. This paper introduces SALN, a method designed to prioritize and select samples within each batch rather than from the entire dataset. By utilizing jointly selected batches, SALN enhances training efficiency compared to independent batch selection. The proposed method applies a spectral analysis-based heuristic to identify the most informative data points within each batch, improving both training speed and accuracy. The SALN algorithm significantly reduces training time and enhances accuracy when compared to traditional batch prioritization or standard training procedures. It demonstrates up to an 8x reduction in training time and up to a 5% increase in accuracy over standard training methods. Moreover, SALN achieves better performance and shorter training times compared to Google's JEST method developed by DeepMind. The code and Jupyter notebooks are available at github.com/rezasharifi82/SALN.", "sections": [{"title": "1 Introduction", "content": "The effectiveness of a deep learning model's training process largely depends on the quality of the data to which the model is exposed. High-quality data enables the model to learn accurately and generalize well to new, unseen data, improving its overall performance. [5]. Training a deep neural network model on a curated dataset can significantly improve both accuracy and efficiency [8].\nThere are many well-known methods that apply a kind of data point selection heuristic on the individual manner [7]. However, batches of data can have inter-dependencies [1] that may lead to much better results than processing them individually [23]. Moreover, there are some approaches for selecting well-informed batches of data and using them in the training process. These methods represent algorithms or criteria for selecting batches of data that may gain the most information from our dataset [8].\nIn this paper, a heuristic is proposed to select the most informative batch of data, which can lead to better accuracy as well as shorter training time. This method is based on the principles of eigenvalues and eigenvectors, leveraging these mathematical tools to identify the core structure and the most informative parts of the data [16, 20], rather than using random selection improved by loss control [8]."}, {"title": "2 Related Work", "content": "JEST: Selecting an informative batch of data can lead to a dramatic improvement in training time and computational resource usage of the model. Recent research conducted by Google DeepMind has shown that selecting an informative batch of data can lead the model to significantly reduce iterations and computations [8]. Moreover, it can achieve the same or even better performance compared to using all of the data.\nSpectral approaches:Past research has explored various methods for enhancing the efficiency of machine learning models, particularly through the use of spectral techniques. Eigenvectors and eigenvalues have been central to Principal Component Analysis (PCA) and Spectral Clustering, where they are used to identify the most significant dimensions in the data. These methods have proven effective in reducing the complexity of data and uncovering latent structures, which can improve model performance [16, 20]. In the context of data selection, prior work has focused primarily on filtering or pruning individual samples based on heuristics such as importance sampling or noise reduction [24]. Additionally, methods such as Core-set selection have been introduced to identify a small but representative subset of data for training large-scale models, reducing computational costs while maintaining accuracy as good as possible [2]. These approaches, however, often operate on individual samples and do not take significant advantage of the relationships between batches of data points. Recent works have also explored spectral learning techniques to guide batch selection. Methods like Spectral-Net [25] utilize the spectral properties of data to group data points into batches that better capture the underlying structure of the dataset, resulting in more efficient training.\nBatch Selection: Batch selection in machine learning has gained considerable attention in recent years [8] as a strategy to improve the efficiency of training models, particularly deep neural networks. Unlike methods that focus on individual data point selection, batch selection methods aim to identify groups of data that are most informative or representative together, thereby accelerating the learning process and improving model performance [8]. Early work on mini-batch gradient descent [4] demonstrated that training on small batches of data could significantly reduce computational time while retaining much of the accuracy of full-batch methods. Since then, several approaches have been developed to enhance the effectiveness of batch selection. For instance, Core-set selection [24] selects small but representative subsets of data that approximate the original dataset's distribution, leading to faster convergence and lower resource usage.\nActive Learning: In the context of active learning, batch active learning approaches such as Batch-BALD [17] seek to select batches of data that maximize the information gain during training. This contrasts with traditional active learning techniques that focus on one data point at a time, demonstrating the benefits of jointly selecting data in batches for training efficiency."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Data Preparation", "content": "Data preprocessing plays a crucial role in the training process of machine learning models [11]. Proper preprocessing can mitigate issues such as overfitting [19] and ensure the model generalizes better to unseen data [3]. In this study, I have used the Oxford-IIIT Pet Dataset [22] as the primary dataset and the CIFAR-10 as the secondary dataset for training the model. The Oxford-IIIT Pet Dataset contains images of 37 different breeds of cats and dogs, which are labeled and categorized. Meanwhile, the CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class and training and test sets of 50,000 and 10,000 images, respectively.\nThe data augmentation techniques used in this study are as follows:\n\u2022 Horizontal Flip: Random horizontal flipping helps the model improve its robustness to horizontal orientation variations [19].\n\u2022 Rotation: Random rotation by $\\alpha = 15$ can significantly help the model become invariant to rotational changes [26].\n\u2022 Change on jitter: To prevent overfitting, one of the most significant augmentations is to adjust the jitter parameters [13]. That involves the following configurations:\nBrightness: 0.2\nContrast: 0.2\nSaturation: 0.2\nHue: 0.1\n\u2022 Resize: This ensures a uniform input size for all images, which is essential for deep neural networks [27].\n\u2022 Normalization: Normalizing the pixel values to standardize the input data. The parameters of mean and standard deviation has been set to an appropriate number due to ImageNet statistics. [15, 19].\nMean: [0.485,0.456, 0.406]\nStd.: [0.229, 0.224, 0.225]\nThese augmentations were applied to the training data. However, for the test data, since data augmentation is not appropriate [10, 26], I have just used resizing, cropping, and normalization. Also, I split my dataset into training, validation, and test sets."}, {"title": "3.2 The model", "content": "In this study, I used the pre-trained ResNet-18 model as my main classifier for several reasons:\n\u2022 Vanishing Gradients problem: The residual connections in ResNet-18 efficiently prevent the vanishing gradient problem by allowing information to bypass layers, facilitating the training of deeper networks [12].\n\u2022 Strong Transfer Learning Performance: ResNet-18 is strong enough to capture complex features while still being lightweight, which is enhanced by its pre-trained configuration [18, 12].\n\u2022 Accessability: Resnet-18 is available in PyTorch and there is no need to be built from scratch.\nSo, because of all the reasons listed above, I decided to use the pre-trained version of ResNet-18 as a lightweight, reliable model that is generalized enough to perform significant classification tasks. As the final layer of this model, I modified the output of the fully-connected layer to have appropriate units, which is corresponds to the number of classes in each dataset."}, {"title": "3.3 Loss function and Optimizer", "content": "For all of the experiments, I have used Cross Entropy-Loss as the ideal loss function for multi-class classification tasks. It combines LogSoftMax and negative log likelihood into one function [10].\nMeanwhile, for the optimization task, I have used Stochastic Gradient Descent (SGD) with momentum = 0.9 and learning rate = 0.001."}, {"title": "3.4 Joint example selection", "content": "Joint example selection with sigmoid loss is a new method introduced by Google DeepMind [8]. It is an intuitive procedure for selecting data in a meaningful way from a batch, rather than relying on random selection, to feed into the model. This algorithm is based on several important principles [8]:\n\u2022 Loss calculation: This algorithm needs both reference loss and learner loss.\nReference loss: The model's current performance on each example.\nLearner loss: A reference loss that could represent a baseline(performance of a reference model).\n\u2022 Filter ratio: This ratio is a hyperparameter which is used to select the amount of data.\n\u2022 Interaction: This algorithm uses an intuitive procedure to prioritize those data from a batch which are most informative rather than others. In fact, it calculates the impression of selected data on the remaining ones, as well as the remaining ones on the selected data. In the other word, it can measure how the selected data could interact with a remaining one.\n\u2022 Not using old ones: This algorithm uses a probability distribution which assigns a very-low weight to the previously selected samples. This action would guarantee the algorithm will not choose the previously-selected samples.\n\u2022 Size of chunks: This algorithm has a parameter that adjusts the number of data points representable in each chunk, associated with the filter ratio."}, {"title": "3.5 SALN(My method)", "content": "In this approach, I've utilized spectral analysis to identify and prioritize the most informative data based on their structural significance within a batch. Since using spectral approaches can leverage the core structure of the data [20, 21], my methodology consists of the following steps:\n1. Feature extraction: Using a pre-trained model without fine-tuning to extract features from the whole dataset can dramatically reduce time and computational resources [10]. This step is crucial in this study in order to have a comprehensive features for each data point. I have used ResNet-50 as a reference pre-trained model to extract the features of each data and convert them to a vector.\n2. Compute similarity matrix: For each batch of data,computing a similarity matrix $S$ using cosine similarity can significantly improve the algorithm's understanding of meaningful structures, which is useful for further spectral analysis. [28, 20].\n3. Degree matrix: The degree matrix $D$ is a diagonal matrix where each element $D_{ii}$ represents the sum of the similarities of all relevant data point $i$ in a dataset. The degree matrix plays a key role in defining the Laplacian matrix [6].\n4. Laplacian matrix: The Laplacian matrix $L$ is derived from the degree matrix $D$ and the similarity matrix $S$, typically computed as $L = D \u2013 S$. The Laplacian matrix encodes the structure of the dataset by capturing the relationships between data points.\n5. Eigen decomposition: To score the data and their relationships, the second smallest eigenvalue of the Laplacian matrix is calculated, which corresponds to the Fiedler value [9]. This approach utilizes the spectral properties of the Laplacian to identify samples that are essential for preserving the structural integrity of the current batch [9, 20, 21].\n6. Informative indices: The resulting vector consists of indices corresponding to the most informative data points in the current batch. The size of this vector is determined by the filter ratio.\n7. Joint batch sampling: Similar to the JointExample Selection algorithm [8], the introduced method implicitly uses the similar criteria, with a slight difference in the scoring mechanism, which is handled through spectral analysis."}, {"title": "3.6 Algorithm", "content": "The following algorithm describes the procedure for Joint batch sampling using Laplacian matrix and spectral analysis. The algorithm takes as input a batch of feature vectors corresponding to current batch's data. These vectors had been extracted in the manner which has described in previous section. Now the algorithm selects data points which are the most informative in this batch relative to filter ratio parameter. The filter"}, {"title": "4 Experiments", "content": "All the experiments were conducted on those datasets which have described in the previous section. The experiments were conducted on a Google-Colab service wit T4-GPU. Number of epochs and other important factors were kept the same. The experiments have been conducted on the same model, ResNet-18, with the same hyper-parameters and configurations. The only difference between the experiments is the method of selecting the data points in each batch during the 25 epochs."}, {"title": "4.1 Primary Dataset: Oxford-IIIT Pet", "content": "The primary dataset used in this study is the Oxford-IIIT Pet Dataset, which contains images of 37 different breeds of cats and dogs. The dataset is labeled and categorized, making it suitable for classification tasks."}, {"title": "4.1.1 Standard training and SALN", "content": "The following experiment has been conducted to compare the performance of the standard training method as the baseline, with the proposed SALN method. The standard training method uses no specific selection criteria in order to select the data, while SALN employs spectral analysis to identify the most informative data points in each batch. The standard training method processes the entire dataset without any form of selective sampling or data reduction. Every data point is used during training, and no prioritization or filtering is applied to optimize the learning process. The results of this experiment consisted from several reports:\n\u2022 Comparison between training and validation accuracy(%) of SALN and standard training, which has represented in Table 1.\n\u2022 Comparison between training and validation loss of SALN and standard training, which has represented in Table 2."}, {"title": "4.1.2 JEST and SALN", "content": "The following experiment has been conducted to compare the performance of the JEST method which has proposed by Google DeepMind, and the SALN method. The JEST method uses a specific selection criteria in order to select the informative data. This criteria were discussed in the previous section. The results of this experiment consisted from several reports:\n\u2022 Comparison between training and validation accuracy(%) of SALN and JEST, which has represented in Table 5.\n\u2022 Comparison between training and validation loss of SALN and JEST, which has represented in Table 6.\n\u2022 Comparison between test set accuracy of SALN and JEST, which has represented in Table 7.\n\u2022 Comparison between training time of SALN and JEST, which has represented in Table 16."}, {"title": "4.2 Secondary Dataset: CIFAR-10", "content": "The secondary dataset used in this study is the CIFAR-10 dataset, which contains 60,000 32x32 color images in 10 classes, with 6,000 images per class and training and test sets of 50,000 and 10,000 images, respectively."}, {"title": "4.2.1 Standard training and SALN", "content": "Same as the primary dataset, the following experiment has been conducted to compare the performance of the standard training method as the baseline, with the proposed SALN method. The results of this experiment consisted from several reports:\n\u2022 Comparison between training and validation accuracy(%) of SALN and standard training, which has represented in Table 9.\n\u2022 Comparison between training and validation loss of SALN and standard training, which has represented in Table 10.\n\u2022 Comparison between test set accuracy of SALN and standard training, which has represented in Table 11.\n\u2022 Comparison between training time of SALN and standard training, which has represented in Table 12."}, {"title": "4.2.2 JEST and SALN", "content": "Same as primary dataset, the following experiment has been conducted to compare the performance of the"}, {"title": "4.3 SALN data-selection visualization", "content": "In this section, the data selection process of the SALN algorithm will be visualized. The visualization will show the top 50% data points that were selected by the algorithm in each batch. Here, I have selected Batch No.0 and Batch No.1 from the Oxford-IIIT Pet dataset and, Batch No.50 and Batch No.51 from the CIFAR-10 dataset."}, {"title": "4.4 Analysis of SALN Weights and Filters", "content": "In this section, an analysis of the model which has trained on Oxford-IIIT Pet dataset will be presented."}, {"title": "5 Discussion", "content": "The proposed method, SALN, introduces a novel approach to jointly batch sampling through the application of spectral methodologies. This technique enhances the selection of data points during training, aiming to accelerate the learning process when compared to standard training methods.\nThe results from our experiments indicate that SALN offers a substantial reduction in training time by utilizing an optimized jointly batch sampling mechanism. The spectral techniques employed in this method, enable the model to prioritize the most informative data.\nAnd thereby, improving both the speed and effectiveness of training procedure.\nAdditionally, the experiments demonstrate the high potential of data bootstrapping in deep neural network training process. By focusing on batches that maximize the learnability, SALN ensures that the model is consistently exposed to the most challenging and relevant examples, which contributes to more efficient and robust learning.\nIn summary, SALN provides a promising advancement in batch sampling strategies, offering significant improvements in training efficiency, using less time than JEST, without sacrificing model performance."}]}