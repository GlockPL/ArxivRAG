{"title": "BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger Visual Cues", "authors": ["Sara Sarto", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "abstract": "Effectively aligning with human judgment when evaluating machine-generated image captions represents a complex yet intriguing challenge. Existing evaluation metrics like CIDEr or CLIP-Score fall short in this regard as they do not take into account the correspond-ing image or lack the capability of encoding fine-grained details and penalizing hallucinations. To overcome these issues, in this paper, we propose BRIDGE, a new learnable and reference-free image caption-ing metric that employs a novel module to map visual features into dense vectors and integrates them into multi-modal pseudo-captions which are built during the evaluation process. This approach results in a multimodal metric that properly incorporates information from the input image without relying on reference captions, bridging the gap be-tween human judgment and machine-generated image captions. Experiments spanning several datasets demonstrate that our proposal achieves state-of-the-art results compared to existing reference-free evaluation scores. Our source code and trained models are publicly available at: https://github.com/aimagelab/bridge-score.", "sections": [{"title": "1 Introduction", "content": "The objective of image captioning is to produce natural language descriptions conditioned on input images, that closely resemble human language and align to human intentions. As such, the captioning task involves the recognition and understanding of the visual content of the image, including fine-grained elements such as objects, attributes, and their relationships. Advances in training methodologies and architectures have contributed to the progress in the field, significantly improving the generation quality. Recent innovations include fully-attentive models [13, 14, 17], improved connections between visual and textual modalities [14,41], and the incorporation of objects and tags at an architectural level [3, 31,63]. Additionally, there has been a notable focus on increasing the robustness of cross-modal features [5, 32, 49], which consequently can increase description accuracy.\nAs constant improvements are made on the generation side, it becomes cru-cial to enhance the evaluation process as well. In this regard, image captioning"}, {"title": "2 Related Work", "content": "Classical Reference-based Captioning Metrics. Several widely used cap-tioning evaluation metrics were originally developed in the context of NLP tasks and rely on n-gram matching techniques. Among these classical metrics, BLEU [42] is designed to focus on precision and incorporates a penalty for sen-tence brevity. METEOR [4], instead, combines precision and recall to evaluate the quality of captions, while others, such as ROUGE [33], were initially born for summarization tasks and later adapted to image captioning. More recently, two metrics tailored for visual captioning have emerged: CIDEr [56], which measures n-gram similarity and is based on TF-IDF, and SPICE [2], which quantifies graph-based similarity through scene graphs constructed from candidate and reference captions. Overall, focusing on textual-level comparisons, these metrics assume that human-written references accurately represent the image content.\nLearnable Captioning Metrics. With the rise of large pre-trained models, image captioning evaluation now frequently exploits these models to compare textual-only [61, 64] or visual-textual [18, 21, 22, 26, 27, 58] contents. Notably, the BERT score and its improved version use pre-trained BERT embeddings to compare word tokens in generated and ground-truth sentences.\nSome metrics, like BLEU and CIDEr, rely solely on text matching between reference captions and machine-generated captions, potentially introducing bias in evaluations due to non-accurate reference captions. To mitigate these issues, alternative solutions leverage the multimodal nature of vision-and-language mod-els. As an example, TIGEr [22] considers the similarities between words and im-age regions and assesses how well machine-generated captions represent image content and their alignment with human-generated captions.\nIn contrast, other approaches [18, 24, 27] leverage web-scale vision-and-language models such as VilBERT [39] and CLIP [43] for more robust metrics. For example, in [24], CLIP visual-textual features are used to compute negative Gaussian cross-mutual information. Other works, instead, have employed diffu-sion models in text-only tasks [65] or exploited the zero-shot language modeling capabilities of large language models [8] to evaluate candidate captions."}, {"title": "3 BRIDGE for Captioning Evaluation", "content": "In the following, we present our reference-free captioning evaluation approach, termed BRIDGE. Our approach leverages a dual-encoder architecture, which comprises both a language encoder and a vision encoder. Given a frozen pre-trained model, we train a mapping module responsible for filling the holes of a masked template caption with pseudo language tokens that are enriched with visual information. An overview of our model is depicted in Fig. 2."}, {"title": "3.1 Preliminaries", "content": "Our approach relies on CLIP (Contrastive Language-Image Pre-training) [43], a powerful vision and language model designed to align images and corresponding text captions within a shared embedding space. For a given input image $I$, the image encoder $E_v$ extracts the visual information $v = E_v(I) \\in \\mathbb{R}^d$. On the textual side, an input caption $T$ is tokenized and a textual representation is obtained by passing it through the textual encoder $E_\u0442$, obtaining $t = E_\u0442(T) \\in \\mathbb{R}^d$. Once the textual and visual features, $t$ and $v$ respectively, are projected in a common space, visual and textual inputs can be compared via cosine similarity.\nThe relationships learned by CLIP can be exploited to build an image cap-tioning evaluator. In CLIP-Score [18] the authors directly compare candidate captions and images in the embedding space and show that this achieves a good correlation with human judgments. In detail, to assess the quality of a candidate generation, they feed both the image and the candidate caption through their respective feature extractors, and compute the cosine similarity of the resultant embeddings:\n$CLIP-Score(I, T) = w\\cdot max(cos(v, t), 0),$ (1)\nwhere $w$ is a rescaling factor employed to stretch the score distribution while ensuring the ranking results remain unchanged."}, {"title": "3.2 Injecting Fine-Grained Visual Features", "content": "Unlike CLIP-Score, our approach does not rely exclusively on global image de-scriptors for evaluating image-text alignments. Instead, we focus on employing stronger visual information. To do so, we draw inspiration from the Pic2Word ap-proach [47] and represent the input image through a multimodal pseudo-caption, an embedding representation that contains stronger visual elements.\nBuilding Template Captions. In order to create multimodal pseudo-captions, we first build template captions for a given input image. These are skeletal textual representations of the image, obtained by masking out all the relevant textual concepts from the descriptions generated by a captioner. Through these template captions, we aim to provide the model only with a templated textual structure which can then be filled with more fine-grained visual features. In particular, given an automatically generated caption describing the input image, such as 'A man running with a white dog', we remove the main subjects within the caption (e.g. 'man' and 'white dog') and mask them with a [MASK] token. This will allow the model to fill in these gaps by incorporating more fine-grained features from the image encoder. Since a primary subject might be described by words other than just its corresponding nouns (e.g. adjectives), we utilize noun chunks. Fig. 3 reports template captions and corresponding noun chunks.\nGiven a sentence containing N noun chunks, we independently encode them through the mapping network. To this aim, we replicate the template caption as many times as the number of noun chunks and mask a different noun chunk in each of the replicas. We thus obtain N different versions of the template caption, each one masking a single noun chunk, for instance\n['A [MASK] running with a white dog',\n'A man running with a [MASK]']."}, {"title": "Mapping with Fine-Grained Visual Features.", "content": "The above-described masked replicas are then fed to the mapping module 4. Specifically, our approach ex-ploits the visual information extracted from the visual encoder $E_v$ to enrich the replicas with the informative content of the image I. To get a more fine-grained representation of the image, we directly take the grid of features from the last layer, v. For instance, in the case of a ViT-B/32 backbone, this will have a shape of 50 \u00d7 d, where d is the dimensionality of the last embedding of the network.\nThe mapping network is implemented as a stack of Transformer [55] encoder layers interleaved with cross-attention layers. Its role is to refine each template captions with visual information. Since each template caption is processed inde-pendently, the mapping module returns a set of sequences, each with the same length as the corresponding input template caption. From the output of the mapping module, we keep only the predictions for the masked tokens in each template caption and copy them back into the original templates.\nTherefore, by providing a masked input template in the form $T = [w_1, ..., w_{j-1}, \\text{MASK}, w_{j+1}, ..., w_T]$, where ${w_j}_i$ represent original tokens from the input caption, we obtain $T = [0,1,..., w_1, ..., w_{j-1}, \\psi(T_i) j, w_{j+1}, ..., w_T]$, where $(\\mathcal{T}_i,\\theta);$ represents the output of the mapping network at the position corre-sponding to the masked input token position. In the case of noun chunks consist-ing of more than one token, multiple consecutive tokens are replaced with the corresponding outputs from the mapping network. By injecting the outputs of the mapping token into the initial template caption, we effectively complete the original templates with visually enriched vectors. Notably, these newly gener-ated pseudo-captions combine word sequences from the template captions with dense vectors obtained by the mapping module. Consequently, they cannot be decoded as standard captions. As a last step, the obtained pseudo-captions are fed into the pre-trained CLIP language encoder."}, {"title": "3.3 Training Protocol", "content": "To train our mapping network, the loss is defined as a weighted version of the symmetric InfoNCE loss [40], where positive and negative items are weighted according to the number of noun chunks in each caption.\nSpecifically, given a batch in the form $B = \\{(I_i, T_i)\\}_{i=1}^N$, where $I_i$ and $T_i$ rep-resent image-caption pairs, each image $I_i$ is expanded in $N_i$ multimodal pseudo-captions as outlined above, where $N_i$ is the number of noun chunks in caption $T_i$. Further, let $t_{ij}$ represent the embedding vector of the j-th pseudo-caption derived from the i-th image, $v_i$ the embedding vector of the i-th image and $t_i$"}, {"title": "3.4 Inference and Score Computation", "content": "At inference time, given an image-candidate caption pair (I,T), we extract all pseudo-captions from I using our mapping network. Subsequently, we compute the mean pseudo-caption embedding as $t^* = \\frac{1}{N} \\sum_i^* t_i^*$, where $t_i^*$ indicates the i-th pseudo-caption extracted from I and N here indicates the overall number of pseudo-captions associated with I.\nAt that point, given the visual embedding v of the image and the embedding of the candidate caption t, the matching score between I and T is defined as\n$BRIDGE(I, T) = 0.5\\cdot [CLIP-Score(I, T) + w \\cdot max(cos(t^*, t), 0)],$ (4)\nwhere cos indicates the cosine similarity and w is a constant scaling factor."}, {"title": "4 Experimental Evaluation", "content": "4.1 Implementation Details\nArchitecture and Training Details. Building upon prior research [18,24,51], we use either CLIP [43] ViT-B/32 or ViT-L/14 as backbone for the visual and textual encoder. The mapping module is composed of two Transformer layers and is trained on the COCO dataset [34], which contains more than 120k images annotated with five captions. In particular, we employ the splits introduced by Karpathy et al. [23], where 5,000 images are used for both validation and testing and the rest for training. To map the grid visual features to an embedding space of dimension 512, we employ a simple linear projection. For the regularization branch, we utilize a two-layer multi-layer perceptron.\nDuring training, we use AdamW [38] as optimizer with a learning rate equal to 0.0001 and a batch size of 256. The A1, A2, and 3 values are selected with a grid search, choosing the combination that provides the best validation loss. Specifically, we set both A\u2081 and A3 to 0.01, while A2 is set to 1.0. The training stage lasts around one day on a single A100 GPU.\nTemplate Caption Generation. The template captions used as input for the mapping module are generated using the BLIP model [29]. In particular, we use the ViT-L/14 version pre-trained on 129M image-text pairs and finetuned on the COCO dataset. After this generation phase, the primary subjects of the template sentences are extracted by using the NLTK library [6]. During training, two noun chunks are randomly chosen from the set identified during the extraction step. In the evaluation phase, otherwise, all identified noun chunks are included."}, {"title": "4.2 Datasets", "content": "To evaluate the correlation of the proposed metric with human ratings, we conduct experiments on the Flickr8k-Expert, Flickr8k-CF, Composite, and Pascal50-S datasets [1,19,56]. In addition, for detecting hallucinations in textual sentences, we extend our analysis to the FOIL dataset [50]. Except for Pascal-50S and FOIL in where accuracy scores are used, evaluation on all other datasets relies on Kendall \u0442\u044c, Kendall Te, and Spearman p correlation scores.\nFlickr8k-Expert and Flickr8k-CF [19]. These datasets consist of image-caption pairs with corresponding human ratings. Specifically, Flickr8k-Expert comprises 17k expert annotations for visual-textual pairs, with a total of 5,664 different images. Each pair receives a score ranging from 1 (lack of correlation) to 4 (accurate depiction), where 1 indicates a lack of correlation between the caption and the image, and 4 indicates an accurate depiction of the image without errors.\nOn the other hand, Flickr8k-CF is composed of 145k binary quality judgments, collected from CrowdFlower, for 48k image-caption pairs containing 1,000 unique images. Each pair is annotated with at least three binary scores, where \"yes\" denotes that the caption correlates with the image. To measure the alignment with human judgment, we compute the mean proportion of \"yes\" annotations as the score for each pair."}, {"title": "4.3 Ablation Studies and Analysis", "content": "To evaluate the effectiveness of our metric, we start by analyzing variations of our main architectural components. Then, we assess the impact of caption templates in our score formulation. All these experiments are performed using CLIP VIT-B/32 as backbone and reported in Table 1.\nContribution of Architectural Components. We first investigate the per-formance of the most straightforward implementation of a mapping module, structured as a two-layer MLP following [47]. We also validate the importance"}, {"title": "4.4 Comparison with State-of-the-Art Captioning Metrics", "content": "Evaluating Sample-Level Human Correlation. We evaluate the sample-level human correlation on the Flickr8k [19] and Composite [1] datasets. Follow-ing previous works [18,64], we compute Kendall correlation scores in both \u0463 and Te versions and also include the Spearman p score. Results are reported in Ta-ble 3, where we compare our proposed BRIDGE metric against other reference-free evaluation scores like UMIC [26], CLIP-S [18], and PAC-S [48]. Moreover, we also compare with standard captioning evaluation metrics (i.e. BLEU [42], \u039c\u0395-TEOR [4], CIDEr [56], and SPICE [2]) and more recent reference-based solutions that exploit text-only or cross-modal learned embeddings, such as BERT-S [64], BERT-S++ [61], TIGEr [22], VilBERTScore [27], and MID [24]. For complete-ness, we also include the reference-based versions of CLIP-S and PAC-S, termed RefCLIP-S and RefPAC-S, which however are not directly comparable with our solution as both rely on a set of five reference captions.\nAs it can be seen, BRIDGE outperforms other reference-free metrics in terms of correlation with human judgment, achieving the highest scores on almost all datasets. Specifically, compared to CLIP-S, BRIDGE shows improvements in terms of Kendall T of +3.6 and +2.8 points when using ViT-B/32 and ViT-L/14"}, {"title": "4.5 System-level Correlation", "content": "Finally, we delve into the efficacy of our proposed metric when evaluating popular existing captioning models. To this aim, we generate predictions of several state-of-the-art captioning models on the COCO test set, including Show and Tell and Show, Attend and Tell which are among the first image captioning models based on deep learning, Up-Down [3], SGAE [60], AoANet [20], M\u00b2 Transformer [14], X-Transformer [41] which all include region-based image features with either LSTM-based or Transformer-based language models, and the recently proposed COS-Net model [32] that incorporates CLIP features. In addition to reporting evaluations on traditional captioning models, we also include recent LLM-based captioning models, including ZeroCap [52] and SmallCap [45], which are based on GPT-2 [44], and MiniGPT-v2 [9], BLIP-2 [28], IDEFICS [25], LLaVA-1.5 [35,36], and InstructBLIP [15] which instead are based on larger-scale LLMs like Flan-T5 [12], Vicuna [11], or LLaMA [53,54]."}, {"title": "5 Conclusion", "content": "In this paper, we have presented a novel learnable, and reference-free image captioning metric that combines text and dense visual features. Our proposal, BRIDGE, employs templated captions that are enriched with fine-grained vi-sual cues thanks to a mapping network. Through experimental evaluation, we demonstrate that BRIDGE outperforms existing reference-free metrics in terms of correlation with human judgment and sensitivity to hallucinated objects."}, {"title": "A Weighted Contrastive Loss", "content": "In Section 3.2 of the main paper, we state that we employ a weighted variant of the symmetric InfoNCE loss [40]. Specifically, our method involves building mini-batches of multimodal pseudo-captions derived from a set of image-caption pairs. To recall the notation of that section, the mini-batched are in the form B = {(Ii, Ti)}1, where I\u2081 and T\u00bf represent image-caption pairs. Each image I\u2081 is expanded in Ni multimodal pseudo-captions, with Ni representing the number of noun chunks in caption Ti. As in the main paper, we denote tij as the embedding vector of the j-th pseudo-caption derived from the i-th image, vi as the embedding vector of the i-th image, and ti as the embedding vector of the i-th ground-truth caption. Finally, let M be the total number of noun chunks in the mini-batch, i.e. M = -1 Ni.\nThe first weighted contrastive loss aligns the embeddings of the multimodal pseudo-captions with the global visual features of the corresponding images. This step ensures that each pseudo-caption is appropriately contextualized within the overall visual context. This loss is defined as a weighted version of the symmetric InfoNCE loss because positive and negative items are weighted according to the number of noun chunks in each caption. The rationale behind this choice is that captions having more noun chunks tend to have more visual variance. We therefore assign them a higher weight to promote the transfer of proper visual features. Formally, the loss is defined as follows\n$L\u2081 = \\frac{1}{M} \\sum_{i=1}^N \\sum_{j=1}^{N} - log \\frac{exp(cos(v_i, t_{ij*})/\\tau)}{\\sum_{k\u2260i}^N \\mathcal{N}_k exp(cos(v_k, t_{ij*})/\\tau)} + $\n$\\frac{1}{M} \\sum_{i=1}^N \\sum_{j=1}^{N} - log \\frac{exp(cos(v_i, t_{ij*})/\\tau)}{\\sum_{k\u2260i}^N  exp(cos(v_i, t_{ik*})/\\tau)},$ (5)\nNoticeably, differently from the standard InfoNCE loss, we also remove the pos-itive item from the denominator of each loss component.\nIn addition to the above-defined loss, we define a second loss component that promotes the alignment between pseudo-captions and the textual feature vector of the ground-truth caption corresponding to the input image. This makes sure that pseudo-captions are aligned also on a textual space, in addition to being"}, {"title": "B Additional Experimental Results", "content": "Effect of Changing the Underling Backbone. The proposed BRIDGE score is based on the standard CLIP model without fine-tuning its original weights. However, recent solutions like PAC-S [48] improve the performance of CLIP-S by fine-tuning the final projections of visual and textual encoders with curated data. In Table 7, we assess whether applying the proposed BRIDGE approach to the fine-tuned backbone employed in PAC-S can further improve its final results. Interestingly, BRIDGE can not only enhance the results of a standard CLIP-based model but can also achieve improved correlation with human judgment when using the fine-tuned CLIP model presented in [48], termed as PAC in the table. This further demonstrates the effectiveness of our evaluation score and its generalization capabilities when employing different backbones.\nAdditional Ablation Studies. In the upper part of Table 8, we present the results across different datasets when employing the standard contrastive loss instead of considering the number of noun chunks in each caption. The results indicate that employing the standard loss does not enhance the final perfor-mance, thereby confirming the advantages of prioritizing captions that contain a greater number of noun chunks.\nIn the bottom part of Table 8, we report the results ablating other architec-tural choices. In particular, instead of taking the output of the mapping module in the correspondence of the [MASK] tokens, we feed the entire output to the textual encoder. Employing this model variant, referred to as \"w/ entire map-ping module output\", when computing the BRIDGE metric leads to significantly lower correlation scores."}]}