{"title": "Good things come in small packages: Should we adopt Lite-GPUs in AI infrastructure?", "authors": ["Burcu Canakci", "Junyi Liu", "Xingbo Wu", "Nathana\u00ebl Cheriere", "Paolo Costa", "Sergey Legtchenko", "Ant Rowstron", "Dushyanth Narayanan"], "abstract": "To match the blooming demand of generative AI workloads, GPU designers have so far been trying to pack more and more compute and memory into single complex and expensive packages. However, there is growing uncertainty about the scalability of individual GPUs and thus AI clusters, as state-of-the-art GPUs are already displaying packaging, yield, and cooling limitations. We propose to rethink the design and scaling of AI clusters through efficiently-connected large clusters of Lite-GPUs, GPUs with single, small dies and a fraction of the capabilities of larger GPUs. We think recent advances in co-packaged optics can be key in overcoming the communication challenges of distributing AI workloads onto more Lite-GPUs. In this paper, we present the key benefits of Lite-GPUs on manufacturing cost, blast radius, yield, and power efficiency; and discuss systems opportunities and challenges around resource, workload, memory, and network management.", "sections": [{"title": "INTRODUCTION", "content": "Demand for AI is growing and expensive to support [27]. These challenges are expected to only get harder as the diversity, complexity, and scale of AI models are growing, making it crucial for Al service providers to build powerful and efficient Al infrastructure [2]. However, scaling AI infrastructure is encountering significant obstacles [30]. We have already reached the limit on how big a compute die can get, leading GPU designers to focus on advanced packaging technologies to pack more transistors into the same package (Figure 1). However, scaling an individual GPU package is becoming less and less sustainable for manufacturing due to multiple reasons, including power [47], cooling [17], yield [15, 45], packaging costs [43], and failure blast radius [21]. For instance, the latest generation of NVIDIA GPUs is facing deployment delays due to packaging and cooling issues [14, 44]. We observe that there is an exciting alternative approach to scaling AI clusters. What if we replace large, powerful GPU packages with highly-connected clusters of Lite-GPUs, that each have only a single, smaller compute die and fractional performance? Smaller GPUs present many promising hardware characteristics: they have much lower cost for fabrication and packaging, higher bandwidth to compute ratios, lower power density, and lighter cooling requirements. In addition, they can also unlock desirable systems opportunities such as improved fault tolerance and finer-grained resource allocation. To date, distributing AI workloads to large number of GPUs has been challenging due to the data flow demanding very high-bandwidth communication across GPUs [53]. Nevertheless, driven by recent advances in co-packaged optics, in the next decade, we expect off-package communication bandwidth to improve by 1-2 orders of magnitude with much better reach (10s of meters), compared to copper-based communication [28, 42, 54]. Co-packaged optics intergrates electronic and optical components within millimeters, compared to current pluggable optics, cutting signalling distance and yielding better power efficiency. While there are open questions and active research on utilizing co-packaged optics, we think that it has the potential to disrupt the trade-off space around designing AI infrastructure, enabling Lite-GPUs that are equipped with high-bandwidth and energy-efficient optical interconnects, to communicate with many far-off Lite-GPUs at petabit per second bandwidths [28, 42]. In this paper, we look at Al infrastructure through the lens of Lite-GPUs. Though we give an overview of recent"}, {"title": "THE LITE-GPU", "content": "In recent years, state-of-the-art data-center GPUs have been increasing compute FLOPS, memory bandwidth, and network bandwidth to support growing AI workloads. As we have already reached the limit of what can be done with a single die [23], improvements have relied on advanced packaging efforts to pack more transistors into the same GPU. For example, most recently, NVIDIA has featured a multi-die GPU design, using high-bandwidth die-to-die interfaces to bind two dies in its Blackwell GPU platform [47]. Alternatively, AMD has proposed chiplets, breaking up monolithic silicon into smaller specialized chips, co-packaged together through 3D stacking [24]. While these techniques have succeeded in improving GPU performance for their generation, there is not a clear path to scaling them further, and in fact, such complex GPU designs are already leading to several difficulties such as maintaining high yield rates, managing high power consumption, and applying efficient cooling [15, 17, 43, 45]. Additionally, as the die gets larger, its area increases faster than its perimeter (\u201cshoreline\u201d) that determines the bandwidth it can utilize. This leads to GPUs with high compute-to-bandwidth ratios, which is not always the best fit for Al workloads and results in compute under-utilization [4]. Through Lite-GPUs, we propose an alternative way of scaling AI clusters: with smaller but more GPUs connected through a performant and scalable network, realized through co-packaged optics. A Lite-GPU features a single compute-die GPU package where the die area is much smaller than that of state-of-the-art, leading to several hardware benefits. Figure 2 gives an example of Lite-GPU system where each NVIDIA H100 GPU is replaced with four Lite-GPUs. In this paper, we use this example for discussion and analysis that project future benefits of Lite-GPUs in AI clusters. First, as the die area is smaller per GPU, Lite-GPUs have largely reduced cost of manufacturing due to higher hardware yield rates. For example, the yield rate can be increased by 1.8x when a H100-like compute die area is reduced by 1/4th, corresponding to almost 50% reduction in manufacturing cost [29]. Second, reducing the compute die area increases the shoreline to die ratio. For example, reducing the die area to 1/4th doubles the perimeter exposed to the four dies, yielding a cluster with 2\u00d7 the bandwidth-to-compute ratio. Although a fraction of the extra bandwidth may be required for additional networking, we show later in our case-study that Lite-GPUs can achieve higher performance efficiency for I/O-bound workloads, such as parts of LLM inference. Third, smaller packages also greatly reduce complexity of cooling. Today's cutting-edge GPUs already throttle compute frequency to avoid overheating [10, 16]. Smaller single-die GPUs can be air-cooled separately and even sustain higher clock frequencies without requiring advanced cooling. Overall, we expect the operational cost of Lite-GPUs to be substantially lower due to better hardware yield and lower packaging cost. While the cost of networking should increase, we expect the net gains to be positive as the networking cost is only a small fraction of the GPU cost today. Additionally, there are many active efforts to scale networking cost sub-linearly with network size using circuit switching [6, 19], which would allow for even larger Lite-GPU clusters."}, {"title": "SYSTEMS OPPORTUNITIES", "content": "Consider a cluster of NVIDIA H100 GPUs, which is the most frequently deployed GPU in AI clusters today. Each H100 GPU can be replaced with a number of Lite-H100 GPUs, each Lite-GPU having a fraction of its compute and memory capabilities. Depending on how the Lite-GPUs are customized, compared to the original cluster, the cluster with Lite-GPUs can feature equivalent or better compute, memory, and cost characteristics. As highlighted in the previous section, Lite-GPUs offer many hardware benefits and they can unlock the path to-wards efficient and scalable AI clusters. Nevertheless, some key research questions should be addressed so that we can realize the Lite-GPU disruption.\nScale of distribution Some of the research questions around using Lite-GPUs are not new or unique, but potentially amplified. For example, Lite-GPUs would result in more distributed systems in the datacenter, e.g., small models previously served by a single GPU are now distributed over multiple Lite-GPUs. For larger models that already need multiple GPUs, the number of devices would be multiplied. These can potentially amplify issues such as synchronization and straggling GPUs. Nevertheless, distributed ML training and inference is already in practice, and there is a lot of focus in having efficient and robust distributed AI platforms [5, 20, 36].\nFiner-granularity of resource management With Lite-GPUs, we can allocate and access smaller units of compute and memory, leading to greater flexibility in managing an AI cluster. For example, consider power management. A GPU's compute clock frequency can be dynamically tuned to lower power consumption [34]. However, the granularity of down-clocking is on all Streaming Multiprocessors (SMs). SMs are processors designed for efficient parallel processing and each GPU consists of multiple SMs, similar to cores in a CPU. Down-clocking all SMs of a large GPU can lead to wasted resources or suboptimal performance. In a Lite-GPU cluster, we can control down-clocking at finer granularity to achieve better power efficiency, akin to down-clocking only a portion of SMs in a larger GPU. Another example is around GPU configuration. Note that today, AI clusters with heterogeneous GPUs are already used to serve requests as power-efficiently as possible, e.g., by deploying different phases of transformer inference on different GPU hardware [32]. We can customize and deploy Lite-GPUs for different profiles of Al workloads, similar to Splitwise, but at much finer scale, e.g., racks of custom Lite-GPUs as opposed to clusters of custom racks. Also, Lite-GPUs can allow for both easier over-clocking and higher bandwidth-to-compute ratios, potentially achieving higher performance efficiency at cluster-scale [33, 39]. Third, these smaller GPU units may assist future Al as a service offerings. The ability to allocate small customizable Lite-GPU clusters per customer, separated physically, providing isolation and security, can be quite powerful.\nWorkload management Careful workload parallelisation, deployment, and scheduling is a must in order to obtain the benefits of Lite-GPUs and to mask their overhead. Most importantly, with Lite-GPUs, we move previously in-silicon traffic to optical network, potentially inducing additional latency and network load. Nevertheless, with AI workloads, there are several techniques we can use. First, AI workloads are highly predictable and pipelined so extra latency can be masked through pre-fetching [12]. In fact, since Lite-GPUs can feature a higher memory bandwidth-to-compute ratio, they may even allow for reduced request-level latency in AI workloads, as less batching may be required to improve compute utilization. Second, large ML models today are already distributed over many GPUs and communicate through highly efficient collectives to minimize the amount of data exchanged, e.g., through tensor parallelism while calculating matrix-matrix multiplications. One can increase the level of tensor parallelism on a deployment of Lite-GPUs to minimize the end-to-end latency.\nFault-tolerance Reducing the size of the GPU naturally reduces the blast radius should a GPU fail due to excessive temperatures, dust or debris, or transistor faults; leading to higher available FLOPS, memory capacity, and memory bandwidth at any time. However, the number of GPUs in the cluster is increased, potentially leading to different failure profiles. To maximize the benefit from smaller blast radii, building a robust and efficient software stack is crucial. Note that today's large-scale inference pipelines already impose larger blast radii than the hardware-imposed blast radii: if one GPU out of group of GPUs serving a model instance fails, the entire instance is taken offline [19]. Active work on resolving this issue can also help with Lite-GPU clusters [26, 40].\nMemory management Each Lite-GPU has only the fraction of the memory capacity of a larger GPU. This can be a problem for workloads that require high memory capacity and do not distribute efficiently. So, there are many open questions about the design of the memory system in a cluster of Lite-GPUs. For instance, do we need memory-sharing across multiple Lite-GPUs to be an option? What should shared memory semantics look like, e.g., do we need to operate with a load/store GPU-to-memory network across Lite-GPUs to prevent extra HBM usage due to network buffering? Additionally, in a heavily-accessed shared memory setting, how can we alleviate the programming and performance challenges that stem from different tiers of memory?\nNetwork management Through Lite-GPUs, communication previously in-silicon in a large GPU is now on the Lite-GPU to Lite-GPU network. Firstly, the total traffic in a cluster and the total power consumption of the network can be higher. Secondly, in-silicon traffic assumes very high-bandwidth, low latency, and energy-efficient communication. Since the performance and efficiency of communication is degraded outside of silicon, the parallelization and distribution of the workload must be co-designed to minimize the impact of this degradation. With regards to building an efficient, high-bandwidth Lite-GPU network, we have several options. First, as the traffic across Lite-GPUs that replace one large GPU is predictable, we can build a direct-connect topology within that group of Lite-GPUs and leave the remaining network as is. This is an approximation to the original network, though it eliminates the benefits of the smaller blast radius of Lite-GPUs. Alternatively, we can consider a (flat or hierarchical) switched network for the entire Lite-GPU cluster. Using circuit switching, in part or cluster-wide, may be crucial to achieve such a network at low cost. Circuit switching presents the following benefits over packet switching: (i) more than 50% better energy efficiency, (ii) lower latency, and (iii) more ports at high bandwidth, which allows for larger and flatter networks [6].\nData-center management With Lite-GPUs the number of devices per area is increased, however, the energy per unit area is decreased. There is active research to handle data-center management at scale using various automation techniques which can be applicable to Lite-GPU clusters [18]. Additionally, though the number of devices per rack may increase, the overall cooling requirements of the rack can be lighter due to the more efficient cooling of Lite-GPUs combined with co-packaged optics. This can eliminate the need for liquid cooling racks in the data-center, which comprise a significant portion of racks, and thus space, in an NVIDIA B200 cluster [1]."}, {"title": "CASE STUDY: LLM INFERENCE", "content": "In this section, we present a case study of Lite-GPUs in the context of a trending AI workload \u2013 LLM inference [48]. LLM inference involves two distinct phases. The prompt prefill phase processes input tokens to compute reusable intermediate states, i.e., the Key-Value (KV) cache, and generates the first new token. The prefill phase is usually highly parallelizable and efficient in utilizing the compute resources. The decode phase generates output tokens one at a time, with each new token building on the entire KV cache and appending to it. This phase is often memory-bound and less efficient in compute utilization. In the evalulation, we assume that different phases can execute on different Lite-GPU clusters [32, 55] to demonstrate the hardware benefits that can be achieved with Lite-GPUs. With our case study on serving latest generative AI workloads, we aim to highlight future advantages of Lite-GPUs that are modified from today's leading GPUs. Methodology and workload We use roofline modeling [49] to capture important hardware and software characteristics and model a Lite-GPU cluster running LLM inference. We model important metrics including FLOPS, memory access, and network traffic of collectives. The modeling measures compute stages individually, including projection, MLP, and fused FlashAttention [35]. Compute, memory I/O, and network I/O can overlap within each stage and tensor parallelism is used to distribute execution within each cluster. NVIDIA H100 is the baseline GPU for comparison [9]. An H100 cluster consists of one to eight H100 GPUs. Each H100 includes 132 SMs. The Lite-GPU is modeled based on H100 by reducing its capabilities to 1/4 of the original, denoted as \"Lite\" in Table 1. Accordingly, a Lite-H100 cluster can consist of one to 32 Lite-GPUs, to match the total maximum number of SMs of the H100 cluster. Recall that for Lite-H100, we expect the bandwidth-to-compute can increase to 2x of H100 and that it can deliver higher sustainable FLOPS due to improved cooling efficiency. To explore how these hardware improvements can impact performance, we further define customized Lite-GPUs for comparison, as denoted and summarized in Table 1, with changed parameters in blue and red color. We evaluate performance with three LLM models with different sizes and structures: Llama3-70B, GPT3-175B, and Llama3-405B [7, 25]. We define the search criteria based on Splitwise's latency requirements, with TTFT (time-to-first-token) \u2264 1s and TBT (time-between-tokens) \u2264 50ms constraints [32]. We set a constant prompt sequence length of 1500 tokens, the reported median size in a production workload for coding [32]. The search sweeps all possible batch sizes and number of GPUs to find the configuration with the best performance efficiency, that is the highest throughput per SM (tokens/s/SM). Note that while we sweep up to the maximum number of GPUs per cluster as defined in Table 1, the search may return that running a model with less GPUs than maximum yields better throughput per SM."}, {"title": "RELATED WORK", "content": "Running Al workloads on small chips has gained traction in the past years. For example, Apple has been shipping Neural Engine in their mobile devices since 2017 [46]. Most recently, NVIDIA announced DIGITS as a powerful GPU workstation for engineering AI models prior to deployment on the cloud [31]. Also from the model design direction, improving inference for single GPUs has gained significant research attention [3, 37, 50-52]. While these efforts aim to maximize Al capabilities on a single device, they do not address the challenges of scaling demanding AI workloads in the data-center.\nOn the other hand, Google's TPUs are an example of scaling AI workloads across many tensor processors [19]. While they employ advanced networking technologies for lower cost and power consumption, performance and flexibility limitations remain, such as a long reconfiguration periods and multi-device blast radii, due to which a failure can render a group of TPUs inactive. TPUs share similar principles with Lite-GPUs. However, TPUs are specialized and offer less programming flexibility compared to GPUs. Additionally, TPUs have also packed more transistors into the same package across generations and are on a similar path to current complex GPUs [8, 19].\nThere is a plethora of work that propose systems solutions for improving performance [4, 11], energy efficiency [34, 38], parallelism [22, 36], and scheduling [13, 41] of AI workloads in the data-center. These are complementary to the hardware and systems efforts on delivering cost-effective scaling of AI workloads using Lite-GPUs."}, {"title": "CONCLUSION", "content": "We are already facing uncertainty on the amount of compute and memory that can fit into a single GPU package, as cutting-edge GPUs already display the packaging, cooling, and cost-related challenges due to their complex designs. In this paper, we propose an alternative way of scaling AI infrastructure: by using Lite-GPUs instead of complex, expensive, and power-hungry large GPUs. Motivated by the yield, power, and operational benefits of smaller GPU packages, we look at Al infrastructure within the context of Lite-GPUs. We provide an overview of key research questions around workload, memory, and network management. We also present how Lite-GPUs can improve energy management, performance efficiency, and fault-tolerance. With this paper, we aim to start a discussion around Lite-GPUs and their potential to turn the tide on the many issues we face while building and operating GPU clusters in the era of generative AI."}]}