{"title": "Enhancing Heterogeneous Knowledge Graph Completion with a Novel GAT-based Approach", "authors": ["Wanxu Wei", "Yitong Song", "Bin Yao"], "abstract": "Knowledge graphs (KGs) play a vital role in enhancing search results and recommendation systems. With the rapid increase in the size of the KGs, they are becoming inaccuracy and incomplete. This problem can be solved by the knowledge graph completion methods, of which graph attention network (GAT)-based methods stand out since their superior performance. However, existing GAT-based knowledge graph completion methods often suffer from overfitting issues when dealing with heterogeneous knowledge graphs, primarily due to the unbalanced number of samples. Additionally, these methods demonstrate poor performance in predicting the tail (head) entity that shares the same relation and head (tail) entity with others. To solve these problems, we propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH incorporates two separate attention network modules that work synergistically to predict the missing entities. We also introduce novel encoding and feature transformation approaches, enabling the robust performance of GATH in scenarios with imbalanced samples. Comprehensive experiments are conducted to evaluate the GATH's performance. Compared with the existing SOTA GAT-based model on Hits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the FB15K-237 dataset, and by 4.5% and 14.6% on the WN18RR dataset, respectively.", "sections": [{"title": "1 INTRODUCTION", "content": "Knowledge graphs (KGs) store entities and their relations as triplets. Each triplet, which is also called fact, can be denoted as $(h, r, t)$, where h and t are the head and tail entities, and r is their relation (e.g., the head entity h is \"Kobe\", the relation r is \"Career\", and the tail entity t is \"Basketball Player\"). Brimming with vast amounts of facts, KGs are widely employed to enhance the search results and refine the recommendation results. However, as KGs expand rapidly in size, the facts it contains may become inaccurate and incomplete, hindering the ability to provide satisfactory search and recommendation results.\n The knowledge graph completion (KGC) technique is dedicated to completing incomplete triples in KGs. Currently, several studies on knowledge graph completion focus on link prediction, which predicts missing values in a possible triplet. Link prediction is usually defined as, given a query triplet (?, r, t) or (h, r, ?), where? represents the missing value that needs to be predicted. For instance, given the query (?, isMarriedTo, William), the goal of link prediction is to predict that the missing value is \"Kate\". In this paper, we also study the link prediction problem for knowledge graph completion.\nRecently, knowledge graph embedding models based on graph neural network (GNN) have emerged as a successful method for the link prediction task. These approaches offer a comprehensive and nuanced representation of the knowledge graph by capturing the semantics of individual triplets as well as the structural information of the whole KG. Among all GNN-based methods, GAT-based approaches (e.g., GAT, r-GAT, KAME, DisenKGAT) exhibit superior performance. This is due to the fact that GAT-based approaches assign different weights or importance to each neighbor during the information aggregation process. This allows the model to prioritize learning from important neighbors while diminishing the influence of unimportant connections. While significant progress has been made with GAT-based methods, further improvements are still needed for heterogeneous KGs. Heterogeneous KGs consist of diverse types of entities and relations, which provide a better representation of complex and varied real-world data. However, it becomes more challenging to predict entities and relations in such graphs using GAT-based methods."}, {"title": "Challenge 1: Existing GAT-based approaches overfit in sparse entity and relation predictions.", "content": "In deep learning, overfitting is a fundamental problem characterized by a model that can match the training set with high accuracy but performs poorly when modeling other data. [39] mentions that when the number of model parameters exceeds the amount of information in the training set. the model tends to memorize the training data rather than learn the underlying patterns, resulting in overfitting. The distribution of information in the real world tends to be skewed, resulting in some entities and relations appearing infrequently. For example, in Fig. 1(a), only one triple contains node $E_6$ and relation $r_3$. We call these entities and relations with low frequency \"sparse entities\" and \"sparse relations\", respectively. Existing GAT-based models often have a large number of parameters, which can lead to overfitting when modeling sparse entities and relations due to the parameter quantity exceeding the number of data points. Therefore, existing GAT-based methods suffer from performance degradation due to the presence of sparse entities and relations. For example, in Fig. 1(a), only one triple contains $r_3$. Given the head entity $E_3$ and relation $r_3$, the model is likely to predict the entity $E_4$, whereas the other potential tail entities are masked or overlooked. This is because overfitting occurs when using a large number of parameters to model sparse relations $r_3$ with less information. As another example, there only exists one triple containing $E_6$, i.e., $(E_6, r_2, E_1)$. And $E_5$ is also related to $E_1$ by $r_2$. Assuming $r_2$ as a Teammate_of relation, there exists a fact $(E_6, r_2, E_5)$ in the real world but not in the KG. In this case, existing models fail to predict the entity $E_5$ when provided with the head entity $E_6$ and relation $r_2$. This failure is primarily due to insufficient embedded information for $E_6$."}, {"title": "Challenge 2: Existing GAT-based approaches demonstrate poor performance in predicting the tail (head) entity that shares the same relation and head (tail) entity with others.", "content": "In heterogeneous KGs, it is quite common to encounter scenarios where different tail (head) entities share the same relation with a particular head (tail) entity, as depicted in Fig. 1(b). In such cases, existing GAT-based approaches assign weights to entities based on relations. This will directly lead to the following two limitation: 1) Some information about neighboring entities may be lost. Relations may prioritize specific information about entities while disregarding other information. For example, consider the example illustrated in Fig. 2, where the entity of a basketball player, Ming Yao, encompasses various aspects of information such as family information and occupation information. However, the relation Teammate_of primarily focuses on the occupation information while neglecting other aspects. 2) Another limitation lies in the inability to capture the varying emphasis that the head entity places on different tail entities within the same relation. Even through the same relation, the head entity will have different inclinations towards neighboring entities. For example, as shown in Fig. 2, Ming Yao may have varying inclinations towards different teammates because of information unrelated to occupation information, such as personality and family. This difference cannot be explained if we only consider the relation Teammate_of.\nIn this paper, we propose a novel end-to-end GAT-based method for heterogeneous knowledge graph completion, aiming to overcome the limitations of existing GAT-based methods. In response to challenge 1, that is, the model is overfitting on sparse entities and relations due to a large number of parameters, our proposed solution is to reduce model parameters. Specifically, we first use reducing features to reduce the number of parameters representing relations. Instead of using matrices, we represent relations using embedding vectors. Additionally, reducing parameters may limit the model's ability to model relations with rich information. To address this, we introduce weight sharing to enhance the feature extraction capabilities of relations. We use the same attention projection matrices for all relations, which have a global perspective of KG. By applying these two methods, we reduce the parameters"}, {"title": "2 RELATED WORK", "content": "Knowledge graph completion methods can be divided into translation-based models, semantic matching-based models, GCN-based models, GAT-based models, and language model-based models, as described below.\nTranslation-based models. TransE [1] is a pioneering work of translation-based model. Inspired by word2vec [17], TransE utilizes the translation invariance of word vectors. It maps the head entity, relation, and tail entity respectively to the dense vectors h, r, t in a low-dimensional space, and then adjusts h, r, t so that $h + r \\approx t$. Subsequent models, such as TransH [34], TransR [15], TransD [12], etc., proposed novel methods of relational translation, increasing the complexity of models and improving their performance. These models are also known as translation models or additive models. However, translation-based models have limited consideration of semantic information and struggle to handle one-to-many or many-to-many connections effectively. Despite their ease of expansion, these models have inherent limitations in capturing complex relations.\nSemantic matching-based models. As opposed to translation-based models, semantic matching-based models evaluate the rationality of facts by matching the potential semantics in embedded entities and relations. These models utilize scoring functions to quantify the semantic similarity between entities. RESCAL [19] declares a matrix for each relation and uses the bilinear function to calculate the rationality of triples. DistMult [36] simplifies RESCAL by replacing the relation-specific matrix with a diagonal matrix. Complex [26] extends DistMult to the complex space and is the first model to introduce complex number embeddings for knowledge graphs. However, although semantic matching-based models overcome the limitations of translation models by effectively capturing the symmetry or asymmetry of relations, they often struggle to efficiently model complex relational patterns. These models typically require a large number of parameters, leading to potential memory inefficiencies.\nGCN-based models. GCN-based models enhance the vanilla GCN for knowledge graph embedding through the incorporation of 1) relation-specific linear transformations, 2) weighted aggregation, and 3) relation representation transformations. Among these, RGCN [21] is the first work that introduces GCN to knowledge graph embedding. RGCN aggregates neighbor information through a relation-specific matrix. After that, SACN [22] assigns static weights to each relation type. CompGCN [28] jointly embeds entities and relations and uses combination operators to model their interaction. However, GCN-based models often exhibit limited flexibility and require loading the entire knowledge graph into memory, leading to scalability issues and posing challenges when applied to large knowledge graphs.\nGAT-based models. GAT-based models achieve state-of-the-art performance in knowledge graph embedding by dynamically modeling the interactions between entities and relations. In contrast to GCN, GAT [30] introduces attention mechanisms to dynamically adjust node weights. Based on GAT, RGAT [2] utilizes self-attention to derive relation-related weights for relation features. It combines the inter-entity attention results from GAT to obtain the final outcome. DisenKGAT [35] assumes that entities consist of multiple independent factors and leverages attention mechanisms within each sub-representation space to aggregate neighbor information. HRAN [14] incorporates an entity-level encoder to generate neighborhood features for each relation type and a relation-level encoder to capture their relative importance. MRGAT [3] utilizes two matrices for each relation type, embedding the head and tail entities into the query and key vectors for attention calculation. However, these models often face challenges such as overfitting on sparse data and limited scalability when dealing with a large number of rapidly increasing relations.\nLanguage model-based models. Knowledge graph text has been successfully leveraged to pre-train embeddings in language model-based models. KG-BERT [37] is the first model to apply BERT [5], a pre-training language model widely used in various Natural Language Processing (NLP) tasks, to knowledge graph embedding. KG-BERT offers two variants for knowledge graph completion: KG-BERT (variant a) models triples using their textual descriptions, while KG-BERT (variant b) predicts relations using only the textual descriptions of the head and tail entities. On the other hand, LMKE [31] utilizes a language model to generate embeddings for both entities and relations in the knowledge graph. Both KG-BERT and LMKE can effectively use large amounts of text data to pre-train embeddings for knowledge graph entities and relations. However, it is essential to note that the reliance on text data limits the application scenarios of these models."}, {"title": "3 BACKGROUND", "content": "3.1 Knowledge graph\nKnowledge Graph. A knowledge graph (KG) can be denoted as $G = (\\mathcal{E}, \\mathcal{R}, \\mathcal{T})$, where $\\mathcal{E}$ represents the set of entities, $\\mathcal{R}$ represents the set of relations, $\\mathcal{T} \\subset \\mathcal{E} \\times \\mathcal{R} \\times \\mathcal{E}$ represents the set of triplets.\nKnowledge Graph Embedding. A KG G can be embedded into a low-dimensional vector space. In other words, a knowledge graph embedding is defined as, $KG_{\\mathcal{E}} = \\{(h, r, t) | h, t \\in \\mathcal{H}, r \\in \\mathcal{R}\\}$, where $\\mathcal{H}$ and $\\mathcal{R}$ represent the embedding sets of entities and relations which are $n_e \\times d_e$ and $n_r \\times d_r$ dimensions respectively. $n_e$ and $n_r$ represent the number of entities and relations respectively. $d_e$ and $d_r$ represent the embedding dimensions of entities and relations respectively."}, {"title": "3.2 Attention mechanism", "content": "Self-Attention Mechanism. Transformer [29] introduces the self-attention mechanism, which has been widely adopted in various NLP tasks. Several language models, such as GPT [20] and BERT [5], are built upon the self-attention mechanism and have achieved state-of-the-art (SOTA) performance in semantic analysis, machine translation, and other NLP tasks. The self-attention mechanism is responsible for encoding an input sequence consisting of $d_h$-dimensional vectors $H = [h_1, ..., h_N]^T \\in \\mathbb{R}^{N \\times d_h}$, and it can be formulated as follows,\n$\\text{self\\_att}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$ (1)\n$Q = HW_q, K = HW_k, V = HW_v$ (2)\nwhere $W_q, W_k \\in \\mathbb{R}^{d_h \\times d_k}$, $W_v \\in \\mathbb{R}^{d_h \\times d_o}$ project the input sequence H to the query matrix Q, key matrix K and value matrix V respectively. Q and K are multiplied and divided by $\\sqrt{d_k}$, and the attention score can be obtained by performing softmax normalization. Finally, we can get the output of the self-attention function by multiplying the attention score and V.\nMulti-Head Attention. The self-attention mechanism is limited to capturing features within a single projection space of H, which makes it sensitive to initialization. To overcome this limitation and capture more diverse features, a multi-head attention mechanism is introduced, drawing inspiration from Convolutional Neural Networks (CNNs). The idea is to leverage multiple projection spaces to capture different aspects of the input. Specifically, if we aim to capture features from M projection spaces, the multi-head attention mechanism can be formulated as follows,\n$\\text{Multihead}(H) = [\\text{head}_1; ...; \\text{head}_M] W_o$ (3)\n$\\text{head}_m = \\text{self\\_att}(Q_m, K_m, V_m)$ (4)\n$Q_m = HW_q^m, K_m = HW_k^m, V_m = HW_v^m$ (5)\nwhere $W_o \\in \\mathbb{R}^{M*d_o \\times d_{h'}}$ is the projection matrix used to convert the slices of multiple self-attention outputs to the original dimension. $\\text{head}_m$ corresponds to the output sequence of the m-th self-attention function. $W_q^m, W_k^m \\in \\mathbb{R}^{d_h \\times d_k}$ and $W_v^m \\in \\mathbb{R}^{d_h \\times d_o}$ respectively project H to the query matrix $Q_m$, key matrix $K_m$ and value matrix $V_m$ of the m-th attention head, while $m \\in \\{1, ..., M\\}$"}, {"title": "4 METHOD", "content": "In this section, we will provide a detailed introduction to GATH. We first introduce the overall framework of GATH and then describe each module and its interconnections in detail. Finally, the loss function used in GATH is proposed.\n4.1 Overall Framework\nThe architecture of GATH follows the Encoder-Decoder framework, as depicted in Fig. 3. The encoder is responsible for generating embeddings of entities by incorporating both the structural and neighborhood information from the KG. Specifically, the encoder consists of an attention module that calculates attention scores between the central entity and its neighbors, as well as an aggregation module that combines information from all neighboring entities into a single representation. To begin, the attention module computes attention scores via entity pair and relation type, respectively. Subsequently, the aggregation module utilizes these attention scores to aggregate relational and neighborhood information toward the central entity. Finally, the encoder feeds the generated entity embeddings into a decoder. The decoder can be any generic decoder (such as TransE or ConvE) for knowledge graph embedding models. In our case, we employ a decoder based on ConvE with certain enhancements to better capture diverse relational patterns in KGs. Next, we describe each component in detail.\n4.2 Encoder\nThe encoder of GATH consists of two primary components: 1) the entity-specific attention network, and 2) the entity-relation joint attention network. These two parts calculate the attention scores of head and tail entities respectively from the entity feature space and the entity-relation joint feature space. They are described separately as follows.\nEntity-Specific Attention Network. To address the challenge of poor performance in predicting the tail (head) entities that share the same relation and head (tail) entities with others, we introduce an entity-specific attention network that is relation-agnostic. This module aims to capture the intrinsic interaction between a central entity and its neighbor entities, allowing GATH to assign different attention weights to neighbor entities based on their relevance to the central entity regardless of the specific relation involved. By doing so, GATH can efficiently handle the situation shown in Fig. 1(b)."}, {"title": "Entity-Relation Joint Attention Network.", "content": "In heterogeneous KGs, entities and relations are embedded in different feature spaces, which poses a challenge to effectively capture the joint features of entities and relations. A general approach is to generate a query projection matrix $W_Q \\in \\mathbb{R}^{D \\times F}$ and a key projection matrix $W_K \\in \\mathbb{R}^{D \\times F}$ for each relation, project head, and tail entities as query and key vectors, respectively, and then obtain the attention score, where D represents the dimension of entity embedding and F represents the dimension of query vector and key vector. For a KG with n types of relations, the model needs $2 * n * D * F$ space to store relation features. Therefore, this calculation method requires too many parameters for handling numerous relations in large-scale KGs while risking overfitting on sparse relations. Moreover, it is difficult to extend this approach to handle KGs with many different relations due to its high space complexity. To simplify the parameters representing the relation, we use a D-dimensional embedding vector to represent the relation. Furthermore, we make two shared attention projection matrices to map head (tail) entity embeddings transformed by relations to query (key) vectors.\nWe assume that each entity is represented by a combination of information in different scenarios. For example, as shown in Fig. 2, a person entity may be represented by a combination of information in various scenarios, such as occupation information and family information. Specifically, in the entity feature space, information in different dimensions corresponds to entity information in different scenarios. A relation is a concrete representation of a scene, so each relation focuses on information under the specific dimensions of the entity feature. We refer to entity features transformed by specific relations as entity-relation joint features. We will compute the attention score based on the entity-relation joint feature.\nIn practice, in the D-dimensional relation embedding that matches the entity embedding dimension, the value under each dimension indicates the degree of attention of the relation to the entity information under the corresponding dimension. In other words, a D-dimensional entity embedding can be represented as consisting of D subspaces of dimension 1. The relation-based entity feature transformation is that the relation enhances or weakens the characteristics of the entity information in each subspace. The following is a detailed demonstration of the calculation process.\nThe entity-relation joint attention network internally preserves relation features $R^{(l)} \\in \\mathbb{R}^{N_r \\times D^{(l-1)}}$ and receives entity embeddings $H^{(l-1)}$, where $N_r$ represents the number of types of relations and $D^{(l-1)}$ denotes the dimensionality of each entity embedding. We then obtain the joint features of entities and relations through the following operations,\n$q^{(l)}_{joint(i)} = h^{(l-1)}_i \\odot r^{(l)}, q^{(l)}_{joint(j)} = h^{(l-1)}_j \\odot r^{(l)}$ (9)\nwhere $r^{(l)}$ represents the embedding of the r-th relation, and $(v_i, r_r, v_j)$ is in $\\mathcal{N}_i$. $q^{(l)}_{joint(i)}$ represents the joint feature of the head entity and relation, and similarly, $q^{(l)}_{joint(j)}$ corresponds to the joint feature of the tail entity and relation.\nThe parameter matrices shared by all relations project the joint features to the query and key vectors. This process can make the information in each dimension of the joint features interact. The attention score will be calculated as follows,\n$q^{(r)}_{(r)i} = q^{(l)}_{joint(i)} W^{(l)}_{qk}, k^{(r)}_{(r)j} = q^{(l)}_{joint(j)} W^{(l)}_{qk}$ (10)\n$att^{(l)}_{(r)ij} = f(q^{(r)}_{(r)i} \\odot k^{(r)}_{(r)j})$ (11)\nwhere $W^{(l)}_{(r)q}, W^{(l)}_{(r)k} \\in \\mathbb{R}^{D^{(l-1)} \\times d_k}$ are learnable projection matrices shared by all relations. f(.) is a fully connected layer with an output dimension of 1.\nIn order to make the attention scores more comparable, a softmax operation will be performed,\n$\\alpha^{(l)}_{(r)ij} = softmax_j (att^{(l)}_{(r)ij}) = \\frac{exp(att^{(l)}_{(r)ij})}{\\sum_{(v_i, r_r, v_k) \\in \\mathcal{E}} exp(att^{(l)}_{(r)ik})}$ (12)\nwhere $\\alpha^{(l)}_{(r)ij}$ represents the relation-specific head-to-tail attention score.\nAnalysis of Model Parameters. The model space complexity of GATH is $O(L(N_eD + N_rD + DF))$, while the model space complexity of MRGAT is $O(L(N_eD + N_rDF))$, where D represents the embedding dimension. $N_e = |\\mathcal{E}|$ and $N_r = |\\mathcal{R}|$ represent the number of entities and relations respectively. F represents the dimension of the query and key vectors. L represents the number of encoder layers. It can be seen that reducing features and weight sharing strategies we proposed significantly reduce the space complexity. Specifically, 1) Reduced features: The reduced features are shown in Formula 9. We assume that the relation is a specific representation in a certain scene, which will enhance or weaken the information of different dimensions in the entity embedding. The parameter amount of each relation feature is D. 2) Weight sharing: If a learnable query projection matrix and key projection matrix are generated for each relation, the parameter amount is 2nDF. Inspired by the feature map sharing convolution kernel in CNN, we make all the relations share the projection matrix, as shown in Formula 10. Finally, we reduced the relation features from 2nDF to nD + 2DF.\nAggregation. The aggregation module linearly combines the embeddings of the neighbors according to the attention scores calculated above, and then aggregates this information to generate a new embedding for the central node. For node $v_i$, the formula for aggregating neighborhood information is as follows,\n$h^{(l)}_{(h)i} = \\sum_{v_j \\in \\mathcal{N}_i} \\alpha^{(l)}_{(h)ij} h^{(l-1)}_j$ (13)\n$h^{(l)}_{(r)i} = \\sum_{(v_i, r_r, v_j) \\in \\mathcal{N}_i} \\alpha^{(l)}_{(r)ij} h^{(l-1)}_j$ (14)"}, {"title": "Aggregation.", "content": "The aggregation module linearly combines the embeddings of the neighbors according to the attention scores calculated above, and then aggregates this information to generate a new embedding for the central node. For node $v_i$, the formula for aggregating neighborhood information is as follows,\n$h^{(l)}_{(h)i} = \\sum_{v_j \\in \\mathcal{N}_i} \\alpha^{(l)}_{(h)ij} h^{(l-1)}_j$ (13)\n$h^{(l)}_{(r)i} = \\sum_{(v_i, r_r, v_j) \\in \\mathcal{N}_i} \\alpha^{(l)}_{(r)ij} h^{(l-1)}_j$ (14)\nwhere $W^{(l)} \\in \\mathbb{R}^{D^{(l-1)} \\times d_o}$ is a learnable transformation matrix that projects entity embedding to value vectors. $d_o$ represents the dimension of the value vector. $h^{(l)}_{(h)i}$ and $h^{(l)}_{(r)i}$ denote the embeddings that aggregate the neighborhood from the entity-specific attention network module and the entity-relation joint attention network module, respectively.\nThe multi-head attention mechanism enables simultaneous focus on different feature spaces, allowing for the extraction of richer feature information. This has two main advantages. Firstly, it can accelerate model convergence by capturing diverse aspects of the input data. Secondly, it effectively mitigates the sensitivity of the attention mechanism to initialization, enhancing the stability and robustness of the model. Therefore, GATH adopts a multi-head attention mechanism instead of the single-head attention mechanism in order to learn different semantic features more effectively. Assuming there are M attention heads, we use the following formula to concatenate the output of M heads and project them into the entity feature space,\n$h^{(l)}_i = CONCAT(h^{(l),1}_{(h)i}, h^{(l),1}_{(r)i}, ..., h^{(l),M}_{(h)i}, h^{(l),M}_{(r)i}) W^{(l)}$ (15)\nwhere $h^{(l),m}_{(h)i}$ and $h^{(l),m}_{(r)i}$ denote the embeddings generated by the m-th entity-specific attention network module and entity-relation joint attention network module respectively. $W^{(l)} \\in \\mathbb{R}^{2Md_o \\times D}$ transforms the multi-head attention output to the entity feature space. $D$ denotes the dimension of the output entity embedding of the aggregation module.\nUp to this point, the entities have gathered all the neighborhood information. It is worth noting that the information of the entities themselves is not propagated, which may lead to an over-smoothing of the graph, i.e., the entity embeddings tend to be the same. Inspired by ResNet[8], we introduce self-loops as residual connections and a learnable parameter $\\beta$ for each entity to make information propagation more flexible. The updated formula for entity embedding propagation is as follows,\nh^{(l)}_i = \\sigma (h^{(l)} + \\beta h^{(l-1)}_i )W^{(l)} (16)\nwhere $\\sigma(\\cdot)$ is a non-linear activation function. $W^{(l)} \\in \\mathbb{R}^{D^{(l-1)} \\times D}$ is propagation matrix."}, {"title": "4.3 Decoder", "content": "GATH can adopt any knowledge graph embedding model's decoder, such as TransE or ConvE, etc. Our decoder is implemented based on ConvE, which uses 2D convolutions to model KGs and has better performance in predicting highly connected entities. In addition, inspired by semantic matching-based models, our decoder additionally introduces relation-based entity feature transformation to accurately evaluate the plausibility of triples.\nOur encoder generates entity embeddings H and feeds them into the decoder. The decoder defines the relation embedding matrix $R \\in \\mathbb{R}^{N_r \\times D}$, initialized with Gaussian distribution sampling. The STACK operation, which will concatenate a sequence of vectors on the first dimension, is performed on the entity embeddings, relation embeddings, and their transformed features to obtain the input."}, {"title": "4.4 Loss function", "content": "If the input triple (h, r, t) is valid in KG, then we want the predicted probability p(h, r, t) to be 1. Otherwise, we want the predicted probability to be 0[3]. So the loss function is defined as follows,\n$\\mathcal{L}(h, r, t) = -\\frac{1}{N} \\sum_{i=1}^{N} (y(h, r, t_i) \\cdot log(p(h, r, t_i)) + (1-y(h, r, t_i)) \\cdot log(1 - p(h, r, t_i)))$ (20)\nin which\n$y(h, r, t_i) = \\{\\begin{matrix} 1 & if(h, r, t_i) \\in \\mathcal{T} \\\\ 0 & if(h, r, t_i) \\notin \\mathcal{T} \\end{matrix}$ (21)\nwhere y(h, r, t_i) indicates whether the triplet (h, r, t_i) is a positive triplet, and its value is {0,1}. N represents the total number of tail nodes to be predicted. $\\mathcal{T}$ represents the set of valid triples."}, {"title": "5 EXPERIMENT", "content": "5.1 Datasets\nMost of the knowledge graph embedding models and knowledge graph completion models [3, 7, 22, 27, 35, 37] are developed on the FB15K-237 and WN18RR datasets. Likewise, these two datasets will also be used to evaluate the performance of GATH. A short introduction to these two datasets is given below.\nFB15K-237. FB15K-237[25] is a dataset for link prediction with 14541 entities, 237 relations, and 310,116 triples. It is the subset of FB15K[1]. In FB15K, many triples are inverses that cause leakage from the training to testing and validation splits. And the inverse relations are removed in FB15K-237.\nWN18RR. WN18RR[4] is a subset of WN18[1]. The WN18 also suffers from test leakage, similar to FB15K. Therefore, WN18RR has revised WN18 and deletes the inverse relations. In all, WN18RR contains more than 40,000 entities, 11 relations, and more than 90,000 triples. Compared with FB15K-237, WN18RR has more entities, but the complex connections are reduced a lot.\n5.2 Baselines\nIn order to demonstrate the superiority of GATH in knowledge graph completion, we mainly conduct comprehensive comparisons with the following 5 categories of knowledge graph embedding models: 1) translation-based models, 2) semantic matching-based"}, {"title": "5.3 Experiment setup", "content": "The number of GATH's encoding layer is set to 2, and AdamW[16] is used as an optimization algorithm for the training of GATH. The above settings are applied to all baselines based on GCN and GAT. At the same time, in order to speed up convergence and prevent model overfitting, GATH also adopts dropout[23] and batch normalization [11] for entity and relation embeddings. Periodic decay of the learning rate is applied to GATH and baselines. A high learning rate makes the loss drop rapidly in the early stage of training and enters a plateau in the middle stage, and finally, a small learning rate makes the model converge slowly. Therefore, we set the initial learning rate to 0.01, and let it decay to 0.985 of the current learning rate every epoch. batch_size and embedding_size are set to 128 and 200 respectively. For baselines that can use the block-diagonal-decomposition regularization scheme [21], we set num_blocks to 50. The environment of GATH is python3.9+pytorch1.12 and it runs on NVIDIA 4090 Graphics Processing Units. The computation time of GATH for each epoch is about 4.3 minutes and 1.6 minutes for the FB15K-237 and WN18RR datasets respectively.\n5.4 Evaluation indicators\nTo evaluate the performance of GATH, we perform the link prediction task on the test set. Specifically, for each triplet (h, r, t) in the test set, we construct the corresponding reverse triplet (t, r_reverse, h). Each triplet (e1, r, e2) in the constructed test set will be used for prediction. For the decoder, it receives the head entity e1 and relation r as input and outputs the probabilities of all tail entities. The prediction result is a list pred, whose length is the number of all entities, and the corresponding element pred[i] is the probability of triplet (e1, r, ev\u2081) in KGs. Since the triplets in the training set and test set will also appear in the prediction list, pred needs to be filtered, that is, the probability of the corresponding triplet is set to 0. Sort pred from high to low to generate spred, and the index of e2 in spred is the ranking score for (e1, r, e2). The specific metrics include Mean Reciprocal Rank (MRR), Mean Rank (MR) and the"}, {"title": "6 RESULT", "content": "6.1 Overall performance\nThe performance of GATH is shown in Table 3. When compared with other existing knowledge graph embedding models", "3": "nFirst, we compare decoder-only baselines with our decoder. On FB15K-237, ConvTransE has better performance than other decoder-only baselines. It can be seen that our decoder is 1.7% higher than ConvTransE on the Hits@10 indicator, 7.3% on the Hits@1 indicator, and 4.5% on the MRR indicator. Compared with FB15K-237, WN18RR has a lot less complex connections, but our decoder can still maintain the competition. This demonstrates that relation-based entity feature transformation can improve performance when modeling heterogeneous KGs with complex connections.\nSecond, our proposed GATH is able to perceive structural information through graph attention networks, improving the quality of entity embeddings. By comparing GATH with our decoder, all indicators have been significantly improved. On FB15K-237, GATH improves Hits@1, Hits@10, and MRR by 15.5%, 11.2%, and 13.5% compared to our decoder. Similarly, on WN18RR, the performance of GATH improves by 7.0"}]}