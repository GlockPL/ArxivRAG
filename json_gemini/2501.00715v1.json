{"title": "eRevise+RF: A Writing Evaluation System for Assessing Student Essay Revisions and Providing Formative Feedback", "authors": ["Zhexiong Liu", "Diane Litman", "Elaine Wang", "Tianwen Li", "Mason Gobat", "Lindsay Clare Matsumura", "Richard Correnti"], "abstract": "The ability to revise essays in response to feedback is important for students' writing success. An automated writing evaluation (AWE) system that supports students in revising their essays is thus essential. We present eRevise+RF, an enhanced AWE system for assessing student essay revisions (e.g., changes made to an essay to improve its quality in response to essay feedback) and providing revision feedback. We deployed the system with 6 teachers and 406 students across 3 schools in Pennsylvania and Louisiana. The results confirmed its effectiveness in (1) assessing student essays in terms of evidence usage, (2) extracting evidence and reasoning revisions across essays, and (3) determining revision success in responding to feedback. The evaluation also suggested eRevise+RF is a helpful system for young students to improve their argumentative writing skills through revision and formative feedback.", "sections": [{"title": "Introduction", "content": "Young student writers often struggle with identifying convincing evidence and connecting it to claims when writing argumentative essays; however, developing persuasive argumentation itself helps students improve their thinking and reasoning skills (Kuhn et al., 2017). This motivates recent research in developing Automated Writing Evaluation (AWE) systems for supporting students in writing and revising argumentative essays. For instance, Zhang et al. (2019) developed the eRevise system to score student essays on text-based evidence usage and to provide associated feedback to guide revision. Although students attempted to respond to the feedback, their revisions often did not yield substantive essay improvement (Wang et al., 2020). Similar findings have been found for other AWE systems (Graham et al., 2015), suggesting that students often lack the skills necessary for effective revision (Roscoe et al., 2013)."}, {"title": "Related AWE Work on Revision", "content": "While AWE systems typically allow submissions of multiple drafts in response to feedback, most provide feedback on single essay drafts rather than on the revisions between two drafts (Wilson et al., 2021; Huawei and Aryadoust, 2023; Fleckenstein et al., 2023). However, feedback on revisions has been identified as an important area for AWE research (Guo et al., 2023; Correnti et al., 2024; Li et al., 2024), and some systems have visualized NLP revision analyses, e.g., to assist students in self-monitoring writing (Zhang et al., 2016; Shibani et al., 2018; Litman et al., 2022). While these systems display revisions, they do not provide feedback messages based on an NLP assessment of a revision's success. Although prior NLP revision work has created annotated revision corpora (Zhang et al., 2017; Anthonio et al., 2020; Spangher et al., 2022; Du et al., 2022; D'Arcy et al., 2024) and has developed models for isolated tasks like identifying revision purposes (Afrin et al., 2020; Kashefi et al., 2022; Du et al., 2022; Mita et al., 2024; Jourdan et al., 2024), generating revisions (Chong et al., 2023; Ziegenbein et al., 2024), or evaluating revision quality (Liu et al., 2023), our work is the first to integrate multiple such tasks into a deployed and evaluated AWE system."}, {"title": "System Architecture", "content": "The backend system (in Figure 2) builds on and integrates NLP algorithms from our prior research (Zhang et al., 2019; Liu et al., 2023) to process argumentative essays, revisions between drafts, and provide feedback on both. It consists of an AES (automated essay scoring) system and an AES+RF system, of which the former provides evidence use feedback and the latter focuses on revision feedback. The AES system uses NLP algorithms to extract a set of scoring indicators and then uses the indicators to select from a set of expert-designed evidence feedback messages. The AES+RF system uses revision classifiers in addition to scoring indicators to select from another set of expert-designed revision feedback messages.\nEvidence Scoring Indicators. The algorithms used in the AES and AES+RF systems to extract features from essays were adapted from the eRevise system (Zhang et al., 2019). Given the text of an essay draft, the algorithm computes the Number of Pieces of Evidence (NPE), an integer encoding the number of evidence topics mentioned in the essay\u00b2. A sliding window is applied to extract NPE on the raw text. If a window contains a similar word from a manually crafted list of keywords that are associated with evidence topics, the window is confirmed to contain text-based evidence that is related to the topic. The eight-word sliding window, Glove embedding (Pennington et al., 2014), cosine similarity with a 0.9 threshold are used in the implementation based on the training data from previous RTA deployments (Zhang et al., 2019). Also, the indicator Specificity (SPC) of the essay uses the same sliding window to determine if it contains words from another manually crafted list, which is a vector of integers that encodes the number of specific examples mentioned in 8 categories for MVP and 7 categories for SPACE, taken from Rahimi et al. (2017). SPC score is the sum of the vector, whose value is a positive integer. The NPE and SPC compute the breadth and depth of the text-based evidence usage in an essay, respectively.\nRevision Classifiers. In addition to the scoring"}, {"title": "System Deployment", "content": "The eRevise+RF system was deployed in collaboration with 6 teachers and 406 students in grades 4 to 8 during Spring 2024 for the MVP article and Fall 2024 for the SPACE article. The teachers overlapped for the 2 deployments, while the students were different (e.g., no grade 6 students in Fall). The participants were from one school in Pennsylvania (PA), where 35% of the students are students of color, representing 45 different zip code areas, and speaking more than 10 languages at home; two schools in Louisiana (LA), where 98% and 65% enrollment are minorities, respectively, and student reading proficiency rates range from 46% to 47%. The system was deployed in three sessions administered over 3 to 5 days (see Figure 2). The 2 deployments received a total of 194 draft1, 194 draft2, and 172 draft3 for MVP and 176 draft1, 176 draft2, and 150 draft3 for SPACE essays since students might not have consistently participated in all three sessions due to absences. Our analysis is based on the essays from students who participated in all three sessions, yielding 3 drafts of 172 MVP essays (516 total) and of 150 SPACE essays (450 total). Table 1 shows the essay revision statistics across grades, where higher-grade students usually have longer essays, and students are inclined to make adding revisions across all grades."}, {"title": "Results and Analysis", "content": "We use the 516 MVP essays for manual annotation, to evaluate the accuracy of the scoring indicator algorithms. We employed experts to annotate NPE and SPC scores for every sampled essay. The expert annotators participated in prior studies with Kappas of 0.8 and 0.84 for Evidence and Reasoning (Liu et al., 2023), and ICCs of 0.84 and 0.69 for NPE and SPC (Correnti et al., 2022), respectively. Table 2 shows that Quadratic Weighted Kappa (QWK) between system-predicted and human-annotated NPE and SPC are 0.67 and 0.82, respectively. The agreements are substantial and almost perfect, respectively (Landis and Koch, 1977), which suggests the scoring indicators (NPE and SPC) are effective. Also, we investigate the performance of the EF level predictions on the 172 first drafts since this EF is mainly designed for the first drafts. The QWK between the predicted and annotated evidence feedback levels\u00b3 is 0.66, which suggests the AES(+RF) system used to select evidence use feedback levels for students' first drafts is also reasonably effective.\nAgain, we employed the trained annotators to annotate 172 pairs of first and second MVP essay drafts, to evaluate the performance of the revision classifiers. The annotators annotated surface or content for 1,525 revision pairs, and evidence or reasoning labels for 1,046 out of 1,525 extracted revision pairs, given the other 479 revision pairs were labeled with surface, and were removed since RC-Evidence only handles content revisions. Moreover, the annotators annotated the RER scheme (Afrin et al., 2020) on the revision pairs based on the prior work (Liu et al., 2023) and obtained 1,024 annotated successful and unsuccessful labels, given the other 22 revision pairs were labeled with claim, which were removed since RC-Success only handles evidence and reasoning revisions. Table 7 in the Appendix shows an annotated essay example.\nUsing these annotations to evaluate the three revision classifiers yields RC-Content, RC-Evidence, and RC-Success F1 scores of 0.96, 0.66, and 0.70, respectively (shown in Table 2). Precision/recall figures and a grade breakdown are shown in Table 8 in the Appendix, and suggest that RC-Evidence has relatively lower F1 in grade 8 while RC-Success has relatively lower precision in grades 6 and 7. Confusion matrices are shown in Table 9 in the Appendix, and suggest that RC-Evidence is likely to predict evidence as reasoning and RC-Success makes errors mostly on true unsuccessful revisions. In sum, both the evidence score indicators and revision classifiers are reasonably effective in using NLP to assess student argumentative writing and revisions (RQ1)."}, {"title": "Student Performance", "content": "Similar to prior RTA studies (Zhang et al., 2019; Correnti et al., 2022), we evaluated changes in essay quality across drafts using the metrics of Word Count (WC), Evidence Score (ES), NPE and SPC.\nThe NPE and SPC are predicted based on the algorithms described earlier in the evidence score indicators (in Sec. 3.1). The ES is predicted with a BERT (Devlin et al., 2019) model trained on a previously collected corpus with 0.89 QWK on MVP essays and 0.94 QWK on SPACE essays4. Table 3 shows that essay quality improves over three drafts, where for all metrics, higher is better. Specifically, MVP essays have a 24.6% NPE increase and a 38.5% SPC increase, while SPACE essays have a 27.2% NPE increase and a 29.8% SPC increase from the first to the second drafts, respectively. ES increases over the three drafts, with the greatest increase (14.69% on MVP and 22.0% on SPACE) occurring from the first to the second drafts. These results reveal observable improvement of the essays from the first to the second draft after evidence use feedback, and from the second to the third drafts after revision feedback.\nAdditionally, we use the same manually annotated 516 student essays as in Table 2 to evaluate the quality changes between drafts, now using gold rather than predicted values. The annotated NPE improves on average from 2.20, 2.79, to 2.93 in the first, second, and third drafts, respectively, while the predicted NPE improves from 2.56, 3.19, to 3.34 (shown in Table 3). Although the system tends to overscore NPE, the inferences with respect to improvement across drafts are still the same (e.g., 26.8% vs. 24.6% improvement from the first to the second drafts). The annotated SPC improves from 9.00, 12.04, to 13.59 across drafts, while the predicted SPC improves from 8.81, 12.21, to 13.98 (shown in Table 3), which suggests using predictions yields the same conclusions as annotations regarding improvement in essay quality."}, {"title": "Feedback Effectiveness", "content": "To better understand whether students are responding to the particular feedback message they receive, we investigate both annotated and predicted NPE and SPC changes between two MVP drafts (old drafts before and new drafts after receiving feedback) in Table 4. For the draft1-draft2 essays, the annotated NPE increases 128% when the old drafts receive EF1 (needing more evidence) feedback, which suggests that students do follow the EF1 to add more evidence, given NPE measures the number of pieces of evidence used. Also, the annotated SPC increases 55.0% when the old drafts receive EF2 (needing more specific details), which also confirms students' essays have an improvement in specificity after revising essays based on EF2. When the old drafts had EF3 (needing more explanation), annotated NPE mostly remains no change (\u0394\u039d\u03a1\u0395=+1.98%) and SPC has slight improvement. This is because the RF guided students to add explanations, not evidence. These observations confirm that how student revisions change their evidence usage is as expected for all three EF levels. Similarly, we compare draft2-draft3 essays before and after receiving revision feedback. We observe annotated NPE improvement when draft2 has EF1 (e.g., RF was either RF3, RF4, RF5, or RF6 in Figure 3), which suggests revision feedback helps students add more evidence. However, revision feedback may not be helpful when draft2 receives EF2 given the annotated NPE average drops by 11.3%, which suggests that student essays might slightly weaken the completeness of the evidence while revising toward specificity. These observations are consistent when using predicted NPE and SPC in Table 4. Also, we analyze predicted NPE and SPC changes for SPACE essays in Table 10 in the Appendix, which shows similar patterns. In sum, the average increases of NPE and SPC for two RTA essays suggest that both evidence use feedback and revision feedback are helpful for students to improve their writing, in ways that are in alignment with the system's feedback messages (RQ2)."}, {"title": "Case Study", "content": "We show a student's three argumentative essay drafts in Figure 4 to illustrate how the feedback helps the student improve the essay through revision. The first draft does not include sufficient evidence to support the claim as it has predicted NPE of 2. Thus, the student receives EF that \u201cadding more evidence would make your argument even more convincing.\u201d Afterward, the student makes surface (typo) and content revisions in the second draft by deleting one piece of evidence about school (purple) and adding another piece of evidence about farming (yellow). The second draft has the same predicted NPE of 2 but SPC increases from 1 to 5 since it replaces school with farming and adds more details about farming. Additional RF is provided \u201c...When writers revise their text-based essays, they generally add new content from the text and delete content that is not based on the text.\u201d In response, the student makes additional surface and content revisions in the third draft by adding a piece of evidence about hospital with more details, which increases predicted NPE to 3 and SPC to 9. This example suggests the feedback is helpful for the student to reflect on revisions that have been made and further revise the essay. We show additional cases in Table 11 and 12 in the Appendix."}, {"title": "Conclusion", "content": "We present eRevise+RF, an advanced version of the eRevise system that extends and integrates algorithms to assess argumentative essay revisions, and to provide formative feedback on both evidence usage and revision success in response to feedback. Although the algorithms are primarily adapted from our prior published work, their integration into a complete and deployed system is a novel contribution. Through our pilot studies, we show promise of using NLP algorithms to scaffold young students in writing and revising text-based argumentative essays. In future work, we will deploy the system in more classrooms and with other RTA articles, to generalize our findings."}, {"title": "Ethics", "content": "eRevise+RF was used to collect our essay corpus under standard protocols approved by an institutional review board (IRB). Our data is not publicly available at the moment to ensure the safety of the private information of young students. The system is currently accessible only to the teachers and students participating in our study who have been assigned user accounts, in compliance with IRB constraints and data privacy requirements that restrict open user registration. However, the system source code is made publicly available on GitHub to support contributions to the NLP community. The system does not pose any ethical concerns because of the limited access to the data, but there might be a risk that the system may give poor advice based on incorrect scoring indicators or classifier predictions, or that the predictive models may learn biases due to small annotated training corpora. However, due to restrictions of IRB and policies of partner schools, only school-level but not student-level demographics are available which makes it difficult to evaluate potential biases."}, {"title": "Limitations", "content": "System. Although the eRevise+RF system demonstrates promise in helping students revise essays in response to formative feedback, its evaluation is based on a relatively small essay corpus (i.e., 516 MVP and 450 SPACE essays). Nonetheless, the corpus exhibits substantial diversity in both grade levels and demographics. For example, one school in PA includes students from 45 different zip code areas, while two schools in LA have minority enrollments of 98% and 65%, respectively. The analyses in Sec. 5.2 demonstrate the robustness of students' essay improvement across these diverse settings. Although the system deployments were pilot and lacked a control group where students wrote without receiving feedback since we didn't have enough classrooms to make this feasible, it is a goal of future studies. However, our analysis in Sec. 5.3 was designed to start teasing apart the impact of the system on students compared to unscaffolded revision. Our results show that the student revisions were in fact largely responsive to the feedback. Although we currently only assess the evidence dimension of the RTA, we started there because a persistent criticism of AWE systems is that they have not been designed to meet ambitious writing standards such as text-based argument writing (Burstein et al., 2020). But, we note other aspects of writing quality such as creativity need to be studied in future work.\nLLM. Our methods are mostly based on rubrics designed by RTA experts rather than advanced LLM. This is because we notice that for our deployment with young students, hallucinations and other LLM generation concerns such as offensive content and biases would need to be carefully studied and addressed. Additionally, a recent study (Behzad et al., 2024) compares LLM-generated essay feedback with human feedback and concludes that human feedback is more effective in delivering specific and actionable comments that address the most critical issues in an essay. This suggests young student learners may benefit more from expert feedback tailored to their specific writing challenges. In future work, we aim to integrate both expert and LLM-generated feedback into the system to provide nuanced and adaptive feedback messages.\nAlgorithm. The evidence scoring indicators described in Sec 3.1 sometimes overcount because the text-based evidence is not perfectly captured. Since the initial deployment, the collected and annotated essays have been used to tune the sliding window and text similarity algorithms in a data-driven manner. We have since modified the NPE algorithm based on an error analysis and it has now achieved a QWK of 0.87 compared to the original 0.67 in Table 2. The scoring indicator's reliance on manual keywords may not adapt well to different writing topics or styles unless continuously updated. Thus, other methods (Zhang and Litman, 2020, 2021) to avoid dependence on manual keyword identification could be implemented. Also, the trained revision classifiers RC-Evidence and RC-Success are not excellent, thus may cause the AES+RF system to provide incorrect revision feedback, which could be improved in future work."}]}