{"title": "On the Limitations of Compute Thresholds as a Governance Strategy.", "authors": ["Sara Hooker"], "abstract": "At face value, this essay is about understanding a fairly esoteric governance tool called compute thresholds. However, in order to grapple with whether these thresholds will achieve anything, we must first understand how they came to be. This requires engaging with a decades-old debate at the heart of computer science progress, namely, is \"bigger always better?\" Hence, this essay may be of interest not only to policymakers and the wider public but also to computer scientists interested in understanding the role of compute in unlocking breakthroughs. Does a certain inflection point of compute result in changes to the risk profile of a model? This discussion is increasingly urgent given the wide adoption of governance approaches that suggest greater compute equates with higher propensity for harm. Several leading frontier AI companies have released responsible scaling policies. Both the White House Executive Orders on AI Safety (EO) and the EU AI Act encode the use of FLOP or \"floating-point operations\" as a way to identify more powerful systems. What is striking about the choice of compute thresholds to-date is that no models currently deployed in the wild fulfill the current criteria set by the EO. This implies that the emphasis is often not on auditing the risks and harms incurred by currently deployed models - but rather is based upon the belief that future levels of compute will introduce unforeseen new risks. A key conclusion of this essay is that compute thresholds as currently implemented are shortsighted and likely to fail to mitigate risk. Governance that is overly reliant on compute fails to understand that the relationship between compute and risk is highly uncertain and rapidly changing. It also overestimates our ability to predict what abilities emerge at different scales. This essay ends with recommendations for a better way forward.", "sections": [{"title": "1 Understanding Risk", "content": "Inherent to the human experience is our desire to limit risk. We avoid walking down dark streets at night; we wear sunscreen to reduce the risk of skin damage; we use seatbelts when driving. Seeking to proactively control risk is one of the key differentiators of modern society. As the historian Peter Bernstein said, \"The ability to define what may happen in the future and to choose among alternatives lies at the heart of contemporary societies.\"\nRisk is a particularly challenging concept to formulate an effective governance response to, because it requires both 1) a successful estimate of the level and origins of risk to society and 2) aligning on a proportionate response. History is replete with examples where one or both requirements fail. For example, the large hu- man toll incurred by the black death is a good example of the difficulty of estimating what vectors amplify risk, where inadequate medical knowledge in the 1300s led to a failure to identify rats as one of the main carriers of the disease. In other cases, the risk is well known yet the response is inadequate. In 1966, the famous London fire swept through the city and dev-"}, {"title": "2 The Uncertain Relationship Between Compute and Risk.", "content": "Many inventions are re-purposed for means unintended by their designers. Initially, the magnetron tube was developed for radar technology during World War II. In 1945, a self-taught American engineer, Percy Spencer, noticed that a chocolate bar melted in his pocket when- ever he was close to a radar set. This innocuous dis- covery resulted in the patent for the first microwave. In a similar vein, deep neural networks only began to work when an existing technology was unexpectedly re-purposed. A graphical processing unit (GPU) was originally introduced in the 1970s as a spe- cialized accelerator for video games and for develop- ing graphics for movies and animation. In the 2000s, like the magnetron tube, GPUs were re-purposed for an entirely unimagined use case to train deep neural networks. GPUs had one critical advantage over CPUs - they were far better at paralleliz- ing matrix multiples, a mathemetical operation which dominates the definition of deep neural network layers. This higher number of float- ing operation points per second (FLOP/s) combined with the clever distribution of training between GPUs unblocked the training of deeper networks. The depth of the network turned out to be critical. Performance on ImageNet jumped with ever deeper networks in 2011, 2012 and 2015. A striking example of this jump in compute is a comparison of the now fa- mous 2012 Google paper which used 16,000 CPU cores to classify cats to a paper published a mere year later that solved the same task with only two CPU cores and four GPUs.\nThis would ignite a rush for compute which has led to a bigger-is-better race in the number of model parame- ters over the last decade. The computer scientist Ken Thompson famously said \u201cWhen in doubt, use brute force.\" This was formalized as the \"bitter les- son\" by Rich Sutton who posited that computer sci- ence history tells us that throwing more compute at a problem has consistently outperformed all attempts to leverage human knowledge of a domain to teach a model. In a punch to the ego of every computer scientist out there, what Sutton is saying is that sym- bolic methods that codify human knowledge have not worked as well as letting a model learn patterns for itself"}, {"title": "2.1 A shift in the relationship between compute and performance", "content": "In complex systems, it is challenging to manipulate one variable in isolation and foresee all implications. Throughout the 20th century doctors recommended re- moving tonsils in response to any swelling or infection, but research has recently shown the removal may lead to higher incidence of throat cancer. Early televised drug prevention advertisements in the 2000s led to increased drug use. In a similar vein, the belief that more compute equates with more risk belies a far more complex pic- ture that requires re-examining the relationship between performance and compute. A key limitation of simply throwing more scale at a task is that the relationship between additional compute and generalization remains poorly understood. A growing body of research suggests that the relationship between compute and performance is far more complex. Empirical evidence suggests that small models are rapidly becoming more performant and riskier.\nData quality reduces reliance on compute. Mod- els trained on better data do not require as much com- pute. A large body of work has emerged which shows that efforts to better curate training corpus, including de-duping , data pruning , or data prioritization can com- pensate for more weights. This suggests that the num- ber of learnable parameters is not definitively the con- straint on improving performance; investments in better data quality mitigate the need for more weights. If the size of a training dataset can be reduced without impacting performance , training time is reduced. This directly impacts the number of training FLOP and means less compute is needed."}, {"title": "3 Avoiding a FLOP FLOP", "content": "Are FLOP a reliable proxy for overall compute? Even if the relationship between compute and generaliza- tion were stable there are difficulties operationaliz- ing FLOP as a metric. FLOP refers to floating-point operations, and has a fairly straightfor- ward definition: sum up all the math operations in float- ing point (such as addition, subtraction, multiplication, and division). In the 1950s and 1960s, as computers were becoming more prevalent, the need for a standard measure of performance arose. FLOP are particularly useful in fields that require floating-point calculations, such as scientific computations, advanced analytics, and 3D graphics processing. This is because all these areas are dominated by simple primitive mathematical opera- tions - for example, FLOP tend to be closely associated with the size of models because deep neural network lay- ers are dominated by a single operation \u2013 matrix mul- tiplies - which can be decomposed into a set of floating point operations.\nWe first begin by noting there are some reasons FLOP are attractive as a policy measure. The pri- mary one is that FLOP provides a standardized way to compare across different hardware and software stacks. FLOP counts don't change across hardware - the num- ber of mathematical operations is the same no matter what hardware you train a model on. In a world where hardware is increasingly heterogeneous and it is hard to replicate the exact training setting due to a lack of software portability , it is attractive to use a metric that doesn't depend on replicating exact infrastructure. It also neatly sidesteps reporting issues that could occur if relying only on the number of hardware devices used to train a model. The rapidly increasing performance of new hardware gener- ations , as well as engineering investments in training infrastructure , mean that over time much larger models will be trained using the same number of de- vices. FLOP is also a metric which could potentially be inferred by cloud providers. Given most machine learn- ing workloads are run by a few key cloud providers, this may make administering such a measure effectively eas- ier.\nA key conundrum posed by FLOP thresholds is that policymakers are using FLOP as a proxy for risk, but FLOP doesn't say anything about end performance of a model only about the number of operations applied to the data. For example, if you compare two models trained for the same number of FLOP but one has had safety alignment during post-training and the other has none these two models will still be accorded the same level of risk according to number of FLOP but one will present a far lower risk to society because of safety alignment.\nAnother key hurdle governance which adopts compute threshold will have to overcome is the lack of clear guid- ance in all the policy to-date about how FLOP will ac- tually be measured in practice. This ambiguity risks FLOP as a metric being irrelevant or at the very least easy to manipulate. Developing principled standards for measuring any metric of interest is essential for ensuring that safety measures are applied in a proportionate and appropriate way. In the followings Section, we specify some of the key ways in which it is easy to manipulate FLOP if it is left underspecified as a metric."}, {"title": "3.1 Challenges of using FLOP as a metric", "content": "Training FLOP doesn't account for post-training leaps in performance Applying scrutiny and regula- tion based upon training FLOP ignores that a lot of compute can be spent outside of training to improve performance of a model. This can be grouped under \"inference-time compute\" and can result in large perfor- mance gains that dramatically increase the risk profile of a model. The limited work to-date which has evalu- ated a subset of 'inference-time compute\" improvements estimates these can impart gains between 5x and 20x of base level post-training performance. \"Inference-time compute\u201d includes best-of-n sam- pling techniques, chain-of-thought reasoning and model distillation using synthetic data . All these techniques require more compute at test-time because of the need to perform more forward passes of the model to gen- erate additional samples. However, these are not re-\nflected in training time costs and indeed can often re- duce the compute needed during training. For example, smaller, more performant models are often trained on smaller amounts of synthetic data from a highly per- formant teacher. These improvements dramatically improve performance but are currently completely ignored by compute thresholds since they don't contribute to train- ing FLOP.\nIncreasing the context-length and retrieval augmented systems are additional exam- ples of introducing additional computational overhead at test-time by increasing the number of tokens to pro- cess. Retrieval augmented models (RAG) have become a mainstay of state-of-art models yet are often intro- duced after training. Most RAG systems are critical for keeping models up-to-date with knowledge yet con- tribute minimal or no FLOP. Retrieval augmented mod- els are particularly good at supplementing models with search capabilities or external knowledge, which can enhances risks which depend on up-to-date knowledge such as biorisk and cybersecurity threats.\nAdditionally increasing the context length often requires minimal FLOP but can dramatically increase perfor- mance of a model. Entire books can be passed in at test time dramatically improving model performance on spe- cialized tasks (Gemini has 2M context window) . This can make the number of FLOP irrel- evant if sensitive biological data can be passed at infer- ence time in a long-context window.\nDifficulty Tracking FLOP across model lifecycle. Increasingly, training a model falls into distinct stages that all confer different properties. For example, unsu- pervised pre-training dominates compute costs because the volume of data is typically in the trillions of tokens . Following this, there is in- struction finetuning, which confers the model the abil- ity to follow instructions and then preference training , which aligns model performance with human values. Between each of these steps models are often released publicly"}, {"title": "4 We are not very good at predicting the relationship between compute and risk", "content": "The choice of where compute thresholds are set will have far-ranging implications - too low and too many mod- els will be selected for additional auditing and bench- marking each year. In contrast, if it is set too high, not enough models will be audited for risk, and the threshold risks become decorative rather than a mean- ingful indicator of risk. None of the policies to date have provided justification about where they have set their thresholds, or why it excludes almost all models deployed in the wild today. In Section 2.1, we grappled with the changing overall relationship between compute and performance. However, scientific justification for a threshold requires predicting how downstream risk scales with additional compute. Indeed, ideally the choice of hard coded threshold reflects scientific con- sensus as to when particular risk factors are expected to emerge due to scale. Hence, it is worth considering our success to date in estimating how different model properties change with scale.\nWarren Buffet once said \u201cDon't ask the barber if you need a haircut.\" In the same vein, don't ask a computer scientist or economist whether you can predict the fu- ture. The temptation to say yes often overrides a neces- sary humility about what can and cannot be predicted accurately. One such area where hubris has overridden common sense is attempts to predict the relationship be- tween scale and performance in the form of scaling laws which either try and predict how a model's pre-training loss scales or how down- stream properties emerge with scale. It is the latter task which is urgently needed by policymakers in order to an- ticipate the emergence of unsafe capabilities and inform restrictions (such as compute thresholds) at inflection points where risk increases with scale.\nOne of the biggest limitations of scaling laws is that they have only been shown to hold when predicting a model's pre-training test loss , which measures the model's ability to correctly predict how an incomplete piece of text will be continued. Indeed, when actual performance on downstream tasks is used, the results are often murky or inconsistent. Indeed, the term emerging properties is often used to describe this discrepancy : a property that appears \"suddenly\" as the complexity of the system increases and cannot be predicted. Emergent properties imply that scaling laws don't hold when you try to predict downstream perfor- mance instead of predicting test loss for the next word token.\nEven when limited to predicting test loss, there have been issues with replicability of scaling results under slightly different assumptions about the distribution . Research has also increasingly found that many downstream ca- pabilities display irregular scaling curves or non power-law scaling. For complex systems that require projecting into the future, small errors end up accumulating due to time step dependencies being modelled. This makes"}, {"title": "5 The Way Forward", "content": "Compute thresholds could be much improved by moving to dynamic instread of static thresholds An unpredictable relationship between compute and performance means that there will likely be false nega- tives when a hard threshold is set. That is, as smaller models become more performant, models which should be audited because of the risk they present avoid doing so because they fall underneath the threshold. Further- more, it is likely that policymakers will constantly have to revisit and redefine a sensible threshold, which im- poses technical overhead and creates issues with credi- bility.\nSophist Protagora (c. 485-410 B.C.) said Man is the measure of all things, implying that most of how we arrive at judgement is based upon relative perception. Instead of leveraging hard-coded thresholds, in the face of unknown distributions, it is more sensible to have rel- ative approaches for auditing that are easier to adapt"}, {"title": "5.1 Moving Away from Hard Coded Compute Thresholds", "content": "Compute thresholds to date propose a single number (1026 or 1025) to distinguish risky systems which merit more scrutiny. This hard-coding of a single threshold reflects a philosophy of absolutism, a legal and philo- sophical view that at least some truths in the relevant domain apply to all times, places or social and cultural frameworks. From a data-centric perspective, abso- lutism makes sense as a governance philosophy when the data distribution is well known and follows a predictable statistical pattern. For example, the use thresholds in medicine for classifying diabetes detection or for allocating additional care to infants based upon birth weight . These hard-coded thresholds have stood the test of time because these data distributions tend to be well-behaved and predictable.\nIn your introduction to machine learning class, this type of bell-shaped distribution was introduced to you as a normal distribution. In Figure 8, we plot some very common examples of close to normal distributions found in the wild. Unlike other distributions, the normal dis- tribution is well-behaved and remarkably symmetrical, with an equal number of outliers on each side. Nor- mal distributions in the real world also tend to coincide with distributions that don't change much over time. For example, the distribution of baby weights is un- likely to change tomorrow or even in the next 10 years. For these type of stable distributions where the data is well behaved hard thresholds make sense as a gover- nance tool. The stability of these distributions make it easy to determine outliers and have confidence that a set threshold will have longevity and not have to change ev-"}, {"title": "5.2 Parting Thoughts", "content": "It is very hard to trace how compute thresholds gained such traction in a short amount of time over national and international governance of AI. Compute thresh- olds are striking because they have emerged with no clear scientific support for either the thresholds chosen at 1026 and 1025, and largely only apply to future mod- els. One key recommendation that emerges from this essay is that we should be transparent about what risks we are concerned about. This is both to allow everyday citizens to weigh in on how government resources are allocated and also to allow for needed scientific scrutiny as to whether compute thresholds are a successful pro- tocol for estimating and mitigating risk.\nAny recommendation of compute as a metric to triage risk should be technically motivated by scientific evidence. When policy is introduced, it is often hard to change. The initial values chosen by the Executive Order, as described by the Computer Sci- entist Suresh Venkatasubramanian had huge \"signaling power\" and likely influenced the de- fault framing of discussion in the European Union that informed the EU Act. Given this intertia, it is even more critical that governance strategies like thresholds are motivated by scientific evidence. The choice of 1026 and 1025 rather than a number smaller or larger has not been justified in any of the policies implementing com- pute thresholds as a governance strategy. To motivate a compute threshold we should be able to articulate what risks we believe will be mitigated by investing in scrutiny of models at that threshold.\nGiven the wide adoption of compute thresholds across governance structures, scientific support seems neces- sary in the same way precautionary policies that aim to present harm from climate change or policies to improve public health are justified after weighing the scientific evidence. Govern- ments should invite technical reports from a variety of experts before adopting thresholds. If hard thresholds are chosen as part of national or international gover- nance, they should be motivated by scientific consensus.\nPolicymakers face a formidable task ahead of them. What is humbling and, at times, overwhelming to pon- der is that computer science as a discipline is incredibly young it has been a mere 68 years since the Dart- mouth workshop where the term Artificial Intelligence was coined. Much remains to be discovered, and new tools will pose formidable risks and benefits. Perhaps one of the key takeaways of this essay, is that we must have necessary humility about our ability to predict the future. Compute thresholds are currently presented as a very rigid governance tools because of the emphasis on a single static number to tier risk. These types of estimates are prone to failure precisely because of how rapidly the landscape is changing. Instead, we should focus on flexible tools for monitoring risk that are not tied to static numbers. Furthermore, FLOP as a mea- sure can be greatly improved by standardizing reporting and closing possible loopholes. In the previous Section 5.1, we discussed some of these recommendations. As to what comes next, the only certain thing is that some- thing will come next. Perhaps fitting to conclude with a quote from Alan Turing \"We can only see a short dis- tance ahead, but we can see plenty there that needs to be done.\"\""}, {"title": "A Technical Challenges of Measuring FLOP", "content": "How to handle quantized models? Models are often quantized during training to reduce memory require- ments . Increasingly modern net- works are robust to higher level of quantization and can trained with weights at different levels of precision, such as FP16, FP8, INT8 and INT4. While the US Executive Order acknowledges the widespread use of quantization by applying the same compute threshold of 1026 to in- teger operations, the EU AI Act fails to specify how to handle integer operations. Both end up failing to handle quantized models in a meaningful way. In the case of the US Executive Order, setting the same threshold for integers and floating points makes no sense because typ- ically lower precision operations impacts performance significantly . Hence, a quan- tized model will not present the same risk profile as a non-quantized model with the same number of FLOP. However, the EU AI Act risks completely ignoring any model with quantized operations and hence creates a loophole for application of compute thresholds.\nDifference between theoretical and practical FLOP The current legislation also fails to specify whether theoretical or practical FLOP will serve as the unit of measurement. Theoretical FLOP refers to the maximum number of FLOP a computer or pro- cessor can do based on its architecture and specifica- tions. Measured FLOP, on the other hand, represents the actual computational performance observed during real-world applications. Theoretical FLOP are easier to measure because of the difficulty of consistently mea- suring FLOP across very different types of hardware.\nNote that theoretical FLOP ignores practical factors, such as which parts of the model can be parallelized or hardware-related details like the cost of a memory access.\nTheoretical FLOP decreases with drop-out and sparsity. Theoretical FLOP can be minimized by us- ing drop-out and sparsity despite these models having comparable or even superior performance to fully dense models. For example, unstructured pruning and weight-specific quantization are very successful compression techniques in deep neural networks. This keeps the overall structure of the origi- nal model, while significantly reducing the FLOP of the most expensive operations. Dropout is a popular regularization strategy during pre- training, where weights are temporarily set to zero, but all weights are fully utilized during inference. However, both these techniques minimize the theoretical FLOP feasible."}, {"title": "BA wider view of what determines return on compute", "content": "Additional details on why convolutional and transformers unlock new patterns on scaling. The introduction of a new architecture design can fun- damentally change the relationship between compute and performance and render any compute threshold that is set irrelevant. For example, the key breakthroughs in AI adoption around the world were the introduction of architectures like convolutional neural networks (CNNs) for vision and Transformers for language modeling .\nBoth of these architectures have design details that make the search space for learning a good representa- tion much more efficient. For example, convolutional neural networks apply the same set of filters across dif- ferent regions of the input image. This assumes that the same feature can appear at different locations in the in- put image - for example \"sky\" can be in different parts of an image across a dataset. This local connectivity and weight sharing exploit the inherent spatial struc- ture and local correlations present in natural images. It is also incredibly efficient, leading to a significant reduc- tion in the number of parameters compared to fully con- nected neural networks; advantageous for vision prob-"}, {"title": "C Energy Requirements of Al Workloads over Time", "content": "It is important to make a distinction between the shift- ing trends between compute and performance, and over- all computational overhead of AI as a whole. While we will see ever smaller, more performant models - AI workloads will also be deployed in many more settings. This means that this essay should not be taken as a position that the overall environmental impact and en- ergy cost of AI is not a formidable problem. This essay does not speak to the overall energy requirements of A\u0399 workloads over time. It only speaks to the bifurcation of trends where individual workloads are smaller and more performant. This caveate is important to make, because typically most energy requirements of AI workloads is not in training, but instead in deploying at test time. This means even if model size is trending smaller, over- all energy requirements may still grow by AI be used in more and more places. While in the long run, smaller models help with efficiency and energy management, the widespread adoption of AI means overall energy require- ments will likely continue to rise and is non-negligible . More work is needed to understand the intersection of these two dynamics, and how it impacts overall energy needs."}]}