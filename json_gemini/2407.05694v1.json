{"title": "On the Limitations of Compute Thresholds as a Governance Strategy.", "authors": ["Sara Hooker"], "abstract": "At face value, this essay is about understanding a fairly esoteric governance tool called compute thresholds. However, in order to grapple with whether these thresholds will achieve anything, we must first understand how they came to be. This requires engaging with a decades-old debate at the heart of computer science progress, namely, is \"bigger always better?\" Hence, this essay may be of interest not only to policymakers and the wider public but also to computer scientists interested in understanding the role of compute in unlocking breakthroughs. Does a certain inflection point of compute result in changes to the risk profile of a model? This discussion is increasingly urgent given the wide adoption of governance approaches that suggest greater compute equates with higher propensity for harm. Several leading frontier AI companies have released responsible scaling policies. Both the White House Executive Orders on AI Safety (EO) and the EU AI Act encode the use of FLOP or \"floating-point operations\" as a way to identify more powerful systems. What is striking about the choice of compute thresholds to-date is that no models currently deployed in the wild fulfill the current criteria set by the EO. This implies that the emphasis is often not on auditing the risks and harms incurred by currently deployed models - but rather is based upon the belief that future levels of compute will introduce unforeseen new risks. A key conclusion of this essay is that compute thresholds as currently implemented are shortsighted and likely to fail to mitigate risk. Governance that is overly reliant on compute fails to understand that the relationship between compute and risk is highly uncertain and rapidly changing. It also overestimates our ability to predict what abilities emerge at different scales. This essay ends with recommendations for a better way forward.", "sections": [{"title": "1 Understanding Risk", "content": "Inherent to the human experience is our desire to limit risk. We avoid walking down dark streets at night; we wear sunscreen to reduce the risk of skin damage; we use seatbelts when driving. Seeking to proactively control risk is one of the key differentiators of modern society. As the historian Peter Bernstein said, \"The ability to define what may happen in the future and to choose among alternatives lies at the heart of contemporary societies.\"\nRisk is a particularly challenging concept to formulate an effective governance response to, because it requires both 1) a successful estimate of the level and origins of risk to society and 2) aligning on a proportionate response. History is replete with examples where one or both requirements fail. For example, the large hu- man toll incurred by the black death is a good example of the difficulty of estimating what vectors amplify risk, where inadequate medical knowledge in the 1300s led to a failure to identify rats as one of the main carriers of the disease (Benedictow, 2004). In other cases, the risk is well known yet the response is inadequate. In 1966, the famous London fire swept through the city and dev-"}, {"title": "2 The Uncertain Relationship Between Compute and Risk.", "content": "Many inventions are re-purposed for means unintended by their designers. Initially, the magnetron tube was developed for radar technology during World War II. In 1945, a self-taught American engineer, Percy Spencer, noticed that a chocolate bar melted in his pocket when- ever he was close to a radar set. This innocuous dis- covery resulted in the patent for the first microwave (Zhang, 2017). In a similar vein, deep neural networks only began to work when an existing technology was unexpectedly re-purposed. A graphical processing unit (GPU) was originally introduced in the 1970s as a spe- cialized accelerator for video games and for develop- ing graphics for movies and animation. In the 2000s, like the magnetron tube, GPUs were re-purposed for an entirely unimagined use case to train deep neural networks (Chellapilla et al., 2006; Hooker, 2021; Oh & Jung, 2004; Payne et al., 2005). GPUs had one critical advantage over CPUs - they were far better at paralleliz- ing matrix multiples (Brodtkorb et al., 2013; Dettmers, 2023), a mathemetical operation which dominates the definition of deep neural network layers (Fawzi et al., 2022; Davies et al., 2024). This higher number of float- ing operation points per second (FLOP/s) combined with the clever distribution of training between GPUs unblocked the training of deeper networks. The depth of the network turned out to be critical. Performance on ImageNet jumped with ever deeper networks in 2011 (Ciresan et al., 2011), 2012 (Krizhevsky et al., 2012) and 2015 (Szegedy et al., 2014).\nThis would ignite a rush for compute which has led to a bigger-is-better race in the number of model parame- ters over the last decade (Canziani et al., 2016; Strubell et al., 2019b; Rae et al., 2021; Raffel et al., 2020; Bom- masani et al., 2021; Bender et al., 2021). The computer scientist Ken Thompson famously said \u201cWhen in doubt, use brute force."}, {"title": "2.1 A shift in the relationship between compute and performance", "content": "In complex systems, it is challenging to manipulate one variable in isolation and foresee all implications. Throughout the 20th century doctors recommended re- moving tonsils in response to any swelling or infection, but research has recently shown the removal may lead to higher incidence of throat cancer (Liang et al., 2023). Early televised drug prevention advertisements in the 2000s led to increased drug use (Terry-McElrath et al., 2011). In a similar vein, the belief that more compute equates with more risk belies a far more complex pic- ture that requires re-examining the relationship between performance and compute. A key limitation of simply throwing more scale at a task is that the relationship between additional compute and generalization remains poorly understood. A growing body of research suggests that the relationship between compute and performance is far more complex. Empirical evidence suggests that small models are rapidly becoming more performant and riskier.\nData quality reduces reliance on compute. Mod- els trained on better data do not require as much com- pute. A large body of work has emerged which shows that efforts to better curate training corpus, including de-duping (Taylor et al., 2022; Kocetkov et al., 2022), data pruning (Marion et al., 2023; Singh et al., 2024a; Sorscher et al., 2023; Albalak et al., 2024; Tirumala et al., 2023; Chimoto et al., 2024) or data prioritization (Boubdir et al., 2023; Thakkar et al., 2023) can com- pensate for more weights. This suggests that the num- ber of learnable parameters is not definitively the con- straint on improving performance; investments in better data quality mitigate the need for more weights (Singh et al., 2024a; Penedo et al., 2023; Raffel et al., 2020; Lee et al., 2022). If the size of a training dataset can be reduced without impacting performance (Marion et al., 2023), training time is reduced. This directly impacts the number of training FLOP and means less compute is needed."}, {"title": "3 Avoiding a FLOP FLOP", "content": "Are FLOP a reliable proxy for overall compute? Even if the relationship between compute and generaliza- tion were stable there are difficulties operationaliz- ing FLOP as a metric. FLOP (Goldberg, 1991) refers to floating-point operations, and has a fairly straightfor- ward definition: sum up all the math operations in float- ing point (such as addition, subtraction, multiplication, and division). In the 1950s and 1960s, as computers were becoming more prevalent, the need for a standard measure of performance arose. FLOP are particularly useful in fields that require floating-point calculations, such as scientific computations, advanced analytics, and 3D graphics processing. This is because all these areas are dominated by simple primitive mathematical opera- tions - for example, FLOP tend to be closely associated with the size of models because deep neural network lay- ers are dominated by a single operation \u2013 matrix mul- tiplies - which can be decomposed into a set of floating point operations (Fawzi et al., 2022; Davies et al., 2024).\nWe first begin by noting there are some reasons FLOP are attractive as a policy measure. The pri- mary one is that FLOP provides a standardized way to compare across different hardware and software stacks. FLOP counts don't change across hardware - the num- ber of mathematical operations is the same no matter what hardware you train a model on. In a world where hardware is increasingly heterogeneous (Hooker, 2021) and it is hard to replicate the exact training setting due to a lack of software portability (Mince et al., 2023), it is attractive to use a metric that doesn't depend on replicating exact infrastructure. It also neatly sidesteps reporting issues that could occur if relying only on the number of hardware devices used to train a model. The rapidly increasing performance of new hardware gener- ations (Hobbhahn et al., 2023), as well as engineering investments in training infrastructure (Yoo et al., 2022; Lepikhin et al., 2020), mean that over time much larger models will be trained using the same number of de- vices. FLOP is also a metric which could potentially be inferred by cloud providers. Given most machine learn- ing workloads are run by a few key cloud providers, this may make administering such a measure effectively eas- ier (Heim et al., 2024).\nA key conundrum posed by FLOP thresholds is that policymakers are using FLOP as a proxy for risk, but FLOP doesn't say anything about end performance of a model only about the number of operations applied"}, {"title": "3.1 Challenges of using FLOP as a metric", "content": "Training FLOP doesn't account for post-training leaps in performance Applying scrutiny and regula- tion based upon training FLOP ignores that a lot of compute can be spent outside of training to improve performance of a model. This can be grouped under \"inference-time compute\" and can result in large perfor- mance gains that dramatically increase the risk profile of a model. The limited work to-date which has evalu- ated a subset of 'inference-time compute\" improvements estimates these can impart gains between 5x and 20x of base level post-training performance (Davidson et al., 2023). \"Inference-time compute\u201d includes best-of-n sam- pling techniques (Team et al., 2024a), chain-of-thought reasoning (Wei et al., 2023; Hsieh et al., 2023; Wang et al., 2023c) and model distillation using synthetic data (Aryabumi et al., 2024; Shimabucoro et al., 2024; \u00dcst\u00fcn et al., 2024; Team et al., 2024a). All these techniques require more compute at test-time because of the need to perform more forward passes of the model to gen- erate additional samples. However, these are not re-"}, {"title": "4 We are not very good at predicting the relationship between compute and risk", "content": "The choice of where compute thresholds are set will have far-ranging implications - too low and too many mod- els will be selected for additional auditing and bench- marking each year. In contrast, if it is set too high, not enough models will be audited for risk, and the threshold risks become decorative rather than a mean- ingful indicator of risk. None of the policies to date have provided justification about where they have set their thresholds, or why it excludes almost all models deployed in the wild today. In Section 2.1, we grappled with the changing overall relationship between compute and performance. However, scientific justification for a threshold requires predicting how downstream risk scales with additional compute. Indeed, ideally the choice of hard coded threshold reflects scientific con- sensus as to when particular risk factors are expected to emerge due to scale. Hence, it is worth considering our success to date in estimating how different model properties change with scale.\nWarren Buffet once said \u201cDon't ask the barber if you need a haircut.\" In the same vein, don't ask a computer scientist or economist whether you can predict the fu- ture. The temptation to say yes often overrides a neces- sary humility about what can and cannot be predicted accurately. One such area where hubris has overridden common sense is attempts to predict the relationship be- tween scale and performance in the form of scaling laws (Kaplan et al., 2020; Hernandez et al., 2021; Dhariwal et al., 2021) which either try and predict how a model's pre-training loss scales (Bowman, 2023) or how down- stream properties emerge with scale. It is the latter task which is urgently needed by policymakers in order to an- ticipate the emergence of unsafe capabilities and inform restrictions (such as compute thresholds) at inflection points where risk increases with scale (Anthropic, 2023; OpenAI, 2023; Kaminski, 2023).\nOne of the biggest limitations of scaling laws is that they have only been shown to hold when predicting a model's pre-training test loss (Bowman, 2023), which measures the model's ability to correctly predict how an incomplete piece of text will be continued. Indeed, when actual performance on downstream tasks is used, the results are often murky or inconsistent (Ganguli et al., 2022; Schaeffer et al., 2023; Anwar et al., 2024b; Gan- guli et al., 2022; Schaeffer et al., 2024b; Hu et al., 2024). Indeed, the term emerging properties is often used to describe this discrepancy (Wei et al., 2022; Srivastava et al., 2023): a property that appears \"suddenly\" as the complexity of the system increases and cannot be predicted. Emergent properties imply that scaling laws don't hold when you try to predict downstream perfor- mance instead of predicting test loss for the next word token.\nEven when limited to predicting test loss, there have been issues with replicability of scaling results under slightly different assumptions about the distribution (Besiroglu et al., 2024; Anwar et al., 2024a). Research has also increasingly found that many downstream ca- pabilities display irregular scaling curves (Srivastava et al., 2023) or non power-law scaling (Caballero et al., 2023). For complex systems that require projecting into the future, small errors end up accumulating due to time step dependencies being modelled. This makes\""}, {"title": "5 The Way Forward", "content": "Compute thresholds to date propose a single number (1026 or 1025) to distinguish risky systems which merit more scrutiny. This hard-coding of a single threshold reflects a philosophy of absolutism, a legal and philo- sophical view that at least some truths in the relevant domain apply to all times, places or social and cultural frameworks. From a data-centric perspective, abso- lutism makes sense as a governance philosophy when the data distribution is well known and follows a predictable statistical pattern. For example, the use thresholds in medicine for classifying diabetes detection (Saudek et al., 2008) or for allocating additional care to infants based upon birth weight (Cutland et al., 2017; Seri & Evans, 2008). These hard-coded thresholds have stood the test of time because these data distributions tend to be well-behaved and predictable.\nIn your introduction to machine learning class, this type of bell-shaped distribution was introduced to you as a normal distribution. In Figure 8, we plot some very common examples of close to normal distributions found in the wild. Unlike other distributions, the normal dis- tribution is well-behaved and remarkably symmetrical, with an equal number of outliers on each side. Nor- mal distributions in the real world also tend to coincide with distributions that don't change much over time. For example, the distribution of baby weights is un- likely to change tomorrow or even in the next 10 years. For these type of stable distributions where the data is well behaved hard thresholds make sense as a gover- nance tool. The stability of these distributions make it easy to determine outliers and have confidence that a set threshold will have longevity and not have to change ev-"}, {"title": "5.1 Moving Away from Hard Coded Compute Thresholds", "content": "Compute thresholds could be much improved by moving to dynamic instread of static thresholds An unpredictable relationship between compute and performance means that there will likely be false nega- tives when a hard threshold is set. That is, as smaller models become more performant, models which should be audited because of the risk they present avoid doing so because they fall underneath the threshold. Further- more, it is likely that policymakers will constantly have to revisit and redefine a sensible threshold, which im- poses technical overhead and creates issues with credi- bility.\nInstead of leveraging hard-coded thresholds, in the face of unknown distributions, it is more sensible to have rel- ative approaches for auditing that are easier to adapt"}, {"title": "A Technical Challenges of Measuring FLOP", "content": "How to handle quantized models? Models are often quantized during training to reduce memory require- ments (Ahmadian et al., 2023; Marchisio et al., 2024; Frantar et al., 2022; Xiao et al., 2023; Dettmers et al., 2023; 2022; Lin et al., 2024a). Increasingly modern net- works are robust to higher level of quantization and can trained with weights at different levels of precision, such as FP16, FP8, INT8 and INT4. While the US Executive Order acknowledges the widespread use of quantization by applying the same compute threshold of 1026 to in- teger operations, the EU AI Act fails to specify how to handle integer operations. Both end up failing to handle quantized models in a meaningful way. In the case of the US Executive Order, setting the same threshold for integers and floating points makes no sense because typ- ically lower precision operations impacts performance significantly (Ahmadian et al., 2023). Hence, a quan- tized model will not present the same risk profile as a non-quantized model with the same number of FLOP. However, the EU AI Act risks completely ignoring any model with quantized operations and hence creates a loophole for application of compute thresholds.\nDifference between theoretical and practical FLOP The current legislation also fails to specify whether theoretical or practical FLOP will serve as the unit of measurement. Theoretical FLOP refers to the maximum number of FLOP a computer or pro- cessor can do based on its architecture and specifica- tions. Measured FLOP, on the other hand, represents the actual computational performance observed during real-world applications. Theoretical FLOP are easier to measure because of the difficulty of consistently mea- suring FLOP across very different types of hardware (Sevilla et al., 2022b).\nNote that theoretical FLOP ignores practical factors, such as which parts of the model can be parallelized or hardware-related details like the cost of a memory access (Dehghani et al., 2021).\nTheoretical FLOP decreases with drop-out and sparsity. Theoretical FLOP can be minimized by us- ing drop-out and sparsity despite these models having"}, {"title": "B A wider view of what determines return on compute", "content": "Additional details on why convolutional and transformers unlock new patterns on scaling. The introduction of a new architecture design can fun- damentally change the relationship between compute and performance (Tay et al., 2022; Sevilla et al., 2022a; Ho et al., 2024a) and render any compute threshold that is set irrelevant. For example, the key breakthroughs in AI adoption around the world were the introduction of architectures like convolutional neural networks (CNNs) for vision (Ciresan et al., 2011; Krizhevsky et al., 2012; Szegedy et al., 2014) and Transformers for language modeling (Vaswani et al., 2023).\nBoth of these architectures have design details that make the search space for learning a good representa- tion much more efficient. For example, convolutional neural networks apply the same set of filters across dif- ferent regions of the input image. This assumes that the same feature can appear at different locations in the in- put image - for example \"sky\" can be in different parts of an image across a dataset. This local connectivity"}, {"title": "C Energy Requirements of Al Workloads over Time", "content": "It is important to make a distinction between the shift- ing trends between compute and performance, and over- all computational overhead of AI as a whole. While we will see ever smaller, more performant models - AI workloads will also be deployed in many more settings. This means that this essay should not be taken as a position that the overall environmental impact and en- ergy cost of AI is not a formidable problem. This essay does not speak to the overall energy requirements of A\u0399 workloads over time. It only speaks to the bifurcation of trends where individual workloads are smaller and more"}]}