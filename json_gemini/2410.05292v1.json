{"title": "CALMFLOW: VOLTERRA FLOW MATCHING USING CAUSAL LANGUAGE MODELS", "authors": ["Sizhuang He", "Daniel Levine", "Ivan Vrkic", "Marco Francesco Bressana", "David Zhang", "Syed A. Rizvi", "Yangtian Zhang", "Emanuele Zappala", "David van Dijk"], "abstract": "We introduce CaLMFlow (Causal Language Models for Flow Matching), a novel\nframework that casts flow matching as a Volterra integral equation (VIE), leverag-\ning the power of large language models (LLMs) for continuous data generation.\nCaLMFlow enables the direct application of LLMs to learn complex flows by\nformulating flow matching as a sequence modeling task, bridging discrete lan-\nguage modeling and continuous generative modeling. Our method implements\ntokenization across space and time, thereby solving a VIE over these domains. This\napproach enables efficient handling of high-dimensional data and outperforms ODE\nsolver-dependent methods like conditional flow matching (CFM). We demonstrate\nCaLMFlow's effectiveness on synthetic and real-world data, including single-cell\nperturbation response prediction, showcasing its ability to incorporate textual con-\ntext and generalize to unseen conditions. Our results highlight LLM-driven flow\nmatching as a promising paradigm in generative modeling, offering improved\nscalability, flexibility, and context-awareness.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in deep learning have revolutionized generative modeling for complex, high-dimensional data. In particular, methods based on ordinary differential equations (ODEs), such as continuous normalizing flows (CNFs) (Chen et al., 2018) and flow matching (Lipman et al., 2022), have emerged as efficient tools for modeling continuous data distributions. However, many ODE systems suffer from stiffness making them numerically unstable and computationally expensive to solve accurately (Kushnir & Rokhlin, 2012; Zappala et al., 2024). In contrast, integral equations (IEs) offer a more generalized framework for capturing dynamics, with IE solvers demonstrating greater numerical stability than their ODE counterparts (Kushnir & Rokhlin, 2012; Zappala et al., 2024). Recent work in operator learning (Xiong et al., 2021; Cao, 2021; Zappala et al., 2024) has also connected solving integral equations with transformers, the foundational architecture of large language models (LLMs), inspiring the use of LLMs to model dynamical systems through the lens of IEs.\nBuilding on these insights, our work introduces Causal Language Models for Flow Matching (CaLMFlow), a novel approach that models flow matching using Volterra integral equations (Zappala et al., 2023), enabling learning flows in a more robust manner. By leveraging causal language models (CLMs) to solve Volterra integral equations, our method capitalizes on the ability of CLMs to comprehend natural language, allowing for the modeling of complex data distributions conditioned"}, {"title": "2 RELATED WORK", "content": "Flow Matching and Continuous Normalizing Flows: Flow matching has significantly enhanced the efficiency and scope of continuous normalizing flows (CNFs) (Chen et al., 2018; Papamakarios et al., 2021). Conditional Flow Matching (CFM) (Lipman et al., 2022) allows for precise control over the generative process by optimizing conditional vector fields tailored for specific distribution paths, including those based on Gaussian distributions. Tong et al. (2024) generalized the conditional paths and introduced mini-batch optimal transport and Schr\u00f6dinger bridge CFM, improving the efficiency and performance of CFM models. In Hu et al. (2024), flow matching is applied to text generation in a non-autoregressive manner, showing improvements compared to other diffusion-based text generation models such as DiffSeq (Gong et al., 2023). Our work, however, is primarily concerned with adapting LLMs to generate continuous data conditioned on text.\nText-conditional Generation: Text-conditional image generation has made significant strides through the integration of diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021) and large language models (LLMs). State-of-the-art systems like Stable Diffusion (Rombach et al., 2022), DALLE-2 (Ramesh et al., 2022), and DINOv2 (Oquab et al., 2024) leverage LLM embeddings to generate high-quality images from textual descriptions. Recent research (Ding et al., 2021; Yu et al., 2022; Ge et al., 2024; Zhan et al., 2024) has focused on adapting LLMs for multimodal generation, often employing vector quantization (van den Oord et al., 2017; Razavi et al., 2019; Ge et al., 2023) to extend LLM vocabularies with latent tokens representing non-textual data. Our CaLMFlow method introduces a novel approach as the first flow matching-based text-conditional generative model that produces continuous tokens, potentially offering greater flexibility and expressiveness compared to discrete token-based methods.\nIntegral Equations in Neural Network Frameworks: The fusion of neural networks and differential equations was introduced by Chen et al. (2018) with neural ordinary differential equations. This concept has since been extended to integral equations, particularly Volterra equations, in several studies (Fu & Hirsa, 2022; Zappala et al., 2023; 2024). Notably, Zappala et al. (2024) exploited the relationship between attention mechanisms and integral kernels (Xiong et al., 2021; Cao, 2021) to model integral equations using transformers. Concurrently, approaches like physics-informed neural networks (PINNs) (Raissi et al., 2019; Lu et al., 2021; Goswami et al., 2022) have emerged, incorporating physical laws as prior knowledge to enhance model accuracy and generalization in operator learning tasks. Our work builds upon these advancements, extending the application of Volterra integral equations to the flow matching framework."}, {"title": "3 VOLTERRA FLOW MATCHING", "content": ""}, {"title": "3.1 FLOW MATCHING AS VOLTERRA INTEGRAL EQUATIONS", "content": "Flow matching (Lipman et al., 2022) is typically formulated as learning the time-dependent vector field v(x, t) generating the flow $(x, t) related by the ordinary differential equation (ODE)\n$\\frac{d\\phi}{dt} = v(\\phi, t), \\qquad \\phi(x, 0) = x,$\n(1)\ntransforming an initial distribution po into a target distribution p\u2081 through the application of a numerical solver applied to the learned vector field.\nHowever, ODEs can suffer from stiffness, especially when modeling systems with rapid changes or long-range dependencies (Rokhlin, 1985; 1990; Kushnir & Rokhlin, 2012; Zappala et al., 2024) and, as a result, solving such systems are highly numerically unstable and computationally expensive. To address these challenges, we reformulate the flow matching problem using Volterra integral equations (VIEs) of the second kind:\n$z(t) = z(0) + \\int_{0}^{t} G(z(s),t,s)ds$\n(2)\nwhere G(z(s), t, s) is a Urysohn kernel function encoding the influence of past states on the current state. VIEs generalize ODEs, as shown by the transformation of Equation (1) into the equivalent integral form:\n$\\phi(t) = \\Phi_0 + \\int_{0}^{t} v(\\phi(s), s)ds.$\n(3)\nThis VIE formulation offers several advantages: it inherently accounts for nonlocal components in the dynamics, is more flexible and robust for modeling complex systems with memory effects, and avoids issues like stiffness and underflowing that ODE solvers encounter (Kushnir & Rokhlin, 2012; Zappala et al., 2024). Consequently, Volterra flow matching provides a more general and stable approach to modeling continuous flows between distributions."}, {"title": "3.2 SOLVING VOLTERRA INTEGRAL EQUATIONS WITH CAUSAL LANGUAGE MODELS", "content": "In CaLMFlow, we define the flow using a more general form of Equation 3:\n$z_t = f(z_t, t) + \\int_{0}^{t} G(z_s,t,s)ds,$\n(4)\nwhere 1) zt (also referred to as z(t)) is the state at time t, 2) f (zt, t) is the inhomogeneous term (20 in the case of Equation 3), and 3) The integral term $\\int_{0} G(z_s,t,s) ds$ captures the accumulated influence of past states.\nWe discretize and model the entire integral equation using a causal language model (CLM) following Xiong et al. (2021), Cao (2021), and Zappala et al. (2024). The time interval [0, 1] is first discretized into N time steps to, t1, ..., tv, resulting in a sequence (zto, Zt1, ..., Ztn). We can define a sequence of functions (z\u00ba (t), z\u00b9(t), . . ., zN (t)) where z\u00b2(t) is defined on [0, ti] and z\u00b2(tj) = Zt3. The CLM then acts as an iterative integral equation solver on the discretized VIE by predicting zi+1 from zi:\n$z^{i+1} = f(x^i, t_{i+1}) + \\sum_{j=0}^{i} \\Delta t_{i+1}G(z_j, t_{i+1}, t_j),$\n(5)\nwhere Atk = tk - tk-1. Further theoretical discussion in the language of Banach spaces can be found in Appendix C, and a generalization to higher dimensional integrals where space is also included is straightforward.\nSimilar to the discussion in Lipman et al. (2022), the na\u00efve Volterra flow matching objective\n$L_{VFM} = E_{p(z^N)} ||\\hat{z}_N - z_N||^2,$\n(6)\nwhere \u00bfN = (2to, 2t1, ..., 2tn) is the sequence of states predicted from the ground truth marginal trajectories (Equation 5) and p(zN) is the distribution of trajectories over the flow, is intractable due to the inaccessibility of ground truth marginal trajectories zN. Instead, we use the optimal transport conditional paths (Lipman et al., 2022; Tong et al., 2024) $z_{z_0,z_N}^{(t)} := (1 - t)z_0 + tz_N$ and optimize the conditional Volterra flow matching objective\n$L_{CVFM} = E_{p_0(z_0),q(z_N)} ||z_{z_0,z_N}^{(t)} - \\hat{z}||^2,$\n(7)\nwhere po is the initial source distribution (e.g., a Gaussian) and q is the target data distribution. A theoretical discussion of the different objective functions can be found in Appendix D. We note that next-token prediction in CLMs is simulation-free during training, as the next token is predicted based on the ground truth history. Full trajectory simulation occurs only during inference. As such, CaLMFlow, like CFM, operates as a simulation-free approach during training."}, {"title": "3.3 CONTINUOUS SPACE TOKENS VIA VARIATIONAL DECODING", "content": "We introduce variational decoding with a Kullback-Leibler divergence regularizer in our experiments. Unlike standard CLMs, which rely on a fixed vocabulary and model the next-token distribution through softmax probabilities, our approach enables the CLM to model a continuous distribution of next tokens. Specifically, we use a probabilistic encoder q(z|x) to map the CLM output tokens x to a posterior latent distribution N (z; \u03bc, \u03c3\u00b2I), and a probabilistic decoder py(x|z) to reconstruct the tokens x, where latent variable z acts as a continuous representation in a latent space.\nBoth qq and py are optimized by maximizing the evidence lower bound (ELBO) (Kingma & Welling, 2022):\n$-L_{VAE} = E_{q_\\phi (z|x)} [log p_\\theta (x|z)] - \\beta KL(q_\\phi (z|x)||p(z)),$\n(8)\nwhere p(z) := N(z; 0, I) is a prior over the latent variable z, \u03b2 is a scaling hyperparameter as introduced in (Higgins et al., 2017), and KL(q||p) is the Kullback-Leibler divergence. Using the ELBO, our Volterra flow matching objective becomes\n$L_{VCVFM} = L_{CVFM} + \\beta KL(q_\\phi (z|x)||p(z))$\n(9)\nTo control the generation of continuous tokens at inference, we use a temperature parameter 7 that scales the variance of the encoded posterior, analogous to the use of temperature in LLMs (Renze &\nGuven, 2024; Peeperkorn et al., 2024). Specifically, the posterior latent distribution is modified as q(z|x) ~ N(z; \u03bc, \u03c4\u03c32\u0399), where \u315c \u2265 0 is the temperature parameter. An ablation experiment for the temperature parameter is in Section 6.1."}, {"title": "4 SPATIOTEMPORAL AND MULTI-TRAJECTORY TOKENIZATION", "content": "In this section, we introduce our spatiotemporal and multi-trajectory tokenization method depicted in Figure 1 to effectively represent data for CLM training."}, {"title": "4.1 SPATIOTEMPORAL TOKENIZATION", "content": "We discretize the temporal domain [0, 1] using a fixed, evenly spaced grid {ti}1 of length N.\nThe discretized temporal dynamics are encoded as input matrix X \u2208 RT\u00d7Din comprising T tokens, where each token xi is of Din dimensions.\nTo introduce spatial structure into the representation, we further subdivide each temporal token xi into K tokens {xij}}=1. The splitting can be either learned or predefined, depending on the data. For instance, for single-cell data, we map the encoded gene expression features with a learned network So: RDin \u2192 RK\u00b7Din and split the token to K spatial tokens. In the case of images, we use an encoder to extract low-level features, resulting in a feature map that is split into tokens.\nThe final spatiotemporal token sequence Xst \u2208 RK\u00b7Din is represented as\n$X_{st} := [X_{11},..., X_{1K},..., X_{N1}, ..., X_{NK}]^T.$\n(10)\nThis spatiotemporal sequence of tokens is processed by the CLM, which approximates a Volterra integral equation over both space and time. The discussion in Section 3.2 extends to the spatiotemporal VIE:\n$z(x,t) = f(z,x,t) + \\int_{0}^{t} \\int_{\\Omega} G(z, x,x', t,s)dx'ds.$\n(11)"}, {"title": "4.2 MULTI-TRAJECTORY TOKENIZATION", "content": "We tokenize more than one conditional trajectory as input to the CLM by sampling M spatiotemporal sequences of tokens as above. The overall tokenized sequence Xsdt \u2208 RMK\u00b7Din becomes\n$X_{sdt} := [X_{111},..., X_{1K1}, ..., X_{1KM}, ..., X_{N11},..., X_{NKM}]^T.$\n(12)\nWe empirically find that providing information across a batch of trajectories as context benefits model performance. We observe that such benefits are unique to CLMs, whereas methods like CFM cannot natively model multiple trajectories. While multi-trajectory training and inference are related to integration over function spaces, exploring this connection is beyond the scope of our work and we leave it for future exploration."}, {"title": "5 EXPERIMENTS", "content": "We evaluate CaLMFlow on synthetic datasets to showcase the advantages of integral equations for modeling high-dimensional dynamical systems, and apply it to single-cell data generation, demonstrating its ability to model complex distributions and leverage natural language understanding."}, {"title": "5.1 SYNTHETIC DATASETS", "content": ""}, {"title": "5.1.1 HIGH DIMENSIONAL DATA", "content": "Stiffness is a well-known challenge in the numerical integration of ODEs (Kushnir & Rokhlin, 2012; Zappala et al., 2024), particularly in systems with high dimensionality. Conversely, the embedding dimensions of causal language models (CLMs) are inherently large, as demonstrated by pretrained models like GPT-2, which has an embedding dimension of 768. As such, we hypothesize that CaLMFlow is better at modeling high dimensional data than ODE-based methods. To evaluate the robustness of CaLMFlow in high-dimensional settings, we compare its performance against traditional ODE-based methods, which typically degrade as dimensionality increases.\nOur results, summarized in Table 1, demonstrate that while CFM breaks down at higher dimensions, CaLMFlow maintains strong performance. This suggests that CaLMFlow is an effective alternative to ODE-based approaches for modeling high-dimensional problems, providing stability and accuracy where methods like CFM fail."}, {"title": "5.1.2 MULTI-TRAJECTORY CONTEXT", "content": "While approaches like CFM focus on modeling the flow of individual points, CaLMFlow is able to sample multiple trajectories and model them simultaneously. The results in Table 2 show this approach improves the performance of CaLMFlow on generating synthetic data. Visualizations of generated results, as shown in Figure 2, further demonstrate that modeling several trajectories at the same time allows CaLMFlow to distribute data more evenly and accurately by leveraging trajectory context."}, {"title": "5.2 SINGLE-CELL GENERATION", "content": "We apply CaLMFlow to immune tissue single-cell expression data (Dong et al., 2023) to demonstrate its effectiveness in both unconditional and conditional generation of complex, high-dimensional real-world data. We utilize the first 1,000 principal components of the gene expression data as features. The dataset comprises annotations for 7 cell types, 10 perturbations, and 2 chronicities, leading to 140 unique combinatorial labels. In the unconditional generation experiment, the model generates the overall target distribution from Gaussian noise as initial conditions, regardless of labels. In the conditional generation experiment, five combinations of the labels are held out as a test set,"}, {"title": "5.2.1 UNCONDITIONAL GENERATION OF SINGLE-CELL DATA", "content": "The results in Table 3, demonstrate that CaLMFlow consistently outperforms CFM and all other methods across all metrics. Furthermore, as illustrated in Figure 6, CaLMFlow generates cells with distributions more closely aligned to the ground truth data compared to other methods. These findings underscore CaLMFlow's superior performance in capturing and reproducing the complex high-dimensional distributions inherent in single-cell expression data."}, {"title": "5.2.2 SINGLE-CELL PERTURBATION RESPONSE PREDICTION", "content": "We leverage CLMs' inherent capabilities to encode and comprehend natural language by representing perturbation conditions as simple text prompts (see A.1.3 for details). These prompts are prepended to the embedded flow-matching conditional trajectories and processed through the CLM's tokenizer and embedding layers. For details on conditional encodings for other models, see A.1.3.\nOur architecture is based on a customized Pythia (Biderman et al., 2023) model as the CLM. To investigate the benefit of natural language capabilities of CLMs, we compare random initialization to natural language pretraining.\nThe results shown in Table 4 highlight CaLMFlow's ability to generate data distributions that closely align with the ground truth, outperforming competing models. The correlation statistics shown in Table 5 underscore CaLMFlow's effectiveness in producing realistic cell expression profiles that correspond to the specified combinatorial labels. Notably, both tables show that leveraging pretrained CLM weights enhances CaLMFlow's performance, showcasing the power of utilizing natural language understanding abilities of CLMs in the CaLMFlow framework.\nFurthermore, as shown in Figure 3, both variants of CaLMFlow generate data that closely overlaps with the ground truth distribution, demonstrating CaLMFlow's superior ability to model data under unseen conditions. Figure 7 illustrates the distributions produced by each model, showing that CaLMFlow's generated data most accurately reflects the ground truth. In contrast, other models are either unable to differentiate between combinatorial labels or generate unrealistic distributions. These visualizations reinforce the high quality of data generated by CaLMFlow, emphasizing its capability to model complex distributions and effectively utilize natural language prompts."}, {"title": "6 ABLATION EXPERIMENTS", "content": ""}, {"title": "6.1 TEMPERATURE", "content": "To investigate the impact of the temperature parameter on CaLMFlow's performance at inference, we varied the temperature for the 8-Gaussians to 2-Moons dataset. Figure 4 shows the best MMD and 2-Wasserstein values at \u0442 = 0.2, where the generated data closely matches the ground truth. Deviations from this value lead to less accurate transformations. Interestingly, it has been empirically found that the optimal temperature in LLMs is often below 1.0 to mitigate inference noise, an observation that aligns with our findings. The experiment also highlights the importance of the VAE component, as its removal significantly degrades performance."}, {"title": "6.2 NUMBER OF TIME POINTS", "content": "To evaluate the impact of the number of time points on CaLMFlow's performance, we varied the number of time points during training and inference for the 8 Gaussians to 2 Moons dataset. Figure 8 shows that as the number of time points increases, both the MMD and 2-Wasserstein consistently decrease, indicating improved model accuracy. This demonstrates that increasing the number of time points improves CaLMFlow's ability to capture the transformation dynamics, leading to better performance."}, {"title": "6.3 NUMBER OF SPATIOTEMPORAL TOKENS AND TRAJECTORIES", "content": "To test the impact of spatiotemporal tokenization, we use a similar setup as in Tong et al. (2024) on the MNIST dataset (details in Subsection 4.1). All key hyperparameters, optimizers, and training configurations were kept identical to ensure consistency. Our results, as shown in Table 6, demonstrate that CaLMFlow outperforms other methods and increasing the number of spatial tokens improves inception scores."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "We introduce CaLMFlow, a novel framework for flow matching that leverages causal language models by casting flow matching as a Volterra integral equation. CaLMFlow outperforms traditional flow matching models like CFM, especially on high-dimensional datasets, such as single-cell generation and perturbation response prediction. It generates more accurate and realistic data in both synthetic and real-world tasks. Future work will formalize CaLMFlow's multi-trajectory approach using integral equations over function spaces and explore its potential as an iterative solver to refine entire trajectory outputs, enhancing its ability to model systems with complex global dynamics."}, {"title": "B ADDITIONAL RESULTS", "content": ""}, {"title": "CBANACH SPACES, VOLTERRA INTEGRAL EQUATIONS AND CLMS", "content": "We adopt the standard next-token prediction paradigm used in CLMs. Given tokens xo to xk, the model predicts xk+1, where the sequence (x0,...,xk) corresponds to portions of the conditional flow matching trajectory. In this section, we give the implementation details of the theoretical discussion given in Section 3.2.\nOur training procedure enables the causal language model (CLM) to learn system dynamics by modeling sequences of varying lengths. During training, the model predicts the next state in the trajectory given previous states, similar to next-token prediction in language models, but with tokens representing continuous trajectory states."}, {"title": "D VOLTERRA FLOW MATCHING OBJECTIVE", "content": "The goal of this section is to connect the conditional Volterra flow matching object to the CFM objective. We recall some notation from Lipman et al. (2022). Let t \u2208 [0, 1], ut(x) the marginal time dependent vector field associated with the flow 4, ut (x|x1) the conditional time dependent vector field, vt (x) the model learning the vector field ut, q the data distribution, and pt the probability density path. Then we can define the flow matching objective and its conditional variant, LFM(0) = Et,pt(x) ||Vvt(x) \u2013 ut(x)||2 and LCFM(0) = Et,q(x1),Pt(x|x1) || Vt (X) - Ut (X|X1)||2. The key observation is that the gradients of these two objective functions are equivalent:"}]}