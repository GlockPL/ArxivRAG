{"title": "A Black Swan Hypothesis in Markov Decision Process via\nIrrationality", "authors": ["Hyunin Lee", "David Abel", "Ming Jin", "Javad Lavaei", "Somayeh Sojoudi"], "abstract": "Black swan events are statistically rare occurrences that carry extremely high risks.\nA typical view of defining black swan events is heavily assumed to originate from an\nunpredictable time-varying environments; however, the community lacks a comprehensive\ndefinition of black swan events. To this end, this paper challenges that the standard view is\nincomplete and claims that high-risk, statistically rare events can also occur in unchanging\nenvironments due to human misperception of their value and likelihood, which we call as\nspatial black swan event. We first carefully categorize black swan events, focusing on spatial\nblack swan events, and mathematically formalize the definition of black swan events. We\nhope these definitions can pave the way for the development of algorithms to prevent such\nevents by rationally correcting human perception.\nKeywords: Risk, Reinforcement learning, Irrationality, Cumulative prospect theorem.", "sections": [{"title": "Introduction", "content": "Life is the cumulative effect of a handful of significant shocks.\nThe Black Swan: The Impact of the Highly Improbable, Nassim Nicholas Taleb\nThe reason behind the Lehman Brothers bankruptcy, an unexpected event with ex-\ntremely negative impacts on the world economy, remains controversial. However, a\nstrong explanation points to the irrationality of human decision-making. The firm declared\nbankruptcy within 72 hours without any precursor (McDonald and Robinson (2009)), and\nthe only factor that changed during those three days was investors' faith in the company\n(Housel (2023); Mawutor (2014); Fleming and Sarkar (2014)). Faith was intrinsically\nbelieved by investors as an axiom. They made optimal decisions (being rational) based on\nthis faith, which turned out to be suboptimal (being irrational) once the faith was revealed\nto be false during those 72 hours. Referring to the unexpected bankruptcy event men-\ntioned previously, we call such a rare and high-risk event, often rationalized retrospectively\nwith the benefit of hindsight, a black swan (Taleb (2010)).\nBlack swan events remain one of the unsolved problems in machine learning safety\n(Hendrycks et al. (2021)), and this work focuses on the origin of black swan events from\nthe perspective of human perception, our main messages being Hypothesis 2 and Definition\n7, rather than on how to design robust algorithms against them. We expect that providing\na novel perspective to understand black swan events can offer new insights for designing\nsafe machine learning algorithms. Supported by extensive documentation of black swan\nevents, such as the dissolution of the Soviet Union, the terrorist attacks of September\n11, 2001, and the Brexit vote (Taleb (2010)), we focus on specific types of black swans\nthat occur even in a stationary environment. We refer to these as spatial black swans.\nBased on the above example we deduce that certain types of black swan occur due to\nmisperception in the way humans perceive the world (Hypothesis 2). Executing an optimal\npolicy based on misperception inevitably causes the agent to encounter unavoidable risks,\nwhich we define as spatial black swans (Definition 7) From a broad perspective, our\nwork proposes the following informal hypothesis regarding the origin of black swan events\nand provides an informal definition of spatial black swans built upon this hypothesis as\nfollows."}, {"title": "Spatial and temporal blackswans", "content": "Based on Hypothesis 1, this prompts us to investigate the concept of misperception. Ini-\ntially, we must clearly define what constitutes perception. According to the definition of\nan agent by Barandiaran et al. (2009) and Orseau and Ring (2012), an agent views its\nenvironment through the lens of so-called spatio-temporal dimensions. Consequently, if a\nblack swan event arises from misperception, the conceptual framework of perception leads\nus to question whether the misperception originates from spatial or temporal dimensions.\nThis first leads us to define the black swan event dimension as follows.\nIn Definition 2, unit time refers to any variable that represents its time heterogeneity\nsuch as discrete step or discrete episode (Lee et al. (2024) or discrete real-time (Abel et al.\n(2024); Dong et al. (2022)) in general Markov Decision Process setting.\nAssumption 1 ensures that if the environment and the agent's perception of the environ-\nment are fixed at time t, then the black swan events are fully determined. This assumption\nis also used to classify black swans in Proposition 1, Example 1, and Remark 1.\nThe definition 2 leads us to first classify whether the black swan comes from a mis-\nconception within the space (s,a), termed spatial mispercpetion, or a misperception along\nthe unit time (t), termed temporal misperception. We define temporal misperception as\nthe inherent inaccuracies in time-series data prediction algorithms from agent, exempli-\nfied by black swans such as COVID-19 or earthquakes, and define temporal black swans\nas those originating from temporal misperception. This type of misperception inevitably\nresults from the non-stationarity of the environment, which impacts the algorithms' abil-\nity to predict future events accurately. In contrast, we define spatial misperception as a\nmisperception that occurs in a stationary environment, often due to incorrect assessments\nor misunderstandings of the spatial aspects of the data. We define spatial black swans as\nthose originating from spatial misperception, such as the Russia-Ukraine war, the Lehman\nBrothers bankruptcy, the Brexit vote or the September 11, 2001 terrorist attacks. Intu-\nitively, temporal black swans occur due to the non-stationarity of the environment, while\nspatial black swans occur even in stationary environments. Based on the above description,\nwe provide a definition of spatial and temporal black swans as follows."}, {"title": "Necessity of a new perspective to understand black swans and\nevidence for Hypothesis 1", "content": "In this section, we focus not only on addressing the necessity of a new perspective to un-\nderstand black swan events but also on providing evidence for the proposed perspective\nof black swan origin (Hypothesis 1). This is concretized by examining the following two\nquestions. First, in Subsection 3.1, we discuss the insufficiency of existing decision-making\nrules under risk by exploring related works, which support the need for a new perspective\nto understand black swans. Specifically, we address why existing safe reinforcement learn-\ning strategies for solving Markov Decision Processes are insufficient to handle black swan\nevents?. If this premise is validated, then in Subsection 3.2, we elaborate on the motivation\nand related works that support our informal hypothesis of black swan origin (Hypothesis\n1). Specifically, we explore how irrationality relates to misperception and how irrationality\ncould bring about black swan events."}, {"title": "Decision Making Under Risk", "content": "Based on the comprehensive survey on safe reinforcement learning in Garcia and Fern\u00e1ndez\n(2015), the algorithms can be classified into threefold: worst case criterion, risk-sensitive\ncriterion and constraint criterion. We elaborate on why the existence of black swans in the\nenvironment renders these three approaches insufficient.\nWorst case criterion. Learning algorithms of the worst case criterion focus on devis-\ning a control policy that maximizes policy performance under the least favorable scenario\nencountered during the learning process, defined as $\\max_{\\pi \\in \\Pi} \\min_{w \\in W} J(\\pi;w)$, where $W$ rep-\nresents the set of uncertainties. This criterion can be categorized based on whether $W$\nis defined in the environment or in the estimation of the model. The presence of black\nswan events in the worst case, where $W$ represents aleatoric uncertainty of the environ-\nment (Heger (1994); Coraluppi (1997); Coraluppi and Marcus (1999, 2000)), results in"}, {"title": "How irrationality relates with spatial black swans.", "content": "Before starting Subsection 3.2, we clarify that the term irrationality is used here to denote\nrational behavior based on a false belief. In this subsection, we first review existing work\non the four rational axioms and then claim how two of these axioms should be modified to\naccount for irrationality in human decision-making.\nRationality in decision making. In the foundation of decision theory, rationality\nis understood as internal consistency (Sugden (1991); Savage (1972)). A prerequisite for\nachieving rationality in decision making is the ability to compare outcomes, denoted as set\n$O$, through a preference relation in a rational manner. In Neummann and Morgenstern\n(1944), it is demonstrated that preferences, combined with rationality axioms and proba-\nbilities for possible outcomes, denoted as $p_i$ which is a probability of outcome $o_i \\in O$, imply\nthe existence of utility values for those outcomes that express a preference relation as the\nexpectation of a scalar-valued function of outcomes. Define the choice (or lotteries) as set\n$C$, which is a combination of selecting total $n$ outcomes, that is, $\\sum_{i=1}^n p_i o_i$. The essential\nrationality axioms are as follows."}, {"title": "Agent-Environment intersects as perception", "content": "Thus far, we have informally introduced the black swan hypothesis (Hypothesis 1) and\nspatial black swan definition (Definition 1) in Section 1 and elaborated on its necessity\n(Subsection 3.1) and supporting evidence (Subsection 3.2) in Section 3. Then, we introduce\none existing work to concretize misperception, CPT, in Section 4. In Section 5, we utilize\nCPT to elaborate the Hypothesis 1 by introducing universe, human, and human-estimation\nMDP."}, {"title": "Misperception is information loss", "content": "Based on Hypothesis 1, this prompts us to investigate the concept of misperception. Ini-\ntially, we must clearly define what constitutes perception. In The Quest for a Common\nModel of the Intelligent Decision Maker, Sutton defines perception as one of four princi-\npal components of agents, stating: \"The perception component processes the stream of\nobservations and actions to produce the subjective state, a summary of the agent-world\ninteraction so far that is useful for selecting action (the reactive policy), for predicting fu-\nture reward (the value function), and for predicting future subjective states (the transition\nmodel)\" Sutton (2022). This definition leads us to consider misperception as the informa\u0430-\ntion loss occurring when processing observations into the subjective state, such that the\nreward and transition model are not equivalent to those from the environment. The inter-\npretation of misperception as information loss during processing is somewhat ambiguous,\ndepending on how the boundary between the agent and the environment is defined. The\nconcept of a boundary between the agent and environment was first proposed by Turing as\na 'skin of an onion' Turing (2009), and later, Jiang (2019) suggested that algorithms are\nnot boundary-invariant.\nTherefore, we propose a new agent-environment framework that incorporates the notion\nthat misperception is the information loss from an agent's processing. This framework\npositions perception at the intersection between the agent and the environment. We provide\na detailed description of our agent-environment framework in Figure 2 and further elaborate\nin the following subsection."}, {"title": "Universe, Human, and Human-Estimation MDP", "content": "Restricting the Markov Decision Process introduced in Section 3 in a stationary envi-\nronment, we consider a single episode finite horizon stationary Markov Decision Process\n(MDP) denoted as $M = (S, A, P, R, \\gamma,T)$, where $S$ represents the state space, $A$ denotes\nthe action space, $P: S \\times A \\rightarrow \\Delta(S)$ is the transition probability function, $R : S \\times A \\rightarrow R$ is\na reward function, $\\gamma$ is the discount factor, and $T \\geq 0$ is a Horizon. We define $M$ as the\nuniverse MDP and operate under the assumption that the abstraction from the universe\nto the model $M$ is lossless, preserving all relevant information. Given a policy $\\pi$, the agent\ncollects data $\\{s_0, a_0, r_0, s_1,...\\}$ as it interacts with the environment at discrete time steps $t$.\nThe process starts from a fixed initial state $s_0$, and we define the value function as follows:\n$V(s_0) := E[\\sum_{t=0}^{T-1} \\gamma^t R(s_t,a_t) | \\pi, P]$\nTo fully leverage Hypothesis 1, and 'perception as information loss from agent's process-\ning' from subsection 5.1, we define the human MDP $M^{\\dagger} = (S^{\\dagger}, A^{\\dagger}, P^{\\dagger}, R^{\\dagger}, \\gamma)$ where the\nagent experiences distortions in cumulative distribution of normalized visitation probabil-\nity $P^{\\pi}(s, a) := \\frac{1}{1-\\gamma} \\sum_{t=0}^{T-1} \\gamma^t P((s_t, a_t) = (s, a)|s_0, \\pi, P)$ and reward function $R$ by functions"}, {"title": "Case study: how optimal decision deviates under irrationality", "content": "So far, we have informally introduced the black swan hypothesis (Hypothesis 1) and spatial\nblack swans (Definition 1) in Section 1, then elaborated on its necessity (Subsection 3.1) and\nprovided evidence (Subsection 3.2) in Section 3. Subsequently, in Section 4, we introduced\nthe Cumulative Prospect Theorem (CPT) to model human irrationality. In Section 5,\nwe incorporated CPT into the Markov Decision Process (Subsection 5.2), viewed through\nthe lens of existing work on defining perception (Subsection 5.1) by introducing universe,\nhuman, and human-estimation MDPs.\nGiven our main hypothesis that black swan events occur due to human misperception\nof the real world, we further investigate whether optimal policy also deviates due to mis-\nperceptions of value or probability. This is critical as overestimating or underestimating all\nvalues of probability does not necessarily lead to changes in optimal policy. For example,\nrevisiting Example 4, overestimating or underestimating $\\{r(s_{base}),r(s_{premium}),r(s_{risk})\\}$\nto the same magnitude does not alter the optimal policy for a human to choose $a_p$. More\nfundamentally, a crucial question that this paper addresses is how misperception influ-\nences the deviation of optimal policy. Therefore, in Section 6, we first present some case\nstudies in MDPs with small complexity to demonstrate how optimal policy varies under\nmisperception.\nBefore proceeding, we need to establish the core event of subjective probability. For\nexample, while agents (humans) might distort transition probabilities and perceive them\nsubjectively at a low level which is an intuitive way to model misperception. It is essential\nto recall that in the Markov Decision Process, the quality of an event is revealed through\nrewards defined over specific states and actions. This suggests that it is more reasonable\nto define the minimal object (event) as the state and action, and conduct modeling as\ndistortion on the visitation probability of state and action rather than on the transition\nprobability itself. Since this approach has not been investigated in existing work, we\nexamine both cases in Section 6 and Section 7. Specifically, in Section 6, we assume\ndistortion of transition probability within a non-stationary Markov decision process and\ninvestigate how optimal policy deviates due to misperception of transition probability.\nThen, in Section 7, we explore the distortion of the visitation probability within a stationary\nMarkov decision process."}, {"title": "Problem setup for Section 6", "content": "In this section, we consider discrete state and action stationary Markov Decision Process.\nBuild upon value function (Equation 4), we define value function and state value function"}, {"title": "Agent-Environment intersects as perception", "content": "of time $t$ as\n$V_t^{\\pi}(s) := E[\\sum_{t'=t}^{T-1} \\gamma^{T-1-t'}u(R(s_{t'}, a_{t'})) | P^{\\dagger},\\pi, S_t = s]$\n$Q_t^{\\pi}(s,a) := E[\\sum_{t'=t}^{T-1} \\gamma^{T-1-t'}u(R(s_{t'}, a_{t'})) | P^{\\dagger},\\pi, S_t = s, a_t = a]$\nand define the optimal policy as time $t$ as $\\pi_t^{\\dagger} = arg max_a V_t^{\\dagger}$. Then the following bellman\nequation holds,\n$V_t^{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)Q_t^{\\pi}(s,a)$\n$Q_t^{\\dagger}(s, a) = u(R(s, a)) + \\sum_{s_{[i]} \\in S} \\gamma w(\\sum_{j=1}^{i} P(s_{[i]}|s, a)) - w(\\sum_{j=1}^{i-1} P(s_{[i]}|s, a)) V_{t+1}^{\\dagger}(s_{[i]})$\nwhere $P(s_{[1]}|s, a) \\leq P(s_{[2]}|s,a) \\leq ... \\leq P(s_{[\\vert S \\vert ]}|s, a)$ holds. In addition, we assume that\n$\\forall t \\in [T - 1], \\forall s\\in S$ and $\\forall a\\in A$, $R(s_t,a_t) = 0$. We only consider the reward function at\nthe final stage $R(s_T, a_T)$. Although this assumption may appear unconventional, it aligns\nwith standard practices in reinforcement learning, especially when focusing on terminal\nrewards. Especially, for each trajectory $\\tau \\in S^\\prime$, where $\\tau = (s_0,a_0,...,s_H,a_H)$ represents a\nT-step decision sequence, set the terminal reward as $r_f(\\tau) := \\sum_{t\\in [T]} r_t (s_t, a_t)$, and $r_f(t) = 0$\nfor all $t \\in [T - 1]$"}, {"title": "Case 1. Contextual bandit (T = 1)", "content": "We begin with a simple case where the decision horizon is $T = 1$, commonly referred to\nas a contextual bandit (Lattimore and Szepesv\u00e1ri (2020)). Surprisingly, in this setting,\nthe human optimal policy coincides with the real-world optimal policy. This is somewhat\ncounterintuitive, as several significant examples (Examples 2,3, and 4) suggest that human\ndecision-making often exhibits irrationality."}, {"title": "Case 2. |S| = 2 when T>1", "content": "Now, let us consider the simplest case where $H > 1$ where $\\vert S \\vert = 2$. Surprisingly, we find\nresults similar to those presented in Section\nThe proof of Theorem 2 is based mainly on the assumption that $\\vert S \\vert = 2$, notably\nusing the property that $w(1) = 1$. However, this technique cannot be applied directly\nif $\\vert S \\vert \\geq 3$. For example, in Example 4, where $\\vert S \\vert = 3$, it is demonstrated that human\ndecision making results in a suboptimal policy in the real-world. This outcome may seem\ncounterintuitive. However, a heuristic analysis suggests that having only two states in the\nstate space indicates that the actions do not introduce a varied randomness. Essentially, if\nrandomness is introduced by any action, it would likely affect both states $s_0$ and $s_1$ if the\naction provides a nondeterministic next state. Thus, every action provides the same next-\nstate set, which makes a mere comparison between two states. This observation implies\nthat in scenarios with a smaller state space, being irrational (believing on false belief)\ndoes not affect to deviate from optimal policy. We can also interpret this result as if $\\vert S \\vert$\nis small, sequential decision-making problems are easy so that humans can always provide\nthe optimal action in the real world."}, {"title": "Case 3: |S| = 3 with unbiased reward perception", "content": "We consider the hypothetical scenario where $u(r) = r$, indicating that humans have an\nunbiased perception of their reward.\nNow, we introduce the definition of black swan inspired by CPT."}, {"title": "Black swan hypothesis", "content": "First, based on the newly proposed human model $M^{\\dagger}$, we concretize the informal hypothesis\n1 as follows."}, {"title": "A definition of spatial black swans", "content": "As a preliminary step, we define spatial black swan events within a discrete state and action\nspace. Note that we continue the problem setting that is discussed in deriving Equation\n(7) in Subsection 5.2."}]}