{"title": "A Black Swan Hypothesis in Markov Decision Process via Irrationality", "authors": ["Hyunin Lee", "David Abel", "Ming Jin", "Javad Lavaei", "Somayeh Sojoudi"], "abstract": "Black swan events are statistically rare occurrences that carry extremely high risks. A typical view of defining black swan events is heavily assumed to originate from an unpredictable time-varying environments; however, the community lacks a comprehensive definition of black swan events. To this end, this paper challenges that the standard view is incomplete and claims that high-risk, statistically rare events can also occur in unchanging environments due to human misperception of their value and likelihood, which we call as spatial black swan event. We first carefully categorize black swan events, focusing on spatial black swan events, and mathematically formalize the definition of black swan events. We hope these definitions can pave the way for the development of algorithms to prevent such events by rationally correcting human perception.", "sections": [{"title": "1 Introduction", "content": "Life is the cumulative effect of a handful of significant shocks.\nThe Black Swan: The Impact of the Highly Improbable, Nassim Nicholas Taleb\nThe reason behind the Lehman Brothers bankruptcy, an unexpected event with ex-tremely negative impacts on the world economy, remains controversial. However, a strong explanation points to the irrationality of human decision-making. The firm declared bankruptcy within 72 hours without any precursor (McDonald and Robinson (2009)), and the only factor that changed during those three days was investors' faith in the company (Housel (2023); Mawutor (2014); Fleming and Sarkar (2014)). Faith was intrinsically believed by investors as an axiom. They made optimal decisions (being rational) based on this faith, which turned out to be suboptimal (being irrational) once the faith was revealed to be false during those 72 hours. Referring to the unexpected bankruptcy event men-tioned previously, we call such a rare and high-risk event, often rationalized retrospectively with the benefit of hindsight, a black swan (Taleb (2010)).\nBlack swan events remain one of the unsolved problems in machine learning safety (Hendrycks et al. (2021)), and this work focuses on the origin of black swan events from the perspective of human perception, our main messages being Hypothesis 2 and Definition 7, rather than on how to design robust algorithms against them. We expect that providing a novel perspective to understand black swan events can offer new insights for designing safe machine learning algorithms. Supported by extensive documentation of black swan events, such as the dissolution of the Soviet Union, the terrorist attacks of September 11, 2001, and the Brexit vote (Taleb (2010)), we focus on specific types of black swans that occur even in a stationary environment. We refer to these as spatial black swans. Based on the above example we deduce that certain types of black swan occur due to misperception in the way humans perceive the world (Hypothesis 2). Executing an optimal policy based on misperception inevitably causes the agent to encounter unavoidable risks, which we define as spatial black swans (Definition 7) From a broad perspective, our work proposes the following informal hypothesis regarding the origin of black swan events and provides an informal definition of spatial black swans built upon this hypothesis as follows."}, {"title": "Hypothesis 1 (Black swan origin (informal)).", "content": "Black swan events occur due to temporal and spatial misperception, and spatial black swans occur due to human misperception of the real world."}, {"title": "Definition 1 (Spatial black swan (informal)).", "content": "A spatial black swan event is a state and action pair that humans perceive as an infeasible event and assess its reward in a pessimistic manner."}, {"title": "Paper structure.", "content": "Starting from the informal hypothesis and definition of spatial black swans as mentioned above (Hypothesis 1, Definition 1), we structure the paper to provide concrete foundations for our main message: Hypothesis 2, and Definition 7 as follows. First, in Section 2, we define spatial and temporal black swans, then focus on spatial black swans for the rest of the paper. Section 3 provides evidence for the soundness of Hypothesis 1. Specifically, in Subsection 3.1, we emphasize the necessity of a novel per-spective to understand black swans by demonstrating that the existing decision-making frameworks under risk are insufficient, and in Subsection 3.2, we explain how misperception can be related to black swans. Section 4 introduces Cumulative Prospect Theorem (CPT), a well-known theorem for explaining irrational human behavior in the real world, to mathematically formalize the term misperception in Hypothesis 1 and Definition 1. In Section 5, we introduce three types of Markov Decision Processes: Universe MDP, Human MDP, and Human-Estimation MDP, which we denote as M, M\u2020, and M\u2020, respectively. Before presenting the concrete definition and hypothesis of spatial black swans, we provide case studies in Section 6 to illustrate how human misperception can lead to suboptimal policy (Theorems 1, 2, and 3). Subsequently, in Section 7, we present our main message, proposing the spatial black swan hypothesis (Hypothesis 2) and a definition of spatial black swan events (Definition 7) utilizing M, M\u2020, M\u2020, and CPT. It is worth noting that Defi-nition 7 employs notations from M, M\u2020, and M\u2020 and is mathematically characterized by concepts from CPT. Finally, in Section 8, we explore the properties of spatial black swan events, particularly how their presence establishes a lower bound on achieving true policy performance (Theorem 4) and affects the timing of black swan event occurrences (Theorem 5).\nNotations. The sets of natural, real, nonnegative, and nonpositive real numbers are denoted by N, R, R\u22650, and R\u22640 respectively. For a finite set Z, the notation |Z| represents its cardinality, and \u2206(Z) denotes the probability simplex on Z. Given X, Y \u2208 N with X < Y, we define [X] := {1, 2, ..., X}, the closed interval [X, Y] := {X, X + 1,...,Y}. For x \u2208 R+, the floor function \u230ax\u230b is defined as max{n \u2208 N \u222a {0} | n \u2264 x}.\nMarkov Decision Process. We consider a finite horizon non-stationary Markov Decision Process (MDP) denoted as M = (S, A, {Pt}T\u22121t=0, {Rt}T\u22121t=0, \u03b3), where S represents the state space, A denotes the action space, Pt : S \u00d7 A \u2192 \u2206(S) is the transition probability function at time t, Rt : S \u00d7 A \u2192 R is a reward function as time t, \u03b3 is the discount factor, and T \u2208 N is the horizon length. We denote a policy as \u03c0 \u2208 \u03a0, where \u03a0 : S \u2192 \u2206(A) is a set of policies, and its performance as J(\u03c0) = E\u03c0,M[G] where G = \u2211T\u22121t=0 \u03b3t Rt(st, at) is a scalar return. We denote T length trajectory from M with policy \u03c0 as {s0, a0, r0, s1, a1, r1,..., sT}."}, {"title": "2 Spatial and temporal blackswans", "content": "Based on Hypothesis 1, this prompts us to investigate the concept of misperception. Ini-tially, we must clearly define what constitutes perception. According to the definition of an agent by Barandiaran et al. (2009) and Orseau and Ring (2012), an agent views its environment through the lens of so-called spatio-temporal dimensions. Consequently, if a black swan event arises from misperception, the conceptual framework of perception leads us to question whether the misperception originates from spatial or temporal dimensions. This first leads us to define the black swan event dimension as follows."}, {"title": "Definition 2 (Black swan dimension).", "content": "In a Markov Decision Process, a dimension of a black swan event is defined on S \u00d7 A \u00d7 [T] with S representing the state space and A representing the action space and T is time (horizon length). We collect black swan events as a set B (B will be more elaborated on Hypothesis 2)."}, {"title": "Assumption 1.", "content": "For fixed time t' \u2208 [T], black swan events at time t', i.e., the set {(s, a)|(s, a, t = t') \u2208 B} is invariant set under static agent's perception of MDP at time t'."}, {"title": "Definition 3 (Spatial black swans).", "content": "For a given MDP M for a time interval [T], suppose (s, a, tbs) is a black swan where tbs \u2208 [T]. If (s, a, t) is a black swan event for \u2200t \u2208 [T], then we define (s, a, tbs) as a spatial black swan."}, {"title": "Definition 4 (Temporal black swans).", "content": "For a given MDP M for a time interval [T], suppose (s, a, tbs) is a black swan where tbs \u2208 [T]. If there exists t \u2208 [T] such that (s, a, t) is not a black swan event, then we define (s, a, tbs) as a temporal black swan."}, {"title": "Proposition 1.", "content": "If (s, a, tbs) is a black swan event, then there exists a time interval [t1, t2] \u2286 [T] that classifies (s, a, tbs) as a spatial black swan within [t1, t2]."}, {"title": "Example 1.", "content": "Suppose (s, a, tbs) is a black swan event.\ncase 1. Mis a non-stationary MDP where Pt, Rt changes for every unit time, i.e. Pt \u2260 Pt+1, Rt \u2260 Rt+1\u2022 If t1 = t2 = tbs, then (s, a, tbs) is a spatial black swan. If t1 \u2260 t2 where tbs \u2208 [t1, t2], then (s, a, tbs) could not be identified as spatial black swan or temporal black swan.\ncase 2. Mis a piecewise non-stationary MDP where Pt, Rt changes for [T/k] times, i.e. Pt = Pt+1, Rt = Rt+1 for t \u2208 [kj, kj + (k \u2212 1)], j = 0, 1,..., \u230aT/k\u230b. If tbs \u2208 [kjbs, kjbs + (k \u2212 1)], then (s, a, tbs) is a spatial black swans when t1 = kjbs, t2 = kjbs + (k \u2212 1).\ncase 3. Mis stationary MDP where Pt = Pt+1, Rt = Rt+1 for \u2200t \u2208 [T \u2212 1], then (s, a, tbs) is a spatial black swan regardless of interval [t1, t2]."}, {"title": "Remark 1.", "content": "If M is stationary, then any black swan events (s, a, t) are spatial black swans, denoted as (s, a)."}, {"title": "Proposition 2 (Spatial black swan (informal)).", "content": "For given MDP M = (S, A, P, R, \u03b3). We define an event \u03c3 as a state and action (s, a) \u2208 S \u00d7 A. If \u03c3 is a spatial black swan event, then it satisfies that"}, {"title": "3 Necessity of a new perspective to understand black swans and evidence for Hypothesis 1", "content": "In this section, we focus not only on addressing the necessity of a new perspective to un-derstand black swan events but also on providing evidence for the proposed perspective of black swan origin (Hypothesis 1). This is concretized by examining the following two questions. First, in Subsection 3.1, we discuss the insufficiency of existing decision-making rules under risk by exploring related works, which support the need for a new perspective to understand black swans. Specifically, we address why existing safe reinforcement learn-ing strategies for solving Markov Decision Processes are insufficient to handle black swan events?. If this premise is validated, then in Subsection 3.2, we elaborate on the motivation and related works that support our informal hypothesis of black swan origin (Hypothesis 1). Specifically, we explore how irrationality relates to misperception and how irrationality could bring about black swan events."}, {"title": "3.1 Decision Making Under Risk", "content": "Based on the comprehensive survey on safe reinforcement learning in Garcia and Fern\u00e1ndez (2015), the algorithms can be classified into threefold: worst case criterion, risk-sensitive criterion and constraint criterion. We elaborate on why the existence of black swans in the environment renders these three approaches insufficient."}, {"title": "Worst case criterion.", "content": "Learning algorithms of the worst case criterion focus on devis-ing a control policy that maximizes policy performance under the least favorable scenario encountered during the learning process, defined as max\u03c0\u2208\u03a0 minw\u2208W J(\u03c0; w), where W rep-resents the set of uncertainties. This criterion can be categorized based on whether W is defined in the environment or in the estimation of the model. The presence of black swan events in the worst case, where W represents aleatoric uncertainty of the environ-ment (Heger (1994); Coraluppi (1997); Coraluppi and Marcus (1999, 2000)), results in overly conservative, and thus potentially ineffective, policies. This occurs because the sig-nificant impact of black swan events inflates the size of W, even though such events are rare. In practical terms, this could manifest itself as abstaining from any economic activ-ity (\u03c0), such as not investing in stocks or not depositing a check against future potential bankruptcies (minw\u2208W J(\u03c0; w)) in order to maximize its income (max\u03c0\u2208\u03a0(\u00b7)), or maintain-ing constant health precautions such as wearing mask or maintaining distance with groups (\u03c0) to prepare for a possible pandemic (\u00b7 = minw\u2208W J(\u03c0; w)) in order to maintain its health (max\u03c0\u2208\u03a0). Similarly, when W encompasses the uncertainty of the model parameter Bagnell et al. (2001); Iyengar (2005); Nilim and El Ghaoui (2005); Wiesemann et al. (2013); Xu and Mannor (2010) - as seen in robust MDP or distributionally robust MDP - this aligns closely with our black swan hypothesis, where misperception of the world model is similar to uncer-tainty in model estimation. However, the need to accommodate black swan events requires enlarging the possible set of models (|W|), leading to extremely conservative policies. This can be likened to performing an overly pessimistic portfolio optimization (\u03c0), where every bank is assumed to have a minimal but possible risk of bankruptcy (minw\u2208W J(\u03c0; w)), thus influencing asset allocation strategies (max\u03c0\u2208\u03a0 minw\u2208W J(\u03c0; w)) to be extremely con-servative in asset investing."}, {"title": "Risk sensitive criterion.", "content": "Risk-sensitive algorithms strike a balance between maxi-mizing reinforcement and mitigating risk events by incorporating a sensitivity factor \u03b2 < 0 (Howard and Matheson (1972); Chung and Sobel (1987); Patek (2001)). These algorithms optimize an alternative value function J(\u03c0) = \u03b2\u22121 log E\u03c0[exp \u03b2G], where \u03b2 controls the de-sired level of risk. However, it is recognized that associating risk with the variance of the return is practical, as in J(\u03c0) = \u03b2\u22121 log E\u03c0[exp \u03b2G] \u2248 max\u03c0E\u03c0[G] \u2212 \u03b2 var(G) + O(\u03b22), and the existence of black swan events does not significantly affect the returns of variance (var(G)) due to their rare nature. It should be noted that risk-sensitive approaches are not well suited for handling black swan events, as the same policy performance with small variance can entail substantial risks (Geibel and Wysotzki (2005)). More generally, the ob-jective of the exponential utility function is one example of risk-sensitive learning based on a trade-off between return and risk, i.e., max\u03c0\u2208\u03a0(E\u03c0[G] \u2212 \u03b2\u03c9) (Zhang et al. (2018)), where \u03c9 is replaced by Var(G). This approach is known in the literature as the variance-penalized cri-terion (Gosavi (2009)), the expected value-variance criterion (Taha (2007); Heger (1994)), and the expected-value-minus-variance criterion (Geibel and Wysotzki (2005)). However, a fundamental limitation of using return variance as a risk measure is that it does not ac-count for the fat tails of the distribution (Huisman et al. (1998); Bradley and Taqqu (2003); Bubeck et al. (2013); Agrawal et al. (2021)). Consequently, risk can be underestimated due to the oversight of low probability but highly severe events (black swans).\nFurthermore, a critical question arises regarding whether the log-exponential function belongs to appropriate utility function class for defining real-world risk. Risk-sensitive MDPs have been shown to be equivalent to robust MDPs that focus on maximizing the worst-case criterion, indicating that the log-exponential utility function may not be bene-ficial in the presence of black swans (Osogami (2012); Moldovan and Abbeel (2012); Leqi et al. (2019))."}, {"title": "Constrained Criterion.", "content": "The constrained criterion is applied in the literature to constrained Markov processes where the goal is to maximize the expected return while maintaining other types of expected utilities below certain thresholds. This can be formu-lated as max\u03c0\u2208\u03a0E\u03c0[G] subject to N multiple constraints hi(G) \u2264 ai, for i \u2208 [N], where hi : R \u2192 R is a function of return G (Geibel (2006)). Typical constraints include ensuring the expectation of return exceeds a specific minimum threshold (a), such as E[G] \u2265 a, or softening these hard constraints by allowing a permissible probability of violation (\u03f5), such as P(E[G] \u2265 a) \u2265 1 \u2212 \u03f5, known as chance-constraint (Delage and Mannor (2010); Ponda et al. (2013)). Constraints might also limit the return variance, such as Var(G) \u2264 a (Di Cas-tro et al. (2012)). However, the presence of black swans highlights one of the challenges with the Constrained Criterion, specifically the appropriate selection of a. The presence of black swans necessitates a lower a, which in turn leads to more conservative policies. Furthermore, a black swan event is determined at least by the environment's state and its action, rather than its full return. Therefore, constraints should be redefined over more fine-grained inputs\u2014not merely returns, but in terms of state and action\u2014which leads to our definition of black swan dimensions (Definition 2)."}, {"title": "3.2 How irrationality relates with spatial black swans.", "content": "Before starting Subsection 3.2, we clarify that the term irrationality is used here to denote rational behavior based on a false belief. In this subsection, we first review existing work on the four rational axioms and then claim how two of these axioms should be modified to account for irrationality in human decision-making."}, {"title": "Rationality in decision making.", "content": "In the foundation of decision theory, rationality is understood as internal consistency (Sugden (1991); Savage (1972)). A prerequisite for achieving rationality in decision making is the ability to compare outcomes, denoted as set O, through a preference relation in a rational manner. In Neummann and Morgenstern (1944), it is demonstrated that preferences, combined with rationality axioms and proba-bilities for possible outcomes, denoted as pi which is a probability of outcome oi \u2208 O, imply the existence of utility values for those outcomes that express a preference relation as the expectation of a scalar-valued function of outcomes. Define the choice (or lotteries) as set C, which is a combination of selecting total n outcomes, that is, \u2211ni=1 pioi. The essential rationality axioms are as follows."}, {"title": "4 Cumulative prospect theorem", "content": "In this section, we provide a preliminary overview of Cumulative Prospect Theory (CPT), which offers a framework for understanding human decision making under risk and uncer-tainty. We utilize the principles of CPT to elaborate the concept of misperception and incorporate it into the MDP (Section 5) and further to define spatial black swan events (Section 7).\nFor a random variable X, let pi where i = 1,..., K denote the indices for the prob-ability of incurring a gain or loss xi for each i = 1,..., K. Given a utility function u and a weighting function w, the Prospect Theory (PT) value is defined as V(X) = \u2211Ki=1 u(xi)w(pi), and the Cumulative Prospect Theory (CPT) value is defined as V(X) = \u2211Ki=1 u(xi) (w(\u2211Ki=1 pj) \u2212 w(\u2211i\u22121j=1 pj)). Contrary to expected utility theory, which models decisions that perfectly rational agents would make (Rabin (2013)), i.e. V(x) = \u2211Ki=1 xipi,\nPT seeks to describe the actual behavior of humans, attempting to encompass their ir-rational decision-making processes. Specifically, PT introduces the concept of probability distortion, where individuals overestimate the likelihood of rare events and underestimate the likelihood of moderate to highly probable events (Figure 1b). Value distortion refers to the way individuals assess gains and losses (x-axis of Figure 1a), often valuing losses more heavily than equivalent gains, which reflects a behavior known as loss aversion (Figure 1a) Kahneman and Tversky (2013); Fennema and Wakker (1997). How PT explains human decision-making is well-described in the following example.\nExample 4 (Insurance policies). Consider an example where the probability of an insured risk is 1%, the potential loss is 1,000, and the insurance premium is 15. According to CPT, most would opt to pay the 15 premium to avoid the larger loss.\nExample 4 illustrates how a seemingly straightforward decision can be analyzed as a sequential decision-making problem within a two-step Markov Decision Process framework, where S = {sbase, spremium, srisk} and A = {ap, anp}. The states sbase, spremium, and srisk represent receiving a loss of 0, -15, and \u22121000, respectively. The actions ap and anp denote paying and not paying the premium, respectively. At time t = 0, humans make a choice between ap and anp based on a policy \u03c0 : S \u2192 \u2206(A) stating at initial state s0 = sbase. Choosing ap results in a guaranteed transition to state s1 = spreimium. Otherwise, choosing anp potentially leads to state sbase with a reward of 0 with a 0.99 probability, or to state srisk with a reward of -1000 with a 0.01 probability. According to the expected utility theorem, which assumes rationality, the estimated value of choosing ap is calculated as V(sbase) = 1 \u00d7 r(spremium) = 1 \u00d7 (\u221215) = \u221215, and the value of choosing anp as V(sbase) = 0.99 \u00d7 r(sbase) + 0.01 \u00d7 r(srisk) = 0.99 \u00d7 0 + 0.01 \u00d7 (\u22121000) = \u221210. Rationality would lead humans to prefer anp since its expected cost is lower than that of ap, i.e. anp = arg maxa\u2208A V(sbase). However, this choice is counterintuitive and often does not align with real-world human decision making.\nCPT uses a similar measure as PT, except that the w is a function of cumulative probabilities. The concept involves using an S-shaped utility function, which adheres to the diminishing sensitivity property. If we set the weighting function w or utility function u to be the identity function, then we retrieve the classical expected utility mode (Rabin (2013))."}, {"title": "5 Agent-Environment intersects as perception", "content": "Thus far, we have informally introduced the black swan hypothesis (Hypothesis 1) and spatial black swan definition (Definition 1) in Section 1 and elaborated on its necessity (Subsection 3.1) and supporting evidence (Subsection 3.2) in Section 3. Then, we introduce one existing work to concretize misperception, CPT, in Section 4. In Section 5, we utilize CPT to elaborate the Hypothesis 1 by introducing universe, human, and human-estimation MDP."}, {"title": "5.1 Misperception is information loss", "content": "Based on Hypothesis 1, this prompts us to investigate the concept of misperception. Ini-tially, we must clearly define what constitutes perception. In The Quest for a Common Model of the Intelligent Decision Maker, Sutton defines perception as one of four princi-pal components of agents, stating: \"The perception component processes the stream of observations and actions to produce the subjective state, a summary of the agent-world interaction so far that is useful for selecting action (the reactive policy), for predicting fu-ture reward (the value function), and for predicting future subjective states (the transition model)\" Sutton (2022). This definition leads us to consider misperception as the informa-tion loss occurring when processing observations into the subjective state, such that the reward and transition model are not equivalent to those from the environment. The inter-pretation of misperception as information loss during processing is somewhat ambiguous, depending on how the boundary between the agent and the environment is defined. The concept of a boundary between the agent and environment was first proposed by Turing as a 'skin of an onion' Turing (2009), and later, Jiang (2019) suggested that algorithms are not boundary-invariant.\nTherefore, we propose a new agent-environment framework that incorporates the notion that misperception is the information loss from an agent's processing. This framework positions perception at the intersection between the agent and the environment. We provide a detailed description of our agent-environment framework in Figure 2 and further elaborate in the following subsection."}, {"title": "5.2 Universe, Human, and Human-Estimation MDP", "content": "Restricting the Markov Decision Process introduced in Section 3 in a stationary envi-ronment, we consider a single episode finite horizon stationary Markov Decision Process (MDP) denoted as M = (S, A, P, R, \u03b3, T), where S represents the state space, A denotes the action space, P : S \u00d7 A \u2192 \u2206(S) is the transition probability function, R : S \u00d7 A \u2192 R is a reward function, \u03b3 is the discount factor, and T \u2265 0 is a Horizon. We define M as the universe MDP and operate under the assumption that the abstraction from the universe to the model M is lossless, preserving all relevant information. Given a policy \u03c0, the agent collects data {s0, a0, r0, s1,...} as it interacts with the environment at discrete time steps t. The process starts from a fixed initial state s0, and we define the value function as follows:\n$$V(s_0) := \\mathbb{E} \\left[\\sum_{t=0}^{T-1} \\gamma^t R(s_t, a_t) | \\pi, P \\right] $$\\nTo fully leverage Hypothesis 1, and 'perception as information loss from agent's process-ing' from subsection 5.1, we define the human MDP M\u2020 = (S\u2020, A\u2020, P\u2020, R\u2020, \u03b3) where the agent experiences distortions in cumulative distribution of normalized visitation probabil-ity PT(s, a) := \u2211T\u22121t=0 PT((st, at) = (s, a)|s0, \u03c0, P) and reward function R by functions"}, {"title": "Definition 5 (Biased and perceived reward and visitation).", "content": "For given constant \u03bar, \u03bad \u2208 R+, if max(s,a) |R\u2020(s, a) \u2212 R\u2021(s, a)| \u2264 \u03bar holds, then R\u2021(s, a) is \u03bar-biased reward. Also, if max(s,a) |PT,\u2020(s, a) \u2212 P\u03c0,\u2021(s, a)| \u2264 \u03bad holds, then PT,\u2021(s, a) is \u03bad-biased visitation probability."}, {"title": "Lemma 1.", "content": "If maxs,a ||P(.|s, a) \u2212 P\u2021(.\\s, a)||1 \u2264 (1\u2212\u03b3)2 \u03bad where \u03bad > 0, then the agent can guarantee \u03bad-perceived visitation probability (See Definition 5 for the definition of \u03bad-perceived)."}, {"title": "Assumption 2 (Utility function).", "content": "A function u+ : R\u22650 \u2192 R\u22650 is a non-decreasing con-cave function that satisfies limh\u21920+(u+)'(h) \u2264 1. A function u\u2212 : R\u22640 \u2192 R\u22640 is a nonde-creasing convex function that satisfies limh\u21920\u2212(u\u2212)'(h) > 1."}, {"title": "Assumption 3 (Weight function).", "content": "Let w+, w\u2212 : [0, 1] \u2192 [0, 1] be a differentiable func-tion, then those satisfy"}, {"title": "6 Case study: how optimal decision deviates under irrationality", "content": "So far, we have informally introduced the black swan hypothesis (Hypothesis 1) and spatial black swans (Definition 1) in Section 1, then elaborated on its necessity (Subsection 3.1) and provided evidence (Subsection 3.2) in Section 3. Subsequently, in Section 4, we introduced the Cumulative Prospect Theorem (CPT) to model human irrationality. In Section 5, we incorporated CPT into the Markov Decision Process (Subsection 5.2), viewed through the lens of existing work on defining perception (Subsection 5.1) by introducing universe, human, and human-estimation MDPs (Figure 2).\nGiven our main hypothesis that black swan events occur due to human misperception of the real world, we further investigate whether optimal policy also deviates due to mis-perceptions of value or probability. This is critical as overestimating or underestimating all values of probability does not necessarily lead to changes in optimal policy. For example, revisiting Example 4, overestimating or underestimating {r(sbase), r(spremium), r(srisk)} to the same magnitude does not alter the optimal policy for a human to choose ap. More fundamentally, a crucial question that this paper addresses is how misperception influ-ences the deviation of optimal policy. Therefore, in Section 6, we first present some case studies in MDPs with small complexity to demonstrate how optimal policy varies under misperception.\nBefore proceeding, we need to establish the core event of subjective probability. For example, while agents (humans) might distort transition probabilities and perceive them subjectively at a low level which is an intuitive way to model misperception. It is essential to recall that in the Markov Decision Process, the quality of an event is revealed through rewards defined over specific states and actions. This suggests that it is more reasonable to define the minimal object (event) as the state and action, and conduct modeling as distortion on the visitation probability of state and action rather than on the transition probability itself. Since this approach has not been investigated in existing work, we examine both cases in Section 6 and Section 7. Specifically, in Section 6, we assume distortion of transition probability within a non-stationary Markov decision process and investigate how optimal policy deviates due to misperception of transition probability. Then, in Section 7, we explore the distortion of the visitation probability within a stationary Markov decision process."}, {"title": "6.1 Problem setup for Section 6", "content": "In this section, we consider discrete state and action stationary Markov Decision Process. Build upon value function (Equation 4), we define value function and state value function of time t as"}, {"title": "6.2 Case 1. Contextual bandit (T = 1)", "content": "We begin with a simple case where the decision horizon is T = 1, commonly referred to as a contextual bandit (Lattimore and Szepesv\u00e1ri (2020)). Surprisingly, in this setting, the human optimal policy coincides with the real-world optimal policy. This is somewhat counterintuitive, as several significant examples (Examples 2, 3, and 4) suggest that human decision-making often exhibits irrationality.\nTheorem 1 (One-step Human Optimal Policy). If T = 1, then the optimal policy from the universe MDP aligns with the optimal policy of the human MDP, i.e. \u03c0\u2217 = \u03c0\u2217,\u2020.\nAn important insight from Theorem 1 is that when decisions are not sequential, the typical distortions in human perception do not affect the alignment with the optimal policy of the real-world, suggesting that human irrationality is less influential in a single-step decision-making setting. This is further explained in the following remark."}, {"title": "6.3 Case 2. |S| = 2 when T > 1", "content": "Now, let us consider the simplest case where H > 1 where |S| = 2. Surprisingly, we find results similar to those presented in Section\nTheorem 2 (Multi-step human optimal policy). If |S| = 2, then the optimal policy from the universe MDP also aligns with the optimal policy of the human MDP, that is, \u03c0\u2217,\u2020 t = \u03c0\u2217 t for \u2200t \u2208 [T].\nThe proof of Theorem 2 is based mainly on the assumption that |S| = 2, notably using the property that w(1) = 1. However, this technique cannot be applied directly if |S| \u2265 3. For example, in Example 4, where |S| = 3, it is demonstrated that human decision making results in a suboptimal policy in the real-world. This outcome may seem counterintuitive. However, a heuristic analysis suggests that having only two states in the state space indicates that the actions do not introduce a varied randomness. Essentially, if randomness is introduced by any action, it would likely affect both states s0 and s1 if the action provides a nondeterministic next state. Thus, every action provides the same next-state set, which makes a mere comparison between two states. This observation implies that in scenarios with a smaller state space, being irrational (believing on false belief) does not affect to deviate from optimal policy. We can also interpret this result as if |S| is small, sequential decision-making problems are easy so that humans can always provide the optimal action in the real world."}, {"title": "6.4 Case 3: |S| = 3 with unbiased reward perception", "content": "We consider the hypothetical scenario where u(r) = r, indicating that humans have an unbiased perception of their reward.\nTheorem 3 (Two step optimal decision when |S| = 3). Given any state space S where |S| = 3 and a decision horizon T = 2, there exists a transition probability P and reward R of the real world such that the optimal policy of human MDP differs from that of universe MDP."}, {"title": "7 A definition of spatial black swans", "content": "Now, we introduce the definition of black swan inspired by CPT."}, {"title": "7.1 Black swan hypothesis", "content": "First, based on"}]}