[{"title": "A Theoretical Perspective for Speculative Decoding Algorithm", "authors": ["Ming Yin", "Minshuo Chen", "Kaixuan Huang", "Mengdi Wang"], "abstract": "Transformer-based autoregressive sampling has been the major bottleneck for slowing down large language model inferences. One effective way to accelerate inference is Speculative Decoding, which employs a small model to sample a sequence of draft tokens and a large model to validate. Given its empirical effectiveness, the theoretical understanding of Speculative Decoding is falling behind. This paper tackles this gap by conceptualizing the decoding problem via markov chain abstraction and studying the key properties, output quality and inference acceleration, from a theoretical perspective. Our analysis covers the theoretical limits of speculative decoding, batch algorithms, and output quality-inference acceleration tradeoffs. Our results reveal the fundamental connections between different components of LLMs via total variation distances and show how they jointly affect the efficiency of decoding algorithms.", "sections": [{"title": "1 Introduction", "content": "The recent surge of scaling Transformer models has led to the flourishing of AI, where success has been witnessed in wide areas such as natural language Touvron et al. [2023], Achiam et al. [2023], computer vision Dosovitskiy et al. [2020], Han et al. [2022], video generations Arnab et al. [2021], Ho et al. [2022], and robotics Brohan et al. [2022], Shridhar et al. [2023]. In the meantime, the decoding process also becomes more and more time-consuming as the model size scales up. This is mainly due to the autoregressive nature of Transformers, where each generated token also serves as the input for future generations. As a result, decoding T tokens would take T forward passes of the full model.\nA recent effort to tackle this challenge is speculative decoding (SD) Chen et al. [2023a], Leviathan et al. [2023], where the autoregressive sampling is performed on a small draft model and the large language model verifies tokens generated by draft model to decide whether it should be accepted/rejected. Once a token is rejected, the generation process will start from the most recently accepted token, until a full response is completed. Speculative decoding achieves 2-2.5\u00d7 LLM inference speedup empirically, while preserving the quality of generation.\nSubsequently, numerous studies Miao et al. [2023], Liu et al. [2023], Kim et al. [2023], Zhou et al. [2023] have expanded this methodology, enabling further inference acceleration. Intuitively, for speculative decoding, when the generation distribution of small model p and large model q are close"}, {"title": "1.1 Related works", "content": "Speculative Decoding and its applications. Speculative execution, where performing speculative work can expedite computation on parallel machines by allowing certain tasks to commence before their necessity is confirmed, can date back to Burton [1985], Gabbay and Mendelson [1996]. Recently, Chen et al. [2023a], Leviathan et al. [2023] formalize this idea with rejection sampling based design for LLM Decoding and achieve multiple-time inference acceleration compared to vanilla auto-regressive decoding. There are fruitful studies Xia et al. [2023], Miao et al. [2023], Liu et al. [2023], Sun et al. [2023], Kim et al. [2023], Zhou et al. [2023], Spector and Re [2023], Mitchell et al. [2023], He et al. [2023], Bae et al. [2023], Su et al. [2023], Ahn et al. [2023], Santilli et al. [2023], Xu et al."}, {"title": "2 Preliminaries", "content": "In this section, we provide the mathematical formulation for decoding problems using Markov Chains, and we explain auto-regressive models and speculative decoding algorithm based upon that."}, {"title": "2.1 Background for decoding problems", "content": "A Markov Chain Model for Decoding. We denote x0 as the prompt. xn is the n-th token output and x1:T is the trajectory output by the algorithm with T to be the fixed decoding horizon.. Then any decoding algorithm can be characterized by a Markov Chain: state at t is described by history x0:t, the initial state is x0; the state transition Pt maps state x0:t to state x0:t+1. In the context of decoding problem, the transition matrix is defined as P(x0:t+1|x0:t) := p(xt+1|x1:t). In particular, we use p to denote the (conditional) distribution for the draft model that resembles small speculative model, and q for the target model that represents large language model. We use [q \u2212 p]+ to denote the normalized distribution for max{0, q(x) \u2212 p(x)}, \u2200x \u2208 V with V being the token vocabulary. We also denote Ex\u223cf [g(x)] := \u2211x f(x)g(x)."}, {"title": "2.2 Problem Setup", "content": "Speculative Decoding has two key merits. First, it maintains output quality, meaning the output distribution by the algorithm is identical to the output distribution of the large model q, and we term it distribution unbiasedness. Second, it has fast inference property since the auto-regressive sampling is performed on the small model and the target model only verifies. With (possibly) multiple draft tokens being accepted very round, Speculative Decoding can be much faster than direct decoding on large models, and its inference is bottlenecked by the parallel score computation q(\u00b7|X1:n\u22121, In:n+i), i\u2208 [T \u2212 n]. To highlight this, we defined it as the oracle call and make an assumption based on that.\nDefinition 1. We define one trigger of obtaining logits for In:T in Step 7 of 2 as one Oracle call.\nAssumption 1. We assume that: (1) compared to the large model, the computational cost of the draft/small model is negligible. (2) each oracle call has runtime O(1).\nRemark 1. We assume the above only for the theoretical cleanliness. With negligible draft model, the lookhead K = T in Algorithm 2. In other words, given X1:n\u22121, instead of sampling the next K draft tokens In:n+K\u22121, we are allowed to sample until the end, i.e. \u00cen:T. In practice, Assumption 1(1) also holds true in many cases. One example of negligible-cost model c \u2248 0 is n-gram models, and the empirical evidence in Leviathan et al. [2023], Ou et al. [2024] shows n-gram draft model speeds up inference pretty well. In addition, for summarization tasks where long sequences are likely to repeat, any draft model that reuses tokens from the context with a matching prefix, is also cost negligible. Assumption 1(2) is also a standard abstraction, since parallelization consumes (roughly) equal computation as for a single logit. It will consume more memory, but such aspect is beyond the scope of our theoretical study.\nMetric for measuring efficiency. Desired algorithms should preserve the output quality and be efficient for decoding. To formalize, our theory aims to study the following two quantities:\n\u2022 Inference acceleration: the ratio between the inference time of decoding large model q and the inference time of decoding the algorithm;\n\u2022 Output quality: the distribution bias between the algorithm and the large model distribution q. An algorithm maintains the output quality if it is distribution unbiased."}, {"title": "3 Analysis on Efficiency and Optimality for Speculative Decoding", "content": "We start with the following theorem at the beginning of the section. It covers output quality, which is measured by distribution bias, and expected number of rejections for speculative decoding. Its proof is deferred to Appendix B.\nTheorem 1. We have the following two results for Speculative Decoding.\n(1) We define random variables Rn \u2208 {0, 1} that indicates whether the n-th token is rejected (with 1 being rejected). Here rejection means Line 7 of Algorithm 1 is executed. Then, the total number of rejections Nrej = \u2211Tn=1 Rn. For Speculative Decoding (here TV denote the TV distance):\nE[Nrej] = \u2211Tn=1 Ex1:2\u22121\u223cq[TV(pn(\u00b7|x1:n\u22121), qn (\u00b7|X1:n\u22121))].\n(2) The output distributions of Algorithm 1 and the target model q are identical, i.e. for any output sequence x1:T \u2208 VT, the joint the distributions over 11:T satisfies: [PSD (x1:T) = q(x1:T).\nThe first part of Theorem 1, to our knowledge, is the first result that characterizes the expected rejection for speculative decoding a sequence of length T using p and q. The second part of Theorem 1 shows the distribution unbiasedness for SD, which has been presented in Chen et al. [2023a], Leviathan et al. [2023]. There are three interesting indications:\n\u2022 If E[TV(pn, qn)(\u00b7|X1:n\u22121)] = 0 for all n, all tokens are accepted and the accelerate rate = T;\n\u2022 If E[TV(pn, qn)(\u00b7|X1:n\u22121)] = 1 for all n, all tokens are rejected and the accelerate rate = 1, i.e. all T tokens are sampled from the large model q;\n\u2022 In general, the accelerate rate for SD is T/\u03a3Tn=1E[TV(pn, qn)(\u00b7|x1:n\u22121)].\nRemark 2. Leviathan et al. [2023] derive the expected number of token generated per run of Speculative Decoding as 1/E[TV(p, q)] for K = \u221e.4 Their result equals T/ \u03a3n=1T \u0415[TV(pn, qn)(\u00b7|X1:n\u22121)] when E[TV(pn, qn)(\u00b7|X1:n\u22121)] is identical for all n, and this is due to their assumption that the acceptance rates \u1e9e are i.i.d.. In contrast, our guarantee holds for the case that allows the sequential dependence between different decoding steps.\nSimulation. We also provide a simulation of Speculative Decoding and compare it with our Theorem 1 in the left panel of Figure 2(a) with horizon T = 50, pn, qn, n = 1, . . ., 50 are nonstationary Markov Chains. The green line is the empirical average rejections among 100 \u00d7 N runs of Algorithm 1 and the orange line the theoretical value computed via Theorem 1. From the simulation, after 5000 runs the empirical average rejections converge to our theoretical value 16.41. In this example, the acceleration rate is 50/16.41 = 3.05. The specifications of the simulation is included in Appendix F."}, {"title": "3.1 Optimality of Speculative Decoding", "content": "Now we have analyzed the efficiency of speculated decoding and provided mathematical characterization of the number of expected rejections, which is depicted by the TV distance and scales linearly with the inference time. A natural next-step question to ask is: Is there any other rejection-based algorithm 2 that can do better? We answer this question in the next theorem.\nTheorem 2 (Instance-dependent Rejection Lower Bound). For an instance P := (p, q), where p, q stand for the distributions of the draft model and the target model respectively, defining the family of algorithms as F := {A : A is a specification of Algorithm 2 that satisfies PA = qt \u2200t (i.e., unbiased)}. For an algorithm A, denote Nrej as the number of rejections. Then we have the lower bound\ninfA\u2208F E [Nrej] > \u03a3Tn=1 Ex1:n\u22121\u223cq [TV(pn, qn)(\u00b7|X1:n\u22121)].\nTakeaways. Via Theorem 2, the answer to the key question is: No rejection improvement can be made in Algorithm 2 by changing the acceptance probability bt and the distribution Pt if we want to keep the distribution unbiasedness. We point out that lower bound of Theorem 2 matches the complexity upper bound of Speculative Decoding given by Theorem 1. This result confirms that speculative decoding is optimal in the class of all rejection-based methods.\nThe practical implication is that there is no need to tweak acceptance probability, as it will not make performance better. In the next Section 3.2, we will see that Speculative Decoding can be provably improved, as long as one can decode and verify multiple sequence of tokens in parallel.\nConnection to optimal transport. We mention Sun et al. [2023] nicely consider maximizing the acceptance probability from the optimal transport perspective. For a single token, the optimal transport cost is \u2211 min(p(x), q(x)), which corresponds to the optimal expected rejection TV(p, q). However, for the sequential T tokens, their Section 5,6 does not provide a explicit formulation for optimal acceptance/rejections. In this sense, their optimality result can be cast as a special case of ours. However, we do emphasis there are differences in the settings, where Sun et al. [2023] consider the optimal transport and we study the class F."}, {"title": "3.2 Analysis for Batch Speculative Decoding", "content": "To further improve the provable efficiency, we consider batch algorithms that extend the speculative decoding with multiple candidate draft sequences. Most of the existing works Spector and Re [2023], Su et al. [2023], Miao et al. [2023], Yang et al. [2024], Jeon et al. [2024] formulates batch sequences via a speculation tree structure. In particular, the representative work Miao et al. [2023] propose the merged token tree structure that combines sequences with the same prefix. A depth-first search is designed for speculation to ensure the unbiasedness of the algorithm. In addition, Stern et al. [2018] devise the parallel decoding structure that speculate the token of each responses given its previous"}, {"title": "4 Analysis on the Optimal Rejection-Distribution Bias Tradeoff", "content": "In earlier sections, we focus on analyzing SD or Batch SD which are distribution unbiased. To further reduce rejections, the algorithm may have to compromise on output quality. Nevertheless, it is usually acceptable for many real-world applications. For instance, for the family of Gemini models Team et al. [2023], Google may deploy the small model Gemini Nano as the draft model p and Gemini Ultra as the target model q and tune the acceptance probability b (in Algorithm 2) higher such that Gemini Nano is applied more often for the purpose of less expenditure. Therefore, an intriguing question to ask is: For biased algorithms, what is the optimal tradeoff between rejection and distribution bias?"}, {"title": "4.1 An Optimization Formulation and Pareto-optimal Characterization", "content": "We measure the distribution bias between PA and q via TV distance. Concretely, for any algorithm A := (b, P) in 2 with acceptance probability b and rejection distribution P, we fix b and aim to optimize the following objective\nLossy(b) := minP TV[PA, q], where A := (b, P).\n(1)\nWe wish to solve the above since it characterizes the minimal distribution bias for any design b. We present our solution in the next."}, {"title": "4.2 Experiment", "content": "We provide an additional simple experiment to show the effectiveness of our Pareto-optimal solution in Theorem 4. Consider objective (1). For the given acceptance probability b > min{1, q/p}, we have two options for P: 1. Baseline: select P to be suboptimal to (1), i.e. simply set P := q, which is the target distribution; We call this method Decoding-UNO. 2. Set P := P\u2217 be the optimal distribution of Theorem 4. We call this method Decoding-OPT. Instead of TV distance, we measure the quality via WinRate in our experiment. Concretely, for each prompt, we let Decoding-UNO and Decoding-OPT to generate responses independently, and use score models to compare whose genera tion has higher quality. We specify draft model p as pythia-70m and target model q as pythia-2.8b from EleutherAI Biderman et al. [2023]. We apply the score model to be RM-Mistral-7B or GPT-4. We test 200 prompts from Alpaca-Farm-Eval Dataset Dubois et al. [2023] with 500 respons es/comparisons per prompt. For a given prompt, Decoding-OPT wins if for more than 250 comparisons, score model prefer its response over Decoding-UNO. Then the WinRate for each method is computed as #wins/200."}, {"title": "5 Discussions", "content": "On the optimality for batch algorithms. Unlike their non-batch counterparts, our batch algorithm studies do not come with optimality guarantees. This is largely due to the diverse and arbitrary nature"}, {"title": "A Proof Sketch", "content": null}, {"title": "A.1 Proof sketch of Theorem 2.", "content": "For any algorithm A \u2208 F, its design bn can be written as a function of any sequence X1:n\u22121, In with 0 \u2264 bn (xn, X1:n\u22121) \u2264 1. Based on this, we can further define the new function en : V \u00d7 Vn\u22121 \u2192 R according to the following equation:\nbn (In, X1:n\u22121) = min {1,qn(In|X1:n-1)/pn(In|X1:n-1)}.\n(2)\nIndeed, we can choose en := bn. Pn qn, and the validity of the definition is guaranteed by the Lemma 1. Then, the acceptance probability bn > min{1, qn/pn } implies en > 0.\nNext, we show for any A \u2208 F, it must satisfy bn \u2264 min{1, qn/pn In} for all n. To this end, we design the two token sets V+ = {x : \u2203n s.t. En(x) > 0} and V\u2212 = {x : \u2203n s.t. En(x) \u2264 0}and prove V+ = \u00d8, V\u2212 = \u03bd.\nFinally, by Lemma 2 in Appendix, any algorithm satisfies bn \u2264 min{1,qn/pn}, \u2200n \u2208 [T] must have E [Nrej] > \u03a3Tn=1 Eq[TV(Pn, qn)(\u00b7|x1:n\u22121)]. Since A\u2208 F is arbitrary, this concludes the proof.\nThe full proof is included in C."}, {"title": "A.2 High Level Proof Sketch for the second part of Theorem 3", "content": "The derivation for the number of expected rejections using the batch speculative decoding is more involved than the Algorithm 4 due to the parallel response structure. The key step is to compute the intermediate quantity PA(\u00een acc, In = Xn|X1:n\u22121). Let \u00cen\u22121 \u223c p(\u00b7|x1:n\u22122), then there are two cases: In\u22121 accepted or rejected. We have\nIPA (In acc, In = Xn|X1:n\u22121) = PA(In acc, In = Xn, In\u22121acc|x1:n\u22121) + PA(In acc, In = Xn, In\u22121rej|x1:n\u22121)\n= PA(\u00een acc, \u00een = Xn|In\u22121acc, X1:n\u22121) PA (In\u22121acc|X1:n\u22121)+\nPA(\u00een acc, \u00een = Xn|In\u22121rej, X1:n\u22121) PA(\u00cen\u22121rej|x1:n\u22121)\n(3)\nIn the process of finding pb, we need to compute the the quantity f(x1:n) := P(X1:n {n-th draft token rejected}) and it can be recursively computed via (22) using p, q."}, {"title": "A.3 Proof sketch for Theorem 4", "content": "Due to space constraint, we only summarize the high-level proof ideas for Theorem 4. Since \u03a3xA(x) = 1, the original objective (1) can be equivalently rewritten as\nminP \u2211x |A(x) \u2212 P(x)|, s.t.\u2211x P(x) = 1, P(x) \u2265 0, \u2200x \u2208 V.\n(4)\nWe now find the solution of objective (4) in two steps.\nStep1: Recall A+ = {x \u2208 V : A(x) \u2265 0} and A\u2212 = {x \u2208 V : A(x) < 0}, then any optimal P\u2217 must satisfy P\u2217(x) = 0 for all x \u2208 A. This can be shown by contradiction via an alternative construction P' to reason P\u2217 is suboptimal to P'."}, {"title": "B Proof of Theorem 1", "content": "Theorem 6 (Restatement of the first part of Theorem 1). We define random variables Rn \u2208 {0, 1} indicating whether the n-th token is rejected with 1 being rejected (here rejection means Line 6 of Algorithm 1 is executed). Then, the total number of rejections Nrej = \u2211Tn=1 Rn. For Speculative Decoding,\nE[Nrej] = \u2211Tn=1 Ex1:n\u22121\u223cq[TV(pn(\u00b7|X1:n\u22121), qn (\u00b7|X1:n\u22121))].\nHere TV denote the TV distance between two distributions.\nProof. Given the verified the tokens X1:n\u22121, we first compute P(Reject at n|x1:n\u22121). Denote the candidate draft token x \u223c Pn (\u00b7|X1:n\u22121), then by law of total probability\nP(Reject at n|x1:n\u22121) = \u2211 P(Reject at n, x|x1:n\u22121)\n= \u2211 P(Reject at n, x|x1:n\u22121)P(X|X1:n\u22121)\n= \u2211 P(Reject at n, x|x1:n\u22121)Pn (X|X1:n\u22121)\n= \u2211 (1-min{1,qn(x|x1:n-1)/pn(x|x1:n-1)})Pn (X|X1:n\u22121)\n= \u2211 max{0, pn \u2212 qn} (X|X1:n\u22121) = TV(pn, qn)(\u00b7|X1:n\u22121),\nwhere the third equal sign uses draft token is sampled from pn and the fourth equality is by design of Speculative Decoding (Algorithm 1, Line 4).\nLastly, by law of total expectation and above\nT\nE[Nrej] = \u2211E[Rn] = \u2211E [E[Rt|x1:t\u22121]]\nt=1 t=1\nT\n=\u2211\u2211E[Rt|x1:t\u22121]P(x1:n\u22121)\nt=1 x1:n\u22121\nT\n=\u2211\u2211E[Rt|x1:t\u22121]q(x1:n\u22121)\nt=1 x1:n\u22121\nT\n=\u2211\u2211 P(Reject at n|x1:n\u22121)q(x1:n\u22121)\nt=1 x1:n\u22121\nT\n= \u2211 Ex1:n\u22121\u223cq[TV(pn(\u00b7|X1:n\u22121), qn (\u00b7|X1:n\u22121))].\nn=1\nHere the fourth equal sign comes from Speculative Decoding keep the distribution q (Proposition ??), the fifth equal sign comes from the event {Rn = 1} = {Reject at n}.\nTheorem 7 (Restatement of the second part of Theorem 1). The output distributions of Speculative Decoding Algorithm 1 and the large model q are identical, i.e. for any output sequence x1:T \u2208 VT, the joint the distributions over 11:T satisfies: [PSpecDecoding (X1:T) = q(x1:T)."}, {"title": "C Lower Bound", "content": "Theorem 8 (Restatement of Theorem 2). Define the arbitrary instance P := (p, q), and define the family of algorithms as\nF := {A : A is a specification of Algorithm 2 that satisfies P+A = qt \u2200t (i.e., distribution unbiased)}.\nFor an algorithm A, denote Nrej as the number of rejections. Then we have\nT\ninf E [Nrej] > \u2211 Ex1:n\u22121\u223cq [TV(Pn, qn) (\u00b7|X1:n\u22121)] := C(P).\nA\u2208F\nn=1\nRemark 4. Theorem 8 shows the rejections of Algorithm 1 is tight at the instance level (over the family of algorithms F). Therefore, it attains the instance-optimality over sequential decoding algorithms family F.\nProof. For any algorithm A \u2208 F, its bn can be written as a function of the sequence X1:n\u22121, In with 0 \u2264 bn (xn, x1:n\u22121) \u2264 1. Based on this, we define the new function en : V \u00d7 Vn\u22121 \u2192 R according to the following equation:\nbn (Xn, X1:n\u22121) = min {1,qn(In|X1:n\u22121) + En(In,X1:n\u22121)/pn(In|X1:n\u22121)}\n(9)\nIndeed, we can choose En := bn. Pn qn, and the validity of the definition is guaranteed by the Lemma 1. Recall A \u2208 F is a distribution unbiased algorithm w.r.t. q. Let x1, . . . , In be the validated sequence of A, then we have\nPA(xn|X1:n\u22121) = PA(x1:n)/PA(x1:n-1) = qn(x1:n)/qn(x1:n-1) = qn(xn|x1:n\u22121).\n(10)\nOn the other hand, let \u00cen \u223c Pn (\u00b7|X1:n\u22121), the decomposition holds\nPA(xn|X1:n\u22121) = PA(xn, In accept|x1:n\u22121) + P(xn, In reject|x1:n\u22121)\n= PA(\u00een accept \u00een = Xn, X1:n\u22121)PA (In = Xn|X1:n\u22121)\n+ PA(xn|In reject, x1:n\u22121)PA(\u00een reject|x1:n\u22121)\n= bn(xn, X1:n\u22121) \u00b7 Pn (xn|x1:n\u22121) + PA(xn|In reject, x1:n\u22121) \u00b7 PA(\u00een reject|x1:n\u22121)\n= bn (xn, X1:n\u22121) \u00b7 Pn (Xn|X1:n\u22121) + P(xn|In reject, x1:n\u22121)(1 \u2212 \u2211 bn (X, X1:n\u22121)Pn (X|X1:n\u22121)),\nx\n(11)\nwhere the third equal sign uses PA(In = Xn|x1:n\u22121) = Pn(xn|x1:n\u22121) and the last equal sign is due to\nPA (In reject|x1:n\u22121) = 1 \u2212 P(In accept|x1:2\u22121) = 1 \u2212 \u2211 PA (In accept, In = x|xX1:n\u22121)\n=1 \u2212 \u2211bn (X, X1:n\u22121)Pn (X|X1:n\u22121).\nLet's do some further simplifications. First off,\nbn (Xn, X1:n\u22121). \u2022 Pn(xn|x1:n\u22121) = Pn(xn|X1:n\u22121) min {1,qn(xn|x1:n-1) + En(Xn,X1:n\u22121)/pn(xn|X1:n-1)} = min {pn (xn|x1:n\u22121), qn (xn|x1:n\u22121) + En (Xn, X1:n\u22121)},\nand\n1 \u2212 \u2211 bn (X, X1:n\u22121)Pn (X|X1:n\u22121) =1\u2212min {pn (X|X1:n\u22121), qn (xn|X1:n\u22121) + En (X, X1:n\u22121)}\nx\n= \u2211 max {0, Pn (x|x1:n\u22121) \u2212 qn (xn|X1:n\u22121) \u2212 En (x, x1:n\u22121)}\nx\n(12)\n(13)\nFor Speculative Decoding, its bn (In, X1:n\u22121) = min {1,qn(xn|x1:n\u22121)/pn(xn|x1:n-1)}."}, {"title": "D Batch Speculative Decoding", "content": "We split the proofs for unbiasedness and rejections in two parts.\nTheorem 9 (Unbiasedness of Theorem 3). Denote the Algorithm 4 as AB. For any sequence x1:T (where xi \u2208 V)", "Step1": "Let x1:n\u22121 be the accepted tokens up to n \u2212 1. We first show PAB\nn(xn|x1:n\u22121) = PLLM\nn(xn|x1:n\u22121) \u2200xn \u2208 V.\nWe partition the generation of xn into two cases: (i). accepted as the first token of the m-th responses (m = 1", "pnp(\u00b7|x1": "n\u22121q", "then\nPAB\nn(xn|x1": "n\u22121) = PAB\nn(xn", "acc|x1": "n\u22121) + PAB\nn(xn", "rej|x1": "n\u22121)\n= PAB\nn(x\u02dcmn = xn|x1:n\u22121)PAB\nn(x\u02dcmn acc|x\u02dcmn = xn", "x1": "n\u22121)\n+ PAB\nn(x\u02dcmn rej|x1:n\u22121)PAB\nn(xn|x\u02dcmn rej"}, {"x1": "n\u22121)\n= pn(xn|x1:n\u22121) min{1", "qn(xn|x1": "n\u22121)/pn(xn|x1:n\u22121)"}], "qn(x1|x1": "n\u22121) \u2212 pn(x1|x1:n\u22121)"}, "max{0", "qn(xn|x1:n\u22121) \u2212 pn(xn|x1:n\u22121)", "n\u2211x1 max{0", "qn(x1|x1:n\u22121) \u2212 pn(x1|x1:n\u22121)", "qn(xn|x1:n\u22121).\nBy the construction of Algorithm 4 (Line 3)", "when xn is accepted/rejected as the tp\u2265 2q-th draft token of certain response", "we have qn(xn|x1:n\u22121) = PLLM\nn(xn|x1:n\u22121). This gives PAB\nn(xn|x1:n\u22121) =\nPLLM\nn(xn|x1:n\u22121).\nFor the first case. This part of the proof largely follows Theorem 4.2 of Miao et al. [2023]. In this case", "the n-th generated token xn has M + 1 possibilities: accepted at the m-th response or rejected by all M responses and sample at Line 28. Since the algorithm will iterate all M responses if not accepted", "we denote qmn as the qn for the m-th response. Then we have the recursion\nqmn+1 = max{0", "qmn \u2212 pn", "rm", "nwhere rm = \u2211x max{0", "qmn (x\u2032|x1:n\u22121) \u2212 pn(x\u2032|x1:n\u22121)", "is normalizing factor and q1n = PLLM. Let xn \u223c p(\u00b7|x1:n\u22121)", "then PAB\nn(xn acc|x1:n\u22121) = PAB\nn(xn", "Ec1|x1:n\u22121) + PAB\nn(xn", "E1|x1:n\u22121)\n= min{pn(xn|x1:n\u22121)", "q1np(xn|x1:n\u22121)", "PAB\nn(xn", "E1|x1:n\u22121)\n= min{pn(xn|x1:n\u22121)", "q1np(xn|x1:n\u22121)", "r1 \u00b7 PAB\nn(xn|E1", "x1:n\u22121)\nDenote PAB\nn(xn|x1:n\u22121) := PAB\nn(xn|E0", "x1:n\u22121)", "by similar calculation we have in general\nPAB\nn(xn|E0:m\u22121", "x1:n\u22121) = min{pn(xn|x1:n\u22121)", "qmnp(xn|x1:n\u22121)", "rm \u00b7 PAB\nn(xn|E0:m", "x1:n\u22121).\nNext we prove PAB\nn(\u00b7|E0:m", "x1:n\u22121) = qm+1np(\u00b7|x1:n\u22121) \u2200m \u2208 {0", 1, ".", ".", ".", "M", "by back ward induction. First of all", "PAB\nn(\u00b7|E0:M", "x1:n\u22121) = \u201cqM+1n\u2212pnpp|x1:n\u22121)\nmaxt0", "qMnp\u2212pnuppp|x1:n-1) = qM+1n. Suppose PAB\nn(\u00b7|E0:m+1", "x1:n\u22121) = qm+2np(\u00b7|x1:n\u22121)", "then\nPAB\nn(\u00b7|E0:m", "x1:n\u22121) = PAB\nn(\u00b7", "Ecm+1|E0:m", "x1:n\u22121) + PAB\nn(\u00b7", "Em+1|E0:m", "x1:n\u22121)\n= min{pnp\u00b7|x1:n\u22121q", "qm+1np\u00b7|x1:n\u22121q + PAB\nn(\u00b7", "Em+1|E0:m", "x1:n\u22121)\n= min{pnp\u00b7|x1:n\u22121q", "qm+1np\u00b7|x1:n\u22121q + rm+1 \u00b7 PAB\nn(\u00b7|E0:m+1"]