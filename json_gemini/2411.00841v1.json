{"title": "A Theoretical Perspective for Speculative Decoding Algorithm", "authors": ["Ming Yin", "Minshuo Chen", "Kaixuan Huang", "Mengdi Wang"], "abstract": "Transformer-based autoregressive sampling has been the major bottleneck for slowing down large language model inferences. One effective way to accelerate inference is Speculative Decoding, which employs a small model to sample a sequence of draft tokens and a large model to validate. Given its empirical effectiveness, the theoretical understanding of Speculative Decoding is falling behind. This paper tackles this gap by conceptualizing the decoding problem via markov chain abstraction and studying the key properties, output quality and inference acceleration, from a theoretical perspective. Our analysis covers the theoretical limits of speculative decoding, batch algorithms, and output quality-inference acceleration tradeoffs. Our results reveal the fundamental connections between different components of LLMs via total variation distances and show how they jointly affect the efficiency of decoding algorithms.", "sections": [{"title": "1 Introduction", "content": "The recent surge of scaling Transformer models has led to the flourishing of AI, where success has been witnessed in wide areas such as natural language Touvron et al. [2023], Achiam et al. [2023], computer vision Dosovitskiy et al. [2020], Han et al. [2022], video generations Arnab et al. [2021], Ho et al. [2022], and robotics Brohan et al. [2022], Shridhar et al. [2023]. In the meantime, the decoding process also becomes more and more time-consuming as the model size scales up. This is mainly due to the autoregressive nature of Transformers, where each generated token also serves as the input for future generations. As a result, decoding T tokens would take T forward passes of the full model.\nA recent effort to tackle this challenge is speculative decoding (SD) Chen et al. [2023a], Leviathan et al. [2023], where the autoregressive sampling is performed on a small draft model and the large language model verifies tokens generated by draft model to decide whether it should be accepted/rejected. Once a token is rejected, the generation process will start from the most recently accepted token, until a full response is completed. Speculative decoding achieves 2-2.5\u00d7 LLM inference speedup empirically, while preserving the quality of generation.\nSubsequently, numerous studies Miao et al. [2023], Liu et al. [2023], Kim et al. [2023], Zhou et al. [2023] have expanded this methodology, enabling further inference acceleration. Intuitively, for speculative decoding, when the generation distribution of small model p and large model q are close"}, {"title": "1.1 Related works", "content": "Speculative Decoding and its applications. Speculative execution, where performing speculative work can expedite computation on parallel machines by allowing certain tasks to commence before their necessity is confirmed, can date back to Burton [1985], Gabbay and Mendelson [1996]. Recently, Chen et al. [2023a], Leviathan et al. [2023] formalize this idea with rejection sampling based design for LLM Decoding and achieve multiple-time inference acceleration compared to vanilla auto-regressive decoding. There are fruitful studies Xia et al. [2023], Miao et al. [2023], Liu et al. [2023], Sun et al. [2023], Kim et al. [2023], Zhou et al. [2023], Spector and Re [2023], Mitchell et al. [2023], He et al. [2023], Bae et al. [2023], Su et al. [2023], Ahn et al. [2023], Santilli et al. [2023], Xu et al."}, {"title": "2 Preliminaries", "content": "In this section, we provide the mathematical formulation for decoding problems using Markov Chains, and we explain auto-regressive models and speculative decoding algorithm based upon that.\nA Markov Chain Model for Decoding. We denote $x_0$ as the prompt. $x_n$ is the n-th token output and $x_{1:T}$ is the trajectory output by the algorithm with $T$ to be the fixed decoding horizon. Then any decoding algorithm can be characterized by a Markov Chain: state $a_t$ is described by history $x_{0:t}$, the initial state is $x_0$; the state transition $P_t$ maps state $x_{0:t}$ to state $x_{0:t+1}$. In the context of decoding problem, the transition matrix is defined as $P(x_{0:t+1}|x_{0:t}) := p(x_{t+1}|x_{1:t})$. In particular, we use $p$ to denote the (conditional) distribution for the draft model that resembles small speculative model, and $q$ for the target model that represents large language model. We use $[q - p]_+$ to denote the normalized distribution for $\\max{0, q(x) - p(x)}$, $\\forall x \\in V$ with $V$ being the token vocabulary. We also denote $E_{x \\sim f}[g] := \\sum_{x} f(x)g(x)$."}, {"title": "2.1 Background for decoding problems", "content": "Auto-Regressive Decoding. Consider sampling a trajectory $x_{1:T}$ from an auto-regressive model, where for the given $x_{1:n-1}$, the next token $x_n$ follows the conditional distribution $q$. This decoding mechanism (Algorithm 3) is the prototype for Transformer-based LLMs (e.g. GPT-4). As mentioned in Qian et al. [2024], the accuracy performance of Transformer-based LLMs has been shown to scale with model size, with larger models demonstrating improved capabilities Kaplan et al. [2020]. However, this improvement comes at the cost of higher latency during inference and increased computational requirements.\nSpeculative Decoding. Different from large models, small models are usually much faster at the inference stage. Consequently, we can use a small model to perform auto-regressive sampling and assign large model as a verifier, where the goal of the large model is to check whether the token sampled by the small model should be accepted/rejected. Concretely, this procedure can be summarized into the following three steps:\n\\bullet Draft sampling: given the verified tokens $x_{1:n-1}$, the draft model obtains $K$ sequential candidate tokens $x_{n:n+K-1}$ via sampling from $p(\\cdot|x_{1:n-1}, x_{n:n+i})$, $i \\in [K - 1]$;\n\\bullet Conditional score computation: given $x_{n:n+K-1}$, computing the logits of the $K$ tokens $q(x_{1:n-1}, x_{n:n+i})$, $i \\in [K - 1]$ in parallel;\n\\bullet Token validation: Accept the candidate token $x_t$ (as $x_n$) with some probability $0 \\leq b_t \\leq 1$. If accepted, continue to validate the next candidate $x_{t+1}$, otherwise reject and sample $x_n$ from some designed distribution $P_t$.\nThe above process repeats until $x_{1:T}$ are generated, and the whole algorithm is summarized as the general rejection-based decoding 2. Specifically, Speculative Decoding Chen et al. [2023a], Leviathan et al. [2023] designs $b_t(x_t) := \\min\\{1, \\frac{p_t(x_t)}{q_t(x_t)}\\}$ and distribution $P_t(\\cdot) := [q_t - p_t]_+(\\cdot)$ (see Algorithm 1 and Figure 1)."}, {"title": "2.2 Problem Setup", "content": "Speculative Decoding has two key merits. First, it maintains output quality, meaning the output distribution by the algorithm is identical to the output distribution of the large model q, and we term it distribution unbiasedness. Second, it has fast inference property since the auto-regressive sampling is performed on the small model and the target model only verifies. With (possibly) multiple draft tokens being accepted very round, Speculative Decoding can be much faster than direct decoding on large models, and its inference is bottlenecked by the parallel score computation $q(\\cdot|x_{1:n-1}, x_{n:n+i})$, $i \\in [T \u2013 n]$. To highlight this, we defined it as the oracle call and make an assumption based on that.\nDefinition 1. We define one trigger of obtaining logits for $x_{n:T}$ in Step 7 of 2 as one Oracle call.\nAssumption 1. We assume that: (1) compared to the large model, the computational cost of the draft/small model is negligible. (2) each oracle call has runtime $O(1)$.\nRemark 1. We assume the above only for the theoretical cleanliness. With negligible draft model, the lookhead $K = T$ in Algorithm 2. In other words, given $x_{1:n-1}$, instead of sampling the next $K$ draft tokens $x_{n:n+K-1}$, we are allowed to sample until the end, i.e. $\\hat{x}_{n:T}$. In practice, Assumption 1(1) also holds true in many cases. One example of negligible-cost model $c \\approx 0$ is n-gram models, and the empirical evidence in Leviathan et al. [2023], Ou et al. [2024] shows n-gram draft model speeds up inference pretty well. In addition, for summarization tasks where long sequences are likely to repeat, any draft model that reuses tokens from the context with a matching prefix, is also cost negligible. Assumption 1(2) is also a standard abstraction, since parallelization consumes (roughly) equal computation as for a single logit. It will consume more memory, but such aspect is beyond the scope of our theoretical study."}, {"title": "Metric for measuring efficiency.", "content": "Desired algorithms should preserve the output quality and be efficient for decoding. To formalize, our theory aims to study the following two quantities:\n\\bullet Inference acceleration: the ratio between the inference time of decoding large model $q$ and the inference time of decoding the algorithm;\n\\bullet Output quality: the distribution bias between the algorithm and the large model distribution $q$. An algorithm maintains the output quality if it is distribution unbiased."}, {"title": "3 Analysis on Efficiency and Optimality for Speculative Decoding", "content": "We start with the following theorem at the beginning of the section. It covers output quality, which is measured by distribution bias, and expected number of rejections for speculative decoding. Its proof is deferred to Appendix B.\nTheorem 1. We have the following two results for Speculative Decoding.\n(1) We define random variables $R_n \\in \\{0, 1\\}$ that indicates whether the n-th token is rejected (with 1 being rejected). Here rejection means Line 7 of Algorithm 1 is executed. Then, the total number of rejections $N_{rej} = \\sum_{n=1}^T R_n$. For Speculative Decoding (here TV denote the TV distance):\n$$E[N_{rej}] = \\sum_{n=1}^T E_{x_{1:n-1} \\sim q}[TV(P_n(\\cdot|x_{1:n-1}), q_n(\\cdot|x_{1:n-1}))].$$\n(2) The output distributions of Algorithm 1 and the target model q are identical, i.e. for any output sequence $x_{1:T} \\in V^T$, the joint the distributions over $x_{1:T}$ satisfies: $P_{SD}(x_{1:T}) = q(x_{1:T})$.\nThe first part of Theorem 1, to our knowledge, is the first result that characterizes the expected rejection for speculative decoding a sequence of length T using p and q. The second part of Theorem 1 shows the distribution unbiasedness for SD, which has been presented in Chen et al. [2023a], Leviathan et al. [2023]. There are three interesting indications:\n\\bullet If $E[TV(p_n, q_n)(\\cdot|x_{1:n-1})] = 0$ for all n, all tokens are accepted and the accelerate rate = T;\n\\bullet If $E[TV(p_n, q_n)(\\cdot|x_{1:n-1})] = 1$ for all n, all tokens are rejected and the accelerate rate = 1, i.e. all T tokens are sampled from the large model q;\n\\bullet In general, the accelerate rate for SD is $T/\\sum_{n=1}^T E[TV(p_n, q_n)(\\cdot|x_{1:n-1})]$.\nRemark 2. Leviathan et al. [2023] derive the expected number of token generated per run of Speculative Decoding as $1/E[TV(p, q)]$ for $K = \\infty$. Their result equals $T/\\sum_{n=1}^T E[TV(p_n, q_n)(\\cdot|x_{1:n-1})]$ when $E[TV(p_n, q_n)(\\cdot|x_{1:n-1})]$ is identical for all n, and this is due to their assumption that the acceptance rates $\\beta$ are i.i.d.. In contrast, our guarantee holds for the case that allows the sequential dependence between different decoding steps."}, {"title": "3.1 Optimality of Speculative Decoding", "content": "Now we have analyzed the efficiency of speculated decoding and provided mathematical characterization of the number of expected rejections, which is depicted by the TV distance and scales linearly with the inference time. A natural next-step question to ask is: Is there any other rejection-based algorithm 2 that can do better? We answer this question in the next theorem.\nTheorem 2 (Instance-dependent Rejection Lower Bound). For an instance $P := (p, q)$, where $p,q$ stand for the distributions of the draft model and the target model respectively, defining the family of algorithms as $F := \\{A : A \\text{ is a specification of Algorithm 2 that satisfies }P_A = q_t \\forall t \\text{ (i.e., unbiased)}\\}$. For an algorithm A, denote $N_{rej}$ as the number of rejections. Then we have\n$$\\inf_{A \\in F} E [N_{rej}] \\geq \\sum_{n=1}^T E_{x_{1:n-1} \\sim q} [TV(p_n, q_n)(\\cdot|x_{1:n-1})].$$\nTakeaways. Via Theorem 2, the answer to the key question is: No rejection improvement can be made in Algorithm 2 by changing the acceptance probability $b_t$ and the distribution $P_t$ if we want to keep the distribution unbiasedness. We point out that lower bound of Theorem 2 matches the complexity upper bound of Speculative Decoding given by Theorem 1. This result confirms that speculative decoding is optimal in the class of all rejection-based methods.\nThe practical implication is that there is no need to tweak acceptance probability, as it will not make performance better. In the next Section 3.2, we will see that Speculative Decoding can be provably improved, as long as one can decode and verify multiple sequence of tokens in parallel."}, {"title": "Connection to optimal transport.", "content": "We mention Sun et al. [2023] nicely consider maximizing the acceptance probability from the optimal transport perspective. For a single token, the optimal transport cost is $\\sum_x \\min(p(x), q(x))$, which corresponds to the optimal expected rejection TV(p, q). However, for the sequential T tokens, their Section 5,6 does not provide a explicit formulation for optimal acceptance/rejections. In this sense, their optimality result can be cast as a special case of ours. However, we do emphasis there are differences in the settings, where Sun et al. [2023] consider the optimal transport and we study the class F."}, {"title": "3.2 Analysis for Batch Speculative Decoding", "content": "To further improve the provable efficiency, we consider batch algorithms that extend the speculative decoding with multiple candidate draft sequences. Most of the existing works Spector and Re [2023], Su et al. [2023], Miao et al. [2023], Yang et al. [2024], Jeon et al. [2024] formulates batch sequences via a speculation tree structure. In particular, the representative work Miao et al. [2023] propose the merged token tree structure that combines sequences with the same prefix. A depth-first search is designed for speculation to ensure the unbiasedness of the algorithm. In addition, Stern et al. [2018] devise the parallel decoding structure that speculate the token of each responses given its previous"}, {"title": "Theorem 3 (Unbiasedness and efficiency of batch SD).", "content": "Recall the definition of $R_n$ and $N_{rej}$ in Thm 1, and iteratively define: $q^{m+1} = [q^m\u2212p]_+, \\forall m \\in [M]$ with $q^1 = q$ being the target distribution. Then, for Batch Speculative Decoding 4, $[P_{Batch}(x_{1:T}) = q(x_{1:T}), \\forall x_{1:T} \\in V^T$. Moreover,\n$$E[N_{rej}] = \\sum_{n=1}^T E_{x_{1:n-1} \\sim q}[TV[q, p](\\cdot|x_{1:n-1})] \u2013 \\sum_{n=1}^T E_{x_{1:n-1} \\sim f} [TV(q, p) (x_{1:n-1}) \u2013 \\sum_{m=1}^M [ TV(q^m, p) (x_{1:n-1})]],$$\nBatch Improvement\nwhere $f(x_{1:n}) := P(x_{1:n} \\sim \\{\\text{n-th draft token rejected}\\}) $. $f$ can be iteratively computed via $p, q$.\nTakeaways. The expected rejection of Batch Decoding composes of two parts: (i) $\\sum E_q[TV[q,p]]$ which is the rejection that matches the non-batch speculative decoding; (ii) the batch improvement (BI) which comes from our design and is always non-negative. To better understand how the batch improvement scale with M, we instantiate the single token $BI(q, p) := TV[q, p] \u2013 \\prod_{m=1}^M TV[q^m, p]$ with two simple examples."}, {"title": "Uniform Distribution.", "content": "For the whole space V, let p be a uniform distribution with support V and q be a uniform distribution over a subset of V with size V' < V. Let $V/V' = r$, in this case $BI(Unif(V'), Unif(V)) = (1 \u2013 \\frac{1}{r}) \u2013 (1 \u2013 \\frac{1}{r})^M, \\frac{V}{V'}= r$. Its pattern is visualized in the lower right panel of Figure 3. The improvement converges to 1 \u2013 1/r and is always positive as long as"}, {"title": "4 Analysis on the Optimal Rejection-Distribution Bias Tradeoff", "content": "In earlier sections, we focus on analyzing SD or Batch SD which are distribution unbiased. To further reduce rejections, the algorithm may have to compromise on output quality. Nevertheless, it is usually acceptable for many real-world applications. For instance, for the family of Gemini models Team et al. [2023], Google may deploy the small model Gemini Nano as the draft model p and Gemini Ultra as the target model q and tune the acceptance probability b (in Algorithm 2) higher such that Gemini Nano is applied more often for the purpose of less expenditure. Therefore, an intriguing question to ask is: For biased algorithms, what is the optimal tradeoff between rejection and distribution bias?"}, {"title": "4.1 An Optimization Formulation and Pareto-optimal Characterization", "content": "We measure the distribution bias between $P_A$ and $q$ via TV distance. Concretely, for any algorithm $A := (b, P)$ in 2 with acceptance probability $b$ and rejection distribution $P$, we fix $b$ and aim to optimize the following objective\n$$Loss_y(b) := \\min_P TV[P_A, q], \\text{ where } A := (b, P).$$                                                       (1)\nWe wish to solve the above since it characterizes the minimal distribution bias for any design b. We present our solution in the next."}, {"title": "Theorem 4 (Optimal solution to optimization (1)).", "content": "We can show the objective (1) is equivalent to\n$$Loss_y(b) := \\frac{1}{2} \\min_P \\sum_x |q(x) \u2013 b(x)p(x)| \u2013 P(x) \\sum_x |P(x) \u2013 q(x) - b(x)p(x)|,  \\text{ st. } \\sum_x P(x) = 1, P(x) \\geq 0, \\forall x.$$\nSuppose $\\sum_x [1 \u2013 b(x)]p(x) > 0$. Define $A(x) := \\frac{q(x)-b(x)p(x)}{\\sum_x [1 \u2013 b(x)]p(x)}$, and the sets $A_+ = \\{x \\in V : A(x) \\geq 0\\}$, $A_- = \\{x \\in V : A(x) < 0\\}$. Then the set of optimal distributions of objective (1) is characterized as $\\{P^* : P^*|_{A_-} (\\cdot) = 0; 0 \\leq P^*|_{A_+} (\\cdot) \\leq A(\\cdot)\\}$, and the optimal value is $Loss_y(b) = \\sum_x |q(x) \u2013 b(x)p(x)| \u2013 \\sum_x (1 \u2013 b(x))p(x) \\geq 0$.\nTheorem 4 is a universal characterization that contains both biased and unbiased situations. 1. If b is less than Speculative Decoding threshold, i.e. $b \\leq \\min\\{1, \\frac{q}{p}\\}$ then A becomes a probability distribution and the optimal distribution $P^*$ equals A which is also unbiased ($Loss_y(b) = 0$); 2. If b exceeds the Speculative Decoding threshold, then A is no longer a distribution and there are multiple optimal distributions $P^*$. In this case, the optimal distribution bias $Loss_y(b) > 0$ for Algorithm 2.\nMain Takeaways. Via Theorem 4, we derive the pareto front (the optimal tradeoff) between rejection probability $P(reject)$ vs. distribution distance $TV[P_A, q]$ (Left panel of Figure 4). The blue region can be realized by some algorithm 2, and the red region cannot. (0,0) is the \u201cperfect algorithm\u201d (no rejection, no bias) which does not exists, and, in particular, (0, TV[p, q]) stands for Speculative Decoding. Surprisingly, the pareto front is a straight line connecting (0, TV[p, q]) and (TV[p, q], 0), which represents a linear relationship between the rejection probability and the optimal TV deviation. This is guaranteed by the following theorem."}, {"title": "Theorem 5 (Pareto front).", "content": "For any Algorithm A in the class 2 that satisfies $\\min\\{1, \\frac{q(x)}{p(x)}\\} < b(x) \\leq 1, \\forall x \\in V$. Then\n$$P_A(reject) + Loss_y(b) = TV[p, q].$$\nHere $P_A(reject) = 1 \u2013 \\sum_x b(x)p(x)$ and $Loss_y(b) := \\min_p TV[P_A, q]$.\nWe plot the numerical example using two Markov Chains in the middle and right panel of Figure 4 that coincides with our theoretical finding. In the figure, the acceptance probability is set to be $b(x) = \\min\\{1, \\frac{q(x)}{p(x)} + \\epsilon\\}$. The orange line in (c) and the green boundary in (b) are computed via $Loss_y(b)$ from Theorem 4. The complete proofs for Theorem 5 and Theorem 4 are deferred to Appendix E. For the clearness of illustration, we focus on the pareto front between rejection vs. the minimal TV deviation for a single token."}, {"title": "4.2 Experiment", "content": "We provide an additional simple experiment to show the effectiveness of our Pareto-optimal solution in Theorem 4. Consider objective (1). For the given acceptance probability b > min{1, q/p}, we have two options for P: 1. Baseline: select P to be suboptimal to (1), i.e. simply set P := q, which is the target distribution; 2. Set P := P* be the optimal distribution of Theorem 4. We call the first method Decoding-UNO and the latter one Decoding-OPT. Instead of TV distance, we measure the quality via WinRate in our experiment. Concretely, for each prompt, we let Decoding-UNO and Decoding-OPT to generate responses independently, and use score models to compare whose generation has higher quality. We specify draft model p as pythia-70m and target model q as pythia-2.8b from EleutherAI Biderman et al. [2023]. We apply the score model to be RM-Mistral-7B or GPT-4. We test 200 prompts from Alpaca-Farm-Eval Dataset Dubois et al. [2023] with 500 responses/comparisons per prompt. Table 1 shows that Decoding-OPT achieves better performance than Decoding-UNO across different choice of $\\epsilon$'s. Due to space constraint, the missing details are deffered to Appendix G."}, {"title": "5 Discussions", "content": "On the optimality for batch algorithms. Unlike their non-batch counterparts, our batch algorithm studies do not come with optimality guarantees. This is largely due to the diverse and arbitrary nature"}, {"title": "A Proof Sketch", "content": "For any algorithm A \u2208 F, its design bn can be written as a function of any sequence x1:n-1, xn with 0 \u2264 bn (xn, x1:n-1) \u2264 1. Based on this, we can further define the new function en : V \u00d7 Vn-1 \u2192 R according to the following equation:\n$$b_n (x_n, x_{1:n-1}) = min {1, \\frac{q_n (x_n| x_{1:n-1})}{p_n (x_n | x_{1:n-1})} }.$$  (2)\nIndeed, we can choose en := bn. pn qn, and the validity of the definition is guaranteed by the Lemma 1. Then, the acceptance probability bn > min{1, qn } implies en > 0.\nNext, we show for any A \u2208 F, it must satisfy bn \u2264 min{1, qn } In} for all n. To this end, we design the two token sets V+ = {x : \u2203n s.t. En(x) > 0} and V_ = {x : \u2203n s.t. En(x) \u2264 0}and prove V+ = \u00d8, V_ = \u03bd.\nT\nFinally, by Lemma 2 in Appendix, any algorithm satisfies bn \u2264 min{1, qn }, \u2200n \u2208 [T] must have E [Nrej] > \u03a3=1 Eq[TV(Pn, qn)(\u00b7|x1:n-1)]. Since A\u2208 F is arbitrary, this concludes the proof.The full proof is included in C."}, {"title": "A.2 High Level Proof Sketch for the second part of Theorem 3", "content": "The derivation for the number of expected rejections using the batch speculative decoding is more involved than the Algorithm 4 due to the parallel response structure. The key step is to compute the intermediate quantity PA(xn acc, xn = xn x1:n-1). Let xn-1 ~ p(\u00b7|x1:n-2), then there are two cases: xn-1 accepted or rejected. We have$$P^A(x_n acc, x_n = x_n | x_{1:n-1}) = P^A(x_n acc, x_n = x_n, x_{n-1} acc | x_{1:n-1}) + P^A(x_n acc, x_n = x_n, x_{n-1} rej|x_{1:n-1})\n= P^A(x_n acc, x_n = x_n | x_{n-1} acc, x_{1:n-1}) P^A (x_{n-1} acc|x_{1:n-1})\n+ P^A(x_n acc, x_n = x_n | x_{n-1} rej, x_{1:n-1}) P^A(x_{n-1} rej|x_{1:n-1}).$$ (3)\nIn the process of finding pb, we need to compute the the quantity f(x1:n) := P(x1:n {n-th draft token rejected}) and it can be recursively computed via (22) using p, q."}, {"title": "A.3 Proof sketch for Theorem 4", "content": "Due to space constraint, we only summarize the high-level proof ideas for Theorem 4. Since \u03a3A(x) = 1, the original objective (1) can be equivalently rewritten as\n$$\\min_P \\frac{1}{2} \\sum_x |A(x) \u2013 P(x)|, \\text{ s.t. } \\sum P(x) = 1, P(x) \\geq 0, \\forall x \\in V.$$ (4)\nWe now find the solution of objective (4) in two steps.\nStep1: Recall A+ = {x \u2208 V : A(x) \u2265 0} and A_ = {x \u2208 V : A(x) < 0}, then any optimal P* must satisfy P*(x) = 0 for all x \u2208 A. This can be shown by contradiction via an alternative construction P' to reason P* is suboptimal to P'."}, {"title": "B Proof of Theorem 1", "content": "Theorem 6 (Restatement of the first part of Theorem 1). We define random variables Rn \u2208 {0, 1} indicating whether the n-th token is rejected with 1 being rejected (here rejection means Line 6 of Algorithm 1 is executed). Then, the total number of rejections Nrej = 1=1 Rn. For Speculative Decoding,\n$$E[N_{rej}] = \\sum_{n=1}^T E_{x_{1:n-1} \\sim q}[TV(P_n(\\cdot|x_{1:n-1}), q_n(\\cdot|x_{1:n-1}))].$$\nHere TV denote the TV distance between two distributions.\nProof. Given the verified the tokens x1:n\u22121, we first compute P(Reject at n|x1:n-1). Denote the candidate draft token x ~ Pn (\u00b7|x1:n-1), then by law of total probability\n$$P(Reject \\text{ at } n|x_{1:n-1}) = \\sum_x P(Reject \\text{ at } n, x|x_{1:n-1})\n= \\sum_x P(Reject \\text{ at } n|x, x_{1:n-1})P(x|x_{1:n-1})\n= \\sum_x P(Reject \\text{ at } n|x, x_{1:n-1})P_n (x|x_{1:n-1})\n= \\sum_x (1-\\min\\{1, \\frac{q_n(x|x_{1:n-1})}{P_n(x|x_{1:n-1})}\\})P(x|x_{1:n-1})\n= \\sum_x max\\{0, p_n - q_n\\} (x|x_{1:n-1}) = TV(p_n, q_n)(\\cdot|x_{1:n-1}),$$\nwhere the third equal sign uses draft token is sampled from pn and the fourth equality is by design of Speculative Decoding (Algorithm 1, Line 4).\nLastly, by law of total expectation and above\n$$\\sum_{n=1}^T \u0395[R_n] = \\sum_{n=1}^T E[E[R_n|x_{1:n-1}]]\n=\\sum_{n=1}^T \\sum_{x_{1:n-1}} E[R_n|x_{1:t-1}]P(x_{1:n-1})\n= \\sum_{n=1}^T \\sum_{x_{1:n-1}} E[R_t|x_{1:t-1}]q(x_{1:n-1})\n= \\sum_{n=1}^T \\sum_{x_{1:n-1}} P(Reject \\text{ at } n|x_{1:n-1})q(x_{1:n-1})\n= \\sum_{n=1}^T  E_{x_{1:n-1} \\sim q}[TV(p_n(\\cdot|x_{1:n-1}), q_n(\\cdot|x_{1:n-1}))].$$\nHere the fourth equal sign comes from Speculative Decoding keep the distribution q (Proposition ??), the fifth equal sign comes from the event {Rn = 1} = {Reject at n}."}, {"title": "C Lower Bound", "content": "Theorem 8 (Restatement of Theorem 2). Define the arbitrary instance P := (p", "as\nF": {"A": "A is a specification of Algorithm 2 that satisfies PtA = qt Vt (i.e.", "N_{rej}": "geq \\sum_{n=1"}, "E_{x_{1": "n-1"}, "sim q} [TV(P_n, q_n) (\\cdot|x_{1:n-1})"], "x1": "n\u22121) \u2264 1. Based on this"}