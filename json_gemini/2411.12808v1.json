{"title": "CONVERSATIONAL MEDICAL AI: READY FOR PRACTICE", "authors": ["Antoine Liz\u00e9e", "Pierre-Auguste Beaucot\u00e9", "James Whitbeck", "Marion Doumeingts", "Ana\u00ebl Beaugnon", "Isabelle Feldhaus"], "abstract": "The shortage of doctors is creating a critical squeeze in access to medical expertise. While conversational Artificial Intelligence (AI) holds promise in addressing this problem, its safe deployment in patient-facing roles remains largely unexplored in real-world medical settings. We present the first large-scale evaluation of a physician-supervised LLM-based conversational agent in a real-world medical setting.\nOur agent, Mo, was integrated into an existing medical advice chat service. Over a three-week period, we conducted a randomized controlled experiment with 926 cases to evaluate patient experience and satisfaction. Among these, Mo handled 298 complete patient interactions, for which we report physician-assessed measures of safety and medical accuracy.\nPatients reported higher clarity of information (3.73 vs 3.62 out of 4, p < 0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with AI-assisted conversations compared to standard care, while showing equivalent levels of trust and perceived empathy. The high opt-in rate (81% among respondents) exceeded previous benchmarks for AI acceptance in healthcare. Physician oversight ensured safety, with 95% of conversations rated as \u201cgood\u201d or \u201cexcellent\u201d by general practitioners experienced in operating a medical advice chat service.\nOur findings demonstrate that carefully implemented AI medical assistants can enhance patient experience while maintaining safety standards through physician supervision. This work provides empirical evidence for the feasibility of AI deployment in healthcare communication and insights into the requirements for successful integration into existing healthcare services.", "sections": [{"title": "1 Introduction", "content": "Globally, persistent shortages and inequitable distribution of the health workforce contribute to decreased access to health services and poorer quality of care. Projections indicate a shortage of 10 million health workers worldwide by 2030 [1]. Countries across Europe are facing shortages in primary care physicians, aggravated by aging populations and increased chronic disease burden [2]. Regional disparities are particularly pronounced, with urban areas generally having higher physician densities than rural regions [3, 4]. Studies report deteriorating access to care, especially in these underserved areas, leading to increased workloads and burnout among practitioners [5]. Physician burnout is associated with reduced engagement and lower quality of care [6]. The limited availability of primary care services not only restricts access to preventive and routine care, but also creates additional strain on emergency services, ultimately degrading the overall quality of care [2].\nWhile the successful deployment of machine learning and Artificial Intelligence (AI) in healthcare settings is not new, these technologies are typically not directly engaged in patient care and communications. Their functions have been largely reserved for expert use in signal processing, predictive analytics, medical image analysis, and medical devices innovations [7, 8, 9].\nRecent advances in general-purpose large language models (LLMs) and generative AI have opened new opportunities for healthcare applications, particularly through conversational AI agents optimized for medical use [10]. Such agents can serve a number of critical roles fundamental to a patient's care, health literacy, coordination, and management. By directly answering patients' medical questions more readily, collecting relevant diagnostic information, and facilitating patient-provider communication, they could help address the growing challenges in access and quality of care. This potential has prompted active research into the safety, accuracy, and effectiveness of conversational Al agents in healthcare settings.\nRetrospective and modeling analyses show that AI agents perform increasingly well on metrics evaluating diagnostic accuracy, answers to patient-directed medical questions, knowledge recall, and medical reasoning [10, 11, 12, 13]. In Tu et al. (2024), AMIE (Articulate Medical Intelligence Explorer), an LLM-based AI system optimized for clinical history-taking and diagnostic dialogue, demonstrated greater diagnostic accuracy and superior performance compared to physicians in simulated consultations with patient actors [10]. Evaluating the safety and performance of patient-facing conversational AI agents in a real-world setting is among the next steps forward.\nAlan, a health and insurance company operating in France, Belgium, Spain, and Canada, has offered a medical chat advice service to its members since 2020. Using the Alan mobile app, any Alan member can ask a question directly to an on-call physician through the privacy-compliant chat. In 2024, Alan introduced Mo, an LLM-based conversational agent, to this medical advice chat service staffed by its general practitioners.\nIn this study, we present our findings from this experiment in introducing conversational AI into medical practice.\nOur primary contributions are:\n\u2022 We introduced Mo, a patient-facing medical agent designed as an AI system. To this end, we developed a comprehensive evaluation framework combining clinical knowledge and reasoning assessment, real-world conversation analysis, and automated testing through simulated patient interactions.\n\u2022 We integrated Mo into a pre-existing medical advice chat service, with a focus on ethical design for patients, physician oversight, and quality assurance.\n\u2022 We ran a randomized controlled experiment, collecting data over 3 weeks to compare patient satisfaction and experience between conversations when Mo was proposed and a control group of patients that interacted solely with human physicians. The experiment highlighted that overall satisfaction and perceived clarity were higher in conversations with Mo, while trust in the received information and perceptions of empathy were similar between the two groups. We also show that patient engagement is higher in conversations with Mo, evidenced by shorter response times from patients.\n\u2022 We evaluated safety and medical accuracy through physician reviews. 95% of the conversations were assessed as \"good\" or \"excellent\u201d, while no conversation was considered as potentially dangerous overall.\n\u2022 Finally, we discussed the implications of our findings for the broader adoption of AI in healthcare, focusing on patient empowerment, access to care, and the evolution of healthcare delivery models."}, {"title": "2 Mo, an LLM-based medical conversational agent deployed in Alan's medical chat", "content": null}, {"title": "2.1 Context", "content": "Alan is a health and insurance company established in 2016 and headquartered in Paris, France. With operations across France, Belgium, Spain, and Canada, Alan provides health coverage for approximately 700,000 members as of October 2024. To accomplish its mission of making health simpler, transparent, and accessible for all of its members, the company designs, develops, and releases innovative digital products for the personalized use of its members. This capacity is built on Alan's dual expertise in technology (i.e., software engineering and research) and healthcare, allowing the company to build digital solutions that serve members' health needs.\nIn 2020, Alan introduced a medical advice chat service as a way to enhance its product and service offerings for its members. Using the Alan mobile app, members can directly contact a general practitioner or specialist physician to receive answers to their medical questions during extended hours (from 7 am to 12 am, seven days a week). The medical advice chat service is fully compliant with health privacy regulations in France and the European Union (EU), and uses end-to-end encryption for the messages between members and physicians.\nBetween January 1 and October 1, 2024, Alan's medical advice chat service facilitated over 58,000 conversations between members and health professionals. These conversations were split between general practitioners (62%) and other healthcare professionals specializing in physiotherapy, nutrition, gynecology, pediatrics, dermatology and sexual health. At the beginning of the study, general practitioners (GPs) had been operating the service for an average of 2.8 years (range: 0.8 - 4.0). Towards supporting the doctors operating the service, Alan introduced an LLM-based conversational agent into its medical advice chat service over the summer of 2024."}, {"title": "2.2 Developing Mo, an LLM-based Medical Conversational Agent", "content": null}, {"title": "Objective", "content": "The objective of Alan's conversational AI agent, called Mo, is to provide users (i.e., patients) with clear, appropriate, and actionable responses to their medical and healthcare questions. Achieving this objective requires the agent to effectively acquire information from the user, analyze the information, and formulate a reliable response and recommendation grounded in sound medical knowledge and reasoning - all while maintaining positive rapport and trust."}, {"title": "A Multi-Agent Aystemic Approach", "content": "Rather than a single, standalone LLM, the agent behind Mo is an LLM-based Al system, consisting of several sub-agents (i.e., LLMs) that run in parallel. This multi-agent systemic approach allows Mo to use the best model for each specific task, integrating the strengths of different models within the system [14, 15, 16]. Multi-agent systems are particularly relevant for tasks requiring deep, specialized knowledge of multiple domains as well as high accuracy and performance, as is characteristic of medicine and healthcare.\nUsing a multi-agent development framework, Mo leverages several models initially developed by OpenAI, Anthropic, and Mistral AI. The models are served by Microsoft Azure and Google Cloud Platform (GCP) in compliance with EU privacy regulations and French health data protection requirements (HDS certification). Leveraging the existing capabilities of these models for healthcare applications requires extensive tailoring and optimization. A robust evaluation process determines which models perform best for each task and under which circumstances."}, {"title": "Design Process and Offline Evaluation", "content": "To design Mo's AI system architecture and select its constituent LLMs, we developed a comprehensive offline evaluation framework. The selection process for individual models was guided by core capabilities: medical knowledge, reasoning, and communication style, alongside operational requirements of speed, privacy compliance, and available capacity. We developed three critical assets for offline evaluation: (i) a clinical knowledge and reasoning benchmark, (ii) anonymized past conversations from the medical advice chat, and (iii) simulated conversations with patient agents.\nClinical knowledge and reasoning benchmark. To evaluate single models on medical knowledge and clinical reasoning, we developed a benchmark focused on French medical practice and guidelines. We extracted 800+ multiple-answer closed questions from the French national exam used to match medical school graduates to residency programs and specialties. We submitted all models to this benchmark and used their performance to inform whether and how to use them in the larger AI system.\nReal-world medical advice conversations. The agent's goal is to provide reliable and informed replies to patients' questions. To test this, we curated a proprietary dataset of anonymized conversations conducted on Alan's medical advice chat service. We truncated dialogues at points where a GP was expected to respond and submitted the unfinished conversations to the agent to test its subsequent response (see Figure 1). A physician reviewed the agent's proposed messages to evaluate behavior, tone, and content accuracy at specific points in the conversation. While this method effectively assessed the quality of individual responses, it couldn't capture the agent's ability to drive full conversations independently. In particular, it didn't evaluate how well the agent could proactively gather the information needed to make sound medical assessments and recommendations.\nSimulated conversations with patient agents. To address this limitation, we developed a method to evaluate complete end-to-end conversations between patients and the agent. We implemented a separate LLM-based agent designed to emulate patients in chat conversations (see Figure 1). This patient agent operates based on \"patient cards\": structured inputs that define the simulated patient's demographic characteristics, medical history, underlying medical condition and contextual information. In order to represent a range of patient communication styles and personalities, the patient card also directed how the simulated patient should behave during the exchange. This allowed evaluation of the agent in an end-to-end setup that closely mimicked reality. Simulated conversations assessed the agent's ability to gather relevant information, drive the dialogue, and issue reliable and appropriate recommendations. This approach also allowed us to over-represent rare or yet unseen cases, thereby evaluating the agent's behavior in difficult scenarios and a wide range of emergency situations."}, {"title": "2.3 Integrating Mo into the medical advice chat service", "content": "A product team of engineers, designers, doctors, and user researchers collaborated to integrate Mo into the medical advice chat service in a safe, intuitive, and transparent way. Mo was deployed between 9 am and 11 pm for conversations addressed to GPs in France, with patients who consented to automated treatment of their data.\nEthical Compliance\nWe established comprehensive guidelines to ensure ethical compliance. We anticipated the entry into force of the EU AI Act [17], augmenting its recommendations to ensure responsible implementation and a transparent interface that patients can easily understand.\nTo ensure responsible AI deployment, we implemented the following safeguards: (1) timely human review consisting in physician oversight (2) explicit and implicit (e.g., color of text bubbles) differentiation between AI agents and human actors, (3) consent collection for health data processing using LLMs, (4) requiring positive action for interaction with Mo (see Figure 2b), and (5) clearly limiting the scope of conversations for which Mo can operate. For example, in cases of psychological emergency, Mo was inactivated."}, {"title": "Physician Oversight", "content": "Mo operates under the supervision and responsibility of the physicians of the medical advice chat service.\nPhysician-agent interface. GPs have the authority and capability to stop Mo and intervene during any patient-agent conversation, regardless of whether Mo is composing a message or waiting for the patient to reply. Mo never resumes the conversation once stopped. The GP is required to check in with the patient after the exchange between Mo and the patient is complete.\nMessage review. As a conversation between a patient and Mo unfolds, a GP assigned to the conversation is required to review each message from Mo within 15 minutes. GPs can hide Mo's messages when necessary. Hiding a message requires the GP to take over the discussion, and displays the message in a \u201chidden\u201d state to the patient while keeping it visible to the GP. In cases of urgency, GPs can immediately establish direct contact with patients using their provided contact information.\nGeneral conversation review. If Mo has been involved in a conversation, the assigned GP must perform a general review. This review consists of examining the complete Mo-patient dialogue to evaluate the medical advice provided and identify any potential gaps or concerns. The GP then documents their assessment and engages directly with the patient for a mandatory check-in to confirm their oversight, validate Mo's medical recommendations, provide complementary guidance when needed, and address any remaining questions (see Figure 2b)."}, {"title": "Staged Roll-out and Quality Assurance", "content": "Mo's deployment progressed through three sequential stages over a four-month period ending in October 2024. The first stage limited access to Alan employees only, allowing for initial validation. The service was then extended to a small proportion of Alan members under the supervision of GPs selected and trained to support Mo's development. Finally, access was expanded to 50% of members with oversight from all GPs of the medical advice chat service after they received specific training. Each stage lasted as long as necessary to reach defined safety and stability milestones.\nThroughout the integration, a team of physicians and engineers continuously monitored safety and stability metrics established during development, enabling data-driven improvements while maintaining rigorous quality standards."}, {"title": "3 Methods", "content": null}, {"title": "3.1 Study Design", "content": "We conducted a randomized controlled experiment to evaluate the effect of Mo, our LLM-based conversational agent, on patient experience. Of all conversations where Mo was activated, only those considered in scope were eligible to have Mo engage with the patient. From this pool of eligible conversations, Mo was proposed to a random 50% sample of patients to comprise the treatment group. The remaining eligible conversations, where Mo was not proposed, served as the control group. We evaluated patient experience across three domains: (i) overall satisfaction, (ii) quality metrics (clarity, trust, and empathy), and (iii) engagement metrics (response patterns).\nIn addition to assessing patient experience, we evaluated Mo's safety and medical accuracy from the physician message and general conversation reviews.\nData was prospectively collected from September 30 to October 20, 2024."}, {"title": "3.2 Outcome measures", "content": "We developed questionnaires to evaluate both the patient experience of conversations with Mo and the physician assessments of safety and accuracy of Mo's responses. To do so, we surveyed existing standards for evaluation of patient-doctor interactions (PACES exam [18], GMC Patient Questionnaire [19], Best Practice for Patient Centered Care [20]) and extracted core information on our specific domains of interest. We differentiated between patient-related outcomes to be reported by the patient and medical assessment to be conducted by a physician, while considering constraints in length and user experience to maximize completion rate.\nPatient Ratings\nFollowing each conversation, patients were asked to rate their experience across four dimensions: overall satisfaction, clarity, trust, and empathy (see Table 1, Supplementary Figure S1). Information on patient satisfaction was captured using a 5-point Likert scale and free text. Clarity, trust, and empathy were assessed using a 4-point Likert scale."}, {"title": "Statistical Analysis", "content": "We compared distributions of patient and GP ratings using the Wilcoxon test. Demographic comparisons were conducted using Student's t-test for age and chi-squared test for gender.\nWe excluded from the study all conversations with attachments (document, picture) and conversations with Alan employees.\nData from conversations requesting unavailable services (prescriptions, sick leave certificates, or medical certificates) were excluded from the patient experience analysis.\nAll statistical analyses were conducted using R version 4.3.1."}, {"title": "Data Privacy and Consent for Research Use", "content": "All members included in this study were informed of the use of aggregated and/or anonymized data for research and statistical purposes in Alan's Privacy Policy. This privacy policy specifies that data collected by Alan may be utilized for scientific research in a manner compatible with the original purpose of collection, ensuring that all data analyzed remains non-identifiable and protects individual privacy. Additionally, members who used this specific service provided explicit consent through a dedicated consent screen for the automated processing of their health data using LLM technology."}, {"title": "4 Results", "content": null}, {"title": "4.1 Sample Profile", "content": "Over the study period, 1,566 conversations were initiated in Alan's medical advice chat service during Mo's active hours (Figure 4). Mo deemed 640 conversations (41%) out of scope, due to questions that contained insurance or administrative matters or signs of mental health distress that, by established protocols, required human intervention."}, {"title": "4.2 Patient Experience", "content": "Patient ratings were available for 20% of eligible conversations. Ratings were more prevalent in the control group (24% vs 17%), and demographic characteristics were comparable between the two groups (mean age difference: 1.6 years [95% CI: -0.8 to 3.9]; difference in female proportion: -3% [95% CI: 11% to 17%]).\nMo received higher general satisfaction scores compared to the control group (mean: 4.58 vs 4.42 out of 5, p < 0.05) (Figure 5). Both treatment and control groups showed similar ratings for trust (mean: 3.63 vs 3.65 out of 4) and empathy (mean: 3.72 vs 3.70 out of 4). However, Mo achieved significantly higher clarity ratings (mean: 3.73 vs 3.62 out of 4, p < 0.05).\nNotably, extremely low ratings (score of 1) were rare. Mo received only one such rating across all dimensions, and the control group received one rating of 1 for empathy only. A detailed analysis of all ratings below 3 (n = 8) revealed no systematic patterns of dissatisfaction (Supplementary Table S1)."}, {"title": "4.3 Patient Engagement", "content": "We analyzed conversation dynamics by measuring response times for each turn of dialogue between participants (Figure 6). In the control group, these turns were exclusively between patients and GPs, while in the Mo group, turns included both Mo-patient and GP-patient interactions.\nAs expected, since Mo responds almost instantaneously, response times from providers differed significantly (median: 0.2 vs 4.8 minutes, p < 0.001). Interestingly, this difference in provider response times was accompanied by a change in patient behavior: in conversations with Mo, patients also responded more quickly compared to control conversations (median: 1.1 vs 2.8 minutes, p < 0.001)."}, {"title": "4.4 Safety and Medical Accuracy", "content": "GPs supervising the medical advice chat service evaluated Mo's performance at both message and conversation levels (Figure 7). At the message level, supervising GPs reviewed each of Mo's responses within 15 minutes of sending. Among 1,265 messages sent by Mo, 95% were rated positively, while 45 messages (3.6%) were rated as \u201cpoor\" and 3 messages were hidden from patients. No harm resulted from the messages that were subsequently hidden from patient view.\nFollowing the completion of each conversation, GPs provided an overall assessment. For completed conversations (n=298), 95% received positive ratings (\u201cgood\u201d or \u201cexcellent\") for overall performance, with similar distributions for question quality (96%) and advice appropriateness (94%). No conversation was deemed potentially dangerous overall.\nIn the assessment of medical accuracy, 95% of conversations contained no inaccuracies, with one conversation flagged for the presence of potentially dangerous inaccuracies.\""}, {"title": "5 Discussion", "content": "This study presents the first large-scale evaluation of a physician-supervised LLM-based conversational agent in a real-world medical setting. By integrating Mo into an existing medical advice chat service, we demonstrated that AI-assisted conversations achieved comparable or superior patient experience while maintaining robust safety standards under physician oversight. Notably, patients reported higher information clarity and overall satisfaction when interacting with Mo compared to standard care, while showing equivalent levels of trust and perceived empathy. General practitioners with extensive experience in medical chat services assessed 95% of Mo's conversations as good or excellent. Together, these findings from both patients and physicians suggest strong potential for AI augmentation in healthcare communication."}, {"title": "5.1 Bridging AI Research and Clinical Practice", "content": "The transition from AI research to clinical implementation represents a critical frontier in healthcare innovation. This section examines the current landscape and contextualizes our contributions within existing literature."}, {"title": "Evaluation of Large Language Models tailored to the medical field", "content": "Substantial effort has focused on developing and evaluating LLMs specifically trained for the health domain (e.g., Med-Palm [12], DrBert [21]). While these studies demonstrated promising capabilities on medical knowledge benchmarks, their evaluations primarily employed objective closed-question assessments that do not fully capture the complexities of patient interactions. In a related direction, Ayers et al. (2023) retrospectively demonstrated superior quality and empathy of LLM responses compared to those coming from physicians on a public forum, though this baseline may not reflect professional medical care [22]."}, {"title": "AI-driven Clinical Decision-Making", "content": "The development of large-scale symptom assessment systems for disease diagnosis and patient triage marks a significant advancement in AI-driven healthcare. The large study (n=102,059) of Zeltzer et al. (2023) demonstrated the potential for AI to enhance primary care triage [11]. However, these systems typically operate within narrowly defined parameters of structured symptom assessment, leaving unexplored the broader range of medical queries that arise in primary care settings.\nThe research conducted by Hager et al. (2024), analyzing 2,400 cases of abdominal pathology, revealed that LLMs had notably lower diagnostic accuracy compared to human physicians [23]. Although newer proprietary LLM versions and multi-agent systems might improve these results, their findings advocate for a supervised integration of LLMs in clinical practice (healthcare professional oversight, continuous validation, ongoing research) as complementary tools rather than fully autonomous systems."}, {"title": "Simulated Clinical Interactions", "content": "The AMIE system represents a major step forward in patient-facing medical AI, showing superior diagnostic accuracy and performance in clinical dialogue [10]. Their robust evaluation framework, including a double-blind comparison with physicians, provides valuable insights. However, the study's limitations should be noted: it was conducted in a simulated environment with patient actors, and the participating physicians were new to chat-based consultations, potentially failing to reflect the expertise of clinicians experienced in digital healthcare delivery."}, {"title": "Limited-Scale Real-World Applications", "content": "Several studies have explored real-world deployments of conversational AI agents in specific healthcare contexts, including postoperative recovery ([24], n=26), older adult patient-provider communication ([25], n=19), and loneliness mitigation ([26], n=34). While these studies consistently report improved patient satisfaction and reduced provider workload, their limited sample sizes constrain broader generalization."}, {"title": "5.2 Understanding Patient Experience: Satisfaction, Trust, and Engagement", "content": null}, {"title": "Implications for Healthcare Delivery", "content": "Building on these promising but limited pilots, our study presents the first large-scale deployment of an AI medical assistant in a real-world healthcare setting, with close to 300 completed patient conversations. Our findings on patient satisfaction merit careful interpretation within the broader context of healthcare delivery. Patient satisfaction is a crucial prerequisite for broader acceptance and adoption of AI in healthcare. The comparable or superior satisfaction ratings achieved in conversations with Mo indicates the feasibility of AI deployment in clinical settings. This acceptance could enable significant reconfiguration of healthcare delivery systems, potentially allowing for more efficient allocation of human medical expertise while maintaining or improving access to care. Specifically, AI agents could evolve into daily health companions, fundamentally shifting healthcare from episodic interventions to continuous support, where patients are empowered to better understand and manage their health journey, while being efficiently connected to physician expertise when needed."}, {"title": "Dimensions of Patient Satisfaction", "content": "The granular analysis of satisfaction metrics reveals important nuances in patient experience. The significantly higher clarity ratings suggest that AI-assisted communications may excel at providing clear, structured information, aligning with previous findings that standardized communication approaches can enhance patient understanding [27].\nThe equivalent ratings for trust and empathy warrant particular attention. Unlike studies where raters were unaware of AI involvement (e.g., [10, 22]), our transparent setup explicitly identified Mo as an AI agent. Previous research on AI interactions suggests that perceived humanness increases feelings of trust and empathy [28, 29]. Therefore, the comparable ratings are especially significant given that knowledge of Mo's AI status could have influenced patient expectations. Two factors likely contributed to maintaining trust despite transparent AI use: Mo's consistent responsiveness and structured communication style, and our protocol ensuring that a physician personally engages with the patient at the end of each conversation."}, {"title": "Patient Engagement and Communication Dynamics", "content": "Analysis of conversation dynamics revealed intriguing patterns in patient engagement. Mo's nearly instantaneous responses were associated with faster patient response times, suggesting more fluid and engaged conversations. Beyond mere efficiency, these accelerated exchanges could fundamentally improve healthcare delivery. Fluid dialogue leads to more comprehensive information gathering, while rapid response times could lower the barrier to seeking medical advice, encouraging patients to address health concerns earlier. The combination of AI responsiveness and physician oversight creates a new model where patients benefit from both immediate attention and expert medical judgment. This finding aligns with previous research showing that reduced response latency can enhance user engagement and satisfaction in healthcare communications [25, 30].\nThe high opt-in rate (81% among respondents) indicates strong patient acceptance of AI-assisted healthcare services, setting a higher benchmark for user acceptance than previously suggested in the literature [31, 32]. Through user interviews, we identified three factors potentially contributing to this success: (i) members' trust in Alan, built over time (ii) an iteratively refined user experience, and (iii) an emphasis on transparency.\nThese findings suggest that successful integration of AI in healthcare services depends not only on technical capabilities but also on careful attention to user experience, institutional trust, and transparent implementation practices. The results demonstrate that when properly implemented, AI-assisted healthcare services can achieve high levels of patient acceptance while maintaining high quality standards in medical communication."}, {"title": "5.3 Ethical, Privacy, and Safety concerns of AI-based Communication Systems for Health", "content": "From a safety perspective, the results of our study are encouraging yet warrant careful consideration. While 95% of Mo's messages received positive physician reviews and only three messages (out of 1,265) required intervention, the few cases where mitigation was required by the supervising GP confirms the need for physician oversight in this setup and continued research. In particular, extended data collection will allow observation of a broader range of rare cases that may elicit inappropriate responses from the agent.\nEarlier studies emphasized several prerequisites for deploying patient-facing AI systems in healthcare: stringent quality control measures, sufficient guardrails, adequate oversight by qualified physicians, ethical design and development, as well as strict adherence to privacy regulations and informed consent procedures [30, 33, 34, 35]. The integration of Mo in Alan's medical advice chat demonstrates a practical realization of these requirements in a real-world healthcare setting.\nThe following steps were critical in ensuring its reliability. First, we established comprehensive offline evaluation procedures, comprising of: (i) the constitution of an internal closed-questions benchmark, tailored to the needs relevant to the deployment of the agent, and unlikely to be used in the prior training of the LLMs we use, (ii) the use of anonymized past conversation data representative of the specific task, and (iii) the development of an automated conversation evaluation framework involving patient agents. Second, we carefully integrated the agent in the final product, insisting on (i) the thoughtful design of the interaction between the physician and the agent, prioritizing physician oversight and leveraging user experience to elicit the right actions (e.g., timely message review), and (ii) a staged rollout to enable learning and iterations before full-scale implementation.\nThis study was made possible by two critical aspects of our development process. First, we build upon a pre-existing medical service. Second, the agent and its integration into the patient-facing product were developed by a multidisciplinary team that included a dedicated GP, aligning with recommendations made by others [36]."}, {"title": "5.4 Study Limitations", "content": "This real-world evaluation, while providing valuable insights, has several important limitations. First, the three-week duration of our study may not capture the full range of medical presentations. Seasonal variations in health issues could be underrepresented, and longer-term patterns in patient-AI interactions remain to be explored. More importantly, this sample size, though substantial for an initial deployment, may not be sufficient to detect rare but significant safety issues that could emerge in broader medical practice.\nThe evaluation of patient experience was constrained by our survey response rate of 20%. While this rate is typical for embedded product surveys, it introduces potential selection bias in our satisfaction metrics. Despite finding no significant demographic differences between respondents and non-respondents, there may be unmeasured factors influencing survey participation that correlate with patient satisfaction.\nOur study scope was also limited in several practical ways. We restricted Mo's deployment to general practitioner conversations, excluding consultations with other specialists, which might present different challenges. The exclusion of conversations requiring document review or image analysis, while necessary for our initial deployment, leaves important use cases unexplored. Additionally, as the study was conducted within a single healthcare system with an established digital presence, our findings about patient acceptance may not generalize to other healthcare contexts, particularly those without pre-existing patient trust in digital services."}, {"title": "5.5 Future Research Priorities", "content": "Our study demonstrates the potential of AI-assisted medical communication, while highlighting key areas for future research."}, {"title": "Clinical Impact Studies", "content": "Building on our initial safety and satisfaction findings, longer-term studies should examine how AI assistance affects healthcare delivery and outcomes. Critical questions include the impact on patient health-seeking behavior, the quality of preventive care, and physician workload and burnout. Particularly important is understanding how AI assistance influences the patient journey through the healthcare system, including timely specialist referrals and follow-up care."}, {"title": "Healthcare System Integration", "content": "Deeper integration into healthcare workflows presents both opportunities and challenges. Research should focus on optimizing the collaboration between AI systems and healthcare professionals, establishing efficient oversight models, and developing protocols for seamless care transitions. This includes studying how AI can enhance rather than disrupt existing care pathways, and identifying best practices for maintaining quality while improving healthcare access and efficiency."}, {"title": "Technical Evolution", "content": "Several technical advances could expand the system's utility in clinical practice. Integration with electronic health records would provide richer context for patient interactions, while capabilities for handling medical documents and images would enable more comprehensive care support. Continued research into improving the handling of complex medical presentations and rare conditions remains essential for reliable deployment at scale."}, {"title": "6 Conclusion", "content": "Our findings demonstrate the feasibility and far-reaching potential of AI-assisted medical communication, while highlighting the importance of careful implementation and oversight. The success of this implementation relied heavily on the integration of medical expertise throughout development, robust privacy protections, and continuous safety monitoring. While results are promising, longer-term studies with larger sample sizes are needed to fully understand the impact of AI-assisted medical communication on healthcare delivery, access and quality of care, and patient outcomes."}]}