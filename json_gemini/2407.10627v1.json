{"title": "Arena Learning : Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena", "authors": ["Haipeng Luo", "Qingfeng Sun", "Can Xu", "Pu Zhao", "Qingwei Lin", "Jianguang Lou", "Shifeng Chen", "Yansong Tang", "Weizhu Chen"], "abstract": "Assessing the effectiveness of large language models (LLMs) presents substantial challenges. The method of conducting human-annotated battles in an online Chatbot Arena is a highly effective evaluative technique. However, this approach is limited by the costs and time required for human annotation. In this paper, we introduce Arena Learning, an innovative offline strategy designed to simulate these arena battles using AI-driven annotations to evaluate battle outcomes, thus facilitating the continuous improvement of the target model through both supervised fine-tuning and reinforcement learning. Arena Learning comprises two key elements. First, it ensures precise evaluations and maintains consistency between offline simulations and online competitions via WizardArena, a pipeline developed to accurately predict the Elo rankings of various models using a meticulously designed offline test set. Our results demonstrate that WizardArena's predictions closely align with those from the online Arena. Second, it involves the continuous improvement of training data based on the battle results and the refined model. We establish a data flywheel to iteratively update the training data by highlighting the weaknesses of the target model based on its battle results, enabling it to learn from the strengths of multiple different models. We apply Arena Learning to train our target model, WizardLM-\u03b2, and demonstrate significant performance enhancements across various metrics. This fully automated training and evaluation pipeline sets the stage for continuous advancements in various LLMs via post-training. Notably, Arena Learning plays a pivotal role in the success of WizardLM-22, and this paper serves both as an exploration of its efficacy and a foundational study for future discussions related to WizardLM-2 and its derivatives.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of natural language processing (NLP) has witnessed a remarkable transformation, driven by the rapid advancements in large language models (LLMs). These models, trained on vast amounts of text data, have demonstrated an exceptional ability to understand, generate, and interact with human language in a wide range of tasks [1-3]. One of the most exciting applications of LLMs has been in the realm of conversational AI [4\u20138], where they have been utilized to create powerful chatbots capable of engaging in naturalistic dialogues. One of the key factors contributing to the success of LLM-powered chatbots is the ability to leverage large-scale high-quality instruction following data for effective post-training [9-13]. By exposing these models to a diverse range of conversational tasks and instructional scenarios, researchers have been able to imbue them with a deep understanding of how to effectively communicate and assist humans.\nWith the rapid implementation of various large model applications and the reduction of inference costs, the interest and demand from businesses and consumers in using large language model services have increased rapidly. As shown in the Figure 1, just the OpenRouter platform will process more than 60B tokens every day. At the same time, with the innovation and deepening of application scenarios, this requires those models to continue to evolve to adapt to the user's new intentions and instructions. Therefore, building an efficient data flywheel to continuously collect feedback and improve model capabilities has become a key direction for next generation AI research.\nIn this context, the emergence of the LMSYS Chatbot Arena [14, 15] has been a significant development. This is a platform that facilitates the assessment and comparison of different chatbot models by pitting them against each other in a series of conversational challenges and rank with Elo rating system [16]. By leveraging a diverse set of human evaluators, the Chatbot Arena provides a more robust and comprehensive evaluation of chatbot performance, going beyond the limitations of traditional benchmarking approaches. At the same time, it also opened up some real direct chat and battle preferences data [17], which have been proven to be valuable resources for model post-training and developmental guidance [18]. However, the human-based evaluation process poses its own challenges: Manually orchestrating and waiting the interactions between chatbots and human evaluators can be time-consuming and resource-intensive, limiting the scale and frequency of evaluation and training data opensource cycles. On the other hand, due to their priority limitations [19], most models are unable to participate in arena evaluations, and the community can only obtain 10% of the chat data at most, making it hard to directly and efficiently guide the development of the target model based on this Arena. Therefore, the need for a more efficient and scalable arena-based pipeline to chatbot post-training and evaluation has become increasingly pressing.\nTo address these challenges, this paper introduces a novel approach called Arena Learning, which is a training and evaluation pipeline fully based on and powered by AI LLMs without human evaluators."}, {"title": "2 Approach", "content": "In this section, we elaborate on the details of the proposed Arena Learning. As illustrated in Figure 2, the closed loop pipeline mainly contains three components: Offline Pair-wise LLM Battle Arena, Iterative Post-training, and Model Evaluation."}, {"title": "2.1 ChatBot Arena and Elo Ranking", "content": "The Chatbot Arena is a pioneering platform that has revolutionized the way chatbot models are evaluated and compared. It facilitates the assessment of different chatbot models by pitting them against each other in a series of conversational challenges. At the core of this Arena lies the concept of Elo rankings, a widely adopted rating system originally devised for chess players. Elo rankings [16] are used to quantify the relative performance of chatbot models based on a series of head-to-head battles. Each model is initially assigned an arbitrary Elo rating, which is then updated after every battle based on the outcome (win, loss, or tie) and the rating difference between the competing models. If a higher-rated model defeats a lower-rated one, its Elo rating increases slightly, while the loser's rating decreases by a corresponding amount."}, {"title": "2.2 Using a Powerful LLM as Judge to Simulate Human Annotators", "content": "At the core of the simulated arena battles in Arena Learning lies a powerful LLM that serves as the 'judge model\u201d. This judge model is specifically prompted and adjusted by us on a diverse range of conversational pair data, enabling it to evaluate the quality, relevance, and appropriateness of the models' responses objectively and consistently. The judge model's role is to analyze and compare the responses provided by the pair battle models for each conversational sample. Specifically, to assess the response quality of each LLM, we use prompt engineering with the Llama3-70B-Chat model [22]. The inputs are dialogue history, user instruction, and the responses of two LLMs. The outputs consist of scores for each LLM, along with explanations focused on various factors, such as coherence, factual accuracy, context-awareness, and overall quality, to determine whether one response is superior to the other. To mitigate potential position bias [14, 23, 24], we employ a two-game setup, alternating the positions of the two LLMs. Each model receives an overall score on a scale of 1 to 10, where a higher score reflects superior overall performance. Following, we will use this \"judge\" model in both Arena Learning post-training and WizardArena evaluation stages."}, {"title": "2.3 Build a Data Flywheel to Post-train LLMs", "content": ""}, {"title": "2.3.1 Collect Large-Scale Instruction Data", "content": "To facilitate leveraging the simulated arena battles among models to train WizardLM-B, Arena Learning relies on a large-scale corpus of conversational data D. The data collection process involves several stages of filtering, cleaning, and deduplication to ensure the quality and diversity of the instruction data. The simulated arena battle outcomes are then used to generate training data for the WizardLM-B, tailored to different training strategies: supervised fine-tuning (SFT), direct preference optimization (DPO), and proximal policy optimization (PPO). We split the data equally into some parts D = {Do, D1, D2, ..., DN} for following iterative training and updates respectively."}, {"title": "2.3.2 Iterative Battle and Model Evolving", "content": "Arena Learning employs an iterative process for training and improving the WizardLM-\u03b2. After each round of simulated arena battles and training data generation, the WizardLM-8 is updated using the appropriate training strategies (SFT, DPO, and/or PPO). This updated model is then re-introduced into the arena, where it battles against the other SOTA models once again. This iterative process allows the WizardLM-B to continuously improve and adapt to the evolving landscape of the arena. As the model becomes stronger, the simulated battles become more challenging, forcing the WizardLM-B to push its boundaries and learn from the latest strategies and capabilities exhibited by the other models.\nAdditionally, the iterative nature of Arena Learning enables the researchers to monitor the progress and performance of the WizardLM-\u1e9e over time, providing valuable insights into the effectiveness of the different training strategies and potential areas for further improvement or refinement.\nThe following is the first training iteration 11: Before that, we first train the initial version of WizardLM-B-SFT-10 with Do, then select some other state-of-the-art LLMs M which ranking top on WizardArena testset, following we let WizardLM-B-SFT-Io as the competitor model, and battle with M on D\u2081, and focus on extracting instances where the WizardLM-\u1e9e's response is considered inferior to the winning model's response, as determined by the judge model. These instances are collected, and the winning model's response is used as the target output for fine-tuning the next WizardLM-B-SFT-11 model. For DPO, we use WizardLM-B-SFT-I\u2081 as competitor to battle with M on D2, and then we treat win and loss responses as the < choice, reject > pairs to training the WizardLM-B-DPO-11. For PPO, we leverage the same battle process between WizardLM-B-DPO-11 and M on D3 to obtain the < choice, reject > pairs to train the reward model and WizardLM-B-PPO-11. In the second training iteration I2, we select the best WizardLM-B-PPO-I\u2081 on the WizardArena as the initial competitor model of I2, and adopt similar process to train next SFT, DPO, and PPO models."}, {"title": "2.4 Evaluate LLMs with WizardArena", "content": "To accurately evaluate the performance of chatbot models and predict their Elo rankings, Arena Learning relies on a carefully curated offline test set, which is designed to strike a balance between diversity and complexity [14, 24, 25], ensuring a comprehensive assessment of the models' capabilities across a wide range of conversational scenarios. Inspired by WizardLM [11] In-Breadth Evolving and In-Depth Evolving, we construct the following two subsets:\nDiverse Subset The diverse subset of the test set is constructed to capture a broad range of topics, styles, and conversational contexts. To achieve this, we employs text clustering techniques on a large corpus of instructions and conversational data. The clustering process begins by representing all the instructions in a conversation as a high-dimensional vector using state-of-the-art embedding models (i.e., gte-large [26]). These vectors capture the semantic and contextual information within the text, enabling the clustering algorithm to group similar samples together. Once the clustering is complete, we selects a representative sample from each cluster, ensuring that the diverse subset of the test set capture a broad range of scenarios. This approach helps to mitigate potential biases or blindspots that may arise from relying solely on simply random sampling.\nHard Subset This subset is specifically designed to challenge the capabilities of even the most advanced chatbot models. To construct this subset, we leverages the power of LLMs to predict the difficulty level of each instruction. We then selects the top-ranking samples according to the predicted difficulty scores, ensuring that the hard subset of the test set comprises the most challenging and complex scenarios. This data serves as a rigorous benchmark for evaluating the robustness and capability of chatbot models in handling intricate and nuanced conversational tasks.\nWith the above \u201cjudge\u201d model and the offline WizardArena test set in place, we proceeds to evaluate the performance of various chatbot models through a series of pair-wise battles. The outcomes of the battles are then used to compute the Elo rankings of the participating chatbot models. WizardArena adopts the same Elo rating system used in LMSYS Chatbot Arena, which has proven effective in ranking players or entities based on their head-to-head performance."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Setup", "content": "Training Data. We random sample 10k ShareGPT data to train a initial model WizardLM-\u03b2-I0. We then collected some instructions from open available datasets [10, 11, 17, 27, 28], and optimized them using the following steps: first, we filtered out all illegal and toxic conversations; second, we removed conversations with instruction lengths of less than 10; third, we eliminated duplicate instructions with prefixes of 10; next, we employed the MinHashLSH technique [29] for data deduplication; subsequently, we used an embedding model gte-large [26] to exclude instructions from the top 5 matches in semantic similarity with benchmarks (i.e., WizardArena, Arena-Hard Auto [24], MT-Bench [14], AlpacaEval [25], OpenLLM Leaderboard [30\u201334]) to prevent test data leakage. Finally, we removed all non-English instructions. After completing these steps, we obtain the refined 276K dataset D, and randomly split it to 9 parts.\nOffline Diverse & Hard WizardArena test set. Firstly, we processed the source data using K-Means clustering into 500 categories. From each category, we randomly selected two samples to construct 1,000 diversity samples, named as the Offline-Diverse WizardArena. Additionally, 20 samples from each category were selected at random to form a data set of 10,000 entries, we then used GPT-4-1106-preview to rate each instruction on a difficulty scale from 0 to 10 in descending order, and selected the top 1,000 entries to create the hard test set, denoted as the Offline-Hard WizardArena. The Offline-Mix WizardArena combines the Diverse and Hard test sets in 2,000 samples. Different from Arena-Hard-v1.0 [24], which mainly focuses on single-turn dialogue data, WizardArena-Mix incorporates multi-turn dialogue data. Figures 4 and 5 display the distribution of dialogue turn and the categories statistics within WizardArena-Mix, respectively. The data indicates that our multi turn conversation data accounts for a large proportion, and the distribution of topics is also diverse.\nLLM Battle. We selected some popular models and conducted pairwise battles in the Offline-Mix WizardArena. Llama3-70B-Instruct [22] served as the \"judge\" model, with the higher-scoring model declared the winner. Following LMSYS Chatbot Arena, we adopt the Bradley-Terry model [35] to calculate the final ELO scores for each model. To mitigate potential position bias, we used a two-game setup, swapping the models between the first and second positions for each instance [23]. We use multiple bootstraps (i.e., 100), and select the median as the model's ELO score. The 95% CI is determined from the 2.5% to 97.5% range of confidence interval scores. Table 2 contrasts the differences between WizardArena and LMSYS Arena. WizardArena leverages LLM to conduct Battles, whereas LMSYS Chatbot Arena relies on human annotation. At the same battle count, if we use sixteen 80G GPUs for inference and judgement, the process will be completed in 9 days, achieving a 40x speedup increase compared to the 12 months required by LMSYS ChatBot Arena."}, {"title": "3.2 Offline WizardArena closely align with the Online LMSYS ChatBot Arena.", "content": "Figure 6 and Table 4 present the rankings for some popular models across several evaluation benchmarks: LMSYS ChatBot Arena-EN [19], MT-Bench [14], and WizardArena. The results reveal that employing the LMSYS ChatBot Arena as the reference benchmark in the real-world scenarios, WizardArena displays the good ranking consistency, however MT-Bench shows the large fluctuations. In addition, there is a significant difference in performance between WizardArena diverse and hard subsets: Vicuna-33B [9] and Qwen1.5-32B-Chat [7] are more effective in diverse tasks, while Tulu-2-DPO-70B [38] and Nous-Hermes-2-Mixt-DPO [39] achieves better results in hard tasks. We therefore use WizardArena-Mix as the final evaluation benchmark of Arena Learning to balance the strengths of different models.\nTable 3 illustrates that the Offline WizardArena-Mix significantly outperforms MT-Bench across several consistent metrics which refer to the Appendix A for details: a 19.87% higher Spearman Correlation, a 73.07% increase in Human Agreement with 95% CI, and a 74.57% improvement in Differentiation with 95% CI. It achieves an average consistency of 98.79% with the LMSYS ChatBot Arena by human judgment, outperforming Arena-Hard-v1.0 [24] by 10.91% and MT-Bench [14] by 55.84%. In contrast to MT-Bench and Arena-Hard-v1.0 which use proprietary models (i.e. GPT-4) as the judge model, our approach employs current SOTA open-source model Llama-3-70B-Chat, which not only has a significantly lower cost but also achieves strong consistency. Moreover, the Offline WizardArena-Mix, which integrates both Diverse and Hard test sets, achieves 0.87% higher average consistency compared to WizardArena-Diverse and 0.82% higher than WizardArena-Hard. This indicates that balancing diversity and complexity is crucial for the effective offline evaluation of large language models. Above results also further prove the feasibility of using the \u201cjudge\u201d model to judge the battles between LLMs and generate a large amount of post-training data in simulated arena."}, {"title": "3.3 Can Arena Learning build an effective data flywheel with post-training?", "content": "Table 4 demonstrates the impact of using the Arena Learning method to post-train WizardLM-B models during three data flywheel iterations, where Ii represents the i-th iteration. In each iteration from I\u2081 to I3, we always use 90k data for post-training. Starting from WizardLM-B-7B-I0, the next 3 iterations have improved by 343 points, 32 points, and 28 points on Wizardarena-Mix Elo, respectively. At the same time, the MT-bench score of this model has also achieved significant improvement (from 6.41 to 8.16). Specifically, the WizardLM-B-7B-I\u2081 even surpasses WizardLM-70B-v1.0 and the WizardLM-B-7B-I3 also shows comparable performance with Starling-LM-7B-Beta. It is worth noting that we have also observed the same trend on WizardLM-B-8x22B models, and even achieved a more significant increase in both Wizardarena-Mix Elo (+460) and MT-Bench (+2.07). This model also beats both Command R+ and Claude 3 Haiku. Figure 7 presents the win rates of 32 models in WizardArena-Mix, with each model involving in 2k x 31 battles. Compared to those baselines, our model has achieved significant improvements in win rate from the Io to 13. Specifically, using GPT-40 as the battle target, our WizardLM-\u03b2-8x22B's win rate increased by 26% (8% -> 22% -> 27% ->34%), WizardLM-\u03b2-7B's win rate also increased by 14% (6% -> 16% -> 18% ->20%).\nAbove results highlight that continuous battle with SOTA models with Arena Learning and updating weights with new selected data can progressively enhance model capacities compared to its rivals. Hence, Arena Learning builds an effective data flywheel and utilizing the Arena Learning can significantly improve model performance in post-training."}, {"title": "3.4 Scaling Iterative SFT, DPO, and PPO with Arena Learning .", "content": "As the core question of this paper asks how Arena Learning improves a model's performance with post-training, in this section we examine how performance is affected by different post-training technology and data flywheel iterations. Figure 8 explores the results of WizardLM-B-7B model. As expected, we observe that each performance across the SFT and RL models improves step by step as we add more selected data from more Arena Learning battle iterations. Specifically, from SFT-10 to PPO-I3, the WizardArena-Mix ELO score improves from 871 to 1274, achieves a huge gain of 403 points, and the Arena-Hard Auto ELO score also rises by 26.3 points (from 5.2 to 31.5). Additionally, the AlpacaEval 2.0 LC win rate improved by 26%, from 8.2% to 34.2%, and the MT-Bench score increased by 1.75 points, from 6.41 to 8.16. Significant improvements across four key benchmarks highlight the effectiveness and scalability of the iterative training approach proposed by Arena Learning in enhancing post-training LLMs during the SFT, DPO, and PPO stages."}, {"title": "3.5 Ablation Study", "content": "Data Selection strategy. To explore the efficiency of our pair-judge data selection method, we compare it with some widely used data selection strategies during the first round of SFT stage. In Table 5, we use 10k samples for each method except for the Original D\u2081. The results indicate that data selected via the pair-judge method yielded a 29-point improvement in the WizardArena-Mix ELO over the all original 30k data, surpassed the diversity-based K-Means Cluster method by 23 points, and exceeded the instruction complexity-based INSTAG [43] method by 12 points. On MT-bench, the pair-judge method also demonstrated superior performance, with improvements of 0.35 points over Original Data, 0.25 points over K-Means Cluster, and 0.11 points over INSTAG. This advantage is attributed to that the pair-judge method focuses on instructions where the base model underperforms, particularly in diverse and complex tasks, effectively addressing the model's weaknesses. Simultaneously, these results underscore the effectiveness of the pair-judge method in selecting high-quality data during the SFT stage to target and strengthen the weakness of the base model.\nThe relationship between data size and performance. An intuitive question is whether the improvement in model performance is solely due to the increase in data size. Therefore, in this section, we discuss the impact of data size and quality on model performance. Threshold is an important hyperparameter in Arena Learning that controls the size of SFT data and gap between <chosen, reject> pairs of RL data. We conducted the experiments of WizardLM-\u03b2-7B-SFT-I\u2081 and WizardLM-B-7B-DPO-I1 where threshold ranges from 0 to 5. The result is shown in the Figure 9, and we did observe the best threshold of SFT and DPO data are 3.0 and 2.0 respectively in I\u2081. In SFT, compared to threshold=0, although half of the training data (30k -> 14.6k) is left when the threshold=3, the ELO of the model actually brings a 70-point improvement (1047 -> 1117). Similarly in DPO, setting the threshold=2 reduced the data to 18.1k compared to threshold=0, and the ELO of the model improved by 22 points (1165 -> 1187). This indicates that the battle helps us filter out the truly needed data, thereby constructing a more efficient data flywheel with a more streamlined scale.\nLlama3-Chat Judge or GPT-4 Judge? In most previous works, people were accustomed to use GPT-4 as a judge for evaluation or generating synthetic data, but the GPT-4 API cost required for large-scale data flywheel is enormous for most research and production scenarios. Therefore, we explore whether it is possible to replace GPT-4 with advanced open source models. Table 6 explores the consistency between Llama3-70B-Instruct and GPT-4 as judge models in the WizardArena-Mix Arena. Using GPT-4 judge's ELO as the reference benchmark, the Spearman correlation coefficient between Llama3-70B-Instruct judge and GPT-4 judge is 99.26%, and the Human Agreement with 95% CI is 96.15%. The overall average consistency between the two judge models is 97.71%. Furthermore, combining GPT-4 and Llama3-70B-Instruct as the judge model resulted in an overall average consistency of 98.40% for LMSYS ChatBot Arena, a slight 0.25% improvement over using only Llama3-70B-Instruct (98.40% vs. 98.15%). Consequently, employing Llama3-70B-Instruct as a cost-effective judge model achieves high consistency with both GPT-4 and LMSYS Chatbot Arena by human judgment, ensuring the reliability of the WizardArena evaluation and post-training with Arena Learning in this paper."}, {"title": "Number of battle models.", "content": "Figure 10 presents an ablation study investigating the impact of the number of other battle models. According to Table 4, the models are ranked in descending order based on WizardArena-Mix ELO scores. Subsequently, models ranging from Command R+ to OpenChat 3.5 are selected for battle. As the number of models participating in the battle increases, the performance of the WizardLM-B-7B-SFT-I\u2081 model gradually increases. Specifically, on WizardArena-Mix, the ELO rating of WizardLM-8-7B increases from 876 to 1159, a gain of 283 points. Concurrently, the MT-Bench score rises from 6.41 to 7.66, an increase of 1.25 points. This demonstrates the scalability of our method and its compatibility with different models, providing a basis for future large-scale application of Arena Learning . However, as relationship between the complexity of the battle O(\u00b7) and the number of models n is O(n\u00b2), and in order to balance the computational cost and model performance, we chose 3 other models to battle with WizardLM-8 as the default setting in this paper.\nThe impact of different battle modes. In order to explore the necessity of using multiple models pairwise battle to construct a data flywheel, we designed various battle modes on D\u2081 SFT data, including: i) {ours + 1 other model} pairwise battle with each other, ii) randomly split D\u2081 into 3 parts, ours battle with one other model on each part respectively, iii) {ours + 2 other models} pairwise battle with each other, iv) {ours + 3 other models} pairwise battle with each other. We use WizardLM-B-7B-SFT-10, Openchat-3.5, Qwen-1.5-72B, and CommandR+ as the battle group in this section, the output model is WizardLM-\u03b2-7B-SFT-I\u2081. As shown in the Table 7, the mode (iv) achieved best performance on WizardArena and Outperformed the (i) mode {Only Command R+ battle} by 89 points and the (iii) mode {Command R+ & Qwen1.5-72B-Chat Battle} by 22 points. To this end, we finally leverage multiple models pairwise battle with each other to build the simulated offline Chatbot Arena.\nPerformance on more benchmarks. Table 8 highlights the performance of WizardLM-\u03b2 across various metrics after three iterations, including LMSYS Arena-Hard Auto, AlpacaEval 2.0 LC, and the OpenLLM Leaderboard. In LMSYS Arena-Hard Auto, WizardLM-B-7B's score rises from 5.2 to 31.5, with a gain of 26.3 points, surpassing GPT-3.5-Turbo-0613 by 6.7 points and Llama 3-8B-Instruct by 10.9 points, closely aligning with Command R+. WizardLM-B-8x22B's performance outperforms Llama-3-70B-Instruct by 23.2 points, is also better than GLM-4-0520 and Yi-Large. In AlpacaEval 2.0 LC, WizardLM-B-7B's win rate increases from 8.2% to 34.2%, exceeding GPT-3.5-Turbo-0613 by 11.5 points and Mixtral-8x22b-Instruct-v0.1 by 3.3 points, matching closely with Llama3-70B-Instruct. Moreover, WizardLM-B-8x22B's win rate even surpasses Llama-3-70B-Instruct by 14.5 points and GPT-4-0314 by 13.6 points. On the OpenLLM Leaderboard, WizardLM-\u03b2-7B's average score increases from 57.75 to 68.08, surpassing Llama-2-70B-Chat by 1.28 points and comparable to Starling-LM-7B-beta. WizardLM-\u03b2-8x22B is also compareable with Command R+, exceeds Deepseek-LLM-67B-Chat by 3.06 points, and closely approaches Qwen1.5-72B-Chat and Llama-3-70B-Instruct. The above results indicate that: 1) Utilizing the Arena Learning method to generate training data significantly improves the performance of the model by multiple training iterations. 2) Arena Learning can improves the generalization and scalability of the model performance.\nData count and difficulty of each iteration. In table 9 we show in detail the data size, difficulty, and threshold division for each round of the SFT. As the number of iteration rounds increased, we adjusted the threshold from 3 to 1, but the data size of SFT still significantly decreased (30k -> 7.8k). This is because as the model's ability evolved, the number of battles it lost also sharply declined. We also found that the difficulty of each round of data gradually increases (4.7 -> 7.4) and we only need totally around 1/3 data for final SFT (90k -> 33.7k) and the average difficulty is 6.4. It indicates that a reasonable data flywheel should focus more on finding those challenging data for target model to fill in the shortcomings of its capabilities."}, {"title": "4 Related Works", "content": ""}, {"title": "4.1 Large Language Models", "content": "LLMs have made significant strides in Natural Language Processing (NLP), serving as a versatile foundation for numerous applications [50-52]. These models, which often contain hundreds of billions of parameters, are trained on expansive text datasets. Notable examples include OpenAI's GPT-3 and GPT-4 [4, 53], Anthropic's Claude [54], Google's PaLM [55, 56], Gemini [6], Gemma [47], and DeepMind's Chinchilla [57]. The AI field has recently seen a surge in open-source LLMs, providing public access to model codes and parameters. Notable releases include BigScience's BLOOM [58], Mistral AI's Mistral [36], Microsoft's Phi [48], Meta's Llama family [3, 22, 59] and GAL [60], NVIDIA's Nemotron-4 340B [61], Tsinghua University's ChatGLM [62, 63], and TII's Falcon [64]. New entries such as Command R [37], DBRX [49], Reka [65], Baichuan [66], Qwen [7], Yi [45], DeepSeek [40], InternLM [67], MiniCPM [68] and Llemma [69] have also emerged. Presently, models like Alpaca [10], Vicuna [9], Guanaco [70], Orca [71], OpenChat [12], Tulu2 [38], WizardLM [11], XwinLM [72, 73], StarlingLM [18] and Zephyr [41] are being developed through supervised fine-tuning based on Llama [3, 22, 59] and Mistral [36]. However, how to measure the performance of current all models in real-world, open scenarios is a challenging task. LMSYS has developed a chatbot arena [19] that utilizes anonymous battle and human judgment, but assessing all models is both time-consuming and costly. In this paper we simulate an offline chatbot arena and employ advanced LLM (i.e., Llama3-70B-Chat [59]) for judgment, significantly improving efficiency and reducing time requirements by 40x."}, {"title": "4.2 LLM Post-training", "content": "The alignment performance of Large Language Models (LLMs) is significantly influenced by the quality of Supervised Fine-Tuning (SFT) data, which encompasses task difficulty [71], query complexity [11, 74, 75], semantic diversity [10, 13], and sample size [76]. For instance, [10] generates diverse queries through self-instruct [77] methods, while [11, 74, 75, 78] enhances model alignment by increasing query complexity. [71] boosts NLP task performance by optimizing FLAN [27] queries and responses with specialized LLMs, and [13] has introduced UltraChat. To select data efficiently, some strategies like IFD [42], INSTAG [43], DEITA [79], MODS [80], and ALPAGASUS [81] are adopted. [71] employs ChatGPT to label instructional data, ensuring both diversity and complexity. Here, we select training data using the \"judge pair\" method with different advanced models.\nTo better adapt to preferences beyond SFT, models are trained with feedback-based methods like RLHF and RLAIF [2, 22, 54, 82, 83], employing Proximal Policy Optimization (PPO) [84] to align with model preferences. [85\u201387] improve weak to strong model generalization. WizardMath [75] adopts RLEIF, introducing process supervision and instruction quality scoring reward model to improve the mathematical reasoning ability of large language models. Due to RLHF's complexity and instability, simpler alternatives like DPO [20], RRHF [88], KTO [89], IPO [90], sDPO [91], and ORPO [92] are utilized. DPO [20] merges reward modeling with preference learning. RRHF [88] uses ranking loss to prioritize preferred answers, and KTO [89] operates without needing paired preference datasets. In this paper, in order to efficiently manage massive data, we have established a dynamic data flywheel for model post-training through the pair-wise judge battle method to consistently collect feedback from the advanced models. Furthermore, we propose Arena Learning to perform iterative battle and training process (SFT-DPO-PPO), where the WizardLM-\u1e9e is continuously updated and re-evaluated against the SOTA models, progressively enhancing the performance of our model."}, {"title": "4.3 LLM Benchmarks", "content": "Large Language Models (LLMs) have transformed the way people interact with computing systems and are extensively used in everyday life and work [50]. The existing benchmarks [93\u201395] are mainly divided into two categories: 1) Specialized tasks. Knowledge and Capability: MMLU [32], CMMLU [96], and C-Eval [97]; Reasoning: ARC [98], HellaSwag [33], PIQA [99], GSM8k [100], MATH [101]; Programming: HumanEval [102], MBPP [103], LiveCodeBench [104]; Safety and Truthfulness: ToxicChat [105], OLID [106], BIG-Bench [107], TruthfulQA [34]. They focus on assessing LLM performance in specific areas. 2) General tasks: like MT-Bench [14, 108] and AlpacaEval [25, 109, 110], encompass categories such as writing, role-playing, and mathematics, highlighting the models' comprehensive abilities and multi-turn dialogue performance.\nReal-world benchmarks, (i.e., LMSYS ChatBot Arena [19] and Allenai WildBench [111]) use anonymous battles, ELO [16, 112] rankings, and human judgments, but have time delay and often do not timely reflect the models' true performance and require large time and human labor intensive. [113, 114] propose an automatic evaluation tool for instruction-tuned LLMs. Additionally, most models overfit on leaderboards like MT-Bench [14], OpenLLM leaderboard [30, 115], showing inconsistent performance with real-world ChatBot scenarios and low differentiation among models. Therefore, we have developed the simulated offline WizardArena, which not only effectively differentiates model performance but also aligns closely with the online human-based LMSYS ChatBot Arena [19], which achieves an average consistency of 98% with LMSYS ChatBot Arena, simultaneously making it suitable for selecting the optimal models and predicting the performance of models while significantly enhancing model post-training through battle data."}, {"title": "5 Conclusion", "content": "This paper introduces Arena Learning, a simulated offline chatbot arena that utilizes AI LLMs to bypass the manual and time-intensive cost typically associated with preparing the arena battle data, while preserving the core advantages of the arena-based evaluation and training. The effectiveness of Arena Learning is validated through the high consistency in predicting Elo rankings across various LLMs compared, when compared with the human-based LMSys Chatbot Arena. Furthermore, the model trained iteratively on synthetic data generated by Arena Learning exhibits significant performance improvements using various training strategies. Overall, Arena Learning emerges as a cost-effective and reliable alternative to conventional human-based evaluation systems, providing a sustainable approach to progressively enhance and scale the capabilities of large language models.\nLimitations and Broader Impacts. If the judge model fails to accurately imitate human evaluators, the generated rankings and training data may be compromised. Moreover, similar to the other LLMs, our model could generate potentially unethical or misleading information."}, {"title": "A Three consistency metrics between two Arenas", "content": "To more effectively align the online arena (i.e. LMSYS ChatBot Arena) with real-world human preferences and to enhance differentiation among models, we developed a simulated offline arena. This platform is designed to evaluate the actual performance of the models and to facilitate the selection of optimal model checkpoints. We employ several key criteria [24] that define an effective benchmark for evaluating Large Language Models (LLMs) in chatbot applications, aiming to enable meaningful functional comparisons across different models.\n\u2022 Alignment with Human Preference : The benchmarks should maintain high alignment with real-world human preferences in responses to the diverse and hard instructions, ensuring that the models' outputs meet user expectations.\n\u2022 Ranking Accuracy: The benchmark should align closely with the reference standard to ensure that the rankings of different models on the leaderboard are reliable and accurate.\n\u2022 Differentiation: The benchmark should be capable of accurately differentiating the performance of various models by providing confidence intervals with minimal overlap. This feature is crucial to ensure that the more effective models can be reliably distinguished.\nWe define the alignment of Benchmark A with reference to Benchmark B, for a model pair (m1, m2) that B can confidently differentiate, using the following formulation:\nThe agreement score, $s(m_1, m_2)$, is determined as:\n$s(m_1, m_2) =\\begin{cases}1.0 & \\text{if A confidently separates } m_1 \\text{ from } m_2 \\text{ and their ranking aligns with B} \\\\-1.0 & \\text{if A confidently separates } m_1 \\text{ from } m_2 \\text{ and their ranking conflicts with B} \\\\0.0 & \\text{if A cannot confidently separate } m_1 \\text{ from } m_2 \\end{cases}$\nTo assess ranking accuracy, we employed Spearman's rank correlation coefficient to analyze the correlation between the two sets of ranking data.\n$\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2-1)}$\nwhere $\\rho$ is the Spearman's rank correlation coefficient, $d_i$ is the difference between the ranks of corresponding variables, and n is the number of observations.\nWe define the differentiation of models based on their performance scores, which are represented by confidence intervals $CI_1$ and $CI_2$ via bootstrapping. If the two confidence intervals do not overlap, then models $M_1$ and $M_2$ are considered to be separable.\n$CI_1 \\cap CI_2 = \\emptyset$"}]}