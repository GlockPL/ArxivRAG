{"title": "Arena Learning : Build Data Flywheel for LLMs\nPost-training via Simulated Chatbot Arena", "authors": ["Haipeng Luo", "Qingfeng Sun", "Can Xu", "Pu Zhao", "Qingwei Lin", "Jianguang Lou", "Shifeng Chen", "Yansong Tang", "Weizhu Chen"], "abstract": "Assessing the effectiveness of large language models (LLMs) presents substan-\ntial challenges. The method of conducting human-annotated battles in an online\nChatbot Arena is a highly effective evaluative technique. However, this approach\nis limited by the costs and time required for human annotation. In this paper,\nwe introduce Arena Learning, an innovative offline strategy designed to simulate\nthese arena battles using AI-driven annotations to evaluate battle outcomes, thus\nfacilitating the continuous improvement of the target model through both super-\nvised fine-tuning and reinforcement learning. Arena Learning comprises two key\nelements. First, it ensures precise evaluations and maintains consistency between\noffline simulations and online competitions via WizardArena, a pipeline developed\nto accurately predict the Elo rankings of various models using a meticulously\ndesigned offline test set. Our results demonstrate that WizardArena's predictions\nclosely align with those from the online Arena. Second, it involves the continuous\nimprovement of training data based on the battle results and the refined model.\nWe establish a data flywheel to iteratively update the training data by highlighting\nthe weaknesses of the target model based on its battle results, enabling it to learn\nfrom the strengths of multiple different models. We apply Arena Learning to train\nour target model, WizardLM-\u03b2, and demonstrate significant performance enhance-\nments across various metrics. This fully automated training and evaluation pipeline\nsets the stage for continuous advancements in various LLMs via post-training.\nNotably, Arena Learning plays a pivotal role in the success of WizardLM-2, and\nthis paper serves both as an exploration of its efficacy and a foundational study for\nfuture discussions related to WizardLM-2 and its derivatives.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of natural language processing (NLP) has witnessed a remarkable transfor-\nmation, driven by the rapid advancements in large language models (LLMs). These models, trained\non vast amounts of text data, have demonstrated an exceptional ability to understand, generate, and\ninteract with human language in a wide range of tasks [1-3]. One of the most exciting applications\nof LLMs has been in the realm of conversational AI [4\u20138], where they have been utilized to create\npowerful chatbots capable of engaging in naturalistic dialogues. One of the key factors contributing\nto the success of LLM-powered chatbots is the ability to leverage large-scale high-quality instruction\nfollowing data for effective post-training [9-13]. By exposing these models to a diverse range of"}, {"title": "2 Approach", "content": "In this section, we elaborate on the details of the proposed Arena Learning. As illustrated in Figure 2,\nthe closed loop pipeline mainly contains three components: Offline Pair-wise LLM Battle Arena,\nIterative Post-training, and Model Evaluation."}, {"title": "2.1 ChatBot Arena and Elo Ranking", "content": "The Chatbot Arena is a pioneering platform that has revolutionized the way chatbot models are\nevaluated and compared. It facilitates the assessment of different chatbot models by pitting them\nagainst each other in a series of conversational challenges. At the core of this Arena lies the concept\nof Elo rankings, a widely adopted rating system originally devised for chess players. Elo rankings\n[16] are used to quantify the relative performance of chatbot models based on a series of head-to-head\nbattles. Each model is initially assigned an arbitrary Elo rating, which is then updated after every\nbattle based on the outcome (win, loss, or tie) and the rating difference between the competing models.\nIf a higher-rated model defeats a lower-rated one, its Elo rating increases slightly, while the loser's\nrating decreases by a corresponding amount."}, {"title": "2.2 Using a Powerful LLM as Judge to Simulate Human Annotators", "content": "At the core of the simulated arena battles in Arena Learning lies a powerful LLM that serves as the\n'judge model\u201d. This judge model is specifically prompted and adjusted by us on a diverse range of\nconversational pair data, enabling it to evaluate the quality, relevance, and appropriateness of the\nmodels' responses objectively and consistently. The judge model's role is to analyze and compare\nthe responses provided by the pair battle models for each conversational sample. Specifically, to\nassess the response quality of each LLM, we use prompt engineering with the Llama3-70B-Chat\nmodel [22]. The inputs are dialogue history, user instruction, and the responses of two LLMs. The\noutputs consist of scores for each LLM, along with explanations focused on various factors, such\nas coherence, factual accuracy, context-awareness, and overall quality, to determine whether one\nresponse is superior to the other. To mitigate potential position bias [14, 23, 24], we employ a\ntwo-game setup, alternating the positions of the two LLMs. Each model receives an overall score on\na scale of 1 to 10, where a higher score reflects superior overall performance. Following, we will use\nthis \"judge\" model in both Arena Learning post-training and WizardArena evaluation stages."}, {"title": "2.3 Build a Data Flywheel to Post-train LLMs", "content": "To facilitate leveraging the simulated arena battles among models to train WizardLM-B, Arena\nLearning relies on a large-scale corpus of conversational data D. The data collection process involves\nseveral stages of filtering, cleaning, and deduplication to ensure the quality and diversity of the\ninstruction data. The simulated arena battle outcomes are then used to generate training data for the\nWizardLM-B, tailored to different training strategies: supervised fine-tuning (SFT), direct preference\noptimization (DPO), and proximal policy optimization (PPO). We split the data equally into some\nparts D = {Do, D1, D2, ..., DN} for following iterative training and updates respectively."}, {"title": "2.3.2 Iterative Battle and Model Evolving", "content": "Arena Learning employs an iterative process for training and improving the WizardLM-\u03b2. After each\nround of simulated arena battles and training data generation, the WizardLM-8 is updated using the\nappropriate training strategies (SFT, DPO, and/or PPO). This updated model is then re-introduced into\nthe arena, where it battles against the other SOTA models once again. This iterative process allows\nthe WizardLM-B to continuously improve and adapt to the evolving landscape of the arena. As the\nmodel becomes stronger, the simulated battles become more challenging, forcing the WizardLM-B to\npush its boundaries and learn from the latest strategies and capabilities exhibited by the other models."}, {"title": "2.4 Evaluate LLMs with WizardArena", "content": "To accurately evaluate the performance of chatbot models and predict their Elo rankings, Arena\nLearning relies on a carefully curated offline test set, which is designed to strike a balance between di-\nversity and complexity [14, 24, 25], ensuring a comprehensive assessment of the models' capabilities\nacross a wide range of conversational scenarios. Inspired by WizardLM [11] In-Breadth Evolving\nand In-Depth Evolving, we construct the following two subsets:\nDiverse Subset The diverse subset of the test set is constructed to capture a broad range of topics,\nstyles, and conversational contexts. To achieve this, we employs text clustering techniques on a large\ncorpus of instructions and conversational data. The clustering process begins by representing all the\ninstructions in a conversation as a high-dimensional vector using state-of-the-art embedding models\n(i.e., gte-large [26]). These vectors capture the semantic and contextual information within the text,\nenabling the clustering algorithm to group similar samples together. Once the clustering is complete,\nwe selects a representative sample from each cluster, ensuring that the diverse subset of the test set\ncapture a broad range of scenarios. This approach helps to mitigate potential biases or blindspots that\nmay arise from relying solely on simply random sampling.\nHard Subset This subset is specifically designed to challenge the capabilities of even the most\nadvanced chatbot models. To construct this subset, we leverages the power of LLMs to predict\nthe difficulty level of each instruction. We then selects the top-ranking samples according to the\npredicted difficulty scores, ensuring that the hard subset of the test set comprises the most challenging\nand complex scenarios. This data serves as a rigorous benchmark for evaluating the robustness and\ncapability of chatbot models in handling intricate and nuanced conversational tasks.\nWith the above \u201cjudge\u201d model and the offline WizardArena test set in place, we proceeds to evaluate\nthe performance of various chatbot models through a series of pair-wise battles. The outcomes of the\nbattles are then used to compute the Elo rankings of the participating chatbot models. WizardArena\nadopts the same Elo rating system used in LMSYS Chatbot Arena, which has proven effective in\nranking players or entities based on their head-to-head performance."}, {"title": "3 Experiments", "content": "Training Data. We random sample 10k ShareGPT data to train a initial model WizardLM-\u03b2-I0. We\nthen collected some instructions from open available datasets [10, 11, 17, 27, 28], and optimized them\nusing the following steps: first, we filtered out all illegal and toxic conversations; second, we removed\nconversations with instruction lengths of less than 10; third, we eliminated duplicate instructions\nwith prefixes of 10; next, we employed the MinHashLSH technique [29] for data deduplication;\nsubsequently, we used an embedding model gte-large [26] to exclude instructions from the top 5\nmatches in semantic similarity with benchmarks (i.e., WizardArena, Arena-Hard Auto [24], MT-\nBench [14], AlpacaEval [25], OpenLLM Leaderboard [30\u201334]) to prevent test data leakage. Finally,\nwe removed all non-English instructions. After completing these steps, we obtain the refined 276K\ndataset D, and randomly split it to 9 parts."}, {"title": "3.2 Offline WizardArena closely align with the Online LMSYS ChatBot Arena.", "content": "Figure 6 and Table 4 present the rank-\nings for some popular models across\nseveral evaluation benchmarks: LM-\nSYS ChatBot Arena-EN [19], MT-\nBench [14], and WizardArena. The\nresults reveal that employing the LM-\nSYS ChatBot Arena as the reference\nbenchmark in the real-world scenar-\nios, WizardArena displays the good\nranking consistency, however MT-\nBench shows the large fluctuations.\nIn addition, there is a significant dif-\nference in performance between Wiz-\nardArena diverse and hard subsets:\nVicuna-33B [9] and Qwen1.5-32B-\nChat [7] are more effective in diverse\ntasks, while Tulu-2-DPO-70B [38]\nand Nous-Hermes-2-Mixt-DPO [39]\nachieves better results in hard tasks.\nWe therefore use WizardArena-Mix\nas the final evaluation benchmark\nof Arena Learning to balance the\nstrengths of different models."}, {"title": "3.3 Can Arena Learning build an effective data flywheel with post-training?", "content": "Table 4 demonstrates the impact of using the Arena Learning method to post-train WizardLM-B\nmodels during three data flywheel iterations, where Ii represents the i-th iteration. In each iteration\nfrom I\u2081 to I3, we always use 90k data for post-training. Starting from WizardLM-B-7B-I0, the next 3\niterations have improved by 343 points, 32 points, and 28 points on Wizardarena-Mix Elo, respectively.\nAt the same time, the MT-bench score of this model has also achieved significant improvement (from\n6.41 to 8.16). Specifically, the WizardLM-B-I\u2081 even surpasses WizardLM-70B-v1.0 and the\nWizardLM-B-I3 also shows comparable performance with Starling-LM-7B-Beta. It is worth\nnoting that we have also observed the same trend on WizardLM-B-8x22B models, and even achieved\na more significant increase in both Wizardarena-Mix Elo (+460) and MT-Bench (+2.07). This model\nalso beats both Command R+ and Claude 3 Haiku. Figure 7 presents the win rates of 32 models\nin WizardArena-Mix, with each model involving in 2k x 31 battles. Compared to those baselines,\nour model has achieved significant improvements in win rate from the Io to 13. Specifically, using\nGPT-40 as the battle target, our WizardLM-\u03b2-8x22B's win rate increased by 26% (8% -> 22% ->\n27% ->34%), WizardLM-\u03b2-7B's win rate also increased by 14% (6% -> 16% -> 18% ->20%).\nAbove results highlight that continuous battle with SOTA models with Arena Learning and updating\nweights with new selected data can progressively enhance model capacities compared to its rivals.\nHence, Arena Learning builds an effective data flywheel and utilizing the Arena Learning can\nsignificantly improve model performance in post-training."}, {"title": "3.4 Scaling Iterative SFT, DPO, and PPO with Arena Learning .", "content": "As the core question of this paper asks how Arena Learning improves a model's performance with\npost-training, in this section we examine how performance is affected by different post-training\ntechnology and data flywheel iterations. Figure 8 explores the results of WizardLM-B-7B model.\nAs expected, we observe that each performance across the SFT and RL models improves step by\nstep as we add more selected data from more Arena Learning battle iterations. Specifically, from\nSFT-10 to PPO-I3, the WizardArena-Mix ELO score improves from 871 to 1274, achieves a huge\ngain of 403 points, and the Arena-Hard Auto ELO score also rises by 26.3 points (from 5.2 to\n31.5). Additionally, the AlpacaEval 2.0 LC win rate improved by 26%, from 8.2% to 34.2%, and the\nMT-Bench score increased by 1.75 points, from 6.41 to 8.16. Significant improvements across four\nkey benchmarks highlight the effectiveness and scalability of the iterative training approach proposed\nby Arena Learning in enhancing post-training LLMs during the SFT, DPO, and PPO stages."}, {"title": "3.5 Ablation Study", "content": "Data Selection strategy. To explore the effi-\nciency of our pair-judge data selection method,\nwe compare it with some widely used data se-\nlection strategies during the first round of SFT\nstage. In Table 5, we use 10k samples for each\nmethod except for the Original D\u2081. The results\nindicate that data selected via the pair-judge\nmethod yielded a 29-point improvement in the\nWizardArena-Mix ELO over the all original 30k\ndata, surpassed the diversity-based K-Means\nCluster method by 23 points, and exceeded\nthe instruction complexity-based INSTAG [43]\nmethod by 12 points. On MT-bench, the pair-\njudge method also demonstrated superior performance, with improvements of 0.35 points over\nOriginal Data, 0.25 points over K-Means Cluster, and 0.11 points over INSTAG. This advantage is\nattributed to that the pair-judge method focuses on instructions where the base model underperforms,\nparticularly in diverse and complex tasks, effectively addressing the model's weaknesses. Simultane-\nously, these results underscore the effectiveness of the pair-judge method in selecting high-quality\ndata during the SFT stage to target and strengthen the weakness of the base model.\nThe relationship between data size and performance. An intuitive question is whether the\nimprovement in model performance is solely due to the increase in data size. Therefore, in this\nsection, we discuss the impact of data size and quality on model performance. Threshold is an\nimportant hyperparameter in Arena Learning that controls the size of SFT data and gap between\n<chosen, reject> pairs of RL data. We conducted the experiments of WizardLM-\u03b2-7B-SFT-I\u2081 and\nWizardLM-B-7B-DPO-I1 where threshold ranges from 0 to 5. The result is shown in the Figure\n9, and we did observe the best threshold of SFT and DPO data are 3.0 and 2.0 respectively in I\u2081. \nIn SFT, compared to threshold=0, although half of the training data (30k -> 14.6k) is left when the\nthreshold=3, the ELO of the model actually brings a 70-point improvement (1047 -> 1117). Similarly\nin DPO, setting the threshold=2 reduced the data to 18.1k compared to threshold=0, and the ELO of\nthe model improved by 22 points (1165 -> 1187). This indicates that the battle helps us filter out the\ntruly needed data, thereby constructing a more efficient data flywheel with a more streamlined scale.\nLlama3-Chat Judge or GPT-4 Judge? In most previous works, people were accustomed to use\nGPT-4 as a judge for evaluation or generating synthetic data, but the GPT-4 API cost required for\nlarge-scale data flywheel is enormous for most research and production scenarios. Therefore, we\nexplore whether it is possible to replace GPT-4 with advanced open source models. Table 6 explores\nthe consistency between Llama3-70B-Instruct and GPT-4 as judge models in the WizardArena-Mix\nArena. Using GPT-4 judge's ELO as the reference benchmark, the Spearman correlation coefficient\nbetween Llama3-70B-Instruct judge and GPT-4 judge is 99.26%, and the Human Agreement with\n95% CI is 96.15%. The overall average consistency between the two judge models is 97.71%.\nFurthermore, combining GPT-4 and Llama3-70B-Instruct as the judge model resulted in an overall\naverage consistency of 98.40% for LMSYS ChatBot Arena, a slight 0.25% improvement over using"}, {"title": "4 Related Works", "content": "LLMs have made significant strides in Natural Language Processing (NLP), serving as a versatile\nfoundation for numerous applications [50-52]. These models, which often contain hundreds of\nbillions of parameters, are trained on expansive text datasets. Notable examples include OpenAI's\nGPT-3 and GPT-4 [4, 53], Anthropic's Claude [54], Google's PaLM [55, 56], Gemini [6], Gemma [47],\nand DeepMind's Chinchilla [57]. The AI field has recently seen a surge in open-source LLMs,\nproviding public access to model codes and parameters. Notable releases include BigScience's\nBLOOM [58], Mistral AI's Mistral [36], Microsoft's Phi [48], Meta's Llama family [3, 22, 59]\nand GAL [60], NVIDIA's Nemotron-4 340B [61], Tsinghua University's ChatGLM [62, 63], and\nTII's Falcon [64]. New entries such as Command R [37], DBRX [49], Reka [65], Baichuan [66],\nQwen [7], Yi [45], DeepSeek [40], InternLM [67], MiniCPM [68] and Llemma [69] have also\nemerged. Presently, models like Alpaca [10], Vicuna [9], Guanaco [70], Orca [71], OpenChat [12],\nTulu2 [38], WizardLM [11], XwinLM [72, 73], StarlingLM [18] and Zephyr [41] are being developed\nthrough supervised fine-tuning based on Llama [3, 22, 59] and Mistral [36]. However, how to\nmeasure the performance of current all models in real-world, open scenarios is a challenging task.\nLMSYS has developed a chatbot arena [19] that utilizes anonymous battle and human judgment, but\nassessing all models is both time-consuming and costly. In this paper we simulate an offline chatbot\narena and employ advanced LLM (i.e., Llama3-70B-Chat [59]) for judgment, significantly improving\nefficiency and reducing time requirements by 40x."}, {"title": "4.2 LLM Post-training", "content": "The alignment performance of Large Language Models (LLMs) is significantly influenced by the\nquality of Supervised Fine-Tuning (SFT) data, which encompasses task difficulty [71], query com-\nplexity [11, 74, 75], semantic diversity [10, 13], and sample size [76]. For instance, [10] generates\ndiverse queries through self-instruct [77] methods, while [11, 74, 75, 78] enhances model alignment\nby increasing query complexity. [71] boosts NLP task performance by optimizing FLAN [27] queries\nand responses with specialized LLMs, and [13] has introduced UltraChat. To select data efficiently,\nsome strategies like IFD [42], INSTAG [43], DEITA [79], MODS [80], and ALPAGASUS [81] are\nadopted. [71] employs ChatGPT to label instructional data, ensuring both diversity and complexity.\nHere, we select training data using the \"judge pair\" method with different advanced models.\nTo better adapt to preferences beyond SFT, models are trained with feedback-based methods like\nRLHF and RLAIF [2, 22, 54, 82, 83], employing Proximal Policy Optimization (PPO) [84] to align\nwith model preferences. [85\u201387] improve weak to strong model generalization. WizardMath [75]\nadopts RLEIF, introducing process supervision and instruction quality scoring reward model to\nimprove the mathematical reasoning ability of large language models. Due to RLHF's complexity\nand instability, simpler alternatives like DPO [20], RRHF [88], KTO [89], IPO [90], sDPO [91], and\nORPO [92] are utilized. DPO [20] merges reward modeling with preference learning. RRHF [88] uses\nranking loss to prioritize preferred answers, and KTO [89] operates without needing paired preference\ndatasets. In this paper, in order to efficiently manage massive data, we have established a dynamic\ndata flywheel for model post-training through the pair-wise judge battle method to consistently collect\nfeedback from the advanced models. Furthermore, we propose Arena Learning to perform iterative\nbattle and training process (SFT-DPO-PPO), where the WizardLM-\u1e9e is continuously updated and\nre-evaluated against the SOTA models, progressively enhancing the performance of our model."}, {"title": "4.3 LLM Benchmarks", "content": "Large Language Models (LLMs) have transformed the way people interact with computing systems\nand are extensively used in everyday life and work [50]. The existing benchmarks [93\u201395] are\nmainly divided into two categories: 1) Specialized tasks. Knowledge and Capability: MMLU [32],\nCMMLU [96], and C-Eval [97]; Reasoning: ARC [98], HellaSwag [33], PIQA [99], GSM8k [100],\nMATH [101]; Programming: HumanEval [102], MBPP [103], LiveCodeBench [104]; Safety and"}, {"title": "5 Conclusion", "content": "This paper introduces Arena Learning, a simulated offline chatbot arena that utilizes AI LLMs to\nbypass the manual and time-intensive cost typically associated with preparing the arena battle data,\nwhile preserving the core advantages of the arena-based evaluation and training. The effectiveness of\nArena Learning is validated through the high consistency in predicting Elo rankings across various\nLLMs compared, when compared with the human-based LMSys Chatbot Arena. Furthermore,\nthe model trained iteratively on synthetic data generated by Arena Learning exhibits significant\nperformance improvements using various training strategies. Overall, Arena Learning emerges as a\ncost-effective and reliable alternative to conventional human-based evaluation systems, providing\na sustainable approach to progressively enhance and scale the capabilities of large language models.\nLimitations and Broader Impacts. If the judge model fails to accurately imitate human evaluators,\nthe generated rankings and training data may be compromised. Moreover, similar to the other LLMs,\nour model could generate potentially unethical or misleading information."}, {"title": "A Three consistency metrics between two Arenas", "content": "To more effectively align the online arena (i.e. LMSYS ChatBot Arena) with real-world human\npreferences and to enhance differentiation among models, we developed a simulated offline arena.\nThis platform is designed to evaluate the actual performance of the models and to facilitate the\nselection of optimal model checkpoints. We employ several key criteria [24] that define an effective\nbenchmark for evaluating Large Language Models (LLMs) in chatbot applications, aiming to enable\nmeaningful functional comparisons across different models.\n\u2022 Alignment with Human Preference : The benchmarks should maintain high alignment with\nreal-world human preferences in responses to the diverse and hard instructions, ensuring\nthat the models' outputs meet user expectations.\n\u2022 Ranking Accuracy: The benchmark should align closely with the reference standard to\nensure that the rankings of different models on the leaderboard are reliable and accurate.\n\u2022 Differentiation: The benchmark should be capable of accurately differentiating the perfor-\nmance of various models by providing confidence intervals with minimal overlap. This\nfeature is crucial to ensure that the more effective models can be reliably distinguished.\nWe define the alignment of Benchmark A with reference to Benchmark B, for a model pair (m1, m2)\nthat B can confidently differentiate, using the following formulation:\nThe agreement score, s(m1, m2), is determined as:\n$s(m_1, m_2) = \\begin{cases}\n1.0 & \\text{if A confidently separates } m_1 \\text{ from } m_2 \\text{ and their ranking aligns with B} \\\\\n-1.0 & \\text{if A confidently separates } m_1 \\text{ from } m_2 \\text{ and their ranking conflicts with B} \\\\\n0.0 & \\text{if A cannot confidently separate } m_1 \\text{ from } m_2\n\\end{cases}$\nTo assess ranking accuracy, we employed Spearman's rank correlation coefficient to analyze the\ncorrelation between the two sets of ranking data.\n$\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}$\nwhere p is the Spearman's rank correlation coefficient, di is the difference between the ranks of\ncorresponding variables, and n is the number of observations.\nWe define the differentiation of models based on their performance scores, which are represented by\nconfidence intervals CI\u2081 and CI2 via bootstrapping. If the two confidence intervals do not overlap,\nthen models M1 and M2 are considered to be separable.\n$CI_1 \\cap CI_2 = \\emptyset$"}]}