{"title": "Advances in Preference-based Reinforcement Learning: A Review", "authors": ["Youssef Abdelkareem", "Shady Shehata", "Fakhri Karray"], "abstract": "Reinforcement Learning (RL) algorithms suffer from the dependency on accurately engineered reward functions to properly guide the learning agents to do the required tasks. Preference-based reinforcement learning (PbRL) addresses that by utilizing human preferences as feedback from the experts instead of numeric rewards. Due to its promising advantage over traditional RL, PbRL has gained more focus in recent years with many significant advances. In this survey, we present a unified PbRL framework to include the newly emerging approaches that improve the scalability and efficiency of PbRL. In addition, we give a detailed overview of the theoretical guarantees and benchmarking work done in the field, while presenting its recent applications in complex real-world tasks. Lastly, we go over the limitations of the current approaches and the proposed future research directions.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) is a sub-field of machine learning that has been widely implemented in many applications such as robot control [1], games [2], and medical domains [3]. A learning agent continuously interacts with an environment by doing actions and receives feedback signals (rewards) with the target of maximizing the cumulative reward by the end of the interaction phase. The reward received is assumed to be numerically generated from a specific reward function. The main issue that arises is the high sensitivity of the performance to the design of the reward function. Specifically, one of the challenges in reward engineering is called Reward Hacking which occurs when the agent comes up with ways to maximize the cumulative rewards without executing the intended task [4]. Reward Shaping is also another problem that involves finding the optimal balance between providing rewards that direct the agent to the final goal (extrinsic motivation) while guiding them to also do the intended task at the same time (intrinsic motivation) [5].\n The field of preference-based reinforcement learning (PbRL) promises a solution to the aforementioned problems. It revolves around providing the agent with non-numeric reward signals in the form of pairwise preferences rather than absolute rewards. This shift in approach broadens the scope of RL algorithms to non-expert users where accurate reward engineering is no longer required. In this work, we present a coherent framework for PbRL, summarized in Figure 1. Our framework is an extension of the one proposed by [6] where we include the most recent advances in PbRL.\n We start with a formal definition of the problem in Section II, followed by the different design choices in Section III. We then go over the PbRL algorithms with theoretical guarantees in Section IV and the available benchmarking frameworks in Section V. Applications of PbRL in the Natural Language Processing (NLP) domain are presented in Section VI. Lastly, we conclude by analyzing the shortcomings of the surveyed methods and propose future research directions in Section VII."}, {"title": "II. PROBLEM FORMULATION", "content": "In PbRL algorithms, we are trying to solve the traditional RL problems using preferences between pairs of states, actions, or trajectories rather than absolute numerical rewards. The MDP for preferences (MDPP) [6] is represented as a sextuple (S, \u0391, \u03bc, \u03b4, \u03b3, \u03c1). Similar to the original MDP, S and A denote the state and action spaces that could be either discrete, with sizes S and A|, or continuous. d(s'|s, a) represents a stochastic state transition model, while y is the discount factor \u2208 [0,1), \u03bc(s) is the initial state distribution and h is the horizon length in finite-horizon settings. Trajectories (7) define a sequence of state-action pairs and \u03c0(\u03b1\u03c2) represents the policy. The difference between MDP and MDPP is that a preference relation over trajectories T1 \u227b T2, where T\u2081 is more preferred than T2, is received by the agent instead of the numeric reward signal r(s, a). p(T1 \u227b T2) denotes the probability that a preference relation holds. Regarding the objective, we would like the agent to learn an optimal policy \u03c0* that generates trajectories that satisfy the set of all preference relations (\u03b6 received from the expert during training. The objective of PbRL for a single preference relation assumes that the optimal policy is the one that maximizes the difference between the probabilities of obtaining the more preferred trajectory (dominating) and the less preferred one (dominated)."}, {"title": "III. DESIGN CHOICES", "content": "In the literature, the types of preferences that are received from experts could be divided into action, state, and trajectory preferences.\n1) Action Preferences: Action preferences reduce the preference relation to the comparison of a pair of actions for the same state, where a1 \u227bs a2 denotes that action a\u2081 is more preferred than action a2 for state s."}, {"title": "A. Preference Type Design Choices", "content": "Utilizing action preferences that optimize short-term rewards would be hard as they're only valid for a given state. [7] uses action-based preferences that deal with long-term optimality and evaluate the relation a1 \u227bs a2 by doing a roll-out for trajectories starting by state s, doing action a, and following the estimate of the policy to get the expected returns of each action. Action preferences with long-term optimality are considered demanding for experts since the long-term outcome should be known.\n2) State Preferences: Regarding state-preferences, a relation 81 \u227b 82 indicates that s\u2081 is preferred over 82. Since those relations correspond to segments of the state space, they give more information compared to action preferences and are less demanding to the expert since no comparisons between actions are needed. [8], [9] follow long-term optimality by proposing that selecting the most preferred successor state for every state could be a viable solution in their long-term state-based setting. [10] uses short-term state preferences and mitigates their issues by trying to estimate a cost function for every state using the support vector ranking approach [11].\n3) Trajectory Preferences: The most common type of preference relations are trajectory preferences where T1 \u227b T2 indicates that trajectory T\u2081 dominates over 72. Such preferences are desirable since they can be easily evaluated by experts by assessing the full trajectories and their results. Most of the methods presented in the rest of this paper use trajectory preferences. A general challenge is relating the final preference over trajectories to their most relevant states and actions."}, {"title": "B. Learning Problem Design Choices", "content": "There have been several proposed approaches to use the preference feedback received from the expert to optimize the policy. Our main focus will be on approaches that directly learn a policy distribution, or estimate a utility function. Other approaches like [7] learn a preference model, usually modeled as a classifier, to predict whether a preference relation holds between two actions for a given state.\n1) Learning a Policy: Some methods directly derive an estimation of the policy. [12] learns a policy distribution using a Bayesian likelihood function. Specifically, they utilize the posterior distribution of policies P(\u03c0|\u03b6) given the preference relations to sample two parameterized policies which are used to generate two full trajectories 71, 72 starting from the same state. The sampled trajectories are transformed into a trajectory preference relation (T1 \u227b T2) and added to the existing buffer storing all the preference relations. The posterior policy distribution P(\u03c0|\u03b6) is represented with Bayes theorem through the multiplication of the prior distribution of the policy P(\u03c0) with the likelihood of all the trajectory preferences P(T1 \u227b T2|\u03c0) \u2208 \u03da and approximated using Markov Chain Monte Carlo Simulation [13]. The downside is that the likelihood is modeled in terms of the euclidean distance between the policy-realized trajectories which constrains the algorithm to perform properly on low-dimensional continuous state spaces only. [14], [15] mitigate this issue by not requiring a distance function in their objective. They make a direct policy comparison by presenting trajectory preference queries comparing trajectories T1, T2 generated by the two policies \u03c01, \u03c02.\n The sets of all preference relations and policies are used to generate a ranking between policies returning the highest-ranked policies as the optimal ones. However, their method requires a higher number of preference queries than [12], as their preferences are non-reusable, making it not feedback-efficient.\n2) Learning a utility function: Learning a policy directly can be highly sample-inefficient, therefore, some methods try to estimate a surrogate utility function U(x), where x denotes trajectories or state-action pairs, to extract more information from the preferences. This utility function is analogous to the reward function seen in RL, however, they are not directly related since the definition of what is optimal is dependent on the views of the expert giving the preference feedback. There are two types of utility functions used in the literature which are linear and non-linear. For both types, trajectories and state-action pairs are assumed to have a feature vector representation denoted by \u03c8(7) and (s,a) respectively. The general objective is to obtain the optimal policy by maximizing a link function d that represents the difference between the utilities of the dominating and dominated preference relation terms.\n a) Linear utility functions: This type of utility is formu-lated for trajectories in terms of an unknown weight vector \u03b8 yielding U(T) = \u03b8T\u03c8(\u03c4). Methods that use such type of linear utility formulate the link function for a trajectory preference relation as follows d(\u03b8, T1 \u227b \u03c42) = \u03b8T (\u03c8(T1)-\u03c8(T2)) and find the optimal @ that maximizes the link function for all preference relations. The optimization depends on choosing a proper loss function which differs based on the proposed methods. [16] incorporated the hinge loss that is approximated using the ranking SVM method following [17] and estimated the hand-coded feature representations \u03c8(7) for the trajectories using e-means clustering [18]. [19] proposes an enhancement by accounting for the inaccuracies of the expert through the introduction of a piece-wise loss. The loss represents the inaccuracies using a ridge noise model controlled by a dynamically changing hyper-parameter to consider preferences that change over time.\n b) Non-linear utility functions: There are different ways to introduce non-linearity in the utility functions. Our focus is on recent methods that utilize deep neural networks (DNNs) as a non-linear representation for the utility. Those methods can be categorized based on using online or offline RL algorithms in their formulation.\n b.1) DNNs with Online RL: [20] is the first method to represent the parameters @ of the utility function Ue(s, a) with a DNN. Due to their high representational capacity, DNNs opened the door to experiment on more challenging robotic [21] and Atari [22] tasks. In addition, the experts are queried with trajectory segments (\u03c3) instead of full trajectories (7) which reduces the effort done by them compared to [16]. The utility of the segments are sum-decomposable in terms of the non-linear state-action utilities yielding Uo(\u03c3) = \u2211i Uo(si, ai)."}, {"title": "IV. THEORETICAL GUARANTEES", "content": "There is a current research target aiming to develop novel PbRL algorithms that are tractable for theoretical analysis. Those algorithms focus on reaching either regret or finite-sample guarantees which will be discussed in detail in this section."}, {"title": "A. Regret Guarantees", "content": "Regret denotes the difference between the current expectation of total rewards and the maximum rewards generated by the optimal policy. Providing theoretical guarantees on the bounds of the regret has been an important aim for various RL and bandit approaches [30], [31]. DPS [32] is the first paper to propose a PbRL algorithm with solid regret guarantees. They base their algorithm on the Thompson Sampling algorithm [33] while formulating Bayesian regret bounds by borrowing the information-theoretic concepts from [34]. DPS uses trajectory preferences within a model-based approach. They assume having a non-linear utility function U(s, a) for state-action pairs to estimate the reward model. Both the reward and transition dynamics models are represented as Bayesian posterior distributions. Similar to [20], the trajectory utilities are a summation of state-action utilities yielding U(t) = \u2211iU(si, ai). The link function is linear with d(0, \u03c4\u2081 \u227b T2) = U(T1)-U(T2) where 0 represents the parameters of the distribution of the utility function. Their algorithm samples two distinct pairs of transition and utility models from their distributions and applies value iteration to yield the two corresponding policies. The transition and utility distributions are then updated using the queried preferences history present in a buffer.\n By proving the asymptotic convergence of the transition and reward models, they were able to reach an asymptotic sublinear regret rate of |S|\u221a2|A|Nh log|A|, where N is the number of iterations of the algorithm.The main limitation of [32] is the exponential complexity in the time horizon h for the asymptotic convergence of the reward and transition models. [35] proposed a more general PbRL algorithm with regret guarantees by assuming that the underlying utility function represents non-Markovian rewards. The utility function is linear in terms of the trajectory features with dimension d, such that Uo(t) = 0T\u03c8(\u03c4). They utilize the same link function in (1) in terms of full trajectories and add an extra L2 regularization term on 9. The foundation of their approach is based on bounding @ under a parameter with the rescaling concepts used in [36]. Consequently, they propose two algorithms that assume having known and unknown transition models achieving near-optimal regret bounds of O' (Qdlog(N/\u03b4)\u221aN), with a probability of at least 1-8, and O'((\u221ad+h\u00b2 + |S|)\u221adN+ \u221a|S||A|Nh), respectively. The notation O' hides any logarithmic factors in the variables."}, {"title": "B. Finite-Sample Guarantees", "content": "[37] is the only PbRL algorithm in the literature to derive finite-sample guarantees in terms of the number of preferences queries and the number of interactions with the environment (number of steps). Trajectory preferences are used under the observation that those preferences need to be noisy to derive a unique optimal policy. The target of the algorithm is to efficiently obtain an e-optimal policy. They utilize black-box PAC-Dueling Bandits (P-DB) algorithms like [38] and [39] to make policy comparisons based on the collected trajectory preferences. Specifically, one of their proposed algorithms (PEPS) explores the state space by synthesizing a reward function, similar to reward-free RL [40], and optimizes it using a tabular RL algorithm (EULER; [41]). The P-DB algorithm then generates action queries during learning which are transformed into trajectory preference queries by rolling out the trajectories with the current policy estimated with EULER. If the P-DB algorithm requires no target accuracy in advance, the PEPS method reaches an e-optimal policy with a step complexity of O(h2S2A+S4Ah\u00b3\u00b3) \u2713) and a preference query complexity of O(SPA), ), where denotes the log factors. The main limitations of this approach are the sub-optimal sample complexity and the dependency on the guarantees of the underlying P-DB algorithm which only hold under some restrictions on the structure of the preferences."}, {"title": "V. BENCHMARKING", "content": "There has been a large focus in the literature to create benchmarks for various RL domains such as Offline RL [42], and Safe RL [43]. [44] is the first paper to propose a benchmark for the consistent evaluation of PbRL algorithms without relying on expensive human feedback. To achieve that, they simulate an expert providing preference based on the total sum of the ground-truth rewards while explicitly accounting for the human errors.\n Modeling the stochasticity of the simulated expert prefer-ences is achieved by following the link function formulation in (1) to model the preference probability of segments in terms of ground-truth rewards yielding\n\n$$P(\\sigma_{1} \\succ \\sigma_{2}) = \\frac{e^{\\beta\\gamma^{|s,a|}}}{e^{\\beta\\gamma^{|s,a|}} + e^{\\beta\\gamma^{|s,a|}}},$$\n\n where \u1e9e controls the degree of expert determinism and y is a discount factor used to model the short-sightedness by giving higher weights to recent rewards. In addition, incomparable queries are skipped if the total segment reward is less than a threshold. The simulated expert is allowed to make mistakes by flipping (2) randomly with probability \u20ac.\n[44] quantitatively measures the performance of PbRL al-gorithms by normalizing the average predicted returns for the true reward. In their experiments, they focus on comparing the performance of the state-of-the-art deep PbRL algorithms with non-linear utility functions [20], [27] explained in Section III-B2. The tasks used have absolute-valued states and dense rewards and are taken from the DeepMind Control Suite [45] and the Meta-world benchmark [46]. The results of the experiments indicate that both [20] and [27] only perform well in cases where the expert doesn't make errors while the performance significantly degrades once the expert preferences become more stochastic."}, {"title": "VI. APPLICATION AREAS IN NLP", "content": "PbRL algorithms have been used in practical applications that involve robot teaching tasks, board games, and others. We refer the reader to a detailed overview of those application domains discussed in [6]. In this survey, we focus on the application of PbRL algorithms to the text summarization task in Natural Language Processing (NLP) which predicts a qualitative summary by extracting the important information in an input piece of text. Solving the task using preferences instead of rewards, like ROUGE scores, led to a higher correlation with the quality of the summaries as judged by humans [47]. The two main categories of the task are extractive and abstractive summarization."}, {"title": "A. Extractive summarization", "content": "Summaries of this type are built by extracting specific sentences from the original text. [48] is one of the initial approaches utilizing PbRL algorithms for the task. In their MDP formulation, the state is the summary made so far and actions correspond to possible sentences to be extracted. They follow a linear utility-based PbRL approach discussed in Section III-B2 with the link function formulation in (1). Similar to [20], the utility resembles the reward function and is used to optimize the policy using a simple policy gradient RL algorithm. [49] proposed enhancements by including a better preference querying approach and a neural policy learning method with temporal differences."}, {"title": "B. Abstractive Summarization", "content": "Summaries of this type are built by inducing the un-derlying ideas and concisely stating them. Its high degree of subjectivity makes it much more challenging than the extractive task."}, {"title": "VII. ANALYSIS & FUTURE WORK", "content": "It was seen how the integration of deep RL algorithms into the PbRL framework by [20] revolutionized the scalability of PbRL to complex tasks and motivated a large amount of follow-up work that enhanced the efficiency successfully. Some concurrent work [35] managed to put solid foundations of theoretical guarantees for PbRL algorithms proving their robustness, while others proposed a benchmarking tool [44] for consistent and fair comparison of PbRL algorithms. Preferences also proved their effectiveness in achieving human-desired performance in challenging real-world tasks such as text summarization [51]. However, open problems still exist in the current literature work along with potential future research directions."}, {"title": "A. Formulation and performance of PbRL algorithms", "content": "State-of-the-art methods perform poorly whenever the experts make mistakes in their preferences [44]. A possible solution could be to explicitly consider the stochasticity of the expert while designing the link functions. Also, to further enhance the feedback and sample efficiency, one could experiment with using model-based approaches like [32] in more challenging environments [20]. Additionally, not enough research has been done on handling incomparable trajectories, that could have contradicting preferences, to get Pareto-optimal policies. This could be addressed by learning a set of utility functions that are non-scalar (multi-dimensional) and utilizing multi-objective RL methods [53] to get the corresponding Pareto-optimal policies. A limiting assumption in most work is the fact that all the trajectories in the preferences start at the same state. Such constraint could be relieved by utilizing the advantage function to represent the utility in terms of the expected rewards from the different initial states. Some methods [51], [54] utilized extra supervision signals like expert demonstrations to supplement the low amount of information gained from preferences. Other signals that could be worth experimenting with are providing explanations along with the preferences or enhancing the model output by allowing the experts to directly edit them.\nFurthermore, concepts from representation learning [55] could be borrowed to extend PbRL to partially observable RL or sparse reward settings which require a rich representation of states."}, {"title": "B. Safety in PbRL", "content": "To the best of our knowledge, no prior work investigated the application of PbRL in risk-averse domains. This could be implemented by over-weighing high-risk trajectories within the preferences to prioritize learning not to prefer them. In addition, further analysis should be made on adapting PbRL methods in real-world scenarios while mitigating the possibility of malicious users incurring bias in the preferences to let the model learn undesirable behaviors."}, {"title": "C. Theoretical Guarantees", "content": "Future work could focus on coming up with algorithms that exhibit both regret and finite-sample guarantees at the same time. Also, extending one of the approaches with regret or finite-sample guarantees to work within complex state-action spaces or infinite horizon settings can allow for their utilization in real-world domains."}, {"title": "D. Applications of PbRL", "content": "The state-of-the-art summarization approach by [51] still suffers from large feedback inefficiency. An interesting direction could integrate the pseudo-labeling approach by [29] to label the existing summaries automatically without querying the expert. Moreover, there are different NLP applications involving subjective tasks which could leverage PbRL to learn human-desired behaviors and these include dialogue, machine translation, and question answering."}, {"title": "VIII. CONCLUSION", "content": "PbRL algorithms have demonstrated the possibility of uti-lizing human preferences as reward signals without resorting to explicit reward engineering. This survey presented the most recent advances in the field coherently while providing insights on current open problems and potential research directions. We conclude that utility-based PbRL algorithms, especially ones with non-linear formulations, provide the stepping stone to generalizing PbRL to more complex and practical application domains. However, the high cost as-sociated with the preference feedback from experts and environment interactions creates an important research target to achieve both feedback and sample efficiency. In addi-tion, the recent formulation of concrete regret and finite-sample guarantees initiated the tractable theoretical analysis of PbRL and future work could focus more on relaxing the assumptions of existing methods while operating under more complex environment settings. Moreover, the introduction of an open-source benchmarking tool is expected to advance the consistent evaluation of PbRL methods. Lastly, PbRL proved to reach human-desired behavior in text summarization and future work should be focused on expanding it to other subjective and challenging real-world tasks."}]}