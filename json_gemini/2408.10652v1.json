{"title": "Vocabulary-Free 3D Instance Segmentation with Vision and Language Assistant", "authors": ["Guofeng Mei", "Luigi Riz", "Yiming Wang", "Fabio Poiesi"], "abstract": "Most recent 3D instance segmentation methods are open vocabulary, offering a greater flexibility than closed-vocabulary methods. Yet, they are limited to reasoning within a specific set of concepts, i.e. the vocabulary, prompted by the user at test time. In essence, these models cannot reason in an open-ended fashion, i.e., answering \u201cList the objects in the scene.\u201d. We introduce the first method to address 3D instance segmentation in a setting that is void of any vocabulary prior, namely a vocabulary-free setting. We leverage a large vision-language assistant and an open-vocabulary 2D instance segmenter to discover and ground semantic categories on the posed images. To form 3D instance mask, we first partition the input point cloud into dense superpoints, which are then merged into 3D instance masks. We propose a novel superpoint merging strategy via spectral clustering, accounting for both mask coherence and semantic coherence that are estimated from the 2D object instance masks. We evaluate our method using ScanNet200 and Replica, outperforming existing methods in both vocabulary-free and open-vocabulary settings.", "sections": [{"title": "1. Introduction", "content": "3D instance segmentation (3DIS) is a challenging research problem as it requires instance-level semantic understanding. Given a 3D scene (point cloud), 3DIS aims to produce a set of binary masks associated with their semantic labels, where each of them correspond to an object instance. Traditional methods addressing 3DIS follow a closed-vocabulary paradigm [2, 30, 34, 39], where the set of semantic categories that can be encountered at test time is the same as that seen at training time. With advances in language and vision models, the 3DIS literature has rapidly evolved from closed-vocabulary methods to open-vocabulary methods [25, 33], where the semantic categories at test time can be different from those seen during training. Open-vocabulary 3DIS methods mainly focus on obtaining i) instance-level 3D masks by leveraging class-agnostic 3D segmentation methods (e.g. Mask3D [30] or superpoints [9]), and ii) the corresponding text-aligned mask representation by aggregating text-aligned visual representations from posed images, e.g. obtained by CLIP. Scene semantics area"}, {"title": "2. Related work", "content": "Vocabulary-free models. Conti et al. [3] pioneered the vocabulary-free setting, that is assigning \u201can image to a class that belongs to an unconstrained language-induced semantic space at test time, without a vocabulary\u201d. Their method, named CaSED, retrieves captions from a database that are semantically closer to the input image. From these captions, candidate categories are extracted through text parsing and filtering. CaSED estimates the similarity score between the input image and each candidate category using CLIP, lever- aging both visual and textual information, to predict the best matching candidate. Subsequent works remain mostly in the image domain, e.g. by extending vocabulary-free image clas- sification to semantic image segmentation [4], and exploring vision language assistants for fine-grained image classifica- tion [21]. To the best of our knowledge, VoF3DIS is the first that extends such vocabulary-free setting to 3D (point-level) instance segmentation Instead of performing retrieval from web-scale database, PoVo leverages a language and vision assistant to obtain relevant semantic concepts regarding the target 3D scene, being more flexible and versatile.\nOpen-vocabulary 3D scene understanding. Recent ad- vancements in 3D scene understanding mostly involve the adaptation of Vision-Language Models (VLMs) to the 3D domain, enabling semantic understanding in the open-"}, {"title": "3. Vocabulary-free 3D scene understanding", "content": "Definition. Vocabulary-free 3D instance segmentation (VoF3DIS) aims to assign a semantic label to each 3D in- stance mask in a point cloud without relying on any prede- fined list of categories (vocabulary) at test time. Formally, given a point cloud $P = \\{p\\}$, where $p \\in R^d$ s.t. $d > 3^1$, The point cloud is decomposed into a set of 3D instance masks $M^{3D}$, where each mask $M^{3D}_i \\in M^{3D}$ is set of binary val- ues with ones indicating its corresponding points belong to the $i^{th}$ object instance, and zeros otherwise. VoF3DIS involves assigning a class $c \\in S$ to each 3D instance mask $M^{3D}_i$, where $S$ represents a unconstrained semantic space. For example, BabelNet [24] contains millions of semantic concepts, that is four magnitudes larger than the semantic classes annotated in ScanNet200 [29]. The objective is to design a function $f$ that maps 3D masks to concepts, for- mally defined as $f : M^{3D} \\rightarrow S$. At test time, the function $f$ has access to the point cloud $P$ and to a source that pro- vides vast semantic concepts approximating $S$. Potential semantic sources, as discussed in [3], can be either in the"}, {"title": "4. Our approach", "content": "Given the point cloud $P$ of a 3D scene and the corresponding set of $N$ posed images $V = \\{I_n\\}_{n=1}^N$, our method PoVo predicts 3D instance masks with their associated semantic labels without knowing a predefined vocabulary. As shown in Fig. 2, PoVo first utilizes a large vision-language assistant and an open-vocabulary 2D instance segmentation model to identify and ground objects on each posed image $I_n$, forming the scene vocabulary $C$ mitigating the risk of hallucination by the vision-language assistant.\nMeanwhile, we partition the 3D scene $P$ into geometrically-coherent superpoints $Q$, to serve as initial seeds for 3D instance proposals. Then, with the semantic- aware instance masks from multi-view images, we propose a novel procedure in representing superpoints and guiding their merging into 3D instance masks, using both the grounded semantic labels and their instance masks. Specifically, by projecting each 3D superpoint onto image planes, and check- ing its overlapping with 2D instance masks, we can aggre- gate the semantic labels from multiple views within each superpoint. Once each superpoint is associated to a semantic label, we then perform superpoint merging to form 3D in- stance masks via spectral clustering. To do so, we define an affinity matrix among superpoints constructed by both mask coherence scores computed with the 2D instance masks, and semantic coherence scores computed with the per-superpoint textual embeddings. Finally, for each 3D instance proposal, we obtain the text-aligned representation by aggregating the CLIP visual representation of multi-scale object crops from multi-view images (as in [25]). We further entich such vision-based representation with textual representation com- ing from the merged superpoints. The text-aligned mask representation enables the semantic assignment to instance"}, {"title": "4.1. Scene vocabulary generation", "content": "PoVo first utilizes a large vision-language assistant to iden- tify the scene vocabulary $C$, i.e. the list of object cate- gories in the scene, that are grounded in the multi-view images. Specifically, for each posed image $I_n$, we prompt the vision-language assistant with \"List the object names in the scene\". We then parse the response to obtain the list of objects, $C_n$, present in the image from the answer. To mitigate the potential hallucination of object pres- ence by the vision-language assistant, we subsequently em- ploy an open-vocabulary 2D instance segmentation model, e.g. grounded SAM, to ground all categories in $C_n$, obtain- ing the grounded object categories $C_n$, as well as the set of masks $M^{2D}_n$ for each object on the 2D posed image $I_n$.\nWith $C_n$, we then construct the scene vocabulary $C$ for each point cloud by retaining only the unique categories from the combined sets of $C_n$, formally as $C = \\cup_{n=1}^N C_n$, where $\\cup$ denotes the union operation.\nThe grounded object categories $C_n$, and their correspond- ing instance masks $M^{2D}_n$ on each 2D posed image, are fur- ther exploited in the process of representing and merging superpoints towards 3D instance masks $M^{3D}$, as detailed in the following section."}, {"title": "4.2. 3D instance mask formation", "content": "We leverage over-segmented geometrically-coherent super- points to initialize the process of 3D instance formation. In addition to be a common practice in prior works [25, 32, 36], we deem superpoint initialization is more generalizable, thus suits better the zero-shot setup, compared to pre- trained class-agnostic 3D instance segmentation models, e.g. Mask3D [30], which has been trained on datasets that are used for method evaluation. In the following, we detail how superpoints are generated and how they are merged into 3D instances by leveraging the results of instance segmentation on posed images.\nSuperpoint generation. We use graph cut to group points into geometrically homogeneous regions, yielding a set of $M$ superpoints $Q = \\{Q_i\\}_{i=1}^M$, where $Q_i$ is a binary mask of points in $P$. Superpoints are dense partitions of the 3D scene. Neighboring superpoints are likely to the semantic label. For example, a table, according to geometric features, might be partitioned into multiple superpoints corresponding to different surface planes, with each plane sharing the same semantic label i.e. \u201cTable\u201d. Via merging superpoints, we can form semantically-coherent 3D instance masks.\nSuperpoint merging by spectral clustering. We aim to"}, {"title": "4.3. Text-aligned point representation", "content": "Different from prior works [12, 26] that only leverages the visual encoder of pre-trained vision and language models, such as CLIP, we employ both the vision encoder and the text encoder for per-point representation to avoid the potential modality gap observed in [3]. Specifically, given a point $p_i$ and its corresponding superpoint $Q_i$, we first use the vision feature extraction method provided by Open3DIS [25] to extract the per-point feature $f_i^v$. The CLIP vision encoder is leveraged to encode image crops from multiple posed images at multiple scales, obtained by projecting their corresponding 3D mask to the posed images. We then enrich such vision- based representation with the superpoint-level feature $f_{Q_i}$, obtaining the final point-level feature $f_i$ via mean pooling."}, {"title": "5. Experiments", "content": "We evaluate PoVo on the 3D scene instance segmentation task in open-vocabulary and vocabulary-free settings. We compare PoVo with state-of-the-art methods by using two common benchmark datasets. We present quantitative and qualitative results and ablation studies.\nDatasets. We quantitatively evaluate PoVo by using 3D scans of real scenes from the ScanNet200 [29] and Replica [31] datasets. These datasets include both instance and semantic (vocabulary) annotations. ScanNet200 [29] contains a validation set of 312 indoor scans with 200 object categories, which is significantly more than its predecessor, that is ScanNet [5], which features 20 semantic classes only. Replica [31] consists of 8 evaluation scenes with 48 classes. In the supplementary material, we also present additional results obtained on the S3DIS dataset [1] (Sec. B).\nPerformance Metric. Following the experimental setup of ScanNet [5], we compute the score at average mask thresh- olds ranging from 50% to 95% in 5% increments as the Average Precision (AP). We then compute the AP at specific mask overlap thresholds of 50% and 25% as AP50 and AP25, respectively. For ScanNet200, we report results for different category groups such as APhead, APcom, and APtail [29]."}, {"title": "5.1. Analysis of the results", "content": "ScanNet200. Tab. 1 reports OV3DIS and VoF3DIS results on the ScanNet200 dataset. Following [11, 24], we test both the OV3DIS and VoF3DIS setting in the validation set.\nThe first and second sections of the table compare PoVo adapted to the open-vocabulary setting (3D/2D mask Open- vocab. semantic) and baselines. Although not explicitly de- signed for an open vocabulary setting, PoVo outperforms the other baselines. A significant distinction between PoVo and Open3DIS lies in the processing of multi-view images. PoVo retrieves concepts through an assistant and transfers their features to the 3D points, whereas Open3DIS pre-processes"}, {"title": "5.2. Ablation study", "content": "To evaluate the effectiveness of our model design, we con- ducted a series of ablation studies on the validation set of ScanNet200. More ablation studies are given in the Supple- mentary Material.\nHow effective are superpoints to guide mask represen- tation? There are two technical designs that we deem im- portant for PoVo: text embedding enhanced features and"}, {"title": "6. Conclusions", "content": "We presented a novel approach to 3D scene understanding that that operates without the need for a predefined vocab- ulary. By integrating a large vision-language assistant with an open-vocabulary 2D instance segmenter, our PoVo can autonomously identify and label each 3D instance in a scene. Furthermore, our innovative use of superpoints, in conjunc- tion with spectral clustering, enables our system to generate robust 3D instance proposals. We evaluated PoVo on two point cloud datasets, ScanNet200 and Replica, and showed that PoVo outperforms recent approaches adapted to the VoF3DIS setting, as well as in the open vocabulary setting.\nBecause PoVo can effectively exploit language and vision assistant understanding with point cloud data in a training- free manner, an exciting future research direction includes"}]}