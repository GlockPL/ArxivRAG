{"title": "Understanding Defects in Generated Codes by Language Models", "authors": ["Ali Mohammadi Esfahani", "Nafiseh Kahani", "Samuel A. Ajila"], "abstract": "This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the accuracy and functionality of the output remains a significant challenge. By using a structured defect classification method to understand their nature and origins this study categorizes and analyzes 367 identified defects from code snippets generated by LLMs, with a significant proportion being functionality and algorithm errors. These error categories indicate key areas where LLMs frequently fail, underscoring the need for targeted improvements. To enhance the accuracy of code generation, this paper implemented five prompt engineering techniques, including Scratchpad Prompting, Program of Thoughts Prompting, Chain-of-Thought Prompting, Chain of Code Prompting, and Structured Chain-of-Thought Prompting. These techniques were applied to refine the input prompts, aiming to reduce ambiguities and improve the models' accuracy rate. The research findings suggest that precise and structured prompting significantly mitigates common defects, thereby increasing the reliability of LLM-generated code.", "sections": [{"title": "I. INTRODUCTION", "content": "Pre-trained Large Language Models (LLMs) have demonstrated the capacity to manage substantial datasets and showcase exceptional performance across a diverse array of tasks [1]. In the software development context, language models trained specifically on source code, known as Code Language Models (CLMs), have shown remarkable capabilities in automating various activities such as defect fixing, code summarization, and code generation from natural language descriptions [2]\u2013[9]. This paper focuses on the latter: code generation by CLMs. Despite their impressive performance, particularly in generating syntactically correct code, these models are not without flaws. Even the best models achieve an exact match accuracy of around 20%, and the generated code often contains defects that can hinder its practical utility, posing challenges for developers who rely on these tools.\nThere is a strong body of research focused on creating more effective models by improving learning algorithms, increasing model size, and incorporating large, high-quality datasets. While these efforts are foundational, achieving perfect CLMs does not seem attainable in the near future. Therefore, to effectively apply and adapt these models in the software engineering context, it is essential to understand and account for the limitations of existing CLMs to achieve successful outcomes. Although some efforts have been made in this area (e.g., [10], [11]), more research is needed.\nThis paper aims to analyze the defects in generated code and investigate how prompt engineering techniques can fix some of these defects. Understanding both aspects is crucial for the better application and improvement of CLMs. Identifying the types of defects not only helps to direct foundational research but also ensures that CLMs can be applied more safely, particularly in applications requiring a more conservative approach. Additionally, prompt engineering is a cost-effective technique to improve CLM results, as opposed to retraining, which is very extensive and time-consuming. More specifically, our research address the following Research Questions (RQs):\nRQ1 What are the types of defects in the generated code, and how can they be classified based on their characteristics?\nWe selected two existing CLMs based on their availability and features, and applied these models to the well-known open-source benchmark, HumanEval. From the results, we extracted all generated code with defects. We then manually analyzed and classified these defects based on existing categorizations for source code. Additionally, we analyzed the complexity of the generated code to understand if there is a relationship between code complexity and the performance of the CLMs. The results show that the majority of defects are related to Functional and Logic errors. Interestingly, there is no correlation between the correctness of the generated code and its complexity. This is surprising, as it differs from defects in developer-written code, where the probability of failure typically increases with complexity.\nRQ2 Can existing prompt engineering techniques help in fixing the problematic code? We studied existing prompt engineering techniques and applied them to the defective generated code identified in RQ1, focusing on their potential to produce correct code. The results, using the Exact Match (EM) metric, indicate that applying these techniques can improve outcomes by approximately 33.1% for CodeT5+ moel and 27.8% model for CodeGen. Among the techniques, the Structured Chain-of-Thought (SCoT) technique achieved the highest improvement.\nThe remainder of this paper is organized as follows: Section"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "This section covers the core concepts on which our work is based and examines existing studies relevant to our work.\nLanguage models are designed to handle tasks involving language by estimating the probability of sequences of tokens to predict an output, typically a sequence of generated tokens [12]. CLMs are a specific type of language models tailored for programming-related activities, which are central to our research. These models are typically trained on a variety of programming languages as well as textual data [2], [13], [14]. CLMs are particularly effective for tasks including code summarization, code generation, program repair, and code translation. The architecture of language models predominantly draws from the integration of a self-attention layer with a feed-forward network, a concept first introduced by Vaswani et al. [15]. These components are repeated in sequences to build encoders and decoders, which are fundamental in constructing language models. In this study, the focus is on two types of language model architectures: encoder-decoder models and decoder-only models. Encoder-only models are not used in this research as they lack the capability to perform generative tasks without an additional decoder [16], and are therefore not relevant to our analysis.\nEncoder-decoder models are a type of model that incorporates both an encoder and a decoder. The sequence-to-sequence transformer [15] exemplifies this model type, serving as a foundation for many modern language models. In an encoder-decoder model, the encoder processes the input token sequence and transforms it into a hidden intermediary representation. This representation is then utilized by the decoder to produce the output token sequence [17], [16]. The encoder in these models typically includes several layers, each consisting of two components: a multi-head self-attention mechanism and a fully connected feed-forward network. Similarly, the decoder comprises several layers, each equipped with three sub-layers: a masked multi-head attention mechanism, a multi-head self-attention mechanism, and a fully connected feed-forward network [15]. The multi-head self-attention mechanism applies linear transformations to the inputs, creating multiple projections of the input that are later recombined into the final output. The decoder sequentially produces the output tokens in an auto-regressive manner, choosing each subsequent token based on both the input to the model and the partially formed output sequence. In this study, we used CodeT5+ [13]- an encoder-decoder model.\nDecoder-only models consist solely of a decoder, without an accompanying encoder. These models initiate with a baseline state and incrementally construct an output sequence, taking into account the tokens previously generated [17], [18]. The functionality of the decoders in these models mirrors that found in encoder-decoder architectures, with a focus on comprehending the target language and its subtleties [17]. In this study, we used CodeGen [14]- a decoder-only model."}, {"title": "B. Related work", "content": "Prior research has often not deeply analyzed why specific models, like CodeBERT [19] and GraphCodeBERT [20], succeed or fail in certain scenarios. For instance, a study by Mohammadkhani et al. [11] explores how these models allocate attention to different token types during tasks such as code summarization and translation, revealing specific deficiencies when handling complex or lengthy code segments. While their work directly contributes to understanding model failures through attention-based explanations, our research goes further by providing a more comprehensive defect categorization. Unlike their study, which primarily focuses on attention allocation, this research not only identifies where models fail, but also explores why, by evaluating a broader range of defect types across multiple dimensions of code generation tasks.\nThe study by Tambon et al. [10] provides a foundational analysis of bugs in code generated by LLMs, identifying ten specific bug patterns through empirical methods supported by insights from 34 industry and academic practitioners. While this work highlights common issues such as syntax errors and misinterpretations, it covers a narrower spectrum of defect types compared to our research. In contrast, our study delves into a more exhaustive classification of defect categories, which encompasses not only syntax and semantic errors but also extends to logical and execution errors that affect the functionality of the generated code. Moreover, our research employs an approach integrating structured prompt engineering to systematically reduce these defects. By broadening the defect categories and introducing an intervention method that directly impacts code generation performance, our research provides a more comprehensive solution to the challenges of LLM-generated code.\nA study by Tan et al. [21] introduces the Codeflaws benchmark, a valuable contribution to the field of automated program repair through its detailed classification of 39 defect types derived from programming competition submissions. This benchmark enables a detailed evaluation of repair tools by categorizing defects based on syntactic differences between buggy and corrected programs. While the benchmark provides a foundational approach to understanding syntactic defects, this research broadens this perspective by including semantic and logical error categories. These additional categories allow for a more comprehensive assessment of the deeper functional inaccuracies that occur in code generation by LLMs. This approach not only broadens the scope of defect detection but also enhances the understanding of underlying issues in code generation processes."}, {"title": "III. STUDY DESIGN AND RESULTS", "content": "This study aims to develop a comprehensive understanding of defects associated with code generation using LLMs by analyzing their types, underlying causes, and characteristics. To achieve this goal, we seek to answer the following RQS:\n\u2022 RQ1 What are the types of defects in the generated code, and how can they be classified based on their characteristics?\n\u2022 RQ2 Can existing prompt engineering techniques help in fixing the problematic code?\nThe first question focuses on analyzing and classifying the defects in the generated code, while the second investigates the effectiveness of prompt engineering techniques in addressing these issues. In the following, we describe the evaluation metrics we employ. We then explain the experimental process, present the results, and discuss their practical implications.\nTo evaluate the correctness of generated code by LLM, we used three evaluation metrics, including exact match accuracy (EM) [22], pass@k [23], and CodeBLEU [24].\nThe EM metric refers to the proportion of instances where the response from the system exactly matches the correct or expected answer. In EM, if the model's generated answer is an exact match with the ground truth, the EM is 1; otherwise, it is 0.\n$EM = \\begin{cases}\n1 & \\text{if the model's answer exactly matches} \\\\\n  & \\text{the provided answer} \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nChen et al. [5] show that metrics like BLEU [25] fail to capture essential semantic aspects of coding, and propose adjustments to better reflect the unique features of code. Further, they highlight that match-based metrics do not consider the wide range of functionally equivalent programs that can meet the same requirements as the reference solution. To overcome these limitations, recent studies in fields such as unsupervised code translation [26] and pseudocode-to-code translation [23] have shifted towards measuring functional correctness. In this approach, a code sample is considered correct if it successfully passes a predefined set of unit tests. This shift acknowledges that functional correctness aligns more closely with the criteria used by developers in real-world scenarios. In pass@k metric [23], multiple (k) code samples are generated for each problem, and a problem is considered successfully solved if any of these samples pass all relevant unit tests.\n$pass@k := Eproblems 1 - {\\binom{n-c}{k} \\over {\\binom{n}{k}}}$\nThe BLEU metric [25], originally designed for evaluating machine translation tasks, measures the extent of overlapping n-grams between a candidate translation and a set of reference translations. However, BLEU is inadequate for evaluating code-related tasks as it does not account for the tree structure and logical flow inherent in code. Code-BLEU [24] addresses this limitation by incorporating code-specific information into BLEU measurements. CodeBLEU integrates traditional n-gram matching with syntactic analysis using abstract syntax trees, which compares the structural alignment of code segments. It also includes semantic analysis via data-flow graphs, which checks for semantic consistency by analyzing how data moves through the code."}, {"title": "C. Defect Classification", "content": "To provide a better understanding of defects in generated code, we have classified them into six categories and nineteen subcategories. Our classification builds upon the work of Ni et al. [28], who examined key aspects of defect resolution and the origins of these defects using the Orthogonal Defect Classification (ODC) method [29]. This approach offers a comprehensive framework for analyzing defects, with particular emphasis on localization, elimination, cause analysis, and prevention. The ODC method classifies defects into eight distinct attributes: Activity, Triggers, Impact, Target, Type, Qualifier, Source, and Age, focusing specifically on attributes related to source code. Ni et al.'s study [28] investigated the correlation between defects fixes and the causes of defects, using the ODC method to categorize causes based on defect type attributes related to the source code.\nTo have a more refined categorization, we have also integrated the IEEE classification method into our analysis. The IEEE standard for classifying software anomalies [30] provides a detailed framework for categorizing a wide spectrum of software issues, outlining a systematic approach to managing software anomalies throughout the software lifecycle. This classification is executed through four sequential steps, interspersed with three administrative activities, ensuring a thorough characterization of software defects. The table I outlines our classification, which includes six categories: Function, Logic, Computation, Assignment, Runtime, and Others.\n\u2022\nFunction category refers to defects include logical inconsistencies, unexpected behaviors, or outcomes that do not align with the desired code behavior, indicating a gap between what is expected and what is produced. We have identified two subcategories including functional error and algorithm error. Functionality errors represent discrepancies between the generated code, and the specified requirements. Algorithm errors point towards inaccuracies in the algorithmic steps employed to solve a particular problem or calculation. Such discrepancies result from faults in calculations, faulty implementations of algorithms, or a misinterpretation of the underlying computational logic.\n\u2022\nLogic category refers to defects in the logical structure of the code, such as incorrect branch and loop, ignore extreme conditions, redundant logic, conditional test error, and logical order error. The details of these defects can be found in Table I.\n\u2022\nComputation category refers to defects in the mathematical and computational operations within the code, which includes incorrect operand, operator error, and insufficient precision.\n\u2022\nAssignment category refers to defects related to data assignment, which includes incorrect data range or type, and input or output data error.\n\u2022\nRuntime category refers to defects that occur during the execution of the code, which includes typo, index error, type error, overflow, and zeroDivisionError.\n\u2022\nOthers category refers to defects not covered by the above categories."}, {"title": "D. Model Selection", "content": "A comprehensive review by Hou et al. [17] examined the use of LLMs in software engineering, identifying over 50 models applicable to various tasks within this field. Given the impracticality of evaluating all available LLMs due to computational constraints, we have developed specific criteria to select the LLMs for our study addressing RQ1.\nOpen Source. The study focused on models that are open-source to facilitate replicability. This criterion excludes models such as GPT-4 [31], AlphaCode [32], PaLM-Coder [33], and Codex [5].\nPre-Trained on Code. This study aims to understand defects in LLM-generated code, so it is necessary to use models that have been specifically pre-trained on large code datasets. As a result, we are not considering models trained on general natural language datasets. For instance, models such as T5 [34], GPT-Neo [35], and GPT-J [36] are excluded based on this criterion.\nOld Models. Models that have been outdated by more recent versions were excluded from our study. For example, we have not included CodeT5 [37] because it has been replaced by its updated version, CodeT5+ [13].\nUsing the above criteria, we limited our selection to two LLMS: CodeT5+ [13], an encoder-decoder model with 770 million parameters, and CodeGen Multi [14], a decoder-only model with 350 million parameters."}, {"title": "E. RQ1 What are the types of defects in the generated code, and how can they be classified based on their characteristics?", "content": "1) Overall Results: To answer this question, we presented each prompt from the benchmark to the selected models and executed each generated code snippet by the models to test their functionality against the benchmark's test cases. For the CodeT5+ model, our results indicated that 26 instances (15.85%) successfully produced accurate and functional code\u2014passed all test cases. However, 34 instances (20.73%) either failed to generate any code, produced incomplete snippets, or simply returned the given prompt without a code snippet. Furthermore, 104 instances (64.41%) generated code but contained one or more defects.\nSimilarly, for the CodeGen model, out of 164 instances, only 11 instances (6.67%) successfully produced accurate and functional code, passing all test cases. Additionally, 32 instances (19.51%) failed to generate any code or returned incomplete snippets, while 121 instances (73.78%) generated code that contained one or more defects. These findings highlight the challenges LLMs face in producing error-free and complete code under automated conditions.\nExact Match (EM), pass@k (with K=5), and CodeBLEU are used to assess the correctness of the code generation models. The EM metric for CodeT5+ showed that 15.85% of the code generated matched the ground truth, indicating an understanding of the tasks. For the CodeGen model, this metric significantly dropped to 6.67%, emphasizing the model's challenges in achieving syntactic and semantic precision.\nThe pass@k metric, which evaluates the likelihood of achieving at least one correct solution within five attempts for each problem, showed an effectiveness of 20.7% for CodeT5+ but was much lower for CodeGen, standing at 8.42%. This metric is particularly valuable as it provides a more lenient and realistic measure of a model's capability in practical applications, compared to the more stringent EM metric.\nFor the CodeBLEU metric, which combines syntactic and semantic evaluation with traditional n-gram overlap, the average score for CodeT5+ was 29.5%, indicating moderate success in generating code that, while not always perfect, maintains high degrees of coherence and functionality. The score for CodeGen was lower, at 20%, further highlighting the model's issues with generating functionally and syntactically coherent code. This metric provides insight into the practical utility of generated code in real coding environments.\nThe evaluation metrics employed to assess the code generation performance of the models are summarized in Table III. This table provides a clear understanding of the accuracy of the different models used in this study."}, {"title": "2) Classification of the Defects", "content": "We conducted a detailed manual inspection of the code snippets generated by the LLMs to identify potential defects. The manual review of code generated by LLMs revealed 367 defects, categorized in Table II, which provides a detailed breakdown of various defect categories and their respective frequencies. In the following, we discuss the most frequent defects that appeared in the generated code based on their category."}, {"title": "Function Category.", "content": "Algorithm and functionality errors were the main defects, appearing in 108 (29.3%) and 116 (31.6%) instances, respectively. These defects highlight challenges in the model's capacity to understand and correctly execute the intended functionality. For instance, given task to the model is (Figure 1d): \u201cGiven a positive integer \u201cn\u201d, return the product of the odd digits. Return 0 if all digits are even\". The generated code checks whether the given integer \"n\" is even and returns \"n\" if it is. However, the task requires finding the product of the odd digits. Algorithm errors, on the other hand, account for 108 instances, indicating a distinct set of challenges related to the computational aspects of the generated code. Algorithm errors highlight a crucial aspect where the model's understanding of the specified computational procedures fails, resulting in deviations from the intended problem-solving approach. For instance, the given task is (Figure 1b): \"You're a hungry rabbit, and you already have eaten a certain number of carrots, but now you need to eat more carrots to complete the day's meals. you should return an array of [total number of eaten carrots after your meals, the number of carrots left after your meals] if there are not enough remaining carrots, you will eat all remaining carrots, but will still be hungry.\". The generated code does not correctly calculate the total number of eaten carrots and the carrots left after the meals. Instead, it returns in various conditions without considering the actual calculations based on the input parameters."}, {"title": "Logic Category.", "content": "Conditional test error occurs in 89 (24.1%) instances. For example, the task is (Figure 1c): \"given a non-empty array of integers arr and an integer k, return the sum of the elements with at most two digits from the first k elements of arr\". The generated code instead summed all elements after the first k elements. Incorrect loop error where the loop logic is not correct, appeared in 17 instances (24.1%). Example of this defect (Figure 1a), where the task is: \"Given two positive integers a and b, return the even digits between a and b, in ascending order\". The generated code used a while loop that continued until \"a\" becomes zero. The generated code did not consider the range of numbers between a and b."}, {"title": "3) Complexity of the Defective Generated Code:", "content": "In addition to classifying the defects, we examined the complexity of the expected codes for all samples to determine if the defects in the generated code by the LLM models are related to the code complexity. In our study, we used cyclomatic complexity to assess the complexity of the generated code. Cyclomatic complexity measures the number of linearly independent paths through a program's source code [38].\nGiven the nature of our data, which consists of a binary outcome (1 for success and 0 for failure) and continuous data for cyclomatic complexity, we employed logistic regression [39] to perform correlation analysis. Logistic regression is well-suited for scenarios where the dependent variable is categorical, allowing us to model the probability of success as a function of cyclomatic complexity. The results of the logistic regression indicated a Pseudo R-squared value of 0.005523, suggesting that the model explains a very small portion of the variance in success rates. Additionally, the logistic regression model yielded a non-significant p-value (0.418) for the coefficient of cyclomatic complexity, indicating that increases in complexity do not significantly decrease the likelihood of successful code generation by LLMs.\nFurthermore, the correlation analysis between cyclomatic complexity and success rate yielded a coefficient of -0.063, indicating a very weak negative relationship. This weak correlation supports the logistic regression results, suggesting that complexity alone may not be a decisive factor in predicting code generation success. This suggests the need for new research in this area to understand the root causes of model failure. It further motivates our next research question, where we investigate how do prompt engineering techniques impact the quality of the code generated by LLMs."}, {"title": "F. RQ2 Can existing prompt engineering techniques help in fixing the problematic code?", "content": "As discussed in RQ1, a significant portion, namely 34%, of the defects identified pertained to functionality errors. In addressing the issues discussed in RQl, one key challenge with LLMs is the potential for misunderstandings between humans and LLMs. Questions that seem clear to humans may be misinterpreted by LLMs. To improve the reliability of these models, we used five prompt engineering techniques commonly applied to code generation by LLMs: Scratchpad Prompting [40], Program of Thoughts (PoT) Prompting [41], Chain-of-Thought (CoT) Prompting [42], Chain of Code (CoC) Prompting [43], and Structured Chain-of-Thought (SCOT) Prompting [44]. These techniques enhance the effectiveness of our dataset prompts. Each prompt in the dataset was carefully engineered based on the specific technique used. In the following, we describe these prompt engineering techniques and demonstrate their application to the odd digits example discussed in Section III-E2 (also shown in Figure 1d). We then present the results of their application.\nScratchpad Prompting. Scratchpad [40] provides a mechanism for the model to generate and record intermediate steps or thoughts, which helps in breaking down complex tasks into simpler, manageable parts. This method logs each computational step, ensuring clarity and accuracy. Tuning the scratchpad model is crucial to ensure that it captures and processes these intermediate steps effectively. The tuning involves structuring the training data to reflect the detailed breakdown of tasks into smaller subtasks, mimicking a step-by-step human problem-solving approach. Each training example consists of a sequence of operations with their expected intermediate outputs, teaching the model to progressively build towards the final answer. In tasks requiring multi-step calculations like integer addition, training examples explicitly detail each arithmetic step and the intermediate sums. This method improves out-of-distribution generalization by exposing the model to a variety of computational pathways during training.\nIn the context of the running example that asks the model to calculate the product of the odd digits in a given number (e.g., 235), the process begins by initializing the product to 1 (input: none, output: product = 1) and creating an empty list for odd digits (input: none, output: empty list). The model then processes the integer 235 by splitting it into its digits (input: 235, output: [2, 3, 5]). Each digit is evaluated individually: the digit 2 is even and is skipped (input: 2, output: none), while the digits 3 and 5 are identified as odd. The product is updated sequentially with each odd digit (input: 3, output: product = 3; input: 5, output: product = 15). The final product of 15 is obtained after multiplying the odd digits (input: [3, 5], output: product =15).\nProgram of Thoughts (PoT) Prompting. PoT [41] leverages LLMs to facilitate step-by-step reasoning and computation for complex numerical reasoning tasks. Unlike traditional methods that combine reasoning and computation within the model, PoT distinctly separates these processes. It allows the model to focus on generating a programmatic description of the reasoning process in a high-level programming language, such as Python. The actual computations are then performed by an external interpreter. This separation enhances the model's performance by reducing computational faults common in language models, particularly with complex or iterative calculations. Also, PoT supports more structured reasoning by enabling detailed programming instructions, which can include loops, conditional statements, and function calls, leading to more accurate and interpretable solutions.\nIn the context of the example, PoT first defines a Python function, product-of-odd-digits, to compute the product of odd digits from the input integer (input: function definition, output: function structure). The function processes the number 235 by converting it into a list of its digits (input: 235, output: [2, 3, 5]). A list comprehension filters out even digits, leaving only the odd digits (input: [2, 3, 5], output: [3, 5]). It initializes a product variable at 1 and iterates over the odd digits, multiplying them together (input: [3, 5], output: product = 15). If no odd digits were present, the function would return 0 (input: empty list, output: 0). But, in this case, it computes the product to be 15 (input: execute function with 235, output: 15). The function is executed to provide the result (Figure 2d).\nChain-of-Thought (CoT) Prompting. This technique enhances the reasoning capabilities of language models by guiding them to articulate a logical sequence of thoughts leading to a solution [42]. This encourages models to simulate a human-like problem-solving approach where each step in reasoning is explicitly stated before arriving at the final answer. By employing CoT prompting, models are better equipped to tackle a variety of complex reasoning tasks, such as arithmetic, commonsense, and symbolic reasoning. CoT does not require extensive retraining with large datasets but instead leverages few-shot learning, where the model learns from a small number of examples to generalize its reasoning capabilities to new tasks. The transparency provided by CoT also offers a valuable window into the model's cognitive process, allowing for easier debugging and understanding of how models arrive at certain conclusions.\nIn the context of the running example, CoT method begins by splitting the number 235 into its individual digits (input: 235, output: [2, 3, 5]). It identifies which digits are odd (input: [2, 3, 5], output: [3, 5]) and then calculates the product of these odd digits (input: [3, 5], output: 15). Each step is carefully detailed, from the initial input of the integer to the intermediate identification of odd digits and the final computation of their product. The final answer of 15 (input: calculate product, output: 15) reflects a clear sequence of thought and computation (Figure 2c).\nChain of Code (CoC) Prompting. CoC [43] improves upon the traditional CoT [42] by integrating executable code snippets, which allows language models not only to generate code but also to execute it. The technique works by encouraging language models to format solutions as flexible pseudocode. When executed, this pseudocode is processed directly by a code interpreter for computable tasks, or simulated by the language model itself for tasks that involve semantic understanding or are not directly executable. This dual capability expands the types of problems the model can solve, from straightforward computational tasks to complex reasoning problems involving semantic analysis.\nIn the context of the example, CoC starts with the integer 235, which the model translates into executable code that first parses this integer into a list of its individual digits (input: 235, output: [2, 3, 5]). It then filters this list to retain only the odd digits (input: [2, 3, 5], output: [3, 5]). Following this, the model computes the product of the retained odd digits (input: [3, 5], output: 15). The entire sequence-extraction, filtering, and multiplication is dynamically generated as a coherent block of code and executed to confirm the solution (input: generated code execution, output: 15).\nStructured Chain-of-Thought (SCoT). This technique [44] refines the concept of CoT by integrating structured programming paradigms into the reasoning process of language models. It asks the model to first conceptualize the problem-solving steps using common programming structures-sequence, branch, and loop-before generating the actual code. This method leverages the structured nature of programming to enhance the model's ability to generate syntactically correct and logically coherent code.\nSCOT guides the model to think in terms of control flows and data structures, reflecting a more accurate simulation of a programmer's thought process when faced with a coding task.\nIn the context of the example, SCoT begins by initializing a product variable to 1 and a boolean flag foundOdd to False, preparing for the processing of each digit within the integer n (input: None, output: product = 1, foundOdd = False). The integer n is then converted into a string to allow iteration over each character, representing the digits (input: 235, output: '2', '3', '5'). As the iteration proceeds, each digit is evaluated to determine its parity. Even digits are bypassed, while odd digits trigger an update to the product variable, which is multiplied by the digit value, and the foundOdd flag is set to True (input: '3', output: product = 3; input: '5', output: product = 15). This sequence of checks and updates-initialization, digit evaluation, and conditional updating ensures that each digit is processed. Following the completion of the loop, a final evaluation checks the foundOdd flag. If no odd digits were found (foundOdd = False), the function returns 0, indicating the absence of odd digits (input: foundOdd = False, output: 0). Otherwise, the accumulated product of the odd digits is returned (input: foundOdd = True, output: 15)."}, {"title": "Results:", "content": "We applied each of the discussed prompt engineering techniques to all defective samples within the benchmark. This systematic application allowed us to comprehensively evaluate the effectiveness of each technique in enhancing the performance of generated code by CodeT5+ and CodeGen models. The results are shown in Table IV. These improvements were measured using the EM metric, which gauges the accuracy of the code generated by the models. The data reveals that each technique enhances the model's code generation capabilities by refining the prompts to reduce ambiguities and improve logical flow in the generated code.\nThe SCOT prompting technique yielded the highest improvement, with a 17.25% increase for CodeT5+ and a 21.13% increase for CodeGen. This technique effectively guides the language model through a structured reasoning process, enhancing the model's ability to handle complex coding tasks by breaking them down into manageable, logically connected steps. Conversely, the CoT prompting technique, while still beneficial, provided the least improvement among the techniques we tested. This method improved EM by 12.05% for CodeT5+ and 10.93% for CodeGen. Although it offers clear advantages, its relatively lower performance suggests that it may be more suited to tasks that require straightforward sequential reasoning rather than complex algorithmic processes.\nThe PoT prompting and Scratchpad prompting techniques also showed notable improvements. PoT resulted in a 15.35% improvement for CodeT5+ and a 15.93% improvement for CodeGen. Scratchpad prompting improved EM by 12.75% for CodeT5+ and 14.73% for CodeGen. Lastly, the CoC prompting technique provided improvements, with a 16.45% increase for CodeT5+, and a 19.53% increase for CodeGen."}, {"title": "IV. CONCLUSION", "content": "Our analysis of code generation by Code Language Models (CLMs) revealed challenges. This paper categorizes and analyzes 367 defects identified primarily as functionality and algorithmic errors. These errors indicate areas where LLMs frequently fail and the need for improvement. The paper also demonstrated that the prompt engineering techniques can improve code accuracy and reliability. On average, the techniques improved the Exact Match (EM) accuracy by approximately 15.44% for CodeT5+ and 16.45% for CodeGen, with the SCOT Prompting showing the most significant gains at 17.25% and 21.13% respectively. This suggests that future research should focus on developing advanced prompt engineering strategies and assessing their impact across diverse coding environments.\nFuture studies could expand the model comparison to include potentially more capable LLMs such as CodeLlama, GEMMA, and Llama3. Additionally, a detailed examination of how each prompt engineering technique affects specific categories of defects could be included. Such analysis would enable a more targeted approach in applying these techniques based on the nature of the defect, potentially leading to more refined improvements in code generation accuracy and reliability."}]}