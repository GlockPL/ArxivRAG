{"title": "From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures", "authors": ["Minglu Zhao", "Dehong Xu", "Tao Gao"], "abstract": "Attention is a cornerstone of human cognition that facilitates the efficient extraction of information in everyday life. Recent developments in artificial intelligence like the Transformer architecture also incorporate the idea of attention in model designs. However, despite the shared fundamental principle of selectively attending to information, human attention and the Transformer model display notable differences, particularly in their capacity constraints, attention pathways, and intentional mechanisms. Our review aims to provide a comparative analysis of these mechanisms from a cognitive-functional perspective, thereby shedding light on several open research questions. The exploration encourages interdisciplinary efforts to derive insights from human attention mechanisms in the pursuit of developing more generalized artificial intelligence.", "sections": [{"title": "1. Introduction", "content": "Over the past few years, the field of artificial intelligence (AI) has experienced a significant transformation with the introduction of the Transformer architectures, which have rapidly become the cornerstone of many state-of-the-art models in natural language processing (NLP), computer vision, and beyond. These architectures incorporate the concept of attention which echos the complex cognitive process of human attention, a remarkable ability that enables us to focus on specific aspects of our environment while effectively filtering out extraneous information. Our review article aims to provide an in-depth comparative analysis of human attention and Transformer architectures, systematically examining the similarities and differences from various perspectives including vision, language, and agency.\nAttention has been one of the most studied topics in cognitive psychology and influences a broad range of cognitive processes, contributing to perception, memory, and cognitive control. Classic studies in cognitive psychology have underlined attention's role as a filtering mechanism that selectively processes relevant information from the environment while managing cognitive resources. Additionally, attention as a mental construct is intertwined with self-regulation and social communication processes, facilitating not only individuals' task commitment but also cooperative interactions. On the other hand, AI models with the notion of attention, exemplified by Transformer architectures, have shown great versatility and robustness in various applications. Initially designed for language processing, Transformer models have profoundly transformed the NLP landscape. By utilizing self-attention mechanisms to grasp the contextual relationships in sequences, they have made remarkable advancements in various tasks, including machine translation, sentiment analysis, and text summarization. In the computer vision domain, Transformer models have been applied to large-scale visual tasks and are able to capture long-range dependencies and hierarchies in image data. Several recent architectures further incorporate Transformer architectures in decision-making tasks, treating decisions as a sequence generation task and thereby enabling the model to learn optimal action strategies based on past experiences.\nThe shared terminology of \u201cattention\u201d in human cognitive studies and Transformer architecture has given rise to intriguing parallels yet certain ambiguities concerning their relationship. Although both emphasize the selective processing of contextual information, human attention and the Transformer model present distinct disparities in their capacity constraints, attentional pathways, and intentional mechanisms. In this article, we systematically compare the two domains around these functional attributes. We note that this article does not serve as an exhaustive review of either the human attention mechanism or the Transformer architecture, but rather a focused comparative analysis seeking to identify open directions for better incorporating insights from human attention to attention-based models in AI. We would also like to point out several reviews for a more comprehensive review of the topics."}, {"title": "2. Attention modeling and Transformer architecture", "content": "The principle of attention has been incorporated into various domains of AI", "attention\" mechanism that takes into account different words' relevance in a sentence when processing each word. This approach can be compared to how we, as humans, selectively focus on certain aspects of a scene or conversation while tuning out the rest. Specifically, this is achieved through the self-attention mechanism, which computes the relationships between all pairs of elements in an input sequence. The core idea is to assign different weights to different elements based on their relevance to the current element being processed. The self-attention mechanism is defined as follows": "n$$Attention(Q", "W^V": "Q = XW^Q", "asking questions\\\"\nabout certain words (the queries) and \\\"looking up answers\\\" in other words (the keys). This \u201cquestioning": "nd \u201canswering\" process allows the Transformer to understand the interdependencies between words in a sentence. The relevance between one word's key and another word's query is computed by taking the dot product of the respective query and key ($Q K^T$)", "content\" that the Transformer wants to focus on, and the role of the keys and queries is to determine how much attention each value should be given.\nMulti-head attention The multi-head attention mechanism in the Transformer model is a crucial step that allows the model to capture different aspects of the meaning of a sentence. Essentially, multi-head attention means running not one, but multiple attention mechanisms (or \u201cheads\u201d) in parallel, each focusing on different \"types\" or \"aspects\" of the input sequence's information. This is defined in the following equations": "n$$MultiHead(Q", "as": "n$$head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V).$$"}]}