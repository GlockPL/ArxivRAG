{"title": "Knowledge-enhanced Transformer for Multivariate Long Sequence Time-series Forecasting", "authors": ["Shubham Tanaji Kakde", "Rony Mitra", "Jasashwi Mandal", "Manoj Kumar Tiwari"], "abstract": "Multivariate Long Sequence Time-series Forecasting (LSTF) has been a critical task across various real-world applications. Recent advancements focus on the application of transformer architectures attributable to their ability to capture temporal patterns effectively over extended periods. However, these approaches often overlook the inherent relationships and interactions between the input variables that could be drawn from their characteristic properties. In this paper, we aim to bridge this gap by integrating information-rich Knowledge Graph Embeddings (KGE) with state-of-the-art transformer-based architectures. We introduce a novel approach that encapsulates conceptual relationships among variables within a well-defined knowledge graph, forming dynamic and learnable KGES for seamless integration into the transformer architecture. We investigate the influence of this integration into seminal architectures such as PatchTST, Autoformer, Informer, and Vanilla Transformer. Furthermore, we thoroughly investigate the performance of these knowledge-enhanced architectures along with their original implementations for long forecasting horizons and demonstrate significant improvement in the benchmark results. This enhancement empowers transformer-based architectures to address the inherent structural relation between variables. Our knowledge-enhanced approach improves the accuracy of multivariate LSTF by capturing complex temporal and relational dynamics across multiple domains. To substantiate the validity of our model, we conduct comprehensive experiments using Weather and Electric Transformer Temperature (ETT) datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "RESEARCH in multivariate Long Sequence Time-series Forecasting (LSTF) has gained prominence in recent years due to its wide practical applications [1], [2], [3]. Real-world applications of time-series forecasting span a wide range of domains, including weather and climate forecasting [4], energy grid analysis [5], [6], traffic volume estimation [7], [8], [9], retail sales forecasting [10] and financial market predictions [11], [12]. These comprehensive applications highlight the continuing significance of time-series forecasting and provide scope for evolving methodologies to address these applications. Furthermore, the time-series forecasting problem itself presents significant challenges due to its intricate temporal patterns and complex dependencies among variables. Incorporating the additional parameter of multivariable long-sequence forecasting adds a layer of complexity to the analysis. Numerous classical methods, including statistical models such as Vector Autoregression (VAR) [13], Gaussian Process [14], Support Vector Regression [15] and deep learned-based models such as Multi-layer perceptrons (MLP), Recurrent Neural Networks (RNNs), Long-Short Term Memory (LSTM) and Convolutional Neural Networks (CNNs) have been employed extensively and have demonstrated significant potential in short sequence forecasting across various domains.\nHowever, classical methods are not devoid of limitations. They often struggle to capture complex temporal patterns. Additionally, the non-stationarity of real-world time-series data poses a significant challenge thereby resulting in lack of robustness. Thus, accurately quantifying complex multivariate relationships continues to pose formidable challenge. For deep learning techniques in the context of LSTF, issues such as suboptimal parameter estimation, poor generalization and gradient instability are frequently encountered.\nWith the advent of enhanced parallel computational systems [16] and the groundbreaking success of the Transformer [17] architecture, the landscape of research in time-series forecasting has undergone a significant transformation. These attention mechanism-based architectures have demonstrated an exceptional ability to discern critical patterns in sequential data, thus marking a pivotal shift in the field. Studies such as Informer [18], Autoformer [19], FEDFormer [20] and PatchTST [21] have been seminal applications of the Transformer architecture for long sequence time-series forecasting (LSTF). These studies have surpassed the conventional forecasting methods on various benchmark datasets from real-world applications such as Weather [22], Traffic [23], ETT [18], Electricity [24], ILI [25] and many more. The attention mechanisms are adept at analyzing multivariate time-series data and are highly effective in identifying temporal patterns on an individual timestamp level. However, it is pertinent to recognize that the attention mechanism can overlook crucial multivariate structural information, particularly in long-term forecasting [26]. This oversight is especially relevant to the nuanced structural or spatial relationships crucial for a comprehensive understanding of the dataset [27]. To address this issue, incorporating spatio-temporal training in LSTF problem offers a compelling opportunity to explore the integration of spatial relations alongside temporal dependencies for more accurate forecasting.\nGraph-based methodologies in LSTF have attracted considerable interest owing to their potential in modeling complex interactions between spatial and temporal dependencies [28], [29], [30], [31]. However, current research heavily relies on dynamic knowledge graph frameworks and computationally intensive Graph Neural Networks (GNN) or intricate Graph Convolutional Networks (GCN) [32], [33]. Despite their complexity, these approaches frequently fall short when assessed against standardized configurations of state-of-the-art (SOTA) models like PatchTST, Autoformer, Informer and FedFormer, particularly for longer forecast horizons. Researchers have primarily emphasized dynamic knowledge graphs to comprehend spatio-temporal dynamics. The integration of intuitive and conventional knowledge graphs is less evident due to the superior performance of dynamic counterparts in temporal tasks. Hence, there is a significant gap in the research landscape concerning experiments with the integration of knowledge graphs with complex deep-learning architectures for temporal tasks.\nOur proposed approach aims to overcome the constraints of current graph-based architectures in LSTF by introducing a novel approach that encapsulates inherent relationships among variables within a knowledge graph and integrates them into transformer-based frameworks by forming dynamic and learnable Knowledge Graph Embeddings (KGEs). In the context of multivariate LSTF, we construct a basis for initial information relationships among the variables in the form of a knowledge graph by deriving conceptual relationships from literature, further producing the dynamic KGE. These embeddings augment transformer-based architectures with crucial spatial relations, thereby enhancing forecasting accuracy without introducing unnecessary complexity. The construction of the knowledge graph entails a comprehensive analysis of the multivariate time series to ensure its efficacy in capturing relevant domain knowledge. The main contributions of the papers are highlighted as follows:\n1) This paper introduces an intuitive knowledge-graph enhancement within SOTA transformer-based architectures for effective forecasting on multivariate Long Sequence Timeseries Forecasting (LSTF) problems.\n2) We propose a novel approach that constructs the conceptual relations of variables in the form of a well-defined knowledge graph and develops dynamic and learnable knowledge graph embeddings, enhancing the spatio-temporal relationships in SOTA transformer architectures.\n3) Through rigorous experimentation on longer forecast horizons, this study unveils the substantial potential of this approach, paving the way for further advancements and refinement in time-series forecasting methodologies.\nTo the best of our knowledge, this study is the first attempt to integrate dynamic knowledge graph embeddings to transformers based on the initial information from a knowledge graph for LSTF problem. It is also pertinent to note that the primary objective of the study is to refine the accuracy of LTSF by introducing a novel learnable knowledge graph embedding approach in the baseline architectures rather than focusing on uncovering the ground-truth graph structure.\nIn the following sections, we conduct a thorough exploration of the existing literature in section II, followed by a detailed exposition of our methodology in section III. We then present a comprehensive analysis of our experimental findings in section IV. Finally, we conclude by reflecting on the implications of our study's results and outline the potential directions for future research in the concluding section V."}, {"title": "II. RELATED WORK", "content": "Time-series forecasting has been a pivotal research area in predictive analytics for decades, owing to its ubiquitous relevance across fields like finance, economics, meteorology and manufacturing. Early researchers have pioneered canonical statistical methodologies, including Autoregressive Integrated Moving Average (ARIMA) models [34], Exponential Smoothing techniques [35], Seasonal Decomposition of Time Series (STL) [36], Gaussian Processes (GP) [14], Bayesian Structural (BSTS) Models [37] and Dynamic Linear Models (DLM) [38]. These traditional techniques have long dominated the realm of time-series analysis owing to their interpretability, simplicity and historical performance.\nHowever, the landscape of data has evolved dramatically, marked by unprecedented volumes, varieties and velocities. Traditional statistical methods excel at capturing linear patterns and simple seasonal variations, yet they encounter difficulties accommodating the inherent complexities of time-series data. These approaches often struggle to fully mitigate challenges stemming from irregular fluctuations, non-stationarity, high-frequency data and intricate interdependencies. To address these issues, researchers and practitioners have gravitated towards exploiting deep learning techniques like Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks and hybrid approaches that blend statistical methods with machine learning algorithms. These methodologies capture the nuanced patterns present in dynamic time-series data, thus offering greater efficacy.\nWith the emergence of robust parallel computing systems [16] and the notable advancements in the Transformer [17] architecture, the field of time-series forecasting underwent a significant evolution. Numerous studies [18], [19], [20], [21], [39] capitalized on the canonical attention mechanism of transformer for timeseries forecasting problems and demonstrated superior performance compared to both traditional and other deep learning approaches. However, particularly remarkable is their efficacy in addressing the intricacies of Long Sequence Timeseries Forecasting (LSTF), a task where conventional techniques frequently encounter difficulties. The seminal work of LSTNet [40] revolutionized the understanding of short-term and long-term temporal patterns by introducing a temporal attention layer. This innovative integration amplified the efficacy of deep learning features extracted from CNN and RNN, enabling the adept capture of both short-term local dependencies and long-term trends within multivariate time-series data.\nIn the realm of transformer architectures within LSTF, a surge of research has concentrated on refining attention mechanisms to streamline training and reduce time complexities. Informer [18] introduced an enhanced ProbSparse self-attention mechanism to mitigate the quadratic time complexity and high memory consumption challenges inherent in Transformer. Similarly, Pyraformer [39] proposed a pyramidal attention module for effectively capturing both short and long-term temporal dependencies in timeseries data with reduced space-time complexities. Alternative approaches for attention mechanisms such as Local-sensitive hashing [41] and Attention-free LSTM blocks [42] have demonstrated competitive performances. Beyond the modifications of the attention mechanisms, significant studies [43], [44], [45] have focused on enhancing the novel positional encoding of the transformer to elevate performance results on time-series forecasting and Neural Machine Translation (NMT) tasks.\nIn recent years, there has been a notable trend towards integrating statistical decomposition methods with attention blocks to tackle non-stationarity in timeseries data. This convergence of attention mechanisms with statistical concepts like non-stationarity and seasonality-trend decomposition techniques marks a promising advancement in the field of time-series forecasting methodologies. Autoformer [19] addressed the LSTF problem through a decomposition architecture, drawing inspiration from established decomposition approaches and renovating the self-attention mechanism by incorporating auto-correlation blocks. Similarly, FEDformer [20] proposed a mixture-of-experts framework in the transformer to enhance seasonal-trend decomposition components using Fourier and Wavelet blocks, which could efficiently capture global trends in time-series data. Additionally, Fedformer achieves a linear computational complexity, ensuring both effectiveness and efficiency. Non-stationary Transformer [46] addressed the challenges of non-stationarity in real-world data by introducing a de-stationary attention module, effectively preserving intrinsic non-stationarity while enhancing predictive accuracy. PatchTST [21] introduced a novel approach to time-series forecasting by segmenting data into subseries-level patches and ensuring channel independence. This enables the model to efficiently capture the local semantic information in order to attend to a longer historical context. This design significantly reduces computational complexity and outperforms previous architectures on large datasets.\nThe advancements in transformer-based architectures for forecasting present a strong case for their robustness and reliability. However, while attention mechanisms excel at capturing temporal patterns, they often overlook essential multivariate structural information. To bridge this gap, researchers are actively exploring spatio-temporal architectures within LSTF. Notably, the application of graph-based methodologies like Graph Neural Networks (GNN) [47], Graph Convolutional Networks [48] have emerged as promising avenues. These methodologies offer the potential to seamlessly integrate spatial and temporal dynamics, enhancing the predictive capabilities of forecasting models. The utilization of graph-based methodologies within sequential deep learning architectures for timeseries forecasting has garnered considerable attention in the literature. GNN and GCN frameworks are frequently employed to facilitate multivariate timeseries forecasting by autonomously discerning spatial and temporal interdependencies among variables [33], [49], [50], [51], [52], [53] [54].\nNotably, DHSL [51] model captured high-order correlations in multivariate time series forecasting by generating and optimizing dynamic hypergraph structures using the K-Nearest Neighbors method. DSTAGCN [52] highlights dynamic spatial-temporal dependencies in traffic forecasting by linking the latest time slice to the past by leveraging a fuzzy neural network to generate a dynamic adjacency matrix for enhanced performance. STGCN [53] devised a comprehensive convolution-based structure for their forecasting model, resulting in expedited training owing to fewer parameters while maintaining competitive forecasting outcomes. StemGNN [54] introduced a spectral temporal GNN that amalgamates the Graph Fourier Transform and Discrete Fourier Transform to capture cross-correlations and temporal dependencies in multivariate timeseries data. Moreover, the integration of learnable graph blocks within a transformer framework has emerged as a burgeoning research domain [28], [31], [49], [55], [56], [57]. Particularly, Spacetimeformer [28] and Graphformer [57] advocate for this approach, outlining the incorporation of graph convolution blocks in a transformer setting for forecasting tasks. These methodologies typically harness dynamic interrelationships among variables acquired through the GNN or GCN blocks.\nHowever, while endeavoring to capture the intricate spatio-temporal dynamics of timeseries data, they often manifest over-generalization, thereby diminishing forecast accuracies. Furthermore, these approaches perform well in short-term forecasting but often exhibit notable errors for longer forecast horizons [53]. Comparative evaluations against equivalently sized graph-based transformer within the same setting highlight the superior performance of conventional transformer-based on the considered benchmark datasets [19], [21], [28], [49], [57].\nBesides, it is noteworthy that prior studies have primarily focused on automatically constructing dynamic graphs from the data. However, there remains a considerable dearth of analysis concerning the variables within a multivariate timeseries and deriving inherent relationships solely from those variables. In this study, we propose leveraging learnable knowledge graph embeddings to effectively capture the inherent spatial relationships among variables while harnessing temporal dependencies from SOTA transformer-based architectures."}, {"title": "III. PROPOSED METHODOLOGY", "content": "We consider the following problem: For a set of multivariate time-series dataset $X = \\{X^{(1)}_{t=1}, ..., X^{(M)}_{t=1}\\}$, where M is the number of variates, L is the look-back window and $X^{(i)}_t$ denotes the ith variable in tth timestamp. The Long Sequence Time-series Forecasting (LSTF) task is to forecast the set $X = \\{x^{(1)}_{t=L+1},...,x^{(M)}_{t=L+H}\\}$ wherein H denotes the prediction horizon. In this study, we address the Direct Multi-step Estimation [58] approach, which entails considering the entire forecasting horizon simultaneously within the objective function. This approach is implemented to enhance computational efficiency and minimize the risk of bias accumulation, as observed in Iterative Multi-Step (IMS) forecasting [59], [60]."}, {"title": "A. Model Structure", "content": "In this multivariate LSTF problem, we propose an integration of learnable knowledge graph embeddings with novel architectures (Transformer, Autoformer, Informer, PatchTST). For illustration purposes, we exemplify the integration of knowledge graph embeddings (KGE) with the original Transformer [17] architecture. A detailed flow of the model structure is shown in Fig. 2."}, {"title": "1) Input Embeddings:", "content": "The input embeddings are enhanced by the proposed knowledge graph embedding with the existing embeddings as given in equations 1 and 2:\n$W_{emb} = W_{KGE}+W_{PE} + W_{VE} + W_{TE}$ (1)\n$Z = X + W_{emb}$ (2)\nHere,\n\u2022 $W_{KGE}$ refers to the proposed knowledge graph embedding. It represents the inherent domain's structural knowledge in the variables\n\u2022 $W_{PE}$ is the sinusoidal positional embedding [17] which captures the order of the sequence\n\u2022 $W_{VE}$ is convoluted value embeddings [18] used for representing the magnitudes of the time-series variables\n\u2022 $W_{TE}$ [60] refers to temporal embeddings constructed from timestamps to encode temporal patterns\n\u2022 Z is the enhanced input sequence that incorporates element-wise aggregation between X and $W_{emb}$\nThese embeddings are selectively employed by the existing benchmark architectures. In our replication of the benchmark results, we adhere to this judicious approach to embedding integration with KGE."}, {"title": "2) Multi-Head Attention in Encoder-Decoder:", "content": "The encoder-decoder blocks are the core of the Transformer-based architecture. We employ the canonical multi-head attention (MHA) mechanism [17] in these encoder-decoder blocks. For this, we first perform the linear projections on input embeddings from equation 2 to obtain the following matrices:\n$Q = ZW_Q; K = ZW_K; V = ZW_V$ (3)\nHere, Q, K, V are the query, key and value matrices derived from the input embeddings such that $W_Q \u2208 R^{D\u00d7d_k}, W_K \u2208 R^{D\u00d7d_k}, W_V \u2208 R^{D\u00d7d_v}$\n\u2022 $d_k$ is the dimensionality of the query and key matrices\n\u2022 $d_v$ is the dimensionality of the value matrix\n\u2022 D is the dimensionality of the model\nThe attention score for each head is calculated through the weighted aggregation of a scaled dot-product of the query and key matrices, with the value matrices acting as weights. This is followed by a softmax operation to normalize the scores, given in equation 5.\n$head = Attention(Q_i, K_i, V_i)$ (4)\n$Attention(Q_i, K_i, V_i) = softmax(\\frac{Q_iK_i^T}{\\sqrt{d_k}})$ (5)\n$Multihead(Q, K, V) = Concat(head_1, ..., head_h) W_O$ (6)\nIn equation 6, $W_O \u2208 R^{hd_v\u00d7D}$, where h is the number of heads in multi-head attention. This segmentation of the single attention head into multiple heads facilitates the simultaneous attention to information from diverse representational subspaces. As this employs a straightforward dimensionality reduction, it does not burden the total computational cost [17]. The encoder-decoder components are highlighted in the knowledge-enhanced transformer in Fig. 2. Here, MHA allows each position in the encoder-decoder to attend to all positions from the input sequence. Moreover, in the decoder, the masking of future timestamps is enabled to avoid attending to unseen information. This is crucial for ensuring causality and maintaining the autoregressive property of the model, where each prediction can only depend on past and present information.\nSimilar to Transformer, we incorporate KGE with Autoformer, Informer and PatchTST and provide a comparative study in further sections."}, {"title": "3) Loss Function:", "content": "Mean Squared Error (MSE) loss is used to assess the deviation between the prediction and the ground truth. The loss across each channel is aggregated and then averaged across M time-series to obtain the overall objective loss, given by equation 7.\n$L = \\frac{1}{M} \\sum_{i=1}^M ||\\hat{X}^{(i)}_{L+1:L+H} - X^{(i)}_{L+1:L+H}||^2$ (7)\nwhere $\\hat{X}^{(i)}_{L+1:L+H}$ represent the prediction and $X^{(i)}_{L+1:L+H}$ denotes the actual ground truth values."}, {"title": "B. Construction of Dynamic and Learnable Knowledge Graph Embeddings from Knowledge Graph", "content": "To leverage the full potential of conceptual and spatio-temporal relations among variables, we propose a hybrid approach that highlights conceptual knowledge graphs to form information-rich dynamic knowledge graph embeddings. The dynamic influence in the KGEs is induced by the backpropagation process in the transformer-based architectures. We define our knowledge graph G = (V, E) for the multivariate time-series such that V represents a set of nodes each representing a variable in X. In other words, V = {$v^{(1)},...,v^{(M)}$} where node $v^{(i)}$ corresponds to $X^{(i)}_t$ and $e_{ij} \u2208 E$ where $e_{ij}$ represent edge between nodes $v^{(i)}$ and $v^{(j)}$. In this knowledge graph, we construct an adjacency matrix A as a binary matrix representing the presence of edges between nodes. Thus, A = {$a_{ij}$} \u2208 {$0, 1$}$^{|V|\u00d7|V|}$, where each element $a_{ij}$ is set to 1 if there is an edge between nodes $v^{(i)}$ and $v^{(j)}$ and 0 otherwise.\nTo construct dynamic and learnable knowledge graph embeddings $W_{KGE}$, we employ a sparse matrix transformation to convert the adjacency matrix into learnable weights. The adjacency matrix A is multiplied by a learnable weight matrix $W_l$, followed by an Einstein summation with a learnable projection weight matrix $W_p$. This projection matrix is introduced to align with the shape of the positional embeddings. Here, $W_l \u2208 R^{V\u00d7D}$ and $W_p \u2208 R^{L\u00d7D}$. Fig. 3 shows the visualization for the construction of $W_{KGE}$."}, {"title": "IV. EXPERIMENTS", "content": "The proposed knowledge graph embedding strategy is evaluated on five publicly available datasets, including Weather [22] and 4 ETT (Electric Transformer Temperature) [18] datasets (ETTm1, ETTm2, ETTh1, ETTh2). These datasets are widely accepted as standard benchmarks for testing the performance of novel architectures [18], [19], [20], [21], [60]. Table I depicts the statistical highlights of the benchmark datasets. Our evaluation encompasses a diverse range of dataset sizes to ensure a comprehensive assessment of our proposed integration. Moreover, we have excluded datasets that either contain limited data or lack conceptual or any existing structural relationships between their variables, as these characteristics are crucial for the effective application of our methodology [23], [24], [25]."}, {"title": "B. Knowledge Graphs for benchmark datasets", "content": "In this study, we utilize knowledge graph embeddings (KGEs), initially derived from well-defined conceptual knowledge graphs and projected to higher dimensions for infusing the dynamic nature tailored fit to individual datasets. Unlike conventional approaches that employ temporal graphs or graph convolution techniques to capture spatio-temporal dynamics, our method leverages a simple and intuitive knowledge graph for spatial features and integrates it with the novel attention-based mechanism in the form of dynamic KGEs to identify temporal patterns. This approach facilitates the generation of the dynamic embeddings through the extraction of factual relationships among the variables.\nThe construction of these embeddings incorporates insights from various sources: physical laws and observed empirical relationships for Weather [22] and operational principles for ETT [18]. These underlying principles are elaborated in the Appendix, where we describe the specific relational frameworks used. For illustrative purposes, the intuitive knowledge graph for the Weather dataset is presented in Fig. 4. This structured presentation clearly outlines the foundational knowledge graphs integral to developing KGE. We replicate benchmark results for all the architectures and then incorporate our proposed KGE integration. For a precise comparison, we train all the models with the same experimental setup, viz. with a lookback window (L) of 336 and across four horizons H \u2208 {96, 192, 336, 720}. We maintain the consistency in configurations for finetuning of the original replication and the proposed ones. More details on the baseline configuration setup of each model are provided in the Appendix. We evaluate these architectures on Mean Square Error (MSE) and Mean Absolute Error (MAE)."}, {"title": "C. Experimental Setup", "content": "We experiment with four popular architectures: Autoformer, Informer, PatchTST and Vanilla Transformer. Each of these architectures is distinguished by unique features, such as the decomposition blocks in Autoformer, ProbSparse attention by Informer, channel-independent patching by PatchTST and the canonical multi-head attention mechanism in Transformer."}, {"title": "D. Results and Comparison", "content": "Table III presents the findings of a comprehensive comparative analysis between original model architectures and their enhanced iterations across varying horizon lengths. Within this multivariate framework, our methodology consistently surpasses benchmark results, particularly evident in the Weather dataset. Quantitatively, our proposed integration of knowledge graph embedding with Informer yields notable improvements, with a remarkable 54.5% average reduction in Mean Squared Error (MSE) and a 39.3% decrease in Mean Absolute Error (MAE). Similarly encouraging results are observed with Transformer (22.6% in MSE; 19.3% in MAE) and Autoformer (7.9% in MSE; 8% in MAE). For PatchTST, we observe a 2.5% reduction in MSE and 1.2% in MAE for the Weather dataset as illustrated in Table IV.\nUpon initial examination of the ETT dataset, striking enhancements stand out, particularly with ETTm1 across various models with notable improvements, exemplified by average reductions in MSE and MAE. For instance, Autoformer demonstrates a remarkable 21.9% decrease in MSE and an 11.4% decrease in MAE, Transformer shows average reductions of 2.5% in MSE and 5.6% in MAE, while Informer displays an improvement of 1% in MSE and 1.2% in MAE. Moreover, when focusing on ETTh2, the Autoformer model exhibits an average enhancement of 2.2% in both MSE and MAE.\nIt is noteworthy that these performance boosts are more conspicuous when forecasting over longer horizons, highlighting the robustness of our approach necessary for handling extended prediction tasks within the ETT dataset. Conversely, sub-optimal results emerge when considering the impact of knowledge graph embeddings, particularly evident in ETTm2 and smaller datasets like ETTh1 and ETTh2. Here, we observe a muted influence; for instance, We observe a negligible improvement in the integration of KGE with PatchTST for ETT datasets. This could be potentially attributable to the simplicity of the knowledge graph structure in ETT and thus could lead to a redundant increment in model parameters by the introduction of KGEs."}, {"title": "V. CONCLUSION", "content": "In this study, we propose an intuitive knowledge graph embeddings (KGE) approach for enhancing the state-of-the-art (SOTA) architectures for the multivariate Long Sequence Timeseries Forecasting (LSTF) problem. These KGEs are learnable and dynamic as they are updated during the training of transformer-based architectures. These embeddings are derived from the adjacency matrix of the conceptual knowledge graph after they undergo a series of matrix transformations. Furthermore, they are fused with positional embeddings and other embeddings to train transformer-based architectures effectively.\nWhile purely dynamic graphs and their applications in time-series forecasting have been extensively explored, integration of learnable knowledge graph embeddings in transformers through initial knowledge graphs remains underutilized. This paper demonstrates that a knowledge graph curated solely from conceptual relationships among variables can generate dynamic knowledge graph embeddings from time series data, delivering competitive results compared to benchmark models. Given the suboptimal performance of dynamic graph approaches for spatio-temporal LSTF compared to SOTA transformer variants. The initial basis for a knowledge graph is meticulously derived through a comprehensive analysis of potential conceptual relations among variables within the domain. We validate the performance of this integration through experimentation across various datasets with varying prediction lengths. After a comprehensive evaluation, it is evident that the dynamic KGE enhancement proved beneficial in understanding the inherent interdependencies among variables for LSTF.\nWhile KGEs have demonstrated success, particularly on extensive datasets characterized by intuitive variable relationships, our proposed approach does present certain limitations that necessitate attention. The construction of a knowledge graph is inherently subjective and domain-specific. Thus, it is pertinent to recognize that this approach necessitates a thorough analysis of variables to establish relations. In such cases, KGE based on purely dynamic graph approaches may offer insights into variable relationships and could be a possible scope of research. However, it is crucial to emphasize that the primary objective of this study is to explore the potential of well-defined knowledge graphs for conceptual relations in variables that form the initial basis for dynamic KGEs. Nevertheless, for smaller datasets marked by significant temporal variations, the integration of KGE proves less advantageous. This study highlights that the introduction of KGEs has a clear scope of application wherever established relationships among variables exist."}]}