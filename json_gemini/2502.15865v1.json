{"title": "Position: Standard Benchmarks Fail \u2013 LLM Agents Present Overlooked Risks for Financial Applications", "authors": ["Zichen Chen", "Jiaao Chen", "Jianda Chen", "Misha Sra"], "abstract": "Current financial LLM agent benchmarks are inadequate. They prioritize task performance while ignoring fundamental safety risks. Threats like hallucinations, temporal misalignment, and adversarial vulnerabilities pose systemic risks in high-stakes financial environments, yet existing evaluation frameworks fail to capture these risks. We take a firm position: traditional benchmarks are insufficient to ensure the reliability of LLM agents in finance. To address this, we analyze existing financial LLM agent benchmarks, finding safety gaps and introducing ten risk-aware evaluation metrics. Through an empirical evaluation of both API-based and open-weight LLM agents, we reveal hidden vulnerabilities that remain undetected by conventional assessments. To move the field forward, we propose the Safety-Aware Evaluation Agent (SAEA)*, grounded in a three-level evaluation framework that assesses agents at the model level (intrinsic capabilities), workflow level (multi-step process reliability), and system level (integration robustness). Our findings highlight the urgent need to redefine LLM agent evaluation standards by shifting the focus from raw performance to safety, robustness, and real world resilience.", "sections": [{"title": "1. Introduction", "content": "The financial domain has long been recognized where precision, safety, and trust are important. In recent years, the development of large language models (LLMs) has unlocked huge potential for many tasks across finance, from regulatory compliance and market forecasting to advanced analytics of high-volume textual data. Benchmarks such as InvestorBench and Pixiu have emerged as an important way for evaluating LLM agents in finance.\nYet, as LLM agents become ever more integrated into critical financial workflows, an urgent question arises:\nDo current benchmarks adequately assess the safety, reliability, and robustness of LLMs for high-stakes financial tasks?\nWe argue that the answer is no. Existing benchmarks typically focus on task-specific metrics such as accuracy, F1 score, or ROUGE, which capture financial performance but ignore the unique safety challenges of finance. Unlike many standard LLM settings, financial systems are highly fragile and uncertain, where even minor errors can lead to cascading failures and substantial losses.\nThe safety of LLM agents extends beyond their ability to generate accurate outputs; it includes their resilience in navigating the intricacies and dynamics of financial systems. A model that appears \"successful\" under conventional benchmarks may still fail in real-world scenarios, incurring systemic consequences. For example, the Freysa AI agent lost $47,000 due to a security vulnerability, where users exploited attack prompts to manipulate the model into bypassing security checks and executing unauthorized transactions. Similarly, a user lost $2,500 due to GPT-generated phishing content, which recommended a fraudulent site while writing a transaction bot. These cases highlight the immediate and vulnerable risks posed by LLM agents in financial domain.\nLLM agents are fundamentally predictive models that generate outputs based on statistical patterns in training data. While this enables impressive performance across many tasks, it also introduces unique vulnerabilities. A significant issue is illusory confidence, where agents generate outputs with a confident tone despite reasoning errors. Such misplaced confidence can lead to incorrect recommendations. Similarly, hallucination, an inherent behavior of LLM agent, carries a clear risk. For example, fabricated financial metrics can influence decision-making systems and lead to monetary losses.\nBeyond these direct errors, LLM agents lack temporal awareness, which is a critical limitation in dynamic environments like finance. The models struggle to adapt to rapidly evolving markets, producing outputs based on outdated information. The inability to handle domain-specific reasoning further increase risks, as financial systems require understanding of domain-specific language, regulations, and data structures. Another vulnerability is adversarial manipulation. Even subtle changes to prompts or malicious inputs can exploit weaknesses in the agent system, resulting in misleading outputs.\nCurrent evaluation metrics for LLM agents in financial applications fall short. Accuracy-based benchmarks assess task performance but ignore fundamental safety concerns - robustness, reliability, and resilience in high-stakes environments. Financial systems demand more: an evaluation framework that puts agents to the test in dynamic environments, under adversarial conditions, and while meeting the practical requirements of real-world tasks.\nIn this work, we challenge the machine learning community to rethink how LLMs are evaluated for high-stakes domains. In finance, as in other critical domains, traditional benchmarks are insufficient to ensure safety. We posit a shift in focus from what LLM agents can do to what they must not do in financial domains. Our work provides a roadmap for developing LLM agents that are not only powerful but also risk-aware, ensuring safer deployment in financial decision-making. Our key contributions include:\n\u2022 A study of financial LLM benchmarks, identifying critical safety gaps and introducing three-dimensional risk-aware evaluation metrics (model-level, workflow-level and system-level).\n\u2022 An empirical evaluation of both API-based and open-weight LLM agents under our proposed risk metrics, revealing risks overlooked by traditional benchmarks.\n\u2022 The introduction of a Safety-Aware Evaluation Agent (SAEA), to provide a comprehensive assessment of LLM agents in financial applications."}, {"title": "2. Related Work", "content": "Recent advanced progress of Large Language Models has significantly boosted the development of language agents that could interact with environments to perform complex tasks. Methods such as prompt engineering, tool or code use, self-improvement, multi-model collaboration or finetuning with trajectories has enabled impressive performances on a wide range of real-world tasks that are involved with web, desktop and mobile platforms. With these success in general tasks, language agents are also recently applied into financial domain such as financial question answering, financial decision making, and financial simulation. In this work, we will focus on this nuanced application of language agents in financial domain.\nExisting financial benchmarks primarily focus on task performance, such as accuracy and performance , which may not be sufficient to capture the real-world financial risks. The primary evaluation metrics can be categorized into two groups: (1) accuracy-based metrics, and (2) investment per-"}, {"title": "3. Safety Challenges Specific to LLM Agents", "content": "High-stakes financial systems require stringent demands on reliability, accuracy, and robustness\u2014qualities that are not always guaranteed by current LLMs. We categorize the risks associated with LLMs in finance into two classes: intrinsic challenges that originate from the models themselves (Section 3.1) and external challenges that arise from the interaction of LLMs with external systems and workflows (Section 3.2). We provide an overview of the risks and challenges in Figure 1."}, {"title": "3.1. Intrinsic Risks from LLM Agents", "content": "LLMs generate outputs based on patterns learned from large-scale text corpora . While this results in fluent language, it can also lead to meaningful errors. In casual usage like chatbots, these mistakes might not matter much. However, in finance, even small errors can lead to serious consequences, including financial losses, legal risks, and damage to reputation.\nIllusory Confidence In finance, decision-making is rooted in uncertainty quantification and risk assessment , as formalized in modern portfolio theory and utility theory . They emphasize the need for precise confidence intervals to build investment strategies. However, LLMs often output results with unwarranted certainty, regardless of factual correctness. This behavior contradicts the principles of probabilistic risk assessment for financial systems. For instance, a model's overconfident erroneous market trend analysis may lead to portfolio misallocation, undermining an investor's risk-adjusted return expectations.\nHallucination LLMs' hallucination poses a fundamental challenge to the integrity of financial analytics . Although minor hallucinations may appear harmless in casual scenarios, they can have severe consequences in finance. For instance, fabricating earnings data can lead to false signals that influence the algorithmic trading systems and human decision-making. Given that finance is predicated on accurate information, hallucinations may lead to financial losses and legal liabilities.\nLack of Temporal Awareness The dynamic nature of financial markets requires continuous adaptation, as reflected in dynamic portfolio optimization ."}, {"title": "Poor Handling of Domain-Specific Reasoning", "content": "Financial decision-making often involves interpreting domain-specific language, complex contracts, and regulatory documents. LLMs trained on general-purpose text frequently struggle to differentiate between domain-specific terminologies or rules, leading to errors in domain-specific reasoning. For example, LLM agents might misinterpret the purpose of a smart contract in blockchain transactions. The lack of domain-specific reasoning can result in misleading decisions, particularly in areas where precise understanding of terminology and mechanics is essential.\nAdversarial Vulnerabilities Adversaries can exploit LLM agents through carefully designed prompts that manipulate their outputs. For example, subtle input modifications may cause an agent to generate biased market analyses. This is particularly concerning in trading systems, where such manipulations can influence downstream processes, leading to flawed strategies or improper risk assessments.\nLLM agents operating autonomously are especially susceptible to cascading errors. In multi-step workflows, adversarially manipulated inputs can propagate through decision pipelines, increase the impact of initial errors. For instance, an agent tasked with analyzing market sentiment and generating investment recommendations might amplify false information from an initial manipulated source, resulting in systemic failures.\nDependency on Prompt Design Users of LLM agents in financial domain may encounter variability in the outputs depending on how prompts are phrased. The unpredictability complicates their practical use, particularly in tasks requiring consistent and reliable results. For instance, slight changes in how a user phrases a risk assessment query can lead to different outputs, even when the task remains the same. This inconsistency poses a challenge for users who rely on LLM agents for critical decision-making. In automated trading pipelines, such variability can lead to inefficiencies, errors, or missed opportunities."}, {"title": "Lack of Interpretability", "content": "The risk management frameworks require transparency and accountability in decision-making processes. However, an LLM's \"black-box\" nature makes it challenging to explain how it derives its outputs. Regulators and institutional investors emphasize the need for explainable models to ensure traceability and compliance. The inability to clarify how LLMs produce their outputs creates challenges for adoption in high-stakes domains, where decision-making processes must be fully understood and auditable to users.\nLimitations in Multimodal Integration Financial decision-making relies on the integration of multimodal data, combining textual analysis, numerical computations, and visual representations such as stock charts. Decision science highlights the importance of processing diverse and complex information in dynamic environments. However, current LLMs are primarily text-focused and often fail to integrate textual, numerical, and visual data effectively. This limitation is evident in quantitative research and algorithmic trading, where understanding relationships across varied data types is essential. The lack of robust multimodal capabilities reduces the analytical potential of LLM agent and limits their ability to provide actionable insights for financial decisions."}, {"title": "3.2. Risks from External Interactions", "content": "API/Tool Dependency LLM agents frequently rely on external APIs and tools . For example, to retrieve live market data, execute trades, or query regulatory information. While such integrations expand a LLM's capabilities, they also involve dependencies on systems that can be error-prone. A malfunctioning data feed might provide inaccurate prices, or a compromised API could intentionally inject misleading content. Because the LLM treats these external tools as supplementary knowledge sources, any errors or manipulations can propagate unchecked.\nMulti-Step Tasks Financial operations often involve complex, multi-step workflows: evaluating a company's fundamentals, performing sentiment analysis on news, applying risk models, and ultimately placing trades . Small errors at any step in these workflows may accumulate into larger failures. For example, an incorrect news interpretation might lead to a flawed risk assessment, which in turn could trigger an inappropriate trade. Unlike single-turn tasks that can be manually reviewed, multi-step tasks can magnify small inaccuracies, culminating in decisions that carry substantial monetary risks."}, {"title": "4. A Safety-Aware Evaluation Agent", "content": "As demonstrated in Section 3, deploying LLM agents in financial systems requires evaluation beyond standard performance metrics. We propose a Safety-Aware Evaluation Agent (SAEA) to measure the potential risks of using LLM agents in the financial domain. Our design is anchored by two complementary ideas:\n1. Risk-Sensitive Metrics Design: We present evaluation metrics that are adaptive to the safety risks intrinsic and extrinsic to LLMs. These metrics are designed to capture risks identified in Section 3.\n2. Scenario-Driven Stress Testing: Inspired by stress testing in robust software engineering and the finance industry , where stress tests help expose system fragility, we design edge scenarios to evaluate the LLM agents's safety under differerent real-world conditions.\nThese ideas are integrated into a three-dimensional evaluation framework: model-level (intrinsic LLM capabilities), workflow-level (multi-step process reliability), and system-level (integration robustness). The overview of SAEA is presented in Figure 2."}, {"title": "4.1. Evaluation Dimensions", "content": "Let $\\mathcal{M}$ be the LLM agent under evaluation. For each task $s \\in \\mathcal{S}$, the agent produces a decision trajectory $\\mathcal{D} = \\mathcal{M}(s)$. The SAEA then uses pre-designed prompts and external tools to audit $\\mathcal{D}$, generating a score for each metric. Finally, the SAEA aggregates these scores to yield a risk profile for $\\mathcal{M}$."}, {"title": "4.1.1. MODEL-LEVEL METRICS", "content": "Hallucination Detection A critical concern for LLMs is the fabrication of facts. We defineHallucination Score $H_{score}(o)$ as:\n$$H_{score}(o) = \\frac{\\sum_{i=1}^{W} (f_{fact}(D_s) \\neq y_s) \\cdot w_s}{W}$$\nwhere $f_{eval}(D_s)$ denotes the SAEA's fact-checking function that parses the agent's reasoning, action and output, and identifies mismatches with ground-truth $y_s$ (retrieved from a fact-checking API, e.g., Yahoo Finance ), and $w_s$ is an impact weight, we set $w_s = 1$ in this work. The indicator function $\\mathbb{I}(\\cdot)$ returns 1 if hallucination is detected, and 0 otherwise. A larger $H_{score}$ indicates higher propensity to hallucinate critical details.\nTemporal Awareness In fast-moving financial domains, LLMs must prioritize up-to-date information while discounting outdated data. We implement a temporal accuracy check function that compares the agent's output $O_{model}(s)$ against necessary time information $O_{necessary}(s)$:\n$$T_{score} = f_{time}(O_{model}(s), O_{necessary}(s)),$$\nwhere $f_{time}$ is a function that determines the temporal accuracy, and $T_{score}$ is the temporal accuracy score. It returns a scalar in [0, 100] indicating how well the agent's response reflects certain time-sensitive information. A lower $T_{score}$ indicates better temporal alignment."}, {"title": "Confidence Understanding", "content": "Our SAEA provides a numeric confidence score $c_s \\in [0,100]$ to determine how certain $\\mathcal{M}$ is about its decision trajectory $\\mathcal{D}_s$. The confidence score is generated by a function $f_{conf}(\\cdot)$: $f_{conf}(\\mathcal{D}_s)$, which maps the $\\mathcal{D}_s$ to an inferred confidence score, based on the knowledge of SAEA."}, {"title": "Adversarial Robustness", "content": "We define an adversarial rule set $\\mathcal{S}_{adv}$ that specifies subtle manipulations aimed at challenging the agent's decision-making process, focusing on whether the agent disregards or fails to utilize external tool outputs and instead generates potentially manipulated information via backdoor insertion. To evaluate the impact of these adversarial manipulations, we apply an evaluator function $f_{rob}(\\mathcal{D}_{adv})$, which assigns a severity rating based on the potential financial consequences of the agents' oversights. A higher score indicates more severe adversarial vulnerabilities."}, {"title": "Interpretability and Explainability", "content": "To access the interpretability and explainability of the LLM agent, we use function $f_{expl}$ to evaluate the agent's trajectory $\\mathcal{D}_s$. The score is range [0, 100], where lower values mean the agent's trajectory is more interpretable and explainable."}, {"title": "4.1.2. WORKFLOW-LEVEL METRICS", "content": "Error Propagation Agent's trajectory $\\mathcal{D}_s$ is a multi-step process. We assume it has $n$ steps. We define a function $f_{error}(\\mathcal{D}_s)$ that examines each reasoning step for possible misinterpretations, incorrect logic, or other mistakes. Based on the magnitude of error propagation and its potential financial impact, $f_{error}(\\mathcal{D}_s)$ assigns a severity score. Our approach highlights how small, early missteps can accumulate into larger vulnerabilities.\nPrompt Sensitivity We generate a prompt (query) variant $p'$ that are semantically equivalent but syntactically different. The LLM outputs $\\mathcal{D}_s$. We use a function $f_{sim}$ to compute the variance in a semantic embedding space. A large score implies higher variability (and thus vulnerability) to minor prompt changes."}, {"title": "4.1.3. SYSTEM-LEVEL METRICS", "content": "Response Degradation Dependency For external dependencies (e.g., APIs, data resources), we use $f_{deg}(\\mathcal{D}_s)$ to quantify the degradation of agent's trajectory $\\mathcal{D}_s$, when external resources are delayed or corrupted. The Response Degradation Score $d_s \\in [0,100]$ represent a degradation score indicating how severely the LLM fails. A higher $d_s$ corresponds to stronger negative impact from data feed delays or corruptions."}, {"title": "Multimodal Integration", "content": "If task $s$ presents both textual data and visual representation, we let SAEA to convert the visual representation into a textual format. We use $t_s$ to denote the text-only version of $\\mathcal{D}_s$, and use $v_s$ to denote the visual version of $\\mathcal{D}_s$. The LLM agent's outputs should reconcile both sources consistently. We define:\n$$S_{multi} = f_{multi}(M(t_s), M(v_s)),$$\nwhere $f_{multi}(\\cdot)$ is a function that measures the similarity between two outputs. Higher $T_{multi}$ indicates robust multimodal reasoning."}, {"title": "Scenario-Based Stress Testing", "content": "We measure an LLM agent's resilience under extreme disruptions, we define a stress score that reflects the agent's ability under simulated extreme events. We define a function $f_{stress}$ that inspects the agent's reasoning to detect system shocks (e.g., no external data, API failures) and judges whether the agent can mitigate them. We assigns a stress severity score from 0 to 100, where low values indicate minimal impact from shocks and high values reflect significant financial risk."}, {"title": "4.2. Architecture of the Safety-Aware Evaluation Agent", "content": "The SAEA unifies all these metrics into a modular pipeline: (1) Task & Trajectory Analysis: The SAEA reviews the task $s$ of $\\mathcal{M}$ and trajectory $\\mathcal{D}_s$ to identify potential risks. It then adaptively selects relevant metrics for evaluation; (2) Evaluation Agent: Based on the selected metrics, SAEA associates a set of evaluators {$E_1(f), ..., E_k(f)$}. Each focused on specific metrics; (3) Metric Aggregator and Analyzer: Gathers all results and generate a composite risk profile. This profile includes safety scores and can be further used to fine-tune the LLM agent.\n5. Evaluation\nIn this section, we validate our safety-aware evaluation framework on diverse financial tasks with multiple LLM agents. We include a comparison between traditional benchmark performance and our proposed risk-aware metrics.\nWe consider API-based and open-weights LLMs as the agents' backbone. For the API-based agents, we use GPT-40 and Claude-3.5-Sonnet. For the open-weights models, we use Llama-3.3-70b, Llama-3.1-8b , and DeepSeek-R1. We evaluate these agents on three categories of high-impact financial tasks to capture real-world complexities."}, {"title": "6. Discussion", "content": "Real-World Uncertainty Traditional benchmarks fall short in capturing the real-world risks faced by LLM agents. Existing benchmarks often impose restrictions on both the scenarios and the sources of risk, limiting their applicability. However, real-world applications encounter unpredictable risks and unknown attack vectors, introducing what we refer to as open-world risks. An LLM agent operates within a complex system composed of multiple components, each of which is susceptible to different types of failures and vulnerabilities. As a result, evaluating an agent's robustness requires a more comprehensive framework that accounts for these diverse and evolving real-world threats.\nLLM Agent Vulnerabilities Are Domain-Sensitive Our empirical findings indicate that even top-tier LLM agents exhibit different failure modes under adversarial vs. normal scenarios. For instance, Hallucination may remain relatively low during benign queries but explode in complex prompts that combine partial truths with fabricated data."}, {"title": "6.2. Challenges in Implementation", "content": "Accessing Real-Time Financial Data Many LLMs lack the ability to integrate up-to-date market feeds. APIs are often restricted, and data vendors charge high fees for real-time financial information. Ensuring timely data updates without overcomplicating system design is a nontrivial challenge.\nConstructing Risk-Focused Benchmarks Designing test sets that reflect the risks in financial systems is both logistically and ethically challenging. On one hand, scenarios must be realistic to expose weaknesses; on the other, they must be wide-ranging enough to cover edge cases. Achieving this balance often requires human-in-the-loop oversight to inject domain expertise into synthetic data generation and adversarial prompt design.\nPerformance vs. Safety Trade-offs By employing SAEA to audit decision in the LLM agents, financial institutions can more effectively detect potential safety failures. However, this auditing can introduce overheads, which can increase response latency to constraints on generative breadth and may reduce raw performance on conventional benchmarks. For example, real-time inspections of reasoning constraint checks might slow down high-frequency trading systems. Thus, financial institutions face a trade-off: achieving maximum throughput and fluency versus ensuring robust safeguards against manipulation or error propagation.\n6.3. Post-Hoc Analysis and Traceable Pipelines via SAEA Conventional metrics (e.g., accuracy) offer static snapshots of performance but rarely reveal how small missteps escalate into systemic faults. For example, the 2007 subprime mortgage crisis caused partly by rating-model oversights and lax underwriting, illustrates the need for retrospective inquiry. By the time negative outcomes surfaced, investigators had to reconstruct a cascade of failures spanning multiple stakeholders. It highlights the importance of post-hoc analysis in finance: it enables practitioners to identify root causes and track how minor lapses can lead to broad disruptions.\nSAEA for Risk Explanation and Analysis Our SAEA anchors the post-hoc diagnostic process. We preserve a comprehensive audit trail of every operational step, including reasoning records, prompt interpretations, and references to external data sources. Our structured log makes it possible to revisit an unexpected outcome, such as an anomalous portfolio allocation or questionable market advisory, and determine exactly where the system deviated. SAEA collects the agent data and analyzes its safety."}, {"title": "7. Alternative Views", "content": "While our position highlights the distinct risks posed by LLM agents in finance and the need for risk-aware evaluations, we acknowledge two prevalent perspectives that question this stance."}, {"title": "7.1. View 1: Standard Benchmarks Already Capture LLM Agent Reliability", "content": "One perspective states that traditional NLP evaluation metrics, such as accuracy, F1 scores, and even domain-specific financial indicators (e.g., sentiment classification accuracy in earnings call analyses), are sufficient for assessing the overall reliability of LLMs in real-world financial applications.\nCounterarguments. We do not deny the usefulness of these benchmarks for basic proficiency. However, purely accuracy-driven evaluations overlook LLM-specific vulnerabilities, such as:"}, {"title": "7.2. View 2: Engineering Solutions Can Mitigate LLM Agent Risks", "content": "Another perspective argues that direct engineering interventions can mitigate the inherent risks associated with LLM agents, such as guardrail modules , advanced prompt filtering , and extended fine-tuning. Thus, it is sufficient to rely on these solutions to ensure the safety of LLMs in financial applications.\nCounterarguments. We support robust engineering interventions. However, even comprehensive alignment strategies may not capture all vulnerabilities inherent to LLMs, unless they are repeatedly tested under realistic, agent-centric scenarios:"}, {"title": "8. Conclusion", "content": "The current approach to benchmarking financial LLM agents is biased, as it prioritizes performance while neglecting critical safety risks. Vulnerabilities such as hallucinations, adversarial manipulation, and lack of temporal awareness pose systemic threats in high-stakes financial environments. To address this gap, we introduce the Safety-Aware Evaluation Agent (SAEA), a comprehensive framework that evaluates LLM agents across three key dimensions: model, workflow, and system level. Our findings demonstrate that existing evaluation methods overlook crucial safety concerns, making risk-aware benchmarking essential for the responsible deployment of LLM agents in finance. Without such measures, AI-driven financial decision-making remains exposed to significant, unchecked risks."}, {"title": "A. Detailed Prompts of SAEA", "content": "In this section, we present the prompts used by SAEA to evaluate on each metric. In the prompts, {trajectory_data} is the place holder for trajectory data."}, {"title": "B. Examples of SAEA Evaluation", "content": "We present an example of a trajectory labeled as \"safe\" in the finance management task dataset. We then show the SAEA evaluation output using DeepSeek-R1 and GPT-40, respectively."}]}