{"title": "Grounding Language Models in Autonomous Loco-manipulation Tasks", "authors": ["Jin Wang", "Nikos Tsagarakis"], "abstract": "Abstract-Humanoid robots with behavioral autonomy have\nconsistently been regarded as ideal collaborators in our daily\nlives and promising representations of embodied intelligence.\nCompared to fixed-based robotic arms, humanoid robots offer\na larger operational space while significantly increasing the\ndifficulty of control and planning. Despite the rapid progress\ntowards general-purpose humanoid robots, most studies remain\nfocused on locomotion ability with few investigations into whole-\nbody coordination and tasks planning, thus limiting the poten-\ntial to demonstrate long-horizon tasks involving both mobility\nand manipulation under open-ended verbal instructions. In this\nwork, we propose a novel framework that learns, selects, and\nplans behaviors based on tasks in different scenarios. We com-\nbine reinforcement learning (RL) with whole-body optimization\nto generate robot motions and store them into a motion library.\nWe further leverage the planning and reasoning features of the\nlarge language model (LLM), constructing a hierarchical task\ngraph that comprises a series of motion primitives to bridge\nlower-level execution with higher-level planning. Experiments in\nsimulation and real-world using the CENTAURO robot show\nthat the language model based planner can efficiently adapt\nto new loco-manipulation tasks, demonstrating high autonomy\nfrom free-text commands in unstructured scenes.\nIndex Terms-LLM, loco-manipulation, humanoid robot", "sections": [{"title": "I. INTRODUCTION", "content": "Maintaining autonomy during the execution of a task in\na real-world environment is both essential and challenging\nfor robots, especially when performing tasks that require\ninteraction with surroundings and manipulation of objects.\nThis involves robots being able to reason the given se-\nmantic instructions and plan their behaviors while using\nmulti-modality to perceive and infer affordances and spatial\ngeometric constraints of the environment to determine proper\nmotions.\nRecently, the rise of large language models (LLMs) and\ntheir remarkable capabilities in robotic planning have made\nit possible to perform logical reasoning and construct hierar-\nchical action sequences for complex tasks [1]. However, ex-\ntending language model based algorithms to humanoid robots\nremains a challenge stemming from their complex dynamics\nand precise coordination between different components.\nIn this study, we present a language model based frame-\nwork enabling task reasoning and autonomous behavior plan-\nning towards humanoid loco-manipulation. We use a decom-\nposed training strategy that modularly selects the components\nneeded for specific tasks and maps low-dimensional space\ntrajectories to the whole-body space with a unified motion\ngenerator. The trained actions are stored as skill primitive in\na motion library. We adopt the LLM to decompose complex\ninstructions consisting of multiple sub-tasks that select skills\nfrom the motion library and arranges a sequence of actions,\nreferred to as a task graph. By leveraging the interaction\nof distilled spatial geometry and 2D observation with a\nvisual language model (VLM) to ground knowledge into a\nmotion morphology selector to choose appropriate actions\nin single- or dual-arm, legged or wheeled locomotion. We\nfurther illustrate through experiments how our framework\ncan be learned and deployed on a high-DoF, hybrid wheeled-\nleg robot, performing zero-shot online planning under human\ninstruction. Corresponding video can be seen here [2]."}, {"title": "II. METHODOLOGY", "content": "We illustrate how the proposed framework enables the\nhumanoid robot CENTAURO [3] to autonomously per-\nform loco-manipulation guided by semantic instructions. As\nshown in Fig. 2, we divide the pipeline into four main\ninterrelated sectors that are learned and deployed sim-to-real\nmanner. The motion generation sector selects RL training\nconfigurations for specific tasks and conducts training in\nparallel. The trajectory obtained from the training is provided\nas a reference to the optimizer, which ultimately generates\nwhole-body motion skills and the skills will be stored in the\nmotion library. The user input sector contains a user interface\nas well as pre-defined basic prompts, function options, and\nmotion library, all of which together constitute the textual\nmaterial fed to the LLM. After receiving a command, the\ntask planning sector generates a hierarchical task graph using\nthe LLM. Once the task graph is loaded, it is interpreted\nas a Behavior Tree to guide the robot's execution. When a\ntask requires selecting the motion morphology, depth-sensing\ninformation is invoked and distilled into 2D images and\ngeometric features. These data, along with the task state\nand prompts, are fed to the VLM, which then selects the\nmorphology capable of achieving the goal. Through the\ncoordination of these sectors, the study facilitates semantic\ncommand understanding and zero-shot behavioral planning\nand action execution for CENTAURO robot."}, {"title": "III. EXPERIMENT", "content": "We validate the ability of LLM to plan motion primi-\ntives for different loco-manipulation tasks. Experiments were\nconducted on tasks requiring a combination of perceptions\nand actions. We recorded the success rate and the impact of\ndifferent errors of 4 representative tasks and provided quanti-\ntative evaluations in Fig.8. The results show the LLM based\nplanner can effectively plan for semantic instructions based\non learned skills and guide the robot to complete a variety of\ntasks according to the action sequences, achieving a desired\nsuccess rate (\u2265 60%) on real-world robot. And adding failure\ndetection and recovery (FR) to the planning increases the\nsuccess rate of task execution. Whereas selecting multiple\nfunctional modules as input also increases the difficulty of\nplanning, and execution errors mainly stem from intricate\ndynamical constraints on the actions and misalignment of\nthe floating sensing with robot execution."}, {"title": "IV. CONCLUSION", "content": "In this work, we present a novel framework that tackles\nbehavior planning towards different tasks and enables hu-"}]}