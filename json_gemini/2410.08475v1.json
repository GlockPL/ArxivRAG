{"title": "GIVE: STRUCTURED REASONING WITH KNOWLEDGE\nGRAPH INSPIRED VERACITY EXTRAPOLATION", "authors": ["Jiashu He", "Mingyu Derek Ma", "Jinxuan Fan", "Dan Roth", "Wei Wang", "Alejandro Ribeiro"], "abstract": "Existing retrieval-based reasoning approaches for large language models (LLMs)\nheavily rely on the density and quality of the non-parametric knowledge source\nto provide domain knowledge and explicit reasoning chain. However, inclusive\nknowledge sources are expensive and sometimes infeasible to build for scientific\nor corner domains. To tackle the challenges, we introduce Graph Inspired Veracity\nExtrapolation (GIVE), a novel reasoning framework that integrates the parametric\nand non-parametric memories to enhance both knowledge retrieval and faithful\nreasoning processes on very sparse knowledge graphs. By leveraging the external\nstructured knowledge to inspire LLM to model the interconnections among relevant\nconcepts, our method facilitates a more logical and step-wise reasoning approach\nakin to experts' problem-solving, rather than gold answer retrieval. Specifically,\nthe framework prompts LLMs to decompose the query into crucial concepts and\nattributes, construct entity groups with relevant entities, and build an augmented\nreasoning chain by probing potential relationships among node pairs across these\nentity groups. Our method incorporates both factual and extrapolated linkages to\nenable comprehensive understanding and response generation. Extensive exper-\niments on reasoning-intense benchmarks on biomedical and commonsense QA\ndemonstrate the effectiveness of our proposed method. Specifically, GIVE enables\nGPT3.5-turbo to outperform advanced models like GPT4 without any additional\ntraining cost, thereby underscoring the efficacy of integrating structured informa-\ntion and internal reasoning ability of LLMs for tackling specialized tasks with\nlimited external resources.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have been shown to be able to generate fluent language, answer\nquestions and induce knowledge from the given text in recent benchmarks (Ouyang et al., 2022; et al.,\n2024a;b; 2022; 2020; Raffel et al., 2020). Though it shows a great performance for general question\nanswering, we do not see a similar level of success on similar tasks under the scientific domains or\nsettings that require specialized knowledge tailored to a certain context (Cai et al., 2024; Zhang et al.,\n2024; Dorfner et al., 2024; Dong et al., 2023; Zhong et al., 2023; Deng et al., 2024; Ma et al., 2024a).\nTwo technical disadvantages of LLMs might explain the unsatisfactory performance. On the one\nhand, LLMs are not aware of specialized domain knowledge (Ge et al., 2023; Bang et al., 2023; Xie\net al., 2024; Ma et al., 2024b), such as protein-gene relations, drug-disease associations, and actor\nprofiles of newly introduced movies. The specialized knowledge is not obtained through training and\nneeds to be constantly updated. On the other hand, LLMs are not equipped to lay out a multi-step\nlogic chain with domain expertise to identify and solve sub-questions following a correct thinking\nprocess (Wei et al., 2023; Wang et al., 2023; Jiang et al., 2024). For example, to identify whether a\ndrug is capable of treating a disease, the model needs to figure out the root cause and the affecting\norgan of the disease, chemicals that interact with the virus, and the drug formulas that contain the"}, {"title": "2 PRELIMINARIES", "content": "Retrieval-Augmented Generation. An LLM p\u03b8(y|x) generates the probability distribution of output\ny given an input x. RAG-based systems (Lewis et al., 2021) enhance the capability of language\nmodels in knowledge-intensive tasks by leveraging a retriever-generator framework. The retriever\nmodel is denoted as p\u03b7(z|x, D), where x is an input query, D is a comprehensive knowledge base,\nand it generates knowledge distribution over the given input and knowledge base. The generator\np\u03b8 (y|z, x) then autoregressively generates the output sequence based on the retrieved knowledge z\nand the input context x. The likelihood of generating an output sequence y = Y1:N can be estimated\nas:\n$$p(y|x, D) := p_\\eta(z|x, D)p_\\theta(y|x, z)$$\nReasoning on Structured Knowledge Base. Structured knowledge bases like knowledge graphs\n(KG) provides better knowledge traceability and correct-ability due to the structured nature of\nthe knowledge source, thus provides the RAG-framework with better flexibility during knowledge\nretrieval. Previous studies encode the KGs (Saxena et al., 2020; Zan et al., 2022) and queries, answer\nis generated using similarities between node embedding and query embedding. To contrast, we\npropose a one-shot solution for information retrieval from sparse KG and structured reasoning chain\ndevelopment without any training cost. Recently, ToG (Sun et al., 2024) proposes to iteratively\nquery LLM to search and prune the optimal knowledge paths to include; GoG (Xu et al., 2024)\ndecomposes the query into a set of sub-questions and prompts LLM to iteratively solve each of"}, {"title": "3 GIVE: GRAPH-INSPIRED VERACITY EXTRAPOLATION", "content": "Our proposed method prompts faithfully inductive reasoning by 1) Using LLM to decompose the\nquery into important concepts and attributes; 2) constructing entity groups by combining the key entity\nin the query and its relevant concepts in the knowledge graph; 3) inducing inner-group connections\nbetween the queried entity and related concepts using parametric knowledge of LLM; 4) build\ninter-group connections by probing and pruning all pairwise possible connections, and considering\nintermediate concept groups to facilitate multi-hop reasoning for complicated questions. We present\nthe overall algorithm for GIVE in Appendix 1 and the prompt and example output in Appendix D."}, {"title": "3.1 QUERY INFORMATION EXTRACTION", "content": "Given query x, GIVE first leverages the LLM to retrieve the entity and relation sets Ex and Rx:\n$$x \\rightarrow LLM \\rightarrow E_x, R_x$$\nwhere Ex = {ex, en...en} denotes the top-k concepts, and Rx = {r,r...rm} is the top-m relations\nor attributes in the query.\nx"}, {"title": "3.2 ENTITY GROUP CONSTRUCTION", "content": "The goal of this step is to bridge the gap between the limited richness of knowledge base corpus\nand the complexity of the potential input. To this end, we search through the knowledge space, to\nconstruct a cluster of similar concepts, for each of the entities that we identified as important for the\ngiven query. For each e \u2208 Ex, GIVE leverages an underlying pre-trained LM encoder w to encode\nthe concepts in the knowledge base, and retrieve p most similar concepts to each queried entity by\ncomparing cosine similarities:\n$$Y_k = \\{Y_{x_0}, Y_{x_1} ... Y_{x_p} \\} = \\underset{Y_i \\in E_G}{argmin} \\{cos(w(e_x), w(\\hat{y_i}))\\}$$\nThe set Yk contains all entities that are semantically similar to the queried concept er, and ek is\nappended to Yk to formulate the entity group Nk:\n$$N_k = \\{e_k\\} \\cup Y_k  \\hspace{0.5cm} N = \\{N_k\\}_{k=1}^{|E_x|}$$\nThere are two advantages of the entity set Ns: (1) Inducing inter-group connections between the\nqueried entity and its \"sibling\" concepts naturally lead to a reasoning chain on the KG concepts. (2)\nIt relaxes the strict information retrieval on the few queried entity, to relationship inference over a\nlarger set of relevant concepts."}, {"title": "3.3 INNER-GROUP CONNECTIONS", "content": "Firstly, considering each N in Section 3.2, we induce connections between each queried entity and\nits other semantically-similar concepts in its own entity group. The purpose of this step is to inspire\nLLM to conduct divergent thinking on the related similar concepts, not only focus on the queried\nentity itself. The hard problem of inducing relationships directly between two queried entities is\nreleased to finding any possible relations between two sets of similar concepts.\nTo this end, we utilize LLM to openly fill in the relationship between the queried entity and each of\nthe in-group concept. In other words, the richness of corpus of the external knowledge base serves as\nan intermediate to inspire the LLM to expand the possible relation set Rx."}, {"title": "3.4 INTER-GROUP CONNECTIONS", "content": "In this section, we provide evidence for the language model to induce relationships between the\nnode pairs that cross two entity groups. Given two concept groups N and N, we first identify all\nlegitimate relations that could be used to connect any node pairs across these two groups, and use\nLLM to prune these psedu-connections. We also consider intermediate groups to facilitate multi-hop\nreasoning."}, {"title": "3.4.1 POTENTIAL RELATIONS INDUCTION", "content": "Between each pair of entity groups, we consider two kinds of potential relations: (1) Relations\nmentioned in the question. The relations asked by the ultimate QA task are the critical connections to\nbe considered. We induce the relation in questions by prompting the LLM while providing instruction\nand examples on identifying relations and the content of the question as input. The relation would be\na sub-sequence of the original question. (2) KG relations that exists between these two groups.\nSince each group contains semantically similar concepts, the existing cross-group KG connections\ncould potentially connect nodes in two node groups with correct semantic meaning."}, {"title": "3.4.2 INTERMEDIATE NODE GROUP DISCOVERY FOR MULTI-STEP REASONING", "content": "Considering only nodes and connections directly related to the ones mentioned in the question would\nlimit the thinking scope, this is especially the case when dealing with scientific questions when\nneither LLM nor the external knowledge source itself has enough information to connect two entity\ngroups directly. For example, when answering a query about the effect of certain drug to a disease,\na natural reasoning chain is to build the (drug, compound, disease) connections, to form the claim\n\"because certain compound entity is contained in the drug, and the diseases that can be treated by the\nentity, it could be inferred that the drug can treat entity\".\nTo provide sufficient knowledge and thinking hints for such complicated tasks that require linking\ntarget entities through multi-step reasoning, GIVE explores new node groups as intermediate stopovers\nof the thinking process. Firstly, all length-2 paths between two node groups are discovered. Secondly,\nLLM is prompted to automatically select the most helpful multi-hop thinking process that benefits the\nultimate question-answering task, where each multi-hop thinking process comes from verbalization\nof the length-2 path we discovered. Using the intermediate node of the optimal length-2 path, GIVE\nconstructs an intermediate entity group by leveraging the same process as illustrated in Section\n3.2. Note that the intermediate node group is created to build the multi-hop connections between\ntwo queried entity groups. The intermediate node groups contain a set of similar \u201cintermediate\u201d\nentities in the knowledge graph, whereas the queried entity group contains an entity in the query and\nsemantically similar concepts to the queried entity that are contained in the knowledge graph."}, {"title": "3.4.3 KG-STRUCTURE GUIDED REASONING", "content": "In the previous sections, we pre-process the external structured knowledge source by constructing (1)\ngroups of important concepts in Section 3.2, and any possible intermediate groups in Section 3.4.2.\n(2) possible connections between any two groups in Section 3.4.1. GIVE utilizes these non-parametric\nevidence Nx and Rx to inspire LLM conduct reasoning using its parametric knowledge, and formally\nbuild the inter-group knowledge set.\nAssigning relations with external evidence. If there exists an edge on the external knowledge\ngraph between a pair of nodes, we directly inherit the ground-truth relation from the original KG\nG. We consider all knowledge described on the external KG as ground-truth known facts. When we\nverbalize such edge in the prompt, we use affirmative tense to indicate such a fact is true with very\nhigh confidence.\nVeracity Extrapolation with internal knowledge. The potential relations between node groups\ninduced in Section 3.4.1 are crucial to inform us about the possible connections between nodes. The\npre-training stage of LLM equips the model with rich factual knowledge from the unstructured corpus.\nIt is important to concertize the relevant internal knowledge to affirm the model's decision or to reject\nwrong answers with explicit context. Thus, we prompt the LLM to assign a label to each potential\nrelation among two node groups: \u201cyes\u201d, \u201cno\u201d, or \u201cmaybe\u201d. If the LLM yields \"yes\" for a certain\nrelation, it indicates the model is confident that such a claim is factual."}, {"title": "3.5 PROGRESSIVE ANSWER GENERATION", "content": "From the reasoning process presented in Section 3.4.3, we obtained three kinds of knowledge: (1)\nAffirmative knowledge set that contains all inner-group connection; all the potential cross-group\nconnections that are labeled as \"yes\" by the LLM; and the connections that are built purely by the\ninternal knowledge of LLM. (2) Counter-factual knowledge set that contains all the potential relations\nthat are labeled as \u201cno\u201d by LLM. (3) The ground-truth connection contained in the KG.\nTo prevent hallucination, we adopt a progressive manner to generate the final answer by first giving\nonly the affirmative knowledge set. Then we ask the LLM to refine this answer by giving the full\ncontext of the previous step plus the counter-factual knowledge set. The final answer is generated by\nproviding details of all previous context and the ground-truth knowledge contained in the original\nknowledge graph."}, {"title": "4 EXPERIMENTS", "content": "The experiments in this section are designed to answer the following research questions: (1) Is GIVE\nable to provide structured high-quality knowledge using very sparse external resources thus result in\nhigher QA accuracy? We answer this in Section 4.2 by conducting experiments on various biomedical\nQA benchmarks using a very sparse UMLS knowledge graph (Li et al., 2023). (2) Is GIVE robust\nto different scarcities of KG to retrieve useful information? To this end, we conduct experiments\nby randomly sampling different portions of triplets in ConceptNet (Speer et al., 2018), and use the\nresulted subgraphs to test on CommonsenseQA (Talmor et al., 2019). (3) Is the performance of GIVE\nsensitive to the number of additional concepts in each group (which is the only hyper-parameter\nfor our method)? We perform ablation studies in Section 4.4 to answer this. (4) On what kind of\nquestions GIVE achieve the best performance? We give a detailed analysis in Appendix C.1 and\nconclude that GIVE improves the performance of LLM by achieving very high accuracy on the\nquestions where the ratio of expert KG knowledge in the overall retrieved knowledge set is relatively\nhigh. (5) Is there any other factors that may influence the performance of GIVE? We conducted\nadditional ablation studies of the subset of each dataset in Appendix B.1, B.2 and B.3 on the number\nof seeded examples, prompting strategies and encoder model size for entity group construction."}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "4.1.1 REASONING-RICH QA DATASETS\nSince we aim to enable LLM to conduct faithful reasoning utilizing very sparse KG, thus the general\nKB-QA pairs with ground truth knowledge path does not align with the purpose of our experiments.\nFor biomedical reasoning experiments we use a sparse UMLS (Li et al., 2023) with 135 nodes and\n5877 edges, test the accuracy on the \"yes-no\" datasets PubmedQA (Jin et al., 2019) and BioASQ\n(Krithara et al., 2023), and multiple-choice dataset Processbank (Berant et al., 2014). In Section 4.3,"}, {"title": "4.4 ABLATION STUDY", "content": "GIVE achieves the best performance using only small number of additional KG entities. The\nkey parameter of GIVE is the number of additional KG entities we introduced to each concept group.\nTo study how that influences the performance of GIVE, we conduct experiments on biomedical\nreasoning with GPT3.5-turbo, using number of KG entities from 0 to 3. As shown in Figure 4, the\nperformance of GIVE improves first with increasing number of KG entities per group from 0 to 2,\nand decreases when we increase it to 3 and the observation is uniform across all datasets. This is\nbecause GIVE first enables LLM to conduct structured reasoning using the additional information.\nSince we are using a sparse KG with only 135 nodes, when k is greater than 2, it is very likely to\nintroduce entities that is not directly related to the queried concepts, and thereby causes hallucination.\nGet inspired, do not recite. The performance jump when increasing the number of KG entities\nfrom 0 to 1 proves the effectiveness of the \"Graph Inspiration\" process we proposed in Section\n3.4.3, by introducing additional related concepts from external source and \"inspire\" the LLM to\nconduct divergent reasoning using these external clues. This points out that the ability of divergent\nthinking of LLMs may have long been ignored, as we have been focusing on retrieving the gold\nknowledge for the model to \"recite\". Instead, further studies should be conducted on how to utilize\nthe external knowledge as a high-level clue to \u201cinspire\u201d LLMs conduct reasoning, rather than a\n\"long-answer\" style gold context. This is especially the case when we deploy LLMs using limited\nexternal knowledge. Intelligent agents should not recite the context, they get inspired from the\nexternal clue and conduct faithful reasoning."}, {"title": "5 LIMITATION STATEMENT", "content": "It remains a heuristic on how to eliminate hallucination caused by in-accurate knowledge GIVE\nintroduced, as there is no performance guarantee on the LLM's ability to prune out the correct\npotential knowledge from the wrong. In fact, it is related to the size of the LLM and how extensively\nit has been trained on the specific domain knowledge. Regarding the complexity of GIVE, suppose\nwe have m entity groups and each group has n concepts, between two entity groups there r candidate\nrelations. The inner-group connections (Section 3.3) takes O(mn) LLM calls. For inter-group\nconnections (Section 3.4.3), the number of LLM calls needed equal to the number of generalized\npotential connections, which is O(rm\u00b2n\u00b2). However, as shown in Section 4.4, GIVE achieves best\nperformance when n = 1 or 2. In Appendix C.2 we further prove that (1) average value for m is\naround 3 for biomedical reasoning datasets and 4 for CommonsenseQA; (2) average value for r is\nupper-bounded by 4 for all datasets; (3) complexity of GIVE can be further reduced by pruning\ncandidate knowledge in batches. For example, if we give 5 candidate relations between two concepts\nin one prompt, and let LLM decide which ones are true or false, this will reduce the number of LLM\ncalls approximately 80%."}, {"title": "6 CONCLUSION", "content": "We propose Graph Inspired Veracity Extrapolation (GIVE), a knowledge extrapolation framework\nfor structured reasoning of LLM on sparse knowledge graphs. GIVE neither focuses on explicit"}, {"title": "A ALGORITHM FOR GIVE", "content": "We summarize the comprehensive procedure of GIVE and present its detailed algorithm in Algorithm\n1\nAlgorithm 1: GIVE\nInput: Entity groups Nx; Possible relations between two entity groups R; Knowledge Graph G\nOutput: T(G), the approximated gold knowledge set that helps to solve query x\n1 \u0164a(G) \u2190 \u00d8\n2\u0164 (G) \u2190 \u00d8\n3 T(G) \u2190 \u00d8\n4 for all queried entity ex and their corresponding relevant concepts y \u2208 N do\n// build inner-group connections\n5 (ex,, y) \u2192 LLM \u2192 (ex, r, y)\n6 \u0164a(G) \u2190 \u0164a(G) u {(e, ru,y)}\n7 for (N, Ni) pairs in Nx \u00d7 Nx do\n// build inter-group connections\n8 retrieve all triplets Te(Gii) \u2208 EG connecting any node in N and any node in N\n9 T(G) = T(G) UT(Gij)\n10 R\u2190 set of relation types in Te (Gii) R\u2190 RUR\n11 for all triplets (n, ri, n) in (N\u00b0 \u00d7 R \u00d7 N\u2082) do\n12 (n, r, n) \u2192 LLM \u2192 yes,no or maybe\n13 if yes then\n14 T(G) = (G) u (n, ri,n)\n15 if no then\n16 T(G) = T(G) u (n, not rj, n)\n17 return (G), \u0164(G), T(G)"}, {"title": "B ADDITIONAL ABLATION STUDIES", "content": "In addition to Section 4.4, we conduct more detailed ablation studies for GIVE to study the robustness\nof the proposed method and other factors that may influence its performance. All experiments in this\nSection are based on 50 randomly generated examples for each dataset, whereas in Section 4.4, we\nstudy the influence of different numbers of entities per group on the full dataset."}, {"title": "B.1 NUMBER OF SEEDED EXAMPLES", "content": "To better understand how difficult it is for LLMs to get the generalized ability to adopt the knowledge\ngenerated by GIVE to build the structured reasoning chain, we study the performance of GIVE by\nproviding different number of examples in the prompt. For yes-no datasets PubmedQA (Jin et al.,\n2019) and BioASQ (Krithara et al., 2023), we randomly choose k of {0, 1, 2, 3, 4, 5} examples.\nFor multiple-choice datasets Processbank (Berant et al., 2014) and CSQA (Talmor et al., 2019), we\nchoose k of {0, 1, 3, 5, 7, 10}. The results are presented in Figure 5.\nWe observe that although the performance of GIVE increases as we give more seeded examples\nin the prompt, the only one large performance upgrade happens when we increase the number of\nexamples from 0 to 1. This implies that GIVE is a generalizable framework for the LLM to easily\nadopt. The high performance of GIVE does not rely on large number of examples, but stems from the\nhigh quality of the synthetic data it generates."}, {"title": "B.2 DIFFERENT WAYS OF PROMPTING", "content": "We perform additional experiments in this sub-section to study how different prompting strate-gies influence the performance of GIVE. We verbalize the retrieved knowledge and prompt them in the form of triplets and text, and the results are presented in in Table 4. We notice that in most cases, prompting the knowledge in triplets yields to higher accuracy than prompting knowledge in text. This is because the struc-ture of triplets naturally provides an easier way for the LLM to connect the related entities and build faithful logical chain to solve the question. However, for text-based information, additional analyzing step is needed to understand the text before it links the useful information together, which is a difficult task for reasoning-intensive queries where the volumn of additional knowledge is high."}, {"title": "B.3 ENCODING MODEL SIZE", "content": "In Section 3.2, we employ SentenceTransformer as encoder model to measure the text similarities for\nentity group construction. We investigate the impacts of using different sizes on the performance\nof GIVE, and demonstrate the results in Table 5. We see that although larger size encoder models\nachieve better sentence embedding or performance semantic search performance, small to middle\nsize encoders tend to perform more consistently on all datasets. For the best-performing GIVEa+c+e,\nthe 80M encoder (all-MiniLM-L6-v2) achieves 8% higher accuracy than the 420M one (all-mpnet-\nbase-v2). The results show that larger size encoders do not necessarily better measure text similarity\nbetween specific domain terms. On the other hand, the performance of GIVE does not rely on the\nsize of the models employed, which enhances the efficiency of GIVE."}, {"title": "C DETAILED ANALYSIS OF GIVE", "content": "C.1 WHAT MAKES GOOD \"INSPIRATIONS\"?\nWe noticed that the accuracy improvement of GIVE alternates across different QA datasets, to carefully examine what makes the different capabilities of GIVE in boosting LLM's performance, we randomly sample 50 questions from PubemdQA (Jin et al., 2019), BioASQ (Krithara et al., 2023), ProcessBank (Berant et al., 2014) and CSQA (Talmor et al., 2019). Specifically, From Table 1 and Table 3, we found that when vanilla LLM does not have enough internal knowledge (I/O prompting gets poor performance), the accuracy improvement achieved by GIVE has the trend of BioASQ > PubmedQA > Processbank > CSQA. The different performance of GIVE stems from the ratio of expert KG knowledge in the whole retrieved knowledge set. To better see this, we define expert ratio for a query x to be $\\frac{|\\~{T_x(G)}|}{|\\~{T_x(G)}|+|\\=T(G)|+|\\hat{T(G)}|}$, where Tx (G), T\u300f(G)\u00ba are the set of expert KG knowledge, affirmative knowledge and counter-factual knowledge we retrieved from Section 3.4.3. We then calculate the average expert ratio of 50 randomly chosen samples for each dataset and demonstrate the relationship between the average expert ratio and the best accuracy gain of GIVE compared to I/O prompting in Figure 6.\nThere is a positive correlation between the performance of GIVE and the ratio of expert KG knowledge. The reason behind this is with the larger number of seeded expert triplets, GIVE would have more concrete candidate relations and related entities for the LLM to conduct divergent thinking in the proposed \"inspiration\" process. The quality of the synthetic knowledge depends on the number of seeded expert KG knowledge provided. In the case that there are only few KG knowledge (for CSQA on the 50%-triplet Conceptnet), most retrieved knowledge are based only on LLM's internal knowledge to decide openly what the relationship is between two concepts. When the give KG is rich in information, the ground truth triplets provide a high-quality \"supervise\" for the \"inspiration\" process to \"hint\" the model what kind of relationship may exist between the entities. To further backup this statement, we divide these 50 randomly sampled questions from PubemdQA, BioASQ and Processbank into sub-groups according to their expert ratio, and we calculate the average accuracy"}, {"title": "C.2 EFFICIENCY OF GIVE", "content": "In Section 5 we discussed the efficiency of GIVE and concluded that the key factors that influence the number of LLM calls required by GIVE is the number of entity groups detected for the query and the number of candidate relations between each pair of entity groups. To conduct a more detailed study on the scale of them, we run GIVE 5 times on 50 randomly selected questions for each of the datasets we included in Section 4.2 and 4.3, and we report the average number of entity groups, average number of candidate relations to connect two entity groups, and average percentage of questions that requires intermediate entity groups (3.4.2) for multi-step reasoning. The results are presented in Figure 8.\nWe observe that on average, GIVE requires around 3 entities groups for each question in the Biomedical datasets (PubmedQA, BioASQ, Processbank), between each datasets, there could be 1 to 6 candidate relations. For commonsenseQA, 4 entity groups on average are detected because the dataset has 5 candidate options, between each pair of entity groups, only 1 candidate relation is detected in general. We also notice that 60% of the questions in PubMedQA requires intermediate group. That is the reason why PubmedQA tends to need more entity groups than BioASQ as a \"yes-no\" QA dataset. This implies one of the potential method to improve efficiency of GIVE is to disable intermediate group detection. On the other hand, we can use the LLM to prune the candidate connections in batches, which means in Section 3.4.3, instead of asking LLM \"yes\" or \"no\" for each potential connection, we can prompt the LLM with a set k of relations and let it select out which ones are true of false, which will devide the total number of LLM calls by the factor of k for GIVE."}, {"title": "C.3 DETAILED COMPARISON WITH EXISTING RETRIEVAL METHODS", "content": "In addition to Table 1 and 2, we conduct detailed performance comparison against text-based retrieval method RAG (Lewis et al., 2021) and KG-LLM retrieval method (Sun et al., 2024), we calculate the portions of questions answered correctly by each method and present the statistics in Figure 9 and Figure 10.\nWe observe that on three of the four datasets (BioASQ, Processbank, CommonsenseQA), we included in our experiments, most of the questions answered correctly by ToG or RAG is also answered correctly by GIVE. We see this by calculating the ratio $\\frac{only \\ ToG/RAG \\ correct}{only \\ ToG/RAG \\ correct+both \\ correct}$. For example, its"}, {"title": "D.1 IO PROMPT", "content": "You are a helpful assistant that answers a given question about medical knowledge with yes, no or maybe, based on your own knowledge.\n[k-shot EXAMPLES]\nQ: Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?\nOutput: no"}, {"title": "D.2 COT PROMPT", "content": "You are a helpful assistant that answers a given question about medical knowledge with yes, no or maybe, based on your own knowledge.\n[k-shot EXAMPLES]\nQ: Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?\nLet's think step by step.\nOutput: maybe"}, {"title": "D.3 RAG PROMPT", "content": "For RAG, we provide both the correct textual knowledge and reasoning chain for each of the k-shot examples.\nYou are a helpful assistant that answers a given question about medical knowledge with yes, no or maybe, based on the retrieved textual knowledge \"entity relation entity\" from an expert knowledge graph.\n[k-shot EXAMPLES]\nQ: Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?\nKnowledge: [Textual knowledge]\nOutput: no"}, {"title": "D.4 TOG PROMPT", "content": "We follow the official implementation of ToG (Sun et al., 2024) at official ToG GitHub and use the default prompts. We replace the k-shot examples to be examples randomly selected for each dataset, and we provide the correct reasoning chain. Overall, we use exact the same k-shot examples for ToG and our method to guarantee fair comparison.\nExemplar prompt for retrieving top entities:\nPlease retrieve the top entities (separated by semicolon) that contribute to the question.\n[EXAMPLES]\nQ: Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?\nOutput: [Entities retrieved]\nExemplar prompt for pruning relations:\nPlease retrieve 1 relation that contributes to the question the most from the given relation list. The answer must be one of the given relations.\n[EXAMPLES]\nQ: Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?\nRelations: [Relations list]\nOutput: [Relationship selected]\nExemplar prompt for pruning entities:\nPlease score the entities' contribution to the question on a scale from 0 to 1 (the sum of the scores of all entities is 1). [EXAMPLES]\nQ: Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?\nRelation: [Relationship selected]\nEntities: [Entities list]\nOutput: [Entity selected]\nExemplar prompt for evaluating knowledge sufficiency:\nGiven a question and the associated retrieved knowledge graph triplets (entity, relation, entity), you are asked to answer whether it's sufficient for you to answer the question with these triplets and your knowledge (yes or no).\n[EXAMPLES]\nQ: Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?\nKnowledge triplets: [currently retrieved knowledge triplets]\nOutput: [yes/no]\nExemplar prompt for ToG answering the question:\nGiven a question and the associated retrieved knowledge graph triplets (entity, relation, entity), you are asked to answer the question with these triplets and your knowledge.\n[k-shot EXAMPLES]\nQ: Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?\nKnowledge triplets: [retrieved knowledge triplets]\nOutput: maybe"}, {"title": "D.5 GIVE PROMPT", "content": "Exemplar prompt for extracting and ranking the entities in the question:\nPlease retrieve the top entities that contribute to the question. Answer only the top entities", "comma.\n[EXAMPLES": "nQuestion: Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?\nOutput: ['traumatic aortic injury'", "anatomy": "aortic arch", "aortic trauma severity": "nExemplar prompt for extracting the relationships in the question:\nPlease retrieve the relationships that connect the given entities in the question.\n[EXAMPLES", "nQuestion": "Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?\nEntities: traumatic aortic injury", "severity\nOutput": ["influence"], "entities": "nYou are a helpful assistant that answers a short relationship in a few words between two given biomedical entities.\n[EXAMPLES", "nEntities": "traumatic aortic injury", "poisoning\nOutput": "is a\"\nExemplar prompt for determining if relations exists between cross group entities:\nYou are a helpful assistant that answers yes", "true?\nOutput": "No\"\nExemplar prompt for selecting optimal 2-hop path for intermediate entity"}]}