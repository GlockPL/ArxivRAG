{"title": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability", "authors": ["Tsz Ting Chung", "Leyang Cui", "Lemao Liu", "Xinting Huang", "Shuming Shi", "Dit-Yan Yeung"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in a wide range of natural language processing tasks when leveraging in-context learning. To mitigate the additional computational and financial costs associated with in-context learning, several prompt compression methods have been proposed to compress the in-context learning prompts. Despite their success, these methods face challenges with transferability due to model-specific compression, or rely on external training data, such as GPT-4. In this paper, we investigate the ability of LLMs to develop a unified compression method that discretizes un-informative tokens, utilizing a self-supervised pre-training technique. By introducing a small number of parameters during the continual pre-training, the proposed Selection-p produces a probability for each input token, indicating whether to preserve or discard it. Experiments show Selection-p achieves state-of-the-art performance across numerous classification tasks, achieving compression rates of up to 10 times while experiencing only a marginal 0.8% decrease in performance. Moreover, it exhibits superior transferability to different models compared to prior work. Additionally, we further analyze how Selection-p helps maintain performance on in-context learning with long contexts.", "sections": [{"title": "1 Introduction", "content": "In-context learning has shown remarkable success in various natural language processing tasks (Brown et al., 2020), such as classification task (Min et al., 2022), and mathematical reasoning task (Wei et al., 2023), enabling Large Language Models (LLMs) to tackle complex and diverse tasks using only few-shot samples. However, in-context learning also significantly extends the length of prompts, resulting in increased computational and financial costs. Recently, a line of work has been focusing on prompt compression, which aims to compress the original prompts while minimizing information loss. They are categorized into discrete compression (Li et al., 2023; Jiang et al., 2023; Pan et al., 2024) and continuous compression (Mu et al., 2023; Chevalier et al., 2023; Ge et al., 2024); the former compresses the context into discrete tokens, while the latter compresses it into a short sequence of continuous vectors.\nObserving redundant and repetitive content in a given input, discrete compression methods aim to eliminate less informative context without significantly compromising the model's performance. For example, LLMLingua (Jiang et al., 2023) proposes to perform iterative token truncation based on the content perplexity, requiring multi-round decoding (Jiang et al., 2023). LLMLingua-2 (Pan et al., 2024), distilled from GPT-4 (OpenAI, 2023), addresses the potential misalignment between entropy and the compression objective, as well as the distribution gap between the perplexity of the compression model and the target model. High costs are still involved in the training data construction. Meanwhile, optimizing the distribution for specific LLMs (GPT-4) may, on the other hand, hinder the transferability of the compressed content to other LLMs. More details are discussed in Section 4.4.\nContinuous compression (Bulatov et al., 2022;"}, {"title": "2 Related Work", "content": "2.1 Hard Compression\nSome studies focus on token pruning (Goyal et al., 2020; Kim and Cho, 2021; Rao et al., 2021; Kim et al., 2022; Modarressi et al., 2022) and token merging (Bolya et al., 2023) but they are designed primarily for smaller models like BERT. More recently, Selective Context (Li et al., 2023) is the first to propose to prune less important tokens based on information entropy. Subsequently, LLMLingua (Jiang et al., 2023) refined the approach by integrating the selection of demonstrations and the allocation of compression budgets for various segments of the input prompt. No training is required in these models but their efficacy in downstream tasks with compression applied to in-context demonstrations remains limited. Pan et al. (2024) extended the idea and addressed the potential misalignment between entropy and the compression objective, leveraging full bidirectional context by training on their proposed GPT-4 distilled compression dataset. Our simple yet effective approaches outperform previous studies.\n2.2 Soft Compression\nGist (Mu et al., 2023) is first proposed to compress prompt with soft tokens. Subsequently, Autocompressor (Chevalier et al., 2023) and ICAE (Ge et al., 2024) extend the idea to handle long contexts with different pretraining approaches. ICAE further conducts instruction tuning to enhance model performance. The downstream performance of these models heavily relies on the tuned compression model, with a fixed compression rate. Additionally, retraining is necessary for different versions of LLMs. Compared to these approaches, our work offers greater flexibility and transferability, while simultaneously surpassing the performance of existing compression models."}, {"title": "3 Methodology", "content": "Under the intuition that redundant texts often exist and their removal does not hinder human understanding of the text, we assume that LLMs behave in a similar manner. To efficiently identify less informative tokens within a given context, we propose a simple pre-training objective encouraging the model to predict the same token both before and after discarding less informative tokens."}, {"title": "3.1 Preliminary", "content": "Language Model. Given a context [X1,X2,...,xn\u22121], the objective of the language model is to predict the next token Xn, formed as P(xn|X1,X2,...,Xn\u22121). In case of training with the causal language modeling (CLM) loss, we will have,\nLCLM = -log P(xi | X1,X2, ..., Xi\u22121; 0)\nTokens Selection Models. LLMLingua (Jiang et al., 2023) and its variant (Li et al., 2023; Pan et al., 2024) select tokens according to the computed distribution of the targeted LLM. Specifically, suppose xi is a token in the prompt, if the probability of a token xi is less than a threshold, then the token is selected to be compressed."}, {"title": "3.2 Selection-p", "content": "Following the tokens selection models (Li et al., 2023; Jiang et al., 2023; Pan et al., 2024), Selection-p includes the two steps for testing, i.e. the selection (compression) step and the inference step. In the selection step, unlike LLMLingua and its variant, we instead define a selection model to select less informative tokens within the context in a discriminative way. In the inference step, the tokens selected via the selection model are passed to our targeted inference model.\nSelection. Assume pi \u2208 [0, 1] denote a measure of the informativeness of token xi. Theoretically, we can customize a deep neural model to instantiate pi. In practice, we directly take a pre-trained language model and adopt the last layer of hidden representation h\u00b9 from a pre-trained LM to the new linear projection layer. Formally, suppose h = {h1,h2,\u2026, hn} denotes the sequence of hidden states for all tokens xi at the last layer of a pre-trained language model. The selection model p = {P1, P2, \u2026\u2026\u2026, pn} is defined as follows:\np = \u03c3(Wh\u00b9 + b) (1)\nwhere o is the sigmoid function, W and b are parameters of the projection matrix and bias vector, respectively.\nBy using the selection model p, it is straightforward to compress the context for inference: we directly prune the corresponding tokens according to our desired compression rate in retaining the top k% of tokens in the context. To ensure the efficiency in performing compression, a single forward pass on our model creates the token preservation probability for all input tokens for simplicity.\nTraining. Our training criterion for the selection model aims to preserve the language modeling ability of the LLM while also learning to discard tokens effectively. To this end, we employ the self-supervised approach to optimize the selection model and therefore we do not need external resources to train the selection model compared with Pan et al. (2024).\nTo keep the training process consistent with the inference process, we first discretize the selection model p. Let pi denote the discretized binary model of pi. In other words, pi is 1 if it ranks the top k% of tokens with the highest p values and 0 otherwise. Then we use the discretized model as a mask to define the CLM loss function as follows:\nLCLM = \u2211log P(xi | P1x1,..., Pi\u22121Xi\u22121; 0) (2)\nwhere pixi denotes whether the token xi is masked or not depending on the value of pi, where the above language model P is set as the same model as that used in the selection model in Eq. 1."}, {"title": "Training Details for Transferability.", "content": "One of our goals is to achieve a transferable compression method. Therefore, the parameters that achieve the best loss on the language models in Eq. 1 and Eq. 2 in training may not be transferred to the targeted language model in inference since the language models in training and inference can be different. As a result, to ensure the better transferability of the optimized selection model, we freeze the pre-trained language model in Eq. 1 and employ LORA (Hu et al., 2021) to train a partial parameter in the language model in Eq. 2.1 In summary, the CLM-based training loss is illustrated in Figure 1."}, {"title": "4 Experiment", "content": "4.1 Setting\nWe finetune a LLaMA-2-7b model (Touvron et al., 2023) on 100M tokens from RedPajama (TogetherAI, 2023) on split segments of 1,024 tokens via LORA (Hu et al., 2022). To provide a comprehensive analysis of the model capabilities, our evaluation is conducted on traditional classification tasks as well as the long-context classification task.\nFor each task, we randomly sample from the training set to construct the demonstration set for In-Context Learning (ICL), which also serves as our compression target for token selection. During inference, the compression process only needs to be computed once for all subsequent inferences on the testing instances.\nTraditional Classification Tasks. Following Chevalier et al. (2023), we evaluate and compare different compression models on nine classification tasks, including six tasks from SuperGlue (Wang et al., 2019). The predictions by LLMs are determined by iterating through all possible answer options for the instance and selecting the option with the minimum negative log-likelihood. The in-context demonstrations have been carefully selected to approximate a size of 750 tokens, and the complete demonstration is employed. This is referred to as the \"full-shot\". Since the average token length for a single demonstration varies for different tasks (e.g., a single demonstration in RTE averages about 75 tokens, resulting in a 10-shot setup under the full-shot setting), the exact number of shots differs depending on the task."}, {"title": "Long Context Classification Tasks.", "content": "Recent research by Li et al. (2024) shows the failure of in-context learning tasks when applied to long-context scenarios. To investigate whether compression models can serve as a viable solution in long-context settings, we compare Selection-p with long-context models, including LLaMA-2-7B-LongLora (Chen et al., 2023) and Long-LLaMA-code-7B (Tworkowski et al., 2023) on the BANKING77 dataset (Casanueva et al., 2020). The dataset contains 77 classes where traversing all the instances with unique labels requires approximately two thousand tokens. Evaluation is conducted at 2K, 4K, and 7K token levels, and we adopt a compression rate of 10x for Selection-p and LLMLingua-2 among all levels. Since the long in-context demonstration is used, chunking is performed for every 2,048 tokens in Selection-p. The compressed results are concatenated together with a space token between each pair of chunks. We again follow the evaluation setting by Chevalier et al. (2023) on the models' prediction, with the result presented in Table 3."}, {"title": "4.2 Baselines", "content": "We compare the Selection-p with the following state-of-the-art compression models.\n\u2022 LLMLingua (Jiang et al., 2023) employs an iterative compression algorithm to filter less informative tokens based on the token-level perplexity. To further boost the performance, Jiang et al. (2023) also conducts a budget controller to allocate varying budgets across different demonstrations and questions. We find that there is also a significant discrepancy observed between the prescribed compression rate and the actual compression rate through"}, {"title": "4.3 Evaluation Result", "content": "Traditional Classification Tasks. None of the compression models can achieve superior performance compared to the full-shot demonstration setting, which is in line with our expectations given the information loss during compression. However, certain tasks show a notable improvement when compared to both the zero-shot and full-shot approaches, e.g., all the hard compression models surpass zero-shot and full-shot by approximately 20% in the WSC task. Among all compression models, Selection-p demonstrates the highest performance in conducting ICL, with an average accuracy of 67.4% across all 10 tasks as presented in Table 2. Examples of in-context demonstration before and after compression are shown in Appendix A. To demonstrate the effectiveness of our model, we have included one-tenth of the original demonstration set as the baseline. Our model significantly outperforms the baseline with a comparable number of tokens, therefore highlighting the effect of performing compression at the token level.\nLong Context Classification Tasks. Our model outperforms LLaMA-2-7B-LongLora, Long-LLaMA-code-7B and LLMLingua-2 at all token size levels, and achieves similar results to Li et al. (2024)'s findings on the long-context models."}, {"title": "4.4 Transferability", "content": "Compression is first performed on the demonstration set for ICL with Selection-p. Subsequently, the compressed tokens are passed to a separate downstream model (i.e. LLaMA-2-13B or the black-box models) as the compressed demonstration prompt for evaluation.\n\u03a4\u03bf LLAMA-2-13B. We follow the same setting of evaluation across different classification tasks in Section 4.3. To assess the transferability of the compression models, we compress demonstrations with Selection-p and input the compressed demonstration tokens into LLaMA-2-13B. In comparing different compression models, since retraining is required for soft compression methods, no results can be obtained for AutoCompressor (Chevalier et al., 2023). In the case of LLMLingua, token-level perplexity is calculated with LLaMA-2-13B instead of LLaMaA-2-7B in this experiment.\nOur approach outperforms all other compression models as shown in Table 4. In addition, a small deviation is observed between the 10x compression rate and the full shot setting, demonstrating the great transferability of our models. Notably, with Selection-p, the tasks that outperform the full-shot setting in LLaMA-2-7b also exhibit similar patterns in LLaMA-2-13B.\nTo Black-box Models. Taking cost into consideration, we select ChatGPT (OpenAI, 2023) and Gemini (Team, 2023) for evaluation to examine its transferability to LLMs. Traditional classification tasks often have a simple nature and the potential issue of data contamination, leading to high accuracy and causing an insignificant evaluation. Therefore, we use BANKING77 (Casanueva et al., 2020) for evaluation. Following a similar setup as described in Section 4.3, we adopt a token size of 750 for examination. However, in case the compression rate is too high, ChatGPT and Gemini are more likely to deviate from the instructions and provide task-irrelevant responses. Therefore, we adopt a compression rate of 3x and use the EM metric for this experiment given their black-box nature. Note that there may be variation in the result of Gemini"}, {"title": "5 Analysis", "content": "5.1 Flexibility\nPerformance with Different Number of Initial Tokens. The result in long context classification tasks in Section 4.3 shows the effectiveness of chunk-wise compression in long context. We further analyze if compression models work well in normal few-shot settings in classification tasks. In this experiment, the in-context demonstrations are selected with an approximate size of 250 tokens. The comparison to the result with the token size of 750 in Section 4.3 is presented in Table 6.\nSelection-p shows the best performance under the constraint of 250 tokens when compared to other compression models. Additionally, it also follows the full-shot (i.e. 750 tokens level) trend, the average performance across all classification"}, {"title": "5.2 Latency Analysis", "content": "We analyze end-to-end latency on A100-80G GPU with the WSC task, illustrated in Table 7. Our method can achieve 5.3x speed up on 10x compressed in-context demonstration. Compared to the inference time, negligible time is required for compression on the ICL task setting, demonstrating high efficiency in adopting our models for compression. We also compared LLMLingua with the disabled content Budget Controller. It requires iterative decoding on the segmented context while Selection-p only requires a single inference on all tokens and demonstrates a good performance."}, {"title": "5.3 Correlation with Attention and Perplexity", "content": "With the p-value ranging between 0 and 1 for each token, we further study whether any correlations exist among p, the mean attention value during the forward pass, and the tokens level perplexity (i.e. a core component in LLMLingua (Jiang et al., 2023)). Since the value of p is derived from the last hidden state of the model, we only consider the last layer mean attention of our tuned model. We employed Spearman's Rank Correlation Coefficient (Spearman, 1904) to compute the correlation between the three variables. It is calculated for different traditional classification tasks and the averaged value across tasks. The result presented in Figure 2 indicates only a weak correlation observed between the p value and the other two variables while the correlation between the last layer mean attention and perplexity is more significant. Among all tasks,"}, {"title": "5.4 Tokens Level Part-of-Speech Analysis", "content": "To further interpret the rationale behind our compression models, we analyze what kinds of words are likely preserved by Selection-p. Under the discreteness of our compression result, we locate the corresponding words from the compressed tokens and obtain the Part-of-Speech (PoS) tags with an NLTK tagger. For each type of PoS tag, we compute the token preservation percentage with\n\\frac{|compressed\\_tokentag|}{|total\\_tokentag|}\nfor each PoS tag tagi. The experiment is conducted between the compressed result and the original demonstrations among the nine traditional classification tasks with four demonstration sets per task. We analyze tags with a frequency of appearance greater than 1%.\nFrom the result presented in Figure 3, PRP and punctuations (i.e. indicating the start of the next sentence or phrase) are more likely preserved. The potential reason for the high preservation ratio on PRP (personal pronoun) likely corresponds to the pronoun resolution task of WSC. Under the task"}, {"title": "5.5 On Fair Comparison with LLMLingua", "content": "As described in Section 4.2, LLMLingua conducts demonstration selection prior to compression at the token level, while other methods compress directly on the token level. Since the demonstration selection process can also be incorporated into other"}, {"title": "6 Conclusion", "content": "We introduce a simple yet effective self-supervised approach in context compression and conduct evaluation across 10 classification tasks in both few-shot and long-context settings. Our approach also demonstrated great transferability to both the open-"}, {"title": "Limitations", "content": "Under the consideration of cost, we did not perform further analysis on other LLMs apart from ChatGPT and Gemini. In addition, our model which builds up LLaMA-2-7B does not achieve better latency than models like LLMLingua-2 and Auto-Compressor. Under the ICL setting, minimal time is required for compression, leading to insignificance in end-to-end inference time. While Auto-Compressor offers better latency, its soft compression nature limits its applicability to other LLMs. Overall, our experiments across various tasks and settings demonstrate better performance and transferability, with the benefits outweighing the latency issue."}]}