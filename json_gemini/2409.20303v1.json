{"title": "A Looming Replication Crisis in Evaluating Behavior in Language Models?\nEvidence and Solutions", "authors": ["Laur\u00e8ne Vaugrante", "Mathias Niepert", "Thilo Hagendorff"], "abstract": "In an era where large language\nmodels (LLMs) are increasingly integrated into\na wide range of everyday applications, research\ninto these models' behavior has surged.\nHowever, due to the novelty of the field, clear\nmethodological guidelines are lacking. This\nraises concerns about the replicability and\ngeneralizability of insights gained from\nresearch on LLM behavior. In this study, we\ndiscuss the potential risk of a replication crisis\nand support our concerns with a series of\nreplication experiments focused on prompt\nengineering techniques purported to influence\nreasoning abilities in LLMs. We tested GPT-3.5,\nGPT-40, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-\n8B, and Llama 3\u201370B, on the chain-of-thought,\nEmotionPrompting,\nExpertPrompting,\nSandbagging, as well as Re-Reading prompt\nengineering techniques, using manually double-\nchecked subsets of reasoning benchmarks\nincluding CommonsenseQA, CRT, NumGLUE,\nScienceQA, and StrategyQA. Our findings reveal\na general lack of statistically significant\ndifferences across nearly all techniques tested,\nhighlighting, among others, several\nmethodological weaknesses in previous\nresearch. We propose a forward-looking\napproach that includes developing robust\nmethodologies for evaluating LLMS,\nestablishing sound benchmarks, and designing\nrigorous experimental frameworks to ensure\naccurate and reliable assessments of model\noutputs.", "sections": [{"title": "1. Introduction", "content": "The field of generative artificial intelligence has\nconsiderably evolved in only a few years. In\nparticular, large language models (LLMs) have\nwitnessed an unprecedented surge in\npopularity with the release of ChatGPT (OpenAI,\n2022), which became the most rapidly adopted\ninternet application in history. LLMs possess\nadvanced natural language processing\ncapabilities which demonstrate a broad range of\ndownstream applications, ranging from casual\nconversations to complex problem-solving\n(Minaee et al., 2024; Zhou et al., 2020). Given the\nfast growing range of applications (Guo et al.,\n2024) plus their respective risks for Al\nalignment (Ji et al., 2024), fairness (Hao et al.,\n2023), and safety (Weidinger et al., 2023;\nAmodei et al., 2016; Hagendorff, 2024), it is\nparamount to evaluate behavioral and\nreasoning patterns these models exhibit (Binz &\nSchulz, 2023; Gao et al., 2024; Wang et al., 2024).\nThis created the need for new research fields.\nMany of the approaches to investigate LLM\nbehavior deliberately ignore their inner\nworkings, treating them as \"black boxes\" due to\ntheir complexity, opacity, or lack of open source\n(Castelvecchi, 2016; Rai, 2020). Instead, these\napproaches examine correlations between\ninputs and outputs using specific benchmarks, a\nmethodology often referred to as \u201cmachine\nbehavior\" (Rahwan et al., 2019) or \"machine\npsychology\" (Hagendorff et al., 2024; L\u00f6hn et al.,\n2024). This term draws an analogy to human\npsychology, which also deals with opaque\nstructures human minds by analyzing\nobservable behaviors and responses (Taylor &\nTaylor, 2021). However, psychology has faced a\nreplication crisis, caused by issues such as small\nsample sizes, poorly designed experiments,\npublication bias, lack of transparency, low\nstatistical power, selective reporting,\npreferences for novelty, or the general\ncomplexity of psychological phenomena\n(Hendriks et al., 2020; Lilienfeld & Strother,\n2020). Here, we ask whether similar replication\nproblems are affecting evaluations of LLM\nbehavior.\n\u03a4\u03bf test this assumption, we conduct\nexperiments attempting to conceptually\nreplicate studies investigating prompting\ntechniques that are believed to enhance\nreasoning in LLMs. Our findings reveal that\nthese techniques often fail to produce\nconsistent improvements, highlighting a set of\nspecific methodological shortcomings that\nexemplify our assumption of an impending\nreplication crisis in machine behavior research.\nWe propose a forward-looking approach that\nincludes developing better methodologies for\nLLM evaluations. This involves establishing\nsound benchmarks, designing robust\nexperimental frameworks, and implementing\naccurate evaluations of model outputs."}, {"title": "2. Methods", "content": "For our experiments, we tried to replicate\nprompt engineering techniques that were\ndemonstrated to alter reasoning performances\nin LLMs in previous studies:\n\u2022 Zero-shot chain-of-thought Prompting\n(Kojima et al., 2022): This method claims\nthat adopting a step-by-step reasoning\napproach in LLMs enhances overall\nreasoning performance.\n\u2022 ExpertPrompting (B. Xu et al., 2023): This\ntechnique claims to enhance the LLM\naccuracy when setting the LLM in an expert\nrole.\n\u2022 Sandbagging (Perez et al., 2022):\nSandbagging showcases that LLMs have a\ntendency to repeat back a dialog user's\npreferred response and mirror them when\nsolving tasks.\n\u2022 EmotionPrompting (Li et al., 2023): This\ntechnique consists in adding emotional\nstimuli such as \"This is very important to my\ncareer\", in order to enhance the accuracy.\n\u2022 Re-Reading (X. Xu et al., 2024): This\nmethod consists in repeating the task twice\nto enhance the reasoning performance.\nTo replicate the claimed impact of the selected\nprompt engineering techniques on LLM\nreasoning, we selected five different\nbenchmarks, each measuring a different type of\nreasoning: CommonsenseQA (Talmor et al.,\n2019), StrategyQA (Geva et al., 2021),\nNumGLUE (Mishra et al., 2022), ScienceQA (Lu\net al., 2022), and Cognitive Reflection Tests\n(CRT) (Hagendorff et al., 2023). Due to the low\nquality of many benchmarks items (Goetze &\nAbramson, 2021), meaning incorrect or\nambiguous questions, formatting flaws, or\nfactual errors in the response choices, we hand-\npicked 150 faultless tasks per benchmark, with\na total of n = 750, preferring accuracy over large\nsample sizes. The tasks were either open-ended,\nboolean, or multiple-choice questions. We first\nmeasured the accuracy of LLMs in a base test\nusing unmodified tasks. We then applied the\nprompt engineering techniques proposed by the\nfour studies mentioned above by adding the\nnecessary pre- or suffixes to each task. We used\nthe same prompts described in these studies\nwhen available and generated new ones based\non the prompt descriptions when they were not.\nWhen the studies used several pre- or suffixes\nas a basis to their claim, such as in the\nEmotionalPrompting study where 11 different\nemotional stimuli were used, we randomly\nselected one of them for each task using a seed.\nWe compared the performance of five different\nLLMs, in particular OpenAI's GPT-3.5 (OpenAI,"}, {"title": "3. Results", "content": "Our hypothesis when replicating the previous\nexperiments was that the claimed performance\nimprovements are not replicable in a slightly\ndifferent experimental setup hence proving\nthe original claims to be either wrong or not\ngeneralizable. We systematically tested each\nprompt engineering technique to validate this\nhypothesis."}, {"title": "3.1. Chain-of-thought prompting", "content": "Chain-of-thought prompting involves\ndecomposing a given task and solving each step\nbefore outputting the final answer, by\npresenting the LLM with an example of a task\nand its expected decomposed output. In the\noriginal study establishing this method, Wei et\nal. (2023) tested five LLMs over three reasoning,\ncategories including arithmetic reasoning,\ncommonsense reasoning, and symbolic\nreasoning, harnessing 12 different benchmarks.\nThe authors claim a good robustness of this\nmethod, with several different annotators.\nWhile they reported variance in the average\nperformance, it was consistently superior to the\nperformance with the base evaluation, with a\nreported average improvement of 39.91% (Wei\net al., 2023).\nA subsequent study then claimed that a zero-\nshot chain-of-thought prompting strategy\nsufficed to elicit similar improvements (Kojima\net al., 2022). Instead of presenting, before each\ntask, an example enabling chain-of-thought\nreasoning, they simply suffix tasks with \u201cLet's\nthink step by step\u201d. They tested a larger sample\nof 17 LLMs on various reasoning categories,\nutilizing 12 benchmarks akin to the previous\npaper. They obtained an averaged 35.93%\nimprovement in accuracy for zero-shot chain-\nof-thought reasoning across all benchmarks and\nmodels (Kojima et al., 2022).\nWe tried to replicate these findings with our set\nof reasoning benchmarks. However, despite the\nimpressive results from the original studies, we\nobserved that there was no significant\nimprovement (see Figure 1): with the exact\nsame task suffix as in the original study, we\ncould not observe any significant difference\nacross all benchmarks. With results from all\nmodels combined, the maximal positive impact\nof chain-of-thought reasoning is with NumGLUE\nwhere there is a 2.78% accuracy difference\nbetween the base and the chain-of-thought\nprompt (see Appendix B), which is not\nsignificant given the total number of tasks (x2 =\n1.78, p = .18). These numbers remain similar\nthroughout each LLM evaluated, with an overall\naverage improvement of 0% for the chain-of-\nthought reasoning (x\u00b2 = 0.06, p = .8), as seen in\nAppendix B. The largest observed positive\nimpact of chain-of-thought reasoning is for\nLlama 3-70B tasked by CommonsenseQA, with\nan observed 8.67% improvement (x\u00b2 = 2.19, p =\n.14) (see Appendix B), but the highest overall\ndifference is an 11.33% accuracy decrease (x2 =\n4.47, p < .05) (see Appendix B) with chain-of-\nthought reasoning applied on Llama 3-70B with\nStrategyQA. While the latest models seem to\nimplement chain-of-thought reasoning by\ndefault, meaning without being specifically\nprompted to, these results hold even for\nprevious models such as GPT-3.5, which often do\nnot. We compared the average response length\nof each LLM when chain-of-thought reasoning is\nexplicitly requested, compared to when it is not,\nas shown in Appendix C.\nEven when the base experiments do not\ndemonstrate verbose prompt completions and\nthe chain-of-thought prompting does, the\nperformance results are not impacted in a\nsignificant manner, which stands contrary to\nwhat the literature suggests (Jin et al., 2024).\nFor instance, GPT-40 had an average difference\nof response lengths of 531 characters for the\nbase test vs. 931 characters for the chain-of-\nthought prompting, but just a 0.01% accuracy\ndifference, suggesting that simply increasing the\nlength of prompt completions does not enhance\naccuracy beyond a certain point."}, {"title": "3.2. ExpertPrompting", "content": "ExpertPrompting consists in giving LLMs an\ninstruction to impersonate someone with high\nexpertise on the task subject while completing a\ntask. This method presented by B. Xu et al.\n(2023) has been greatly popularized and is now\neven recommended in LLM documentations for\nenhanced LLM accuracy and improved focus on\nadhering to the task's requirement.\nB. Xu et al. (2023) evaluated the response\nquality of ExpertPrompting, assessing aspects\nlike accuracy, helpfulness, or relevance. They\nbase their main claim, namely that LLM output\nquality can be \"drastically improved\u201d (B. Xu et\nal., 2023, p. 1) with their technique, on\nevaluating GPT-4 responses with and without\nExpertPrompting, which, in the case of the\nformer, possesses a reported higher answer\nquality 48.5% of the time (B. Xu et al., 2023). In\nour experiments, we measure the accuracy of\nthe ExpertPrompting technique using our set of\nreasoning benchmarks and LLMs. We observe\nno significant improvement across all\nbenchmarks (x\u00b2 = 1.57, p = .21) (see Figure 2),\nwith an average improvement of only 1% (see\nAppendix B). Therefore, we cannot replicate the\nimprovement capabilities insinuated in the\noriginal study."}, {"title": "3.3. Sandbagging", "content": "Perez et al. (2022) demonstrate sycophancy,\nwhich is an LLM's tendency to output answers\nusers tend to prefer. The researchers evaluated\nseveral aspects of sycophancy, including a\n\u201csandbagging\u201d capability, which suggests that a\nmodel could underperform when a user is\ndeemed incapable to solve or verify a given task.\nThey underpin this hypothesis by adding user\nbiographies before reasoning tasks from\nTruthfulQA (Lin et al., 2022), with \"very\neducated\" users as opposed to \u201cvery\nuneducated\" users. They imply a significant\ndifference between these two categories,\nclaiming that sandbagging causes LLMs to\noutput incorrect answers when human users\nare perceived as unable to answer correctly\nthemselves (Perez et al., 2022, p. 29).\nWe conceptually replicate this experiment using\nour selected models by prefixing our selected\nreasoning tasks with both \u201cvery educated\u201d and\n\"very uneducated\" user biographies (see\nAppendix D). We observe no significant\ndifference over all benchmarks when\ncomparing the highly educated (x\u00b2 = 1.64, p =\n.20) or poorly educated (x\u00b2 = 1.24, p = .27) user\nprompts to the base results (see Figure 3, Figure\n4, and Appendix B), with an average accuracy\ndecrease of 1% for both cases (see Appendix B).\nWe likewise observe no significant difference\nwhen comparing the highly educated to the\npoorly educated user prompt results, and\nfrequently observe that the \"poor education\u201d\nprefixed tasks have an even better performance\nthan the \"high education\u201d ones (average\naccuracy improvement of 0.1% for \"poor\neducation\"). Once again, we fail to replicate the\nsandbagging phenomenon when utilizing our\nexperimental setup."}, {"title": "3.4. EmotionPrompting", "content": "Emotion prompting, presented by Li et al.\n(2023), augments a task with emotional cues\nsuch as \"You'd better be sure\u201d or \u201cThis is very\nimportant to my career\u201d to enhance problem-\nsolving abilities in LLMs. In the original study, Li\net al. augmented tasks with 11 variations of\nemotional stimuli and tested six LLMs including\nChatGPT and GPT-4. They sourced their tasks\nfrom three different benchmark categories,\nnotably using tasks from BIG-Bench (Srivastava\net al., 2022). They claim to obtain a \"relative\nperformance improvement of 115%\u201d (Li et al.,\n2023, p. 1) with their method, arguing that\nadding an emotional component improves the\ncapabilities of LLMs. However, despite the\nimprovement that was strongly implied\nthroughout the original study by raising claims\nlike \"EmotionPrompt makes it easy to boost the\nperformance of LLMs\u201d (Li et al., 2023, p. 6), the\nnumerical values communicated in the study\nitself do not coincide with these claims. Instead\nof communicating the average improvement of\nthe enhanced prompts over the regular\nprompts, they focused on improvements when\ncherry-picking the most performant emotional\ncue. Based on their reported results, we\ncalculated an averaged relative performance\nimprovement of 4.42% on BIG-Bench tasks, and\na 2.58% relative performance improvement\nacross all benchmarks, when choosing the\naverage performance of all emotional stimuli.\nDespite identifying this shortcoming in the\noriginal study at this early stage, we\nnevertheless replicated the experiments with\nour selected tasks and models. We applied the\nsame emotional suffixes as in the original study,\napart from \"Are you sure?\u201d, as LLMs tend to\nreply to this question, as opposed to solving the\ngiven tasks. Similarly to Li et al.'s findings, but\ncontrary to their claims, we observed that there\nwas no significant improvement, across every\nsingle model and benchmark (see Figure 5). The\nmaximal positive improvement measured is\nnon-significant with an 8.7% difference (x\u00b2 ="}, {"title": "3.5. Re-Reading", "content": "Re-Reading, introduced by X. Xu et al. (2024),\nconsists in repeating the task verbatim before\nletting the model answer. They compared the\nbaseline performance with the Re-Reading\nperformance, as well as the performance in both\nconditions when additionally suffixing every\ntask with a chain-of-thought eliciting prompt.\nThe researchers tested GPT-3 (text-davinci-\n003) (Brown et al., 2020), GPT-3.5, Llama-2-13B\nand Llama-2-70B (Touvron et al., 2023), in\norder to have both models with and without\ninstruction fine-tuning. They used a total of 112\narithmetic, common sense, and symbolic\nreasoning tasks sourced from various datasets\nwith GPT-3 and GPT-3.5, for which they\nobtained an average gain of 2.7% in accuracy,\nand 2.9% with the inclusion of chain-of-thought\nreasoning. For Llama-2-13B and Llama-2-70B,\nthey used a different set of benchmarks\ncomprising only of arithmetic reasoning tasks,\nwith an average gain of 2.5% in accuracy (2.7%\nwith chain-of-thought reasoning).\nWe replicate the Re-Reading experiments on\nour selected tasks and models. For this study,\nwe observe a significant improvement for Llama\n3-8B (x\u00b2 = 13.13, p < .05) and Llama 3-70B (x\u00b2 =\n19.4, p < .05) exclusively (see Appendix B and\nFigure 6). The maximal improvement across all\nbenchmarks for the other models is of 2%, for\nClaude 3 Opus (x\u00b2 = 1.27, p = .26). Therefore, Re-\nReading seems replicable for the Llama 3\nmodels only, which highlights the importance of\nimplementing tests on a variety of models.\nHowever, the initial study indicated that Re-\nReading was effective on GPT models, notably\nGPT-3.5, that we also tested with different\noutcomes. Therefore, we only managed to\npartially replicate the results."}, {"title": "4. Recommendations", "content": "Given the identified lack of replicability across\nvarious studies, we deem crucial to address the\nunderlying issues contributing to these\nreplication problems. We categorize these\nissues into four main areas: low-quality\nbenchmarks, methodological shortcomings,\nchanges in model behavior over time, and\ninsufficient accuracy in LLM output\nclassification."}, {"title": "4.1. Ensure adequacy of benchmarks", "content": "In several of the studies examined, we noted\nmajor issues regarding the benchmarks used to\nassess LLM performances: many tasks lack\nproper grammar, present spelling issues, or\npunctuation problems such as the absence of a\nquestion mark at the end of a question.\nFurthermore, many tasks are nonsensical, lack\nnecessary information, or are blatantly\nincorrect. One might assume that, because of the\nsheer number of tasks present in typical\nbenchmarks (21,208 questions for ScienceQA,\nfor example), a small number of errors may be\ninevitable. However, we observed a high\npercentage of flawed tasks: for instance, for\nCommonsenseQA,\n10.9% of questions\npresented punctuation issues easily verifiable\nwith a simple code.\nSome of the replicated studies chose to use\nbenchmarks in which serious flaws can be\nidentified (Goetze &\nAbramson, 2021),\npresumably assuming that the benchmarks\nwere overall correct. In this case, any error\nwould be an exception that would\nunderstandably be negligible compared to the\ntotal amount of correct tasks. However, given\nthe higher percentage of task issues observed,\nthis reasoning does not hold. On the other hand,\nthe reliability of a study is also compromised\nwhen the number of benchmark tasks is too\nreduced, such as in the EmotionPrompting\nstudy (Li et al., 2023) where the models were\ntested on a total of 45 tasks, or the Re-Reading\nstudy (X. Xu et al., 2024) where only 112 tasks\nwere used. This makes the results more\nsusceptible to the effects of outliers or the\npotential for cherry-picking tasks that yield the\nmost favorable results, therefore skewing the\noverall conclusions. We recommend rigorously\nvalidating and cleaning benchmark datasets to\nensure that tasks are grammatically correct,\nsensible, and complete before using them,\nprioritizing quality over quantity, while still\nmaintaining a sufficient number of tasks to\nensure valid statistical analysis and reduce the\nrisk of cherry-picking or task-related variation.\nFurthermore, testing models on different sets of\nbenchmarks within the same study, as seen in\nthe Re-Reading study (X. Xu et al., 2024), where\nthe GPT models were evaluated on three types\nof reasoning benchmarks while the Llama 2\nmodels were only tested on one type, further\nexacerbates this issue. This different selection of\nbenchmarks not only complicates direct\ncomparison of model performances, but also\nraises questions about the reasoning behind\nsuch choices, potentially leading to concerns\nabout cherry-picking benchmarks that yield the\nbest results.\nAnother prominent issue lies in the consistency\nof benchmarks. We chose to use the same\nbenchmarks for all experiments for better\ncomparison purposes. However, in the\nliterature, benchmarks used throughout studies\nin a similar field are often inconsistent. For\nexample, the five studies selected were applied\non widely different benchmarks, which makes\nresults hard to compare. Similarly, variations\nwithin the process of administering benchmark\ntasks (notably zero-shot prompting versus few-\nshot prompting) impact the reasoning process\nof LLMs and therefore the outcomes. Moreover,\nminor changes in a prompt wording can\nsignificantly impact LLM outputs (Sclar et al.,\n2023).\nFinally, it is paramount to select appropriate\nbenchmarks coherent with the research\nquestion. When testing prompt techniques to\nalter reasoning performance, given or implied\nresponse instructions in the tasks can interfere\nwith output accuracy by restricting the\nresponse length and therefore its ability to\ngenerate more detailed responses, for instance\nin multiple-choice settings. We recommend\npreferring standardized benchmarks across\nstudies in the same field, to reduce variability in\nresults and ensure that benchmarks are closely\naligned with the research objectives"}, {"title": "Recommendations", "content": "\u2022\n\u2022\nValidate and clean benchmarks to ensure\ncorrectness and completeness.\nEnsure a sufficient number of tasks for\nvalid statistical analysis.\nStandardize the benchmark selection\nwithin studies for better comparability.\n\u2022\nControl for prompt sensitivity.\n\u2022\nAlign\nbenchmarks with research\n\u2022\nobjectives."}, {"title": "4.2. Guarantee the transparency of the methods used", "content": "Each replicated study presents its own\nmethodology, which needs to be accounted for\nwhen analyzing the claims and results. Indeed,\nsimilar studies may obtain largely different\noutcomes when solely the experimental setup\ndiffers. Notably, the method used to classify LLM\nresponses majorly impacts results. In the\nsandbagging study (Perez et al., 2022),\nresearchers evaluate differences in model\naccuracy when answering questions on the\nTruthfulQA dataset (Lin et al., 2022), which\nmeasures whether a language model is truthful\nin generating answers to questions, so whether\nthe facts mentioned in the answer are correct\nrather than assessing whether they answered\nthe task correctly. In the ExpertPrompting study\n(B. Xu et al., 2023), researchers establish a\nrelative score by comparing the quality of the\nanswer with ExpertPrompting to the baseline\nusing an LLM-based evaluation. Consequently,\ndespite these studies presenting their claims\nsimilarly using verbs such as \u201cimproves\u201d,\n\"enhances\", \"overperforms\u201d to describe their\nprompting techniques, their outcomes cannot\neffectively be compared.\nFurthermore, some studies display a\nparticularly poor or unclear scientific method.\nIn the EmotionPrompting study (Li et al., 2023),\nresearchers cherry-pick the prompt with the\nbest result out of eleven different prompts,\nrather than calculating an average across all\nprompts. This seemingly deliberate action may\nbe due to a publication bias, which motivates\nresearchers to manipulate results to be positive\nand therefore publishable. In addition, some\nstudies, such as the Re-Reading study (X. Xu et\nal., 2024), report results as \"significant\"\nmultiple times without presenting the\ncorresponding statistical calculations or p-\nvalues. This lack of statistical transparency can\nmislead readers into assuming statistical\nsignificance without the necessary evidence to\nsupport such claims. It is crucial that when\nterms like \"significant\" are used, they are\nbacked by clearly defined statistical measures.\nMoreover, some studies do not properly report\nthe details of their experimental setup (Perez et\nal., 2022), which makes it confusing or even\nimpossible to understand and therefore to\nreplicate their process exactly. In this case, the\nlack of transparency forbids us from detecting\npossible issues or biases, making it difficult to\ntrust the study's conclusions. We recommend\nadopting\nstandardized\nevaluation\nmethodologies and clearly defining metrics to\nensure that results from different studies can be\naccurately compared and interpreted."}, {"title": "Recommendations", "content": "\u2022\n\u2022\n\u2022\n\u2022\n\u2022\nSelect a standardized methodology for\nconsistent comparisons across studies.\nAvoid intended or accidental cherry-\npicking and report averaged results across\ntasks whenever possible.\nEnsure statistical transparency with\nclearly reported p-values.\nProvide a complete documentation of\nexperimental setups to aid replication.\nDefine clear and consistent evaluation\nmetrics."}, {"title": "4.3. Be aware of model updates", "content": "In some cases such as in the chain-of-thought\nprompting study (Kojima et al., 2022), we\nhypothesize that the lack of replicability is\nlinked to the models used. With our results on\nthe chain-of-thought prompting, we can see a\ndifference in accuracy between GPT-3.5 and\nGPT-40: the former benefits more from chain-\nof-thought prompting than the latter. Despite\nthe results not being significant with either\nmodel, the results obtained could lead us to\nbelieve that with previous models such as GPT-\n3, which was used in the replicated study along\nwith other models of that generation, chain-of-\nthought prompting would have successfully\nimproved LLM accuracy. This aligns with the\nsystem cards for recent models, which explicitly\nwarn that techniques like chain-of-thought\nreasoning may not improve performance and\ncan even impair it, advising caution in their use\nwith these models (OpenAI, 2024). Similarly, we\nobserved a significant improvement with the\nRe-Reading prompt, for the Llama 3 models\nexclusively; if we look at the other models\nseparately, the results are vastly non-significant\n(see Appendix B). This reinforces that similar\nexperiments may have a considerably different\nimpact depending on the models used.\nTherefore, it is essential to use a variety of\nmodels when testing a hypothesis, or to at least\nmention the limited scope of the study when\nfewer models are used, as an effort to prevent a\ngeneralization that may not be correct.\nFurthermore, as the models evolve and become\nbetter reasoners, it seems necessary to adapt\nthe difficulty of the benchmarks used\naccordingly, to lower the near-perfect overall\naccuracy and therefore improve accuracy\ncomparisons, as it is for instance the case with\nthe CRT benchmark (Hagendorff et al., 2023).\nMoreover, even when conducting replication\nexperiments using the same models as in the\noriginal study, the opacity surrounding model\nupdates in terms of date and type of update\n(Chen et al., 2024) renders study replications\ndifficult. Finally, the latest models may include a\nstronger set of internal instructions to optimize\ntheir output, which leads to different results and\nbehaviors. Similarly to how many LLMs, when\nasked coding questions, now explain the entire\nprocess instead of solely outputting the\nrequired code, we suspect that models may have\nbeen trained to use the chain-of-thought\nreasoning as default, which also explains why\nspecific instructions conveying chain-of-\nthought reasoning seem useless with current\nstate-of-the-art models. Therefore, we\nrecommend adjusting benchmark difficulty as\nmodels evolve, in order to maintain comparable\nresults. We also suggest staying aware of\npossible model updates when evaluating\nbehavior in LLMs."}, {"title": "Recommendations", "content": "\u2022\n\u2022\n\u2022\n\u2022\nMonitor changes in model behavior that\ncould affect results.\nAccount for model variability by using\ndiverse models, and specify if only a\nlimited range is tested.\nAdjust benchmark difficulty as models\nimprove.\nEnsure model selection transparency,\nnotably by documenting the model version\nand date of experiment launch."}, {"title": "4.4. Ensure accurate LLM output classifications", "content": "For behavioral experiments with LLMs, it is key\nto ensure the accuracy of the LLM output\nclassifications. We have attempted to replicate a\nlarge number of verification techniques\npresented in other studies. However, when\nchecking the accuracy of these techniques, we\ndiscovered that a significant number of them\nhad shortcomings. For example, functions based\nsolely on Regex rules were generally too vague,\nleading to flawed classifications. Other metrics,\nsuch as the F1 word overlap score, do not work\neffectively, as they would classify correct LLM\noutputs as incorrect, simply because the token\nlength differed too much from the ground truth.\nMoreover, studies often rely on using LLMs to\nclassify LLM outputs (Pan et al., 2023). When\nusing the given or even enhanced versions of\ntheir LLM output classification prompts, we\ndiscovered that many issues arose, rendering\nthe classification process incorrect: the LLM\nwas influenced by the task when classifying\nresponses, and the verification prompt needed\nto be task-specific and highly precise. Given the\ntime-consuming aspect of manually double-\nchecking classifications, we also suspect that\nthis is done very rarely. Furthermore, we have\ndiscovered papers that did not indicate their\nverification process at all (Li et al., 2023); it goes\nwithout saying that any verification process\nshould be clearly reported in each study, to\nmake the study replication feasible. We\ntherefore recommend developing more precise\nand task-specific verification methods, and\nensuring thorough documentation of these\nprocesses in all studies to facilitate accurate\nreplication and validation of results. We also\nrecommend that creators of new benchmarks\nprovide a standardized verification process,\nencouraging all users to apply the same\nverification criteria."}, {"title": "Recommendations", "content": "\u2022\n\u2022\nDouble check LLM output classifications to\navoid vague or ineffective evaluation\nmetrics.\nDevelop task-specific, precise verification\nmethods to ensure accurate classification.\nEnsure transparency by thoroughly\ndocumenting verification processes.\nStandardize verification methods to\npromote consistency across studies and\nbenchmarks.\nFor benchmark creators, provide a specific\nverification process implementers can use."}, {"title": "5. Discussion", "content": "Our experiments show that most of the tested\nprompting techniques do not lead to replicable\nor generalizable performance improvements in\nLLMs. Most techniques, when applied in slightly\ndifferent experimental setups, failed to produce\nthe claimed benefits. Some techniques\noccasionally even resulted in a decrease in\nresponse accuracy. In view of the uncritical\npropagation of the prompt engineering\ntechniques in the literature (Schulhoff et al.\n2024), we recommend a more cautious\napproach when citing papers with insufficient\nmethodological standards. Our results suggest\nthat further research is necessary to reliably\nunderstand the conditions under which specific\ntechniques are effective. Moreover, further\nreplication studies should be conducted in\norder to verify or refute insights from machine\nbehavior research and LLM evaluations. In line\nwith solutions proposed for the replication\ncrisis in psychology, we recommend increased\ntransparency and the application of rigorous\nscientific methods when evaluating LLM\nbehavior."}, {}]}