{"title": "Approximating Probabilistic Inference in Statistical &L with Knowledge Graph Embeddings", "authors": ["Yuqicheng Zhu", "Nico Potyka", "Bo Xiong", "Trung-Kien Tran", "Mojtaba Nayyeri", "Evgeny Kharlamov", "Steffen Staab"], "abstract": "Statistical information is ubiquitous but drawing valid conclusions from it is prohibitively hard. We explain how knowledge graph embeddings can be used to approximate probabilistic inference efficiently using the example of Statistical &L (SEL), a statistical extension of the lightweight Description Logic &L. We provide proofs for runtime and soundness guarantees, and empirically evaluate the runtime and approximation quality of our approach.", "sections": [{"title": "1 INTRODUCTION", "content": "Description logics (DLs) [5] are logical languages designed for representing ontological knowledge. They allow first-order expressions while balancing expressive power and reasoning complexity. One of the most prominent DLs is the Existential Language (EL) [8], which supports conjunction and existential quantification. &L has polynomial reasoning complexity and has become one of the underlying formalisms of the standardized Web Ontology Language (OWL2 EL) [15].\nEL ontologies express knowledge by deterministic axioms like Lung Cancer Lung Disease, wich expresses that lung cancers are lung diseases. However, &L cannot express uncertainty about knowledge. Statistical &L [35] (SEL) is a probabilistic extension of EL that allows reasoning about statistical proportions. SEL ontologies are composed of probabilistic conditionals of the form (D | C) [l, u]. For example, (\u2203has.Chest Pain | \u2203has.Lung Disease) [0.19, 0.21] expresses that around 19-21 % of persons with a lung disease suffer from chest pain. Given a set of such conditionals, we want to compute the probabilities of related events. For example, if we also knew that 1 in 1000 patients have a lung disease and that 1 in 500 patients have chest pain, the laws of probability theory would allow us to infer that the probability that a patient with chest pain has a lung disease is between 38 and 42%. While a doctor trained in probability theory could derive the probability using Bayes' rule, SEL allows deriving such probabilities automatically. Unfortunately, reasoning in SEL is ExpTime-hard [9] and, therefore, provably intractable.\nWe explain how knowledge graph embeddings can be used to approximate reasoning in SEL efficiently. To do so, we first embedd concepts in a vector space while maintaining statistical proportions in the ontology."}, {"title": "2 RELATED WORK", "content": "Probabilistic first-order logics can be classified as type 1 (statistical), type 2 (subjective) or type 3 (combined) logics [18]. Type 1 probabilities represent proportions in the domain, while type 2 probabilities represent a degree of belief that a statement is true. Technically, type 1 semantics define probabilities of formulas based on proportions in the domain of interpretations, while type 2 semantics assign probabilities to interpretations and define probabilities of formulas by adding the probabilities of interpretations that satisfy them. To the best of our knowledge, most probabilistic DLs are type 2 logics, for example, [17, 26, 34]. The only other type 1 probabilistic DL that we are aware of has been sketched in the appendix of [27]. Consistency-checking is ExpTime-hard for this logic as well, and we are unaware of any implementations. While there are some implementations of type 2 logics, the problem that they are solving is not comparable to the one that we consider here. In particular, the subjective uncertainty is often introduced over domain elements. For example, in [17], a statement of the form P\u2265pC for some probability p\u2208 [0, 1] and some concept C evaluates to the set of all domain elements e such that our subjective belief that e belongs to C is p. Such statements are not meaningful in a statistical setting because the objective probability is either 1 (e belongs to the concept) or 0 (e does not belong to the concept). Furthermore, while the type 1 reading of a conditional (D | C) [p] is that the proportion of Cs that are also Ds is 100 p%, the type 2 reading is that our subjective belief that all Cs are Ds is p. So even if a type 2 reasoner supports the conditonals that we consider here, it will interpret them in a different way. In principle, probabilistic reasoning in SEL can be performed exactly by constructing a linear optimization problem, where the numerical variables and constraints are derived from a type elimination procedure and the conditionals in the knowledge base as sketched in the appendix of [27]. However, this approach is difficult to implement and not practical because the number of types rapidly explodes. A more pragmatic approach for classical inference would be creating a proof system for SEL similar to proof systems designed for propositional probabilistic logics [13, 19]. However, even in propositional logic, completeness of such proof systems could only been shown for very limited fragments [13].\nKnowledge graph (KG) embeddings [10] map entities and relations into a vector space. Prominent examples include translational"}, {"title": "3 BACKGROUND ON STATISTICAL EL", "content": "The DL &L [2] describes concepts and their relationships using a set Nc of concept names and a set NR of role/relation names. Every concept name A \u2208 Nc and the symbol \u22a4 (called top) are (atomic) EL concepts. If C1, C2 are &L concepts and r\u2208 NR, then so are C1 \u2293C2 and \u2203r.C. An &L interpretation I = (\u2206I,\u00b7I) consists of a non-empty set \u2206I called the domain of I and a mapping \u00b7I that maps every concept name to a subset of \u2206I, and every relation to a relation over \u2206I\u00d7\u25b3I. \u00b7I is extended to arbitrary concepts as follows. We let \u22a4I = \u2206I, (C1 \u2293C2)I = C1I \u2229C2I and (\u2203r.C)I = {a \u2208 \u2206I | \u2203b \u2208 CI : (a,b) \u2208 rI}. An &L TBox contains general concept inclusions (GCIs) of the form C\u2286 D, where C, D are concepts. An interpretation I satisfies C\u2286 D iff CI \u2286 DI. We write C \u2261 D as a shorthand for the two GCIs C\u2286 D and D\u2286 C.\nStatistical EL, SEL for short, is a probabilistic extension of EL that allows reasoning about statistical statements [35]. The basic syntactic elements are (probabilistic) conditionals (D | C) [l, u], where C, D are &L concept descriptions and l, u \u2208 [0, 1] \u2229 Q are (rational) probabilities such that l < u. If l = u, we just write (D | C) [p]. The intuitive reading of (D | C) [l, u] is that the conditional probability of D given C is between l and u. The probabilities are interpreted statistically. That is, (D | C) [l, u] means that the proportion of elements in C that also belong to D is between l and u. The formal semantics of SEL is defined with respect to &L interpretations I with finite domain \u2206I. I satisfies (D | C) [l, u], denoted as I |= (D | C) [l, u], iff either CI = 0 or |(D\u2229C)I||CI|\u2208 [l, u]. This is equivalent to saying that I satisfies (D | C) [l, u] iff\n$$l\u00b7|CI| \u2264 |(D\u2229C)I | \u2264 u \u00b7 |CI |.$$\nAs shown in [35, Proposition 4], SEL generalizes &L in the sense that every GCI can be represented by a (deterministic) conditional.\nLEMMA 1 ([35]). For all &L interpretations I, we have I |= C\u2286 D iff I |= (D | C) [1].\nThe consistency problem for SEL turned out to be EXP-complete [4, 9]. Here, we are interested in the following inference problem for SEL: Given an SEL TBox T and a query (D | C), where C, D are SEL concepts, find the largest lower bound l and the smallest upper bound u such that every interpretation I that satisfies T also satisfies (D | C) [l, u]. Note that the consistency problem can be reduced to the decision variant of the inference problem because the knowledge base is inconsistent iff [l, u] = 0."}, {"title": "4 A NORMAL FORM FOR SEL", "content": "Embeddings of the classical DL &L [24, 39] exploit that every &L knowledge base can be transformed into an equivalent normal form that contains only GCIs of the following form [3]:\n$$A \u2286 B, A_1 \u2293A_2 \u2286 B, A \u2286 \u2203r.B, \u2203r.A \u2286 B,$$\nwhere A, A1, A2, B are concept names or \u22a4. Every &L TBox T can be transformed into a TBox T' in normal form with only a linear blowup of the size of the knowledge base [3]. We show the corresponding transformation rules in Figure 2. We can generalize the normal form to SEL as follows.\nDEFINITION 1 (SEL NORMAL FORM). A SEL TBox T is in normal form if for every conditional (D | C) [l, u] \u2208 T either\n(1) l = u = 1 and C \u2286 D is in &L normal form or\n(2) C, D \u2208 Nc are concept names.\nSo in a normalized SEL TBox, for each conditional (D | C) [l, u], either C and D are just concept names or the conditional is deterministic (l = u = 1) and therefore just represents an EL GCI as explained in Lemma 1. To normalize an SEL TBox, we need only one additional rule that replaces complex concepts C, D in non-deterministic conditionals with new concept names A1, A2:\n$$(D | C) [l, u] \u2192 (A_2 | A_1) [l, u], C \u2261 A_1, D \u2261 A_2,$$\nwhere l < 1 (the conditional is non-deterministic) and C or D are complex concepts. In order to normalize an SEL TBox, we can then do the following:\n(1) Apply rule SNF0 to replace all complex concepts in conditionals with new concept names and create new equivalences (2 GCIs) along the way.\n(2) Normalize the GCIs using the &L transformation rules.\n(3) Apply Lemma 1 to transform the normalized GCIs into deterministic conditionals.\nAs we prove in appendix A, the resulting SEL TBox is in SEL normal form, entailment-equivalent to the original TBox and its size is linear in the size of the original TBox.\nPROPOSITION 1. Let T be an SEL TBox and let T' denote the TBox resulting from our normalization procedure.\n(1) T' is in SEL normal form.\n(2) For all SEL conditionals (D | C) [l, u] built up over the concept names occurring in T, we have T |= (D | C) [l, u] if and only if T' |= (D | C) [l, u].\n(3) The size of T' is linearly bounded by the size of T."}, {"title": "5 EMBEDDING AND APPROXIMATING SEL", "content": "We will now generalize the BoxEL embedding from [39] to SEL and then explain how it can be used to perform approximate (probabilistic) inference.\nTo begin with, we need vector representations of concepts and relations. Following [39], we represent concepts by boxes and relations by affine transformations. Formally, we represent the embedding of concepts by two functions mw, Mw that are parameterized by a learnable parameter vector w. Intuitively, mw : NC \u2192 Rn maps concept names to the lower left corner, and Mw : NC \u2192 Rn maps them to the upper right corner of their box representation, where n is the dimension of embeddings. Formally, the box associated with C\u2208 NC is defined as\n$$Box_w(C) = \\{x \u2208 \\mathbb{R}^n | m_w (C) \u2264 x \u2264 M_w(C)\\},$$\nwhere the inequality is defined elementwise-wise. The volume of the box is the product of its side lengths:\n$$Vol (Box_w (C)) = \\prod_{i=1}^n max (0, M_w (C)_i \u2013 m_w (C)_i).$$\nWe associate every role name r \u2208 NR with an affine transformation Tr(x) = Dx + b, where D is an (n\u00d7n) diagonal matrix with non-negative entries and b \u2208 Rn is a vector. Applying T to the box associated with a concept C results in the box Tr(Boxw(C)) = {Tr(x) | x \u2208 Boxw(C)} with lower corner Tr(mw(C)) and upper corner Tr(Mw(C)) [39].\nWhen we compute an n-dimensional embedding, our parameter vector contains 2n parameters for every concept name C\u2208 NC (mw(C) and Mw (C)), and 2n parameters for every role name r \u2208 NR (b and diagonal entries ofD). Hence, the overall size of w is n \u00b7 (2|NC|+2 \u00b7 |NR|).\nIn order to compute the parameter vector w of our embedding, we minimize a loss function L(w). L(w) is composed of multiple terms that correspond to the four axiom types that can occur in the normalized TBox. We first define disjoint measurement of two boxes B1 and B2, which is guaranteed to be between 0 and 1, is 0 whenever B1 \u2229 B2 \u2260 \u2205 and is 1 whenever B1 \u2229 B2 = \u2205 [39],\n$$Disjoint(B_1, B_2) = 1 - \\frac{Vol(B_1 \u2229 B_2)}{Vol(B_1)}$$\nNF1: Atomic Subsumption. Axioms of the form C \u2286 D with D \u2260 \u22a4 are encoded by the loss term\n$$L_{C\u2286D}(W) = Disjoint (Box_w (C), Box_w(D)).$$\nIf D = \u22a4 and C is not a nominal, that is, C \u2262 \u22a5, the loss term is\n$$L_C(w) = max(0, M_w (C)_0 - m_w(C)_0 + \u03b5).$$\nIf C is a nominal, the axiom is inconsistent and this can be reported to the user.\nNF2: Conjunctive Subsumption. Axioms of the form C \u2293D \u2286 E with E \u2260 \u22a4 are encoded by the loss term\n$$L_{C\u2293D\u2286E} (W) = Disjoint (Box_w (C) \u2229 Box_w (D), Box_w(E)).$$\nFor E = \u22a4, the loss term is\n$$L_{C\u2293D\u2286\\top}(W) = \\frac{Vol(Box_w (C) \u2229 Box_w (D))}{Vol(Box_w(C)) + Vol (Box_w (D))}$$\nNF3: Right Existential. Axioms of the form C \u2286 \u2203r.D. are encoded by the loss term\n$$L_{C\u2286\u2203r.D}(w) = Disjoint(T(Box_w(C)), Box_w(D)).$$\nNF4: Left Existential. Axioms of the form \u2203r.C \u2286 D are encoded by the loss term\n$$L_{\u2203r.C\u2286D}(w) = Disjoint(T_w (Box_w (C)), Box_w(D)),$$\nAs shown in [39], it holds for all encodings that La(w) = 0 implies that the corresponding axiom \u03b1 is satisfied by the geometric interpretation Iw corresponding to the embedding.\nWe will introduce a new loss term to encode the meaning of probabilistic axioms. Before we do so, let us note that every embedding can be seen as an SEL interpretation.\nDEFINITION 3 (GEOMETRIC SEL INTERPRETATION). A geometrical SEL interpretation (of dimension n) I = (\u2206I,\u00b7I) is an EL interpretation where \u2206I = Rn.\nWe associate the parameter vector w of an n-dimensional SEL embedding with the SEL interpretation Iw defined by\nCIw = Boxw(C) for all C\u2208 NC,\nrIw = {(a, b) \u2208 \u2206I \u00d7 \u0394I | Tr(a) = b} for all r \u2208 NR.\nWe can adapt the SEL semantics naturally to (infinite) geometric interpretations by replacing the cardinality of concepts with their volume. The satisfaction condition for the conditional (D | C) [l, u] from equation (1) then becomes\n$$l\u00b7 Vol(CI) \u2264 Vol(DI \u2229CI) \u2264 u \u00b7 Vol(CI).$$\nIf one volume is infinite, we regard the condition as violated. We introduce one loss term for the upper and one for the lower bound in (11) as follows:\n$$L_{(D/C)[L,u]} (w) = [l\u00b7 Vol(CI) \u2013 Vol(DI \u2229CI)]_+,$$\n$$L_{(D|C) [l,u]} (w) = [Vol(DI \u2229CI) \u2013 u \u00b7 Vol(CI)]_+,$$\nwhere [x]+ = max{0, x}."}, {"title": "6 SOUNDNESS AND RUNTIME GUARANTEES", "content": "We will now discuss some soundness and runtime guarantees of our approach. As explained in the previous section, we compute the parameter vector w of the embedding by minimizing a loss function of the following form:\n$$L_T(w) = \\sum_{\u03b1\u2208T} L_\u03b1(w),$$"}, {"title": "7 EXPERIMENTS", "content": "Before presenting the experimental results, we will describe our implementation, the procedure for generating test ontologies and our evaluation protocol.\n7.1 Implementation\nWe implemented a first prototype which supports conditionals that belong to one of the following types:\n$$(B|A) [p], (B|A_1 \u2293A_2) [p], (B|\u2203r.A) [p], (\u2203r.B|A) [p],$$\nwhere A, B \u2208 NC are concept names. As their structure corresponds to the four axioms occuring in &L normal form, we refer to them as PNF1 - PNF4 in the following.\n[39] added a location regularizer Lloc (w) to the loss function that is defined as\n$$\\sum_{C\u2208N_C} \\sum_{i=1}^n [M_w(C)_i - \u03b2 + \u03b5]_++ [-m_w(C)_i \u2013 \u03b5]_+,$$\nwhere \u03b2 is a hyperparameter that can be seen as an upper bound on the side length of boxes. Intuitively, Lloc (w) encourages that concepts are embeddeded in the hypercube [0, \u03b2]n. As we show in the ablation study in table 9, just regularizing the volume of boxes can yield better results. We therefore also consider the following volume regularizer:\n$$\\sum_{i=1}^m [\u03b2^n \u2013 Vol(C_i) - \u03b5]_+$$\nEven though Lloc and Lvol have some redundancy, we found that their combination can work better than each term individually. The loss function that we minimize is therefore\n$$L(w) = L_T(w) + L_{loc} (w) + L_{vol}(w).$$\nWe minimize the loss function by gradient descent. One practical problem is that if the intersection of boxes is zero with respect to our current parameter vector (embedding) w, it will still be 0 in a small environment of w. This means that the gradient will be 0, which can prevent gradient descent from bringing disjoint boxes closer together when necessary. This issue can be addressed by approximating the volume with the softplus volume [33]\n$$SVol (Box_w (C)) = \\prod_{i=1}^d Softplus_t (M_w (C)_i \u2013 m_w (C)_i),$$\nwhere the softplus function Softplus, is defined as $$softplus_t (x) = \\frac{t}{\\log (1+ e^{(1+e^{x/t})}}$$\nand t is called the temperature parameter. Because of the exponential growth of the exponential function, we have 1 + ex/t \u2248 ex/t when x/t is large, so that softplus, (x) \u2248 t log (ex/t ~ x. By letting t go to 0 as the search progresses (to let x/t become bigger), the softmax volume will converge to the real volume as the algorithm progresses. Our cooling schedule starts with a high temperature to learn fast at the beginning and decreases the temperature gradually to converge to a proper minimum. We guarantee Mw(C) \u2265 mw(C) for all concepts C\u2208 Nc by changing the parametrization. Instead of using Mw(C), we use a vector dw(C) of the same dimension and let Mw (C) = mw(C) + exp(dw (C)) (where the exponential function is applied element-wise)."}, {"title": "8 DISCUSSION AND FUTURE WORK", "content": "We demonstrated how embeddings can be used to approximate probabilistic inference in the probabilistic DL SEL. To do so, we generalized the &L-embedding BoxEL from [39] to SEL. As shown in Section 6, we can use the embeddings to compute point estimates for SEL queries in linear time. If the conditionals can be perfectly embedded, the inference results are guaranteed to be sound. Naturally, the inference error can increase as the embedding error increases. However, if we can find embeddings with low embedding error, we expect that the inference error remains low and demonstrated this empirically. In order to approximate the true intervals, we do not only compute one but multiple embeddings. We then combine the individual point estimates to an interval estimate. In our experiments, we did not overestimate the true intervals significantly and approximated the true intervals quite well as we increased the number of embeddings used for the approximation. Let us note that, in practice, SEL ontologies can be constructed fully automatically from arbitrary knowledge graphs by applying rule mining methods [16, 29, 30] and using the confidence values (which are usually statistical proportions) of rules for the probabilities.\nOur embedding soundness guarantee (Theorem 1) generalizes the one for BoxEL to SEL. Our inference soundness guarantee (Theorem 2) does not have a counterpart in [39] and sheds some light on approximation guarantees of BoxEL: if all conditionals in an S&L ontology are deterministic, it is equivalent to an EL ontology (Lemma 1) and, by design, our SEL embedding will be equivalent to the BoxEL embedding. Our inference soundness theorem thus guarantees that inference under BoxEL embeddings is sound as well if the loss is 0 and our empirical results suggest that the approximation error is small if the loss is small. That is, an entailed subsumption C\u2286 D should be approximately satisfied in the sense that Boxw (C) is almost contained in Boxw (D), which is potentially interesting for paraconsistent reasoning and repairing knowledge bases [6, 7]. While this question is out of scope here, we conjecture that our embedding loss can be related to minimal violation inconsistency measures [37] that measure to which extent probabilistic interpretations (here, embeddings) violate conditional probabilities and that reasoning with embeddings is guaranteed to be continuous (if the embedding error is small, the inference error must be small) like reasoning with minimally violating models (if the constraint violation is small, the inference error must be small) [38, Proposition 6]."}]}