{"title": "Simmering: Sufficient is better than optimal for training neural networks", "authors": ["Irina Babayan", "Hazhir Aliahmadi", "Greg van Anders"], "abstract": "The broad range of neural network training techniques that invoke optimization but rely on ad hoc modification for validity [1? -4] suggests that optimization-based training is misguided. Shortcomings of optimization-based training are brought to particularly strong relief by the problem of overfitting, where naive optimization produces spurious outcomes. [5-7] The broad success of neural networks for modelling physical processes [8-12] has prompted advances that are based on inverting the direction of investigation and treating neural networks as if they were physical systems in their own right.[13\u201316] These successes raise the question of whether broader, physical perspectives could motivate the construction of improved training algorithms. Here, we introduce simmering, a physics-based method that trains neural networks to generate weights and biases that are merely \"good enough\", but which, paradoxically, outperforms leading optimization-based approaches. Using classification and regression examples we show that simmering corrects neural networks that are overfit by Adam [17], and show that simmering avoids overfitting if deployed from the outset. Our results question optimization as a paradigm for neural network training, and leverage information-geometric arguments to point to the existence of classes of sufficient training algorithms that do not take optimization as their starting point.", "sections": [{"title": "INTRODUCTION", "content": "Neural networks provide powerful, data-driven representations of complex phenomena,[18] however constructing well-trained networks is inevitably constrained by discrepancies between data and underlying, ground truth [19\u201321] and the stochastic nature of reality.[22, 23] The use of discrepant data in optimization-based training generically leads to overfitting,[24, 25] whereby training reproduces peculiarities of a given dataset that can fail to represent the ultimate phenomenon in question. Overfit networks introduce inaccuracies that a variety of ad hoc methods attempt to mitigate. For example, early stopping,[1] bagging,[26] , boosting,[27] and dropout.[28]\nHowever, the proliferation of frameworks for mitigating the overfitting that is produced by optimization-based training raises the question of whether optimization is an ideal starting point for training neural networks.[24] Indeed, one would generically expect that given a sufficiently well-parametrized network, an optimally trained network will precisely reproduce the training data, and will therefore be overfit. The fact that data-driven, neural-network representations of phenomena must be overfit to be optimal suggests that training paradigms that are founded on an alternate premise, e.g., sufficiency rather than optimality, could provide means to train neural networks that better mitigate overfitting.\nHere, we demonstrate simmering, an example of a sufficient-training algorithm, can improve on optimization-based training. Using examples of regression and classfication problems via feedforward neural networks, we deploy simmering to \"retrofit\", or reduce overfitting, in networks that are overfit via conventional implementations of Adam.[17] Our approach leverages Nos\u00e9-Hoover chain thermostats from molecular dynamics [29] to treat network weights and biases as \"particles\" imbued with an auxiliary, finite-temperature dynamics with forces generated by backpropagation,[30] that prevents networks from reaching optimal configurations. We also deploy simmering from the outset to train neural networks without first optimizing and show that, in addition to yielding sufficiently-trained neural networks, simmering also yields quantifiable estimates of ground truth in regression and classification.\nOur retrofitting results indicate that simmering is a viable approach to reduce the overfitting that is inherent in optimal training. To understand why simmering works, we use information geometry arguments [31] to show that simmering is but one of a family of sufficient training algorithms that can use ensemble-based techniques to exploit generic features of training landscapes to improve on optimization-based training. Our implementation of simmering is available open source at Ref. [32]. Within the general class of sufficient learning algorithms, information theoretic arguments indicate that simmering is one of a family of algorithms that makes minimally-biased assumptions about the form of discrepancies between training data and the phenomena they represent, and the present work opens the door to the further exploitation of molecular methods for sufficient training."}, {"title": "SUFFICIENT TRAINING BY SIMMERING", "content": "Existing, optimization-based training algorithms that work to mitigate overfitting are engineered to generate non-optimal networks because optimal sets of weights and biases do not reproduce ground truth in generic problems. The fact that network representations of ground truth are non-optimal suggests the need to systematically explore non-optimal configurations. To explore non-optimal configurations in generic problems where it is not known a priori how training data depart from ground truth suggests generating minimally biased deviations from optimality. Information theory suggests employing a generating function that is the Laplace transform [33] of the loss\n$Z(\u03b2) = \\int d^{N}xe^{-\u03b2L(x)}$"}, {"title": "RETROFITTING OVERFIT NETWORKS", "content": "To facilitate comparison between sufficient- and optimal training methods, we first deploy simmering to reduce overfitting in networks trained using Adam. Fig. 1 gives an example of this \u201cretrofitting\" procedure in the case of a standard curve fitting problem. Fig. 1b shows a set of training and testing data that are generated by adding noise to a sinusoidal signal (green line). We use Adam to fit the data using a fully-connected feedforward neural network. Fig. 1a shows the evolution of the loss, with a clear divergence of the training and test loss during the Adam training stage. Fig. 1b shows that the Adam-generated fit discernibly deviates from the true signal.\nTo correct this deviation, we apply simmering, taking the overfit, Adam-generated network as the starting point. We introduce step-wise increases in temperature (grey line, Fig. 1a) from T = 0 to T = 0.05 (we take T to be measured in units of loss). Simmering generates ensembles of sufficiently trained networks at finite T which we aggregate to construct a \u201cretrofitted\" [prediction] of the underlying signal. Fig. 1c shows that simmering has reduced the discrepancies that were present between the Adam-produced fit and the original signal. Fig. 1d shows that a simmering-generated ensemble of sufficiently trained networks at T = 0.05 generates a fit that is virtually indistinguishable from the original sinusoidal signal.\nWe carried out an analogous retrofitting procedure on a set of similar problems. Fig. 1e shows results for classification problems, where, in all tested cases applying simmering to retrofit overfit networks results in improved classification accuracy on test data. Fig. 1f shows results for the application of simmering to regression problems where simmering reduces the residual of the fit for test data compared with overfit, Adam produced networks."}, {"title": "AB INITIO SUFFICIENT TRAINING", "content": "Fig. 1 demonstrated several applications of simmering to retrofit networks that are susceptible to overfitting by conventional, optimization-based training. These results raise the question of whether optimization-based training is necessary, or whether \u201cab initio\" implementations of sufficient training could avoid overfitting without any need for optimal training.\nFig. 2 shows results from sufficiently trained neural networks in which simmering was deployed from the outset, without the need of optimization. Fig. 2 shows results for regression and Fig. 2 shows results from classification. As shown in the inset plots for both cases, a key advantage of simmering over conventional, e.g., Adam, is that in addition to generating re- gression/classification estimates, simmering also yields statistical estimates on the validity of its estimates. This feature of simmering, which is common to other ensemble learning approaches, is important for mitigating the artificial precision that arises from singular, optimization generated solutions."}, {"title": "GENERALIZED SUFFICIENT TRAINING", "content": "The simmering method we presented above demonstrates that sufficient learning consistently outperforms optimal learning in all of the cases we tested. To understand why, it is useful to situate simmering in the context of the relationship between data, the"}, {"title": "I. METHODS", "content": "Take the weights and biases to be represented by x = (w, b). Imagine also some collective coordinates (\u0398) (where we will drop the vector notation hereafter). Consider now the Pareto-Laplace transform of the loss L(x)\n$Z(\u03b2) = \\int d^{N}xe^{-\u03b2L(x)}$\nNow, we can write Eq. 2 in terms of an integral over the collective coordinates @ by inserting the identity operator, yielding\n$Z(\u03b2) = \\int d\u03b8 \\int d^{N}xe^{-\u03b2L(x)} \u03b4(\u0398(x) \u2013 \u03b8).$\nWe can then write Eq. 3 in terms of only the collective variable,\n$Z(\u03b2) = \\int d\u03b8 e^{-\u03b2F(\u03b8)}$,\nwhere\n$e^{-\u03b2F(\u03b8)} = \\int d^{N}xe^{-\u03b2L(x)} \u03b4(\u0398(x) \u2013 \u03b8).$\nUp to an irrelevant overall constant, the probability of sampling a particular value of @ is\n$p(\u03b8) x e^{-\u03b2F(\u03b8)}.$\nEq. (6) can be computed via the mean-value theorem to give\n$e^{-\u03b2F(\u03b8)} = (e^{-\u03b2L})\u03a9(\u03b8),$\nwhere \u03a9(\u03b8) is the volume of the n-dimensional hypersurface of x along which (x) is constant, and (\u00b7) denotes the mean on that surface (also called an ensemble average). In physics it is conventional write this in terms of entropy, S(\u03b8), where \u03a9(\u03b8) = e\u03b2S(0),\nand to take the representation L(\u03b8) from the ensemble average so that\n$F(\u03b8) = L(\u03b8) \u2013 TS(\u03b8).$\nThe most probable network configurations are those that maximize p(\u03b8), and thereby minimize the free energy F(0). Note that these configurations do not minimize L(x) (nor even L(0)) because at finite temperature an entropic force \u2013T\u2202S (0) drives the network away from minimal loss. The entropic drive away from minimal loss is generic because in generic situations there will be more ways to have non-minimal loss than minimal loss (if this wasn't the case generically, one would have a better than even chance of training a network with minimal loss by simply picking a random set of weights and biases).\nThis entropic force provides the mechanism for sufficient learning that simmering relies on. Simmering exploits entropic forces to generate ensembles of sufficiently-learned network configurations which are then able to outperform optimal learning. Although simmering generates non loss-minimizing distributions automatically, additional work is required if one seeks to identify relevant coordinates 0 for particular networks of interest. One can identify the relevant coordinates for computing F(0) via techniques such as information geometry.[38]\nTo do this, consider the Fisher information metric (FIM), which is defined by\n$g_{\u03bc\u03bd}(\u03b2) = - \\frac{\u2202^{2}(\u03b2F(\u03b8))}{\u2202\u03b8_{\u03bc}\u2202\u03b8_{\u03bd}}$\nwhich indicates that in physics language 9\u00b5\u03bd is just the generalized susceptibility\n$g_{\u03bc\u03bd}^{(F)}(\u03b2) = \\int d\u03b8 e^{-\u03b2F(\u03b8)}\\frac{\u2202}{\u2202\u03b8_{\u03bc}}\\frac{\u2202}{\u2202\u03b8_{\u03bd}}(\u03b2F(\u03b8)).$"}, {"title": "B. Neural network parameters as a system of particles", "content": "The Pareto-Laplace transform of the neural network loss in Eq. (1) yields a generating function for neural network parameters. This generating function has the form of a partition function in statistical mechanics, which opens the possibility to generate networks by adapting molecular simulation methods.\nTo employ molecular dynamics techniques in a neural network problem, we treat the neural network parameters as a system of one-dimensional particles in a potential. The value of each weight and bias defines the position of each particle in the physical system, and the loss function acts as the system potential. The negative gradient of the loss function acts as a force on the system of particles that pushes them towards the minimum of the loss function. If we also define a momentum for each particle, we can integrate Hamilton's equations for the physical system and iteratively generate sets of weights and biases. In the absence of other forces, this integration would yield a set of weights and biases of increasing accuracy on the training data, and as such is analogous to the result of any traditional gradient-descent training algorithm.\nModelling the neural network parameters as a physical system in this way allows us to apply ensemble sampling methods from statistical physics to collect an ensemble of models."}, {"title": "C. Nos\u00e9-Hoover Chain Thermostat", "content": "A thermostat, in a molecular dynamics context, is an algorithm that controls the temperature of a physical system.[29] In this work, we use a Nos\u00e9-Hoover chain (NHC) thermostat, but other thermostats can also be employed to achieve constant- temperature conditions. The use of a thermostat allows us to sample from the canonical ensemble of neural network parameters, and thus produce ensembles of models at different temperature scales. The NHC thermostat samples from the canonical ensemble by introducing an interaction between the neural network parameters and a chain of massive virtual particles. [29] The first particle in the chain can exchange energy with the system of neural network parameters, and the rest of the chain can interact with only their neighbouring chain particles. Each virtual particle has a position and and momentum that are computed iteratively along with those of the system of real particles.\nGiven a set of N neural network parameters, we define a set of positions {$X_{1}, X_{2}, ..., X_{N}$}, associated momenta {$P_{1}, P_{2}, ...,P_{N}$} and masses {$m_{1}, m_{2}, ...,M_{N}$}. Using this set of quantities, we can model a system of one-dimensional particles that is in a potential defined by a loss function L($x_{1}, x_{2}, ..., X_{N}$) that depends only on the neural network parameter positions (weights and biases) {x}. This physical system is also interacting with an NHC of length Nc, where each constituent particle also its own mass Qk, position sk and momentum pk. Henceforth, the neural network parameters will be referred to as the \"real particles,\" to contrast with the virtual particles of the NHC.\nThe Hamiltonian of this coupled system is [39, 40]\n$H = H_{system} + H_{NHC}$\n$ = \\sum_{i=1}^{N} \\frac{1}{2}m_{i} + L(X_{1}, ..., X_{N}) + \\sum_{k=1}^{N_{c}}\\frac{P_{k}^{2}}{2Q_{k}} + (N - I_{e} + I_{ine})T_{target} S_{1} + \\sum_{k=2}^{N_{c}}T_{target} S_{k},$\nwhere Ttarget is the target temperature of the coupled system, and Ie and Iine are the equality and inequality constraints respec- tively.\nFor simplicity of notation, we will henceforth set mi = 1, Qk = 1 \u2200 i, k and describe the integration process in terms of the positions and velocities, rather than the positions and momenta. The equations of motion are derived from the Hamiltonian, and are given by\n$X_{i}(t) = v_{i}(t)$\n$v_{i}(t) = a_{i}(t) - \\upsilon_{s_{1}}(t)v_{i}(t)$\n$\\upsilon_{s_{1}} (t) = a_{s_{1}} (t) \u2013 \\upsilon_{s_{2}}(t)\\upsilon_{s_{1}} (t)$\n$\\upsilon_{s_{k}} (t) = a_{s_{k}} (t) - \\upsilon_{s_{k+1}}(t)\\upsilon_{s_{k}}(t),$\nwhere the i subscript denotes real particle quantities, and the sk subscript denotes a quantity affiliated with the kth virtual particle. The accelerations of the particles are given by\n$a_{i}(t) = \\frac{1}{m_{i}} \\bigtriangledown L(x_{i},..., X_{N})$\n$a_{s_{1}} (t) = \\frac{1}{Q_{1}}\\sum m_{i}v^{2} - (N \u2013 N_{e} \u2013 N_{ine})T_{target})$\n$a_{s_{k}}(t) = \\frac{1}{Q_{k}}((Q_{k-1}v^{2} - T_{target}).$\nThe acceleration of the real particles in Eq. 19 is proportional to the negative gradient of the loss with respect to the weights and biases in the neural network. Given Equations 15\u201321, the trajectories of the real and virtual particles, and thus the sequence of neural network weights and biases, can be determined numerically."}, {"title": "D. Numerical implementation", "content": "Verlet integration is used to numerically integrate Eqs. 15-21 because it preserves Hamilton's equations [29]. To discretize the equations of motion, we define a Verlet integrator for position and velocity for both the real and virtual particles.\nThe real and virtual particles' positions and velocities collectively form the phase space of this physical system. The phase space variables, henceforth referred to generally as \u0393, can be split into position-like variables (\u0393x) and velocity-like variables (\u0393\u03bd)[35],\n$\u0393_{X} = {x_{i}, s_{2k}, \\upsilon_{s_{2k-1}}}, \u0393_{V} = {v_{i}, s_{2k-1}, \\upsilon_{s_{2k}}.$\nThe motivating principle behind this categorization is the commutativity between the Liouville operators of the phase space variables. The Liouville operators for a conjugate pair of position and velocity do not commute, and the Liouville operators for thermostat velocities in adjacent levels of the Nos\u00e9-Hoover chain do not commute."}, {"title": "E. Model System Architecture", "content": "These examples demonstrate the performance of simmering on a variety of neural network problems. Different architectures, activation functions, loss functions and initializations are used to highlight the breadth of contexts in which sufficient training is useful.\nWe implemented simmering using the TensorFlow library [42] in Python. For each test case, the built-in training step in TensorFlow is replaced with the Verlet integration scheme described in Equations 31-39 to iteratively produce sets of weights and biases. The acceleration a\u00bf(t + \u2206t/2) in Equation 34 for each iteration is supplied by the automatic differentiation of the loss with respect to the weights and biases in TensorFlow. In all cases, for simplicity, full-batch gradient descent is used, so the batch size is taken to be the size of the entire dataset.\nWe have published an open-source version of the simmering code, which uses the signac data management framework [43, 44] to store and organize simulation data for different sets of thermostat and training parameters is available at Ref. [32]. The published code allows for retrofitting and ab initio sufficient training to be conducted on the noisy sine dataset shown in Fig. 1."}, {"title": "1. Retrofitting Overfit Networks", "content": "To retrofit a neural network, we use the final set of (overfit) weights produced by a traditional optimization algorithm and a first order approximation of the parameters' final velocities as the retrofitted neural network's initial conditions. The final parameter velocities can be computed using the last and second-last iterations' weights,\n$V_{i,t} \\approx \\frac{x_{i,t} x_{i,t-1}}{\u03b3},$\nwhere \u03b3 is the learning rate. We use both the final weights and velocities to ensure that the neural network is initialized in the exact location in neural network parameter phase space where the optimizer stopped. We then define a temperature schedule for the thermostat, starting at T = 0 as the overfit network was not coupled to a thermostat. The published code has the option to implement a temperature schedule that increases in a step-wise manner at equal intervals from T = 0 to the user's target temperature choice. Fig. 1a shows an example of this temperature schedule, and this type of step-wise temperature change was used for all examples shown in panels le and 1f. Once the target temperature is reached, we sample from the resulting finite-temperature ensemble to generate ensemble predictions. For the classification examples shown in le, majority voting was used to aggregate the classifier predictions. For the regression examples in 1f, the ensemble samples were averaged to obtain the ensemble prediction.\nIn each example shown in this work, the overfit networks were produced by training with the Adam optimizer, with its default parameters in TensorFlow, and a constant learning rate of y = 0.002.\nThree different datasets were used to produce the classification results in Figure 1e: the MNIST handwritten digits dataset [45], the Higgs dataset [46], and the Iris dataset [47].\nThe network architecture for learning the MNIST dataset was a LeNet-5 convolutional neural network [48], modified to have ReLU activations rather than sigmoid activations for all layers. Categorical cross-entropy was used as the loss function. 10,000 images were used for training, and 1000 for testing, both of which were selected randomly without overlap from the dataset. The network weights were initialized using the Glorot normal initializer in TensorFlow, and the network was trained using the Adam optimizer for 300 epochs. During simmering, the thermostat temperature was increased from Tinitial = 0 to Ttarget = 0.001 in steps of AT = 0.001 every 200 iterations, with a learning rate of y = \u2206t = 0.002. Simmering was conducted for 6,000 iterations, and the last 4,000 iterations' neural network parameters were used for ensemble predictions.\nFor the Higgs dataset example, the network architecture consisted of 4 512-unit hidden layers, and a linear output layer. The hidden layers were given an exponential linear unit (ELU) activation. The loss, which was chosen to be binary cross-entropy, was computed from logits to account for the linear output. 10,000 samples were used for training, and 1000 samples were used for testing, selected randomly with no overlap from the Higgs dataset. The network weights were initialized using the TensorFlow Glorot uniform initializer. The network was trained using the Adam optimizer for 1000 epochs. During simmering, the thermostat temperature was increased from Tinitial = 0 to Ttarget = 0.0001 in steps of \u2206T = 0.0001 every 500 iterations, with a learning rate of y = \u2206t = 0.002. Simmering was carried out for 15,000 iterations, and the models resulting from the last 12,000 iterations of training were used for producing ensemble predictions.\nFor the Iris dataset example, the network architecture consisted of 3 hyperbolic tangent-activated hidden layers, and a linear output layer. The first hidden layer had 100 units,and the subsequent two hidden layers had 50 units. The categorical cross- entropy was used as the network loss. The Iris dataset consists of four input features, but we used only the \"sepal width\" and the \"petal width\u201d features to classify the flowers. 112 samples were used for training, and 38 were used for testing, partitioned randomly without overlap. The input features were linearly rescaled based on the training data features to map to the range [-1,1]. The network weights were initialized using the TensorFlow Glorot normal initializer, and trained using the Adam optimizer for 200 epochs with no batching. During simmering, the thermostat temperature was increased from Tinitial = 0 to"}, {"title": "2. Ab Initio Sufficient Training", "content": "For ab initio sufficient training, simmering was employed at the outset and a constant temperature was maintained for the entire duration of the training process.\nTo generate the classification example, the Iris dataset was used. The choice of dataset partitioning, network architecture and loss function were the same as in the retrofitting example. The weights were initialized using a Glorot normal initialization, and simmering was employed from the outset at a thermostat temperature of T = 0.002 for 25,000 iterations with a learning rate of y = \u2206t = 0.001. Over the last 10,000 iterations, 2,000 model samples are randomly selected for contribution to the ensemble prediction. Simmering was conducted on models with 36 different random seeds (resulting in distinct instances of the Glorot normal initializations) while keeping the data train/test partition fixed, and the ensemble majority prediction was computed based on the total of votes across from all 36 replications' sampled models. In Figure 2a, the result of the ensemble majority vote decision boundary is shown. The background colour in Figure 2a is a weighted average of the three class colours, and reflects what proportion of the ensemble of models voted for which Iris species.\nFor the regression example, the Auto-MPG dataset was used. As in the single variable retrofitting example, the \u201chorsepower\" feature was used to predict the target (miles per gallon). The dataset was partitioned such that there were 300 training samples and 92 test samples, categorized randomly with no overlap. In this case, the network architecture consisted of 1 10-unit hyperbolic tangent-activated hidden layer, and a linear output. MSE was used as the loss function. The weights were initialized using a modified Glorot normal initialization. The overall distribution of weights for each layer has the same mean and width as the corresponding Glorot normal distribution, but the range [-20, 20] is split into n equal segments (where n is the number of input nodes to that layer), and each weight's value is generated from a normal distribution centred on the midpoint of one of the segments. Simmering was employed from the outset at a thermostat temperature of T = 1 for 40,000 iterations with a learning"}]}