{"title": "Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation", "authors": ["Junhong Lian", "Xiang Ao", "Xinyu Liu", "Yang Liu", "Qing He"], "abstract": "Personalized news headline generation aims to provide users with attention-grabbing headlines that are tailored to their preferences. Prevailing methods focus on user-oriented content preferences, but most of them overlook the fact that diverse stylistic preferences are integral to users' panoramic interests, leading to suboptimal per-sonalization. In view of this, we propose a novel Stylistic-Content Aware Personalized Headline Generation (SCAPE) framework. SCAPE extracts both content and stylistic features from headlines with the aid of large language model (LLM) collaboration. It further adaptively integrates users' long- and short-term interests through a contrastive learning-based hierarchical fusion network. By incorporating the panoramic interests into the headline generator, SCAPE reflects users' stylistic-content preferences during the generation process. Extensive experiments on the real-world dataset PENS demonstrate the superiority of SCAPE over baselines.", "sections": [{"title": "1 Introduction", "content": "Generating attractive news headlines has been firmly established as a special form of text summarization [7, 12]. Numerous efforts have focused on generating attention-grabbing headlines through personalized approaches, recognizing that readers with diverse preferences may find different focal points in the same news [1, 2]. These methods aimed to build engaging headlines tailored to individual users' reading interests by utilizing auxiliary information, such as user profiles and historical clicks [15, 16, 19, 21]. In spite of that, existing personalized approaches primarily emphasize user-oriented content-driven headlines, while largely overlooking the stylistic features of news headlines.\n\nThe stylistic features of news headlines are pivotal in journalism communication [3, 5]. Boosting headline engagement through text style transfer has become a widely-used approach [9, 20]. Style transfer-based methods incorporate specific stylistic elements or employ interrogative forms in news headlines to capture readers' attention [9, 14]. Nevertheless, these style transfer-based methods rely on a single, uniform stylistic strategy to boost engagement risks crossing into clickbait [1, 19]. To improve content consistency, recent works [10, 20] have emphasized the style-content duality in headline generation, focusing on transferring to prototype headline styles while maintaining content decoupling. Despite these efforts, style transfer-based methods are unsatisfactory because they have yet to achieve user-oriented personalization.\n\nIn this work, we focus on encompassing both content interests and stylistic preferences to fully meet users' needs for personalized headline generation. The reason is that even readers with similar content preferences may exhibit distinct style preferences for head-lines due to different personal reading habits. For example, consider the case illustrated in Figure 1, wherein users' content focus and stylistic preferences jointly affect the personalization of headlines."}, {"title": "2 Methodology", "content": null}, {"title": "2.1 Problem Formulation", "content": "Consider a news database denoted as $\\mathcal{D}$ = {ni = (ti, bi)}, where ti and bi represent the original headline and the body of news ni, and |D| is the total number of news. For a given user u, we denoted u's click history as $\\mathcal{H}_{u}$ = [th\u2081, th2, \u2026, th\u2081], comprising L clicked headlines th; (j = 1,\uff65\uff65\uff65, L), where each th; satisfies th; \u2208 {ti | ni = (ti, bi) \u2208 D}. Then, given a candidate news nc = (tc, bc) \u2208 D, our target is to generate a personalized headline $P_{ou}$ for user u based on u's click history $\\mathcal{H}_{u}$ and the body $b_c$ of the candidate news."}, {"title": "2.2 Our SCAPE Framework", "content": "In this section, we describe our proposed SCAPE framework, which is illustrated in Figure 2. It consists of a headline inference module, a hierarchical gated fusion network, and a personalized injection module for headline generator."}, {"title": "2.2.1 Headline Inference", "content": "Extracting the inherent features from headlines without ground-truth labels is challenging, but LLMs with extensive world knowledge offer a promising solution for inferring high-level latent concepts. Building on this, the headline inference module in SCAPE employs an instruction-tuned LLM, denoted as LLMinst, to infer text style and content interests from headlines ti in news database D as follows:\n\n$R_s = LLM_{inst} (t_i, P_{style})$  $R_c = LLM_{inst} (t_i, P_{content})$\n\nwhere Pstyle and Pcontent are the prompts designed to instruct the LLM to infer stylistic features and content interests, respectively, with Rs and Rc as corresponding responses.\n\nInspired by instruction-following text embedding [13], we de-sign task-specific instructions Is and Ic for embedding style and interest. These instructions are concatenated with news headlines and their corresponding responses, which are then encoded by an embedding-based LLM, denoted as LLMemb. The offline stored headline embedding table E is constructed as follows:\n\n$E_s = LLM_{emb} ([t_i, I_s, R_s])$  $E_c = LLM_{emb} ([t_i, I_c, R_c])$\n\n$E = {(E_s (t_i), E_c (t_i)) | n_i = (t_i, b_i) \u2208 D}$\n\nwhere Es(ti) and Ec(ti) represent the style and content embed-dings of headline ti, respectively, and E is the set of style-content embedding pairs for all headlines in D."}, {"title": "2.2.2 Hierarchical Stylistic-Content Awareness Fusion", "content": "Given that historical clicks reflect both stable personal traits and recent short-term characteristics, we employ an attention mechanism to aggre-gate all clicks and a GRU network for the K recent clicks. This yields the user's long-term content interests representation $u_l$ and short-term content interests representation $u_s$ as follows:\n\n$\\bar{E_c}(t_{h_j}) = MLP(E_c(t_{h_j})) \\forall j \\in {1,...,L}$ \n\n$u_l = Attn(\\bar{E_c}(t_{h_1}),..., \\bar{E_c}(t_{h_L}))$\n\n$u_s = GRU([\bar{E_c}(t_{h_{L-K+1}}),..., \\bar{E_c}(t_{h_L})])$ \n\nwhere $\\bar{E_c}(t_{h_j})$ is the projected content embedding for the j-th his-torical click with L > K. Similarly, we obtain the user's long-term and short-term stylistic preferences, denoted as $u^s_l$ and $u^s_s$."}, {"title": "2.2.3 Personalized Injection Module Guided Generator", "content": "Once the user representation U is obtained, SCAPE integrates it into a decoder of the lightweight headline generator through the person-alized injection module. Specifically, the user representation U is added to the input embeddings X of each token in the decoder. This user-specific vector is subsequently propagated through the resid-ual flow, thereby influencing the generated headlines. By integrat-ing user preferences at the token level, the headline generator can better align its output with the user's interests, producing stylistic-content-aware personalized headlines that effectively reflect their panoramic interests, formulated as follows:\n\n$X' = X + (1_n \\otimes U)$\n\nwhere X \u2208 Rn\u00d7d, U \u2208 Rd, n is the sequence length, d is the embed-ding dimension, and 1n \u2208 Rn is an all-ones vector."}, {"title": "2.3 Disentanglement Strategy", "content": "To prevent the $u^c_l$ and $u^s_s$ in Section 2.2.2 from collapsing into trivial forms, we adopt a mechanism inspired by [22] to promote the dis-entanglement of long- and short-term (LS-term) style and content"}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Experimental Setup", "content": null}, {"title": "3.1.1 Datasets and Baselines", "content": "We use the publicly available person-alized news headline generation dataset PENS [2] as benchmark. The dataset comprises 500, 000 anonymized impressions from over 445,000 users and 113, 762 news articles, capturing detailed his-torical click data to reflect nuanced personalized preferences. The test set includes 3, 940 news items annotated by 103 users, each providing 200 unique parallel headlines as the gold standard for personalized headline evaluation.\n\nWe compare SCAPE with SOTA personalized headline genera-tion methods on the PENS benchmark, including pointer-network-based frameworks [1, 2, 21] and methods based on pre-trained language models [15, 19]. We also expanded our evaluation by em-ploying a prompt-based method to comprehensively investigate the performance of LLMs against strong personalized baselines, evaluating both open-source LLMs [4, 18] of various sizes and closed-source LLMs [6, 8, 11] via API services."}, {"title": "3.1.2 Evaluation Metrics", "content": "We use ROUGE metrics to measure lexi-cal similarity, with ROUGE-1 and ROUGE-2 for informativeness, and ROUGE-L for fluency. To evaluate the factual consistency of generated headlines, we follow previous work [19] that reported Fact Scores. As for personalization, due to the lack of a widely accepted evaluation method, we design a pairwise comparison task [17], where strong LLMs evaluate candidates against original headlines based on the user's historical clicks. To minimize bias, we swap the contextual order of candidates and perform two indepen-dent assessments, marking inconsistencies as ties. Personalization performance is reported as \"win/tie/lose\" outcomes between the candidates and original headlines across baseline methods."}, {"title": "3.1.3 Implementation Details", "content": "We use FlanT5-base as the backbone for the headline generator. The model is pre-trained on general headline generation, as [15, 19], for 2 epochs with a peak learning rate (LR) of le-4 and cosine decay. Early stopping is applied within 5 epochs for subsequent steps, with peak LRs set to 1e \u2013 3, 1e 6, and 1e-5, respectively. Other details are consistent with"}, {"title": "3.2 Results and Analysis", "content": "Table 1 compares SCAPE with baseline methods, demonstrating its superior performance and setting a new benchmark for person-alized headline generation in SOTA results. SCAPE significantly outperforms other methods in informativeness and fluency. This result highlights that SCAPE better improves content coverage through its stylistic-content-aware user modeling via LLM collabo-ration. Furthermore, SCAPE emphasizes the importance of balanc-ing long- and short-term content interests and stylistic preferences to improve the factual consistency of personalized headlines.\n\nFigure 3 shows the win rates of personalization evaluation using LLMs with customized prompts as judges. While GPT-40 shows higher win rates than the personalized model FPG, LLMs still strug-gle to generate personalized headlines based on prompt engineering. This underscores the challenge posed by the complexity and di-versity of user's clicks preferences. Our SCAPE framework takes a crucial step forward in personalized headlines generation by consid-ering both user-oriented content interests and modeling linguistic style preferences in users' historical clicks."}, {"title": "4 Conclusion", "content": "In this paper, we propose SCAPE, a novel framework to tackle the challenges of insufficiently user preference modeling in per-sonalized headline generation. SCAPE introduces a headline in-ference module that extracts the inherent stylistic features and content interests from news headlines without explicit supervi-sion. A hierarchical gated fusion mechanism is further introduced to dynamically combine both long- and short-term interests for panoramic user modeling, which then guides the headline generator to produce stylistic-content aware personalized headlines. Exten-sive experiments on the PENS dataset show that SCAPE sets a new state-of-the-art benchmark for personalized headline generation."}]}