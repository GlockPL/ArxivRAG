{"title": "Modulating Language Model Experiences through Frictions", "authors": ["Katherine M. Collins", "Valerie Chen", "Ilia Sucholutsky", "Hannah Rose Kirk", "Malak Sadek", "Holli Sargeant", "Ameet Talwalkar", "Adrian Weller", "Umang Bhatt"], "abstract": "Language models are transforming the ways that their users engage with the world. Despite impressive capabilities, over-consumption of language model outputs risks propagating unchecked errors in the short-term and damaging human capabilities for critical thinking in the long-term, particularly in knowledge-based tasks. How can we develop scaffolding around language models to curate more appropriate use? We propose selective frictions for language model experiences, inspired by behavioral science interventions, to dampen misuse. Frictions involve small modifications to a user's experience, e.g., the addition of a button impeding model access and reminding a user of their expertise relative to the model. Through a user study with real humans, we observe shifts in user behavior from the imposition of a friction over LLMs in the context of a multi-topic question-answering task as a representative task that people may use LLMs for, e.g., in education and information retrieval. We find that frictions modulate over-reliance by driving down users' click rates while minimally affecting accuracy for those topics. Yet, frictions may have unintended effects. We find marked differences in users' click behaviors even on topics where frictions were not provisioned. Our contributions motivate further study of human-AI behavioral interaction to inform more effective and appropriate LLM use.", "sections": [{"title": "1 Introduction", "content": "There is a colloquial adage that \"just because you can, does not mean you should.\" Large language models (LLMs) have seen unprecedented rates of use: OpenAI's ChatGPT had 100 million users within the first two months of release (Raman et al., 2023). However, characterizing regimes of appropriate use is non-trivial: LLMs are general-purpose technologies with a plethora of use cases. LLMs may not be appropriate to deploy in all contexts as we have seen LLMs perform poorly at mathematics and arithmetic (Frieder et al., 2023; Collins et al., 2024), avoiding biased and hateful statements (Guo et al., 2023), and debugging code (Sobania et al., 2023). To better modulate when LLMs are used, we study the selective use of LLMs, thereby limiting access to their responses for specific queries, for specific users.\nMechanisms like reinforcement learning with human feedback (Ouyang et al., 2022) and direct preference optimization (Rafailov et al., 2024) take steps to steer LLM responses away from illegal, undesired, and toxic content. However, there are reasons beyond safety why it could be desirable to curb LLM use. In some contexts, discouraging the use of LLMs for particular users can yield economic or personal benefits. For example, encouraging students to solve problems on their own instead of accessing answers provided by LLMs may encourage a deeper understanding of and engagement with educational material (Zhu and Simon, 1987; Schank et al., 2013). To prevent over-reliance on LLMs, dubbed \u201calgorithm appreciation\u201d (Logg et al., 2019), we advocate for thoughtful interactions with LLMs where users are vigilant about when they use these tools (Zerilli et al., 2022).\nTo promote disuse, a spartan option is to restrict access to the LLM response entirely. For example, in deferral schemes, models abstain from providing predictions on specific task instances (Madras et al., 2018; Mozannar and Sontag, 2020). As opposed to strict disuse wherein the LLM response is hidden, which could impair user freedoms, we consider selectively adding friction to an individual's experience with an LLM. Adapting a definition from Etzioni (2016), we define a friction in the context of LLM assistance as:\nA deliberate design element for increasing the time, effort, or cognitive load of accessing an Al-generated output by prompting conscious consideration of the task at hand.\nAlso referred to as a nudge (Wilk, 1999), microboundary (Cox et al., 2016), or cognitive forcing (Croskerry, 2003), frictions assuage algorithm appreciation and promote the vigilant use of LLMs: users are encouraged to think twice before relying on an LLM. Similar to selecting when to abstain, introducing selective frictions can depend on model behavior, human expertise, or sociotechnical factors (Bhatt and Sargeant, 2024). For instance, a friction could be selectively applied for a user if they are relatively stronger than the LLM at some topic like mathematics.\nWe contribute a case study of the imposition of selective frictions, focusing on selectivity with respect to user expertise. Specifically, we consider a question-answer setting, reminiscent of \"information-seeking\", knowledge retrieval tasks that users may engage in with AI-based search summaries, e.g., Perplexity or Google Summaries. We extend the user interface, Modiste, from Bhatt et al. (2023) to explore the imposition of an extra-click selectively (on only some topics, for some users) before the user can receive LLM assistance. We explore the impact of friction on users' click rates and attainable accuracy on multiple choice question from the popular and challenging NLP benchmark, MMLU (Hendrycks et al., 2021) and other auxiliary measures, such as a user's confidence in their performance and that of the LLM. We observe marked behavior in users' click rates from the introduction of friction, providing initial evidence that frictioning LLM access can serve as one effective \u201clever\u201d to design to modulate user experiences and help titrate overreliance. Yet, our"}, {"title": "2 A Case Study in Selective Frictions", "content": "We begin to explore the design and deployment of selective frictions for LLM experiences. Specifically, we consider assisted question-answering."}, {"title": "2.1 Task", "content": "Prior studies including Mozannar et al. (2023); Bhatt et al. (2023) have explored LLM assistance in answering multiple-choice questions from MMLU (Hendrycks et al., 2021); in particular, we build on the set-up and Modiste interface from Bhatt et al. (2023), which supports rapid prototyping of user studies under various forms of assistance. Participants answer a total of 60 multiple-choice questions sampled from four topics of MMLU: US foreign policy, elementary mathematics, high school computer science, and high school biology.\nIn the baseline condition, participants can press a button to \"query\" the LLM and receive assistance on the current multiple-choice question (as shown in Figure 5), which then highlights one of the four multiple-choice options (as shown in Figure 6). As in many real-world settings, we selected a pool of questions where the model is not always correct, as it may not be in many real-world settings, as discussed in Appendix C.3."}, {"title": "2.2 Instantiation of Selective Friction", "content": "We propose a selective friction on top of this button by presenting the user with a second button requiring them to click again, indicating that they are certain that they want to see the model prediction. While the first button can be considered a friction in and of itself (Bu\u00e7inca et al., 2021); rather, we treat it as a baseline to compare selective frictions.\nWhen to impose a friction? There are a variety of reasons for which a friction could benefit user subgroups, depending on the context. In our case study, we study one characteristic: user expertise. If one is already good at computer science, one may not benefit from access to the LLM prediction, particularly if the model has low accuracy."}, {"title": "2.3 Participants", "content": "We recruit 100 participants from Prolific (Palan and Schitter, 2018) in an institutionally-ethics reviewed study; participants are recruited from the US and required to speak English as a first language. Participants are randomly assigned to either the selective-friction or baseline condition (N = 47 and 53, respectively). In the friction condition, based on Equation 1, 42 of the 53 participants received friction for foreign policy, 49 for mathematics, 12 for computer science, and none for biology. We include more details in Appendix C."}, {"title": "2.4 Metrics", "content": "We focus on three metrics: (i) user accuracy over the questions for a given topic, (ii) click rate for questions in a given topic, which is the proportion of times that the user clicks to see the LLM prediction for M questions within a topic\u00b9, and at the end of the study, (iii) the users' self-reported belief in their performance, as well as the LLM's performance, on each topic."}, {"title": "3 Results", "content": "Key Finding 1: Selective frictions can reduce click rates while maintaining accuracy. We analyze participant accuracy and click rates and conduct Ordinary Least Squares Regressions with Benjamini-Hochberg correction, using a significance level of 0.05. We find that frictioning user experiences, in the way we have done here, indeed significantly lowers user click-through rates as shown in Figure 3 (p < 0.05). These results are encouraging, demonstrating friction may be one way to encourage users to solve problems independently. This finding is buttressed by minimal, not significant change in the users' accuracy, furthering the benefits of friction to users' critical thinking.\nKey Finding 2: Frictions can induce unintentional spillover effects. However, to our surprise, we see that click rates drop for participants in the friction condition for biology even though no participants were frictioned specifically for biology. This observation is important; frictioning users' experience in one region of the task space may influence users' decisions in other regions. We speculate why this may be happening, and encourage future work to empirically investigate this"}, {"title": "4 Discussion", "content": "As users increasingly access powerful AI systems, buttressed by lightweight natural language interfaces, questions around how system designers can encourage and safeguard appropriate use grows more urgent. Our study demonstrates that small changes to user interfaces in the form of frictions can modulate user behavior, while preserving general user freedoms. We show that appropriately designed frictions can reduce user engagement and instances of over-reliance with minimal change in user accuracy. It is thus possible that frictions can serve as a critical tool in promoting the responsible use of LLMs. By incorporating barriers that encourage users to engage more critically with AI-generated content, policymakers can help ensure that LLMs are used thoughtfully and selectively, thus preserving and fostering users' impartiality and autonomy (Barletta et al., 2023; Dignum, 2017; Sunstein, 2016).\nHere, we focused on adding hurdles along a user's path to engaging an LLM. Much work in the behavioral sciences has studied positive interventions to encourage particular kinds of behavior by \"nudging\" (Thaler and Sunstein, 2009). Next steps can explore nudges in our MMLU and other settings, as well as alternate mechanisms for instantiating selective frictions drawing on computational models of human behavior (Callaway et al., 2023). Further, while our selective frictioning design permits tailored user experiences, e.g., by expertise as we have shown, personalization of LLM experiences can come with risks (Kirk et al., 2024).\nWhile we find that our frictions can dampen excess engagement with an LLM when a user has the appropriate expertise, we find that targeted frictions can have \"spillover effects\" wherein users' behavior changes even on topics where frictions were not added nor intended to be added. Our preliminary observations urge caution for the designers of interventions around AI systems \u2013 human behavior is complex, and small changes in the realm of interaction may ripple into another. We are excited by future research at the intersection of human and AI systems interlaced with language interfaces."}, {"title": "Limitations", "content": "Our case study focuses on a single type of friction surrounding selective reminders with respect to user and model expertise; future work is needed to explore whether there are more effective frictions in terms of click rate modulation that may reduce spillover effects. Additionally, from our current study design, we cannot observe whether the user really needed and/or benefited from the LLM prediction. For example, some users clicked simply out of curiosity or to double-check their answers as noted in some post-survey responses (see Appendix D.1), which likely reduced the observed effect. As we only obtain the user's final prediction, future work might consider an alternative study design that asks the user for their answer before they see the LLM prediction, which may change the user's decision-making process.\nAnother limitation of this work is our focus on a single dataset, MMLU. Since some of the tasks in question are quite challenging. We do not see any participants achieve high enough biology performance to be frictioned; many people may feel they need support from the LLM regardless. It is possible, as well, that our quiz does not obtain an adequate appraisal of participant expertise. We only evaluate participants on 5 questions per topic; as such, the expertise profile procured is necessarily an estimate. Future work is needed to explore alternative expertise elicitation schemes, and to understand whether click rate modulation and possible spillover effects in LLM experiences generalize to other settings and user populations."}, {"title": "A Ethics Statement and Potential Risks", "content": "As in the use of nudges from behavioral economics, there are critical conversations that warrant conversation around risks of selective frictions. While we take the stance as Thaler and Sunstein (2009) that the choice to not adjust any access to users' experiences is still a choice, there are important questions around who is deciding when to impose frictions and on which user populations. While selective frictions could be one way to encourage critical thinking, as we begin to demonstrate here, without responsible design, they could negatively shape users' choice environments (Sunstein, 2016). More pressingly, selective use of frictions may lead to disparate treatment of users, as some sub-populations of users may be frictioned on specific task instances more than others (Jones et al., 2020). For instance, user expertise may be distributed unequally across a protected attribute; our friction framework would then disparately friction users increasing the effort required for some users to access the LLM output (Han, 2020; Von K\u00fcgelgen et al., 2022). Systems designers ought to be aware of such disparities and take the necessary precautions when deploying frictioned access to LLMs."}, {"title": "B Related Work", "content": "In this work, we focus on LLM-assisted user interactions and decision-making (Green and Chen, 2019; Lai et al., 2021). Prior studies studying these contexts have shown the tendency for humans to overrely on AI support (Joshi et al., 2023; Chen et al., 2023; Vasconcelos et al., 2023). As such, recent works have considered adapting when AI support is provided to users: Ma et al. (2023) fit a decision tree to offline users' decisions to decide when to show AI support to users, Bu\u00e7inca et al. (2024) use offline reinforcement learning to estimate if AI support would be helpful, and Bhatt et al. (2023) employ online learning techniques to personalize a decision support policy to individuals. Relatedly, others have considered selectively delegating entire tasks to the AI model (Madras et al., 2018; Mozannar and Sontag, 2020; Lai et al., 2022; Wiener and El-Yaniv, 2011; Geifman and El-Yaniv, 2017). In many cases, it may not be possible (or desirable even if possible) to have an LLM make the final decision, and in others, it may not be justifiable to withhold user access to LLMs These constraints motivate our study of frictions, which permit continued model access but require more effort on the user's end to procure access.\nOur work builds on the wealth of prior research into the design and effect of nudges on human behavior (Thaler and Sunstein, 2009; Schmidt and Engelen, 2020; Hummel and Maedche, 2019). The notion of nudges is increasingly permeating machine learning, whether in the use of techniques from machine learning to design nudges (Callaway et al., 2023) or nudging to support more appropriate use of AI systems (Li et al., 2024; Bu\u00e7inca et al., 2021). Relatedly, \"microboundaries\" (Cox et al., 2016), are small, intentional barriers integrated into user interfaces to promote more mindful interactions. Microboundaries can reduce the likelihood of users making errors or engaging in habitual, potentially harmful behaviors by interrupting their flow and requiring them to take an additional step before proceeding. There are several potential advantages of microboundaries (a la warning signals) as a type of design friction (Sundin, 2021; Mejtoft et al., 2019; Alon-Barkat and Busuioc, 2023). To our knowledge, we are the first work to explore selective frictioning of LLM use."}, {"title": "C Additional Details on Human Experiments", "content": "We provide additional details on our user study. Participants receive the quiz for all conditions. The same questions are presented across both conditions, selected from three batches of 60 questions, as in (Bhatt et al., 2023). The \"test\" phase involves 10 questions per topic. Participants are provided feedback as to whether they (and the model, if seen) are correct after each test trial. Feedback is not given in the quiz phase. Participants are paid at a base rate of $9 per hour for an expected 30 minute experiment with an optional bonus up to $10 per hour for correct answers; we apply the bonus to all participants. All data is anonymized, and participants provided informed consent before beginning the study."}, {"title": "C.2 Eliciting Perceived Self- and Model-Ability", "content": "At the end of the study, users are presented with a questionarre asking them to judge their own and the model's ability per topic. Specifically, we asked, for each topic: \"Out of 100 questions on TOPIC, how many do you think the Al would get correct?\" and \"Out of 100 questions on TOPIC, how many do you think you could get correct (without the help of the AI-based model)?\" For each question, users responded on an slider ranging from 0 to 100."}, {"title": "C.3 LLM Predictions", "content": "We use the same model predictions as in (Bhatt et al., 2023), which were sampled from InstructGPT3.5 text-davinci-003 (Ouyang et al., 2022); however, we randomly dampen model performance for the foreign policy and computer science topics such that the models achieve 30% and 60% performance on each, respectively. The model achieves approximately 30% and 90% performance on mathematics and biology. MMLU is challenging for humans (Hendrycks et al., 2021; Bhatt et al., 2023; Mozannar et al., 2024); our selective friction is triggered by user performance relative to the model's average performance on a topic - if the latter is too high, the friction will not be triggered. Accordingly, we expect with the dampening that we should have high trigger rates for mathematics and foreign policy, moderate rates for computer science, and low rates for biology \u2013 enabling us to study user click behavior across a range of model performances and settings wherein in some cases it is indeed rationale to rely on the model whereas in others, it may be disastrous for a user to regularly rely on the model (e.g., in elementary mathematics)."}, {"title": "C.4 User Study Interface", "content": "We include example screenshots of the button interface and friction in Figures 5 and 2, respectively. When the user does click through to the LLM prediction, it is displayed as in 6, following Bhatt et al. (2023)."}, {"title": "D Additional Human Experiment Results", "content": "We include additional exploratory investigations into user behavior. We report average time spent for each topic in Table 2. We observe that the average time spent per problem appears to increase across topics.\nWe also decompose user behavior in Figure 4 within the frictioned condition according to whether the participant received a friction on that topic or not. Here, we can more clearly see that click rates decrease for participants who are explicitly frictioned. However, we caveat these results in that the friction intervention itself induces biases in the user samples across the groups. Participants only see a friction if they are necessarily better than the model; hence, users may already be inclined to click less often."}, {"title": "D.1 Example User Responses", "content": "We asked participants in a post-survey questionarre what factors led them to click. We include a few exemplary responses below:\n\u2022 \"On questions where I didn't immediately know the answer, I clicked to see the model predicted answer in case it would help me. On questions where I didn't know the answer at all, I clicked to see the model predicted answer to help me know where to start.\"\n\u2022 \"If I simply had no idea what the answer was to a question, I would click to see what the prediction would say and then decide if I wanted to go along with it or not. The other instance would be if I was stuck between two choices, I'd click the button to see if the prediction was the same answer as mine or not to have some sort of confirmation.\"\n\u2022 \"not knowing anything about the topic or having any idea as to the right answer i clicked on the the ai button hoping it would help or know more than me. I also clicked it a couple times when i thought i knew the"}, {"title": "4.2", "content": "answer but wasnt 100% sure and if it chose the same as me i felt more confident\"\n\u2022 \"Unsure if I had the right answer and not having enough knowledge or not having used knowledge of the subject for up to 30 years\"\n\u2022 \"I used the prediction button if I felt unsure of the answer or if I wanted to feel more assured of my own answer.\"\n\u2022 \"At first it was to help me with answering the question, then I realized the AI gave wrong answers as well, so for the ones I was sure of the answer I still clicked to see what it would show. It was very satisfactory to see I got it right when AI got it wrong, but very disappointing when AI gave me wrong answer when I didn't know the correct answer\"\n\u2022 \"If I was unsure of the correct answer (or second guessing myself), I checked the model prediction to see if the AI model aligned with what I was thinking\"\nWhen asked why they clicked, we noted that a few participants did report that they were simply curious:\n\u2022 \"In instances where I doubted my answer, I clicked to see the A.I model prediction. After getting an answer wrong I had an urge to click on the prediction. When I had no clue what to answer, I clicked the button. Sometimes, especially, for the math questions I clicked out of curiosity since I observed the A. I often got the answers wrong.\"\n\u2022 \"I was mainly interested to see what it thought the answer was, independent if I thought I knew the answer or not. I had to look.\"\n\u2022 \"I am not familiar with the terminology and am curious how AI responds.\"\nWe also asked participants why they chose not to click:\n\u2022 \"I already 100% knew the answer so I didn't wait to see what the model predicted answer was.\"\n\u2022 \"I thought it would struggle to answer some of the more linguistically complicated questions correctly (like the ones that asked which of these is not true and 3 things are and 1 is not). I also thought for the most basic math problems (like things that were essentially a single computation), there wasn't really a need.\"\n\u2022 \"I didn't click the button for answers where I was highly confident. (I suppose it probably wouldn't have hurt to click it, but it would take a little pride out of it if the AI was correct too...)\"\n\u2022 \"I like to challenge myself naturally, I'd prefer to make a good guess and be wrong to learn from it than to just look up the answer (or in this case, use AI) and immediately forget. I'd argue this is a case of disconnect between an internal effort and reward system."}]}