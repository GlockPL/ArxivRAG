{"title": "Evaluating Machine Learning Models against Clinical Protocols for Enhanced Interpretability and Continuity of Care", "authors": ["Christel Sirocchi", "Muhammad Suffian", "Federico Sabbatini", "Alessandro Bogliolo", "Sara Montagna"], "abstract": "In clinical practice, decision-making relies heavily on established protocols, often formalised as rules. Concurrently, machine learning (ML) models, trained on clinical data, aspire to integrate into medical decision-making processes. However, despite the growing number of ML applications, their adoption into clinical practice remains limited. Two critical concerns arise, relevant to the notions of consistency and continuity of care: (a) accuracy - the ML model, albeit more accurate, might introduce errors that would not have occurred by applying the protocol; (b) interpretability - ML models operating as black boxes might make predictions based on relationships that contradict established clinical knowledge. In this context, the literature suggests using integrated ML models to reduce errors introduced by purely data-driven approaches and improve interpretability. However, there is a lack of appropriate metrics for comparing ML models with clinical rules in addressing these challenges.\nAccordingly, in this article, we first propose a metric to assess the accuracy of ML models with respect to the established protocol. Secondly, we propose an approach to measure the distance of explanations provided by two rule sets, with the goal of comparing the explanation similarity between clinical rule-based systems and rules extracted from ML models. The approach is validated by employing the Pima Indians Diabetes dataset, for which a well-grounded clinical protocol is available, by training two neural networks-one exclusively on data, and the other integrating knowledge. Our findings demonstrate that the integrated ML model achieves comparable performance to that of a fully data-driven model while exhibiting superior relative accuracy with respect to the clinical protocol, ensuring enhanced continuity of care. Furthermore, we show that our integrated model provides explanations for predictions that align more closely with the clinical protocol compared to the data-driven model.", "sections": [{"title": "1. Introduction", "content": "Machine learning (ML) has revolutionised various industries, from manufacturing to finance, and is now making its way into healthcare, a sector traditionally resistant to technological disruptions. ML has achieved remarkable performance in various domains of clinical medicine,\nResistance to embrace ML in clinical settings can be attributed to the prevailing reliance on evidence-based clinical pathways, guidelines, and protocols as the foundation for clinical decision-making [3]. Adherence to established guidelines and practices is at the core of the consistency and continuity of care, defined as the degree to which a series of discrete healthcare events is experienced by people as coherent and consistent over time and across different healthcare providers [4]. Introducing novel decision-support systems offering alternative predictions and explanations may introduce variability among practices and practitioners, potentially compromising the quality and efficiency of care.\nNovel ML models reporting superior performance compared to the current protocol might be found unsuitable for clinical use if they (a) fail to correctly predict cases effectively managed by the protocol in place due to potential liabilities, or (b) make predictions based on confounding variables and erroneous relationships that contradict established clinical knowledge [5]. Therefore, in order to foster continuity of care when developing novel decision-support systems for healthcare, it is imperative not only to attain high overall accuracy but also to provide predictions and explanations adhering to current clinical guidelines. To this end, approaches integrating domain knowledge from clinical protocols into ML models have been proposed and proved effective [6]. However, metrics for evaluating the similarity of a novel model with respect to established protocols in terms of predictions and explanations are still lacking.\nThe main contribution of the manuscript is thus to introduce metrics capturing the adherence of a model to established protocols in terms of accuracy and explanation of its predictions. Specifically, we introduce the notions of relative accuracy to quantify the proportion of samples correctly predicted by the model compared to those handled correctly by the existing protocol, and of explanation similarity to quantify the degree of overlap between local explanations provided by the protocol and the ML model for the dataset instances.\nThrough a comparison between a neural network model, trained solely on data, and a model incorporating domain knowledge encoded in a clinical protocol, we illustrate the potential of these metrics using the PIMA dataset [7]. While conventional performance metrics cannot definitively identify a superior model between the two, our proposed metrics reveal that the integrated model introduces fewer errors into the decision-making process and provides explanations that more closely mirror established practices. Consequently, these newly introduced metrics serve as valuable tools for identifying the ML model that better aligns with the protocol in place and is thus more suitable for integration into clinical practice in the prospect of continuity and consistency of care.\nIncorporating domain knowledge from clinical protocols into ML and developing metrics to evaluate the accuracy and interpretability of such models with respect to the protocol in place represent a pivotal step towards overcoming the limitations of ML and facilitating its seamless integration into medical practice."}, {"title": "2. Background and previous work", "content": "As medical decision-making becomes increasingly complex due to the development of new therapies and diagnostics, as well as the accumulation of health records, ML has emerged as a promising tool to support medical decision-making processes for its ability to model complex interactions between features [8]. However, the application of ML in healthcare presents several challenges, primarily related to the quantity, quality, and composition of clinical data, as well as a lack of explainability and limited robustness [9]. To address these limitations, the literature reports on various integrative approaches that leverage multiple models, data sources, and prior knowledge. A notable advancement is the paradigm of Informed Machine Learning, which integrates data and prior knowledge derived from independent sources to strike a balance between model complexity and effectiveness [10, 11]. This approach has gained attention in the medical domain, where structured knowledge is abundant but data is often limited and noisy. Recent contributions in this area have provided taxonomies of integration strategies applied to the healthcare sector, with a focus on the integration of ML with rule-based expert systems, highlighting that integration can be beneficial across all phases of the ML pipeline, from data preprocessing and feature engineering, to model learning and output evaluation [9, 12, 11]. Particular emphasis is placed on strategies incorporating prior knowledge into the model's loss function, often through regularisation or penalty terms quantifying inconsistencies or violations concerning the knowledge base. This approach has shown promising results in clinical applications, enhancing model performance, robustness, and interpretability [9].\nDespite these advancements, there remains a critical need of metrics to evaluate the resulting hybrid models against the knowledge base to measure adherence, and against the data-driven counterpart to quantify knowledge injection. For a comprehensive comparative analysis, such metrics must evaluate both accuracy and interpretability. This study aims to address this gap by proposing metrics assessing model adherence to a knowledge base in terms of performance and explainability, with immediate applications in evaluating hybrid models in clinical settings."}, {"title": "2.1. Evaluating model performance", "content": "A plethora of scores have been proposed to gauge the correctness of the predictions with respect to the ground truth. In classification tasks, accuracy emerges as an intuitive metric, quantifying the proportion of correctly classified instances. Sensitivity (recall) and specificity measure the proportion of correctly identified true positives and true negatives, respectively, while precision evaluates the ratio of true positives over positive predictions. F1-score, the harmonic mean of precision and recall, balances both metrics. The area under the receiver operating characteristic curves provides a comprehensive view of model performance across different thresholds but is less interpretable to some stakeholders. Overall, accuracy and F1-score are the most popular metrics but may yield overly optimistic results with imbalanced data. Recently, Matthew's correlation coefficient has gained prominence in biomedical data analysis for yielding more trustworthy results in imbalanced datasets [13].\nDetermining the most suitable statistical metric remains challenging, with no consensus reached [13]. Comparative analyses often leverage a diverse array of metrics for a comprehensive evaluation, with the choice of the most appropriate metric contingent upon the specific case at"}, {"title": "2.2. Evaluating model explanations", "content": "As ML architectures become increasingly complex, there arises a pressing need to bridge the gap between the opaque nature of these models and human comprehension, especially in domains like healthcare where transparency and interpretability are essential [14]. Addressing this challenge, the field of eXplainable Artificial Intelligence (XAI) has emerged, developing tools aimed at providing human-understandable explanations for AI-driven decisions, thereby fostering transparency, trust, and collaboration between human expertise and computational intelligence [15]. XAI employs various techniques to provide reasoning on ML decisions, mainly operating on two levels: local and global. In the former, individual model predictions are analysed, while in the latter the overall behaviour of the model is analysed to identify patterns and relationships in the data. Among XAI techniques, feature importance methods have emerged as influential for identifying important variables. Additionally, example-based explanations offer insights by presenting similar instances in the dataset that influenced the predictions.\nRule extraction techniques translate the ML models into human-understandable rules or decision trees which provide insights into the overall behaviour of the model across the entire dataset [16, 17]. Moreover, a rule applicable to a given data instance indicates the conditions that were satisfied to produce the corresponding outcome, offering explanations at the local level. Several rule extraction algorithms exist in the literature. The Rule Extraction From Neural Network Ensemble (REFNE) [18] was initially developed for extracting symbolic rules from neural network ensembles. However, its accuracy decreases when the data is highly complex or nonlinear. C4.5Rule-PANE [19] utilises the C4.5 rule induction algorithm to extract if-then rules from neural networks and, like other tree-based algorithms, is susceptible to over-fitting. TREPAN [20] constructs a decision tree by querying the underlying network to determine output classes. However, it often extracts suboptimal rule sets and requires binary inputs.\nDecision trees, particularly Classification and Regression Trees (CART) [21], remain one of the most prominent approaches in rule extraction. CART constructs a binary tree structure which is then translated into human-readable rules by converting each possible path from the root to the leaves into an if-then rule. Its strengths are simplicity, interpretability, and ability to handle both categorical and numerical data effectively. Several other rule-extraction algorithms exist, as well as software libraries dedicated to knowledge extraction, e.g., the PSyKE platform [22], providing a unified software framework supporting various rule-extraction methods [20, 21, 23, 24, 25].\nSeveral evaluation metrics are documented in the literature to assess the quality of extracted rule sets. Among these metrics, the number of rules and average rule length reflect attributes of the explainability of the rule extractor. The other metrics \u2013 completeness, correctness, fidelity, robustness, and coverage serve as general validation factors applicable to any rule extractor method. These metrics primarily analyse properties of the rules as global explanations for the model, offering a coarse-grained evaluation. Less attention is given to metrics assessing rules"}, {"title": "3. Methods", "content": "This section is structured as follows: Section 3.1 details the dataset and domain knowledge used in our case study, Section 3.2 describes the machine learning model that integrates this domain knowledge, while Section 3.3 introduces the metrics for accuracy and explainability used to evaluate the integrated model against clinical knowledge and a data-driven model."}, {"title": "3.1. Dataset and domain knowledge", "content": "In this work, we present our investigations involving the Pima Indians Diabetes dataset, originally compiled by the National Institute of Diabetes and Digestive and Kidney Diseases from a study of the Pima Indian population, known for its notably high incidence of diabetes [7]. The dataset comprises 768 medical profiles of women aged 21 and above, who underwent an Oral Glucose Tolerance Test (OGTT) to measure their glucose and insulin levels at two hours. The target variable is binary, indicating a diabetes diagnosis within five years. Missing values are present in the attributes I120 (48.70%), ST (29.56%), \u0412\u0420 (4.55%), BMI (1.43%), and G120 (0.65%), and were imputed in this work with the median value of the respective variable, as reported in the literature [29].\nPublic health guidelines on type-2 diabetes risks report that individuals with a high BMI (\u2265 30) and high blood glucose level (> 126) are at severe risk for diabetes, while those with normal BMI (\u2264 25) and low blood glucose level (\u2264 100) are less likely to develop diabetes. These guidelines have been utilised to design rules [30] expressed as logic predicates (see Table 2)."}, {"title": "3.2. Integrated ML model", "content": "The hybrid ML model examined in this study, herein denoted as KB-ML, integrates domain knowledge in the loss function. Specifically, KB-ML is a neural network for binary classification trained using a custom loss function that assigns greater weight to samples accurately predicted by the clinical guidelines represented by the two logic predicates in Table 2. Formally, let $D$ denote a dataset comprising $n$ instances each represented by $x_i$, where $i$ ranges from 1 to $n$. Three $n \\times 1$ vectors $y$, $p$, and $r$ can be defined. Vector $y$ contains the ground-truth binary labels, with each element denoted as $y_i$ and representing the expected outcome for instance $x_i$. Vector $p$ contains the probability of the outcome belonging to the positive class predicted by the neural network, with each element $p_i$ corresponding to $x_i$. Finally, vector $r$ contains the predictions according to the rules in Table 2, i.e., each element $r_i$ takes value 1 if $x_i$ satisfies the conditions of the first rule, 0 if it satisfies the second rule, and N/A otherwise. Then, the Custom Total Loss (CTL) for the integrated model is computed as:\n$CTL(y, p, r, \\alpha) = \\frac{1}{n} \\sum_{i=1}^{n} CSL(y_i, p_i, r_i, \\alpha), $\nwhere $\\alpha$ is the scaling factor controlling the influence of the additional loss term, CSL is the custom binary cross-entropy loss for a single sample defined as\n$CSL(y_i, p_i, r_i, \\alpha) = \\begin{cases}L(y_i, p_i) & \\text{if } r_i \\neq y_i \\\\(\\alpha+1)L(y_i, p_i) & \\text{if } r_i = y_i\\end{cases}$\nand $L$ is the standard binary cross-entropy loss for a single sample\n$L(y_i, p_i) = [y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i)]$"}, {"title": "3.3. Proposed evaluation metrics", "content": ""}, {"title": "3.3.1. Relative accuracy", "content": "Performance metrics can be redefined to evaluate adherence to accurate predictions set by the rules, quantifying errors introduced by the model in comparison to the reference protocol. As in Section 3.2, consider D as a dataset consisting of n samples represented by $x_i$, where i ranges from 1 to n, and let $r_i$ denote the prediction made by a clinical protocol for each $x_i$. Additionally, let $\\hat{y}_i$ represent the binary prediction provided by a ML model for $x_i$. Relative Accuracy (RA) can be defined as the fraction of samples correctly predicted by the protocol that"}, {"title": "3.3.2. Explanation similarity", "content": "Applying XAI in clinical settings requires proper evaluation to ensure the explanations are both technically sound and clinically useful. Rule sets extracted from ML models provide valuable insights into model behaviour. Notably, rules extracted from different ML models can emphasise different variables, even when predicting similar outcomes. Therefore, it is crucial to assess the similarity of explanations provided by rules approximating predictors to those offered by a specified reference protocol. This evaluation helps determine which explanation aligns more closely with the clinical protocol in use and better reflects clinical expertise.\nA novel explanation similarity strategy is here proposed to estimate the similarity of explanations from rule-based predictors, whether extracted from black-box models or built on clinical knowledge. This method allows for comparing explanations from integrated and data-driven models with those provided by a clinical protocol, to verify which aligns better. The method entails the following steps.\n1. Rule extraction symbolic knowledge is extracted from black-box predictors trained on a given dataset and represented as rule sets that are both human- and machine-interpretable and can provide explanations to predictions in the form of first-order logic clauses.\n2. Feature discretisation the features of the dataset are discretised according to the thresholds found in the rules of the considered rule sets. This involves collecting all thresholds associated with each feature and discretising the feature into intervals, accordingly.\n3. Rule vectorisation each rule is assigned a vector representing the feature space, where each element corresponds to an interval of a feature and is assigned a value of 1 if the corresponding feature and interval satisfy the rule, and 0 otherwise.\n4. Local explanation for every rule set, and for each sample in the data set, the rule satisfied by the sample is identified and the corresponding vector is assigned to the sample."}, {"title": "5. Similarity calculation", "content": "the similarity between two rule sets is obtained by computing, for each sample, the similarity between the vectors obtained from the two rule sets, and averaging across all samples, while the similarity among more than two rule sets is obtained by calculating the similarity between each pair of rule sets, and averaging all scores.\nFormally, let D represent a dataset comprising n samples denoted by xs, where s ranges from 1 to n. Each sample is described by m input features, labelled as V1, V2, . . ., Um. Here, x represents the value of feature vk in the instance x5. For each input \u00e6s, ys denotes the corresponding outcome. Dr and Dy denote the domains of the inputs and outputs, respectively:\n$(X_s \\in D_x) \\land (Y_s \\in D_y), \\forall s = 1,2,...,n$.\nRule extraction. Let us consider a predictive function F\n$F: D_x \\rightarrow D_y, F(x_s) = \\hat{y}_s,$\nwhere \u0177s is the value predicted by F for the instance x5. Then, a rule set R mapping instances to outputs and approximating the input-output relationship of F can be obtained by analysing F. Let P be a set of p rule sets, either obtained from predictive functions by rule extraction or available from domain knowledge, which we aim to compare:\n$P = \\{R^1, R^2, ..., R^P\\},$\nwhere\n$R^i : D' \\subseteq D_x \\rightarrow D_y \\forall i = 1,2,..., p$.\nEach rule set consists of rules denoted by R. For instance, if rule set i comprises q rules, then R\u00b2 = {R\u2081, R, ..., R}. Each rule R in rule set R\u00b9 is represented as a tuple (C, \u00dd), where C constitutes a set of t conditions {1,2,..., it} and represents the outcome associated to that rule. Each condition ch can be expressed by a tuple (vih, ljh, jh), where h is the variable included in the condition, and lih and wish are the lower and upper bounds for the condition. If a condition is defined over a discontinuous interval, it is separated into distinct conditions. If a condition is of the type less than or greater than, the lower or upper bound is replaced with the minimum or maximum value of the variable for that feature in the dataset.\nFeature discretisation. For the set of predictors P, we define the set of thresholds Tas:\n$T = \\{T(v_1), T(v_2),...,T(v_m)\\},$"}, {"title": "5. Similarity evaluation.", "content": "Let $S(V^1, V^2)$ be a similarity function on two binary vectors $V^1$ and $V^2$. The similarity $S(\\mathcal{R}^1, \\mathcal{R}^2)$ for two rule sets $\\mathcal{R}^1$ and $\\mathcal{R}^2$ in $\\mathcal{P}$ can then be computed as\n$S(\\mathcal{R}^1, \\mathcal{R}^2, S, D_{\\mathcal{R}}) = \\frac{1}{|D_{\\mathcal{R}}|} \\sum_{x_s \\in D_{\\mathcal{R}}} S(V^1(x_s), V^2(x_s)).$\nThe similarity among more than two sets is computed by calculating the pairwise similarity between each pair of rule sets and then averaging across all rule sets. For a set $\\mathcal{P}$ of $p$ rule sets the similarity is computed as:\n$S(\\mathcal{P}, S, D_{\\mathcal{R}}) = \\frac{2}{p(p-1)} \\frac{1}{|D_{\\mathcal{R}}|} \\sum_{f=1}^{p-1} \\sum_{g=f+1}^{p} \\sum_{x_s \\in D_{\\mathcal{R}}} S(V^f(x_s), V^g(x_s)).$\nTo compute the similarity of two binary vectors $V^1$ and $V^2$ of length $w$, various similarity metrics $S$ are available in the literature.\nXNOR similarity considers matching and non-matching elements:\n$\\text{XNOR}(V^1, V^2) = \\frac{\\sum_{i=1}^{w} \\delta(V^1[i], V^2[i])}{w}$\nwhere $\\delta(V^1[i], V^2[i])$ equals 1 if $V^1[i] = V^2[i]$ and 0 otherwise.\nJACCARD similarity considers the intersection over the union of elements in both vectors:\n$\\text{JACCARD}(V^1, V^2) = \\frac{\\sum_{i=1}^{w} V^1[i] \\cdot V^2[i]}{\\sum_{i=1}^{w} V^1[i] + \\sum_{i=1}^{w} V^2[i] - \\sum_{i=1}^{w} V^1[i] \\cdot V^2[i]}$\nwhere $\\sum_{i=1}^{w} V^1[i] \\cdot V^2[i]$ counts the elements that are 1 in both vectors (intersection), while $\\sum_{i=1}^{w} V^1[i] + \\sum_{i=1}^{w} V^2[i]$ counts the elements that are 1 in either rule vector (union).\nCOSINE similarity computes the cosine of the angle between the vectors:\n$\\text{COSINE}(V^1, V^2) = \\frac{\\sum_{i=1}^{w} V^1[i] \\cdot V^2[i]}{\\sqrt{\\sum_{i=1}^{w} V^1[i]^2} \\cdot \\sqrt{\\sum_{i=1}^{w} V^2[i]^2}}$\nwhere $\\sqrt{\\sum_{i=1}^{w} V^1[i]^2} \\cdot \\sqrt{\\sum_{i=1}^{w} V^2[i]^2}$ computes the product of the magnitudes of the vectors.\nDICE similarity divides twice the number of matching elements by the number of elements:\n$\\text{DICE}(V^1, V^2) = \\frac{2 \\cdot \\sum_{i=1}^{w} V^1[i] \\cdot V^2[i]}{\\sum_{i=1}^{w} V^1[i] + \\sum_{i=1}^{w} V^2[i]}$"}, {"title": "3.4. Evaluation strategy", "content": "The study conducted a comparison between two neural networks trained on the Pima Indians Diabetes dataset. One model, termed the data-driven model (DD-ML), was exclusively trained on data, while the other, referred to as the integrated or knowledge-based model (KB-ML), was trained with a custom loss function incorporating knowledge from a knowledge base (KB), as detailed in Section 3.2. Both neural networks were designed as feed-forward models, comprising three fully connected layers: two hidden layers with rectified linear unit activation functions and an output layer with a sigmoid activation function. DD-ML was trained using binary cross-entropy loss, whereas KB-ML employed a customised loss function defined in Eq. 1 with parameter a, tuning the contribution of KB to model learning, ranging from 0.5 to 4 at intervals of 0.5. All neural networks were trained with a batch size of 20 for 25 epochs.\nIn all experiments, data was divided into training and testing sets using a 10-times 10-fold stratified cross-validation approach [31]. The performance and explainability metrics computed for the integrated model were evaluated against the corresponding metrics for the data-driven model using paired Student-t tests with the Nadeau and Bengio correction [32]. Performance evaluation encompassed a range of metrics, including Accuracy (A), F1-score (F1), Recall (R), Precision (P), Balanced Accuracy (BA), the Area Under the Receiver Operating Characteristic Curve (ROC AUC), and Matthews Correlation Coefficient (MCC). Moreover, the Relative Accuracy (RA), Sensitivity (RR) and Specificity (RS) metrics herein introduced were computed for all models.\nInterpretable models approximating the predictions of the neural networks were obtained by rule extraction using CART [21] available from the PSyKE library [33]. Rule sets were extracted from DD-ML and KB-ML (trained with the tuning parameter a set to 1.5) and denoted as DD-MLx and KB-MLx, respectively. Thus, each experiment yields three rule sets: KB, which formalises the clinical protocol; DD-MLx, which approximates the data-driven model; and KB-MLx, which approximates the integrated model. The maximum number of leaves, and thus rules, in the CART rule-extraction process, varied from 2 to 12. The fidelity of the obtained rule set was evaluated in terms of accuracy and F1-score with respect to the black-box model.\nThe proposed explanation similarity metrics (leveraging XNOR, Dice, Jaccard, and Cosine similarity) were computed between DD-MLx and KB, and between DD-MLx and KB on two subsets of the dataset. Initially, explanation similarity metrics were computed over samples for which all considered predictors (KB, DD-ML, KB-ML) could make predictions, thus excluding samples not handled by the protocol. Subsequently, explanation similarity metrics were computed over samples for which all considered models made correct predictions. Finally, the explanation similarity metrics were utilised to gauge the robustness of explanations. A comparison was made among the 100 instances of the KB-ML model trained over the 10-times 10-fold cross-validation. A 100x100 similarity matrix was generated, computing pairwise explanation similarity with XNOR operation between each pair of model instances. The similarities were then averaged across all elements of the matrix. The same process was repeated for DD-ML."}, {"title": "4. Results and discussion", "content": "4.1. Relative accuracy evaluation\nThis integration of domain knowledge, modulated by the parameter a, influences the model's performance, which varies with respect to a as shown in Figure 2a. For standard metrics, the performance increases, peaking between a values of 1 and 1.5, subsequently declining for A and MCC, while stabilising for ROC. This trend suggests that while the introduced learning bias given by the protocol could be beneficial, excessive bias might impede the learning process, leading to decreased accuracy that falls below that of the data-driven model for a greater than 2. The proposed RA metric increases with a, effectively detecting the reduction of errors introduced by the integrated model with respect to the reference model. For values of a around 1.5, optimal scores of standard metrics are achieved, as well as improved RA. This evaluation highlights the need of tuning integration to maximise adherence without compromising performance.\nA comprehensive array of metrics comparing the data-driven model with the integrated model at a equal to 1.5, along with relative p-values indicating statistical significance, are reported in Table 3. The integrated model yields superior scores across all metrics except precision, with statistical significance observed for BA, ROC, and R. Nonetheless, precision significantly decreases, and improvements in MCC, F1, and A lack statistical significance. Therefore, it remains challenging to conclusively state that one model is superior to the other. However, the RA metric significantly improved from 0.90 to 0.97, driven by the increased RR (since RS is maximal for both models). These findings highlight the greater alignment with the clinical protocol, also seen in Figure 2b, making the integrated model preferable overall, and demonstrate the role of the proposed metrics in facilitating this assessment."}, {"title": "4.2. Explanation similarity evaluation", "content": "The model incorporating domain knowledge also offers explanations that better align with the underlying reasoning of the knowledge base. Given the black-box nature of both the data-driven and integrated neural networks, explanations for each prediction are provided via surrogate rule sets, with a number of rules varying from 2 to 12, serving as approximations of the model's decision-making process. The surrogate models KB-MLx and DD-MLx closely mirror the behaviour of the black-box models, reporting accuracy and F1 scores consistently above 0.85 across all rule set sizes, as shown in Figure 3a."}, {"title": "5. Conclusions and future work", "content": "This study introduces novel metrics to evaluate the adherence of models to established protocols in terms of accuracy and explanation of predictions. Through comparative analysis on a benchmark dataset, we illustrate that models incorporating protocol knowledge exhibit superior alignment with established practices, making them more suitable for integration into clinical decision-making processes.\nIn future research, we aim to extend this investigation to other datasets, retrieving the corresponding domain knowledge either by translating established protocols into rules or by consulting clinicians to encode that knowledge. Having demonstrated adherence to the clinical protocol across different datasets and clinical applications, we also plan to consult respective experts to verify that the trained ML model is trustworthy also outside the domain of application of a protocol, by evaluating whether the learning criteria align with clinicians' judgement in borderline cases. Additionally, we plan to validate the proposed approach using other automatic rule extraction algorithms, including those based on fuzzy logic, such as neuro-fuzzy models. Finally, we intend to enhance the explanation similarity metrics by scaling intervals based on their length or the number of samples within them, rather than assigning binary values."}, {"title": "Availability of data and code", "content": "The dataset analysed is publicly available (https://www.kaggle. com/datasets/uciml/pima-indians-diabetes-database), and the code to replicate the experiments can be found in the GitHub repository (https://github.com/ChristelSirocchi/XAI-similarity)."}]}