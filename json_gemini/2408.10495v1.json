{"title": "How Well Do Large Language Models Serve as End-to-End Secure Code Producers?", "authors": ["Jianian Gong", "Nachuan Duan", "Ziheng Tao", "Zhaohui Gong", "Yuan Yuan", "Minlie Huang"], "abstract": "The rapid advancement of large language models (LLMs) such as GPT-4 has revolutionized the landscape of software engineering, positioning these models at the core of modern development practices. As we anticipate these models to evolve into the primary and trustworthy tools used in software development, ensuring the security of the code they produce becomes paramount. How well can LLMs serve as end-to-end secure code producers? This paper presents a systematic investigation into LLMs' inherent potential to generate code with fewer vulnerabilities. Specifically, We studied GPT-3.5 and GPT-4's capability to identify and repair vulnerabilities in the code generated by four popular LLMs including themselves (GPT-3.5, GPT-4, Code Llama, and CodeGeeX2). By manually or automatically reviewing 4,900 pieces of code, our study reveals that: (1) large language models lack awareness of scenario-relevant security risks, which leads to the generation of over 75% vulnerable code on the SecurityEval benchmark; (2) LLMs such as GPT-3.5 and GPT-4 are unable to precisely identify vulnerabilities in the code they generated; (3) GPT-3.5 and GPT-4 can achieve 33.2%~59.6% success rates in repairing the insecure code produced by the 4 LLMs, but they both perform poorly when repairing self-produced code, indicating self-repair \"blind spots\". To address the limitation of a single round of repair, we developed a lightweight tool that prompts LLMs to construct safer source code through an iterative repair procedure based on the insights gained from our study. Experiments show that assisted by semantic analysis engines, our tool significantly improves the success rates of repair to 65.9%~85.5%.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformer-based large language models\u00b9 have fundamentally reshaped software engineering in recent years. As LLM-powered programming assistants such as GitHub Copilot and CodeGeeX are widely adopted by IT companies and individual developers, traditional developer-centered software engineering (i.e., Software 1.0 and Software 2.0 where all source code is manually written) is rapidly evolving to LLM-centered software engineering (i.e., Software 3.0 where most source code is generated by AI). According to research by GitHub, its Copilot contributes to more than 46% of all source code that its 1.3 million users have developed [17].\nAs more LLM-generated code is accepted by developers and thus becomes part of the software, its security is gaining increasing concern. Previous studies [21, 24, 28] have revealed that although large language models are capable of generating functionally correct code, they are not free of security vulnerabilities, since their training sets contain real-world insecure code. Therefore, raw LLM-generated source code cannot be trusted to be deployed in security-sensitive scenarios.\nThe increasingly significant role of LLMs in software engineering, coupled with disturbing vulnerabilities in the code they generate, compels us to explore methods for producing safer code with LLMs. Standard end-to-end software development practices, which include writing, reviewing, and refactoring code, result in significantly higher quality compared to focusing solely on code writing [18]. This inspires us to consider: How well do large language models function as comprehensive, end-to-end secure code producers? In the context of this research, end-to-end refers to a process wherein a code snippet is not only generated but also undergoes review and, if necessary, remediation of identified vulnerabilities before being presented to users. To achieve this level of competence, LLMs must demonstrate proficiency in three key areas: generating functionally correct code, conducting thorough self-reviews of their output, and effectively repairing any detected security flaws.\nOur investigation focuses specifically on evaluating these capabilities within the Python programming ecosystem. This choice is motivated by two primary factors: firstly, Python's widespread adoption in web development, a domain where security considerations are paramount; and secondly, the relative scarcity of existing research examining the security implications of AI-generated Python code.\nHence, we seek answers to the following research questions (RQs):\nRQ1 How do LLMs perform when generating Python code in security-sensitive scenarios? This research question aims to address the lack of exclusive research on the security of LLM-generated Python code. Additionally, the identified insecure code in this RQ will be used in subsequent research questions.\nRQ2 How effective are LLMs in identifying LLM-generated code vulnerabilities? This research question seeks to investigate LLMs' potential to self-review their generated code, as accurate identification of vulnerabilities is crucial for mitigating weaknesses in the source code.\nRQ3 How effective are LLMs in repairing LLM-generated code vulnerabilities? This RQ investigates LLMs' capability to self-repair their generated code, which is the final and most crucial step in constructing secure source code.\nRQ4 How effective is an iterative strategy in improving LLMs' repair capability? A single round of vulnerability detection and repair may not be sufficient to eliminate weaknesses in code. Therefore, we introduce an iterative strategy, allowing LLMs to repair the generated code through multiple attempts with feedback information provided\nPrevious studies such as [28, 29, 41] have respectively examined the capability of large language models in generating secure code, detecting vulnerabilities in real-world code, and repairing them. However, these studies typically focus on a limited range of scenarios. Furthermore, there has been no comprehensive pipeline study integrating the processes of generation, detection, and repair to evaluate the potential of LLMs to produce secure code in an end-to-end manner.\nOur novel contributions are as follows:\n(1) We present a systematic study evaluating the performance of 4 LLMs in generating secure Python code across 67 CWEs, offering a more extensive analysis than previous research as well as addressing the lack of relevant research focusing on Python. Overall, we found that the 4 tested LLMs generated over 75% vulnerable code on the SecurityEval benchmark.\n(2) To the best of our knowledge, we are the first to conduct a study on LLMs' efficacy in judging and fixing insecure code generated by LLMs themselves, while previous work focuses primarily on manually written code in open-source projects. Our experiments revealed that GPT-3.5 and GPT-4 are unable to accurately identify weaknesses in LLM-generated code. While they achieve certain success"}, {"title": "2 STUDY SETTING", "content": "SecurityEval is a dataset dedicated to measuring the security of machine learning-based code generation techniques, proposed by Siddiq et al. [32] In its current version [1], the dataset contains 121"}, {"title": "2.3 Methodology", "content": "2.3.1 Methodology for RQ1~RQ3. Figure 3 shows the overall workflow of RQ1~RQ3, which goes in a pipeline fashion. The overall procedure aims to assess LLMs' end-to-end capability to generate secure code.\nIn RQ1, we prompt GPT-3.5, GPT-4, Code Llama, and CodeGeeX2 to complete the 121 code generation tasks in SecurityEval. Results that do not meet basic functional requirements (with syntax errors or with obvious deviation from the intent of the prompt) will be regenerated until they do. We then review them to determine whether they are with the specific CWE vulnerabilities (explained in 2.3.3) and draw conclusions.\nIn RQ2, we prompt large language models to inspect every piece of code generated in RQ1 for the presence of the corresponding CWE vulnerability. We then use the review results from RQ1 as the ground truth to evaluate the LLMs' ability to identify weaknesses in their self-produced code.\nIn RQ3, the vulnerable code identified in RQ1 will be provided to LLMs for repair, along with information about its corresponding CWE. The Repaired code produced by these models will then undergo the review procedure again. We will assess the LLMs' capability of repairing self-produced code based on these review results.\n2.3.2 Algorithm for RQ4. To exploit the potential of LLMs in producing safer code, we design an iterative repair algorithm for RQ4, as shown in Figure 4. The algorithm takes code generation tasks as input. It initially utilizes a LLM to generate code according to given task descriptions. Subsequently, it employs an automated technique to scan the generated code for vulnerabilities. If a piece of code is free of weaknesses, it is output as secure code. Otherwise, it is returned to the LLM for repair, with CWE information provided. This scan-and-repair process is conducted iteratively until the code is secure or the predefined maximum number of iterations is reached.\nIt is worth noting that the \"automated scanning technique\" in the algorithm is not predefined. Initially, we planned to use LLMs for vulnerability self-detection. However, as discovered in RQ2, LLMs are unable to provide reliable detection results. Alternatively, semantic analysis engines such as CodeQL and Bandit have been adopted.\n2.3.3 Method of Code Review. Code snippets generated or repaired by LLMs need to undergo review to determine whether they are vulnerable to the specified CWEs. For instance, code generated for task CWE-089/author_1.py will be examined for the existence of CWE-89 (SQL Injection). We do not inspect the presence of other CWE vulnerabilities for (1) it is impossible for any method to encompass the entire spectrum of possible CWEs; (2) according to the design of SecurityEval, the generated code is most likely to exhibit the predefined vulnerabilities; and (3) focusing on predefined vulnerabilities is a common practice in previous studies such as [28, 29, 32]. To ensure maximum accuracy, we perform 3 independent rounds of review for each piece of code:\n(1) CodeQL Scan. CodeQL is an open-source semantic code analysis engine developed and maintained by GitHub, capable of detecting code vulnerabilities in various programming languages [5]. It is also the tool supporting GitHub Advanced Security features [6]. CodeQL officially supports scanning (i.e., provides query scripts) for 29 CWEs and 1 CVE directly related to security in Python [12]. Of these, 26 CWEs are among the 69 CWEs in SecurityEval, covering 67 out of 121 pieces of its code. We use CodeQL CLI v2.16.6 in our research. We choose CodeQL because of its status as an industry-leading engine for static code analysis and its widespread use in software security research.\n(2) Bandit Scan. Bandit is a semantic code analysis tool specifically designed for Python by PyCQA (an organization for Python code standardization). Bandit is able to traverse code files, build abstract syntax trees, and report potential security vulnerabilities in CWE [4]. The version utilized in our experiment is Bandit 1.7.8. We chose Bandit for our research because it is tailored for Python, aligning closely with the focus of our work. While PyCQA has not officially announced Bandit's coverage for CWE vulnerabilities, our practical experience reveals its capability to identify a range of vulnerabilities that may be overlooked by CodeQL.\n(3) Manual Review. Given that CodeQL and Bandit do not comprehensively cover all the CWEs in SecurityEval, we conduct manual code review as a supplementary measure. In this process, the authors primarily refer to (1) the example insecure code provided by the SecurityEval dataset for each task, and (2) the CWE definitions\n2.4 Prompts and Parameters\nThis subsection describes the prompts (in zero-shot fashion) and parameters used in our research.\n(1) Prompt for Code Generation. By referring to the OpenAI official website for guidance on code generation [10, 26], we construct the following prompt for code generation, where <Code> denotes a generation task in the SecurityEval dataset:\n\"You are a programming assistant, your task is to complete the following Python code according to comments faithfully: <Code>\"\nThe above prompt is used in RQ1 and RQ4.\n(2) Prompt for Vulnerability Detection. We refer to relevant work on LLMs for vulnerability detection [19, 30, 39] to construct the following prompt used in RQ2:\n\"Your task is to determine whether the following Python code is vulnerable to CWE - <CWE Serial Number> (<CWE Definition>). Return True for vulnerable or False for invulnerable.\"\n(3) Prompt for Vulnerability Repair. We refer to the examples for fixing bugs in Python programs on the OpenAI website [11] to construct the following prompt for code repair which is used in RQ3 and RQ4:\n\"You will be provided with a piece of Python code vulnerable to CWE - <CWE Serial Number> (<CWE Definition>). Your task is to generate the complete fixed code.\"\n(4) Parameters. To strike a balance between the models' creativity and the reproducibility of our work, we did not override the default parameters of the models. For the GPT family, default values of the parameters are temperature=1, top_p=1, etc. [26], while for Code Llama they are temperature=0.1, top_p=1,etc [13]. Unfortunately, the default values of CodeGeeX2's parameters remain undisclosed. We prompt the LLMs to complete all given tasks in separate conversations.\n2.5 Experimental Platform\nIn this work, we access all LLMs through remote APIs. Results generated by GPT-3.5 and GPT-4 are obtained via APIs provided by OpenAI4. Results from Code Llama is derived from free APIs provided by NVIDIA5. CodeGeeX2 is accessed through the CodeGeeX extension v2.8.0 for VSCode, published by Zhipu AI.\nBoth of the semantic code analysis tools are run locally on a single desktop-class PC equipped with an Intel i5-11300H processor and 16 GB DDR4 RAM."}, {"title": "3 RQ1: HOW DO LLMS PERFORM WHEN GENERATING PYTHON CODE IN SECURITY-SENSITIVE SCENARIOS?", "content": "In this section, we have GPT-3.5, GPT-4, Code Llama, and CodeGeeX2 generate code for tasks in the SecurityEval dataset with the prompt and parameters described in 2.4. Overall, 484 pieces of code have been generated (121 pieces by each of the 4 LLMs). We then automatically and manually review all the generated code with the method described in 2.3.3.\n3.1 Experiment Results\nFigure 5 provides a detailed visual representation of the performance of the four large language models on SecurityEval, highlighting their efficacy across various CWE scenarios. Additionally, Table 1 quantifies the results, showing the number and percentage of insecure code pieces generated by each model.\n3.2 Result Analysis\nA. Statistical Analysis\nAll four models performed poorly in generating secure code, with an average of 76.2% of the generated code being insecure. Despite their advanced capability in generating functionally correct code, these models struggled to produce code that met security standards. This highlights a significant challenge in using current LLMs for secure code generation and underscores the need for further research and development to enhance their reliability and effectiveness in security-sensitive applications.\nIn terms of specific models, GPT-4 and Code Llama generate code with slightly better security, while CodeGeeX2 produces the most insecure code. However, the differences are subtle, highlighting a uniform inability across large language models to generate secure code. Therefore, it is recommended that developers avoid the direct use of code generated by LLMs in security-sensitive scenarios.\nB. Scenario-relevant Analysis\nDuring our manual code review, we discovered that all four models exhibit a tendency to generate code that directly responds to prompts (functional requirements of tasks) without recognizing the concealed vulnerabilities pertinent to the task scenarios. Consequently, while the models excel in fulfilling functional requisites, their generated code frequently contains vulnerabilities specified by the dataset, as if falling into well-designed \"traps\". This observation does not necessarily imply an inherent incapacity of LLMs to generate code with fewer vulnerabilities, but rather underscores their deficiency in recognizing potential security risks not directly mentioned in prompts. Accordingly, developers may consider explicitly pointing out potential security risks in prompts, in order to remind LLMs to mitigate vulnerabilities."}, {"title": "4 RQ2: HOW EFFECTIVE ARE LLMS IN IDENTIFYING LLM-GENERATED CODE VULNERABILITIES?", "content": "In this section, we investigate whether large language models are qualified code self-reviewers by prompting them to identify vulnerabilities in code produced by themselves. In specific, we ask them whether the specified CWE weakness exists in the code they have generated. Detailed prompts are presented in 2.4.\nInstead of using all four models, we use only GPT-3.5 and GPT-4, as they have demonstrated the ability to generate coherent responses to our queries for vulnerability detection. Code Llama and CodeGeeX2 are excluded because, during our preliminary tests for the capability to detect, they often generate nonsensical responses, regardless of our command to identify weaknesses (part of their responses can be found in our public repository for reference). Overall, GPT-3.5 and GPT-4 generate 968 pieces of judgment (484 by each of the 2 models).\n4.1 Experiment Results\nUsing the review results from RQ1 as the ground truth, we assessed the correctness of a LLM's judgment by checking whether it is in agreement with the established ground truth. Equation 2 illustrates the method in a formalized way, in which Tech represents either GPT-3.5 or GPT-4.\n$\\text{Acc}_{\\text{Tech}}(x) = \\begin{cases} \\text{True} & \\text{if } \\text{Tech}(x) == \\text{Vul}(x) \\\\ \\text{False} & \\text{else} \\end{cases}$ (2)\nDetailed results of this section are presented in Appendix A.1, while Table 2 provides a statistical summary of the accuracy of GPT-3.5 and GPT-4 in detecting vulnerabilities in code produced by all four LLMs.\n4.2 Result Analysis\nOn average, GPT-3.5 achieves 43.6% accuracy, which is comparable to the random-guess baseline (~50%). This suggests that GPT-3.5's ability to detect vulnerabilities is of limited practical value. GPT-4, on the other hand, achieves an average accuracy of 74.6%, demonstrating its superior ability to understand and analyze code. The experimental results are largely consistent with previous related work based on real-world code [30].\nIn addition to accuracy, false positive rate (FPR) is another crucial metric for assessing the reliability of a detection technique. By only considering the results of the manual review in RQ1 as the ground truth, we determine a judgment as false positive using Equation 3, in which $\\text{FP}_{\\text{Tech}}(x)$ denotes whether a result of Tech (GPT-3.5, GPT-4, CodeQL, or Bandit) is false positive.\n$\\text{FP}_{\\text{Tech}}(x) = \\begin{cases} \\text{True} & \\text{if } \\text{Tech}(x) && !\\text{Manual}(x) \\\\ \\text{False} & \\text{else} \\end{cases}$ (3)\nFigure 11 presents the false positive rates of GPT-3.5, GPT-4, CodeQL, and Bandit. The FPRs of CodeQL and Bandit serve as baselines for assessing the trustworthiness of GPT-3.5 and GPT-4 as code reviewers.\nOverall, the false positive rate of GPT-3.5 reaches 4.3%, while that of GPT-4 is 3.1%, both of which are unacceptably high. In contrast, CodeQL and Bandit yield relatively low false positive rates (below 1%). This indicates that both tested LLMs have a significant likelihood of incorrectly identifying secure code as vulnerable. Consequently, neither GPT-3.5 nor GPT-4 can be relied upon for accurate vulnerability detection in the code they generated."}, {"title": "5 RQ3: HOW EFFECTIVE ARE LLMS IN REPAIRING LLM-GENERATED CODE VULNERABILITIES?", "content": "In this section, we investigate whether large language models are capable of effective code self-repair by prompting them to fix weaknesses in previously identified vulnerable code snippets. Only GPT-3.5 and GPT-4 are included in this analysis due to their superior ability to comprehend the intent of prompts, as was the case in RQ2. For details on the prompts used in this part, refer to 2.4. In total, GPT-3.5 and GPT-4 generated 738 pieces of repaired code (369 pieces of vulnerable code generated by the 4 LLMs in RQ1, repaired respectively by GPT-3.5 and GPT-4), all of which underwent both manual and automated review as outlined in 2.3.3.\n5.1 Experiment Results\nTable 3 presents the success rates of repair computed using equation 4, in which $N_{vul}$ denotes the number of vulnerable code snippets and $N_{fix}$ denotes the number of those who were successfully repaired (superscript 1 stands for one single attempt).\n$R_{fix} = \\frac{N_{fix}}{N_{vul}} \u00d7 100\\%$ (4)\n5.2 Result Analysis\nA. Statistical Analysis\nTable 3 demonstrates that GPT-3.5 and GPT-4 are capable of repairing a range of LLM-generated insecure code when provided with a description of the CWE type. Notably, GPT-4 performs significantly better than GPT-3.5, with nearly twice the success rate of repair. Although there are no pre-existing techniques as baselines to compare with (to the best of our knowledge, there's no automated technique such as APR tools that can effectively fix security vulnerabilities in Python programs), it can be concluded that advanced large language models such as GPT-4 have a promising level of ability to repair vulnerabilities in the code generated by themselves or other LLMs.\nAs emphasized in Table 3, GPT-3.5 achieves a success rate of 30.4% when attempting to fix its own generated code, marking its poorest performance across all code repair tasks. Similarly, GPT-4 has its lowest success rate of 54.3% when fixing the code it generated. This evidence suggests that large language models tend to experience a decline in performance when attempting to fix vulnerabilities generated by themselves. However, this finding needs to be tested across a broader spectrum of scenarios to ensure its validity. This insight is particularly interesting and noteworthy, as it sheds light on the limitations of LLMs in improving the content that they generated.\nB. Scenario-relevant Analysis\nFigure 12 illustrates the CWE scenarios in which GPT-3.5 and GPT-4 successfully repaired all vulnerabilities in the code generated by the four LLMs. GPT-3.5 successfully repaired all the vulnerable code snippets in 12 CWE categories, while GPT-4 achieved this in 26 CWE categories, with 11 CWEs being common to both models. It is evident that GPT-3.5's coverage of success repair is nearly a subset of GPT-4's, indicating that GPT-4 was able to address significantly more CWE scenarios than GPT-3.5. This highlights GPT-4's superior capability in repairing security vulnerabilities compared to GPT-3.5, reaffirming its effectiveness in enhancing code security."}, {"title": "6 RQ4: HOW EFFECTIVE IS AN ITERATIVE STRATEGY IN IMPROVING LLMS' REPAIR CAPABILITY?", "content": "RQ2 and RQ3 respectively represent the stages of code review and refactoring of the end-to-end procedure. However, one single round may not be sufficient to address all security issues, as indicated in RQ3. Therefore, in RQ4, we investigate the effectiveness of implementing an iterative strategy that repeatedly conducts vulnerability detection and repair to enhance LLMs' repair capability.\nSince this process produces a large amount of generated and repaired code, it is impractical to manually review all of it. Alternatively, we developed an automated tool implementing the algorithm described in 2.3.2. As depicted in Table 4, our tool for RQ4 consists of a code generator, a vulnerability scanner, and a vulnerability repairer. The roles of the code generator and repairer are performed by the LLMs being evaluated. Instead of using LLMs as scanners, we utilize reliable semantic code analysis engines (CodeQL and Bandit) in the tool, given that GPT-3.5 and GPT-4 have demonstrated their inability to correctly identify vulnerabilities in RQ2.\nThe tool takes files containing code generation tasks as input. It first calls the API of the LLM which serves as the generator to produce code snippets based on the task description. Its scanner then scans the generated code files for weaknesses according to CWE specifications. A piece of generated code is deemed vulnerable by the tool if reported as insecure by either of the two engines:\n$\\text{Vul}(x) = \\begin{cases} \\text{True} & \\text{if } \\text{Codeql}(x) || \\text{Bandit}(x) \\\\ \\text{False} & \\text{else} \\end{cases}$ (5)\nCode snippets free of weaknesses will be output as secure code, while those found to have vulnerabilities are returned to the LLM who serves as repairer for remediation, with CWE information of the weaknesses provided. This scan-and-repair process is conducted in an iterative manner until all code is regarded as secure code or the predefined maximum iterations are reached. This tool is available in the public repository of our work.\nAs outlined in 2.3.3, CodeQL is capable of scanning for 26 CWEs in SecurityEval, corresponding to 67 code generation tasks, while Bandit's coverage remains undisclosed to us. To align with the detection capabilities of the automated analysis tools, we used only the 67 out of 121 code generation tasks from the SecurityEval dataset that are directly analyzable by CodeQL in RQ4. This approach ensures that the predefined CWE risks are detectable by at least one of the two analysis tools.\nFour experimental setups were designed for RQ4: GPT-3.5 self-repairing code from GPT-3.5, GPT-4 self-repairing code from GPT-4, GPT-3.5 cross-repairing code from GPT-4, and GPT-4 cross-repairing code from GPT-3.5. We conducted 5 independent experiments for each setup to mitigate random factors. Overall, the 20 experiments produced and automatically examined about 3,500 pieces of code.\n6.1 Experiment Results\nFigure 13 depicts the results of repair across each iteration using the tool we developed."}, {"title": "6.2 Result Analysis", "content": "Across the 20 experiments (5 experiments for each of the 4 setups), CodeQL and Bandit initially identified an average of about 45 pieces of vulnerable code generated by GPT-3.5 and GPT-4. As the iterative repair process progressed, the number of detected vulnerabilities significantly decreased. After the final iteration, only an average of 10 pieces of code are still found to have weaknesses. On average, GPT-3.5 successfully repaired 65.9% of the vulnerable code snippets it generated and 67.6% of those generated by GPT-4. In comparison, GPT-4 repaired 85.5% of its own generated vulnerable code and 77.4% of GPT-3.5's. These success rates are significantly higher than those observed in RQ2, where only a single repair attempt was made. It is important to note that the numbers of vulnerable code may not be entirely accurate, as they were derived from automated engines and have not undergone manual review. Nonetheless, the results highlight feedback-driven self-iterative repair as a promising approach for LLMs to enhance security in the code they have generated.\nIt is also noteworthy that the reduction in the number of vulnerable code snippets slows down considerably after the second repair iteration, indicating that the efficacy of iterative repairs is nearing its limit. While iterative repair does improve the success rate of repair, it becomes evident that excessive iterations contribute little to enhancing the overall repair efficiency. Moreover, as the number of iterations increases, deviations from the original task specifications may accumulate, ultimately leading to a degradation of functionality. Additionally, excessive iterations can be time-consuming and expensive, potentially adding extra costs to the software development process. This highlights a critical trade-off between code functionality, security, and development efficiency."}, {"title": "7 IMPLICATIONS AND DISCUSSIONS", "content": "Our study identifies several important implications and suggestions for the research of large language models for code and vulnerability repair.\nA. The need for a larger coverage of semantic code analysis engines for vulnerability detection\nSemantic code analysis engines such as CodeQL are renowned for their reliability in identifying vulnerabilities. These tools, driven by manually written query scripts targeting specific weaknesses, typically exhibit low false positive rates in practical scenarios. Therefore, they are widely used in research for software security [28, 29]. However, current engines fall short in terms of their coverage (i.e., the number of detectable CWEs). Consequently, many studies, including ours, resort to manual code review to ensure comprehensive coverage, albeit at the expense of time and effort. Expanding the coverage of these analysis engines would significantly boost the efficiency and reproducibility of relevant research endeavors.\nB. LLMs' awareness of security risks\nIn RQ1, it was observed that large language models produced a significant amount of insecure code when tasked with scenarios involving specific security risks. However, the result does not necessarily imply that LLMs are incapable of generating more secure code. One piece of evidence is their ability to correct many of the vulnerabilities present in their generated code when prompted to do so. We posit that the production of vulnerable code by LLMs largely stems from their lack of awareness regarding security issues, as they primarily prioritize fulfilling functional requirements. In real-world scenarios, software developers do not always provide LLMs with information about relevant risks. Therefore, it is crucial to enhance the scenario-relevant security awareness of LLMs, particularly that of code language models which are designed for code-related tasks. Additionally, it is recommended that users explicitly include brief descriptions of potential security weaknesses in prompts to guide LLMs in preventing them.\nC. Self-repair \"blind spots\" of LLMs\nAn intriguing observation from RQ3 is that both GPT-3.5 and GPT-4 achieve their lowest success rates when repairing code generated by themselves (as revealed in Table 3). This suggests that similar to human programmers who tend to overlook the weaknesses in their self-written source code, LLMs also exhibit \"blind spots\" in code self-repairing. We presume that the phenomenon exists because LLMs are too dependent on the programming patterns learned from their training stage that they tend to \"insist\" on these patterns rather than exploring alternative approaches, leaving vulnerabilities unfixed when prompted to address self-produced weak code. Conversely, when fixing insecure code generated by other models, a large language model can leverage its unique patterns to address weaknesses that other LLMs may overlook, resulting in a slightly higher success rate of repair.\nD. General large language models versus code language models\nIn the studies of RQ2, RQ3, and RQ4, we excluded Code Llama and CodeGeeX2 due to their inability to generate responses coherent with our prompts. Although these two language models achieve remarkable results on code generation tasks [9], they perform poorly on other code-related tasks such as vulnerability detection and repair, often generating either garbled code or self-conflicting responses. In contrast, general-purpose large language models like GPT-3.5 and GPT-4 can comprehend prompts for detection and repair, thus generating satisfactory results. The disparity may be attributed to the fact that larger-scale language models are trained on extensive natural language datasets, which enables them to comprehend prompts and generate coherent responses. Accordingly, future frameworks for automated secure code construction may either leverage the advantages of general-purpose large language models or utilize specialized models that have been fine-tuned for vulnerability detection and repair."}, {"title": "8 THREATS TO VALIDITY", "content": "1) Reproducible Code Generation. As generative models, LLMs used in this work are unable to produce completely reproducible output. Given the time-consuming nature of manual code review, we instructed the LLMs to generate only one output for each task (otherwise the amount of code to review would multiply). Consequently, our results may be affected by random factors, as LLMs can produce different outputs when given the same prompt. We contend that such influence is minimal, as all results were generated under default parameters with medium model temperatures. To alleviate doubts, we particularly had GPT-4 generate three parallel outputs for each generation task in RQ1. Manual review confirms that GPT-4 consistently produced similar results across these outputs7.\n2) Choice of the Dataset. The SecurityEval dataset used in our work was released two years prior to our research and might have been included in the training data of LLMs. Despite this possibility, all 4 tested LLMs exhibit poor performance in terms of security quality when assessed against this benchmark. Therefore, the security problem of large language models for code remains an open challenge. Furthermore, our conclusions drawn from the experiments with SecurityEval maintain their validity and relevance, as they"}, {"title": "9 RELATED WORK", "content": "(1) Vulnerabilities in LLM-generated code. With large amounts of code being generated by LLMs and deployed (sometimes without thorough examination) into production environments every day, their security has become a significant concern for both academia and industry. Pearce et al. evaluated the security of C and Python code generated by GitHub Copilot across 25 CWE cases, finding that 40% of the code was vulnerable [28]. Similarly, Khoury et al. assessed GPT-3.5's ability to generate code in multiple programming languages for security-critical scenarios and found it failed to meet secure coding standards in 16 out of 21 tasks [21]. Additionally, Nair and coauthors demonstrated that ChatGPT produces insecure hardware code if not carefully prompted [24]. A more recent study by Tihanyi et al. examined the security of C code generated by GEMINI-pro, GPT-4, and other models, revealing that at least 63.47% of the generated programs were vulnerable [34], which is a number close to our findings on Python. These results highlight the inability of current large language models to consistently generate secure code without elaborately designed prompts.\n(2) LLMs for detecting vulnerabilities in real-world code. Recent research has increasingly focused on the direct application of LLMs in enhancing code security, particularly in the areas of vulnerability detection and repair [41]. Fu et al. investigated the ability of LLMs to detect and classify weaknesses in real-world code [19]. Purba et al. applied 4 well-known LLMs to detect vulnerabilities in 2 datasets (code gadgets [23] and CVEfixes [16], both derived from real-world programs). They found a significant performance gap between the studied LLMs and static analysis tools, primarily due to the high false positive rates of LLMs [30], a result consistent with our conclusion in RQ2. Other research also highlighted the limitations of current LLMs in vulnerability detection compared to static analysis tools or specially trained, deep learning-based models [33, 39]. Contrary to these findings, some researchers have observed LLMs' superiority in specific experimental settings. Zhou et al. [42] and Akuthota et al. [15] reported better performance of LLMs in certain scenarios. Ullah et al. designed SecLLMHolmes, an"}, {"title": "10 CONCLUSIONS AND PERSPECTIVES", "content": "In this paper, we seek an answer to the question of how well large language models serve as end-to-end secure code producers. We first investigate the vulnerabilities present in Python source code generated by GPT-3.5, GPT-4, Code Llama, and CodeGeeX2 on the SecurityEval benchmark. Subsequently, we explore LLMs' potential to independently enhance the security of the code through code self-review and vulnerable code self-repair. Overall, we manually review 1,452 pieces of code (in RQ1 and RQ3) and automatically examine approximately 4,900 pieces of code (in RQ1, RQ3, and RQ4).\nOur study reveals several key findings: (1) large language models tend to generate insecure code in security-critical programming tasks because of their shortage of scenario-relevant awareness of potential risks; (2) large language models such as GPT-3.5 and GPT-4 are not capable of accurately identifying vulnerabilities in the source code they produce, primarily due to their high false positive rates; (3) advance LLMs can achieve up to a 60% success rate repairing insecure code generated by other LLMs, but they exhibit relatively poor performance when repairing self-produced code; (4) Leveraging semantic code analysis engines, a feedback-driven self-iterative repair approach of LLMs significantly enhances the security of LLM-generated code.\nWhile we hold the belief that future large language models have the potential to produce secure code in an end-to-end fashion, current models are unable to accurately fix vulnerable code without assistance from established tools like semantic code analysis engines."}, {"title": "A APPENDIX", "content": "A.1 Detailed results of RQ2\nA.2 Detailed results of RQ"}]}