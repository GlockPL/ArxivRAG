{"title": "RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?", "authors": ["Haotian Xu", "Xing Wu", "Weinong Wang", "Zhongzhi Li", "Da Zheng", "Boyuan Chen", "Yi Hu", "Shijia Kang", "Jiaming Ji", "Yingying Zhang*", "Zhijiang Guo", "Yaodong Yang", "Muhan Zhang", "Debing Zhang*"], "abstract": "Can scaling transform reasoning? In this work, we explore the untapped potential of scaling Long Chain-of-Thought (Long-CoT) data to 1000k samples, pioneering the development of a slow-thinking model, RedStar. Through extensive experiments with various LLMs and different sizes, we uncover the ingredients for specialization and scale for Long-CoT training. Surprisingly, even smaller models show significant performance gains with limited data, revealing the sample efficiency of Long-CoT and the critical role of sample difficulty in the learning process. Our findings demonstrate that Long-CoT reasoning can be effectively triggered with just a few thousand examples, while larger models achieve unparalleled improvements. We also introduce reinforcement learning (RL)-scale training as a promising direction for advancing slow-thinking systems. RedStar shines across domains: on the MATH-Hard benchmark, RedStar-code-math boosts performance from 66.2% to 81.6%, and on the USA Math Olympiad (AIME), it solves 46.7% of problems using only 21k mixed-code-math datasets. In multimodal tasks like GeoQA and MathVista-GEO, RedStar-Geo achieves competitive results with minimal Long-CoT data, outperforming other slow-thinking systems like QvQ-Preview. Compared to QwQ, RedStar strikes the perfect balance between reasoning and generalizability. Our work highlights that, with careful tuning, scaling Long-CoT can unlock extraordinary reasoning capabilities-even with limited dataset and set a new standard for slow-thinking models across diverse challenges. Our data and models are released at https://huggingface.co/RedStar-Reasoning.", "sections": [{"title": "Introduction", "content": "LLM-driven Slow-thinking reasoning systems, represented by models like o12 and its variants-DeepSeek-R13, k0-math\u2074, Macro-01 [1], and QwQ\u2014have opened up new avenues for addressing intricate reasoning challenges. At the core of these advancements is the long chain-of-thought (Long-COT) paradigm, which prioritizes methodical, step-by-step reasoning over quick, shallow responses. This approach has proven particularly effective in tasks such as mathematical problem-solving, where structured, logical steps are essential for accurate conclusions. Long-COT's success stems from its ability to break down intricate problems into smaller, manageable sub-tasks, mirroring human cognitive processes and fostering deeper, more analytical reasoning.\nRecent advancements in slow-thinking reasoning systems can be categorized into tree-search-based and distillation-based methods. Tree-search approaches use data synthesis and search strategies to explore diverse reasoning paths [1-3], while distillation methods focus on refining performance by fine-tuning with high-quality reasoning chains [4\u20137]. Both approaches leverage synthetic Long-COT data to enhance models' step-by-step reasoning capabilities. However, despite their contributions, the scalability of Long-COT datasets\u2014both in terms of volume and quality has not been thoroughly investigated. Additionally, key factors such as multi-source data synthesis, model capacity, effective training strategies, and multimodal integration remain underexplored. These limitations hinder the broader adoption of Long-COT for optimizing reasoning performance across diverse and complex tasks.\nIn this paper, we investigate the potential of Long-COT from the perspective of data scaling up in Question-Box 1. We attempt to answer whether there exists a scaling effect in constructing a large volume of Long-COT from general data, specifically, the influence of different model bases and model parameter scales on Long-COT scaling. We demonstrate that even small datasets, when properly synthesized from a moderate difficulty level, can lead to significant improvements in reasoning capabilities. Specifically, we curate a dataset consisting of 1.3k prompts and 4k prompt-response pairs, which are used to fine-tune reasoning models in a supervised manner. The results indicate that Long-COT fine-tuning yields substantial performance improvements across a range of reasoning tasks.\nWe first explore the impact of varying data sizes and model configurations on Long-COT scaling tuning. Our findings show that larger datasets lead to significant gains in performance, particularly in complex reasoning tasks. Smaller models, such as the 7B-Instruct, benefit substantially from larger datasets, while larger models, like the 14B-Instruct, show notable improvements even with smaller datasets. This underscores the importance of both data scaling and model capacity in optimizing reasoning performance, suggesting that tailored data sizes can enhance training efficiency depending on the model's capabilities."}, {"title": "Long-COT Data Curation", "content": "In order to effectively train models on long-chain-of-thought (Long-COT) reasoning tasks, we curated a high-quality dataset that encompasses a wide range of mathematical problem-solving scenarios. The process involved data crawling, augmentation, and rigorous verification to ensure that the dataset met the specific requirements for effective reasoning at scale. This section details the methodology employed in assembling and refining the dataset used for training Long-COT models, which plays a critical role in enhancing the performance of such models in complex reasoning tasks. The following content describes the curation and augmentation of the dataset used for Long-COT training, derived from the Metamath-qwen2-math dataset.\nPrompt Collection Metamath-qwen2-math Overview: Metamath-qwen2-math dataset contains approximately 900k math problems, each with a chain-of-thought (CoT) solution. It utilizes prompts"}, {"title": "Scaling Data and Models for Long-COT Efficiency", "content": "We investigate the impact of varying Long-COT dataset sizes and model scales on reasoning performance, focusing on mathematical tasks. The evaluation datasets include: Math-hard, Olympiad-Bench, College_Math, High_School_League-24, and AIME24.\n\u2022 Math-hard includes the level 5 results from the MATH dataset, totaling 1.3k problems[10].\n\u2022 Olympiad-Bench consists of English-language Olympiad-level math problems from Olympiad-Bench[11], totaling 675 test prompts.\n\u2022 College_Math focuses on college-level math problem solving, as proposed by [12].\n\u2022 High_School_League-24 contains 2024 Chinese high school mathematics league problems from [9], used to assess out-of-distribution (OOD) performance.\n\u2022 AIME24 is a widely used benchmark for evaluating math problem-solving performance."}, {"title": "Long-COT Data Scaling Performance and Efficiency Analysis", "content": "The results in Table 2 clearly demonstrate that increasing the scale of Long-COT data leads to significant performance improvements across various tasks.\nWith the incorporation of Long into the training process, the model's reasoning capabilities can achieve continuous improvement, particularly when addressing difficult and complex problems. For example, the performance of the 7B-Math-Instruct model improves from 36.8 to 41.0 with 4k Long-COT examples, and further increases to 45.4 when the data scale reaches 1000k examples. The 14B-Instruct model also benefits from data scaling, achieving average scores of 35.8, 44.3 across the same data configurations.\nAn interesting pattern emerges when comparing sample efficiency across different models. While the overall performance gain from 0 to 1000k examples is around 10.0 for all models, the contributions of smaller and larger datasets differ. From 0 to 4k examples, the smaller 7B-Instruct model shows minimal improvement, while the larger 14B-Instruct and 7B-Math-Instruct model achieves a significant improvement of 8.5 and 4.2."}, {"title": "Model Specialization or Parameter Scaling Improves Performance", "content": "The results further highlight that model capability plays a crucial role in enhancing performance. Larger and more specialized models consistently outperform smaller or general-purpose models, particularly in tasks that require complex reasoning. For instance, the 14B-Instruct model achieves an average performance of 45.4 with 1000k Long-COT data, outperforming the 7B-Instruct model (41.7) and closely matching the specialized 7B-Math-Instruct model (45.4) at the same data scale.\nAdditionally, the math-specialized 7B-Math-Instruct model consistently outperforms the general-purpose 7B-Instructmodel, demonstrating the significant benefits of domain-specific pretraining. For example, on the Math-hard dataset, the 7B-Math-Instruct model achieves 73.3, surpassing both 7B-Instruct (71.3) and the 14B-Instruct (73.4). This performance gap remains consistent even as the Long-COT dataset size increases, indicating that task-specific tuning plays a critical role in boosting a model's reasoning capabilities.\nThese findings emphasize that increased model capacity-whether through a larger number of parameters or specialized training-leads to better performance. This is particularly evident in datasets that demand advanced reasoning, such as High_school_league and AIME24, where larger models outperform their smaller counterparts with greater ease. Furthermore, models with domain-specific tuning exhibit more robust and consistent improvements across all benchmark categories, showcasing the importance of scale and specialization."}, {"title": "Reinforcement Learning Brings Further Improvement", "content": "We further explore the effectiveness of three commonly used reinforcement learning (RL) algorithms to enhance reasoning performance:\nDirect Preference Optimization (DPO): DPO[13] aligns language models with human preferences by optimizing the model directly based on preference data, bypassing the need for explicit reward modeling.\nProximal Policy Optimization (PPO): PPO[14] is a widely used RL algorithm that balances exploration and exploitation by adjusting policies within constrained updates.\nREINFORCE++: REINFORCE++[15] improves the classic REINFORCE algorithm by reducing variance and improving convergence speed.\nRewards of RL: To perform RL, we propose to use rule-based RM which is used to filter out positive/negative responses in data curation. The dataset utilized in Reinforcement Learning (RL) is a"}, {"title": "Experiment Settings", "content": "For our experiments, we selected the Qwen-32B-Instruct model and utilized the transformers 12 library for training. To mitigate memory overflow issues with the 32B model at a 16K context length, we employed context parallelism via Deepspeed-Ulysess13, ensuring efficient training without sacrificing long-context handling.\nConsidering the performance gains from scaling math-derived Long-COT data from 4k to 1000k have diminished noticeably for the 14B model, for the 32B model, we limited the math-derived Long-COT data to 4k samples. To further diversify the training data, we supplemented this with a 16k code-derived Long-COT dataset. This dataset incorporates reflective prompts and filtered responses to enrich the training process.\nAdditionally, since DPO and PPO produced comparable results in the 14B experiment, but PPO required significantly more resources, we chose DPO as the RL training algorithm for the 32B model."}, {"title": "Main Results", "content": "The results in Table 4 reveal several key insights:\n\u2022 Comparison between Qwen and RedStar models: The RedStar models outperform the Qwen models across most tasks, demonstrating the effectiveness of Long-COT tuning. For instance, RedStar-math achieves an average score of 54.6, which is higher than Qwen-QwQ-32B's 56.4, particularly excelling in tasks such as High_school (56.7) and Aime24 (36.7). On the other hand, RedStar-DPO, which combines reinforcement learning, achieves the highest average score of 58.3, indicating the benefit of incorporating advanced training techniques to improve reasoning quality, especially in more complex reasoning tasks.\n\u2022 Impact of Long-COT: Long-COT tuning significantly boosts performance across multiple domains for RedStar models. RedStar-DPO, which uses both math and code Long-COT datasets, achieves the highest average score (58.3), showcasing the synergy of combining diverse datasets. Specifically, RedStar-code, trained exclusively on code Long-COT, achieves an average score of 43.6, surpassing Qwen2.5-32B-Instruct (37.9). When math datasets are integrated (RedStar-code-math), the average score increases to 55.7, demonstrating the cross-domain benefit of using Long-COT for both math and code tasks. This integration of datasets further strengthens the model's ability to handle complex, multi-faceted reasoning scenarios.\n\u2022 Task-specific performance: RedStar-DPO stands out in task-specific performance, particularly excelling in Math-hard (83.5) and Aime24 (53.3). This reinforces the importance of specialized reinforcement learning strategies, such as DPO, for refining model reasoning in specific domains. RedStar-DPO's performance on these tasks highlights its ability to handle difficult, multi-step reasoning challenges, a task that other models, including Qwen-QwQ-32B, struggle with. This indicates that task-specific fine-tuning can further enhance a model's capabilities and make it more efficient at handling particular types of reasoning problems.\n\u2022 OOD performance: In table 6, we use the latest Chinese Graduate Entrance Mathematics Test datasets which is an OOD dataset for all models to test the OOD performance. RedStar achieves comparable results compared to closed-source apis including DeepSeek-R1, Kimi-Math and open-sourced models inculding QwQ. Since we only use the English datasets to enhence the reasoning abilities of the instruct model, it shows remarkable language transfer capabilities on Chinese OOD-tests."}, {"title": "Impact on General Foundational Capability", "content": "To evaluate the impact of long-COT training on general foundational capabilities, we conducted a comprehensive comparison across multiple benchmarks covering STEM, reasoning, CommonSense, factuality, long-text generation (Hellobench [16]), and SedarEval [17] tasks. These benchmarks provide a comprehensive view of each model's performance in handling general tasks. Table 5 summarizes the aggregated scores across these categories for Qwen2.5-32B-Instruct, Qwen-QwQ-32B, and RedStar.\n\u2022 Overall Performance: RedStar achieves the highest average score (68.8), outperforming Qwen2.5-32B-Instruct (68.1) and Qwen-QwQ-32B (62.8), demonstrating its balanced capabilities across multiple domains.\n\u2022 STEM Strength: In STEM tasks, RedStar achieves a score of 79.2, performing closely to Qwen2.5-32B-Instruct (80.1) and significantly outperforming Qwen-QwQ-32B (71.7), demonstrating its robustness in maintaining proficiency across understanding, computation, knowledge integration, and cross-disciplinary applications.\n\u2022 Reasoning Superiority: RedStar leads in reasoning tasks with a score of 73.1, slightly surpassing Qwen-QwQ-32B (73.0) and notably outperforming Qwen2.5-32B-Instruct (65.5). This highlights RedStar's effectiveness in complex multi-step reasoning.\n\u2022 CommonSense Performance: RedStar demonstrates a solid performance in CommonSense tasks, scoring 65.8, surpassing Qwen-QwQ-32B (60.4) but trailing behind Qwen2.5-32B-Instruct (68.1). This suggests that RedStar still requires further improvements to maintain its common sense performance.\n\u2022 Factuality Improvements: RedStar scores highest in factuality tasks (36.0), outperforming Qwen2.5-32B-Instruct (34.0) and Qwen-QwQ-32B (21.0), underscoring its accuracy and consistency in producing factual outputs.\n\u2022 Long Generation (Hellobench [16]): RedStar achieves strong performance in long-text generation tasks, with a score of 87.1, closely matching Qwen2.5-32B-Instruct (88.0) and outperforming Qwen-QwQ-32B (81.0). This highlights its ability to generate coherent and meaningful long-form text across diverse tasks."}, {"title": "One More Step: Applying Slow-Think to MLLM", "content": "To verify whether Long-COT also provides similar benefits in multimodal reasoning scenarios, we conducted a study on geometric reasoning, widely recognized as a challenging multimodal mathematical reasoning task.\nWe used the GeoQA [18] training set with 3,499 samples, constructing multiple-choice prompts and filtering out non-reflective responses. We also excluded samples with excessively long wait times to improve model efficiency. We ultimately tested on the GeoQA test set and two out-of-distribution (OOD) geometric benchmarks: MathVista-GEO and Geometry3K [19]. In addition to the GeoQA test set, MathVista-GEO [20] includes geometric proof problems such as UniGeo and scenarios with rich visual elements like Geometry3K. Geometry3K, in particular, is a benchmark with relatively rich geometric illustration elements, making end-to-end solving by large models more challenging.\nWe performed distillation based on the QwQ data, with 8 samples taken for each question, and applied a strategy similar to our pure-text approach for response validation. We selected Intern2VL-8B as the base model for Long-COT instruction fine-tuning.\nTo demonstrate the advantages of LongCoT over ShortCoT in multimodal complex reasoning, we compared the results of direct fine-tuning on other large-scale multimodal geometric instruction sets. Geo-Intern2VL-8B and G-LLAVA-7B are trained on a dataset 30 times the size of our instruction set, which includes 170K augmented through data augmentation from GeoQA and Geometry3K. Math-LLaVA, on the other hand, uses a multimodal ShortCoT instruction set derived from the combination of multiple image domains' CoTs, totaling over 800K instructions. The results in Table 7 demonstrate the substantial advantages of applying Long-COT to MLLMs:\n\u2022 Superior Performance of RedStar-Geo-8B: The performance of a model fine-tuned using a small set of long-CoT instructions synthesized solely from GeoQA-train surpasses that of advanced models such as GPT-40/V, as well as fine-tuned models trained on the same dataset size and base model.\n\u2022 Efficiency and Scalability: Compared to the short CoT generated by incorporating large-scale data synthesis and enhancement strategies, the multimodal long CoT achieves a performance similar to"}, {"title": "Related Work", "content": "Slow-Thinking Reasoning In the realm of cognitive science, it has been established that the human brain operates through two distinct functional systems: System-1 and System-2. Recent progress in Computer Vision (CV) and Natural Language Processing (NLP) has facilitated the development of System-1 models, which are adept at rapid cognition and have been applied in a variety of straightforward and efficient methodologies. Nonetheless, tasks demanding intricate reasoning and advanced cognitive capabilities are contingent upon the deliberate and measured processes of System-2. Slow thinking is a distinctive characteristic of the System-2 cognitive system. Unlike directly generating answers through autoregressive methods or other immediate processes, models employing slow thinking can produce outputs more deliberately and cautiously during reasoning tasks.\nThe Slow-Thinker LLM has garnered widespread attention with the emergence of the o1 model and test-time scaling techniques, as compared to directly performing next-token prediction. Currently, various technical approaches have emerged to replicate the performance of 01, with efforts being made to establish a process for reproducing ol's performance. Corresponding work has emerged in fields such as inference, code generation [24], translation [25, 26], medical reasoning [27], and multimodal understanding [6, 28], aimed at building slow-thinking models to reduce hallucinations in language models. Advancements in slow-thinking reasoning systems focus on improving reasoning through Chain-of-Thought (CoT) prompting [27, 29], reinforcement learning [30], and iterative improvement [31\u201336]. [37] proposed a roadmap for O1-like systems using reinforcement learning, emphasizing policy initialization, reward design, and iterative learning, inspired by works like AlphaGo [38]. [39] proposed a Residual-based Energy Model (ResidualEBM) with Monte Carlo Tree Search (MCTS) to enhance LLMs' mathematical reasoning by ranking decision steps, improving accuracy without additional fine-tuning or human feedback. [1] introduced Marco-O1, extending reasoning to open-ended tasks by integrating CoT fine-tuning with MCTS. [5] developed STILL-2, a framework combining imitation, exploration, and self-improvement for training slow-thinking models, drawing from curriculum learning [40]. These works highlight the role of CoT and reinforcement learning in scaling reasoning systems and optimizing step-by-step problem-solving.\nWhile previous works focus on data synthesis improvements and multi-path reasoning strategies, our work diverges by exploring the key aspects of applying long-CoT tuning to build slow-thinking reasoning systems. Specifically, we explore the key roles of model and data scaling, as well as reinforcement learning, in achieving significant reasoning gains. Importantly, we further extend our research to the field of multimodal complex reasoning, represented by geometry reason[18, 21], demonstrating how integrating visual and textual reasoning can enhance the versatility and robustness of reasoning model training."}, {"title": "Conclusion and Future Work", "content": "In this work, we explored the training of slow-thinking reasoning systems, highlighting the effectiveness of long chain-of-thought (Long-COT) tuning in improving reasoning performance with limited data. Our results show that Long-COT can be effectively triggered with minimal samples, yielding significant scale-dependent gains. We also demonstrated that Long-COT tuning does not hinder general task performance and that reinforcement learning (RL)-based training can further enhance scalability. Additionally, Long-COT methods applied to vision-language models (VLMs) resulted in notable improvements across multimodal tasks.\nFuture work will focus on synthesizing high-quality Long-COT datasets using only instruction-tuned (Instruct) models. This approach could increase the accessibility of Long-COT training and enhance reasoning capabilities with more scalable and automated data generation. We also plan to extend Long-COT to additional complex benchmarks to evaluate its generalization potential further."}, {"title": "A Bitter Failure: Step-by-Step Verification", "content": "In an effort to enhance the accuracy of long chain-of-thought (long-COT) reasoning, we introduced a step-by-step verification process to assess the correctness of the reasoning steps generated by the model. This process involves a first-error-step detection mechanism, which uses model voting to flag errors in the reasoning path[45]. To address the inherent challenges of verifying long-COT solutions, we proposed a distant-verification method. Initially, a compact solution is extracted from the long-COT reasoning. Then, we aplpy rejection sampling to filter out solutions with incorrect answers, followed by applying the first-error-step detection method[45] to identify and remove erroneous solutions. We performed five sampling iterations per solution, generating two versions of the dataset: a strict version (no detected errors) and a loose version (allowing at most two detected errors). To further analysis the relationship between critic ability and the final performance, we use Qwen2.5-14B-Instruct as weak-critic and 01-mini as strong-critic. All experiments are conducted using the Qwen2.5-32B-Instruct model with 1.3k prompts.\nDespite the promising nature of the step-by-step verification process, as shown in Table 8, the results revealed that the proposed verification method did not significantly outperform the no-verifier baseline across most tasks. The use of both strict and loose verification rules with the Qwen-14B verifier did not yield notable improvements, especially on more complex problems. While 01-mini [46] is considered the best critic model for error detection in reasoning steps, it still struggled to effectively identify errors in the solutions generated by Long-COT for more challenging problems. This indicates that the current verification methods have limitations, especially when scaling to more difficult reasoning tasks. The failure to achieve substantial improvements suggests that further work is required to develop more robust and scalable oversight mechanisms that can handle the complexity of Long-COT reasoning across various domains.\nThe lack of significant improvement with step-by-step verification could be attributed to several factors. First, the Long-COT process itself, which breaks down problems into multiple steps, may inherently reduce the model's susceptibility to errors that verification methods are designed to detect. Additionally, the challenge of verifying multi-step reasoning in a manner that generalizes across diverse tasks remains an unresolved issue. Moreover, the sensitivity of error detection models like 01-mini to more intricate reasoning patterns may need further refinement, especially when applied to complex reasoning tasks outside the scope of its initial training.\nOverall, while step-by-step verification shows promise, its current implementation did not lead to the expected gains in reasoning performance, suggesting that additional advancements in verification techniques and dataset quality are necessary to fully harness the potential of Long-COT reasoning systems."}, {"title": "RL Experimental Setups", "content": "The experimental configurations for each algorithm are summarized in Table 9."}]}