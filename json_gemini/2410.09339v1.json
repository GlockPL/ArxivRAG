{"title": "Advanced Gesture Recognition in Autism: Integrating YOLOv7, Video Augmentation and VideoMAE for Video Analysis", "authors": ["Amit Kumar Singh", "Trapti Shrivastava", "Vrijendra Singh"], "abstract": "Deep learning and advancements in contactless sensors have significantly enhanced our ability to understand complex human activities in healthcare settings. In particular, deep learning models utilizing computer vision have been developed to enable detailed analysis of human gesture recognition, especially repetitive gestures which are commonly observed behaviors in children with autism. This research work aims to identify repetitive behaviors indicative of autism by analyzing videos captured in natural settings as children engage in daily activities. The focus is on accurately categorizing real-time repetitive gestures such as spinning, head banging, and arm flapping. To this end, we utilize the publicly accessible Self-Stimulatory Behavior Dataset (SSBD) to classify these stereotypical movements. A key component of the proposed methodology is the use of VideoMAE, a model designed to improve both spatial and temporal analysis of video data through a masking and reconstruction mechanism. This model significantly outperformed traditional methods, achieving an accuracy of 97.7%, a 14.7% improvement over the previous state-of-the-art. This demonstrates the efficacy of VideoMAE in extracting and learning robust features for the classification of complex repetitive gestures.\nAdditionally, the research incorporates YOLOv7 (You Only Look Once, version 7), a real-time object detection model that enhances the precision of detecting and segmenting objects within video frames. YOLOv7, known for its speed and efficiency. In this study, YOLOv7 helps preprocess the videos by identifying and masking the largest object of interest in each frame, which is crucial for focusing on repetitive gestures without distractions from irrelevant details. Another crucial aspect of the research is the use of video augmentation techniques to diversify and enrich the dataset, especially given the limited and noisy nature of SSBD videos.", "sections": [{"title": "1 INTRODUCTION", "content": "Repetitive behavioral patterns and difficulty with communication and social interaction are among the many symptoms together referred to as autism spectrum disorder (ASD) [30]. According to the CDC's Autistic and Developmental Disabilities Monitoring (ADDM) Network, about 1 in 44 American children received an ASD diagnosis in 2020 [10]. In 2018, Dr. Narendra Arora [6] carried out a research across many Indian areas and discovered that one in every eight children has at least one neurodevelopmental problem and one in every 100 people under the age of ten has autism. Opinions from pediatricians and parents are essential for the early diagnosis of ASD. New study, however, indicates that many children do not receive a conclusive diagnosis for autism until they are much older (beyond the age of three) [20]. It's still challenging to identify and diagnose ASD in children before the age of two.\nNumerous behavioral and motor characteristics, including stereotypically repetitive motions, can be used to diagnose autism. Such repeated motions are usually the result of passion, anxiety, or fury. Among the signs of autism are erratic behaviors that occur unexpectedly and for short bursts of time. Most of the time, the youngster hurts himself with these motions. Sadly, parents could fail to see these motions and fail to notice them. It is essential to identify these repeated actions in order to diagnose autism in youngsters. Psychologists must spend time and money monitoring their behavior, and the financial burden falls on the family. Recent advancements in technology and deep learning algorithms have made it feasible for artificial intelligence devices to automatically recognize self-stimulatory movements.\nThe aim of this study is to develop a model capable of recognizing children's repetitious behavior. This would assist doctors in selecting the most appropriate behavioral therapy and provide family support in remote places where access to state-of-the-art diagnostic technologies is limited.\nWith the use of contemporary technologies like computer vision and deep learning models, it may be possible to identify children's stereotypical repetitive motions in order to provide a precise diagnosis and prompt therapy intervention. This work developed a real-time computer vision and deep learning system to monitor and identify archetypal repetitive behavior in youngsters.\nWe designed a novel model by leveraging the VideoMAE model for video classification, which combines masked autoencoders for feature extraction and attention mechanisms for gesture classification, and trained this model for the detection of stereotypical repetitive autistic behaviors.\nOur trained VideoMAE model performs significantly better than prior vision-based techniques, according to experimental results on the Self-Stimulatory Behavior Dataset (SSBD) [26]. This provides a solid baseline for the detection of stereotypical repetitive autistic actions in videos."}, {"title": "2 RELATED WORK", "content": "Various approaches have been developed by researchers to examine and monitor behavior to identify autism, such as questionnaires [2, 7, 17, 23, 24], eye gaze [8, 9, 15, 16], functional magnetic resonance imaging (fMRI) [4, 11, 14, 19, 35], facial feature analysis [1,5,28] and multiple modalities in a single study [21, 22, 32]. Most of these studies aim to identify and classify ASD and typical-developed children with different modalities of data. In addition, stereotypical repetitive autistic behaviors have been used to evaluate autism, with most researchers using the SSBD dataset [26]. To evaluate autism with autistic behavior firstly, it is important to classify these stereotypical repetitive behaviors.\nIn the early years, the researcher used haar cascade for person detection and classified the gestures using basic ML or DL algorithms such as MLP, SVM, DCNN, ConvLSTM, etc., and achieved an accuracy of 79% [25]. The SSBD was proposed by Rajagopalan et al. [26, 27] and comprises movies showing autistic children going about their regular lives. They combined a histogram of optical flow with a histogram of dominating movements. Their three-way challenge headbanging, spinning, and hand-flapping classification model produced an accuracy of 76.3%. Single CNN was used to extract the features from each input sequence of headbanging videos and fed into LSTM for further classification. Deng A et al. [12] introduced the video swin transformer for analyzing autistic behaviors and combined visual features with language information and achieved an accuracy of 97%. Washington et al. [33] presented a new model and achieved the mean F1-score of 90.77%. Anish et al. [18] implemented LSTM and MobileNetV2 on SSBD for abnormal hand movement classification, achieving an F1 score of 75.2 \u00b1 0.6. Wei et al. [34] Employed MS-TCN on a modified SSBD dataset, achieving 84% weighted F1 score. They addressed the noise problem in the dataset and resolved it by replacing 11 videos from the actual dataset. Dia et al. [13] Utilized the Perceive model on SSBD and Affect-Net datasets, focusing on facial expressions in ASD children, with 74.5% accuracy. An approach for the behavioral diagnosis of ASD was created by Ali et al. [3]. During the time of their ASD diagnosis, they gathered and documented a collection of recordings of stereotypical children's behavior captured in an uncontrolled setting. The dataset contained 388 videos of 5 categories and combined of the two streams of Inflated 3D Networks (I3D) produced the greatest accuracy (85.6 to 86.04%).\nHowever, these state-of-the-art works recognize stereotypical repetitive autistic behaviors in autistic chil- dren on the SSBD dataset are not very optimized (less than 78%). We also find that the publicly available dataset has a very small amount of data with a high amount of noise due to being collected from an un- controlled environment. The initial research work did not acquire features very accurately due to noisy data.\nTo address these issues, we have implemented the VideoMAE model, which combines masked autoen- coders for improved feature extraction and attention mechanisms to optimize the classification performance."}, {"title": "3 MATERIAL AND METHODS", "content": "Among children with ASD, stereotypical repeated movements are one of the most often seen stimming activities. Three different types of repeated gestures are taken into consideration in this study: arm flipping, head banging, and spinning. These behaviors are characterized by recurrent actions that may only continue for a moment. There are several ways in which arm flapping might appear as a stimming behavior: jerking fingers, clicking fingers, and massive arm movements.\nChildren with autism are susceptible to headbanging, a kind of stimming activity in which they repeatedly strike their head against something like a floor, furniture, door, etc. This might lead to self-harm. Another stimming habit is spinning, in which a child spins things or themselves. Children with autism often utilize these behaviors to control sensory input or to deal with stress or excitement. Obtaining a video dataset is the first stage in our work. The second stage involves trimming the SSBD videos, where the videos are segmented into smaller, more manageable clips. In the third stage, YOLOv7 with masking is applied to these trimmed videos, allowing for precise object detection and masking within each segment. The fourth stage focuses on applying video augmentations, which involves employing various augmentation techniques to increase the diversity and robustness of the video data. These stages collectively prepare the data for further analysis and model training, ultimately supporting the project's goals of effective behavior analysis."}, {"title": "3.1 SSBD DATASET", "content": "The study utilizes Self-Stimulatory Behavioural Data (SSBD) [26], which is specifically designed to aid in the identification and analysis of stereotypical repetitive behaviors in autistic children. It's a comprehensive dataset comprising video recordings of children exhibiting self-stimulatory stimming behaviors of three differ- ent kinds: arm flapping, head banging, and spinning (Fig 2). These \"stimming\" behaviors are characterized by repetitive or unusual movements or noises, often observed in individuals with Autism Spectrum Disorders (ASD). Parents or caregivers captured these videos in unmanaged natural environments.\nThe SSBD dataset contains a total of 75 videos. Each category includes 25 videos. However, some of the videos are not available at the given URL, due to YouTube privacy concerns. We were able to retrieve only 59 videos: 19 for Spinning, 20 for Head Banging, and 19 for Arm Flapping."}, {"title": "3.2 DATA PRE-PROCESSING", "content": "The table 1 summarizes the number of videos in each class\u2014ArmFlapping, HeadBanging, and Spinning\u2014 before and after pre-processing in the SSBD dataset. Initially, each class contains 25 videos in the original dataset. After trimming, the number of videos increases to 29 for ArmFlapping, 41 for HeadBanging, and 54 for Spinning, indicating the removal of redundant or non-informative segments. With augmentation, the dataset size significantly expands to 203, 287, and 378 videos for ArmFlapping, HeadBanging, and Spinning, respectively. This augmentation process enhances the dataset's diversity, aiding in improving model robustness and performance."}, {"title": "3.2.1 YOLOv7 [31] Architecture Overview", "content": "YOLOv7 (You Only Look Once version 7) is a state-of-the-art deep learning model for real-time object detection. The architecture of YOLOv7 builds upon the innovations of previous YOLO versions, incorporat- ing advancements to enhance accuracy, speed, and robustness. YOLOv7 maintains the fundamental design principles of the YOLO family while integrating several novel techniques to improve performance.\nFundamental Architecture\nYOLOv7, like its predecessors, is an end-to-end single neural network architecture that predicts bounding boxes and class probabilities directly from full images in one evaluation. This design contrasts with other detection systems where a preselected set of regions is processed. The architecture of YOLOv7 is designed to be fast and efficient, suitable for real-time applications.\nBackbone: CSPDarknet\nThe backbone of YOLOv7 is based on a modified version of the CSPDarknet53, a network known for its balance between efficiency and accuracy. The CSP (Cross-Stage Partial Network) structure reduces the redundancy in feature map computation, enhancing the learning capability and the inference speed. The CSPDarknet in YOLOv7 has been adjusted to optimize computational efficiency and accommodate newer hardware capabilities.\nNeck: Path Aggregation Network (PAN)\nFollowing the backbone, the network uses a Path Aggregation Network (PAN) that enhances the feature hierarchy by aggregating different scales through bottom-up and top-down pathways. This ensures rich semantic information at various resolution scales, which is crucial for detecting objects of different sizes more effectively.\nHead: Detection Layers\nThe detection head of YOLOv7 utilizes anchor boxes predefined at different scales to predict bounding boxes relative to these anchors. For each bounding box, the network predicts the center coordinates, dimensions, objectness score (the probability that a box contains an object), and class probabilities. The head processes multiple scales simultaneously, allowing for robust detection across a diverse set of object sizes.\nEnhancements in YOLOv7\n1. Automatic Bounding Box Clustering: YOLOv7 automates the selection of anchor box sizes using k-means clustering on the training dataset, improving the alignment between the model's predictions and the typical object dimensions found in the specific data.\n2. EIoU (Eliminate Intersection Over Union): YOLOv7 introduces the EloU loss, which refines the traditional Intersection Over Union (IoU) loss used in bounding box regression. This new metric helps reduce the localization errors in object detection.\n3. Mosaic and MixUp Data Augmentation: YOLOv7 employs advanced data augmentation strate- gies like Mosaic and MixUp to enhance the diversity of training examples and improve model generalization. Mosaic combines four training images into a single one, while MixUp blends two images linearly.\n4. CmBN (Cross Mini-Batch Normalization): The architecture uses a novel batch normalization technique, CmBN, which stabilizes training in smaller batch sizes by normalizing across multiple mini- batches.\n5. SPP (Spatial Pyramid Pooling): The inclusion of SPP layers allows the network to maintain spatial hierarchies, providing robustness against object scale variations.\nTraining and Inference\nTraining YOLOv7 involves using stochastic gradient descent (SGD) or Adam optimization with a specific focus on fine-tuning learning rates, weight decay, and other hyperparameters for optimal convergence. Dur- ing inference, YOLOv7 employs non-maximum suppression (NMS) to refine the detection bounding boxes, ensuring that each detected object is represented by the most accurate bounding box.\nYOLOv7's architecture represents a significant advancement in the field of real-time object detection. It successfully balances speed, accuracy, and the ability to run on various hardware platforms, making it suitable for a wide range of applications from autonomous vehicles to surveillance systems. Its comprehensive design improvements over prior models make it a formidable choice in the competitive landscape of object detection algorithms.\nHandling Multiple Detections in YOLOv7\nIn object detection tasks, especially in crowded scenes, it is common for the detection model to identify multiple bounding boxes around objects of interest. In the context of using YOLOv7 for object detection, my approach prioritizes the largest object in view, based on the area of the detected bounding boxes. This strategy is particularly useful in scenarios where the size of the object correlates with its importance or relevance to the task.\nSelection of the Largest Bounding Box\nDuring the detection phase, after the model predicts bounding boxes for objects in a frame, the following steps are implemented to select the largest box:\n1. Detection and Area Calculation: For each frame processed by YOLOv7, all detected bounding boxes along with their class probabilities and objectness scores are evaluated. The area of each bounding box is calculated using the formula:$Area = (x_2 - x_1) \\times (y_2 - y_1)$ where $x_1$, $y_1$ and $x_2$, $y_2$ are the coordinates of the top-left and bottom-right corners of the bounding box, respectively.\n2. Identifying the Maximum Area: Among all detected boxes, the one with the maximum area is selected. This selection is based on the assumption that larger objects are of higher priority:"}, {"title": "Algorithm 1 Selecting the Largest Detected Bounding Box", "content": "3: for each detection d in detections do\n5:\nconfd.con f\n7:\n8:\n9:\n11:\n1: max_area - 0\n2: max_box \u2190 None\n4: xyxy \u2190 d.bbox\n6: cls \u2190 d.cls\narea \u2190 (xyxy[2] \u2014 xyxy[0]) \u00d7 (xyxy[3] \u2013 xyxy[1])\nif area > max area then\nmax area area\n10: max_box \u2190 \u0445\u0443\u0445\u0443\nend if\n12: end for\nBounding box coordinates\nConfidence score\nClass identifier\nCalculate area\n3. Cropping the Largest Box: Once the largest bounding box is identified, it is cropped from the frame. This cropped region can then be processed further or used as the output, depending on the specific requirements of the application.\nApplication and Implications\nThis methodology is particularly beneficial in applications where the focus is on identifying and tracking the most significant object in a scene, such as in surveillance systems where tracking the largest visible person might be crucial, or in traffic monitoring where larger vehicles might require more immediate attention.\nBenefits\nSimplicity and Efficiency: This approach is computationally efficient as it does not require complex post- processing of all detected objects.\nFocus on Relevant Objects: By focusing on the largest object, the model ensures that smaller, potentially less relevant detections do not distract from the primary object of interest.\nLimitations\nOversight of Smaller Objects: This method may overlook smaller, yet important, objects in the scene, which might be crucial depending on the application.\nDependence on Scene Composition: The effectiveness of this method may vary dramatically with dif- ferent scene compositions or object distributions.\nDetailed Explanation of Masking in Object Detection\nIntroduction to Masking\nIn computer vision, masking refers to the process of isolating a specific part of an image or video frame to focus processing on that area. In the context of object detection using the YOLOv7 model, masking is used to highlight or process only the detected objects, specifically the largest object detected in the video frame as per the defined criteria."}, {"title": "Implementation of Masking", "content": "After detecting objects in each frame, the algorithm isolates the largest object detected by applying a mask. This is achieved through the following steps:\n1. Initialization: A mask is created with the same dimensions as the input frame but contains only zeros. This mask is essentially a binary image where the pixels of interest (i.e., those that belong to the largest detected object) will be set to a high value (255 in this case, representing white), and all other pixels will remain at zero (black).\n2. Drawing the Rectangle: Once the bounding box with the maximum area is determined (i.e., the largest object), a rectangle is drawn on the mask. The coordinates of this rectangle correspond to the bounding box of the largest object. The rectangle is filled completely (denoted by -1 in the thickness parameter), turning the area inside the bounding box white.\n3. Applying the Mask: The mask is then applied to the original frame to create a masked image. This step involves a bit-wise operation where the mask is used to keep the pixels within the bounding box unchanged, while all other pixels are turned to black. This highlights the largest detected object by masking out the rest of the frame.\n4. Resizing\nPurpose of Resizing Image resizing is a common preprocessing step in computer vision workflows, particularly in scenarios where uniform image sizes are required for further processing, visualization, or storage. Resizing the masked image standardizes the output, ensuring that each frame in the processed video maintains a consistent size, which is crucial for consistent visualization and could be beneficial for any subsequent analysis or machine learning tasks.\nImplementation of Resizing After the masking step, where the largest detected object in each frame is highlighted, the masked image is resized to specific dimensions. This is implemented by the VideoMAE model. Here is the detailed breakdown of this step:\nInput: masked image is the image obtained from the previous masking step, where the region outside the largest detected bounding box is blackened, focusing attention on the object.\nTarget Size: The target dimensions for the resizing are specified as (224, 224). This means the output image will be a square of 224 pixels by 224 pixels. Choosing a square shape can be particularly advantageous when dealing with data that will be input into neural networks that expect a fixed size and aspect ratio, or for creating uniformity in video frames for display purposes.\nInterpolation Method: The 'interpolation=cv2.INTER_AREA' parameter is critical for resizing. Interpolation methods determine how the pixel values are adjusted during resizing:\nINTER_AREA: This method is used for shrinking an image. It may be the preferred interpolation method when the goal is to reduce the resolution of an image while preserving the original appearance. It works by using pixel area relation, making it particularly suitable for making images smaller, as it can help avoid moir\u00e9 patterns and maintain image quality.\nBenefits of Resizing in Object Detection\nEnhancement of Image Details\nResizing the masked images to a larger size, specifically to 720x720 pixels, serves a strategic purpose in scenarios where enhancing the visibility of details within the detected object is crucial. This approach is particularly valuable when the subsequent analysis requires a more granular examination of the object's features, which might be essential for accurate classification, detailed inspection, or precise monitoring tasks.\nExplanation:\nIncreased Resolution: By enlarging the images, the approach leverages the higher pixel density to better capture and display finer details of the detected object. This is crucial when the object of interest contains features that are critical for further identification or analysis but might be lost at lower resolutions.\nEnhanced Visualization: Larger images provide a clearer view when visually inspecting the details within the object. This can be especially beneficial for presentations or when manual verification steps are involved in the workflow.\nImproved Analytical Accuracy: In applications involving detailed feature recognition, such as defect detection in manufacturing or intricate pattern recognition in wildlife monitoring, larger image sizes can allow algorithms to more accurately detect and classify subtle nuances.\nTrade-offs:// While resizing images to a larger size can enhance detail, it is important to acknowledge the trade-offs:\nIncreased Computational Load: Contrary to resizing for reduced dimensionality, increasing image size can lead to a higher computational burden. This requires more memory and processing power, which might affect the throughput of real-time systems.\nPotential Overfitting: In machine learning scenarios, higher resolution images might lead to models focusing too much on minute, possibly irrelevant details, which can cause overfitting. It's essential to balance the level of detail with the generalizability of the model.\n5. Output: The processed (masked and resized) image is then written to the output video file, ready for review or further analysis.\nBenefits of Masking in Object Detection\nFocus on Relevant Objects: Masking allows for the isolation and focus on significant objects within a scene. In applications such as surveillance, traffic monitoring, or advanced research, focusing on the largest object could be critical for behavior analysis or event detection.\nReduction of Noise: By masking out irrelevant parts of the frame, the algorithm reduces the computa- tional load for any subsequent processing and minimizes the noise that could interfere with the analysis.\nEnhanced Visualization: Masking provides a clear visual representation of the detected object, which can be crucial for presentations, further manual assessments, or when visual outputs are necessary for decision- making processes."}, {"title": "Video Augmentation Techniques applied on dataset", "content": "1. Horizontal Flip\nPurpose: Mirrors each frame of the video across the vertical axis. This augmentation helps in training models to recognize objects irrespective of their orientation, enhancing the robustness of object detection models.\nImplementation: Utilizes va.HorizontalFlip() from the vidaug library, which applies a horizontal flip trans- formation to the video frames.\nFor a frame represented as a matrix F, the horizontal flip can be represented as:\n$F_flipped(i, j) = F(i, W - j + 1)$\nWhere W is the width of the frame,i ranges over the rows, and j ranges over the columns from 1 to W.\n2. Vertical Flip\nPurpose: Mirrors each frame across the horizontal axis. Similar to horizontal flipping, vertical flipping helps models learn invariant features, useful in scenarios where vertical orientation can vary, such as aerial imagery analysis.\nImplementation: Executed through va. VerticalFlip(), flipping the frames upside down.\nFor the vertical flip, the transformation can be represented as:\n$F_flipped(i, j) = F(H - i + 1, j)$\nWhere H is the height of the frame, i ranges over the rows from 1 to H, and j ranges over the columns.\n3. Upsample\nPurpose: Increases the resolution of each frame by a specified scale factor. This is particularly useful for examining finer details within frames, which might be necessary for high-resolution applications or to test the performance of models at higher resolution levels.\nImplementation: Each frame's dimensions are increased by a factor (e.g., 1.5 times the original size) us- ing cv2.resize with cv2.INTER_CUBIC interpolation, which helps in preserving the smoothness of the image.\nWhen upscaling a frame by a factor a, the new dimensions are a original dimensions. If F has dimensions WXH, the upsample frame F' has dimensions:\n$W' = a \\times W, H' = a \\times H$\nInterpolation is used to estimate pixel values in F', typically using cubic interpolation:"}, {"title": "4. Random Rotate", "content": "Purpose: Rotates each frame by a fixed angle (e.g., 25 degrees). Rotation augments are critical for training models to detect objects at various angles, enhancing detection accuracy in uncontrolled environments.\nImplementation: The custom_random_rotate function uses OpenCV's cv2.getRotationMatrix2D and cv2.warpAffine, with the rotation center set to the frame's center and cv2.BORDER_REFLECT101 to manage edge pixels.\nFor a rotation by angle 0, the transformation matrix M can be given by: $M = \\begin{bmatrix}  \\cos \\theta & - \\sin \\theta & 0 \\\\  \\sin \\theta & \\cos \\theta & 0 \\\\  0 & 0 & 1  \\end{bmatrix}$\nA point (x,y) in the original frame is mapped to: $\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = M \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}$\nWhere (x',y')are the coordinates in the rotated frame.\n5. Invert Color\nPurpose: Inverts the color of each frame. Color inversion is useful for testing models against unusual lighting conditions and color variability, ensuring robustness against diverse environmental conditions.\nImplementation: This is achieved by subtracting each pixel value from 255 (i.e., 255 - frame), effectively reversing the color spectrum.\nThe color inversion for a pixel value p in frame F can be represented as: F_inverted (i, j) = 255 - F(i, j)\n6. Downsample\nPurpose: Reduces the frame rate of the video by a specified factor. Downsampling is used to simulate lower frame rate videos and to test the efficiency and effectiveness of models under reduced temporal resolution conditions.\nImplementation: The function selectively writes frames to the output video based on the downsample factor; for instance, writing every second frame for a factor of 2.\nIf the original frame rate is R and the downsample factor is \u03b2, the new frame rate R' is: $R' = \\frac{R}{\\beta}$ and frames are selected according to: F'(t) = F(\u03b2 \u00d7 t)\nWhere t is the frame index in the downsampled video.\nThese augmentations introduce variability into the dataset, which is essential for training robust machine learning models, particularly in object detection and video analysis tasks. By applying these transformations, models can be trained to perform well across a variety of real-world conditions, ensuring they are not only accurate but also versatile in handling different scenarios and challenges."}, {"title": "3.3 Proposed Methodology", "content": "This study uses videos to show how to recognize and classify human behavior using a robust deep learning model. Videos, on the other hand, are collections of images put in a certain order to create motion. For video categorization, a variety of techniques might be used. This study suggested using VideoMAE along with YOLOv7 and video augmentation. We provide a complete explanation of the essential elements included in our suggested models in the parts that follow, along with a detailed display of their architectural layouts."}, {"title": "3.4 VideoM\u0410\u0415 [29]", "content": "The intrinsic properties of video data, such as temporal redundancy-where successive frames often contain similar information and temporal correlation where events unfold over time-present unique opportuni- ties and challenges for machine learning models. The Video Masked Autoencoder (VideoMAE) framework leverages these properties, utilizing a strategy adapted from successes in natural language processing (NLP) and still image analysis, where masked autoencoders have demonstrated significant efficacy."}, {"title": "VideoMAE Architecture", "content": "Masking Strategy\nTube Masking: Central to VideoMAE is its novel tube masking strategy, which applies a consistent mask across the temporal dimension of video clips. This approach ensures that if a segment is masked in one frame, the corresponding segments in adjacent frames are also masked. This strategy significantly increases the reconstruction challenge, preventing the model from merely interpolating spatially adjacent frames to fill in gaps, thus encouraging the learning of robust spatiotemporal features.\nEncoder-Decoder Model\nAsymmetric Architecture: VideoMAE adopts an asymmetric encoder-decoder architecture, common in masked autoencoder designs but tailored for video. The heavy encoder processes the unmasked frames, transforming them into a rich, compressed latent space. Conversely, the lightweight decoder aims to reconstruct the original full video from this compressed representation.\nVision Transformer (ViT): Unlike many conventional video processing models that rely on 3D con- volutions, VideoMAE utilizes a plain ViT, treating video frames as sequences of flattened 2D patches (tokens). This adaptation allows the model to leverage the powerful self-attention mechanisms of trans- formers, facilitating a global understanding of video content across both spatial and temporal dimensions.\nMethodology\nSelf-Supervised Pre-training\nTraining Objective: The model is trained in a self-supervised manner by predicting the video parts that are masked out, using only the visible segments. The objective is to minimize the difference between the reconstructed video and the original unmasked video, typically measured by a reconstruction error such as Mean Squared Error (MSE).\nHigh Masking Ratio: VideoMAE employs an unusually high masking ratio (up to 95%), a strategy made viable by the redundancy in video data. This extensive masking forces the model to infer significant portions of the video, enhancing its ability to learn predictive and generative video features without relying on vast amounts of labeled data."}, {"title": "4 EXPERIMENTAL RESULT", "content": "The High-Performance Computing (HPC) - SURYA was used to train the model. The HPC infras- tructure features 16 compute nodes, we utilized one HPC node equipped with 40 CPU cores, 2 NVIDIA V100 GPUs with 24GB RAM each, and 376GB of system RAM."}, {"title": "4.1 PERFORMANCE PARAMETERS", "content": "In the discipline of deep learning, a classification report serves as a statistical gauge of performance. Its goal is to show off the classification model's accuracy, recall, F1 score, precision, and loss metrics. For multiclass classification sparse-categorical-cross-entropy loss function is used as a loss function.\nAccuracy: The accuracy of a model is determined by dividing the total number of predictions by the number of accurate predictions it made. The formula to calculate accuracy is as follows:\n$Accuracy = \\frac{\\sum_{i=1}^{N} I(Yi-Yi)}{N}$\nWhere: N is the total number of data, y; is true value of ith data, \u0177; is predicted value of ith data.\nRecall: It is the ratio of accurately recognized instances of a given class to the total number of actual instances of that class for that specific class.\n$Recall_{class_i} = \\frac{TP_{class i}}{TP_{class_i} + FN_{class_i}}$\nPrecision: It is defined as the ratio of successfully recognized instances of a given class to all instances projected to belong to that class.\n$Precision_{class i} = \\frac{TP_{class i}}{TP_{class_i} + FP_{class_i}}$\nF1 score: The F1 Score is a balance between precision and recall, calculated as the harmonic mean of the two. When there is an uneven distribution of classes, it is very useful.\n$F1 Score_{class_i} = 2 \\times \\frac{Precision_{class i} \\times Recall_{class_i}}{Precision_{class_i} + Recall_{class_i}}$\nLoss: The sparse-categorical-crossentropyloss function is best-suited loss function for multiclass classi- fication. The negative log-likelihood of the expected probability of the true class is the loss for a single observation. The average of the various losses is the overall loss over the whole dataset (or over a batch during training):\n$Sparse_Categorical_Crossentropy = - \\frac{1}{N} \\sum_{i=1}^{N} log(P_{i,y})$"}, {"title": "4.2 Ablation Studies", "content": "In our research, we conducted an ablation study to assess the impact of different hyperparameters on model performance. By systematically varying batch size, split ratio, and learning rate, we were able to evaluate their individual contributions to the overall effectiveness of the model. This analysis enabled us to identify optimal configurations, thus refining the model's performance and ensuring more robust and accurate results. The insights gained from this study were crucial for optimizing the model's architecture and guiding its further development."}, {"title": "4.2.1 For variation in Learning Rate On original SSBD data", "content": "The table 3 presents the training time and performance metrics of the VideoMAE model across various learning rates, highlighting the relationship between these parameters. As the learning rate decreases from 0.01 to 0.0001, a notable trend emerges: training time decreases significantly while accuracy improves, peaking at 0.58 with a learning rate of 0.0001. This rate also yields the highest precision at 0.81, indicating a strong ability to correctly identify relevant instances. In contrast, the learning rates of 0.01 and 0.001 show lower accuracy and precision, with longer training times of 16 hours and 15.1 hours, respectively. The learning rate of 0.00001, while still efficient in training time (5.2 hours), demonstrates a slight drop in both accuracy (0.5) and precision (0.46). Overall, the findings suggest that a careful selection of the learning rate is essential for optimizing the model's performance, balancing accuracy, precision, and training efficiency."}, {"title": "4.2.2 For variation in Batch Size On Original SSBD data", "content": "The table4 summarizes the training time and performance metrics of the VideoMAE model with varying batch sizes, illustrating the impact of batch size on model efficiency and effectiveness. As the batch size increases from 2 to 4, there is a significant improvement in accuracy, which rises to 0.83 with a training time of 5.3 hours. This batch size also achieves high precision at 0.88, indicating a strong performance in correctly identifying relevant instances. However, when the batch size is increased further to 8 and 16, the accuracy declines to 0.66, with precision values of 0.63 and 0.74, respectively, despite maintaining the same training time of 4.5 hours. The recall and F1 scores also reflect this trend, emphasizing that while larger batch sizes can enhance training speed, they may compromise the model's ability to generalize effectively. Overall, these findings suggest that a batch size of 4 is optimal for balancing training time and model performance, yielding the best accuracy and precision for the VideoMAE model."}, {"title": "4.2.3 For Variation in Split Ratio On Original SSBD data", "content": "The table5 presents the training time and performance metrics of the VideoMAE model based on different data split ratios", "60": 20, "70": 15}]}