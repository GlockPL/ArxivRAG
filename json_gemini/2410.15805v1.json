{"title": "RAG4ITOps: A Supervised Fine-Tunable and Comprehensive RAG\nFramework for IT Operations and Maintenance", "authors": ["Tianyang Zhang", "Zhuoxuan Jiang", "Shengguang Bai", "Tianrui Zhang", "Lin Lin", "Yang Liu", "Jiawei Ren"], "abstract": "With the ever-increasing demands on Question\nAnswering (QA) systems for IT operations and\nmaintenance, an efficient and supervised fine-\ntunable framework is necessary to ensure the\ndata security, private deployment and contin-\nuous upgrading. Although Large Language\nModels (LLMs) have notably improved the\nopen-domain QA's performance, how to effi-\nciently handle enterprise-exclusive corpora and\nbuild domain-specific QA systems are still less-\nstudied for industrial applications. In this pa-\nper, we propose a general and comprehensive\nframework based on Retrieval Augmented Gen-\neration (RAG) and facilitate the whole business\nprocess of establishing QA systems for IT oper-\nations and maintenance. In accordance with the\nprevailing RAG method, our proposed frame-\nwork, named with RAG4ITOps, composes of\ntwo major stages: (1) Models Fine-tuning &\nData Vectorization, and (2) Online QA Sys-\ntem Process. At the Stage 1, we leverage a\ncontrastive learning method with two negative\nsampling strategies to fine-tune the embedding\nmodel, and design the instruction templates to\nfine-tune the LLM with a Retrieval Augmented\nFine-Tuning method. At the Stage 2, an effi-\ncient process of QA system is built for serv-\ning. We collect enterprise-exclusive corpora\nfrom the domain of cloud computing, and the\nextensive experiments show that our method\nachieves superior results than counterparts on\ntwo kinds of QA tasks. Our experiment also\nprovide a case for applying the RAG4ITOps to\nreal-world enterprise-level applications.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of IT operations and main-\ntenance has become increasingly significant due\nto the rapid expansion of massive data and com-\nplex IT systems, such as in cloud computing and\ntelecommunications (Liu et al., 2023). Efficient IT\noperations and maintenance are critical for provid-\ning the high-quality performance, reliability, and\nsecurity for customers in the business area (Du\net al., 2017; Guo et al., 2024).\nTraditionally, to operate and maintain those sys-\ntems, it highly depends on IT operators' personal\nexperience, while often leading to difficulties in\nincident management, problem resolution, and\nmaintaining service quality (J\u00e4ntti and Cater-Steel,\n2017). Later with the advancements of QA tech-\nniques, some QA systems are developed, and IT op-\nerators can leverage them to retrieve useful informa-\ntion and make a plan on troubleshooting efficiently\nin a natural-language human-machine interacting\nmanner (Huang et al., 2023b; J\u00e4ntti and Cater-Steel,\n2017; Galup et al., 2009). As shown in Figure 1,\nthe two typical and important QA tasks are Knowl-\nedge Acquisition and Troubleshooting (Rijal et al.,\n2022). The former is usually for junior IT opera-\ntors to promote their experience, while the latter\nis for senior ones to obtain guidance on resolving\ndifficult software and hardware faults during their\ndaily work. Therefore, QA systems have become\ngreatly important in contemporary IT operations\nand maintenance."}, {"title": "2 Related work", "content": "IT Operations and Maintenance. Traditionally,\nthe quality of IT operations and maintenance varies\nbecause it highly depends on the IT operators' per-\nsonal experience (Notaro et al., 2020). To culti-\nvate IT operators and meanwhile manage the ever-\nincreasing IT-related information and knowledge\nwell, QA systems are essential to improve effi-\nciency across various application scenarios, de-\nveloped by leveraging the development of NLP\ntechniques (Huang et al., 2023a; Elhoone et al.,\n2020). These systems aim to help IT operators\nquickly access useful information and develop trou-\nbleshooting plans (Rijal et al., 2022). However,\nin practice, the IT operators may interact with the\nQA systems by several times to make a plan for\ndifficult tasks like troubleshooting, because current\nQA systems are not intelligent enough to provide\na comprehensive solution answer just within once\ninteraction.\nLarge Language Models. Recent LLMs have\ndemonstrated significant advancements in open-\ndomain QA tasks (Brown et al., 2020; Achiam\net al., 2023). As to those closed-source models,\nlike GPT-4, Claude and Gemini, they cannot an-\nswer domain-specific or even enterprise-exclusive\nquestions well since they do not trained on any\nprivate documents. The other thing is that those\nopen-source models, like LLaMA-3 (Touvron et al.,\n2023), Qwen, and ChatGLM3 (Zeng et al., 2022;\nDu et al., 2022), can be directly fine-tuned on\nspecific corpora and then provide QA services.\nHowever there are two major concerns. Firstly,\nLLMs often tend to generate hallucinated infor-\nmation (Guo et al., 2023), which is unbearable\nin industrial area. Secondly, faced with the ever-\nincreasing massive data, the QA systems based on\nLLMs have to be refine-tuned frequently, leading\nto a much high expense. Therefore, intuitively,\nRAG frameworks can remove the concerns and\nstrengthen the LLMs-based QA systems. A recent\neffort to develop domain-specific LLMs, such as\nOWL (Guo et al., 2024), have shown promise. But\nit still struggles to be adaptive for grounded QA\nscenarios in real-world industrial IT operations.\nRetrieval Augmented Generation. To address\nthe limitations of LLMs in factual issue and\ndomain-specific applications, the RAG framework\nhas emerged as a promising approach (Gao et al.,\n2023). RAG techniques aim to enhance the ca-\npabilities of LLMs by incorporating relevant ex-\nternal information into the input queries, thereby\nimproving the accuracy and factuality of generated\nresponses. In many domain-specific applications,\nRAG has proven highly effective for modeling\ndomain-related semantics and improving the LLMs\nto output factual and satisfactory answers (Gupta\net al., 2024; Wang et al., 2024; Zhang et al., 2024).\nRecent researches have further expanded RAG's\npotential, exploring the fine-tuning methods of pre-\ntrained LLMs specifically for RAG tasks (Lin et al.,\n2023; Zhang et al., 2024; Wang et al., 2023a; Xu\net al., 2023b). This paper also follows the idea of\nRAG, while we propose a more comprehensive and\npractical RAG framework specific for the domain\nof IT operations and maintenanc"}, {"title": "3 Methodology", "content": "To facilitate the business process of data modeling\nand model fine-tuning of QA systems for IT opera-\ntions and maintenance, we present the RAG4ITOps\nframework and introduce its details in this section.\nAs shown in Figure 2, the framework includes two\nstages. One is for offline model fine-tuning and\ndata vectorization, and the other is about the online\nQA system process based on RAG mechanism."}, {"title": "3.1 Data Preprocessing", "content": "Data preprocessing is particularly important for\nenterprise-level applications. Due to data privacy\nand data heterogeneity, a good data processing\npipeline is essential to generate high-quality dataset"}, {"title": "3.2 Instruction Template Design", "content": "To effectively guide the LLM in generating ap-\npropriate responses for different QA tasks, we de-\nsigned specific instruction templates. These tem-"}, {"title": "3.3 Stage 1: Models Fine-tuning & Data\nVectorization", "content": "With the preprocessed text chunks and two datasets,\nthe embedding model and LLM can be fine-tuned\nto better adapt for the enterprise-exclusive seman-\ntics and QA tasks. As shown in Figure 2, the\nData-EM dataset is used to fine-tune the embed-\nding model, while the Data-LLM dataset is used to\nfine-tune the LLM. Especially with the fine-tuned\nembedding model, the text chunks dataset can be\nvectorized as embeddings which are stored in the\nvector database for later online retrieval."}, {"title": "3.3.1 Fine-tuning Embedding Model", "content": "More technically, during fine-tuning the embed-\nding model, we employ the Dense Passage Re-\ntrieval (DPR) framework (Karpukhin et al., 2020)\nas our base retrieval method. DPR uses embedding\nmodels to generate dense vector representations of\nboth queries and passages, enabling efficient and\naccurate retrieval. Specifically, we begin with the\npretrained embedding model, BGE-M3 (Xiao et al.,"}, {"title": "3.3.2 Fine-tuning LLM", "content": "For fine-tuning the LLM of RAG4ITOps, we use\na state-of-the-art LLM Qwen-14b-Base (Bai et al.,\n2023) as the backbone. Also we leverage two train-\ning methods to enhance the LLM's ability.\nContinue Pre-Training With the preprocessed\ndomain-specific datasets, we aim to imbue the\nQwen-14b-base model with specialized knowledge\nin IT operations and maintenance, enhancing its\nability to understand and generate relevant content\nin this domain. The method is aligned with the\nstandard approach (Gururangan et al., 2020).\nRetrieval Augmented Fine-Tuning Method To\nenhance the LLM's ability to utilize retrieved in-\nformation in IT operations and maintenance tasks,\nwe implement a retrieval-augmented fine-tuning\napproach. Based on the Data-LLM dataset, we\nconstruct an extended training dataset (Data-LLM)\n$D = \\{(x^{(i)} \\circ I^{(i)}, y^{(i)})\\}_{i=1}^M$, where $x^{(i)} \\circ I^{(i)}$ repre-\nsents an input query $x^{(i)}$ accompanied by retrieved\nchunks $I^{(i)}$, and $y^{(i)}$ represents the output answer.\nFor each example $(x^{(i)}, y^{(i)}) \\in D$, we retrieve\nthe top-k relevant text chunks $I^{(i)} \\subset C$ based\non $x^{(i)}$. We then create the fine-tuning instances\nby combining each retrieved chunk with the ques-\ntion using an instruction template (detailed in Ap-\npendix A.3).\nThe objective function of this supervised instruc-\ntion tuning can be denoted as:\n$L_m = -\\frac{1}{M} \\sum_{i=1}^M \\mathbb{E}_{x,y,I \\in D} log P(y^{(i)}|I^{(i)} \\circ x^{(i)})$, (2)\nwhere $P(y^{(i)}|I^{(i)} \\circ x^{(i)})$ is the probability of gen-\nerating the correct output $y^{(i)}$ given the input $x^{(i)}$\naugmented with the retrieved chunks $I^{(i)}$. This ap-\nproach offers two key benefits: it adapts the LLM\nto utilize relevant and latest background knowledge,\nand it enables the LLM to generate factual answers."}, {"title": "3.4 Stage 2: Online QA System Process", "content": "At Stage 2, as shown in Figure 2, the IT operators\ncan ask a question. Then the fine-tuned embed-\nding model transforms the question into an embed-\nding and the embedding is used to retrieve relevant\ncontents from the vector database. We leverage"}, {"title": "4 Experiment", "content": "4.1 Evaluation Dataset\nWe collect a dataset called Data-Eval for evalua-\ntion. It comprises 319 questions created by domain\nexperts, among which 236 questions for the knowl-\nedge acquisition task and 83 for the troubleshooting\ntask. Each question is paired with relevant chunks\nfrom the enterprise-exclusive corpora, and all ques-\ntions have labbeled answers."}, {"title": "4.2 Baselines and Metrics", "content": "We consider the following popular text embedding\nmodels as the baselines for our embedding model\nevaluation: GTE-large-zh, BGE-M3, Text2Vec-\nbase, M3E-base, jina-embeddings-v2-base-zh, and\nBGE-large-zh-v1.5. For the LLM evaluation, our\nmethod are compared with several state-of-the-art\nlanguage models: Chatglm3-6b, Qwen-7b-Chat,\nLlama3-8B-Instruct, and Qwen-14b-Chat.\nTo evaluate the effectiveness of embedding\nmodel in the knowledge acquisition and trou-\nbleshooting tasks, we assessed performance using\nthe top-k retrieval accuracy (Acc@K). The formal\ndefinition of Acc@K can be defined as follows:\n$R(q, C) \\rightarrow \\hat{C}$ takes as input question q and chunks\nC and returns a much smaller set $\\hat{C}$, where $\\hat{C} \\subseteq C$\nand $|\\hat{C}| = k < |C|$. Top-k retrieval accuracy is the\nfraction of questions for which $\\hat{C}$ contains a span\nthat can answer the question. In our experiments,\nwe separately present the results of log retrieval\nwhere the k is set 1, 5 or 20.\nTo assess the performance of LLM, we employ\ntwo evaluation methods: single-score mode and\npairwise-score mode (Huang et al., 2024; Xu et al.,\n2023a; Guo et al., 2024; Zheng et al., 2024). In\nSingle-score mode, we first select the model to be\ntested and generate answers based on given ques-\ntions and fixed reference chunks, using the BGE-\nM3 embedding model as default. We then utilize\nGPT-4 (Achiam et al., 2023) as a scoring model to\nevaluate the responses on a scale of 1 to 10, with\nhigher scores indicating better quality. To ensure\nreliability, we run GPT-4 three times for each re-\nsponse, and report the average score in our results.\nIn pairwise-score mode, both models generate an-\nswers to identical questions using the same refer-\nence chunks. A scoring model then assesses which\nmodel's responses are superior, assigning a win to\nthe better performer and a loss to the other. If the\nperformance is comparable, both models receive a\ntie. Detailed prompts and procedures for both eval-\nuation modes are provided in the Appendix A.3."}, {"title": "4.3 Evaluation Results for Embedding Model", "content": "In Table 1, our domain knowledge augmented em-\nbedding model demonstrates superior performance\ncompared to baseline models across both tasks.\nSpecifically for the QA for Knowledge Acqui-\nsition task (QA for KA), our full model with Ho-\nmogeneous In-Batch Sampling (HIS) and Auxil-\niary Hard Negative Sampling (AHNS) achieves\nthe highest Acc@5 and Acc@20 scores of 0.919\nand 0.979 respectively, outperforming the BGE-M3\nbaseline by 4.3% and 2.2% on these metrics. In\nthe QA for Troubleshooting task (QA for TS), our\nfull model demonstrates the strongest performance,"}, {"title": "4.4 Evaluation Results for LLM", "content": "For single-score mode, we compared our proposed\nmodel against several baseline models, including\nChatglm3-6b, Qwen-7b-Chat, Llama3-8B-Instruct,\nand Qwen-14b-Chat. Table 4 shows that our model\nwith Continue Pre-Training (CPT) and Retrieval\nAugmented Fine-Tuning Method (RAFT) achieves\nthe highest mean scores in both QA for Trou-\nbleshooting (6.68) and QA for Knowledge Acqui-\nsition (6.88) tasks. These scores represent improve-\nments of 0.63 and 0.29 points respectively over\nthe Qwen-14b-Chat baseline. As for the pairwise\nscores (see Figure 4), our model outperforms all\nbaselines in both tasks."}, {"title": "4.5 Ablation study", "content": "For the embedding model, we evaluated the impact\nof HIS and AHNS. Results in Table 2 show that\nboth techniques contribute to performance gains,\nwith their combination yielding the best results\nacross all metrics in both tasks.\nFor the LLM, we conducted an ablation study\nto examine the importance of CPT and RAFT. In\nTable 4, the baseline model scored 6.05 for QA for\nTroubleshooting and 6.59 for QA for Knowledge\nAcquisition. In Table 5, Supervised Fine-Tuning\nwithout chunks (w/o CPT w/o RAFT) showed im-\nprovements over the baseline. RAFT alone (w/o"}, {"title": "5 Conclusion", "content": "In this paper, we introduce RAG4ITOps, a com-\nprehensive framework for QA systems tailored for\nIT operations and maintenance. Initially, we de-\nveloped a dataset construction pipeline, incorpo-\nrating data cleaning, chunking, and distillation of\nenterprise-exclusive corpora. Additionally, we fine-\ntuned an embedding model and enhanced its re-\ntrieval performance using Homogeneous In-Batch\nNegative Sampling and Auxiliary Hard Negative\nSampling strategies. Furthermore, we leveraged\nand fine-tuned a LLM enhancing its capabilities\nfor domain-specific QA tasks with Continue Pre-\nTraining and Retrieval Augmented Fine-Tuning.\nWe evaluated our framework through a series of\nexperiments, designed to assess its performance on\ndistinct QA tasks with different difficulties, demon-\nstrating the effectiveness of our approach in the\ndomain of IT operations and maintenance."}, {"title": "A Appendix", "content": "A.1 Experimental Settings\nWhen fine-tuning the embedding model, the learn-\ning rate we set is 10\u20136, a batch size of 1024. We\nuse Adam as the optimization algorithm with $\u03b21$\n= 0.9, $\u03b22$ = 0.99. We also implemented the ho-\nmogeneous in-batch sampling strategy, where all\nsamples in the same batch come from the same\ntask, and utilized negatives cross-device to enhance\nthe diversity of negative samples. The model was\ntrained for 1 epoch using 8*A100 80G GPUs.\nFor Continue Pre-Training the LLM, we set the\nlearning rate to 2 \u00d7 10\u22125, with a weight decay of\n0.1, and global batch size of 128. The sequence\nlength is set at 2048. We use the Adam optimiza-\ntion algorithm with $\u03b2\u2081$ = 0.9 and $\u03b22$ = 0.99. The\ntraining epoch is 3. For Retrieval Augmented Fine-\nTuning, the learning rate is increased to 5 \u00d7 10\u22125,\nmaintaining the same weight decay of 0.1, the\nglobal batch size is 512. The sequence length\nremains 2048. Adam is again used as the opti-\nmization algorithm, with the same $\u03b2$ values. The\ntraining duration for this phase is 1 epoch. We\nconduct full parameter training for Continue Pre-\nTraining using 8*A100 80G GPUs and LoRA (Hu\net al., 2021) fine-tuning for Retrieval Augmented\nFine-Tuning using 8*A100 80G GPUs.\nA.2 Dataset Construction\nHigh-quality datasets are essential for effective\nLarge Language Model(LLM) implementation, of-\nten more crucial than model architecture updates.\nWith improved data collection and processing tech-\nniques, we can perform Continue Pre-Training\nand Retrieval Augmented Fine-Tuning (RAFT)\non the model more effectively and achieve bet-\nter Retrieval-Augmented Generation (RAG) per-\nformance. We designed a sophisticated dataset\nconstruction pipeline including phases such as col-\nlection, chunking, distillation, and combination.\nThis pipeline is capable of extracting features from\neach type of data and provides robust support for\nthe LLM to meet the specific requirements of the\nIT operations and maintenance group.\nIT Operations Data This data was provided by\nthe IT operations group and contains documents\nand QA pairs. The documents include Word files\nwith internal knowledge such as tool descriptions,\noperation examples, system configurations, and\nscripts. These documents contain text, images, and\ntables. As we currently focus on language model-\ning, we only extracted texts and tables using the\npython-docx.\nMaintenance Data The maintenance group pro-\nvided 47k pairs of error logs and corresponding\nanalyses. The error logs contain detailed descrip-\ntions of errors, functions, and related platforms.\nThe analyses are human-labeled and include er-\nror scenarios, problem localization, and solutions.\nSpecifically, problem localization contains func-\ntion names, function descriptions, error reasons,\npriorities, and impacts.\nA.2.1 Data Processing\nGeneral Processing We convert all information\ninto text format to make documents easy to han-\ndle. By using python-docx, we fully extract all ta-\nbles from Word files and convert them to plain text\nbased on LaTeX standards. Each row is joined by a\nline break, and each column is joined by a vertical\nline. This approach enables the model to recog-\nnize all information within tables. Furthermore,\nwe standardize texts by removing noisy tokens and\nconverting illegal tokens to their normal forms. We\nuse these processed texts from documents to form\nthe pre-training dataset.\nChunking Techniques As documents often con-\ntain very long and complex structures, we split\neach document into several chunks. Chunking tech-\nniques are essential in our task. Complete and\nreasonable chunks can provide meaningful context\nto enhance performance in data distillation and data\nretrieval. Since most of the current documents are\nin a fixed format, we designed a targeted chunking\nmethod for these documents to achieve better re-\nsults than general splitting methods. Moreover, we\nalso designed a general chunking method for new\nincoming documents to do online training.\nAt the beginning of chunking, we first remove\nnoisy content using heuristic methods. As each doc-\nument contains a menu with clear signs, we explore\nthe scope of menus and remove them all. Addition-\nally, due to the presence of technical documents,\nwe remove noisy sentences and tables containing\nwords like \"Script Maintainer\" or \"Version Num-\nber\" which is only for human understanding and"}, {"title": "A.2.2 Data Distillation", "content": "In addition to using RAG to enhance the LLM's\nunderstanding of documents, we also implement\ndata distillation to generate a number of real-world\ncases and provide additional guidance to the model.\nFor chunks extracted from documents, we col-\nlect QA pairs from each chunk by calling the APIs\nof GPT-3.5 and GPT-4. These QA pairs simulate\nreal questions and expected answers based on the\ndocuments. In the training phase, we combine the\ndistilled questions and corresponding RAG con-\ntexts as input, and use the expected answers as\noutput to maintain a data format similar to real"}, {"title": "A.2.3 Data Combination", "content": "As described above, for Continue Pre-Training, we\ndirectly use the texts extracted from documents\n(Data-Pretrain). For Retrieval Augmented Gen-\neration(RAG), we combine the text chunks from\ndocuments (TC), QA pairs for knowledge acqui-\nsition (QAK-Log and QAK-GPT), and QA pairs\nfor troubleshooting (QAT-Log) to create the RAG\ndataset. We also combine the QAK-GPT, QAK-\nLog, and QAT-Log to form the final RAFT dataset\n(Data-LLM), containing 65k rows of data."}, {"title": "A.3 Instruction Templates and Prompts", "content": "This section presents the detailed prompts used\nin our question-answering system for IT opera-\ntions and maintenance. Table 7 and 8 presents two\nkey prompts used in our evaluation process: the\nPairwise-Score Mode Prompt and the Single-score\nMode Prompt. Table 9 presents additional prompts\nused in our data preparation pipeline. The first\nprompt is designed for rewriting sentences while\npreserving their meaning, which is useful for data\naugmentation and diversity. The second prompt is\nused in our data distillation process. The third and\nforth prompts are instruction template."}, {"title": "A.4 Case study", "content": "To provide insight into real-world applications of\nour RAG4ITOps framework, we present two repre-\nsentative cases: one illustrating a QA scenario for\ntroubleshooting as shown in Table 10, and another\ndemonstrating a QA scenario for knowledge acqui-\nsition in IT operations and maintenance as shown\nin Table 11."}]}