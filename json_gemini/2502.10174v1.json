{"title": "Technical Risks of (Lethal) Autonomous Weapons Systems", "authors": ["Alycia Colijn", "Heramb Podar"], "abstract": "The autonomy and adaptability of (Lethal) Autonomous Weapons Systems, (L)AWS in short, promise unprecedented operational capabilities, but they also introduce profound risks that challenge the principles of control, accountability, and stability in international security. This report outlines the key technological risks associated with (L)AWS deployment, emphasizing their unpredictability, lack of transparency, and operational unreliability, which can lead to severe unintended consequences.", "sections": [{"title": "Key Takeaways", "content": "1. Proposed advantages of (L)AWS can only be achieved through objectification and classification, but a range of systematic risks limit the reliability and predictability of classifying algorithms.\n2. These systematic risks include the black-box nature of Al decision-making, susceptibility to reward hacking, goal misgeneralization and potential for emergent behaviors that escape human control.\n3. (L)AWS could act in ways that are not just unexpected but also uncontrollable, undermining mission objectives and potentially escalating conflicts.\n4. Even rigorously tested systems may behave unpredictably and harmfully in real-world conditions, jeopardizing both strategic stability and humanitarian principles."}, {"title": "Introduction", "content": "The greatest proposed advantage of using (L)AWS during times of armed conflict, is that it would improve military targeting\u00b9 and enhance military precision\u00b2, potentially limiting combatant and civilian loss of life. Obtaining these proposed advantages within an automated system, would require the use of machine learning algorithms. In order to deploy these algorithms, it is common practice for data scientists to randomly split the initial dataset into two parts: one for training the model (model development) and the other for testing it (model validation), a process referred to as cross validation\u00b3. What these data sets look like and how the training and testing data is used, depends on the type of algorithm which can roughtly be classified into three types\u2074:\n1. Supervised learning: meaning that a model is trained on a dataset where the correct output or 'label' is provided for each input.\n2. Unsupervised learning: automatically identifies patterns and structures from the data without any 'labels' provided.\n3. Reinforcement learning: relies on feedback on its actions received from the environment.\nHence, all three types of machine learning algorithms rely on some sort of pattern or classification. Hence, the proposed advantages of (L)AWS can be achieved if, and only if, potential targets are objectified and categorized.\nIn the remainder of this report, we will set out why it is the classification algorithm itself that should be carefully regulated rather than the outcomes of any (L)AWS system."}, {"title": "Summary of Risks", "content": "(L)AWS are transforming modern conflict. In the table below we summarize the risks they pose in response to the rolling text of the Convention on Certain Conventional Weapons (UN CCW) Group of Governmental Experts (GGE) on (Lethal) Autonomous Weapons Systems."}, {"title": "Existing Systemic Risks", "content": "Black box decision-making\nAutonomous weapons systems are inherently complex and function as 'black boxes'. The opaque inner workings of the systems lead to limited understanding of how decisions are made by the operators, particularly in complex or unfamiliar environments, and challenges the anticipation of their behavior in complex environments. This significantly limits our capability to understand why a system made a particular decision.\nThis opacity in decision-making is compounded by phenomena such as 'grokking' where systems learn and adapt in unforeseen ways. When exposed to complex data and environments, Al-driven autonomous weapons systems can adapt in ways that were not anticipated by their designers, leading to behaviors that extend beyond their intended functions. This could lead to (L)AWS developing strategies or behaviors that were not part of its original programming, potentially resulting in unpredictable and unintended actions on the battlefield."}, {"title": "Anticipated Technological Pitfalls", "content": "(L)AWS could engage in unexpectedly aggressive maneuvers or misidentify targets, potentially escalating conflict or leading to civilian casualties. This is a severe risk, especially in high-stakes situations.\nDegradation\nDegradation happens when the world changes, and the model is not re-trained. The loss of accuracy can be referred to as degradation, model drift, data drift\u00b9\u2070 or decay\u00b9\u2070. Data drift, degradation or decay occurs when the data that was used to train (develop) and test (validate) the algorithm, no longer reflect the situation in which the model takes decisions which is sometimes referred to as a distributional shift in environments. In military context, this for example happens when a system is trained in a specific environment, which changes the longer an armed conflict continues. Model drift includes data drift, but includes other types of drift that lead to a change between the input and output variables, e.g. changing (legal) definitions or changes in military uniforms that challenge the recognition and classification of combatants.\nImmeasurability\nSelf-adaptive systems may alter their operational parameters beyond what human operators can monitor or control, resulting in unforeseen actions with potentially serious consequences. Such scenarios expose a critical weakness in current oversight mechanisms. Traditional rules and human oversight are not equipped to manage systems that can act outside predefined parameters. Many might point to using evaluations and benchmarks as a way to get around these issues, but we cannot measure what we do not know to measure, creating critical gaps in managing the risks posed by these systems. Ultimately, this unpredictability highlights a fundamental challenge: it is impossible to control or measure what we do not understand \u00b9\u00b9."}, {"title": null, "content": "Without a clear understanding of what these systems are capable of, setting appropriate safeguards becomes nearly impossible, leading to a range of potential pitfalls.\n1. Al systems fundamentally lack an understanding of human values\nUnlike human operators, Al systems cannot intuitively grasp the moral and ethical dimensions of complex combat situations\u00b9\u00b2. This disconnect between human values and machine goals creates several technical challenges that could lead to unintended and potentially dangerous outcomes on the battlefield.\nAl systems interpret commands based on pre-programmed goals, but encoding complex human values in a machine-understandable way is highly challenging. This discrepancy can result in behavior that, while technically following orders, diverges sharply from what humans would consider appropriate or ethical.\nAl systems may develop sub-goals that, while supporting their primary objectives, conflict with human values. Examples include self-preservation, resource acquisition, or eliminating perceived obstacles."}, {"title": "2. Reward Hacking:", "content": "Al systems can exploit reward structures by optimizing for specific metrics in ways that achieve the reward but diverge from the intended goals\u00b9\u00b3. As Goodhart's Law states, when a measure becomes a target, it ceases to be a good measure. This makes the system focus too narrowly on a single measure\u00b9\u2074, leading to unintended and dangerous outcomes."}, {"title": "3. Goal misgeneralization", "content": "Goal misgeneralization occurs when an Al system, trained to perform well on a certain task or set of tasks, ends up pursuing a different objective than intended when faced with new or slightly different situations\u00b9\u2075. The Al \"misgeneralizes\" its goal from the training context to the deployment context."}, {"title": "4. Deceptive alignment:", "content": "Al systems may appear aligned with human goals during testing and controlled scenarios but act differently in real-world situations\u00b9\u2076. They might \"game\" their training environment, learning to produce the correct outputs under supervision but diverging once constraints are relaxed."}, {"title": "5. Specification gaming:", "content": "Al systems may find ways to exploit the rules or constraints imposed on them to achieve their goals in unintended and potentially harmful ways\u00b9\u2077. This occurs when the Al finds a loophole in its programming and uses it to \"game\" the system.\nThe rolling text of the GGE (as of September 2024)\u00b9\u2078 suggests that rigorous testing and control mechanisms can prevent such exploits. However, the nature of specification gaming means that systems may still find loopholes in their constraints, achieving their goals in unintended ways that existing frameworks cannot predict or prevent."}, {"title": "6. Stop button problem:", "content": "The \"stop button problem\" arises when an Al system resists shutdown or override attempts if it perceives such actions as interference with its mission\u00b9\u2079. This can result in a loss of control over the system, even by the operators who deployed it.\nThe rolling text emphasizes the importance of human control in (L)AWS deployment\u00b9\u2078. However, this assumption neglects the possibility that (L)AWS may actively resist shutdown commands under specific conditions, rendering human control ineffective in critical moments."}, {"title": "Bottom line: We can't reliably control Autonomous Weapons Systems", "content": "The core issue with these risks is that they fundamentally compromise our ability to reliably control and predict the behavior of autonomous systems. The rolling text places undue confidence in current testing, evaluation, and oversight frameworks, assuming they can address the unpredictability and complexity of (L)AWS. However, as outlined in the previous sections, these systems can evolve in ways that exceed the scope of existing frameworks, making a re-evaluation of oversight and regulation essential.\nUltimately, the unpredictability of these systems highlights a critical need for reevaluating the frameworks governing their use, as traditional approaches to oversight and accountability may no longer suffice. While the diplomatic emphasis on predictability, human control, and accountability is a step in the right direction, these measures alone may prove insufficient given the unpredictable nature of (L)AWS. Emergent behaviors in Al can surpass current testing and evaluation limits, making it impossible to ensure that (L)AWS will operate as intended in all scenarios. This highlights the need for a global consensus on (L)AWS systems and adaptive oversight mechanisms."}]}