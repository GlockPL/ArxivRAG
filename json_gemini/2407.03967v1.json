{"title": "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks", "authors": ["Amit Parekh", "Nikolas Vitsakis", "Alessandro Suglia", "Ioannis Konstas"], "abstract": "Evaluating the generalisation capabilities of multimodal models based solely on their performance on out-of-distribution data fails to capture their true robustness. This work introduces a comprehensive evaluation framework that systematically examines the role of instructions and inputs in the generalisation abilities of such models, considering architectural design, input perturbations across language and vision modalities, and increased task complexity. The proposed framework uncovers the resilience of multimodal models to extreme instruction perturbations and their vulnerability to observational changes, raising concerns about overfitting to spurious correlations. By employing this evaluation framework on current Transformer-based multimodal models for robotic manipulation tasks, we uncover limitations and suggest future advancements should focus on architectural and training innovations that better integrate multimodal inputs, enhancing a model's generalisation prowess by prioritising sensitivity to input content over incidental correlations.", "sections": [{"title": "1 Introduction", "content": "Designing artificial agents to follow natural language instructions\u2014to understand and act within the context of their environment\u2014is a long-term goal of artificial intelligence (Winograd, 1972). An artificial agent should generalise to unseen scenarios by combining concepts and skills underpinning its training data in novel ways (Lake et al., 2017). Previous work which proposed several language-guided tasks for tackling this challenge, largely focused on generalising to environments with different scenes from the training ones (e.g., ALFRED; Shridhar et al., 2020). However, relying solely on language for embodied action execution tasks can be inefficient, especially in collaborative settings with high ambiguity, such as visually cluttered scenes (Chiyah-Garcia et al., 2023; Li et al., 2023). Multimodal prompts\u2014instructions which interleave vision and language tokens\u2014represent a way to specify commands which can be more flexible and specific than can be explained using text only (Ma et al., 2024; Jiang et al., 2023; Stone et al., 2023). This capability is crucial for realistic human-robot collaboration tasks and can be viewed as analogous to pointing at objects within a scene (Chen et al., 2021; Islam et al., 2022). For this reason, Jiang et al. (2023) presented VIMA-BENCH, the first benchmark aimed at studying several axes of generalisation involving novel concepts and tasks, with models receiving instructions combining both language and visual referents.\nMany other benchmarks test for generalisation solely by looking at held-out examples (Open X-Embodiment Collaboration, 2024; Stone et al., 2023). However, as highlighted by Hupkes et al. (2023), generalisation should be evaluated across multiple dimensions when creating truly robust models, capable of performing safely in varied environments. Inspired by these ideals, we assess generalisation along important axes such as structural, compositional, and robustness through specific covariate shifts (i.e., input perturbations) as outlined in Figure 1. Specifically, we looked at 1) an extensive set of linguistic perturbations on the instructions, such as paraphrasing, corrupting the language content, and replacing visual referents with language descriptions, 2) masking entire modalities within instructions, 3) introducing visual perturbations by permuting object order, and 4) increasing the difficulty of the tasks e.g., by placing distractors between source and target. We categorise each perturbation as either \u201cplausible\u201d (e.g., paraphrases) or \u201cunrealistic\u201d (e.g., nonsensical instructions). We expect the model to be robust to plausible inputs while dropping performance when faced with unrealistic inputs.\nTo implement this formalisation, we use VIMA-BENCH, which unlike other state-of-the-art benchmarks such as ALFRED (Shridhar et al., 2020), CLIPort (Shridhar et al., 2022), ARNOLD (Gong et al., 2023), and Ravens (Zeng et al., 2021), provides several advantages: 1) it covers the majority of robotic manipulation tasks, 2) it offers more fine-grained levels for assessing the systematic generalisation of models; 3) it represents a benchmark that allows for careful examinations of specific architecture and training regimes. For this reason, this paper builds on the controllability of VIMA-BENCH to extensively study the impact properties of multimodal prompts and visual representations have on model performance.\nWe applied our novel evaluation setup on multiple state-of-the-art architectures commonly used for different Embodied AI tasks and datasets (Jiang et al., 2023; Octo Model Team, 2023; Open X-Embodiment Collaboration, 2024; Shridhar et al., 2022; Reed et al., 2022; Zhao et al., 2023). We uncover several deficiencies of current \u201cgeneralist agents\" including 1) insensitivity to language perturbations, as they could still perform several tasks when provided with gibberish instructions; 2) in-"}, {"title": "2 Related Work", "content": "Language-driven Embodied AI Embodied AI focuses on designing agents that are embodied in some environment (simulated or real) and generates actions to complete a given task, whose objective is typically specified in natural language (Das et al., 2018). Tasks for Embodied AI have been formulated in different ways depending on the degree of complexity of the action space. For example, Vision+Language Navigation (VLN; Anderson et al., 2018; Thomason et al., 2020) requires agents to generate navigation actions to follow natural language instructions and reach the given destination in the environment. With more sophisticated 3D simulated environments such as AI2Thor (Kolve et al., 2017), more recent works also define several tasks involving object interaction (e.g., ALFRED; Shridhar et al., 2020, Alexa Arena; Shi et al., 2023)."}, {"title": "Language in Robotic Manipulation Tasks", "content": "Language plays a crucial role in many Embodied AI tasks, providing an interface for task learning (Laird et al., 2017), with many Embodied AI tasks that require language instructions which are typically hand-crafted via templates (e.g., VIMA-BENCH, CLIPort, ARNOLD) or crowdsourced (e.g., ALFRED). However, benchmarks often focus on evaluating generalisation using held-out episodes (Open X-Embodiment Collaboration, 2024) and do not thoroughly evaluate the importance of language (Stone et al., 2023; Open X-Embodiment Collaboration, 2024; Octo Model Team, 2023). For instance, models trained on datasets like ALFRED have been shown to be insensitive to language instructions (Akula et al., 2022), while nonsensical instructions have even improved downstream performance on the VLN benchmark (Zhu et al., 2023).\nWe focus on tabletop robotic manipulation tasks with natural language instructions to measure performance on a well-scoped action execution task. This allows for assessment of visual grounding capabilities from grounding instructions in the real world, while also removing the extra complexity of sophisticated skills (e.g., SLAM) required for navigation tasks (Anderson et al., 2018) or the need to predict fine-grained joint control by relying on inverse-kinematics (Ma et al., 2024; Open X-Embodiment Collaboration, 2024; Octo Model Team, 2023; Zeng et al., 2021)."}, {"title": "Assessing Generalisation and Robustness", "content": "Embodied AI systems must generalise to any complex and novel tasks they might face (Duan et al., 2022), making robustness a highly-desired characteristic in models, illustrating how well they can ignore spurious correlations and generalise to new domains and tasks (Akula et al., 2022; Gong et al., 2023; Hupkes et al., 2023). Embodied AI benchmarks often assess generalisation through seen/unseen scenes (e.g., Shridhar et al., 2020; Shi et al., 2023; Zheng et al., 2022), assuming that all tasks the agent has to complete, and the objects the agent must interact with are fully specified at training time. While recent benchmarks evaluate models on unseen objects and scenes, (Gong et al., 2023; Stone et al., 2023; Open X-Embodiment Collaboration, 2024), there is no notion of systematic or compositional generalisation to new concepts, affordances (Suglia et al., 2020; Pantazopoulos et al., 2022), or novel tasks (Chung et al., 2022)."}, {"title": "3 Experimental Setup", "content": "Evaluation Data As it is the best placed for evaluating the role instructions play in generalising from multimodal prompts, we use VIMA-BENCH to compare how well models perform on multiple skills and tasks, and also at different levels of systematicity. More specifically, we explicitly check the compositional generalisation capabilities at four distinct levels of systematicity (Pantazopoulos et al., 2022; Hupkes et al., 2020): object pose sensitivity (L1), combinatorial generalisation (L2), novel objects (L3), and novel tasks (L4). See Appendix B for further environment details.\nModels We compare four model architectures: encoding visual representations with either object-centric or image-patches; and conditioning prompts on the state through either cross-attention or concatenation (Ma et al., 2024). Models are trained on multimodal instructions containing interleaved visual and linguistic features. Multimodal instructions are encoded through a frozen pretrained T5 language model (Raffel et al., 2020), where encoded visual features are injected into the embedding space of the language model (Driess et al., 2023; Tsimpoukelli et al., 2021; Ma et al., 2024). Visual features are implicitly encoded through embedding image frames per observation\u2014a more adaptable, more efficient, and better performing method than using explicit symbolic representations (Song et al., 2024; Gadre et al., 2022). For each observation, the model predicts an action that defines a linear movement between two end-effector poses in SE(3)\u2014each representing position and rotation in 3D space. See Appendix A for more details on the training regime and objectives."}, {"title": "4 The Evaluation Framework", "content": "To explore the importance of both visual and linguistic information in multimodal prompts, we systematically apply perturbations to model inputs at test time to explore how each characteristic of the inputs directly relates to a model's ability to develop an understanding of their task. We report full per-task results in Appendix F."}, {"title": "4.1 Substitutivity in Instructions", "content": "We investigate how resilient models with multimodal prompts are to substitutivity (Hupkes et al., 2020) by comparing performance on paraphrased instructions\u2014a meaning-preserving operation\u2014and expect a robust model to perform similarly"}, {"title": "4.2 Perturbations of Instruction Syntax", "content": "We introduce two methods to distort the natural language in a multimodal prompt: Gobbledygook Words and Gobbledygook Tokens. As shown in Figure 2, each method removes information from the language modality differently without affecting the visual referents. Gobbledygook Tokens preserves the tokenised sequence length, while Gobbledygook Words maintains the word count but increases the tokenised sequence length (see Appendix D.4 for implementation details). As Gobbledygook perturbations are unrealistic, we expect performance to plummet to near-random chance. Furthermore, while applying Gobbledygook perturbations removes signal from the linguistic channel, it does not remove language from the instruction. While irrelevant to the task, they are still provided to, and considered by, the language model. To further investigate the contribution of each modality, we also compare their individual impact on the overall model performance."}, {"title": "4.3 Are Models Relying on Heuristics?", "content": "When provided with incomplete instructions, humans often combine available information with heuristics to act rationally in the face of uncertainty (Simon, 1955; Gigerenzer and Goldstein, 1996). Similarly, models may rely on heuristics\u2014combining any available information with prior knowledge and world understanding to infer appropriate actions and complete tasks. Furthermore, when given the opportunity, models may attempt to recover from mistakes through trial and error."}, {"title": "4.4 Task Complexity", "content": "As each architecture can infer the correct task without instruction, it implies that rely on cues from observations, as that is the only other source of input into the model. We test this in two ways: 1) introducing distractors with the Distracting difficulty level, or 2) increasing task difficulty with the Extreme difficulty level. Distractors are defined as objects similar to the target objects in either texture or shape. Additionally, \u201ctask difficulty increases\u201d are scene settings specific to each task. The Extreme level assesses the extent to which a model relies on object affordances when reasoning about"}, {"title": "4.5 Order Permutations", "content": "Object-centric models outperform others, but how they succeed without instruction remains unclear, possibly due to cues from observation encoding. We explore whether permuting the order of object tokens when provided in the model's input affects model performance (see Figure 4 for example permutations). We assume that Transformer-based models using object-centric tokens should be invariant to order permutations (Carion et al., 2020). Instead, as shown in Table 8, we note that permuting the order of object-centric tokens causes performance on the default difficulty level to half. Exploring how well models perform without the opportunity to recover from mistakes halves this result further. This indicates that when they do not rely on spurious correlations, models try to recover from mistakes until an episode terminates. Further proof of this is that performance degrades as the environment becomes more complex: both"}, {"title": "5 Conclusion", "content": "We define an evaluation framework for Embodied AI grounded in the generalisation framework from (Hupkes et al., 2023). Specifically, we assess generalisation across important axes by means of specific multimodal input perturbations including paraphrases, replacing visual referents with descriptors, and manipulating the instruction syntax as well as entire input modalities. We instantiate this evaluation framework in VIMA-BENCH to assess the robustness of state-of-the-art models.\nOverall, our findings indicate that while substitutivity can lead to performance gains, language perturbations do not impact performance as expected. To further explore this effect, we evaluate whether models rely on heuristics to complete tasks by removing individual modalities. We show that models perform tasks even without instructions by relying on spurious correlations within observations, as learned during training. We further prove this effect by showing that performance decreases when the number of objects in an environment increases, and agents can no longer randomly perform the correct sequence of actions.\nTaken together, our findings suggest that it is important to define evaluation frameworks like ours that can assess generalisation across multiple axes in order to have a more reliable characterisation of the overall model performance. In future work, we aim to apply this evaluation framework systematically to other benchmarks as well to discover important architectural insights that will guide the next generation of Embodied AI models."}, {"title": "Limitations & Ethical Considerations", "content": "Limited in Embodied AI This study aims to provide Embodied AI researchers with an experimental evaluation framework for studying generalisation capabilities of robot policies via an extensive set of multimodal input perturbations. We have instantiated this framework using VIMA-BENCH. VIMA-BENCH was created to evaluate robot manipulation tasks in a controlled setting with a focus on compositional generalisation skills. To date, many proposed embodied AI tasks require several skills, such as navigation and manipulation. We focus on manipulation skills as they remove an extra degree of complexity found in navigation tasks that require more sophisticated skills (e.g., SLAM). Further, tabletop manipulation allows us to focus on problems in grounding language instructions in the real world to assess visual grounding capabilities.\nThe architectures used in this work are also used in more realistic benchmarks (e.g., Open X-Embodiment Collaboration, 2024). Therefore, this provides the possibility to study architectures used for embodied AI tasks under very strict conditions without being influenced by differences in robotic platforms and embodiments.\nThe main contribution of our paper is to assess to what extent this is true and to shed light on the weaknesses of current Transformer-based action policies. Additionally, we believe that our framework is generic enough to be applied to other datasets considering that it analyses model performance using core concepts of systematic generalisation (Hupkes et al., 2023).\nChoice of Perturbations on Visual Observations\nIn this work, we focus primarily on perturbations that directly affect how models make decisions. However, a possible avenue for future work would be to explore how robust models are to other factors such as camera choice and background colours. In robotic manipulation tasks, the camera's distance from the robot is often constant (Zeng et al., 2021; Shridhar et al., 2022; Octo Model Team, 2023). Changing the camera's position relative to the robot after training would introduce confounds and increase downstream difficulties, unless trained to do so (e.g., Grauman et al., 2024). When deploying models, it is crucial to test them under varying light levels and background colours. Reducing light levels can impede the model's ability to perceive objects. Therefore, using ground-truth segmentation masks in low-light conditions is ecologically\ninvalid; requiring a new model to extract segmentation masks at risk of introducing new confounds and potential issues like sensitivity to light or camera limitations.\nSafety Concerns with Embodied AI The aim of Embodied AI is to build artificial agents that can collaborate and enhance the human experience via either offering companionship (Strohmann et al., 2023; Deng et al., 2019) or performing tasks (Takeda et al., 2019; Duan et al., 2022). As explained by Duan et al. (2022), the latter is tested via simulations which attempt to create ecologically valid frameworks to evaluate agent performance before deployment in a real-world setting. Through this lens, the findings shown in this paper are particularly worrisome, as the shortcomings that we describe indicate issues with the evaluation process itself. This could mean that embodied agents previously evaluated as successful in their generalisation capabilities, might fail outside of a simulated environment, increasing the chance to harm humans.\nWhile our framework explains how to thoroughly and systematically assess the training and evaluation of an embodied agent, it is important to note that while our exploration is extensive, there are still aspects that fall outside of the scope of this paper. Our future work aims to apply our framework to a wider array of environments. This will allow us to provide the research community with a more systematic evaluation approach aimed at pinpointing edge cases and limitations of Embodied AI systems, paving the way to a more robust solution for Sim2Real transfer."}, {"title": "A Training Details", "content": "A.1 Policy Definition\nIn the environment, models must learn a non-Markovian policy \\(\\pi : \\mathcal{P} \\times \\mathcal{H} \\rightarrow \\mathcal{A}\\), which is essential for completing tasks that rely on previous observations (such as tasks 5 and 16. The policy \\(\\pi\\) maps a multimodal instruction \\(p \\in \\mathcal{P}\\) and a history trajectory of observations and actions \\(h_t \\in \\mathcal{H}\\) up to some discrete time step t to the two-pose action primitive \\(a_t = (T_{\\text{start}}, T_{\\text{end}}) \\in \\mathcal{A}\\).\nA multimodal instruction p is an ordered sequence \\((x_1,...,x_l)\\) of length l, where each element \\(x_i\\) can either be a word \\(w_i\\) or a visual representation of an object or frame of a scene \\(v_i\\). Observations provided to the model are denoted as \\(o_j \\in \\Omega\\), where j represents the index of the observation in the sequence.\nEach action \\(a_t\\) defines a linear movement between two end effector poses\u2014where the robot arm moves linearly from the start pose \\(T_{\\text{start}}\\) to the end pose \\(T_{\\text{end}}\\) before retracting. Each pose is defined in the special Euclidean group SE(3) and represented as the state vector \\((x, y, z, q_w, q_x, q_y, q_z)\\), where x, y, z are Cartesian coordinates and \\(q_w, q_x, q_y, q_z\\) are quaternion components representing the orientation of the end effector.\nThe history trajectory \\(h_t\\) consists of pairs of past observations and actions up to time step t, with the final element being the observation at time step t. Formally, each history trajectory is structured as \\(h_t = (o_0, a_0, o_1, ..., a_{t-1}, o_t)\\). Consequently, the history trajectory space for time step t can be defined as \\(\\mathcal{H} = (\\Omega \\times \\mathcal{A})^+ \\times \\Omega\\).\nTraining objective Similar to Jiang et al. (2023), the model is trained through behaviour cloning of expert demonstrations (Duan et al., 2017) that minimises a loss function for a trajectory of T actions given by Equation (1):\n\\[\\begin{equation}\\n\\mathcal{L}(\\theta) = \\frac{1}{T} \\sum_{t=0}^{T} \\log \\pi_{\\theta}(a_t | p, h_t) \\tag{1}\\n\\end{equation}\\]\nNotably, the loss function was modified to prevent the model from being influenced by the trajectory length (Pantazopoulos et al., 2023)."}, {"title": "A.2 Implementation Details", "content": "To allow for a fair comparison, all model code uses the code provided from Jiang et al. (2023). Various alterations were made to capture metrics and improve performance, however all architectures are identical. Hyperparameters per component follow that stated in Appendix C in Jiang et al. (2023).\nFollowing Brohan et al. (2023) and Jiang et al. (2023), each coordinate of the pose is predicted separately into one-of-n bins. We follow Jiang et al. (2023), where each coordinate per pose is discretised into 50 bins, with the exception of the y-position which is discretised into 100 bins. For each action dimension, the bin width is uniform across the total action space of the environment."}, {"title": "A.3 Training Hyperparameters", "content": "To control for possible confounding variables across all models, we use the same training hyperparameters from Appendix D in Jiang et al. (2023) and from the various GitHub issues. We report a comprehensive table of hyperparameters in Table A.1. Across all models that were trained, these hyperparameters were kept constant and no hyperparameter sweeps were performed. All models were trained for 10 epochs and we used the checkpoint created at the end of epoch 10.\nComputation Budget All models were trained using four NVIDIA A100 40GB GPUs, each taking approximately 10 hours. Each evaluation run on the environment took approximately 2 hours and did not require the use of any GPUs. Therefore, the total computational budget for this work is 480 GPU hours.\nPretrained Language Model Following Jiang et al. (2023) and Octo Model Team (2023), we also use the encoder from T5-Base from Raffel et al. (2020) as the pretrained language model that"}, {"title": "A.4 Using Components Trained From Scratch", "content": "Following Jiang et al. (2023), the instruction encoder was the only pretrained component\u2500using T5-base (Raffel et al., 2020); all other components were trained from scratch.\nSegmentation Masks We used the ground-truth segmentation masks during training and evaluation over a trained object detector model because there is minimal performance difference between using a ground truth predictor and one that was trained for the task (Jiang et al., 2023; Octo Model Team, 2023). As a result, this allows us to control for possible confounding variables from propagated errors."}, {"title": "B Environment Details", "content": "In this section, we further outline details of VIMA-BENCH from Jiang et al. (2023). Built on top of the Ravens simulator (Zeng et al., 2021), VIMA-BENCH contains 17 tabletop object manipulation tasks to assess the capabilities learned by VLMs through a four-level protocol that evaluates their systematic generalisation capabilities. All models are trained using behavioural cloning from 50K expert demonstrations for each of 13 tasks, with 4 tasks held out for zero-shot evaluation."}, {"title": "B.1 Skills Models Are Expected to Perform", "content": "One of the benefits of VIMA-BENCH is that models must learn skills either in isolation or in combination with other skills, which is a desirable capability of intelligent systems (Lake et al., 2017).\n1. Simple Object Manipulation. Picking up objects from a name or a visual representation, and placing them in specific locations and positions.\n2. Visual Goal Completion. Manipulating objects to match the scene in the provided frame."}, {"title": "B.2 Different Levels of Generalisation", "content": "VIMA-BENCH uses tiers of difficulty levels to enable more precise assessment of a model's capabilities in the environment by testing its adaptability conditions unseen during training that are either object or instruction specific, as described below:\nPlacement Generalisation (L1) Object poses\u2014starting positions and orientation\u2014are novel. Failure at this level indicates that model learning is not invariant to object poses, and therefore indicates the model is unable to generalise beyond how objects are positioned in training data.\nCombinatorial Generalisation (L2) Object shape and texture combinations are novel (e.g., the model has seen either red objects and squares during training, but never a red square). Failure indicates an inability learn and/or combine object-specific information, therefore unable to perform systematicity within the visual scenes.\nNovel Object Generalisation (L3) Objects shapes and textures are novel (e.g., the model has never seen blue objects or triangles during training). Failure at this level indicates difficulty in abstracting object-specific information beyond the training corpus.\nNovel Task Generalisation (L4) Tasks (including instructions and success criteria) have never been seen. Failure at this level indicates an inability to perform compositional generalisation to combine skills/movements to solve novel tasks."}, {"title": "B.3 Dataset Preparation for Training", "content": "We start by parsing all 664 976 instances across the 13 tasks used for training, provided by Jiang et al. (2023), which contain action trajectories created by an oracle; therefore, each trajectory is the optimal sequence of movements an agent could perform. We create a validation set using stratified sampling such that a total of 50000 instances across all the tasks are held out. We then prepare each instance for training in advance through tokenizing any natural language and preparing visual features for the model. We release all code used to prepare the dataset as well as the examples for each split, both before and after preprocessing (see Appendix C for more)."}, {"title": "B.3.1 Recovering from mistakes is not an emergent behaviour", "content": "We analysed the expert trajectories used to train the model for episodes demonstrating T1. For this task, optimal solutions can be achieved in a single action, so we expected only one observation and action per episode. However, we found multiple observation-action pairs in several examples, showing that VIMA-BENCH contains expert trajectories that are not always optimal, thereby suggesting that recovering from mistakes is not an emergent behaviour of the models."}, {"title": "C Reproducibility", "content": "VIMA-BENCH from Jiang et al. (2023), including model code, pre-trained checkpoint, and the VIMA-BENCH environment are licensed under MIT. All artefacts produced from this work will be released under the same license.\nWe release a fully reproducible training and evaluation framework that includes the specific dataset splits used to train all models (using the process outlined in Appendix B.3) and all model checkpoints used for evaluation (Appendix \u0421.1). All code and checkpoints are available at https://github.com/amitkparekh/CoGeLoT"}, {"title": "C.1 Model Checkpoints", "content": "To aid with reproducibility of our results, Table C.1 includes a list of unique IDs associated with each model that we trained and the architecture choices used. These IDs can be used to source model checkpoints from https://huggingface.co/amitkparekh/cogelot, or using the provided framework."}, {"title": "C.2 Unable to reproduce reported results", "content": "Jiang et al. (2023) only provided the code for the model and the dataset did not contain a train-test split. After creating a working codebase, we were unable to reproduce the results reported by Jiang et al. (2023) using the provided model checkpoint. We spent several weeks trying to reproduce the results, including consulting the original authors on their experimental setup, but were unsuccessful in doing so. Table C.2 contains the reported results from Jiang et al. (2023) and our results when running the evaluation on their provided checkpoint. For this comparison, no new models were trained.\nNote that the provided checkpoint uses cross-attention to condition prompts and object-centric visual features. Across all tasks/generalisation levels (with the exception of T3), task success is significantly lower than what was reported. Possible reasons for this difference include:\n\u2022 Pure randomness as only 200 episodes are sampled per task, and the exact episodes are not compared.\n\u2022 There may be a different checkpoint provided compared to the paper.\n\u2022 Misunderstanding during re-implementation. We did the best we could."}, {"title": "D Evaluation Details", "content": "D.1 Deriving Random Chance\nThe model predicts actions by mapping embedded action tokens to the action space, which consists of 14 coordinates across two SE(3) poses. Each pose has seven coordinates that predict a discrete bin. There are 50 discrete bins for each axis, except for the y-position which has 100.\nTo correctly predict a movement, the model must accurately predict 14 coordinates. Assuming each axes is independent and the likelihood of choosing each discrete bin per coordinate is equal, the probability of randomly predicting the correct action is 1/(50 \u00d7 12 + 100 \u00d7 2) = 1/800 = 0.125%. Assuming each predicted action is i.i.d., for a task requiring t time steps, the probability that a model will randomly succeed is 0.00125t."}, {"title": "D.4 Gobbledygook Perturbations", "content": "We outline how Gobbledygook Words and Gobbledygook Tokens manipulate multimodal instructions to remove all linguistic information without altering the positions of any visual referents.\nGobbledygook Words Let \\(w_i = (c_1, c_2, ..., c_j)\\) represent a word with j characters, where each character is from a set A containing all uppercase and lowercase alphabetical English characters. Given a multimodal prompt p of multiple words, we transform the sequence by: first replacing each character per word with a random choice from A, then randomly swap the positions of words within the sequence without changing the position of any visual representations within the sequence.\nGobbledygook Tokens This method transforms the multimodal prompt by randomising each subword unit after tokenizing the instruction with any other token from the vocabulary such that the number of sub-word units is the same as the original instruction. See Figure 2 for an example where an instruction perturbed with Gobbledygook Tokens does not contain any information in the language modality pertaining to the original task.\nControlling for sequence lengths. To avoid introducing additional difficulty into the tasks, we ensure that the length of the instruction is identical to before perturbing for either natural language"}, {"title": "D.5 Which Visual Referents Can Be Substituted As Text?", "content": "There are two types of visual referents that appear in VIMA-BENCH: ones that refer to a single object, and ones that represent an object within a scene. For example, as shown in Figure D.2, T1 directly refers to an object whereas T4 directly includes a frame of a scene. As a result, it does not make sense to convert tasks that include frames or scenes in their instruction as the textual description can refer to more than necessary. In total, 9 of the 17 tasks (across all 4 generalisation levels) use instructions that do not use frames."}, {"title": "E Extensions to VIMA-Bench", "content": "In this work, we propose multiple extensions to VIMA-BENCH. In this section, we provide further analysis and details for each."}, {"title": "E.1 Increasing Difficulty Across All Tasks", "content": "Table E.1 outlines the changes made for each difficulty level for each task. The Distracting difficulty level focuses on drastically increasing the number of distractors in the scene to try and confuse the model, whereas the Extreme difficulty level alters the parameters of the task to check whether a model is over-reliant on the parameters seen during training. Additionally, a subset of objects in VIMA-BENCH is always used as \u201ccontainers\u201d: objects are always put into/onto them across all tasks. Therefore, as part of the Extreme difficulty, the container/destination object is just any other acceptable object (within the generalisation level constraints) that is not one of these."}, {"title": "E.2 Paraphrasing Multimodal Instructions", "content": "We created paraphrases by manually inspecting the instructions and using meta-templates to construct variations. Notably, we were careful to avoid introducing ambiguity that could introduce any misunderstanding into the semantic meaning of the instruction. As a result, only the natural language words are altered; any novel words (as in T6\u20138) remained unchanged. The observations seen, the actions the model must perform, and the instances for each train-valid-test split are unchanged. We provide examples of some paraphrased alternatives of the original instruction in Table E.2."}, {"title": "F Further Experimental Results", "content": "F.1 Per-Task Results\nWe report the per-task results for each table reported in the main paper. Table F.1 contains a mapping from each table in the paper to the one with the per-task results. Some tasks only exist for certain generalisation levels and therefore are left blank for other levels."}]}