{"title": "Focus on Focus: Focus-oriented Representation Learning and Multi-view Cross-modal Alignment for Glioma Grading", "authors": ["Li Pan", "Yupei Zhang", "Qiushi Yang", "Tan Li", "Xiaohan Xing", "Maximus C. F. Yeung", "Zhen Chen"], "abstract": "Recently, multimodal deep learning, which integrates histopathology slides and molecular biomarkers, has achieved a promising performance in glioma grading. Despite great progress, due to the intra-modality complexity and inter-modality heterogeneity, existing studies suffer from inadequate histopathology representation learning and inefficient molecular-pathology knowledge alignment. These two issues hinder existing methods to precisely interpret diagnostic molecular-pathology features, thereby limiting their grading performance. Moreover, the real-world applicability of existing multimodal approaches is significantly restricted as molecular biomarkers are not always available during clinical deployment. To address these problems, we introduce a novel Focus on Focus (FoF) framework with paired pathology-genomic training and applicable pathology-only inference, enhancing molecular-pathology representation effectively. Specifically, we propose a Focus-oriented Representation Learning (FRL) module to encourage the model to identify regions positively or negatively related to glioma grading and guide it to focus on the diagnostic areas with a consistency constraint. To effectively link the molecular biomarkers to morphological features, we propose a Multi-view Cross-modal Alignment (MCA) module that projects histopathology representations into molecular subspaces, aligning morphological features with corresponding molecular biomarker status by supervised contrastive learning. Experiments on the TCGA GBM-LGG dataset demonstrate that our FoF framework significantly improves the glioma grading. Remarkably, our FoF achieves superior performance using only histopathology slides compared to existing multimodal methods.", "sections": [{"title": "I. INTRODUCTION", "content": "As the most common type of brain tumors, gliomas are classified into Grade II to IV by the World Health Organization, correlating with varied prognoses and intervention approaches [1]. The gold standard for grading gliomas is the observation of representative histopathology features in biopsies [2]. However, histopathology slides present a complex milieu of cells, necrosis, and microenvironments, which complicates the localization of tumor foci, necessitating the expertise of senior pathologists [3]. The recent advances in computer-assisted cancer grading reveal promising performance in identifying glioma grades from histopathology slides [4], [5]."}, {"title": "II. RELATED WORK", "content": "Glioma is a significant type of ma-lignant tumor that occurs in the glial cells of the brain or spinal cord and can be classified into Grades II to IV, each correlating with different prognoses [2]. Histologic properties observed under a microscope from biopsy samples are the gold standard for tumor assessments in clinical practice, e.g., the presence of necrosis and/or microvascular proliferation leads to the diagno-sis of high-grade glioma (HGG) [15]. To reduce the workload of pathologists and promote consistent diagnosis, researchers have utilized traditional machine learning and deep learning models to classify morphological features [10]. For example, Ker et al. employed a pre-trained convolutional neural network to classify histopathology slides into normal, low-grade glioma (LGG), or HGG categories [16]. Rathore et al. extracted conventional features (e.g., intensity) and textural features (e.g., gray-level co-occurrence matrix) from specimens, and used a support vector machine for classification [17]. However, handcrafted features only capture detailed textural information, and deep learning models are prone to overfitting it due to the high complexity of histopathology slides. As a result, these approaches are inefficient at capturing the representative patterns of gliomas, lacking both generalization ability and interpretability.\nAccording to the latest WHO classifi-cation of tumors of the central nerve system [1], integrating molecular biomarkers with histopathology slides provides a more comprehensive and precise analysis of gliomas. Inspired by this medical insight, researchers attempt to project the histopathology and molecular data into a uniform latent space, fusing the features of the two modalities through various com-binations, such as concatenation [7], [18], Kronecker Product [6], [19], and cross attention [8]. Yet, molecular biomarkers, such as the status of IDH mutation and 1p/19q codeletion, contain precise information that directly correlates with the grading of gliomas [14]. As a result of this heterogeneity between the two modalities, existing methods that rely on uniform projection and late fusion are insufficient at aligning cross-modal information through simple combinations and overlook the correlations among different molecular biomark-ers. In contrast, our FoF framework encourages the model to adaptively focus on representative regions and directly align these regions with biomarkers, resulting in more robust representation learning and improved interpretability."}, {"title": "B. Multimodal Glioma Grading with Missing Modality", "content": "Although molecular biomarkers are essential for the clinical assessment of cancers, their acquisition necessitates immuno-histochemistry (IHC) staining and DNA/RNA sequencing. These processes are time-consuming and costly, thus result-ing in limited accessibility, particularly in underrepresented areas [20]. Fusion-based multimodal methods [6] necessitate the presence of paired histopathology slides and molecular biomarkers, further restricting their practical application in real-world clinical settings. To alleviate this challenge, DDM-net [21] proposed to reconstruct the features of unavailable modality from the available one with the transformation func-tion learned from paired data. Most recent studies have imple-mented distillation-based methods to enhance the image-based model by distilling knowledge from a pathology-genomic teacher [10]. For instance, Xing et al. [19] devised a novel distillation framework that effectively transfers knowledge from a multimodal teacher to a uni-modal student, achieving"}, {"title": "III. METHODOLOGY", "content": "As illustrated in Fig. 1, our FoF framework improves the glioma grading by highlighting the diagnostic molecular-pathology features. To enhance the representation learning on histopathology slides, we introduce FRL to identify the posi-tive $x_{pos}$ and negative regions $x_{neg}$ from the input images $x_{glb}$ with the pixel-wise contribution score $A \\in R^{H \\times W}$, encour-aging the model $f\u00b7g$ to focus on the most important areas. In MCA, we project the multi-view features $\\{v_{glb}, v_{pos}, v_{neg}\\}_{i=1}^{B}$ into individual molecular subspaces with distinct projectors $h^{n}(\u00b7)$ and employ biomarkers as labels. During inference, the model predicts the glioma grades $f\u00b7g(x_{glb})$ with only histopathology slides."}, {"title": "B. Focus-oriented Representation Learning", "content": "Existing representation learning on histopathology slides is inadequate, which can be attributed to the lack of focus on the diagnostic morphological patterns [13]. To boost histopathol-ogy representation learning, we propose the Focus-oriented Representation Learning module to encourage the focus of the model on areas most relevant to gliomas and enhance these representations with a consistency constraint. Specifically, FRL quantifies the contribution score of each pixel towards accurate classification $A \\in R^{H \\times W}$ and identifies the areas that are positively and negatively related to grading using a threshold. This module separately feeds the positive $x_{pos}$ and negative $x_{neg}$ regions into the ViT encoder $g$ and enhances the visual representations with a consistency constraint on the positive $v_{pos}$ and global features $v_{glb}$.\nTechnically, locating the focus of models has been for-mulated as identifying the input pixels that contribute most significantly to the output [22]. To identify the diagnostic regions within a specific slide $x$, FRL initially processes the whole image through the model to obtain the predictions $f\u00b7g(x)$. It then aggregates the gradient of the prediction for the ground-truth class $s$ across each feature map layer $v^{k}$, thereby estimating the contribution score of each pixel towards accurate grading as follows:\n$A = Y(ReLU(\\sum_{k} \\alpha^{k}v^{k}), [H, W])$, (1)\nwhere $i$ and $j$ represent the location indices within the feature map $v^{k}$, $\u03b1^{k}$ is the weight associated with the k-th feature map $v^{k}$, and $Y(\u00b7, [H, W])$ refers to the reshape and resample function that upscales the feature map to match the input image size. Consequently, the matrix A indicates the contribution of each pixel toward accurately classifying the input image to the target class. Following this step, we divide the pixel-wise contribution score into $P = \\frac{H}{p} \\times \\frac{W}{p}$ patches, where p is the patch size. The FRL calculates the average patch-wise contribution score, filtering it by a threshold to generate the mask of patches $M \\in R^{P}$. We apply the patch-wise mask on the patchified image to select only the positive and negative"}, {"title": "C. Multi-view Cross-modal Alignment", "content": "Current multimodal glioma grading methods encode and fuse the histopathology slides with molecular biomarkers [6]. Notably, unlike histopathology representations, molecular biomarkers usually with integer values, e.g., IDH mutation status, provide direct insights related to glioma grading without necessitating neural network processing [1]. This heterogene-ity between two modalities presents a challenge for exist-ing fusion-based methods, which struggle to align molecular indicators with histologic features in the high-dimensional feature space [14]. To tackle this issue, we propose a Multi-view Cross-modal Alignment module that employs genomic biomarkers as unique labels and aligns the histological feature representations within each molecular subspace by supervised contrastive learning.\nSpecifically, MCA leverages the biomarkers with discrete values, including IDH mutation status, 1p/19q codeletion presence, and the Copy Number Variation (CNV), as la-bels. To accommodate the co-occurrence of biomarkers, this module projects multi-view histopathology representations $v \\in \\{v_{glb}, v_{pos}, v_{neg}\\}$ into individual molecular subspaces, employing distinct projection heads $h^{n}(\u00b7)$ for each. Within these subspaces, it aims to bring features that share the same molecular biomarker values closer together while distanc-ing those that differ, thereby enhancing the concordance of histopathology features regarding each biomarker. The loss of Multi-view Cross-modal Alignment is formulated as follows:\n$L_{MCA} = - \\frac{1}{B} \\sum_{i=1}^{B} \\sum_{j=1}^{B} l_{ij} \\cdot log \\omega(h^{n}(v_{i}), v_{j})$,\n$\\omega(v,i,j) = \\frac{e^{\\phi(v_{i},v_{j})}}{ \\sum_{k=1}^{B} l_{i\u2260k} \u00b7 \\phi(v_{i},v_{k})}$, (5)\n$L_{MCA} = \\sum_{n=1}^{N} L_{MCA}^{n}$,\nwhere N denotes the number of gene biomarkers, n indicates the index of the biomarker, $l$ is an indicator function that returns 1 if the condition is satisfied and 0 otherwise, and $y^{n}$ represents the status of the n-th biomarker. We set the gene labels of positive regions to the same as global regions and set the negative gene labels to the normal status (i.e., wildtype for IDH and 1p/19q, CNV equals 0 for other biomarkers). Following recent medical studies [1], [24], we include the IDH mutation status, 1p/19q codeletion presence, and CNV of PTEN, EGFR, CARD11, and FGFR2 as molecular biomarkers in this study. By enhancing the histological feature represen-tations with FRL and aligning molecular biomarkers to them, FoF promotes a reliable and accurate grading of gliomas, reaching even better performance with sole images than exist-ing multimodal counterparts."}, {"title": "D. Training and Inference", "content": "The overall optimization objective of our FoF framework is summarized as follows:\n$\\mathcal{L} = \\mathcal{L}_{CLS} + \u03bb_{1}\\mathcal{L}_{FRL} + \u03bb_{2}\\mathcal{L}_{MCA}$, (6)\nwhere $\u03bb_{1}$ and $\u03bb_{2}$ indicate the coefficients to control the trade-off of FRL and MCA, respectively. During training, FoF estimates the contribution score A \u2208 RH\u00d7W in the first back-propagation and calculates the losses of FRL and MCA modules in the second back-propagation. At the inference phase, the model $f\u00b7g$ directly outputs the predictions of the input images with no additional computational cost."}, {"title": "V. CONCLUSION", "content": "In this work, we propose the FoF framework that utilizes pathology-genomic knowledge toward accurate glioma grading with histopathology slides. To enhance the representation learning on histopathology slides, we propose the FRL module that encourages the model to focus on diagnostic regions and the MCA scheme which efficiently aligns the molecular biomarkers with visual representations. Experimental results indicate that FoF improves the glioma grading significantly. Particularly, with the sole histopathology slides, FoF achieves superior performance with existing multimodal approaches, which is of great clinical significance."}]}