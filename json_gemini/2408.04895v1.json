{"title": "Better Not to Propagate: Understanding Edge Uncertainty and Over-smoothing in Signed Graph Neural Networks", "authors": ["Yoonhyuk Choi", "Jiho Choi", "Taewook Ko", "Chong-Kwon Kim"], "abstract": "Traditional Graph Neural Networks (GNNs) rely on network homophily, which can lead to performance degradation due to over-smoothing in many real-world heterophily scenarios. Recent studies analyze the smoothing effect (separability) after message-passing (MP), depending on the expectation of node features. Regarding separability gain, they provided theoretical backgrounds on over-smoothing caused by various propagation schemes, including positive, signed, and blocked MPs. More recently, by extending these theorems, some works have suggested improvements in signed propagation under multiple classes. However, prior works assume that the error ratio of all propagation schemes is fixed, failing to investigate this phenomenon correctly. To solve this problem, we propose a novel method for estimating homophily and edge error ratio, integrated with dynamic selection between blocked and signed propagation during training. Our theoretical analysis, supported by extensive experiments, demonstrates that blocking MP can be more effective than signed propagation under high edge error ratios, improving the performance in both homophilic and heterophilic graphs.", "sections": [{"title": "Introduction", "content": "Graph Neural Networks (GNNs) have shown remarkable performance with the aid of a message-passing scheme, where the representation of each node is recursively updated using its neighboring nodes based on structural properties. Early GNNs rely on network homophily, assuming that connected nodes will likely share similar labels. However, many real-world graphs have low homophily (heterophily), where spectral-based GNNs achieve dismal performance under this condition (graph heterophily) as Laplacian smoothing only receives low-frequency signals from neighboring nodes.\nTo separate the embedding of connected but dissimilar nodes, recent algorithms employ high-pass filters by adjusting edge weights for message-passing. Notably, flipping the sign of edges from positive to negative, referred to as signed propagation, has recently achieved remarkable performance."}, {"title": "Preliminaries", "content": "Notations. Let $G = (V,E,X)$ be a graph with $|V| = n$ nodes and $E = m$ edges. The node attribute matrix is $X \\in \\mathbb{R}^{n \\times F}$, where $F$ is the dimension of an input vector. Given $X$, the hidden representation of node features $H^{(l)}$ at the $l$-th layer is derived through message-passing. Here, node $i$'s feature is defined as $h_i$. The structural property of $G$ is represented by its adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$. A diagonal matrix $D$ of node degrees is derived from $A$ as $d_{ii} = \\sum_{j=1}^{n} A_{ij}$. Each node has its label $Y \\in \\mathbb{R}^{n \\times C}$ ($C$ represents the number of classes).\nMessage-Passing. Generally, GNNs employ alternate steps of propagation and aggregation recursively, during which the node features are updated iteratively. This is widely known message-passing scheme is GCN, which can be represented as:\n$H^{l+1} = \\sigma(\\tilde{A}H^lW_l)$ (1)\nHere, $H_0 = X$ is the initial vector and $H^l$ is nodes' hidden representations at the $l$-th layer. $H^{l+1}$ is retrieved through message-passing $(\\tilde{A} = D^{-1}A)$ with an activation function $\\sigma$. $W_l$ is trainable weight matrices. The final prediction is produced by applying cross-entropy $\\sigma(\\cdot)$ (e.g., log-softmax) to $H_L$ and the loss function is defined as below:\n$L_{GNN} = L_{nll}(Y,\\hat{Y}), \\hat{Y} = \\sigma(H_L)$. (2)\nThe parameters are updated by computing negative log-likelihood loss $L_{nll}$ between the predictions $(Y)$ and true labels $(\\hat{Y})$.\nHomophily. The global edge homophily ratio, $H_g$, is defined as:\n$H_g = \\frac{\\sum_{(i,j)\\in E}\\mathbb{I}(Y_i=Y_j)}{|E|}$ (3)\nLikewise, the local homophily ratio, $b_i$, of node $i$ is given as:\n$b_i = \\frac{\\sum_{j=1}^{n} A_{ij} \\cdot \\mathbb{I}(Y_i = Y_j)}{d_{ii}}$ (4)\nGiven a partially labeled training set $V_L$, the goal of semi-supervised node classification is to correctly predict the classes of unlabeled nodes $V_U = \\{V - V_L \\} \\subset V$.\nOver-smoothing. As introduced in , over-smoothing measures the distinguishability of node features as follows:\n$\\mu(H_l) := \\|\\| H_l - \\frac{1}{N} 11^T H_l \\|\\|_F$ (5)\nThe over-smoothing happens if $\\lim_{l \\to \\infty} \\mu(H_l) = 0$, where the node representation converges to zero after infinite propagation. Recently, proved that even the attention-based GNN loses the separability exponentially as $l$ increases."}, {"title": "Motivation", "content": "We first categorize various message-passing algorithms into the following three types: plane, signed, and blocking (pruning). Then, we introduce the prior theorems on the smoothing effect under these message-passing schemes. Lastly, we point out the drawbacks of their understanding, followed by our new theorems with a novel strategy."}, {"title": "Prior Understanding on Smoothing Effect", "content": "We introduce the smoothing effect of three message-passing types under multiple classes , demonstrating the superiority of signed propagation.\nDefinition 1 (Message-Passing Schemes). For theoretical analysis, let us define three message-passing (MP) schemes built upon the Laplacian-based degree normalization . The adjacency matrix $\\tilde{A} = D^{-1}A$ (Eq. 1) of each propagation scheme follows the condition below:\n*   Plane MP stands for the original matrix, consisting only of positive edges.\n$\\forall (i, j) \\in E, \\tilde{A} = D^{-1}A > 0$ (6)\n*   Signed MP assigns negative values to the heterophilic edges, where $Y_i \\neq Y_j$.\n$\\forall (i, j) \\in E, \\tilde{A} \\in \\begin{cases}D^{-1}A, & Y_i = Y_j \\\\ -D^{-1}A, & Y_i \\neq Y_j\\end{cases}$ (7)\n*   Blocked MP blocks the information propagation for heterophilic edges by assigning zero.\n$\\forall (i, j) \\in E, \\tilde{A} \\in \\begin{cases}D^{-1}A, & Y_i = Y_j \\\\ 0, & Y_i \\neq Y_j\\end{cases}$ (8)\nTo analyze the smoothing effect, we first inherit several useful notations defined in  as follows: (1) For all nodes $i = \\{1,...,n\\}$, their degrees $\\{d_i\\}$ and latent features $\\{h_i\\}$ are i.i.d. random variables. (2) Each class has the same population. (3) The scale of each class distribution after initial embedding is identical, $\\|E(h) = \\sum_X W^{(0)}|_{y_i}\\| = \\mu$. The latent feature after a single hop propagation $E(h^{(1)}|y_i)$ can be defined as follows.\nDefinition 2 (Binary Contextual Stochastic Block Models). Assume a binary class $E(h^{\\{0\\}}|y_i) \\sim (\\pm \\mu, 0|y_i)$ with probability $(p + q = 1)$ and node feature is sampled from Gaussian distribution ($N$) . If $y_i = 0$, the updated distribution is given by:\n$E(h^{(1)}|y_i) \\sim N((\\frac{p-q}{p+q}) \\mu, \\frac{1}{\\sqrt{deg(i)}}$ (9)\nextended binary CSBM (Eq. 9) to multiple (ternary) classes using additional angle $\\phi$ as below:\n$E(h|y_i) = (\\mu, \\phi, \\theta)$, (10)\nwhere $\\phi = \\pi/2$, and $0 \\leq \\theta < 2\\pi$. Note that the above equation satisfies the origin symmetry as in the binary case since $(\\mu, \\pi/2, \\theta) = -(\\mu,\\pi/2, \\pi)$. The CSBM in multiple classes can be defined as follows."}, {"title": "Edge Classification Error and Over-smoothing", "content": "Based on this understanding, we derive a new insight into the smoothing effect of each propagation scheme. To begin with, message-passing has lower separability (discrimination power) in case the coefficient of a specific MP $E(h|y_i)$ is smaller than others . Based on this observation, we first compare the plane MP with the signed and blocked MPs below.\nCorollary 6 (Plane vs Signed, Blocked MPs). Since $d'_i/(d_i + 1)$ is shared for all MPs (Eq. 11 - 13), we can compare the separability by omitting them as follows:\n*   (Plane vs Signed MP) By comparing Eq. 11 and 12:\n$Z_1 = E_p - E_s = (2e - 1)\\{b_i k + (b - 1)k'\\}$ (14)\n*   (Plane vs Blocked MP) By comparing Eq. 11 and 13:\n$Z_2 = E_p - E_b = e b_i k + (1 - e)(1 - b_i)k'$ (15)\nRegardless of the variables $b_i$ and $e$, the $\\int \\int Z_1, \\int \\int Z_2$ are always negative, meaning that plane MP shows inferior performance compared to the signed and blocked MPs. Thus, we can focus solely on the separability of the signed and blocked MPs to find the best propagation scheme.\nCorollary 7 (Signed vs Blocked MP). The difference between signed and blocked MP is given by:\n$Z_3 = E_s - E_b = (1 - 2e)k + (b_i - e)k'$ (16)\nUnder the condition of i.i.d. neighbors ($k' = -k$), the above equation becomes $Z_3 = (1 - e - b_i)k$ .\nPrevious studies assume a perfect edge classification scenario ($e = 0$), leading to the conclusion that $Z = 1 - b \\geq 0$ (signed MP outperforms blocked MP). However, this optimal condition is hard to achieve under a semi-supervised setting with few training nodes. Therefore, we propose estimating two parameters, $e$ and $b_i$, for the selection of precise MP schemes. For better understanding, we provide an example by varying $e$ below.\nCorollary 8 (Numerical example on edge error ratio). We employ three different edge error ratios, $e = \\{1,0.5,0\\}$ given that $Z = (1 - e - b_i)k$ in Eq. 16 as follows:\n$Z_4 = E_s - E_b = \\begin{cases} -b_i k, & e = 1 \\\\ (-b + 0.5)k, & e = 0.5 \\\\ (1 - b_i)k, & e = 0 \\end{cases}$ (17)\nWe can infer some useful insights from the above Corollary: (1) Under a high error ratio ($e = 1$, initial stage of training), it might be better to not propagate rather than using signed edges since $-b_i k < 0$ ($E_s < E_b$). In addition, an error in signed propagation increases the uncertainty more than blocked GNNs as shown in Lemma 9. (2) If the error is mediocre ($e = 0.5$), the sign of $Z_4$ is dependent on the homophily ratio $b_i$. In this condition, signed MP may perform well ($Z_4 \\geq 0$) under heterophily ($b < 0.5$), but it is still advantageous not to propagate messages under homophily. (3) However, if the edges are perfectly classified ($e = 0$), signed MP might be the best option, as demonstrated in prior work . Therefore, we suggest the estimation of these two parameters, $b_i$ and $e$ for precise training. The details are introduced in the following section.\nLemma 9 (Uncertainty). Under a high error ratio e, the uncertainty of signed GNNs is greater than the blocked GNNs. Proof can be found in Appendix A."}, {"title": "Methodology", "content": "Selecting an appropriate MP scheme during training is crucial for reducing smoothing and uncertainty. However, this may not be tractable under semi-supervised learning. Thus, we employ an EM algorithm, where the E-step estimates the homophily ($b_i$) and edge error ratio ($e$), followed by the M-step for optimization."}, {"title": "(E-Step) Parameter Estimation", "content": "We introduce the strategy of homophily ($b_i$) and edge error ($e_t$) estimation in Theorem 10 and 11, respectively.\nTheorem 10 (Homophily estimation). The homophily ratio $b_i$ can be inferred using the mechanism of MLP  and EvenNet  as follows:\n$b_i = B B^T, B := \\sigma (\\sum_{l=0}^{[L/2]} XW + \\sum_{l=0}^{[L/2]} A^{2l}XW^l)$ (18)\nwhich can take advantage of both the heterophily robustness of MLP and the low variance of EvenNet. The proof is provided in Appendix A."}, {"title": "M-Step) Optimization with Calibration", "content": "Based on the separability of signed and blocked MP in Corollary 7, one can determine the propagation type given the estimated values, $b_i$ (Eq. 18) and $e$ (Eq. 19). For each training step $t$, we first compare their discrimination power as below:\n$Z_t = (1 - b_i - e_t)k$ (22)\nIf $Z_t < 0$, we block signed messages to reduce the smoothing effect (Corollary 7) and uncertainty (Lemma 9). For a numerical definition, let us assume the adjacency matrix derived from the downstream task as $\\tilde{A}$ as defined from Eq. 6 to 8. Then, we can calibrate the edges based on the following conditions:\n$\\tilde{A}^{ij} = \\begin{cases} 0, & \\tilde{A}_{ij} < 0 \\land Z_t < 0 \\\\ A_{ij}, & otherwise \\end{cases}$ (23)\nThe above method is very simple and intuitive and can be applied to general GNNs. Given this, we can replace the normalized adjacency matrix $(\\tilde{A})$ in Eq. 1 with $A$ as follows:\n$H^{(l+1)} = \\sigma(A H^{(l)} W^{(l)})$ (24)\nwhich can be optimized through Eq. 2."}, {"title": "Theoretical Justification", "content": "In this section, we aim to show that the proposed method can relieve the smoothing effect of signed MP using the notion of (1) spectral radius (Thm. 16) and (2) CSBM (Thm. 17).\nDefinition 12 (Spectral Radius, Perron-Frobenius eigenvalue). Let $\\lambda_1, ..., \\lambda_n$ be the eigenvalues of an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$. Then, the spectral radius of $A$ is given by:\n$\\rho(A) = max\\{\\|\\lambda_1\\|, ..., \\|\\lambda_n\\|\\}$ (25)\n$A^\\infty$ is well known to converge $\\Leftrightarrow \\rho(A) < 1$ or $\\rho(A) = 1$ and $\\lambda_1 = 1$ is the only eigenvalue on the unit circle."}, {"title": "Conclusion", "content": "In this paper, we have presented a comprehensive study on the impact of edge uncertainty and over-smoothing in signed Graph Neural Networks (GNNs). Our key contributions are two-fold. Firstly, we scrutinize the smoothing effect under different edge error ratios, providing a novel perspective that challenges the assumption that all propagation schemes exhibit similar edge classification error ratios. Secondly, we introduce an innovative training mechanism that dynamically selects between blocked and signed propagation based on these error ratios, effectively mitigating over-smoothing and enhancing performance. Our theoretical analysis, supported by extensive experiments, demonstrates that blocking message propagation can be more effective than traditional message-passing schemes under high edge error ratios. This insight is crucial for improving node classification accuracy in both homophilic and heterophilic graphs. We hope that future work can explore further refinements of these techniques for more complex graph structures."}]}