{"title": "Can Vision-Language Models Replace Human Annotators: A Case Study with CelebA Dataset", "authors": ["Haoming Lu", "Feifei Zhong"], "abstract": "This study evaluates the capability of Vision-Language Models (VLMs) in image data annotation by comparing their performance on the CelebA dataset in terms of quality and cost-effectiveness against manual annotation. Annotations from the state-of-the-art LLaVA-NeXT model on 1000 CelebA images are in 79.5% agreement with the original human annotations. Incorporating re-annotations of disagreed cases into a majority vote boosts AI annotation consistency to 89.1% and even higher for more objective labels. Cost assessments demonstrate that AI annotation significantly reduces expenditures compared to traditional manual methods-representing less than 1% of the costs for manual annotation in the CelebA dataset. These findings support the potential of VLMs as a viable, cost-effective alternative for specific annotation tasks, reducing both financial burden and ethical concerns associated with large-scale manual data annotation. The AI annotations and re-annotations utilized in this study are available on GitHub.", "sections": [{"title": "1 Introduction", "content": "High-quality annotated data is recognized as a pivotal factor in the advancement of deep learning [6, 13]. Nevertheless, manual data annotation presents significant challenges in terms of cost and ethical considerations [5]. Recent evolution in large language models (LLMs) [21] has sparked enormous interest in their application to annotation and generation of text datasets [15, 3, 1, 18, 17]. Meanwhile, there are limited efforts [20, 7] exploring the capabilities of vision language models (VLMs) [19] in processing unlabeled image data. Previous studies have verified that VLMs can create various types of annotations on raw image data. However, comprehensive evaluations of their annotation quality and cost-effectiveness are essential to assess their potential to replace manual annotation.\nIn this paper, we evaluated the capability of AI-driven image data annotation by comparing the quality and costs between manual annotations and annotations generated by one of the SOTA VLMS (LLaVA-NeXT [10]) on the CelebA [11] dataset. Overall, the contributions of this work include:\n\u2022 We confirmed that for specific image classification tasks, AI models can achieve performance comparable to human annotators, particularly excelling on more objective labels.\n\u2022 Based on the similar quality and significantly lower costs, we argue that AI models already possess the potential to replace human annotation within certain scopes."}, {"title": "2 Background", "content": "CelebFaces Attributes Dataset (CelebA) [11] is a public face attributes dataset with over 200K celebrity images, each with 40 binary attribute annotations. Although the dataset was manually created, analysis [16] has pointed out errors and inconsistencies in the annotations."}, {"title": "3 Experiments", "content": "LLaVA-NeXT [10] is an open-sourced SOTA multimodal model that enhances visual reasoning and OCR capabilities compared to LLaVA-1.5 [9], which served as the foundation of many comprehensive studies of data, model, and capabilities of large multimodal models (LMM).\nFor each of the 40 binary attributes in the CelebA dataset, we designed a question that required the model to respond with only yes or no. For instance, the question posed for the attribute Eyeglasses was: Is this person wearing eyeglasses? Answer with only yes or no. Annotations were generated on a randomly selected subset of 1000 images. Two reviewers re-annotated the attributes where AI and manual annotations differed. Inspired by [16], the attributes were divided into 3 groups based on their levels of objectivity. As illustrated in Figure 1, AI and the original human annotation reached 79.5% overall consensus, with higher numbers on more objective attributes. Conducting a majority vote with the additional re-annotation further boosts the match rate between AI annotations and the final consensus to 89.1%. Specifically, the re-annotation showed an equal preference for AI and human annotations, with each method favored more in exactly 20 attributes.\nIn our experiment, 40K labels for 1000 images were generated by a single NVIDIA RTX A6000 GPU at float16 precision using the HuggingFace implementation of the LLaVA-NeXT-8B model [12]. However, considering that in real life, the unit price of manual annotation decreases with scale, to fairly compare the costs of AI and human annotation, the cost estimation is based on constructing the entire CelebA dataset-generating 40 labels on each of the 200K images. The pricing for manual annotation is calculated based on publicly available quotes, whereas the cost of AI annotations is estimated from the average inference time per label and prices of renting GPU instances from Lambda [8]. As shown in Table 1, for image classification tasks like those in the CelebA dataset, AI annotation costs can be less than 1% of human annotation costs."}, {"title": "4 Conclusions and Limitations", "content": "In this study, we evaluated the differences in quality and costs between the CelebA dataset constructed using AI models and the one manually created. Given the comparable quality and substantial cost advantages, Vision-Language Models (VLMs) have shown the potential to replace one or more annotation sources in scenarios where multiple annotations are utilized to enhance accuracy. Limited by the experiment's scale, the results may have been influenced by the biases of the AI model and the annotators' subjectivity. Future research could focus on whether VLMs can effectively handle more complex tasks and whether better interactions with AI models can improve the quality of annotations."}, {"title": "A Grouping of CelebA attributes", "content": "As shown in Table A, the 40 attributes in CelebA dataset are divided into three groups:\n\u2022 Objective: These attributes are based on clear traits or easily identifiable elements, and are not open to personal interpretation. For example, whether the person is wearing glasses.\n\u2022 Semi-Subjective: The attributes are based on ambiguous or less recognizable features that may be interpreted differently between observers. For instance, although youthfulness is a relatively objective attribute, people may have different perspectives on specific individuals.\n\u2022 Subjective: These attributes are based on personal judgment and can vary significantly between observers. For example, whether a person is considered attractive entirely depends on the observer's preferences."}, {"title": "B Detailed comparison of AI annotation, original human annotation, and re-annotation", "content": "Table 3 contains the detailed information that is illustrated in Figure 1. Column AI=Human is the number of images where the AI annotation is identical to the original human annotation; column Re=AI and Re=Human indicate the number of images where the re-annotation agrees with the AI annotation and the original human annotation respectively."}, {"title": "C Calculation of AI annotation costs", "content": "For inference with NVIDIA RTX A6000 GPU, the cost to annotate 1000 samples is estimated by\n$0.49 \\times 40 \\times 1000 \\times \\frac{1}{3600} \\times 0.8 = 4.36$\nwhere 0.49 is the average inference time (seconds) per label, 40 and 1000 are the number of attributes per image and number of images, and 0.8 is the price per hour according to Lambda [8].\nSimilarly, for A100, the cost is\n$0.28 \\times 40 \\times 1000 \\times \\frac{1}{3600} \\times 1.29 \\approx 4.01$"}, {"title": "D Confidence intervals of the agreement percentages", "content": "Below are the agreement percentages between AI annotation and human re-annotation / majority votes by different label categories (subject, semi-subjective, objective)."}]}