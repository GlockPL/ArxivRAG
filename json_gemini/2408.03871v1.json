{"title": "BeeManc at the PLABA Track of TAC-2023: Investigating LLMs and Controllable Attributes for Improving Biomedical Text Readability", "authors": ["Zihao Li", "Samuel Belkadi", "Nicolo Micheletti", "Lifeng Han", "Matthew Shardlow", "Goran Nenadic"], "abstract": "In this system report, we describe the models and methods we used for our participation in the PLABA2023 task on biomedical abstract simplification, part of the TAC 2023 tracks. The system outputs we submitted come from the following three categories: 1) domain fine-tuned T5-like models including Biomedical-T5 and Lay-SciFive; 2) fine-tuned BART-Large model with controllable attributes (via tokens) BART-w-CTs; 3) ChatGPT-prompting. We also present the work we carried out for this task on BioGPT fine-tuning.\nIn the official automatic evaluation using SARI scores, BeeManc ranks 2nd among all teams and our model Lay-SciFive ranks 3rd among all 13 evaluated systems. In the official human evaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score 92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; It also produced a high score 91.57 on Fluency in comparison to the highest score 93.53. In the second round of submissions, our team using ChatGPT-prompting ranks the 2nd in several categories including simplified term accuracy score 92.26 and completeness score 96.58, and a very similar score on faithfulness score 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations.\nOur codes, fine-tuned models, prompts, and data splits from the system development stage will be available at https://github.com/ HECTA-UOM/PLABA-MU", "sections": [{"title": "Introducing the Methodologies", "content": "The overall framework of our experimental design is displayed in Figure 1. In the first step, we fine-tune selected Large Language Models (LLMs) including T5, SciFive, BioGPT, and BART, apply prompt-based learning for ChatGPTs, and optimise control mechanisms on BART model. Then, we select the best performing two models using quantitative evaluation metrics SARI, BERTscore, BLEU, and ROUGE. Finally, we chose a subset of the testing results of the two best-performing models for our human evaluation. In this section, we first introduce the models we used, followed by LoRA efficient training, and then introduce the quantitative metrics we applied."}, {"title": "Models", "content": "The baseline models we investigated in our work include T5, SciFive, GPTs, BioGPT, BART, and Control Mechanisms. We give more details below."}, {"title": "T5", "content": "T5 (Raffel et al., 2020) used the same Transformer structure from (Vaswani et al., 2017) but framed the text-to-text learning tasks using the same vocabulary, sentence piece tokenisation, training, loss, and decoding. The pre-fixed tasks include summarisation, question answering, classification, and translation. T5 used the common crawl corpus and filtered to keep only natural text and de-duplication processing. They extracted 750GB of clean English data to feed into the model for multi-task pre-training. Different masking strategies are integrated into the T5 model to facilitate better performances of specific fine-tuning tasks. Notably, it has been successfully employed in various fields such as Clinical T5 by Lehman and Johnson (2023) and Lu et al. (2022).\nFurthermore, T5's pre-training on an extensive and diverse corpus of text data endows it with a"}, {"title": "SciFive", "content": "Using the framework of T5, SciFive is a Large Language Model pre-trained on the biomedical domain and has demonstrated advanced performances on multiple biomedical NLP tasks (Phan et al., 2021).\nWhile preserving the abilities of T5 in sequence-to-sequence tasks, SciFive offers a deep understanding of medical terminology, concepts, and language structures. As a result, SciFive emerges as a strong candidate for text summarisation and medical language processing tasks, offering the potential to generate clear and accurate simplifications of medical texts.\nSimilarly to our work on T5, we fine-tuned two versions of SciFive, namely SciFive-base and"}, {"title": "OpenAI's GPTS", "content": "Given the remarkable performance demonstrated by OpenAI's GPT models in text simplification(Jeblick et al., 2022), we decided to apply simplifications using \u201cGPT-3.5-turbo\u201d and GPT-4 via their API\u00b9. Both models were accessed. Example prompts we used can be found in Figure 2."}, {"title": "BioGPT", "content": "BioGPT (Luo et al., 2022) is an advanced language model specifically designed for medical text generation. BioGPT is built upon GPT-3 but is specifically trained to understand medical language, terminology, and concepts. BioGPT follows the Transformer language model backbone and is pre-trained on 15 million PubMed abstracts. It has demonstrated a high level of accuracy and has great potential for applications in medicine.\nBioGPT is fine-tuned on our training and validation set, as with other encoder-decoder models (Figure 1) producing our Lay-BioGPT."}, {"title": "BART", "content": "Like the default transformer structure, BART (Lewis et al., 2020) aims to address the issues in BERT and GPT models by integrating their structures with a bi-directional encoder and an autore-gressive decoder. In addition to the single token masking strategy applied in BERT, BART provides various masking strategies, including deletion, span masking, permutation, and rotation of sentences. Compared to GPT models, BART provides both leftward and rightward context in the encoders."}, {"title": "Controllable Mechanisms", "content": "We applied the modified control token strategy in (Li et al., 2022) for both BART-base and BART-large models. The training includes 2 stages, leveraging both Wikilarge training set (Zhang and Lapata, 2017) and our split of training set from PLABA (Attal et al., 2023).\nThe four attributes for control tokens (CTs) are listed below:\n\u2022 <DEPENDENCYTREEDEPTH_x> (DTD)\n\u2022 <WORDRANK_x> (WR)\n\u2022 <REPLACEONLYLEVENSHTEIN_x> (LV)\n\u2022 <LENGTHRATIO_x> (LR)\nThey represent 1) the syntactic complexity, 2) the lexical complexity, 3) the inverse similarity of input and output at the character level, and 4) the length ratio of input and output respectively.\nBefore training, the four CTs are calculated and prepared for the 2 stage training sets. In both stages, we pick the best model in 10 epochs based on the training loss of the validation set. We applied the best model from the first stage as the base"}, {"title": "LORA and LLMs", "content": "To evaluate bigger model architectures, we fine-tune FLAN-T5 XL (Chung et al., 2022) and BioGPT-Large, which have 3 billion and 1.5 billion parameters, respectively. FLAN-T5 XL is based on the pre-trained T5 model with instructions for better zero-shot and few-shot performance. To optimise training efficiency, and as our computational resources do not allow us to fine-tune the full version of these models, we employ the LORA (Hu et al., 2022) technique, which allows us to freeze certain parameters, resulting in more efficient fine-tuning with minimal trade-offs."}, {"title": "Model Development", "content": "The only data set we used for the model fine-tuning development is from PLABA (Attal et al., 2023). So, our team belongs to the data-constrained category if other teams applied broader and external biomedical data for their model development."}, {"title": "Data Preprocessing and Setup", "content": "To investigate the selected models for training and fine-tuning, we divided the original PLABA data into Train, Validation, and Test sets, aiming for an 8:1:1 ratio. However, in the real implementations, we found that there are only a few 1-to-0 sentence pairs, which might cause a negative effect in training the simplification models. Thus we eliminated all 1-to-0 sentence pairs. In addition, to better leverage the SARI score, we picked sentences with multi-references for validation and testing purposes. As a result, we ended up with the following sentence pair numbers according to the source sentences (5757, 814, 814). Note that for"}, {"title": "Automatic Evaluations", "content": "In this section, we list quantitative evaluation scores and some explanations for them. The results for T5 Small, T5 Base, T5 Large, FLAN-T5 XL with LORA, SciFive Base, SciFive Large, and BART models with CTs (BART-w-CTS) are displayed in Table 1. Interestingly, the fine-tuned T5 Small model obtains the highest scores in both BLEU and ROUGE metrics including ROUGE-1, ROUGE-2, and ROUGE-L. The fine-tuned BART Large with CTs (BART-L-w-CTs) produces the highest SARI score at 46.54; while the fine-tuned T5 Base model achieved the highest BERTScore (72.62) with a slightly lower SARI score (44.10). The fine-tuned SciFive Large achieved the highest SARI score (44.38) among T5-like models, though it is approximately 2 points lower than BART Large with CTs.\nThe quantitative evaluation scores of GPT-like models are presented in Table 2 including GPT-3.5 and GPT-4 using prompts, and fine-tuned BioGPT with LoRA. GPT-3.5 reported relatively higher scores than GPT-4 on all lexical metrics except for SARI, and a much higher BERTScore than GPT-4 (58.35 vs 46.99). In comparison, BioGPT-Large with LoRA reported the lowest SARI score (18.44) and the highest BERTScore (62.9) among these three GPT-like models. Comparing the models across Table 1 and Table 2, the GPT-like models did not beat T5-Base on both SARI and BERTScore and did not beat BART-W-CTs on SARI. Therefore, we did not pick GPT-like models for our submission to the shared task, since each team can only submit up to three system outputs.\nTo look into the details of model comparisons from different epochs on the extracted testing set, we present the learning curve of T5, Sci-Five, BART-base on WikiLarge (BART-b-Wiki), and continuously-learned BART-b-Wiki on the PLABA data (our training set only) in Figure 3. We also present the learning curve of T5 Base and"}, {"title": "Human Evaluation", "content": "In our human evaluation, we randomly sampled 80 sentences from the test set split and evaluated the corresponding outputs of BART-large with CTs and T5-base in an anonymised setting. In the human evaluation form, we randomly assigned the order of the two systems' outputs and put them beside the input sentence. Based on the comparison of input and output sentences, the annotators need to answer two questions: \u201cTo what extent do you agree the simplified sentence keeps the major information\" and \"To what extent do you agree the simplified sentence is well simplified\". The answer is limited to a 5-point Likert scale, from"}, {"title": "BeeManc Submissions to PLABA2023", "content": "For system submissions, we chose our domain and task fine-tuned BART-w-CTs as our first system, Biomedical-T5, and Lay-SciFive as our second and third ones.\nFrom the official human evaluation in Figure 5 (upper part), our submission BART-w-CTs ranks 2nd on Simp.sent 92.84, 3rd on simp.term 82.33, among the 7 evaluated systems. Note that BART-w-CTs also produced very comparable scores with the highest system 91.57 vs 93.53, even though it ranked 5th on simp.fluency.\nFrom the official automatic evaluations using 4-reference-based SARI scores in Figure 6, BeeManc team ranked the 2nd after valeknappich team among all teams. Our model BeeManc 3 (Lay-SciFive) ranks 3rd among all system submissions with a score 0.420299 after the systems"}, {"title": "Conclusions and Future Work", "content": "In this system description, we have reported our model development using LLMs and Control Mechanisms for the text simplification task on biomedical abstract using the PLABA data set and our submissions to PLABA-2023 track. From three of our submitted systems BART-w-CTs, Biomedical-T5, and Lay-SciFive, we have the following highlights in the official outcomes:\n\u2022 Our team (BeeManc ) ranks 2nd among all teams in the automatic evaluation using the SARI score.\n\u2022 Our system Lay-SciFive ranks 3rd out of 13 evaluated systems using the SARI score.\n\u2022 BART-w-CTs produced 2nd and 3rd highest scores on sentence-simplicity and term-"}, {"title": "Acknowledgements", "content": "We thank XC for the valuable discussions, thank Brian Ondov and Hoa T. Dang for keeping in touch with us during the shared task. LH and GN are grateful for the support from the grant \u201cAssembling the Data Jigsaw: Powering Robust Research on the Causes, Determinants and Outcomes of MSK Disease\". The project has been funded by the Nuffield Foundation, but the views expressed are those of the authors and not necessarily the Foundation. Visit www.nuffieldfoundation.org.\nLH and GN were also supported by the grant \"Integrating hospital outpatient letters into the"}]}