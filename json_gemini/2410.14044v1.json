{"title": "Best in Tau@LLMJudge: Criteria-Based Relevance Evaluation\nwith Llama3", "authors": ["Naghmeh Farzi", "Laura Dietz"], "abstract": "Traditional evaluation of information retrieval (IR) systems relies on human-annotated rele-\nvance labels, which can be both biased and costly at scale. In this context, large language models\n(LLMs) offer an alternative by allowing us to directly prompt them to assign relevance labels for\npassages associated with each query. In this study, we explore alternative methods to directly\nprompt LLMs for assigned relevance labels, by exploring two hypotheses:\nHypothesis 1 assumes that it is helpful to break down \"relevance\" into specific criteria-\nexactness, coverage, topicality, and contextual fit. We explore different approaches that prompt\nlarge language models (LLMs) to obtain criteria-level grades for all passages, and we consider\nvarious ways to aggregate criteria-level grades into a relevance label. Hypothesis 2 assumes that\ndifferences in linguistic style between queries and passages may negatively impact the automatic\nrelevance label prediction. We explore whether improvements can be achieved by first synthesizing\na summary of the passage in the linguistic style of a query, and then using this summary in place\nof the passage to assess its relevance.\nWe include an empirical evaluation of our approaches based on data from the LLMJudge\nchallenge run in Summer 2024, where our \"Four Prompts\" approach obtained the highest scores\nin Kendall's tau.", "sections": [{"title": "1 Introduction", "content": "The evaluation of information retrieval (IR) systems has historically depended on human-assigned\nrelevance labels to assess the relevance of retrieved passages in response to different queries. However,\nacquiring large-scale relevance judgments from human annotators is costly and not free from biases.\nThe emergence of large language models (LLMs) presents a scalable alternative, offering the potential\nfor automated relevance assessments by directly prompting LLMs to evaluate and assign relevance\nlabels [7, 5, 8, 2]. While this approach facilitates labeling large datasets, it is met with skepticism\nregarding their reliability, primarily due to a lack of transparency in the underlying reasoning.\nWe study the task of automatically predicting passage-level relevance labels, to be used with a\nstandard IR evaluation tool:\nTask Statement: Given a query and a passage, predict the relevance on a graded scale. An\nexample is the scale used in the LLMJudge [6] challenge, which ranges from 0 to 3, where 3 indicates\nperfect relevance and 0 indicates no relevance.\nIn this study, we explore two key ideas for improving the relevance labeling process.\nIn Hypothesis 1, we assume that explicitly grading multiple criteria of relevance will help derive\nthe final relevance label. We develop approaches by individually grading passages according to specific\npredefined relevance criteria that will be detailed in Section 2. To this end, we explore different\nprompt-based grading methods. Additionally, to aggregate the criteria-specific grades into the final\nrelevance labels, we consider different approaches-ranging from prompt-based to using Naive Bayes\naggregations. In line with experience from Chain-of-Thought methods [9], which gained from breaking\ndown complex tasks, we believe this can enhance the evaluation quality in addition to rendering the\ngrading process more interpretable for a human that oversees the evaluation process.\nWith Hypothesis 2 we assume that it is beneficial to substitute the passage with a more query-\nlike representation. Hence we generate a synthesis of the passage text to be in line with the linguistic\nstyle of the query to create a more comparable basis in terms of presentation and content. The goal\nof this synthesis is to capture the overall topic and all relevant facets mentioned within the passage.\nWe study whether bringing the passage into a question form will lead to a more accurate evaluation\nof relevance.\nWe empirically study the extent to which our approaches for assigning relevance labels enhance\nthe evaluation of information retrieval systems, focusing on how well the resulting system rankings\nalign with manual evaluations. We demonstrate a high inter-annotator agreement using Kendall's tau\ncorrelation coefficient [3]. We also analyze the label-wise inter-annotator agreement between predicted\nrelevance labels and manual judgments in terms of Cohen's Kappa and Krippendorff's Alpha [4]\nmeasures. Additionally, the criteria-based grades provided by our Hypothesis 1 methods can offer\nvaluable insights to human evaluators, helping them understand why a passage received a particular\nrelevance judgment.\nOutline. In this study, we develop five relevance labeling approaches to predict such labels for each\nquery-passage pair, each approach is based on one of our two hypotheses. In Section 2, we will develop\napproaches based on Hypothesis 1, which involves grading passages based on predefined relevance\ncriteria before deriving the relevance label. Section 3 will cover an approach based on Hypothesis 2,\nwhere we propose representing passages in a way that aligns with the query's linguistic style before the\ncomparison. Finally, in Section 4, we present our experimental evaluation, comparing the effectiveness\nof these approaches by in terms of label-based inter-annotator agreement, and the correlation of the\nleaderboards, where different systems are ordered based on their evaluation score."}, {"title": "2 Hypothesis 1 Approaches: Relevance Criteria", "content": "Hypothesis 1 posits that \"relevance\" can be broken down into four specific predefined criteria: ex-\nactness, coverage, topicality, and contextual fit, as elaborated below. We explore whether grading\neach criterion individually before deriving an overall relevance label can lead to improved evaluation.\nThis methodology is based on two distinct phases: Criterion-Specific Grading and Relevance Label\nPrediction.\nPhase One: Criterion-Specific Grading: Once the relevance criteria are defined in a structured\nset, each criterion is evaluated independently of the others for every passage-query pair, enabling\na focused assessment of just one relevance criterion at a time.\nPhase Two: Relevance Label Prediction: After grading the criteria, we aggregate the individual\ncriterion grades from Phase One to produce a final relevance label that reflects the passage's\noverall relevance in the context of the query. This aggregation allows for a more structured\nassessment of relevance.\nA human overseeing this automatic process can verify each criterion grade as well as how it is\naggregated to obtain a relevance label, providing an avenue for human interpretability in automatic\ngrading.\nRelevance Criteria. In this study, we are using the following relevance criteria. We will be using\nthe criterion name and description as part of the prompt-based grading approach in Phase One as\nelaborated in Table 2.\n1. Criterion Name: Exactness\nCriterion Description: How precisely does the passage answer the query.\nRationale: Exactness requires that when a query presents a specific question or statement, the\npassage must provide a direct and accurate answer. This criterion emphasizes that the text\nshould address the query with precise information, rather than offering a general explanation"}, {"title": "2.1 Four Prompts", "content": "The \"Four Prompts\" method involves evaluating the relevance of a passage to a query based on four\nspecific predefined relevance criteria elaborated above: Exactness, Coverage, Topicality, and Contex-\ntual Fit. These are graded independently and then aggregated separately. Below, we explain each of\nthe two phases involved in the grading process used in this approach:\nPhase One: Criterion-Specific Grading: For each of the predefined relevance criteria-Exactness,\nCoverage, Topicality, and Contextual Fit\u2014we obtain grades by prompting the LLM to evaluate each\ncriterion individually. The \"Criterion-Specific Grading Prompt\" is detailed in Table 2, where the cri-\nterion name and description are inserted as previously defined, alongside the query and passage text.\nThese four criterion-specific prompts direct the LLM to evaluate and score the passage based on each\ncriterion in accordance with the system message mentioned in Table 2.\nPhase Two: Prompt-Based Aggregation of Criterion Grades into The Relevance Label:\nAfter obtaining the criterion-based grades in Phase One, this phase uses these grades to determine\nthe overall relevance of the passage to the query and predict a single relevance label. An aggregator\nprompt, detailed in Table 3, instructs the LLM to assess the passage's overall relevance by integrating\nthe criterion-specific grades along with the query and passage. This allows the LLM to generate a\ncomprehensive relevance label based on all inputs, including the criterion grades from Phase One."}, {"title": "2.2 Four Prompts + Summation Aggregation", "content": "This method uses the same criterion-specific grades as the \"Four Prompts\" method but aggregates the\ngrades differently to predict the relevance label.\nPhase One: Criterion-Specific Grading: As in \u201cFour Prompts\" methods (Section 2.1).\nPhase Two: Summation-based Aggregation of Criterion Grades into The Relevance\nLabel This phase involves aggregating the criteria grades from Phase One, using a summation-based\napproach. The individual grades from each relevance criterion are summed to produce a total grade.\nTo assign an overall relevance label to the passage, we establish specific thresholds for the total grade,\ndetermining the strictness or leniency of the relevance evaluation. These thresholds can be adjusted\nto align with the desired evaluation standards.\nTable 4, presents a sample mapping between the total grades and relevance labels used in our\nexperiments. This table illustrates how varying total grades are translated into specific relevance\nlabels based on the defined thresholds."}, {"title": "2.3 Four Prompts + Gaussian Naive Bayes Aggregation", "content": "This method also uses the criterion-level grades from the \"Four Prompts\" method as inputs to a\nGaussian Naive Bayes model to predict the relevance label.\nPhase One: Criterion-Specific Grading: As in \"Four Prompts\" methods (Section 2.1).\nPhase Two: Gaussian Naive Bayes Aggregation of Criterion Grades into The Rele-\nvance Label In this phase, we take the criterion grades from Phase One to predict the passages'\nrelevance labels. This is achieved by applying a Gaussian Naive Bayes model, implemented using\nthe sklearn.naive_bayes function from the Scikit-learn framework. This method involves train-\ning a Gaussian Naive Bayes classifier on the relevance criteria grades, allowing it to predict the final\nrelevance label."}, {"title": "2.4 Binary Check + Subset of Four Prompts", "content": "In this approach, we hypothesize that different relevance criteria may carry varying levels of importance\nin different relevance regimes (lower relevance vs. high relevance). This method is based on the\nassumption that if we know whether a passage is more or less relevant, we can more effectively determine\nwhich criteria to prioritize in the relevance label.\nFirst, we use a direct relevance labeling prompt (here we use Sun's prompt [7]) which acts as a\nbinary check heuristic. This initial check categorizes the passage as either relevant or not relevant.\nBased on the results of this binary check, we proceed with a two-phased evaluation process. Phase One\ninvolves criterion-specific grading using a subset of relevance criteria informed by the binary check.\nPhase Two aggregates these criterion-specific grades to produce a final relevance label. Table 5 shows\nthe prompts and system message in this process, and the detailed process provided in Algorithm 1.\nPhase Zero: Binary Relevance Check: Obtain binary relevance by prompting the LLM to provide\na simple \"Yes\" or \"No\" answer regarding the passage's relevance to the query. This relevance check\nallows us to later be able to differentiate passages of lesser from higher relevance.\nPhase One: Criterion-Specific Grading: In this phase, we evaluate the relevance of the passage\nbased on a subset of relevance criteria, depending on the binary relevance response. We use the same\nrelevance-criterion grading prompts as in the \u201cFour Prompts\" method (Section 2.1)."}, {"title": "3 Hypothesis 2 Approaches: Passage-to-Query Generation", "content": "This approach explores the hypothesis that aligning the linguistic styles of passages and queries can im-\nprove relevance judgments. We posit that differences in linguistic styles between queries and passages\ncan complicate comparisons, potentially impacting relevance labeling. To address this, we propose gen-\nerating a query-like representation for each passage (which we call the generated query) that captures\na potential query that this passage would answer. We study whether the predicted relevance label can\nbe improved by comparing the similarity between the original query and this generated query\u2014instead\nof the original passage text.\nThe first phase of this approach involves prompting the LLM to synthesize a query from a passage,\nusing the prompt in Table 6, top. This generated query serves as a representative summary of the\npassage's content, specifically tailored to align with the query's linguistic style and length.\nIn the second phase, we prompt the LLM to evaluate the similarity between the generated query\nand the original query using a scale from 0 to 3, which directly corresponds to our relevance labeling\nsystem. Higher similarity scores indicate a closer alignment between the passage's content and the\nquery's intent. For this assessment, we use the prompt in Table 6, bottom."}, {"title": "4 Experimental Evaluation", "content": "In this section, we present the experimental evaluation of our approaches based on Hypothesis 1,\nwhich involves grading predefined criteria prior to assigning relevance labels, and Hypothesis 2, which\nfocuses on representing the passage in a query-like form to achieve closer linguistic alignment with the\noriginal query prior to comparison."}, {"title": "4.1 Experimental Setup", "content": "Dataset. To assess the effectiveness of our methods, we participated in the LLMJudge challenge as\npart of the LLM4Eval workshop at SIGIR 2024. The challenge is based on queries and run files of the\nTREC Deep Learning 2023 dataset. The organizers collected a set of systems to be evaluated\u00b2, obtained\njudgment pools, and asked challenge participants to predict relevance labels for each passage/query in\nthe pool.\nThe dataset is built upon the passage retrieval task dataset of the TREC 2023 Deep Learning\ntrack\u00b3 (TREC-DL 2023) [1] and includes 25 test queries and 25 development queries, with a total of\n7,224 passages for the test queries and 4,414 passages for the development queries. For both sets, there\nis an average of approximately 81 relevant passages (including both relevant and highly relevant) per\nquery. The relevance grading scale follows that of the TREC Deep Learning track, where grades 0, 1\nindicate non-relevant passages, and 2, 3 relevant passages (higher = more relevant).\nModel setup. All of our methods use an out-of-the-box Meta-Llama-3-8B-Instruct language\nmodel, which comprises 8 billion parameters, deployed on NVIDIA A40 hardware. While any LLM\ncould be used here, we find that this model is particularly well-suited for grading according to different\ncriteria.\nHandling of invalid prompt responses. The current implementation employs error-handling\nstrategies to manage various unwanted situations during the evaluation process.\nWhen a CUDA out-of-memory error is detected often due to a passage exceeding the length limit\nor not being properly cleaned the script logs the error, skips the problematic passage, and assigns a\nlabel of 0. This prevents the evaluation process from halting due to memory constraints and ensures\nthat the process continues smoothly.\nSimilarly, if the predicted label is not a valid integer, the script logs the error and assigns a label\nof 0 to maintain consistency and avoid invalid entries in the results.\nThe script also manages responses from the language model to extract a valid relevance label.\nOccasionally, the model may return a full sentence explaining the answer alongside the numeric rele-\nvance label. To handle this, the script parses the response and extracts the first standalone number\nwithin the range of 0 to 3. This ensures that the final label conforms to the expected grading scale,\nmaintaining accuracy and consistency in the evaluation results. If the label is not acceptable or falls\noutside the expected range, a label of 0 is logged for that query-passage pair.\nEvaluation measures. We focus on three evaluation measures provided by challenge organizers:\nleaderboard correlation measured by Kendall's Tau, and inter-annotator agreement measures by Krip-\npendorff's Alpha and Cohen's Kappa.\nFocusing on the evaluation of submitted systems, our main metric is Kendall's tau, which measures\nthe rank correlation of systems on the leaderboard based on manual assessments (ground truth) and the\npredicted system evaluation scores using the relevance label of each method. Kendall's Tau assesses\nthe ordinal relationship between leaderboard rankings, ranging from -1 to +1, where +1 indicates\nperfect agreement and 0 indicates no agreement. This measure is crucial for evaluating how well\ndifferent evaluation methods rank the quality of retrieval systems. This aligns without the goal of\ndifferentiating retrieval systems by their quality, placing the best systems on the top of the leaderboard,\nand better-performing systems above inferior systems."}, {"title": "4.2 Results", "content": "Leaderboard Correlation (Kendall's Tau). Our findings reveal that the \"Four Prompts\" method\nyields the best Kendall's Tau among all approaches in the LLMJudge challenge. This highlights the\nperformance of using our method for the purpose of evaluating different information retrieval systems\nto study their respective quality in comparison.\nThe superior performance of the \"Four Prompts\" method underscores that breaking down relevance\ninto predefined criteria and using specific criterion grades to assign relevance labels indeed leads to\nimproved evaluation performance.\nInter-annotator Agreement. We focus on leaderboard correlation measures like Kendall's tau, as\nthese are the most important for the purpose of evaluating different systems. However, we acknowledge\nthat LLM-based relevance labeling approaches may also be used to generate synthetic training data.\nFor the purpose of generating training data, it is more important to obtain a high inter-annotator\nagreement (as measured with Cohen's Kappa and Krippendorff's Alpha).\nThe best method in terms of Krippendorff's Alpha is \"Olz-gpt4o\" the best method in terms of\n4-point Cohen's Kappa is \"willia-umbrelal\" and the best method in terms of 01 vs. 23 Cohen's Kappa\nis \"h2oloo-fewself\". These methods obtain a noticeably lower leaderboard correlation.\nJack-of-all-Trades. Among our submitted approaches, the \"TREMA-sumdecompose\" method ob-\ntains a higher inter-annotator score than \u201cTREMA-4prompts\", both in terms of Krippendorff's alpha\nand different Cohen's Kappa, while still outperforming \"Olz-gpt4o\", \"willia-umbrelal\", and \"h2oloo-\nfewself\" in terms of Tau. This demonstrates that the criterion-based approach can yield strong agree-\nment with manual relevance judgments when the aggregation method is chosen differently providing\na good balance between leaderboard correlation and inter-annotator agreement."}]}