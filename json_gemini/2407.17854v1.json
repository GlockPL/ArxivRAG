{"title": "Shapley Value-based Contrastive Alignment for Multimodal Information Extraction", "authors": ["Wen Luo", "Yu Xia", "Shen Tianshu", "Sujian Li"], "abstract": "The rise of social media and the exponential growth of multimodal communication necessitates advanced techniques for Multimodal Information Extraction (MIE). However, existing methodologies primarily rely on direct Image-Text interactions, a paradigm that often faces significant challenges due to semantic and modality gaps between images and text. In this paper, we introduce a new paradigm of Image-Context-Text interaction, where large multimodal models (LMMs) are utilized to generate descriptive textual context to bridge these gaps. In line with this paradigm, we propose a novel Shapley Value-based Contrastive Alignment (Shap-CA) method, which aligns both context-text and context-image pairs. Shap-CA initially applies the Shapley value concept from cooperative game theory to assess the individual contribution of each element in the set of contexts, texts and images towards total semantic and modality overlaps. Following this quantitative evaluation, a contrastive learning strategy is employed to enhance the interactive contribution within context-text/image pairs, while minimizing the influence across these pairs. Furthermore, we design an adaptive fusion module for selective cross-modal fusion. Extensive experiments across four MIE datasets demonstrate that our method significantly outperforms existing state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "The exponential growth of social media platforms has initiated a new phase of communication, characterized by the exchange of multimodal data, primarily texts and images. This diverse landscape necessitates advanced techniques for multimodal information extraction (MIE) [10, 39, 45], which primarily aims to utilize auxiliary image inputs to enhance the performance of identifying entities or relations within the unstructured text.\nTo the best of our knowledge, the majority of previous methods of MIE mainly concentrate on the direct interaction between images and text. These approaches either (1) encode images directly and employ efficient attention mechanisms to facilitate image-text interactions [4, 30, 34, 37, 41, 43, 44], or (2) detect finer-grained visual objects within images and use Graph Neural Networks or attention mechanisms to establish interactions between text and objects [9-11, 17, 18, 42, 45, 46, 51, 53]. Despite the advancements made by these methods, this direct Image-Text interaction paradigm still suffers from the simultaneous existence of semantic and modality gaps. The semantic gap refers to the disparity between the meaning conveyed by the text and the actual content depicted by the image. For instance, in Figure 1, the word \"Rocky\" could refer to the dog in the image but could also be a name for a cat or a person. Furthermore, even the direct alignment of the word \"dog\" with an image of a dog remains a challenge due to the representation inconsistency, resulting in a modality gap. The presence of these two gaps weakens the connection between images and text, potentially leading to erroneous predictions of entities or relations."}, {"title": "2 Related Work", "content": "Multimodal Information Extraction Multimodal Information Extraction (MIE) is an evolving research domain primarily aimed at enhancing the recognition of entities and relations by utilizing supplemental image inputs. This field can be primarily subdivided into three critical tasks: Multimodal Named Entity Recognition (MNER), Multimodal Relation Extraction (MRE) and Multimodal Joint Entity-Relation Extraction (MJERE). In particular, the tasks of MNER [37, 39] and MRE [10, 51] are concerned with identifying entities and relations separately, while the MJERE task[45] aims to extract entities and their associated relations jointly. Most of existing methods are concentrated around the paradigm of direct Image-Text interaction [9, 30, 34, 37, 42-45, 51, 53]. Wang et al. [41] designed a fine-grained cross-modal attention module to enhance the cross-modal alignment. Sun et al. [37], Xu et al. [43], and Liu et al. [28] focus on quantifying and controlling the influence of images on texts through gate mechanisms or text-image relationship inference. To facilitate the alignment between visual objects and text, Zhao et al. [50] and Yuan et al. [45] propose a heterogeneous graph network and an edge-enhanced graph neural network, respectively. Additionally, there are some other studies [18, 39] that employ external knowledge, such as machine reading comprehension [18], to foster model reasoning. Regardless of these substantial advancements, these approaches still fail to address the potential dual gaps existing between images and texts due to representation inconsistencies. In this study, we aim to comprehensively consider these semantic and modality gaps and propose Shap-CA, which leverages the bridging context to mitigate these gaps and achieve a more coherent and compatible representation.\nLarge Multimodal Models Large multimodal models have recently gained substantial traction in the research community [2, 23]. Similar to the trend observed with large language models, several studies have indicated that scaling up the training data [6, 12, 49] or model size [6, 31] can significantly enhance these large multimodal"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Task Definition", "content": "Given an input text t = {t1, ..., tn\u2081 } with nt tokens and its attached image I, our method aims to predict the output entity/relation labels y. The format of the labels y is task-dependent. Specifically, for MNER, they are sequential labels. For MRE and MJERE, they are word-pair labels [45]."}, {"title": "3.2 Overview", "content": "The architecture of Shap-CA is shown in Figure 2. Initially, we utilize a LMM, e.g., LLaVA-1.5 [26], to extract the textual bridging context from the image. Subsequently, we employ a pretrained textual transformer to extract features from the text and the context. In parallel, an image encoder is used to derive visual features from the image. Following this, we apply a Shapley value-based contrastive alignment to construct more coherent representations. The adaptive fusion module is then employed to obtain the informative features across modalities. Finally, these comprehensive representations are fed into a CRF or a word-pair contrastive layer for prediction."}, {"title": "3.3 Encoding Module", "content": "Context Generation Given an image I, we utilize a pretrained LMM to generate a textual context as the bridging context c = {c1,..., cnc} with nc tokens.\nTextual and Visual Encoding In order to acquire contextualized textual features, we concatenate the input text t with the bridging context c. Following this, we employ a pretrained textual transformer to extract both sentence-level and token-level features:\nxt, Ht, xc, Hc = Transformer([t; c])\nwhere xt, xc \u2208 Rd represent the sentence-level features, while Ht \u2208 Rnt\u00d7d, Hc \u2208 Rnc\u00d7d denote the token-level features of the input text and context, respectively. Simultaneously, we employ an image encoder to extract the visual features of the image I:\nxv, H = ImageEncoder(I)\nwhere xv, Ho denotes the global visual feature and the regional visual features of the image, respectively."}, {"title": "3.4 Shapley Value-based Contrastive Alignment", "content": "To construct more coherent representations, we leverage the Shapley value to perform both context-text alignment and context-image alignment. The Shapley value offers a solution for the equitable allocation of total utility among players based on their individual marginal contributions and has widespread application [15, 16, 20]."}, {"title": "3.4.1 Preliminary.", "content": "In the context of a cooperative game, suppose we have k players, represented by K = {1, ..., k}, and a utility function u : 2k \u2192 R that assigns a reward to each coalition (subset) of players. The Shapley value of player i is then defined as [19]:\n\u03a6i(u) = \u2211\nS\u2286K\\{i}\n|S|!\n(k \u2212 |S| \u2212 1)!\nk!\n[u(S \u222a {i}) \u2212 u(S)]"}, {"title": "3.4.2 Context-Text Alignment.", "content": "Shapley Value Approximation. In the context-text alignment, inputs are a mini-batch of k context-text pairs {(xca, xta)}k\na=1. Here, we view the k bridging contexts as players, denoted as K = {1,..., k} for simplicity. These players, or contexts, collaboratively contribute to the semantic comprehension of a specific text feature. Consider the j-th pooled text feature, x(j)t, and a selected subset of context players, denoted as a coalition S \u2286 K. The central idea is based on an assumption: if all the contexts within the subset S and the text xt form positive pairs, the utility of S for xt would be represented by the expected semantic overlap between them. This utility captures the collective semantic relationships between the text and the contexts within the coalition, as formalized by:\nuj(S) = \u2211\nies\npisim(xt, xci)\npi =\nesim(xt,xci)/\u03c4\n\u2211a\u2208S esim(xt,xca)/\u03c4\nHere, sim(xt, xci) denotes the semantic overlap between the text and each context (i.e., individual semantic contribution), measured by cosine similarity. The weight pi, computed through a softmax operation with a temperature of \u03c4, models the cooperative behavior among different contexts by normalizing these individual semantic contributions. This approach intuitively suggests that the stronger the semantic overlap a context shares with the text (i.e., the larger the semantic contribution), the more likely it is to form a true pair with the text in real-world situations. From this perspective, the utility of the coalition can be interpreted as an expectation over the semantic overlaps of each context within S with the text, where the expectation weights are given by the likelihood of each context-text pair being positive. This method naturally prioritizes contexts that have a higher degree of semantic overlap with the text, thereby refining the overall semantic understanding.\nHowever, as indicated by Eqn. 3, the computation of the Shapley value requires an exponentially large number of computations relative to the size of the mini-batch, which poses a challenge during training. To address this, we extend Monte-Carlo approximation methods [8, 33] to our training setting for estimating the Shapley value. We present the detailed algorithm in Alg. 1. It begins with a random permutation of k context players and an initial stride of s, which is set to batch_size/2. At each iteration, the algorithm scans each player in the current permutation and calculates the"}, {"title": "3.4.3 Context-Image Alignment.", "content": "Similarly, we can employ Alg. 1 to estimate the Shapley value for a mini-batch of k context-image pairs {(xca, xva)}k\na=1 and derive the context-to-image loss Lc2v and the image-to-context loss Lv2c. The modality loss Lmodality is the average of these two contrastive loss:\nLmodality=\n1\n2\n(Lc2v + Lv2c)"}, {"title": "3.5 Adaptive Fusion", "content": "To facilitate a finer-grained fusion across modalities, we develop an adaptive attention fusion module. This module dynamically weights the importance of different features in two modalities, based on their relevance to the context that connects them. Given the representations Ht, Hc, Hv obtained from Section 3.3, we initially employ linear projection to transform them into a set of matrices: a query matrix Qv \u2208 Rno\u00d7D, a key matrix Kt \u2208 Rnt\u00d7D, a value matrix Vt \u2208 Rnt\u00d7D, and a context matrix C\u2208 Rnc\u00d7D. Subsequently, we calculate the bridging term B as follows:\nB = mean\nCC\nVD\nERD"}, {"title": "3.6 Classifier", "content": "In addition to the image-awared text features derived from Eqn. 11, we also incorporate each token's part-of-speech embeddings MS \u2208 Rnt\u00d7d1, and positional embeddings MP \u2208 Rnt\u00d7d1, to enrich the information available for decoding. For MNER, a widely used CRF layer [1, 21] is employed to predict label sequences. For MRE and MJERE, we propose a word-pair contrastive layer to predict word-pair labels [45].\nCRF layer Initially, we employ a multi-layer perceptron to integrate the three channels of features:\nM = MLP1(Mt; Ms; MP) \u2208 Rnt\u00d7d2\nSubsequently, we feed M = {M1, ..., Mnt } into a standard CRF layer to derive the final distribution P(y|t). The training loss for the input sequence t with gold labels y* is measured using the negative log-likelihood (NLL):\nLmain = -log P(y*|t)\nWord-Pair Contrastive layer For each word pair (ti, tj), we initially employ three multi-layer perceptrons to separately obtain the three channels of pair-wise features:\nXi,j = MLP1(Mt; Ms; MP \u2212 MP) \u2208 Rd2\nwhere l\u2208 {t, s, p} represents the different feature channels. Following this, we use only the text features to generate the initial distribution P(0)i,j and incorporate the three channels of features to derive the enhanced distribution P(t,s,p)i,j over the label set:\nP(0)i,j = Softmax(MLP2(Xt))\nP(t,s,p)i,j = Softmax(MLP3(Xt, Xs, XP))\nThe final distribution is refined by contrasting the two distributions:\npfinali,j = Softmax(Pt,s,p + \u03bblogP0i,jPt,s,pp0i,j)\nwhere \u03bb is the refinement scale. We use the cross-entropy loss for the input sequence t and the gold word-pair labels y*:\nLmain = \u2212 \u2211nt\ni=1 \u2211nt\nj=1y\u2217i,jlog(pfinali,j)"}, {"title": "3.7 Training", "content": "In summary, our framework incorporates two self-supervised learning tasks and one supervised learning task, resulting in three loss functions. Considering the semantic loss Lsemantic, the modality loss Lmodality from Sec. 3.4, and the prediction loss Lmain from Sec. 3.6, the final loss function is defined as follows:\nL = \u03b1Lsemantic + \u03b2Lmodality + Lmain\nwhere \u03b1, \u03b2 are hyperparameters."}, {"title": "4 Experiments", "content": "We conduct our experiments on four MIE datasets and compare our method with a number of previous approaches."}, {"title": "4.1 Datasets", "content": "Experiments are conducted on 2 MNER datasets, 1 MRE dataset and 1 MJERE dataset: Twitter-15 [48] and Twitter-17 [44] for MNER\u00b9, MNRE2 [52] for MRE, and MJERE\u00b3 [45] for MJERE, noting that the last dataset and the task share the same name. These datasets contain 4,000/1,000/3,357, 3,373/723/723, 12,247/1,624/1,614, and 3,617/495/474 samples in the training, validation, and test splits, respectively."}, {"title": "4.2 Implementation Details", "content": "Model Configuration In order to fairly compare with the previous approaches [13, 17, 40-43, 45, 46, 51], we use BERT-based model with the dimension of 768 as the textual encoder. For the visual encoder, we experiment with the ViTB/32 in CLIP and ResNet152 models as potential alternatives. We find ResNet152 with the dimension of 2048 to provide the most effective and consistent results in our settings. To generate the textual descriptive contexts of images, we use LLaVA-1.5 [26], a newly proposed visual instruction-tuned, large multimodal model. The sampling temperature is set to 1.0. We apply the spaCy library of the en_core_web_sm version to parse the given sentence and obtain each word's part of speech. The dimensions of positional embeddings and part-of-speech embeddings are 100.\nTraining Configuration Shap-CA is trained by Pytorch on a single NVIDIA RTX 3090 GPU. During training, the model is finetuned by AdamW [29] optimizer with a warmup cosine scheduler of ratio 0.05 and a batch size of 28. We use the grid search to find the learning rate over [1 \u00d7 10\u22125,5 \u00d7 10\u22124]. The learning rate of encoders is set to 5 \u00d7 10\u22125. The learning rates of other modules are set to 3 \u00d7 10\u22124, 1 \u00d710\u22124, 1\u00d710-4 and 1\u00d710-4 for Twitter-15, Twitter-17, MJERE and MNRE, respectively. The refinement scale \u03bb in Eqn. 17 is set to 1.0. An early stopping strategy is applied to determine the number of training epochs with a patience of 7. We choose the model performing the best on the validation set and evaluate it on the test set. We report the average results from 3 runs with different random seeds."}, {"title": "4.3 Baselines", "content": "We compare Shap-CA with previous state-of-the-art methods, which primarily fall into two categories. The first group of methods only consider text input, including BERT-CRF [13], MTB [7], ChatGPT and GPT-4. Secondly, we consider several latest multimodal methods, including OCSGA [42], UMGF [46], MEGA [51], MAF [43], ITA [40], CAT-MNER [41], MNER-QG [17], EEGA [45], and MQA [38]."}, {"title": "4.4 Main Results", "content": "Table 1 provides a comprehensive comparison of our proposed method with baseline approaches across various modalities, including text-based methods and previous state-of-the-art multimodal methods. Through this comparative analysis, we get several noteworthy findings as follows: (1) Superior Performance of Shap-CA: Our method, Shap-CA, demonstrates significant superiority across all datasets compared to baseline approaches. Notably, in tasks involving entity or relation extraction, such as Twitter-15, Twitter-17, MJEREe, and MNRE, Shap-CA consistently outperforms the best competitor by substantial margins of 1.33%, 1.07%, 0.7%, and 1.05% in terms of F1 scores, respectively. These results underscore the effectiveness of our proposed Image-Context-Text paradigm in enhancing information extraction tasks. (2) Effective Handling of Complex Tasks: In the most challenging task, MJEREj, which requires simultaneous extraction of entities and their associated relations, Shap-CA achieves the highest F1 score of 55.95%. This outcome highlights Shap-CA's efficacy in managing complex MIE scenarios, where accurate identification of both entities and relations is crucial. (3) Remarkable Adaptability: In contrast to other models that are typically specialized in one or two specific tasks, Shap-CA demonstrates exceptional adaptability by consistently delivering state-of-the-art performance across a diverse range of tasks, which underscores the model's ability to adapt to various environments and tasks, highlighting its flexibility in managing different information extraction challenges. (4) Enhanced Performance with Visual Information: Incorporating visual information from images generally enhances the performance of MIE when comparing state-of-the-art multimodal methods to their text-only counterparts. This observation suggests the integration of visual information provides a more holistic understanding of the input, underlining its value in improving the accuracy of information extraction tasks, particularly in scenarios where text contents alone may be insufficient for prediction. (5) Performance of Large Models: Contrary to the common belief that larger models possess superior generalization abilities, our experimental results show that methods based on LLMs (e.g., ChatGPT, GPT-4) and LMMs (e.g., MQA) perform less well than our proposed method and other previous multimodal approaches. This suggests that the current large models, despite their extensive capabilities, might not yet be fully optimized for information extraction tasks. This potential discrepancy could arise from their open-ended generative nature and pre-training scenarios that are misaligned with the specifics of MIE."}, {"title": "5 Discussion and Analysis", "content": ""}, {"title": "5.1 Ablation Study", "content": "We conduct a comprehensive ablation study to further analyze the effectiveness of our method. The results of these model variants are presented in Table 1 and Table 2."}, {"title": "5.2 Further Analysis", "content": "How Alignments Affect Performance In this section, we further discuss the influence of alignments on our model's performance on the MJERE dataset. Specifically, we examine two key coefficients, \u03b1 and \u03b2 in Eqn. 19, which represent context-text alignment and context-image alignment, respectively. As shown in Figure 3, the model's performance exhibits a consistent pattern of variation in response to changes in \u03b1 and \u03b2. The red line draws the performance trend of changing \u03b1 when fixing \u03b2 as 0.4. \u03b1 = 0 means w/o context-text alignment, resulting in the inability to effectively bridge the semantic gap. When \u03b1 = 0.2, the model reaches its optimal performance. However, when \u03b1 exceeds 0.2, the alignment task may interfere with the main task, leading to declining performance. The blue line draws the performance trend of \u03b2 with the fixed value of \u03b1 (i.e., 0.2), which is similar to that of \u03b1. The peak performance is reached when \u03b2 is 0.4.\nImpact of Different Context Generators In practical situations, robustness to a range of contexts from different generators is crucial, as these contexts may vary significantly in quality. To investigate this, we conduct experiments to assess the impact of employing various context generators. Specifically, we examine the results produced by five different context generators when applied to the MJERE dataset. These generators include traditional image captioning models: BU [3] and VinVL [47], and newly developed large multimodal models: BLIP-2 [24], LLaVA-1.5 [26], and GPT-4. As shown in Table 3, the performance disparity across these context generators is minimal. Notably, the use of GPT-4 as a context generator achieves the best overall performance. This observation implies that our method is robust to the variability inherent in different context generators, maintaining consistent performance regardless of the quality of the generated context.\nModel Scale The scale of a model is a critical aspect of real-world applications. We present a comparative analysis of the total parameters and performance of our proposed method, Shap-CA, alongside other existing state-of-the-art multimodal approaches, whose results are achieved by their official implementations. As shown in Table 4, Shap-CA, with a parameter count of only 170.7M, is the most lightweight model among those evaluated, suggesting a reduced computational burden."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a novel paradigm of Image-Context-Text interaction to address the challenges posed by the semantic and modality gaps in conventional MIE approaches. In line with this paradigm, we propose a novel method, Shapley Value-Based Contrastive Alignment (Shap-CA). Shap-CA first employs the Shapley value to access the individual contributions of contexts, texts, and images to the overall semantic or modality overlaps, and then applies a contrastive learning strategy to perform the alignment by maximizing the interactive contribution within context-text/image pairs and minimizing the influence across these pairs. Furthermore, Shap-CA incorporates an adaptive fusion module for selective cross-modal fusion. Extensive experiments across four MIE datasets demonstrate that Shap-CA significantly outperforms previous state-of-the-art approaches."}]}