{"title": "Shapley Value-based Contrastive Alignment for Multimodal Information Extraction", "authors": ["Wen Luo", "Yu Xia", "Shen Tianshu", "Sujian Li"], "abstract": "The rise of social media and the exponential growth of multimodal communication necessitates advanced techniques for Multimodal Information Extraction (MIE). However, existing methodologies primarily rely on direct Image-Text interactions, a paradigm that often faces significant challenges due to semantic and modality gaps between images and text. In this paper, we introduce a new paradigm of Image-Context-Text interaction, where large multimodal models (LMMs) are utilized to generate descriptive textual context to bridge these gaps. In line with this paradigm, we propose a novel Shapley Value-based Contrastive Alignment (Shap-CA) method, which aligns both context-text and context-image pairs. Shap-CA initially applies the Shapley value concept from cooperative game theory to assess the individual contribution of each element in the set of contexts, texts and images towards total semantic and modality overlaps. Following this quantitative evaluation, a contrastive learning strategy is employed to enhance the interactive contribution within context-text/image pairs, while minimizing the influence across these pairs. Furthermore, we design an adaptive fusion module for selective cross-modal fusion. Extensive experiments across four MIE datasets demonstrate that our method significantly outperforms existing state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "The exponential growth of social media platforms has initiated a new phase of communication, characterized by the exchange of multimodal data, primarily texts and images. This diverse landscape necessitates advanced techniques for multimodal information extraction (MIE) [10, 39, 45], which primarily aims to utilize auxiliary image inputs to enhance the performance of identifying entities or relations within the unstructured text.\nTo the best of our knowledge, the majority of previous methods of MIE mainly concentrate on the direct interaction between images and text. These approaches either (1) encode images directly and employ efficient attention mechanisms to facilitate image-text interactions [4, 30, 34, 37, 41, 43, 44], or (2) detect finer-grained visual objects within images and use Graph Neural Networks or attention mechanisms to establish interactions between text and objects [9\u201311, 17, 18, 42, 45, 46, 51, 53]. Despite the advancements made by these methods, this direct Image-Text interaction paradigm still suffers from the simultaneous existence of semantic and modality gaps. The semantic gap refers to the disparity between the meaning conveyed by the text and the actual content depicted by the image. For instance, in Figure 1, the word \u201cRocky\u201d could refer to the dog in the image but could also be a name for a cat or a person. Furthermore, even the direct alignment of the word \u201cdog\u201d with an image of a dog remains a challenge due to the representation inconsistency, resulting in a modality gap. The presence of these two gaps weakens the connection between images and text, potentially leading to erroneous predictions of entities or relations.\nTo mitigate the semantic and modality gaps, we propose an approach that relies on an intermediary, rather than direct interactions between texts and images. Given that text is the primary source of information extraction and images serve a supporting role, we suggest the use of a textual intermediary to bridge these gaps. As large multimodal models (LMMs) demonstrate impressive results on instruction-following and visual comprehension [26, 27], and excel at generating descriptive textual context for images, we employ the powerful generative ability of LMMs to create an intermediate context for reducing the burden of aligning image and text directly. As shown in Figure 1, with the divide-and-conquer strategy, we only need to model the connections between the generated context and the original text or image, forming an Image-Context-Text interaction paradigm.\nThe core challenge now is aligning the context-text pairs and context-image pairs in the hidden feature space, respectively, where the alignment with text bridges the semantic gap, and the alignment with images addresses the modality gap. In this scenario, contexts, texts, and images can be considered as three kinds of elements in the hidden space trying to achieve optimal collective semantic and modality overlaps. Drawing inspiration from cooperative game theory, we adapt the concept of the Shapley value [14], which provides an equitable mechanism for assessing each individual\u2019s contribution towards collective utility. In this paper, we propose a novel Shapley Value-based Contrastive Alignment (Shap-CA) method, which leverages a contrastive learning strategy to construct coherent and compatible representations for these three elements based on their Shapley values. Within this framework, each element is considered a player, and the collective utility of a set of elements is defined as the total semantic or modality overlap. Shap-CA initially calculates the average marginal contribution of each player to the collective utility to estimate their Shapley value. Intuitively, contexts contributing significantly to the overall overlap (i.e., having a larger Shapley value) should have a higher probability of forming a true pair with the text or image. Consequently, a contrastive learning strategy is then employed to enhance the interactive contribution within context-text/image pairs, while minimizing the influence across these pairs. Through this process, Shap-CA not only strengthens the intrinsic connections within each pair but also accentuates the disparities between different pairs, effectively bridging the semantic and modality gaps. Furthermore, we design an adaptive fusion module to obtain the informative fused features across modalities. This module assesses the relevance of each modal feature to the bridging context, strategically weighing their importance to achieve a finer-grained selective cross-modal fusion. Finally, a linear-chain CRF [22] or a word-pair contrastive layer is employed for prediction.\nOverall, the main contributions of this paper can be summarized as follows:\n\u2022 We are the first to introduce the Image-Context-Text interaction paradigm and leverage LMMs to generate descriptive context as a bridge to mitigate semantic and modality gaps for MIE.\n\u2022 We propose a novel Shapley value-based contrastive alignment method, capturing semantic and modality relationships within and across image-text pairs for effective multimodal representations.\n\u2022 Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art methods on four MIE datasets."}, {"title": "2 Related Work", "content": "Multimodal Information Extraction Multimodal Information Extraction (MIE) is an evolving research domain primarily aimed at enhancing the recognition of entities and relations by utilizing supplemental image inputs. This field can be primarily subdivided into three critical tasks: Multimodal Named Entity Recognition (MNER), Multimodal Relation Extraction (MRE) and Multimodal Joint Entity-Relation Extraction (MJERE). In particular, the tasks of MNER [37, 39] and MRE [10, 51] are concerned with identifying entities and relations separately, while the MJERE task[45] aims to extract entities and their associated relations jointly. Most of existing methods are concentrated around the paradigm of direct Image-Text interaction [9, 30, 34, 37, 42\u201345, 51, 53]. Wang et al. [41] designed a fine-grained cross-modal attention module to enhance the cross-modal alignment. Sun et al. [37], Xu et al. [43], and Liu et al. [28] focus on quantifying and controlling the influence of images on texts through gate mechanisms or text-image relationship inference. To facilitate the alignment between visual objects and text, Zhao et al. [50] and Yuan et al. [45] propose a heterogeneous graph network and an edge-enhanced graph neural network, respectively. Additionally, there are some other studies [18, 39] that employ external knowledge, such as machine reading comprehension [18], to foster model reasoning. Regardless of these substantial advancements, these approaches still fail to address the potential dual gaps existing between images and texts due to representation inconsistencies. In this study, we aim to comprehensively consider these semantic and modality gaps and propose Shap-CA, which leverages the bridging context to mitigate these gaps and achieve a more coherent and compatible representation.\nLarge Multimodal Models Large multimodal models have recently gained substantial traction in the research community [2, 23]. Similar to the trend observed with large language models, several studies have indicated that scaling up the training data [6, 12, 49] or model size [6, 31] can significantly enhance these large multimodal models\u2019 capabilities. Moreover, visual instruction tuning can equip large multimodal models with excellent instruction-following, visual understanding, and natural language generation abilities [27]. This advancement empowers these models to excel in interpreting images according to instructions and generating informative textual contexts. However, their open-ended generative characteristics have resulted in less than satisfactory performance when directly applied to information extraction tasks [38]."}, {"title": "3 Methodology", "content": "3.1 Task Definition\nGiven an input text $t = \\{t_1, ..., t_{n_t} \\}$ with $n_t$ tokens and its attached image $I$, our method aims to predict the output entity/relation labels $y$. The format of the labels $y$ is task-dependent. Specifically, for MNER, they are sequential labels. For MRE and MJERE, they are word-pair labels [45].\n3.2 Overview\nThe architecture of Shap-CA is shown in Figure 2. Initially, we utilize a LMM, e.g., LLaVA-1.5 [26], to extract the textual bridging context from the image. Subsequently, we employ a pretrained textual transformer to extract features from the text and the context. In parallel, an image encoder is used to derive visual features from the image. Following this, we apply a Shapley value-based contrastive alignment to construct more coherent representations. The adaptive fusion module is then employed to obtain the informative features across modalities. Finally, these comprehensive representations are fed into a CRF or a word-pair contrastive layer for prediction.\n3.3 Encoding Module\nContext Generation Given an image $I$, we utilize a pretrained LMM to generate a textual context as the bridging context $c = \\{c_1,..., c_{n_c}\\}$ with $n_c$ tokens.\nTextual and Visual Encoding In order to acquire contextualized textual features, we concatenate the input text $t$ with the bridging context $c$. Following this, we employ a pretrained textual transformer to extract both sentence-level and token-level features:\n$x_t, H_t, x_c, H_c = Transformer([t; c])\\qquad(1)$\nwhere $x_t, x_c \\in \\mathbb{R}^d$ represent the sentence-level features, while $H_t \\in \\mathbb{R}^{n_t \\times d}, H_c \\in \\mathbb{R}^{n_c \\times d}$ denote the token-level features of the input text and context, respectively. Simultaneously, we employ an image encoder to extract the visual features of the image $I$:\n$x_v, H_o = ImageEncoder(I)\\qquad(2)$\nwhere $x_v, H_o$ denotes the global visual feature and the regional visual features of the image, respectively.\n3.4 Shapley Value-based Contrastive Alignment\nTo construct more coherent representations, we leverage the Shapley value to perform both context-text alignment and context-image alignment. The Shapley value offers a solution for the equitable allocation of total utility among players based on their individual marginal contributions and has widespread application [15, 16, 20].\nWe are the first to utilize the Shapley value for multimodal alignment through contrastive learning.\n3.4.1 Preliminary. In the context of a cooperative game, suppose we have $k$ players, represented by $K = \\{1, ..., k\\}$, and a utility function $u : 2^K \\rightarrow \\mathbb{R}$ that assigns a reward to each coalition (subset) of players. The Shapley value of player $i$ is then defined as [19]:\n$\\Phi_i(u) = \\sum_{S\\subset K\\backslash\\{i\\}} { {|S|\\choose k-1}}^{-1} [u(S \\cup \\{i\\}) - u(S)]\\qquad(3)$\nThe Shapley value essentially quantifies the average marginal contribution of a player to all potential coalitions (subsets). We detail the context-text alignment as an example as follows.\n3.4.2 Context-Text Alignment.\nShapley Value Approximation. In the context-text alignment, inputs are a mini-batch of $k$ context-text pairs $\\{(x_c^a, x_t^a)\\}_{a=1}^k$. Here, we view the $k$ bridging contexts as players, denoted as $K = \\{1,..., k\\}$ for simplicity. These players, or contexts, collaboratively contribute to the semantic comprehension of a specific text feature. Consider the $j$-th pooled text feature, $x_t^{(j)}$, and a selected subset of context players, denoted as a coalition $S \\subset K$. The central idea is based on an assumption: if all the contexts within the subset $S$ and the text $x_t$ form positive pairs, the utility of $S$ for $x_t$ would be represented by the expected semantic overlap between them. This utility captures the collective semantic relationships between the text and the contexts within the coalition, as formalized by:\n$u_j(S) = \\sum_{i\\in S} p_i \\text{sim}(x_t^{(j)}, x_c^i)$\n$p_i = \\frac{e^{\\text{sim}(x_t^{(j)}, x_c^i) / \\tau}}{\\sum_{a \\in S} e^{\\text{sim}(x_t^{(j)}, x_c^a) / \\tau}}\\qquad(4)$\nHere, $\\text{sim}(x_t^{(j)}, x_c^i)$ denotes the semantic overlap between the text and each context (i.e., individual semantic contribution), measured by cosine similarity. The weight $p_i$, computed through a softmax operation with a temperature of $\\tau$, models the cooperative behavior among different contexts by normalizing these individual semantic contributions. This approach intuitively suggests that the stronger the semantic overlap a context shares with the text (i.e., the larger the semantic contribution), the more likely it is to form a true pair with the text in real-world situations. From this perspective, the utility of the coalition can be interpreted as an expectation over the semantic overlaps of each context within $S$ with the text, where the expectation weights are given by the likelihood of each context-text pair being positive. This method naturally prioritizes contexts that have a higher degree of semantic overlap with the text, thereby refining the overall semantic understanding.\nHowever, as indicated by Eqn. 3, the computation of the Shapley value requires an exponentially large number of computations relative to the size of the mini-batch, which poses a challenge during training. To address this, we extend Monte-Carlo approximation methods [8, 33] to our training setting for estimating the Shapley value. We present the detailed algorithm in Alg. 1. It begins with a random permutation of $k$ context players and an initial stride of $s$, which is set to batch_size/2. At each iteration, the algorithm scans each player in the current permutation and calculates the\nmarginal contribution when the player is added to the coalition formed by the preceding players. This marginal contribution serves as an unbiased estimate of the Shapley value for that player, improving with each iteration. Subsequently, the algorithm updates the permutation through a cyclic shift of the current stride and halves the stride for the next iteration. This gradual reduction in stride results in more stable marginal contributions over time, as the size of the subsets for each player changes less dramatically on average. The process continues until the stride falls to zero.\nContrastive Learning. After acquiring the approximated Shapley value $\\{\\Phi_1 (u_j), ..., \\Phi_k (u_j) \\}$, we introduce a context-to-text contrastive loss that aims to maximize the average marginal semantic contribution from each context player to the text with which it forms a true positive pair (i.e., the text from the same pair), while simultaneously minimizing the contributions between the true negative pairs (i.e., all other texts not paired with this context).\n$L_{c2t} = - \\sum_{j=1}^k \\Phi_j (u_j) + \\sum_{i \\neq j} \\Phi_i (u_j)\\qquad(5)$\nIn a symmetrical manner, we can treat the $k$ pooled text as players, and derive the text-to-context contrastive loss $L_{t2c}$. The semantic loss $L_{semantic}$ is the average of these two contrastive losses:\n$L_{semantic} = \\frac{1}{2} (L_{c2t} + L_{t2c})\\qquad(6)$\n3.4.3 Context-Image Alignment. Similarly, we can employ Alg. 1 to estimate the Shapley value for a mini-batch of $k$ context-image pairs $\\{(x_c^a, x_v^a)\\}_{a=1}^k$ and derive the context-to-image loss $L_{c2v}$ and the image-to-context loss $L_{v2c}$. The modality loss $L_{modality}$ is the average of these two contrastive loss:\n$L_{modality} = \\frac{1}{2} (L_{c2v} + L_{v2c})\\qquad(7)$\n3.5 Adaptive Fusion\nTo facilitate a finer-grained fusion across modalities, we develop an adaptive attention fusion module. This module dynamically weights the importance of different features in two modalities, based on their relevance to the context that connects them. Given the representations $H_t, H_c, H_v$ obtained from Section 3.3, we initially employ linear projection to transform them into a set of matrices: a query matrix $Q_v \\in \\mathbb{R}^{n_o \\times D}$, a key matrix $K_t \\in \\mathbb{R}^{n_t \\times D}$, a value matrix $V_t \\in \\mathbb{R}^{n_t \\times D}$, and a context matrix $C \\in \\mathbb{R}^{n_c \\times D}$. Subsequently, we calculate the bridging term $B$ as follows:\n$B = \\text{mean} (\\frac{CC^T}{\\sqrt{D}}) \\frac{V_t}{\\mathbb{R}^D}\\qquad(8)$\nThis bridging term is then employed to dynamically modify the content of queries $Q_v$ and keys $K_t$ based on their relevance to the bridging term:\n$Q_v' = g_q \\odot Q_v + (1 - g_q) \\odot B\\qquad K_t' = g_k \\odot K_t + (1 - g_k) \\odot B\\qquad(9)$\nwhere $\\odot$ represents the Hadamard product, $g_q \\in \\mathbb{R}^{n_o \\times D}$ and $g_k \\in \\mathbb{R}^{n_t \\times D}$ are gating vectors that are used to capture the relevance between the text (or image) and the bridging context:\n$g_q = \\text{tanh}(\\text{Linear}_1 (Q_v, B))\\qquad g_k = \\text{tanh}(\\text{Linear}_2 (K_t, B))\\qquad(10)$\nFollowing this, we acquire the image-awared text features $M^t \\in \\mathbb{R}^{n_t \\times D}$ using the gated cross-attention:\n$M^t = \\text{CrossAttention}(Q_v', K_t', V_t)\\qquad(11)$\n3.6 Classifier\nIn addition to the image-awared text features derived from Eqn. 11, we also incorporate each token\u2019s part-of-speech embeddings $M^s \\in \\mathbb{R}^{n_t \\times d_1}$, and positional embeddings $M^p \\in \\mathbb{R}^{n_t \\times d_1}$, to enrich the information available for decoding. For MNER, a widely used CRF layer [1, 21] is employed to predict label sequences. For MRE and MJERE, we propose a word-pair contrastive layer to predict word-pair labels [45].\nCRF layer Initially, we employ a multi-layer perceptron to integrate the three channels of features:\n$M = \\text{MLP}_1 (M^t; M^s; M^p) \\in \\mathbb{R}^{n_t \\times d_2}\\qquad(12)$\nSubsequently, we feed $M = \\{M_1, ..., M_{n_t} \\}$ into a standard CRF layer to derive the final distribution $P(y|t)$. The training loss for the input sequence $t$ with gold labels $y^*$ is measured using the negative log-likelihood (NLL):\n$L_{main} = -\\text{log} P(y^*|t)\\qquad(13)$\nWord-Pair Contrastive layer For each word pair $(t_i, t_j)$, we initially employ three multi-layer perceptrons to separately obtain the three channels of pair-wise features:\n$X^l_{i,j} = \\text{MLP}_l (M_i; M_j; |M_i - M_j|)\\in \\mathbb{R}^{d_2}\\qquad(14)$\nwhere $l \\in \\{t, s, p\\}$ represents the different feature channels. Following this, we use only the text features to generate the initial distribution $P_{i,j}^t$ and incorporate the three channels of features to derive the enhanced distribution $P_{i,j}^{t,s,p}$ over the label set:\n$P_{i,j}^t = \\text{Softmax}(\\text{MLP}_2 (X^t_{i,j}))\\qquad(15)$\n$P_{i,j}^{t,s,p} = \\text{Softmax}(\\text{MLP}_3 (X^t_{i,j}, X^s_{i,j}, X^p_{i,j}))\\qquad(16)$\nThe final distribution is refined by contrasting the two distributions:\n$p_{i,j}^{final} = \\text{Softmax}(P_{i,j}^{t,s,p} + \\lambda \\text{log} \\frac{P_{i,j}^{t,s,p}}{P_{i,j}^t})\\qquad(17)$\nwhere $\\lambda$ is the refinement scale. We use the cross-entropy loss for the input sequence $t$ and the gold word-pair labels $y^*$:.\n$L_{main} = - \\sum_{i=1}^{n_t} \\sum_{j=1}^{n_t} y^*_{i,j} \\text{log}(p_{i,j}^{final})\\qquad(18)$\n3.7 Training\nIn summary, our framework incorporates two self-supervised learning tasks and one supervised learning task, resulting in three loss functions. Considering the semantic loss $L_{semantic}$, the modality loss $L_{modality}$ from Sec. 3.4, and the prediction loss $L_{main}$ from Sec. 3.6, the final loss function is defined as follows:\n$L = \\alpha L_{semantic} + \\beta L_{modality} + L_{main}\\qquad(19)$\nwhere $\\alpha, \\beta$ are hyperparameters."}, {"title": "4 Experiments", "content": "We conduct our experiments on four MIE datasets and compare our method with a number of previous approaches.\n4.1 Datasets\nExperiments are conducted on 2 MNER datasets, 1 MRE dataset and 1 MJERE dataset: Twitter-15 [48] and Twitter-17 [44] for MNER\u00b9,\nMNRE2 [52] for MRE, and MJERE\u00b3 [45] for MJERE, noting that the last dataset and the task share the same name. These datasets contain 4,000/1,000/3,357, 3,373/723/723, 12,247/1,624/1,614, and 3,617/495/474 samples in the training, validation, and test splits, respectively.\n4.2 Implementation Details\nModel Configuration In order to fairly compare with the previous approaches [13, 17, 40\u201343, 45, 46, 51], we use BERT-based model with the dimension of 768 as the textual encoder. For the visual encoder, we experiment with the ViTB/32 in CLIP and ResNet152 models as potential alternatives. We find ResNet152 with the dimension of 2048 to provide the most effective and consistent results in our settings. To generate the textual descriptive contexts of images, we use LLaVA-1.5 [26], a newly proposed visual instruction-tuned, large multimodal model. The sampling temperature is set to 1.0. We apply the spaCy library of the en_core_web_sm version to parse the given sentence and obtain each word\u2019s part of speech. The dimensions of positional embeddings and part-of-speech embeddings are 100.\nTraining Configuration Shap-CA is trained by Pytorch on a single NVIDIA RTX 3090 GPU. During training, the model is finetuned by AdamW [29] optimizer with a warmup cosine scheduler of ratio 0.05 and a batch size of 28. We use the grid search to find the learning rate over [1 \u00d7 10\u22125,5 \u00d7 10\u22124]. The learning rate of encoders is set to 5 \u00d7 10\u22125. The learning rates of other modules are set to 3 \u00d7 10\u22124, 1 \u00d710\u22124, 1\u00d710-4 and 1\u00d710-4 for Twitter-15, Twitter-17, MJERE and MNRE, respectively. The refinement scale \u039b in Eqn. 17 is set to 1.0. An early stopping strategy is applied to determine the number of training epochs with a patience of 7. We choose the model performing the best on the validation set and evaluate it on the test set. We report the average results from 3 runs with different random seeds."}, {"title": "5 Discussion and Analysis", "content": "5.1 Ablation Study\nWe conduct a comprehensive ablation study to further analyze the effectiveness of our method. The results of these model variants are presented in Table 1 and Table 2."}]}