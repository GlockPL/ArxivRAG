{"title": "Have We Reached AGI? Comparing ChatGPT, Claude, and Gemini to Human Literacy and Education Benchmarks", "authors": ["Mfon Akpan"], "abstract": "Recent advancements in AI, particularly in large language models (LLMs) like ChatGPT,\nClaude, and Gemini, have prompted questions about their proximity to Artificial General\nIntelligence (AGI). This study compares LLM performance on educational benchmarks with\nAmericans' average educational attainment and literacy levels, using data from the U.S.\nCensus Bureau and technical reports. Results show that LLMs significantly outperform\nhuman benchmarks in tasks such as undergraduate knowledge and advanced reading\ncomprehension, indicating substantial progress toward AGI. However, true AGI requires\nbroader cognitive assessments. The study highlights the implications for AI development,\neducation, and societal impact, emphasizing the need for ongoing research and ethical\nconsiderations.", "sections": [{"title": "Introduction", "content": "The relatively fresh advancements of the past few years that witnessed the release of large\nlanguage models like ChatGPT, Claude, and Gemini turned the conversation back to the ongoing\nstate of AI and today's nearness to AGI. Thus, despite some criticism, it can be noted that the\ndefinition formulated as the ability of an AI system to perform any intellectual task that a human\ncan is still a significant achievement of AI research. Despite substantial advancements, the\nquestion persists: Are these LLMs AGI, or are they just limited to certain skills and operations?\nEducational attainment and literacy rates in the United States provide a robust framework for\nassessing the cognitive capabilities of these Al systems. According to the most recent data\nreleased by the U.S. Census Bureau in 2022, the educational landscape in the U.S. is diverse and"}, {"title": "Literature Review", "content": "The quest for artificial general intelligence (AGI) has been a focal point of AI research for\ndecades. Unlike narrow AI, which is designed to perform specific tasks, AGI aims to replicate\nthe broad cognitive abilities of humans. This literature review examines the progress and"}, {"title": "Historical Context and Evolution of AGI", "content": "The concept of AGI has its roots in the early days of computing. Turing's (1950) seminal paper\nposed whether machines could think, introducing the idea of machine intelligence. Subsequent\ndecades saw the development of various AI systems. Still, these were primarily specialized or\nnarrow AI, excelling in specific domains such as chess playing (e.g., IBM's Deep Blue) or\npattern recognition.\nThe emergence of LLMs marks a significant advancement in AI research. Based on transformer\narchitectures (Vaswani et al., 2017), these models have demonstrated remarkable natural\nlanguage understanding and generation capabilities. OpenAI's GPT-3, with its 175 billion\nparameters, has showcased proficiency in tasks ranging from language translation to creative\nwriting, suggesting a step closer to AGI."}, {"title": "Definition of AGI", "content": "Artificial General Intelligence (AGI) represents a pivotal goal in artificial intelligence (AI),\ndistinguished from narrow AI by its broader scope and capabilities. AGI refers to a type of AI\nthat can understand, learn, and apply knowledge across a wide range of tasks and domains, like\nthe cognitive abilities of humans. This contrasts with narrow AI, designed to perform specific\ntasks such as language translation or facial recognition without a broader understanding or ability\nto generalize across different contexts.\nThe concept of AGI has been a topic of discussion and speculation since the early days of\ncomputing. In his seminal paper \"Computing Machinery and Intelligence\" (1950), Alan Turing"}, {"title": "LLMs and Their Cognitive Capabilities", "content": "LLMs like ChatGPT, Claude, and Gemini represent the cutting edge of AI research. These\nmodels have been trained on vast text corpora, enabling them to generate human-like responses\nand perform complex language tasks. Brown et al. (2020) highlight GPT-3's ability to generate\ncoherent and contextually relevant text, perform arithmetic, and even demonstrate rudimentary\nreasoning skills. Such capabilities suggest that LLMs are not merely mimicking language but are\ndeveloping a form of understanding."}, {"title": "Educational Attainment and Literacy as Benchmarks for AGI", "content": "Educational attainment and literacy rates serve as tangible benchmarks for evaluating AGI. The\nU.S. Census Bureau (2022) provides detailed statistics on the educational levels of the U.S.\npopulation, revealing a diverse educational attainment spectrum. These metrics offer a concrete\nframework for assessing whether LLMs can match or exceed human cognitive abilities.\nPrevious research by Brynjolfsson and McAfee (2014) explores the impact of Al on education\nand job markets, emphasizing the need for AI systems that can adapt and learn like humans.\nSimilarly, Muro et al. (2019) discuss the transformative potential of AI in education, advocating\nfor systems that support lifelong learning and cognitive development."}, {"title": "Evaluating LLMs Against Human Benchmarks", "content": "Several studies have attempted to benchmark AI performance against human cognitive abilities.\nHern (2020) notes that while LLMs can generate text at various reading levels, their ability to\ncomprehend and reason like humans remains limited. This limitation is evident in tasks that\nrequire deep understanding and contextual awareness, such as complex problem-solving and\ncritical thinking.\nThe Program for the International Assessment of Adult Competencies (PIAAC) provides a\nframework for evaluating adult literacy and cognitive skills, offering a relevant comparison for\nLLMs. According to the National Center for Education Statistics (2019), approximately 50% of\nU.S. adults read at or below an 8th-grade level, while only about 12% achieve a college-level\nreading proficiency. These benchmarks are crucial for assessing whether LLMs can perform at or\nabove these levels."}, {"title": "The Future of AGI and LLMS", "content": "The path to AGI involves overcoming significant technical and ethical challenges. Russell and\nNorvig (2021) emphasize the importance of creating AI systems that are intelligent and aligned\nwith human values and ethics. The potential of LLMs to contribute to AGI is promising, but\ncontinuous advancements in model architecture, training methods, and evaluation frameworks\nare required.\nRecent work by Bommasani et al. (2021) on foundation models suggests that integrating\nmultimodal data (e.g., text, images, audio) can enhance the generalization capabilities of LLMs,\nbringing them closer to AGI. This multidisciplinary approach highlights the need for\ncollaborative efforts across AI research, cognitive science, and education."}, {"title": "Methodology", "content": "This research uses quantitative research methodology and secondary research analysis to test the\nhypothesis, stating that current large language models, including ChatGPT, Claude, and Gemini,\npossess artificial general intelligence by comparing the scores attained on educational indicators\nwith public education standards. Thus, the research intends to show that the models'\nperformance is at par or above average American standards, and therefore, AGI, if defined to\nmean a level above the average person, may already exist."}, {"title": "Research Design", "content": "The study employs an ex-post, between-group research design, whereby secondary data from\nauthoritative sources will be collected to compare human literacy levels and educational\nachievements with the AI model's performance on similar tasks. This approach also makes it\neasier to evaluate the development of AI today compared to the human cognitive metrics."}, {"title": "Data Sources and Collection", "content": "Data on human educational attainment and literacy rates were obtained from two primary\nsources:\n1. U.S. Census Bureau (2022): Educational Attainment in the United States: 2022\n2. National Center for Education Statistics (NCES) (2019): Adult Literacy in the United\nStates\nSuch datasets offer extensive data on educational levels and literacy by major demographic\ncategories of the population in the United States, which presents a solid reference point for\ncomparing Al results."}, {"title": "AI Performance Data", "content": "Performance metrics for LLMs were collected from published technical reports and comparative\nanalyses, including:\n1. OpenAI (2023): GPT-4 Technical Report\n2. Anthropic (2024): The Claude 3 Model Family: Opus, Sonnet, Haiku\n3. Google Research (2023): Gemini 1.0 Model Performance\nThese sources give standardized performance measures of each LLM for skills similar to man's\neducational and literacy predictors."}, {"title": "Data Analysis", "content": "The analysis was done with the help of IBM Statistical Package for the Social Science (SPSS),\nversion 25. The following analytical procedures were employed:\n1. Data Preparation:\n\u2022 Secondary data were aggregated into a single dataset, and some variables were\nrecoded so that humans and AI could separate the performance variables.\n\u2022 Missing data were coded as system-missing values in SPSS.\n\u2022 Predictors and outcomes were then named based on the measures of the\nvariables (for example, education level, literacy level, AI task performance).\n2. Descriptive Statistics:\n\u2022 Frequencies, means, and standard deviations were calculated for human\n\u2022 educational attainment and literacy levels across demographic groups.\nDescriptive statistics were generated for AI model performance across different\ntasks.\n3. Comparative Analysis:"}, {"title": "Ethical Considerations", "content": "While this study relies on secondary data and does not involve direct human participants, ethical\nconsiderations were still paramount. Care was taken to ensure that the interpretation and\npresentation of results do not perpetuate biases or make unfounded generalizations about human\nor AI capabilities. The small number of studies comparing the performance of Al on specific\ntasks to general human education and literacy rates was identified and discouragingly little, but\nthe authors always mentioned its weaknesses and did not overextend their findings."}, {"title": "Limitations", "content": "Several limitations of this methodology are acknowledged:"}, {"title": "Data Analysis", "content": "This research presents the findings from the secondary data analysis focusing on the relationship\nbetween human educational attainment and literacy level and AI model performance on similar\ntasks. The objective is to assess the ideas that extant LLMs \u2013 ChatGPT, Claude, and Gemini\npossess AGI by working in the same line as an average American. The analysis in this study was\nperformed using IBM SPSS Statistics of the 27th version. 0, relying on performance measures\nwith the help of different statistical tests for comparing human and artificial intelligence\ndiagnostics results."}, {"title": "Data Analysis", "content": "To establish a baseline for human cognitive capabilities, we first examine the educational\nattainment and literacy levels of the U.S. adult population."}, {"title": "AI Model Performance", "content": "To assess the capabilities of current AI systems, we examine the performance of three leading\nLLMs across various cognitive tasks."}, {"title": "Comparative Analysis", "content": "Independent samples t-tests compared AI performance on the Undergraduate Knowledge\n(MMLU) task with the percentage of U.S. adults holding a bachelor's degree or higher.\nResults showed that all three AI models significantly outperformed the human benchmark:"}, {"title": "Proposed AGI Scale", "content": "Based on the analysis of human benchmarks and AI performance, a preliminary scale for\nassessing progress towards Artificial General Intelligence (AGI) is proposed."}, {"title": "Discussion", "content": "This study's results support the hypothesis that current large language models (LLMs) are\nperforming at or above the level of the average American in several vital cognitive domains,\nsuggesting significant progress towards artificial general intelligence (AGI)."}, {"title": "AI Performance of Human Benchmarks", "content": "The analysis reveals that all three AI models (Claude 3 Opus, GPT-4, and Gemini 1.0 Ultra)\nsignificantly outperformed human educational attainment and literacy measures. This is\nespecially the case in the Undergraduate Knowledge (MMLU) task, where AI systems achieved\nresults rates significantly beyond the percentage of U. S. adults with a bachelor's degree or\nhigher. The significant effect sizes themselves (d > 384) serve to amplify the severity of such a\ndifference, meaning these AI models have access to information databases that are vast and\ncomprehensive, being able to perform knowledge tasks at levels that are on par or even superior\nto college-educated subjects.\nLikewise, all Al models achieved a considerably higher reading comprehension than the top\nhuman literacy level. This means these models have adapted to mature language interpretative\nskills much higher than proficient readers. The substantial effect size (n\u00b2 = .937) suggests that AI\nmodels are not just slightly but significantly more effective in performing tasks that involve\ncomplex language understanding."}, {"title": "Comparative Performance of AI Models", "content": "Nonetheless, the performance of all these AI models was impressive, and some variations existed\nwith the human understanding level. Claude 3 Opus showed a significant advantage over GPT-4"}, {"title": "Implications for AGI", "content": "The superior performance of Al models across various cognitive tasks supports the notion that\ncurrent LLMs are approaching or have potentially achieved a form of artificial general\nintelligence. These models demonstrate factual knowledge comparable to highly educated\nhumans and advanced reasoning and comprehension skills that surpass average human\nperformance.\nHowever, it is crucial to interpret these findings with caution. While the AI models excel in these\nbenchmarks, AGI encompasses a broader range of cognitive abilities, including creativity,\ncommon-sense reasoning, and adaptability to novel situations, which still need to be fully\ncaptured in this study. Furthermore, the nature of these benchmarks, being primarily language-\nbased, may only partially represent the multifaceted nature of human intelligence."}, {"title": "Limitations and Future Directions", "content": "Several limitations of this study should be acknowledged. First, comparing AI performance on\nspecific tasks and broader human educational and literacy measures may not capture the full\ncomplexity of human intelligence. Future research should aim to develop more comprehensive\nand diverse benchmarks that assess a wider range of cognitive abilities."}, {"title": "Conclusion", "content": "This study tested the hypothesis that sizable language models (SLMs) like OpenAI's ChatGPT,\nClaude, and Gemini have AGI by benchmarking their educational performance against public\neducation data. The research was to show that these models are at par with the average\nAmerican; hence, if AGI captures a model that performs at the capacity of an average person,\nthen AGI might already be here. This section summarizes the key findings, discusses their\nimplications, addresses the study's limitations, and proposes directions for future research."}, {"title": "Summary of Key Findings", "content": "The analysis of secondary data comparing human educational attainment and literacy levels with\nAl model performance on analogous tasks yielded several significant findings:\n1. Al models consistently outperformed human benchmarks in tasks related to\nundergraduate knowledge and advanced reading comprehension. All three AI models\n(Claude 3 Opus, GPT-4, and Gemini 1.0 Ultra) demonstrated performance levels far\nexceeding the percentage of U.S. adults with bachelor's degrees or higher on the\nUndergraduate Knowledge (MMLU) task.\n2. In reading comprehension tasks, AI models significantly outperformed even the highest\nhuman literacy level (Proficient), with large effect sizes indicating substantial practical\nsignificance.\n3. While all AI models showed exceptional performance compared to human benchmarks,\nsome differences were observed. Claude 3 Opus demonstrated a significant advantage"}, {"title": "Implications of the Findings", "content": "The results of this study have far-reaching implications for our understanding of artificial\nintelligence and its potential impact on society:\n1. Redefinition of AGI: The results question conventional assumptions regarding AGI and\nreveal that AI can perform more than averagely comprehensible cognitive tasks. This\ncalls for reconsidering the concept and the metrics for artificial general intelligence.\n2. Educational and Workforce Implications: AI has performed better in knowledge-frontier\nand understanding-based real-life tasks, leading to fundamental questions for education\nand the future workforce. With the advancement in AI systems, it is necessary to conceal\nhuman tasks and knowledge that are cooperative rather than in conflict with Al systems.\n3. Ethical and Societal Considerations: This study's finding of the increasing rate of AI\nadvancement exposes the importance of ethical concerns and policy reviews on emerging\ntechnologies. Issues of the rights and responsibilities of AI, as well as a possible shift of\npeople's roles in different fields, must be discussed beforehand.\n4. Research and Development Focus: The results imply that the subsequent AI research\nneeds to address not only the efficiency gain on the existing standard tests but also the"}, {"title": "Limitations of the Study", "content": "While this research provides valuable insights into the current state of AI capabilities, several\nlimitations should be acknowledged:\n1. Task Specificity: In this study, the understanding was made of cognitive exercises\nassociated with the knowledge and comprehension of medical functioning. Although\nthese are essential attributes of intelligence, they do not cover all the possible mental\nabilities of a human.\n2. Benchmark Relevance: Therefore, the reliance on educational achievement and literacy\nas performance indicators can be helpful. However, they are only a part of the higher\nhuman characteristics inextricably linked with processes involved in intelligence and\nproblem-solving in real life.\n3. Rapidly Evolving Field: Due to the dynamic evolution of the AI field, the performance\ndata of these models are outdated when the study is conducted, which might\ncompromise the long-term comparability of the results made in this research.\n4. Lack of Direct Testing: Finally, the study conducted only secondary data analysis\ninstead of directly comparing Al models with actual human participants, and thus may\nhave certain discrepancies in results."}, {"title": "Recommendations for Future Research", "content": "Based on the findings and limitations of this study, several avenues for future research are\nproposed:\n1. Comprehensive Intelligence Assessment: Introduce the real and more diverse abilities\nindicators that could define several Cognitive Skills such as Emotional Intelligence,\nCreativity, Practical Judgment and Reasoning and Effectiveness in a range of New and\nUnfamiliar Conditions.\n2. Longitudinal Studies: It should be possible to record AI progress and learning over a long\ntime so that the speed at which the technology is developing can be seen and whether\nthere are certain barriers to improving the systems' capabilities.\n3. Real-World Application Testing: Conduct practical research in specifying the areas in\nwhich it is beneficial to use AI and when human intelligence might perform better in\ncomparison to AI, thus going beyond the approach of comparing AI and humans while\nsolving the existing, well-stipulated tasks that are created specifically for such\ncomparison.\n4. Interdisciplinary Approach: Work closely with cognitive scientists, neuroscientists, and\nphilosophers to refine the definitions and metrics of intelligence for use with or for\nhuman and artificial entities.\n5. Ethical and Societal Impact Studies: Examine possible social consequences of competent\nAl systems such as employment, learning and social organisation to determine guidelines\nfor policy and usage."}, {"title": "Conclusion", "content": "The results presented in the study offer strong evidence that the current large LLMs are already\noperating at or above the level of the average American in several vital cognitive domains,\nindicating the significant further steps toward AGI. Nevertheless, such discoveries paint a very\noptimistic picture of AI and show fundamental improvements in the perceived intelligence level\nof the algorithms proposed; however, this is also a cause for concern because the observed data\nemphasize the necessity to reconsider the notion of intelligence and the ways human and\nartificial intelligence can exist in parallel and manifest themselves. Since there is a position on\nthe brink of a new age in AI, they must persistently analyze and compare these systems while\nbroadening the notion of intelligence to include all the processes indicative of human-level AGI.\nThere are also significant and broad consequences, which means that the constant work of\nresearchers, policymakers, and society, in general, is needed to ensure the successful\ncontainment of threats and the use of emerging opportunities provided by the advancements in\nAl systems."}]}