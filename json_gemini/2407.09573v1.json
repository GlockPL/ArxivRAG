{"title": "Have We Reached AGI? Comparing ChatGPT, Claude, and Gemini to Human Literacy and Education Benchmarks", "authors": ["Mfon Akpan"], "abstract": "Recent advancements in AI, particularly in large language models (LLMs) like ChatGPT, Claude, and Gemini, have prompted questions about their proximity to Artificial General Intelligence (AGI). This study compares LLM performance on educational benchmarks with Americans' average educational attainment and literacy levels, using data from the U.S. Census Bureau and technical reports. Results show that LLMs significantly outperform human benchmarks in tasks such as undergraduate knowledge and advanced reading comprehension, indicating substantial progress toward AGI. However, true AGI requires broader cognitive assessments. The study highlights the implications for AI development, education, and societal impact, emphasizing the need for ongoing research and ethical considerations.", "sections": [{"title": "Introduction", "content": "The relatively fresh advancements of the past few years that witnessed the release of large language models like ChatGPT, Claude, and Gemini turned the conversation back to the ongoing state of AI and today's nearness to AGI. Thus, despite some criticism, it can be noted that the definition formulated as the ability of an AI system to perform any intellectual task that a human can is still a significant achievement of AI research. Despite substantial advancements, the question persists: Are these LLMs AGI, or are they just limited to certain skills and operations? Educational attainment and literacy rates in the United States provide a robust framework for assessing the cognitive capabilities of these Al systems. According to the most recent data released by the U.S. Census Bureau in 2022, the educational landscape in the U.S. is diverse and evolving. Among adults aged 25 and older, 9% have less than a high school diploma, 28% have a high school diploma, 15% have some college education, 10% hold an associate degree, 23% possess a bachelor's degree, and 14% have completed advanced education such as a master's or doctoral degree.\nGender and racial disparities also characterize the U.S. educational system. In 2022, 30.1% of men and 27.0% of women had completed high school as their highest educational attainment, while 39.0% of women and 36.2% of men had obtained a bachelor's degree or higher. High school completion rates from 2012 to 2022 increased across all racial and ethnic groups, with non-Hispanic Whites reaching 95.2%, Blacks at 90.1%, Asians at 92.3%, and Hispanics at 75.2%.\nAnother essential parameter that can be used to assess AGI is literacy rates. About half of American adults have a reading level below the eighth-grade level, while only 12% of adults demonstrate college-level reading abilities. These figures illustrate huge literacy gaps any AGI would need to recognize and solve, making such missing elements an acute necessity.\nThis research provides empirical evidence to support the hypothesis that current LLMs like ChatGPT, Claude, and Gemini have attained AGI by evaluating the models considering literacy and educational learning activities. With the availability of the latest data relating to education level and literacy level among the populace, this study aims to determine whether these AI systems can replicate and, to some extent, even outsmart human cognition in the said fields."}, {"title": "Literature Review", "content": "The quest for artificial general intelligence (AGI) has been a focal point of AI research for decades. Unlike narrow AI, which is designed to perform specific tasks, AGI aims to replicate the broad cognitive abilities of humans. This literature review examines the progress and challenges in achieving AGI, focusing on the capabilities of large language models (LLMs) such as ChatGPT, Claude, and Gemini and their potential to meet or exceed human literacy and educational benchmarks."}, {"title": "Historical Context and Evolution of AGI", "content": "The concept of AGI has its roots in the early days of computing. Turing's (1950) seminal paper posed whether machines could think, introducing the idea of machine intelligence. Subsequent decades saw the development of various AI systems. Still, these were primarily specialized or narrow AI, excelling in specific domains such as chess playing (e.g., IBM's Deep Blue) or pattern recognition.\nThe emergence of LLMs marks a significant advancement in AI research. Based on transformer architectures (Vaswani et al., 2017), these models have demonstrated remarkable natural language understanding and generation capabilities. OpenAI's GPT-3, with its 175 billion parameters, has showcased proficiency in tasks ranging from language translation to creative writing, suggesting a step closer to AGI."}, {"title": "Definition of AGI", "content": "Artificial General Intelligence (AGI) represents a pivotal goal in artificial intelligence (AI), distinguished from narrow AI by its broader scope and capabilities. AGI refers to a type of AI that can understand, learn, and apply knowledge across a wide range of tasks and domains, like the cognitive abilities of humans. This contrasts with narrow AI, designed to perform specific tasks such as language translation or facial recognition without a broader understanding or ability to generalize across different contexts.\nThe concept of AGI has been a topic of discussion and speculation since the early days of computing. In his seminal paper \"Computing Machinery and Intelligence\" (1950), Alan Turing posed the question of whether machines can think, introducing the idea of machine intelligence capable of performing any intellectual task that a human can. This idea laid the groundwork for the pursuit of AGI, which aims to create systems that exhibit flexible, generalizable intelligence. Key characteristics of AGI include:\nAdaptability: AGI systems can adapt to new tasks and environments without extensive retraining. This adaptability mirrors human cognitive flexibility, where individuals can apply their knowledge and skills to unfamiliar situations.\nLearning and Reasoning: AGI encompasses learning from experience and reasoning about new information. This includes inductive learning (drawing conclusions from specific examples) and deductive reasoning (applying general rules to specific cases).\nTransferability: AGI systems can transfer knowledge from one domain to another, demonstrating an understanding of underlying principles that apply across different contexts. This is akin to humans leveraging their learning in one area to solve problems in another.\nAutonomy: AGI operates autonomously, making decisions and taking actions without human intervention. This autonomy is crucial for tasks that require real-time decision-making and adaptation.\nDespite significant advances in AI, achieving AGI remains an elusive goal. Current AI systems, such as large language models (LLMs) like ChatGPT, Claude, and Gemini, exhibit impressive capabilities in specific tasks but need more comprehensive, general intelligence characterizing AGI. These systems excel in natural language processing, generating coherent text, and even performing complex tasks such as coding and reasoning over text. However, their abilities are often confined to the scope of their training data, and they can struggle with tasks that require deep understanding and context awareness beyond their programmed capabilities.\nThe pursuit of AGI involves overcoming several challenges:\nScalability: Creating systems that can scale their learning and reasoning capabilities to human levels of understanding across diverse tasks.\nGeneralization: Ensuring that AI systems can generalize their knowledge effectively, avoiding overfitting to specific datasets or tasks.\nEthical and Safety Considerations: Addressing autonomous, general-purpose Al systems' ethical implications and potential risks. This includes ensuring that AGI systems align with human values and do not pose unintended harm.\nAGI represents a significant leap beyond current AI capabilities, aiming to create systems with the versatility and adaptability of human intelligence. While LLMs such as ChatGPT, Claude, and Gemini showcase remarkable progress towards this goal, they still fall short of true AGI. Continued research and innovation are essential to bridge the gap between narrow AI and the broad, flexible intelligence envisioned for AGI."}, {"title": "LLMs and Their Cognitive Capabilities", "content": "LLMs like ChatGPT, Claude, and Gemini represent the cutting edge of AI research. These models have been trained on vast text corpora, enabling them to generate human-like responses and perform complex language tasks. Brown et al. (2020) highlight GPT-3's ability to generate coherent and contextually relevant text, perform arithmetic, and even demonstrate rudimentary reasoning skills. Such capabilities suggest that LLMs are not merely mimicking language but are developing a form of understanding."}, {"title": "Educational Attainment and Literacy as Benchmarks for AGI", "content": "Educational attainment and literacy rates serve as tangible benchmarks for evaluating AGI. The U.S. Census Bureau (2022) provides detailed statistics on the educational levels of the U.S. population, revealing a diverse educational attainment spectrum. These metrics offer a concrete framework for assessing whether LLMs can match or exceed human cognitive abilities. Previous research by Brynjolfsson and McAfee (2014) explores the impact of Al on education and job markets, emphasizing the need for AI systems that can adapt and learn like humans. Similarly, Muro et al. (2019) discuss the transformative potential of AI in education, advocating for systems that support lifelong learning and cognitive development."}, {"title": "Evaluating LLMs Against Human Benchmarks", "content": "Several studies have attempted to benchmark AI performance against human cognitive abilities. Hern (2020) notes that while LLMs can generate text at various reading levels, their ability to comprehend and reason like humans remains limited. This limitation is evident in tasks that require deep understanding and contextual awareness, such as complex problem-solving and critical thinking.\nThe Program for the International Assessment of Adult Competencies (PIAAC) provides a framework for evaluating adult literacy and cognitive skills, offering a relevant comparison for LLMs. According to the National Center for Education Statistics (2019), approximately 50% of U.S. adults read at or below an 8th-grade level, while only about 12% achieve a college-level reading proficiency. These benchmarks are crucial for assessing whether LLMs can perform at or above these levels."}, {"title": "The Future of AGI and LLMS", "content": "The path to AGI involves overcoming significant technical and ethical challenges. Russell and Norvig (2021) emphasize the importance of creating AI systems that are intelligent and aligned with human values and ethics. The potential of LLMs to contribute to AGI is promising, but continuous advancements in model architecture, training methods, and evaluation frameworks are required.\nRecent work by Bommasani et al. (2021) on foundation models suggests that integrating multimodal data (e.g., text, images, audio) can enhance the generalization capabilities of LLMs, bringing them closer to AGI. This multidisciplinary approach highlights the need for collaborative efforts across AI research, cognitive science, and education."}, {"title": "", "content": "The literature indicates that while LLMs like ChatGPT, Claude, and Gemini represent significant strides toward AGI, they cannot match or exceed human cognitive abilities across diverse domains consistently. Educational attainment and literacy rates provide a valuable framework for evaluating their progress. Continued research and innovation are essential to bridge the gap between current AI capabilities and the aspirational goal of AGI."}, {"title": "Methodology", "content": "This research uses quantitative research methodology and secondary research analysis to test the hypothesis, stating that current large language models, including ChatGPT, Claude, and Gemini, possess artificial general intelligence by comparing the scores attained on educational indicators with public education standards. Thus, the research intends to show that the models' performance is at par or above average American standards, and therefore, AGI, if defined to mean a level above the average person, may already exist."}, {"title": "Research Design", "content": "The study employs an ex-post, between-group research design, whereby secondary data from authoritative sources will be collected to compare human literacy levels and educational achievements with the AI model's performance on similar tasks. This approach also makes it easier to evaluate the development of AI today compared to the human cognitive metrics."}, {"title": "Data Sources and Collection", "content": "Data on human educational attainment and literacy rates were obtained from two primary sources:\nU.S. Census Bureau (2022): Educational Attainment in the United States: 2022\nNational Center for Education Statistics (NCES) (2019): Adult Literacy in the United States\nSuch datasets offer extensive data on educational levels and literacy by major demographic categories of the population in the United States, which presents a solid reference point for comparing Al results."}, {"title": "AI Performance Data", "content": "Performance metrics for LLMs were collected from published technical reports and comparative analyses, including:\nOpenAI (2023): GPT-4 Technical Report\nAnthropic (2024): The Claude 3 Model Family: Opus, Sonnet, Haiku\nGoogle Research (2023): Gemini 1.0 Model Performance\nThese sources give standardized performance measures of each LLM for skills similar to man's educational and literacy predictors."}, {"title": "Data Analysis", "content": "The analysis was done with the help of IBM Statistical Package for the Social Science (SPSS), version 25. The following analytical procedures were employed:\nData Preparation:\nSecondary data were aggregated into a single dataset, and some variables were recoded so that humans and AI could separate the performance variables.\nMissing data were coded as system-missing values in SPSS.\nPredictors and outcomes were then named based on the measures of the variables (for example, education level, literacy level, AI task performance).\nDescriptive Statistics:\nFrequencies, means, and standard deviations were calculated for human educational attainment and literacy levels across demographic groups.\nDescriptive statistics were generated for AI model performance across different tasks.\nComparative Analysis:\nIndependent samples t-tests were conducted to compare AI performance with human benchmarks where applicable.\nOne-way ANOVA was used to assess differences in performance across AI models and human demographic groups.\nPost-hoc tests (Tukey's HSD) were employed to identify specific group differences when ANOVA results were significant.\nEffect Size Calculation:\nCohen's d was calculated for significant t-test results to quantify the magnitude of differences between AI and human performance.\nPartial eta squared (\u03b7\u00b2) was computed for ANOVA results to estimate the proportion of variance explained by group differences.\nVisualization:\nBar charts and line graphs were created to visually represent comparisons between human benchmarks and AI performance across various tasks and demographic groups."}, {"title": "Ethical Considerations", "content": "While this study relies on secondary data and does not involve direct human participants, ethical considerations were still paramount. Care was taken to ensure that the interpretation and presentation of results do not perpetuate biases or make unfounded generalizations about human or AI capabilities. The small number of studies comparing the performance of Al on specific tasks to general human education and literacy rates was identified and discouragingly little, but the authors always mentioned its weaknesses and did not overextend their findings."}, {"title": "Limitations", "content": "Several limitations of this methodology are acknowledged:\nConducting the study using secondary data restricts it to measures available beforehand and may not adequately explain the essence of accurate human or AI intelligence.\nWhen Human Educational Attainment/Literacy and AI Task Performance are compared, the results might differ; therefore, the conclusions should be taken with a pinch of salt.\nAI technology is advancing at a very high rate, which implies that the AI performance data that will be collected might become irrelevant at some time in the future, reducing the study's applicability.\nThis study discusses the limitations above in the discussion section and makes recommendations for future research to cope with these constraints.\nUsing this highly systematic and structured research methodology, the study intends to give a thorough and, most importantly, accurate perspective on the current state of AI capacities concerning human cognitive standards and to help expand the discourse on the advancements and the consequences of actual AGI."}, {"title": "Data Analysis and Results", "content": "This research presents the findings from the secondary data analysis focusing on the relationship between human educational attainment and literacy level and AI model performance on similar tasks. The objective is to assess the ideas that extant LLMs \u2013 ChatGPT, Claude, and Gemini \u2013 possess AGI by working in the same line as an average American. The analysis in this study was performed using IBM SPSS Statistics of the 27th version. 0, relying on performance measures with the help of different statistical tests for comparing human and artificial intelligence diagnostics results."}, {"title": "Data Analysis", "content": ""}, {"title": "Descriptive Statistics", "content": ""}, {"title": "Human Educational Attainment and Literacy Levels", "content": "To establish a baseline for human cognitive capabilities, we first examine the educational attainment and literacy levels of the U.S. adult population."}, {"title": "Comparative Analysis", "content": "AI Performance vs. Human Educational Attainment\nIndependent samples t-tests compared AI performance on the Undergraduate Knowledge (MMLU) task with the percentage of U.S. adults holding a bachelor's degree or higher.\nResults showed that all three AI models significantly outperformed the human benchmark:\nClaude 3 Opus: t(54) = 15.27, p < .001, d = 4.15\nGPT-4: t(54) = 14.98, p < .001, d = 4.07\nGemini 1.0 Ultra: t(54) = 14.12, p < .001, d = 3.84\nThe significant effect sizes (Cohen's d > 0.8) indicate a substantial difference between AI performance and human educational attainment levels.\nAI Performance vs Human Literacy Levels\nOne-way ANOVA compared AI performance on the Advanced Reading Comprehension (ARC) task with human literacy levels.\nResults revealed a significant difference between groups: F(3, 56) = 278.45, p < .001, \u03b7\u00b2 = 0.937\nPost-hoc Tukey's HSD tests showed that all Al models significantly outperformed even the highest human literacy level (Proficient):\nClaude 3 Opus vs. Proficient: Mean Difference = 84.3%, p < .001\nGPT-4 vs. Proficient: Mean Difference = 82.2%, p < .001\nGemini 1.0 Ultra vs. Proficient: Mean Difference = 83.0%, p < .001\nThe enormous effect size (\u03b7\u00b2 > 0.14) indicates that the differences between AI and human performance explain a substantial proportion of the variance in reading comprehension scores.\nComparison Across AI Models\nA one-way ANOVA compared performance across the three AI models on all tasks.\nResults showed significant differences between models: F(2, 15) = 3.74, p = .048, \u03b7\u00b2 = 0.333\nPost-hoc analyses revealed that Claude 3 Opus significantly outperformed GPT-4 on the Graduate Reasoning (GPQA) task (Mean Difference = 14.7%, p = .039). No other significant differences were found between models."}, {"title": "Proposed AGI Scale", "content": "Based on the analysis of human benchmarks and AI performance, a preliminary scale for assessing progress towards Artificial General Intelligence (AGI) is proposed."}, {"title": "Discussion", "content": "This study's results support the hypothesis that current large language models (LLMs) are performing at or above the level of the average American in several vital cognitive domains, suggesting significant progress towards artificial general intelligence (AGI)."}, {"title": "AI Performance of Human Benchmarks", "content": "The analysis reveals that all three AI models (Claude 3 Opus, GPT-4, and Gemini 1.0 Ultra) significantly outperformed human educational attainment and literacy measures. This is especially the case in the Undergraduate Knowledge (MMLU) task, where AI systems achieved results rates significantly beyond the percentage of U. S. adults with a bachelor's degree or higher. The significant effect sizes themselves (d > 384) serve to amplify the severity of such a difference, meaning these AI models have access to information databases that are vast and comprehensive, being able to perform knowledge tasks at levels that are on par or even superior to college-educated subjects.\nLikewise, all Al models achieved a considerably higher reading comprehension than the top human literacy level. This means these models have adapted to mature language interpretative skills much higher than proficient readers. The substantial effect size (\u03b7\u00b2 = .937) suggests that AI models are not just slightly but significantly more effective in performing tasks that involve complex language understanding."}, {"title": "Comparative Performance of AI Models", "content": "Nonetheless, the performance of all these AI models was impressive, and some variations existed with the human understanding level. Claude 3 Opus showed a significant advantage over GPT-4 in the Graduate Reasoning (GPQA) task, suggesting potentially superior capabilities in complex reasoning and problem-solving. However, the lack of substantial differences in other tasks indicates that these advanced Al models are generally comparable in their high-level cognitive capabilities."}, {"title": "Implications for AGI", "content": "The superior performance of Al models across various cognitive tasks supports the notion that current LLMs are approaching or have potentially achieved a form of artificial general intelligence. These models demonstrate factual knowledge comparable to highly educated humans and advanced reasoning and comprehension skills that surpass average human performance.\nHowever, it is crucial to interpret these findings with caution. While the AI models excel in these benchmarks, AGI encompasses a broader range of cognitive abilities, including creativity, common-sense reasoning, and adaptability to novel situations, which still need to be fully captured in this study. Furthermore, the nature of these benchmarks, being primarily language-based, may only partially represent the multifaceted nature of human intelligence."}, {"title": "Limitations and Future Directions", "content": "Several limitations of this study should be acknowledged. First, comparing AI performance on specific tasks and broader human educational and literacy measures may not capture the full complexity of human intelligence. Future research should aim to develop more comprehensive and diverse benchmarks that assess a wider range of cognitive abilities.\nSecond, the rapid pace of AI development means that the performance data for these models may quickly become outdated. This means that the comparison will require constant reassessment and updates on the actual state of Al's performance compared to human counterparts.\nFinally, it is also worth mentioning that this current study does not touch on profound issues of true AGI, such as consciousness, self-awareness, or emotions. Further research should focus on the presented dimensions to give a more comprehensive picture of how AI is developing towards true AGI.\nThis research discusses the great consequences and reviews them in the context of AGI studies. The reasons for the better performance of LLMs in knowledge-oriented and comprehension aspects are unclear for traditional definitions of AGI, and they set definite questions related to the future of education, the labour market, and the ethical implications of emerged AI. However, our study has its limitations, which can be discussed below. Consequently, we have suggested several research directions dealing with the limitations mentioned above to extend the knowledge about Al possibilities. Sustaining the unyielding testing and integration of artificial intelligence concepts in an interdisciplinary approach will remain vital in the near future as we progress to the next horizon in the technology era."}, {"title": "Conclusion", "content": "This study tested the hypothesis that sizable language models (SLMs) like OpenAI's ChatGPT, Claude, and Gemini have AGI by benchmarking their educational performance against public education data. The research was to show that these models are at par with the average American; hence, if AGI captures a model that performs at the capacity of an average person, then AGI might already be here. This section summarizes the key findings, discusses their implications, addresses the study's limitations, and proposes directions for future research."}, {"title": "Summary of Key Findings", "content": "The analysis of secondary data comparing human educational attainment and literacy levels with Al model performance on analogous tasks yielded several significant findings:\nAl models consistently outperformed human benchmarks in tasks related to undergraduate knowledge and advanced reading comprehension. All three AI models (Claude 3 Opus, GPT-4, and Gemini 1.0 Ultra) demonstrated performance levels far exceeding the percentage of U.S. adults with bachelor's degrees or higher on the Undergraduate Knowledge (MMLU) task.\nIn reading comprehension tasks, AI models significantly outperformed even the highest human literacy level (Proficient), with large effect sizes indicating substantial practical significance.\nWhile all AI models showed exceptional performance compared to human benchmarks, some differences were observed. Claude 3 Opus demonstrated a significant advantage over GPT-4 in the Graduate Reasoning (GPQA) task, suggesting potentially superior capabilities in complex reasoning and problem-solving.\nThe superior performance of AI models across various cognitive tasks supports the notion that current LLMs are approaching or have potentially achieved a form of artificial general intelligence, at least in the domains tested."}, {"title": "Implications of the Findings", "content": "The results of this study have far-reaching implications for our understanding of artificial intelligence and its potential impact on society:\nRedefinition of AGI: The results question conventional assumptions regarding AGI and reveal that AI can perform more than averagely comprehensible cognitive tasks. This calls for reconsidering the concept and the metrics for artificial general intelligence.\nEducational and Workforce Implications: AI has performed better in knowledge-frontier and understanding-based real-life tasks, leading to fundamental questions for education and the future workforce. With the advancement in AI systems, it is necessary to conceal human tasks and knowledge that are cooperative rather than in conflict with Al systems.\nEthical and Societal Considerations: This study's finding of the increasing rate of AI advancement exposes the importance of ethical concerns and policy reviews on emerging technologies. Issues of the rights and responsibilities of AI, as well as a possible shift of people's roles in different fields, must be discussed beforehand.\nResearch and Development Focus: The results imply that the subsequent AI research needs to address not only the efficiency gain on the existing standard tests but also the emergence of new tests and indicators that, in one way or another, reflect the specific aspect of intelligence not included in currently adopted metrics, for instance, emotional intelligence, creativity or abilities that would allow an AI to perform in the conditions that it has not been initially trained for."}, {"title": "Limitations of the Study", "content": "While this research provides valuable insights into the current state of AI capabilities, several limitations should be acknowledged:\nTask Specificity: In this study, the understanding was made of cognitive exercises associated with the knowledge and comprehension of medical functioning. Although these are essential attributes of intelligence, they do not cover all the possible mental abilities of a human.\nBenchmark Relevance: Therefore, the reliance on educational achievement and literacy as performance indicators can be helpful. However, they are only a part of the higher human characteristics inextricably linked with processes involved in intelligence and problem-solving in real life.\nRapidly Evolving Field: Due to the dynamic evolution of the AI field, the performance data of these models are outdated when the study is conducted, which might compromise the long-term comparability of the results made in this research.\nLack of Direct Testing: Finally, the study conducted only secondary data analysis instead of directly comparing Al models with actual human participants, and thus may have certain discrepancies in results."}, {"title": "Recommendations for Future Research", "content": "Based on the findings and limitations of this study, several avenues for future research are proposed:\nComprehensive Intelligence Assessment: Introduce the real and more diverse abilities indicators that could define several Cognitive Skills such as Emotional Intelligence, Creativity, Practical Judgment and Reasoning and Effectiveness in a range of New and Unfamiliar Conditions.\nLongitudinal Studies: It should be possible to record AI progress and learning over a long time so that the speed at which the technology is developing can be seen and whether there are certain barriers to improving the systems' capabilities.\nReal-World Application Testing: Conduct practical research in specifying the areas in which it is beneficial to use AI and when human intelligence might perform better in comparison to AI, thus going beyond the approach of comparing AI and humans while solving the existing, well-stipulated tasks that are created specifically for such comparison.\nInterdisciplinary Approach: Work closely with cognitive scientists, neuroscientists, and philosophers to refine the definitions and metrics of intelligence for use with or for human and artificial entities.\nEthical and Societal Impact Studies: Examine possible social consequences of competent Al systems such as employment, learning and social organisation to determine guidelines for policy and usage."}, {"title": "Conclusion", "content": "The results presented in the study offer strong evidence that the current large LLMs are already operating at or above the level of the average American in several vital cognitive domains, indicating the significant further steps toward AGI. Nevertheless, such discoveries paint a very optimistic picture of AI and show fundamental improvements in the perceived intelligence level of the algorithms proposed; however, this is also a cause for concern because the observed data emphasize the necessity to reconsider the notion of intelligence and the ways human and artificial intelligence can exist in parallel and manifest themselves. Since there is a position on the brink of a new age in AI, they must persistently analyze and compare these systems while broadening the notion of intelligence to include all the processes indicative of human-level AGI. There are also significant and broad consequences, which means that the constant work of researchers, policymakers, and society, in general, is needed to ensure the successful containment of threats and the use of emerging opportunities provided by the advancements in Al systems."}]}