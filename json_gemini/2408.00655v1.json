{"title": "SentenceVAE: Faster, Longer and More Accurate Inference with Next-sentence Prediction for Large Language Models", "authors": ["Hongjun An", "Yifan Chen", "Xiaozhen Qiao", "Zhe Sun", "Xuelong Li"], "abstract": "Contemporary large language models (LLMs) predominantly utilize a next-token prediction method for inference, which significantly impedes their processing speed. In this paper, we introduce a novel inference methodology termed next-sentence prediction, aimed at enhancing the inference efficiency of LLMs. We present SentenceVAE, a tiny model consisting of an encoder and a decoder. The encoder effectively condenses the information within a sentence into a singular token, while the decoder reconstructs this compressed data back into its original sentential form. By integrating SentenceVAE into the input and output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference approach, markedly accelerating inference speeds. SentenceVAE also maintains the integrity of the original semantic content by segmenting the text into sentences, thereby preserving accuracy while boosting inference speeds. Compared to traditional LLMs, SLLMs process fewer tokens over equivalent context lengths, significantly reducing memory demands for Self-Attention computations and facilitating the handling of longer contexts. Our experimental findings reveal that this method can increase inference speeds by 204~365%, reduce perplexity (PPL) to 46~75% of its original metric, and decrease memory overhead by 86~91% for the same context length. The advantages of this approach are further amplified with increases in model parameters.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated remarkable proficiency in understanding human intent through the acquisition of knowledge from vast and diverse datasets. These models provide rich and accurate information and responses, making them indispensable in applications such as machine translation, chatbots, and question-answering systems. The predominant approach to inference in LLMs involves next-token prediction, where the model generates tokens sequentially. While this token-by-token generation method has achieved significant performance, it is inherently limited in that it produces only one token per inference step. This limitation results in substantial computational overhead and increases the overall inference time, thereby constraining the scalability and efficiency of LLMs. As a result, there is a pressing need to explore alternative methods that can mitigate these limitations while maintaining or improving upon the accuracy and responsiveness of LLMs.\nTo enhance the inference efficiency of Large Language Models (LLMs) and reduce computational costs, Gloeckle et al. proposed training LLMs to predict multiple tokens simultaneously, aiming to accelerate the inference process. However, the number of tokens predicted at once is fixed. For instance, when this number is set to five, regardless of the input text, it is forcibly segmented into units of five tokens. While this multi-token prediction method improves the inference speed of LLMs, it may disrupt the original semantics of the text by aggregating unrelated tokens together, potentially compromising inference accuracy.\nTo enhance inference speed while maintaining accuracy, we explore methods for adaptively selecting the optimal number of predicted tokens based on the semantic content of the text. Consequently, we introduce SentenceVAE, which comprises a sentence encoder and a sentence decoder. The schematic of SentenceVAE is illustrated in Fig. 1. The sentence encoder is designed to condense the information of an entire sentence into a single token, while the sentence decoder reconstructs this compressed token back into a sentence. By integrating the encoder and decoder of SentenceVAE into the input and output layers of Large Language Models (LLMs), these models can perform next-sentence prediction, thereby accelerating the reasoning process. Moreover, since SentenceVAE segments text at the sentence level, the semantic integrity is better preserved, ensuring the accuracy of the inferrence.\nSpecifically, prior to inputting text into LLMs, SentenceVAE segments the text into sentences and utilizes the sentence encoder to compress the information contained within each sentence into a single token. These compressed tokens are then fed as input to the LLMs. The LLMs predict their output based on these compressed tokens. Finally, the sentence decoder decodes the predicted token into the final output of the LLMs.\nThe contribution of the proposed method can be described as follows:\n\u2022 We propose SentenceVAE, a model capable of compressing the information content of a sentence into a single token, and conversely, expanding such tokens to reconstruct sentences with high informational fidelity.\n\u2022 We embed the encoder and decoder components of SentenceVAE into the input and output layers of LLMs, enabling these models to implement an effective inference method for next-sentence prediction.\n\u2022 Extensive experiments conducted on various LLMs have demonstrated that the proposed method significantly accelerates inference speed while preserving the accuracy of these models."}, {"title": "Related Work", "content": "Large language models: In recent years, large language models (LLMs) have achieved remarkable performance in the field of natural language processing (NLP). However, these models typically possess a vast number of parameters from billions to hundreds of billions, exemplified by models such as TeleChat (1B-52B) from TeleAI, Llama (7B-405B) from Meta, InternLM (1.8B-20B) from Shanghai AI Lab etc. The sheer magnitude of these parameters renders the inference process computationally intensive and time-consuming. Moreover, most LLMs employ a one-token prediction method, where the model predicts only one token at a time. This token-by-token approach further exacerbates the inference time of LLMs. As a result, accelerating the inference speed of LLMs while maintaining their accuracy presents a significant challenge for researchers in the field.\nMulti token prediction: The transformer model enables the parallel processing of sequences through its self-attention mechanism, providing a technical foundation for the implementation of multi-token prediction . The BERT model utilizes bidirectional encoder representations to capture contextual information, further solidifying the basis for the implementation of multi-token prediction . The T5 model incorporates multi-token prediction during the training process for the first time, significantly enhancing the coherence and quality of the generated text. The Better & Faster LLMs apply multi-token prediction to LLMs, allowing them to directly predict multiple tokens in a single inference step, thereby reducing the number of inference iterations and associated computational costs. However, the number of predicted tokens is fixed, which may inadvertently combine unrelated tokens, potentially disrupting the original semantics of the sentence and consequently decreasing inference accuracy.\nEncoder-decoder model: The concept of an encoder is introduced by Yann LeCun in his doctoral dissertation, wherein he posited that an encoder could transform input data into shorter representations without losing the core information, thereby providing a foundational approach to compressing high-dimensional data. The Variational Autoencoder (VAE) model introduces the use of hidden variables between the encoder and decoder to learn the data distribution, laying the groundwork for deep-learning-based encoders and decoders . Sutskever et al. introduces the encoder-decoder framework into the field of deep learning, utilizing recurrent neural networks (RNNs) as both the encoder and decoder for sequence-to-sequence (Seq2Seq) learning, demonstrating exceptional performance in machine translation tasks . The SegNet applies the encoder-decoder model to the field of image segmentation, significantly enhancing segmentation performance . Subsequently, the encoder-decoder model has achieved great success in various domains including machine translation, text-to-speech conversion, computational imaging, and beyond, fully demonstrating the capability of the encoder-decoder architecture to efficiently compress and restore data.\nBy leveraging the advantages of multi-token prediction and the encoder-decoder model, we propose SentenceVAE."}, {"title": "Method", "content": "In this section, we introduce the SentenceVAE framework. Embedding this framework into a LLM enables the model to implement a sentence-by-sentence prediction method, significantly accelerating its inference speed. The inference process of the proposed method compared to traditional LLMs is illustrated in Fig. 2. It is evident that for the same text, an LLM embedded with SentenceVAE requires only three inference steps to obtain the inference result, whereas traditional LLMs necessitate twenty-free inference steps. Furthermore, the proposed method divides the text into sentences without compromising the original semantics, thereby enhancing the inference speed of LLMs while maintaining inference accuracy.\nOur SentenceVAE primarily consists of a Sentence Encoder and a Sentence Decoder. The Sentence Encoder encodes multiple word-level tokens from a sentence into a single sentence-level token, while the Sentence Decoder reconstructs this sentence-level token back into the original sequence of word-level tokens. To ensure that the multiple tokens input to the Sentence Encoder come from individual sentences, we propose a specialized Sentence Segmentation Mechanism. Additionally, we introduce a Feature Fusion Mechanism, which enable the encoder to encode variable-length token sequences into a single sentence-level token.\nSentence Segmentation Mechanism: To ensure that all word-level tokens input to the sentence encoder per time come from a single sentence, we use regular matching to split sentences by punctuation marks such as \",\", \".\", \"?\", \"!\" before tokenizing. Let the original string be S, and after partitioning, the set \\{s_i|1 \\le i \\le n\\} is obtained, satisfying S = s_1s_2s_3...s_n.\nSentence Encoder: Taking s = s_t(1 \\le t \\le n) as an example, after tokenization, we obtained a sequence of L token ids D = [d_1, d_2, d_3, ..., d_L] representing a sentence. This sequence was then passed through an embedding layer, resulting in word-level embedding vectors of E, as shown in Eq.1.\n\nE = (e_i)_{L\\times \\text{hidden\\_size}} = \\text{Embed}(D)\n\nThese embeddings were subsequently input into self-attention based encoder blocks, yielding hidden features H, as shown in Eq. 2.\n\nH = (h_i)_{L\\times \\text{hidden\\_size}} = \\text{EncoderBlocks}(E)\n\nTo derive the sentence embedding vector, we fused these L features into a single vector \\Omega_t \\in \\mathbb{R}^{\\text{hidden\\_size}}. This sentence embedding vector is then fed into the decoder-only LLM, which generates a new sentence embedding vector \\Omega_{t+1} at each time step."}, {"title": "Feature Fusion Mechanism", "content": "After the encoder blocks generates H, we propose a method to fuse them into a single sentence embedding vector \\Omega_t. This method involves accumulating the L vectors and normalizing them using Layer Normalization as shown in Eq. 3.\n\n\\Omega_t = \\text{LayerNorm} (\\sum_{i=1}^{L} h_i)\n\nSentence Decoder: The Sentence Decoder contains a combination of masked self-attention and cross-attention blocks, where the cross-attention mechanism uses the individual sentence embedding vector \\Omega_{t+1} as key (K) and value (V).\nIn the prediction phase, given \\Omega_{t+1}, initialize the input token d_0 =< bos >, the decoder outputs d_i, and then input [d_0, d_1] and \\Omega_{t+1} into the decoder. Repeat this process until d_{v+1} =< eos >, and terminate the iteration, as shown in Eq.4.\ninit: d_0 =< bos >, l' = 0\ndo:\n\nD_v = [d_0, d_1, d_2, ..., d_v]\n\nP_{l'} = (p_i)_{(\\text{l'}+1)\\times \\text{hidden\\_size}} = \\text{DecoderBlocks}(D_v, \\Omega_{t+1})\n\nd_v = \\text{argmax}(p_i)\n\nD_{i+1} = \\text{concat}(D_v, d_v)\n\nl' = l' + 1\nuntil: d_{v'} =< eos >\nret: D_{l'}\n\nAfter the iteration is completed, the detokenization process is used to convert the output D_{L'} back into a string.\nIn the training phase, parallel training is adopted, based on masked self-attention mechanism, to ensure that only tokens before d_u can be seen when inferring d_{v+1}. Given an input sequence of D_{\\text{train}} = [d_0, d_1, ..., d_{L'}], model output logits of P\\in []\\mathbb{R}^{(L'+1)\\times \\text{hidden\\_size}}, and ground truth of D_{\\text{ground\\_truth}} = [d_1, d_2, ..., d_{L'}, <eos >], focal loss is taken as the loss function, as shown in Eq. 5.\n\nP = \\text{DecoderBlocks}(D_{\\text{train}}, \\Omega_{t+1})\n\nP' = \\text{softmax}(P)\n\nLoss = \\sum_{i=1}^{L'} - (1 - P_{d_i})^{y} \\log(p_{d_i}), y = 2"}, {"title": "Sentence-level Large Language Models (SLLMs)", "content": "Currently, almost all decoder-only LLMs consist of the following components: an embedding layer \\text{EMB}_{ilm}, N decoder-only blocks \\text{DB}_{ilm}, and a fully connected layer \\text{FC}_{ilm} for outputting logits. As shown in Eq. 6, given context tokens D_{\\text{context}} = [d_0, d_1,..., d_v], the model generates the next token d_{v+1}.\n\nE_{\\text{context}} = \\text{EMB}_{ilm}(D_{\\text{context}})\n\nH_{ilm} = \\text{DB}_{ilm}(E_{\\text{context}})\n\nP_{ilm} = (P_{ilm,i})_{ (l'+1)\\times \\text{hidden\\_size}} = \\text{FC}_{ilm} (H_{ilm})\n\nd_{v+1} = \\text{argmax}(P_{ilm,l'})\n\nIn this section, we introduce a unified grafting method to graft SentienceVAE onto the beginning and end of any LLM. This approach transforms the LLMs into Sentence-level Large Language Models (SLLMs), enabling it to operate effectively in the sentence-level embedding space.\nDiscard Embedding Layer: In a SLLM, the N decoder-only blocks no longer receives word-level tokens directly. Instead, it receives sentence embedding vectors encoded by the sentence encoder. Consequently, the traditional embedding layer in the LLM architecture is removed. Therefore, the model follows Eq. 7.\n\nE_{\\text{sllm}} = [\\Omega_1, \\Omega_2, ..., \\Omega_t]\n\nH_{\\text{sllm}} = (h_{\\text{sllm},i})_{t\\times \\text{hidden\\_size}} = \\text{DB}_{ilm} (E_{\\text{sllm}})\n\nTermination judgment layer: In traditional LLMs, the final fully connected layer \\text{FC}_{ilm} typically converts an H_{ilm} \\in \\mathbb{R}^{t\\times \\text{hidden size}} to a probability vector (logits) P_{ilm} \\in \\mathbb{R}^{t\\times V}, where V represents the size of the vocabulary. This allows the model to determine the next token output using sampling algorithms, e.g., Greedy Search, Beam Search, Random Sampling, Temperature Sampling, Top-K Sampling, and Nucleus Sampling. If the current token output is a special token like eos (end-of-sequence), the iteration terminates.\nHowever, in SLLMs, the output of \\text{DB}_{ilm} is a sentence-level hidden state vector H_{\\text{sllm}}. Therefore, traditional token generation methods are not applicable. Instead, we use a new fully connected layer \\text{FC}_{sllm} called termination judgment layer that converts the H_{\\text{sllm}} into a 2-dimensional boolean vector B_{\\text{stop\\_flag}}, as shown in Eq. 8. This vector helps determine whether the current sentence-level hidden state vector signifies the end of a sentence (stop flag) or needs further decoding by the sentence decoder. If the vector indicates an end flag, the iteration terminates. Otherwise, the sentence decoder processes the embedding to generate corresponding tokens, as shown in Eq. 9.\n\nB_{\\text{stop-flag}} = (b_i)_{t\\times 2} = \\text{FC}_{sllm} (H_{\\text{sllm}})\n\n\\Omega_{t+1} = \\begin{cases} <eos >, & \\text{argmax}(b_t) = 1 \\\\ h_{\\text{sllm},t}, & \\text{argmax}(b_t) = 0 \\end{cases}\n\nDuring the training phase, calculate the focal loss for B_{\\text{stop flag}} as part of the global loss.\nInferencing sentence by sentence: In the SLLM architecture, we use a small model (SentenceVAE) to encode multiple tokens within a sentence into a single sentence embedding vector. This enables the LLM to conduct inference on a sentence-by-sentence basis rather than the traditional token-by-token approach. Experimental results indicate that this approach can significantly reduce the computational cost required for model inference and enhance inference speed."}, {"title": "Experiment", "content": "To validate our idea, we first trained individual Sentience-VAEs using self supervised methods, demonstrating that multiple tokens in a sentence can be compressed into a single sentence vector by an encoder and restored by a decoder. Subsequently, we grafted the encoder and decoder at the beginning and end of the open-source LLMs, proving that with simple modifications, LLMs can be upgraded to SLLMs and work in sentence embedding space, while preserving PPL and improving inference speed. Meanwhile, by observing the loss curve, we found that SLLMs still follows the Scaling Law.\nThe experiments described in this paper were conducted using either a single 4-card RTX 4090 (24G) or a 4-card A100 (40G) (for SLLM-1.3B) setup, with training carried out in a data parallel distribution.\nDataset: We use the English subset (EN/WebText) of the Wanjuan-1.0 dataset as the training dataset. The number of training samples depends on the number of iterations, with SentenceVAE training samples consisting of approximately 153.6M sentences (approximately 1.7B tokens); The training sample size of SLLM is about 6.4M paragraphs (approximately 5.6B tokens). The validation set consists of 1,000 random sentences or paragraphs that are mutually exclusive with the training set.\nHyperparameters: When training SentenceVAE, we assume each sentence has a maximum of 64 tokens, with a batch size of 128/card, a base learning rate of le-7/batch/card, and a maximum of 300K iterations.\nFor training SLLM, we assume each sentence has a maximum of 64 tokens, each paragraph has a maximum of 64 sentences (due to memory limitations), a batch size of 1/card, a base learning rate of le-6/batch/card, and a maximum of 1.6M iterations.\nAll experiments were conducted using the AdamW optimizer with AMP enabled, a weight decay coefficient of 0.01, and a maximum L2 norm of 1 for gradient clipping.\nThe first 5K iterations utilized a linear learning rate schedule, starting at 1/1000 of the base learning rate. After 5K iterations, a cosine annealing learning rate schedule was employed with a period of 20K iterations. Additionally, all experiments implemented the Exponential Moving Average (EMA) strategy.\nMetrics: We use perplexity (PPL) as an evaluation metric. The formula for calculating perplexity for output logits P of Sentence Decoder and ground truth is as Eq. 10.\n\n PPL = exp( - \\frac{1}{L'} \\sum_{i=1}^{L'} log(p_{d_i}))\n\nA series of word-level tokens can be represented by a sentence-level embedding with strong robustness\nIn this experiment, we trained individual SentienceVAEs using a self-supervised approach. If the Sentence Encoder can compress multiple tokens from the input sentence into a single sentence embedding vector and the Sentence Decoder can use this vector to reconstruct the original token sequence, it would validate the feasibility of our theory.\nWe trained SentenceVAEs with hidden sizes of 768, 1024, and 2048, corresponding to OPT-125M, OPT-350M, and OPT-1.3B, respectively. For each hidden size, we varied the number of block layers to 1, 2, and 4. We evaluated the models using cross-entropy loss and perplexity (PPL) metrics on the validation set.\nThe evaluation results demonstrate that Sentence VAEs fulfill their intended purpose, confirming the theoretical validity."}, {"title": "LLMs can work in setence-level embedding space with faster inference speed, more accurate PPL, and longer context, resulting SLLMs", "content": "After verifying the feasibility of Sentence level embedding, we grafted the encoder and decoder of SentenceVAE onto the beginning and end of the OPT series models, corresponding to the relationship shown in Table 4.\nWe did not test SLLM-1.3B-H4 due to memory overflow issues encountered during data-parallel distributed training. The benchmark test results of OPTs and SLLMs are shown in Table 3.\nFaster inference speed: We assessed the throughput rate of SLLMs using the PyTorch framework, without incorporating optimization methods such as PagedAttention or acceleration engines like TensorRT-LLM or LMDeploy. The throughput rate of the original OPT model served as the baseline for comparison. The experimental results show that SLLMs is 2~3\u00d7 faster on average than LLMs. The larger the number of model parameters, the smaller the proportion of inferencing cost occupied by SentenceVAE, and the more obvious the speed benefit.\nMore accurate PPL: We observed that the PPL metric for SLLMs was better than that of the baseline OPT models. This improvement is likely due to the SLLM paradigm enabling the model to process natural language at a higher sentence level, leading to enhanced performance. Additionally, the attention computation in the sentence embedding space allows SLLMs to manage shorter relative contexts for texts of the same length, which further contributes to the improved model performance.\nLonger context length: The context length that a model can handle is primarily constrained by GPU memory. Under the SLLM framework, multiple original tokens can be compressed into a single token, allowing the model to use less memory when processing contexts of the same length. This effectively increases the context length that the model can handle given the same hardware resources.\nBy analyzing the loss curve of SLLM during the training process, we observed that it adheres to the Scaling Law, as shown in Fig. 3. This suggests that the SLLM framework can be effectively extended to larger-scale language models through scaling."}, {"title": "Conclusion & Future Trends", "content": "In this section, we summarize our work and suggest several potential directions for future development.\nScaling up SLLMs with better structures\nIn this work, to rapidly validate our ideas, we utilized the classic Transformer architecture implemented in the PyTorch framework. Due to constraints in computing power and time, we limited our validation to models ranging from 125M to 1.3B parameters. However, by verifying the Scaling Law, we inferred the applicability of our approach to larger scale models. Numerous state-of-the-art architectural optimizations can be integrated into the SLLM framework, such as Rotational Position Encoding (ROPE), which may address the order problem observed in Sample 6. Currently, our models are trained solely on English corpora, but there is potential to add multilingual support in the future. In summary, while the SLLM framework is still in its early stages, it holds significant potential for further optimization.\nThe SLLM framework is inherently a hybrid architecture combining small and large models. By identifying the optimal balance point, the small model (SentenceVAE) and the large model (LLM) can be effectively distributed between the edge and cloud, balancing computational load and enhancing user experience.\nCurrently, embodied intelligence based on the LLM Agent paradigm struggles to interact directly with underlying hardware, relying instead on intermediate components to translate high-level LLM instructions into traditional real-time control logic. The primary limitation is the insufficient rate of \"token\" generation on the edge side. SLLM, however, can process and generate more \u201ctokens\u201d within the same computational and temporal constraints, offering the potential for embodied intelligent large models to interface directly with underlying hardware.\nThe \"frames\" in videos and \"trunks\" in audio are, to some extent, equivalent to \"tokens\" in language models. Therefore, the SLLM paradigm can similarly be applied to multimodal large models to increase the processing \u201cframe rate\u201d, enhance user experience, and achieve performance comparable to or even surpassing that of GPT-4o."}]}