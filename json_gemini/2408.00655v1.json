{"title": "SentenceVAE: Faster, Longer and More Accurate Inference with Next-sentence Prediction for Large Language Models", "authors": ["Hongjun An", "Yifan Chen", "Xiaozhen Qiao", "Zhe Sun", "Xuelong Li"], "abstract": "Contemporary large language models (LLMs) predominantly\nutilize a next-token prediction method for inference, which\nsignificantly impedes their processing speed. In this paper,\nwe introduce a novel inference methodology termed next-\nsentence prediction, aimed at enhancing the inference effi-\nciency of LLMs. We present SentenceVAE, a tiny model con-\nsisting of an encoder and a decoder. The encoder effectively\ncondenses the information within a sentence into a singular\ntoken, while the decoder reconstructs this compressed data\nback into its original sentential form. By integrating Sen-\ntenceVAE into the input and output layers of LLMs, we de-\nvelop Sentence-level LLMs (SLLMs) that employ a sentence-\nby-sentence inference approach, markedly accelerating infer-\nence speeds. SentenceVAE also maintains the integrity of the\noriginal semantic content by segmenting the text into sen-\ntences, thereby preserving accuracy while boosting inference\nspeeds. Compared to traditional LLMs, SLLMs process fewer\ntokens over equivalent context lengths, significantly reducing\nmemory demands for Self-Attention computations and facili-\ntating the handling of longer contexts. Our experimental find-\nings reveal that this method can increase inference speeds by\n204~365%, reduce perplexity (PPL) to 46~75% of its orig-\ninal metric, and decrease memory overhead by 86~91% for\nthe same context length. The advantages of this approach are\nfurther amplified with increases in model parameters.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated remark-\nable proficiency in understanding human intent through the\nacquisition of knowledge from vast and diverse datasets.\nThese models provide rich and accurate information and re-\nsponses, making them indispensable in applications such as\nmachine translation (Zhang, Haddow, and Birch 2023; Wang\net al. 2023), chatbots (Islam and Moushi 2024; Wang et al.\n2024), and question-answering systems (Singhal et al. 2023;\nLazaridou et al. 2022). The predominant approach to in-\nference in LLMs involves next-token prediction, where the\nmodel generates tokens sequentially. While this token-by-\ntoken generation method has achieved significant perfor-\nmance, it is inherently limited in that it produces only one\ntoken per inference step. This limitation results in substan-\ntial computational overhead and increases the overall infer-\nence time, thereby constraining the scalability and efficiency\nof LLMs. As a result, there is a pressing need to explore al-\nternative methods that can mitigate these limitations while\nmaintaining or improving upon the accuracy and responsive-\nness of LLMs.\nTo enhance the inference efficiency of Large Language\nModels (LLMs) and reduce computational costs, Gloeckle\net al. (Gloeckle et al. 2024) proposed training LLMs to pre-\ndict multiple tokens simultaneously, aiming to accelerate the\ninference process. However, the number of tokens predicted\nat once is fixed. For instance, when this number is set to five,\nregardless of the input text, it is forcibly segmented into units\nof five tokens. While this multi-token prediction method im-\nproves the inference speed of LLMs, it may disrupt the orig-\ninal semantics of the text by aggregating unrelated tokens\ntogether, potentially compromising inference accuracy.\nTo enhance inference speed while maintaining accuracy,\nwe explore methods for adaptively selecting the optimal\nnumber of predicted tokens based on the semantic con-\ntent of the text. Consequently, we introduce SentenceVAE,\nwhich comprises a sentence encoder and a sentence de-\ncoder. The schematic of SentenceVAE is illustrated in Fig.\n1. The sentence encoder is designed to condense the infor-\nmation of an entire sentence into a single token, while the\nsentence decoder reconstructs this compressed token back\ninto a sentence. By integrating the encoder and decoder\nof SentenceVAE into the input and output layers of Large\nLanguage Models (LLMs), these models can perform next-\nsentence prediction, thereby accelerating the reasoning pro-\ncess. Moreover, since SentenceVAE segments text at the\nsentence level, the semantic integrity is better preserved, en-\nsuring the accuracy of the inferrence.\nSpecifically, prior to inputting text into LLMs, Sentence-\nVAE segments the text into sentences and utilizes the sen-\ntence encoder to compress the information contained within\neach sentence into a single token. These compressed tokens\nare then fed as input to the LLMs. The LLMs predict their\noutput based on these compressed tokens. Finally, the sen-\ntence decoder decodes the predicted token into the final out-\nput of the LLMs.\nThe contribution of the proposed method can be described\nas follows:\n\u2022 We propose SentenceVAE, a model capable of compress-\ning the information content of a sentence into a single\ntoken, and conversely, expanding such tokens to recon-\nstruct sentences with high informational fidelity.\n\u2022 We embed the encoder and decoder components of Sen-\ntenceVAE into the input and output layers of LLMs, en-\nabling these models to implement an effective inference\nmethod for next-sentence prediction.\n\u2022 Extensive experiments conducted on various LLMs have\ndemonstrated that the proposed method significantly ac-\ncelerates inference speed while preserving the accuracy\nof these models."}, {"title": "Related Work", "content": "Large language models: In recent years, large language\nmodels (LLMs) have achieved remarkable performance in\nthe field of natural language processing (NLP). However,\nthese models typically possess a vast number of parameters\nfrom billions to hundreds of billions, exemplified by models\nsuch as TeleChat (1B-52B) from TeleAI (Wang et al. 2024;\nLi et al. 2024b), Llama (7B-405B) from Meta (Touvron et al.\n2023a,b), InternLM (1.8B-20B) from Shanghai AI Lab (Cai\net al. 2024), etc. The sheer magnitude of these parameters\nrenders the inference process computationally intensive and\ntime-consuming. Moreover, most LLMs employ a one-token\nprediction method, where the model predicts only one token\nat a time. This token-by-token approach further exacerbates\nthe inference time of LLMs. As a result, accelerating the\ninference speed of LLMs while maintaining their accuracy\npresents a significant challenge for researchers in the field.\nMulti token prediction: The transformer model en-\nables the parallel processing of sequences through its self-\nattention mechanism, providing a technical foundation for\nthe implementation of multi-token prediction (Vaswani et al.\n2017). The BERT model utilizes bidirectional encoder rep-\nresentations to capture contextual information, further so-\nlidifying the basis for the implementation of multi-token\nprediction (Devlin et al. 2018). The T5 model incorporates\nmulti-token prediction during the training process for the\nfirst time, significantly enhancing the coherence and quality\nof the generated text (Raffel et al. 2020). The Better & Faster\nLLMs apply multi-token prediction to LLMs, allowing them\nto directly predict multiple tokens in a single inference step,\nthereby reducing the number of inference iterations and as-\nsociated computational costs (Gloeckle et al. 2024). How-\never, the number of predicted tokens is fixed, which may\ninadvertently combine unrelated tokens, potentially disrupt-\ning the original semantics of the sentence and consequently\ndecreasing inference accuracy.\nEncoder-decoder model: The concept of an encoder\nis introduced by Yann LeCun in his doctoral dissertation,\nwherein he posited that an encoder could transform in-\nput data into shorter representations without losing the\ncore information, thereby providing a foundational ap-\nproach to compressing high-dimensional data. The Varia-\ntional Autoencoder (VAE) model introduces the use of hid-\nden variables between the encoder and decoder to learn the\ndata distribution, laying the groundwork for deep-learning-\nbased encoders and decoders (Kingma and Welling 2013).\nSutskever et al. introduces the encoder-decoder framework\ninto the field of deep learning, utilizing recurrent neu-\nral networks (RNNs) as both the encoder and decoder\nfor sequence-to-sequence (Seq2Seq) learning, demonstrat-\ning exceptional performance in machine translation tasks\n(Sutskever, Vinyals, and Le 2014). The SegNet applies the\nencoder-decoder model to the field of image segmentation,\nsignificantly enhancing segmentation performance (Badri-\nnarayanan, Kendall, and Cipolla 2017). Subsequently, the\nencoder-decoder model has achieved great success in vari-\nous domains including machine translation (Cho et al. 2014;\nMakin, Moses, and Chang 2020), text-to-speech conversion\n(Li et al. 2020), computational imaging (Chen et al. 2023; Li\net al. 2024a), and beyond, fully demonstrating the capability\nof the encoder-decoder architecture to efficiently compress\nand restore data.\nBy leveraging the advantages of multi-token prediction\nand the encoder-decoder model, we propose SentenceVAE."}, {"title": "Method", "content": "In this section, we introduce the SentenceVAE framework.\nEmbedding this framework into a LLM enables the model to\nimplement a sentence-by-sentence prediction method, sig-\nnificantly accelerating its inference speed. The inference\nprocess of the proposed method compared to traditional\nLLMs is illustrated in Fig. 2. It is evident that for the\nsame text, an LLM embedded with SentenceVAE requires\nonly three inference steps to obtain the inference result,\nwhereas traditional LLMs necessitate twenty-free inference\nsteps. Furthermore, the proposed method divides the text\ninto sentences without compromising the original semantics,\nthereby enhancing the inference speed of LLMs while main-\ntaining inference accuracy."}, {"title": "Sentence Variational Autoencoder (SentenceVAE)", "content": "Our SentenceVAE primarily consists of a Sentence Encoder\nand a Sentence Decoder. The Sentence Encoder encodes\nmultiple word-level tokens from a sentence into a single\nsentence-level token, while the Sentence Decoder recon-\nstructs this sentence-level token back into the original se-\nquence of word-level tokens. To ensure that the multiple to-\nkens input to the Sentence Encoder come from individual\nsentences, we propose a specialized Sentence Segmentation\nMechanism. Additionally, we introduce a Feature Fusion\nMechanism, which enable the encoder to encode variable-\nlength token sequences into a single sentence-level token.\nSentence Segmentation Mechanism: To ensure that all\nword-level tokens input to the sentence encoder per time\ncome from a single sentence, we use regular matching to\nsplit sentences by punctuation marks such as \",\", \".\",\n\"?\", \"!\" before tokenizing. Let the original string be S,\nand after partitioning, the set {si|1 \u2264 i \u2264 n} is obtained,\nsatisfying S = 818283...Sn.\nSentence Encoder: Taking s = st(1 \u2264 t \u2264 n) as an\nexample, after tokenization, we obtained a sequence of L\ntoken ids D = [d1, d2, d3, ..., d\u2081] representing a sentence.\nThis sequence was then passed through an embedding layer,\nresulting in word-level embedding vectors of E, as shown\nin Eq.1."}, {"title": "", "content": "E = (ei)Lxhidden_size = Embed(D) \t\t\t\t\t(1)\nThese embeddings were subsequently input into self-\nattention based encoder blocks, yielding hidden features H,\nas shown in Eq. 2."}, {"title": "", "content": "H = (hi) Lxhidden_size = EncoderBlocks(E) \t\t\t\t\t(2)\nTo derive the sentence embedding vector, we fused these\nL features into a single vector t \u2208 Rhidden_size. This sen-\ntence embedding vector is then fed into the decoder-only\nLLM, which generates a new sentence embedding vector\nt+1 at each time step."}, {"title": "Feature Fusion Mechanism", "content": "After the encoder blocks\ngenerates H, we propose a method to fuse them into a single\nsentence embedding vector \u03a9t. This method involves accu-\nmulating the L vectors and normalizing them using Layer\nNormalization (Ba, Kiros, and Hinton 2016), as shown in\nEq. 3."}, {"title": "", "content": "\u03a9 = LayerNorm (\u2211hi)\t\t\t\t\t(3)\ni=1\nSentence Decoder: The Sentence Decoder contains a\ncombination of masked self-attention and cross-attention\nblocks, where the cross-attention mechanism uses the in-\ndividual sentence embedding vector t+1 as key (K) and\nvalue (V).\nIn the prediction phase, given t+1, initialize the input\ntoken do =< bos >, the decoder outputs d\u2081, and then input\n[do, d1] and Nt+1 into the decoder. Repeat this process until\ndv+1 =< eos >, and terminate the iteration, as shown in\nEq.4."}, {"title": "", "content": "init: do =< bos >,l' = 0\ndo:\nDv = [do, d1, d2, ..., dv]\nPl' = (Pi) (l'+1)xhidden_size\n= DecoderBlocks(Dv, \u03a9t+1)\t\t\t\t\t(4)\ndy = argmax(p)\nDi+1 = concat(Dv, dv)\nl' = l' + 1\nuntil: dy =< eos >\nret: DL'='\nAfter the iteration is completed, the detokenization pro-\ncess is used to convert the output DL back into a string.\nIn the training phase, parallel training is adopted, based on\nmasked self-attention mechanism, to ensure that only tokens\nbefore du can be seen when inferring dv+1. Given an input\nsequence of Dtrain = [do,d1, ..., d\u2081'], model output logits\nof P\u2208 []R(L'+1)\u00d7hidden_size, and ground truth of Dgroud_truth =\n[d1, d2, ..., dL', <eos >], focal loss (Lin et al. 2017) is taken\nas the loss function, as shown in Eq. 5."}, {"title": "", "content": "P = DecoderBlocks(Dtrain, Nt+1)\nP' = softmax(P)\nLoss = \u2211 - (1 - Pa\u2081) log(pd\u2081), y = 2\t\t\t\t\t(5)\ni=1"}, {"title": "Sentence-level Large Language Models (SLLMs)", "content": "Currently, almost all decoder-only LLMs consist of the\nfollowing components: an embedding layer EMBilm, N\ndecoder-only blocks DBilm, and a fully connected layer\nFCilm for outputting logits. As shown in Eq. 6, given con-\ntext tokens Dcontext = [do,d1,..., dv], the model generates\nthe next token dv+1."}, {"title": "", "content": "Econtext = EMBilm (Dcontext)\nHilm = DBilm (Econtext)\nFCllm (Hllm)\nPilm = (Pllm,i) (l'+1)\u00d7hidden_size\t\t\t\t\t(6)\ndv+1 = argmax(Pilm,l')\nIn this section, we introduce a unified grafting method\nto graft SentienceVAE onto the beginning and end of any\nLLM. This approach transforms the LLMs into Sentence-\nlevel Large Language Models (SLLMs), enabling it to oper-\nate effectively in the sentence-level embedding space.\nDiscard Embedding Layer: In a SLLM, the N decoder-\nonly blocks no longer receives word-level tokens directly.\nInstead, it receives sentence embedding vectors encoded by\nthe sentence encoder. Consequently, the traditional embed-\nding layer in the LLM architecture is removed. Therefore,\nthe model follows Eq. 7."}, {"title": "", "content": "Esllm = [1, 2, ..., \u03a9t]\nHsllm = (hsllm,i)txhidden_size\n= DB1lm (Esllm)  \t\t\t\t\t(7)\nTermination judgment layer: In traditional LLMs, the\nfinal fully connected layer FC1lm typically converts an\nHllm \u2208 Rtxhidden size to a probability vector (logits) Pilm \u2208\nRt\u00d7V, where V represents the size of the vocabulary. This\nallows the model to determine the next token output us-\ning sampling algorithms, e.g., Greedy Search, Beam Search\n(Graves 2012), Random Sampling, Temperature Sampling\n(Hinton, Vinyals, and Dean 2015), Top-K Sampling (Fan,\nLewis, and Dauphin 2018), and Nucleus Sampling (Holtz-\nman et al. 2019). If the current token output is a special token\nlike eos (end-of-sequence), the iteration terminates.\nHowever, in SLLMs, the output of DB1lm is a sentence-\nlevel hidden state vector Hsllm. Therefore, traditional token\ngeneration methods are not applicable. Instead, we use a new\nfully connected layer FCsllm called termination judgment\nlayer that converts the Hsllm into a 2-dimensional boolean\nvector Bstop_flag, as shown in Eq. 8. This vector helps deter-\nmine whether the current sentence-level hidden state vector\nsignifies the end of a sentence (stop flag) or needs further\ndecoding by the sentence decoder. If the vector indicates an\nend flag, the iteration terminates. Otherwise, the sentence\ndecoder processes the embedding to generate corresponding\ntokens, as shown in Eq. 9."}, {"title": "", "content": "Bstop-flag = (bi)tx2\t\t\t\t\t(8)\n= FCsllm (Hsllm)\n\u03a9t+1 = {\t< <eos >, argmax(bt)\thsllm,t , argmax(bt) = 0  = 1\t\t\t\t\t(9)\nDuring the training phase, calculate the focal loss for\nBstop flag as part of the global loss.\nInferencing sentence by sentence: In the SLLM archi-\ntecture, we use a small model (SentenceVAE) to encode"}, {"title": "", "content": "PPL = exp( 1 \u2211 log(pd\u2081))\t\t\t\t\t(10)\ni=1"}, {"title": "Experiment", "content": "To validate our idea, we first trained individual Sentience-\nVAEs using self supervised methods, demonstrating that\nmultiple tokens in a sentence can be compressed into a sin-\ngle sentence vector by an encoder and restored by a decoder.\nSubsequently, we grafted the encoder and decoder at the be-\nginning and end of the open-source LLMs, proving that with\nsimple modifications, LLMs can be upgraded to SLLMs and\nwork in sentence embedding space, while preserving PPL\nand improving inference speed. Meanwhile, by observing\nthe loss curve, we found that SLLMs still follows the Scaling\nLaw."}, {"title": "Experimental Setting", "content": "The experiments described in this paper were conducted us-\ning either a single 4-card RTX 4090 (24G) or a 4-card A100\n(40G) (for SLLM-1.3B) setup, with training carried out in a\ndata parallel distribution.\nBase Models: Due to memory and time limitations, we\nonly use the 125m, 350m, and 1.3B models of the OPT se-\nries (Zhang et al. 2022) as the base LLMs, and demonstrate\nthe feasibility of extending to larger scale models through\nScaling Law (Kaplan et al. 2020).\nDataset: We use the English subset (EN/WebText) of the\nWanjuan-1.0 dataset (He et al. 2023) as the training dataset.\nThe number of training samples depends on the number of\niterations, with SentenceVAE training samples consisting of\napproximately 153.6M sentences (approximately 1.7B to-\nkens); The training sample size of SLLM is about 6.4M\nparagraphs (approximately 5.6B tokens). The validation set\nconsists of 1,000 random sentences or paragraphs that are\nmutually exclusive with the training set.\nHyperparameters: When training SentenceVAE, we as-\nsume each sentence has a maximum of 64 tokens, with\na batch size of 128/card, a base learning rate of le-\n7/batch/card, and a maximum of 300K iterations.\nFor training SLLM, we assume each sentence has a max-\nimum of 64 tokens, each paragraph has a maximum of\n64 sentences (due to memory limitations), a batch size of\n1/card, a base learning rate of le-6/batch/card, and a maxi-\nmum of 1.6M iterations.\nAll experiments were conducted using the AdamW opti-\nmizer (Loshchilov and Hutter 2017) with AMP enabled (Mi-\ncikevicius et al. 2017), a weight decay coefficient of 0.01,\nand a maximum L2 norm of 1 for gradient clipping.\nThe first 5K iterations utilized a linear learning rate sched-\nule, starting at 1/1000 of the base learning rate. After 5K\niterations, a cosine annealing learning rate schedule was\nemployed with a period of 20K iterations. Additionally, all\nexperiments implemented the Exponential Moving Average\n(EMA) strategy.\nMetrics: We use perplexity (PPL) as an evaluation metric.\nThe formula for calculating perplexity for output logits P of\nSentence Decoder and ground truth Dgroud_truth is as Eq. 10."}, {"title": "A series of word-level tokens can be represented by\na sentence-level embedding with strong robustness", "content": "In this experiment, we trained individual SentienceVAEs\nusing a self-supervised approach. If the Sentence Encoder\ncan compress multiple tokens from the input sentence into\na single sentence embedding vector and the Sentence De-\ncoder can use this vector to reconstruct the original token\nsequence, it would validate the feasibility of our theory.\nWe trained SentenceVAEs with hidden sizes of 768, 1024,\nand 2048, corresponding to OPT-125M, OPT-350M, and\nOPT-1.3B, respectively. For each hidden size, we varied the\nnumber of block layers to 1, 2, and 4. We evaluated the mod-\nels using cross-entropy loss and perplexity (PPL) metrics on\nthe validation set. The experimental results are presented in\nTable 1.\nThe evaluation results demonstrate that Sentence VAEs\nfulfill their intended purpose, confirming the theoretical va-\nlidity. Table 2 presents several test cases. Among these,\nSamples 1, 2, 3, 5, 6 are convention samples. To assess\nthe model's robustness, we also included several out-of-\ndistribution samples, highlighted in blue in the table:\n\u2022 Sample 4, 7: Data preprocessing involves segmenting\nsentences based on punctuation marks, such as \",\" and\n\".\". Consequently, these samples will be split into mul-\ntiple inputs during both training and practical application.\nDirectly inputting these samples without preprocessing is\nconsidered invalid input for the model.\n\u2022 Sample 8: This is the pinyin of a famous Chinese saying,\nnot English, so it is illegal input."}, {"title": "LLMs can work in setence-level embedding space\nwith faster inference speed, more accurate PPL,\nand longer context, resulting SLLMs", "content": "After verifying the feasibility of Sentence level embedding,\nwe grafted the encoder and decoder of SentenceVAE onto\nthe beginning and end of the OPT series models, corre-\nsponding to the relationship shown in Table 4.\nWe did not test SLLM-1.3B-H4 due to memory overflow\nissues encountered during data-parallel distributed training.\nThe benchmark test results of OPTs and SLLMs are shown\nin Table 3.\nFaster inference speed: We assessed the throughput\nrate of SLLMs using the PyTorch framework (Imambi,\nPrakash, and Kanagachidambaresan 2021), without incorpo-\nrating optimization methods such as PagedAttention (Kwon\net al. 2023) or acceleration engines like TensorRT-LLM or"}, {"title": "SLLMs assist in multimodal large model", "content": "The \"frames\" in videos and \"trunks\" in audio are, to some\nextent, equivalent to \"tokens\" in language models. There-\nfore, the SLLM paradigm can similarly be applied to multi-\nmodal large models to increase the processing \u201cframe rate\u201d,\nenhance user experience, and achieve performance compa-\nrable to or even surpassing that of GPT-40."}]}