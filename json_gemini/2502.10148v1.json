{"title": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis", "authors": ["Zhiyuan Li", "Wenshuai Zhao", "Joni Pajarinen"], "abstract": "Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASS achieves up to 30% higher win rates than state-of-the-art MARL algorithms in symmetric scenarios.", "sections": [{"title": "1. Introduction", "content": "A major long-term goal for the field of cooperative multi-agent systems (MAS), e.g. multi-robot control (Gao et al., 2024; Feng et al., 2024), power management (Monroc et al., 2024), and multi-agent games (Samvelyan et al., 2019; Kurach et al., 2020), is to build a protocol of collaboration among the agents. Multi-agent reinforcement learning (MARL), proven as an advanced paradigm of distributed artificial intelligence (AI), holds promise for discovering collective behavior from interactions. One line of works follows centralized training decentralized execution (CTDE) paradigm (Rashid et al., 2020; Yu et al., 2022; Liu et al., 2024; Lowe et al., 2017; Zhu et al., 2025; Li et al., 2024b). CTDE assumes a central controller that exploits global information, while the individual policies are designed to allow for decentralized execution. However, in many real-world scenarios, the central controller becomes unfeasible due to the communication overhead that exponentially scales with the number of agents, thereby compromising the scalability of MAS. In contrast, the decentralized training decentralized execution paradigm (DTDE) discards this assumption and is scalable to large-scale systems (Su & Lu, 2024; Zhang et al., 2018; Ma et al., 2024a). However, DTDE requires complicated learning and planning under uncertainty, as partial observability magnifies the discrepancy between each agent's local observation and global information. Although much progress has been made, MARL suffers from compromised sample efficiency, interpretability, and transferability.\nThe emergence of Large Language Models (LLMs) has revitalized this field. LLM-based multi-agents have been proposed to leverage their remarkable capacity to perform task-oriented collective behaviors (Mandi et al., 2024; Zhang et al., 2024; Gong et al., 2024; Zhang et al., 2025; Nayak et al., 2025). The LLMs are used for high-level planning to generate centralized (Gong et al., 2024; Nayak et al., 2025; Deng et al., 2024) or decentralized plans (Mandi et al., 2024; Zhang et al., 2024), often adopting a hierarchical decision-making structure in conjunction with a pre-defined low-level controller. While these methods have succeeded in a set of multi-agent problems including Overcooked-AI (Carroll et al., 2019), SMAC (Samvelyan et al., 2019), and VirtualHome (Puig et al., 2018), heavy reliance on text-based observation prevents them from learning from multi-modal information. Moreover, they ignore the non-Markovian nature of MAS, where learning and planning necessitate a decentralized closed-loop solution.\nIn this paper, we mitigate the existing research limitations and advance general decision-making for cooperative multi-agent systems. At a high level, COMPASS combines a vision language models (VLMs)-based planner with a dynamic skill library for storing and retrieving complex behaviors, along with a structured communication protocol. A diagram of COMPASS is provided in Figure. 1. Inspired by Cradle (Tan et al., 2024), the VLM-based planner perceives the visual and textual observation and suggests the most suitable executable code from the skill library. We adopt the code-as-policy paradigm (Wang et al., 2024d) instead of task-specific primitive actions, as it constrains generalizability and fails to fully leverage foundation models' extensive world knowledge and sophisticated reasoning capabilities.\nTraditional open-loop methods struggle to produce effective plans that adapt to dynamics in stochastic, partially observable environments. To address this challenge, the VLM-based planer attempts to solve challenging and ambiguous final tasks, such as \"Defeat all enemy units in the StarCraft multi-agent combat scenario while coordinating with allied units\", by progressively proposing a sequence of clear, manageable sub-tasks while incorporating environmental feedback and task progress. COMPASS generates Python scripts through LLMs as semantic-level skills to accomplish sub-tasks, incrementally building a skill library throughout the task progress. Each skill is indexed through its documentation embeddings, enabling retrieval based on task-skill relevance. However, developing the skill library from scratch requires extensive exploration to discover viable strategies. In contrast, we pre-collect demonstration videos and introduce a \"warm start\" by initializing the skill library with strategies derived from the expert-level dataset.\nMoreover, building autonomous agents to cooperate in completing tasks under partial observation requires an efficient communication protocol. However, naive communication leads to the risks of hallucination caused by meaningless chatter between agents (Li et al., 2024a). Inspired by entity-based MARL (Iqbal et al., 2021; Ding et al., 2023), we present a structured communication protocol to formulate the communication among agents along with a global memory that allows all agents to retrieve. The protocol incorporates a multi-hop propagation mechanism, enabling agents to infer information about entities beyond their field of view through information shared by teammates. Similar to previous approaches, each agent maintains a local memory to preserve current and historical experiences.\nEmpirically, COMPASS demonstrates effective adaptation and skill synthesis in cooperative multi-agent scenarios. Through its dynamic skill library, it creates reusable and interpretable code-based behaviors that evolve during task execution. We evaluate COMPASS systematically in the improved StarCraft Multi-Agent Challenge (SMACv2) using both open-source (Qwen2-VL-72B (Wang et al., 2024c)) and closed-source (GPT-4o-mini\u00b9, Claude-3-Haiku\u00b2) VLMs. COMPASS achieves strong results in Protoss scenarios with a 57% win rate, substantially outperforming state-of-the-art MARL algorithms, including QMIX (Rashid et al., 2020), MAPPO (Yu et al., 2022), HAPPO (Kuba et al., 2022), and HASAC (Liu et al., 2024). COMPASS maintains moderate performance in Terran scenarios and handles asymmetric settings effectively, though showing limited success in Zerg task. We further demonstrate COMPASS's ability to bootstrap effective strategies from expert demonstrations."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Agents in the StarCraft Multi-Agent Challenge", "content": "SMAC (Samvelyan et al., 2019), a predominant cooperative MARL benchmark based on the StarCraft II real-time strategy game (Vinyals et al., 2017), focuses on decentralized micromanagement scenarios where each unit operates under decentralized execution with partial observability to defeat enemy units controlled by Starcraft II's built-in AI opponent. Previous research in SMAC resort to MARL which can be divided into two categories: 1) Online MARL: One line of representative research is value decomposition (VD) (Rashid et al., 2020; Wang et al., 2021; Son et al., 2019), which decomposes the centralized action-value function into individual utility functions. On the other hand, multi-agent policy gradient (MAPG) methods (Yu et al., 2022; Kuba et al., 2022; Liu et al., 2024; Li et al., 2023; Wen et al., 2022; Hu et al., 2024; Na et al., 2024) extend single-agent policy gradient algorithm to multi-agent with coordination modeling. Researches such as MAPPO (Yu et al., 2022), HAPPO (Kuba et al., 2022), and HASAC (Liu et al., 2024) combine trust region and maximum entropy with MARL in a non-trivial way respectively. To encourage coordination, communication methods (Hu et al., 2024; Lo et al., 2024), sequential modeling methods (Li et al., 2023; Wen et al., 2022), and cooperative exploration methods (Mahajan et al., 2019; Na et al., 2024) have been proposed. 2) Offline MARL: Recent efforts such as MADT (Meng et al., 2023), ODIS (Zhang et al., 2022), and MADiff (Zhu et al., 2025) leverage data-driven training via pre-collected offline datasets to enhance policy training efficiency. However, the near-optimal performance of these existing approaches on SMAC highlights the benchmark's limited stochasticity and partial observability. To address these limitations, SMACv2 (Ellis et al., 2024) introduces more complexity to necessitate decentralized closed-loop control policies.\nThere have been some recent attempts (Li et al., 2024b; McClellan et al., 2024; Formanek et al., 2023; Li et al., 2024c) to evaluate MARL algorithms on SMACv2, and the results confirm the complexity. However, current learning-based multi-agent methods are computationally inefficient and non-interpretable. In the quest to find methods that are sample-efficient and interpretable, LLM-SMAC (Deng et al., 2024) leverage LLMs to generate centralized decision tree code under global information in an open-loop framework. Unlike prior works, COMPASS integrates a Vision-Language Model (VLM) with each agent in a decentralized closed-loop manner under partial observability, improving both real-world applicability and scalability."}, {"title": "2.2. LLM-based Multi-Agent System", "content": "Based on the inspiring capabilities of LLMs, such as zero-shot planning and complex reasoning (Kojima et al., 2024; Zhao et al., 2024; Wei et al., 2024; Besta et al., 2024), embodied single-agent researches have demonstrated the effectiveness of LLMs in solving complex long-horizon tasks (Wang et al., 2024b; Yao et al., 2023; Shinn et al., 2023; brian ichter et al., 2022; Ma et al., 2024b; Wang et al., 2024d; Tan et al., 2024). Despite significant advances in single-agent applications, developing real-world multi-agent systems with foundation models remains challenging, primarily due to the nature of decentralized control under partial observability in multi-agent settings (Pajarinen & Peltonen, 2011). Most prior efforts (Mandi et al., 2024; Zhang et al., 2024; 2025; Nayak et al., 2025; Gong et al., 2024) leverage a hierarchical framework with components like perception, communication, planning, execution, and memory to build multi-agent systems with collective behaviors. These approaches can be roughly classified into two groups. 1) Centralized plan: MindAgent (Gong et al., 2024) adopts a centralized planning scheme with a pre-defined oracle in a fully observable multi-agent game. LLaMAR (Nayak et al., 2025) employs LLMs to manage long-horizon tasks in partially observable environments without assumptions about access to perfect low-level policies. 2) Decentralized plan: ProAgent (Zhang et al., 2025) introduces Theory of Mind (ToM), enabling agents to reason about others' mental states. RoCo (Mandi et al., 2024) and CoELA (Zhang et al., 2024) assign separate LLMs to each embodied agent for collaboration with communication. However, RoCo and COELA assume a skill library with a low-level heuristic controller, which is impractical in real-world applications. Moreover, RoCo's open-loop plan-and-execute paradigm fails to incorporate environmental feedback during decision-making. In contrast, our work does not assume any pre-defined low-level controller and generates code-based action through VLMs in a closed-loop manner."}, {"title": "3. Preliminaries", "content": "We model a fully cooperative multi-agent game with N agents as a decentralized partially observable Markov decision process (Dec-POMDP) (Oliehoek & Amato, 2016), which is formally defined as a tuple $G = (N, S, O, O, B, A,T, \\Omega, R, \\gamma, \\rho_0)$. N = {1, ..., N} is a set of agents, s \u2208 S denotes the state of the environment and $ \\rho_0 $ is the distribution of the initial state. A = $ \\Pi_{i=1}^{N} A^i$ is the joint action space, O = $ \\Pi_{i=1}^{N} O^i$ is the set of joint observations. At time step t, each agent i receives an individual partial observation $ o_i \\in O^i$ given by the observation function $ O : (a_t, S_{t+1}) \\rightarrow P(O_{t+1} | a_t, S_{t+1})$ where $a_t, S_{t+1}$ and $O_{t+1}$ are the joint actions, states and joint observations respectively. Each agent i uses a stochastic policy $ \\pi^i(a_i|h^i, w_i)$ conditioned on its action-observation history $h^i = (o_0, a_0, ..., o_{t-1}, a_{t-1})$ and a random seed $w_i \\in \\Omega_t$ to choose an action $a_t \\in A$. A belief state $b_t$ is a probability distribution over states at time t, where $b_t \\in B$, and B is the space of all probability distributions over the state space. Actions at drawn from joint policy $ \\pi(a_t|s_t, w_t)$ conditioned on state $s_t$ and joint random seed $w_t = (w_1, ..., w_N)$ change the state according to transition function T : $(s_t, a_1, ..., a_N) \\rightarrow P(s_{t+1}|s_t, a_1, ..., a_N)$. All agents share the same reward $r_t = R(s_t, a_1,...,a_N)$ based on $s_t$ and $a_t$. $ \\gamma$ is the discount factor for future rewards. Agents try to maximize the expected total reward, $I(\\pi) = E_{\\delta_0, a_0, ...} [\\sum_{t=0}^{\\infty} \\gamma^t r_t]$, where $s_0 \\sim \\rho_0(s_0), a_t \\sim \\pi(a_t | S_t, w_t)$."}, {"title": "4. Methods", "content": "COMPASS, illustrated in Figure 1, is a decentralized closed-loop framework for cooperative multi-agent systems that continuously incorporate environmental feedback for strategy refinement. The architecture comprises three core components: 1) a VLM-based closed-loop planner that iteratively perceives, reasons, reflects and acts to adaptively complete tasks (Sec. 4.1); 2) an adaptive skill synthesis mechanism for generating executable codes tailored to proposed sub-tasks (Sec. 4.2); and 3) a structured communication protocol that enables agents to share visible entity information under partial observability (Sec. 4.3)."}, {"title": "4.1. VLM-based Closed-Loop Planner", "content": "Inspired by recent advances in cognitive architectures for autonomous systems (Tan et al., 2024), COMPASS implements a sophisticated modular planning framework that emulates key aspects of cognitive decision-making. The planner adopts a modular formulation, utilizing four specialized models: Perception, Task Reasoning, Self-Reflection, and Actor. Each model fulfills a distinct yet interconnected role in the decision-making process. The Perception model processes multi-modal inputs, integrating both visual and textual information to build comprehensive environmental understanding. The Task Reasoning model analyzes the perceived information to decompose complex objectives into manageable sub-tasks, ensuring systematic progress toward the final goal. The Self-Reflection model continuously evaluates task execution and outcome quality, enabling adaptive behavior refinement. The Actor model translates plans into actions by selecting and executing the most appropriate skills from the skill library. We next discuss the various components in detail:\nPerception forms the foundation of COMPASS's decision-making capabilities by enabling robust multi-modal understanding of complex environments. Solving complex real-world tasks often involves data of multiple modalities (Wang et al., 2024a), each contributing unique and complementary information for decision-making. We leverage the VLMs' ability to fuse and analyze a broader spectrum of data, including text- and image-based environment feedback, to enable agents to sense the surrounding environment. The system's perception mechanism operates at two levels: direct observation processing and collaborative information synthesis. At the direct level, VLMs process raw inputs to extract meaningful features and relationships from both visual and textual data. At the collaborative level, COMPASS addresses the inherent challenge of partial observability in multi-agent systems through an innovative multi-hop communication protocol (detailed in Sec. 4.3) that enables agents to construct a more holistic understanding of their environment by sharing and aggregating observations. This dual-level perception architecture ensures that each agent maintains both detailed local awareness and broader contextual understanding, essential for effective decision-making in complex cooperative tasks.\nTask Reasoning enables COMPASS to systematically approach complex cooperative challenges through collective task decomposition. Given a simple general final task in the cooperative multi-agent setting, e.g., \"defeat all enemy units\", in order to complete the task more efficiently, agents are required to decompose it into multiple sub-tasks and figure out the right one to focus on, while considering alignment among others (See Figure. 2). COMPASS harnesses"}, {"title": "4.2. Adaptive Skill Synthesis", "content": "COMPASS employs a dynamic skill library that maintains and evolves a collection of executable behaviors. Each skill is represented as an executable Python function with comprehensive documentation describing its functionality and corresponding embedding that enables semantic retrieval. This skill library undergoes continuous refinement through two complementary mechanisms: incremental synthesis, where new skills are generated and existing ones are refined during task execution, and demonstration-based bootstrapping, which initializes the library with behaviors extracted from expert demonstrations.\nIncremental Synthesis With the Task Reasoning component consistently proposing sub-tasks, COMPASS first attempts to retrieve relevant skills from the library using semantic similarity between the sub-task description and skill documentation embeddings. If no suitable skill exists, or if existing skills prove inadequate, the VLM generates a new Python script specifically tailored to the sub-task.\nBootstrapping However, developing the skill library from scratch requires extensive interactions with environments, which potentially leads to inefficient learning in the early stages. Inspired by offline MARL approaches (Meng et al., 2023; Zhang et al., 2022; Zhu et al., 2025), which leverage pre-collected datasets to enhance sample efficiency, we leverage MAPPO as the behavior policy to collect experiences, which are recorded as video sequences. The VLMs then analyze these demonstrations through a multi-stage process: first identifying key strategic patterns and behavioral primitives, then translating these patterns into executable Python functions with appropriate documentation. This initialization methodology establishes a foundational set of validated skills, substantially reducing the exploration overhead typically required for discovering effective behaviors. The resulting baseline skill library enables efficient task execution from the onset while maintaining the flexibility to evolve through incremental synthesis."}, {"title": "4.3. Structured Communication Protocol", "content": "To facilitate effective collaboration under partial observability, recent LLM-based multi-agent work (Li et al., 2024a; Zhang et al., 2024) employs conversational framework with unconstrained communication protocol. However, while natural language offers flexibility, unrestricted communication can lead to potential hallucinations caused by ambiguous or irrelevant messages between agents. Drawing from advances in structured communication frameworks (Hong et al., 2024) and entity-based MARL (Iqbal et al., 2021; Ding et al., 2023), COMPASS implements a hierarchical communication protocol that focuses on efficient entity-based information sharing and multi-hop propagation. Each agent maintains an observation buffer containing information about entities in its field of view. At each timestep, agents share their local observations, which are then aggregated into a global entity memory accessible to all. COMPASS employs a multi-hop communication mechanism to propagate information about distant entities, enabling agents to build a more holistic observation of the environment by leveraging the collective knowledge of the team."}, {"title": "5. Experiments", "content": "We conducted a comprehensive experimental evaluation of COMPASS to assess its performance and capabilities in complex multi-agent scenarios. Our evaluation focused on the improved StarCraft Multi-Agent Challenge (SMACv2) (Ellis et al., 2024), which provides an ideal testbed for examining cooperative behavior under partial observability and stochasticity. Through systematic experimentation, we investigated two fundamental questions: (1) How does COMPASS perform compared to state-of-the-art MARL methods? (2) What are the individual contributions of the adaptive skill synthesis mechanism? Experiments utilize both open-source (Qwen2-VL-72B) and closed-source VLMs (GPT-4o-mini, Claude-3-Haiku), with Jina Al embeddings for skill retrieval. All results are averaged over 5 seeds to account for environmental stochasticity."}, {"title": "5.1. Experimental Setup", "content": "Scenarios Our evaluation scenarios span three distinct race matchups (Protoss, Terran, and Zerg) and two categories (symmetric and asymmetric), as detailed in Table 1. The symmetric scenarios (5v5) test coordination in balanced engagements, while asymmetric scenarios (5v6) evaluate adaptation to numerical disadvantages. Each race combination presents unique tactical challenges due to different unit abilities and constraints.\nBaselines We compared COMPASS against the state-of-the-art MARL algorithms representing both value-based and policy-gradient approaches:\nValue-Based Methods: QMIX (Rashid et al., 2020) uses a mixing network architecture to decompose joint action-values while maintaining monotonicity constraints.\nPolicy Gradient Methods: MAPPO (Yu et al., 2022) extends PPO to multi-agent settings with the CTDE paradigm. HAPPO (Kuba et al., 2022) performs sequential policy updates by utilizing other agents' newest policy under the CTDE framework and provably obtains the monotonic policy improvement guarantee. HASAC (Liu et al., 2024) combines the maximum entropy framework with trust region optimization to enhance exploration and coordination.\nDatasets To enable effective bootstrapping of the skill library, we constructed a comprehensive demonstration dataset capturing diverse multi-agent strategies and interactions. We employed MAPPO with original hyperparameters as our behavior policy for data collection, leveraging its strong performance in cooperative multi-agent tasks. Our final dataset comprises over 300 complete game episodes, each recorded as a video sequence capturing the full state-action trajectory. These demonstrations span all symmetric scenario types described in Table 1."}, {"title": "5.2. Main Results", "content": "Performance As shown in Table 2, COMPASS demonstrates significant performance advantages in SMACv2, particularly excelling in Protoss scenarios where it achieves a 57% win rate in symmetric engagements using GPT-4o-mini, substantially outperforming traditional approaches like QMIX (27%), MAPPO (32%), and HAPPO (34%).\nHowever, performance varies across race matchups. While maintaining strong results in Terran scenarios (39% win rate), COMPASS shows limited effectiveness in Zerg scenarios (16% win rate). This performance disparity can be attributed to the unique mechanics of Zerg combat units, which demand more fine-grained micromanagement due to their shorter attack ranges and reliance on swarm-based tactics.\nIn asymmetric scenarios (5v6), COMPASS consistently outperforms MARL baselines in Protoss and Terran matchups, demonstrating its ability to execute effective strategies despite being outnumbered. Its success in these settings suggests that COMPASS can adapt dynamically, using coordinated tactics and learned skills to counteract numerical disadvantages. The robust performance holds across different VLM implementations, with GPT-4o-mini consistently achieving the strongest results.\nMoreover, COMPASS demonstrates particular advantages in scenarios with sparse reward, where traditional MARL approaches significantly underperform. As shown in Table 3, under sparse reward settings, baseline methods struggle to achieve any meaningful success.\nSkill Analysis We now analyze COMPASS's capability to synthesize and execute diverse tactical behaviors. COMPASS develops four key tactical patterns: (1) An exponentially-scaled focus fire implementation that coordinates multiple units' target selection based on allied attacker density (Figure 4), (2) A position-aware kiting mechanism"}, {"title": "5.3. Ablation Studies", "content": "The Effectiveness of Skill Initialization To evaluate the impact of our skill initialization, we analyze the performance of COMPASS using only the initialized skill library derived from expert demonstrations. The results in Table 4 demonstrate that skill initialization alone achieves non-trivial performance across different scenarios, particularly in symmetric matchups. Moreover, the gap between initialized skills and COMPASS underscores the necessity of incremental skill synthesis. A script example for skill initialization is in Appendix."}, {"title": "6. Conclusion", "content": "We present COMPASS, a novel framework for cooperative multi-agent systems that integrates vision-language models, a dynamic skill library, and structured communication. Through decentralized closed-loop planning, COMPASS enables agents to iteratively decompose tasks and adapt strategies via environmental feedback. Our skill library, initialized from expert demonstrations and refined through execution, provides interpretable code-based behaviors. Our hierarchical communication protocol enhances coordination under partial observability through entity-level information sharing. Evaluations on SMACv2 demonstrate COMPASS's effectiveness in several scenarios, particularly in Protoss, while highlighting areas for improvement in others like the Zerg setting. These results suggest that COMPASS provides a promising direction for developing interpretable and adaptable multi-agent systems suitable for real-world applications."}, {"title": "Impact Statement", "content": "We believe that the proposed work enhances the capacity for intelligent decision-making in complex and dynamic environments, and can have a positive impact on real-world multi-agent applications such as robotics, traffic management, and resource allocation. However, it is essential to consider potential concerns such as the discrepancy between the simulated environment and the real world. Another potential effect of directly implementing the derived policy is that it could lead to biased decision-making and privacy infringements. Mitigation strategies to address potential hazards could include the establishment of ethical guidelines and regulatory frameworks alongside the integration of transparency."}, {"title": "A. Prompts used in COMPASS", "content": null}, {"title": "Prompt for Perception", "content": "You are an AI assistant helping with academic research in the StarCraft II's SMAC (StarCraft Multi-Agent Challenge) environment, controlling a <unit_type> unit with ID <unit_id> in micromanagement scenarios <scenario_name> to help your team defeat the enemy forces. You operate under decentralized execution with partial observability, making decisions based only on local observations within your unit's field of view. Your advanced capabilities enable you to process and interpret gameplay screenshots and other relevant information.\nI will give you the following information:\n<few_shots>\nReasoning for the last episode:\n<last_episode_reasoning>\nStrategic situation analysis:\n<info_summary>\nBelow is the current in-game screenshot and its description:\n<image_introduction>\nMinimap information:\n<ego_minimap>\nCurrent task:\n<task_description>\nTactics recommendation:\n<web_search>\nBased on the above information, you should first analyze the current game situation by integrating the information from the in-game screenshot, its description, and other provided information.\nGame situation:\nYou should think step by step and provide detailed reasoning to determine the current state of the game. You need to answer the following questions step by step:\n1. What is your unit_id, unit type?\n2. What map borders are you near? Check which cardinal directions (N/S/E/W) have unavailable movement actions.\n3. What is the current health status of your unit? What is the current shield status of your unit?\n4. Are there any enemy units visible, either in observation or minimap?\n5. Are there any ally units visible, either observation or minimap?\n6. Are you positioned at the optimal attack range from enemies, or do you need to reposition based on the enemies' locations and directions?\nRegion of interest:\nWhat unit or location should be interacted with to complete the task based on the current screenshot and the current task? You should obey the following rules:\n1. If your chosen region of interest is a unit, format the output as \"[Enemy/Ally] #[target_id]\" (e.g., \"Enemy #0\" for enemy unit with ID 0, \"Ally #1\" for ally unit with ID 1)\n2. If your chosen region of interest is location, format the output as \"Location: [direction]\" where direction must be one of: \"North\", \"Northeast\", \"East\", \"Southeast\", \"South\", \"Southwest\", \"West\", \"Northwest\", \"Center\" (e.g., \"Location: Northeast\")\n3. If there are units visible, prioritize using unit as region of interest.\n4. If the target_id is required, you MUST only use enemy/ally's unit_ids that are currently visible in your shooting range.\n5. If your chosen region of interest is location, you MUST verify its availability.\n6. If shared minimap information reveals enemies outside your sight range, prioritize moving to those locations unless there are enemies within your current vision range.\n7. Your chosen region of interest should align with the current task description and ally's intentions.\n8. Your chosen region of interest should enable you to quickly engage in combat or efficiently achieve the task in cooperation with allies?\nReasoning of region of interest:\nWhy was this region of interest chosen?\nYou should only respond in the format described below with a line break after each section colon (##Section##:) and NOT output comments or other information:"}, {"title": "Prompt for Task Reasoning", "content": "You are an AI assistant helping with academic research in the StarCraft II's SMAC (StarCraft Multi-Agent Challenge) environment, controlling a <unit_type> unit with ID \u00a1unitidi, in micromanagement scenarios <scenario_name> to help your team defeat the enemy forces. You operate under decentralized execution with partial observability, making decisions based only on local observations within your unit's field of view. You will be sequentially given <event_count> screenshots and corresponding descriptions of recent events. You will also be given a summary of the history that happened before the last screenshot. By analyzing these inputs, you gain a comprehensive understanding of the current context and situation within the game. You should assist in summarizing the next immediate task to do in SMACv2. Your ultimate goal is to help your team defeat the enemy forces as quickly as possible.\nI will give you the following information:\nReasoning for the last episode:\n<last_episode_reasoning>\nCumulative reward for the executing skill:\n<cumulative_reward>\nCurrent task:\n<task_description>\nAlly's tasks:\n<ally_task>\nMinimap information:\n<ego_minimap>\nCurrent game situation:\n<game_situation>\nTactics recommendation:\n<web_search>\nThe following are successive screenshots:\n<image_introduction>\nSkill set in Python format to select the next skill:\n<skill_library>\nCurrent executing skill:\n<previous_action>\nImplementation of the skill:\n<action_code>\nReasoning for the skill:\n<previous_reasoning>\nSelf-reflection for the last executed skill:\n<previous_self_reflection_reasoning>\nTask_guidance:\nBased on the comprehensive game state analysis and team context, decompose the primary objective of \"defeat all enemy units\" into ONE specific tactical sub-task that enhances either target prioritization (score_target) or behavior control (control_logic). This sub-task should be concrete, implementable, and aligned with team coordination. Consider the following in your task decomposition:\n1. Final Objective: Defeat enemy forces while preserving allies\n2. Team Context:\nYour unit's current assigned task - Ally units' assigned tasks - Progress made on previous tasks 3. Tactical Layer:\nEnemy unit compositions and strategies - Team formation and positioning The task should follow one of these formats:\nFor target prioritization (score_target):\n\"Adjust [scoring weight/multiplier/threshold] to [specific combat calculation] based on [unit composition + battle state] where [precise condition]\"\nFor behavior control (control_logic):\n\"Implement [unit movement pattern/formation/targeting] when [combat state + ally positions] satisfy [precise conditions]\" Task Requirements:\nSpecificity: Must define exact behavior modification Measurability: Must have clear success criteria Actionability: Must be achievable using available atomic actions Coordination: Must support team tactical objectives Adaptability: Must respond to changing battle conditions If current task implementation remains unsuccessful, output 'null'.\nReasoning_of_task:\nWhy was this new task chosen, or why is there no need to propose a new task?\nSkill_guidance:\nBased on the current executing skill and the proposed next task, evaluate if there is alignment between them. Output True if the current skill effectively supports the task requirements, or False if a new skill is needed."}, {"title": "Prompt for Skill Generation", "content": "You are an AI assistant helping with academic research in the StarCraft II's SMAC (StarCraft Multi-Agent Challenge) environment", "effectiveness": "nReasoning for the last episode:\n<last_episode_reasoning>\nCumulative reward for the executing skill:\n<cumulative_reward>\nCurrent task:\n<task_description>\nAlly's tasks:\n<ally_task>\nMinimap information:\n<ego_minimap>\nCurrent game situation:\n<game_situation>\nSkill set in Python format to select the next skill:\n<skill_library>\nCurrent executing skill:\n<previous_action>\nImplementation of the skill:\n<action_code>\nReasoning for the skill:\n<previous_reasoning>\nSelf-reflection for the last executed skill:\n<previous_self_reflection_reasoning>\nCombat Analysis Task:\n1. Analyze the provided script's effectiveness\n2. Analyze the score_target(unit) function's effectiveness and weaknesses.\n3. Analyze the control_logic() function's effectiveness and weaknesses.\n4. Based on the current executing skill, the existing skills in skill library, and current task, evaluate if there is alignment between them.\n5. If a new skill is needed, design tactical improvements while maintaining code structure.\n6. If the current skill or there is any skill in skill"}]}