{"title": "Artificial Intelligence for Geometry-Based Feature Extraction, Analysis and Synthesis in Artistic Images: A Survey", "authors": ["Mridula Vijendran", "Jingjing Deng", "Shuang Chen", "Edmond S. L. Ho", "Hubert P. H. Shum"], "abstract": "Artificial Intelligence significantly enhances the visual art industry by analyzing, identifying and generating digitized artistic images. This review highlights the substantial benefits of integrating geometric data into AI models, addressing challenges such as high inter-class variations, domain gaps, and the separation of style from content by incorporating geometric information. Models not only improve AI-generated graphics synthesis quality, but also effectively distinguish between style and content, utilizing inherent model biases and shared data traits. We explore methods like geometric data extraction from artistic images, the impact on human perception, and its use in discriminative tasks. The review also discusses the potential for improving data quality through innovative annotation techniques and the use of geometric data to enhance model adaptability and output refinement. Overall, incorporating geometric guidance boosts model performance in classification and synthesis tasks, providing crucial insights for future AI applications in the visual arts domain.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) techniques find use in the art industry for tasks such as 3D scan analysis, art recommendation systems, identification of art design principles, deconstructivism art generation with fragment models [1, 2]. These techniques mainly involve three key processes: extraction, analysis, and synthesis. Extraction methods classify paintings based on style, identify and authenticate artwork, and provide exhibit and tour information to establishments like museums [3] and historic cathedrals [4] to enhance their visitors' experience. Analysis techniques interpret and facilitate searching and comparing art elements such as geometric patterns of compositional elements across multiple scales in art collections [5]. Synthesis methods are used for scanning and enhancing the details of artifacts in the Cultural Heritage field [6, 7] and deal with the preservation, documentation and collection of historical and cultural objects. They classify paintings based on style, identify and authenticate artwork, and provide virtual access to historic cathedrals to enhance their consumers' experience. It is used for scanning and enhancing the details of artifacts in the Cultural Heritage field [6-8] which deals with the preservation, documentation and collection of historical and cultural objects.\nThis paper explores a broad spectrum of AI techniques applied to artworks, encompassing both deep learning and non-deep learning methods. While deep learning models, such as CNNs and GANs, have shown significant success in tasks like object detection and image synthesis, we also discuss traditional methods, including Deformable Part Models (DPM), Histogram of Oriented Gradients (HoG), and Thin Plate Spline (TPS) interpolation. These non-deep learning techniques are crucial for understanding specific geometric features and enhancing overall model performance when integrated with deep learning approaches.\nLearning from Art datasets using artificial intelligence models is challenging due to the generally smaller dataset size and larger inter-class variations [9, 10], as well as incomplete or inaccurate data annotations [11, 12]. This review paper broadly covers 3D art such as sculptures, archaeological sites and surface art such as walls, cloth or tattoos. Additionally, it also considers 2D forms such as paintings, sketches, digital art, cartoons, logos, anime and manga. Synthetic art through the stylization of real-world data without content separation commonly suffers from the bleeding of colors from the foreground to the background and the blurring of boundaries. Despite depicting the same content, the stylistic differences between various art media highlight the impor-tance of separating style and content in model design for different art-related tasks.\nTo illustrate these problems, we use t-distributed Stochastic Neighbor Embedding (T-SNE), which is a commonly used nonlinear dimensionality reduction algorithm to visualize embeddings and understand similar images from the dataset according to the model, VGG-19, embedding the data. From the T-SNE visualization in Figure 1, we see that art domains of one type cluster closely, with some overlap for those sharing similar shape representations with cartoons and sketches. The large inter-class varia-tions in the art modalities lead to their distributions looking roughly like anisotropic Gaussians, with their spread creating overlaps with images of similar subjects between the other classes. The stylization of real-world data without content separation leads to the bleeding of colors from the foreground to the background and the blurring of"}, {"title": "1.1 Background", "content": "Computational art is a field that finds applications in the visual art industry for mod-eling collections of art [13], simulating visual art for artwork exploration [14], and replicating artworks for preservation [15]. As the field grows, it is essential to recognize that the distinction between human-created and AI-generated artwork remains subjec-tive [1, 16, 17]. The papers discussed in the literature use datasets from the art domain for training, thus modeling the domain, even if the art dataset is a subtask or a subset of the training dataset at least. For clarity, we use the term 'AI-generated graphics' to refer to AI-generated works, acknowledging the ongoing debate over whether AI art qualifies as visual artwork. Additionally, when using the terms 'art', 'visual art' and 'artwork' we refer to the input data collection. The survey paper discusses research where AI image synthesis and generation models result in different interpretations of artworks from existing styles, diversifying visual arts datasets [18] or producing artis-tic counterparts to real-world images in the introduction and synthesis sections. A\u0399 tools assist in the extraction and analysis of growing collections of art to enhance interactive experiences while expanding interpretation across selected geometry priors in collections [3, 19].\nIt evolved from a form of static procedural art, building from preset rules with random perturbations through models such as the AARON model in the 1960s. Its later iterations were more data-centric, allowing for generation of more dynamic art using computer graphics through non-photorealistic rendering techniques that distilled artistic styles to simple parameters such as brush strokes and other learned statis-tics forming popular techniques such as style transfer [16]. These artworks were used to form interactive art pieces where both the artist and the audience influence the displayed design. Computational art evolved to include dynamic content generation with the evolution of image generative models, data-driven approaches learn how to synthesize AI-generated graphics through GAN-based models in the form of CAN, pix2pix, Cycle-GAN and GANVAS or learnable style transfer from DeepArt [20], digi-tal art proliferated depictions of various media in different existing styles. Other styles involved repurposing existing computer vision models for hallucinating emergent styles from natural images such as DeepDream. With a demand for controllable AI-generated graphics with the insertion and deletion of objects of varying poses and views, the AI art community developed text-to-image models such as DALL-E and Muse. Current iterations of image generative models aim towards high-quality AI-generated graphics using diffusion models such as GLIDE or Stable Diffusion. They currently suffer from mitigating data biases from the confounding of the style and color choices of particular art movements [21] that result in hallucinating structures."}, {"title": "1.2 Geometry in the Visual Arts Industry", "content": "Identifying and analysis of artworks often use geometries ranging from global cues such as composition, perspective and proportions to identify stylistic characteristics common to an artist, to local cues such as stroke patterns, directions and shapes.\nGeometry is widely used in the art industry to represent perspective and lighting, as well as to reconstruct 3D shapes and locations of objects from 2D pictures. Models that incorporate proxy geometry onto artworks [20, 26] find applications in animations and VR-/AR-based museum tourism. The learned proxy geometry are geometric fea-tures or model embeddings that learn style invariances or shape and geometric data information. The 3D proxy or 3D geometric features is an intermediate representation upon which these creative applications perform operations such as relighting and novel views from their 2D projections or 2D geometric features. The use of the generation of 3D generated models extends to content recovery for art conservation projects [5] in image searching for art historians and experts. Depending on the art style the object geometry is similar or exaggerated compared to their real-world counterparts [27]. The geometry data, such as pose [28], keypoints [29] or bounding-box [30], can then be used as labels to retrieve and match images with objects that range from highly struc-tured to highly varied geometries. They also take the form of extra input maps along with the photographs of artworks like murals or paintings on surfaces like pots, walls and robes [6] provide extra information when projected together to form 3D models.\nA 3D proxy is an intermedi-ate representation upon which these creative applications perform operations such as relighting and novel views from their 2D projections. The use of the generation of 3D models extends to content recovery for art conservation projects [5] in image search-ing for art historians and experts. Depending on the art style the object geometry is similar or exaggerated compared to their real-world counterparts [27]. The geometry features, such as pose [28], keypoints [29] or bounding-box [30], can then be used as"}, {"title": "1.3 Paper Organization", "content": "To understand how geometry contributes to artwork tasks, we discuss the artificial intelligence techniques facilitating the use of geometry in extracting, analyzing and synthesizing artworks. We believe that discussing extraction, analysis, and synthe-sis is pertinent as these are the primary applications of AI tools in the art industry, utilized by experts, critics, and visitors in art collections. These processes are inter-connected through the common thread of geometric considerations in various learned representations or additional constraints, which play a crucial role in enhancing the understanding and appreciation of art. We aim to emphasize the evolving nature of art and how AI tools are increasingly being integrated into creative processes and their interpretations. The focus on geometry in this discussion aims to improve the quality of the generated media by providing form guidance amidst the fluidity and variety of styles while allowing more control for artistic expression. Additionally, it addresses common failings in these AI tools [32] with regards to generated images, visual composition and geometric deformations.\nWe first discuss the extraction of geometric labels for humans and objects from 2D images to 3D models in Section 2. The object labels are divided into bounding boxes, key points and segmentation masks whereas people range from pose skeletons, landmarks and gestures. The 3D features range from explicit surfaces to implicit sur-faces, and parametric models. Then, we explain the analysis of the effectiveness of the extracted geometric data on discriminative tasks in Section 3. The feature extrac-tion section discusses the extraction of entities or subsets of visual art collections where geometric information is used directly as constraints or selection criteria to improve discrimination or indirectly by augmenting the collection to improve model classification. Next, we detail the synthesis and manipulation of artwork for novel"}, {"title": "1.4 Related Surveys", "content": "Using Geometric information in artificial intelligence models facilitates the learning of representations that encode the inherent structure of visual elements. This paper cov-ers 2D and 3D artistic visual media while discussing the extractable geometric features following it with the discriminative and generative tasks they can be incorporated into. The closest work related to ours considers geometric features at the local and image level through feature descriptors or hardware (e.g. 3D printers and scanners) for 3D models in cultural heritage [8]. Unlike our work, they focus on preservation, reg-istration, reconstruction and enhancement, and do not consider deep learning based techniques. In this section, we first cover more recent surveys in the field before covering the works that incorporate AI and geometry in visual art.\nAI-based methods have found use in creative applications such as content gen-eration in multimedia, captioning, spoofing and AR/VR [20]. These involve models deployed for production in games such as GameGAN, storyline generation with MADE or Vid2Vid, and artwork generation from Hypercube-based NEAT that uti-lizes geometric regularities. A survey on computer vision in art history highlighted its applications in image search and retrieval [43] for art historians. They identify the importance of recognizing contexts such as clothing, architecture, materials, faces, patterns on objects, and artist signatures for recognizing time periods, geography and culture. Other papers focus on deep learning approaches in paintings [44] and digital art collections [45] for content recognition such as classification, retrieval and detection in images or multimodal domains. They also cover a subset of art synthe-sis using image generative models with losses that exaggerate styles or through latent space guidance with other models such as Contrastive Language-Image Pre-Training (CLIP). We cover the discriminative and generative tasks only if they consider geo-metric information in the form of annotations or pseudo geometry in the form of intermediate representations, model functions or dataset transformations to induce model invariance.\nA study on mixed 2D and 3D non-photorealistic media covers the task of art conservation using multiple input spectrums [6]. They only use machine learning or statistical information for detection when dealing with the visual spectrum, with the majority of their study covering technologies for diagnosis and imaging. A similar review covers these multispectral inputs for paintings only, which extracts 3D geo-metric information through correspondence matching with feature descriptors such as SIFT [46]. However, we explore not only art restoration but also the repurposing"}, {"title": "2 Geometric Features Extraction", "content": "Object detection uses geometric features that act as descriptors learned from special classifier architectures to learn external geometry data [52] from labeled boxes or pixels that enclose the object using bounding boxes or semantic maps. These descriptors can be specialized [85] for human detection by adding more structure and robustness to affine transformations via pose skeletons at different hierarchical levels. Common geometric data capture regions of interest for both humans and objects at the semantic and instance level. The latter has not been explored in the existing works in favor of the problems of domain adaptation and limited data. Some data augmentation techniques and extra input induce style invariance and other context invariances such as time periods respectively to force models to process the input data as geometric feature embeddings that capture structure information, thereby forming geometric techniques.\nPaintings are challenging to computer vision models due to their background clut-ter and object composition [86]. With their high diversity in poses and shapes as compared to their real-world counterparts, they can even lead to spurious detections, mistaking people for other mammals [75] or from occlusions. Furthermore, some paint-ing datasets do not have reliable annotations with some subjects missing. To relax the problem, existing works account for the deviation of the geometry and depictions from the real world to the artistic image domain with modifications to their real-world detector training and inference pipelines."}, {"title": "2.1 Object-Centric Features", "content": "Object-based geometrical feature annotations involve varying amounts of information contained in excess for bounding boxes, exactly in segmentation masks and minimally in keypoints. Alternatively, data pre-processing induces the underlying class of trans-formations during model training or removes non-geometric information. These labels are summarized and visualized in"}, {"title": "2.1.1 Bounding Boxes", "content": "Bounding boxes provide the target objects' position and scale in paintings with rect-angles by manual or automatic annotation using object detection techniques. Object detection is divided into single-stage or two-stage models for speed-accuracy trade-off, and scores the overlap between their proposed regions of interests and the bound-ing boxes [87]. In the painting domain, these models account for the gap between the artistic depiction of the object and its real-world counterpart by transforming the input data [55] or modifying its stages [11, 30, 56, 66] depending on the architecture choice. Modifications to one-stage models proceed in an end-to-end fashion to clas-sify detected objects while multi-stage detectors optimize their constituent stages to produce better candidate regions [67]."}, {"title": "2.1.2 Patch-Based Region Selection", "content": "In the absence of labelled bounding boxes, patch-based selection methods approximate object locations with feature engineering and simple classifiers. They enforce faithful geometry through spatial correspondences [67] embedded in model features with a further selection stage that accounts for a particular class of transformations. These matches at the feature level narrow down the area of interest from the image to a patch level to return the target object. Object retrieval for paintings typically uses a"}, {"title": "2.1.3 Geometric Transformations for Content Selection", "content": "Simple geometric transformations like affine transformations and cropping [95] also help models train on small datasets while learning invariance properties from the data. Data augmentation techniques provide the additional benefit by mitigating the need for extensive data labeling by generating additional training examples that capture variations in object appearance and their contexts. There are techniques for generating new data from the image level to preserve the training distribution. LogoMix [51] synthesizes samples from overlapping different logo patches, effectively preventing the model from overfitting to synthetic data and ensuring it learns robust features from real-world data.\nStyle transfer creates a hybrid image that preserves the structure of the content image of the real-world while its style takes after the painting images. Other data aug-mentation techniques often drastically transform the training distribution to account for the gap in diversity between domains [96]. They can have fidelity towards different aspects of the input pair depending on if they were trained on multiple styles, a single style, or through iterative optimization. The synthetic data aggressively transforms the training dataset, provided the input data is not independent of texture such as edge maps while needing only a limited set of style images."}, {"title": "2.2 Human-Centric Features", "content": "Human detection on the other hand involves highly regular structures at the face, body and hand level captured by bounding boxes, pose skeletons and landmarks. Pose and body shape information provides information for perception and identities [35] through the proportion of their parts. Human poses can vary in depiction across time periods and represented with different topology or motifs [63] where the referential deviations can represent artistic signatures or movements. They can also increase the difficulty in detecting poses by blurring contours, distorting proportions or occluding joints through apparel or other objects or lighting, sometimes even changing the car-dinality of the parts to indicate mythological creatures. Face detection in uncontrolled settings like modern artistic styles [61] is very challenging due to occlusion as well as variations in shape, color, texture and face size. In end-to-end training, the ratio of the preservation of the geometric features or the pose regression loss to the style transfer loss is vital, unlike training them as separate parts for achieving high pose accuracy [34]. There is still a cross-domain generalization problem since the joint positioning is more accurate with real-world images compared to artistic images, but more data from stylization gives better performance after a cutoff. Paintings retrieval can benefit from pose annotations at different levels of abstractions [83] to compensate for mis-labeling. Inverted label propagation produces these levels of poses through producing annotations induced from the source to the target image provided that the dataset size is sufficiently large."}, {"title": "2.2.1 Hand Gestures", "content": "The detection of hand poses involves the position and orientation of the hands and their fingers with respect to the body in the form of templates or pose skeletons. In portraits and paintings, they commonly form hand signs and iconographic meaning with irregular finger positions or unnatural gestures with hand actions [97]. They indicated a group's, family or religious memberships and ranks, personality traits"}, {"title": "2.2.2 Facial Landmarks", "content": "Facial landmarks in paintings have more variations compared to their real-world coun-terparts leading to its architectures disentangling the style and pose into separate multistage models or a need for data augmentation.\nMultistage models such as the Cascaded Pose Transform network (CPTNetV2) can simulate head and face pose animation [80] to model pose displacements while inpaint-ing the facial features separately to disentangle the problem into the two separate poses transformations. These models need a refinement stage to add details while maintain-ing consistency. Then, a fusion generator utilizes both the pose information that was disentangled by imposing masks to guide their individual generations. Other two-stage models can detect modalities like bounding boxes and keypoints for the human figure [81] from the photograph domain to that of paintings using a semi-supervised learning method with transformers through a teacher-student model distillation. It predicts a fixed set of proposals for each image, removing the need to account for overlapping boxes and imbalance between the foreground and background. Distilling geometric information for domain adaptation provides better results than fine-tuning or style transfer with additional label conditioning.\nArtistic augmentation [62] for landmark detection requires image transformation through style transfer techniques followed by a part-based feature correction step for landmark warping to account for structural shifts and decorrelating parts. Techniques like part-based correction and tuning and Geometric style transfer account for extreme styles and higher variation in landmark points. The stylized portraits' landmarks are warped to the mean facial shape vector of the target style to capture a signature"}, {"title": "2.2.3 Body Skeleton", "content": "Poses between people in paintings and photographs [32] can be effectively aligned by pose detections and matching them through geometric transformations. The latter validates and measures similarity under different scales and positioning while making the detection robust to noise and missing parts. The method is not robust to unknown poses, occlusions, ambiguous poses, or any spurious connections that arise from these challenges. Style transfer can bridge the gap between photographs and paintings for both person and pose detection in curvilinear surfaces like vases and create a dataset to fine-tune the HRNet [99] model for the tasks. With a perceptual loss on both tasks, the model can adapt the annotations to the pose and detection losses with the stylized data.\nTo compensate for limited painting data, the pose estimators can also be pre-trained from 3D renderings [33] of artistic media like anime or manga which provides joint positions from the underlying rigging. These models can simply be fine-tuned on a smaller dataset of drawings to effectively ignore the problem of domain gap from models pre-trained on photographs. The pose information can be utilized in other tasks such as image retrieval. Pose similarity followed by clustering helps retrieve similar paintings [36] through methods like K-medians with metrics which are invariant to scaling, rotations and translations after detecting them through pre-trained models like OpenPose."}, {"title": "2.3 Segmentation Masks", "content": "Image segmentation partitions the image into pixels that group into multiple classes which can be further grouped into individual objects that belong to the same class in the case of instance segmentation. This requires fine localization of objects in the scene regardless of scale, occlusions from clutter or other objects, or appearance changes from lighting or environmental conditions. Earlier works use deformable models, which provide an object shape template representing a distribution of warped objects, and graph cut to partition an image into regions while providing boundary separation [100]. By rephrasing the image segmentation as a deformable model optimization problem, they represent the Chinese paintings by their unique color choice in neighborhoods and the direction of texture. The deformation model splits the image into connected regions, while the texture directions represent flexible sparse foreground/background features like edge convolution filters to detect orientations."}, {"title": "2.4 3D Features", "content": "The feature representation of 3D models can take generic forms such as implicit neural structures and explicit geometric structures or specific versions from parametric con-strained deformable models. Implicit neural networks represent 3D shapes and surfaces in a pointwise manner with their learned function, while Explicit models represent the objects as a 3D point collection and differ in usage by the efficiency and ease of use of their data structure for different tasks. Parametric models, on the other hand, use a small number of shape and pose parameters to efficiently represent objects in a class like Skinned Multi-Person Linear (SMPL) for human bodies with strong con-straints to prevent large structural deviations. These representations are summarized with visualizations in Table 3."}, {"title": "2.4.1 Implicit Models", "content": "Implicit geometry structures can separate the natural scene geometry from artistic stylization by utilizing a two-stage model with a Neural Radiance Field (NeRF) and a 2D stylization decoder which gets the projected view to style [106]. The desired style can be customized by conditioning the latent code that is the input to the decoder which also serves to deblur the rendered scene from the NeRF depth output. The model requires multiple stages [107] for view projections and style transfer to mix their outputs to get a stylized scene. When transferring style onto a mesh, unseen style inputs result in blurry reconstructions with na\u00efve methods transferring only the overall color tone of the style image. By rendering the geometry and stylistic aspects separately, the ARF paper [108] showcases the transfer of the subtle textural details of the watercolor feather image onto their Family statue scene example.\nTo optimize the transfer process [108] for better representations in view extrac-tion, the image level style transfer uses a deferred back-propagation at a patch level to accumulate over all the patches at the neural field. To reduce computational com-plexity, the final model can only consider the components in the visible field of view [109] but still requires all direction illumination and material information. Alterna-tively, better representations can be learned using an image generative model like a Style-GAN [110] and fed to the NeRF module, facilitating detailed feature extraction while conditioning the generator on geometric priors such as pose. For specific types of extractions such as human sculptures, implicit models like Pixel-Aligned Implicit"}, {"title": "2.4.2 Mesh Structures", "content": "Direct 3D representative models such as voxel grids [102] ignore object artifacts found in the neural field representations in two-stage scene stylization. These are limited by the resolution of the extracted 3D model and the computational size of the interme-diate features. Perspective changes warp the projected image from moving the lines of convergence constraints [103], thereby changing the vanishing point. They achieve this view emphasis by warping the quad mesh and the corresponding homographic matrices while constraining the projection geometry.\nIt is possible to extract more faithful 3D models using a dictionary of surface gradients and exploiting the symmetry of such mesh structures in other views. The utilization of such self-similarity with inpainting in point clouds [104] finds applications in reconstructing damaged and structurally deformed architecture and sculptures. Stylistic renderings of multiple views for stereo paintings can use 3D paint strokes on top of partial grid mesh structures as a two-stage model [105]."}, {"title": "2.4.3 Parametric Models", "content": "Artistic 3D models such as sculptures closely resemble the human figure, but are limited in dataset size or consist of larger variations in pose or structure to provide emphasis to foreground or background characters, sometimes exaggerating the shape from certain viewpoints. The SMPL model provides a minimal, resilient pose and shape representation through low-dimensional vectors. The customizability of intrin-sic parameters accounts for the anatomical differences in artistic statues, making it handy for reconstructing statues like the Wounded Amazon with a different sized arm or for those with missing limbs such as the Esquiline Venus [116]. By adding a Signed Distance Field (SDF) to check the occupancy of particle effect selected by the artist, the body can be textured and locally manipulated by keeping track of the normals and tangents on the SMPL's deformable mesh [114]. Instead of manual deformation and texturing, the SMPL model can be simply extended with a CLIP loss to make the mesh representation similar to that of text control with a differentiable renderer [115]. Integrating these joint interactions and their confidence coefficients with transformers increases the accuracy of human reconstruction and the speed of the mesh extraction [112]. The model works well when the character images are clear, but failing in abstract paintings in Picasso's works where parts of the head are missing or incomplete. When the body parts are occluded, it leverages joint relations to recover the skeleton topol-ogy and pose but encounters significant errors, particularly with parts like feet that have fewer adjacent joints. They use a High-Resolution Net (HRNet) to extract these human features. Representative keypoints from all of its output keypoints (e.g. nose representing the face) are then selected and the model fuses the joint and mesh infor-mation using a graph transformer model. In 3D scene extraction, template skeletons can be conditioned with bas-relief geometry, contours and silhouette information, for particular styles of sculpting to estimate 3D skeletal poses from 2D poses [113]. The"}, {"title": "2.5 Effectiveness of Geometry-based Methods in Extraction", "content": "Object-centric tasks benefit from input region proposal selection strategies and addi-tional geometric labels to add context to the input or act as pseudo-ground labels. Region selection by voting helps find and localize small motifs, achieving a maximum retrieval performance of 91.3 mAP for the LTLL dataset where other models suffer from selecting regions with low correspondences [30]. Mixing of regions selected with-out overlap and missing content helps in data augmentation for small datasets for logo detection for an improvement of 7.05% mAP [51]. Encoding the context instead of missing regions from cropping helps improve fine-tuning object detectors by 3.5% mAP for unseen categories with an additional 2.5% for seen categories [49]. Leverag-ing this prior knowledge of pre-trained models and semantically aligning them with a newer domain helps improve performance even with difficult subdomains such as abstract paintings [71]. Geometrically enhanced annotations also enhance the quality of the training dataset with maps enhancing salient regions [73] while eliminating irrel-evant areas in crack detection or time-specific label predictions for object detection in paintings [72].\nHuman-centric labels such as facial landmarks and hand or body pose differ in depictions in the paintings domain, resulting in poor results with simply fine-tuning the model on the task. Without accounting for style, body pose estimators reach less than 60% AP [75] while face detection algorithms reach less than 35% F1 score on modern face datasets [61]. When stylizing images without modifying the corresponding pose, models can get an improvement of 6% on the mAP even without labeled data [28]. Style-tuned models that pre-train on stylized content and poses gain an improvement of 36.7 mAP for the specific task of pose estimation and 34.5 mAP for the more generic person detection task. In image retrieval, performing geometric verification after fast annotation matching retrieves a longer sequence of visually similar links as compared to other models that simply match feature embeddings.\nParametric model-based extraction methods utilize prior knowledge to account for lower-quality data, modeling complex environments with missing information, and"}, {"title": "3 Discriminative Geometric Features Analysis", "content": "Various painting analysis tasks utilize geometric features from low level (i.e., local feature descriptors such as brushstrokes or optical flow maps encoding direction ) and intermediate level (i.e., cross spatial correspondences between objects to identify keypoints and landmarks). These analysis methods even utilize outputs from feature extraction methods such as a list of bounding boxes. These geometric features and data are utilized in the tasks that are sub-categorized into scene classification, retrieval and style classification.\nScene classification relies on similarity measures to determine object arrange-ment and assign scene labels. It involves three core stages: feature extraction (e.g ResNet without fully connected layers), spatial correspondence encoding (e.g Atten-tion, K-means clustering), and output mapping (e.g. Aggregation of bounding boxes, multiclass classifiers with Softmax activations). In contrast, style classification focuses on identifying unique styles in artistic images, which can vary in visual cues rather than content. Notably, stylistic manipulations in images can impact feature extraction and geometry due to object detectors, posing challenges on both scene classification and retrieval. Scene retrieval aims to find images resembling a reference by mapping output features to identify closely matching candidate images of a specific scene class."}, {"title": "3.1 Object Detection", "content": "Objects in paintings have large shape exaggerations in modalities such as cartoons, vary drastically in their composition with cluttered scenes consisting of objects of different scales and spatial arrangements. The spatial layouts are crucial in scene understanding [11, 49], with objects representing visual motifs for artists or indicating time periods and culture by their co-occurrence with other objects. These artistic datasets are small in size [50], with some providing only image-level annotations [118, 119] or missing object-level labels [11, 49].\nTraditionally, object detectors in landscape scenes focused on analyzing and under-standing low-level features, such as brushstrokes, for capturing scene dynamics using optical flow although the result can be suboptimal when noisy object regions are not effectively detected as principal components. By integrating these low-level details with region-based segmentation algorithms like Comaniciu's mean shift clustering, which groups input scenes by color, the system provided a deeper understanding of"}, {"title": "3.2 Style Classification", "content": "Style classification involves artist identification and the common visual elements, tech-niques and forms used in their works. The artists attribute the forms to lower-level textures, such as their choice of color palette, brushstroke, or materials, up to the higher-level choice of fine-art painting compositions. Style classifiers benefit from fea-ture fusion techniques that merge geometric image representations with deep learning features as input to a multiclass classifier. These geometric representations are hand-crafted for the problem to account for the large inter-class variation in styles and class imbalances stemming from artist-based classification, which result from variations in the artists' prolificity. In older works, CNNs, were used purely as object feature extrac-tors which is less effective in capturing image representations compared to learned ensembles of handcrafted features like Classemes or PiCoDes [54]. However, their per-formance improved significantly when the object region was first extracted and used as input to the model. This approach highlights the importance of isolating relevant regions for analysis, enabling CNNs to better understand and represent the essen-tial characteristics of the image, thus offering a more accurate interpretation. When a DPM provides class-specific regions to a multiscale CNN to provide a holistic encod-ing and learn a distribution of local encodings through a GMM, their joint embedding after aggregation through techniques like Fisher vector gives better performance [58].\nMore recent work has enhanced CNNs' ability to analyze and understand image repre-sentations by incorporating discriminative signals from an SVM-based classifier. This approach refines the clustering criteria, allowing the model to generate centroids that more closely align with the original target label distribution [57]. This combination of deep learning and SVM-based analysis facilitates a deeper understanding of the underlying data structure, ensuring that the representations captured by CNNs better reflect the true characteristics of the target labels."}, {"title": "3.3 Scene Classification", "content": "Scene classification involves the general subject matter or the semantics in the paint-ing and considers categories like outdoor and indoor-based scenes, landscapes and portraits, seascapes and landscapes, still life or other labels describing the scene type. Simple methods like segmentation (eg. Normal Cuts) can extract visual descriptors like HOG or GIST from regions at the image level [121] for representing the structure and texture within each segmented region. These object descriptors are used as the input to a Bayes classifier to form the Rol pooling operation in a multistage object detection model. Due to the simplicity, the classifier focuses on colors and results in mistaking images from nudes and portraits since they both contain skin colors or they can not distinguish between cityscapes and landscapes due to the latter being the superset. Later models use CNNs to represent generalizable feature representations"}, {"title": "3.4 Human Perception Analysis", "content": "Objects in paintings can appear distorted despite being portrayed with the correct geometry based on the viewer's vantage point from large visual angles that tilt and straighten, reduced saliency of peripheral objects, to depth-wise elongations [122].\nExperiments involving participants to move towards the painting until they saw the object of interest take the desired shape or subtend an angle showcased similar results to that of projective geometric analysis, but to varying extents. When measuring their response, the farther vantage points had varying perceptions of distance in periph-eral objects for large paintings or those approximating 3D scenes. A case study of Piranesi's painting composition hints at possible approaches to balance the trade-off between accurate scene geometry against perceptive distortion [123] with the pieces utilizing projections from multiple viewpoints along the central vantage line. While this reconstruction shows inconsistency using geometric restitution as a tool for anal-ysis, it does provide a view to different proportions, sizes and relative distances of non-distorted objects from different viewing angles."}, {"title": "3.5 Effectiveness of Geometry-based Methods in Analysis", "content": "In the task of cross-domain object detection, additional representations such as CAM [11", "49": "can compensate for a lack of ground truth (GT) label while bringing in contextual information as learned by pre-trained models on a larger, well-annotated dataset. CAM acts as a pseudo-GT beating the S"}]}