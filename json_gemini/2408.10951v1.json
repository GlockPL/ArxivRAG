{"title": "Wave-Mask/Mix: Exploring Wavelet-Based Augmentations for Time Series Forecasting", "authors": ["Dona Arabi", "Jafar Bakhshaliyev", "Ayse Coskuner", "Kiran Madhusudhanan", "Kami Serdar Uckardes"], "abstract": "Data augmentation is important for improving machine learning model performance when faced with limited real-world data. In time series forecasting (TSF), where accurate predictions are crucial in fields like finance, healthcare, and manufacturing, traditional augmentation methods for classification tasks are insufficient to maintain temporal coherence. This research introduces two augmentation approaches using the discrete wavelet transform (DWT) to adjust frequency elements while preserving temporal dependencies in time series data. Our methods, Wavelet Masking (WaveMask) and Wavelet Mixing (WaveMix), are evaluated against established baselines across various forecasting horizons. To the best of our knowledge, this is the first study to conduct extensive experiments on multivariate time series using Discrete Wavelet Transform as an augmentation technique [15]. Experimental results demonstrate that our techniques achieve competitive results with previous methods. We also explore cold-start forecasting using downsampled training datasets, comparing outcomes to baseline methods.", "sections": [{"title": "Introduction", "content": "Synthetic data generation is crucial when real-world data is scarce or insufficient. Data augmentation creates additional training samples by applying different transformations to existing data. This expands the dataset to include a wider variety of data patterns, strengthening model resilience and improving its ability to generalize to new data [13].\nTime series forecasting is crucial in diverse fields such as finance, healthcare, meteorology, and manufacturing, and data augmentation techniques have become essential for tasks such as classification, forecasting, anomaly detection, and clustering [2]. Data augmentation (DA) is increasingly essential in the realm of time series forecasting due to its ability to enhance model performance and improve generalization. Time series data, unlike image data, presents unique characteristics in both the time and frequency domains, making its analysis more intricate. While current augmentation methods are tailored for classification purposes, it is crucial in the context of time series forecasting to emphasize diversity and consistency with the original temporal patterns. In Time Series Forecasting (TSF), data points within the look-back window and forecasting horizon serve as both the data and the label, enabling the modeling of complex temporal relationships by accurately capturing data within the specified time frame [2,20].\nThe existing fundamental augmentation techniques encompass scaling, flipping [14], window cropping or slicing [7], Gaussian noise injection [16], dynamic time warping [15], and other methods. However, these methods have a significant limitation as they may introduce missing values or alter periodic patterns within the series, rendering them unsuitable for TSF research. Consequently, the precision required for augmented data-label pairs in TSF exceeds that needed for other time series analysis tasks [2]. STAug, a decomposition-based augmentation approach [20], is effective for TSF but is inefficient and memory-intensive, limiting its application to large datasets. One of the baseline methods is the frequency-domain augmentation technique, FrAug [2], which motivated our study. Due to its utilization of Fourier transform to decompose the signal, a key drawback is its exclusive focus on frequency components at the expense of time resolution [2]. Our methods, employing wavelet decomposition, exhibit potential for superior results compared to alternative time-frequency analysis techniques. To grasp the limitations of each approach, we consider a signal with unknown frequency components. The Fourier transform provides detailed information about frequency components over time, showcasing high resolution in the frequency domain while overlooking variations in the time domain. Short-time Fourier Transform (STFT) addresses this limitation by analyzing localized frequency variations in the time domain. Nonetheless, STFT's fixed resolution poses a challenge, as its window boundaries remain constant [9,12]. Overcoming this limitation, the wavelet transform offers variable window sizes, providing high frequency domain resolution and low time domain resolution for small frequency values and large frequency values respectively [1,9,8]. Given this information, adjusting the signal's frequency components at different resolutions is expected to yield favorable outcomes.\nIn this study, we introduce two innovative yet direct and simple data augmentation methods, termed as wavelet masking (WaveMask) and wavelet mixing (WaveMix). These techniques utilize the discrete wavelet transform (DWT) to obtain wavelet coefficients (both approximation and detail coefficients) by breaking down the signal and adjusting these coefficients, in line with modifying frequency components across different time scales. The DWT accomplishes this decomposition by employing a sequence of filtering and downsampling procedures, resulting in a structured representation of the signal at different levels of resolution."}, {"title": "Related Work", "content": "Basic Augmentation Methods: As mentioned earlier, basic augmentation operations are scaling, flipping [14], window cropping or slicing [7], Gaussian noise injection [16], dynamic time warping [15], and others. Scaling operations alter the magnitude of data within a defined range by using a randomly generated scalar multiplier, while flipping reverses the sequence of data points along the time axis [14]. Window cropping is a method that selects random sections of a specific length from time series data, and each section is classified using a majority voting approach during testing. Gaussian noise injection is a technique that involves adding small amounts of noise or outliers to time series data while keeping the original labels intact [15]. These data augmentation techniques are mostly suitable for time series classification or anomaly detection as they do not maintain temporal coherence, which is crucial for time series forecasting. When these methods are applied for forecasting, they yield inferior results [2].\nDecomposition-based Augmentation Methods: Some decomposition-based approaches break down time series data into constituent elements such as trend, seasonality, and residual components through techniques such as RobustSTL or STL. Subsequently, these elements are transformed and merged to generate new time series data by modifying weights and implementing statistical models [15]. Some methods that rely on decomposition use Empirical mode decomposition (EMD), and one particular method is known as STAug method. The EMD algorithm breaks down a signal into multiple components called Intrinsic Mode Functions (IMFs). The initial IMFs capture high-frequency components, while the final IMFs represent low-frequency trend information. STAug utilizes two distinct instances, applies EMD to break them into components, and then reconstructs these subcomponents using randomly sampled weights from a uniform distribution. It then applies linear interpolation of these two weights to obtain augmented data for time series forecasting [20]. We have conducted experiments using the STAug method, which serves as one of our baseline approaches.\nFrequency-Domain Augmentation Methods: Some studies investigate data augmentation techniques that work on only the frequency domain; one of the most recent works was conducted by Gao et al. [6]. They use the Fourier transform to manipulate frequency components, such as adding perturbations or eliminating a few of them [15]. One of the recent works is also FrAug methods, which consist of two methods: frequency masking (FreqMask) and frequency mixing (FreqMix) [2]. These techniques form the fundamental basis of our paper and serve as our baselines. FreqMask employs the Fast Fourier transform (FFT) to compute the frequency domain representations of input data. Subsequently, it generates augmented data by randomly masking specific frequency components and applies the inverse real FFT for signal reconstruction. The procedure for FreqMix is similar, except that it randomly substitutes the frequency components in one training instance with the frequency components of another training instance [2]."}, {"title": "Problem Formulation", "content": "A multivariate time series sequence of length T and channels K is denoted as X = (x_1,...,x_T) \u2208 R^{T\u00d7K}. As it is a Time Series Forecasting Problem, we only observe data up to timestamp t < T. Based on the forecasting horizon, we aim to forecast future values starting from t + 1,..., T. Given input X = (x_1,...,x_t) \u2208 R^{t\u00d7K}, the objective is to learn a model f that forecasts F"}, {"title": "Methodology", "content": "This section provides a detailed structure of how the training framework works with our proposed methods. We will also detail these two simple methods with illustrations and pseudocodes.\nOverview\nThe discrete wavelet transform (DWT), as discussed in Appendix A, is proposed for enhancing time series data by preserving intricate data characteristics and facilitating multi-resolution analysis that accounts for various frequencies at different resolutions. The illustration in Fig. 2 depicts a framework of the training stages incorporating wavelet augmentations, which involve the concatenation of the look-back window and the forecasting horizon prior to transformation and augmentation. Batch sampling of the generated synthetic data is conducted according to a predefined hyperparameter called the sampling rate. These batches are subsequently used to split the data into the look-back window and target horizon, after which they are concatenated with the original data. By maintaining consistency with the notations presented in the Problem Formulation section, the random batch sampling operation can be defined as:\nf_{sample} : R^{bxt\u00d7K} \u2192 R^{nxtxK}, X \u21a6 f_{sample}(X)\nwhere b is our initial batch size and n is determined based on the hyperparameter sampling rate with n < b.\nOur selected model, DLinear [19], is characterized by a decomposition scheme and a linear layers. Given the look-back window as X \u2208 R^{L\u00d7K}, DLinear model initially decomposes it into a trend component X_t \u2208 R^{L\u00d7K} and a residual part"}, {"title": "Wavelet Masking (WaveMask)", "content": "Our first method, WaveMask, creates a synthetic dataset based on the input, which is a concatenation of the look-back window and the target horizon (Line 1 in Alg. 1). The method is illustrated in Fig. 3. WaveDec (Wavelet Decomposition) performs multilevel discrete wavelet transform (DWT) on the input signal. It decomposes the signal into the last approximation and all detail coefficients for each resolution level L (Line 3 in Alg. 1). We mask the wavelet coefficients based on the array of hyperparameter rates (\u03bc_{L+1}), which contains parameter rate for the last approximation and all detail coefficients; it has a length of L+1 (Line 4 in Alg. 1). It determines the probability of each coefficient being masked (set to zero) at that specific level, allowing for level-specific control over the masking process. CreateRandomMask generates boolean values (True or False) to determine whether to mask or not mask sections according to the rate, and the Masking function replaces True elements with zeros (Line 5 & 6 in Alg. 1). WaveRec (Wavelet Reconstruction) applies the inverse discrete wavelet transform (IDWT) to reconstruct the signal from its wavelet coefficients. It synthesizes the original signal structure from the modified/masked wavelet coefficients, effectively reversing the decomposition process (Line 8 in Alg. 1). We implemented it for each channel (c) separately (Line 2 in Alg. 1). Masking wavelet coefficients allow us to eliminate frequency components at various time scales selectively. When we reconstruct the signal using the masked coefficients, we obtain a modified version of the original signal in which specific frequency components have been eliminated."}, {"title": "Wavelet Mixing (WaveMix)", "content": "The second method, WaveMix, extends the WaveMask concept to mix two input signals using wavelet transformation. It takes two different input pairs, concatenates these look-back windows with the corresponding target horizons, and outputs augmented data after applying the transformation pipeline. The pipeline is illustrated in Fig. 4. In the same way as our first method, WaveMask, we implement wavelet decomposition for both input pairs separately. CreateRandomMask generates a binary mask based on the mix rate \u03bc[i] (i = 1, . . ., L + 1) for each wavelet level, determining which coefficients to select from each input signal. BitwiseNOT operation creates a complementary mask, ensuring that coefficients not selected from one signal are taken from the other. Masking applies the generated masks to the wavelet coefficients of each input signal and we add both to select portions from each effectively (Lines 5-7 in Alg. 1). We apply the same procedure as wavelet reconstruction to obtain the augmented data. By exchanging wavelet coefficients between inputs, the frequency components of one data instance are integrated into the other, and vice versa. This process enables the mixing of frequency information across different time granularities between the two data instances. We ran the process for each channel separately."}, {"title": "Experiments", "content": "The first section provides an overview of the datasets, their statistical information, the baseline methods, and the experimental setup. Section 5.2 presents the main results regarding four datasets. The final section presents the ablation study of cold-start forecasting, wherein models are trained on down-sampled training datasets."}, {"title": "Datasets, Baselines and Experimental Setup", "content": "Our methods are assessed using four datasets: ETTh1, ETTh2, Weather, and ILI [21,10]. We selected these datasets as they represent a diverse range of time series forecasting tasks across different domains. The statistical details for the datasets can be found in Table 1. Our approach follows the methodology delineated in the prior research [2,19], which utilized a 24-length look-back window"}, {"title": "Main Results", "content": "Table 2 and Table 3 present comparisons between our methods and baselines in terms of the metrics Mean Squared Error (MSE) and Mean Absolute Error (MAE). The best result is indicated in bold, while the second most favorable outcome is underlined. We conduct the experiments a total of 10 times and report standard deviations in contrast to the experiments conducted by the previous studies [2,20]. Every forecasting horizon is treated as an individual task, and"}, {"title": "Ablation Study", "content": "We carried out an evaluation of the cold-start forecasting task utilizing the ETTh1, ETTh2, Weather, and ILI datasets across different forecasting horizons. The downsampling rates indicate the percentage of the training dataset that was retained while keeping the test dataset unchanged. We chose different down-sampling rates of 15%, 30%, and 75% for the evaluations. The performance, as depicted in Fig. 5, is presented using the Mean Squared Error (MSE) metric, with the forecasting horizon specified in parentheses. The optimal performance, indicated by the minimum value of MSE values, was chosen from the FreqMask & FreqMix and WaveMask & WaveMix combinations, respectively. It is obvious that our methods deliver superior outcomes, particularly noticeable when the downsampling rate is as low as 15%. Furthermore, it is noteworthy that we did"}, {"title": "Discussion & Conclusion", "content": "All of these data augmentation methods can be assessed across a wide range of diverse dataset domains, in addition to the experiments already conducted. To the best of our knowledge, there are no state-of-the-art techniques other than those we evaluated, except TimeGAN [17], to compare with our methods as the methods we used are also evaluated with little data in cold-start forecasting setting. The evaluations cannot be directly compared to the paper [2] because we conducted them using 30 epochs instead of 10 epochs and selected the best model from the validation dataset. In addition, we performed the running task 10 times and reported the standard deviation. Instead of utilizing Random Search, more sophisticated search techniques could be employed to discover optimal parameters for WaveMask and WaveMix that may result in exceptional outcomes. We initially employed DLinear model [19], however, for further progress, TSMixer [3] can be utilized as it demonstrates superior outcomes and attains state-of-the-art performance. Furthermore, the augmentation methods can be implemented following linear projection layers and techniques that function on representations of time series, such as in TS2Vec [18].\nIn conclusion, both our methods, WaveMask and WaveMix, outperform the baseline methods [2,20] and are more straightforward and comprehensive techniques compared to methods that solely focus on the frequency domain without considering the time domain. Based on the ablation study, our methods have also been shown to achieve outstanding results in cold-start forecasting."}, {"title": "Discrete Wavelet Transform (DWT)", "content": "During a wavelet transform, a signal is subjected to multiplication by a wavelet function, which is a localized wave with finite energy. The resulting transform is then analyzed for each segment. The equation that defines a continuous wavelet transform (CWT) is as follows:\nH(x) = \\frac{1}{\\sqrt{\\zeta}} \\int x(t) . \\psi*(\\frac{t-\\tau}{\\zeta}) dt\nwhere H(x) is the wavelet transform for the signal x(t) as a function of time t, \u03b6 is the scale parameter, \u03c4 is the time parameter, and \u03c8 is the mother wavelet or the basis function. High frequencies, or small scales, compress the signal and convey global information, while low frequencies, or large scales, expand the signal and reveal hidden detailed information within it. Nevertheless, the cost remains high, and DWT offers effective computation through subband coding, where the signal undergoes filtering with distinct cutoff frequencies at various scales. The DWT is calculated sequentially by applying high-pass and low-pass filters to a signal, resulting in detail and approximation coefficients. The half-band filters reduce the signal's sampling rate by a factor of 2 at each level of decomposition. This method of breaking down and refining can be iterated until the desired level has been achieved. The original signal can be reconstructed by up-sampling the"}, {"title": "Hyperparameters", "content": "Random Search is applied to optimize the hyperparameters for our methods, WaveMask and WaveMix, while Grid Search is used for the remaining methods. The parameters that have been optimized are provided below:"}]}