{"title": "The Surprising Ineffectiveness of Pre-Trained Visual Representations for Model-Based Reinforcement Learning", "authors": ["Moritz Schneider", "Robert Krug", "Narunas Vaskevicius", "Luigi Palmieri", "Joschka Boedecker"], "abstract": "Visual Reinforcement Learning (RL) methods often require extensive amounts of data. As opposed to model-free RL, model-based RL (MBRL) offers a potential solution with efficient data utilization through planning. Additionally, RL lacks generalization capabilities for real-world tasks. Prior work has shown that incorporating pre-trained visual representations (PVRs) enhances sample efficiency and generalization. While PVRs have been extensively studied in the context of model-free RL, their potential in MBRL remains largely unexplored. In this paper, we benchmark a set of PVRs on challenging control tasks in a model-based RL setting. We investigate the data efficiency, generalization capabilities, and the impact of different properties of PVRs on the performance of model-based agents. Our results, perhaps surprisingly, reveal that for MBRL current PVRs are not more sample efficient than learning representations from scratch, and that they do not generalize better to out-of-distribution (OOD) settings. To explain this, we analyze the quality of the trained dynamics model. Furthermore, we show that data diversity and network architecture are the most important contributors to OOD generalization performance.", "sections": [{"title": "Introduction", "content": "Reinforcement Learning (RL) provides an elegant alternative to classic planning and control schemes, as it allows for complex behaviors to emerge by just specifying a reward, rather than hand-modelling and tuning environments and agents. Despite their success, most methods need extensive data and can be used only on their respective task, lacking the generalization capabilities needed to handle the complexity of real world tasks. On hardware, RL is costly in terms of time and wear, therefore model-based approaches are attractive as they promise to improve sample efficiency. For many real-life problems, vision is an invaluable source of state information, but due to its high-dimensional nature it is challenging to incorporate it in RL algorithms. Therefore, the use of pre-trained visual representations (PVRs) is attractive as, intuitively, it promises to improve sample efficiency and generalization. Most existing approaches use or investigate PVRs in the context of model-free RL. For example, CLIP [1] is already widely used as pre-trained vision model for model-free robotic RL tasks [2, 3, 4]. One would assume that the benefits such representations yield for model-free settings equally apply to model-based methods. In model-based reinforcement learning (MBRL), features of convolutional neural networks (CNNs) are usually used as visual state representations, whereas other representation types such as keypoints, surface normals, depth, segmentation and pre-trained"}, {"title": "Related Work", "content": "Model-Based Reinforcement Learning (MBRL) combines planning and learning for sequential decision making by using a (learned) predictive model of the environment, a learned value function and/or a policy. These methods have shown impressive results in various domains [8, 9, 10, 11]. However, MBRL is often applied to problems featuring complete state information derived from proprioception [12, 13], which may not always be available in practical scenarios like robotics. Also,"}, {"title": "Experiments Setup", "content": "As downstream RL algorithms we utilize DreamerV3 [11] and TD-MPC2 [46] which achieve state- of-the-art performance in many benchmarks and are often used in the field [47, 48, 49, 18, 50, 51, 52]. An overview of the algorithms and how we integrated the PVRs is shown in Figure 1."}, {"title": "Model-Based Reinforcement Learning", "content": "As downstream RL algorithms we utilize DreamerV3 [11] and TD-MPC2 [46] which achieve state- of-the-art performance in many benchmarks and are often used in the field [47, 48, 49, 18, 50, 51, 52]. An overview of the algorithms and how we integrated the PVRs is shown in Figure 1."}, {"title": "Pre-Trained Visual Representations", "content": "We refrain from utilizing proprioceptive information (such as end-effector poses and joint positions) to ensure a fair comparison among vision models that solely rely on visual observations. Generally we use the largest published model of each PVR. For more details we refer to Appendix A.3.\nWe chose to investigate a variety of PVRs, some of which are popular in the field of policy learning (like CLIP) whereas others are less considered in other works (e.g. mid-level representations). Most of them are trained on self-supervised objectives and use either Vision Transformers (ViT) [54] or ResNets [55]. Furthermore, all of them are open-source and easily available. Specifically, we include the following models: CLIP [1], R3M [31], Taskonomy [56], VIP [32], DinoV2 [53], OpenCLIP [57], VC-1 [35], R2D2 [58]. A more in-depth discussion and overview can be found in Appendix A.3.\nWe additionally include our own pre-trained autoencoders that are trained on task data. This allows us to investigate the influence of the pre-training data on the performance of the PVRs. During pre-training, the autoencoders see the same distribution of data as the other approaches during the reinforcement learning procedure and thus they should have a significant advantage in the subsequent reinforcement learning phase (in which only the encoder is used)."}, {"title": "Domains", "content": "We evaluate all representations across a total of 10 diverse control tasks from 3 different domains: DeepMind Control Suite (DMC) [59], ManiSkill2 [60] and Miniworld [61]. All environment observations consist of 256 \u00d7 256 RGB images, which corresponds to the resolution used for all pre-trained vision models. Most PVRs crop those images down to 224 \u00d7 224. An overview of the environments and tasks is given in Figure 2.\nThe agents are trained under a distribution of visual changes in the environment (which we refer to as In Distribution (ID) and are evaluated later under a different distribution of unseen changes (OOD changes). ID training and OOD evaluation are implemented through randomizations of visual attributes in the environments by splitting all possible randomizations into ID training and OOD evaluation sets. We focus exclusively on the setting of visual distribution shifts.\nWe train instances of each agent with different random seeds each performing 12 evaluation rollouts in the training environment every 50000 environment steps during training. For ID and OOD evaluation we perform 200 episode rollouts for each instance after training, resulting in 1200 episodes for each representation per environment. We train DMC agents for 3 million and ManiSkill2 as well as Miniworld agents for 5 million environment steps. Furthermore, TD-MPC2 agents are trained with a smaller set of PVRs on the DMC tasks due to the high computational costs of the experiments and the algorithm.\nDeepMind Control Suite (DMC) [59] is a widely used and applied benchmark environment for continuous control based on the MuJoCo simulator [62]. It includes locomotion as well as object manipulation tasks in which the agent applies low-level control to the environment. We consider five tasks from the suite: Cartpole-Swingup, Cheetah-Run, Cup-Catch, Finger-Spin, and Walker-Walk.\nTo measure ID and OOD performance, we slightly adapt the original tasks from the DMControl Generalization Benchmark [63] by changing the background colors of the tasks. Furthermore, we randomize all dimensions of the simulation geometries randomly around their initial values. More specifically, we sample the size values uniformly in a range of [0.7 \u00d7 lorg, 1.3 \u00d7 lorg] of the original simulation value lorg. The OOD evaluation set is a held-out set of 20% of all colors and sizes included in the DMControl Generalization Benchmark. The ID training set therefore consists of 80% of the colors and sizes.\nEven though the Deepmind Control Suite represents a standard benchmark in RL, none of the PVRs are trained on a visual data distribution similar to DMC providing an even stronger OOD generalization test.\nManiSkill2 [60] is a suite of robotic manipulation tasks based on the Sapien simulator [64]. The tasks are more challenging than those of DMC, due to their contact-rich nature. Many of the tasks include cluttered and diverse environments and thus are more suitable for testing the generalization capabilities of PVRs. Since many of the PVRs are trained on robotic manipulation data, those tasks represent visually easier shifts than DMC, as a distribution shift still exists but is not as large as in aforementioned DMC tasks. Nevertheless, the tasks are still challenging due to the necessity of precise control skills. We consider four tasks from the suite: StackCube, PlugCharger, PickClutterYCB, and AssemblingKits.\nSimilar to our DMC experiments, we randomize different aspects of the tasks to differentiate between ID and OOD. For StackCube we randomize size and color of the cubes but leave the semantic meaning of the colors to solve the task untouched (i.e. picking up a red cube and placing it onto a green one is still the task goal). For PlugCharger we randomize shape, size and color of the charger and wall. For both tasks we exclude 20% of the possible randomizations from training and use them for OOD evaluation. PickClutterYCB and AssemblingKits use diverse combinations"}, {"title": "Results", "content": "Here we present the results of our evaluation and answer the research questions outlined in Section 1. We start with a general comparison of the data efficiency of PVRs in MBRL (Section 4.1). Afterwards, we evaluate the OOD generalization of PVRs in MBRL (Section 4.2). Furthermore, we investigate which properties of PVRs are important for OOD generalization (Section 4.3). Finally, we analyze the prediction quality of the world models in order to explain our results before (Section 4.4). For better comparisons throughout domains, returns are normalized as recommended by Agarwal et al. [66]. For further information, we refer to Appendix A.5."}, {"title": "Data Efficiency", "content": "In the following we want to answer research question i and investigate the sample-efficiency of a PVR-based MBRL agent against the same agent using a visual representation learned from scratch. In general, one would expect that agents using PVRs are more data efficient than their counterparts with representations trained via reinforcement learning. The common assumption is that PVRs are able to capture relevant information of the environment due to their pre-training phase. Therefore, the downstream learning algorithm can focus on learning the dynamics of the environment. This ought to be especially pronounced for visual foundation models, which promise to generalize to different domains [67]. The results for our MBRL setting are summarized in Figure 3.\nSurprisingly, representations learned from scratch are in most cases equally or even more data-efficient than the PVRs. We want to highlight that this is also true for autoencoders which are pre-trained"}, {"title": "Generalization to OOD Settings", "content": "Even if PVRs are not able to perform equally well as representations learned from scratch in the ID case, they might be able to perform better in OOD domains. This should be especially true for visual foundation models which are often trained on diverse data [1, 68, 53, 69, 57]. Therefore, here we want to answer research question ii and evaluate the OOD performance on both domains after training. The results are visualized in Figure 4. With the exception of VC-1, it is noticeable that no PVR performs good in both domains. Even autoencoders trained in-distribution on task-specific data perform worse compared to training an encoder from scratch. This is surprising, since some PVRS are trained on large sets of diverse data and should therefore generalize well to OOD domains which, however, we find not to hold true when compared to agents with representations learned from scratch. This is especially evident for the DMC environments where the PVRs perform worse than training from scratch."}, {"title": "Properties of PVRs for Generalization", "content": "The results so far show that PVRs perform not as well in MBRL and for OOD generalization as they do in policy learning settings [40, 35, 41]. Also, the results do not explain which properties of the different PVRs are relevant for OOD generalization. This is the subject of our next research question iii. Based on Table 2, we group the PVRs into categories and evaluate the ID and OOD performance of the PVRs in each category. The exact categorization can be found in Appendix B. The results are depicted in Figure 5 and discussed below.\nLanguage Conditioning. All CLIP-based PVRs as well as R3M are conditioned on language in their training procedure. However, the combined performance of these PVRs shown in Figure 5 indicates that language conditioning is not necessary for good OOD generalization. This is surprising, since language conditioning is a popular technique to improve capabilities of vision- based agents [2, 70, 31, 4, 71, 72, 48] and is assumed to be a strong bias for a visual model as it should provide semantically relevant features [31]. Nevertheless, our results suggest that pre-training representations with language might not be as helpful for control tasks as it might be for other direct downstream learning tasks.\nSequential Data. Reinforcement learning is a sequential decision making problem. Thus, a good representation should capture the sequential dynamics of the environment. This is even more relevant for MBRL, where the representation is additionally used to predict the state evolution. Therefore,"}, {"title": "World Model Differences", "content": "According to research question iv, we investigate how the different visual representations influence the quality of the world model which is learned in the downstream MBRL algorithm. To this end, we train model-based agents on the Pendulum Swingup task of DMC with the same setup as described in Section 3.3. We then use a pre-collected dataset of 200 diverse trajectories to analyze the world models of the agents. We plot the error of the complete trajectory and a smaller window of the same trajectories with a horizon of 33 timesteps to show the differences between long-term and shorter predictions.\nDynamics Prediction Error. For planning purposes, MBRL algorithms employ a forward dynamics model of the environment. The model can either be given [8, 9] or learned [16, 73, 11, 74, 46]. DreamerV3 as well as TD-MPC2 belong to the latter category and we can therefore analyze the dynamics prediction error of the underlying models. The results are shown in Figure 6. The plots show that the state evolution prediction accuracy of PVR-based approaches are comparable to model- based agents using representations learned from scratch. Furthermore, we observe a slight negative correlation between the values presented in Figures 4 and 6 (r = -0.22, p = 0.4). This suggests that the quality in the dynamics prediction plays a minor role in the performance since the models with the best task performance do not necessarily have the lowest dynamics prediction error. Thus the dynamics prediction error is not the only important factor for the performance of the agent.\nReward Prediction Error. The reward model is a crucial part of the world model. Because state- of-the-art model-based algorithms are actor-critic based (as in our case), they use learned value"}, {"title": "Conclusion", "content": "In this work, we evaluate the efficiency and generalization capabilities of different PVRs in the context of model-based RL. We provide empirical evidence that PVRs neither improve sample efficiency of model-based RL, nor ultimatively empower MBRL agents to generalize better to completely unseen out-of-distribution shifts. Experiments analyzing the quality of the trained world model suggest that model-based agents utilizing PVRs are not able to learn good reward models for the tasks compared to agents learning representations from scratch. This indicates that PVR-based approaches can learn comparable dynamics models but struggle to learn good reward models which are crucial for the performance of MBRL agents. We conclude that PVRs might lack the needed information for reward model learning, and that training visual representations for MBRL requires extra attention compared to model-free RL. Additionally, we conducted experiments to find relevant performance properties of PVRs. Here, diversity in the pre-training data as well as a ViT architecture seem to improve generalization capabilities to OOD settings of PVRs. Conversely, a language-conditioned loss or sequential training data seem to play minor roles.\nLimitations. This paper aims not to be the final word on the topic of OOD generalization of PVRs, RL and MBRL in particular. Our findings hopefully inspire more researchers to dig into the untapped potential of utilizing PVRs in MBRL, since more experiments are needed to draw a final conclusion. Naturally, a benchmark like the one presented in this work is inevitably computationally demanding. It was therefore necessary to make certain design decisions and restrict the number of representations. From our point of view, we focused on the most important PVRs but other MBRL algorithms and PVRs are certainly of interest as well. Furthermore, we evaluate the PVRs on 3 domains. Experiments on other domains, especially in the real-world are needed."}, {"title": "Implementation Details", "content": "We run all our experiments on a compute cluster using 6 to 8 cores and 32GB memory per experiment with either a single NVIDIA V100 or A100 GPU depending on the PVR used. Depending on the representation/input size more memory might be needed (e.g. 96GB for the ManiSkill2 from scratch experiments)."}, {"title": "Dreamer V3", "content": "For each task and encoding type (i.e. PVR or from scratch) combination we train 6 instances of DreamerV3 for DMC and ManiSkill2 with another random seed each resulting in 756 trained instances. For Miniworld, we train 4 instances of DreamerV3 with another random seed each resulting in 16 additional trained instances. The PVR networks are implemented as environment wrappers taking image observations of as input while returning the representation xt. The representation is then fed into DreamerV3 and is stored for further training in the replay buffer. Thus, the original DreamerV3 algorithm works solely on the representations x as input using an additional MLP or linear layer. As a result, the algorithm decodes the encoding x only and not the whole input image o. To keep the encoder-decoder structure of DreamerV3, we replace the original encoder partly only instead of removing it fully. We use the implementation from https://github.com/danijar/dreamerv3 (MIT license). Hyperparameters are shown in Table 1 but in most cases do not deviate from the implementation."}, {"title": "TD-MPC2", "content": "For each task and encoding type (i.e. PVR or from scratch) combination we train 4 instances of TD-MPC2 with different random seeds. We use the implementation from https://github.com/ nicklashansen/tdmpc2 (MIT license). The encoder trained from scratch uses the unchanged implementation.\nThe PVR encoding is implemented as an environment wrapper which takes image observations of as input and returns the representation xt. Similar to the original training from scratch, we stack the last 3 embeddings Xt\u22123:t of a PVR into a single feature vector. The stack is then fed into TD-MPC2 and is stored for further training in the replay buffer. The original TD-MPC2 algorithm works solely on the representations x as input using an additional MLP or linear layer. To keep the encoder structure of TD-MPC2 in the PVR-based approaches, we replace the original encoder partly only instead of removing it fully. The residual hyperparameters do not deviate from the original implementation for visual RL."}, {"title": "PVRS", "content": "For all representations we use the same preprocessing steps as described by the authors. We use the implementations and the pre-trained models from:"}, {"title": "Properties Groups", "content": ""}, {"title": "Linear Layers Versus Multilayer Perceptrons", "content": "To ensure that the training does not depend on the choice of encoder size (linear layer vs. MLP), we trained such DreamerV3 instances on a handful environments with VC-1 as well as representations from scratch. Comparing the results in Figure 9 shows that the performance is not dependent on the choice of encoder size and that differences between the MLPs and linear layers are negligible."}, {"title": "Exploration Reward", "content": "We use an additional exploration reward $r_t^{expl}$ in all experiments which changes the per-timestep reward rt for the agent to\n$r_t = r_{env} + \\beta r^{expl}_t$\nwhere $r_{env}$ is the original reward from the environment and \\beta determines the amount of exploration. We use Plan2Explore [76] to calculate $r_t^{expl}$ with an ensemble size of 10 and $\\beta$ = 1. This helps in the overall performance in the environments."}, {"title": "Encoder Trained From Scratch", "content": "The representations which are trained from scratch are using the unmodified code from the implementation. Due to high computational demands we train the DMC as well as the Miniworld baselines on 64 \u00d7 64 and the ManiSkill2 baseline on 128 \u00d7 128 pixels."}, {"title": "Return Normalization", "content": "For all our evaluations we normalize returns Go by\n$\\frac{G_{0, agent} - G_{0, random}}{G_{0, maximum} - G_{0, random}}$\nwhere Go,random is the return of a random policy and Go,maximum is the theoretical maximum achievable return (i.e. the maximum achievable timesteps of the environment for ManiSkill2 and DMC; 200 for ManiSkill2, 1000 for DMC and 5 for Miniworld). The return of the random policy is calculated by averaging the returns of 2500 episodes of the random policy."}]}