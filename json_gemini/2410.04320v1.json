{"title": "Channel-Aware Throughput Maximization for Cooperative Data Fusion in CAV", "authors": ["Haonan An", "Zhengru Fang", "Yuang Zhang", "Senkang Hu", "Xianhao Chen", "Guowen Xu", "Yuguang Fang"], "abstract": "Connected and autonomous vehicles (CAVs) have garnered significant attention due to their extended perception range and enhanced sensing coverage. To address challenges such as blind spots and obstructions, CAVs employ vehicle-to-vehicle (V2V) communications to aggregate sensory data from surrounding vehicles. However, cooperative perception is often constrained by the limitations of achievable network throughput and channel quality. In this paper, we propose a channel-aware throughput maximization approach to facilitate CAV data fusion, leveraging a self-supervised autoencoder for adaptive data compression. We formulate the problem as a mixed integer programming (MIP) model, which we decompose into two sub-problems to derive optimal data rate and compression ratio solutions under given link conditions. An autoencoder is then trained to minimize bitrate with the determined compression ratio, and a fine-tuning strategy is employed to further reduce spectrum resource consumption. Experimental evaluation on the OpenCOOD platform demonstrates the effectiveness of our proposed algorithm, showing more than 20.19% improvement in network throughput and a 9.38% increase in average precision (AP@IoU) compared to state-of-the-art methods, with an optimal latency of 19.99 ms.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, autonomous driving has emerged as a promising technology for smart cities. By leveraging communication and artificial intelligence (AI) technologies, autonomous driving can significantly enhance the performance of a city's trans-portation system. This improvement is achieved through real-time perception of road conditions and precise object detection from onboard sensors (such as radars, LiDARs, and cameras), thereby improving road safety without human intervention [1]. Moreover, the ability of autonomous vehicles to adapt to dynamic environments and communicate with surrounding infrastructure and vehicles is crucial for maintaining the time-liness and accuracy of collected data, thereby enhancing the overall system performance [2]-[9].\nJoint perception among connected and autonomous vehicles (CAVs) is a key enabler to overcome the limitations of individual agent sensing capabilities [10]. Specifically, coop-erative CAVs enable a CAV to have a longer perception range and avoid blind spots caused by occlusions. Compared to individual vehicle perception, the advantage of collaborative perception lies in enhancing observations from different per-spectives and extending the perception range beyond line of sight, up to the maximum sensing boundary within all CAVs [11]. There are three methods for information fusion through V2V communications: (1) early fusion for raw sensed data, (2) intermediate fusion for intermediate features from a deep learning model, and (3) late fusion for detection results [12]. Recent state-of-the-art [13] indicates that intermediate fusion is the trade-off between perception accuracy and bandwidth requirements.\nHowever, due to the large amount of sensed data (e.g., point clouds and image sequences), data transmissions for CAVs in cooperative perception require massive network throughput, whereby the limitation of capacity results in communication bottlenecks. According to the KITTI dataset [14], each frame generated by 3-D Velodyne laser scanners consists of 100,000 points, while the smallest recorded scene comprises 114 frames, amounting to over 10 million points. Therefore, it is impractical to transmit such massive amounts of data by V2V networks among large-scale vehicular nodes. To overcome the challenge of network throughput, existing studies have attempted to use either a communication-efficient collaborative perception framework [15], [16], a point cloud feature-based perception framework [17], [18], or a frame for sending compressed deep feature maps [19].\nCollaborative sensing capabilities are not limited to road vehicles but extend to various autonomous platforms, such as underwater robots and unmanned surface vessels, which also share sensory data to improve perception and decision-making accuracy [20]-[23]. These platforms can take advantage of wireless communication technologies to improve cooperative navigation and perception, particularly in harsh environments. Cooperative object classification [24] is another important aspect of collaborative sensing that improves the identification of objects through data sharing among multiple autonomous agents.\nIn terms of wireless communication, the challenges in providing stable and reliable connections among moving ve-hicles are crucial, particularly under dynamic and uncertain channel conditions [25]\u2013[27]. Machine learning has played an important role in enabling adaptive wireless communication, where learning-based methods such as federated learning,\nsplit learning, and edge intelligence have been leveraged to improve the efficiency of communication among CAVs and other autonomous systems [28]-[36].\nFor collaborative perception in autonomous driving, there have been recent advancements focusing on improving co-ordination among connected vehicles to enhance perception accuracy, especially under real-world constraints like limited bandwidth and high mobility [37]-[50]. Visual data collected by multiple cameras from different vehicles provide a richer set of observations, enabling more reliable detection of objects and hazards. However, it also introduces significant commu-nication overhead due to the large size of image data, mo-tivating the development of efficient collaborative perception frameworks for visual data [51]-[53].\nAttention mechanisms, such as those used in Transformer models, have proven effective for capturing relationships in time-series data, making them particularly useful in collab-orative perception for autonomous driving [54]. Dual-stage attention-based recurrent neural networks [55] and multi-time attention networks [56] have demonstrated the ability to handle irregularly sampled data, which is often the case in vehicular sensor networks due to unpredictable communication delays and packet loss. Moreover, interpolation-prediction networks have been proposed to address challenges in irregularly sam-pled time series by learning both interpolation and prediction simultaneously [57], which is relevant for fusing data from multiple CAVs with varying data rates.\nDynamic generative models, such as dynamic Gaussian mixture-based deep generative models, have also been pro-posed to improve forecasting and compression in sparse multivariate time series, making them highly suitable for the dynamic environments faced by CAVs [58]. Additionally, set functions for time series [59] provide a novel way to represent and process data collected from multiple vehicles, ensuring efficient handling of temporal dependencies. Graph-guided networks for irregularly sampled time series [60] further enhance the capability to model complex relationships in vehicular networks, leading to more efficient collaborative perception.\nSLAM (Simultaneous Localization and Mapping) is another critical component in autonomous driving systems, provid-ing a foundation for localizing vehicles and mapping their surroundings. Surveys on SLAM highlight the importance of robust techniques for mapping and localization [61]. Prob-abilistic data association methods for SLAM are essential for achieving semantic localization in dynamic environments [62], [63]. Semantic mapping is crucial for data association in SLAM, ensuring robust data fusion from multiple sources [64]. Dynamic visual SLAM, when combined with deep learning, further improves the accuracy and robustness of SLAM in changing environments [65].\nObject tracking is another important aspect of autonomous driving, enabling vehicles to maintain awareness of surround-ing objects over time. Simple online and real-time tracking (SORT) has been proposed as an effective approach for achieving this [66]. Simulators such as CARLA [67] have played a significant role in testing and validating autonomous driving systems, providing a controlled environment to eval-uate new algorithms under different traffic scenarios. Object detection methods, like PointPillars [68], are also crucial for processing point cloud data efficiently, which is fundamental for perception in autonomous driving.\nFor cooperative perception among autonomous agents, ef-fective communication strategies are necessary to ensure spa-tial coordination and maximize perception accuracy. Multi-agent spatial coordination techniques [69], [70] are key in overcoming obstructions and improving information sharing among vehicles. Furthermore, feature-level consensus is crit-ical for ensuring reliable cooperative perception, even under noisy pose conditions [71]. Additionally, aerial monocular 3D object detection has been explored for improved perception from aerial perspectives, which can complement ground-based autonomous vehicles [72].\nAlthough the aforementioned studies on cooperative per-ception have investigated the impact of lossy communication [10] and link latency [73], most existing works [74], [75] were conducted under an unrealistic communication channel with time-invariant links. Moreover, it is noteworthy that how to determine fusion link establishment among nearby CAVs is still an open problem. For example, extitWho2com utilizes a multi-stage handshake communication mechanism to decide which agents' information should be shared [76], while extitWhen2com exploits a communication framework to decide when to communicate with other agents [77]. However, these approaches are based on simplistic proximity-based design, such as fixed communication ranges or predefined neighborhood structures. They cannot capture the dynamic na-ture of the wireless channel among nearby CAVs. In contrast, graph-guided networks for irregularly sampled time series [60] and graph-based optimization techniques, such as maximum matchings in bipartite graphs [78], can model proximity as edges in a graph, allowing for more flexible and adaptive link establishment based on actual signal strength, bandwidth, and other resources, leading to improved network throughput.\nIn addition to link establishment for fusion, compressing the data shared in V2V networks is equally important to max-imize network throughput. Taking into account the burden of transmitting point cloud data, CAVs exploit multiple cameras to collect the surrounding data. However, each camera can also produce a significant amount of image data. For example, Google's autonomous vehicle is capable of amassing up to 750 megabytes of sensor data per second [79]. Thus, we have to compress those data by reducing their spatial and temporal redundancy. To reduce spatial redundancy, the gen-eral approach is to convert the raw data from high-definition data into a 2D matrix representation and then apply image compression methods to compress the data. As for tempo-ral redundancy of streaming data, video-based compression methods are utilized to predict the content of the enclosed frames, such as MPEG [80]. Learning-based compression methods with data-driven tools exhibit superior performance over JPEG, JPEG2000 [81], and BPG [82]. Learning-based"}, {"title": "II. SYSTEM MODEL", "content": "Fig. 1 illustrates a V2V network including four CAVs.\nEach CAV are equipped with sensing and communication modules, such as cameras and signal transmitter/ receiver. The role of cameras is to perceive the surrounding environment, and transmit the processed data to nearby CAVs through communication units. We assume that CAV0 is an ego vehicle, which makes decisions based on data collected from its four cameras as well as surrounding reachable CAVs. CAV1-CAV3 are nearby CAVs (or agents), sharing sensing data with CAV0 in terms of their surrounding environments. In so doing, CAV0 can observe invisible occlusions through other CAVs. Furthermore, CAV0 can run a throughput maximization scheme to determine the link establishment and transmission rate according to the acquired channel state information (CSI). As shown in Fig. 1(a), CAV1 and CAV3 are allowed to connect to the ego vehicle CAV0, while CAV2 is disconnected from CAV0. In Fig. 1(b), each CAV transmits its local sensing data to CAV0 after link establishment. In Fig. 1(c), It can be observed that we leverage a self-supervised autoencoder in non-ego vehicles to adaptively compress raw data according to CSI. In Fig. 1(d), the ego vehicle decodes and fuses the compressed data to obtain the prediction of the bird eye's view (BEV)\u00b9.\nLet $G = (V, E)$ represent the topology of the considered V2V network, where $V = {V_1, V_2, ..., V_n}$ denotes the set of CAVs, and $E$ is the set of the links between CAVs. Moreover, according to the 3GPP standards for 5G [84], the V2V com-munication exploits the cellular vehicle-to-everything (C-V2X) with Orthogonal Frequency Division Multiplexing (OFDM). In"}, {"title": "III. PROBLEM FORMULATION", "content": "Based on the above system model, we next provide a formal description of the throughput maximization problem. Throughput is one of the indicators for CAV scenario in terms of the preservation of perception accuracy and ensuring safety. To address these objectives, we focus on optimizing three key matrix variables: link establishment $S$, data transmission rate $D$, and compression ratio $P$. High network throughput in CAVs ensures seamless communications between vehicles and the underlying network infrastructure, facilitating efficient data exchange for perception, decision-making, and coordinated actions. Let $T_{sum}(S, D)$ denote the whole data processing throughput of the considered system, as obtained by:\n$$T_{sum} (S, D) = \\sum_{j=1}^{N}A_j + \\sum_{i=1,i\\neq j}^{N} S_{ij}d_{ij}$$\nBy combining the constraints and objective function Eq. (10), we formulate the throughput maximization problem as:\n$$P: \\qquad \\max_{P,S,D} \\sum_{j=1}^{N}A_j + \\sum_{i=1,i\\neq j}^{N} S_{ij}d_{ij}$$\nIt is noted that problem P is an MIP problem since the optimization variables for link establishment S is discrete while data dissemination D and compression ratio $P$ are continuous. The MIP problem is generally known to be NP-hard [85]. Due to the difficulties of coupling variables in this MIP problem P, it is computationally hard to find the optimal solution when the V2V network scale is large. While the control and decision of CAVs are latency-sensitive, we have to conceive a real-time optimization solver to address the issue of finding optimal solution to the MIP problem P. Therefore, we decompose problem P into two sub-problems, i.e., the first part is to obtain the optimal data transmission rate and compression ratio (Sub-problem $P_1$) while the second part is to get the optimal link establishment decision by solving $P_2$.\n(1) The first sub-problem $P_1$ in the $n$th round: Given the current link establishment $S^{(n-1)}$, we optimize the variable matrices of the adaptive compression ratio $P$ and data trans-mission rate $D$. Then, $P_1$ can be formulated as follows:\n$$P_1: \\qquad \\max_{P,D} \\sum_{j=1}^{N}A_j + \\sum_{i=1,i\\neq j}^{N} S_{ij}^{(n-1)}d_{ij}$$\nwhere $j = 1,2,..., N$. The sub-problem $P_1$ is a type of nonlinear programming (NLP), because of the nonlinear constraints (3), (12a) and (12b)\u00b2. For non-convex problems, global optimization methods like branch and bound, genetic algorithms, or simulated annealing might be more appropriate, but these methods can be more computationally intensive. Thus, we attempt to fully linearize the original problem. Let $U = P \\odot D = [U_{ij}]_{N\\times N}$, where $\\odot$ denotes the Hadamard product and $u_{ij} = p_{ij}d_{ij}$. By doing so, we we linearize the product term in the constraints, then $P_1$ can be reformulated as follows:\n$$P_{1-1}: \\qquad \\max_{U,D} \\sum_{j=1}^{N}A_j + \\sum_{i=1,i\\neq j}^{N} S_{ij}^{(n-1)}d_{ij}$$\nIt is noted that the constraint (13c) can be derived from (12a) and (12b). Though we introduce a bilinear equality constraint $u_{ij}, (13b) is still nonlinear. However, the objective function\nof $P_{1-1}$ is to maximize $ - \\sum_{j=1}^{N}A_j + \\sum_{i=1,i\\neq j}^{N} S_{ij}^{(n-1)}d_{ij} $, and we obtain\nthe following inequality according to (13b):\n$$ \\frac{u_{ij}}{p_{j,max}} \\leq d_{ij} \\leq \\frac{u_{ij}}{max (p_{j,min}, \\eta e^{-L_{ij}})} $$\nwhich provides an upper bound on the optimal value of the original problem $P_{1-1}$. In order to maximize $d_{ij}$, we can maximize its upper bound. Therefore, we have a relaxation of the original problem as follows:\n$$ P_{1-2}: \\qquad \\max  \\sum_{j=1}^{N}A_j + \\sum_{i=1,i\\neq j}^{N} \\frac{u_{ij}}{max (p_{j,min}, \\eta e^{-L_{ij}})} $$\nProblem $P_{1-2}$ is a general linear programming problem, which can be solved by the simplex method or interior-point methods. We assume that the optimal result of $P_{1-2}$ is $u_{ij}^{(n)}$. Thus, the current optimal solutions of data transmission rate and adaptive compression ratio are $d_{ij}^{(n)} = u_{ij}^{(n)} [max (p_{j,min}, \\eta e^{-L_{ij}})]^{-1}$ and $p_{ij}^{(n)} = u_{ij}^{(n)}/d_{ij}^{(n)}$, respectively. Given that the values of $d_{ij}^{(n)}$ and $p_{ij}^{(n)}$ can be taken at the boundaries of the feasible region, the optimal solution of the problem $P_{1-2}$ equates to the optimal solution of the original problem $P_{1-1}$.\n(2) The second sub-problem $P_2$ in the $n$th round: Given the adaptive compression ratio $P^{(n)} = [p_{ij}^{(n)}]_{N\\times N}$ and data transmission rate $D^{(n)} = [d_{ij}^{(n)}]_{N\\times N}$, we optimize the variable matrix of the link establishment $S$. Then, $P_2$ can be formulated as follows:\n$$ P_2: \\qquad \\max_S \\sum_{j=1}^{N}A_j + \\sum_{i=1,i\\neq j}^{N} S_{ij}^{(n)}d_{ij}^{(n)} $$\nwhere $x_{ij}^{(n)} = \\frac{E_j\\tau}{(E_j\\tau + \\tau p_{ij}^{(n)}d_{ij}^{(n)})}$.\nSince the variables $s_{ij}$ are binary, $P_2$ is a maximal flow problem, which can be solved by adding or removing links to obtain a higher throughput, i.e., the Ford-Fulkerson algorithm [86]. Specifically, as for the nth round, the associated link establishment $S^{(n)} \\leftarrow S^{(n)}\\{S_{ij}\\}$ if link $(i, j)$ decreases the network throughput (Removing the link). Otherwise, we have $S^{(n)} \\leftarrow S^{(n)} \\cup \\{s_{ij}\\}$ (Adding the link). As for $P_2$, we need to repeat adding/removing links until an optimal solution that satisfies all constraints is found, or a preset number of iterations is reached.\nComplexity analysis: Regarding the difficulty of solving the mentioned problems, the initial problem $P_1$ can be changed into a linear programming problem $P_{1-2}$. Suppose there are $N$ general CAVs and one ego CAV. In that case, the time complexity for $P_{1-2}$ is $O(N^{3.5}L)$, where $L$ is the size of the input data, and the method used for optimization is the interior"}, {"title": "IV. ADAPTIVE COMPRESSION SCHEME", "content": "The optimal solution obtained for compression in previous section is under fixed network topology and channel models. However, as the network evolves with time, topology and channels are subject to change, and hence the compression scheme must be adapted. In this section, we introduce our proposed adaptive compression scheme, building upon the optimization results from Section III. Firstly, we investigate traditional compression algorithms and the currently popular deep learning-based autoencoders in Sec. IV-A. In Sec. IV-B, we conceive an adaptive rate-distortion (R-D) trade-off scheme to dynamically adjust the obtained compression ratio $P$. In Sec. IV-C, a fine-tuning based scheme was proposed to further eliminate the temporal redundancy of CAV data. Finally, we summarize our proposed adaptive compression scheme with the whole architecture of the compression network.\nTraditional lossy image compression techniques such as JPEG typically adopt a two-step paradigm, encompassing an encoding and decoding procedure, which are given by:\n(1) The first stage is the encoding process, where the input image, denoted as $i \\in R^N$, is transformed into a latent representation $h$ using a specific algorithm such as the Discrete Cosine Transform (DCT), which is represented mathematically as $h = f(i)$. To further minimize the data volume, a quantizer $Q$ is then applied to transform the continuous $h$ into a discrete vector $q\\in H^D$, following the relationship $q = Q(h)$. In the V2V scenario, $q$ is subsequently binarized and serialized into a bitstream $b$ for transmission. To optimize the transmission further, additional entropy coding techniques are employed to eliminate data redundancy, thus allowing for higher spectrum efficiency.\n(2) The second stage is the decoding stage. The received bitstream $b$ or the discrete vector $q$ undergoes a series of inverse transformations, including dequantization represented as $h = Q^{-1}(q)$ and a reconstruction function defined as $i = H(h)$. These processes aim to recover the original image from the compressed data, leading to the reconstructed image as the final output.\nWhile traditional lossy image compression techniques have their merits, they often fall short when dealing with the dynamic and complex scenarios of V2V networks, where topology and channel conditions are subject to high variability. These traditional methods, heavily relying on static algorith-mic solutions, struggle to adapt to these changing conditions, often resulting in compromised image quality and spectrum inefficiency.\nConversely, the advent of Deep Learning-Based Compres-sion (DBC) presents an innovative alternative, capitalizing on the adaptive capability of deep learning algorithms. Unlike their traditional counterparts, DBC methods leverage learned parameters from vast training data from roadside units, en-abling them to accommodate the dynamic changes inherent in V2V scenarios.\nIn the DBC framework, both encoder and decoder consist of convolutional layers. The encoder efficiently transduces the input image into a latent representation $h = f(i;\\theta)$, where the transformation parameters $\\theta$ are learned from the training data. Correspondingly, the decoder utilizes another set of parameters $\\xi$, learned from the training phase, to reconstruct the image as $i = H(h;\\xi)$. The training process is guided by the minimization of the following expression:\n$$\\mathop{\\arg \\min}\\_{\\theta,\\xi} R(b) + \\beta D(i, \\hat{i})$$\nwhere we follow the description in [83] that $R(b) = E[-\\log\\_2 P\\_r(b)]$ denotes the bitrate, and $P\\_r(b)$ can be es-"}, {"title": "V. PERFORMANCE EVALUATION", "content": "In this section, we first have conducted extensive experi-ments to evaluate the network throughput in terms of differ-ent communication settings, such as bandwidth, transmission power, the number of accessed CAVs, and vehicle distribution. Then, we compare the performance of raw data reconstruction with or without fine-tuned compression strategy. Finally, we provide the predicted results of BEV and the associated IoU, which illustrate the performance of cooperative perception."}, {"title": "VII. CONCLUSIONS", "content": "In this paper, we have developed a channel-aware through-put maximization scheme for CAV cooperative perception. The proposed TMAC algorithm, combined with an adaptive compression scheme, enables us to dynamically adapt the compression rate for V2V transmissions under dynamic com-munication conditions, enhancing the performance of network throughput and perception accuracy. Additionally, we have also introduced a fine-tuning strategy to further eliminate spatial and temporal redundancies in the transmitted data. Experimentation on the OpenCOOD platform verifies the superiority of our algorithm compared to the existing state-of-the-art methods. The results demonstrate that TMAC can improve the network throughput by 20.19% and 2 times over DMDDA [88] and FTS [89], respectively. Regarding perception accuracy (AP@IoU), TMAC outperforms DMDDA and FTS for BEV prediction with different number of CAVs by a minimum of 18.5% and 9.38%, respectively. Furthermore, after exploiting the historical information, the finetuned TMAC can save at least 42.0% of spectral resources and the optimal latency of our proposed algorithm achieved is 19.99 ms."}]}