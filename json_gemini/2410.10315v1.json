{"title": "EasyRAG: Efficient Retrieval-Augmented Generation Framework for Network Automated Operations", "authors": ["Zhangchi Feng", "Dongdong Kuang", "Zhongyuan Wang", "Zhijie Nie", "Yaowei Zheng", "Richong Zhang"], "abstract": "This paper presents EasyRAG, a simple, lightweight, and efficient retrieval-augmented generation framework for network automated operations\u00b9. The advantages of our solution are:\nAccurate Question Answering: We designed a straightforward RAG scheme based on (1) a specific data processing workflow (2) dual-route sparse retrieval for coarse ranking (3) LLM Reranker for reranking (4) LLM answer generation and optimization. This approach achieved first place in the GLM4 track in the preliminary round and second place in the GLM4 track in the semifinals.\nSimple Deployment: Our method primarily consists of BM25 retrieval and BGE-reranker reranking, requiring no fine-tuning of any models, occupying minimal VRAM, easy to deploy, and highly scalable; we provide a flexible code library with various search and generation strategies, facilitating custom process implementation.\nEfficient Inference: We designed an efficient inference acceleration scheme for the entire coarse ranking, reranking, and generation process that significantly reduces the inference latency of RAG while maintaining a good level of accuracy; each acceleration scheme can be plug-and-play into any component of the RAG process, consistently enhancing the efficiency of the RAG system.\nOur code and data are released at https://github.com/BUAADreamer/EasyRAG.", "sections": [{"title": "1 Introduction", "content": "Our solution can be summarized by Fig. 1, which includes a data processing workflow (Section 1.1) and the RAG process (Section 1.2)."}, {"title": "1.1 Ingestion", "content": null}, {"title": "1.1.1 zedx file processing", "content": "Due to the discovery that the original processing script missed some files, we have reprocessed the zedx files using the following steps:\n1. zedx Decompression: Decompress the official source data from four.zedx files, obtaining four packages of HTML documents.\n2. Path Parsing: Read the knowledge path and the actual file path from the nodetree.xml in each document package.\n3. Document Extraction: Extract the text, image titles, and image paths from each HTML document using BeautifulSoup.\n4. Saving: Save the document text in txt format, maintaining the relative location consistent with the HTML document. Also, save the knowledge path, file path, and image path information."}, {"title": "1.1.2 Text Segmentation", "content": null}, {"title": "Segmentation Settings", "content": "We used SentenceSplitter for document segmentation, initially splitting into sentences using Chinese punctuation, then merging according to the set text block size. The used block size (chunk-size) is 1024, and the block overlap size (chunk-overlap) is 200."}, {"title": "Eliminating Path Influence in Segmentation", "content": "In practice, we found that the original implementation of llama-index used a simple but unstable method of handling path information, subtracting the file path length from the text length to determine the actual text length used. This approach could cause different segmentation results with the same chunk-size and chunk-overlap, depending on the data path. During the preliminary competition, we observed that changing paths could lead to a fluctuation of"}, {"title": "1.1.3 Image Information Extraction", "content": null}, {"title": "Image Content Extraction Using a Multimodal Large Model", "content": "First, we extracted information from all images using GLM-4V-9B (GLM et al., 2024). We found that the following simple prompt achieves good results:\nBriefly describe the image"}, {"title": "Image Filtering Based on Various Rules", "content": "We found that a small number of images are beneficial for the final question answering, but not all images are useful. Therefore, we designed a flexible strategy to filter out useless images using the following steps:\n1. Use the PP-OCRv4 model\u00b2 to extract text content from images and filter out images that do not contain Chinese.\n2. Filter images whose titles contain specific keywords (e.g., network diagrams, architecture).\n3. Filter images that are referenced in the text in a specific way (e.g., configuration as shown in Figure x, file as shown in Figure x)."}, {"title": "1.2 RAG Pipeline", "content": null}, {"title": "1.2.1 Query Rewriting", "content": "During the competition, given that the queries were very brief and we identified issues with some queries being semantically awkward or having unclear keywords. For instance, \"What types of alarms are there in EMSPLUS?\" and \"What are the sources of faults?\". Before inputting these queries into the RAG Pipeline, we used a Large Language Model (LLM, GLM4) for query rewriting, which involved two methods: query expansion and Hypothetical Document Embedding (HyDE) (Gao et al., 2022).\nQuery Expansion During the preliminary round, we summarized the characteristics of queries in the current operational maintenance scenario:\n\u2022 Technical keywords in queries are crucial.\n\u2022 Queries are short and vary greatly in the amount of information provided.\nIn this context, we attempted to summarize the key terms in the queries or other potentially relevant keywords using the LLM, i.e., using the LLM's knowledge for keyword association and summary in the fields of operation and communication. This is referred to as keyword expansion."}, {"title": null, "content": "After manually annotating several data points with keywords and potential associations, we utilized the LLM (GLM4) for few-shot keyword summarization and expansion. Following (Wang et al., 2023), we generated new queries by directly concatenating the expanded keywords with the original query and then re-summarizing them using a large language model.\nLet $L$ represent the Large Language Model LLM, with $q$ and $p$ denoting the initial query and the prompt, respectively. $P_{exp}$ represents the expanded query prompt, including manually annotated data points, and $p_{sum}$ represents the prompt for summarizing and concatenating the sentence and expanded keywords using the large model.\nHyDE In situations where queries lack specificity or identifiable elements, making it difficult for both dense and sparse retrieval methods to locate the target document, we designed a set of hypothetical document embedding methods, inspired by (Gao et al., 2022).\nFor the generation of fictional documents, we devised two approaches, as shown in Figure 2. Initially, following the paper's methodology, we input the prompt $P_{hy}$ and the original question $q$ into the large language model $L$ to produce the fictional document $q_f$. However, during the semifinals, we discovered that such fictional documents contained a significant amount of irrelevant keywords and redundant information due to the large model's hallucinations, greatly affecting the effectiveness of the retrieval process. Therefore, we attempted to minimize the hallucinations and redundant information in the initial fictional document $q_f$ by using the BM25 algorithm and dense retrieval (using GTE-QWEN encoding) to identify the most relevant top1 document and use it for context prompting.\nFor the generated fictional documents, we also adopted two application methods: 1. Using the fictional document $q'$ combined with the original document $q$ for coarse ranking retrieval. 2. Using only the fictional document $q'$ combined with the original document $q$ for re-ranking of retrieval results."}, {"title": "1.2.2 Dual-route Sparse Retrieval for Coarse Ranking", "content": "In the sparse retrieval section, we utilized the BM25 algorithm to construct the retriever. The core idea of BM25 is based on term frequency (TF) and inverse document frequency (IDF), and it also"}, {"title": null, "content": "incorporates document length information to calculate the relevance between the document and query $q$. Specifically, the BM25 retriever primarily consists of a Chinese tokenizer and a stopword list. We will introduce each component in detail.\nChinese Tokenizer For the Chinese tokenizer, we used the widely known jieba Chinese tokenizer\u00b3, which is lightweight and supports multi-threaded mode to accelerate tokenization and part-of-speech analysis. It also allows for customization of word frequency or dictionaries to adjust tokenization preferences. For the tokenizer, we also attempted to customize the vocabulary; in the current 5G communication maintenance scenario, we chose a related IT field lexicon collected by Tsinghua University loaded into the tokenizer. However, the results in practice were mediocre, so we ultimately continued using the original jieba lexicon.\nStopword List For the Chinese stopword list, we adopted the common Chinese stopword list collected by Harbin Institute of Technology as a reference for filtering out meaningless words during Chinese tokenization. By filtering out irrelevant words and special symbols, we improve the hit rate of valid keywords and increase the recall rate of correct documents.\nDual-route Retrieval The BM25 dual-route retrieval for coarse ranking consists of text block retrieval and path retrieval.\n1. Text block retrieval. Use BM25 to search the segmented text blocks, recalling the top 192 text blocks with a coarse ranking score greater than 0.\n2. Path retrieval. Considering that some questions are highly relevant to our extracted knowledge paths, such as the question \"How"}, {"title": null, "content": "many types of VNF elasticity are there?\", where both VNF and elasticity can be directly found in related knowledge paths. Hence, we designed a path search using BM25 to search the knowledge paths, recalling the top 6 text blocks with a coarse ranking score greater than 0.\nRetrieval Process The BM25 retriever follows the document retrieval process below for a given query q:\n1. Document Expansion. For text block retrieval, we concatenate the file path and each text block together to serve as expanded documents for retrieval.\n2. Document Preprocessing. First, filter all documents (text blocks or paths) with stopwords, then use the Chinese tokenizer for tokenization, and pre-compute the IDF scores of the documents.\n3. Query Processing. Filter the query $q$ with stopwords and perform Chinese tokenization.\n4. Similarity Recall. Count the keywords of query $q$ and calculate the TF values of each document, compute the relevance scores based on TF and IDF values, and recall relevant documents based on scores.\n5. File Path Filtering. For text block retrieval, we use the file paths provided in the competition to compare metadata, filtering out text blocks from other sources."}, {"title": "1.2.3 Dense Retrieval for Coarse Ranking", "content": "In the dense retrieval section, we employed the gte-Qwen2-7B-instruct model developed by Alibaba (Li et al., 2023), which has achieved advanced results on the MTEB benchmark.\nRetrieval Process The dense retriever for a given query q follows the specific document retrieval process as outlined below:\n1. Document Expansion. We concatenate the file path with each text block to serve as expanded documents for retrieval.\n2. Document Encoding. All text blocks are input into the model to be encoded and the representations are stored in a Qdrant7 vector database."}, {"title": null, "content": "3. Query Encoding. Using a query prompt template, we transform $q$ into an input suitable for the GTE model and encode it using the model.\n4. Similarity Recall. During retrieval, cosine similarity is used for matching, recalling the top 288 text blocks.\n5. File Path Filtering. Using the file paths provided in the competition, we employ a Qdrant filter to eliminate text blocks from other sources."}, {"title": "1.2.4 LLM Reranker Re-ranking", "content": "We utilized the bge-reranker-v2-minicpm-layerwise model (Chen et al., 2024), a LLM Reranker trained on a hybrid of multiple multilingual ranking datasets using MiniCPM-2B-dpo-bf16. This model exhibits advanced ranking performance in both Chinese and English and includes accompanying tool code, which can be conveniently fine-tuned for specific scenarios.\nRe-ranking Process The LLM-Reranker for a given query $q$ and $k'$ coarsely ranked text blocks follows the specific document ranking process as outlined below:\n1. Document Expansion. We concatenate the knowledge paths with each text block to serve as expanded documents for retrieval.\n2. Text Processing. Combine $q$ with the $k'$ text blocks to form $k'$ query-document pairs, which are then input into the tokenizer to generate input data for the LLM.\n3. Similarity Ranking. The input data is fed into the LLM to obtain re-ranking scores for the query and each text block, and the blocks are sorted according to these scores. The highest ranked $k$ (typically 6) text blocks are returned."}, {"title": "1.2.5 Multi-route Ranking Fusion", "content": "Fusion Algorithm Since we designed multiple routes for coarse retrieval, it is also necessary to design corresponding ranking fusion strategies. We primarily used two strategies: simple merging and Reciprocal Rank Fusion (RRF). The simple merging strategy directly de-duplicates and merges text blocks obtained from multiple routes. Reciprocal Rank Fusion sums the reciprocals of the ranks of the same document across multiple retrieval paths to compute the fusion score for re-ranking.\nCoarse Ranking Fusion The most straightforward use of ranking fusion is to merge the text blocks obtained from multi-route coarse retrieval into a single set of text blocks, which are then passed to the Reranker for re-ranking. In the semifinals, we used simple merging to combine results from two sparse retrieval routes.\nRe-ranking Fusion We can also perform fusion after coarse ranking and re-ranking for each route. In the preliminary rounds, we fused text blocks from sparse and dense retrieval routes. For these two routes, we designed three re-ranking fusion methods. (1) Use RRF to merge the results after coarse and fine ranking. (2) Input the text blocks from each route into the LLM to obtain respective answers, selecting the longer answer as the final one. (3) Input the text blocks from each route into the LLM to obtain respective answers and directly concatenate the answers from all routes."}, {"title": "1.2.6 LLM Answer Generation", "content": "In this section, we first concatenate the contents of the top 6 text blocks obtained from re-ranking using the following template to create a context string:\n### Document 0: {chunk_i}\n### Document 5: {chunk_i}\nNote that the text blocks input into GLM4 here include concatenated image content, whereas the text blocks in the previous coarse and re-ranking processes did not include image content.\nWe then combine the context string and the question using the following question-and-answer template, and input it into GLM4 to obtain an answer:\nThe context information is as follows:\n{context_str}\nPlease answer the following question based on the context information and not your own knowledge. Answers can be itemized. If the context does not contain relevant information, you may respond with \"uncertain\" and should not restate the context information:\n{query_str}\nAnswer:\nAdditionally, we have designed other formats of question-and-answer templates. Inspired by Chain-of-Thought (Wei et al., 2022), we designed a Chain-of-Thought question-and-answer template (see Appendix A.2). Drawing from COSTAR (Teo, 2023), we designed a markdown format question-and-answer template (see Appendix A.1). To emphasize the importance of the top1 document, we designed a focused question-and-answer template (see Appendix A.3). Related experimental results are discussed therein."}, {"title": "1.2.7 LLM Answer Optimization", "content": "Due to our observation that the LLM gives attention to each text block, which may result in the effective information from the top1 text block not being fully utilized, we designed an answer integration prompt (see Appendix B). This prompt allows us to integrate and supplement the answers derived from the 6 text blocks using the top1 text block, leading to the final answer."}, {"title": "2 Accuracy", "content": null}, {"title": "2.1 Abbreviations Introduction", "content": "For ease of writing, we first introduce some important component identifiers.\nData represents the official processed txt data. represents our own processed version 0 txt data, which supplements some missing data compared to the official data. \u2461 is similar to \u2460, but each txt begins with a concatenated knowledge path. \u2462 represents our own processed version 1 txt data,"}, {"title": "2.2 Preliminary Experiments", "content": "In the preliminary round, our main results are displayed in Table 1, and improvements were made in the following four stages:\n1. Single-route coarse retrieval (0-2). We explored the retrieval effects of the bge-zh-v1.5 series (small, base, large) and bm25. We found that bge-base-zh-v1.5 and bm25 performed best when the top 8 results were taken; too many or too few results could lead to inaccuracies in LLM comprehension or a lack of necessary information."}, {"title": "2.3\nSemi-final Experiments", "content": "In the preliminary round, we displayed our main results in Table 2, and we made improvements through the following four stages:\n1. Exploration of Coarse Ranking Schemes (0-4). We optimized the data processing from the preliminary round, preserving more structured semantic information, and explored some of the better strategies from the preliminary round. We found that dense retrieval"}, {"title": "2.4 Exploratory Experiments", "content": null}, {"title": "2.4.1 Query Rewriting", "content": "For the query expansion and HyDE methods mentioned in Section 1.2.1, we tested them during both the preliminary and semi-final stages, with results displayed in Tables 3 and 4, respectively. Overall, since the query terms in the preliminary and semi-final competitions were already relatively specific, query rewriting did not bring any benefit. These rewriting methods might be more effective when user queries are incomplete."}, {"title": "2.4.2 Prompt Types", "content": "We tested different prompt types mentioned in Section 1.2.6 during the semi-final stage, and"}, {"title": "3 Resource Consumption", "content": "In our RAG process, only the Reranker requires significant GPU memory consumption. Thanks to enabling bfloat16, model loading requires only 5GB of GPU memory. With the default batch size of 32, the total GPU memory consumption during inference is 12GB."}, {"title": "4 Deployment Difficulty", "content": "The RAG framework is encapsulated as a process class, facilitating easy loading and use, allowing for one-click deployment. We provide a Docker deployment script, with the Docker image size being approximately 28GB. We also offer API deployment scripts based on FastAPI and a WebUI based on Streamlit10, making it convenient for use."}, {"title": "5 Inference Latency", "content": null}, {"title": "5.1 Standard Scheme", "content": null}, {"title": "Standard Time Delay", "content": "In the semi-final's standard scheme, we set the batch size for re-ranking to 32, with the inference latency for a question being 26 seconds, of which document sorting takes 6 seconds, and calling GLM4 twice takes 20 seconds.\nRemoving Answer Integration By eliminating the answer integration step and directly returning the top 6 generated answers, only one call to GLM4 is needed, reducing the inference latency to 16 seconds.\nIncreasing Re-ranking Batch Size Increasing the batch size to 256 increases the GPU memory usage but can reduce the inference latency to 24 seconds.\nFull Process Acceleration Scheme Beyond simple optimization strategies, we have also designed a full process acceleration scheme, which will be introduced in the following three subsections. This scheme aims to reduce time costs at each step. Due to the instability of GLM4 outputs, all experiments in this section terminate after the first generation of answers, without the final answer integration step, allowing for a more rigorous comparison of the impact of various acceleration methods on performance."}, {"title": "5.2 BM25 Acceleration", "content": "Since our retrieval stage relies heavily on BM25 for keyword matching, we introduced the bm25s (L\u00f9, 2024) library to optimize the speed of BM25 retrieval."}, {"title": "5.3 Reranker Acceleration", "content": "We used the bge-reranker-v2-minicpm-layerwise model developed by the Zhejiang University's Institute for AI (Chen et al., 2024) as the LLM Reranker. This model supports customization of the number of inference layers, allowing selection from"}, {"title": null, "content": "8-40 layers based on one's needs and resource constraints, thus reducing GPU memory overhead. In our preliminary experiments, we found that 28 layers performed slightly better than 40 layers, with a difference of about 0.2 points, consistent with the empirical research conclusions given in the original repository. Therefore, both the preliminary and semi-final accuracy experiments utilized 28 layers.\nHowever, since the Reranker is time-consuming in practical inference, we considered whether fewer layers could be used to speed up the process. Classic early-exit techniques in BERT, such as FastBERT (Liu et al., 2020) and DeeBERT (Xin et al., 2020), use information entropy exceeding a threshold as the condition for early exit, which is computationally intensive and results in unstable effects. Therefore, we designed a model early-exit algorithm based on maximum similarity selection, that is, for each query, we check if the softmax similarity output at the 12th layer in the first batch contains any values exceeding a certain threshold; if so, this query is inferred using just 12 layers, otherwise, 28 layers are used. We conducted an experiment using an A100 40G GPU to explore inference time, GPU memory usage, and accuracy at a batch size of 32, comparing different layers and early-exit methods. We randomly selected 10 queries and chose 192 text blocks for each, including 6 ground truth text blocks sorted using 28 layers in the complete RAG and 186 other random blocks. We predicted the sum of softmax scores of ground truth blocks relative to all blocks using various methods. Then, we assessed the similarity accuracy by dividing the predicted proportion by the proportion obtained with 28 layers, and compared the ranking accuracy of predicted ground truth with the 28-layer results, yielding the results shown in Table 7. It can be seen that our proposed model early-exit method, while reducing inference time by 33%, is able to maintain ranking results consistent with those obtained using 28 layers directly, surpassing the entropy selection methods."}, {"title": "5.4 Context Compression", "content": "We designed a context compression method based on BM25 semantic similarity, which we call BM25-Extract. For each chunk, we first split it into sentences, then use BM25 to calculate the similarity between the query and each chunk, and finally add sentences to the list in order of decreasing similarity until a set compression rate is reached. The sentences are then concatenated in their original"}, {"title": null, "content": "relative positions. We compared BM25-Extract with advanced context compression methods LLM-Lingua (Jiang et al., 2023a) and LongLLMLingua (Jiang et al., 2023b) as shown in Table 8. Our method has advantages of no GPU memory usage, faster speed, and higher accuracy, making it evidently more effective for cost-sensitive operational maintenance tasks."}, {"title": "6 Scalability", "content": "Document Scalability Our solution is primarily based on BM25 retrieval and Reranker re-ranking, requiring only processing of the latest documents, followed by re-segmentation and IDF value calculation. The entire process has a small time overhead and can be completed within 5 minutes.\nUser Scalability Our solution has low GPU memory usage, and we have designed inference acceleration methods for various stages, allowing the use of specific optimization strategies depending"}, {"title": null, "content": "on the user's scale. Even using a fully unaccelerated solution, a single 80GB GPU can support at least six RAG processes, returning answers to users within half a minute."}, {"title": "7 Conclusion", "content": "This paper presents EasyRAG, an accurate, lightweight, efficient, flexible, and scalable retrieval-augmented question-answering framework aimed at automated network operations."}]}