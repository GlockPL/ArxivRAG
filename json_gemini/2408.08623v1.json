{"title": "SketchRef: A Benchmark Dataset and Evaluation Metrics for Automated Sketch Synthesis", "authors": ["Xingyue Lin", "Xingjian Hu", "Shuai Peng", "Jianhua Zhu", "Liangcai Gao"], "abstract": "Sketch, a powerful artistic technique to capture essential vi-sual information about real-world objects, is increasingly gaining attention in the image synthesis field. However, evaluating the quality of synthesized sketches presents unique unsolved challenges. Current evaluation methods for sketch synthesis are inadequate due to the lack of a unified benchmark dataset, over-reliance on classification accuracy for rec-ognizability, and unfair evaluation of sketches with different levels of simplification. To address these issues, we introduce SketchRef, a benchmark dataset comprising 4 categories of reference photos-animals, human faces, human bodies, and common objects-alongside novel evaluation metrics. Considering that classification accuracy is insufficient to mea-sure the structural consistency between a sketch and its refer-ence photo, we propose the mean Object Keypoint Similarity (mOKS) metric, utilizing pose estimation to assess structure-level recognizability. To ensure fair evaluation sketches with different simplification levels, we propose a recognizability calculation method constrained by simplicity. We also collect 8K responses from art enthusiasts, validating the effectiveness of our proposed evaluation methods. We hope this work can provide a comprehensive evaluation of sketch synthesis algorithms, thereby aligning their performance more closely with human understanding.", "sections": [{"title": "Introduction", "content": "Sketching, a form of drawing commonly used by human artists, aims to capture the key features of real-world objects through line and contour. Despite the sketch being simple, consisting of only a few lines, humans can still recognize the structure and category of the main object from the sketch.\nResearchers have recently ventured into deep learning to automate sketch synthesis. Methods leveraging Generative Adversarial Networks (GANs) (Li et al. 2019; Yi et al. 2020; Goodfellow et al. 2020), Reinforcement Learning (RL) (Muhammad et al. 2018), and Contrastive Language-Image Pre-Training (CLIP) (Vinker et al. 2022, 2023; Radford et al. 2021) have emerged as promising approaches. Compared to human sketching, these automatic synthesis methods can effectively save time and financial costs.\nWhen synthesizing sketches, it is inevitable to assess the quality of the sketches. Unlike general image synthesis evaluation, sketch evaluation needs to consider the connection between the sketch and the reference photo. It is necessary to examine the visual features retained at different levels of simplification, which is closely related to the human brain's ability to understand abstract images. This makes it difficult to directly borrow evaluation metrics designed for general synthesis assessment.\nHowever, how to evaluate sketches comprehensively based on their unique characteristics remains an unresolved and challenging problem. As shown in Table 1, there are significant drawbacks in the existing methods of sketch synthesis evaluation. Firstly, in quantitative evaluation, there is no unified dataset serving as a benchmark. Each study selects its own dataset for evaluation, and these datasets are often limited in content to animals and objects, frequently ignoring faces and human poses. Secondly, when assessing sketch recognizability, only the classification accuracy is used as a quantitative metric, defined as category-level recognizability. Existing works ignore the consistency of structural information between the sketch and the reference photo, which we define as structure-level recognizability. Thirdly, although some studies discover the impact of the level of simplification on recognizability, noting that higher simplification leads to lower recognizability, current studies use the same recognizability metrics to evaluate sketches of different levels of simplification. This approach is unfair to sketches with varying levels of simplification.\nTo address the above issues, we introduce a benchmark dataset for sketch evaluation and propose new evaluation metrics focusing on recognizability and simplicity. By building connections between the sketch and the reference photo, we establish a benchmark designed for evaluating sketches, SketchRef. It includes 4 sub-datasets: animals, human faces, human bodies, and common things. Especially, we find that for cases of animals, human faces, and human bodies, the sketches and reference photos could share the same visual keypoint annotation, which is important structural information. Based on this observation, we propose a new metric called mean Object Keypoint Similarity (mOKS), which quantifies the structure-level recognizability of sketches with a general pose estimation model. More importantly, considering the tradeoff between recognizability and simplicity in sketches, we introduce a recognizability calculation method constrained by simplicity. This method enables a fair evaluation of sketches with varying"}, {"title": "Related Work", "content": "Sketch Dataset. There is currently no standardized dataset for sketch synthesis evaluation. Different study chooses different datasets for assessment. Clipasso (Vinker et al. 2022) employs 200 randomly selected images from 10 categories within the SketchyCOCO dataset (Gao et al. 2020). Photosketch (Li et al. 2019) collects 1,000 outdoor images sourced from Adobe Stock. LineDrawing (Chan, Durand, and Isola 2022) utilizes the MIT-Adobe FiveK dataset (Bychkovsky et al. 2011) and test data from UPDG (Yi et al. 2020). As shown in Table 1, these evaluation datasets largely focus on common objects and animals (Eitz, Hays, and Alexa\nEvaluation Metrics in Sketch Synthesis. Previous works (Vinker et al. 2022; Muhammad et al. 2018; Kampelmuhler and Pinz 2020; Song et al. 2018; Vinker et al. 2023) evaluate sketch recognizability using pre-trained classifiers' classification accuracy. However, this approach only assesses sketch quality at the category level, neglecting the visual structural consistency between the sketch and the reference photo. Lu et al. (Lu, Wang, and Fan 2023) recognize the importance of structural consistency, but their approach focuses on predicting similar corner points, relying on manually annotated data, which lacks higher-level representativeness. There is a need for a general method, which can directly predict keypoints consistent between the sketch and the reference photos, enabling the automatic evaluation of structure-level recognizability.\nTradeoff between Recognizability and Simplicity. Stroke-based sketch generation methods (Vinker et al. 2022, 2023; Muhammad et al. 2018) view sketching as a tradeoff between simplicity and recognizability, suggesting that using more strokes in sketch synthesis increases recognizability, which aligns with intuitive understanding. The SEVA study (Mukherjee et al. 2024), which collects human sketches drawn over varying time intervals, finds that sketches drawn with more time tend to be visually more complex and are perceived as more recognizable by humans. Although this influence of simplicity on recognizability is acknowledged, previous work overlooks simplicity when evaluating sketches and focuses solely on recognizability. This approach is unfair to simpler sketches, highlighting the need for a recognizability metric that accounts for simplicity."}, {"title": "Pose Estimation", "content": "Current pose estimation methods, including bottom-up (Geng et al. 2021; Yang et al. 2023) and top-down approaches (Gu, Yang, and Yao 2021; Jiang et al. 2023; Li et al. 2022), can effectively detect keypoints in real-world photos of the human body, face, and animals. It has not yet been explored whether the recognition capability of these deep-learning methods on sketches aligns with human perception. RTMPose (Jiang et al. 2023) is a general pose estimation framework designed for robust keypoint detection across various targets. Earlier works on sketch assessment have not explored the use of such a universal pose estimation model to predict keypoints in sketches and subsequently evaluate structural recognizability."}, {"title": "SketchRef Benchmark Dataset", "content": "As shown in Figure 1, our proposed dataset, SketchRef, consists of three components: reference photos required for sketching, corresponding synthesized sketches, and annotations shared between sketches and reference photos, which include both visual and semantic annotations. There are 4 sub-datasets: Human Body, Human Face, Animal, and Things. Specifically, for Human Body, Human Face, and Animal, the sketches and reference photos share common visual keypoint information, as shown in Figure 1(a). By detecting these keypoints in sketches, we can assess the structure-level recognizability of sketches. For Animal and Things, we provide classification labels, as shown in Figure 1(b). By computing the similarity between sketch images and class names, we can evaluate the category-level recognizability of sketches.\nFor Human Body, to ensure that the human poses are clearly visible and occupy the central position of the image, we collect 1,137 photos of human models from a free public human figure reference website for artists. We annotate human keypoints using the COCO format (Lin et al. 2014), identifying 17 points at major joints such as the left eye, nose, and right ankle. For Human Face, we collect 950 face photos from the FFHQ dataset (Karras, Laine, and Aila 2019), which includes different ages, genders, and ethnicities, with 106 dense keypoints for each target. For Animal, we collect 950 photos from the Animal-Pose evaluation dataset (Cao et al. 2019), with five animal class labels(dog, cat, cow, horse, and sheep), and 20 keypoints for each target. For Things, we collect 1,500 photos from SEVA (Mukherjee"}, {"title": "Evaluation Methods", "content": "Structure-level & Category-level Recognizability\nTo address the limitation of category-level recognizability in capturing the structural features of sketches, we introduce the concept of structure-level recognizability. By evaluating sketches using both types of recognizability, we achieve a comprehensive assessment that spans from coarse-grained category-level recognition to fine-grained structural analysis. To enable the subsequent calculation of recognizability methods constrained by simplicity, the metrics we propose or employ are all computed on a per-sketch basis.\nStructure-level Recognizability aims to measure whether a sketch preserves the key structural features of the reference photo. For sketches of the human body, human face, and animals, it means retaining the keypoints from the reference photo, which contains necessary pose information. We use RTMPose (Jiang et al. 2023), a general pose estimation model, to predict these keypoints in the sketch. Although pose evaluation metrics (such as mAP) can effectively evaluate the overall ability on plenty of sketches, they are not reliable for a single sketch due to the limited number of targets. Therefore, to evaluate the structure-level recognizability of a single sketch, xskt, we propose the mean Object Keypoint Similarity (mOKS) metric. We calculate mOKS between the set of predicted poses $\\mathbb{Y} = {Y_i}_{i=1}^N$ and the set of ground truth poses $\\hat{\\mathbb{Y}} = {\\hat{Y}_i}_{i=1}^M$ using the Hungarian algorithm:\nmOKS(xskt) = \\frac{1}{K} \\max_{\\sigma \\in S_K} \\sum_{i=1}^K \\text{OKS}(Y_i, \\hat{Y}_{\\sigma(i)}), K = \\max(M, N), (1)\nwhere $\\sigma \\in S_K$ is a permutation of K elements and OKS(,) stands for computing Object Keypoint Similarity between two sets of keypoints. Similar to DETR (Carion"}, {"title": "Recognizability Constrained by Simplicity", "content": "Sketching involves simplifying or adding details based on reference photos, as shown in Figure 2. This process inherently involves a tradeoff between simplicity and recognizability. To validate this tradeoff, we use Clipasso, a sketch synthesis method that allows for control over the number of strokes, to synthesize sketches with 8, 16, 32, and 64 strokes on 150 randomly selected images from the Human Body and Things datasets respectively. As shown in Figure 3, when the number of strokes increases(the simplicity of the sketch decreases), both the structure-level and category-level recognizability increase. If recognizability is calculated without accounting for simplicity, it can lead to unfair evaluations, especially for highly simplified sketches. Therefore, it is essential to develop a method for evaluating recognizability that considers the level of simplification.\nOne challenge is how to quantify the level of simplification when it comes to pixel images. Strokes provide a natural measure of complexity, but this is not the case for pixel images. We propose a method to measure simplicity, which involves comparing the sketch xskt to its reference photo xref, defined as relative Simplicity Ratio (SR):\nSR(xskt) = \\frac{C(x_{\\text{ref}})}{C(x_{\\text{skt}})}, (3)\nwhere C(.) is a complexity assessment method. The value of SR ranges from 0 to positive infinity, where SR > 1 indicates that the sketch is simpler than the original image, and SR < 1 indicates that the sketch is more complex than the original image. We test several traditional (Harris, Stephens et al. 1988; Rosten and Drummond 2006; Machado et al. 2015; Corchs et al. 2016) and deep learning methods (Feng et al. 2022) for complexity assessment and choose the Compression Ratio (Machado et al. 2015) method due to its high alignment with human perception.\nBuilding on this simplicity measurement, we propose a method for evaluating recognizability constrained by levels of simplification. For a given set of sketches, we establish a threshold a for the simplification level. We then compute the mean Recognizability on Structure (mRS) and mean Recognizability on Category (mRC) constrained by a:\nmRS@a = \\frac{1}{N} \\sum_{i=1}^N \\text{MOKS}(x_i) I(\\text{SR}(x_{\\text{skt}}) > a), (4)\nmRC@a = \\frac{1}{N} \\sum_{i=1}^N \\text{SCLIP}(x_i) I(\\text{SR}(x_{\\text{skt}}) > a), (5)\nwhere N is the total number of sketches, xskt represents the i-th sketch, \u2161(\u00b7) is indicator function. We choose a = 0.75, 1.75, representing two distinct levels of simplification. By evaluating each sketch synthesis method's recognizability at these controlled simplification levels, we ensure a fair comparison across methods."}, {"title": "Collecting Human Assessment", "content": "To investigate human assessments of sketches on structure-level recognizability and simplicity, we conduct a user experiment. Considering that sketching is a form of artistic expression, we recruit participants from an art community"}, {"title": "Experimental Setups", "content": "Datasets. To evaluate sketch synthesis methods, we use our proposed dataset, SketchRef, which contains four sub-datasets: Human Body, Human Face, Animal, and Things, with 1,137, 950, 950, and 1,500 reference photos respectively. We use 8 synthesis methods to generate sketches from these reference photos. All images used for synthesis and evaluation are of 224x224 pixels.\nEvaluation Metrics. We use mRS@a (see Equation (4)) and mRC@a (see Equation (5)) to quantify the structure-level and category-level recognizability, at fixed simplification level a. We choose a = 0.75, 1.75 in the experiment. We measure the simplification level of sketch by the the relative simplicity ratio(SR)(see Equation (3)).\nSketch Synthesis Methods. We evaluate 8 sketch synthesis methods: 1) Clipasso (Vinker et al. 2022) synthesizes sketches using a set of vector strokes, based on computing semantic and geometric losses with CLIP. We generate sketches with 64 strokes and convert vectors to pixels for evaluation. 2) Contour, 3) Anime, and 4) OpenSketch represent three sketch styles in LineDrawings (Chan, Durand, and Isola 2022). These styles are respectively trained on The Contour Drawings dataset (Li et al. 2019), The Anime Colorization dataset (Kim 2020), and OpenSketch (Gryaditskaya et al. 2019). 5) PhotoSketch (Li et al. 2019) employs a conditional GAN method to predict salient contours in reference photos. 6) UPDG1, 7) UPDG2, and 8) UPDG3 represent three sketch styles in UPDG (Yi et al. 2020). The styles are respectively trained on images from different artists and illustration websites."}, {"title": "Pose Estimation Models", "content": "To calculate mOKS, we choose RTMPose (Jiang et al. 2023), due to its generality for images of the human body, face, and animals. We utilize the pre-trained weights of RTMPose, corresponding to the models RTMPose on Face6, RTMPose on Body8, and RTMPose on AP-10K from MMDetection (Chen et al. 2019). Additionally, to compare the alignment with human responses on body sketches, we select 10 human pose estimation models from MMPose (Contributors 2020). These models are pre-trained on the COCO2017 Keypoint Detection benchmark (Lin et al. 2014), and we do not retrain them using any sketch images. Evaluation is done by calculating mOKS without using any annotated information. For the top-down pose estimation model, we first use RTMDet (Lyu et al. 2022), which is pre-trained on COCO from MMDetection (Chen et al. 2019), to predict bounding boxes in the reference photos, and then perform pose estimation within the bounding boxes using the top-down model.\nTo the best of our knowledge, there are currently no metrics specifically for the structure-level recognizability of sketches. Therefore, we refer to the Inception Score (IS) (Salimans et al. 2016) and Fr\u00e9chet Inception Distance (FID) (Heusel et al. 2017) used in image generation, using similarities of output features f(\u00b7) and probabilities p(\u00b7) from the Inception v3 network (Szegedy et al. 2016) pre-trained on ImageNet (Deng et al. 2009) to measure recognizability of sketches. The similarities between pairs of reference photos xref and sketches xskt can be calculated in the three approaches: Cosine Similarity, L2 Distance, and KL Divergence. It is particularly noted that neither method uses any annotated information, making the comparison fair."}, {"title": "Image Complexity Assessment", "content": "Image complexity assessment methods are used to measure our proposed relative Simplicity Ratio(SR). We compare traditional and deep learning methods to measure the complexity of sketches. Traditional methods include Corner Detection (Harris (Harris, Stephens et al. 1988), Fast (Rosten and Drummond 2006)), Entropy (one-dimensional and two-dimensional) (Corchs et al. 2016), and Compression Ratio (Machado et al. 2015). For deep learning methods, we adopt ICNet (Feng et al. 2022), a complexity assessment model trained on images of diverse areas with human judgments."}, {"title": "Correlation Strength Measurements", "content": "We use two rank correlation coefficients to evaluate the alignment of various metrics with human perception: Spearman's rank correlation coefficient \u03c1 (Spearman 1961) and Kendall's rank correlation coefficient \u03c4 (Kendall 1938). Both coefficients range from -1 to 1, where 1 indicates a perfect agreement, -1 indicates a perfect disagreement, and 0 indicates no correlation."}, {"title": "Implementation Details", "content": "We implement all the models based on the PyTorch framework and conduct model inference using one NVIDIA RTX-2080Ti GPU."}, {"title": "Quantitative Results", "content": "Evaluate sketch synthesis algorithms with proposed evaluation methods."}, {"title": "Human Assessment", "content": "We evaluate the mean Recognizability on Structure (mRS) of eight sketch methods across the Human Body, Human Face, and Animal datasets, as shown in Table 2. When the simplification threshold \u03b1 is set to 0.75, which tests all sketches with simplification levels above this threshold, the Anime (Chan, Durand, and Isola 2022) method exhibits the highest performance across all three datasets. This superior performance is attributed to its detailed rendering, resulting in higher recognizability. At a higher simplification threshold \u03b1 of 1.75, which filters out methods like Anime and OpenSketch that retain a significant amount of detail, CLIPasso (Vinker et al. 2022) achieves the best performance on the Human Body dataset. On the Human Face dataset, UPDG3 shows superior performance, likely due to UPDG's training focus on facial data (Yi et al. 2020). On the Animal dataset, UPDG1 outperforms other methods, indicating its robustness in this category. It is notable that PhotoSketch (Li et al. 2019) consistently demonstrates the lowest recognizability across all datasets and at both simplification thresholds \u03b1. This is likely due to its tendency to render only the coarse outlines while neglecting internal details that could provide structural hints, or to produce overly chaotic details.\nIn Table 3, we assess the mean Recognizability on Category (mRC) of the same eight sketch methods across the Animal and Things datasets. CLIPasso emerges as the top performer in terms of category-level recognizability on both datasets at simplification thresholds \u03b1 of 0.75 and 1.75. This consistent performance can be attributed to CLIPasso's consideration of semantic loss during training. The differences in category-level recognizability across methods are generally less pronounced than those observed in structure-level recognizability. This is likely because category-level recognizability primarily depends on the sketch's ability to adhere to the category of the reference photo. Methods like PhotoSketch, which only captures key outlines, can enable the model to correctly identify the subject's category label."}, {"title": "Compare the alignment of metrics with human perception", "content": "We collect user feedback on structure-level recognizability and simplicity. We calculate the correlation of tested methods with human perception on the same sets of"}, {"title": "Visualization", "content": "Similar to Image Quality Assessment (IQA), the metrics we propose can be applied to evaluate individual sketches. We select a reference photo and eight synthesized sketches from the Animal dataset. We then score the recognizability and simplicity of these sketches through mOKS, SCLIP and SR, as illustrated in Figure 4. For example, PhotoSketch exhibits the lowest structure-level recognizability (mOKS = 0.27) and the highest simplicity (SR = 3.01). This sketch significantly simplifies key details, such as the cat's eyes and nose, and introduces unnecessary lines in the tail. Anime and OpenSketch have similar simplicity levels, with SR values of 1.16 and 1.12, respectively. However, Anime's structure-level recognizability is significantly higher than that of OpenSketch, with a value of 0.90. The Anime sketch clearly delineates the subject's posture and features, closely reflecting the original photograph, whereas OpenSketch suffers from chaotic lines and excessive detail, making the structure difficult to discern.\nIn terms of category-level recognizability, the differences among the various methods are minimal. This is because the presence or absence of internal structures has a limited impact on class recognition; depicting key features that represent the category is sufficient for the model to recognize the subject as a cat. For instance, all sketches include the cat's ears. Therefore, to comprehensively evaluate sketch quality, it is essential to consider not only category-level recognizability but also structure-level recognizability to assess the consistency of structural information."}, {"title": "Conclusion", "content": "We introduce SketchRef as a benchmark for the quantitative evaluation of sketch synthesis methods. This benchmark provides a comprehensive and diverse dataset for assessing various types of sketches, and proposes a new metric for measuring structural consistency with the reference photos, addressing the current gap in evaluating structural aspects in this field. We also propose a novel general recognizability evaluation method that accounts for the simplicity of sketches, ensuring fairness in the evaluation of sketches with varying levels of abstraction. Based on SketchRef, we conduct a comprehensive evaluation of eight sketch synthesis methods and validate the high consistency between SketchRef evaluations and human perception. We also demonstrate the effectiveness of the proposed metrics in evaluating individual sketches. We expect this benchmark can guide for future sketch synthesis and sketch understanding."}]}