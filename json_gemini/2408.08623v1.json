{"title": "SketchRef: A Benchmark Dataset and Evaluation Metrics for Automated Sketch Synthesis", "authors": ["Xingyue Lin", "Xingjian Hu", "Shuai Peng", "Jianhua Zhu", "Liangcai Gao"], "abstract": "Sketch, a powerful artistic technique to capture essential visual information about real-world objects, is increasingly gaining attention in the image synthesis field. However, evaluating the quality of synthesized sketches presents unique unsolved challenges. Current evaluation methods for sketch synthesis are inadequate due to the lack of a unified benchmark dataset, over-reliance on classification accuracy for recognizability, and unfair evaluation of sketches with different levels of simplification. To address these issues, we introduce SketchRef, a benchmark dataset comprising 4 categories of reference photos-animals, human faces, human bodies, and common objects-alongside novel evaluation metrics. Considering that classification accuracy is insufficient to measure the structural consistency between a sketch and its reference photo, we propose the mean Object Keypoint Similarity (mOKS) metric, utilizing pose estimation to assess structure-level recognizability. To ensure fair evaluation sketches with different simplification levels, we propose a recognizability calculation method constrained by simplicity. We also collect 8K responses from art enthusiasts, validating the effectiveness of our proposed evaluation methods. We hope this work can provide a comprehensive evaluation of sketch synthesis algorithms, thereby aligning their performance more closely with human understanding.", "sections": [{"title": "Introduction", "content": "Sketching, a form of drawing commonly used by human artists, aims to capture the key features of real-world objects through line and contour. Despite the sketch being simple, consisting of only a few lines, humans can still recognize the structure and category of the main object from the sketch.\nResearchers have recently ventured into deep learning to automate sketch synthesis. Methods leveraging Generative Adversarial Networks (GANs), Reinforcement Learning (RL), and Contrastive Language-Image Pre-Training (CLIP) have emerged as promising approaches. Compared to human sketching, these automatic synthesis methods can effectively save time and financial costs.\nWhen synthesizing sketches, it is inevitable to assess the quality of the sketches. Unlike general image synthesis evaluation, sketch evaluation needs to consider the connection between the sketch and the reference photo. It is necessary to examine the visual features retained at different levels of simplification, which is closely related to the human brain's ability to understand abstract images. This makes it difficult to directly borrow evaluation metrics designed for general synthesis assessment.\nHowever, how to evaluate sketches comprehensively based on their unique characteristics remains an unresolved and challenging problem. As shown in Table 1, there are significant drawbacks in the existing methods of sketch synthesis evaluation. Firstly, in quantitative evaluation, there is no unified dataset serving as a benchmark. Each study selects its own dataset for evaluation, and these datasets are often limited in content to animals and objects, frequently ignoring faces and human poses. Secondly, when assessing sketch recognizability, only the classification accuracy is used as a quantitative metric, defined as category-level recognizability. Existing works ignore the consistency of structural information between the sketch and the reference photo, which we define as structure-level recognizability. Thirdly, although some studies discover the impact of the level of simplification on recognizability, noting that higher simplification leads to lower recognizability, current studies use the same recognizability metrics to evaluate sketches of different levels of simplification. This approach is unfair to sketches with varying levels of simplification.\nTo address the above issues, we introduce a benchmark dataset for sketch evaluation and propose new evaluation metrics focusing on recognizability and simplicity. By building connections between the sketch and the reference photo, we establish a benchmark designed for evaluating sketches, SketchRef. It includes 4 sub-datasets: animals, human faces, human bodies, and common things. Especially, we find that for cases of animals, human faces, and human bodies, the sketches and reference photos could share the same visual keypoint annotation, which is important structural information. Based on this observation, we propose a new metric called mean Object Keypoint Similarity (mOKS), which quantifies the structure-level recognizability of sketches with a general pose estimation model. More importantly, considering the tradeoff between recognizability and simplicity in sketches, we introduce a recognizability calculation method constrained by simplicity. This method enables a fair evaluation of sketches with varying"}, {"title": "Related Work", "content": "Sketch Dataset. There is currently no standardized dataset for sketch synthesis evaluation. Different study chooses different datasets for assessment. Clipasso employs 200 randomly selected images from 10 categories within the SketchyCOCO dataset. Photosketch collects 1,000 outdoor images sourced from Adobe Stock. LineDrawing utilizes the MIT-Adobe FiveK dataset and test data from UPDG. As shown in Table 1, these evaluation datasets largely focus on common objects and animals, and most of them only annotate sketches with category-level labels. There is no differentiation between sketches synthesized from different photo categories, overlooking the distinctive features of specific sketch types, such as pose information in human body sketches.\nEvaluation Metrics in Sketch Synthesis. Previous works evaluate sketch recognizability using pre-trained classifiers' classification accuracy. However, this approach only assesses sketch quality at the category level, neglecting the visual structural consistency between the sketch and the reference photo. Lu et al. recognize the importance of structural consistency, but their approach focuses on predicting similar corner points, relying on manually annotated data, which lacks higher-level representativeness. There is a need for a general method, which can directly predict keypoints consistent between the sketch and the reference photos, enabling the automatic evaluation of structure-level recognizability.\nTradeoff between Recognizability and Simplicity. Stroke-based sketch generation methods view sketching as a tradeoff between simplicity and recognizability, suggesting that using more strokes in sketch synthesis increases recognizability, which aligns with intuitive understanding. The SEVA study, which collects human sketches drawn over varying time intervals, finds that sketches drawn with more time tend to be visually more complex and are perceived as more recognizable by humans. Although this influence of simplicity on recognizability is acknowledged, previous work overlooks simplicity when evaluating sketches and focuses solely on recognizability. This approach is unfair to simpler sketches, highlighting the need for a recognizability metric that accounts for simplicity."}, {"title": "Pose Estimation", "content": "Current pose estimation methods, including bottom-up and top-down approaches, can effectively detect keypoints in real-world photos of the human body, face, and animals. It has not yet been explored whether the recognition capability of these deep-learning methods on sketches aligns with human perception. RTMPose is a general pose estimation framework designed for robust keypoint detection across various targets. Earlier works on sketch assessment have not explored the use of such a universal pose estimation model to predict keypoints in sketches and subsequently evaluate structural recognizability."}, {"title": "SketchRef Benchmark Dataset", "content": "As shown in Figure 1, our proposed dataset, SketchRef, consists of three components: reference photos required for sketching, corresponding synthesized sketches, and annotations shared between sketches and reference photos, which include both visual and semantic annotations. There are 4 sub-datasets: Human Body, Human Face, Animal, and Things. Specifically, for Human Body, Human Face, and Animal, the sketches and reference photos share common visual keypoint information, as shown in Figure 1(a). By detecting these keypoints in sketches, we can assess the structure-level recognizability of sketches. For Animal and Things, we provide classification labels, as shown in Figure 1(b). By computing the similarity between sketch images and class names, we can evaluate the category-level recognizability of sketches.\nFor Human Body, to ensure that the human poses are clearly visible and occupy the central position of the image, we collect 1,137 photos of human models from a free public human figure reference website for artists. We annotate human keypoints using the COCO format, identifying 17 points at major joints such as the left eye, nose, and right ankle. For Human Face, we collect 950 face photos from the FFHQ dataset, which includes different ages, genders, and ethnicities, with 106 dense keypoints for each target. For Animal, we collect 950 photos from the Animal-Pose evaluation dataset, with five animal class labels(dog, cat, cow, horse, and sheep), and 20 keypoints for each target. For Things, we collect 1,500 photos from SEVA, including 127 types of object class labels, such as bag, car, etc.\nWe segment the photos with U2Net, ensuring a blank background. For sketch production, we employ existing sketch synthesis methods, including CLIPasso, Photosketch, UPDG and LineDrawing. Each reference photo produces 8 synthesized sketches in different styles(we show 5 of them in Figure 1)."}, {"title": "Evaluation Methods", "content": "To address the limitation of category-level recognizability in capturing the structural features of sketches, we introduce the concept of structure-level recognizability. By evaluating sketches using both types of recognizability, we achieve a comprehensive assessment that spans from coarse-grained category-level recognition to fine-grained structural analysis. To enable the subsequent calculation of recognizability methods constrained by simplicity, the metrics we propose or employ are all computed on a per-sketch basis.\nStructure-level Recognizability aims to measure whether a sketch preserves the key structural features of the reference photo. For sketches of the human body, human face, and animals, it means retaining the keypoints from the reference photo, which contains necessary pose information. We use RTMPose, a general pose estimation model, to predict these keypoints in the sketch. Although pose evaluation metrics (such as mAP) can effectively evaluate the overall ability on plenty of sketches, they are not reliable for a single sketch due to the limited number of targets. Therefore, to evaluate the structure-level recognizability of a single sketch, $x_{skt}$, we propose the mean Object Keypoint Similarity (mOKS) metric. We calculate mOKS between the set of predicted poses $\\mathbf{Y} = \\{Y_i\\}_{i=1}^M$ and the set of ground truth poses $\\mathbf{\\hat{Y}} = \\{\\hat{Y}_i\\}_{i=1}^N$ using the Hungarian algorithm:\n$mOKS(x_{skt}) = \\frac{1}{K} \\max_{\\sigma \\in S_K} \\sum_{i=1}^K OKS(Y_i, \\hat{Y}_{\\sigma(i)}), K = max(M,N),$   (1)\nwhere $\\sigma \\in S_K$ is a permutation of K elements and OKS(,) stands for computing Object Keypoint Similarity between two sets of keypoints. Similar to DETR"}, {"title": "Category-level Recognizability", "content": "Category-level Recognizability refers to the ability of a sketch to be accurately identified as the category of the reference photo. Although classification accuracy can measure the overall category-level recognizability of a set of sketches, it is not suitable for assessing a single sketch. Therefore, following previous works, we compute the average cosine similarity between the CLIP embeddings of the class names and the images. The category-level recognizability for a single sketch $x_{skt}$ is calculated as $S_{CLIP}$:\n$S_{CLIP}(x_{skt}) = cos \\left(E_{text}(class), E_{image}(x_{skt})\\right),$   (2)\nwhere $cos()$ is the cosine similarity, $class$ represents the class name for the sketch, $E_{text}(\\cdot)$ is the CLIP text embedding of the class name, $E_{image}(\\cdot)$ is the CLIP image embedding of the sketch."}, {"title": "Recognizability Constrained by Simplicity", "content": "Sketching involves simplifying or adding details based on reference photos, as shown in Figure 2. This process inherently involves a tradeoff between simplicity and recognizability. To validate this tradeoff, we use Clipasso, a sketch synthesis method that allows for control over the number of strokes, to synthesize sketches with 8, 16, 32, and 64 strokes on 150 randomly selected images from the Human Body and Things datasets respectively. As shown in Figure 3, when the number of strokes increases(the simplicity of the sketch decreases), both the structure-level and category-level recognizability increase. If recognizability is calculated without accounting for simplicity, it can lead to unfair evaluations, especially for highly simplified sketches. Therefore, it is essential to develop a method for evaluating recognizability that considers the level of simplification.\nOne challenge is how to quantify the level of simplification when it comes to pixel images. Strokes provide a natural measure of complexity, but this is not the case for pixel images. We propose a method to measure simplicity, which involves comparing the sketch $x_{skt}$ to its reference photo $x_{ref}$, defined as relative Simplicity Ratio (SR):\n$SR(x_{skt}) = \\frac{C(x_{ref})}{C(x_{skt})},$  (3)\nwhere C(.) is a complexity assessment method. The value of SR ranges from 0 to positive infinity, where SR > 1 indicates that the sketch is simpler than the original image, and SR < 1 indicates that the sketch is more complex than the original image. We test several traditional and deep learning methods for complexity assessment and choose the Compression Ratio method due to its high alignment with human perception.\nBuilding on this simplicity measurement, we propose a method for evaluating recognizability constrained by levels of simplification. For a given set of sketches, we establish a threshold a for the simplification level. We then compute the mean Recognizability on Structure (mRS) and mean Recognizability on Category (mRC) constrained by a:\n$\\begin{aligned} m R S \\vert @ a &=\\frac{1}{N} \\sum_{i=1}^{N} m O K S(x_{skt}^i) \\mathbb{I}\\left(S R\\left(x_{skt}^i\\right)>a\\right), \\quad(4) \\\\ m R C \\vert @ a &=\\frac{1}{N} \\sum_{i=1}^{N} S C L I P\\left(x_{skt}^i\\right) \\mathbb{I}\\left(S R\\left(x_{skt}^i\\right)>a\\right), \\\\end{aligned}$   (5)\nwhere N is the total number of sketches, $x_{skt}^i$ represents the i-th sketch, \\mathbb{I}(\u00b7) is indicator function. We choose a = 0.75, 1.75, representing two distinct levels of simplification. By evaluating each sketch synthesis method's recognizability at these controlled simplification levels, we ensure a fair comparison across methods."}, {"title": "Collecting Human Assessment", "content": "To investigate human assessments of sketches on structure-level recognizability and simplicity, we conduct a user experiment. Considering that sketching is a form of artistic expression, we recruit participants from an art community"}, {"title": "Experimental Setups", "content": "Datasets. To evaluate sketch synthesis methods, we use our proposed dataset, SketchRef, which contains four sub-datasets: Human Body, Human Face, Animal, and Things, with 1,137, 950, 950, and 1,500 reference photos respectively. We use 8 synthesis methods to generate sketches from these reference photos. All images used for synthesis and evaluation are of 224x224 pixels.\nEvaluation Metrics. We use mRS@a (see Equation (4)) and mRC@a (see Equation (5)) to quantify the structure-level and category-level recognizability, at fixed simplification level a. We choose a = 0.75, 1.75 in the experiment. We measure the simplification level of sketch by the the relative simplicity ratio(SR)(see Equation (3)).\nSketch Synthesis Methods. We evaluate 8 sketch synthesis methods: 1) Clipasso synthesizes sketches using a set of vector strokes, based on computing semantic and geometric losses with CLIP. We generate sketches with 64 strokes and convert vectors to pixels for evaluation. 2) Contour, 3) Anime, and 4) OpenSketch represent three sketch styles in LineDrawings. These styles are respectively trained on The Contour Drawings dataset, The Anime Colorization dataset and OpenSketch. 5) PhotoSketch employs a conditional GAN method to predict salient contours in reference photos. 6) UPDG1, 7) UPDG2, and 8) UPDG3 represent three sketch styles in UPDG. The styles are respectively trained on images from different artists and illustration websites."}, {"title": "Pose Estimation Models", "content": "To calculate mOKS, we choose RTMPose, due to its generality for images of the human body, face, and animals. We utilize the pre-trained weights of RTMPose, corresponding to the models RTMPose on Face6, RTMPose on Body8, and RTMPose on AP-10K from MMDetection. Additionally, to compare the alignment with human responses on body sketches, we select 10 human pose estimation models from MMPose. These models are pre-trained on the COCO2017 Keypoint Detection benchmark, and we do not retrain them using any sketch images. Evaluation is done by calculating mOKS without using any annotated information. For the top-down pose estimation model, we first use RTMDet which is pre-trained on COCO from MMDetection, to predict bounding boxes in the reference photos, and then perform pose estimation within the bounding boxes using the top-down model.\nTo the best of our knowledge, there are currently no metrics specifically for the structure-level recognizability of sketches. Therefore, we refer to the Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) used in image generation, using similarities of output features $f(\u00b7)$ and probabilities $p(\u00b7)$ from the Inception v3 network pretrained on ImageNet to measure recognizability of sketches. The similarities between pairs of reference photos $x_{ref}$ and sketches $x_{skt}$ can be calculated in the three approaches: Cosine Similarity, L2 Distance, and KL Divergence. It is particularly noted that neither method uses any annotated information, making the comparison fair."}, {"title": "Image Complexity Assessment", "content": "Image complexity assessment methods are used to measure our proposed relative Simplicity Ratio(SR). We compare traditional and deep learning methods to measure the complexity of sketches. Traditional methods include Corner Detection, Fast, Entropy (one-dimensional and two-dimensional), and Compression Ratio. For deep learning methods, we adopt ICNet, a complexity assessment model trained on images of diverse areas with human judgments."}, {"title": "Correlation Strength Measurements", "content": "We use two rank correlation coefficients to evaluate the alignment of various metrics with human perception: Spearman's rank correlation coefficient $\\rho$ (Spearman 1961) and Kendall's rank correlation coefficient $\\tau$ (Kendall 1938). Both coefficients range from -1 to 1, where 1 indicates a perfect agreement, -1 indicates a perfect disagreement, and 0 indicates no correlation."}, {"title": "Implementation Details", "content": "We implement all the models based on the PyTorch framework and conduct model inference using one NVIDIA RTX-2080Ti GPU."}, {"title": "Quantitative Results", "content": "Evaluate sketch synthesis algorithms with proposed evaluation methods."}, {"title": "Human Assessment", "content": "Compare the alignment of metrics with human perception. We collect user feedback on structure-level recognizability and simplicity. We calculate the correlation of tested methods with human perception on the same sets of"}, {"title": "(1) Structure-level Recognizability", "content": "We evaluate the alignment between the mOKS metric (see Equation (1)) obtained using different pose estimation models and three similarity metrics calculated using the Inception v3 network and user-assessed structure-level recognizability. As shown in Table 4, the similarity evaluation methods of the Inception v3 network are essentially unrelated to user-assessed recognizability, failing to capture the structural connections between figure sketches and the original reference photos. In contrast, the mOKS results obtained from different pose estimation models, even without training on sketch data, still exhibited a strong correlation, which validates the effectiveness of the mOKS metric. Notably, although RTMPose does not show the highest correlation with human perception on sketches of the human body, we still choose this model due to its generality and applicability, for images of human pose, face, and animals."}, {"title": "(2) Simplicity", "content": "We analyze the correlation between the SR metric calculated by different image complexity assessment methods (see Equation (3)) and user-assessed simplicity. As shown in Table 5, the Compression Ratio algorithm shows a high correlation with human perception ($\\rho$=0.7618, $\\tau$=0.6422). Compared to traditional complexity metrics, ICNet displays the lowest correlation ($\\rho$=0.47, $\\tau$=0.37). Although IC-Net is trained with paintings included in its dataset, the majority of its training data consists of real photographs, which limits its generalization capabilities for sketches. Considering that sketches are a relatively simple form of visual expression, we believe that employing the Compression Ratio method for measuring simplicity is sufficient."}, {"title": "Visualization", "content": "Similar to Image Quality Assessment (IQA), the metrics we propose can be applied to evaluate individual sketches. We select a reference photo and eight synthesized sketches from the Animal dataset. We then score the recognizability and simplicity of these sketches through mOKS, SCLIP and SR, as illustrated in Figure 4. For example, PhotoSketch exhibits the lowest structure-level recognizability (mOKS = 0.27) and the highest simplicity (SR = 3.01). This sketch significantly simplifies key details, such as the cat's eyes and nose, and introduces unnecessary lines in the tail. Anime and OpenSketch have similar simplicity levels, with SR values of 1.16 and 1.12, respectively. However, Anime's structure-level recognizability is significantly higher than that of OpenSketch, with a value of 0.90. The Anime sketch clearly delineates the subject's posture and features, closely reflecting the original photograph, whereas OpenSketch suffers from chaotic lines and excessive detail, making the structure difficult to discern.\nIn terms of category-level recognizability, the differences among the various methods are minimal. This is because the presence or absence of internal structures has a limited impact on class recognition; depicting key features that represent the category is sufficient for the model to recognize the subject as a cat. For instance, all sketches include the cat's ears. Therefore, to comprehensively evaluate sketch quality, it is essential to consider not only category-level recognizability but also structure-level recognizability to assess the consistency of structural information."}, {"title": "Conclusion", "content": "We introduce SketchRef as a benchmark for the quantitative evaluation of sketch synthesis methods. This benchmark provides a comprehensive and diverse dataset for assessing various types of sketches, and proposes a new metric for measuring structural consistency with the reference photos, addressing the current gap in evaluating structural aspects in this field. We also propose a novel general recognizability evaluation method that accounts for the simplicity of sketches, ensuring fairness in the evaluation of sketches with varying levels of abstraction. Based on SketchRef, we conduct a comprehensive evaluation of eight sketch synthesis methods and validate the high consistency between SketchRef evaluations and human perception. We also demonstrate the effectiveness of the proposed metrics in evaluating individual sketches. We expect this benchmark can guide for future sketch synthesis and sketch understanding."}]}