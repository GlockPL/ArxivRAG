{"title": "Enhancing Multi-Agent Consensus through Third-Party LLM Integration: Analyzing Uncertainty and Mitigating Hallucinations in Large Language Models", "authors": ["Zhihua Duan", "Jialin Wang"], "abstract": "Large Language Models (LLMs) still face challenges when dealing with complex reasoning tasks, often resulting in hallucinations, which limit the practical application of LLMs. To alleviate this issue, this paper proposes a new method that integrates different LLMs to expand the knowledge boundary, reduce dependence on a single model, and promote in-depth debate among agents. The main contributions include: 1) Introducing third-party LLMs to adjust the attention weights of agents through uncertainty estimation and confidence analysis, optimizing consensus formation in multi-agent systems; 2) Experiments on arithmetic datasets have validated the effectiveness of the method, surpassing traditional multi-agent baselines. This research provides a new perspective for large models to alleviate hallucination phenomena when dealing with complex tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have exhibited robust reasoning and creative capabilities. In order to manage more intricate tasks, AutoGen amalgamates the functionalities of LLMs, human inputs, and various tools by constructing agents with distinct roles, thereby offering a tailored multi-agent dialogue setting [1]. This has catalyzed the evolution of multi-agent debate systems. In these systems, multiple agents articulate their arguments, while a neutral moderator oversees the debate process to facilitate the attainment of a final resolution [2].By employing multiple instances of language models and engaging in several rounds of proposal and debate regarding their respective answers and reasoning processes, a consensus final answer is ultimately achieved [3].\nIn the process of multi-agent debate, it is common to deploy the same Large Language Model (LLM) for each agent and assign them the same role. This approach has proven effective in multi-agent debates, but it also has limitations, especially when the model's knowledge scope is restricted and lacks external feedback from different models. The use of a single model can lead to a monolithic viewpoint in the debate process, limiting the depth and breadth of the debate.\nTherefore, this paper raises a question: Can a third-party model be introduced to complete tasks through the collaboration of multiple Large Language Models? It explores combinations of different LLMs to estimate the confidence levels of each agent and seeks to achieve a broader consensus.\nOur research motivation is to explore how the introduction of third-party Large Language Models (LLMs) and strategies for combining different LLMs can enhance the decision-making capabilities and problem-solving depth of multi-agent systems. Special emphasis is placed on pairing external LLMs with specific domain knowledge with existing LLMs; exploring different combinations of LLMs is deemed valuable to foster debate and collaboration among agents, thereby achieving a comprehensive and in-depth analysis of complex problems. The main contributions of this paper are as follows:\n1. This study introduces third-party Large Language Models (LLMs) to investigate the application of uncertainty estimation and confidence in multi-agent systems. By adjusting attention weights, the LLM model can learn from the responses of other agents, thereby facilitating the formation and optimization of consensus. 2. The research conducted in this paper involved experiments on an arithmetic dataset, demonstrating that the method proposed in this paper outperforms previous multi-agent baselines."}, {"title": "II. RELATED WORK", "content": "A. Uncertainty in LLMs\nWhile uncertainty in conventional natural language processing (NLP) tasks has been extensively researched, it provides a robust theoretical foundation and methodological guidance for harnessing uncertainty in NLP tasks [4] [5] [6].However, the field of uncertainty quantification in large models is still understudied. In generative AI large models, text is typically generated based on probabilistic models, considering different contexts and internal states during each generation. Any generated answer that is semantically consistent with the actual answer can be considered correct [7] [8] [9]. Proposed to quantify uncertainty by directly prompting the language model to answer questions related to the uncertainty of its own generation [10].\nA new method for uncertainty quantification called Shifting Attention to more Relevant (SAR) has been proposed. This method focuses on more relevant components at both the token and sentence levels to achieve better uncertainty quantification [11]. A multi-agent debate framework is proposed, which assesses the confidence level of agents through uncertainty measurement and adjusts the attention mechanism of the LLM to adjust token weights based on confidence levels. Text prompts are used to convey confidence, thereby enhancing reliability in multi-agent debates [3].\nThis study aims to introduce a third-party large model to specify uncertainty metrics, and to describe the uncertainty of generated tokens by transforming the logits values in the generation results.\nB. Applications of Agents in LLMs\nRecently, significant progress has been made in automated task resolution through the use of multi-agents driven by Large Language Models (LLMs). ReConcile is a multi-model multi-agent framework that enhances collaborative reasoning between Large Language Models (LLMs) through multi-round discussions, employing a confidence-weighted voting mechanism to achieve better consensus [12].\nBy integrating large language models and tools, multiple agents can be introduced to facilitate mutual promotion of different roles, thereby enhancing reasoning capabilities through multi-agent debate [1] [13] [14]. A smart personalized digital banking assistant based on LangGraph and thought chains has been proposed, utilizing Large Language Models (LLMs) and a multi-agent framework to improve task efficiency [15].\nA multi-agent system approach has been adopted, implementing system 2 thinking through the CrewAI framework, which integrates in-depth analysis and reasoning decision-making to enhance AI's capabilities in handling complex problems [16].\nThis article constructs four agents and conducts three rounds of question-and-answer sessions, where agents share response information and uncertainty probabilities with each other to refine their answers, ultimately determining the final answer based on the principle of majority voting."}, {"title": "III. METHODS", "content": "To accurately grasp multi-round dialogue information and enhance the reasoning capabilities of large models, as shown in Figure 2, this paper proposes a simple yet powerful fine-grained reasoning method. The entire dialogue session is divided into the context information of user questions and responses from four agent proxies, constructing a context prompt that includes the response information of four dialogue agents, and requiring the current agent to provide an updated answer based on the opinions of other agents, and to state the final answer, guiding the agent to consider all agents' dialogue and confidence levels comprehensively.\nAttention weight update: In the open-source large model Llama, the Attention Scaling Range Weights mechanism is used, multiplying the range weights on the basis of attention, allocating more attention to the information of agents with larger range weights, thereby dynamically adjusting the large model's attention weights to different agents, and updating its output or decision. In this way, the model can not only consider the contribution of each agent but also adjust the importance of each agent's dialogue according to confidence levels, thus generating a more comprehensive and balanced response. The system takes into account information from multiple agents when making decisions and can weight this information based on the credibility of each information source, thereby reaching a more accurate conclusion. After conversion through logits, it outputs text completion information and uncertainty measures.\nIn multi-agent dialogues, the model focuses on agents with higher confidence. After each round of dialogue, responses from each agent are received. Attention scaling is only applied to the answers from the previous round. For example, the second round scales attention weights based on the responses from the first round, and the third round scales attention weights based on the responses from the second round. The large model uses Transformer decoder layers, where the attention mechanism creates \"query,\" \"key,\" and \"value\" vectors for each token. The similarity between the \"query\" vector of the current token and the \"key\" vectors of each token is used to calculate the weights for each token, which are normalized through the softmax function to ensure their sum is 1, and are used to create the output vector. The weight of each token determines its influence on the generation of the next token.\nBy modifying these weights, the model can adjust its focus on each token in the input [17].\nThe calculation formula for updating weights using range weights is as follows:\n1. The original formula for calculating Transformer attention weights.\n$A = QK^T$\n2. The improved formula applying weighted importance and range limitations.\n$A[:, :, :, start : rend] \\leftarrow A[:, :, :, start : rend] \\cdot A \\cdot r_w$\n3. The calculation of attention weights after normalization and application of the softmax function.\n$Attention(Q, K, V) = softmax \\left(\\frac{A}{\\sqrt{d_k}}\\right)V$\nA represents the attention weight matrix, Q, K and V represent the Query, Key and Value matrices. $d_k$ is the dimension of the key vectors. $start$ and $rend$ represent the start and end positions of the range. A is a scalar used to adjust the importance weights, and $r_w$ is a weight factor related to the range.\nAs depicted in Algorithm 1, the algorithmic framework employed in this study is based on the code implementation proposed in reference [3], with further optimizations applied. We provide a detailed description of the confidence-based attention adjustment mechanism within the LlamaAttention class. The process begins by evaluating the sum of attention weights across specified ranges, represented by range weights. For each range, the mean and standard deviation of the attention weights are calculated to derive a weighted importance measure. This measure, in conjunction with the range weights, is used to adjust the original attention weights, enhancing the impact of more confident predictions. Post-adjustment, a normalization step ensures that the adjusted weights maintain the same distribution as the original weights, thus preserving the overall attention distribution. This approach allows the model to focus more on areas with higher confidence, thereby improving the accuracy of the attention mechanism."}, {"title": "IV. EXPERIMENT DESIGN", "content": "This experiment utilizes the Arithmetic dataset, which is a collection of randomly generated arithmetic problems in the mathematical form of a+b\u00d7c+d, where 0 \u2264 a, b, c, d < 30,As shown in Table 1. Considering the cost of running, only a dataset of 100 problems was constructed, and the uncertainty measures and methods for these samples were evaluated.\nAs shown in Figure 3, this experiment involves four agents and three rounds of question-and-answer, with the first three agents using the same LLM (Llama3 [8]). The fourth agent introduces a third-party large model (ERNIE [18])."}, {"title": "V. RESULTS", "content": "This study proposes a new method where the first three agents use the Llama large model, and the fourth agent introduces the third-party ERNIE large model. By employing uncertainty and dynamically adjusting attention weights, the task performance is significantly improved after three rounds of dialogue. As shown in Table 2, the experimental results indicate that our method achieved significant effectiveness under the Attention-All setting, with an accuracy rate as high as 0.940, surpassing other baseline methods. Among them, the standard baseline method has an accuracy rate of 0.478, and the entropy-based baseline methods have accuracy rates of 0.482 and 0.518, respectively. The TokenSAR baseline method's accuracy rate ranges from 0.464 to 0.500. The Oracle method's accuracy rates are 0.542, 0.654, and 0.732."}, {"title": "VI. LIMITATIONS", "content": "This study demonstrates potential in specific tasks, but there are limitations in computational efficiency, application of attention mechanisms, and cross-domain experiments. Future research needs to address these issues to expand the application scope of LLMs.\n1. Computational Overhead and Real-time Application Challenges: This method enhances the ability to handle complex tasks by integrating third-party large models, but this correspondingly increases the computational cost. Therefore, when implementing a multi-agent architecture, it is necessary to consider the potential impact on computational efficiency and make corresponding trade-offs.\n2. Application of Attention Weighting Mechanism: Future research can further explore the applicability of this mechanism in diverse tasks and how to adjust the attention weights according to the characteristics of different tasks to enhance the model's overall performance.\n3. Extensive Experiments in Specific Domains: This study has achieved certain results in model experiments in specific domains, but the scope of the experiments is limited. Although the performance of this method on datasets such as MMLU and TruthfulQA has not yet reached the performance level of current state-of-the-art (SOTA) large models, there is potential for future improvement by employing more powerful large models."}, {"title": "VII. CONCLUSION", "content": "This paper effectively addresses the trust issue when multiple agents provide different answers by introducing a third-party Large Language Model (LLM) and integrating its responses and confidence levels into multi-agent dialogues. To calculate confidence, this paper optimizes the attention mechanism of the LLM by setting confidence parameters for the third-party LLM and combining them with the confidence of the Llama model, achieving attention weight adjustment based on confidence levels. Experiments show that this method can more effectively convey information to the LLM compared to traditional text prompts, allowing the model to consider the contributions and confidence levels of each agent when generating responses, thus producing more comprehensive and balanced outputs. This study not only enhances the dialogue system's ability to handle uncertainty but also provides a new perspective for the future application of LLMs in multi-agent interactions. Future work will explore how to further apply the attention scaling mechanism to a broader range of scenarios."}]}