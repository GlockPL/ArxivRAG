{"title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models", "authors": ["Yinlam Chow", "Guy Tennenholtz", "Izzeddin Gur", "Vincent Zhuang", "Bo Dai", "Sridhar Thiagarajan", "Craig Boutilier", "Rishabh Agarwal", "Aviral Kumar", "Aleksandra Faust"], "abstract": "Recent studies have indicated that effectively utilizing inference-time compute is crucial for attaining better performance from large language models (LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm, in which the model is fine-tuned in a manner that directly optimizes the performance of the inference-time strategy. We study this paradigm using the simple yet effective Best-of-N (BoN) inference strategy, in which a verifier selects the best out of a set of LLM-generated responses. We devise the first imitation learning and reinforcement learning (RL) methods for BoN-aware fine-tuning, overcoming the challenging, non-differentiable argmax operator within BoN. We empirically demonstrate that our BoN-aware models implicitly learn a meta-strategy that interleaves best responses with more diverse responses that might be better suited to a test-time input\u2014a process reminiscent of the exploration-exploitation trade-off in RL. Our experiments demonstrate the effectiveness of BoN-aware fine-tuning in terms of improved performance and inference-time compute. In particular, we show that our methods improve the Bo32 performance of Gemma 2B on Hendrycks MATH from 26.8% to 30.8%, and pass@32 from 60.0% to 67.0%, as well as the pass@16 on HumanEval from 61.6% to 67.1%.", "sections": [{"title": "1. Introduction", "content": "An effective method for improving the performance of large language models (LLMs) is to leverage addi-tional computation at inference-time: various works (Hosseini et al., 2024; Kumar et al., 2024; Lightmanet al., 2023; Wu et al., 2024) have shown that by using search, re-ranking, multi-turn revision, and moregenerally, any approach that makes use of more tokens and inference-time compute, the performance ofLLMs on various tasks can be significantly improved\u2014so much that investing in improving inference-timecomputation might prove more beneficial than increasing model pre-training compute (Snell et al., 2024).\nDespite this promise, existing work largely considers using inference-time computation as an optionalpost-hoc design choice, after conventional pre-training and fine-tuning. However, decoupling trainingand inference-time computation is not optimal; for example, if we knew that an LLM is allowed tomake multiple attempts to solve a math problem, then it may be better to fine-tune it to explore diverseproblem-solving strategies, rather than simply generating the candidates that represent the model's bestattempt at solving the problem. Within the context of reasoning problems, these performance gains maybe significant, as LLMs often fail due to their inability to draw complex inferences about the input andtheir internal knowledge (Chen et al., 2024).\nWe argue that the effectiveness of inference-time computation can be substantially increased by explicitlyconsidering the inference procedure during training. We study this inference-aware fine-tuning paradigmusing the Best-of-N (BoN) inference strategy, where the LLM generates multiple candidate responses,and a verifier selects the best one according to some scoring function (Cobbe et al., 2021). When thisverifier is the ground-truth scoring function, BoN is equivalent to pass@N, a widely-used method for"}, {"title": "2. Inference-Aware Fine-Tuning: A Case Study with Best-of-N", "content": "Standard fine-tuning methods typically train LLMs to produce the best response for a given prompt.In LLM fine-tuning, a model (or policy) is trained via supervised fine-tuning (SFT), by maximizing thelikelihood w.r.t. ground-truth data. Formally, we search for a policy $\u03c0 : X \u2192 \u2206y$ that maximizes thelikelihood $Ex~P,y~\u03c0* (y|x) [log \u03c0 (y|x)]$, where here, X and Y are the space of prompts and outputs of anLLM, P is the prompt distribution, and $\u03c0*$ is a distribution of expert responses. Alternatively, the policycan be fine-tuned via reinforcement learning (RL) (Schulman et al., 2017): $ma\u0445\u043b\u0454\u043f Ex~P,y~n(x) [R(x, y)]$,to align the LLM's behaviors with the reward function R(x, y). While popular, these methods have nottaken the LLM's inference-time strategies into the account.\nInference-Aware Fine-Tuning. To address the gap between how LLMs are trained and how they are usedat inference time, we develop inference-aware fine-tuning. During inference, the learned policy n is oftennot directly used; rather some inference strategy $I : \u220f \u00d7 X \u2194 \u2206y$ is applied to it. For example, I can be theBoN strategy, which samples multiple candidate responses, and selects the best using the score functionof some verifier; or I might be a search mechanism (Lightman et al., 2023) or self-correction (Kumaret al., 2024). To account for this inference strategy I, we alter the objective SFT and RL objectives to be\u201caware\" of the inference strategy:\n$max Ex~P,y~* (y|x) [log I(\u03c0, x) (y)]$, and\n\u03c0\u0395\u03a0\n(Inference-Aware SFT)\n$max J(\u03c0) := Ex~P,y~1(n,x) [R(x, y)]$,\n(Inference-Aware RL)\n\u03c0\u0395\u03a0\nIndeed, Inference-Aware SFT and Inference-Aware RL are aware of the strategy I. In what follows, we"}, {"title": "3. Supervised BoN-Aware Fine-Tuning", "content": "We begin by developing the BoN-aware SFT framework. Under this setting we assume we do not haveaccess to the true reward, and only wish to maximize the likelihood of a dataset of expert examples.Recall the definition of the BoN policy bon in Equation (1). The Inference-Aware SFT version of BoN is:\n$max E(x,y)~D[log \u03c0bon(y | x; \u03c0) ]$,\n(2)\n\u03c0\u03b5\u03a0\nA major difficulty in solving Equation (2) is the non-differentiability of the arg max operator in the BoNprocedure. To address this, we can use the variational approximation of bon (see Section A.1)\n$\u03c0bon(y|x) x [\u03c0(y|x) \u00b7 exp (\u03bb\u0272Q\u03c0(x, y))]$,\n(3)\nwhere $Qn (x, y) = Ey'\u223c\u03c0(\u00b7|x) [1r(x,y)\u2265r(x,y')] $is the expected win-rate over base policy r, characterizing theprobability for which a response y outperforms the responses generated by the base over the verifierscore r. The constant $\u03bb\u1fc3 > 0$ is a solution of a 1D-search problem (Gui et al., 2024) (see details inAppendix A.1). It can be shown that \u03bb\u06c1 is monotonically increasing in N, and $\u03bb\u06c1 \u221d N$ approximately forlarge N. Plugging the variational form of Equation (3) into Equation (2) yields:\n$max E(x,y)~D [log bon(y|x)] := E(x,y)~D log \u03c0 (y|x) + \u03bb\u03bd \u00b7 Qn (x, y)\u2212log Zn(x)\n\u03c0\u0395\u03a0\n(x)\nLikelihood\nInference-Awareness\nwhere $Z\u300f(x) = \u0395\u03c0(y|x) [exp (\u03bb\u03bd\u00b7 Q\u03c0 (x, y))] $is the partition function.\nThe above optimization problem reveals two term. While the first term tries to push the base policy \u043b"}, {"title": "4. BoN-Aware Fine-Tuning Using Reinforcement Learning", "content": "Training LLMs that are amenable to BoN sampling can be framed within the RL paradigm, which trainsan agent (LLM) that optimizes its actions within an environment. In this context, the LLM generates Nresponses (candidates actions) for a given prompt (contexts). A separate macro agent (verifier) selectsthe candidate deemed most suitable according to a predefined criterion (e.g., probability of success). Thisaction is then deployed to the environment, yielding a reward (e.g., task completion). The key challengein training this agent lies in achieving two objectives simultaneously: (i) Enhancing agent's explorationcapabilities to generate diverse candidates that cover the space of potential solutions and align with theverifier's preferences; (ii) Maximizing the environment reward of the final response. Motivated by thisobservation, we utilize RL for BoN-aware fine-tuning, enabling the development of more robust andadaptable LLMs. A schematic of the BoN-Aware RL framework is shown in Figure 2.\nThe BoN-Aware RL problem takes the following form:\n$max J(\u03c0) := Ex~P,y~rbon (\u00b7|x;r,r,N,T) [R(x, y)]$.\n(6)\n\u03c0\u0395\u03a0\nWe train the BoN policy bon (paramterized by \u03c0) to attain a high environment reward. Apart fromenabling better exploration, using the environment reward R(x, y) in BoN-RL allows the base policyto tolerate potential errors in the verifier r(x, y). We first develop a general algorithm for solving theBoN-aware RL problem. We then study an important subclass which assumes a binary reward, a commonfeature of many reasoning problems (e.g., math, code).\nWe begin with deriving a gradient estimator to the objective in Equation (6). Exploiting the connectionbetween the BoN policy and its energy-based policy counterpart in Equation (10), and using derivationsanalogous to those in Lemma 1, we compute the gradient of J(0), which leads to a REINFORCE-stylealgorithm (Williams, 1992) (see Appendix A.3 for proof):"}, {"title": "6. Related Work", "content": "Large language models (LLMs) can leverage inference-time computation to improve the quality of theirgenerated outputs (Welleck et al., 2024), particularly on reasoning tasks. One common approach isto use chain-of-thought (Wei et al., 2022), where the model generates a step-by-step rationale beforegenerating the final output. Another useful approach that can be combined with chain-of-thought isBest-of-N rejection sampling (Charniak and Johnson, 2005; Stiennon et al., 2020), which is our focus inthis work. In Best-of-N, we generate multiple candidate outputs from an LLM and select the best output.BoN re-ranking can be done either using oracle information, such as checking final answers for solvingmath problems, which is also known as pass@N (Chen et al., 2021), or learned verifiers (Cobbe et al.,2021; Hosseini et al., 2024; Lightman et al., 2023; Zhang et al., 2024). Recent work also empiricallyanalyzes strategies that optimally trade off additional test-time compute for improved performance (Snell\net al., 2024; Wu et al., 2024).\nClosely related to our approach is prior work that fine-tunes LLMs to improve their self-correctioncapabilities (Kumar et al., 2024; Snell et al., 2024) or search capabilities on planning tasks (Gandhi et al.,2024; Lehnert et al., 2024), which allows for more efficient scaling with test-time compute. By contrast,"}, {"title": "7. Conclusion", "content": "We introduced inference-aware fine-tuning, a novel paradigm that bridges the gap between training andinference for LLMs. Specifically for the Best-of-N inference strategy, we discovered a co-scaling law for BoNthat guides the optimization of temperature and sample size, developed a gamut of fine-tuning algorithmsthat handle various imitation learning and reinforcement learning settings, training LLMs to generatediverse and high-quality outputs tailored for BoN inference, demonstrated the efficacy of these methodsby significantly improving on BoN accuracy and pass@N on the standard MATH reasoning benchmarkover state-of-the-art baselines, highlighting the robustness and generalizability of our approaches acrossvarious BoN configurations.\nOur work exemplified how BoN-aware fine-tuning learns a meta-strategy, which interleaves best responseswith more diverse responses that might be better suited for BoN sampling. These findings underscore thepotential of inference-aware fine-tuning to unlock previously undiscovered capabilities in LLMs throughaligning training methodologies with inference-time compute strategies. Future work includes extendingthis framework to incorporate more complex, inference algorithms (e.g., reasoning, critique-and-revise,MCTS), developing contextual BoN-aware algorithms that can generalize to various tasks, investigatingthe interplay between the co-scaling of temperature, sample size, and BoN-aware fine-tuning, andapplying our algorithms to more larger-scale problems."}, {"title": "A. Theoretical Derivations", "content": "We assume that the verifier score r(x, y) is unique for all x, y, and the base model \u03c0 has a finite set ofpossible outcomes for each context (Beirami et al., 2024).\nProposition 5 (Theorem 2 in Gui et al. (2024)). With negligible error, one may effectively approximate\u03c0bon as the solution to the following optimization problem:\n$\u03c0bon (y|x) \u2208 arg max Ey\u223c\u03bc [Qn (x, y)]--KL (\u03bc||\u03c0) (x)$,\n(10)\n\u03bc(x)\u2208\u0394\u03b7\n\u03bb\u03b7\nwhere $Qn (x, y) = Ey'\u223c\u03c0(\u00b7|x) [1r(x,y)\u2265r(x,y')] $is the expected win-rate over \u03c0, and\n$(\u03bb\u2116 \u2212 1) exp (N + 1)$\n$exp \u03bb\u03bc \u2013 1$\n$log$\n$N-1$\n$N$\n$= log N$\n(11)\n$exp (\u03bb\u03bc \u2013 1)$\n\u03bb\u03c0\nthrough \u03bb\u1fc3 scaling sub-linearly with BoN number of samples N.\nWe can show the optimal solution to Equation (10) has a closed form on $\u221d [\u03c0\u00b7 exp (\u03bbNQ\u03c0)](y|x)$. Thiscan also be revealed by viewing Equation (10) as the variational form of Bayes' rule (Dai et al., 2016;Williams, 1980; Zellner, 1988; Zhu et al., 2014), whose optimal solution is the posterior. This impliesbon can be represented by an exponential-twisting policy (Gerber et al., 1993) over base policy n withenergy function \u03bb\u03bd\u00b7 Q\u03c0 (y, x), partition function $Z\u300f(x) = \u0395\u03c0(y|x) [exp (\u03bb\u03bd \u00b7 Qn (x, y))]$, and an appropriateA from Equation (11).\nIn this section we will provide proofs for the technical results in this paper."}, {"title": "B. Pseudo-code and Implementation Details", "content": "Pseudo-code for all our SFT and RL methods is presented in Algorithms 1 to 4. Our implementationfollows the standard use of an anchor policy, updated using exponential moving average. The policy istrained via BoN-aware losses, with additional KL divergence loss to the anchor policy. Table 2 shows thehyper-parameters used for all of our experiments.\nWe use linear annealing for the KL-coefficient. For all our RL experiments, we use a value baselines toreduce variance of our reward estimates. We normalize our advantage estimates w.r.t. the batch. ForBoN-RLB the value network estimates Pfail(x). We add additional clipping of the coefficients g, gn byclipping the value estimates for Pfail."}, {"title": "C. Algorithmic Extensions", "content": "We would like to study an entropy-regularized RL problem for the bon policy. Recall that generally inentropy-regularized RL, we solve\n$max Ex~D [Ey~\u03c0(\u00b7|x) [R(x, y)] \u2013 \u03b2 \u00b7 KL(\u03c0||\u03c0\u03b2)(x)]$,\n(16)\n\u03c0(\u0399\u03a7) \u0395\u0394\nwhere R(x, y) is the environment reward (that is not necessarily identical to the verifier score model),\u03c0\u03b2 is a baseline policy, and $\u1e9e > 0$ is the weight for the KL regularization term. Using the consistencycondition of KL-regularized MDP, solving for the optimal policy of this problem is equivalent to finding asolution pair of V and \u03c0\u2208 \u2206 of the following equation:\n$V(x) = R(x, y) + \u03b2 log \u03c0\u03b2 (y|x) \u2013 \u03b2 log \u03c0(y|x), \u2200x \u2208 D, Vy$\n(17)\nNow, we further parameterize the policy variable n with the BoN policy \u03c0\u266don, then with the sufficiencypart of the consistency condition one can show that bon is an optimal RL policy of Equation (16) if thereexists a pair of V and \u03c0 that satisfies the following equation\n$V(x) = R(x, y) + \u03b2 log \u03c0\u03b2(y|x) \u2013 \u03b2 (log \u03c0(y|x) + \u03bb\u03bd \u2022 Q\u03c0 (y, x) \u2013 log Z\u300f(x)), \u2200x \u2208 D, Vy$\n(18)\nThere are two ways to approximately find the solution in Equation (18). The first way is to reformulatethe above equation with a condition that equates the values between any pairwise states and outputs(x, y, y'):\n$R(x, y') +\u1e9e log$\n$\u03c0\u03b2(y|x)$\n\u00b7 +\u03b2\u03bb\u1fc3\u00b7 Q\u03c0 (\u03b3\u0384, x) = R(x, y) +\u1e9e log$\n$\u03c0\u03c1(\u03b3\u0384|x)$\n\u00b7 +\u03b2\u03bb\u1fc3\u00b7Q\u03c0 (y, x), \u2200x \u2208 D, Vy, y'.$\n(19)\n$\u03c0(y'x)$\n$\u03c0(y|x)$\nSuppose one have access to pairwise labels in the data-set, then this formulation eliminates any termsthat are independent to y and circumvents the need of solving for the value function V. One mayapproximately solve Equation (19) by minimizing the following l2 loss:\n$minE(x,y,y')\u2208D [(g(x, y; \u03c0) \u2013 (g(x, y'; \u03c0))2]$,\n\u03c0\u0395\u0394\n$\u03c0\u03c1(y|x)$\ng(x, y; \u03c0) := R(x, y) + \u1e9e log+ \u03b2\u03bb\u03bd\u00b7 Q\u03c0 (y, x).\n$\u03c0(y|x)$\nThis formulation is similar to that in IPO (Azar et al., 2024). However, unlike IPO, where the termg(x, y; \u03c0) is linear in the logits of n and therefore one can show that its l2 minimization problem has aunique solution, in this case g(x, y; \u03c0) also depends on Q\u016b, which is a function of \u03c0 (and thus a nonlinearfunction of its logits), preventing us from drawing similar conclusions that the l2 minimization problemhas a unique solution. Therefore, even if one can exactly solve this l2 minimization problem (and makethe loss zero), there is no guarantee that the solution policy \u03c0* corresponds to the base policy of anoptimal bon policy to the KL-regularized RL problem.\nFor the second approach, consider the following linear programming reformulation of Equation (18):\n$min ExED [V(x)]$\n(20)\n\u03bd,\u03c0\u0395\u0394\ns.t. V(x) \u2265 R(x, y) + \u03b2 log \u03c0\u03b2(y|x) \u2013 \u03b2 (log \u03c0(y|x) + \u03bb\u03bc \u00b7 Q\u03c0 (y, x) \u2013 log Z\u300f(x)), \u2200x \u2208 D, Vy\nSince the inequality constraint is a convex function in \u03c0 and an affine function in V, by strong duality ithas the following equivalent Lagrangian-dual formulation:\n$max min E(x,y)ED V(x) V(x) +\u043a(x, y). (R(x, y) + \u03b2\u03b2(y|x)$\n\u03ba(\u00b7,\u00b7) \u22650V,\u03c0\u03b5\u0394\n(21)\n-\u03b2 (\u03bb\u03c0. Q\nQ\u03c0 (y, x) - log Zn (x) - V(x))\n=$ max min E [(1 \u2013 \u043a(x, y)) \u00b7 V(x) + \u043a(x, y) \u00b7 (R(x, y) + \u03b2\u00b7 \u03c0\u03c1(y|x))]min E1 [\u03ba(x, y) \u00b7 log abon (y|x; \u03c0)]"}]}