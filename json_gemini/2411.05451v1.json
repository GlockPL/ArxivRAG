{"title": "WORKFLOWLLM: ENHANCING WORKFLOW ORCHES-TRATION CAPABILITY OF LARGE LANGUAGE MODELS", "authors": ["Shengda Fan", "Xin Cong", "Yuepeng Fu", "Zhong Zhang", "Shuyan Zhang", "Yuanwei Liu", "Yesai Wu", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Recent advancements in large language models (LLMs) have driven a revolutionary paradigm shift in process automation from Robotic Process Automation to Agentic Process Automation by automating the workflow orchestration procedure based on LLMs. However, existing LLMs (even the advanced OpenAI GPT-4o) are confined to achieving satisfactory capability in workflow orchestration. To address this limitation, we present WorkflowLLM, a data-centric framework elaborately designed to enhance the capability of LLMs in workflow orchestration. It first constructs a large-scale fine-tuning dataset WorkflowBench with 106, 763 samples, covering 1, 503 APIs from 83 applications across 28 categories. Specifically, the construction process can be divided into three phases: (1) Data Collection: we collect real-world workflow data from Apple Shortcuts and RoutineHub, transcribing them into Python-style code. We further equip them with generated hierarchical thought via ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task queries to enrich the diversity and complexity of workflows. (3) Workflow Generation: we leverage an annotator model trained on collected data to generate workflows for synthesized queries. Finally, we merge the synthetic samples that pass quality confirmation with the collected samples to obtain the WorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain WorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong capacity to orchestrate complex workflows, while also achieving notable generalization performance on previously unseen APIs. Additionally, WorkflowBench exhibits robust zero-shot generalization capabilities on an out-of-distribution task planning dataset, T-Eval. Our data and code are available at https://github.com/OpenBMB/WorkflowLLM.", "sections": [{"title": "1 INTRODUCTION", "content": "Process Automation (PA) (Cichocki et al., 1997), as a long-standing pursuit of the human race, aims to automate repetitive tasks to minimize human labor and improve efficiency. Tracing back to the agricultural era, humanity has employed waterwheels and oxen to automate farming practices. Robotic Process Automation (RPA), the current predominant PA technique, abstracts the repetitive task into a workflow (i.e., a program that can execute automatically) by orchestrating various actions (e.g., functions or APIs) (Ivan\u010di\u0107 et al., 2019; Hofmann et al., 2020; Wewerka & Reichert, 2020; Agostinelli et al., 2020; Ferreira et al., 2020). While RPA successfully reduces the human labor via automated workflow execution, the process of orchestrating workflows still requires substantial manual effort. Recently, large language models (LLMs) (OpenAI, 2022; 2023; Touvron et al., 2023a;b; Dubey et al., 2024) have achieved remarkable performance beyond natural language processing (Ahn et al., 2022; Cheng et al., 2023; Qian et al., 2024). The emergence of LLMs has unveiled a paradigm shift trend, moving from Robotic Process Automation to Agentic Process Automation (APA) (Ye et al., 2023; Zeng et al., 2023; Huang et al., 2024; Wornow et al., 2024; Li et al., 2024) which automates the workflow orchestration process by utilizing LLMs to build the workflow."}, {"title": "2 RELATED WORK", "content": "Process Automation RPA has gained considerable attention for automating repetitive tasks in various productivity scenarios (Ivan\u010di\u0107 et al., 2019; Hofmann et al., 2020; Wewerka & Reichert, 2020; Agostinelli et al., 2020; Ferreira et al., 2020). RPA predominantly relies on handcrafted workflows (e.g., programming, recording human behavior), making them highly suitable for automating well-structured, routine processes (Herm et al., 2020). However, such approaches require substantial efforts and in-depth domain expertise, resulting in high setup costs and limited adaptability. Recent advancements in LLMs have spurred interest in integrating these models into RPA to enhance flexibility and reduce dependency on manual workflow creation. Ye et al. (2023) introduced the concept of APA, which utilizes LLMs to autonomously orchestrate workflows based on human instructions. Subsequently, several studies have sought to apply APA in various domains, including travel planning (Xie et al., 2024), smartphone applications (Huang et al., 2024), enterprise automation (Wornow et al., 2024), financial question answering (Zeng et al., 2023), and data analysis (Li et al., 2024). Despite relying on advanced LLMs (e.g., GPT-4), these approaches have often exhibited suboptimal performance, highlighting challenges faced by existing LLMs in workflow orchestration. While Li et al. (2024) made an effort to fine-tune Mixtral-8\u00d77B (Jiang et al., 2024), it could only orchestrate sequential workflows with an average of 15.6 actions, remaining insufficient for real-world requirements. This work addresses a critical gap by proposing WorkflowLLM framework to enhance the workflow orchestration capabilities of LLMs to meet real-world demands.\nTool Learning Workflow orchestration driven by LLMs frequently depends on external tools, such as APIs, to extend their operational capabilities. Recent studies have demonstrated that LLMs can effectively acquire and utilize external tools by learning from their documentation, thereby solving complex tasks that would otherwise be beyond the model's native capabilities (Wu et al., 2023; Schick et al., 2024; Qin et al., 2023b; 2024). This integration enables LLMs to access real-time knowledge and perform specialized operations, particularly for executing intricate processes (Yang et al., 2023; Nakano et al., 2021; Qin et al., 2023a; Wang et al., 2024c; Gao et al., 2023). To further enhance this capability, several efforts have introduced datasets specifically designed to fine-tune LLMs for tool interactions (Zhuang et al., 2024; Qin et al., 2024; Wang et al., 2024a). However, these datasets are often constrained to limited actions scale, thus limiting their effectiveness for managing complex, real-world workflows. Compared to tool learning scenarios, orchestrating workflows demands more sophisticated planning and reasoning that current LLMs have yet to fully realize. In response to these limitations, we present WorkflowLLM to significantly improve LLMs' capabilities in workflow orchestration. Besides, Shen et al. (2024) also used Apple's Shortcuts but aimed to assess LLMs' tool utilization ability. In contrast, we emphasize a different scenario, workflow orchestration and aim to enhance the workflow orchestration ability rather than evaluation alone."}, {"title": "3 WORKFLOWLLM", "content": "As Figure 2 shows, WorkflowLLM introduces a data-centric framework to enhance the capability of LLMs in workflow orchestration by constructing a high-quality supervised fine-tuning dataset WorkflowBench. In this section, we outline the dataset construction process, which is carried out in three distinct phases: Data Collection, Query Expansion, and Workflow Generation."}, {"title": "3.1 DATA COLLECTION", "content": "We first give the introduction to Apple Shortcuts and RoutineHub, and describe how we crawl and filter to get high-quality data. We then convert the shortcuts into Python-style workflow code. Inspired by Chain-of-Thought (Wei et al., 2022; Chen et al., 2023), we prompt ChatGPT to generate hierarchical thoughts, including comments, task plans, and task queries, progressing from fine-grained to coarse-grained details for each shortcut.\nApple Shortcuts and RoutineHub Apple Shortcuts, as a representative application of RPA, is developed by Apple Inc. This tool facilitates the automation of a series of actions, enabling users to efficiently perform a diverse range of tasks. The actions within Shortcuts are APIs provided by both built-in Apple applications, such as Safari, and third-party applications like OpenAI. Each application may provide multiple actions. For instance, OpenAI provides APIs that facilitate voice conversations and text interactions with ChatGPT. Through a simple drag-and-drop interface, users can construct complex workflows, such as navigating to the nearest coffee shop or downloading watermark-free images from TikTok.\nRoutineHub\u00b9 is a prominent community for sharing shortcuts, with a collection of thousands of shortcuts across both iOS and macOS platforms. All shortcuts on RoutineHub are categorized into 28 workflow categories (e.g., Business, Health & Fitness, Productivity, etc). RoutineHub records the metadata of each shortcut (e.g., title, description, iCloud URL), providing valuable information.\nCrawling and Filtering For each shortcut, we crawl the title, developer-provided description, and iCloud URL linked to Apple. As RoutineHub does not provide the source code for these shortcuts, we further crawl it from their iCloud URLs. Besides, we merge shortcuts collected by ShortcutsBench (Shen et al., 2024), sourced from platforms like ShareShortcuts\u00b2 and MacStories\u00b3, to further expand the scale of our dataset. However, the source code of these shortcuts lacks detailed information about the involved actions, such as API metadata. Inspired by ShortcutsBench (Shen et al., 2024), we extract action information from macOS's built-in definition files and third-party application interface definition files. For each API, we record its name, description, parameter names, parameter types, default values, return value types, and return value name, which provides a valuable resource for LLMs to efficiently interpret and utilize these APIs, even in zero-shot scenarios.\nTo ensure compatibility between the crawled shortcuts and the action interfaces, we implement a stringent filtering mechanism to verify that all API calls are executed correctly. During this process, we identify that some shortcuts contain non-interpretable binary sequences as API parameters, potentially disrupting the training process of language models. To maintain data quality, we remove these samples from the dataset. As a result, we curate a final set of 14, 771 high-quality shortcuts, ensuring the reliability of the dataset for subsequent data expansion and model training.\nShortcuts Transcription The original shortcut source codes are written in property lists format (Hummert & Humphries, 2022), which sequentially encodes logical constructs like branches and loops. This encoding is notably different from the types of data commonly used in the pre-"}, {"title": "Thought Generation", "content": "To provide informative guidance for LLMs in orchestrating workflows, we design a three-level thought hierarchy from fine-grained to coarse-grained: (1) Low-level comments are intended to clarify the purpose of each action within the workflow. (2) Median-level plans represent an abstraction over a sequence of actions, outlining the collective goal of these steps. (3) High-level queries reflect the user's requirements, specifying the intended outcome without prescribing specific methods to achieve it. These three levels of thought are generated through a bottom-up approach. Specifically, given the transcribed workflow w, let the set of actions in the workflow w be denoted as A, where each action \\(a_i \\in A\\) corresponds to a function calling in the Python code. For each action \\(a_i\\), we generate a corresponding comment \\(c_i\\) by prompting ChatGPT. Subsequently, given the action set A = {\\(a_i\\)} and comments C = {\\(c_i\\)} of workflow w, we prompt ChatGPT to generate the corresponding task plan P. We combine the task plan P, the comments C, and the action set A of the workflow w to generate the high-level task query Q. This bottom-up manner is analogous to the summarization task, effectively ensuring content reliability and minimizing the risk of hallucination.\nFinally, as Figure 3 shows, each workflow w is represented as: w = {Q,D, P, A}, where the workflow w consists of the task query Q, action documentation D for all involved actions, the task plan P, and all actions represented as annotated Python code A. A detailed example can be found in Appendix D."}, {"title": "3.2 QUERY EXPANSION", "content": "After performing a comprehensive statistical analysis on the collected data, we find that the data exhibits significant complexity, with an average of 70.4 actions and 12 branches, surpassing the complexity of existing workflow-related benchmarks. However, the diversity of the data is relatively low. Specifically, 40.3% of the workflows fall under the Utilities category, and over 99% of the APIs used are Apple's built-in APIs (i.e., those classified as is_workflow_actions APP).\nTherefore, we intend to expand the dataset by focusing on two key aspects: (1) Diversity: making up for the lack of diversity in real data and covering a broad range of APIs and workflow categories to enhance the model's utility and robustness; (2) Complexity: matching the action scale and logical complexity of the real-world data to ensure that they can effectively represent real-world problems and orchestrate nodes accordingly. To this end, we sample APIs from diverse applications and multiple workflows with representative logical structures (e.g., whether they contain branches or loops) to synthesize additional queries."}, {"title": "3.3 WORKFLOW GENERATION", "content": "To annotate the corresponding workflows of the synthesized queries effectively, we train an annotator model based on the collected shortcuts data to support more diverse applications and categories, while ensuring consistency with the real-world data as much as possible.\nAnnotator Training First, we construct the supervised fine-tuning (SFT) dataset based on the collected human-labeled shortcuts. Specifically, each workflow data point comprises a query Q, the corresponding action documentation D, the task plan P, and the workflow represented as annotated Python code Acommented. During the SFT process, as shown in Figure 3, we take the query Q, the corresponding action documentation D as the input to guide the model to generate a task plan P, followed by the step-by-step generation of the current thought (i.e., the comment ci) and the corresponding action ai, which includes the action name and its associated parameters. We use the trained annotator to generate workflows A' from synthesized queries.\nQuality Confirmation Due to the limited accuracy of the annotator model, the generated workflows may contain errors to some extent. For example, we identify issues in A' (e.g., extraneous branches not relevant to the query and incorrect function call formats). To enhance the overall quality, we prompt ChatGPT with in-context samples to refine both A'commented and P', ensuring that the workflow accurately addresses the query. Then, we use rule-based filtering to remove workflows with fundamental errors. Specifically, we remove samples that don't incorporate code, don't utilize the given APIs, or violate parameter constraints associated with those APIs.\nFinally, we derive a synthesized dataset of 91, 992 instances, which is combined with the initially collected data to form the final WorkflowBench. It contains 106, 763 instances with 1,503 APIs across 83 applications, which are used to train WorkflowLlama. The statistics of WorkflowBench are listed in Table 1 and the distribution comparisons of workflow categories, APPs, and the number of actions between the collected data and final data are demonstrated in Figure 4. From the statistical results, we can see that the synthetic data maintains complexity while expanding diversity."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUP\nTraining Details We fine-tune the annotator and WorkflowLlama on LLaMA-3.1-8B (Dubey et al., 2024) for 3 epochs using the AdamW optimizer (Loshchilov & Hutter, 2019). A linear learning rate scheduler is used with a peak learning rate of 2 \u00d7 10-5 and a warm-up ratio of 0.1. Each mini-batch contains 32 examples, and the maximum sequence length is set as 8, 192 tokens.\nBaselines To provide a comprehensive comparison, we select several representative LLMs as baselines for our experiments. These baselines include proprietary models such as GPT-4o-mini and GPT-4o, as well as open-source models like Qwen2-7B (qwe, 2024), Llama-3.1-8B, and Llama-3.1-70B (Dubey et al., 2024). Additionally, we apply in-context learning (ICL) (Dong et al., 2022) with one random-sampled instance to these baselines to better adapt them for workflow orchestration.\nMetrics In the main experiments, we use both reference-code-based metrics and a model-based evaluation to comprehensively evaluate the quality of the generated workflows. For reference-based metrics, we apply CodeBLEU (Ren et al., 2020) with four components:\nBLEU measures N-gram overlap for token-level similarity.\nWeighted N-Gram Match assigns higher weights to critical code tokens like keywords.\nSyntactic AST Match compares the Abstract Syntax Trees (ASTs) to assess syntactic accuracy.\nSemantic Data-Flow Match evaluates logical correctness by comparing data-flow relationships between variables.\nTogether, these components provide a comprehensive evaluation of both syntactic and semantic aspects of the workflows. We follow Ren et al. (2020), setting the four components to 0.1, 0.1, 0.4, and 0.4, respectively, and calculate a weighted sum to obtain the CodeBLEU score. For model-based evaluation, we elaborately prompt ChatGPT as the automatic evaluator to evaluate the Pass Rate of the generated workflows."}, {"title": "4.2 EFFECTIVENESS OF EVALUATOR", "content": "To validate the reliability of the ChatGPT evaluator in terms of Pass Rate, we sample 30 instruction-response pairs (i.e., task queries and their corresponding workflow codes) for each model in Table 2, forming a human-evaluated dataset of 330 instances (30 \u00d7 11 = 330). First, we use GPT-4o-mini to label whether each instance could complete the given tasks only using the provided APIs. Then, human evaluators re-label the sampled data according to the same criteria. Ultimately, 268 instances are labeled consistently by both the ChatGPT evaluator and human evaluators, achieving an agreement rate of 81.2%, demonstrating the reliability and effectiveness of the evaluator."}, {"title": "4.3 MAIN EXPERIMENTS", "content": "Settings The main experiments are conducted using the test set of WorkflowBench. Ideally, by scaling both the quantity and diversity of instructions and unique tools within the training data, WorkflowLlama is expected to generalize to novel instructions and APIs that are not seen during training. This is particularly important because it enables users to define custom APIs and allows WorkflowLlama to adapt based solely on the provided documentation. To evaluate this capability, we assess WorkflowLlama's generalization performance at two levels: (1) Unseen Instructions, considers an In-Distribution (ID) setting, which involves using the same set of APIs as those in the training data, and (2) Unseen APIs, considers an Out-Of-Distribution (OOD) setting, involving only 50 common APIs required to construct workflows and APIs that are absent from the training data. Since WorkflowBench contains a comprehensive set of APIs, which poses a substantial challenge for LLMs in terms of API comprehension and selection, we provide the correct APIs directly as input. It allows us to focus on the workflow orchestration, bypassing the issue of API selection.\nMain Results The results are placed in Table 2, from which we derive that:\nAlthough multiple workflows can successfully complete a query, there is a positive correlation between the reference-free Pass Rate metric and the reference-based CodeBLEU metric. Given that the Pass Rate metric derived from ChatGPT aligns with human evaluations over 80% of the time, CodeBLEU serves as a reliable proxy for evaluating workflow orchestration capabilities.\nAll models demonstrate a certain capacity for workflow orchestration. This may stem from their inherent instruction-following and code-generation capabilities. We find that models like GPT-4o and Llama-3.1-70B, which perform better on generic tasks, also excel in workflow orchestration. In addition, prompting with in-context samples significantly enhances the models' performance.\nWe find that scores on text overlap metrics such as BLEU and weighted N-gram are low for all models. Even the fine-tuned WorkflowLlama only achieves 8.2% and 9.7% on these two metrics. This is because the reference codes consist mainly of workflows with function names and arguments, and contain few Python-related keywords, making exact matching challenging. In contrast, models achieve better scores on syntactic AST match and semantic data-flow match.\nAfter fine-tuning, WorkflowLlama shows a significant improvement in its ability to orchestrate actions. The performance of WorkflowLlama even outperforms powerful closed-source models GPT-4o with ICL by a large margin. Specifically, WorkflowLlama achieves a 39.3% score on CodeBLEU and a 76.9% Pass Rate under ID settings, demonstrating the validity of our proposed WorkflowLLM framework and WorkflowBench dataset.\nWorkflowLlama demonstrates strong generalization capabilities. Even though it has not been trained on the same instructions or APIs, it still significantly outperforms the vanilla Llama-3.1"}, {"title": "4.4 ANALYSIS OF WORKFLOW COMPLEXITY", "content": "To evaluate the models' ability to generate workflows of varying complexity, we break down the performance of CodeBLEU according to the total number of actions, the number of branches and loops, and the nested depth of the reference code. As shown in Figure 5, the performance of all models deteriorates as the number of actions or the logical complexity increases, indicating the challenge of orchestrating complex workflows. However, across all levels of complexity, WorkflowLlama significantly outperforms all other models. Moreover, the relative performance of WorkflowLlama improves as the complexity of the workflow increases, which demonstrates fine-tuning with WorkflowBench significantly enhances the model's ability to handle more complex workflows."}, {"title": "4.5 OUT-OF-DISTRIBUTION GENERALIZATION TO T-EVAL (CHEN ET AL., 2024)", "content": "Settings To further evaluate the generalization capability of WorkflowLlama, we conduct experiments using an OOD benchmark, T-Eval, a widely-used benchmark to evaluate the multi-step decision-making capability of LLMs to utilize APIs. The original data format in T-Eval is based on JSON or strings, which differ significantly from the Python-based format employed in WorkflowBench. To ensure the evaluation metrics' consistency between ours and the original paper, we convert WorkflowBench into JSON format while preserving the metadata of workflows and the specifics of queries. Subsequently, we retrain WorkflowLlama on the transformed dataset. We employ the F1 Score proposed in the original paper to measure the alignment with the reference API sequences.\nResults The results are shown in Table 3. As observed, WorkflowLlama demonstrates strong OOD generalization performance on the T-Eval benchmark, despite being trained on different domains and tasks using different APIs. Notably, WorkflowLlama significantly outperforms the vanilla Llama3.1-8B as well as larger open-source models like Llama-2-70B and Qwen-72B, highlighting that fine-tuning with WorkflowBench enhances the model's out-of-distribution planning ability."}, {"title": "4.6 ABLATION STUDY", "content": "Settings To assess the efficacy of WorflowBench's components, we conduct an ablation study under the settings of unseen instructions (i.e., the ID setting).\nResults Table 4 presents the performance results when the model is trained under different conditions: without synthetic data, without the task plan P, without action-level comments C, and without both C and P. The experimental results reveal two key findings. First, the two types of natural language thoughts enhance the reasoning capabilities of the model. Removing either type of thought leads to a decline in CodeBLEU performance. Second, training on large-scale synthetic data further improves performance, highlighting the effectiveness of the WorkflowBench expansion process."}, {"title": "4.7 CASE STUDY", "content": "To further illustrate the effect of fine-tuning on WorkflowBench, we present a typical example in Figure 6. In this case, the vanilla Llama-3.1 model exhibits two types of errors. First, the model does not adhere to the given instructions for workflow orchestration, using APIs outside the provided list, i.e., hallucination APIs. Specifically, it uses the time.sleep() function instead of is_workflow_actions_delay () to set a timer. Second, due to its relatively weak workflow orchestration capabilities, the model fails to complete all user instructions. Specifically, it does not activate airplane mode using the is workflow_actions-airplanemode_set() function. Fine-tuning on WorkflowBench effectively alleviates these two issues. However, we observe that fine-tuning also introduces redundant actions. For instance, WorkflowLlama repeats the parsing check of the clipboard's content. We will address this redundancy problem in future work."}, {"title": "5 CONCLUSION", "content": "In this paper, we present WorkflowLLM to enhance the capability of large language models in workflow orchestration. In WorkflowLLM, WorkflowBench is constructed covering 106, 763 workflows with 1,503 APIs across 83 applications through a three-phase pipeline. By fine-tuning Llama-3.1-"}, {"title": "E LIMITATIONS", "content": "While the framework proposed in this paper represents notable progress in workflow orchestration, it also has certain limitations that warrant discussion. First, the APIs used in our work are exclusively derived from Apple Shortcuts application, resulting in a lack of coverage across more diverse fields and potentially limiting the generalizability of the dataset to broader application contexts. Second, our approach lacks evaluation through actual execution. This limitation arises due to the complexities involved in executing workflows, such as the need for intricate user registration and permission acquisition. Moreover, the APIs are subject to frequent changes as applications continue to evolve, making it challenging to implement a consistent executable evaluation. Consequently, our evaluation is limited to static analysis."}, {"title": "F ETHICAL STATEMENT", "content": "In this study, the dataset construction process was fully automated using LLMs and algorithms for data annotation, eliminating the need for human annotators and thereby avoiding concerns related to annotator compensation and working conditions. The data utilized was collected through web scraping from publicly accessible sources, with strict adherence to the Terms of Service (ToS) of the respective websites. Scraping was avoided on platforms where such activity is explicitly prohibited, ensuring compliance with ethical standards. Additionally, no personally identifiable information (PII) or private user data was collected at any stage of the research process. All data was anonymized to protect privacy and mitigate any potential ethical concerns related to user information."}, {"title": "A ALGORITHM OF TRANSCRIBING SHORTCUTS", "content": "Algorithm 1: Recursive Parsing of Property List to Construct Abstract Syntax Tree\nData: Shortcut file to be transcribed\nResult: Abstract syntax tree of the actions\nInitialize an empty tree with a root node and set current_node to root\nforeach action in action list do\nDetermine action type and mode from action\nif action_type is Conditional then\nHandleConditional(mode, action)\nelse if action_type is RepeatEach then\nHandleLoop(mode, action)\nelse if action_type is RepeatCount then\nHandleLoop(mode, action)\nelse if action_type is ChooseFromMenu then\nHandleMatchCase(mode, action)\nelse\n| HandleDefault(action)\nFunction HandleConditional(mode, action):\nif mode == 0 (start if) then\nAddNode(action)\nSet current_node to new node\nelse if mode == 1 (else) then\nMove current_node to parent node\nAddNode(action)\nSet current_node to new node\nelse if mode == 2 (end if) then\nMove current_node to parent node\nFunction HandleLoop(mode, action):\nif mode == 0 (start loop) then\nAddNode(action)\nSet current_node to new node\nelse if mode == 2 (end loop) then\nMove current_node to parent node\nFunction HandleMatchCase(mode, action):\nif mode == 0 (start match) then\nAddNode(action)\nSet current_node to new node\nelse if mode == 1 (start case) then\nif current_node is match node then\nAddNode(action)\nSet current_node to new node\nelse\nMove current node to parent match node\nAddNode(action)\nSet current_node to new node\nelse if mode == 2 (end match) then\nMove current_node to parent node\nFunction HandleDefault(action):\n| AddNode(action)\nFunction AddNode(action):\nCreate new node with action\nAppend new node to current_node.children\nSet parent of new node to current_node"}, {"title": "\u0412 PROMPT DESIGN", "content": "\u0412.1 WORKFLOW ORCHESTRATION PROMPT\nYou are a very helpful AI assistant who can write corresponding\nPython main code based on user's query and usable Python\nfunction interface.\nPlease generate python main code based on the following query :\n{query}\nYou can start by using natural language to plan your tool call\nstrategy, and then generate the complete code. For example,\nThought:\n<tool call strategy>\nCode:\npython\n<main code>\nNote that your output should always include `Code:\n```python\n<main code>\n`, formatted accordingly.\nHere are some useful function interface you may use:\n{apis_docs}\n\u0412.2 EVALUATOR PROMPT\nYou are a kindly code reviewer, I will provide you with a query, a\nlist of allowed apis and a piece of code to be reviewed, you\nhelp me to check if the code to be reviewed is compliant with\nour specifications.\nThe requirements are as follows:\n1. You **should return True even if the code implements additional\nfunctionality not required in the query**, as long as it\nroughly implements the requirements in the query.\n2. We don't impose any requirements on code readability or naming\nconventions. You **should return True as long as the reviewed\ncode doesn't use disallowed functions and reasonably\naccomplishes what is asked in the query in general terms**.\nThere's no need to get strictly hung up on the details.\n3. Return False if the code fails to fulfill the requirement in\nthe query. e.g. if it is proposed in the query to turn down\nthe battery level of the phone and the brightness of the\nscreen, it is a failure to fulfill only any one of the\nfunctions.\n4. Built-in python syntax such as `if`, `loop`, `input()`, and\nprint()` are allowed. Return False if the code uses **any\nexternal functions or apis** not in allowed apis list and not\na built-in function such as input(), print(). For example, if\nI provide the is_workflow_openurl function, this should be\nused. Any use of any other library like requests etc. is a\nFalse.\nquery:{query}\nlist of allowed apis: {apis}\ncode to review: {code}\nYour answer: [True or False with interpretation]"}, {"title": "\u0412.3 \u0421\u043eMMENT GENERATION PROMPT", "content": "A Shortcut is a sequence of actons, where each action is an API\ncall, to execute user-provided queries.\nAs a user-friendly and patient assistant, your task is to provide\na set of description of each line of the code scrippet. \u03a4\u03bf\nsave time, I have retrieved all the lines exclusive of blank\nlines of the code snippet and listed as a dictionary below the\ncode.\nYour answer should be in the json format as follows:\n{\n`json\n\"line x\": \"<description-of-line-x>\",\n\"line x+1\": \"<description-of-line-x+1>\",\n\"...\":\n\"line x+n\": \"<description-of-line-x+n>\"\n}`\nThe code is :\n{code}\nThe lines are {lines}"}, {"title": "B.4 TASK PLAN GENERATION PROMPT", "content": "Based on this line by line description of the code, generate a\nflowchart of a workflow by natural language.\nThis is the code:\n{code}"}, {"title": "B.5 TASK QUERY GENERATION PROMPT", "content": "As a helpful assistant, please help me craft a query. This query,\nformatted as a question, should describe the task a user wants\nto complete and adhere to the following criteria:\n1. One of the solution to the task described in the query could be\nthe python code below.\n2. It should be close to real-world problems or requests.\n3. It should include major parts of the code.\n4. The query should not specify python.\nFor example, the code is:\n{ICL_code }\nAnd the expected output query should be similar to:\n{ICL_query}\nPlease craft a query based on the examples and the following code:\n{code}"}, {"title": "B.6 QUERY EXPANSION PROMPT", "content": "You are exceptionally skilled at crafting real-world user queries\ngiven some apis. Here are examples:{examples}. Please gain\ninspiration from the following api docs to create a high-\nquality realworld query.\nApi docs for inspiration:\npython"}, {"title": "B.7 QUALITY CONFIRMATION PROMPT", "content": "You are exceptionally skilled at polishing tool calling plan (i.e\nthought) and python code given a task.\nGiven task:\n{query}\nOld tool calling plan:\n{thought}\nOld code:\n{code}\nUsed API doc:\n{apis}\nHere are examples for you to refer:{ICL_context}.\nPlease make sure the code is logically correct and operational.\nRequirements:\n[1] Ensure that both plan and code respond correctly to the task\nand that code calls match the plan, which you can do by\ntweaking, embellishing, and modifying both plan and code.\nPlan does not have to be one-to-one correspondence of code; plan\ncan be abbreviated.\n[2] Please ensure that the code conforms to python syntax. Ensure\nthat all python code is complete and runnable. You can add\ncode when necessary.\n[3] Every line of code should be preceded by a comment marked with\na \"#\". When modifying the code, please modify the in-line\ncomments accordingly.\n[4] Ensure that all function parameter calls are correct and you\ncan change the code in case of errors.\n[5] Thought and code should be as concise while keeping the\nmeaning intact.\n[6] If there are cases including invalid binary code, replace them\nwith reasonable text, delete them, or replace them with a\nreading operation on a file (especially when the binary code\nis an encoded image).\nRespond strictly with JSON."}, {"title": "B.8 VARIABLE RENAME PROMPT", "content": "You are a helpful assistant for renaming variable names in a code\nsnippet.\nThe following code snippet is a part of a program, and variables\nare named in format 'variablex_'."}, {"title": "C CASE STUDY OF SHORTCUTS", "content": "We provide a real-world shortcut example", "forms": "nthe rwa property list configuration file, the Python code after transcription and variable renaming,\nand the visual interface"}]}