{"title": "AutoCas: Autoregressive Cascade Predictor in Social Networks via Large Language Models", "authors": ["Yuhao Zheng", "Chenghua Gong", "Rui Sun", "Juyuan Zhang", "Liming Pan", "Linyuan L\u00fc"], "abstract": "Popularity prediction in information cascades plays a crucial role in social computing, with broad applications in viral marketing, misinformation control, and content recommendation. However, information propagation mechanisms, user behavior, and temporal activity patterns exhibit significant diversity, necessitating a foundational model capable of adapting to such variations. At the same time, the amount of available cascade data remains relatively limited compared to the vast datasets used for training large language models (LLMs). Recent studies have demonstrated the feasibility of leveraging LLMs for time-series prediction by exploiting commonalities across different time-series domains. Building on this insight, we introduce the Autoregressive Information Cascade Predictor (AutoCas), an LLM-enhanced model designed specifically for cascade popularity prediction. Unlike natural language sequences, cascade data is characterized by complex local topologies, diffusion contexts, and evolving dynamics, requiring specialized adaptations for effective LLM integration. To address these challenges, we first tokenize cascade data to align it with sequence modeling principles. Next, we reformulate cascade diffusion as an autoregressive modeling task to fully harness the architectural strengths of LLMs. Beyond conventional approaches, we further introduce prompt learning to enhance the synergy between LLMs and cascade prediction. Extensive experiments demonstrate that AutoCas significantly outperforms baseline models in cascade popularity prediction while exhibiting scaling behavior inherited from LLMs. Code is available at this repository: https://anonymous.4open.science/r/AutoCas-85C6", "sections": [{"title": "1 Introduction", "content": "As social media platforms such as Twitter, TikTok, Rednote, WeiBo, and Reddit become increasingly embedded in our daily lives, they enable a continuous creation and widespread dissemination of vast amounts of user-generated content. User activity patterns within these networks play a pivotal role in shaping the spread of information, often giving rise to large-scale information cascades [1]. Effectively modeling these cascades to predict the trajectory of information diffusion is of paramount importance across various real-world applications, including fake news detection [2], viral marketing [3], epidemic prevention [4], and recommender systems [5], among others.\nAmong information cascade modeling tasks, cascade popularity prediction has attracted widespread attention from both academia and industry [6\u20139]. An illustration of the information cascade modeling task is shown in Figure 1: given the observed propagation status of an information item, the target is to model its diffusion process and predict the future cascade size [9]. Previous approaches can be summarized into the following three categories.\n(1) Feature-based methods. Early studies [10] focus on extracting hand-crafted features from cascades, e.g., structural features, temporal features, user-related features, etc for cascade prediction.\n(2) Statistics-based methods. These studies model the information diffusion process via the intensity function of the arrival for incoming messages, e.g., Poisson [11] and Hawkes [12] processes.\n(3) Deep learning methods. This type studies explore the potential of deep learning techniques in cascade popularity prediction. Most of them integrate the temporal information, structural information, and complex evolution patterns of cascades with RNNs [7], GNNs [8, 13], Transformer [14], and Neural ODEs [15], among others. Due to powerful generalization capabilities, deep learning methods have become the current mainstream approach in information cascade modeling.\nThe success of LLMs highlights the transformative potential of foundational models across diverse fields, including graph [16, 17], geography [18], and seismology [19]. In the context of information cascades, propagation mechanisms, user behavior, and temporal activity patterns exhibit both universal patterns and significant variability, necessitating a foundational model capable of capturing shared patterns while adapting to diverse dynamics. However, the development of such a model is constrained by the limited availability of cascade data, posing challenges for effective training. Fortunately, recent studies in vision [20], and time-series [21] have demonstrated the feasibility of task reformulation and repurpose LLMs for general task adaptation. Therefore, a natural question arises: Can we adopt powerful LLMs to perform information cascade modeling?\nIntuitively, language and cascade generation share basic commonalities, presenting opportunities to repurpose the off-the-shelf LLMs for cascade modeling. As can be seen in Figure 2, both language generation and cascade diffusion can be regarded as sequence modeling at the token level. Similar observations can be found in [21], where time-series and language are unified via token transitions in sequence modeling to repurpose LLMs as forcasters. Ideally, if cascade data can be tokenized into cascade tokens and aligned with the linguistic embedding space, it would be feasible to repurpose LLMs as the cascade predictor. Nevertheless, unlike text sequence and time-series patch, the cascade data possesses the inherent local topological structure, complex global interactions, and the dynamics of cascade diffusion. Therefore, one challenge of applying LLMs to information cascade modeling lies in whether cascade data can be tokenized to make it compatible with LLM architecture?\nPrior studies have revealed that powerful generalization of LLMs largely stems from the autoregressive modeling paradigm [22]. In essence, autoregressive modeling involves predicting the next value in a sequence based on previous data points [23], which is highly compatible with language generation and has already been the best practice for building LLMs currently [24, 25]. To maximizing its potential, studies which repurpose LLM as the foundational base in fields such as vison [26, 27] and time-series [21] endeavor to reconfigure task formulations, transforming them into autoregressive templates. Notably, the expansion of information cascade is dependent on its historical states, making autoregressive cascade modeling feasible. Consequently, the key to effectively utilizing LLMs for cascade modeling hinges on whether the cascade diffusion process can be reformulated as autoregressive modeling?\nTo further investigate the diffusion trends of information cascade, we place greater emphasis on popularity prediction based on cascade modeling. Compared to conventional deep learning methods, the repurpose of LLM as predictor is likely to cause inadequate task adaptation for cascade popularity forecast. Inspired by the remarkable success in NLP, prompting learning [28] has advanced in facilitating the integration of LLMs into specific tasks across various domains [29, 30]. Since LLMs exhibit preferences towards textual prompts, appropriate textual prompt templates should be deeply coordinated with the cascade data for subsequent predictions. We expect that cascade data can be prompted by relevant contexts, so this last obstacle lies in whether we can integrate LLMs into cascade popularity prediction task via prompt learning?\nIn this paper, we repurpose the LLMs as cascade predictors, and present a novel framework called AutoCas. Technically, we first"}, {"title": "2 Related Works", "content": "Information cascade modeling can be primarily categorized into two types: micro-level [31, 32] and macro-level [6, 8]. The former focuses on predicting next affected user, while the latter concentrates on the overall trends, such as its popularity or outbreak status. In this paper, we focus on the cascade popularity prediction and categorize existing methods into three types:\nFeature-based methods. These works focus on making hand-crafted features for cascade and conducting popularity prediction via traditional machine learning approaches [10]. However, feature-based methods heavily rely on the expert knowledge, has high customization costs, and exhibits limited generalization and suboptimal performance [31].\nStatistics-based methods. These studies assume that information diffusion follow a specific probability statistical model, such as the Poisson process [11], Hawkes process [12]. Statistics-based methods are interpretable but have strong parametric assumptions, making them unsuitable for real-word applications [9].\nDeep leaning methods. These methods adopt deep learning techniques to promote cascade popularity prediction. Early representative works, such as DeepCas [6] and DeepHawkes [33], focus on capturing the temporal dynamics of cascades via RNN or LSTM. Considering the topology in information cascade, GNNs have been introduced to capture local structural patterns. For example, CasCN [8] and CoupledGNN [13] adopt variant GNNs to model the interactions between users and the spread influence. Apart from local\nstructure, CasFlow [7] introduces the social network as global context to enhance popularity prediction. Advanced techniques, such as VAEs [34?], Transformers [14, 35], and Neural ODEs [36, 37], have been further explored in cascade modeling. For more comprehensive reviews, please refer to [9, 38].\nTowards LLM-based methods. Due to the strong generalization capabilities, various fields such as vision [20, 39] and time-series [21, 40] are renovates by the general frameworks based on LLMs. To our best knowledge, this is the first attempt to introduce an LLM-based method in information cascade modeling."}, {"title": "2.2 Autoregressive Modeling", "content": "Autoregression is a fundamental concept in sequence modeling, which uses observations from previous time steps to predict the next value [23]. This paradigm, which provides fine-grained supervision, has become the best practice for training LLMs [24, 25, 41] and has also inspired other fields [27, 42, 43]. Here, we briefly categorize existing works into three types:\nRNN-based methods. Early studies perform the autoregressive modeling based on RNN variants, and achieve success across various domains [42, 44, 45]. However, these approaches come with limitations of RNNs [46], including low computational efficiency and limited capability in long-distance dependencies.\nTransformer-based methods. Following the introduction of Transformer [47], the potential of autoregressive modeling has been further explored, with representative works including iGPT [48], Autoformer [49] and VAR [43]. Currently, transform-based methods dominate the field of autoregressive modeling.\nLLM-based methods. Built upon Transformer architecture, LLMs with large-scale parameters are pretrained on massive datasets, demonstrating superior capabilities in autoregressive modeling. Therefore, researchers attempt to investigate the feasibility of reusing LLMs for autoregressive modeling. For example, Toto [27] treats videos as sequences of visual tokens and reuses the LLMs as backbones to autoregressively predict future tokens. Similar ideas have also been applied in the field of time-series forecasting [21]."}, {"title": "2.3 Prompt Learning", "content": "Prompt learning [50] has emerged as a novel learning paradigm to adapt LLMs to specific tasks by designing textual prompts. Due to the widespread application of LLMs, designing sophisticated textual prompts for specific tasks has become the research hotspot across various fields [30]. For example, LLM4NG [51] designs the class-level semantic prompt templates based on text-attribute on graphs to facilitate the node classification in few-shot scenarios. Autotimes [21] introduces textual timestamps of time-series to enhance LLM-based forecasting. Building on advanced prompting techniques, deft textual prompts [52, 53] for time-series are further explored. Since LLMs have not yet fully entered the field of information cascade modeling, studies on prompt learning for cascade data are still lacking. However, textual information is prevalent in cascade diffusion process, so designing cascade prompt templates based on textual information for cascade popularity prediction holds significant promise."}, {"title": "3 Preliminaries", "content": "Information cascade graph. Given an information item and all its diffusion, the information cascade graph is defined as \\(G_c(t) = {V_c(t), E_c(t)}\\), where \\(V_c(t)\\) is the set of users participated in the cascade diffusion up to time t, and \\(E_c(t)\\) is the set of edges between users, each edge \\(e_{i,j} = (v_i, v_j, t_k)\\) denotes that user \\(v_i\\) retweets the information to user \\(v_j\\) at time \\(t_k\\).\nGlobal context graph. The global context graph \\(G_g = {V_g, E_g}\\) represents the context of cascade scene, e.g., social networks or citation networks, where \\(V_g\\) represents the set of all users and \\(E_g\\) denotes the relationships between them in the context.\nCascade popularity prediction. Given the information cascade graph \\(G_c\\) and global context graph \\(G_g\\), our target is to predict the future cascade popularity \\(P(t_p) = |V_c(t_p)|\\) at a prediction time \\(t_p\\) based on the observation time \\(t_0\\) (\\(t_0 < t_p\\))."}, {"title": "4 Methodology", "content": "This section introduces details of our proposed AutoCas. AutoCas consists of three modules: (1) cascade tokenization, (2) autoregressive cascade modeling, and (3) cascade prompt learning. The overall framework is shown in is shown in Figure 3."}, {"title": "4.1 Cascade Tokenization", "content": "The key to adapting LLMs to information cascade modeling lies in whether cascade data can be tokenized. Unlike nature language, the complex patterns of cascade data pose challenges to tokenization in three aspects: (1) The local topological pattern of the cascade, (2) The global position of cascade within its context, (3) The dynamic of cascade evolution over time. Therefore, cascade tokenization should involve the aforementioned three type information simultaneously. In this paper, we introduce local embeddings and global embeddings to capture the local topology and global context of cascade data, and construct cascade token sequences to capture the evolutionary dynamics.\nLocal embeddings. For local structure, the method of extracting topological patterns of cascade is flexible. Traditional methods like DeepWalk [54], LINE [55], and Node2Vec [56] are feasible, and furthermore, various advanced GNN-based approaches [57] are also available. Given the cascade graph \\(G_c = {V_c, E_c}\\), the local embedding function \\(f_{local}(\\cdot)\\) is to embed each user according to its topological pattern within the cascade diffusion:\n\\[LE = f_{local}(G_c) = f_{local}(V_c, E_c).\\]\nWe follow the previous study [58] and implement \\(f_{local}(\\cdot)\\) with spectral graph wavelets [59]. Specifically, we calculate the spectral wavelets based on graph Laplacian with heat kernel function to obtain local embedding \\(LE_i\\) for user \\(v_i\\).\nGlobal embeddings. The global context graph encompasses the propagation background of users involved in cascades, from which rich semantic information can be extracted [7]. Given the context graph \\(G_g = {V_g, E_g}\\), the global embedding function \\(f_{global}(\\cdot)\\) is to embed each user according to its global position within the context:\n\\[GE = f_{global}(G_g) = f_{global}(V_g, E_g).\\]\nFor large-scale context graphs with millions of nodes, generating position embeddings for each user is challenging due to the high computational overhead. To this end, we implement \\(f_{global}(\\cdot)\\) with structural learning embedding method based on sparse matrix factorization [60] to get global embedding \\(GE_i\\) for user \\(v_i\\), which is proved to be to scalable and efficient in previous studies [7, 58].\nLocal and global pattern fusion has been proved to be effective in cascade modeling [7, 15, 58]. For each user \\(v_i\\), we fuse its local and global embeddings to obtain the final user embedding \\(H_{v_i}\\):\n\\[H_{v_i} = Concat (LE_{v_i}, GE_{v_i}) \\in \\mathbb{R}^d,\\]\nwhere \\(Concat(\\cdot)\\) denotes the concatenation operation and d is the user embedding dimension.\nCascade token sequence. To capture the temporal dependence and evolution dynamics, we divide the cascade diffusion process into N time patches. Cascade token characterizes the user snapshot of cascade within a time patch. Given the set of users \\(V_c(t_n) = {v_1, v_2, ..., v_i}\\) who have participated in the information cascade until the n - th time patch, we construct the corresponding cascade token \\(T_n\\) via the pooling operation:\n\\[T_n = Pooling (H_{v_i}), v_i \\in V_c(t_n).\\]\nHere, \\(Pooling(\\cdot)\\) can take various forms, ranging from the simple sum and concatenation pooling. In our implementation, we concatenate user embeddings according to the propagation order for convenience. Thus, we set a maximum length I to limit the token size and apply default padding, and \\(T_n \\in \\mathbb{R}^{l\\times d}\\) is then flattened into \\(T_n \\in \\mathbb{R}^S\\). After tokenization at each time patch, we can finally obtain the cascade token sequence:\n\\[T = {T_1, T_2, \\dots, T_N} \\in \\mathbb{R}^{N\\times S},\\]\nwhich can subsequently be seamlessly integrated into autoregressive modeling based on the LLM architecture."}, {"title": "4.2 Autoregressive Cascade Modeling", "content": "When reusing the LLMs for cascade modeling, the cascade data, although tokenized, still has to be aligned to language tokens. Therefore, we adopt \\(Projector(\\cdot) : \\mathbb{R}^S \\rightarrow \\mathbb{R}^D\\) to map the cascade tokens into the embedding space of language tokens:\n\\[{Z_1, Z_2, ..., Z_N} = Projector ({T_1, T_2, T}),\\]\nwhere D is consistent with the dimension of the LLM.\nBased on large-scale autoregressive pre-training, prevalent LLMs can effectively predict the next token based on the preceding tokens. To fully unleash the potential of LLM architecture, we redefine the cascade diffusion in a fully consistent approach. We feed the into projected cascade tokens into the intermediate layers of LLM:\n\\[{Z_2, Z_3, ..., Z_N} = LLM ({Z_1, Z_2, \\dots, Z_{N-1}}) .\\]\nThen each predicted patch is mapped back by \\(Adapter(\\cdot) : \\mathbb{R}^D \\leftrightarrow \\mathbb{R}^S\\) into the space of cascade tokens for supervision:\n\\[{T_2, T_3,.... T_N = Adapter ({Z_2, Z_3, ..., Z_N}).\\]\nBoth \\(Projector (\\cdot)\\) and \\(Adapter (\\cdot)\\) can be implemented through a linear layer or an MLP (Multi-Layer Perceptron). Finally, each predicted token is supervised by the token-wise ground truth:\n\\[L_{MSE} = \\frac{1}{N-1}\\sum_{n=2}^{N} ||T_n - \\hat{T_n}||_2,\\]"}, {"title": "4.3 Cascade Prompt Leanring", "content": "When introducing prompt learning, it is necessary to design appropriate prompt templates to facilitate adaptation to specific tasks. Given the LLM backbone, textual prompts are more preferred and have been demonstrated as an enhancement in the prediction task across various fields [21]. Insipred by that, we adopt the textual timestamps as prompts to provide extra semantic information for cascade token \\(T_n\\):\n\\[P_n = LLM (Prompt\\_Template (T_n)),\\]\nwhere \\(Prompt\\_Template()\\) customizes the text of cascade timestamps and \\(P_n\\) is the prompt token embedded by LLM. The specific template of the prompts we adopt can be found in Appendix C. Similar to the functionality of position embedding, the prompt token we designed carry the temporal position of cascade during the diffusion process. So we naturally inject it into the space of language embedding after the projector:\n\\[{Z_2, Z_3, ..., Z_N} = LLM ({Z_1 + P_1, Z_2 + P_2, ..., Z_n + P_v}),\\]\nAfter prompt injection, we shift our focus back to popularity prediction. Since the cascade token retain the semantic of cascade diffusion, we further adopt it to predict the popularity of information item m at the target time \\(t_p\\):\n\\[\\hat{P}(t_p) = Task\\_Head(T_N),\\]\nwhere \\(Task\\_Head()\\) is a two-layer MLP in our implementation. We use the mean square logarithmic error (MSLE) as the objective, and"}, {"title": "5 Experiments", "content": "In this section, we perform experiments on benchmark datasets to evaluate AutoCas and attempt to answer the following questions:\nRQ1: How effective is AutoCas for cascade popularity prediction?\nRQ2: How key components of AutoCas affect its performance?\nRQ3: Does AutoCas exhibit inherited scaling behavior from LLM?\nRQ4: How does AutoCas improve efficiency over other baselines?"}, {"title": "5.1 Experimental settings", "content": "Datasets. To evaluate the performance of AutoCas, we employ two social networks: Weibo [33] and Twitter [61], and one citation network: APS\u00b9. The target is to predict the number of retweets of"}, {"title": "5.2 Performance Comparison (RQ1)", "content": "We compare AutoCas against baselines across the three datasets. The main results are presented in Table 2. From the results, we can observe that AutoCas significantly improves over state-of-the-art methods. It achieves the lowest MSLE and MAPE values across all datasets, exhibiting a remarkable enhancement of approximately 50% or even more compared to the current models. Notably, AutoCas is compatible with any decoder-only LLMs, demonstrating great generalization and applicability. Even in its worst outcomes, AutoCas consistently outperforms all existing methods by a substantial margin. In particular, on the APS dataset, AutoCas delivers an impressive 86.35% improvement over CasDO on MSLE, underscoring its effectiveness. These results thoroughly verify the superiority of LLM-based predictor in information cascading modeling."}, {"title": "5.3 Ablation Study (RQ2)", "content": "To validate the effectiveness of the individual components in our model, we compared the full model with GPT-2 as backbone against seven variants: (1) w/o Auto employs a non-autoregressive approach"}, {"title": "5.4 Scaling Behavior (RQ3)", "content": "Scalability is an crucial characteristic that evolves from small models to large foundation models. In this section, we explore the scaling trends of AutoCas in cascade popularity prediction. From the results in Table 2, we observe that the prediction accuracy generally improves with the increase in LLM parameters, and the largest Qwen-7B consistently delivers optimal predicting performance. These findings validate the scaling behavior of AutoCas empowered by LLMs in cascade popularity prediction.\nThis scaling behavior of LLM-based predictors introduces a tradeoff between performance and computational efficiency. To provide a comprehensive assessment, we evaluate each adapted predictor from three perspectives: performance, training speed, and parameters, using the same batch size and learning rate, as illustrated in Figure 5. Additional experiments can be found in Appendix D. One can observe that the models generally achieve lower MSLE as the number of parameters increases. However, this improvement comes at the cost of longer training times, as seen with larger models like Qwen-7B. While smaller models such as GPT-2 train faster but exhibit higher MSLE, indicating limited effectiveness. Therefore, selecting the optimal model requires balancing accuracy gains against computational efficiency to ensure practical deployment within resource constraints."}, {"title": "5.5 Efficiency Analysis (RQ4)", "content": "To evaluate the time efficiency of AutoCas, especially in comparison to SOTA baselines, we record the training convergence speed and inference time for AutoCas and each baseline on the Weibo and APS dataset, with the results shown in Figure 6. Then we discuss"}, {"title": "6 Conclusion", "content": "In this paper, we present AutoCas, a novel framework that repurposes LLMs as predictors of information cascade. We tokenize the cascade data and align the cascade diffusion process with language generation at the token level. Then we project cascade tokens into the embedding space of LLMs, and propose the paradigm of autoregressive cascade modeling to fully leverage the potential of LLM architecture. Further, we adopt prompt learning in cascade popularity prediction, and design prompt tokens to facilitate specific task adaptation. Finally, our experimental results demonstrate the superior performance of AutoCas in the cascade popularity prediction task, and it exhibits the scaling behavior empowered by LLMs. In future work, we will explore the unified paradigm for integrating social data and natural language, and the possibility of building foundation models in social computing."}]}