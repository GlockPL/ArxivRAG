{"title": "A Survey on Mixup Augmentations and Beyond", "authors": ["Xin Jin", "Hongyu Zhu", "Siyuan Li", "Zedong Wang", "Zicheng Liu", "Chang Yu", "Huafeng Qin", "Stan Z. Li"], "abstract": "As Deep Neural Networks have achieved thrilling breakthroughs in the past decade, data augmentations have garnered increasing attention as regularization techniques when massive labeled data are unavailable. Among existing augmentations, Mixup and relevant data-mixing methods that convexly combine selected samples and the corresponding labels are widely adopted because they yield high performances by generating data-dependent virtual data while easily migrating to various domains. This survey presents a comprehensive review of foundational mixup methods and their applications. We first elaborate on the training pipeline with mixup augmentations as a unified framework containing modules. A reformulated framework could contain various mixup methods and give intuitive operational procedures. Then, we systematically investigate the applications of mixup augmentations on vision downstream tasks, various data modalities, and some analysis & theorems of mixup. Meanwhile, we conclude the current status and limitations of mixup research and point out further work for effective and efficient mixup augmentations. This survey can provide researchers with the current state of the art in mixup methods and provide some insights and guidance roles in the mixup arena. An online project with this survey is available at https://github.com/Westlake-AI/Awesome-Mixup.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Neural Networks (DNNs), such as Convolutional Neural Networks (CNNs) and Transformers, since their powerful feature representation ability that has been successfully applied to a variety of tasks, e.g. Image Classification, Object Detection, and Natural Language Processing (NLP), etc. To accomplish progressively more challenging tasks, DNNs employ a large number of learnable parameters, and that means that without numerous training data, models could easily get overfitting and fail to generalize. However, training data in some scenarios were unavailable and expensive to collect. Causing DNNs to generalize beyond limited training data is one of the fundamental problems of deep learning.\nTo address the data-hungry problem, researchers have proposed Data Augmentation (DA) techniques. Compared to \u201cmodel-centric\u201d and regularization methods, DA is a \u201cdata-centric\" regularization technique that prevents over-fitting by synthesizing virtual training data. DA could introduce useful invariant features by constructing different versions of the same sample. The increase in dataset size and inductive bias brought about by DA also achieves some regularization effect to relieve the over-fitting problem. Recently, data augmentation has been shown to improve the generalization of deep learning models and become a key factor in achieving state-of-the-art performance. Data augmentation can synthesize new data through contrast combination, mixup, and generation.\nIn this survey, we focus on a burgeoning field - Mixup. MixUp [1] generates augmented samples by interpolating two samples with their one-hot labels to one. Essentially, Mixup-based methods mix multiple samples to generate augmented data. In contrast, most existing augmentation techniques modify single samples without altering their unique labels. Unlike these methods, Mixup generates augmented samples from two or more examples, leading to multiple labels that better reflect real-world conditions. Additionally, Mixup demonstrates strong transferability across different datasets and domains. In comparison, alternative combination methods often require extensive time to identify suitable augmentation strategies. The generative method is challenging to apply to large datasets, as it requires an additional generator and discriminator, hindering transferability and limiting application scenarios. In contrast, Mixup does not rely on label-retaining operations but uses a learnable approach to create more effective augmented samples. Unlike traditional data augmentation methods that process single samples, Mixup generates virtual training data by combining multiple samples, producing a large volume of training data without the need for domain knowledge.\" Currently, mixup has been successfully applied to a variety of tasks and training paradigms, including Supervised Learning (SL), Self-Supervised learning (SSL), Semi-Supervised Learning (Semi-SL), NLP, Graph, and Speech."}, {"title": "II. PRELIMINARY", "content": "Table I and Table A1 list the notations and abbreviations used in this survey. We define a total sample set as $X \\in R^{C \\times W \\times H}$, and corresponding label set as $Y \\in R^k$. In Computer"}, {"title": "B. Mixup Framework Modules", "content": "In this subsection, we will detail each module's functions in the mixup method pipeline, as shown in Fig. 2.\nInitialization. Before mixup, some methods select raw samples within the mini-batch to filter those suitable for mixing, e.g. Co-Mix [26] selected suitable samples in the mini-batch to maximize the diversity of the mixed samples obtained. Besides filtering the samples, some saliency-based methods leveraged pre-trained models to locate and obtain feature maps for the samples. Finally, each method obtained the mixup ratio $\\lambda$ from the Beta distribution.\nSample Mixup Policies. In Supervised Learning, we divide the policies into 9 classes, and we detail these classes in Fig. A1. Static Linear methods used $\\lambda$ mixed two or more samples based on interpolation linear. Fearture-based methods used raw samples feature maps obtained by $f(\\cdot)$, and mixed them in interpolation linear. Cutting-based methods are used in various ways, such as cutting, resizing, or stacking to mix samples, with the mixing ratio $\\lambda$ from the mask area. K Sample mixup methods used more than 2 samples mixing. Random Policies methods combined lots of different augmentation methods and some hand-crafted mixup methods, the policy is chosen by each method's weight factor. Style-based mixed samples from their style and content by an additional style extractor. Saliency-based methods used sample feature maps to locate their saliency information and obtained max feature mixed samples. Attention-based methods, similar to saliency-based methods, utilized attention scores rather than saliency maps. Generating Samples used some generative models such as GAN-based models [27] and Diffusion-based models [28] to generating mixed samples.\nLabel Mixup Policies. We divide into 8 classes in SL"}, {"title": "C. Main Steps of Mixup Method", "content": "As shown at the top of Fig. 2, the mixup methods followed these steps: (i). Loading mini-batch raw samples from the training dataset; (ii). For some downstream tasks, which include selecting raw samples and retaining reliable samples, some saliency-based or attention-based methods obtained the feature regions or tokens by loading a pre-train model. Then, define the mixing ratio $\\lambda$, which samples from Beta distribution or Uniform distribution. (iii). After initialization, raw samples were mixed with other samples by sample mixup policies. We illustrate those policies in subsection 3.1. (iv). When the mixed samples $\\hat{x}$, there were two choices: One was sampling, some methods updated the mixing ratio by mask $M$ total pixels, some selected mixed samples for retaining more diversity or challenging samples, and some methods redefined the mixing ratio. Another was label mixup policies, and we illustrate those"}, {"title": "III. MIXUP METHODS FOR CV TASKS", "content": "This section focuses on mixup methods used for CV tasks. We reviewed widely of these methods and divided them into four categories: (1) Supervised Learning, (2) Self-Supervised Learning, (3) Semi-Supervised Learning, and (4) some mainstream downstream tasks in the CV: Regression, Long-tail, Segmentation, and Object Detection. Fig. 3 and Fig. 6 summarise some mixup methods in SL tasks."}, {"title": "A. Sample Mixup Policies in SL", "content": "1) Static Linear: Static Linear methods use ratios to interpolate globally linearly to get mixed samples. Where MixUp [1] is the seminal work of mixup-based methods, as shown in Eq. 1, the plug-and-play, data-independent, model-agnostic, efficient, and simple method can bring immediate performance to the model. At the same time, BC [21] also proposed a similar process to MixUp. Unlike MixUp, which obtained the mixing ratio $\\lambda$ from the Beta distribution, BC chose to receive $\\lambda$ from the Uniform distribution.\n$\\hat{x} = \\lambda * x_i + (1 - \\lambda) * x_j, \\ \\hat{y} = \\lambda * y_i + (1 - \\lambda) * y_j,$ (1)\nAdaMixup [5] and Local Mixup [30] further considered the problem of \u201cManifold Intrusion\u201d. AdaMixup used a learnable ratio to get reliable mixed samples and Local Mixup mining mixed sample pairs $(x_i, x_j)$ by weighting the training losses of the mixed samples with $w$. The weight of each mixed sample, depending on the distance between the corresponding points of $(x_i, x_j)$, is used to avoid interpolation between samples that are too far away from each other in the input samples.\n2) Feature-based: Feature-based methods transfer mixup methods from pixel-level to latent-level. Manifold Mixup [3] used the ratio $\\lambda$ linearly mixing the sample features from the input samples encoded by models:\n$z = \\lambda * f(\\cdot)(x_i) + (1 - \\lambda) * f(\\cdot)(x_j),$ (2)\nwhere the $f(\\cdot)$ denotes the model encoder, and the $\\hat{z}$ denoted the mixed features. However, PatchUp [31] chose to use the generated mask to mask features in two ways: Hard PatchUp and Soft PatchUp. The hard PatchUp indicates that the mask is binary, and the Soft PatchUp way indicates that the mask is $\\lambda$-valued similarly:\n$\\begin{cases}\nM_h = 1, \\  Hard \\ PatchUp, \\\\\nM_s = \\lambda, \\ Soft \\ PatchUp,\n\\end{cases}$ (3)\n$z = M_{h/s} \\odot f(\\cdot)(x_i) + (1 - M_{h/s}) \\odot f(\\cdot)(x_j),$ (4)\nwhere the $M_{h/s}$ denotes the mask which obtained from Eq. 3, and the $\\odot$ is element-wise multiplication.\nMoEx [32] operated on the training image in feature space, exchanging the moments of the learned features between images and interpolating the target labels. Finally, the mixing constants $\\lambda$ are used to combine the Mixup Cross-Entropy (MCE) loss function by Eq. 5. This process is designed to model the extraction of additional training signals from the moments. This is evidenced by studies [33], [34] demonstrating that moments extracted from instance and positional normalization can approximately capture the style and shape information of an image.\n$L^{MCE} = \\lambda * L(f(z), y_i) + (1 - \\lambda) * L(f(z), y_j),$ (5)\nwhere the $f(z_i)$ denotes the feature representation of $x_i$, which has been injected with the moments of $x_j$. In another way, Catch-up Mix [29] found that some CNNs train to produce some powerful filters, and the model will tend to choose their features. Dropping some of the slower-learning filters will limit the performance of the model. Catch-up Mix proposed a filtering module mixing the features learned by poor filters to obtain mixed features, which further improves their capabilities.\n3) Cutting-based: Cutting-based methods used some masks to mix samples. Unlike Cutout drops a patch in samples, CutMix [2] randomness generated rectangular binary masks $M$ and mix samples according to Eq. 6:\n$\\hat{x} = M \\odot x_i + (1 - M) \\odot x_j,$ (6)\nwhere $M \\in W \\times H$, and $r_w = W\\sqrt{1 - \\lambda}, r_h = H\\sqrt{1 - \\lambda}$. GridMix [35] and MixedExamples [36] used grid masks similar to GridMask rather than a single patch to obtain mixed samples. SmoothMix [37] proposed a smoothing mask for mixing samples with a smooth boundary. Similarly, StarMix [38] employed a smooth mask for mixing samples for the vein identification task. SuperGridMix [39] used Superpixel segmentation methods to get the mask. MSDA [40] used MixUp and CutMix to obtain mixed samples. ResizeMix [41] posits that regardless of the chosen cutting or grinding way, there is a possibility that the source sample's feature may be lost due to randomness. The resize way, however, is designed to retain the complete information of the source sample while also maximizing the feature information of the mixed samples to the greatest extent possible according to Eq. 7:\n$\\hat{x} = P(T(x_i, \\lambda), x_j), \\ \\hat{y} = \\lambda * y_i + (1 - \\lambda) * y_j,$ (7)\nwhere $T(\\cdot)$ denotes the resizing function and the scale rate $\\lambda$ is sampled from the uniform distribution $\\lambda \\sim U(\\alpha, \\beta)$, where $\\alpha$ and $\\beta$ denote the lower and upper bound of the range, respectively. $P(\\cdot)$ denotes paste the score sample $x_i$ to the target sample $x_j$. The $\\lambda$ is defined by the size ratio of the patch and target sample.\nUnlike mixed in pixel-level. FMix [42] argued that effective feature is often continuous in an image, so the samples are transformed from the RGB channel to the Fourier space using the Fourier transform, sampling the high&low-frequency information to generate binary masks. Pani MixUp [43] interpolated mixing by GNNs to build relationships between sample patches. Specifically, patch sets are constructed first, then k-nearest sets are further built using the similarity of patches, and similar patches are linearly interpolated to get the mixed samples.\nMoreover, instead of using some mask-based ways, some"}, {"title": "B. Label Mixup Policies in SL", "content": "1) Optimizing Calibration: Calibration is a metric that measures how well a model's confidence aligns with its accuracy. A model should neither be overconfident nor underconfident, as this can lead to incorrect decisions. The Expected Calibration Error (ECE) is a key metric for evaluating model calibration. Manifold Mixup found that MixUp significantly improves network confidence, but it also noted that overconfidence is not desirable. CAMixup [6] found that a simple application of MixUp with Cross-Entropy (CE) increases the ECE of the model, which is contrary to current general intuition. Wen et al. used Label Smoothing for experiments to verify that it is a labeling issue. CAMixup adjusted the mixing ratio based on the difference between the average accuracy and confidence of the class classification $\\lambda$.\nNormally, the model is expected to be more confident in predicting simpler classes. For harder classes, the model is encouraged to be less confident. RankMixup [7] proposed a Mixup-based Ranking Loss (MRL):\n$L_{MRL} = max(0, max_{\\hat{y}} - max_y + m),$ (16)\nwhere $\\hat{y}$ is a predicted probability of the class $k$ for the mixed sample $x_i$, margin $m$ determines acceptable discrepancies between the confidences of raw and augmented samples. It encourages the confidence level of the mixed samples to be lower than that of the raw samples to maintain the ranking relationship. The expectation is that higher confidence values favor larger mixed samples with $\\lambda$ so that the confidence values and the order of the mixing ratio are proportional to each other. SmoothMixup [92] using the inherent robustness of smoothing classifiers to identify semantically non-categorical samples and to improve the performance of classifiers."}, {"title": "C. Self-Supervised Learning", "content": "1) Contrastive Learning: Contrastive learning (CL) has emerged as a prominent training paradigm in SSL, where the objective is to map similar \u201cpositive\u201d samples into a representation space proximate to an \u201canchor\" while mapping disparate \"negative\" samples into regions more distant. Mixup methods have been extensively employed in CL to generate diverse or challenging samples.\nLoss Object Improvement. In terms of improvement, MixCo [105] argued that mixed samples with features of semi-positive samples contribute to learning better representations, and using CL with the positive and negative samples and targets of mixed samples can alleviate the instance discrimination problem by allowing the model to learn the implicit relationship between the positives and negatives, as shown in Eq. 22:\n$L_{MixCo} = - \\sum_{i=1}^{N} (\\lambda \\cdot log(\\frac{exp(z_i \\cdot z_i^* / \\tau)}{\\sum_{n=1}^{N} exp(z_i \\cdot z_{in}/\\tau)}) + (1-\\lambda) \\cdot log(\\frac{exp(z_j \\cdot z_j^* / \\tau)}{\\sum_{n=1}^{N} exp(z_j \\cdot z_{jn}/\\tau)})),$ (22)\nwhere the $\\tau$ denotes the softening temperature for similarity logits, $N$ is the number of mixed samples. Different from MixCo, i-Mix [106] proposed to insert virtual labels in batch size and mixed samples and to insert corresponding virtual labels in the input-level and label-level, respectively. Transforming unsupervised loss functions to supervised losses (e.g., CE losses) according to Eq. 23 and Eq. 24:\n$L_{SimCLR} = - log \\frac{exp(s(z_i, z_{(N+i) mod 2N})/\\tau)}{\\sum_{k=1}^{2N} exp(s(z_i, z_k)/\\tau)},$ (23)\n$L_{N-pair} = \\sum_{n=1}^{N} \\frac{exp(s(z_i, z_n)/\\tau)}{\\sum W_in \\cdot log{\\frac{exp(s(z_i, z_k)/\\tau)}{\\sum_{k=1,k\u2260i}^{N}},}$ (24)\nwhere $N$ is the batch size. Similarly to the i-Mix concept, MCL [107] put forth a CL framework based on the tenets of label smoothing. This framework employed a novel contrastive loss function intending to enhance the classification efficacy of clinical medicine time series data through the use mixup for the generation of new samples.\nSAMix [77] proposed improved infoNCE, $L_+$ and $L_\u2212$, for mixup-based SSL. $L_+$ is called the local term since features of another class are added to the original infoNCE loss; $L_\u2212$ is called the global term, and is the mixed infoNCE by Eq. 25:\n$L_+ = -\\lambda log p_i - (1 - \\lambda) log p_j, \\\\ p_i = \\frac{exp(z \\cdot z_i/\\tau)}{exp(z \\cdot z_i/\\tau) + exp(z \\cdot z_j/\\tau)},$ (25)"}, {"title": "D. Semi-Supervised Learning", "content": "In deep learning tasks, finding huge amounts of unlabeled samples $x_u$ is easy. It is very expensive to make labeling. Therefore, researchers combine a large number of unlabeled samples $x_u$ into a limited number of labeled samples $x$ and train them together, which is expected to improve the model performance, thus giving way to Semi-Supervised Learning (Semi-SL). Semi-supervised learning avoids the waste of resources and, at the same time, solves the problems of poor model generalization ability in Supervised Learning and model inaccuracy in Unsupervised Learning. Fig. 9 shows the processes of mixup methods on Semi-SL tasks.\nSemi-SL. MixMatch [10] is capable of guessing low-entropy labels for data-augmented unlabeled examples and reduces the entropy of the labeling distribution $p$ using an additional sharpening function by Eq. 26. These \"guess\" labels can be utilized to compute the unlabeled loss $L_u$, which serves as a component of the combined with labeled loss $L$ to form loss $\\mathcal{L}$ for Semi-SL by Eq. 27. MixMatch employs mixup to mix labeled and unlabeled data, a novel approach separately.\n$Sharpen(p, T)_i = p_i^{\\frac{1}{T}}/\\sum_{j=1}^{L} p_i^{\\frac{1}{T}},$ (26)\n$\\mathcal{L} = L + \\omega L_u,$ (27)"}, {"title": "E. CV Downstream Tasks", "content": "1) Regression: Regression tasks differ from classification tasks in that they require a relatively accurate prediction of a result rather than a probability of a class. This implies that directly mixing two random samples does not ensure that the mixed samples are effective for the model and may be detrimental in some cases. MixRL [139] proposed using the validation set to learn, for each sample, \"how many nearest neighbors should be mixed to obtain the best model performance\". Similar to AutoAugment [140], Reinforcement Learning (RL) is used to find the lowest model loss on the validation set to determine this. Specifically, an RNN model is used to determine the optimal number of KNNs for the samples, and then an MLP is used to select all the samples.\nC-Mixup [141] adjusts the sampling probability based on the similarity of the labels and then selects pairs of samples with similar labels for mixing. Simple implementation of vanilla MixUp causes noisy samples and labels. For a sample that has already been selected, C-Mixup uses a symmetric Gaussian kernel to calculate the sampling probability of another sample, where the closer sample is more likely to be sampled. Also, using low dimensional label similarity for the calculation will reduce the overhead. Similarly to C-Mixup, ADA [142] uses a prior distribution for select samples and labels, UMAP Mixup [143] uses the Uniform Manifold Approximation and Projection (UMAP) method as a regularizer to encourage and ensure that the mixup generates mixed samples that are on the flow of the feature and label data. ExtraMix [144] proposed a mixup technique that can be extrapolated, expanding the latent space and label distribution. Compared to existing mixup-based methods, ExtraMix minimizes label imbalance. In addition, CVAE [145] is used to optimize pseudo labels in the mixing to deal with the fact that in materials science, new materials with excellent properties are usually located at the tail end of the label distribution. Warped Mix [146] proposed a framework that takes similarity into effect when interpolating without dropping diversity; Warped Mix argued that similarity should influence the interpolation ratio $\\lambda$, not the selection. High similarity should lead to a strong $\\lambda$, and low similarity should lead to essentially no change. The SupReMix [74] aims to make better use of the inherent \u201csequential\u201d relationship between the inputs to facilitate the creation of \"harder\" contrast pairs. The objective is to promote continuity as well as local linearity."}, {"title": "IV. MIXUP FOR OTHER APPLICATIONS", "content": "In this section, we discuss mixup-based methods applied to other tasks. As shown in Fig. 10, we divided them into 2 main subsections: Traning Paradigms and Beyond Vision."}, {"title": "A. Training Paradigms", "content": "1) Federated Learning: Federated Learning (FL) represents a training data decentralized machine learning solution initially proposed by Google in 2016. It is designed to address the issue of data silos by conducting training on distributed data stored in a multitude of endpoints, to develop high-quality centralized machine learning models. To address the problem of homo-distributed data where user-generated data is distributed between devices and tags, Shin at el. propose XOR Mixup [161], which collects encoded data from other devices that are decoded using only each device's data. The decoding provides synthetic but realistic samples that induce a homo-distributed dataset for model training. The main idea of the XOR Mixup key idea is to utilize the dissimilarity operation property: $(x_i \\oplus x_j) \\oplus x_j = x_i, x_i$ and $x_j$ from two individual devices. FedMix [162] proposed a simple framework, Mean Aug-mented Federated Learning (MAFL), in which clients send and receive locally averaged data according to the privacy require-ments of the target application. To alleviate the performance degradation suffered due to the increased dissimilarity of local data between clients. Based on Federated Learning Distillation (FLD) and MixUp, Mix2FLD [163] is proposed. Specifically, each device in Mix2FLD uploaded its local model outputs as in FLD and downloaded the model parameters as in FL, thus coping with the uplink-downlink channel asymmetry. Between the uplink link and downlink, the server runs knowledge distillation to transfer the teacher's knowledge to the untrained student model (i.e. the global model). However, this output-to-model conversion requires additional training samples to be collected from the device, incurring significant communication overhead while violating local data privacy. StatMix [164] computed image statistics for individual nodes, i.e., the mean and standard deviation of each color channel, as content and style; distributes the computed statistics to all nodes via a central server; and performs style delivery using these statistics in individual nodes.\n2) Adversarial Attack & Adversarial Training: Adversarial Attack & Training [165] can markedly enhance model robust-ness since it encourages the model to explore some unseen regions and OOD, mixup methods, moreover, bolster model performance and forestall model overfitting in the task. To improve model robustness, M-TLAT [166] uses the MixUp in addition to a randomly generated dummy label, combines the mixed samples and their labels through a classifier to get the gradient perturbed noise $\\delta$, and then mixes the noise and mixed samples to get the final augmented samples and labels according to the Eq. 28:\n$x_{adv} = x - \\epsilon * sign(\\nabla_x L(\\hat{x}, \\hat{y}, \\theta)),$ (28)\nwhere the $\\epsilon * sign(\\cdot)$ denotes the FGSM [167] or PGD [168] white box attack, and $\\theta$ denotes the parameters of the models. The purpose of the M-TLAT is to increase Corruption robustness, while TLAT aims to increase Adversarial robustness and solve the generalized robustness problem. MI [169] converts the Training Stage into the Inference Stage by linearly mixing the source samples with the target samples that have been noise-added by the adversarial attack, and during the Inference Stage, it is found to be optimal for MI compared to the direct use of the adversarial attack. Differ from MI, AVMixup [170] adds the perturbed noise in the input-level, and also analyses the effectiveness of soft-labeling on Adversarial Feature Overfitting (AFO). IAT [171] combines the clean samples loss and adversarial attack samples loss for training the model and getting a better balance of robustness and accuracy."}, {"title": "B. Future Works", "content": "As an augmentation method, mixup can be applied in lots of tasks. What specific tasks can be performed as part of a \u201cdata-centric\" method, taking into account the latest technologies and discoveries, and we outline some opportunities.\nNumber of samples to Mix. Most mixup methods use 2 sample mixes. In image classification, Co-Mix [26], AdAutoMix [79] instead chooses mixing 2-3 samples for improving model performance. However, for other tasks, there is no option to mix multiple samples. Mixing multiple samples can further increase the diversity of the augmented samples and, at the same time, lead to the sample difficulty. Thus, it is worthwhile research to find and choose the number of mixed samples for the corresponding task.\nApplying on MLLMs and Mixup. Multimodal Large Language Models (MLLMs) have shown a powerful capability. m\u00b2-Mix used the text & image modality mixing to improve CLIP [188]. Different data modalities with various features could bring more features and reduce the gap between different modalities, which can enhance the robustness and generalization of the model when trained with image, text, and audio mixed samples.\nGenerating samples based on Generative Model. GAN [27], VAE [91], Diffusion Model (DM) [243] can generate high-quality samples and some current work such as DiffuseMix [4], DiffMix [191] have demonstrated that generative models can be used as a DA method. However, DM needs a lot of time to generate samples. GAN and VAE can generate them quickly, but the quality of the generated samples is difficult to ensure. How to trade the efficiency and quality is worthwhile to research.\nEmploying a Unified Mixup Framework. Mixup as a plug-and-play, simple, and effective DA tool initially. It has now become a trainable & end-to-end way. We argue"}, {"title": "VII. CONCLUSION", "content": "In this survey, we reformulate mixup methods as a unified framework and summarise those methods' technical details and data modalities on various tasks from 2018 to 2024. In addition, we divided mixup into two main classes: Sample Mixup Policies and Label Mixup Policies, which could contain different improved versions of mixup, and conclude all mixup methods in this survey as two Figures: Fig. A1 and Fig. A2. Also, we summarize the various types of datasets frequently used in mixup methods, the classification results of some mainstream mixup methods for image classification tasks in SL on commonly used datasets based on mainstream models that are displayed in Table A2, Table A3 and Table A4. Finally, we discuss existing problems and worthwhile future works to give researchers some advanced ideas and thoughts in this field."}]}