{"title": "MSP-Podcast SER Challenge 2024: L'antenne du Ventoux\nMultimodal Self-Supervised Learning for Speech Emotion Recognition", "authors": ["Jarod Duret", "Mickael Rouvier", "Yannick Est\u00e8ve"], "abstract": "In this work, we detail our submission to the 2024 edition of the\nMSP-Podcast Speech Emotion Recognition (SER) Challenge.\nThis challenge is divided into two distinct tasks: Categorical\nEmotion Recognition and Emotional Attribute Prediction. We\nconcentrated our efforts on Task 1, which involves the categor-\nical classification of eight emotional states using data from the\nMSP-Podcast dataset. Our approach employs an ensemble of\nmodels, each trained independently and then fused at the score\nlevel using a Support Vector Machine (SVM) classifier. The\nmodels were trained using various strategies, including Self-\nSupervised Learning (SSL) fine-tuning across different modal-\nities: speech alone, text alone, and a combined speech and text\napproach. This joint training methodology aims to enhance the\nsystem's ability to accurately classify emotional states. This\njoint training methodology aims to enhance the system's ability\nto accurately classify emotional states. Thus, the system ob-\ntained F1-macro of 0.35% on development set.", "sections": [{"title": "1. Introduction", "content": "Speech Emotion Recognition (SER) represents a challenging\narea of research. The complexity arises from the nuanced, sub-\njective nature of emotional expression in speech and the chal-\nlenge of extracting effective feature representations. Despite\nthese difficulties, understanding emotions is crucial for enhanc-\nhuman-computer interaction, as emotions significantly in-\nfluence reasoning, decision-making, and social interactions. In\nthe realm of speech and text, emotions, while subjective, are es-\nsential to convey meaning and intent. In recent years, advances\nin deep learning have contributed to notable improvements in\nthe performance of emotion recognition systems by leveraging\nhighly effective features extracted from deep neural networks.\nIn the pursuit of advancing SER capabilities, the MSP-Podcast\nSER Challenge 2024[1] provides a fertile ground for explor-\ning novel methodologies and techniques for emotion recogni-\ntion from naturalistic speech data. Our focus lies primarily on\nTask 1: Categorical Emotion Recognition, which involves clas-\nsifying speech segments into eight specified emotional states:\nAnger (A), Happiness (H), Sadness (S), Fear (F), Surprise (U),\nContempt (C), Disgust (D), and Neutral (N).\nThis paper details our submission to the 2024 edition of the\\MSP-Podcast SER Challenge. Our approach employs an en-\nsemble of models, each trained independently and then fused at\nthe score level using a Support Vector Machine (SVM) classi-\nfier. The models were trained using various strategies, includ-\ning Self-Supervised Learning (SSL) fine-tuning across different\nmodalities: speech alone, text alone, and a combined speech\nand text approach. This joint training methodology aims to"}, {"title": "2. MSP-Podcast SER Challenge 2024", "content": "MSP-Podcast[2] is a large naturalistic speech emotion corpus\nfeaturing speech segments sourced from an audio-sharing web-\nsite. The corpus is annotated for both categorical emotion\nrecognition and emotional attribute prediction. The training set\nconsists of 68,119 speaking turns and the development set con-\ntains 19,815 speaking segments. The test set comprises 2,347\nunique segments from 187 speakers, with the labels not publicly\ndisclosed. The selection of segments for the test set was care-\nfully curated to ensure a balanced representation across primary\ncategorical emotions.\nTwo tasks were proposed, we are only participating in the\nfirst task. The first task involves categorical classification within\neight specified emotional states: Anger (A), Happiness (H),\nSadness (S), Fear (F), Surprise (U), Contempt (C), Disgust (D),\nand Neutral (N). The test set for the challenge has a balanced\ndistribution across the emotional categories. Performance eval-\nuation and model ranking on the leaderboard are based on the\nMacro-F1 score. The Macro-F1 score is calculated by first com-\nputing the F1 score separately for each class, which is the har-\nmonic mean of precision and recall for that class, and then tak-\ning the average of these F1 scores."}, {"title": "3. Related Work", "content": "3.1. Speech Emotion Recognition\nSER involves a two-step process: feature extraction and\nemotion classification. Early SER research focused on hand-\ncrafted features like pitch, energy, and Mel-frequency cepstral\ncoefficients (MFCCs) along with traditional machine learning\nmethods, including Markov models, Gaussian mixture models,\nand support vector machines for classification [3, 4]. More\nrecently, neural-based models have started to replace traditional\nmachine learning approaches [3] [4]. Convolutional Neural\nNetworks (CNNs) and Recurrent Neural Networks (RNNs)\nhave demonstrated improved performance in emotion recog-\nnition tasks [5, 6]. Additionally, transfer learning, particularly"}, {"title": "3.2. Text Emotion Recognition", "content": "Text Emotion Detection and Recognition (TEDR) has signifi-\ncantly evolved over the past few years, transitioning from tra-\nditional machine learning approaches to deep learning mod-\nels. Initial methods primarily utilized classifiers such as Sup-\nport Vector Machine (SVM) and Maximum Entropy (MaxEnt)\nclassifiers, later approaches increasingly relied on deep learning\nmodels in combination with different word embedding meth-\nods. For example, an emotion detection model that combines\nLong Short-Term Memory (LSTM) networks with Convolu-\ntional Neural Networks (CNN) was introduced [12]. This model\nintegrates various word embeddings, including GloVe [13] and\nFastText [14], to capture semantic nuances more effectively.\nMore recently, approaches based on transformer pre-trained\nlanguage models [15] have begun to emerge, offering remark-\nable breakthroughs in text emotion detection. In [15], the au-\nthors conducted a comprehensive comparison of models in-\ncluding BERT [16], RoBERTa [17], DistilBERT [18], and XL-\nNet [19], with RoBERTa emerging as the model achieving the\nbest performance."}, {"title": "4. Overview of the approach", "content": "The system was developed as a two-level architecture. Given a\nspeech segment, the first level extracts outputs from categorical\nemotion recognition based on various sub-systems. The outputs\nof theses sub-systems are fed to a SVM. Five different set of\nsub-systems are used.\nWe formulate the sub-system as a mapping from the con-\ntinuous speech domain into the discrete domain of categorical\nlabels of emotion. As depicted in Figure 1, to achieve this,\nwe first use an encoder (speech and/or text encoder). These\nencoders have been trained on unlabeled data and are capable\nof extracting highly robust feature representations. Following\nthe encoder stage, we employ a pooling strategy to aggregate\nthese features over time, ensuring a fixed-length representation\nregardless of the original speech duration. This representation\nthen feeds into a classifier layer, which serves as the final com-\nponent of our architecture. This layer is responsible for map-\nping the aggregated features to the desired categorical labels of\nemotion, thus completing the process of transforming continu-\nous speech into discrete emotion predictions."}, {"title": "5. Sub-System components", "content": "In this section, we describe the various components used in the\nsub-systems. In Section 5.1, we describe the various speech and\ntext encoders used in our study. Following that, Section 5.2 is\ndedicated to detailing the pooling techniques employed. The\ndiscussion continues in Section 5.3, where we delve into the\nclassifier used. Finally, Section 5.4 covers the aspects of speech\naudio data augmentation."}, {"title": "5.1. Encoder", "content": "In the realm of speech and text processing, the choice of en-\ncoders is essential for provide powerful deep feature learn-\ning. Indeed, these encoders are trained on large unannotated\ndatasets, enabling them to learn rich, complex patterns without"}, {"title": "5.2. Pooling", "content": "Given that speech segments vary in length, we use pooling to\naggregate the features given by the encoder across time, en-\nsuring a fixed-length representation regardless of the original\nspeech duration. Two different types of pooling have been used:\n\u2022 mean-pooling achieves this by averaging the feature val-\nues given by the encoder over the time dimension, which\neffectively summarizes the overall characteristics of the\nspeech signal into a single unified representation.\n\u2022 attention-pooling unlike mean-pooling leverages a\nweighted average, where the weights are learned through\nthe model [20]. This allows the model to focus on more\nrelevant parts of the speech signal, potentially capturing\nnuanced dynamics that mean pooling might overlook."}, {"title": "5.3. Classifier", "content": "For the classifier component, we assume that the SSL encoders\nhave successfully captured all necessary information for pre-\ndicting the targeted categorical emotion label. We choose a\nsimple yet efficient approach by integrating a linear layer to\nfunction as the classifier. This linear classifier takes the pooled\nfeature vectors and assigns them to the a given emotion label."}, {"title": "5.4. Data Augmentation", "content": "The MSP-Podcast corpus includes, for each segment, the emo-\ntion label, text transcription, speaker ID, and gender. However,\nthe test set is provided as speech only, without any annotations.\nTo utilize a text encoder, we needed to automatically generate\ntranscriptions for the test set. For this purpose, we employed\nthe Whisper [21] speech recognition model. Additionally,\nto ensure consistency and avoid any discrepancies between\nprovided transcriptions and Whisper-generated transcriptions,\nwe computed transcriptions for the entire dataset.\nWe observed a significant discrepancies in the distribution of\nemotion classes within both the Training and Development sets.\nAdditionally, a considerable number of samples are labeled as\n\"X,\" indicating that no consensus was reached for these sam-\nples. To minimize the mismatch, we decided to automatically\nrecompute the consensus for samples labeled as \"X\". For each\nsample, we have access to the labels provided by each annotator\nand the consensus. The consensus is determined by identify-\ning the most frequently associated label. In scenarios where no\nsingle label predominates due to an even distribution of votes\namong the labels, the label \"X\" is assigned to indicate the lack\nof a clear consensus.\nOur method for recomputing the consensus is detailed as fol-\nlows: First, we calculate a score for each evaluator by deter-\nmining the ratio of their annotations that match the consensus\nto their total number of annotations. Then, we discard all an-\nnotations from evaluators whose score falls below a predefined\nthreshold K. Finally, we recompute the consensus for the whole\ndataset. In scenarios where there is a tie between a specific la-\nbel and the Neutral (\"N\") label, we opt to drop the label \"N\".\nSamples with newly attributed labels from the Training and De-\nvelopment sets were added to the training set."}, {"title": "6. Sub-Systems", "content": "In this section, we provide a detailed description of the five dis-\ntinct sub-systems employed in the fusion process."}, {"title": "6.1. Sub-System A: WavLM", "content": "Thi system is based on a WavLM, a mean pooling strategy and\nthe outoput is a softmax loss function. In order to optimize\nthis architecture, we employ the Adam optimizer. We fine-tune\nthe pre-trained WavLM model during the training phase. This\nfine-tuning allows the WavLM model to adjust its parameters\nspecifically towards recognizing emotions in speech, leveraging\nthe rich, pre-learned representations and tailoring them to our\ndomain of interest. We set the mini-batch size to 16 and 10\nsteps. And no data-augmentation is done on speech segment."}, {"title": "6.2. Sub-System B: Jeffreys Loss", "content": "This system is identical to System A; however, we propose in\nthis system to replace the softmax loss function by Jeffreys loss\nfunction. The Jeffreys Loss is given in the Equation 1. Incor-\nporating this divergence into the cross-entropy loss function en-\nables the maximization of the target value of the output distri-\nbution while smoothing the non-target values.\n$L = -log (p_k) -\\frac{\\sum_{i\\neq k} p_i \\log p_i}{\\frac{\\alpha}{\\alpha + \\beta} (K-1)} + \\frac{1}{p_k}$"}, {"title": "6.3. Sub-System C: Joint Wav2vec2-WavLM", "content": "This system is based on a joint SSL audio strategy as depicted\nin Figure 2, wherein the upstream model is performed by jointly\ntraining a WavLM and Hubert SSL model. The input speech is\nfed into both SSL models. Fine-tuning of the upstream model\nis achieved throught simultaneous training alongside a straight-\nforward network that implements mean pooling, leading to a\nLinear Classifier, as illustrated in Figure 2."}, {"title": "6.4. Sub-System D: WavLM and ROBERTa", "content": "The following describes the SER model depicted in Figure 3.\nThe WavLM encoder is divided into two parts: the CNN feature\nextractor and the trasnformer-based encoder. We chose to freeze\nthe CNN feature extractor part, fixing all the parameters of these\nCNN layers and only fine-tune the parameters of transformer\nblocks. This method of partial fine-tuning acts as a strategy for\ndomain adaptation. It is designed to maintain the integral fea-\nture extraction capabilities of the lower layers, thereby enabling\nthe model to adjust to new tasks efficiently without compro-\nmising its overall performance. For the text encoder, we opt\nto fine-tune all parameters of the ROBERTa model. During the\nfine-tuning process, we apply three different schedulers to ad-\njust the fine-tuning learning rates of the WavLM and ROBERTa\nencoders, as well as the learning rate of the classifier model.\nEach scheduler utilizes the Adam Optimizer in conjunction with\nthe NewBob technique, which anneals the learning rates based\non the performance during the validation stage. The fine-tuning\nlearning rates for both the WavLM and ROBERTa encoders are\nset to 10-5, while the learning rate for the classifier model is\nset to 10-4. This model is trained using negative log-likelihood\nloss (NLL)."}, {"title": "6.5. Sub-System E : Data Augmentation", "content": "The system E is identical to the system D described in sec-\ntion 6.4. However, this system incorporates the data augmenta-\ntion technique outlined in Section 5.4, setting K = 50%. This\nadjustment aims to mitigate the notable imbalances observed in\nthe distribution of emotion classes."}, {"title": "7. Fusion systems", "content": "The fusion process integrates the outputs from all five sub-\nsystems (A, B, C, D, E) by concatenating them to form a sin-\ngle feature vector. This concatenated vector serves as the input\nto an SVM classifier, which is trained to predict the emotion\nclass. This fusion approach leverages the diverse strengths of\neach sub-system, aiming to enhance the overall performance\nand robustness of emotion classification."}, {"title": "8. Experiments", "content": "In this section, we describe our experimental setup, analyze re-\nsults from individual sub-systems and their combined fusion."}, {"title": "8.1. Experimental setup", "content": "To conduct our experiments, we employed the SpeechBrain\ntoolkit [22], which is built on PyTorch and is specifically\ndesigned for speech-processing tasks. Additionally, we uti-\nlized the Hugging Face versions of the WavLM, HUBERT, and\nROBERTa models. The source code is available on GitHub\u00b9."}, {"title": "8.2. Results", "content": "Impact of the Speech encoder: A first experiments aimed at\nidentifying the most effective Speech encoder. Various speech\nencoders, including Wav2Vec, Hubert and WavLM were eval-\nuated. These experiments were carried out on sub-system D.\nImpact of the Text encoder: An additional experiment was\nconducted using text as input. This experiment, as shown in\nTable 3, reveals that the performance of the RoBERTa text en-\ncoder, achieving a Macro-F1 score of 0.27, is not too far from\nthe best speech encoder, which has a Macro-F1 score of 0.32.\nThis result encourages the exploration of a joint speech and text\nemotion recognition system.\nImpact of the Fusion: Table 1 shows the results achieved by\nthe different sub-systems. We observe that all the sub-systems\nachievd F1-Macro scores between 0.32 and 0.34. The best sub-\nsystem is sub-system E. And we observe that fusion system led\nto an improvement, achieving an F1-Macro score of 0.35%.\nConfusion matrix: Figure 4 shows the confusion matrix ob-\ntained from fusion system. Regarding the diagonal elements,\nwe can observed that the fusion model is particularly effective\nat correctly predicting Neutral (N) and Happiness (H) classes.\nHowever, the fusion system struggles more with accurately pre-\ndicting the Disgust (D) and Fear (F) classes. As for the off-\ndiagonal elements, we can noted that there is a tendency for all\nclasses to be misclassified as Neutral (N) and Disgust (D) mis-\nclassified as Contempt (C)."}, {"title": "9. Conclusion", "content": "This paper describes LIA's participation in the MSP-Podcast\nSER Challenge. Our approach involves developing sub-\nsystems, each of which functions as an emotion classifier. These\nsub-systems model speech segments using different compo-\nnents to provide varied perspectives. For the final fusion step,\nwe concatenate the outputs of the sub-systems and train a SVM\nfor fusion. The fusion system achieved an F1-Macro score of\n0.35% on the development set."}]}